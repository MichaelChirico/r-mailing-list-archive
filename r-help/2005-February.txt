From spencer.graves at pdf.com  Tue Feb  1 00:00:29 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 31 Jan 2005 15:00:29 -0800
Subject: [R] Box-Cox / data transformation question
In-Reply-To: <t6osv09hq7mqoqhbo5f3k6ac8vhqgum542@4ax.com>
References: <41F65AE5.4020601@uni-jena.de>	<n75qv0l7rgshubin4k53c4v1d0fbco7gkt@4ax.com>	<41FD6403.8080505@nauticom.net>
	<t6osv09hq7mqoqhbo5f3k6ac8vhqgum542@4ax.com>
Message-ID: <41FEB88D.4030408@pdf.com>

      What R commands were used to produce this estimate of the required 
transformation?  Did the command used produce a confidence interval for 
the power transformation, as, e.g., "boxcox" in library(MASS) described 
by Venables and Ripley (2002) Modern Applied Statistics with S, 4th ed. 
(Springer, sec. 6.8, p. 172)? 

      In particular, did the confidence interval include more common 
numbers like 0 or 0.5 or 1?  If you describe the application a bit (but 
still briefly), that information along with the confidence interval 
might elicit other useful comments. 

      hope this helps.  spencer graves
p.s  PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html.  It  might help you 
formulate your question in a way that might elicit more useful answers. 

Landini Massimiliano wrote:

>On Sun, 30 Jan 2005 17:47:31 -0500, you wrote:
>
><<<<<-----------------SNIP
>|=[:o)  >
>|=[:o)  >  
>|=[:o)  >
>|=[:o)  Why are you using a double square root transformation? Is the 
>|=[:o)  transformation for the response variable? Transfromation is one way to 
>|=[:o)  help insure that the error distribution is at least approximately 
>|=[:o)  normal. So if this is the reason, it certainly could make sense. 
>
>Are you sure that (data^0.25) had sense??? Coud you explain me which is the
>sense??
>I know sense of boxcox exponents near zero when data are positively skewed and
>log(data) make it  normally distributed, or all those case where variances grow
>proportionally to means or when i know that there are interaction effects that
>not follow additive model (AnOVa assumption);
>
>I know 0.5 exponent (square root) [ as sqr(data) if all data differ from zero
>else sqr(data+.5) else Asconbe propose sqr(data +3/8) else Tukey & Freeman
>propose sqr(data)+sqr(data+1) particularly suitable when data domain is  (0,2) ]
>for right skewed data, frequently  applied to count-data or
>count-of-something-over -a -surface (bacteria, virus, nematode, lions) due to
>n*p*q (variance)  is almost proportional to its mean (n*p)  so AnOVa fundamental
>assumption is basically violated....
>
>I know 1/3 exponent  applied to count-of-something-in-a -volume...and so on...
>
>What is worth is that i'm trying to ask to Christoph to sit down and think: what
>kind of number are these??
>E.coli/mL?? ...so...i try cuberoot transformation and/or log transformation
>Timing of a slug vs snail speed race?? ...so..i think that inverse
>transformation it best.
>
>BoxCox procedure have produced a fantastic implement that can help many people
>but (IMHO) none procedure can be superior than Ripley + Bates + other gurus
>experience. If you ask to those great statisticians how do you manage
>electrophoresis velocity they could respond with "data^-1 why......blah blah
>blah"
>If you push data in BoxCox algorithm it will respond with "-0.97847164..."
>Which answer had more sense???
>I prefer -1
>
> 
>|=[:o) There  is no unique scale for making measurements. We choose a scale that helps 
>|=[:o)  us analyze the data appropriately.
>|=[:o)  
>|=[:o)  Rick B.
>|=[:o)  
>|=[:o)  ______________________________________________
>|=[:o)  R-help at stat.math.ethz.ch mailing list
>|=[:o)  https://stat.ethz.ch/mailman/listinfo/r-help
>|=[:o)  PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>|=[:o)  
>|=[:o)  
>|=[:o)  ---
>|=[:o)  avast! Antivirus: In arrivo messaggio pulito.
>|=[:o)  Virus Database (VPS): 0504-4, 28/01/2005
>|=[:o)  Controllato il: 31/01/2005 16.23.36
>|=[:o)  avast! - copyright (c) 1988-2004 ALWIL Software.
>|=[:o)  http://www.avast.com
>|=[:o)  
>|=[:o)  
>
>
>
>-------------------------------------------------------------------------------------------------------------------------
>Landini dr. Massimiliano
>Tel. mob. (+39) 347 140 11 94
>Tel./Fax. (+39) 051 762 196
>e-mail: numero (dot) primo (at) tele2 (dot) it
>-------------------------------------------------------------------------------------------------------------------------
>Legge di Hanggi: Pi? stupida ? la tua ricerca, pi? verr? letta e approvata.
>Corollario alla Legge di Hanggi: Pi? importante ? la tua ricerca, meno verr?
>capita.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From spencer.graves at pdf.com  Tue Feb  1 00:09:15 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 31 Jan 2005 15:09:15 -0800
Subject: [R] coercing a list to a data frame, lists in foreloops
In-Reply-To: <1107200761.41fe8af9d1859@webmail.uvm.edu>
References: <1107200761.41fe8af9d1859@webmail.uvm.edu>
Message-ID: <41FEBA9B.6060201@pdf.com>

      A data.frame is a list, which may be why "mode(mansNew)" was "list". 

      Have you tried something like the following: 

 > tstList <- list(a=1:3, b=c(NA, 4, 6))
 > DF <- as.data.frame(tstList)
 > mode(DF)
[1] "list"
 > class(DF)
[1] "data.frame"
 > DF
  a  b
1 1 NA
2 2  4
3 3  6
 > DF2 <- DF[c(1,1:3),]
 > DF2[2,] <- 8:9
 > DF2
    a  b
1   1 NA
1.1 8  9
2   2  4
3   3  6

      This may not work with time series, but it clearly worked in this 
simple example. 

      hope this helps.  spencer graves

Benjamin M. Osborne wrote:

>I have a set of time-series climate data with missing entries.  I need to add
>rows for these missing entries to this data set.  The only way I know to do
>this is unsing a foreloop, but this won't work on a list.  I've tried to
>convert the list to a data frame, but that won't happen, either.
>
>I want to fill rows in this table:
>
>  
>
>>newtest[10:15,]
>>    
>>
>    yrmos yearmo snow.sum snow.mean snow.dep.mean prcp.sum prcp.mean tmin.min
>10 195410     NA       NA        NA            NA       NA        NA       NA
>11 195411     NA       NA        NA            NA       NA        NA       NA
>12 195412     NA       NA        NA            NA       NA        NA       NA
>13 195501     NA       NA        NA            NA       NA        NA       NA
>14 195502     NA       NA        NA            NA       NA        NA       NA
>15 195503     NA       NA        NA            NA       NA        NA       NA
>   tmin.mean tmax.max tmax.mean tmean.mean
>10        NA       NA        NA         NA
>11        NA       NA        NA         NA
>12        NA       NA        NA         NA
>13        NA       NA        NA         NA
>14        NA       NA        NA         NA
>15        NA       NA        NA         NA
>  
>
>
>from this one:
>
>  
>
>>mansNew[10:15,]
>>    
>>
>   yearmo snow.sum snow.mean snow.dep.mean prcp.sum prcp.mean  tmin.min
>10 195508    0.000 0.0000000       0.00000  29.5910 0.9545484        NA
>11 195509    0.000 0.0000000       0.00000   9.1948 0.3064933        NA
>12 195510   20.320 0.6554839            NA  13.8684 0.4473677        NA
>13 195511       NA        NA            NA       NA        NA -18.88889
>14 195512   52.324 1.6878710      53.01226   6.4770 0.2089355        NA
>15 195601   46.736 1.5076129            NA   8.0264 0.2589161        NA
>   tmin.mean   tmax.max  tmax.mean tmean.mean
>10        NA         NA         NA         NA
>11        NA         NA         NA         NA
>12        NA         NA         NA         NA
>13  -8.62963 12.2222222 -0.6481481  -4.638889
>14        NA -0.5555556 -9.3906810         NA
>15        NA         NA         NA         NA
>  
>
>This may be a problem:
>  
>
>>newtest<-as.data.frame(newtest)
>>mode(newtest)  ## returns "list"
>>    
>>
>[1] "list"
>  
>
>>mansNew<-as.data.frame(mansNew)
>>mode(mansNew)  ## returns "list"
>>    
>>
>[1] "list"
>  
>
>I've checked to make sure each column is a vector, but the coercion still is not
>allowed.
>
>This is the code with which I'm attempting to perform this manipulation, as well
>as the result:
>
>  
>
>>for (i in 1:100){
>>    
>>
>+ newtest[i,2:12]<-ifelse(is.element(newtest$yrmos[i],mansNew$yearmo),
>subset(mansNew, yearmo == newtest$yrmos[i])[,1:11], c(rep(NA,11)))
>+ }
>  
>
>>newtest[10:15,]
>>    
>>
>    yrmos yearmo snow.sum snow.mean snow.dep.mean prcp.sum prcp.mean tmin.min
>10 195410     NA       NA        NA            NA       NA        NA       NA
>11 195411 195411   195411    195411        195411   195411    195411   195411
>12 195412 195412   195412    195412        195412   195412    195412   195412
>13 195501 195501   195501    195501        195501   195501    195501   195501
>14 195502 195502   195502    195502        195502   195502    195502   195502
>15 195503 195503   195503    195503        195503   195503    195503   195503
>   tmin.mean tmax.max tmax.mean tmean.mean
>10        NA       NA        NA         NA
>11    195411   195411    195411     195411
>12    195412   195412    195412     195412
>13    195501   195501    195501     195501
>14    195502   195502    195502     195502
>15    195503   195503    195503     195503
>  
>
>
>subset...  should return only one row.  This may be a simple comma problem, but
>I think it has something to do with the lists.  Also, if there is a way to do
>this without the foreloop, I'd be happy to hear about it.
>Any suggestions will be appreciated.
>
>Thanks,
>Ben Osborne
>
>  
>



From p.dalgaard at biostat.ku.dk  Tue Feb  1 00:05:28 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 01 Feb 2005 00:05:28 +0100
Subject: [R] Extracting a numeric prefix from a string
In-Reply-To: <XFMail.050131160949.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050131160949.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <x2y8e92o93.fsf@biostat.ku.dk>

(Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> writes:

> On 31-Jan-05 R user wrote:
> > You could use something like
> > 
> > y <- gsub('([0-9]+(.[0-9]+)?)?.*','\\1',x)
> > as.numeric(y)
> > 
> > But maybe there's a much nicer way.
> > 
> > Jonne.
> 
> I doubt it -- full marks for neat regexp footwork!

Hmm, I'd have to deduct a few points for forgetting to escape the dot...

> x <- "2a4"
> y <- gsub('([0-9]+(.[0-9]+)?)?.*','\\1',x)
> y
[1] "2a4"
>  as.numeric(y)
[1] NA
Warning message:
NAs introduced by coercion

and maybe a few more for using gsub() where sub() suffices.

There are a few more nits to pick, since "2.", ".2", "2e-7" are also
numbers, but ".", ".e-2" are not. In fact it seems quite hard even to
handle all cases in, e.g.,

 x <- c("2.2abc","2.def",".2ghi",".jkl")

with a single regular expression. The first one that worked for me was

> r <- regexpr('^(([0-9]+\\.?)|(\\.[0-9]+)|([0-9]+\\.[0-9]+))',x)
> substr(x,r,r+attr(r,"match.length")-1)
[1] "2.2" "2."  ".2"  ""

but several "obvious" attempts had failed.

The problem is that regular expressions try to find the
longest match, but not necessary of subexpressions, so

> sub('(([0-9]+\\.?)|(\\.[0-9]+)|([0-9]+\\.[0-9]+))?.*','\\1',x)
[1] "2." "2." ".2" ""

even though

> sub('(([0-9]+\\.?)|(\\.[0-9]+)|([0-9]+\\.[0-9]+))','XXX',x)
[1] "XXXabc" "XXXdef" "XXXghi" ".jkl"

Actually, this one comes pretty close:

> sub('([0-9]*(\\.[0-9]+)?)?.*','\\1',x)
[1] "2.2" "2"   ".2"  ""

It only loses a trailing dot which is immaterial in the present
context. However, next try extending the RE to handle an exponent
part... 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From MSchwartz at MedAnalytics.com  Tue Feb  1 00:18:22 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Mon, 31 Jan 2005 17:18:22 -0600
Subject: [R] Automatically Extracting F- and P- vals from ANOVA
In-Reply-To: <656a77fc0501311306735a87dd@mail.gmail.com>
References: <656a77fc0501311306735a87dd@mail.gmail.com>
Message-ID: <1107213502.16083.161.camel@horizons.localdomain>

On Mon, 2005-01-31 at 16:06 -0500, Uri wrote:
> Dear R community,
> 
> I'm currently using R to analyze functional Magnetic Resonance Imaging
> data.  Each analysis involves running ~120,000 repeated-measures
> ANOVAs.
> 
> I would like to know if there is any automatic way to access the F-
> and P-value data that are associated with each of these 120,000
> ANOVAs.
> 
> For example, if the summary output (for the 1st ANOVA of 120,000)
> shows the following value for factor "C", then I would like to
> automatically extract the F value (4.1) and P value (0.13) from the
> summary() output:
> 
> > summary(anova.1)
> 
> Error: S
>           Df Sum Sq Mean Sq F value Pr(>F)
> Residuals  1   12.5    12.5
> 
> Error: S:C
>           Df  Sum Sq Mean Sq F value Pr(>F)
> C          3 128.500  42.833  4.2131 0.1341
> Residuals  3  30.500  10.167
> 
> Of course, I would be interested only in factor "C".
> 
> I would then push these values to a hash of hashes (i.e, multi-dim
> array) that summarizes the results of all the ANOVAs.  {Anova1,
> (F=4.21), (P=0.13)} etc'.
> 
> I "dumped" a sample aov object, and my impression was that the F and P
> values for each effect are not hard coded, but constructed on the fly
> by the summary() function.
> 
> Is there a script or function that already does what I am looking for?
>  If not, my second option would be to sink the summary of each anova
> to a text file, and then post-process it, but this latter option is
> sub-optimal.
> 
> Best,
> Uri Hasson


It is actually a little complicated, but you are "most" of the way
there, in that your impression is correct. If you read through the code
for summary.aov, you will see how the output is generated.

Briefly, (since I do not have your data), let me use the output from the
final example in ?aov:

> summary(npk.aovE)

Error: block
          Df  Sum Sq Mean Sq F value Pr(>F)
N:P:K      1  37.002  37.002  0.4832 0.5252
Residuals  4 306.293  76.573

Error: Within
          Df  Sum Sq Mean Sq F value   Pr(>F)
N          1 189.282 189.282 12.2587 0.004372 **
P          1   8.402   8.402  0.5441 0.474904
K          1  95.202  95.202  6.1657 0.028795 *
N:P        1  21.282  21.282  1.3783 0.263165
N:K        1  33.135  33.135  2.1460 0.168648
P:K        1   0.482   0.482  0.0312 0.862752
Residuals 12 185.287  15.441
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1



If you look at the structure of the summary, you get:


> str(summary(npk.aovE))
List of 2
 $ Error: block :List of 1
  ..$ :Classes anova  and `data.frame': 2 obs. of  5 variables:
  .. ..$ Df     : num [1:2] 1 4
  .. ..$ Sum Sq : num [1:2]  37 306
  .. ..$ Mean Sq: num [1:2] 37.0 76.6
  .. ..$ F value: num [1:2] 0.483    NA
  .. ..$ Pr(>F) : num [1:2] 0.525    NA
  ..- attr(*, "class")= chr [1:2] "summary.aov" "listof"
 $ Error: Within:List of 1
  ..$ :Classes anova  and `data.frame': 7 obs. of  5 variables:
  .. ..$ Df     : num [1:7] 1 1 1 1 1 1 12
  .. ..$ Sum Sq : num [1:7] 189.3   8.4  95.2  21.3  33.1 ...
  .. ..$ Mean Sq: num [1:7] 189.3   8.4  95.2  21.3  33.1 ...
  .. ..$ F value: num [1:7] 12.259  0.544  6.166  1.378  2.146 ...
  .. ..$ Pr(>F) : num [1:7] 0.00437 0.47490 0.02880 0.26317 0.16865 ...
  ..- attr(*, "class")= chr [1:2] "summary.aov" "listof"
 - attr(*, "class")= chr "summary.aovlist"



Note that it is composed of two lists, each component of which is in
turn a list, containing a data frame.  Confused yet?  ;-)

Thus, you need to get down a couple of list levels, before you can
extract the elements in the data frame. Thus:

> summary(npk.aovE)[["Error: Within"]][[1]]

          Df  Sum Sq Mean Sq F value   Pr(>F)
N          1 189.282 189.282 12.2587 0.004372 **
P          1   8.402   8.402  0.5441 0.474904
K          1  95.202  95.202  6.1657 0.028795 *
N:P        1  21.282  21.282  1.3783 0.263165
N:K        1  33.135  33.135  2.1460 0.168648
P:K        1   0.482   0.482  0.0312 0.862752
Residuals 12 185.287  15.441
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1



Gives you just the second part of the output as a data frame. Now, use:


> summary(npk.aovE)[["Error: Within"]][[1]]["N:P" , "F value"]
[1] 1.378297

and

> summary(npk.aovE)[["Error: Within"]][[1]]["N:P" , "Pr(>F)"]
[1] 0.2631653


To get the F statistic and p value, respectively.


So, probably in your aov models, you could use something like:

summary(anova.1)[["Error: S:C""][[1]]["C", "F value"]

and 

summary(anova.1)[["Error: S:C""][[1]]["C", "Pr(>F)"]

to get what you want.

If it helps to keep things straight, you could assign the intermediate
extraction to a data frame and then subset if from there. So something
like:

df <- summary(anova.1)[["Error: S:C""][[1]]
df["C", "F value"]

should probably work also.

HTH,

Marc Schwartz
< Now I need a wee bit of single malt after that... ;-) >



From p.dalgaard at biostat.ku.dk  Tue Feb  1 00:22:03 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 01 Feb 2005 00:22:03 +0100
Subject: [R] ML-Fit for truncated distributions
In-Reply-To: <001f01c507cf$cd6218b0$8cac4c86@okuell1>
References: <001f01c507cf$cd6218b0$8cac4c86@okuell1>
Message-ID: <x2u0ox2nhg.fsf@biostat.ku.dk>

"Carsten Steinhoff" <carsten.steinhoff at stud.uni-goettingen.de> writes:

> Hello,
> 
>  
> 
> maybe that my Question is a "beginner"-Question, but up to now, my research
> didn't bring any useful result.
> 
>  
> 
> I'm trying to fit a distribution (e.g. lognormal) to a given set of data
> (ML-Estimation). I KNOW about my data that there is a truncation for all
> data below a well known threshold. Is there an R-solution for an
> ML-estimation for this kind of data-problem? As far as I've seen the
> "fitdistr" in package "MASS" doesn't solve this problem.
> 
>  
> 
> Thank you for any information!

Truncated or censored? (i.e. do you know that an observation is below
threshold or is such an observation just never seen?)

If it really is truncated, just use fitdistr on 

function(x,m,s) dlnorm(x, m, s) / plnorm(threshhold, m, s,
          lower.tail=FALSE)

if censored, use fitdistr on

function(x,m,s) ifelse(x > threshold, dlnorm(x, m, s),
                                      plnorm(threshhold, m, s)

[which, for the mathematically inclined, *is* a density w.r.t. the sum
of a continuous measure on a half-line and a point measure at the
threshold]

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Robert.McGehee at geodecapital.com  Tue Feb  1 00:49:38 2005
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Mon, 31 Jan 2005 18:49:38 -0500
Subject: [R] Extracting a numeric prefix from a string
Message-ID: <67DCA285A2D7754280D3B8E88EB5480206741E49@MSGBOSCLB2WIN.DMN1.FMR.COM>

Perhaps an easier way would be to throw away the offending text at the
end of the strings, rather than matching all possible numeric
formulations at the beginning of the string, that is:

sub("\\.*[[:alpha:]]+$", "", x)

Easier to read, if nothing else, and it allows for 2e-7 as a valid
number. This however (I think correctly) assumes that there aren't
numbers in the middle of the string, i.e. 2a3b.

Robert

-----Original Message-----
From: Peter Dalgaard [mailto:p.dalgaard at biostat.ku.dk] 
Sent: Monday, January 31, 2005 6:05 PM
To: ted.harding at nessie.mcc.ac.uk
Cc: R user; R-help at stat.math.ethz.ch; Mike White
Subject: Re: [R] Extracting a numeric prefix from a string


(Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> writes:

> On 31-Jan-05 R user wrote:
> > You could use something like
> > 
> > y <- gsub('([0-9]+(.[0-9]+)?)?.*','\\1',x)
> > as.numeric(y)
> > 
> > But maybe there's a much nicer way.
> > 
> > Jonne.
> 
> I doubt it -- full marks for neat regexp footwork!

Hmm, I'd have to deduct a few points for forgetting to escape the dot...

> x <- "2a4"
> y <- gsub('([0-9]+(.[0-9]+)?)?.*','\\1',x)
> y
[1] "2a4"
>  as.numeric(y)
[1] NA
Warning message:
NAs introduced by coercion

and maybe a few more for using gsub() where sub() suffices.

There are a few more nits to pick, since "2.", ".2", "2e-7" are also
numbers, but ".", ".e-2" are not. In fact it seems quite hard even to
handle all cases in, e.g.,

 x <- c("2.2abc","2.def",".2ghi",".jkl")

with a single regular expression. The first one that worked for me was

> r <- regexpr('^(([0-9]+\\.?)|(\\.[0-9]+)|([0-9]+\\.[0-9]+))',x)
> substr(x,r,r+attr(r,"match.length")-1)
[1] "2.2" "2."  ".2"  ""

but several "obvious" attempts had failed.

The problem is that regular expressions try to find the
longest match, but not necessary of subexpressions, so

> sub('(([0-9]+\\.?)|(\\.[0-9]+)|([0-9]+\\.[0-9]+))?.*','\\1',x)
[1] "2." "2." ".2" ""

even though

> sub('(([0-9]+\\.?)|(\\.[0-9]+)|([0-9]+\\.[0-9]+))','XXX',x)
[1] "XXXabc" "XXXdef" "XXXghi" ".jkl"

Actually, this one comes pretty close:

> sub('([0-9]*(\\.[0-9]+)?)?.*','\\1',x)
[1] "2.2" "2"   ".2"  ""

It only loses a trailing dot which is immaterial in the present
context. However, next try extending the RE to handle an exponent
part... 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Paul.Sorenson at vision-bio.com  Tue Feb  1 01:19:07 2005
From: Paul.Sorenson at vision-bio.com (Paul Sorenson)
Date: Tue, 1 Feb 2005 11:19:07 +1100
Subject: [R] RE: aggregating dates
Message-ID: <5E06BFED29594F4C9C5EBE230DE320C6068027CF@ewok.vsl.com.au>

The solution I came up with myself was simply to coerce the integer back to POSIXct:

	class(ev$date) = "POSIXct"

Can't say it is the right way to do it but it seem to work.

A second related problem I haven't been able to solve as yet is how to include "incidents" columns (those not in 'x' or 'by') in an aggregate.

	names(ev): "date" "defectnum" "state"
 
	aggregate(ev$date, by=list(ev$defectnum), max)

This returns only the date and defectnum, I also need the state.

I tried writing my own aggregator function:
	maxevent = function(events) {
	    events[which.max(events$date),]
	}

	aggregate(ev, by=list(ev$defectnum), maxevent)

But I get:

	Error in "[.default"(events, which.max(events$date), ) : 
      	  incorrect number of dimensions

I am trying to retrieve only the rows of ev with the latest date for a given defectnum.

cheers

> Message: 29
> Date: Mon, 31 Jan 2005 16:16:35 +1100
> From: "Paul Sorenson" <Paul.Sorenson at vision-bio.com>
> Subject: [R] aggregating dates
> To: <r-help at stat.math.ethz.ch>
> Message-ID: <5E06BFED29594F4C9C5EBE230DE320C6068027CD at ewok.vsl.com.au>
> Content-Type: text/plain;	charset="iso-8859-1"
> 
> I have a frame which contains 3 columns:
> 
> "date" "defectnum" "state"
> 
> And I want to get the most recent state change for a given 
> defect number.  date is POSIXct.
> 
> I have tried:
> 	aggregate(ev$date, by=list(ev$defectnum), max)
> 
> Which appears to be working except that the dates seem to 
> come back as integers (presumably the internal representation 
> of POSIXct).
> 
> When I execute max(ev$date) the result remains POSIXct.
> 
> I have been dredging through the help among DateTimeClasses 
> and haven't found a function that converts these integers to 
> some kind of date class.  Or a method for using aggregate 
> which doesn't perform the conversion in the first place.
> 
> Any clues?



From Simon.Blomberg at anu.edu.au  Tue Feb  1 01:35:48 2005
From: Simon.Blomberg at anu.edu.au (Simon Blomberg)
Date: Tue, 1 Feb 2005 11:35:48 +1100
Subject: [R] an R script editor for Mac
In-Reply-To: <200501221557.j0MFv6Ci029544@ms-smtp-04-eri0.southeast.rr.com>
References: <200501221557.j0MFv6Ci029544@ms-smtp-04-eri0.southeast.rr.com>
Message-ID: <a06110400be247b00b87a@[150.203.51.113]>

I'm surprised nobody has mentioned Alpha. It has highlighting, 
indenting, parenthesis matching, excellent integration with R (or 
that other commercial version of R). There are versions for Classic 
(Alpha8), OS X (Alphax), as well as *NIX and Windows (AlphatTk). 
Alpha is shareware, based on the open source AlphTcl library.

http://alphatcl.sourceforge.net/wikit/

Cheers,

Simon.

>
>
>On Jan 21, 2005, at 5:35 AM, Jacques VESLOT wrote:
>
>>  Dear all,
>>
>>  Could someone please make me know if there is a nice script editor
>>  available
>>  under Mac, similar to Crimson, that offers R syntax highlighting (and
>>  pairs
>>  of parentheses underlining) ?
>>
>>  Thanks in advance,
>>
>>  Jacques VESLOT
>>  Cirad
>>
>>  ______________________________________________
>  > R-help at stat.math.ethz.ch mailing list
>>  https://stat.ethz.ch/mailman/listinfo/r-help
>>  PLEASE do read the posting guide!
>  > http://www.R-project.org/posting-guide.html
>

-- 
Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat.
Visiting Fellow
School of Botany & Zoology
The Australian National University
Canberra ACT 0200
Australia

T: +61 2 6125 8057  email: Simon.Blomberg at anu.edu.au
F: +61 2 6125 5573

CRICOS Provider # 00120C



From ggrothendieck at myway.com  Tue Feb  1 04:44:04 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 1 Feb 2005 03:44:04 +0000 (UTC)
Subject: [R] coercing a list to a data frame, lists in foreloops
References: <1107200761.41fe8af9d1859@webmail.uvm.edu>
Message-ID: <loom.20050201T043948-838@post.gmane.org>

Benjamin M. Osborne <Benjamin.Osborne <at> uvm.edu> writes:

: 
: I have a set of time-series climate data with missing entries.  I need to add
: rows for these missing entries to this data set.  The only way I know to do
: this is unsing a foreloop, but this won't work on a list.  I've tried to
: convert the list to a data frame, but that won't happen, either.
: 
: I want to fill rows in this table:
: 
: > newtest[10:15,]
:     yrmos yearmo snow.sum snow.mean snow.dep.mean prcp.sum prcp.mean tmin.min
: 10 195410     NA       NA        NA            NA       NA        NA       NA
: 11 195411     NA       NA        NA            NA       NA        NA       NA
: 12 195412     NA       NA        NA            NA       NA        NA       NA
: 13 195501     NA       NA        NA            NA       NA        NA       NA
: 14 195502     NA       NA        NA            NA       NA        NA       NA
: 15 195503     NA       NA        NA            NA       NA        NA       NA
:    tmin.mean tmax.max tmax.mean tmean.mean
: 10        NA       NA        NA         NA
: 11        NA       NA        NA         NA
: 12        NA       NA        NA         NA
: 13        NA       NA        NA         NA
: 14        NA       NA        NA         NA
: 15        NA       NA        NA         NA
: >
: 
: from this one:
: 
: > mansNew[10:15,]
:    yearmo snow.sum snow.mean snow.dep.mean prcp.sum prcp.mean  tmin.min
: 10 195508    0.000 0.0000000       0.00000  29.5910 0.9545484        NA
: 11 195509    0.000 0.0000000       0.00000   9.1948 0.3064933        NA
: 12 195510   20.320 0.6554839            NA  13.8684 0.4473677        NA
: 13 195511       NA        NA            NA       NA        NA -18.88889
: 14 195512   52.324 1.6878710      53.01226   6.4770 0.2089355        NA
: 15 195601   46.736 1.5076129            NA   8.0264 0.2589161        NA
:    tmin.mean   tmax.max  tmax.mean tmean.mean
: 10        NA         NA         NA         NA
: 11        NA         NA         NA         NA
: 12        NA         NA         NA         NA
: 13  -8.62963 12.2222222 -0.6481481  -4.638889
: 14        NA -0.5555556 -9.3906810         NA
: 15        NA         NA         NA         NA
: >
: This may be a problem:
: > newtest<-as.data.frame(newtest)
: > mode(newtest)  ## returns "list"
: [1] "list"
: >
: > mansNew<-as.data.frame(mansNew)
: > mode(mansNew)  ## returns "list"
: [1] "list"
: >
: I've checked to make sure each column is a vector, but the coercion still is 
not
: allowed.
: 
: This is the code with which I'm attempting to perform this manipulation, as 
well
: as the result:
: 
: > for (i in 1:100){
: + newtest[i,2:12]<-ifelse(is.element(newtest$yrmos[i],mansNew$yearmo),
: subset(mansNew, yearmo == newtest$yrmos[i])[,1:11], c(rep(NA,11)))
: + }
: > newtest[10:15,]
:     yrmos yearmo snow.sum snow.mean snow.dep.mean prcp.sum prcp.mean tmin.min
: 10 195410     NA       NA        NA            NA       NA        NA       NA
: 11 195411 195411   195411    195411        195411   195411    195411   195411
: 12 195412 195412   195412    195412        195412   195412    195412   195412
: 13 195501 195501   195501    195501        195501   195501    195501   195501
: 14 195502 195502   195502    195502        195502   195502    195502   195502
: 15 195503 195503   195503    195503        195503   195503    195503   195503
:    tmin.mean tmax.max tmax.mean tmean.mean
: 10        NA       NA        NA         NA
: 11    195411   195411    195411     195411
: 12    195412   195412    195412     195412
: 13    195501   195501    195501     195501
: 14    195502   195502    195502     195502
: 15    195503   195503    195503     195503
: >
: 
: subset...  should return only one row.  This may be a simple comma problem, 
but
: I think it has something to do with the lists.  Also, if there is a way to do
: this without the foreloop, I'd be happy to hear about it.
: Any suggestions will be appreciated.

The zoo package has facilities for this:

R> library(zoo)

R> # create a 3x2 zoo matrix whose rows correspond to time points 1,3,4
R> z <- zoo(matrix(1:6, 3), c(1,3,4))

R> # merge z with 0 dimensional zoo series having time points 1,2,3,4
R> merge(z, zoo(,c(1,2,3,4)))
  z.1 z.2
1  1   4 
2 NA  NA 
3  2   5 
4  3   6



From ggrothendieck at myway.com  Tue Feb  1 04:48:02 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 1 Feb 2005 03:48:02 +0000 (UTC)
Subject: [R] RE: aggregating dates
References: <5E06BFED29594F4C9C5EBE230DE320C6068027CF@ewok.vsl.com.au>
Message-ID: <loom.20050201T044426-937@post.gmane.org>

Paul Sorenson <Paul.Sorenson <at> vision-bio.com> writes:

: 
: The solution I came up with myself was simply to coerce the integer back to 
POSIXct:
: 
: 	class(ev$date) = "POSIXct"
: 
: Can't say it is the right way to do it but it seem to work.

That is not entirely correct, although it many cases it will work,
since its also a subclass of POSIXt.  

The RNews article I mentioned in my original response to your
query has the correct solution.

: 
: A second related problem I haven't been able to solve as yet is how to

My original response to your query already had a solution.



: include "incidents" columns (those not
: in 'x' or 'by') in an aggregate.
: 
: 	names(ev): "date" "defectnum" "state"
: 
: 	aggregate(ev$date, by=list(ev$defectnum), max)
: 
: This returns only the date and defectnum, I also need the state.
: 
: I tried writing my own aggregator function:
: 	maxevent = function(events) {
: 	    events[which.max(events$date),]
: 	}
: 
: 	aggregate(ev, by=list(ev$defectnum), maxevent)
: 
: But I get:
: 
: 	Error in "[.default"(events, which.max(events$date), ) : 
:       	  incorrect number of dimensions
: 
: I am trying to retrieve only the rows of ev with the latest date for a given 
defectnum.
: 
: cheers
: 
: > Message: 29
: > Date: Mon, 31 Jan 2005 16:16:35 +1100
: > From: "Paul Sorenson" <Paul.Sorenson <at> vision-bio.com>
: > Subject: [R] aggregating dates
: > To: <r-help <at> stat.math.ethz.ch>
: > Message-ID: <5E06BFED29594F4C9C5EBE230DE320C6068027CD <at> ewok.vsl.com.au>
: > Content-Type: text/plain;	charset="iso-8859-1"
: > 
: > I have a frame which contains 3 columns:
: > 
: > "date" "defectnum" "state"
: > 
: > And I want to get the most recent state change for a given 
: > defect number.  date is POSIXct.
: > 
: > I have tried:
: > 	aggregate(ev$date, by=list(ev$defectnum), max)
: > 
: > Which appears to be working except that the dates seem to 
: > come back as integers (presumably the internal representation 
: > of POSIXct).
: > 
: > When I execute max(ev$date) the result remains POSIXct.
: > 
: > I have been dredging through the help among DateTimeClasses 
: > and haven't found a function that converts these integers to 
: > some kind of date class.  Or a method for using aggregate 
: > which doesn't perform the conversion in the first place.
: > 
: > Any clues?
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://stat.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
: 
:



From ggrothendieck at myway.com  Tue Feb  1 05:05:40 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 1 Feb 2005 04:05:40 +0000 (UTC)
Subject: [R] Help about time series
References: <4BFA0FD18E9ED311AC040000F6AF048905EC6B58@S54X01>
Message-ID: <loom.20050201T050346-854@post.gmane.org>

Ferrari Nicolas <nicolas.ferrari <at> insee.fr> writes:

: 
: Hello,
: 
: When I create a ts object, I would like to get a particular value of this
: time serie according to the date and not the rank. 
: However, it seems necessary to use the rank as if it were a simple vector.


Try this:

oneval <- function(series, date) as.vector(window(series, date, date))

# test
ser <- ts(21:25, 101)
oneval(ser, 102)



From knulp109 at gmail.com  Tue Feb  1 06:22:00 2005
From: knulp109 at gmail.com (knulp)
Date: Tue, 1 Feb 2005 00:22:00 -0500
Subject: [R] GARCH, installing tserise package
Message-ID: <9bc012b5050131212234b81cea@mail.gmail.com>

Hi, there,

First of all, I am not familar with the GARCH concept as well as R interface. 

I have three questions regarding GARCH (1,1).

First, I got tseries package from CRAN. Where exactly am I supposed to
install it?
In my case, I copies it to C:\Program Files\R\rw2001\library. What do
I need to do in order to actually install it?


Second, I am not sure if I've installed it properly, but I've added
"garch" function into my current R workspace. But, I got the following
error message.

> x.garch <- garch(x, order = c(1,1)) 
Error in .C("fit_garch", as.vector(x, mode = "double"), as.integer(n),  : 
        C function name not in DLL for package tseries

I was running the following GARCH (1,1) model.

> n <- 1100
> a <- c(0.01178, 0.003, 0.005)  # ARCH(1) coefficients, assuming the mean is constant!!
> e <- rnorm(n)
> x <- double(n)
> v <- double(n)
> v[1] <- a[1]/(1.0-a[2]-a[3]) 
> x[1] <- rnorm(1, sd = sqrt(v[1])) 
> v[1]
[1] 0.011875
> x[1]
[1] 0.1037611
> for(i in 2:n) { 
+     v[i] <- a[1]+a[2]*x[i-1]^2+a[3]*v[i-1] 
+     x[i] <- e[i]*sqrt(v[i]) 
+ } 
>
> x <- ts(x[101:1100]) 
>
> x.garch <- garch(x, order = c(1,1)) 

Third, as for setting 'n' and 'x <- ts(x[101, 1100])' 
What does 'n=1100' stand for? I mean, why is it 1100  in the above
example? Any why does 'x <- ts(x[101:1100]) ' start from 101 th to
1100? Does the function assume the 1100 periods? then, why does it
start from 101 for 'ts' function?

Sorry for the idiotic questions above, but they have driven me crazy.

Looking forward to seeing your kind, easy to understand help.
 
Knulp



From knulp109 at gmail.com  Tue Feb  1 06:22:00 2005
From: knulp109 at gmail.com (knulp)
Date: Tue, 1 Feb 2005 00:22:00 -0500
Subject: [R] GARCH, installing tserise package
Message-ID: <9bc012b5050131212234b81cea@mail.gmail.com>

Hi, there,

First of all, I am not familar with the GARCH concept as well as R interface. 

I have three questions regarding GARCH (1,1).

First, I got tseries package from CRAN. Where exactly am I supposed to
install it?
In my case, I copies it to C:\Program Files\R\rw2001\library. What do
I need to do in order to actually install it?


Second, I am not sure if I've installed it properly, but I've added
"garch" function into my current R workspace. But, I got the following
error message.

> x.garch <- garch(x, order = c(1,1)) 
Error in .C("fit_garch", as.vector(x, mode = "double"), as.integer(n),  : 
        C function name not in DLL for package tseries

I was running the following GARCH (1,1) model.

> n <- 1100
> a <- c(0.01178, 0.003, 0.005)  # ARCH(1) coefficients, assuming the mean is constant!!
> e <- rnorm(n)
> x <- double(n)
> v <- double(n)
> v[1] <- a[1]/(1.0-a[2]-a[3]) 
> x[1] <- rnorm(1, sd = sqrt(v[1])) 
> v[1]
[1] 0.011875
> x[1]
[1] 0.1037611
> for(i in 2:n) { 
+     v[i] <- a[1]+a[2]*x[i-1]^2+a[3]*v[i-1] 
+     x[i] <- e[i]*sqrt(v[i]) 
+ } 
>
> x <- ts(x[101:1100]) 
>
> x.garch <- garch(x, order = c(1,1)) 

Third, as for setting 'n' and 'x <- ts(x[101, 1100])' 
What does 'n=1100' stand for? I mean, why is it 1100  in the above
example? Any why does 'x <- ts(x[101:1100]) ' start from 101 th to
1100? Does the function assume the 1100 periods? then, why does it
start from 101 for 'ts' function?

Sorry for the idiotic questions above, but they have driven me crazy.

Looking forward to seeing your kind, easy to understand help.
 
Knulp



From Tom.Mulholland at dpi.wa.gov.au  Tue Feb  1 07:56:56 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Tue, 1 Feb 2005 14:56:56 +0800
Subject: [R] RE: aggregating dates
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA7B@afhex01.dpi.wa.gov.au>

I am probably just displaying my ignorance, but I have obviously managed to miss exactly what you are referring to.

Firstly I have to thank you for making me look closer at the article. I had done so but I had obviously skipped over the Comparison table, which I would have found useful in the past. I, like many others have struggled with dates and because I use fSeries etc I have started to used POSIXct as my default way of including dates within my datasets.  (That's where the table might have stopped me if I had read it the first time around)

But my issue is that you say that the article tells you how to convert from an Integer into POSIXct. In case it makes a difference I think the question is how do you make 1104508800 into "2005-01-01 W. Australia Standard Time"

Is the answer, structure(1104508800,class = c("POSIXt","POSIXct"))?

Following Paul's message I tried 

> y <- 1104508800
> class(y) = "POSIXct"
> y
[1] "2005-01-01 W. Australia Standard Time"
> str(y)
Class 'POSIXct'  num 1104508800

> y <- ISOdate(2005,1,1)
> str(y)
`POSIXct', format: chr "2005-01-01 12:00:00"

So checking the table in your article I tought this may shed some light on the topic

> newy <- structure(1104508800,class = c("POSIXt","POSIXct"))
> newy
[1] "2005-01-01 W. Australia Standard Time"
> str(newy)
`POSIXct', format: chr "2005-01-01"

It's the same format as ISOdate so I assume it's the right way to do the conversion.

You suggested that the simple 'class(y) = "POSIXct"' might fail. Since I thought that maybe plot functions might be interfered with I ran

> x <- runif(3)
> y <- as.POSIXct(c("2005-01-01","2005-01-02","2005-01-03"))
> plot(y,x)
NULL
> y <- as.numeric(as.POSIXct(c("2005-01-01","2005-01-02","2005-01-03")))
> class(y) = "POSIXct"
> plot(y,x)
NULL

The plots seemed to be both the same so I am not sure what the implications of your statement is.

I hope this makes sense, because as usual the topic seems to go around in circles. It all makes complete sense and nonsense at the same time. Because rightly or wrongly it seems to me that class(y) <- c("POSIXt","POSIXct") is the same thing as the structure statement. So is it just failing to include the POSIXt that is the issue.

> 

> -----Original Message-----
> From: Gabor Grothendieck [mailto:ggrothendieck at myway.com]
> Sent: Tuesday, 1 February 2005 11:48 AM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] RE: aggregating dates
> 
> 
> Paul Sorenson <Paul.Sorenson <at> vision-bio.com> writes:
> 
> : 
> : The solution I came up with myself was simply to coerce the 
> integer back to 
> POSIXct:
> : 
> : 	class(ev$date) = "POSIXct"
> : 
> : Can't say it is the right way to do it but it seem to work.
> 
> That is not entirely correct, although it many cases it will work,
> since its also a subclass of POSIXt.  
> 
> The RNews article I mentioned in my original response to your
> query has the correct solution.
> 
> : 
> : A second related problem I haven't been able to solve as 
> yet is how to
> 
> My original response to your query already had a solution.
> 
> 
> 
> : include "incidents" columns (those not
> : in 'x' or 'by') in an aggregate.
> : 
> : 	names(ev): "date" "defectnum" "state"
> : 
> : 	aggregate(ev$date, by=list(ev$defectnum), max)
> : 
> : This returns only the date and defectnum, I also need the state.
> : 
> : I tried writing my own aggregator function:
> : 	maxevent = function(events) {
> : 	    events[which.max(events$date),]
> : 	}
> : 
> : 	aggregate(ev, by=list(ev$defectnum), maxevent)
> : 
> : But I get:
> : 
> : 	Error in "[.default"(events, which.max(events$date), ) : 
> :       	  incorrect number of dimensions
> : 
> : I am trying to retrieve only the rows of ev with the latest 
> date for a given 
> defectnum.
> : 
> : cheers
> : 
> : > Message: 29
> : > Date: Mon, 31 Jan 2005 16:16:35 +1100
> : > From: "Paul Sorenson" <Paul.Sorenson <at> vision-bio.com>
> : > Subject: [R] aggregating dates
> : > To: <r-help <at> stat.math.ethz.ch>
> : > Message-ID: <5E06BFED29594F4C9C5EBE230DE320C6068027CD 
> <at> ewok.vsl.com.au>
> : > Content-Type: text/plain;	charset="iso-8859-1"
> : > 
> : > I have a frame which contains 3 columns:
> : > 
> : > "date" "defectnum" "state"
> : > 
> : > And I want to get the most recent state change for a given 
> : > defect number.  date is POSIXct.
> : > 
> : > I have tried:
> : > 	aggregate(ev$date, by=list(ev$defectnum), max)
> : > 
> : > Which appears to be working except that the dates seem to 
> : > come back as integers (presumably the internal representation 
> : > of POSIXct).
> : > 
> : > When I execute max(ev$date) the result remains POSIXct.
> : > 
> : > I have been dredging through the help among DateTimeClasses 
> : > and haven't found a function that converts these integers to 
> : > some kind of date class.  Or a method for using aggregate 
> : > which doesn't perform the conversion in the first place.
> : > 
> : > Any clues?
> : 
> : ______________________________________________
> : R-help <at> stat.math.ethz.ch mailing list
> : https://stat.ethz.ch/mailman/listinfo/r-help
> : PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> : 
> :
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Tue Feb  1 09:10:51 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 1 Feb 2005 08:10:51 +0000 (GMT)
Subject: [R] RE: aggregating dates
In-Reply-To: <5E06BFED29594F4C9C5EBE230DE320C6068027CF@ewok.vsl.com.au>
References: <5E06BFED29594F4C9C5EBE230DE320C6068027CF@ewok.vsl.com.au>
Message-ID: <Pine.LNX.4.61.0502010801080.23610@gannet.stats>

On Tue, 1 Feb 2005, Paul Sorenson wrote:

> The solution I came up with myself was simply to coerce the integer back 
> to POSIXct:
>
> 	class(ev$date) = "POSIXct"
>
> Can't say it is the right way to do it but it seem to work.

It will work partially.  The correct class is c("POSIXt", "POSIXct").
As I believe others have said,  ISOdate(1970,1,1) + x  is safer.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From chenyong.cn at gmail.com  Tue Feb  1 11:57:39 2005
From: chenyong.cn at gmail.com (Willie Y. CHEN)
Date: Tue, 1 Feb 2005 18:57:39 +0800
Subject: [R] How to get a table of MySQL database as a matrix variable in R
Message-ID: <879446cd0502010257d84c408@mail.gmail.com>

It seems that the dbReadTable() method provided by RMySQL could not
get rid of the headers, neither the index column... So

1. Can I use read.table() method to get a table of MySQL database?
>From the help document the argument of 'file' of read.table() method
could be a connection...

2. How can I obtain a matrix from the database table contains all
elements except the headers and the index column?

Thanks a lot.

Regards,

Willie



From sdavis2 at mail.nih.gov  Tue Feb  1 12:04:40 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Tue, 1 Feb 2005 06:04:40 -0500
Subject: [R] installing R on Mac OS X
In-Reply-To: <1f8f0068d56a5423cad3a829d6e01caa@ing.unitn.it>
References: <b932ca9de8e5c5e328f62ae218df9854@ing.unitn.it>
	<E7DB1579-73BB-11D9-9CDE-000D933565E8@mail.nih.gov>
	<1f8f0068d56a5423cad3a829d6e01caa@ing.unitn.it>
Message-ID: <11909D34-7441-11D9-B225-000D933565E8@mail.nih.gov>

Silvia

Are you talking of specific packages?  Installing R from the Disk Image 
does not preclude building individual packages from source.  As for 
your original error, I'm not sure what the problem is, but you are 
probably correct in assuming it may be a setup problem, but I can't 
tell for sure.

Sean

On Feb 1, 2005, at 3:04 AM, silvia simoni wrote:

> The thing is i do need the source code because i need some packages 
> which are not available as binanary code for mac and whern i try to 
> install them, they look for a file .o that is not there.
>
> silvia
>
> Il giorno 31/gen/05, alle 20:11, Sean Davis ha scritto:
>
>> Silvia
>>
>> You may want to use the disk image instead.  It is here (for 
>> italy....):
>>
>> http://microarrays.unife.it/CRAN/bin/macosx/R-2.0.1.dmg
>>
>> Sean
>>
>> On Jan 31, 2005, at 1:52 PM, silvia simoni wrote:
>>
>>> I have a problem in installing R-2.0.1, downloaded from the R web 
>>> site, on mac o sx version 10.3.7.
>>> when i launch the command ./configure i get the following error 
>>> message:
>>>
>>>  checking for C compiler default output file name... configure: 
>>> error: C compiler cannot create executables
>>>
>>> i've aleady insalled Xcode. the version of the gcc in 3.3.
>>> what can i do?
>>> thanks
>>>
>>> silvia
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>
>>



From v.demartino2 at virgilio.it  Tue Feb  1 12:05:36 2005
From: v.demartino2 at virgilio.it (v.demartino2@virgilio.it)
Date: Tue, 1 Feb 2005 12:05:36 +0100
Subject: [R] Rcmdr doesn't seem to work
Message-ID: <41536B8500162309@ims3e.cp.tin.it>

Context: Windows XP - R 2.0.1 with the latest updated packages (including
Rcmdr) 

I'm trying to load Rcmdr to use the 3d plots (e.g. scatter3d) but here it
is what happens:

> library(Rcmdr)
Loading required package: zoo 
Loading required package: strucchange 
Loading required package: sandwich 
Loading required package: relimp 
Loading required package: mvtnorm 
Loading required package: multcomp 
Loading required package: lmtest 
Loading required package: effects 
Loading required package: car 
Loading required package: abind 
Loading required package: rgl 
Error in library(package, character.only = TRUE, logical = TRUE, warn.conflicts
= warn.conflicts,  : 
	'rgl' is not a valid package -- installed < 2.0.0?
Error: .onLoad failed in loadNamespace for 'Rcmdr'
Error in library(Rcmdr) : package/namespace load failed for 'Rcmdr'

What should I do?

Vittorio



From sdavis2 at mail.nih.gov  Tue Feb  1 12:21:33 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Tue, 1 Feb 2005 06:21:33 -0500
Subject: [R] How to get a table of MySQL database as a matrix variable in R
In-Reply-To: <879446cd0502010257d84c408@mail.gmail.com>
References: <879446cd0502010257d84c408@mail.gmail.com>
Message-ID: <6DA78740-7443-11D9-B225-000D933565E8@mail.nih.gov>


On Feb 1, 2005, at 5:57 AM, Willie Y. CHEN wrote:

> It seems that the dbReadTable() method provided by RMySQL could not
> get rid of the headers, neither the index column... So
>

You can use dbSendQuery and fetch to get your results without the index 
column.  If you have a table with columns called col1, col2, col3, and 
indexcol, you could do something like:

drv <- dbDriver("MySQL")
con <- dbConnect(drv, "usr", "password", "dbname")
res <- dbSendQuery(con, "SELECT col1,col2,col3 from mytable")
data <- fetch(res, n = -1)

data will now be a dataframe with columns named col1,col2,and col3.  If 
you want it as a matrix, you can simply do:

my.matrix <- as.matrix(data)

Alternatively, if you use dbReadTable, your result will always be a 
dataframe.  If you want a matrix and your index column is in the first 
column of the dataframe, you can do:

my.matrix <- as.matrix(data[,-1])


> 1. Can I use read.table() method to get a table of MySQL database?
>> From the help document the argument of 'file' of read.table() method
> could be a connection...

No, you cannot use read.table to read from a MySQL database.  You could 
dump the table to disk as a tab-delimited text file and then read it, 
but that defeats the purpose of using RDBMS for data storage and 
retrieval

>
> 2. How can I obtain a matrix from the database table contains all
> elements except the headers and the index column?
>

See above for coercing a dataframe to a matrix.

> Thanks a lot.
>
> Regards,
>
> Willie
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Tue Feb  1 12:52:14 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 01 Feb 2005 12:52:14 +0100
Subject: [R] Rcmdr doesn't seem to work
In-Reply-To: <41536B8500162309@ims3e.cp.tin.it>
References: <41536B8500162309@ims3e.cp.tin.it>
Message-ID: <41FF6D6E.3080904@statistik.uni-dortmund.de>

v.demartino2 at virgilio.it wrote:

> Context: Windows XP - R 2.0.1 with the latest updated packages (including
> Rcmdr) 
> 
> I'm trying to load Rcmdr to use the 3d plots (e.g. scatter3d) but here it
> is what happens:
> 
> 
>>library(Rcmdr)
> 
> Loading required package: zoo 
> Loading required package: strucchange 
> Loading required package: sandwich 
> Loading required package: relimp 
> Loading required package: mvtnorm 
> Loading required package: multcomp 
> Loading required package: lmtest 
> Loading required package: effects 
> Loading required package: car 
> Loading required package: abind 
> Loading required package: rgl 
> Error in library(package, character.only = TRUE, logical = TRUE, warn.conflicts
> = warn.conflicts,  : 
> 	'rgl' is not a valid package -- installed < 2.0.0?
> Error: .onLoad failed in loadNamespace for 'Rcmdr'
> Error in library(Rcmdr) : package/namespace load failed for 'Rcmdr'

What about installing a recent version of rgl, compiled for R-2.0.x?

install.packages("rgl") should do the trick.

Uwe Ligges



> What should I do?
> 
> Vittorio
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From B.Rowlingson at lancaster.ac.uk  Tue Feb  1 13:07:48 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 01 Feb 2005 12:07:48 +0000
Subject: [R] RData loading weirdness
Message-ID: <41FF7114.8040708@lancaster.ac.uk>


  I've just had an interesting thing happen to one of our students. He's 
using R 1.9.1 on Linux, and so I dont expect bugfixes, I'm just 
reporting this out of interest in case anyone else has had this happen.

  Starting R caused a seg fault shortly after "[Previously saved 
workspace restored]". Running "R --no-restore-data" worked fine so I 
suspected a corrupted .RData. It was 67M big, with about 400 objects - 
nothing extreme there. But then doing load(".RData") worked fine. There 
were his objects. How strange.

  I deleted the first 200 objects, saved the .RData, and retried. That 
worked, so I thought it might have been an object in the last 200 or so. 
So I saved them to .RData. Starting up with that worked.

  So, starting afresh with the original load(".RData"), I saved two 
.RData files with each part. Quit, start again, load("part1.RData") and 
load("part2.RData"), then quit and save. Now I had all 400-ish objects 
in one .RData. Time to start R and see if it can startup with it.

  And it did. Worked fine. So I suspect a subtle bug in reading .RData 
files or subtle corruption in the .RData. Most odd. Oh well, I guess I 
could see what R 2.x.x does with it....

Barry



From ripley at stats.ox.ac.uk  Tue Feb  1 13:15:28 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 1 Feb 2005 12:15:28 +0000 (GMT)
Subject: [R] How to get a table of MySQL database as a matrix variable in R
In-Reply-To: <879446cd0502010257d84c408@mail.gmail.com>
References: <879446cd0502010257d84c408@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0502011209450.2437@gannet.stats>

On Tue, 1 Feb 2005, Willie Y. CHEN wrote:

> It seems that the dbReadTable() method provided by RMySQL could not
> get rid of the headers, neither the index column... So
>
> 1. Can I use read.table() method to get a table of MySQL database?
>> From the help document the argument of 'file' of read.table() method
> could be a connection...

?connection will tell you what that is: it is nothing to do with RMySQL.

> 2. How can I obtain a matrix from the database table contains all
> elements except the headers and the index column?

We don't know what you have tried, but note that dbReadTable (and 
read.table) return data frames, not matrices, and data frames must have 
row names and (column) names.  You probably need to research the 
difference between a matrix and a data frame.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Feb  1 13:19:12 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 1 Feb 2005 12:19:12 +0000 (GMT)
Subject: [R] Rcmdr doesn't seem to work
In-Reply-To: <41536B8500162309@ims3e.cp.tin.it>
References: <41536B8500162309@ims3e.cp.tin.it>
Message-ID: <Pine.LNX.4.61.0502011216360.2437@gannet.stats>

The problem is rather that your installation of rgl is out-of-date or 
corrupt, as the error message says.

I do not believe you do have the current rgl, and you need to get it.


On Tue, 1 Feb 2005 v.demartino2 at virgilio.it wrote:

> Context: Windows XP - R 2.0.1 with the latest updated packages (including
> Rcmdr)
>
> I'm trying to load Rcmdr to use the 3d plots (e.g. scatter3d) but here it
> is what happens:
>
>> library(Rcmdr)
> Loading required package: zoo
> Loading required package: strucchange
> Loading required package: sandwich
> Loading required package: relimp
> Loading required package: mvtnorm
> Loading required package: multcomp
> Loading required package: lmtest
> Loading required package: effects
> Loading required package: car
> Loading required package: abind
> Loading required package: rgl
> Error in library(package, character.only = TRUE, logical = TRUE, warn.conflicts
> = warn.conflicts,  :
> 	'rgl' is not a valid package -- installed < 2.0.0?
> Error: .onLoad failed in loadNamespace for 'Rcmdr'
> Error in library(Rcmdr) : package/namespace load failed for 'Rcmdr'
>
> What should I do?

Try install.packages("rgl"), or use the menus to install it.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jtk at cmp.uea.ac.uk  Tue Feb  1 14:53:05 2005
From: jtk at cmp.uea.ac.uk (Jan T. Kim)
Date: Tue, 1 Feb 2005 13:53:05 +0000
Subject: [R] Process to both write to and read from (pipe/fork)?
Message-ID: <20050201135305.GA29528@jtkpc.cmp.uea.ac.uk>

Dear all,

I would like to start a process from an R program in such a way that
I can both feed input into the process and read the process's output.
It seems that in R, I can have a pipe for writing into another process's
input or a pipe for reading from another process's output, but not both.

Doing both necessitates forking, such that the child can start the
external process and feed that with some input, and the parent can
read the output from the external process. Additionally, this requires
obtaining a plain pipe, i.e. one with an input handle (for writing to)
and an output handle (for reading from) prior to forking, so the child
can connect the external process's stdout to the input handle and the
parent can read that from the output handle.

My problem is that I cannot find a way in R to set up such a pipe.

For forking, I've found the fork package, but I can't seem to get a
pipe. I know about the pipe function in the base package, but it seems
to me that that is an interface to popen(2), rather than to pipe(2)
(see attached C source). At least, I can't seem to get anything else but
a popen equivalent from it. I've looked for a pipe package, for pipe in
the fork package, and googled around, to no avail.

For illustrating what I'd like to do, I attach a C program that
replicates the pipe example from the base docs, with the difference
that the input data to be processed with sed are present in the
program rather than in a file.

I start to feel silly and stupid for being unable to figure out such a
basic thing. Thanks in advance for any help -- RTFMs very welcome.

Best regards, Jan
-- 
 +- Jan T. Kim -------------------------------------------------------+
 |    *NEW*    email: jtk at cmp.uea.ac.uk                               |
 |    *NEW*    WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
 *-----=<  hierarchical systems are for files, not for humans  >=-----*
-------------- next part --------------
#include <stdio.h>
#include <unistd.h>
#include <sys/types.h>


int main(int argc, char *argv[])
{
  int fd[2];

  /* get a pipe, fd[0] is "input end", fd[1] is "output end" */
  pipe(fd);

  /* fork a child process to feed something into the pipe */
  if (fork())
  {
    /* this is the parent process which reads from the pipe */
    FILE *f = fdopen(fd[0], "r");
    char buf[100];

    close(fd[1]);
    while (!feof(f))
    {
      if (fgets(buf, 100, f))
	printf("got line: %s", buf);
    }
    fclose(f);
  }
  else
  {
    /* this is the child process which feeds something into the pipe */
    FILE *f = popen("sed -e 's/,$//'", "w");

    close(fd[0]);
    /* connect stdout to pipe's output end, so parent ends up reading
      the outut of sed */
    dup2(fd[1], fileno(stdout));
    fprintf(f, "450, 390, 467, 654,  30, 542, 334, 432, 421,\n");
    fprintf(f, "357, 497, 493, 550, 549, 467, 575, 578, 342,\n");
    fprintf(f, "446, 547, 534, 495, 979, 479\n");
    pclose(f);
  }
  return (0);
}


From thefishfinger at ihug.com.au  Tue Feb  1 12:57:41 2005
From: thefishfinger at ihug.com.au (Sam Ferguson)
Date: Tue, 01 Feb 2005 23:57:41 +1200
Subject: [R] persp plots axis tick-labels
Message-ID: <opslirmfx6o1fugt@samferguson-g5.arch.usyd.edu.au>

Hi R-listers,

I am having trouble with persp plotting and I hope some knowledgeable  
person can help me. I have searched the help files to no avail. I am sure  
there is a way of achieving this simple task. I thought in par maybe but  
that seems not to work for 3d plots. Maybe I have missed something.

I read in ?persp that it expects increasing values for both my x and y  
axes, and a matrix for z. I have been able to graph the data in a way I  
like, excepting that the y axis actually decreases (it could be considered  
categorical). I'd do this with illustrator or something but it's something  
I'd really like to get a handle on. i've tried reshaping the matrix but  
that ruins the visual effect of the data.

All I want to do is substitute some labels for the dummy equally spaced  
numbers I have used for the y axis. I am used to levels and such for  
replacing ordered sequences of numbers with ordered lists of labels. if I  
pass levels(Columns) as the y-axis I get :

non-numeric argument to binary operator

- but I can't work out how to access the labels in any other way.

This is the command I am using :

persp(Rows,Columns,FRm,xlim=c(0,500),theta=28,phi=40,ltheta=20,lphi=135,col='gray',shade=0.7,r=1,box=TRUE,ticktype='detailed',d=1,expand=0.7,xlab='Frequency(Hz)',ylab="Source's  
Distance from Floor",zlab='Floor Related Transfer Function(dB)')

with

yaxis :

> Columns
[1] 1 2 3 4 5 6 7 8

x axis :

> Rows
[1]  46.875  93.750 140.625 187.500 234.375 281.250 328.125 375.000 421.875

matrix :

> FRm
                 V2         V3          V4         V5         V6          V7
46.875   3.0492548  3.1909407  3.29667602  3.3659835  3.3988778  3.39584225
93.75    1.2196200  1.8431528  2.35851554  2.7645735  3.0603714  3.24549087
140.625 -2.2243078 -0.6295752  0.68366429  1.7166743  2.4819218  2.99212770
187.5   -6.2029664 -4.2892634 -1.86802416  0.1535295  1.6414843  2.63151142
234.375 -3.5406118 -5.8918791 -4.97179885 -1.9854102  0.5087716  2.15759798
281.25   0.4005852 -2.4529757 -5.61515030 -4.4797448 -0.9455809  1.56257832
328.125  2.6577261  0.6998110 -2.73925843 -5.8278635 -2.7060864  0.83727527
375      3.5712534  2.6061995  0.05863328 -4.3571602 -4.5478894 -0.02745520
421.875  3.3354403  3.5009804  1.96767411 -1.8611566 -5.6929747 -1.03672351
               V8       V9
46.875  3.357792 3.313598
93.75   3.320284 3.309440
140.625 3.257600 3.302508
187.5   3.169483 3.292799
234.375 3.055572 3.280309
281.25  2.915398 3.265032
328.125 2.748382 3.246961
375     2.553836 3.226087
421.875 2.330961 3.202403

R 2.0.0 on MacOSX 10.3.6



Apologies for what is maybe a simple question.  It's just really hard to  
find the answer.

Thanks
Sam Ferguson



From tghoward at gw.dec.state.ny.us  Tue Feb  1 14:09:47 2005
From: tghoward at gw.dec.state.ny.us (Tim Howard)
Date: Tue, 01 Feb 2005 08:09:47 -0500
Subject: [R] assign connections automatically
Message-ID: <s1ff396a.049@gwsmtp.DEC.STATE.NY.US>

Hi all, 
   I am trying to create a function that will open connections to all
files of 
one type within the working directory.
   I've got the function to open the connections, but I am having a
bugger of a 
time trying to get these connections named as objects in the workspace.
 I am at the point where I can do it outside of the function, but not
inside, using assign.  I'm sure I'm missing something obvious about the
inherent properties of functions.....

#first six lines just setup for this example
> x<-1:20
> y<-20:40
> z<-40:60
> write(x, file="testx.txt")
> write(y, file="testy.txt")
> write(z, file="testz.txt")

> inConnect <- function(){
+ fn <- dir(pattern="*.txt") # grab only *.txt files
+ fn2 <- gsub('.txt', "", fn) # removes the '.txt' from each string
+ for(i in 1:length(fn))
+ assign((fn2[[i]]),file(fn[i], open="r"))
+ }

> showConnections()  #currently, no connections
     description class mode text isopen can read can write
> inConnect()  # run function
> showConnections()  #the connections are now there
  description class  mode text   isopen   can read can write
3 "testx.txt" "file" "r"  "text" "opened" "yes"    "no"     
4 "testy.txt" "file" "r"  "text" "opened" "yes"    "no"     
5 "testz.txt" "file" "r"  "text" "opened" "yes"    "no"     
> ls()  #but NOT there as objects
 [1] "fn"           "fn2"          "inConnect"    "last.warning"
 [5]   "x"            "y"            "z"

> fn <- dir(pattern="*.txt")  #but if I do it manually
> fn2 <- gsub('.txt', "", fn)
> assign((fn2[[3]]),file(fn[3], open="r"))
> ls()  #the connection, testz, appears
 [1] "fn"           "fn2"           "inConnect"    "last.warning"
 [5]  "testz"        "x"            "y"         "z"           

What am I missing? or is there a better way? 

I am using R 2.0.1 on a Windows2K box.

Thanks so much!
Tim Howard



From ripley at stats.ox.ac.uk  Tue Feb  1 14:34:36 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 1 Feb 2005 13:34:36 +0000 (GMT)
Subject: [R] assign connections automatically
In-Reply-To: <s1ff396a.049@gwsmtp.DEC.STATE.NY.US>
References: <s1ff396a.049@gwsmtp.DEC.STATE.NY.US>
Message-ID: <Pine.LNX.4.61.0502011331480.15311@gannet.stats>

You are assigning in the frame of the function, not in the user workspace.
See ?assign and try pos=1 (if that is what you intended) but it might well 
be better to return a list of objects.

On Tue, 1 Feb 2005, Tim Howard wrote:

> Hi all,
>   I am trying to create a function that will open connections to all
> files of
> one type within the working directory.
>   I've got the function to open the connections, but I am having a
> bugger of a
> time trying to get these connections named as objects in the workspace.
> I am at the point where I can do it outside of the function, but not
> inside, using assign.  I'm sure I'm missing something obvious about the
> inherent properties of functions.....
>
> #first six lines just setup for this example
>> x<-1:20
>> y<-20:40
>> z<-40:60
>> write(x, file="testx.txt")
>> write(y, file="testy.txt")
>> write(z, file="testz.txt")
>
>> inConnect <- function(){
> + fn <- dir(pattern="*.txt") # grab only *.txt files
> + fn2 <- gsub('.txt', "", fn) # removes the '.txt' from each string
> + for(i in 1:length(fn))
> + assign((fn2[[i]]),file(fn[i], open="r"))
> + }
>
>> showConnections()  #currently, no connections
>     description class mode text isopen can read can write
>> inConnect()  # run function
>> showConnections()  #the connections are now there
>  description class  mode text   isopen   can read can write
> 3 "testx.txt" "file" "r"  "text" "opened" "yes"    "no"
> 4 "testy.txt" "file" "r"  "text" "opened" "yes"    "no"
> 5 "testz.txt" "file" "r"  "text" "opened" "yes"    "no"
>> ls()  #but NOT there as objects
> [1] "fn"           "fn2"          "inConnect"    "last.warning"
> [5]   "x"            "y"            "z"
>
>> fn <- dir(pattern="*.txt")  #but if I do it manually
>> fn2 <- gsub('.txt', "", fn)
>> assign((fn2[[3]]),file(fn[3], open="r"))
>> ls()  #the connection, testz, appears
> [1] "fn"           "fn2"           "inConnect"    "last.warning"
> [5]  "testz"        "x"            "y"         "z"
>
> What am I missing? or is there a better way?
>
> I am using R 2.0.1 on a Windows2K box.
>
> Thanks so much!
> Tim Howard
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mj_kallen at yahoo.com  Tue Feb  1 14:40:36 2005
From: mj_kallen at yahoo.com (MJK)
Date: Tue, 1 Feb 2005 05:40:36 -0800 (PST)
Subject: [R] package installation problem
Message-ID: <20050201134036.31410.qmail@web52603.mail.yahoo.com>

Hi,

I'm running R-1.9.1 and I'm unable to succesfully
install packages using either 'R CMD INSTALL
xxx.tar.gz' or 'install.packages("xxx").

For example, if I try to install the msm package I
get:

R CMD INSTALL msm_0.4.1.tar.gz
* Installing *source* package 'msm' ...
ERROR: R_HOME ('/root/R-1.9.1') not found
ERROR: installing package DESCRIPTION failed
** Removing '/usr/share/R-1.9.1/library/msm'

In R I get:
> install.packages("msm")
... messages confirming succesful download are given
here ...

WARNING: ignoring environment value of R_HOME
/root/R-1.9.1/bin/Rcmd: /root/R-1.9.1/bin/Rcmd: No
such file or directory

Delete downloaded files (y/N)?

As you can see, R is under /usr/share/R-1.9.1 on my
Slackware 9.1 Linux system. 

paste(file.path(R.home(),"bin","R")) gives me
"/usr/share/R-1.9.1/bin/R" and Sys.getenv("R_HOME")
gives me "/usr/share/R-1.9.1" as does 'R RHOME'.
Everything seems fine, but I do not understand why
it's trying to look for /root/R-1.9.1? How do I solve
this problem?

Any help is greatly appreciated!

Maarten-Jan Kallen
Faculty of Electrical Engineering, Mathematics and
Computer Science
Delft University of Technology
Delft, the Netherlands



From zh107 at york.ac.uk  Tue Feb  1 14:41:52 2005
From: zh107 at york.ac.uk (Zhesi He)
Date: Tue, 1 Feb 2005 13:41:52 +0000
Subject: [R] Error in load(dataFile, myEnv)
Message-ID: <07832314-7457-11D9-937A-000A95AA8E42@york.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050201/42ce01fb/attachment.pl

From ggrothendieck at myway.com  Tue Feb  1 14:41:15 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 1 Feb 2005 13:41:15 +0000 (UTC)
Subject: [R] RE: aggregating dates
References: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA7B@afhex01.dpi.wa.gov.au>
Message-ID: <loom.20050201T141539-618@post.gmane.org>


The issue is just that you need to add POSIXt to the class vector to
be absolutely correct.

   class(x) <- c("POSIXt", "POSIXct")

is the same as

   structure(x, class = c("POSIXt", "POSIXct"))

The only reason the table used the latter was to put it in functional
form rather than lvalue form.  The class form that you are using does
have the advantage of not manipulating the underlying representation
directly.  The way that class(x) <- "POSIXct" might ail would be if there 
were a POSIXt method that should be applied since it would then never
find it.  I don't know of an actual example of this which is why
I mentioned that it will mostly work anyways but its possible that
there are situations where it could fail, at least in in principle.

Regarding the question below on W. Australia Standard Time, POSIXct
stores the datetime internally relative to GMT regardless of how you
intend to use or display it (although it does have the capability of
storing the time zone using the tzone attribute so it can know which 
time zone to operate in when there might be confusion).  

Also note that if you are using fBasics then fBasics does its own
time zone processing using the Olsen data base and only uses POSIXct
as an underlying layer for its GMT processing.  Also, if you use fBasics 
be sure you have set your process or computer to GMT time for it to 
process the times correctly.

Mulholland, Tom <Tom.Mulholland <at> dpi.wa.gov.au> writes:

: 
: I am probably just displaying my ignorance, but I have obviously managed to 
miss exactly what you are
: referring to.
: 
: Firstly I have to thank you for making me look closer at the article. I had 
done so but I had obviously skipped
: over the Comparison table, which I would have found useful in the past. I, 
like many others have struggled
: with dates and because I use fSeries etc I have started to used POSIXct as 
my default way of including dates
: within my datasets.  (That's where the table might have stopped me if I had 
read it the first time around)
: 
: But my issue is that you say that the article tells you how to convert from 
an Integer into POSIXct. In case it
: makes a difference I think the question is how do you make 1104508800 
into "2005-01-01 W. Australia
: Standard Time"
: 
: Is the answer, structure(1104508800,class = c("POSIXt","POSIXct"))?
: 
: Following Paul's message I tried 
: 
: > y <- 1104508800
: > class(y) = "POSIXct"
: > y
: [1] "2005-01-01 W. Australia Standard Time"
: > str(y)
: Class 'POSIXct'  num 1104508800
: 
: > y <- ISOdate(2005,1,1)
: > str(y)
: `POSIXct', format: chr "2005-01-01 12:00:00"
: 
: So checking the table in your article I tought this may shed some light on 
the topic
: 
: > newy <- structure(1104508800,class = c("POSIXt","POSIXct"))
: > newy
: [1] "2005-01-01 W. Australia Standard Time"
: > str(newy)
: `POSIXct', format: chr "2005-01-01"
: 
: It's the same format as ISOdate so I assume it's the right way to do the 
conversion.
: 
: You suggested that the simple 'class(y) = "POSIXct"' might fail. Since I 
thought that maybe plot functions
: might be interfered with I ran
: 
: > x <- runif(3)
: > y <- as.POSIXct(c("2005-01-01","2005-01-02","2005-01-03"))
: > plot(y,x)
: NULL
: > y <- as.numeric(as.POSIXct(c("2005-01-01","2005-01-02","2005-01-03")))
: > class(y) = "POSIXct"
: > plot(y,x)
: NULL
: 
: The plots seemed to be both the same so I am not sure what the implications 
of your statement is.
: 
: I hope this makes sense, because as usual the topic seems to go around in 
circles. It all makes complete sense
: and nonsense at the same time. Because rightly or wrongly it seems to me 
that class(y) <-
: c("POSIXt","POSIXct") is the same thing as the structure statement. So is it 
just failing to include the
: POSIXt that is the issue.
: 
: > 
: 
: > -----Original Message-----
: > From: Gabor Grothendieck [mailto:ggrothendieck <at> myway.com]
: > Sent: Tuesday, 1 February 2005 11:48 AM
: > To: r-help <at> stat.math.ethz.ch
: > Subject: Re: [R] RE: aggregating dates
: > 
: > 
: > Paul Sorenson <Paul.Sorenson <at> vision-bio.com> writes:
: > 
: > : 
: > : The solution I came up with myself was simply to coerce the 
: > integer back to 
: > POSIXct:
: > : 
: > : 	class(ev$date) = "POSIXct"
: > : 
: > : Can't say it is the right way to do it but it seem to work.
: > 
: > That is not entirely correct, although it many cases it will work,
: > since its also a subclass of POSIXt.  
: > 
: > The RNews article I mentioned in my original response to your
: > query has the correct solution.
: > 
: > : 
: > : A second related problem I haven't been able to solve as 
: > yet is how to
: > 
: > My original response to your query already had a solution.
: > 
: > 
: > 
: > : include "incidents" columns (those not
: > : in 'x' or 'by') in an aggregate.
: > : 
: > : 	names(ev): "date" "defectnum" "state"
: > : 
: > : 	aggregate(ev$date, by=list(ev$defectnum), max)
: > : 
: > : This returns only the date and defectnum, I also need the state.
: > : 
: > : I tried writing my own aggregator function:
: > : 	maxevent = function(events) {
: > : 	    events[which.max(events$date),]
: > : 	}
: > : 
: > : 	aggregate(ev, by=list(ev$defectnum), maxevent)
: > : 
: > : But I get:
: > : 
: > : 	Error in "[.default"(events, which.max(events$date), ) : 
: > :       	  incorrect number of dimensions
: > : 
: > : I am trying to retrieve only the rows of ev with the latest 
: > date for a given 
: > defectnum.
: > : 
: > : cheers
: > : 
: > : > Message: 29
: > : > Date: Mon, 31 Jan 2005 16:16:35 +1100
: > : > From: "Paul Sorenson" <Paul.Sorenson <at> vision-bio.com>
: > : > Subject: [R] aggregating dates
: > : > To: <r-help <at> stat.math.ethz.ch>
: > : > Message-ID: <5E06BFED29594F4C9C5EBE230DE320C6068027CD 
: > <at> ewok.vsl.com.au>
: > : > Content-Type: text/plain;	charset="iso-8859-1"
: > : > 
: > : > I have a frame which contains 3 columns:
: > : > 
: > : > "date" "defectnum" "state"
: > : > 
: > : > And I want to get the most recent state change for a given 
: > : > defect number.  date is POSIXct.
: > : > 
: > : > I have tried:
: > : > 	aggregate(ev$date, by=list(ev$defectnum), max)
: > : > 
: > : > Which appears to be working except that the dates seem to 
: > : > come back as integers (presumably the internal representation 
: > : > of POSIXct).
: > : > 
: > : > When I execute max(ev$date) the result remains POSIXct.
: > : > 
: > : > I have been dredging through the help among DateTimeClasses 
: > : > and haven't found a function that converts these integers to 
: > : > some kind of date class.  Or a method for using aggregate 
: > : > which doesn't perform the conversion in the first place.
: > : > 
: > : > Any clues?
: > : 
: > : ______________________________________________
: > : R-help <at> stat.math.ethz.ch mailing list
: > : https://stat.ethz.ch/mailman/listinfo/r-help
: > : PLEASE do read the posting guide! 
: > http://www.R-project.org/posting-guide.html
: > : 
: > :
: > 
: > ______________________________________________
: > R-help <at> stat.math.ethz.ch mailing list
: > https://stat.ethz.ch/mailman/listinfo/r-help
: > PLEASE do read the posting guide! 
: > http://www.R-project.org/posting-guide.html
: >
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://stat.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
: 
:



From v.demartino2 at virgilio.it  Tue Feb  1 14:48:05 2005
From: v.demartino2 at virgilio.it (v.demartino2@virgilio.it)
Date: Tue, 1 Feb 2005 14:48:05 +0100
Subject: [R] Rcmdr doesn't seem to work
In-Reply-To: <41FF6D6E.3080904@statistik.uni-dortmund.de>
Message-ID: <41536B8500162E9D@ims3e.cp.tin.it>


:-- Messaggio originale --
:Date: Tue, 01 Feb 2005 12:52:14 +0100
:From: Uwe Ligges <ligges at statistik.uni-dortmund.de>
:To: v.demartino2 at virgilio.it
:Cc: r-help <r-help at stat.math.ethz.ch>
:Subject: Re: [R] Rcmdr doesn't seem to work
:
:
:v.demartino2 at virgilio.it wrote:
:
:> Context: Windows XP - R 2.0.1 with the latest updated packages (including
:> Rcmdr) 
:> 
:> I'm trying to load Rcmdr to use the 3d plots (e.g. scatter3d) but here
:it
:> is what happens:
:> 
:> 
:>>library(Rcmdr)
:> 
:> Loading required package: zoo 
:> Loading required package: strucchange 
:> Loading required package: sandwich 
:> Loading required package: relimp 
:> Loading required package: mvtnorm 
:> Loading required package: multcomp 
:> Loading required package: lmtest 
:> Loading required package: effects 
:> Loading required package: car 
:> Loading required package: abind 
:> Loading required package: rgl 
:> Error in library(package, character.only = TRUE, logical = TRUE, warn.conflicts
:> = warn.conflicts,  : 
:> 	'rgl' is not a valid package -- installed < 2.0.0?
:> Error: .onLoad failed in loadNamespace for 'Rcmdr'
:> Error in library(Rcmdr) : package/namespace load failed for 'Rcmdr'
:
:What about installing a recent version of rgl, compiled for R-2.0.x?
:
:install.packages("rgl") should do the trick.
:
:Uwe Ligges
:
:
Got it and now it all works....BUT I'm puzzled by the fact that I systematically
update all the packages in R 2.0.1 using 'update.packages()'. Why in this
case R was unable to find the obsolete versions of Rcmdr and rgl and substitued
the with new ones?

Thanks
Vittorio



From paterno at fnal.gov  Tue Feb  1 14:53:42 2005
From: paterno at fnal.gov (Marc Paterno)
Date: Tue, 01 Feb 2005 07:53:42 -0600
Subject: [R] How to write a new "top-level" Trellis/lattice function?
Message-ID: <0IB8003GAJXQ2E@mailgw2.fnal.gov>

Hello,

I am trying to write a new "top level" Trellis/lattice function.
By "top-level", I mean a function like 'xyplot', 'histogram',
'bwplot', etc.
These functions all call 'trellis.skeleton', which I am unable to
call;
an attempt to invoke the function that does so yields the error
message:

-----
Error in do.call("trellis.skeleton", c(list(cond = cond, aspect =
aspect,  : 
        couldn't find function "trellis.skeleton"
-----

It seems that 'trellis.skeleton' is an internal (to lattice) function.
Is this correct, and if so, what is the recommended way to develop a
new
top-level Trellis/lattice function?

best regards,
Marc

----
Dr. Marc Paterno
Fermi National Accelerator Laboratory



From plummer at iarc.fr  Tue Feb  1 15:13:08 2005
From: plummer at iarc.fr (Martyn Plummer)
Date: Tue, 01 Feb 2005 15:13:08 +0100
Subject: [R] Split RPMS for Fedora Core 3
Message-ID: <1107267188.17833.38.camel@seurat>

I have split up the R RPMS for Fedora Core 3 into two subpackages - "R"
and "R-devel".  Following the usual RPM naming convention, the basic "R"
package is now for users, and "R-devel" package is for people wishing to
do program development with R.  **** This includes installing R packages
from source ****

Note that R-devel is not the development version of R, but contains the
files needed for creating R packages.

If you have configured up2date to automatically notify you of R updates
on CRAN, then you will need to manually install R-devel to maintain the
same level of functionality.

At the same time, I have created a separate libRmath-devel package,
which provides the Rmath.h header file and a static version of
standalone Mathlib.

This is an experiment, so please let me know if there are any problems.

Martyn



From ligges at statistik.uni-dortmund.de  Tue Feb  1 15:29:48 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 01 Feb 2005 15:29:48 +0100
Subject: [R] Rcmdr doesn't seem to work
In-Reply-To: <41536B8500162E9D@ims3e.cp.tin.it>
References: <41536B8500162E9D@ims3e.cp.tin.it>
Message-ID: <41FF925C.6080104@statistik.uni-dortmund.de>

v.demartino2 at virgilio.it wrote:

> :-- Messaggio originale --
> :Date: Tue, 01 Feb 2005 12:52:14 +0100
> :From: Uwe Ligges <ligges at statistik.uni-dortmund.de>
> :To: v.demartino2 at virgilio.it
> :Cc: r-help <r-help at stat.math.ethz.ch>
> :Subject: Re: [R] Rcmdr doesn't seem to work
> :
> :
> :v.demartino2 at virgilio.it wrote:
> :
> :> Context: Windows XP - R 2.0.1 with the latest updated packages (including
> :> Rcmdr) 
> :> 
> :> I'm trying to load Rcmdr to use the 3d plots (e.g. scatter3d) but here
> :it
> :> is what happens:
> :> 
> :> 
> :>>library(Rcmdr)
> :> 
> :> Loading required package: zoo 
> :> Loading required package: strucchange 
> :> Loading required package: sandwich 
> :> Loading required package: relimp 
> :> Loading required package: mvtnorm 
> :> Loading required package: multcomp 
> :> Loading required package: lmtest 
> :> Loading required package: effects 
> :> Loading required package: car 
> :> Loading required package: abind 
> :> Loading required package: rgl 
> :> Error in library(package, character.only = TRUE, logical = TRUE, warn.conflicts
> :> = warn.conflicts,  : 
> :> 	'rgl' is not a valid package -- installed < 2.0.0?
> :> Error: .onLoad failed in loadNamespace for 'Rcmdr'
> :> Error in library(Rcmdr) : package/namespace load failed for 'Rcmdr'
> :
> :What about installing a recent version of rgl, compiled for R-2.0.x?
> :
> :install.packages("rgl") should do the trick.
> :
> :Uwe Ligges
> :
> :
> Got it and now it all works....BUT I'm puzzled by the fact that I systematically
> update all the packages in R 2.0.1 using 'update.packages()'. Why in this
> case R was unable to find the obsolete versions of Rcmdr and rgl and substitued
> the with new ones?

The package version has not changed. You need to recompile from sources 
when upgrading the R version - or just reinstalling the appropriate binary.

R-devel's (to be R-2.1.0) update.packages() has a "checkBuilt" argument 
that is documented as follows:
"If 'TRUE', a package built under an earlier minor version of R is 
considered to be 'old'.


Uwe Ligges



> Thanks
> Vittorio
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From mike_saunders at umenfa.maine.edu  Tue Feb  1 15:35:39 2005
From: mike_saunders at umenfa.maine.edu (Mike Saunders)
Date: Tue, 1 Feb 2005 09:35:39 -0500
Subject: [R] Split-split plot ANOVA
Message-ID: <000601c5086b$4d33a400$9ba76f82@CFRU0104>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050201/3a49f1bc/attachment.pl

From tghoward at gw.dec.state.ny.us  Tue Feb  1 15:41:03 2005
From: tghoward at gw.dec.state.ny.us (Tim Howard)
Date: Tue, 01 Feb 2005 09:41:03 -0500
Subject: [R] assign connections automatically
Message-ID: <s1ff4ebb.008@gwsmtp.DEC.STATE.NY.US>

Thank you for the help!  Both, env=.GlobalEnv and pos=1 do the trick.  
I'm embarrassed I didn't glean this from the assign help pages earlier.

?assign suggests that "env" is there for back compatibility so I'm
going with pos.

Tim Howard

>>>>> James Holtman >>>>>>
try:

> inConnect <- function(){
+ fn <- dir(pattern="*.txt") # grab only *.txt files
+ fn2 <- gsub('.txt', "", fn) # removes the '.txt' from each string
+ for(i in 1:length(fn))
+ assign((fn2[[i]]),file(fn[i], open="r"),env = .GlobalEnv)
+ }
__________________________________________________________
James Holtman        "What is the problem you are trying to solve?"
Executive Technical Consultant  --  Office of Technology, Convergys
james.holtman at convergys.com 
+1 (513) 723-2929

>>> Prof Brian Ripley <ripley at stats.ox.ac.uk> 02/01/05 08:34AM >>>
You are assigning in the frame of the function, not in the user
workspace.
See ?assign and try pos=1 (if that is what you intended) but it might
well 
be better to return a list of objects.



From dimitris.rizopoulos at med.kuleuven.ac.be  Tue Feb  1 15:44:05 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Tue, 1 Feb 2005 15:44:05 +0100
Subject: [R] How to write a new "top-level" Trellis/lattice function?
References: <0IB8003GAJXQ2E@mailgw2.fnal.gov>
Message-ID: <00fa01c5086c$7a7e87d0$0540210a@www.domain>

you can see the trellis.skeleton function using the following:

library(lattice)
getAnywhere(trellis.skeleton)
f <- getAnywhere(trellis.skeleton)[[2]][[1]]

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Marc Paterno" <paterno at fnal.gov>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, February 01, 2005 2:53 PM
Subject: [R] How to write a new "top-level" Trellis/lattice function?


> Hello,
>
> I am trying to write a new "top level" Trellis/lattice function.
> By "top-level", I mean a function like 'xyplot', 'histogram',
> 'bwplot', etc.
> These functions all call 'trellis.skeleton', which I am unable to
> call;
> an attempt to invoke the function that does so yields the error
> message:
>
> -----
> Error in do.call("trellis.skeleton", c(list(cond = cond, aspect =
> aspect,  :
>        couldn't find function "trellis.skeleton"
> -----
>
> It seems that 'trellis.skeleton' is an internal (to lattice) 
> function.
> Is this correct, and if so, what is the recommended way to develop a
> new
> top-level Trellis/lattice function?
>
> best regards,
> Marc
>
> ----
> Dr. Marc Paterno
> Fermi National Accelerator Laboratory
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ligges at statistik.uni-dortmund.de  Tue Feb  1 15:53:12 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 01 Feb 2005 15:53:12 +0100
Subject: [R] package installation problem
In-Reply-To: <20050201134036.31410.qmail@web52603.mail.yahoo.com>
References: <20050201134036.31410.qmail@web52603.mail.yahoo.com>
Message-ID: <41FF97D8.2040602@statistik.uni-dortmund.de>

I guess either R has not been installed properly, or you have two (or 
more) versions of R lying around...

Uwe Ligges


MJK wrote:

> Hi,
> 
> I'm running R-1.9.1 and I'm unable to succesfully
> install packages using either 'R CMD INSTALL
> xxx.tar.gz' or 'install.packages("xxx").
> 
> For example, if I try to install the msm package I
> get:
> 
> R CMD INSTALL msm_0.4.1.tar.gz
> * Installing *source* package 'msm' ...
> ERROR: R_HOME ('/root/R-1.9.1') not found
> ERROR: installing package DESCRIPTION failed
> ** Removing '/usr/share/R-1.9.1/library/msm'
> 
> In R I get:
> 
>>install.packages("msm")
> 
> ... messages confirming succesful download are given
> here ...
> 
> WARNING: ignoring environment value of R_HOME
> /root/R-1.9.1/bin/Rcmd: /root/R-1.9.1/bin/Rcmd: No
> such file or directory
> 
> Delete downloaded files (y/N)?
> 
> As you can see, R is under /usr/share/R-1.9.1 on my
> Slackware 9.1 Linux system. 
> 
> paste(file.path(R.home(),"bin","R")) gives me
> "/usr/share/R-1.9.1/bin/R" and Sys.getenv("R_HOME")
> gives me "/usr/share/R-1.9.1" as does 'R RHOME'.
> Everything seems fine, but I do not understand why
> it's trying to look for /root/R-1.9.1? How do I solve
> this problem?
> 
> Any help is greatly appreciated!
> 
> Maarten-Jan Kallen
> Faculty of Electrical Engineering, Mathematics and
> Computer Science
> Delft University of Technology
> Delft, the Netherlands
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jrgonzalez at ico.scs.es  Tue Feb  1 16:26:10 2005
From: jrgonzalez at ico.scs.es (Gonzalez Ruiz, Juan Ramon)
Date: Tue, 1 Feb 2005 16:26:10 +0100
Subject: [R] RV: problems checking a package
Message-ID: <5FF3F11444E3A9439191AA1EDCB69A170BBFBD@icosrvmail01.ICO.SCS.local>

 
Dear R-listers,

I have a very strange problem. I made a package (under Windows and
Linux). The package passed the R CMD Check without problem. Then, I
installed the package and executed a function which calls to a 'dll'

mod<-frailtyPenal(Surv(time,status)~sex+age+cluster(id),
+                   n.knots=8,kappa1=10000,data=kidney)

mod

Call:
frailtyPenal(formula = Surv(time, status) ~ sex + age + cluster(id), 
    data = kidney, n.knots = 8, kappa1 = 10000)


  Shared Gamma Frailty model parameter estimates
  using a Penalized Likelihood on the hazard function 

        coef exp(coef) se(coef) se(coef) HIH      z      p
sex -1.73270     0.177   0.5529       0.5026 -3.134 0.0017
age  0.00812     1.008   0.0125       0.0123  0.651 0.5200

    Frailty parameter, Theta: 0.499 (s.e.: 0.266 ) (s.e. HIH: 0.259 ) 
 
    penalized marginal log-likelihood = -325.64
    n= 76
    n events= 58 
    n groups= 38 
    number of iterations:  20



However, If we try to execute the same function once again we obtain the
following results




mod<-frailtyPenal(Surv(time,status)~sex+age+cluster(id),
+                   n.knots=8,kappa1=10000,data=kidney)

mod

Call:
frailtyPenal(formula = Surv(time, status) ~ sex + age + cluster(id), 
    data = kidney, n.knots = 8, kappa1 = 10000)


  Shared Gamma Frailty model parameter estimates
  using a Penalized Likelihood on the hazard function 

        coef exp(coef) se(coef) se(coef) HIH      z      p
sex -1.74321     0.175   0.5529       0.5026 -3.153 0.0016
age  0.00613     1.006   0.0125       0.0123  0.491 0.6200

    Frailty parameter, Theta: 14797011 (s.e.: 1449 ) (s.e. HIH: 1410 ) 
 
    penalized marginal log-likelihood = 56.63
    n= 76
    n events= 58 
    n groups= 38 
    number of iterations:  351


As you can see, parameters estimates and number of iterations are not
the same. In addition, the second execution takes about 10 seconds and
the first one only 1 or 2. Finnally, if we try to estimate the same
model once again, the PC hunks. I suspect that there is a problem with
Fortran but after proving many changes I do not find it. 

Do someone know what it is happening?


Thank you very much for any help in advance.

Best Regards,

Juan



From m_osm at gmx.net  Tue Feb  1 16:25:57 2005
From: m_osm at gmx.net (Mahdi Osman)
Date: Tue, 1 Feb 2005 16:25:57 +0100 (MET)
Subject: [R] sgeostat
Message-ID: <13428.1107271557@www46.gmx.net>

Hi, list,

I am using 'sgeostat' package by Albrecht Gebhardt and I am trying to put a
correlation coefficient of some kind on the lagplots. Is there a possiblity
to do so?

Thanks
Mahdi

-- 
-----------------------------------
Mahdi Osman (PhD)
E-mail: m_osm at gmx.net
-----------------------------------

GMX im TV ... Die Gedanken sind frei ... Schon gesehen?
Jetzt Spot online ansehen: http://www.gmx.net/de/go/tv-spot



From ripley at stats.ox.ac.uk  Tue Feb  1 16:33:08 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 1 Feb 2005 15:33:08 +0000 (GMT)
Subject: [R] Rcmdr doesn't seem to work
In-Reply-To: <41536B8500162E9D@ims3e.cp.tin.it>
References: <41536B8500162E9D@ims3e.cp.tin.it>
Message-ID: <Pine.LNX.4.61.0502011530490.27656@gannet.stats>

On Tue, 1 Feb 2005 v.demartino2 at virgilio.it wrote:

> Got it and now it all works....BUT I'm puzzled by the fact that I systematically
> update all the packages in R 2.0.1 using 'update.packages()'. Why in this
> case R was unable to find the obsolete versions of Rcmdr and rgl and substitued
> the with new ones?

Were they `obselete versions'?  Almost surely not, just built against the 
wrong version of R.

As from R 2.1.0 there is update.packages(checkBuilt=TRUE) for precisely 
this job.  Update.packages() updates the versions of the packages, not of 
R: why did you think otherwise?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From deepayan at stat.wisc.edu  Tue Feb  1 16:40:29 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Tue, 1 Feb 2005 09:40:29 -0600
Subject: [R] How to write a new "top-level" Trellis/lattice function?
In-Reply-To: <0IB8003GAJXQ2E@mailgw2.fnal.gov>
References: <0IB8003GAJXQ2E@mailgw2.fnal.gov>
Message-ID: <200502010940.29805.deepayan@stat.wisc.edu>

On Tuesday 01 February 2005 07:53, Marc Paterno wrote:
> Hello,
>
> I am trying to write a new "top level" Trellis/lattice function.
> By "top-level", I mean a function like 'xyplot', 'histogram',
> 'bwplot', etc.
> These functions all call 'trellis.skeleton', which I am unable to
> call;
> an attempt to invoke the function that does so yields the error
> message:
>
> -----
> Error in do.call("trellis.skeleton", c(list(cond = cond, aspect =
> aspect,  :
>         couldn't find function "trellis.skeleton"
> -----
> 
> It seems that 'trellis.skeleton' is an internal (to lattice)
> function. Is this correct, and if so, what is the recommended way to
> develop a new
> top-level Trellis/lattice function?

What do you need to do that cannot be done by writing special panel and 
prepanel functions? See for example, the definition of dotplot, or 
xYplot in package Hmisc.

The only reason to have a new top-level function I can think of is if 
you need a fundametally different form of the formula. If that's the 
case, can you give us more details?

Deepayan



From ripley at stats.ox.ac.uk  Tue Feb  1 16:39:32 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 1 Feb 2005 15:39:32 +0000 (GMT)
Subject: [R] Error in load(dataFile, myEnv)
In-Reply-To: <07832314-7457-11D9-937A-000A95AA8E42@york.ac.uk>
References: <07832314-7457-11D9-937A-000A95AA8E42@york.ac.uk>
Message-ID: <Pine.LNX.4.61.0502011536350.27656@gannet.stats>

Looks like you have your own version of a load() function around.  Try
?conflicts to see what you may have masked.

On Tue, 1 Feb 2005, Zhesi He wrote:

> Dear all,
>
> I just found that some of the packages are not able to load any more,
> after I installed R2.0.1 in my Mac, it even affects my old R1.8
> installs.
>
> It gives me errors when I load packages that contains "myEnv" settings.
> such as: RMySQL, DBI, Rggobi, etc. But others that does not require
> "myENV" is all right, like tcltk that only calls the c functions.
>
> The errors are like:
> Error in load(dataFile, myEnv) : unused argument(s) ( ...)
> Error in library(DBI) : .First.lib failed for 'DBI'
>
> I had a look at the package file that contains .First.lib,
>     fullName <- paste("package", pkgname, sep=":")
>     myEnv <- as.environment(match(fullName, search()))
>     dataFile <- file.path(libname, pkgname, "R", "all.rda")
>     rm(.First.lib, envir = myEnv)
>     load(dataFile, myEnv)
>
> I found that myEnv is not set. as the package can not be matched in
> search().
> If I comment the line load(dataFile, myEnv), then the package is loaded
> without errors but there no functions is loaded at all.
>
> It probably is something trivial. Can some one help, please?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Jesus.Frias at dit.ie  Tue Feb  1 16:57:15 2005
From: Jesus.Frias at dit.ie (Jesus Frias)
Date: Tue, 01 Feb 2005 15:57:15 +0000
Subject: [R] Split-split plot ANOVA
In-Reply-To: <000601c5086b$4d33a400$9ba76f82@CFRU0104>
Message-ID: <LGECJJCANFBOOHCMGPJEKEBHDKAA.Jesus.Frias@dit.ie>

Hi Mike,

	*An example of the use of aov() for a split-plot is in MASS

library(MASS)
example(Oats)
	The book also gives a detailed explanation

	*pp 45-52 of the Pinheiro and Bates book gives you an example of the use of
lme() on a split-plot. If you have a non balanced design,  lme() from the
nlme library might be a better tool than aov().

	Also, if you have the lme4 library installed you'll have a lot more
flexibility on the formulation of your random effects.

regards,

Jesus

--------------------------------------------------------------
Jes?s Mar?a Fr?as Celayeta
School of Food Sci. and Env. Health.
Faculty of Tourism and Food
Dublin Institute of Technology
Cathal Brugha St., Dublin 1. Ireland
t +353 1 4024459 f +353 1 4024495
w www.dit.ie/DIT/tourismfood/science/staff/frias.html
--------------------------------------------------------------

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Mike Saunders
> Sent: 01 February 2005 14:36
> To: R Help
> Subject: [R] Split-split plot ANOVA
>
>
> Does someone out there have an example of R-code for a
> split-split plot ANOVA using aov or another function?  The design
> is not balanced.  I never set up one in R before and it would be
> nice to see an example before I tackle a very complex design I
> have to model.
>
> Thanks,
> Mike
>
> Mike Saunders
> Research Assistant
> Forest Ecosystem Research Program
> Department of Forest Ecosystem Sciences
> University of Maine
> Orono, ME  04469
> 207-581-2763 (O)
> 207-581-4257 (F)
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

--
This message has been scanned for content and
viruses by the DIT Information Services MailScanner
Service, and is believed to be clean.
http://www.dit.ie



-- 
This message has been scanned for content and 
viruses by the DIT Information Services MailScanner 
Service, and is believed to be clean.
http://www.dit.ie



From sundar.dorai-raj at pdf.com  Tue Feb  1 17:13:02 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 01 Feb 2005 10:13:02 -0600
Subject: [R] How to write a new "top-level" Trellis/lattice function?
In-Reply-To: <00fa01c5086c$7a7e87d0$0540210a@www.domain>
References: <0IB8003GAJXQ2E@mailgw2.fnal.gov>
	<00fa01c5086c$7a7e87d0$0540210a@www.domain>
Message-ID: <41FFAA8E.8060105@pdf.com>

I agree with Deepayan regarding writing your own (pre)panel functions to 
override xyplot defaults rather than calling trellis.skeleton. However, 
if this is truly what you need, Dimitris' answer can be improved by 
using the `:::' operator which will extract internal functions such as 
trellis.skeleton.

lattice:::trellis.skeleton

--sundar

Dimitris Rizopoulos wrote:
> you can see the trellis.skeleton function using the following:
> 
> library(lattice)
> getAnywhere(trellis.skeleton)
> f <- getAnywhere(trellis.skeleton)[[2]][[1]]
> 
> I hope it helps.
> 
> Best,
> Dimitris
> 
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
> 
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/16/336899
> Fax: +32/16/337015
> Web: http://www.med.kuleuven.ac.be/biostat
>     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
> 
> 
> ----- Original Message ----- From: "Marc Paterno" <paterno at fnal.gov>
> To: <r-help at stat.math.ethz.ch>
> Sent: Tuesday, February 01, 2005 2:53 PM
> Subject: [R] How to write a new "top-level" Trellis/lattice function?
> 
> 
>> Hello,
>>
>> I am trying to write a new "top level" Trellis/lattice function.
>> By "top-level", I mean a function like 'xyplot', 'histogram',
>> 'bwplot', etc.
>> These functions all call 'trellis.skeleton', which I am unable to
>> call;
>> an attempt to invoke the function that does so yields the error
>> message:
>>
>> -----
>> Error in do.call("trellis.skeleton", c(list(cond = cond, aspect =
>> aspect,  :
>>        couldn't find function "trellis.skeleton"
>> -----
>>
>> It seems that 'trellis.skeleton' is an internal (to lattice) function.
>> Is this correct, and if so, what is the recommended way to develop a
>> new
>> top-level Trellis/lattice function?
>>
>> best regards,
>> Marc
>>
>> ----
>> Dr. Marc Paterno
>> Fermi National Accelerator Laboratory
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Tue Feb  1 17:49:20 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 1 Feb 2005 16:49:20 +0000 (UTC)
Subject: [R] RE: aggregating dates
References: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA7B@afhex01.dpi.wa.gov.au>
	<loom.20050201T141539-618@post.gmane.org>
Message-ID: <loom.20050201T174132-743@post.gmane.org>

Gabor Grothendieck <ggrothendieck <at> myway.com> writes:

: 
: The issue is just that you need to add POSIXt to the class vector to
: be absolutely correct.
: 
:    class(x) <- c("POSIXt", "POSIXct")
: 
: is the same as
: 
:    structure(x, class = c("POSIXt", "POSIXct"))
: 
: The only reason the table used the latter was to put it in functional
: form rather than lvalue form.  The class form that you are using does
: have the advantage of not manipulating the underlying representation
: directly.  The way that class(x) <- "POSIXct" might ail would be if there 
: were a POSIXt method that should be applied since it would then never
: find it.  

Just a follow up.

Here is a simple example of 
   class(y) <- "POSIXct" 
failing while 
   class(x) <- c("POSIXt", "POSIXct")
works.

In the first example below (x) the class is correct and converting the
POSIXct date to character gives the expected date representation
but in the second example (y) the class is incomplete and converting
it to character gives an unexpected result:

R> x <- structure(0, class = c("POSIXt", "POSIXct"))
R> as.character(x)
[1] "1969-12-31 19:00:00"
 
R> y <- structure(0, class = "POSIXct")
R> as.character(y)
[1] "0"



From rodrig at stat.wisc.edu  Tue Feb  1 18:31:08 2005
From: rodrig at stat.wisc.edu (Mariana Rodrigues)
Date: Tue, 1 Feb 2005 11:31:08 -0600 (CST)
Subject: [R] glmmPQL
Message-ID: <Pine.LNX.4.58.0502011130220.14321@public02.stat.wisc.edu>


Does someone know what is the integration method of the random effect in
glmmPQL function??

Please advise. Tnks, Mariana.



From news at chasset.net  Tue Feb  1 19:22:08 2005
From: news at chasset.net (Pierre-Olivier Chasset)
Date: Tue, 01 Feb 2005 19:22:08 +0100
Subject: [R] Matrix langage
Message-ID: <41FFC8D0.7090507@chasset.net>

Hello,

I would like to use R in a laboratory. They use actually IML of the 
SAS-Society. With this IMLangage, you can enter a mathematical formula 
with sum from i to n, transpose matrix, sum of a colonne and all this 
stuff. After that, the software can translate it in a program.
I am not currently use R in Matrix langage. But I manipulate data frame. 
I have the feeling that R can do like SAS do.
What do you think about it?
Thanks in advance,

Pierre-Olivier Chasset



From s.conti at sheffield.ac.uk  Tue Feb  1 19:24:25 2005
From: s.conti at sheffield.ac.uk (Stefano Conti)
Date: Tue, 1 Feb 2005 18:24:25 +0000
Subject: [R] Convergence problems with survreg()
Message-ID: <200502011824.25762.s.conti@sheffield.ac.uk>

Dear R mailing list,

I'm trying to fit a censored regression model to a large (dimension of the 
design matrix is 2e5 by 7) right truncated data by means of the 
survreg(Surv()) function, as suggested by Paul Johnson on his "R Tips" web 
page.

Possibly due to the sensitivity to the initial values of the Newton-Raphson 
routine in use by survreg(), resulting regression outputs turn out to be 
unreliable. Should the "init" field be left blank, the maximisation barely 
starts (2 iterations), and ends up yielding estimates close to 0 or +/- 
infinity.

I guess the problem mainly lies in reasonably specifying starting values for 
the optimisation embedded in survreg(). Even by inputing OLS estimates (the 
default in the SAS lifereg method) ensuing inferences are remarkably 
different, and with significantly lower log-likelihood, than the ones 
returned by the SAS proc lifereg.

Has anybody encountered a similar problem? If so, any suggestion/advise would 
be grately appreciated. Many thanks in advance, all best,

-- 
Dr. Stefano Conti
Department of Probability and Statistics
The Hicks Building
University of Sheffield
Sheffield S3 7RH
UK
Phone   # +44 (0)114 2223854
Fax     # +44 (0)114 2223759



From HDoran at air.org  Tue Feb  1 19:27:47 2005
From: HDoran at air.org (Doran, Harold)
Date: Tue, 1 Feb 2005 13:27:47 -0500
Subject: [R] Matrix langage
Message-ID: <88EAF3512A55DF46B06B1954AEF73F740784876A@dc1ex2.air.org>

All of this is possible, and very easy, in R. See the Intro to R book.
Doug Bates also has an article in the recent edition of R News showing
how to perform matrix operations for least squares.



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Pierre-Olivier
Chasset
Sent: Tuesday, February 01, 2005 1:22 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Matrix langage

Hello,

I would like to use R in a laboratory. They use actually IML of the
SAS-Society. With this IMLangage, you can enter a mathematical formula
with sum from i to n, transpose matrix, sum of a colonne and all this
stuff. After that, the software can translate it in a program.
I am not currently use R in Matrix langage. But I manipulate data frame.

I have the feeling that R can do like SAS do.
What do you think about it?
Thanks in advance,

Pierre-Olivier Chasset

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From depire at inrets.fr  Tue Feb  1 19:27:57 2005
From: depire at inrets.fr (Depire Alexandre)
Date: Tue, 1 Feb 2005 19:27:57 +0100
Subject: [R] GRASS - R
In-Reply-To: <13428.1107271557@www46.gmx.net>
References: <13428.1107271557@www46.gmx.net>
Message-ID: <200502011927.57773.depire@inrets.fr>

Hello,
I would like to know if someone uses R with GRASS ( SIG software) .

----------------
Alexandre DEPIRE
INRETS / GARIG



From spencer.graves at pdf.com  Tue Feb  1 19:34:05 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 01 Feb 2005 10:34:05 -0800
Subject: [R] Matrix langage
In-Reply-To: <41FFC8D0.7090507@chasset.net>
References: <41FFC8D0.7090507@chasset.net>
Message-ID: <41FFCB9D.7070807@pdf.com>

      Matrix manipulations are a strength of R.  If A and B are 
matrices, (A+B) adds them and (A %*% B) is the usual matrix 
multiplication;  A*B is elementwise multiplication.  qr(A) = the Q-R 
decomposition;  svd(A) = singular value decomposition.  eigen(A) = 
eigenvalue-eigenvector decomposition (except if you need some special 
Jordon form with repeated roots). 

      Have you looked at ch. 5 in "An Introduction to R" under 
"www.r-project.org" -> Documentation:   Manuals. 

      hope this helps. 
      spencer graves    

Pierre-Olivier Chasset wrote:

> Hello,
>
> I would like to use R in a laboratory. They use actually IML of the 
> SAS-Society. With this IMLangage, you can enter a mathematical formula 
> with sum from i to n, transpose matrix, sum of a colonne and all this 
> stuff. After that, the software can translate it in a program.
> I am not currently use R in Matrix langage. But I manipulate data 
> frame. I have the feeling that R can do like SAS do.
> What do you think about it?
> Thanks in advance,
>
> Pierre-Olivier Chasset
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From v.demartino2 at virgilio.it  Tue Feb  1 20:38:23 2005
From: v.demartino2 at virgilio.it (Vittorio)
Date: Tue, 1 Feb 2005 19:38:23 +0000
Subject: [R] Rcmdr doesn't seem to work
In-Reply-To: <Pine.LNX.4.61.0502011530490.27656@gannet.stats>
References: <41536B8500162E9D@ims3e.cp.tin.it>
	<Pine.LNX.4.61.0502011530490.27656@gannet.stats>
Message-ID: <200502011938.23862.v.demartino2@virgilio.it>

Alle 15:33, marted? 1 febbraio 2005, Prof Brian Ripley ha scritto:
> On Tue, 1 Feb 2005 v.demartino2 at virgilio.it wrote:
> > Got it and now it all works....BUT I'm puzzled by the fact that I
> > systematically update all the packages in R 2.0.1 using
> > 'update.packages()'. Why in this case R was unable to find the obsolete
> > versions of Rcmdr and rgl and substitued the with new ones?
>
> Were they `obselete versions'?  Almost surely not, just built against the
> wrong version of R.
>
> As from R 2.1.0 there is update.packages(checkBuilt=TRUE) for precisely
> this job.  Update.packages() updates the versions of the packages, not of
> R: why did you think otherwise?


I understand now that my mistake was to force the installer of the binaries 
under win xp to use always the same directory to install the different 
versions of the programme, that is d:\programmi\R, without any reference  to 
the version itself. This causes kind of of a sedimentation of obsolete stuff.

Thanks for the answer.

Vittorio



From dgoliche at sclc.ecosur.mx  Tue Feb  1 19:53:57 2005
From: dgoliche at sclc.ecosur.mx (dgoliche)
Date: Tue, 1 Feb 2005 12:53:57 -0600
Subject: [R] polynomials REML and ML in nlme
Message-ID: <001a01c5088f$627d46d0$1002010a@DuncanToshiba>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050201/4a8b711f/attachment.pl

From nicolas.deig at epfl.ch  Tue Feb  1 20:09:49 2005
From: nicolas.deig at epfl.ch (NICOLAS DEIG)
Date: Tue, 01 Feb 2005 20:09:49 +0100
Subject: [R] taking data out of a dataframe
Message-ID: <18e181dc8e.1dc8e18e18@imap.epfl.ch>

hello,

I have a problem with using the results of a certain function.
This function is the "tree" function. If I use it to produce a tree say
with the data "iris" from the help page, I can see the "attributes" of
my result. Now I need to use a column of the attributes of my tree
ir.tr, is there any way or function that permits me to take this column out?
THanks in advance
Nicolas



From Roger.Bivand at nhh.no  Tue Feb  1 20:33:21 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue, 1 Feb 2005 20:33:21 +0100 (CET)
Subject: [R] GRASS - R
In-Reply-To: <200502011927.57773.depire@inrets.fr>
Message-ID: <Pine.LNX.4.44.0502012030180.3529-100000@reclus.nhh.no>

On Tue, 1 Feb 2005, Depire Alexandre wrote:

> Hello,
> I would like to know if someone uses R with GRASS ( SIG software) .

Yes, please see http://grass.itc.it/statsgrass/index.html, and consider 
asking again on the mailing list referenced there, perhaps with more 
details of what you want to use R with GRASS for.

> 
> ----------------
> Alexandre DEPIRE
> INRETS / GARIG
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From stonehusky at gmail.com  Tue Feb  1 20:56:24 2005
From: stonehusky at gmail.com (Donna-n-Doug Finner)
Date: Tue, 1 Feb 2005 14:56:24 -0500
Subject: [R] RODBC - connect is failing
Message-ID: <529e16e0050201115676895f20@mail.gmail.com>

I get the following errors when attempting an ODBC connection to a MySQL db.

1: [RODBC] ERROR: state HY000, Code 0, message [MySQL] [ODBC 3.51
Driver] Could not determine the driver name so could not lookup setup
library.
2: ODBCConnection failed in: odbcDriverConnect(st, case=case,
believeNRows=believeNRows).

Win XP sp2
MySQL 4.1.8
My SQLODBC 3.51.10
Package RODBC (latest) is installed.

LibMySQL.dll is in about a 1/2 dozen directories including
Winnt\System32, my PHP directory, and several MySQL directories.
I note that there are apparently different versions of this file
(different file sizes and dates) so I'm not sure if there is a problem
here.

I've searched the archives and found nothing other than a suggestion
to put the dll in the \system32 directory which didn't help.  If I
missed the key posting, just point me in the right direction and I'll
go get the info.

Not sure where to go from here so any pointers are appreciated.

Doug

ps - the ODBC driver is working - I can test if from the Windows
Control Panel and get a successful connect.



From sue at xlsolutions-corp.com  Tue Feb  1 21:09:50 2005
From: sue at xlsolutions-corp.com (sue@xlsolutions-corp.com)
Date: Tue,  1 Feb 2005 13:09:50 -0700
Subject: [R] New R/Splus Course***"Interactive and Dynamic Graphics for Data
	Analysis Using XGobi/GGobi" in San Francisco
Message-ID: <20050201200950.22084.qmail@webmail09.mesa1.secureserver.net>

 XLSolutions Corporation (www.xlsolutions-corp.com) is proud to
announce our new 2-day course:

"R/Splus Course***"Interactive and Dynamic Graphics for  Data Analysis
Using XGobi/GGobi"

in San Francisco: www.xlsolutions-corp.com/training.htm

****San Francisco, CA ---------------------------- February 21st-22nd,
2005

Reserve your seat now at the early bird rates! Payment due AFTER
the class.

Course Description:

This course is designed as a hands-on introduction to direct
manipulation and dynamic graphics for exploring multi-dimensional
data. The course will focus on the data visualization tools available
in GGobi, which can be accessed from the data analysis package, S/R.

Course Outline:

- Introduction and overview of graphics for multidimensional data with
GGobi
- Exploring the distribution of missing values with GGobi
- Graphics for classification, both supervised and unsupervised with
GGobi
 - Exploratory spatio-temporal data analysis with GGobi
 - Analysis of microarray Data with GGobi
 - Exploring multivariate longitudinal data with GGobi
 - Proximity analysis and multidimensional scaling with GGobi
 - Categorical data and ANOVA with GGobi
 - Inference for data visualization with GGobi
 - Strategies for handling large data with GGobi

Email us for group discounts.
Email Sue Turner: sue at xlsolutions-corp.com
Phone: 206-686-1578
Visit us: www.xlsolutions-corp.com/training.htm
Please let us know if you and your colleagues are interested in this
classto take advantage of group discount. Register now to secure your
seat! Interested in R/Splus Advanced course? email us.


Cheers,
Elvis Miller, PhD
Manager Training.
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com
elvis at xlsolutions-corp.com



From ripley at stats.ox.ac.uk  Tue Feb  1 22:10:21 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 1 Feb 2005 21:10:21 +0000 (GMT)
Subject: [R] RODBC - connect is failing
In-Reply-To: <529e16e0050201115676895f20@mail.gmail.com>
References: <529e16e0050201115676895f20@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0502012109001.31495@gannet.stats>

The message is from the MySQL ODBC driver, as it indicates.
You'll have to debug your setup, as this is not an R nor RODBC issue: the 
latter has done its job and connected to the driver.

On Tue, 1 Feb 2005, Donna-n-Doug Finner wrote:

> I get the following errors when attempting an ODBC connection to a MySQL db.
>
> 1: [RODBC] ERROR: state HY000, Code 0, message [MySQL] [ODBC 3.51
> Driver] Could not determine the driver name so could not lookup setup
> library.
> 2: ODBCConnection failed in: odbcDriverConnect(st, case=case,
> believeNRows=believeNRows).
>
> Win XP sp2
> MySQL 4.1.8
> My SQLODBC 3.51.10
> Package RODBC (latest) is installed.
>
> LibMySQL.dll is in about a 1/2 dozen directories including
> Winnt\System32, my PHP directory, and several MySQL directories.
> I note that there are apparently different versions of this file
> (different file sizes and dates) so I'm not sure if there is a problem
> here.
>
> I've searched the archives and found nothing other than a suggestion
> to put the dll in the \system32 directory which didn't help.  If I
> missed the key posting, just point me in the right direction and I'll
> go get the info.
>
> Not sure where to go from here so any pointers are appreciated.
>
> Doug
>
> ps - the ODBC driver is working - I can test if from the Windows
> Control Panel and get a successful connect.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From gunter.berton at gene.com  Tue Feb  1 22:30:19 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 1 Feb 2005 13:30:19 -0800
Subject: [R] polynomials REML and ML in nlme
In-Reply-To: <001a01c5088f$627d46d0$1002010a@DuncanToshiba>
Message-ID: <200502012130.j11LUJpd019287@compton.gene.com>

Duncan:

Bates and Pinheiro do show explicitly and in detail that the restricted log
likelihood depends on the parameterization: "An important difference between
the likelihood function and the restricted likelihood is that the former is
invariant to one-to-one reparametrizations of the fixed effects ... while
the latter is not." (p.76)

So that answers both your questions: With different fixed effect
parameterizations you get different restricted log likelihoods even for the
same random effects specifications. So you cannot compare models and you see
differences of the sort you saw. That's just the way REML works.

I believe the the basic issue can be summarized as: the restricted log
likelihood is a nonlinear function of the fixed effects specification. Hence
linear transformations (reparametrizations) of the fixed effects change the
log likelihood. But if you want more than that, get the book (or another
reference of your choice). You could also try googling "REML".

All the above is subject to correction by Doug Bates, of course.


-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of dgoliche
> Sent: Tuesday, February 01, 2005 10:54 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] polynomials REML and ML in nlme
> 
> Hello everyone,
>  
> I hope this is a fair enough question, but I don?t have 
> access to a copy
> of Bates and Pinheiro. It is probably quite obvious but the 
> answer might
> be of general interest.
> If I fit a fixed effect with an added quadratic term and then do it as
> an orthogonal polynomial using maximum likelihood I get the expected
> result- they have the same logLik.
>  
> mod2a<-lme(wthole~nplants+I(nplants^2),data=d3,random=~1|field
> /subplot,m
> ethod="ML")
> mod2b<-lme(wthole~poly(nplants,2),data=d3,random=~1|field/subp
> lot,method
> ="ML")
>  
> > anova(mod2a,mod2b)
>       Model df      AIC      BIC    logLik
> mod2a     1  6 6698.231 6723.869 -3343.116
> mod2b     2  6 6698.231 6723.869 -3343.116
>  
> However if I fit the two models by REML they are not considered to be
> the same and I get warned.
>  
> > anova(mod2a.REML,mod2b.REML)
>            Model df      AIC      BIC    logLik
> mod2a.REML     1  6 6680.616 6706.219 -3334.308
> mod2b.REML     2  6 6666.915 6692.518 -3327.457
>  
> Warning message: 
> Fitted objects with different fixed effects. REML comparisons are not
> meaningful. in: anova.lme(mod2a.REML, mod2b.REML) 
>  
> Well yes, I suppose that?s right, they are not the same fixed effects.
> But why does REML give them such different Log likelihoods? And what
> should I do if I want to compare a larger set of models. For 
> example the
> following, admittedly overparameterised model, can be fitted 
> (slowly) by
> either method
>  
> >mod4<-lme(wthole~poly(nplants,2),data=d3,random=~poly(nplants
> ,2)|field/
> subplot,method="ML")
>  
> But this doesn?t work by either method....
>  
> >
> mod4<-lme(wthole~nplants+I(nplants^2),data=d3,random=~nplants+
> I(nplants^
> 2)|field/subplot,method="ML")
>  
> Error in solve.default(pdMatrix(a, fact = TRUE)) : 
>         system is computationally singular: reciprocal 
> condition number
> = 2.58558e-045
>  
> Thanks in advance for clearing up my confusion. The gentler the
> explanation, the more useful it would be as far as I am concerned as I
> am not a statistician and have to admit I am not at all clear how REML
> actually works.
>  
> Duncan
>  
> Dr Duncan Golicher
> Ecologia y Sistematica Terrestre
> Conservaci?n de la Biodiversidad
> El Colegio de la Frontera Sur
> San Cristobal de Las Casas, Chiapas, Mexico
> Tel. 967 1883 ext 9814
> dgoliche at sclc.ecosur.mx
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From pgilbert at bank-banque-canada.ca  Tue Feb  1 22:32:35 2005
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Tue, 01 Feb 2005 16:32:35 -0500
Subject: [R] A "rude" question
In-Reply-To: <20050127050951.GA26565@localhost>
References: <20050127050951.GA26565@localhost>
Message-ID: <41FFF573.6010302@bank-banque-canada.ca>

One point that did not get mentioned in this discussion, and I believe 
deserves
much more publicity, is the impact of packages tests. The design of the 
package
 system allows package developers to put tests in packages, and these 
are checked
regularly (see <http://cran.at.r-project.org/contrib/checkSummary.html>).

These are intended to test the package functionality, but also give R 
what is
perhaps the largest test suite of any statistical software (certainly 
the most
quickly growing). While any single package's test will never guarantee that
the package works perfectly, the ensemble goes a long way toward ensuring
that core R functionality behaves as intended. It seems unlikely to me 
that any
commercial effort will ever be able to catch up.

There are several ways that tests can add to our confidence that 
calculations 
can be trusted. They can
   - check against theoretical results
   - check against published results
   - check against results from other software
   - check that calculations done in different ways give the same result
   - check that monte carlo experiments give distributions that are 
consistent
      with expected results

Some of these are relatively time consuming to set up and check the 
first time,
but after that they can be automatic.

If you have particular calculations with specific packages that you are 
especially
concerned about, I encourage you to participate by devising good tests 
and sending
them to the package developers. (But first check the tests they are 
already doing
in the package tests directory.)

Paul Gilbert

msck9 at mizzou.edu wrote:

>Dear all, 
> I am beginner using R. I have a question about it. When you use it,
> since it is written by so many authors, how do you know that the
> results are trustable?(I don't want to affend anyone, also I trust
> people). But I think this should be a question.
>
> Thanks,
> Ming
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From spencer.graves at pdf.com  Tue Feb  1 22:56:44 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 01 Feb 2005 13:56:44 -0800
Subject: [R] A "rude" question
In-Reply-To: <41FFF573.6010302@bank-banque-canada.ca>
References: <20050127050951.GA26565@localhost>
	<41FFF573.6010302@bank-banque-canada.ca>
Message-ID: <41FFFB1C.8080902@pdf.com>

Hi, Paul: 

      How can I access "the package tests directory" you mentioned?  
Only one of the 52 subdirectories of "library" in my current 
installation of R 2.0.1 has a "test" folder. 

      Thanks,
      Spencer Graves    

Paul Gilbert wrote:

> One point that did not get mentioned in this discussion, and I believe 
> deserves
> much more publicity, is the impact of packages tests. The design of 
> the package
> system allows package developers to put tests in packages, and these 
> are checked
> regularly (see <http://cran.at.r-project.org/contrib/checkSummary.html>).
>
> These are intended to test the package functionality, but also give R 
> what is
> perhaps the largest test suite of any statistical software (certainly 
> the most
> quickly growing). While any single package's test will never guarantee 
> that
> the package works perfectly, the ensemble goes a long way toward ensuring
> that core R functionality behaves as intended. It seems unlikely to me 
> that any
> commercial effort will ever be able to catch up.
>
> There are several ways that tests can add to our confidence that 
> calculations can be trusted. They can
>   - check against theoretical results
>   - check against published results
>   - check against results from other software
>   - check that calculations done in different ways give the same result
>   - check that monte carlo experiments give distributions that are 
> consistent
>      with expected results
>
> Some of these are relatively time consuming to set up and check the 
> first time,
> but after that they can be automatic.
>
> If you have particular calculations with specific packages that you 
> are especially
> concerned about, I encourage you to participate by devising good tests 
> and sending
> them to the package developers. (But first check the tests they are 
> already doing
> in the package tests directory.)
>
> Paul Gilbert
>
> msck9 at mizzou.edu wrote:
>
>> Dear all, I am beginner using R. I have a question about it. When you 
>> use it,
>> since it is written by so many authors, how do you know that the
>> results are trustable?(I don't want to affend anyone, also I trust
>> people). But I think this should be a question.
>>
>> Thanks,
>> Ming
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>>  
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From paulojus at est.ufpr.br  Tue Feb  1 23:14:17 2005
From: paulojus at est.ufpr.br (Paulo Justiniano Ribeiro Jr)
Date: Tue, 1 Feb 2005 20:14:17 -0200 (BRST)
Subject: [R] A "rude" question
In-Reply-To: <41FFFB1C.8080902@pdf.com>
References: <20050127050951.GA26565@localhost>
	<41FFF573.6010302@bank-banque-canada.ca> <41FFFB1C.8080902@pdf.com>
Message-ID: <Pine.LNX.4.58L0.0502012011080.8161@est.ufpr.br>

One problem with distributing packages with the test sub-directory
is that this can overload the daily tests in the CRAN machines

My workaround for that  is:
For the geoR package I run the tests in my machine but remove the tests
directory when submitting to CRAN.
Also I mantain a package web-page where I make the version
with tests available for downloading

P.J.

On Tue, 1 Feb 2005, Spencer Graves wrote:

> Hi, Paul:
>
>       How can I access "the package tests directory" you mentioned?
> Only one of the 52 subdirectories of "library" in my current
> installation of R 2.0.1 has a "test" folder.
>
>       Thanks,
>       Spencer Graves
>
> Paul Gilbert wrote:
>
> > One point that did not get mentioned in this discussion, and I believe
> > deserves
> > much more publicity, is the impact of packages tests. The design of
> > the package
> > system allows package developers to put tests in packages, and these
> > are checked
> > regularly (see <http://cran.at.r-project.org/contrib/checkSummary.html>).
> >
> > These are intended to test the package functionality, but also give R
> > what is
> > perhaps the largest test suite of any statistical software (certainly
> > the most
> > quickly growing). While any single package's test will never guarantee
> > that
> > the package works perfectly, the ensemble goes a long way toward ensuring
> > that core R functionality behaves as intended. It seems unlikely to me
> > that any
> > commercial effort will ever be able to catch up.
> >
> > There are several ways that tests can add to our confidence that
> > calculations can be trusted. They can
> >   - check against theoretical results
> >   - check against published results
> >   - check against results from other software
> >   - check that calculations done in different ways give the same result
> >   - check that monte carlo experiments give distributions that are
> > consistent
> >      with expected results
> >
> > Some of these are relatively time consuming to set up and check the
> > first time,
> > but after that they can be automatic.
> >
> > If you have particular calculations with specific packages that you
> > are especially
> > concerned about, I encourage you to participate by devising good tests
> > and sending
> > them to the package developers. (But first check the tests they are
> > already doing
> > in the package tests directory.)
> >
> > Paul Gilbert
> >
> > msck9 at mizzou.edu wrote:
> >
> >> Dear all, I am beginner using R. I have a question about it. When you
> >> use it,
> >> since it is written by so many authors, how do you know that the
> >> results are trustable?(I don't want to affend anyone, also I trust
> >> people). But I think this should be a question.
> >>
> >> Thanks,
> >> Ming
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide!
> >> http://www.R-project.org/posting-guide.html
> >>
> >>
> >>
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

Paulo Justiniano Ribeiro Jr
LEG (Laborat?rio de Estat?stica e Geoinforma??o)
Departamento de Estat?stica
Universidade Federal do Paran?
Caixa Postal 19.081
CEP 81.531-990
Curitiba, PR  -  Brasil
Tel: (+55) 41 361 3573
Fax: (+55) 41 361 3141
e-mail: paulojus at est.ufpr.br
http://www.est.ufpr.br/~paulojus



From simon at hellscho.com.au  Tue Feb  1 23:10:27 2005
From: simon at hellscho.com.au (Simon Gatehouse)
Date: Wed, 2 Feb 2005 09:10:27 +1100
Subject: [R] Hugh image created from modest data
Message-ID: <20050201221033.377A0B656@mfep6.connect.com.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050202/8ae51be3/attachment.pl

From francoisromain at free.fr  Tue Feb  1 23:12:30 2005
From: francoisromain at free.fr (Romain Francois)
Date: Tue, 01 Feb 2005 23:12:30 +0100
Subject: [R] GRASS - R
In-Reply-To: <200502011927.57773.depire@inrets.fr>
References: <13428.1107271557@www46.gmx.net>
	<200502011927.57773.depire@inrets.fr>
Message-ID: <41FFFECE.5030806@free.fr>

Hello,

I never used it but there is a package on CRAN that interfaces R with 
GRASS, see :

http://finzi.psych.upenn.edu/R/library/GRASS/html/00Index.html

I had 152 hits on searching GRASS on http://www-r.project.org --> Search 
--> RsiteSearch. Maybe you'll find answers there.

Romain.

Le 01.02.2005 19:27, Depire Alexandre a ?crit :

>Hello,
>I would like to know if someone uses R with GRASS ( SIG software) .
>
>----------------
>Alexandre DEPIRE
>INRETS / GARIG
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>


-- 
Romain FRANCOIS : francoisromain at free.fr
page web : http://addictedtor.free.fr/  (en construction)
06 18 39 14 69 / 01 46 80 65 60
_______________________________________________________
Etudiant en 3eme ann?e
Institut de Statistique de l'Universit? de Paris (ISUP)
Fili?re Industrie et Services
http://www.isup.cicrp.jussieu.fr/



From pgilbert at bank-banque-canada.ca  Tue Feb  1 23:23:56 2005
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Tue, 01 Feb 2005 17:23:56 -0500
Subject: [R] A "rude" question
In-Reply-To: <41FFFB1C.8080902@pdf.com>
References: <20050127050951.GA26565@localhost>
	<41FFF573.6010302@bank-banque-canada.ca> <41FFFB1C.8080902@pdf.com>
Message-ID: <4200017C.3000109@bank-banque-canada.ca>



Spencer Graves wrote:

> Hi, Paul:
>      How can I access "the package tests directory" you mentioned?  
> Only one of the 52 subdirectories of "library" in my current 
> installation of R 2.0.1 has a "test" folder. 

The tests directory seems to get stripped when you install a package. If 
you get a source package and untar it then there will be a tests 
directory in many packages (it is not required).

Paul

>
>      Thanks,
>      Spencer Graves   
> Paul Gilbert wrote:
>
>> One point that did not get mentioned in this discussion, and I 
>> believe deserves
>> much more publicity, is the impact of packages tests. The design of 
>> the package
>> system allows package developers to put tests in packages, and these 
>> are checked
>> regularly (see 
>> <http://cran.at.r-project.org/contrib/checkSummary.html>).
>>
>> These are intended to test the package functionality, but also give R 
>> what is
>> perhaps the largest test suite of any statistical software (certainly 
>> the most
>> quickly growing). While any single package's test will never 
>> guarantee that
>> the package works perfectly, the ensemble goes a long way toward 
>> ensuring
>> that core R functionality behaves as intended. It seems unlikely to 
>> me that any
>> commercial effort will ever be able to catch up.
>>
>> There are several ways that tests can add to our confidence that 
>> calculations can be trusted. They can
>>   - check against theoretical results
>>   - check against published results
>>   - check against results from other software
>>   - check that calculations done in different ways give the same result
>>   - check that monte carlo experiments give distributions that are 
>> consistent
>>      with expected results
>>
>> Some of these are relatively time consuming to set up and check the 
>> first time,
>> but after that they can be automatic.
>>
>> If you have particular calculations with specific packages that you 
>> are especially
>> concerned about, I encourage you to participate by devising good 
>> tests and sending
>> them to the package developers. (But first check the tests they are 
>> already doing
>> in the package tests directory.)
>>
>> Paul Gilbert
>>
>> msck9 at mizzou.edu wrote:
>>
>>> Dear all, I am beginner using R. I have a question about it. When 
>>> you use it,
>>> since it is written by so many authors, how do you know that the
>>> results are trustable?(I don't want to affend anyone, also I trust
>>> people). But I think this should be a question.
>>>
>>> Thanks,
>>> Ming
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>>
>>>  
>>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
>
>
>



From christoph.lehmann at gmx.ch  Tue Feb  1 23:28:56 2005
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Tue, 01 Feb 2005 23:28:56 +0100
Subject: [R] vectorization of a data-aggregation loop
Message-ID: <420002A8.6080203@gmx.ch>

Hi
I have a simple question:

the following data.frame

    id iwv type
1   1   1    a
2   1   2    b
3   1  11    b
4   1   5    a
5   1   6    c
6   2   4    c
7   2   3    c
8   2  10    a
9   3   6    b
10  3   9    a
11  3   8    b
12  3   7    c

shall be aggregated into the form:

   id t.a t.b t.c
1  1   6  13   6
6  2  10   0   7
9  3   9  14   7

means for each 'type' (a, b, c) a new column is introduced which
gets the sum of iwv for the respective observations 'id'

of course I can do this transformation/aggregation in a loop (see 
below), but is there a way to do this more efficiently, eg. in using 
tapply (or something similar)- since I have lot many rows?

thanks for a hint

christoph

#------------------------------------------------------------------------------
# the loop-way
t <- data.frame(cbind(c(1,1,1,1,1,2,2,2,3,3,3,3), 
c(10,12,8,33,34,3,27,77,34,45,4,39), c('a', 'b', 'b', 'a', 'c', 'c', 
'c', 'a', 'b', 'a', 'b', 'c')))
names(t) <- c("id", "iwv", "type")
t$iwv <- as.numeric(t$iwv)
t

# define the additional columns (type.a, type.b, type.c)
tt <- rep(0, nrow(t) * length(levels(t$type)))
dim(tt) <- c(nrow(t), length(levels(t$type)))
tt <- data.frame(tt)
dimnames(tt)[[2]] <- paste("t.", levels(t$type), sep = "")
t <- cbind(t, tt)
t

obs <- 0
obs.previous <- 0
row.elim <- rep(FALSE, nrow(t))
ta <- which((names(t) == "t.a")) #number of column which codes the first 
type
r.ctr <- 0
for (i in 1:nrow(t)){
   obs <- t[i,]$id
   if (obs == obs.previous) {
     row.elim[i] <- TRUE
     r.ctr <- r.ctr + 1 #increment
     type.col <- as.numeric(t[i,]$type)
     t[i - r.ctr, ta - 1 + type.col] <- t[i - r.ctr, ta - 1 +
       type.col] + t[i,]$iwv
   }
   else {
     r.ctr <- 0 #record counter
     type.col <- as.numeric(t[i,]$type)
     t[i, ta - 1 + type.col] <- t[i,]$iwv
   }
   obs.previous <- obs
}

t <- t[!row.elim,]
t <- subset(t, select = -c(iwv, type))
t

#------------------------------------------------------------------------------



From pgilbert at bank-banque-canada.ca  Tue Feb  1 23:32:04 2005
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Tue, 01 Feb 2005 17:32:04 -0500
Subject: [R] A "rude" question
In-Reply-To: <41FFF573.6010302@bank-banque-canada.ca>
References: <20050127050951.GA26565@localhost>
	<41FFF573.6010302@bank-banque-canada.ca>
Message-ID: <42000364.3090700@bank-banque-canada.ca>

Paul Gilbert wrote:

> The design of the package system allows package developers to put 
> tests in packages, 

> and these are checked regularly (see ...

This link should have been  
<http://cran.at.r-project.org/src/contrib/checkSummary.html>.

Paul Gilbert



From gunter.berton at gene.com  Tue Feb  1 23:49:23 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 1 Feb 2005 14:49:23 -0800
Subject: [R] Hugh image created from modest data
In-Reply-To: <20050201221033.377A0B656@mfep6.connect.com.au>
Message-ID: <200502012249.j11MnNG2021912@hertz.gene.com>

Simon:

ls(all=TRUE) will show you all objects in your workspace. If you have plot
history recording turned on,for example,.SavedPlots can get quite large and
will be part of your saved image.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Simon Gatehouse
> Sent: Tuesday, February 01, 2005 2:10 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Hugh image created from modest data
> 
> Dear R users,
> 
> I am running R2.1 on windows XP SP2.  I have a problem that I 
> fear I am
> providing insufficient information to resolve.  But if anyone 
> can suggest
> directions to understand the problem I would be most grateful.
> 
>  
> 
>  
> 
> I import modest datasets which create a list of data frames 
> (primary data
> object) amounting to 15.9Mb  when measured internally by 
> object.size() . The
> sizes of the list components in the primary data object  are 
> consistent with
> the total, again when measured by object.size() All other 
> objects in my work
> area contribute less than 2Kb
> 
>  
> 
> After doing some analysis the primary data object would on 
> occasions expand
> to 800Mb but is (usually) cut back prior to saving the image 
> of the work
> area on exiting, or at sparse intervals during processing.
> 
>  
> 
> The situation now is that internally, as measured by 
> object.size(),  my
> working directory is less than 16Megs but image.save is 
> creating an .Rdata
> image in excess of 800Meg. Could this be consistent in some 
> way?  If it is
> not, or is unusual, how would I go about finding the extra, unwanted
> information and removing it.
> 
>  
> 
> Any comment or suggestions would be appreciated
> 
>  
> 
> Simon Gatehouse
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From p.connolly at hortresearch.co.nz  Tue Feb  1 23:52:40 2005
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Wed, 2 Feb 2005 11:52:40 +1300
Subject: [R] =?iso-8859-1?q?New_problem_printing_=B0C_in_plots?=
Message-ID: <20050201225240.GV22446@hortresearch.co.nz>

> version
         _
platform i686-pc-linux-gnu
arch     i686
os       linux-gnu
system   i686, linux-gnu
status
major    2
minor    0.1
year     2004
month    11
day      15
language R
> paste("25", "?C", sep = "")
[1] "25\302\260C"

In ESS, I get  "25\201\260C"

The ?C does end up in the plot alright, but it has an accented A as
well which is not useful.

(The '?C' I get from within Emacs using C - x 8 - o)
GNU Emacs 21.3.1



In earlier versions, I used to get this:

> version
         _
platform i686-pc-linux-gnu
arch     i686
os       linux-gnu
system   i686, linux-gnu
status
major    1
minor    8.1
year     2003
month    11
day      21
language R
> paste("25", "?C", sep = "")
[1] "25?C"
>

That plotted fine.  I can't remember if the problem existed with
R-2.0.0 and I didn't notice a mention the NEWS file/s -- but that
could be because I didn't know what to look for.

Could this be purely an Emacs problem, or does it have to do with
changes in R-2.x.x?

TIA

-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From MSchwartz at MedAnalytics.com  Wed Feb  2 00:18:16 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Tue, 01 Feb 2005 17:18:16 -0600
Subject: [R] vectorization of a data-aggregation loop
In-Reply-To: <420002A8.6080203@gmx.ch>
References: <420002A8.6080203@gmx.ch>
Message-ID: <1107299896.6303.16.camel@horizons.localdomain>

On Tue, 2005-02-01 at 23:28 +0100, Christoph Lehmann wrote:
> Hi
> I have a simple question:
> 
> the following data.frame
> 
>     id iwv type
> 1   1   1    a
> 2   1   2    b
> 3   1  11    b
> 4   1   5    a
> 5   1   6    c
> 6   2   4    c
> 7   2   3    c
> 8   2  10    a
> 9   3   6    b
> 10  3   9    a
> 11  3   8    b
> 12  3   7    c
> 
> shall be aggregated into the form:
> 
>    id t.a t.b t.c
> 1  1   6  13   6
> 6  2  10   0   7
> 9  3   9  14   7
> 
> means for each 'type' (a, b, c) a new column is introduced which
> gets the sum of iwv for the respective observations 'id'
> 
> of course I can do this transformation/aggregation in a loop (see 
> below), but is there a way to do this more efficiently, eg. in using 
> tapply (or something similar)- since I have lot many rows?
> 
> thanks for a hint


Well, I'll get you started using the sample data you have above.

Presuming that your data is in a data frame called 'df':

# Use aggregate to get the summations data by id and type
> df.a <- aggregate(df$iwv, by = list(df$id, df$type), sum)

# Show the results
> df.a
  Group.1 Group.2  x
1       1       a  6
2       2       a 10
3       3       a  9
4       1       b 13
5       3       b 14
6       1       c  6
7       2       c  7
8       3       c  7

# Now use xtabs() to create a contingency table from df.a

> xtabs(x ~ Group.1 + Group.2, data = df.a)
       Group.2
Group.1 a  b  c 
      1  6 13  6
      2 10  0  7
      3  9 14  7

You can now modify the colnames in the result of the xtabs step as you
desire.

It's a little easier in two steps. See ?aggregate and ?xtabs for more
information.

HTH,

Marc Schwartz



From ripley at stats.ox.ac.uk  Wed Feb  2 00:33:37 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 1 Feb 2005 23:33:37 +0000 (GMT)
Subject: [R] =?iso-8859-1?q?New_problem_printing_=B0C_in_plots?=
In-Reply-To: <20050201225240.GV22446@hortresearch.co.nz>
References: <20050201225240.GV22446@hortresearch.co.nz>
Message-ID: <Pine.LNX.4.61.0502012316020.4387@gannet.stats>

That this prints as an octal escape was always the intention: it is your 
OS that is telling R that it is not a printable character.  What locale 
are you in?  For me

1) In en_GB this works (correct, as that is charset ISO-8859-1)
2) In C, I get "25\260C" (correct, as that is not an ASCII char)

My guess is that you have R running in a C locale and emacs in a UTF-8 
locale, since the UTF-8 representation of that symbol is c2b0, in octal 
\302\260.  If that is what is going on, 1.8.1 would have been equally 
confused (it might have printed UTF-8, but it would not plot it), so I 
suspect the important change is not the one you think it is.

Note that at least one set of RPMs now runs R in the C locale (but that's 
not part of R per se).  If you run in en_NZ I think you will find R works 
to your taste.  The good news is that R-devel already supports en_NZ.utf8, 
and so 2.1.0 will in a couple of months.

On Wed, 2 Feb 2005, Patrick Connolly wrote:

>> version
>         _
> platform i686-pc-linux-gnu
> arch     i686
> os       linux-gnu
> system   i686, linux-gnu
> status
> major    2
> minor    0.1
> year     2004
> month    11
> day      15
> language R
>> paste("25", "C", sep = "")
> [1] "25\302\260C"
>
> In ESS, I get  "25\201\260C"
>
> The C does end up in the plot alright, but it has an accented A as
> well which is not useful.
>
> (The 'C' I get from within Emacs using C - x 8 - o)
> GNU Emacs 21.3.1

BTW, the plot device matters too.

> In earlier versions, I used to get this:
>
>> version
>         _
> platform i686-pc-linux-gnu
> arch     i686
> os       linux-gnu
> system   i686, linux-gnu
> status
> major    1
> minor    8.1
> year     2003
> month    11
> day      21
> language R
>> paste("25", "C", sep = "")
> [1] "25C"
>>
>
> That plotted fine.  I can't remember if the problem existed with
> R-2.0.0 and I didn't notice a mention the NEWS file/s -- but that
> could be because I didn't know what to look for.
>
> Could this be purely an Emacs problem, or does it have to do with
> changes in R-2.x.x?

Neither of those, is my guess.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From zh107 at york.ac.uk  Wed Feb  2 01:00:04 2005
From: zh107 at york.ac.uk (Zhesi He)
Date: Wed, 2 Feb 2005 00:00:04 -0000
Subject: [R] Error in load(dataFile, myEnv)
References: <07832314-7457-11D9-937A-000A95AA8E42@york.ac.uk>
	<Pine.LNX.4.61.0502011536350.27656@gannet.stats>
Message-ID: <00cb01c508ba$2853b180$c80313ac@gates>

Dear Prof Ripley,

Thanks for your reply.
I couldn't find anything wrong with Load() function.
It seems that myEnv is not set, as myEnv <- as.environment(match(fullName, 
search())) does not return a valid value.

Also, when I commented the load(dataFile, myEnv) line, the rest of 
.First.lib code does not work (no function is loaded, e.g. when I input the 
function in the package, it doesn't recognise it)

Any suggestions?

Zhesi.





----- Original Message ----- 
From: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>
To: "Zhesi He" <zh107 at york.ac.uk>
Cc: <R-help at stat.math.ethz.ch>
Sent: Tuesday, February 01, 2005 3:39 PM
Subject: Re: [R] Error in load(dataFile, myEnv)


> Looks like you have your own version of a load() function around.  Try
> ?conflicts to see what you may have masked.
>
> On Tue, 1 Feb 2005, Zhesi He wrote:
>
>> Dear all,
>>
>> I just found that some of the packages are not able to load any more,
>> after I installed R2.0.1 in my Mac, it even affects my old R1.8
>> installs.
>>
>> It gives me errors when I load packages that contains "myEnv" settings.
>> such as: RMySQL, DBI, Rggobi, etc. But others that does not require
>> "myENV" is all right, like tcltk that only calls the c functions.
>>
>> The errors are like:
>> Error in load(dataFile, myEnv) : unused argument(s) ( ...)
>> Error in library(DBI) : .First.lib failed for 'DBI'
>>
>> I had a look at the package file that contains .First.lib,
>>     fullName <- paste("package", pkgname, sep=":")
>>     myEnv <- as.environment(match(fullName, search()))
>>     dataFile <- file.path(libname, pkgname, "R", "all.rda")
>>     rm(.First.lib, envir = myEnv)
>>     load(dataFile, myEnv)
>>
>> I found that myEnv is not set. as the package can not be matched in
>> search().
>> If I comment the line load(dataFile, myEnv), then the package is loaded
>> without errors but there no functions is loaded at all.
>>
>> It probably is something trivial. Can some one help, please?
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From jonnybravo104 at hotmail.com  Wed Feb  2 01:17:02 2005
From: jonnybravo104 at hotmail.com (Jon Hurley)
Date: Tue, 01 Feb 2005 16:17:02 -0800
Subject: [R] R-Project Plot: Creating browser map tags
Message-ID: <BAY23-F4252B5ECE438B0B9F8548CEE7E0@phx.gbl>

Hi,

I have started looking into using R for generating graphs displayed in a web 
browser and it produces really nice graphics. One of the requirements I have 
is to construct HTML map tags over the graphs so that users can click on 
elements of the graph (e.g. bars of a histogram) and either see interesting 
information or navigate to another page.

I see how to produce JPEG images with R that can be readily embedded in an 
HTML page but did not see how to output information to produce maps.

Could anyone tell if this is possible and if so how?

Many thanks -- Jon


Security. http://clinic.mcafee.com/clinic/ibuy/campaign.asp?cid=3963



From fciclone at bol.com.br  Wed Feb  2 01:23:25 2005
From: fciclone at bol.com.br (Alex)
Date: Tue,  1 Feb 2005 22:23:25 -0200
Subject: [R] polynomials REML and ML in nlme
Message-ID: <IB9D31$B8EAD28B65FF4F8596B89F2EA8BDE9E1@bol.com.br>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050201/ed619a7e/attachment.pl

From brett at hbrc.govt.nz  Wed Feb  2 01:54:00 2005
From: brett at hbrc.govt.nz (Brett Stansfield)
Date: Wed, 2 Feb 2005 13:54:00 +1300 
Subject: [R] selecting subsets of data for analysis
Message-ID: <3542A1BF5AE1984D9FF577DA2CF8BA98245B2E@MSX2>


can you tell me how to ask R to analyse a subset of data

eg. supposing the data set consists of 9 columns and I only want R to
analyse columns 1, 3 and 5 

how would I command R to conduct eg. boxplots of those variables only?

thanks
brett
Brett Stansfield 
Environmental Scientist - Water Quality 
Hawke's Bay Regional Council 
102 Vautier Street 
Private Bag 6006 
Napier 
Phone (06) 835-9200 extn 9334 
Fax (06) 835-3601



From Tom.Mulholland at dpi.wa.gov.au  Wed Feb  2 01:57:49 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Wed, 2 Feb 2005 08:57:49 +0800
Subject: [R] taking data out of a dataframe
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA7D@afhex01.dpi.wa.gov.au>

In general if you are using a package, it helps to identify the package. I assume it is the tree package.

Is this what you are after (from the example on the ?tree page)

> attr(ir.tr,"ylevels")
[1] "setosa"     "versicolor" "virginica" 

Tom
>
> -----Original Message-----
> From: NICOLAS DEIG [mailto:nicolas.deig at epfl.ch]
> Sent: Wednesday, 2 February 2005 3:10 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] taking data out of a dataframe
> 
> 
> hello,
> 
> I have a problem with using the results of a certain function.
> This function is the "tree" function. If I use it to produce 
> a tree say
> with the data "iris" from the help page, I can see the "attributes" of
> my result. Now I need to use a column of the attributes of my tree
> ir.tr, is there any way or function that permits me to take 
> this column out?
> THanks in advance
> Nicolas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Paul.Sorenson at vision-bio.com  Wed Feb  2 02:17:32 2005
From: Paul.Sorenson at vision-bio.com (Paul Sorenson)
Date: Wed, 2 Feb 2005 12:17:32 +1100
Subject: [R] aggregation with extra columns
Message-ID: <5E06BFED29594F4C9C5EBE230DE320C6068027D2@ewok.vsl.com.au>

R People,

Thanks for your help on my recent questions, Excel is never going to disappear from my office but with graphics from lattice package and some other stuff in R I have been able to add some value.

I have a problem I haven't been able to figure out with aggregation, I mentioned it earlier but didn't state it very clearly.

Basically I have many "defect events" and I want to grab the most recent event for each defect number:

eg:
"date" 	"defectnum" "state"
2004-12-1	10		create
2004-12-2	11		create
2004-12-4	10		close
2004-12-7	11		fix

to:
"date" 	"defectnum" "state"
2004-12-4	10		close
2004-12-7	11		fix

Now with aggregate I can get the rows I want but not with the state "attached":

aggregate(list(date=ev$date), by=list(defectnum=ev$defectnum), max)

Gives me the rows I want but I have lost the "state".  I have tried doing a merge afterwards but now I realise why they warned me avoid using dates as database keys.

What would be handy is somehow getting back the index vector from the aggregate function.  I realize in the general case this wouldn't work for aggregate but in the case of min/max the result is a specific record.

Someone earlier mentioned some tricks with sort but I haven't been able to make that get to where I want.



From p.connolly at hortresearch.co.nz  Wed Feb  2 02:27:41 2005
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Wed, 2 Feb 2005 14:27:41 +1300
Subject: =?iso-8859-1?Q?Re=3A_[R]_New_problem_printing_=B0C_in_plots?=
In-Reply-To: <Pine.LNX.4.61.0502012316020.4387@gannet.stats>
References: <20050201225240.GV22446@hortresearch.co.nz> 
	<Pine.LNX.4.61.0502012316020.4387@gannet.stats>
Message-ID: <20050202012741.GZ22446@hortresearch.co.nz>

On Tue, 01-Feb-2005 at 11:33PM +0000, Prof Brian Ripley wrote:

|> That this prints as an octal escape was always the intention: it is your 
|> OS that is telling R that it is not a printable character.  What locale 
|> are you in?  For me
|> 
|> 1) In en_GB this works (correct, as that is charset ISO-8859-1)
|> 2) In C, I get "25\260C" (correct, as that is not an ASCII char)
|> 
|> My guess is that you have R running in a C locale and emacs in a UTF-8 
|> locale, since the UTF-8 representation of that symbol is c2b0, in octal 
|> \302\260.  If that is what is going on, 1.8.1 would have been equally 
|> confused (it might have printed UTF-8, but it would not plot it), so I 


 1.8.1 was not confused.  Maybe confused to a similar degree, but
confused in a different way.  It prints to screen and it used to plot
to a postscript device fine.  I seem to remember that it didn't plot
to an X11 device correctly.  

Now, X11 and postscript both have the extra character.

[...]

|> 
|> Note that at least one set of RPMs now runs R in the C locale (but that's 
|> not part of R per se).  If you run in en_NZ I think you will find R works 
|> to your taste.  The good news is that R-devel already supports en_NZ.utf8, 
|> and so 2.1.0 will in a couple of months.

I've found that simply using the string "\260C" instead of Emacs's
trick works both in X11 and postscript, so I'll survive until then --
though I kind of miss that bit of WYSIWYG.


Thanks


-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From msck9 at mizzou.edu  Wed Feb  2 02:42:07 2005
From: msck9 at mizzou.edu (msck9@mizzou.edu)
Date: Tue, 1 Feb 2005 19:42:07 -0600
Subject: [R] GUI under linux for R
Message-ID: <20050202014207.GA19000@localhost>

Hi, 
 I'm wondering whether there is GUI under linux. I checked the website,
 it has Emacs(ESS), IDE/Scrip Editors etc. I am looking for one which is
 similar to the GUI under windows.

 Thanks,
 Ming



From htang at hpl.hp.com  Wed Feb  2 02:54:26 2005
From: htang at hpl.hp.com (Hsiu-Khuern Tang)
Date: Tue, 1 Feb 2005 17:54:26 -0800
Subject: [R] R CMD BATCH character limit?
Message-ID: <20050202015426.GO1436@hplhtang1.hpl.hp.com>

Is there a limit on the number of characters in an invocation like

R CMD BATCH --opt1=val1 --opt2=val2 ... --save-to=C:\very\long\string ..\R\script.R

?

I am running R 2.0.1 on Windows XP, and when running a long BATCH
command (about 400--500 characters) the last option gets mangled, as can
be seen from a warning in script.Rout that looks like this:

WARNING: unknown option
+--save-to=C:\very\long\str..\R\script.Rout

Note that "string" is truncated to "str" _and_ concatenated with the
implicit batch output file "..\R\script.Rout".  (The "unknown option"
warning is expected; I am just using commandArgs() within the script to
pick out the arguments.)

I have tried specifying --args before the options, with the same result,
except that the truncation appears earlier.

Thanks in advance for any help!

Best Regards,
Hsiu-Khuern Tang.



From Tom.Mulholland at dpi.wa.gov.au  Wed Feb  2 02:58:13 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Wed, 2 Feb 2005 09:58:13 +0800
Subject: [R] aggregation with extra columns
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA7F@afhex01.dpi.wa.gov.au>

I think Gabor gave you the answer, even if you didn't see it

x <- rep(c("02/27/92", "02/27/92", "01/14/92", "02/28/92", "02/01/92"),each = 5)
z <- data.frame(Date = strptime(x, "%m/%d/%y"))
z$Value <- trunc(runif(25) * 5) + 1
z$State <- c("A","B","C","D","E")[z$Value]
z <- z[order(z$Date),]
z
z[aggregate(1:nrow(z), list(z$State), max)$x,]


Tom

Even if it's not quite right, the technique is there.



> -----Original Message-----
> From: Paul Sorenson [mailto:Paul.Sorenson at vision-bio.com]
> Sent: Wednesday, 2 February 2005 9:18 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] aggregation with extra columns
> 
> 
> R People,
> 
> Thanks for your help on my recent questions, Excel is never 
> going to disappear from my office but with graphics from 
> lattice package and some other stuff in R I have been able to 
> add some value.
> 
> I have a problem I haven't been able to figure out with 
> aggregation, I mentioned it earlier but didn't state it very clearly.
> 
> Basically I have many "defect events" and I want to grab the 
> most recent event for each defect number:
> 
> eg:
> "date" 	"defectnum" "state"
> 2004-12-1	10		create
> 2004-12-2	11		create
> 2004-12-4	10		close
> 2004-12-7	11		fix
> 
> to:
> "date" 	"defectnum" "state"
> 2004-12-4	10		close
> 2004-12-7	11		fix
> 
> Now with aggregate I can get the rows I want but not with the 
> state "attached":
> 
> aggregate(list(date=ev$date), by=list(defectnum=ev$defectnum), max)
> 
> Gives me the rows I want but I have lost the "state".  I have 
> tried doing a merge afterwards but now I realise why they 
> warned me avoid using dates as database keys.
> 
> What would be handy is somehow getting back the index vector 
> from the aggregate function.  I realize in the general case 
> this wouldn't work for aggregate but in the case of min/max 
> the result is a specific record.
> 
> Someone earlier mentioned some tricks with sort but I haven't 
> been able to make that get to where I want.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ivo_welch-rstat8303 at mailblocks.com  Wed Feb  2 03:48:36 2005
From: ivo_welch-rstat8303 at mailblocks.com (ivo_welch-rstat8303@mailblocks.com)
Date: Tue, 01 Feb 2005 18:48:36 -0800
Subject: [R] postscript symbols?
Message-ID: <200502020248.j122mb1A007113@hypatia.math.ethz.ch>


dear R wizards:

is it possible to use a postscript font symbol as a plot symbol?    in 
particular, I want to use the four postscript symbols for playing cards 
(club, heart, spade, diamond) as points.  In LaTeX, these four are

\Pisymbol{psy}{"A7} \Pisymbol{psy}{"A8}
\Pisymbol{psy}{"A9} \Pisymbol{psy}{"A10}

and what I would love to do is place them, at say, (x=1,y=1), 
(x=2,y=2), (x=3,y=3) and (x=4,y=4).  Help appreciated---or merely a 
note that this is impossible.

regards,

/iaw
---
ivo welch



From brett at hbrc.govt.nz  Wed Feb  2 03:52:26 2005
From: brett at hbrc.govt.nz (Brett Stansfield)
Date: Wed, 2 Feb 2005 15:52:26 +1300 
Subject: [R] FW: Document1
Message-ID: <3542A1BF5AE1984D9FF577DA2CF8BA98245B34@MSX2>



>  -----Original Message-----
> From: 	Brett Stansfield  
> Sent:	Wednesday, 19 January 2005 4:25 p.m.
> To:	'S.Ganesh at massey.ac.nz'
> Subject:	Document1
> 
>  <<Doc1.doc>>  I was trying to get R to analyse one variable of the file
> Chicken Weight. when I ask 
> 
> hist(data$weight)
> R says "x must be numeric"
> yet when I check the data using Editor, it says quite clearly that
> variable name weight is numeric.
> what am I doing wrong here? I have attached a view from the console
> 
> Brett
> Brett Stansfield 
> Environmental Scientist - Water Quality 
> Hawke's Bay Regional Council 
> 102 Vautier Street 
> Private Bag 6006 
> Napier 
> Phone (06) 835-9200 extn 9334 
> Fax (06) 835-3601 
> 
> 

From brett at hbrc.govt.nz  Wed Feb  2 03:57:11 2005
From: brett at hbrc.govt.nz (Brett Stansfield)
Date: Wed, 2 Feb 2005 15:57:11 +1300 
Subject: [R] FW: Document1
Message-ID: <3542A1BF5AE1984D9FF577DA2CF8BA98245B35@MSX2>



>  -----Original Message-----
> From: 	Brett Stansfield  
> Sent:	Wednesday, 19 January 2005 4:25 p.m.
> To:	'S.Ganesh at massey.ac.nz'
> Subject:	Document1
> 
>  <<Doc1.doc>>  I was trying to get R to analyse one variable of the file
> Chicken Weight. when I ask 
> 
> hist(data$weight)
> R says "x must be numeric"
> yet when I check the data using Editor, it says quite clearly that
> variable name weight is numeric.
> what am I doing wrong here? I have attached a view from the console
> 
> Brett
> Brett Stansfield 
> Environmental Scientist - Water Quality 
> Hawke's Bay Regional Council 
> 102 Vautier Street 
> Private Bag 6006 
> Napier 
> Phone (06) 835-9200 extn 9334 
> Fax (06) 835-3601 
> 
> 

From brett at hbrc.govt.nz  Wed Feb  2 04:17:35 2005
From: brett at hbrc.govt.nz (Brett Stansfield)
Date: Wed, 2 Feb 2005 16:17:35 +1300 
Subject: [R] (no subject)
Message-ID: <3542A1BF5AE1984D9FF577DA2CF8BA98245B37@MSX2>


can you recommend a good manual for R that starts with a data set and gives
demonstrations on what can be done using R? I downloadedR Langauage
definition and An introduction to R but haven't found them overly useful.
I'd really like to be able to follow some tutorials using a dataset or many
datasets. The datasets I have available on R are

Data sets in package 'datasets':

AirPassengers           Monthly Airline Passenger Numbers 1949-1960
BJsales                 Sales Data with Leading Indicator
BJsales.lead (BJsales)
                        Sales Data with Leading Indicator
BOD                     Biochemical Oxygen Demand
CO2                     Carbon Dioxide uptake in grass plants
ChickWeight             Weight versus age of chicks on different diets
DNase                   Elisa assay of DNase
EuStockMarkets          Daily Closing Prices of Major European Stock
                        Indices, 1991-1998
Formaldehyde            Determination of Formaldehyde
HairEyeColor            Hair and Eye Color of Statistics Students
Harman23.cor            Harman Example 2.3
Harman74.cor            Harman Example 7.4
Indometh                Pharmacokinetics of Indomethicin
InsectSprays            Effectiveness of Insect Sprays
JohnsonJohnson          Quarterly Earnings per Johnson & Johnson Share
LakeHuron               Level of Lake Huron 1875-1972
LifeCycleSavings        Intercountry Life-Cycle Savings Data
Loblolly                Growth of Loblolly pine trees
Nile                    Flow of the River Nile
Orange                  Growth of orange trees
OrchardSprays           Potency of Orchard Sprays
PlantGrowth             Results from an Experiment on Plant Growth
Puromycin               Reaction velocity of an enzymatic reaction
Seatbelts               Road Casualties in Great Britain 1969-84
Theoph                  Pharmacokinetics of theophylline
Titanic                 Survival of passengers on the Titanic
ToothGrowth             The Effect of Vitamin C on Tooth Growth in
                        Guinea Pigs
UCBAdmissions           Student Admissions at UC Berkeley
UKDriverDeaths          Road Casualties in Great Britain 1969-84
UKgas                   UK Quarterly Gas Consumption
USAccDeaths             Accidental Deaths in the US 1973-1978
USArrests               Violent Crime Rates by US State
USJudgeRatings          Lawyers' Ratings of State Judges in the US
                        Superior Court
USPersonalExpenditure   Personal Expenditure Data
VADeaths                Death Rates in Virginia (1940)
WWWusage                Internet Usage per Minute
WorldPhones             The World's Telephones
ability.cov             Ability and Intelligence Tests
airmiles                Passenger Miles on Commercial US Airlines,
                        1937-1960
airquality              New York Air Quality Measurements
anscombe                Anscombe's Quartet of "Identical" Simple
                        Linear Regressions
attenu                  The Joyner-Boore Attenuation Data
attitude                The Chatterjee-Price Attitude Data
austres                 Quarterly Time Series of the Number of
                        Australian Residents
beaver1 (beavers)       Body Temperature Series of Two Beavers
beaver2 (beavers)       Body Temperature Series of Two Beavers
cars                    Speed and Stopping Distances of Cars
chickwts                Chicken Weights by Feed Type
co2                     Mauna Loa Atmospheric CO2 Concentration
discoveries             Yearly Numbers of Important Discoveries
esoph                   Smoking, Alcohol and (O)esophageal Cancer
euro                    Conversion Rates of Euro Currencies
euro.cross (euro)       Conversion Rates of Euro Currencies
eurodist                Distances Between European Cities
faithful                Old Faithful Geyser Data
fdeaths (UKLungDeaths)
                        Monthly Deaths from Lung Diseases in the UK
freeny                  Freeny's Revenue Data
freeny.x (freeny)       Freeny's Revenue Data
freeny.y (freeny)       Freeny's Revenue Data
infert                  Infertility after Spontaneous and Induced
                        Abortion
iris                    Edgar Anderson's Iris Data
iris3                   Edgar Anderson's Iris Data
islands                 Areas of the World's Major Landmasses
ldeaths (UKLungDeaths)
                        Monthly Deaths from Lung Diseases in the UK
lh                      Luteinizing Hormone in Blood Samples
longley                 Longley's Economic Regression Data
lynx                    Annual Canadian Lynx trappings 1821-1934
mdeaths (UKLungDeaths)
                        Monthly Deaths from Lung Diseases in the UK
morley                  Michaelson-Morley Speed of Light Data
mtcars                  Motor Trend Car Road Tests
nhtemp                  Average Yearly Temperatures in New Haven
nottem                  Average Monthly Temperatures at Nottingham,
                        1920-1939
precip                  Annual Precipitation in US Cities
presidents              Quarterly Approval Ratings of US Presidents
pressure                Vapor Pressure of Mercury as a Function of
                        Temperature
quakes                  Locations of Earthquakes off Fiji
randu                   Random Numbers from Congruential Generator
                        RANDU
rivers                  Lengths of Major North American Rivers
rock                    Measurements on Petroleum Rock Samples
sleep                   Student's Sleep Data
stack.loss (stackloss)
                        Brownlee's Stack Loss Plant Data
stack.x (stackloss)     Brownlee's Stack Loss Plant Data
stackloss               Brownlee's Stack Loss Plant Data
state.abb (state)       US State Facts and Figures
state.area (state)      US State Facts and Figures
state.center (state)    US State Facts and Figures
state.division (state)
                        US State Facts and Figures
state.name (state)      US State Facts and Figures
state.region (state)    US State Facts and Figures
state.x77 (state)       US State Facts and Figures
sunspot.month           Monthly Sunspot Data, 1749-1997
sunspot.year            Yearly Sunspot Data, 1700-1988
sunspots                Monthly Sunspot Numbers, 1749-1983
swiss                   Swiss Fertility and Socioeconomic Indicators
                        (1888) Data
treering                Yearly Treering Data, -6000-1979
trees                   Girth, Height and Volume for Black Cherry
                        Trees
uspop                   Populations Recorded by the US Census
volcano                 Topographic Information on Auckland's Maunga
                        Whau Volcano
warpbreaks              The Number of Breaks in Yarn during Weaving
women                   Average Heights and Weights for American Women


Use 'data(package = .packages(all.available = TRUE))'
to list the data sets in all *available* packages.

Brett Stansfield 
Environmental Scientist - Water Quality 
Hawke's Bay Regional Council 
102 Vautier Street 
Private Bag 6006 
Napier 
Phone (06) 835-9200 extn 9334 
Fax (06) 835-3601



From MSchwartz at MedAnalytics.com  Wed Feb  2 04:27:43 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Tue, 01 Feb 2005 21:27:43 -0600
Subject: [R] selecting subsets of data for analysis
In-Reply-To: <3542A1BF5AE1984D9FF577DA2CF8BA98245B2E@MSX2>
References: <3542A1BF5AE1984D9FF577DA2CF8BA98245B2E@MSX2>
Message-ID: <1107314863.6303.26.camel@horizons.localdomain>

On Wed, 2005-02-02 at 13:54 +1300, Brett Stansfield wrote:
> can you tell me how to ask R to analyse a subset of data
> 
> eg. supposing the data set consists of 9 columns and I only want R to
> analyse columns 1, 3 and 5 
> 
> how would I command R to conduct eg. boxplots of those variables only?

#Create a matrix with 9 columns
mat <- matrix(rnorm(90), ncol = 9)

# create a boxplot from cols 1,3 and 5
boxplot(data.frame(mat[, c(1, 3, 5)]))

In this case, passing the 3 columns of the matrix as a data frame
enables boxplot() to plot the 3 columns in a single call. There is an
example of this in ?boxplot.

You might also want to review the help for extracting sections of multi-
dimensional objects:

?Extract or ?"["

and the examples therein.

HTH,

Marc Schwartz



From MSchwartz at MedAnalytics.com  Wed Feb  2 04:31:57 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Tue, 01 Feb 2005 21:31:57 -0600
Subject: [R] GUI under linux for R
In-Reply-To: <20050202014207.GA19000@localhost>
References: <20050202014207.GA19000@localhost>
Message-ID: <1107315117.6303.31.camel@horizons.localdomain>

On Tue, 2005-02-01 at 19:42 -0600, msck9 at mizzou.edu wrote:
> Hi, 
>  I'm wondering whether there is GUI under linux. I checked the website,
>  it has Emacs(ESS), IDE/Scrip Editors etc. I am looking for one which is
>  similar to the GUI under windows.

I am not sure that it is worthwhile at this point, but there is a GNOME
based interface available under Linux. More information is available on
page 16 of the R Administration Manual.

Otherwise, I would strongly advise the use of ESS with either emacs or
Xemacs under Linux. It has a bit of a learning curve, but as with R
itself, you will wonder how you ever did without it.

HTH,

Marc Schwartz



From MSchwartz at MedAnalytics.com  Wed Feb  2 05:08:04 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Tue, 01 Feb 2005 22:08:04 -0600
Subject: [R] (no subject)
In-Reply-To: <3542A1BF5AE1984D9FF577DA2CF8BA98245B37@MSX2>
References: <3542A1BF5AE1984D9FF577DA2CF8BA98245B37@MSX2>
Message-ID: <1107317285.19609.12.camel@horizons.localdomain>

On Wed, 2005-02-02 at 16:17 +1300, Brett Stansfield wrote:
> can you recommend a good manual for R that starts with a data set and gives
> demonstrations on what can be done using R? I downloadedR Langauage
> definition and An introduction to R but haven't found them overly useful.
> I'd really like to be able to follow some tutorials using a dataset or many
> datasets. The datasets I have available on R are

Brett,

First, review the R Posting Guide at:

http://www.r-project.org/posting-guide.html

There are good pointers as to how to get help with R.

The R Language Definition document is a reference, not a tutorial. You
wouldn't read the OED to learn how to speak English right?

If you have found the Intro To R document lacking in some fashion, there
are other Contributed documents available at:

http://cran.r-project.org/other-docs.html

which is referenced in the Posting Guide.

I would suggest starting with those.

HTH,

Marc Schwartz



From MSchwartz at MedAnalytics.com  Wed Feb  2 05:37:25 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Tue, 01 Feb 2005 22:37:25 -0600
Subject: [R] postscript symbols?
In-Reply-To: <200502020248.j122mb1A007113@hypatia.math.ethz.ch>
References: <200502020248.j122mb1A007113@hypatia.math.ethz.ch>
Message-ID: <1107319046.19609.17.camel@horizons.localdomain>

On Tue, 2005-02-01 at 18:48 -0800, ivo_welch-rstat8303 at mailblocks.com
wrote:
> dear R wizards:
> 
> is it possible to use a postscript font symbol as a plot symbol?    in 
> particular, I want to use the four postscript symbols for playing cards 
> (club, heart, spade, diamond) as points.  In LaTeX, these four are
> 
> \Pisymbol{psy}{"A7} \Pisymbol{psy}{"A8}
> \Pisymbol{psy}{"A9} \Pisymbol{psy}{"A10}
> 
> and what I would love to do is place them, at say, (x=1,y=1), 
> (x=2,y=2), (x=3,y=3) and (x=4,y=4).  Help appreciated---or merely a 
> note that this is impossible.


Ivo,

This may not be exactly what you want, but...

Using the Hershey vector fonts, you can plot (using text()) a variety of
symbols.

See ?Hershey and demo(Hershey) for more information.

Here is an example:

plot(0:5, 0:5, type = "n")

text(1:4, 1:4, vfont = c("sans serif", "plain"), 
     labels = c("\\CL", "\\DI", "\\HE", "\\SP"), cex = 2)

The set of escaped characters in the vector for the 'labels' argument
are the codes for clubs, diamonds, hearts and spades, respectively.

HTH,

Marc Schwartz



From jjonphl at gmail.com  Wed Feb  2 05:48:02 2005
From: jjonphl at gmail.com (miguel manese)
Date: Wed, 2 Feb 2005 12:48:02 +0800
Subject: [R] (no subject)
In-Reply-To: <3542A1BF5AE1984D9FF577DA2CF8BA98245B37@MSX2>
References: <3542A1BF5AE1984D9FF577DA2CF8BA98245B37@MSX2>
Message-ID: <d35b9da60502012048391afb30@mail.gmail.com>

try http://cran.r-project.org/other-docs.html

jon

On Wed, 2 Feb 2005 16:17:35 +1300, Brett Stansfield <brett at hbrc.govt.nz> wrote:
> 
> can you recommend a good manual for R that starts with a data set and gives
> demonstrations on what can be done using R? I downloadedR Langauage
> definition and An introduction to R but haven't found them overly useful.
> I'd really like to be able to follow some tutorials using a dataset or many
> datasets. The datasets I have available on R are



From ligges at statistik.uni-dortmund.de  Wed Feb  2 09:53:43 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 02 Feb 2005 09:53:43 +0100
Subject: [R] FW: Document1
In-Reply-To: <3542A1BF5AE1984D9FF577DA2CF8BA98245B35@MSX2>
References: <3542A1BF5AE1984D9FF577DA2CF8BA98245B35@MSX2>
Message-ID: <42009517.2080806@statistik.uni-dortmund.de>

Brett Stansfield wrote:

> 
>> -----Original Message-----
>>From: 	Brett Stansfield  
>>Sent:	Wednesday, 19 January 2005 4:25 p.m.
>>To:	'S.Ganesh at massey.ac.nz'
>>Subject:	Document1
>>
>> <<Doc1.doc>>  I was trying to get R to analyse one variable of the file
>>Chicken Weight. when I ask 
>>
>>hist(data$weight)
>>R says "x must be numeric"
>>yet when I check the data using Editor, it says quite clearly that
>>variable name weight is numeric.

Guess you have a factor rather than numeric. Use R functions to check 
it, e.g. str().



>>what am I doing wrong here? I have attached a view from the console


docs are stripped.


Uwe Ligges


>>Brett
>>Brett Stansfield 
>>Environmental Scientist - Water Quality 
>>Hawke's Bay Regional Council 
>>102 Vautier Street 
>>Private Bag 6006 
>>Napier 
>>Phone (06) 835-9200 extn 9334 
>>Fax (06) 835-3601 
>>
>>
>>
>>
>>------------------------------------------------------------------------
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Christoph.Scherber at uni-jena.de  Wed Feb  2 09:58:16 2005
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Wed, 02 Feb 2005 09:58:16 +0100
Subject: [R] publishing random effects from lme
Message-ID: <42009628.8060001@uni-jena.de>

Dear all,

Suppose I have a linear mixed-effects model (from the package nlme) with 
nested random effects (see below); how would I present the results from 
the random effects part in a publication?

Specifically, I?d like to know:
(1) What is the total variance of the random effects at each level?
(2) How can I test the significance of the variance components?
(3) Is there something like an "r squared" for the whole model which I 
can state?

The data come from an experiment on plant performance with and without 
insecticide, with and without grasses present, and across different 
levels of plant diversity ("div").

Thanks for your help!
Christoph.

lme(asin(sqrt(response)) ~ treatment + logb(div + 1, 2) + grass,
random =  ~ 1 | plotcode/treatment, na.action = na.exclude, method = "ML")

Linear mixed-effects model fit by maximum likelihood

 Data: NULL
        AIC      BIC  logLik
  -290.4181 -268.719 152.209

 Random effects:
Formula:  ~ 1 | plotcode
        (Intercept)
StdDev:  0.04176364

  Formula:  ~ 1 | treatment %in% plotcode
       (Intercept)   Residual
StdDev:  0.08660458 0.00833387

 Fixed effects: asin(sqrt(response)) ~ treatment + logb(div + 1, 2) + grass
                    Value  Std.Error DF   t-value p-value
    (Intercept)  0.1858065 0.01858581 81  9.997225  <.0001
      treatment  0.0201384 0.00687832 81  2.927803  0.0044
logb(div + 1, 2) -0.0203301 0.00690074 79 -2.946073  0.0042
          grass  0.0428934 0.01802506 79  2.379656  0.0197

 Standardized Within-Group Residuals:
       Min          Q1         Med         Q3       Max
 -0.2033155 -0.05739679 -0.00943737 0.04045958 0.3637217

 Number of Observations: 164
 Number of Groups:
 plotcode ansatz %in% plotcode
      82                  164



From maechler at stat.math.ethz.ch  Wed Feb  2 09:58:34 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 2 Feb 2005 09:58:34 +0100
Subject: [R] =?iso-8859-1?q?New_problem_printing_=B0C_in_plots?=
In-Reply-To: <Pine.LNX.4.61.0502012316020.4387@gannet.stats>
References: <20050201225240.GV22446@hortresearch.co.nz>
	<Pine.LNX.4.61.0502012316020.4387@gannet.stats>
Message-ID: <16896.38458.370851.554185@stat.math.ethz.ch>

>>>>> "BDR" == Prof Brian Ripley <ripley at stats.ox.ac.uk>
>>>>>     on Tue, 1 Feb 2005 23:33:37 +0000 (GMT) writes:

    BDR> That this prints as an octal escape was always the
    BDR> intention:

excuse me Brian, but "always" is not entirely correct:
Originally (say 6-8 years ago), the intention was really something like

  `` iso-latin-1 (= iso-8859-1) should work everywhere;
     we don't care yet for anything else ''

and this did work for the most important cases of
ESS, console, X11(), postscript()  {but not in some others, IIRC}.


    BDR> it is your OS that is telling R that it is
    BDR> not a printable character.

yes. Patrick: it's really the case that recent versions of Linux
and other OSes AFAIK really behave differently :  They default
to set locales based on UTF-8 whereas before, often locales
where based on iso-* (e.g. iso-8859-1 for "Western Europe"-like).

And even more problematically (if you want to stay back
continuing to use iso-8859-x instead of UTF-8): Man pages and
other files are delivered encoded in UTF-8 as well.
So you are more or less urged to go along with the wave...

    BDR> What locale are you in? For me

    BDR> 1) In en_GB this works (correct, as that is charset
    BDR> ISO-8859-1)

and the same for me with  de_CH  (but *not* with de_CH.UTF-8
which is what Redhat will set in some circumstances when you
tell it that you are in Zurich ...).

    BDR> 2) In C, I get "25\260C" (correct, as that is not an ASCII char)

    BDR> My guess is that you have R running in a C locale and
    BDR> emacs in a UTF-8 locale, since the UTF-8 representation
    BDR> of that symbol is c2b0, in octal \302\260.  If that is
    BDR> what is going on, 1.8.1 would have been equally
    BDR> confused (it might have printed UTF-8, but it would not
    BDR> plot it), so I suspect the important change is not the
    BDR> one you think it is.

    BDR> Note that at least one set of RPMs now runs R in the C
    BDR> locale (but that's not part of R per se).  If you run
    BDR> in en_NZ I think you will find R works to your taste.
    BDR> The good news is that R-devel already supports
    BDR> en_NZ.utf8, and so 2.1.0 will in a couple of months.

    ......

Martin



From ripley at stats.ox.ac.uk  Wed Feb  2 10:13:25 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 2 Feb 2005 09:13:25 +0000 (GMT)
Subject: [R] =?iso-8859-1?q?New_problem_printing_=B0C_in_plots?=
In-Reply-To: <16896.38458.370851.554185@stat.math.ethz.ch>
References: <20050201225240.GV22446@hortresearch.co.nz>
	<Pine.LNX.4.61.0502012316020.4387@gannet.stats>
	<16896.38458.370851.554185@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.61.0502020905480.19219@gannet.stats>

On Wed, 2 Feb 2005, Martin Maechler wrote:

>>>>>> "BDR" == Prof Brian Ripley <ripley at stats.ox.ac.uk>
>>>>>>     on Tue, 1 Feb 2005 23:33:37 +0000 (GMT) writes:
>
>    BDR> That this prints as an octal escape was always the
>    BDR> intention:
>
> excuse me Brian, but "always" is not entirely correct:
> Originally (say 6-8 years ago), the intention was really something like
>
>  `` iso-latin-1 (= iso-8859-1) should work everywhere;
>     we don't care yet for anything else ''

We do say (and have for at least 7 years) that which are valid characters 
was locale-dependent.  So we did not implement your policy, instead using 
isxxxx functions to do the testing, not entirely consistently.

It seems that older systems allowed non-ASCII Latin-1 characters in the
C locale, which is not correct according to the POSIX definition of that 
locale (but I think allowed by ISO C90).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Wed Feb  2 10:24:27 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 02 Feb 2005 10:24:27 +0100
Subject: package checks and tests;  was: Re: [R] A "rude" question
In-Reply-To: <Pine.LNX.4.58L0.0502012011080.8161@est.ufpr.br>
References: <20050127050951.GA26565@localhost>	<41FFF573.6010302@bank-banque-canada.ca>
	<41FFFB1C.8080902@pdf.com>
	<Pine.LNX.4.58L0.0502012011080.8161@est.ufpr.br>
Message-ID: <42009C4B.9080604@statistik.uni-dortmund.de>

Paulo Justiniano Ribeiro Jr wrote:

> One problem with distributing packages with the test sub-directory
> is that this can overload the daily tests in the CRAN machines
> 
> My workaround for that  is:
> For the geoR package I run the tests in my machine but remove the tests
> directory when submitting to CRAN.
> Also I mantain a package web-page where I make the version
> with tests available for downloading

[moved to R-devel, chosen a more sensible subject]

If tests are running in a reasonable amount of time, it does not make 
sense to remove them. People want to install the package on a huge 
number of platforms you probably have not tested the package on 
yourself. The only solution is to distribute the tests as well.

Overloading the daily tests on CRAN is another issue (strong related, 
though). Installing and checking (in maintainer's mode, i.e. without 
double-installing) all CRAN packages under R-release for Windows takes 8 
hours on a (dual, but using only one) Xeon 3.06GHz machine these days.
The checks (without isntallation) of the 15 packages with most intensive 
  tests take ~1 hour.

Uwe Ligges





> P.J.
> 
> On Tue, 1 Feb 2005, Spencer Graves wrote:
> 
> 
>>Hi, Paul:
>>
>>      How can I access "the package tests directory" you mentioned?
>>Only one of the 52 subdirectories of "library" in my current
>>installation of R 2.0.1 has a "test" folder.
>>
>>      Thanks,
>>      Spencer Graves
>>
>>Paul Gilbert wrote:
>>
>>
>>>One point that did not get mentioned in this discussion, and I believe
>>>deserves
>>>much more publicity, is the impact of packages tests. The design of
>>>the package
>>>system allows package developers to put tests in packages, and these
>>>are checked
>>>regularly (see <http://cran.at.r-project.org/contrib/checkSummary.html>).
>>>
>>>These are intended to test the package functionality, but also give R
>>>what is
>>>perhaps the largest test suite of any statistical software (certainly
>>>the most
>>>quickly growing). While any single package's test will never guarantee
>>>that
>>>the package works perfectly, the ensemble goes a long way toward ensuring
>>>that core R functionality behaves as intended. It seems unlikely to me
>>>that any
>>>commercial effort will ever be able to catch up.
>>>
>>>There are several ways that tests can add to our confidence that
>>>calculations can be trusted. They can
>>>  - check against theoretical results
>>>  - check against published results
>>>  - check against results from other software
>>>  - check that calculations done in different ways give the same result
>>>  - check that monte carlo experiments give distributions that are
>>>consistent
>>>     with expected results
>>>
>>>Some of these are relatively time consuming to set up and check the
>>>first time,
>>>but after that they can be automatic.
>>>
>>>If you have particular calculations with specific packages that you
>>>are especially
>>>concerned about, I encourage you to participate by devising good tests
>>>and sending
>>>them to the package developers. (But first check the tests they are
>>>already doing
>>>in the package tests directory.)
>>>
>>>Paul Gilbert
>>>
>>>msck9 at mizzou.edu wrote:
>>>
>>>
>>>>Dear all, I am beginner using R. I have a question about it. When you
>>>>use it,
>>>>since it is written by so many authors, how do you know that the
>>>>results are trustable?(I don't want to affend anyone, also I trust
>>>>people). But I think this should be a question.
>>>>
>>>>Thanks,
>>>>Ming
>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide!
>>>>http://www.R-project.org/posting-guide.html
>>>>
>>>>
>>>>
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide!
>>>http://www.R-project.org/posting-guide.html
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>
> 
> 
> Paulo Justiniano Ribeiro Jr
> LEG (Laborat?rio de Estat?stica e Geoinforma??o)
> Departamento de Estat?stica
> Universidade Federal do Paran?
> Caixa Postal 19.081
> CEP 81.531-990
> Curitiba, PR  -  Brasil
> Tel: (+55) 41 361 3573
> Fax: (+55) 41 361 3141
> e-mail: paulojus at est.ufpr.br
> http://www.est.ufpr.br/~paulojus
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Wed Feb  2 10:57:17 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 2 Feb 2005 09:57:17 +0000 (GMT)
Subject: [R] postscript symbols?
In-Reply-To: <1107319046.19609.17.camel@horizons.localdomain>
References: <200502020248.j122mb1A007113@hypatia.math.ethz.ch>
	<1107319046.19609.17.camel@horizons.localdomain>
Message-ID: <Pine.LNX.4.61.0502020947440.19603@gannet.stats>

On Tue, 1 Feb 2005, Marc Schwartz wrote:

> On Tue, 2005-02-01 at 18:48 -0800, ivo_welch-rstat8303 at mailblocks.com
> wrote:
>> dear R wizards:
>>
>> is it possible to use a postscript font symbol as a plot symbol?    in
>> particular, I want to use the four postscript symbols for playing cards
>> (club, heart, spade, diamond) as points.  In LaTeX, these four are
>>
>> \Pisymbol{psy}{"A7} \Pisymbol{psy}{"A8}
>> \Pisymbol{psy}{"A9} \Pisymbol{psy}{"A10}
>>
>> and what I would love to do is place them, at say, (x=1,y=1),
>> (x=2,y=2), (x=3,y=3) and (x=4,y=4).  Help appreciated---or merely a
>> note that this is impossible.

Font 5 is the symbol font, so

plot(1:5, type="n")
points(1:4, 1:4, pch=167:170, font=5)

does this (not in that order, but you can make the mapping from that plot).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bates at stat.wisc.edu  Wed Feb  2 11:20:36 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 02 Feb 2005 04:20:36 -0600
Subject: [R] polynomials REML and ML in nlme
In-Reply-To: <200502012130.j11LUJpd019287@compton.gene.com>
References: <200502012130.j11LUJpd019287@compton.gene.com>
Message-ID: <4200A974.2080300@stat.wisc.edu>

Berton Gunter wrote:
> Duncan:
> 
> Bates and Pinheiro do show explicitly and in detail that the restricted log
> likelihood depends on the parameterization: "An important difference between
> the likelihood function and the restricted likelihood is that the former is
> invariant to one-to-one reparametrizations of the fixed effects ... while
> the latter is not." (p.76)
> 
> So that answers both your questions: With different fixed effect
> parameterizations you get different restricted log likelihoods even for the
> same random effects specifications. So you cannot compare models and you see
> differences of the sort you saw. That's just the way REML works.
> 
> I believe the the basic issue can be summarized as: the restricted log
> likelihood is a nonlinear function of the fixed effects specification. Hence
> linear transformations (reparametrizations) of the fixed effects change the
> log likelihood. But if you want more than that, get the book (or another
> reference of your choice). You could also try googling "REML".
> 
> All the above is subject to correction by Doug Bates, of course.

Doug has nothing to add.  Good explanation.  Thanks.



From ripley at stats.ox.ac.uk  Wed Feb  2 11:39:15 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 2 Feb 2005 10:39:15 +0000 (GMT)
Subject: [R] R CMD BATCH character limit?
In-Reply-To: <20050202015426.GO1436@hplhtang1.hpl.hp.com>
References: <20050202015426.GO1436@hplhtang1.hpl.hp.com>
Message-ID: <Pine.LNX.4.61.0502021028400.20029@gannet.stats>

On Tue, 1 Feb 2005, Hsiu-Khuern Tang wrote:

> Is there a limit on the number of characters in an invocation like
>
> R CMD BATCH --opt1=val1 --opt2=val2 ... --save-to=C:\very\long\string ..\R\script.R
>
> ?

Yes, but that is not a valid invocation of R.

> I am running R 2.0.1 on Windows XP, and when running a long BATCH
> command (about 400--500 characters) the last option gets mangled, as can

There is a Windows limit on the length of command lines, and I believe you 
have hit it.  It looks like R.exe/Rcmd.exe were written assuming it is 
MAX_PATH, hence about 260, which it used to be in Win95.

You could use

rterm --args --opt1=val1 --opt2=val2 ... --save-to=C:\very\long\string < foo > bar

instead: BATCH is just a convenient shorthand (and see the rw-FAQ for the 
details).

Shells also have limits, so check this for the one you used.

> be seen from a warning in script.Rout that looks like this:
>
> WARNING: unknown option
> +--save-to=C:\very\long\str..\R\script.Rout
>
> Note that "string" is truncated to "str" _and_ concatenated with the
> implicit batch output file "..\R\script.Rout".  (The "unknown option"
> warning is expected; I am just using commandArgs() within the script to
> pick out the arguments.)
>
> I have tried specifying --args before the options, with the same result,
> except that the truncation appears earlier.

--args would be needed for this to make any sense.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ramasamy at cancer.org.uk  Wed Feb  2 12:18:38 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Wed, 02 Feb 2005 11:18:38 +0000
Subject: [R] (no subject)
In-Reply-To: <3542A1BF5AE1984D9FF577DA2CF8BA98245B37@MSX2>
References: <3542A1BF5AE1984D9FF577DA2CF8BA98245B37@MSX2>
Message-ID: <1107343118.6072.10.camel@ndmpc126.orc.ox.ac.uk>

Please use a sensible subject line. In short, you need help().

Most R functions have a section called "Examples" in the help file. E.g.
help("plot") or even help("help"). Granted this will give you only
snippets of a whole analysis but it is helpful nonetheless.

Since you have already identified the datasets you have, see the help
section of help("AirPassengers") for a more complete analysis.

Perhaps articles and books listed under "Contributed" and "Publications"
under Documentations tab of www.R-project.org might help.

Regards, Adai



On Wed, 2005-02-02 at 16:17 +1300, Brett Stansfield wrote:
> can you recommend a good manual for R that starts with a data set and gives
> demonstrations on what can be done using R? I downloadedR Langauage
> definition and An introduction to R but haven't found them overly useful.
> I'd really like to be able to follow some tutorials using a dataset or many
> datasets. The datasets I have available on R are
> 
> Data sets in package 'datasets':
> 
> AirPassengers           Monthly Airline Passenger Numbers 1949-1960
> BJsales                 Sales Data with Leading Indicator
> BJsales.lead (BJsales)
>                         Sales Data with Leading Indicator
> BOD                     Biochemical Oxygen Demand
> CO2                     Carbon Dioxide uptake in grass plants
> ChickWeight             Weight versus age of chicks on different diets
> DNase                   Elisa assay of DNase
> EuStockMarkets          Daily Closing Prices of Major European Stock
>                         Indices, 1991-1998
> Formaldehyde            Determination of Formaldehyde
> HairEyeColor            Hair and Eye Color of Statistics Students
> Harman23.cor            Harman Example 2.3
> Harman74.cor            Harman Example 7.4
> Indometh                Pharmacokinetics of Indomethicin
> InsectSprays            Effectiveness of Insect Sprays
> JohnsonJohnson          Quarterly Earnings per Johnson & Johnson Share
> LakeHuron               Level of Lake Huron 1875-1972
> LifeCycleSavings        Intercountry Life-Cycle Savings Data
> Loblolly                Growth of Loblolly pine trees
> Nile                    Flow of the River Nile
> Orange                  Growth of orange trees
> OrchardSprays           Potency of Orchard Sprays
> PlantGrowth             Results from an Experiment on Plant Growth
> Puromycin               Reaction velocity of an enzymatic reaction
> Seatbelts               Road Casualties in Great Britain 1969-84
> Theoph                  Pharmacokinetics of theophylline
> Titanic                 Survival of passengers on the Titanic
> ToothGrowth             The Effect of Vitamin C on Tooth Growth in
>                         Guinea Pigs
> UCBAdmissions           Student Admissions at UC Berkeley
> UKDriverDeaths          Road Casualties in Great Britain 1969-84
> UKgas                   UK Quarterly Gas Consumption
> USAccDeaths             Accidental Deaths in the US 1973-1978
> USArrests               Violent Crime Rates by US State
> USJudgeRatings          Lawyers' Ratings of State Judges in the US
>                         Superior Court
> USPersonalExpenditure   Personal Expenditure Data
> VADeaths                Death Rates in Virginia (1940)
> WWWusage                Internet Usage per Minute
> WorldPhones             The World's Telephones
> ability.cov             Ability and Intelligence Tests
> airmiles                Passenger Miles on Commercial US Airlines,
>                         1937-1960
> airquality              New York Air Quality Measurements
> anscombe                Anscombe's Quartet of "Identical" Simple
>                         Linear Regressions
> attenu                  The Joyner-Boore Attenuation Data
> attitude                The Chatterjee-Price Attitude Data
> austres                 Quarterly Time Series of the Number of
>                         Australian Residents
> beaver1 (beavers)       Body Temperature Series of Two Beavers
> beaver2 (beavers)       Body Temperature Series of Two Beavers
> cars                    Speed and Stopping Distances of Cars
> chickwts                Chicken Weights by Feed Type
> co2                     Mauna Loa Atmospheric CO2 Concentration
> discoveries             Yearly Numbers of Important Discoveries
> esoph                   Smoking, Alcohol and (O)esophageal Cancer
> euro                    Conversion Rates of Euro Currencies
> euro.cross (euro)       Conversion Rates of Euro Currencies
> eurodist                Distances Between European Cities
> faithful                Old Faithful Geyser Data
> fdeaths (UKLungDeaths)
>                         Monthly Deaths from Lung Diseases in the UK
> freeny                  Freeny's Revenue Data
> freeny.x (freeny)       Freeny's Revenue Data
> freeny.y (freeny)       Freeny's Revenue Data
> infert                  Infertility after Spontaneous and Induced
>                         Abortion
> iris                    Edgar Anderson's Iris Data
> iris3                   Edgar Anderson's Iris Data
> islands                 Areas of the World's Major Landmasses
> ldeaths (UKLungDeaths)
>                         Monthly Deaths from Lung Diseases in the UK
> lh                      Luteinizing Hormone in Blood Samples
> longley                 Longley's Economic Regression Data
> lynx                    Annual Canadian Lynx trappings 1821-1934
> mdeaths (UKLungDeaths)
>                         Monthly Deaths from Lung Diseases in the UK
> morley                  Michaelson-Morley Speed of Light Data
> mtcars                  Motor Trend Car Road Tests
> nhtemp                  Average Yearly Temperatures in New Haven
> nottem                  Average Monthly Temperatures at Nottingham,
>                         1920-1939
> precip                  Annual Precipitation in US Cities
> presidents              Quarterly Approval Ratings of US Presidents
> pressure                Vapor Pressure of Mercury as a Function of
>                         Temperature
> quakes                  Locations of Earthquakes off Fiji
> randu                   Random Numbers from Congruential Generator
>                         RANDU
> rivers                  Lengths of Major North American Rivers
> rock                    Measurements on Petroleum Rock Samples
> sleep                   Student's Sleep Data
> stack.loss (stackloss)
>                         Brownlee's Stack Loss Plant Data
> stack.x (stackloss)     Brownlee's Stack Loss Plant Data
> stackloss               Brownlee's Stack Loss Plant Data
> state.abb (state)       US State Facts and Figures
> state.area (state)      US State Facts and Figures
> state.center (state)    US State Facts and Figures
> state.division (state)
>                         US State Facts and Figures
> state.name (state)      US State Facts and Figures
> state.region (state)    US State Facts and Figures
> state.x77 (state)       US State Facts and Figures
> sunspot.month           Monthly Sunspot Data, 1749-1997
> sunspot.year            Yearly Sunspot Data, 1700-1988
> sunspots                Monthly Sunspot Numbers, 1749-1983
> swiss                   Swiss Fertility and Socioeconomic Indicators
>                         (1888) Data
> treering                Yearly Treering Data, -6000-1979
> trees                   Girth, Height and Volume for Black Cherry
>                         Trees
> uspop                   Populations Recorded by the US Census
> volcano                 Topographic Information on Auckland's Maunga
>                         Whau Volcano
> warpbreaks              The Number of Breaks in Yarn during Weaving
> women                   Average Heights and Weights for American Women
> 
> 
> Use 'data(package = .packages(all.available = TRUE))'
> to list the data sets in all *available* packages.
> 
> Brett Stansfield 
> Environmental Scientist - Water Quality 
> Hawke's Bay Regional Council 
> 102 Vautier Street 
> Private Bag 6006 
> Napier 
> Phone (06) 835-9200 extn 9334 
> Fax (06) 835-3601
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From sdavis2 at mail.nih.gov  Wed Feb  2 12:25:16 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Wed, 2 Feb 2005 06:25:16 -0500
Subject: [R] R-Project Plot: Creating browser map tags
In-Reply-To: <BAY23-F4252B5ECE438B0B9F8548CEE7E0@phx.gbl>
References: <BAY23-F4252B5ECE438B0B9F8548CEE7E0@phx.gbl>
Message-ID: <1CE51552-750D-11D9-8BE3-000D933565E8@mail.nih.gov>

These two links may be useful.

http://tolstoy.newcastle.edu.au/R/help/02b/2545.html
http://tolstoy.newcastle.edu.au/R/help/04/04/1574.html

Sean

On Feb 1, 2005, at 7:17 PM, Jon Hurley wrote:

> Hi,
>
> I have started looking into using R for generating graphs displayed in 
> a web browser and it produces really nice graphics. One of the 
> requirements I have is to construct HTML map tags over the graphs so 
> that users can click on elements of the graph (e.g. bars of a 
> histogram) and either see interesting information or navigate to 
> another page.
>
> I see how to produce JPEG images with R that can be readily embedded 
> in an HTML page but did not see how to output information to produce 
> maps.
>
> Could anyone tell if this is possible and if so how?
>
> Many thanks -- Jon
>
>
> Security. http://clinic.mcafee.com/clinic/ibuy/campaign.asp?cid=3963
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From iwhite at staffmail.ed.ac.uk  Wed Feb  2 12:38:05 2005
From: iwhite at staffmail.ed.ac.uk (I M S White)
Date: Wed, 2 Feb 2005 11:38:05 +0000 (GMT)
Subject: [R] polynomials REML and ML in nlme
In-Reply-To: <001a01c5088f$627d46d0$1002010a@DuncanToshiba>
References: <001a01c5088f$627d46d0$1002010a@DuncanToshiba>
Message-ID: <Pine.GSO.4.58.0502021122430.27555@holyrood.ed.ac.uk>

The REML loglikelihood includes a term -(1/2)logdet(X'WX) where X is the
design matrix for the fixed effects and W is the inverse covariance matrix
for the observations. Under reparametrisation, X becomes XM with M a
non-singular matrix, and the REML loglikelihood changes by logdet(M).


On Tue, 1 Feb 2005, dgoliche wrote:

> Hello everyone,
>
> I hope this is a fair enough question, but I don?t have access to a copy
> of Bates and Pinheiro. It is probably quite obvious but the answer might
> be of general interest.
> If I fit a fixed effect with an added quadratic term and then do it as
> an orthogonal polynomial using maximum likelihood I get the expected
> result- they have the same logLik.
>
> mod2a<-lme(wthole~nplants+I(nplants^2),data=d3,random=~1|field/subplot,m
> ethod="ML")
> mod2b<-lme(wthole~poly(nplants,2),data=d3,random=~1|field/subplot,method
> ="ML")
>
> > anova(mod2a,mod2b)
>       Model df      AIC      BIC    logLik
> mod2a     1  6 6698.231 6723.869 -3343.116
> mod2b     2  6 6698.231 6723.869 -3343.116
>
> However if I fit the two models by REML they are not considered to be
> the same and I get warned.
>
> > anova(mod2a.REML,mod2b.REML)
>            Model df      AIC      BIC    logLik
> mod2a.REML     1  6 6680.616 6706.219 -3334.308
> mod2b.REML     2  6 6666.915 6692.518 -3327.457
>
> Warning message:
> Fitted objects with different fixed effects. REML comparisons are not
> meaningful. in: anova.lme(mod2a.REML, mod2b.REML)
>
> Well yes, I suppose that?s right, they are not the same fixed effects.
> But why does REML give them such different Log likelihoods? And what
> should I do if I want to compare a larger set of models. For example the
> following, admittedly overparameterised model, can be fitted (slowly) by
> either method
>

======================================
I.White
University of Edinburgh
Ashworth Laboratories, West Mains Road
Edinburgh EH9 3JT
Tel: 0131 650 5490  Fax: 0131 650 6564
E-mail: iwhite at staffmail.ed.ac.uk



From wagner at itp.phys.ethz.ch  Wed Feb  2 12:47:31 2005
From: wagner at itp.phys.ethz.ch (Urs Wagner)
Date: Wed, 02 Feb 2005 11:47:31 +0000
Subject: [R] Rpad on WinXP
Message-ID: <4200BDD3.4050208@itp.phys.ethz.ch>

Has anybody tried out Rpad on WinXP? On my system it does not work 
correctly.

Urs



From phgrosjean at sciviews.org  Wed Feb  2 13:24:52 2005
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Wed, 02 Feb 2005 13:24:52 +0100
Subject: [R] Rpad on WinXP
In-Reply-To: <4200BDD3.4050208@itp.phys.ethz.ch>
References: <4200BDD3.4050208@itp.phys.ethz.ch>
Message-ID: <4200C694.5040807@sciviews.org>

Which version of Rpad do you use? It is under development, especially 
for the local Rpad server under Windows.
Best,

Philippe

..............................................<?}))><........
  ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
  ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
  ) ) ) ) )   Mons-Hainaut University, Pentagone (3D08)
( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
  ) ) ) ) )   8, av du Champ de Mars, 7000 Mons, Belgium
( ( ( ( (
  ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.30.54
( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
  ) ) ) ) )
( ( ( ( (    web:   http://www.umh.ac.be/~econum
  ) ) ) ) )          http://www.sciviews.org
( ( ( ( (
..............................................................

Urs Wagner wrote:
> Has anybody tried out Rpad on WinXP? On my system it does not work 
> correctly.
> 
> Urs
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From christoph.lehmann at gmx.ch  Wed Feb  2 13:49:25 2005
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Wed, 02 Feb 2005 13:49:25 +0100
Subject: [Fwd: Re: [R] vectorization of a data-aggregation loop]
Message-ID: <4200CC55.4080006@gmx.ch>

great! many thanks, Phil

Cheers
christoph

Phil Spector wrote:
> Christoph -
>    I think reshape is the function you're looking for:
> 
>> tt <- data.frame(cbind(c(1,1,1,1,1,2,2,2,3,3,3,3),
> + c(10,12,8,33,34,3,27,77,34,45,4,39), c('a', 'b', 'b', 'a', 'c', 'c', 'c',
> + 'a', 'b', 'a', 'b', 'c')))
>>  reshape(aggregate(as.numeric(tt$iwv),list(id=tt$id,type=tt$type),sum),idvar="id",timevar="type",direction="wide") 
>>
>   id x.a x.b x.c
> 1  1   6  13   6
> 2  2  10  NA   7
> 3  3   9  14   7
> 
>                                        - Phil Spector
>                      Statistical Computing Facility
>                      Department of Statistics
>                      UC Berkeley
>                      spector at stat.berkeley.edu
> 
> 
> On Tue, 1 Feb 2005, Christoph Lehmann wrote:
> 
>> Hi
>> I have a simple question:
>>
>> the following data.frame
>>
>>   id iwv type
>> 1   1   1    a
>> 2   1   2    b
>> 3   1  11    b
>> 4   1   5    a
>> 5   1   6    c
>> 6   2   4    c
>> 7   2   3    c
>> 8   2  10    a
>> 9   3   6    b
>> 10  3   9    a
>> 11  3   8    b
>> 12  3   7    c
>>
>> shall be aggregated into the form:
>>
>>  id t.a t.b t.c
>> 1  1   6  13   6
>> 6  2  10   0   7
>> 9  3   9  14   7
>>
>> means for each 'type' (a, b, c) a new column is introduced which
>> gets the sum of iwv for the respective observations 'id'
>>
>> of course I can do this transformation/aggregation in a loop (see 
>> below), but is there a way to do this more efficiently, eg. in using 
>> tapply (or something similar)- since I have lot many rows?
>>
>> thanks for a hint
>>
>> christoph
>>
>> #------------------------------------------------------------------------------ 
>>
>> # the loop-way
>> t <- data.frame(cbind(c(1,1,1,1,1,2,2,2,3,3,3,3), 
>> c(10,12,8,33,34,3,27,77,34,45,4,39), c('a', 'b', 'b', 'a', 'c', 'c', 
>> 'c', 'a', 'b', 'a', 'b', 'c')))
>> names(t) <- c("id", "iwv", "type")
>> t$iwv <- as.numeric(t$iwv)
>> t
>>
>> # define the additional columns (type.a, type.b, type.c)
>> tt <- rep(0, nrow(t) * length(levels(t$type)))
>> dim(tt) <- c(nrow(t), length(levels(t$type)))
>> tt <- data.frame(tt)
>> dimnames(tt)[[2]] <- paste("t.", levels(t$type), sep = "")
>> t <- cbind(t, tt)
>> t
>>
>> obs <- 0
>> obs.previous <- 0
>> row.elim <- rep(FALSE, nrow(t))
>> ta <- which((names(t) == "t.a")) #number of column which codes the 
>> first type
>> r.ctr <- 0
>> for (i in 1:nrow(t)){
>>  obs <- t[i,]$id
>>  if (obs == obs.previous) {
>>    row.elim[i] <- TRUE
>>    r.ctr <- r.ctr + 1 #increment
>>    type.col <- as.numeric(t[i,]$type)
>>    t[i - r.ctr, ta - 1 + type.col] <- t[i - r.ctr, ta - 1 +
>>      type.col] + t[i,]$iwv
>>  }
>>  else {
>>    r.ctr <- 0 #record counter
>>    type.col <- as.numeric(t[i,]$type)
>>    t[i, ta - 1 + type.col] <- t[i,]$iwv
>>  }
>>  obs.previous <- obs
>> }
>>
>> t <- t[!row.elim,]
>> t <- subset(t, select = -c(iwv, type))
>> t
>>
>> #------------------------------------------------------------------------------ 
>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
> 
>



From stonehusky at gmail.com  Wed Feb  2 13:57:19 2005
From: stonehusky at gmail.com (Donna-n-Doug Finner)
Date: Wed, 2 Feb 2005 07:57:19 -0500
Subject: [R] RODBC - connect is failing
In-Reply-To: <Pine.LNX.4.61.0502012109001.31495@gannet.stats>
References: <529e16e0050201115676895f20@mail.gmail.com>
	<Pine.LNX.4.61.0502012109001.31495@gannet.stats>
Message-ID: <529e16e005020204574b4667a6@mail.gmail.com>

Thank you for your reply and thanks for the 'fishing tip'.  I thought,
however, that I had debugged my setup - I tested the ODBC connection
to MySQL directly from the ODBC driver setup and connected
successfully.  If R, RODBC, and ODBC are all telling me 'life is
good', any tips on the next suspect to check?

Thanks again for the help.

Doug

Off Topic Comment:  I realize this isn't an R issue but one of the
joys (sic) of building complex systems is that any particular problem
is never any one application's fault.  I always have difficulty
finding someone who is conversant in all (heck, even most) of the
pieces of a particular puzzle to be able to help me connect the dots. 
Using my problem as an example, it may turn out the the answer is
sitting in the brain of a user on a PHP or Apache or Windows forum who
just happens to use R but doesn't hang here.  Connecting with that
person is the challenge.  I may have to scrap the
Apache/PHP/MySQL/R/Webby Thing design concept and move through a
series of smaller baby steps - R with lots of manual intervention
using text data files; R via batch processes; R via batch processes
with user Q&A actions; R via...etc etc.  I can see the end point, but
there's just a huge trackless thicket between my current possition and
the finished product and all I have is a pocket knife and a song in my
heart.
End of OT Comment.


On Tue, 1 Feb 2005 21:10:21 +0000 (GMT), Prof Brian Ripley
<ripley at stats.ox.ac.uk> wrote:
> The message is from the MySQL ODBC driver, as it indicates.
> You'll have to debug your setup, as this is not an R nor RODBC issue: the
> latter has done its job and connected to the driver.
> 
> On Tue, 1 Feb 2005, Donna-n-Doug Finner wrote:
> 
> > I get the following errors when attempting an ODBC connection to a MySQL db.
> >
> > 1: [RODBC] ERROR: state HY000, Code 0, message [MySQL] [ODBC 3.51
> > Driver] Could not determine the driver name so could not lookup setup
> > library.
> > 2: ODBCConnection failed in: odbcDriverConnect(st, case=case,
> > believeNRows=believeNRows).
> >
> > Win XP sp2
> > MySQL 4.1.8
> > My SQLODBC 3.51.10
> > Package RODBC (latest) is installed.
> >



From jennifer at zeropopup.com  Wed Feb  2 14:06:49 2005
From: jennifer at zeropopup.com (jennifer@zeropopup.com)
Date: Wed, 2 Feb 2005 05:06:49 -0800 (PST)
Subject: [R] Re: Re: Message
In-Reply-To: <200502021306.j12D6eSl079706@addr-mx01.addr.com>
References: <200502021306.j12D6eSl079706@addr-mx01.addr.com>
Message-ID: <200502021306.j12D6nGu004209@addr20.addr.com>

This is an Autoresponder - If you need to to remove our software do this:

1- Go to the View menu on top of your Internet Explorer then >> Toolbars >> and uncheck any blank line or "tiny search bar"

2- Go to www.tinybar.com , at the bottom of the page click on 'Reset your search page back to MSN' - run this file then reboot.



From MSchwartz at MedAnalytics.com  Wed Feb  2 14:34:25 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Wed, 02 Feb 2005 07:34:25 -0600
Subject: [R] postscript symbols?
In-Reply-To: <Pine.LNX.4.61.0502020947440.19603@gannet.stats>
References: <200502020248.j122mb1A007113@hypatia.math.ethz.ch>
	<1107319046.19609.17.camel@horizons.localdomain>
	<Pine.LNX.4.61.0502020947440.19603@gannet.stats>
Message-ID: <1107351265.25434.8.camel@horizons.localdomain>

On Wed, 2005-02-02 at 09:57 +0000, Prof Brian Ripley wrote:
> On Tue, 1 Feb 2005, Marc Schwartz wrote:
> 
> > On Tue, 2005-02-01 at 18:48 -0800, ivo_welch-rstat8303 at mailblocks.com
> > wrote:
> >> dear R wizards:
> >>
> >> is it possible to use a postscript font symbol as a plot symbol?    in
> >> particular, I want to use the four postscript symbols for playing cards
> >> (club, heart, spade, diamond) as points.  In LaTeX, these four are
> >>
> >> \Pisymbol{psy}{"A7} \Pisymbol{psy}{"A8}
> >> \Pisymbol{psy}{"A9} \Pisymbol{psy}{"A10}
> >>
> >> and what I would love to do is place them, at say, (x=1,y=1),
> >> (x=2,y=2), (x=3,y=3) and (x=4,y=4).  Help appreciated---or merely a
> >> note that this is impossible.
> 
> Font 5 is the symbol font, so
> 
> plot(1:5, type="n")
> points(1:4, 1:4, pch=167:170, font=5)
> 
> does this (not in that order, but you can make the mapping from that plot).


That's definitely better.

Can font=5 be added to ?par since it is not there presently in the
details for 'font'.

After seeing the above, I had a recollection of a prior post from Prof.
Ripley on this, which I found here:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/32463.html

The only other places I found it mentioned was in the announcement for R
0.49:

http://finzi.psych.upenn.edu/R/Rhelpold/archive/0042.html

and here:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/2358.html

Thanks!

Marc



From sdavis2 at mail.nih.gov  Wed Feb  2 14:42:22 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Wed, 2 Feb 2005 08:42:22 -0500
Subject: [R] Combining two histograms
Message-ID: <44083422-7520-11D9-8BE3-000D933565E8@mail.nih.gov>

I have data like:

a <- rnorm(20000)
b <- rep(FALSE,20000)
b[sample(1:20000,15000)] <- TRUE

Using Lattice graphics, I can produce two side-by-side histograms quite 
easily by:

histogram(a | b)

However, I would like to produce a "single" histogram with two bars 
within each bin, one for each group, as the groups are in reality very 
slightly different from each other.  The difference isn't evident 
unless one "overlays" the two histograms in some manner.

Thanks,
Sean



From michael.watson at bbsrc.ac.uk  Wed Feb  2 14:56:18 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Wed, 2 Feb 2005 13:56:18 -0000
Subject: [R] Combining two histograms
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950121B9EE@iahce2knas1.iah.bbsrc.reserved>

Sean
If you want two bars for each bin then that sounds more like barplot() could be used.  I know you can tightly control the distance between bars and the width of the bars, and so with a little coding you could probably use that.
 
Mick

	-----Original Message----- 
	From: r-help-bounces at stat.math.ethz.ch on behalf of Sean Davis 
	Sent: Wed 2/2/2005 1:42 PM 
	To: r-help 
	Cc: 
	Subject: [R] Combining two histograms
	
	

	I have data like:
	
	a <- rnorm(20000)
	b <- rep(FALSE,20000)
	b[sample(1:20000,15000)] <- TRUE
	
	Using Lattice graphics, I can produce two side-by-side histograms quite
	easily by:
	
	histogram(a | b)
	
	However, I would like to produce a "single" histogram with two bars
	within each bin, one for each group, as the groups are in reality very
	slightly different from each other.  The difference isn't evident
	unless one "overlays" the two histograms in some manner.
	
	Thanks,
	Sean
	
	______________________________________________
	R-help at stat.math.ethz.ch mailing list
	https://stat.ethz.ch/mailman/listinfo/r-help
	PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From rolf at math.unb.ca  Wed Feb  2 15:03:48 2005
From: rolf at math.unb.ca (Rolf Turner)
Date: Wed, 2 Feb 2005 10:03:48 -0400 (AST)
Subject: [R] postscript symbols?
Message-ID: <200502021403.j12E3mFi022693@erdos.math.unb.ca>

A propos of these symbols, Henrik Bengtsson (Lund University, Sweden)
posted to this list some time ago a very useful function ``plotSymbols'',
which can be slightly modified as follows:

plotSymbols <- function (fn=1) {
    i <- 0:255
    ncol <- 16
    opar <- par(cex.axis = 0.7, mar = c(3, 3, 3, 3) + 0.1)
    plot(i%%ncol, 1 + i%/%ncol, pch=i, font=fn, xlab = "", ylab = "", 
        axes = FALSE)
    axis(1, at = 0:15)
    axis(2, at = 1:16, labels = 0:15 * 16, las = 2)
    axis(3, at = 0:15)
    axis(4, at = 1:16, labels = 0:15 * 16 + 15, las = 2)
    par(opar)
}

You can use this function to see what you get under various font
specifications.  Of course it would help to know a priori that font
number 5 gave you the postscript symbols!

				cheers,

					Rolf Turner
					rolf at math.unb.ca



From p.dalgaard at biostat.ku.dk  Wed Feb  2 15:03:21 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 02 Feb 2005 15:03:21 +0100
Subject: [R] New problem printing =?iso-8859-1?q?=B0C?= in plots
In-Reply-To: <16896.38458.370851.554185@stat.math.ethz.ch>
References: <20050201225240.GV22446@hortresearch.co.nz>
	<Pine.LNX.4.61.0502012316020.4387@gannet.stats>
	<16896.38458.370851.554185@stat.math.ethz.ch>
Message-ID: <x2vf9b5aae.fsf@biostat.ku.dk>

Martin Maechler <maechler at stat.math.ethz.ch> writes:

> yes. Patrick: it's really the case that recent versions of Linux
> and other OSes AFAIK really behave differently :  They default
> to set locales based on UTF-8 whereas before, often locales
> where based on iso-* (e.g. iso-8859-1 for "Western Europe"-like).
> 
> And even more problematically (if you want to stay back
> continuing to use iso-8859-x instead of UTF-8): Man pages and
> other files are delivered encoded in UTF-8 as well.
> So you are more or less urged to go along with the wave...

Yes. I don't think you do want to stay with the iso-8859-x, even
though it is going to be a pain to switch for those of us that have
masses of files (and file names!) written in 8bit encodings. 

In retrospect, the iso-8859 encodings (and IBM code pages too, for
that matter) were a huge mistake, precisely because they let you have
files in multiple non-ascii encodings without a way to specify which
one was used.

Those of us who have tried every encoding of our national alphabets
since ISO-646 (not to mention FIELDATA) are getting a bit tired of it
all, but there might be some hope that Unicode/UTF-8 is the end of our
troubles (unless Microsoft manages to screw everything up again
by their use of UTF-16...).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From mikewhite.diu at tiscali.co.uk  Wed Feb  2 10:50:00 2005
From: mikewhite.diu at tiscali.co.uk (Mike White)
Date: Wed, 2 Feb 2005 09:50:00 -0000
Subject: [R] Extracting a numeric prefix from a string
References: <XFMail.050131160949.Ted.Harding@nessie.mcc.ac.uk>
	<x2y8e92o93.fsf@biostat.ku.dk>
Message-ID: <001c01c50931$34496580$4f062850@FSSFQCV7BGDVED>

Thanks for you contributions.  Jonnes' solution (after sorting) works fine
for my purposes but it would be useful to have a function that works for any
numeric prefix.  Another case to include would be a signed numeric:
x<-c("+12.3.abc", "-0.12xyz")

Mike
----- Original Message -----
From: "Peter Dalgaard" <p.dalgaard at biostat.ku.dk>
To: <ted.harding at nessie.mcc.ac.uk>
Cc: "R user" <R-user at zutt.org>; <R-help at stat.math.ethz.ch>; "Mike White"
<mikewhite.diu at tiscali.co.uk>
Sent: Monday, January 31, 2005 11:05 PM
Subject: Re: [R] Extracting a numeric prefix from a string


> (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> writes:
>
> > On 31-Jan-05 R user wrote:
> > > You could use something like
> > >
> > > y <- gsub('([0-9]+(.[0-9]+)?)?.*','\\1',x)
> > > as.numeric(y)
> > >
> > > But maybe there's a much nicer way.
> > >
> > > Jonne.
> >
> > I doubt it -- full marks for neat regexp footwork!
>
> Hmm, I'd have to deduct a few points for forgetting to escape the dot...
>
> > x <- "2a4"
> > y <- gsub('([0-9]+(.[0-9]+)?)?.*','\\1',x)
> > y
> [1] "2a4"
> >  as.numeric(y)
> [1] NA
> Warning message:
> NAs introduced by coercion
>
> and maybe a few more for using gsub() where sub() suffices.
>
> There are a few more nits to pick, since "2.", ".2", "2e-7" are also
> numbers, but ".", ".e-2" are not. In fact it seems quite hard even to
> handle all cases in, e.g.,
>
>  x <- c("2.2abc","2.def",".2ghi",".jkl")
>
> with a single regular expression. The first one that worked for me was
>
> > r <- regexpr('^(([0-9]+\\.?)|(\\.[0-9]+)|([0-9]+\\.[0-9]+))',x)
> > substr(x,r,r+attr(r,"match.length")-1)
> [1] "2.2" "2."  ".2"  ""
>
> but several "obvious" attempts had failed.
>
> The problem is that regular expressions try to find the
> longest match, but not necessary of subexpressions, so
>
> > sub('(([0-9]+\\.?)|(\\.[0-9]+)|([0-9]+\\.[0-9]+))?.*','\\1',x)
> [1] "2." "2." ".2" ""
>
> even though
>
> > sub('(([0-9]+\\.?)|(\\.[0-9]+)|([0-9]+\\.[0-9]+))','XXX',x)
> [1] "XXXabc" "XXXdef" "XXXghi" ".jkl"
>
> Actually, this one comes pretty close:
>
> > sub('([0-9]*(\\.[0-9]+)?)?.*','\\1',x)
> [1] "2.2" "2"   ".2"  ""
>
> It only loses a trailing dot which is immaterial in the present
> context. However, next try extending the RE to handle an exponent
> part...
>
> --
>    O__  ---- Peter Dalgaard             Blegdamsvej 3
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907
>



From MSchwartz at MedAnalytics.com  Wed Feb  2 15:20:23 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Wed, 02 Feb 2005 08:20:23 -0600
Subject: [R] postscript symbols?
In-Reply-To: <200502021403.j12E3mFi022693@erdos.math.unb.ca>
References: <200502021403.j12E3mFi022693@erdos.math.unb.ca>
Message-ID: <1107354024.25434.27.camel@horizons.localdomain>

On Wed, 2005-02-02 at 10:03 -0400, Rolf Turner wrote:
> A propos of these symbols, Henrik Bengtsson (Lund University, Sweden)
> posted to this list some time ago a very useful function ``plotSymbols'',
> which can be slightly modified as follows:
> 
> plotSymbols <- function (fn=1) {
>     i <- 0:255
>     ncol <- 16
>     opar <- par(cex.axis = 0.7, mar = c(3, 3, 3, 3) + 0.1)
>     plot(i%%ncol, 1 + i%/%ncol, pch=i, font=fn, xlab = "", ylab = "", 
>         axes = FALSE)
>     axis(1, at = 0:15)
>     axis(2, at = 1:16, labels = 0:15 * 16, las = 2)
>     axis(3, at = 0:15)
>     axis(4, at = 1:16, labels = 0:15 * 16 + 15, las = 2)
>     par(opar)
> }
> 
> You can use this function to see what you get under various font
> specifications.  Of course it would help to know a priori that font
> number 5 gave you the postscript symbols!


There is a "similar" function called TestChars in the examples in 
?postscript, which I had actually reviewed last night before sending my
initial reply using the Hershey vector fonts.

However, you are correct, in that not knowing (or more correctly, not
remembering) about font=5 caused a cerebral vapor lock...  :-)

Marc



From francoisromain at free.fr  Wed Feb  2 15:32:18 2005
From: francoisromain at free.fr (Romain Francois)
Date: Wed, 02 Feb 2005 15:32:18 +0100
Subject: [R] (no subject)
In-Reply-To: <3542A1BF5AE1984D9FF577DA2CF8BA98245B37@MSX2>
References: <3542A1BF5AE1984D9FF577DA2CF8BA98245B37@MSX2>
Message-ID: <4200E472.5050808@free.fr>

Hello,

The MASS book (see bibtex below) is great. It covers all you need to 
start with R and to become a wizaRd.

Romain


  @Book{,
    title = {Modern Applied Statistics with S},
    author = {W. N. Venables and B. D. Ripley},
    publisher = {Springer},
    edition = {Fourth},
    address = {New York},
    year = {2002},
    note = {ISBN 0-387-95457-0},
    url = {http://www.stats.ox.ac.uk/pub/MASS4},
  }


Le 02.02.2005 04:17, Brett Stansfield a ?crit :

>can you recommend a good manual for R that starts with a data set and gives
>demonstrations on what can be done using R? I downloadedR Langauage
>definition and An introduction to R but haven't found them overly useful.
>I'd really like to be able to follow some tutorials using a dataset or many
>datasets. The datasets I have available on R are
>
>Data sets in package 'datasets':
>
>AirPassengers           Monthly Airline Passenger Numbers 1949-1960
>BJsales                 Sales Data with Leading Indicator
>BJsales.lead (BJsales)
>                        Sales Data with Leading Indicator
>BOD                     Biochemical Oxygen Demand
>CO2                     Carbon Dioxide uptake in grass plants
>ChickWeight             Weight versus age of chicks on different diets
>DNase                   Elisa assay of DNase
>EuStockMarkets          Daily Closing Prices of Major European Stock
>                        Indices, 1991-1998
>Formaldehyde            Determination of Formaldehyde
>HairEyeColor            Hair and Eye Color of Statistics Students
>Harman23.cor            Harman Example 2.3
>Harman74.cor            Harman Example 7.4
>Indometh                Pharmacokinetics of Indomethicin
>InsectSprays            Effectiveness of Insect Sprays
>JohnsonJohnson          Quarterly Earnings per Johnson & Johnson Share
>LakeHuron               Level of Lake Huron 1875-1972
>LifeCycleSavings        Intercountry Life-Cycle Savings Data
>Loblolly                Growth of Loblolly pine trees
>Nile                    Flow of the River Nile
>Orange                  Growth of orange trees
>OrchardSprays           Potency of Orchard Sprays
>PlantGrowth             Results from an Experiment on Plant Growth
>Puromycin               Reaction velocity of an enzymatic reaction
>Seatbelts               Road Casualties in Great Britain 1969-84
>Theoph                  Pharmacokinetics of theophylline
>Titanic                 Survival of passengers on the Titanic
>ToothGrowth             The Effect of Vitamin C on Tooth Growth in
>                        Guinea Pigs
>UCBAdmissions           Student Admissions at UC Berkeley
>UKDriverDeaths          Road Casualties in Great Britain 1969-84
>UKgas                   UK Quarterly Gas Consumption
>USAccDeaths             Accidental Deaths in the US 1973-1978
>USArrests               Violent Crime Rates by US State
>USJudgeRatings          Lawyers' Ratings of State Judges in the US
>                        Superior Court
>USPersonalExpenditure   Personal Expenditure Data
>VADeaths                Death Rates in Virginia (1940)
>WWWusage                Internet Usage per Minute
>WorldPhones             The World's Telephones
>ability.cov             Ability and Intelligence Tests
>airmiles                Passenger Miles on Commercial US Airlines,
>                        1937-1960
>airquality              New York Air Quality Measurements
>anscombe                Anscombe's Quartet of "Identical" Simple
>                        Linear Regressions
>attenu                  The Joyner-Boore Attenuation Data
>attitude                The Chatterjee-Price Attitude Data
>austres                 Quarterly Time Series of the Number of
>                        Australian Residents
>beaver1 (beavers)       Body Temperature Series of Two Beavers
>beaver2 (beavers)       Body Temperature Series of Two Beavers
>cars                    Speed and Stopping Distances of Cars
>chickwts                Chicken Weights by Feed Type
>co2                     Mauna Loa Atmospheric CO2 Concentration
>discoveries             Yearly Numbers of Important Discoveries
>esoph                   Smoking, Alcohol and (O)esophageal Cancer
>euro                    Conversion Rates of Euro Currencies
>euro.cross (euro)       Conversion Rates of Euro Currencies
>eurodist                Distances Between European Cities
>faithful                Old Faithful Geyser Data
>fdeaths (UKLungDeaths)
>                        Monthly Deaths from Lung Diseases in the UK
>freeny                  Freeny's Revenue Data
>freeny.x (freeny)       Freeny's Revenue Data
>freeny.y (freeny)       Freeny's Revenue Data
>infert                  Infertility after Spontaneous and Induced
>                        Abortion
>iris                    Edgar Anderson's Iris Data
>iris3                   Edgar Anderson's Iris Data
>islands                 Areas of the World's Major Landmasses
>ldeaths (UKLungDeaths)
>                        Monthly Deaths from Lung Diseases in the UK
>lh                      Luteinizing Hormone in Blood Samples
>longley                 Longley's Economic Regression Data
>lynx                    Annual Canadian Lynx trappings 1821-1934
>mdeaths (UKLungDeaths)
>                        Monthly Deaths from Lung Diseases in the UK
>morley                  Michaelson-Morley Speed of Light Data
>mtcars                  Motor Trend Car Road Tests
>nhtemp                  Average Yearly Temperatures in New Haven
>nottem                  Average Monthly Temperatures at Nottingham,
>                        1920-1939
>precip                  Annual Precipitation in US Cities
>presidents              Quarterly Approval Ratings of US Presidents
>pressure                Vapor Pressure of Mercury as a Function of
>                        Temperature
>quakes                  Locations of Earthquakes off Fiji
>randu                   Random Numbers from Congruential Generator
>                        RANDU
>rivers                  Lengths of Major North American Rivers
>rock                    Measurements on Petroleum Rock Samples
>sleep                   Student's Sleep Data
>stack.loss (stackloss)
>                        Brownlee's Stack Loss Plant Data
>stack.x (stackloss)     Brownlee's Stack Loss Plant Data
>stackloss               Brownlee's Stack Loss Plant Data
>state.abb (state)       US State Facts and Figures
>state.area (state)      US State Facts and Figures
>state.center (state)    US State Facts and Figures
>state.division (state)
>                        US State Facts and Figures
>state.name (state)      US State Facts and Figures
>state.region (state)    US State Facts and Figures
>state.x77 (state)       US State Facts and Figures
>sunspot.month           Monthly Sunspot Data, 1749-1997
>sunspot.year            Yearly Sunspot Data, 1700-1988
>sunspots                Monthly Sunspot Numbers, 1749-1983
>swiss                   Swiss Fertility and Socioeconomic Indicators
>                        (1888) Data
>treering                Yearly Treering Data, -6000-1979
>trees                   Girth, Height and Volume for Black Cherry
>                        Trees
>uspop                   Populations Recorded by the US Census
>volcano                 Topographic Information on Auckland's Maunga
>                        Whau Volcano
>warpbreaks              The Number of Breaks in Yarn during Weaving
>women                   Average Heights and Weights for American Women
>
>
>Use 'data(package = .packages(all.available = TRUE))'
>to list the data sets in all *available* packages.
>
>Brett Stansfield 
>Environmental Scientist - Water Quality 
>Hawke's Bay Regional Council 
>102 Vautier Street 
>Private Bag 6006 
>Napier 
>Phone (06) 835-9200 extn 9334 
>Fax (06) 835-3601
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>


-- 
Romain FRANCOIS : francoisromain at free.fr
page web : http://addictedtor.free.fr/  (en construction)
06 18 39 14 69 / 01 46 80 65 60
_______________________________________________________
Etudiant en 3eme ann?e
Institut de Statistique de l'Universit? de Paris (ISUP)
Fili?re Industrie et Services
http://www.isup.cicrp.jussieu.fr/



From p.dalgaard at biostat.ku.dk  Wed Feb  2 15:30:16 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 02 Feb 2005 15:30:16 +0100
Subject: [R] polynomials REML and ML in nlme
In-Reply-To: <IB9D31$B8EAD28B65FF4F8596B89F2EA8BDE9E1@bol.com.br>
References: <IB9D31$B8EAD28B65FF4F8596B89F2EA8BDE9E1@bol.com.br>
Message-ID: <x24qgvm3uv.fsf@biostat.ku.dk>

"Alex" <fciclone at bol.com.br> writes:

> Duncan,
> 
> I think that the problem in your comparison is locatted in the
> method of estimation of fixed-effects: in REML they are not
> estimated maximizing a joint likelihood function including
> fixed-effects and variance- covariance components as ML does. So
> when it comes down to different fixed-effects specifications, they
> can't be compared directly by means of the log-likelihoods.

Just to pick a small nit, you can actually get both fixed and random
effects parameters by optimizing a single modified likelihood. The
problem is that the modification term (see the post from I. White)
depends on the design matrix for the fixed effects, so it is not
invariant under reparametrization, nor under model reduction.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From michael.watson at bbsrc.ac.uk  Wed Feb  2 15:38:21 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Wed, 2 Feb 2005 14:38:21 -0000
Subject: [R] Combining two histograms
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950121B9F5@iahce2knas1.iah.bbsrc.reserved>

Something a bit more sohpisticated than this:
 

a <- rnorm(20000)
b <- rnorm(20000, mean=1.5)
ah <- hist(a,breaks= seq(-5,6,by=0.2),plot=FALSE)
bh <- hist(b,breaks= seq(-5,6,by=0.2),plot=FALSE)
data <- t(cbind(ah$counts,bh$counts))
barplot(data,beside=TRUE, space=rep(0,2*ncol(data)))
 

	-----Original Message----- 
	From: r-help-bounces at stat.math.ethz.ch on behalf of Sean Davis 
	Sent: Wed 2/2/2005 1:42 PM 
	To: r-help 
	Cc: 
	Subject: [R] Combining two histograms
	
	

	I have data like:
	
	a <- rnorm(20000)
	b <- rep(FALSE,20000)
	b[sample(1:20000,15000)] <- TRUE
	
	Using Lattice graphics, I can produce two side-by-side histograms quite
	easily by:
	
	histogram(a | b)
	
	However, I would like to produce a "single" histogram with two bars
	within each bin, one for each group, as the groups are in reality very
	slightly different from each other.  The difference isn't evident
	unless one "overlays" the two histograms in some manner.
	
	Thanks,
	Sean
	
	______________________________________________
	R-help at stat.math.ethz.ch mailing list
	https://stat.ethz.ch/mailman/listinfo/r-help
	PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From hb at maths.lth.se  Wed Feb  2 16:27:07 2005
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Wed, 2 Feb 2005 16:27:07 +0100
Subject: [R] Combining two histograms
In-Reply-To: <44083422-7520-11D9-8BE3-000D933565E8@mail.nih.gov>
Message-ID: <004201c5093b$a92ee370$4af1ba51@hblaptop>

If you looking for something like 

x1 <- rnorm(1000,  0.4, 0.8)
x2 <- rnorm(1000,  0.0, 1.0)
x3 <- rnorm(1000, -1.0, 1.0)
hist(x1, width=0.33, offset=0.00, col="blue", xlim=c(-4,4),
 main="Histogram of x1, x2 & x3", xlab="x1 - blue, x2 - red, x3 - green")
hist(x2, width=0.33, offset=0.33, col="red", add=TRUE)
hist(x3, width=0.33, offset=0.66, col="green", add=TRUE)

with results as in http://www.maths.lth.se/help/R/plot.histogram/ there is
an extension to hist() (actually plot.histogram()) in the R.basic package
(part of the R.classes bundle). See
http://www.maths.lth.se/help/R/R.classes/ for installation instructions.

BTW, you should also consider plotting density() estimates.

Henrik Bengtsson

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Sean Davis
> Sent: Wednesday, February 02, 2005 2:42 PM
> To: r-help
> Subject: [R] Combining two histograms
> 
> 
> I have data like:
> 
> a <- rnorm(20000)
> b <- rep(FALSE,20000)
> b[sample(1:20000,15000)] <- TRUE
> 
> Using Lattice graphics, I can produce two side-by-side 
> histograms quite 
> easily by:
> 
> histogram(a | b)
> 
> However, I would like to produce a "single" histogram with two bars 
> within each bin, one for each group, as the groups are in 
> reality very 
> slightly different from each other.  The difference isn't evident 
> unless one "overlays" the two histograms in some manner.
> 
> Thanks,
> Sean
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From deepayan at stat.wisc.edu  Wed Feb  2 16:31:24 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Wed, 2 Feb 2005 09:31:24 -0600
Subject: [R] Combining two histograms
In-Reply-To: <44083422-7520-11D9-8BE3-000D933565E8@mail.nih.gov>
References: <44083422-7520-11D9-8BE3-000D933565E8@mail.nih.gov>
Message-ID: <200502020931.24286.deepayan@stat.wisc.edu>

On Wednesday 02 February 2005 07:42, Sean Davis wrote:
> I have data like:
>
> a <- rnorm(20000)
> b <- rep(FALSE,20000)
> b[sample(1:20000,15000)] <- TRUE
>
> Using Lattice graphics, I can produce two side-by-side histograms
> quite easily by:
>
> histogram(a | b)

This should be ~a | b. That 'a | b' works is undocumented, unintutive 
and liable to change.

> However, I would like to produce a "single" histogram with two bars
> within each bin, one for each group, as the groups are in reality
> very slightly different from each other.  The difference isn't
> evident unless one "overlays" the two histograms in some manner.

This is my personal bias to some extent, but I would strongly suggest 
you use densityplot instead, e.g. 

densityplot(~a, groups = b, plot.points = FALSE) 

or if you are suspicious of more sophisticated kernels, 

densityplot(~a, groups = b, plot.points = FALSE, kern = "rect")

Histograms were appropriate for drawing density estimates by hand in the 
good old days, but I can imagine very few situations where I would not 
prefer to use smoother density estimates when I have the computational 
power to do so.

Deepayan



From laura at env.leeds.ac.uk  Wed Feb  2 17:18:23 2005
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Wed, 2 Feb 2005 16:18:23 +0000 (GMT)
Subject: [R] Runnning R remotely
Message-ID: <Pine.LNX.4.44.0502021613280.17308-100000@env-pc-phd13>

Hi,

I was wondering if anyone might be able to help. I am trying to run R on a
remote machine, part of the model run I am attempting writes an external
file output as a png (about 48 iterations per model run). I am running R
1.9.1 on SuSe9.0, and am accessing this via ssh from a Debian machine.

Initially I used the command ssh -X IP.address and whilst I was able to
run the model successfully the model run was extremely slow (which I could
not understand as the output was being written to the remote machine), but
tried to log in again, this time without the X windows facility and found
that I couldn't run the model:

Error in X11(paste("png::", filename, sep = ""), width, height, pointsize,
:
        unable to start device PNG
In addition: Warning message:
unable to open connection to X11 display`'

Could somebody please advise a way around this?

Thanks in advance,
Laura

Laura Quinn
Institute of Atmospheric Science
School of Earth and Environment
University of Leeds
Leeds
LS2 9JT

tel: +44 113 343 1596
fax: +44 113 343 6716
mail: laura at env.leeds.ac.uk



From Christoph.Scherber at uni-jena.de  Wed Feb  2 17:24:41 2005
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Wed, 02 Feb 2005 17:24:41 +0100
Subject: [R] random effects in lme
Message-ID: <4200FEC9.1000909@uni-jena.de>

Dear all,

Suppose I have a linear mixed-effects model (from the package nlme) with 
nested random effects (see below); how would I present the results from 
the random effects part in a publication?

Specifically, I?d like to know:
(1) What is the total variance of the random effects at each level?
(2) How can I test the significance of the variance components?
(3) Is there something like an "r squared" for the whole model which I 
can state?

The data come from an experiment on plant performance with and without 
insecticide, with and without grasses present, and across different 
levels of plant diversity ("div").

Thanks for your help!
Christoph.

lme(asin(sqrt(response)) ~ treatment + logb(div + 1, 2) + grass,
random =  ~ 1 | plotcode/treatment, na.action = na.exclude, method = "ML")

Linear mixed-effects model fit by maximum likelihood

Data: NULL
       AIC      BIC  logLik
 -290.4181 -268.719 152.209

Random effects:
Formula:  ~ 1 | plotcode
       (Intercept)
StdDev:  0.04176364

 Formula:  ~ 1 | treatment %in% plotcode
      (Intercept)   Residual
StdDev:  0.08660458 0.00833387

Fixed effects: asin(sqrt(response)) ~ treatment + logb(div + 1, 2) + grass
                   Value  Std.Error DF   t-value p-value
   (Intercept)  0.1858065 0.01858581 81  9.997225  <.0001
     treatment  0.0201384 0.00687832 81  2.927803  0.0044
logb(div + 1, 2) -0.0203301 0.00690074 79 -2.946073  0.0042
         grass  0.0428934 0.01802506 79  2.379656  0.0197

Standardized Within-Group Residuals:
      Min          Q1         Med         Q3       Max
-0.2033155 -0.05739679 -0.00943737 0.04045958 0.3637217

Number of Observations: 164
Number of Groups:
plotcode ansatz %in% plotcode
     82                  164



From michael.watson at bbsrc.ac.uk  Wed Feb  2 17:35:19 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Wed, 2 Feb 2005 16:35:19 -0000
Subject: [R] Runnning R remotely
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950121B9FC@iahce2knas1.iah.bbsrc.reserved>

This isn't exactly the same as your problem, but I came across access to X11 problems when running R over CGI. 
I got round it by installing Xvfb (X Virtual Frame Buffer) on the server.  
 
Mick

	-----Original Message----- 
	From: r-help-bounces at stat.math.ethz.ch on behalf of Laura Quinn 
	Sent: Wed 2/2/2005 4:18 PM 
	To: r-help at stat.math.ethz.ch 
	Cc: 
	Subject: [R] Runnning R remotely
	
	

	Hi,
	
	I was wondering if anyone might be able to help. I am trying to run R on a
	remote machine, part of the model run I am attempting writes an external
	file output as a png (about 48 iterations per model run). I am running R
	1.9.1 on SuSe9.0, and am accessing this via ssh from a Debian machine.
	
	Initially I used the command ssh -X IP.address and whilst I was able to
	run the model successfully the model run was extremely slow (which I could
	not understand as the output was being written to the remote machine), but
	tried to log in again, this time without the X windows facility and found
	that I couldn't run the model:
	
	Error in X11(paste("png::", filename, sep = ""), width, height, pointsize,
	:
	        unable to start device PNG
	In addition: Warning message:
	unable to open connection to X11 display`'
	
	Could somebody please advise a way around this?
	
	Thanks in advance,
	Laura
	
	Laura Quinn
	Institute of Atmospheric Science
	School of Earth and Environment
	University of Leeds
	Leeds
	LS2 9JT
	
	tel: +44 113 343 1596
	fax: +44 113 343 6716
	mail: laura at env.leeds.ac.uk
	
	______________________________________________
	R-help at stat.math.ethz.ch mailing list
	https://stat.ethz.ch/mailman/listinfo/r-help
	PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jeaneid at chass.utoronto.ca  Wed Feb  2 17:50:49 2005
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Wed, 2 Feb 2005 11:50:49 -0500
Subject: [R] Runnning R remotely
In-Reply-To: <Pine.LNX.4.44.0502021613280.17308-100000@env-pc-phd13>
Message-ID: <Pine.SGI.4.40.0502021143410.9470999-100000@origin.chass.utoronto.ca>

Why do you use png? I think that yu need X11 to save a png file. why not
use a postscript?


This is what ?png says


"R can be compiled without support for either or both of these devices:
this will be reported if you attempt to use them on a system where they
are not supported. They will not be available if R has been started with
--gui=none (and will give a different error message), and they may not be
usable unless the X11 display is available to the owner of the R process."


In addition I think the only thing slowing the machine is png.


Jean

On Wed, 2 Feb 2005, Laura Quinn wrote:

> Hi,
>
> I was wondering if anyone might be able to help. I am trying to run R on a
> remote machine, part of the model run I am attempting writes an external
> file output as a png (about 48 iterations per model run). I am running R
> 1.9.1 on SuSe9.0, and am accessing this via ssh from a Debian machine.
>
> Initially I used the command ssh -X IP.address and whilst I was able to
> run the model successfully the model run was extremely slow (which I could
> not understand as the output was being written to the remote machine), but
> tried to log in again, this time without the X windows facility and found
> that I couldn't run the model:
>
> Error in X11(paste("png::", filename, sep = ""), width, height, pointsize,
> :
>         unable to start device PNG
> In addition: Warning message:
> unable to open connection to X11 display`'
>
> Could somebody please advise a way around this?
>
> Thanks in advance,
> Laura
>
> Laura Quinn
> Institute of Atmospheric Science
> School of Earth and Environment
> University of Leeds
> Leeds
> LS2 9JT
>
> tel: +44 113 343 1596
> fax: +44 113 343 6716
> mail: laura at env.leeds.ac.uk
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From fric at entu.cas.cz  Wed Feb  2 17:43:58 2005
From: fric at entu.cas.cz (Zdenek Fric)
Date: Wed, 02 Feb 2005 17:43:58 +0100
Subject: [R] hier.part limitation
Message-ID: <4201115E.27132.DD9065@localhost>

Do you know, please, why is package "hier.part" limited to the 
maximum number of 12 variables? When is hierarchical partitioning 
used for large-scale comparisons, I found this limitation very un-
usefull. 
I tried to somehow split my data to less variables and computed the 
hierarchical partitioning on limited data, but the results were more 
or less different, especially in joint effects. 
Then I changed in the functions "all.regs", "partition" and 
"hier.part" the amount of used variables. My 15 variables needed some 
60 thousands of regressions and it needed some time (in my machine 
about one hour of work, but it is not bad), but after this the 
program terminated with some error message.
I do not have any idea, where could be the problem.
One problem occured in calling function "all.regs", where were 
problems with allocation of dynamic memmory, when using "print.vars = 
TRUE". But it again terminated calling the function "partition".
Do you know, how to solve the problem and, additionally, is it 
possible to do the regressions more heuristic?
With the best regards

Zdenek Fric



From laura at env.leeds.ac.uk  Wed Feb  2 17:48:29 2005
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Wed, 2 Feb 2005 16:48:29 +0000 (GMT)
Subject: [R] Runnning R remotely
In-Reply-To: <Pine.SGI.4.40.0502021143410.9470999-100000@origin.chass.utoronto.ca>
Message-ID: <Pine.LNX.4.44.0502021646000.17308-100000@env-pc-phd13>

I wasn't aware that it was possible to use postscript in the same fashion
as png, eg:

png(file,width=x,height=y,)
image(map)
text(text)
title(title)
box()
dev.off()

As there are a large number of iterations png has been working nicely
(when not working remotely!), especially as it has proven easy to convery
into gifs and then into movie gifs. Could anyone suggest an alternative
approach in this case?

TIA,
Laura

Laura Quinn
Institute of Atmospheric Science
School of Earth and Environment
University of Leeds
Leeds
LS2 9JT

tel: +44 113 343 1596
fax: +44 113 343 6716
mail: laura at env.leeds.ac.uk

On Wed, 2 Feb 2005, Jean Eid wrote:

> Why do you use png? I think that yu need X11 to save a png file. why not
> use a postscript?
>
>
> This is what ?png says
>
>
> "R can be compiled without support for either or both of these devices:
> this will be reported if you attempt to use them on a system where they
> are not supported. They will not be available if R has been started with
> --gui=none (and will give a different error message), and they may not be
> usable unless the X11 display is available to the owner of the R process."
>
>
> In addition I think the only thing slowing the machine is png.
>
>
> Jean
>
> On Wed, 2 Feb 2005, Laura Quinn wrote:
>
> > Hi,
> >
> > I was wondering if anyone might be able to help. I am trying to run R on a
> > remote machine, part of the model run I am attempting writes an external
> > file output as a png (about 48 iterations per model run). I am running R
> > 1.9.1 on SuSe9.0, and am accessing this via ssh from a Debian machine.
> >
> > Initially I used the command ssh -X IP.address and whilst I was able to
> > run the model successfully the model run was extremely slow (which I could
> > not understand as the output was being written to the remote machine), but
> > tried to log in again, this time without the X windows facility and found
> > that I couldn't run the model:
> >
> > Error in X11(paste("png::", filename, sep = ""), width, height, pointsize,
> > :
> >         unable to start device PNG
> > In addition: Warning message:
> > unable to open connection to X11 display`'
> >
> > Could somebody please advise a way around this?
> >
> > Thanks in advance,
> > Laura
> >
> > Laura Quinn
> > Institute of Atmospheric Science
> > School of Earth and Environment
> > University of Leeds
> > Leeds
> > LS2 9JT
> >
> > tel: +44 113 343 1596
> > fax: +44 113 343 6716
> > mail: laura at env.leeds.ac.uk
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>
>



From gunter.berton at gene.com  Wed Feb  2 17:51:51 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 2 Feb 2005 08:51:51 -0800
Subject: [R] Displaying a distribution -- was: Combining two histograms
In-Reply-To: <200502020931.24286.deepayan@stat.wisc.edu>
Message-ID: <200502021651.j12GppmK016972@compton.gene.com>

May I take this off topic a little to seek collective wisdom (and so feel
free to reply privately).

The catalyst is Deepayan's remark:

> Histograms were appropriate for drawing density estimates by 
> hand in the  good old days, but I can imagine very few situations where I 
> would not prefer to use smoother density estimates when I have the 
> computational power to do so.
> 
> Deepayan

Generally, I agree; but the appearance and thus one's perception and
interpretation of both histograms and density plots depend upon the
parameters chosen for the display (bin boundaries for histograms; bandwidth
and kernel for density plots). Important data peculiarities like arbitrary
rounding, favoring of certain values, resolution limitations, and so forth
are therefore often lost. I would instead advocate that simple quantile
plots -- plot(ppoints(x),sort(x)) -- or perhaps normal qqplots always be the
first plot used to explore univariate data distributions. I believe this
conforms to Bill Cleveland's recommendations, who says in the first sentence
on p. 17 of VISUALIZING DATA on visualizing univariate data: "Quantiles are
essential to visualizing distributions."

While it is true that many people may be unfamiliar with quantile plots, I
think we need to improve modern statistical practice not only by abandoning
histograms in favor of density plots, but also by always first using
quantile plots and explaining why this is necessary.

Difficult issue: What should one do when when there are, say, a million
values?

Alternative views?

 
-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box



From jeaneid at chass.utoronto.ca  Wed Feb  2 18:18:04 2005
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Wed, 2 Feb 2005 12:18:04 -0500
Subject: [R] Runnning R remotely
In-Reply-To: <Pine.LNX.4.44.0502021646000.17308-100000@env-pc-phd13>
Message-ID: <Pine.SGI.4.40.0502021212170.9300058-100000@origin.chass.utoronto.ca>

look intp ?postscript. these is nothingin your code below that you cannot
do in postscript.


to convert the images you can use convert (this is ImageMagick)
on your machine. On my debian machine it was preinstalled if not apt-get
imagemagick should do it. After this you can use convert blbla.eps
blabla.gif


Jean

On Wed, 2 Feb 2005, Laura Quinn wrote:

> I wasn't aware that it was possible to use postscript in the same fashion
> as png, eg:
>
> png(file,width=x,height=y,)
> image(map)
> text(text)
> title(title)
> box()
> dev.off()
>
> As there are a large number of iterations png has been working nicely
> (when not working remotely!), especially as it has proven easy to convery
> into gifs and then into movie gifs. Could anyone suggest an alternative
> approach in this case?
>
> TIA,
> Laura
>
> Laura Quinn
> Institute of Atmospheric Science
> School of Earth and Environment
> University of Leeds
> Leeds
> LS2 9JT
>
> tel: +44 113 343 1596
> fax: +44 113 343 6716
> mail: laura at env.leeds.ac.uk
>
> On Wed, 2 Feb 2005, Jean Eid wrote:
>
> > Why do you use png? I think that yu need X11 to save a png file. why not
> > use a postscript?
> >
> >
> > This is what ?png says
> >
> >
> > "R can be compiled without support for either or both of these devices:
> > this will be reported if you attempt to use them on a system where they
> > are not supported. They will not be available if R has been started with
> > --gui=none (and will give a different error message), and they may not be
> > usable unless the X11 display is available to the owner of the R process."
> >
> >
> > In addition I think the only thing slowing the machine is png.
> >
> >
> > Jean
> >
> > On Wed, 2 Feb 2005, Laura Quinn wrote:
> >
> > > Hi,
> > >
> > > I was wondering if anyone might be able to help. I am trying to run R on a
> > > remote machine, part of the model run I am attempting writes an external
> > > file output as a png (about 48 iterations per model run). I am running R
> > > 1.9.1 on SuSe9.0, and am accessing this via ssh from a Debian machine.
> > >
> > > Initially I used the command ssh -X IP.address and whilst I was able to
> > > run the model successfully the model run was extremely slow (which I could
> > > not understand as the output was being written to the remote machine), but
> > > tried to log in again, this time without the X windows facility and found
> > > that I couldn't run the model:
> > >
> > > Error in X11(paste("png::", filename, sep = ""), width, height, pointsize,
> > > :
> > >         unable to start device PNG
> > > In addition: Warning message:
> > > unable to open connection to X11 display`'
> > >
> > > Could somebody please advise a way around this?
> > >
> > > Thanks in advance,
> > > Laura
> > >
> > > Laura Quinn
> > > Institute of Atmospheric Science
> > > School of Earth and Environment
> > > University of Leeds
> > > Leeds
> > > LS2 9JT
> > >
> > > tel: +44 113 343 1596
> > > fax: +44 113 343 6716
> > > mail: laura at env.leeds.ac.uk
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> > >
> >
> >
>
>



From ggrothendieck at myway.com  Wed Feb  2 18:20:18 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 2 Feb 2005 17:20:18 +0000 (UTC)
Subject: [R] Displaying a distribution -- was: Combining two histograms
References: <200502020931.24286.deepayan@stat.wisc.edu>
	<200502021651.j12GppmK016972@compton.gene.com>
Message-ID: <loom.20050202T181728-622@post.gmane.org>

Berton Gunter <gunter.berton <at> gene.com> writes:

: 
: May I take this off topic a little to seek collective wisdom (and so feel
: free to reply privately).
: 
: The catalyst is Deepayan's remark:
: 
: > Histograms were appropriate for drawing density estimates by 
: > hand in the  good old days, but I can imagine very few situations where I 
: > would not prefer to use smoother density estimates when I have the 
: > computational power to do so.
: > 
: > Deepayan
: 
: Generally, I agree; but the appearance and thus one's perception and
: interpretation of both histograms and density plots depend upon the
: parameters chosen for the display (bin boundaries for histograms; bandwidth
: and kernel for density plots). 

If the problem is distrust of the procedure one can simulataneously
display the raw data with a rug plot, 

example(rug)



From laura at env.leeds.ac.uk  Wed Feb  2 18:25:59 2005
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Wed, 2 Feb 2005 17:25:59 +0000 (GMT)
Subject: [R] Runnning R remotely
In-Reply-To: <Pine.SGI.4.40.0502021212170.9300058-100000@origin.chass.utoronto.ca>
Message-ID: <Pine.LNX.4.44.0502021725330.17308-100000@env-pc-phd13>

Aha, of course!

Thanks for your help - 10 minutes down to 5 seconds, superb.

Laura

Laura Quinn
Institute of Atmospheric Science
School of Earth and Environment
University of Leeds
Leeds
LS2 9JT

tel: +44 113 343 1596
fax: +44 113 343 6716
mail: laura at env.leeds.ac.uk

On Wed, 2 Feb 2005, Jean Eid wrote:

> look intp ?postscript. these is nothingin your code below that you cannot
> do in postscript.
>
>
> to convert the images you can use convert (this is ImageMagick)
> on your machine. On my debian machine it was preinstalled if not apt-get
> imagemagick should do it. After this you can use convert blbla.eps
> blabla.gif
>
>
> Jean
>
> On Wed, 2 Feb 2005, Laura Quinn wrote:
>
> > I wasn't aware that it was possible to use postscript in the same fashion
> > as png, eg:
> >
> > png(file,width=x,height=y,)
> > image(map)
> > text(text)
> > title(title)
> > box()
> > dev.off()
> >
> > As there are a large number of iterations png has been working nicely
> > (when not working remotely!), especially as it has proven easy to convery
> > into gifs and then into movie gifs. Could anyone suggest an alternative
> > approach in this case?
> >
> > TIA,
> > Laura
> >
> > Laura Quinn
> > Institute of Atmospheric Science
> > School of Earth and Environment
> > University of Leeds
> > Leeds
> > LS2 9JT
> >
> > tel: +44 113 343 1596
> > fax: +44 113 343 6716
> > mail: laura at env.leeds.ac.uk
> >
> > On Wed, 2 Feb 2005, Jean Eid wrote:
> >
> > > Why do you use png? I think that yu need X11 to save a png file. why not
> > > use a postscript?
> > >
> > >
> > > This is what ?png says
> > >
> > >
> > > "R can be compiled without support for either or both of these devices:
> > > this will be reported if you attempt to use them on a system where they
> > > are not supported. They will not be available if R has been started with
> > > --gui=none (and will give a different error message), and they may not be
> > > usable unless the X11 display is available to the owner of the R process."
> > >
> > >
> > > In addition I think the only thing slowing the machine is png.
> > >
> > >
> > > Jean
> > >
> > > On Wed, 2 Feb 2005, Laura Quinn wrote:
> > >
> > > > Hi,
> > > >
> > > > I was wondering if anyone might be able to help. I am trying to run R on a
> > > > remote machine, part of the model run I am attempting writes an external
> > > > file output as a png (about 48 iterations per model run). I am running R
> > > > 1.9.1 on SuSe9.0, and am accessing this via ssh from a Debian machine.
> > > >
> > > > Initially I used the command ssh -X IP.address and whilst I was able to
> > > > run the model successfully the model run was extremely slow (which I could
> > > > not understand as the output was being written to the remote machine), but
> > > > tried to log in again, this time without the X windows facility and found
> > > > that I couldn't run the model:
> > > >
> > > > Error in X11(paste("png::", filename, sep = ""), width, height, pointsize,
> > > > :
> > > >         unable to start device PNG
> > > > In addition: Warning message:
> > > > unable to open connection to X11 display`'
> > > >
> > > > Could somebody please advise a way around this?
> > > >
> > > > Thanks in advance,
> > > > Laura
> > > >
> > > > Laura Quinn
> > > > Institute of Atmospheric Science
> > > > School of Earth and Environment
> > > > University of Leeds
> > > > Leeds
> > > > LS2 9JT
> > > >
> > > > tel: +44 113 343 1596
> > > > fax: +44 113 343 6716
> > > > mail: laura at env.leeds.ac.uk
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> > > >
> > >
> > >
> >
> >
>
>



From tchur at optushome.com.au  Wed Feb  2 18:29:25 2005
From: tchur at optushome.com.au (Tim Churches)
Date: Thu, 03 Feb 2005 04:29:25 +1100
Subject: [R] Runnning R remotely
In-Reply-To: <Pine.LNX.4.44.0502021646000.17308-100000@env-pc-phd13>
References: <Pine.LNX.4.44.0502021646000.17308-100000@env-pc-phd13>
Message-ID: <42010DF5.2050805@optushome.com.au>

Laura Quinn wrote:

>I wasn't aware that it was possible to use postscript in the same fashion
>as png, eg:
>
>png(file,width=x,height=y,)
>image(map)
>text(text)
>title(title)
>box()
>dev.off()
>
>As there are a large number of iterations png has been working nicely
>(when not working remotely!), especially as it has proven easy to convery
>into gifs and then into movie gifs. Could anyone suggest an alternative
>approach in this case?
>  
>
Start an Xvfb (X11 virtual frame buffer) server in your remote ssh 
session. R will then use that as an X11 device to produce the PNG 
output. If you are running in a hostile network environment, consider 
using authentication and/or switching off network access to the Xvfb 
session - see the man pages for Xvfb. Xvfb is installed by default on 
most recent Linux distributions - if not, there should be an installable 
package available for it for your flavour of Linux.

Tim C



From mike_saunders at umenfa.maine.edu  Wed Feb  2 18:38:09 2005
From: mike_saunders at umenfa.maine.edu (Mike Saunders)
Date: Wed, 2 Feb 2005 12:38:09 -0500
Subject: [R] Split-split plot ANOVA
References: <LGECJJCANFBOOHCMGPJEKEBHDKAA.Jesus.Frias@dit.ie>
Message-ID: <001001c5094d$f62ca560$9ba76f82@CFRU0104>

Jesus and the rest of the R-help community:

Thanks for your help.  I have been going over and over the examples in MASS 
and the Pinheiro and Bates example, but cannot get my model to run correctly 
with either aov or lme.  Could someone give me a hand with the correct model 
statement?

First a description of the design.  We are studying germination rates for 
various species under a variety of treaments.  This is a blocked split-split 
plot design.  The levels and treatments are:

Blocks:  1-6

Whole plot treatment:
   Overstory:  Yes or No

Split plot treatments:
   Caging (to protect against seed predators):  Yes or No
   Herbaceous competition (i.e., grass):  Yes or No

Split-split plot treatment:
   Tree species:  7 kinds

The response variable is Lag, which is a indication of when the seeds first 
germinated.  I will be doing this analysis for a couple other response 
variables as well in separate analyses.

I have had mixed results using the examples as a guide to build my 
statement; I am unsure how to specify the crossed factors at the split-plot 
level.

Lastly, I have unbalanced data since some treatment combinations never had 
any germination.  Since the data are highly nonnormal, I hope to do a 
permutations test on the F-values for each main effect and interaction in 
order to get my p-values.

Thanks for your help in advance,
Mike

Mike Saunders
Research Assistant
Forest Ecosystem Research Program
Department of Forest Ecosystem Sciences
University of Maine
Orono, ME  04469
207-581-2763 (O)
207-581-4257 (F)

----- Original Message ----- 

From: "Jesus Frias" <Jesus.Frias at dit.ie>
To: "Mike Saunders" <mike_saunders at umenfa.maine.edu>; "R Help" 
<r-help at stat.math.ethz.ch>
Sent: Tuesday, February 01, 2005 10:57 AM
Subject: RE: [R] Split-split plot ANOVA


Hi Mike,

*An example of the use of aov() for a split-plot is in MASS

library(MASS)
example(Oats)
The book also gives a detailed explanation

*pp 45-52 of the Pinheiro and Bates book gives you an example of the use of
lme() on a split-plot. If you have a non balanced design,  lme() from the
nlme library might be a better tool than aov().

Also, if you have the lme4 library installed you'll have a lot more
flexibility on the formulation of your random effects.

regards,

Jesus

--------------------------------------------------------------
Jes?s Mar?a Fr?as Celayeta
School of Food Sci. and Env. Health.
Faculty of Tourism and Food
Dublin Institute of Technology
Cathal Brugha St., Dublin 1. Ireland
t +353 1 4024459 f +353 1 4024495
w www.dit.ie/DIT/tourismfood/science/staff/frias.html
--------------------------------------------------------------

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Mike Saunders
> Sent: 01 February 2005 14:36
> To: R Help
> Subject: [R] Split-split plot ANOVA
>
>
> Does someone out there have an example of R-code for a
> split-split plot ANOVA using aov or another function?  The design
> is not balanced.  I never set up one in R before and it would be
> nice to see an example before I tackle a very complex design I
> have to model.
>
> Thanks,
> Mike
>
> Mike Saunders
> Research Assistant
> Forest Ecosystem Research Program
> Department of Forest Ecosystem Sciences
> University of Maine
> Orono, ME  04469
> 207-581-2763 (O)
> 207-581-4257 (F)
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

--
This message has been scanned for content and
viruses by the DIT Information Services MailScanner
Service, and is believed to be clean.
http://www.dit.ie



-- 
This message has been scanned for content and
viruses by the DIT Information Services MailScanner
Service, and is believed to be clean.
http://www.dit.ie



From gunter.berton at gene.com  Wed Feb  2 18:46:09 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 2 Feb 2005 09:46:09 -0800
Subject: [R] Displaying a distribution -- was: Combining two histograms
In-Reply-To: <loom.20050202T181728-622@post.gmane.org>
Message-ID: <200502021746.j12Hk9R5025937@volta.gene.com>

Rug plots cannot show replicated values -- spikes at discrete values -- in
the data distribution. Quantile plots do.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor 
> Grothendieck
> Sent: Wednesday, February 02, 2005 9:20 AM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] Displaying a distribution -- was: Combining 
> two histograms
> 
> Berton Gunter <gunter.berton <at> gene.com> writes:
> 
> : 
> : May I take this off topic a little to seek collective 
> wisdom (and so feel
> : free to reply privately).
> : 
> : The catalyst is Deepayan's remark:
> : 
> : > Histograms were appropriate for drawing density estimates by 
> : > hand in the  good old days, but I can imagine very few 
> situations where I 
> : > would not prefer to use smoother density estimates when I 
> have the 
> : > computational power to do so.
> : > 
> : > Deepayan
> : 
> : Generally, I agree; but the appearance and thus one's perception and
> : interpretation of both histograms and density plots depend upon the
> : parameters chosen for the display (bin boundaries for 
> histograms; bandwidth
> : and kernel for density plots). 
> 
> If the problem is distrust of the procedure one can simulataneously
> display the raw data with a rug plot, 
> 
> example(rug)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From spencer.graves at pdf.com  Wed Feb  2 18:57:13 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 02 Feb 2005 09:57:13 -0800
Subject: [R] polynomials REML and ML in nlme
In-Reply-To: <Pine.GSO.4.58.0502021122430.27555@holyrood.ed.ac.uk>
References: <001a01c5088f$627d46d0$1002010a@DuncanToshiba>
	<Pine.GSO.4.58.0502021122430.27555@holyrood.ed.ac.uk>
Message-ID: <42011479.3010907@pdf.com>

  Am I correct that changing the parameterization should NOT change the 
estimates of the variance components, because you are minimizing 
essentially the same objective function over the same subspace? The only 
thing that changes is the logdet(X'WX) term mentioned by I White? 
Moreover, letting X = QR, and using the fact that det(AB) = 
det(A)*det(B) if they are both square, we get det(R'Q'WQR) = 
det(Q'WQ)*det(R)^2. Thus, the change in parameterization affects only R, 
not Q, which means that it can't affect det(Q'WQ).

Is this accurate?

As a simple sanity check, I ran a very simple model with the same fixed 
effects in different parameterizations, and got the same estimates for 
the variance components under REEL but different 
"log-restricted-likelihood" (see below).

Comments?
spencer graves
############################
fit2 <- lme(y~i+I(i^2), random=~1|a, data=DF)
fit.p <- lme(y~poly(i, 2), random=~1|a, data=DF)

fit2
fit.p
Linear mixed-effects model
Fixed: y ~ i + I(i^2)
Data: DF
log-restricted-likelihood: -9.969998
Random effects:
Groups Name Variance Std.Dev.
a (Intercept) 1.4212 1.1921
Residual 1.9928 1.4117
# of obs: 6, groups: a, 2

Fixed effects:
Estimate Std. Error DF t value Pr(>|t|)
(Intercept) -0.70889 2.67847 3 -0.2647 0.8084
i 0.19403 1.65425 3 0.1173 0.9140
I(i^2) -0.01081 0.23104 3 -0.0468 0.9656
 > fit.p
Linear mixed-effects model
Fixed: y ~ poly(i, 2)
Data: DF
log-restricted-likelihood: -6.728954
Random effects:
Groups Name Variance Std.Dev.
a (Intercept) 1.4212 1.1921
Residual 1.9928 1.4117
# of obs: 6, groups: a, 2

Fixed effects:
Estimate Std. Error DF t value Pr(>|t|)
(Intercept) -0.193736 1.021136 3 -0.1897 0.8616
poly(i, 2)1 0.495131 1.454805 3 0.3403 0.7560
poly(i, 2)2 -0.066053 1.411677 3 -0.0468 0.9656
 >


I M S White wrote:

>The REML loglikelihood includes a term -(1/2)logdet(X'WX) where X is the
>design matrix for the fixed effects and W is the inverse covariance matrix
>for the observations. Under reparametrisation, X becomes XM with M a
>non-singular matrix, and the REML loglikelihood changes by logdet(M).
>
>
>On Tue, 1 Feb 2005, dgoliche wrote:
>
>  
>
>>Hello everyone,
>>
>>I hope this is a fair enough question, but I don?t have access to a copy
>>of Bates and Pinheiro. It is probably quite obvious but the answer might
>>be of general interest.
>>If I fit a fixed effect with an added quadratic term and then do it as
>>an orthogonal polynomial using maximum likelihood I get the expected
>>result- they have the same logLik.
>>
>>mod2a<-lme(wthole~nplants+I(nplants^2),data=d3,random=~1|field/subplot,m
>>ethod="ML")
>>mod2b<-lme(wthole~poly(nplants,2),data=d3,random=~1|field/subplot,method
>>="ML")
>>
>>    
>>
>>>anova(mod2a,mod2b)
>>>      
>>>
>>      Model df      AIC      BIC    logLik
>>mod2a     1  6 6698.231 6723.869 -3343.116
>>mod2b     2  6 6698.231 6723.869 -3343.116
>>
>>However if I fit the two models by REML they are not considered to be
>>the same and I get warned.
>>
>>    
>>
>>>anova(mod2a.REML,mod2b.REML)
>>>      
>>>
>>           Model df      AIC      BIC    logLik
>>mod2a.REML     1  6 6680.616 6706.219 -3334.308
>>mod2b.REML     2  6 6666.915 6692.518 -3327.457
>>
>>Warning message:
>>Fitted objects with different fixed effects. REML comparisons are not
>>meaningful. in: anova.lme(mod2a.REML, mod2b.REML)
>>
>>Well yes, I suppose that?s right, they are not the same fixed effects.
>>But why does REML give them such different Log likelihoods? And what
>>should I do if I want to compare a larger set of models. For example the
>>following, admittedly overparameterised model, can be fitted (slowly) by
>>either method
>>
>>    
>>
>
>======================================
>I.White
>University of Edinburgh
>Ashworth Laboratories, West Mains Road
>Edinburgh EH9 3JT
>Tel: 0131 650 5490  Fax: 0131 650 6564
>E-mail: iwhite at staffmail.ed.ac.uk
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From htang at hpl.hp.com  Wed Feb  2 19:00:58 2005
From: htang at hpl.hp.com (Hsiu-Khuern Tang)
Date: Wed, 2 Feb 2005 10:00:58 -0800
Subject: [R] R CMD BATCH character limit?
In-Reply-To: <Pine.LNX.4.61.0502021028400.20029@gannet.stats>
References: <20050202015426.GO1436@hplhtang1.hpl.hp.com>
	<Pine.LNX.4.61.0502021028400.20029@gannet.stats>
Message-ID: <20050202180058.GR1436@hplhtang1.hpl.hp.com>

* On Wed 10:39AM, 02 Feb 2005, Prof Brian Ripley (ripley at stats.ox.ac.uk) wrote:
> On Tue, 1 Feb 2005, Hsiu-Khuern Tang wrote:
> 
> >Is there a limit on the number of characters in an invocation like
> >
> >R CMD BATCH --opt1=val1 --opt2=val2 ... --save-to=C:\very\long\string 
> >..\R\script.R
> >
> >?
> 
> Yes, but that is not a valid invocation of R.
> 
> >I am running R 2.0.1 on Windows XP, and when running a long BATCH
> >command (about 400--500 characters) the last option gets mangled, as can
> 
> There is a Windows limit on the length of command lines, and I believe you 
> have hit it.  It looks like R.exe/Rcmd.exe were written assuming it is 
> MAX_PATH, hence about 260, which it used to be in Win95.
> 
> You could use
> 
> rterm --args --opt1=val1 --opt2=val2 ... --save-to=C:\very\long\string < 
> foo > bar
> 
> instead: BATCH is just a convenient shorthand (and see the rw-FAQ for the 
> details).

Thank you!  I tried this and it works (with the mandatory --save or
--no-save).  I also went back to the BATCH command and found that the
truncation occurred at the 217th character after the "R" of R CMD BATCH.

> 
> Shells also have limits, so check this for the one you used.
> 
> >be seen from a warning in script.Rout that looks like this:
> >
> >WARNING: unknown option
> >+--save-to=C:\very\long\str..\R\script.Rout
> >
> >Note that "string" is truncated to "str" _and_ concatenated with the
> >implicit batch output file "..\R\script.Rout".  (The "unknown option"
> >warning is expected; I am just using commandArgs() within the script to
> >pick out the arguments.)
> >
> >I have tried specifying --args before the options, with the same result,
> >except that the truncation appears earlier.
> 
> --args would be needed for this to make any sense.
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595

Best,
Hsiu-Khuern.



From spencer.graves at pdf.com  Wed Feb  2 19:03:58 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 02 Feb 2005 10:03:58 -0800
Subject: [R] Displaying a distribution -- was: Combining two histograms
In-Reply-To: <200502021651.j12GppmK016972@compton.gene.com>
References: <200502021651.j12GppmK016972@compton.gene.com>
Message-ID: <4201160E.8040502@pdf.com>

      There are PP plots and QQ plots for any distribution, and I've 
experimented a little (though not much) with PP plots and with QQ plots 
for uniform, Student's t, chi-square and F distributions.  I've found 
qqnorm plots very useful, but I don't recall learning much from any 
other probability plots. 

      In data mining situations, I've computed hundreds of p-values, 
possibly associated with Student's t or F or log(likelihood ratio) 
approximate chi-squares.  I've found it useful to convert them to normal 
scores via qnorm(p), then make a normal plot of the p-values.  Points 
off the line on the lower tail are statistically significant.  Points on 
the line are there by chance alone.  If the slope of the line is 
different from one or not centered at zero, there may be hidden 
components of variance or serial dependence of various kinds that I'm 
not modeling properly in computing the p-values.  This "p-value plot" 
seems to provide a subtle check and first order correction for a variety 
of different violations of assumptions like this. 

      Comments?
      Best Wishes,
      spencer
p.s.  Regarding normal plots with millions of points:  I find them still 
useful.  However, we need some kind of heuristic to decimate excess 
points so we still get the same visual image without a plot object that 
consumes gigabytes on the hard drive, hours to plot, and can't be 
exported to PowerPoint, for example. 

Berton Gunter wrote:

>May I take this off topic a little to seek collective wisdom (and so feel
>free to reply privately).
>
>The catalyst is Deepayan's remark:
>
>  
>
>>Histograms were appropriate for drawing density estimates by 
>>hand in the  good old days, but I can imagine very few situations where I 
>>would not prefer to use smoother density estimates when I have the 
>>computational power to do so.
>>
>>Deepayan
>>    
>>
>
>Generally, I agree; but the appearance and thus one's perception and
>interpretation of both histograms and density plots depend upon the
>parameters chosen for the display (bin boundaries for histograms; bandwidth
>and kernel for density plots). Important data peculiarities like arbitrary
>rounding, favoring of certain values, resolution limitations, and so forth
>are therefore often lost. I would instead advocate that simple quantile
>plots -- plot(ppoints(x),sort(x)) -- or perhaps normal qqplots always be the
>first plot used to explore univariate data distributions. I believe this
>conforms to Bill Cleveland's recommendations, who says in the first sentence
>on p. 17 of VISUALIZING DATA on visualizing univariate data: "Quantiles are
>essential to visualizing distributions."
>
>While it is true that many people may be unfamiliar with quantile plots, I
>think we need to improve modern statistical practice not only by abandoning
>histograms in favor of density plots, but also by always first using
>quantile plots and explaining why this is necessary.
>
>Difficult issue: What should one do when when there are, say, a million
>values?
>
>Alternative views?
>
> 
>-- Bert Gunter
>Genentech Non-Clinical Statistics
>South San Francisco, CA
> 
>"The business of the statistician is to catalyze the scientific learning
>process."  - George E. P. Box
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From eugenedalt at yahoo.com  Wed Feb  2 19:11:43 2005
From: eugenedalt at yahoo.com (eugene dalt)
Date: Wed, 2 Feb 2005 10:11:43 -0800 (PST)
Subject: [R] Looking for R or S+ advanced programming courses in bay area
	(San Francisco)
Message-ID: <20050202181143.40602.qmail@web30401.mail.mud.yahoo.com>

If you are aware of any upcoming R or S+ advanced
programming courses in bay area (San Francisco),
please let me know. Best alternative is Boston.

Regards - 
Eugene



From bates at stat.wisc.edu  Wed Feb  2 19:12:37 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 02 Feb 2005 12:12:37 -0600
Subject: [R] polynomials REML and ML in nlme
In-Reply-To: <42011479.3010907@pdf.com>
References: <001a01c5088f$627d46d0$1002010a@DuncanToshiba>
	<Pine.GSO.4.58.0502021122430.27555@holyrood.ed.ac.uk>
	<42011479.3010907@pdf.com>
Message-ID: <42011815.1090508@stat.wisc.edu>

Spencer Graves wrote:
>  Am I correct that changing the parameterization should NOT change the 
> estimates of the variance components, because you are minimizing 
> essentially the same objective function over the same subspace? The only 
> thing that changes is the logdet(X'WX) term mentioned by I White? 
> Moreover, letting X = QR, and using the fact that det(AB) = 
> det(A)*det(B) if they are both square, we get det(R'Q'WQR) = 
> det(Q'WQ)*det(R)^2. Thus, the change in parameterization affects only R, 
> not Q, which means that it can't affect det(Q'WQ).
> 
> Is this accurate?
> 
> As a simple sanity check, I ran a very simple model with the same fixed 
> effects in different parameterizations, and got the same estimates for 
> the variance components under REEL but different 
> "log-restricted-likelihood" (see below).
> 
> Comments?

You are correct - it is only the R part that changes.  If you were to 
require an orthonormal parameterization of the fixed effects then the 
REML criterion would be invariant with respect to linear parameter 
transformations.  That's essentially the same as using the ML deviance 
instead of the REML deviance.

Before his untimely death Greg Reinsel was working on likelihood ratio 
tests using the REML criterion instead of the likelihood.  You need to 
adjust the criterion for the submodel relative to the full model but the 
pieces for doing that are sitting around in the lmer object.  I plan to 
include that in a future version of the lme4 package.

Meanwhile your idea looks quite interesting.  It may be possible to 
define the REML criterion in such a way that it produces the same 
estimates as now but does not change with linear transformations of the 
fixed effects.



From ggrothendieck at myway.com  Wed Feb  2 19:46:13 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 2 Feb 2005 18:46:13 +0000 (UTC)
Subject: [R] Displaying a distribution -- was: Combining two histograms
References: <loom.20050202T181728-622@post.gmane.org>
	<200502021746.j12Hk9R5025937@volta.gene.com>
Message-ID: <loom.20050202T194330-60@post.gmane.org>

Berton Gunter <gunter.berton <at> gene.com> writes:

: 
: Rug plots cannot show replicated values -- spikes at discrete values -- in
: the data distribution. Quantile plots do.
: 

One can address this, at least partly, using jitter and the example
I cited does precisely that.



From p.dalgaard at biostat.ku.dk  Wed Feb  2 19:45:36 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 02 Feb 2005 19:45:36 +0100
Subject: [R] polynomials REML and ML in nlme
In-Reply-To: <42011479.3010907@pdf.com>
References: <001a01c5088f$627d46d0$1002010a@DuncanToshiba>
	<Pine.GSO.4.58.0502021122430.27555@holyrood.ed.ac.uk>
	<42011479.3010907@pdf.com>
Message-ID: <x2u0ou3inj.fsf@biostat.ku.dk>

Spencer Graves <spencer.graves at pdf.com> writes:

>   Am I correct that changing the parameterization should NOT change
> the estimates of the variance components, because you are minimizing
> essentially the same objective function over the same subspace? The
> only thing that changes is the logdet(X'WX) term mentioned by I White?
> Moreover, letting X = QR, and using the fact that det(AB) =
> det(A)*det(B) if they are both square, we get det(R'Q'WQR) =
> det(Q'WQ)*det(R)^2. Thus, the change in parameterization affects only
> R, not Q, which means that it can't affect det(Q'WQ).
> 
> Is this accurate?
> 
> As a simple sanity check, I ran a very simple model with the same
> fixed effects in different parameterizations, and got the same
> estimates for the variance components under REEL but different
> "log-restricted-likelihood" (see below).
> 
> Comments?

Yes, that is accurate. Basically, you are optimizing two functions
that differ by a constant, namely logdet(M). And the expected values
are also identical; if the parameter estimates were betahat, after
reparametrization they become inv(M)betahat.

> >The REML loglikelihood includes a term -(1/2)logdet(X'WX) where X is the
> >design matrix for the fixed effects and W is the inverse covariance matrix
> >for the observations. Under reparametrisation, X becomes XM with M a
> >non-singular matrix, and the REML loglikelihood changes by logdet(M).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From spencer.graves at pdf.com  Wed Feb  2 20:28:26 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 02 Feb 2005 11:28:26 -0800
Subject: [R] A modified log transformation with real finite values for
 negatives and zeros?  
Message-ID: <420129DA.9000905@pdf.com>

      Does anyone have any ideas (or even experience) regarding a 
modified log transformation that would assign real finite values to 
zeros and negative numbers?  I encounter this routinely in a couple of 
different situations: 

      * Physical measurements that are often lognormally distributed 
except for values that are less than additive normal measurement error.  
I'd like to take logarithms of the clearly positive values and assign 
some smaller finite number(s) for values less than or equal to zero.  I 
also might like to decompose the values into mean plus variance of the 
logs plus variance of additive normal noise.  However, that would 
require more machinery than is appropriate for exploratory data analysis. 

      * Integers most of which are plausibly Poisson counts but include 
a few negative values.  People in manufacturing sometimes report the 
number of defects "added" between two steps in the process, computed as 
the difference between the number counted before and after intervening 
steps.  These counts are occasionally negative either because defects 
are removed in processing or because of a miscount either before or after. 

      For an example, see "www.prodsyse.com/log0".  There, you can also 
download working R code for such a transformation along with PowerPoint 
slides documenting some of the logic behind the code.  It's not included 
here, because it's too much for a standard R post. 

      Comments? 
      Thanks,
      spencer graves



From carsten.steinhoff at stud.uni-goettingen.de  Wed Feb  2 20:44:10 2005
From: carsten.steinhoff at stud.uni-goettingen.de (Carsten Steinhoff)
Date: Wed, 2 Feb 2005 20:44:10 +0100
Subject: [R] Frequency of Data
Message-ID: <E1CwQQH-0005Ks-Dg@s2.stud.uni-goettingen.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050202/1981fabb/attachment.pl

From rkoenker at uiuc.edu  Wed Feb  2 20:51:43 2005
From: rkoenker at uiuc.edu (roger koenker)
Date: Wed, 2 Feb 2005 13:51:43 -0600
Subject: [R] A modified log transformation with real finite values for
	negatives and zeros? 
In-Reply-To: <420129DA.9000905@pdf.com>
References: <420129DA.9000905@pdf.com>
Message-ID: <6cb84b5dd700aaebce778df436365b55@uiuc.edu>

Bickel and Doksum (JASA, 1981) discuss a modified version of the Box-Cox
transformation that looks like this:

	y -> ( sgn(y)* abs(y)^lambda -1)/lambda

and in the original Box-Cox paper there was an offset parameter that 
gives
rise to some somewhat peculiar likelihood theory as in the 3-parameter
log-normal where one gets an unbounded likelihood by letting the
threshold parameter approach the first order statistic  from below, but
for which the likeihood equations seem to provide a perfectly sensible
root.


url:	www.econ.uiuc.edu/~roger        	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820

On Feb 2, 2005, at 1:28 PM, Spencer Graves wrote:

>      Does anyone have any ideas (or even experience) regarding a 
> modified log transformation that would assign real finite values to 
> zeros and negative numbers?  I encounter this routinely in a couple of 
> different situations:
>      * Physical measurements that are often lognormally distributed 
> except for values that are less than additive normal measurement 
> error.  I'd like to take logarithms of the clearly positive values and 
> assign some smaller finite number(s) for values less than or equal to 
> zero.  I also might like to decompose the values into mean plus 
> variance of the logs plus variance of additive normal noise.  However, 
> that would require more machinery than is appropriate for exploratory 
> data analysis.
>      * Integers most of which are plausibly Poisson counts but include 
> a few negative values.  People in manufacturing sometimes report the 
> number of defects "added" between two steps in the process, computed 
> as the difference between the number counted before and after 
> intervening steps.  These counts are occasionally negative either 
> because defects are removed in processing or because of a miscount 
> either before or after.
>      For an example, see "www.prodsyse.com/log0".  There, you can also 
> download working R code for such a transformation along with 
> PowerPoint slides documenting some of the logic behind the code.  It's 
> not included here, because it's too much for a standard R post.
>      Comments?      Thanks,
>      spencer graves
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From james.holtman at convergys.com  Wed Feb  2 21:08:22 2005
From: james.holtman at convergys.com (james.holtman@convergys.com)
Date: Wed, 2 Feb 2005 15:08:22 -0500
Subject: [R] Frequency of Data
Message-ID: <OFC89CB79F.B81A9AE0-ON85256F9C.006E7FEB@nd.convergys.com>





Try this using strsplit and table:

> dates <- c('29.02.1997','15.02.2001','15.02.2001','23.12.2002')

> x.1 <- do.call('rbind',strsplit(dates,'\\.'))
> x.1
     [,1] [,2] [,3]
[1,] "29" "02" "1997"
[2,] "15" "02" "2001"
[3,] "15" "02" "2001"
[4,] "23" "12" "2002"
> class(x.1) <- 'integer'
> x.1
     [,1] [,2] [,3]
[1,]   29    2 1997
[2,]   15    2 2001
[3,]   15    2 2001
[4,]   23   12 2002

> table(list(x.1[,2], x.1[,3]))

    .2
.1   1997 2001 2002
  2  1    2    0
  12 0    0    1
>
__________________________________________________________
James Holtman        "What is the problem you are trying to solve?"
Executive Technical Consultant  --  Office of Technology, Convergys
james.holtman at convergys.com
+1 (513) 723-2929


                                                                                                                                                     
                      "Carsten Steinhoff"                                                                                                            
                      <carsten.steinhoff at stud.uni-goe        To:       <r-help at stat.math.ethz.ch>                                                    
                      ttingen.de>                            cc:                                                                                     
                      Sent by:                               Subject:  [R] Frequency of Data                                                         
                      r-help-bounces at stat.math.ethz.c                                                                                                
                      h                                                                                                                              
                                                                                                                                                     
                                                                                                                                                     
                      02/02/2005 14:44                                                                                                               
                                                                                                                                                     
                                                                                                                                                     




Hello,

just another problem in R, maybe it's simple to solve for you. I didn't
find
a solution up to now, but I'm convinced that I'm not the only one who
has/had a similar problem. Maybe there's a ready-made function in R?

The prob:

I've imported a CSV-file into R with 1000 dates of an observed event
(there's only information of the date. When there happend no event the date
is not recorded, when there have been two events it's recordet twice). Now
I
want to COUNT the frequency of events in every month or year.

The CSV-data is structured as:

date
25.02.2003
29.07.1997
...

My desired output would be a matrix with n rows for the years and m columns
for the month.

How could a solution look like ? If the format is no matrix it doesn't
matter. Importend is the extraction of frequency from my data.

Thanks for all reply,

Carsten

             [[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From msck9 at mizzou.edu  Wed Feb  2 21:29:49 2005
From: msck9 at mizzou.edu (msck9@mizzou.edu)
Date: Wed, 2 Feb 2005 14:29:49 -0600
Subject: [R] Boxplot by factors
Message-ID: <20050202202948.GA6056@localhost>

Dear all, 
 I have the following data format


 cellnumber    force
 	1	100
	1	230
	1	100
	1 	200
	1	130
	1	210
	2	179
	2	298
	2	400
	2	500
	2	600
	...........
I want to make a boxplot of the force according to the cellnumber. Here
the cellnumber is actually a factor. It has 1, 2 two levels. How can I
do that using boxplot? 

Thanks in advance
Ming



From MSchwartz at MedAnalytics.com  Wed Feb  2 21:38:25 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Wed, 02 Feb 2005 14:38:25 -0600
Subject: [R] Boxplot by factors
In-Reply-To: <20050202202948.GA6056@localhost>
References: <20050202202948.GA6056@localhost>
Message-ID: <1107376705.6632.21.camel@horizons.localdomain>

On Wed, 2005-02-02 at 14:29 -0600, msck9 at mizzou.edu wrote:
> Dear all, 
>  I have the following data format
> 
> 
>  cellnumber    force
>  	1	100
> 	1	230
> 	1	100
> 	1 	200
> 	1	130
> 	1	210
> 	2	179
> 	2	298
> 	2	400
> 	2	500
> 	2	600
> 	...........
> I want to make a boxplot of the force according to the cellnumber. Here
> the cellnumber is actually a factor. It has 1, 2 two levels. How can I
> do that using boxplot? 


boxplot(force ~ cellnumber)

See the formula method and the first example in ?boxplot

HTH,

Marc Schwartz



From Achim.Zeileis at wu-wien.ac.at  Wed Feb  2 21:42:23 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed, 2 Feb 2005 21:42:23 +0100
Subject: [R] Boxplot by factors
In-Reply-To: <20050202202948.GA6056@localhost>
References: <20050202202948.GA6056@localhost>
Message-ID: <20050202214223.1936393a.Achim.Zeileis@wu-wien.ac.at>

On Wed, 2 Feb 2005 14:29:49 -0600 msck9 at mizzou.edu wrote:

> Dear all, 
>  I have the following data format
> 
> 
>  cellnumber    force
>  	1	100
> 	1	230
> 	1	100
> 	1 	200
> 	1	130
> 	1	210
> 	2	179
> 	2	298
> 	2	400
> 	2	500
> 	2	600
> 	...........
> I want to make a boxplot of the force according to the cellnumber.
> Here the cellnumber is actually a factor. It has 1, 2 two levels. How
> can I do that using boxplot? 

Look at ?boxplot and also example(boxplot).

Hint: What you want is already described in the first line of that
example!
Z

> Thanks in advance
> Ming
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From cc2240 at columbia.edu  Wed Feb  2 21:49:27 2005
From: cc2240 at columbia.edu (Chung Chang)
Date: Wed,  2 Feb 2005 15:49:27 -0500
Subject: [R] a nls question
Message-ID: <1107377367.42013cd7d10c0@cubmail.cc.columbia.edu>

Hi,
   I have a question about how to get the residuals and estimations
of the parameters in my program.
For example,
x<-c(1,2,3,4,5,6)
y<-c(2.9, 1.24, 1.71, 2.989358, 1.455979, 1.4)
nls(y ~ a+sin(b*x),start=list(a=2.2,b=1.8),trace=TRUE)
I can get the estimation of a, b and residuals from the output but I
need them to be vectors in order to do futhur calculations in a
program. Is there any options for this function nls similar to
lm$coefficients and lm$residuals which I can get vectors of
estimation and residuals?
Thanks very much for your help

Chung



From eliothan at yahoo.com  Wed Feb  2 21:52:48 2005
From: eliothan at yahoo.com (Tae Sik Han)
Date: Wed, 2 Feb 2005 12:52:48 -0800 (PST)
Subject: [R] How to run R-script from remote VB program
Message-ID: <20050202205248.99881.qmail@web30503.mail.mud.yahoo.com>

I have installed D COM server1.35 and can send a line
of R code and get a result from my VB (.net) program.

So I located all the  R commands in a file ("a.R") in
the dir path ("c:\") then call VB using source as
below.

Dim RLink As StatConnector
RLink = New StatConnector  
RLink.Init("R")

dim str
str = "source(" + """" + "c:\a.R" + """" + ")"
RLink.EvaluateNoReturn(str)


I got an error message  :
An unhandled exception of type
'System.Runtime.InteropServices.COMException' occurred
in MY_VB.exe
Additional information: Object is static; operation
not allowed

"a.R" has codes to load RODBC package and collecting
and running query result to the Oracle DB.

How can I run the R-code script remotely from VB
program ?

I appreciate your help.

Regards,

Tae Sik Han.



From jeaneid at chass.utoronto.ca  Wed Feb  2 22:05:23 2005
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Wed, 2 Feb 2005 16:05:23 -0500
Subject: [R] a nls question
In-Reply-To: <1107377367.42013cd7d10c0@cubmail.cc.columbia.edu>
Message-ID: <Pine.SGI.4.40.0502021604500.9402548-100000@origin.chass.utoronto.ca>

as.vector(coef(nlsobj))
as.vector(resid(nlsobj))



On Wed, 2 Feb 2005, Chung Chang wrote:

> Hi,
>    I have a question about how to get the residuals and estimations
> of the parameters in my program.
> For example,
> x<-c(1,2,3,4,5,6)
> y<-c(2.9, 1.24, 1.71, 2.989358, 1.455979, 1.4)
> nls(y ~ a+sin(b*x),start=list(a=2.2,b=1.8),trace=TRUE)
> I can get the estimation of a, b and residuals from the output but I
> need them to be vectors in order to do futhur calculations in a
> program. Is there any options for this function nls similar to
> lm$coefficients and lm$residuals which I can get vectors of
> estimation and residuals?
> Thanks very much for your help
>
> Chung
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From andy_liaw at merck.com  Wed Feb  2 21:55:14 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 2 Feb 2005 15:55:14 -0500
Subject: [R] a nls question
Message-ID: <3A822319EB35174CA3714066D590DCD50994E639@usrymx25.merck.com>

Assign the nls() output to an object and then use coef() and resid(), just
as you do with lm().

Andy

> From: Chung Chang
> 
> Hi,
>    I have a question about how to get the residuals and estimations
> of the parameters in my program.
> For example,
> x<-c(1,2,3,4,5,6)
> y<-c(2.9, 1.24, 1.71, 2.989358, 1.455979, 1.4)
> nls(y ~ a+sin(b*x),start=list(a=2.2,b=1.8),trace=TRUE)
> I can get the estimation of a, b and residuals from the output but I
> need them to be vectors in order to do futhur calculations in a
> program. Is there any options for this function nls similar to
> lm$coefficients and lm$residuals which I can get vectors of
> estimation and residuals?
> Thanks very much for your help
> 
> Chung
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ggrothendieck at myway.com  Wed Feb  2 22:08:18 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 2 Feb 2005 21:08:18 +0000 (UTC)
Subject: [R] Frequency of Data
References: <E1CwQQH-0005Ks-Dg@s2.stud.uni-goettingen.de>
Message-ID: <loom.20050202T220529-976@post.gmane.org>

Carsten Steinhoff <carsten.steinhoff <at> stud.uni-goettingen.de> writes:

: 
: Hello,
: 
: just another problem in R, maybe it's simple to solve for you. I didn't find
: a solution up to now, but I'm convinced that I'm not the only one who
: has/had a similar problem. Maybe there's a ready-made function in R?
: 
: The prob:
: 
: I've imported a CSV-file into R with 1000 dates of an observed event
: (there's only information of the date. When there happend no event the date
: is not recorded, when there have been two events it's recordet twice). Now I
: want to COUNT the frequency of events in every month or year.
: 
: The CSV-data is structured as:
: 
: date
: 25.02.2003
: 29.07.1997
: ...
: 
: My desired output would be a matrix with n rows for the years and m columns
: for the month.
: 
: How could a solution look like ? If the format is no matrix it doesn't
: matter. Importend is the extraction of frequency from my data.


Assuming that dd is a vector of class Date, create vectors yy and mm 
with the years and months as factors and then use table:

yy <- as.numeric(format(dd, "%Y"))
yy <- factor(yy, levels = seq(min(yy), max(yy)))
mm <- factor(as.numeric(format(dd, "%m")), levels = 1:12)
table(yy,mm)



From HDoran at air.org  Wed Feb  2 22:20:43 2005
From: HDoran at air.org (Doran, Harold)
Date: Wed, 2 Feb 2005 16:20:43 -0500
Subject: [R] Not reproducing GLS estimates
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7407848D68@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050202/a0b6c2ed/attachment.pl

From helprhelp at gmail.com  Wed Feb  2 23:43:38 2005
From: helprhelp at gmail.com (WeiWei Shi)
Date: Wed, 2 Feb 2005 16:43:38 -0600
Subject: [R] feature (attribute) selection
Message-ID: <cdf8178305020214431d042340@mail.gmail.com>

Hi, there:
Recently, I read some papers about feature or attribute selection and
most of them are discussed in the context of supervised learning.

I knew Weka has implementation on some of them but I am wondering if
there is any package available in R which can do this kind of job.

Thanks for advice,

Ed



From stefan.albrecht at allianz.com  Wed Feb  2 23:53:58 2005
From: stefan.albrecht at allianz.com (stefan.albrecht@allianz.com)
Date: Wed, 2 Feb 2005 23:53:58 +0100
Subject: [R] Stefan Albrecht/HV/Finanzen/Allianz-Sach ist
 =?iso-8859-1?q?au=DF?=
 =?iso-8859-1?q?er_Haus=2E_=3A_R-help_Digest=2C_Vol_24=2C_Issue_2?=
Message-ID: <OFE482DDB6.4E811876-ONC1256F9C.007DCA6A@inside.allianz.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050202/543b1e41/attachment.pl

From das at cshl.edu  Thu Feb  3 00:22:13 2005
From: das at cshl.edu (Das, Rajdeep)
Date: Wed, 2 Feb 2005 18:22:13 -0500 
Subject: [R] feature (attribute) selection
Message-ID: <C8696843AE995F4EA4CDC3E2B83482A90A1BEA@mailbox02.cshl.edu>

Hi,

Look for "dprep" package for wrapper based feature selction that use  lda,
knn etc.

Also you can use package "rfe" that implements recursive feature elimination
using SVM.


-----Original Message-----
From: WeiWei Shi
To: R-help at stat.math.ethz.ch
Sent: 2/2/05 5:43 PM
Subject: [R] feature (attribute) selection

Hi, there:
Recently, I read some papers about feature or attribute selection and
most of them are discussed in the context of supervised learning.

I knew Weka has implementation on some of them but I am wondering if
there is any package available in R which can do this kind of job.

Thanks for advice,

Ed

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From paulangonzalez at hotmail.com  Thu Feb  3 02:22:26 2005
From: paulangonzalez at hotmail.com (pau gonzalez)
Date: Thu, 03 Feb 2005 01:22:26 +0000
Subject: [R] non parametric bootstrap
Message-ID: <BAY10-F7275C154DE9CC8193D94BC67F0@phx.gbl>



From Tom.Mulholland at dpi.wa.gov.au  Thu Feb  3 02:30:33 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Thu, 3 Feb 2005 09:30:33 +0800
Subject: [R] Displaying a distribution -- was: Combining two histograms
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA81@afhex01.dpi.wa.gov.au>

I am immediately reminded of something I read which goes

"A sufficiently trained statistician can read the vagaries of a Q-Q plot like a sharman can read a chicken's entrails, with a similar recourse to scientific principles. Interpreting Q-Q plots is more a visceral than an intellectual exercise. The uninitiated are often mystified by the process. Experience is the key here."

http://www.maths.murdoch.edu.au/units/statsnotes/samplestats/qqplot.html

Having said that I would suggest many people have difficulty understanding density plots, but think that they can understand histograms.

I am currently undergoing shaman training ;-) and find that my interpretation of the plots owes more to experience than it does to a structured method of analysis. I see the technique as additional rather than as a replacement for density estimates. As for the order of exploration, I tend to be non-linear in my explorations. In my perfect world I would like them to be simultaneous. The order of any information presentation can impact upon the output, so I tend to have lists of processes to be done without pre-ordaining the order. It could be that I see exploration as a different process to analysis. That is I am more ad-hoc with the generation of pieces of the puzzle and more structured with putting the picture together.

Tom.

> -----Original Message-----
> From: Berton Gunter [mailto:gunter.berton at gene.com]
> Sent: Thursday, 3 February 2005 12:52 AM
> To: 'Deepayan Sarkar'; r-help at stat.math.ethz.ch
> Subject: [R] Displaying a distribution -- was: Combining two 
> histograms
> 
> 
> May I take this off topic a little to seek collective wisdom 
> (and so feel
> free to reply privately).
> 
> The catalyst is Deepayan's remark:
> 
> > Histograms were appropriate for drawing density estimates by 
> > hand in the  good old days, but I can imagine very few 
> situations where I 
> > would not prefer to use smoother density estimates when I have the 
> > computational power to do so.
> > 
> > Deepayan
> 
> Generally, I agree; but the appearance and thus one's perception and
> interpretation of both histograms and density plots depend upon the
> parameters chosen for the display (bin boundaries for 
> histograms; bandwidth
> and kernel for density plots). Important data peculiarities 
> like arbitrary
> rounding, favoring of certain values, resolution limitations, 
> and so forth
> are therefore often lost. I would instead advocate that 
> simple quantile
> plots -- plot(ppoints(x),sort(x)) -- or perhaps normal 
> qqplots always be the
> first plot used to explore univariate data distributions. I 
> believe this
> conforms to Bill Cleveland's recommendations, who says in the 
> first sentence
> on p. 17 of VISUALIZING DATA on visualizing univariate data: 
> "Quantiles are
> essential to visualizing distributions."
> 
> While it is true that many people may be unfamiliar with 
> quantile plots, I
> think we need to improve modern statistical practice not only 
> by abandoning
> histograms in favor of density plots, but also by always first using
> quantile plots and explaining why this is necessary.
> 
> Difficult issue: What should one do when when there are, say, 
> a million
> values?
> 
> Alternative views?
> 
>  
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
>  
> "The business of the statistician is to catalyze the 
> scientific learning
> process."  - George E. P. Box
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ramasamy at cancer.org.uk  Thu Feb  3 03:08:38 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Thu, 03 Feb 2005 02:08:38 +0000
Subject: [R] Combining two histograms
In-Reply-To: <004201c5093b$a92ee370$4af1ba51@hblaptop>
References: <004201c5093b$a92ee370$4af1ba51@hblaptop>
Message-ID: <1107396518.5864.25.camel@dhcp-63.ccc.ox.ac.uk>

Looking at the output from this and the previous response using barplot
worries me for two reasons :

a) The bars in "front" may obscure the bars at the "back". The order of
plotting becomes important here.

b) IMHO, it is distracting and does not convey information well unless
one is willing to risk a headache looking closer at the plots.



Personally, I prefer looking at density curves especially if I want to
compare many variables.

# simulate data
x1 <- rnorm(1000, 0.4, 0.8)
x2 <- rnorm(1000, 0.0, 1.0)
x3 <- rnorm(1000, -1.0, 1.0)

# density plots
plot( density(x1), xlim=range( c(x1, x2, x3) ), main="", xlab="" )
lines(density(x2), col=2)
lines(density(x3), col=3)

# rug plots for displaying actual data points
# you can add jitter() to rug() but with 1000 obs not much difference
rug(x1, col=1, ticksize=0.01, line=2.5)
rug(x2, col=2, ticksize=0.01, line=3.0)
rug(x3, col=3, ticksize=0.01, line=3.5)


Regards, Adai


On Wed, 2005-02-02 at 16:27 +0100, Henrik Bengtsson wrote:
> If you looking for something like 
> 
> x1 <- rnorm(1000,  0.4, 0.8)
> x2 <- rnorm(1000,  0.0, 1.0)
> x3 <- rnorm(1000, -1.0, 1.0)
> hist(x1, width=0.33, offset=0.00, col="blue", xlim=c(-4,4),
>  main="Histogram of x1, x2 & x3", xlab="x1 - blue, x2 - red, x3 - green")
> hist(x2, width=0.33, offset=0.33, col="red", add=TRUE)
> hist(x3, width=0.33, offset=0.66, col="green", add=TRUE)
> 
> with results as in http://www.maths.lth.se/help/R/plot.histogram/ there is
> an extension to hist() (actually plot.histogram()) in the R.basic package
> (part of the R.classes bundle). See
> http://www.maths.lth.se/help/R/R.classes/ for installation instructions.
> 
> BTW, you should also consider plotting density() estimates.
> 
> Henrik Bengtsson
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Sean Davis
> > Sent: Wednesday, February 02, 2005 2:42 PM
> > To: r-help
> > Subject: [R] Combining two histograms
> > 
> > 
> > I have data like:
> > 
> > a <- rnorm(20000)
> > b <- rep(FALSE,20000)
> > b[sample(1:20000,15000)] <- TRUE
> > 
> > Using Lattice graphics, I can produce two side-by-side 
> > histograms quite 
> > easily by:
> > 
> > histogram(a | b)
> > 
> > However, I would like to produce a "single" histogram with two bars 
> > within each bin, one for each group, as the groups are in 
> > reality very 
> > slightly different from each other.  The difference isn't evident 
> > unless one "overlays" the two histograms in some manner.
> > 
> > Thanks,
> > Sean
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From eliothan at yahoo.com  Thu Feb  3 03:46:44 2005
From: eliothan at yahoo.com (Tae Sik Han)
Date: Wed, 2 Feb 2005 18:46:44 -0800 (PST)
Subject: [R] call RODBC function from DCOM 
Message-ID: <20050203024645.42557.qmail@web30507.mail.mud.yahoo.com>

Hi.

My VB program run a R - script located in the server
with R DCOM server. The R - script is designed to send
a query to DB through ODBC. There is NOT any problem
when I run each code from R-window interface. 

However, when I call the R-script from my VB program
it stucked after calling "sqlQuery" statement. I can
check that odbcConnect(dsn, uid, pwd) returns -1.

------------------VB code-----------------------------
        str = "setwd(" + """" + "C:/Program
Files/R/rw2001" + """" + ")"
        RLink.EvaluateNoReturn(str)
        str = "source(" + """" + "a.R" + """" + ")"
        RLink.EvaluateNoReturn(str)


------------------R script:
"a.R"-----------------------------
library(RODBC);

con = odbcConnect("dsn", uid = "...", pwd="s...");
tlist = sqlQuery(con, "select tname from cols");


------------------Error message --------------------

An unhandled exception of type
'System.Runtime.InteropServices.COMException' occurred
in MY_VB.exe

Additional information: Object is static; operation
not allowed
----------------------------------------------------

I need to do more to correctly set up DCOM server for
ODBC ?

Thanks and regards,

Eliot.



From ggrothendieck at myway.com  Thu Feb  3 03:58:41 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 3 Feb 2005 02:58:41 +0000 (UTC)
Subject: [R] Frequency of Data
References: <E1CwQQH-0005Ks-Dg@s2.stud.uni-goettingen.de>
	<loom.20050202T220529-976@post.gmane.org>
Message-ID: <loom.20050203T035529-751@post.gmane.org>

Gabor Grothendieck <ggrothendieck <at> myway.com> writes:

: 
: Carsten Steinhoff <carsten.steinhoff <at> stud.uni-goettingen.de> writes:
: 
: : 
: : Hello,
: : 
: : just another problem in R, maybe it's simple to solve for you. I didn't 
find
: : a solution up to now, but I'm convinced that I'm not the only one who
: : has/had a similar problem. Maybe there's a ready-made function in R?
: : 
: : The prob:
: : 
: : I've imported a CSV-file into R with 1000 dates of an observed event
: : (there's only information of the date. When there happend no event the date
: : is not recorded, when there have been two events it's recordet twice). Now 
I
: : want to COUNT the frequency of events in every month or year.
: : 
: : The CSV-data is structured as:
: : 
: : date
: : 25.02.2003
: : 29.07.1997
: : ...
: : 
: : My desired output would be a matrix with n rows for the years and m columns
: : for the month.
: : 
: : How could a solution look like ? If the format is no matrix it doesn't
: : matter. Importend is the extraction of frequency from my data.
: 
: Assuming that dd is a vector of class Date, create vectors yy and mm 
: with the years and months as factors and then use table:
: 
: yy <- as.numeric(format(dd, "%Y"))
: yy <- factor(yy, levels = seq(min(yy), max(yy)))
: mm <- factor(as.numeric(format(dd, "%m")), levels = 1:12)
: table(yy,mm)
: 

There is also a particularly simple solution if you assume ddc is a vector
of chron dates, e.g. ddc <- chron(dd) where dd is as above.

library(chron)
table(years(ddc), months(ddc))

However, note that it is not 100% equivalent to the prior solution because 
it omits any years that are not included at all whereas the all Date
solution included them.



From helprhelp at gmail.com  Thu Feb  3 04:06:29 2005
From: helprhelp at gmail.com (WeiWei Shi)
Date: Wed, 2 Feb 2005 21:06:29 -0600
Subject: [R] feature (attribute) selection
In-Reply-To: <C8696843AE995F4EA4CDC3E2B83482A90A1BEA@mailbox02.cshl.edu>
References: <C8696843AE995F4EA4CDC3E2B83482A90A1BEA@mailbox02.cshl.edu>
Message-ID: <cdf8178305020219061093fea5@mail.gmail.com>

Hi,
I found the dprep package but I am wondering if there is a document
and some example on how to use it?
I am also curious if it can use cart or similar algorithm instead of
lda? I am also interested in finding some methods using GA in feature
selection.

thanks

Ed

On Wed, 2 Feb 2005 18:22:13 -0500, Das, Rajdeep <das at cshl.edu> wrote:
> Hi,
> 
> Look for "dprep" package for wrapper based feature selction that use  lda,
> knn etc.
> 
> Also you can use package "rfe" that implements recursive feature elimination
> using SVM.
> 
> 
> -----Original Message-----
> From: WeiWei Shi
> To: R-help at stat.math.ethz.ch
> Sent: 2/2/05 5:43 PM
> Subject: [R] feature (attribute) selection
> 
> Hi, there:
> Recently, I read some papers about feature or attribute selection and
> most of them are discussed in the context of supervised learning.
> 
> I knew Weka has implementation on some of them but I am wondering if
> there is any package available in R which can do this kind of job.
> 
> Thanks for advice,
> 
> Ed
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From kjetil at acelerate.com  Thu Feb  3 00:04:40 2005
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Wed, 02 Feb 2005 19:04:40 -0400
Subject: [R] Displaying a distribution -- was: Combining two histograms
In-Reply-To: <200502021746.j12Hk9R5025937@volta.gene.com>
References: <200502021746.j12Hk9R5025937@volta.gene.com>
Message-ID: <42015C88.80107@acelerate.com>

Berton Gunter wrote:

>Rug plots cannot show replicated values -- spikes at discrete values -- in
>the data distribution. Quantile plots do.
>  
>
But you can use rug(jitter(x))

Kjetil

>-- Bert Gunter
>Genentech Non-Clinical Statistics
>South San Francisco, CA
> 
>"The business of the statistician is to catalyze the scientific learning
>process."  - George E. P. Box
> 
> 
>
>  
>
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor 
>>Grothendieck
>>Sent: Wednesday, February 02, 2005 9:20 AM
>>To: r-help at stat.math.ethz.ch
>>Subject: Re: [R] Displaying a distribution -- was: Combining 
>>two histograms
>>
>>Berton Gunter <gunter.berton <at> gene.com> writes:
>>
>>: 
>>: May I take this off topic a little to seek collective 
>>wisdom (and so feel
>>: free to reply privately).
>>: 
>>: The catalyst is Deepayan's remark:
>>: 
>>: > Histograms were appropriate for drawing density estimates by 
>>: > hand in the  good old days, but I can imagine very few 
>>situations where I 
>>: > would not prefer to use smoother density estimates when I 
>>have the 
>>: > computational power to do so.
>>: > 
>>: > Deepayan
>>: 
>>: Generally, I agree; but the appearance and thus one's perception and
>>: interpretation of both histograms and density plots depend upon the
>>: parameters chosen for the display (bin boundaries for 
>>histograms; bandwidth
>>: and kernel for density plots). 
>>
>>If the problem is distrust of the procedure one can simulataneously
>>display the raw data with a rug plot, 
>>
>>example(rug)
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>    
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra





-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From nusbj at hotmail.com  Thu Feb  3 07:08:55 2005
From: nusbj at hotmail.com (Zhen Pang)
Date: Thu, 03 Feb 2005 14:08:55 +0800
Subject: [R] Boxplot by factors
In-Reply-To: <20050202202948.GA6056@localhost>
Message-ID: <BAY22-F35692F4486915686A4A60DAF7F0@phx.gbl>

boxplot(force ~ cellnumber)


>From: msck9 at mizzou.edu
>To: r-help at stat.math.ethz.ch
>Subject: [R] Boxplot by factors
>Date: Wed, 2 Feb 2005 14:29:49 -0600
>
>Dear all,
>  I have the following data format
>
>
>  cellnumber    force
>  	1	100
>	1	230
>	1	100
>	1 	200
>	1	130
>	1	210
>	2	179
>	2	298
>	2	400
>	2	500
>	2	600
>	...........
>I want to make a boxplot of the force according to the cellnumber. Here
>the cellnumber is actually a factor. It has 1, 2 two levels. How can I
>do that using boxplot?
>
>Thanks in advance
>Ming
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From Lorenz.Gygax at fat.admin.ch  Thu Feb  3 07:52:30 2005
From: Lorenz.Gygax at fat.admin.ch (Lorenz.Gygax@fat.admin.ch)
Date: Thu, 3 Feb 2005 07:52:30 +0100 
Subject: [R] random effects in lme
Message-ID: <BF74FADD4B44554CA7E53D0B5242CD6A01FC648E@evd-s7014.evd.admin.ch>


It is unlikely that your request will be answered faster if you post it  a
second time in exactly the same way ...

> Suppose I have a linear mixed-effects model (from the package 
> nlme) with nested random effects (see below); how would I present
> the results from the random effects part in a publication?
> 
> Specifically, I?d like to know:
> (1) What is the total variance of the random effects at each level?

> Random effects:
> Formula:  ~ 1 | plotcode
>        (Intercept)
> StdDev:  0.04176364
> 
>  Formula:  ~ 1 | treatment %in% plotcode
>       (Intercept)   Residual
> StdDev:  0.08660458 0.00833387

What is wrong with an estimted StdDev on the level of plotcode of 0.0418 and
on the level of treatment (which is actually the same as to say that this is
a block of plants within plotcode that received the same treatment?) of
0.087?

> (2) How can I test the significance of the variance components?

Calculate a model with an without the component of interest and compare the
models using anova().

Perhaps you should read Pinheiro & Bates (2000)?

> (3) Is there something like an "r squared" for the whole 
> model which I can state?

None is provided and I do not know how easily it could be calculated. But
the use of the measure can be questioned. It is an absolute measure on how
much of the variability in the data is explained. Imagine you had true
replicates (i.e. several measurements under absolutely identical
situations). Imagine further that these measurements do nevertheless show
some variability. If this variability was substantial, your r-squared would
be low even though your model might pick up all the structure that you can
find in the data. Thus you can only get as good as 'natural' variability in
your data which is not considered by the r-squared measure.

Thus, (corrected) r-squared values might tell you something if you compare
different models based on the same data (in a similar way as the AIC and BIC
criteria) but not if you compare completely different data sets.

Regards, Lorenz
- 
Lorenz Gygax, Dr. sc. nat.
Centre for proper housing of ruminants and pigs
Swiss Federal Veterinary Office
agroscope FAT T?nikon, CH-8356 Ettenhausen / Switzerland



From Lorenz.Gygax at fat.admin.ch  Thu Feb  3 08:00:17 2005
From: Lorenz.Gygax at fat.admin.ch (Lorenz.Gygax@fat.admin.ch)
Date: Thu, 3 Feb 2005 08:00:17 +0100 
Subject: [R] Special paper for postscript
Message-ID: <BF74FADD4B44554CA7E53D0B5242CD6A01FC648F@evd-s7014.evd.admin.ch>


> When I generate a "special" paper postscript image larger than
> "a4" or "letter" using R, I can only see one-page portion of all 
> image, of course.
> What will be the simple solution for this? Is there any way I 
> can set the bounding box information on the image? Or any other
> suggestions?

I often use 'special' paper for graphs that are smaller than A4. I believe
that the postscript files thus created do have the correct bounding box. If
you import such a graph it shows the right boundaries. Alternatively if I
look at such a graph in ghostview I can set the boundaries myself in the
Menu Media->user defined according to the bounding boxes in the postscript
file.

Thus, I am not sure what your problem is ...

Regards, Lorenz
- 
Lorenz Gygax, Dr. sc. nat.
Centre for proper housing of ruminants and pigs
Swiss Federal Veterinary Office
agroscope FAT T?nikon, CH-8356 Ettenhausen / Switzerland



From Lorenz.Gygax at fat.admin.ch  Thu Feb  3 08:09:11 2005
From: Lorenz.Gygax at fat.admin.ch (Lorenz.Gygax@fat.admin.ch)
Date: Thu, 3 Feb 2005 08:09:11 +0100 
Subject: [R] binomia data and mixed model
Message-ID: <BF74FADD4B44554CA7E53D0B5242CD6A01FC6490@evd-s7014.evd.admin.ch>


> The experimental design was suppose to consist of 4 
> treatments replicated 3 time, Source 1 and applied at 10 cm 
> and source 2 applied at 20 cm. During the construction of the
> treatmetns the depths vary considerably so i can't test all my
> samples based on 10 and 20 cm any more the depths are now
> considered random and not fixed.

It is not really clear what you measuring ... but do you know the depth even
if it is not exactly 10 and 20 cm? Then, perhaps you could use this variable
in a continous fashion but still as fixed? I do not really see how you can
treat such a variable as random.

> The data is very non-normal (lots of zeros) therefore the only way 
> to analyze it is to convert to binomial data.

I doubt this is the only way but certainly a valid one.

> Does any one know what type of analysis I should use? I was told
> that a NLmixed model would work but also a GLIM mixed model was
> appropriate. Is there any info using these in R.

I do not know about those but you can conduct binomial mixed effects models
by either using glmmPQL in library 'MASS' or GLMM in library 'lme4'.

Also do read:
  author = 	 {Pinheiro, Jose C and Bates, Douglas M},
  title = 	 {Mixed-Effects Models in {S} and {S}-{P}{L}{U}{S}},
  publisher = 	 {Springer},
  year = 	 {2000},
  address = 	 {New York}
and
	AUTHOR = {Venables, W N and Ripley, B D},
	TITLE = {Modern Applied Statistics with {S}},
	PUBLISHER = {Springer},
	YEAR = {2002},
	ADDRESS = {New York},
	EDITION = {fourth}

Regards, Lorenz
- 
Lorenz Gygax, Dr. sc. nat.
Centre for proper housing of ruminants and pigs
Swiss Federal Veterinary Office
agroscope FAT T?nikon, CH-8356 Ettenhausen / Switzerland



From Lorenz.Gygax at fat.admin.ch  Thu Feb  3 08:25:28 2005
From: Lorenz.Gygax at fat.admin.ch (Lorenz.Gygax@fat.admin.ch)
Date: Thu, 3 Feb 2005 08:25:28 +0100 
Subject: [R] Split-split plot ANOVA
Message-ID: <BF74FADD4B44554CA7E53D0B5242CD6A01FC6491@evd-s7014.evd.admin.ch>

> I have been going over and over the examples in MASS 
> and the Pinheiro and Bates example, but cannot get my model 
> to run correctly with either aov or lme.
>
> Could someone give me a hand with the correct model statement?

It would help to see some of the things you have tried already ...

> First a description of the design.  We are studying 
> germination rates for 
> various species under a variety of treaments.  This is a 
> blocked split-split 
> plot design.  The levels and treatments are:
> 
> Blocks:  1-6
> 
> Whole plot treatment:
>    Overstory:  Yes or No
> 
> Split plot treatments:
>    Caging (to protect against seed predators):  Yes or No
>    Herbaceous competition (i.e., grass):  Yes or No
> 
> Split-split plot treatment:
>    Tree species:  7 kinds
> 
> The response variable is Lag, which is a indication of when 
> the seeds first germinated.

I would try somthing like

lme (fixed= Lag ~ Caging + herbaceous + tree,
     data= your.data,
     random= ~ 1 | Overstory/split/splitsplit)

Perhaps you want/need to add some interactions as well. Overstory, split and
splitsplit would be factors with specific levels for each of the plots,
split plots and split-split plots, respectively.

Thus what I attempted here is to separate the variables of the hierarchical
design of data gathering (which go into the random effects) and the
treatments (which go into the fixed effects).

The degrees of freedom for the fixed effects are automatically adjusted to
the correct level in the hierarchy.

Did you try that? What did not work out with it?

> Lastly, I have unbalanced data since some treatment 
> combinations never had any germination.

In principle, the REML estimates in lme are not effected by unbalanced data.

BUT I do not think that the missing germinations by themselves lead to an
unbalanced data set: I assume it is informative that in some treatment
combinations there was no germination. Thus, your lag there is something
close to infinity (or at least longer than you cared to wait ;-). Thus, I
would argue you have to somehow include these data points as well, otherwise
you can only make a very restricted statement of the kind: if there was
germination, this depended on such and such.

> Since the data are highly nonnormal, I hope to do a 
> permutations test on the F-values for each main effect and 
> interaction in order to get my p-values.

As these are durations a log transformation of your response might be
enough.

Regards, Lorenz
- 
Lorenz Gygax, Dr. sc. nat.
Centre for proper housing of ruminants and pigs
Swiss Federal Veterinary Office
agroscope FAT T?nikon, CH-8356 Ettenhausen / Switzerland



From Achim.Zeileis at R-project.org  Wed Feb  2 20:22:08 2005
From: Achim.Zeileis at R-project.org (Achim Zeileis)
Date: Wed, 2 Feb 2005 20:22:08 +0100
Subject: [R] [R-pkgs] fortune update: 100th fortune
Message-ID: <20050202202208.23379258.Achim.Zeileis@R-project.org>

Dear useRs,

version 1.1-0 of the fortunes package is available from CRAN. It now
contains 100 R fortunes. Thanks to all who contributed.

Best wishes,
Z

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From vera.hofer at uni-graz.at  Thu Feb  3 11:04:03 2005
From: vera.hofer at uni-graz.at (Vera Hofer)
Date: Thu, 03 Feb 2005 11:04:03 +0100
Subject: [R] nonparametric manova
Message-ID: <4201F713.901@uni-graz.at>

Dear colleagues,

has anyone an idea how to carry out a nonparametric manova for comparing 
3 groups?
Thank you for your help.

Vera



From laura at env.leeds.ac.uk  Thu Feb  3 11:10:48 2005
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Thu, 3 Feb 2005 10:10:48 +0000 (GMT)
Subject: [R] Interpretation of PC loadings
Message-ID: <Pine.LNX.4.44.0502021751280.17308-100000@env-pc-phd13>

Hello,

Following on from a paper by Ludwig, Horel and Whiteman in 2004, I have
been using PCA to identify surface wind patterns. In the paper they
approach the problem using the "real vector approach", that is, they
create paired (u,v) vectors for the wind data, in effect artificially
doubling the number of variables, then perform the pca, subsequently repairing the
output and plotting the paired site loadings onto a geographical map and
interpreting the plotted vectors as dominant wind flow patterns.

I have followed their approach, using prcomp() in R, using retx=FALSE and
plotting the pairings from the pca$rotation output. When I replot these
vectors onto a map it seems that a spurious angle has been introduced into
the data (ie the data seem to be angled at approx 45 degrees from where I
would expect them to lie).

Due to this I've also tried switching retx=TRUE and plotting pca$x though
the output seems quite different. I have looked at the help section for
prcomp but it doesn't seem very explicit in it's explanation (ie does $x
take the same form as $rotation - will it be analagous to plot them in the
same fashion for instance?)

Could someone advise me which method will be more reliable in terms of
revealing underlying flow direction when remapping onto the original (x,y)
map?

Any advice most welcome!

Thank you,

Laura Quinn
Institute of Atmospheric Science
School of Earth and Environment
University of Leeds
Leeds
LS2 9JT

tel: +44 113 343 1596
fax: +44 113 343 6716
mail: laura at env.leeds.ac.uk



From vito_ricci at yahoo.com  Thu Feb  3 11:10:58 2005
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Thu, 3 Feb 2005 11:10:58 +0100 (CET)
Subject: [R] Re: nonparametric manova
Message-ID: <20050203101058.76432.qmail@web41207.mail.yahoo.com>

Hi Vera,
See:
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/22273.html
it seems like your problem.
Best regards,
Vito


you wrote:

Dear colleagues,

has anyone an idea how to carry out a nonparametric
manova for comparing 
3 groups?
Thank you for your help.

Vera



=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box

Top 10 reasons to become a Statistician

     1. Deviation is considered normal
     2. We feel complete and sufficient
     3. We are 'mean' lovers
     4. Statisticians do it discretely and continuously
     5. We are right 95% of the time
     6. We can legally comment on someone's posterior distribution
     7. We may not be normal, but we are transformable
     8. We never have to say we are certain
     9. We are honestly significantly different
    10. No one wants our jobs


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/



From Christoph.Scherber at uni-jena.de  Thu Feb  3 11:40:03 2005
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Thu, 03 Feb 2005 11:40:03 +0100
Subject: [R] Split-split plot ANOVA
In-Reply-To: <BF74FADD4B44554CA7E53D0B5242CD6A01FC6491@evd-s7014.evd.admin.ch>
References: <BF74FADD4B44554CA7E53D0B5242CD6A01FC6491@evd-s7014.evd.admin.ch>
Message-ID: <4201FF83.2020700@uni-jena.de>

Hi Mike,

Do you have a schematic drawing of how exactly your treatments were 
applied? In split-plot experiments, it is generally very important to 
clearly define the sequence of plot sizes, because if you don?t do this 
properly, then the output will be confusing. Checking if your degrees of 
freedom at each level are correct should give you a good idea about 
whether you?ve specified the model in the right way.

Generally, I see some problem with your model specification as you seem 
to have two (not one) treatments in some of your subplots.

If I got it right, the sequence of terms should be something like 
Block/Whole.plot/Caging/Competition/Species

at least if it?s a full split-plot.

Can you send me some more details on the design?


Regards,
Christoph



Lorenz.Gygax at fat.admin.ch wrote:

>>I have been going over and over the examples in MASS 
>>and the Pinheiro and Bates example, but cannot get my model 
>>to run correctly with either aov or lme.
>>
>>Could someone give me a hand with the correct model statement?
>>    
>>
>
>It would help to see some of the things you have tried already ...
>
>  
>
>>First a description of the design.  We are studying 
>>germination rates for 
>>various species under a variety of treaments.  This is a 
>>blocked split-split 
>>plot design.  The levels and treatments are:
>>
>>Blocks:  1-6
>>
>>Whole plot treatment:
>>   Overstory:  Yes or No
>>
>>Split plot treatments:
>>   Caging (to protect against seed predators):  Yes or No
>>   Herbaceous competition (i.e., grass):  Yes or No
>>
>>Split-split plot treatment:
>>   Tree species:  7 kinds
>>
>>The response variable is Lag, which is a indication of when 
>>the seeds first germinated.
>>    
>>
>
>I would try somthing like
>
>lme (fixed= Lag ~ Caging + herbaceous + tree,
>     data= your.data,
>     random= ~ 1 | Overstory/split/splitsplit)
>
>Perhaps you want/need to add some interactions as well. Overstory, split and
>splitsplit would be factors with specific levels for each of the plots,
>split plots and split-split plots, respectively.
>
>Thus what I attempted here is to separate the variables of the hierarchical
>design of data gathering (which go into the random effects) and the
>treatments (which go into the fixed effects).
>
>The degrees of freedom for the fixed effects are automatically adjusted to
>the correct level in the hierarchy.
>
>Did you try that? What did not work out with it?
>
>  
>
>>Lastly, I have unbalanced data since some treatment 
>>combinations never had any germination.
>>    
>>
>
>In principle, the REML estimates in lme are not effected by unbalanced data.
>
>BUT I do not think that the missing germinations by themselves lead to an
>unbalanced data set: I assume it is informative that in some treatment
>combinations there was no germination. Thus, your lag there is something
>close to infinity (or at least longer than you cared to wait ;-). Thus, I
>would argue you have to somehow include these data points as well, otherwise
>you can only make a very restricted statement of the kind: if there was
>germination, this depended on such and such.
>
>  
>
>>Since the data are highly nonnormal, I hope to do a 
>>permutations test on the F-values for each main effect and 
>>interaction in order to get my p-values.
>>    
>>
>
>As these are durations a log transformation of your response might be
>enough.
>
>Regards, Lorenz
>- 
>Lorenz Gygax, Dr. sc. nat.
>Centre for proper housing of ruminants and pigs
>Swiss Federal Veterinary Office
>agroscope FAT T?nikon, CH-8356 Ettenhausen / Switzerland
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From heikz at gmx.de  Thu Feb  3 11:59:58 2005
From: heikz at gmx.de (heikz@gmx.de)
Date: Thu, 3 Feb 2005 11:59:58 +0100 (MET)
Subject: [R] logistic regression 3D-plot
Message-ID: <21437.1107428398@www38.gmx.net>

Dear R-helpers,

I tried to create a 3D surface showing the interaction between two
continuous explanatory variables; the response variable is binary (0/1).

The model is:

model<-glm(incidence~sun*trees,binomial)

then I used "wireframe" to create a 3D plot:

xyz<-expand.grid(sun=seq(30,180,1),trees=seq(0,4000,10))

xyz$incidence<-as.vector(predict(model,xyz))

wireframe(incidence~sun*trees,xyz,scales=list(arrows=FALSE))

which gives me a 3D plot, but the scaling of the y-axis is wrong. the range
is not from 0 to 1.
so my question: is there a way to plot these kind of models, with binary
response variables?

thanks for your help, Heike


--



From bitwrit at ozemail.com.au  Tue Feb  1 23:09:38 2005
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Wed, 2 Feb 2005 09:09:38 +1100
Subject: [R] Special paper for postscript
In-Reply-To: <BE23D83D.2B9C%thchung@tgen.org>
References: <BE23D83D.2B9C%thchung@tgen.org>
Message-ID: <20050203110520.YRAL6229.smta06.mail.ozemail.net@there>

On Tuesday 01 February 2005 06:42 am, Tae-Hoon Chung wrote:
> Hi, All;
>
> When I generate a "special" paper postscript image larger than "a4" or
> "letter" using R, I can only see one-page portion of all image, of course.
> What will be the simple solution for this? Is there any way I can set the
> bounding box information on the image? Or any other suggestions?
>
If you mean "How can I view the entire image in Ghostview?", change the 
viewing window to "BoundingBox" and make sure that the "BoundingBox:" 
parameters, usually right near the top of the Postscript file, cover the 
entire area that you want to view.

Jim



From Lorenz.Gygax at fat.admin.ch  Thu Feb  3 12:10:38 2005
From: Lorenz.Gygax at fat.admin.ch (Lorenz.Gygax@fat.admin.ch)
Date: Thu, 3 Feb 2005 12:10:38 +0100 
Subject: [R] logistic regression 3D-plot
Message-ID: <BF74FADD4B44554CA7E53D0B5242CD6A01FC6496@evd-s7014.evd.admin.ch>

> I tried to create a 3D surface showing the interaction between two
> continuous explanatory variables; the response variable is 
> binary (0/1).
> 
> The model is:
> 
> model<-glm(incidence~sun*trees,binomial)
> 
> then I used "wireframe" to create a 3D plot:
> 
> xyz<-expand.grid(sun=seq(30,180,1),trees=seq(0,4000,10))
> 
> xyz$incidence<-as.vector(predict(model,xyz))

xyz$incidence<-as.vector(predict(model,xyz, type= "response")) 
should work

> wireframe(incidence~sun*trees,xyz,scales=list(arrows=FALSE))

Cheers, Lorenz



From calenge at biomserv.univ-lyon1.fr  Thu Feb  3 09:52:15 2005
From: calenge at biomserv.univ-lyon1.fr (=?iso-8859-1?Q?Cl=E9ment?= Calenge)
Date: Thu, 03 Feb 2005 09:52:15 +0100
Subject: [R] [R-pkgs] adehabitat version 1.2
Message-ID: <5.1.0.14.0.20050203092344.00c5c980@biomserv.univ-lyon1.fr>

Dear all,

I have just uploaded to CRAN the version 1.2 of the package
'adehabitat'. This package now contains new functions for
the estimation of home ranges (Nearest neighbour convex
hull, brownian bridge approach for kernel estimation),
estimation of schoener's ratio.  This package now allows a
better management of objects of class "area" (see plot.area,
perarea, ararea). Some arguments of the graphical functions
have also been changed (kernel, image.kasc, scatter.enfa).
Finally, some minor bugs have been corrected. Note that
the package now need that the packages ade4 and gpclib
are installed.

Questions, comments and suggestions are greatly appreciated.

Happy testing,


Cl?ment Calenge.

======================================
Cl?ment CALENGE
LBBE - UMR CNRS 5558 - Universit? Claude Bernard Lyon 1 - FRANCE
tel. (+33) 04.72.43.27.57
fax. (+33) 04.72.43.13.88

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From f.harrell at vanderbilt.edu  Thu Feb  3 13:23:13 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 03 Feb 2005 06:23:13 -0600
Subject: [R] logistic regression 3D-plot
In-Reply-To: <21437.1107428398@www38.gmx.net>
References: <21437.1107428398@www38.gmx.net>
Message-ID: <420217B1.8060809@vanderbilt.edu>

heikz at gmx.de wrote:
> Dear R-helpers,
> 
> I tried to create a 3D surface showing the interaction between two
> continuous explanatory variables; the response variable is binary (0/1).
> 
> The model is:
> 
> model<-glm(incidence~sun*trees,binomial)
> 
> then I used "wireframe" to create a 3D plot:
> 
> xyz<-expand.grid(sun=seq(30,180,1),trees=seq(0,4000,10))
> 
> xyz$incidence<-as.vector(predict(model,xyz))
> 
> wireframe(incidence~sun*trees,xyz,scales=list(arrows=FALSE))
> 
> which gives me a 3D plot, but the scaling of the y-axis is wrong. the range
> is not from 0 to 1.
> so my question: is there a way to plot these kind of models, with binary
> response variables?
> 
> thanks for your help, Heike
> 
library(Design)
d <- datadist(mydata); options(datadist='d')
f <- lrm(incidence ~ sun*trees)  # lrm is for binary or ordinal response
plot(f, sun=NA, trees=NA)
# add method='image' or 'contour' to get other types of graphs
plot(f, sun=NA, trees=NA, fun='plogis')  # probability scale

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From michael.watson at bbsrc.ac.uk  Thu Feb  3 13:24:19 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Thu, 3 Feb 2005 12:24:19 -0000
Subject: [R] Mantel Ranomization test
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950172CD51@iahce2knas1.iah.bbsrc.reserved>

Hi

I found this post from 2001
(https://stat.ethz.ch/pipermail/r-help/2001-April/011073.html) and was
just wondering if this has been updated and included in an R package
yet?

Cheers
Mick



From f.harrell at vanderbilt.edu  Thu Feb  3 13:31:21 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 03 Feb 2005 06:31:21 -0600
Subject: [R] logistic regression 3D-plot CORRECTION
In-Reply-To: <420217B1.8060809@vanderbilt.edu>
References: <21437.1107428398@www38.gmx.net> <420217B1.8060809@vanderbilt.edu>
Message-ID: <42021999.6000902@vanderbilt.edu>

> library(Design)
> d <- datadist(mydata); options(datadist='d')
> f <- lrm(incidence ~ sun*trees)  # lrm is for binary or ordinal response
> plot(f, sun=NA, trees=NA)
> # add method='image' or 'contour' to get other types of graphs
> plot(f, sun=NA, trees=NA, fun='plogis')  # probability scale

Correction: fun=plogis not 'plogis'.  Sorry about that  -FH



From gavin.simpson at ucl.ac.uk  Thu Feb  3 13:40:49 2005
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Thu, 03 Feb 2005 12:40:49 +0000
Subject: [R] Mantel Ranomization test
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950172CD51@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E950172CD51@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <42021BD1.2060602@ucl.ac.uk>

michael watson (IAH-C) wrote:
> Hi
> 
> I found this post from 2001
> (https://stat.ethz.ch/pipermail/r-help/2001-April/011073.html) and was
> just wondering if this has been updated and included in an R package
> yet?
> 
> Cheers
> Mick

see mantel() in package vegan on CRAN. The web site for vegan is:

http://cc.oulu.fi/~jarioksa/softhelp/vegan.html

Gav
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpsonATNOSPAMucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From ckjmaner at carolina.rr.com  Thu Feb  3 14:34:33 2005
From: ckjmaner at carolina.rr.com (Charles and Kimberly Maner)
Date: Thu, 3 Feb 2005 08:34:33 -0500
Subject: [R] Reading Dates in a csv File
Message-ID: <200502031336.j13Dagkd012373@ms-smtp-03-eri0.southeast.rr.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050203/e23721db/attachment.pl

From Jostein.Johansen at ism.uit.no  Thu Feb  3 14:59:12 2005
From: Jostein.Johansen at ism.uit.no (Jostein Johansen)
Date: Thu, 3 Feb 2005 14:59:12 +0100
Subject: [R] No package 'file10181' was found
Message-ID: <A338E29C32EB4D42BD341E2F0062D28E0246DD7F@EDENEVS1.asp.ad.uit.no>

I am running R 2.0.1 and I get this error message when I'm trying to install different packages: No package 'file10181' was found.
Can anyone help?
JJ



From hchandra at soton.ac.uk  Thu Feb  3 15:01:11 2005
From: hchandra at soton.ac.uk (Chandra H.)
Date: Thu, 3 Feb 2005 14:01:11 -0000
Subject: [R] Getting "Information matrix"  in  mixed models
Message-ID: <41310D8A2BD91B4CB42AB540EF248F59019CF49F@exchange4.soton.ac.uk>

Hello,
I am Hukum Chandra and I'm a statistician. I am using R. I have gone
through R but I could not get the way how to find "Information matrix"
in R. In particular,  var-Cov matrix of components of variances in
linear mixed model. Could you please help me in getting the way to
produce it using R?

With best regards
Hukum
===================================
Hukum Chandra
Division of Social Statistics
School of Social Sciences
University of Southampton
SOUTHAMPTON SO17 1BJ, United Kingdom 
 Email : hchandra at soton.ac.uk, hcpatwa1 at rediffmail.com
 Web : http://www.socstats.soton.ac.uk/



From roberts at egs.uct.ac.za  Thu Feb  3 15:05:15 2005
From: roberts at egs.uct.ac.za (Wesley Roberts)
Date: Thu, 03 Feb 2005 16:05:15 +0200
Subject: [R] Memory Cap
Message-ID: <42022F9B.7090302@egs.uct.ac.za>

Hi all

I am trying to use R for some data editing using the "array" function to 
write binary data to a text file. I realise that R was not designed for 
this purpose but I am no C programmer and would prefer to use R (as I 
know how to do it and hate c). Basically I get the following error.

Error: cannot allocate vector of size 1634069 kb

It seems that R is not keen on allocating memory to large arrays. I have 
attached the *.R file. Is there anyway that I could turn off the memory 
cap and allow R to take a larger set of data files.

Thanks Wesley


-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: data_test.R
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050203/ab947ac4/data_test.pl

From bates at stat.wisc.edu  Thu Feb  3 15:36:59 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 03 Feb 2005 08:36:59 -0600
Subject: [R] Getting "Information matrix"  in  mixed models
In-Reply-To: <41310D8A2BD91B4CB42AB540EF248F59019CF49F@exchange4.soton.ac.uk>
References: <41310D8A2BD91B4CB42AB540EF248F59019CF49F@exchange4.soton.ac.uk>
Message-ID: <4202370B.9070407@stat.wisc.edu>

Chandra H. wrote:
> Hello,
> I am Hukum Chandra and I'm a statistician. I am using R. I have gone
> through R but I could not get the way how to find "Information matrix"
> in R. In particular,  var-Cov matrix of components of variances in
> linear mixed model. Could you please help me in getting the way to
> produce it using R?

They are not given for linear mixed models fit by lme (either from the 
nlme package or from the lme4 package) or by lmer (the lme4 package). 
This is intentional.  I don't think they are meaningful and I prefer not 
to give an answer than to give a misleading answer.

The reason I don't think they are meaningful is because an information 
matrix (or, equivalently, standard errors and correlations) are useful 
summaries when we can expect the distribution of the parameter estimates 
to be roughly symmetric.  In the case of a variance component the 
parameter estimate has a distribution that is like a Chi-squared 
distribution and not at all symmetric.

Think of the simple case of obtaining a confidence interval on the 
variance of a sample that is assumed to be i.i.d. normal.  We use a 
Chi-squared reference distribution and expect to obtain an interval that 
is quite asymmetric.  We do not use estimate +/- some multiple of a 
standard error to form an interval.

Many packages that fit mixed models report estimates of variance 
components, their approximate standard errors, a 'z' statistic and a 
p-value for the test of the variance component being greater than zero. 
  The approximations are so bad in this test that I think it is better 
not to have a p-value than to have one that is extremely doubtful.



From andy_liaw at merck.com  Thu Feb  3 15:55:47 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 3 Feb 2005 09:55:47 -0500
Subject: [R] Memory Cap
Message-ID: <3A822319EB35174CA3714066D590DCD50994E63F@usrymx25.merck.com>

If I'm not mistaken, you are dealing with a total of:

> 414180 * 1515 * 4 / 1024^2
[1] 2393.657

or close to 2.4GB, for a single copy of the (combined) array.  Unless you're
on a 64-bit OS running on hardware with sufficient physical RAM, that's not
likely to work so well.

Also, I believe you should close() the connections before rm() them.

Andy

> From: Wesley Roberts
> 
> Hi all
> 
> I am trying to use R for some data editing using the "array" 
> function to 
> write binary data to a text file. I realise that R was not 
> designed for 
> this purpose but I am no C programmer and would prefer to use R (as I 
> know how to do it and hate c). Basically I get the following error.
> 
> Error: cannot allocate vector of size 1634069 kb
> 
> It seems that R is not keen on allocating memory to large 
> arrays. I have 
> attached the *.R file. Is there anyway that I could turn off 
> the memory 
> cap and allow R to take a larger set of data files.
> 
> Thanks Wesley
> 
> 
>



From softblizzard at km.ru  Thu Feb  3 16:02:34 2005
From: softblizzard at km.ru ()
Date: Thu, 3 Feb 2005 18:02:34 +0300
Subject: [R] Handling R Objects  in C++
Message-ID: <42023D0A.000BF4.02548@e-post02.e-se.ru>

Hello!

I want R to call some functions from my DLL, which is written on C++
using Microsoft Visual C++ 6.0. I've looked through "Writing R
extensions" manual and "FAQ for Windows". I achieved, that R can
call void functions without parameters from my C++ DLL.

Then I included R headers files to my DLL project, I was going to use
SEXP type:

#include "Rinternals.h"
#include "R.h"

And as a result, my DLL can't be compiled, the error is:

C:\Program Files\Microsoft Visual Studio\VC98\INCLUDE\math.h(514): error: this declaration may not have extern "C" linkage
  template<class _Ty> inline
  ^
Would you please give me a peice of advise how to handle this problem?



From michael.watson at bbsrc.ac.uk  Thu Feb  3 16:08:32 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Thu, 3 Feb 2005 15:08:32 -0000
Subject: [R] How to convert a list to a matrix
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950121BA04@iahce2knas1.iah.bbsrc.reserved>

Hi

Sorry to ask such a basic question.  I have a list, each element of
which is a vector of two values.  What I actually want is a matrix with
two columns, and one row per element of the list.  Obviously I have
tried as.matrix(), and as.vector() but I didn't expect the latter to
work.

I feel so lame asking this.  Any suggestions?

Mick



From jeff.horner at vanderbilt.edu  Thu Feb  3 16:20:26 2005
From: jeff.horner at vanderbilt.edu (Jeffrey Horner)
Date: Thu, 03 Feb 2005 09:20:26 -0600
Subject: [R] Rinternals.h and iostream don't play nice together'
In-Reply-To: <Pine.LNX.4.61.0501301828460.10677@Chrestomanci>
References: <Pine.LNX.4.61.0501301753460.10677@Chrestomanci>	<Pine.OSF.4.58.0501301708370.374256@odin.mdacc.tmc.edu>
	<Pine.LNX.4.61.0501301828460.10677@Chrestomanci>
Message-ID: <4202413A.8040300@vanderbilt.edu>

Faheem Mitha wrote:
> 
> 
> On Sun, 30 Jan 2005, Paul Roebuck wrote:
> 
>> and if you use the following instead of "Rinternals.h"?
>> extern "C" {
>> #include <Rdefines.h>
>> }
> 
> 
> Still the same errors. As Dirk pointed out, putting iostream first makes 
> the errors go away in either case.

You have to define R_NO_REMAP in your code before you include
Rinternals.h, as the "Writing R Extensions" manual states. If not,
Rinternals.h will define the length macro (and others...) which is
defined somewhere within iostream or included from iostream.

I caught this in the 7th line of error output:

$ g++ -I/usr/local/lib/R/include -g -c foo.cc
In file included from /usr/include/c++/3.3/bits/locale_facets.h:528,
                  from /usr/include/c++/3.3/bits/basic_ios.h:44,
                  from /usr/include/c++/3.3/ios:51,
                  from /usr/include/c++/3.3/ostream:45,
                  from /usr/include/c++/3.3/iostream:45,
                  from foo.cc:3:
/usr/include/c++/3.3/bits/codecvt.h:110:52: macro "length" passed 4
arguments, but takes just 1
...

It's good to read error messages.

Cheers,
-- 
Jeffrey Horner       Computer Systems Analyst         School of Medicine
615-322-8606         Department of Biostatistics   Vanderbilt University



From dimitris.rizopoulos at med.kuleuven.ac.be  Thu Feb  3 16:34:24 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Thu, 3 Feb 2005 16:34:24 +0100
Subject: [R] How to convert a list to a matrix
References: <8975119BCD0AC5419D61A9CF1A923E950121BA04@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <005a01c50a05$d6dbf220$0540210a@www.domain>

try this:

mat <- matrix(unlist(lis), ncol=2, byrow=TRUE)

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "michael watson (IAH-C)" <michael.watson at bbsrc.ac.uk>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, February 03, 2005 4:08 PM
Subject: [R] How to convert a list to a matrix


> Hi
>
> Sorry to ask such a basic question.  I have a list, each element of
> which is a vector of two values.  What I actually want is a matrix 
> with
> two columns, and one row per element of the list.  Obviously I have
> tried as.matrix(), and as.vector() but I didn't expect the latter to
> work.
>
> I feel so lame asking this.  Any suggestions?
>
> Mick
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From andy_liaw at merck.com  Thu Feb  3 16:33:04 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 3 Feb 2005 10:33:04 -0500
Subject: [R] How to convert a list to a matrix
Message-ID: <3A822319EB35174CA3714066D590DCD50994E640@usrymx25.merck.com>

do.call() is your friend:

mat <- do.call("rbind", myList)

Andy

> From: michael watson (IAH-C)
> 
> Hi
> 
> Sorry to ask such a basic question.  I have a list, each element of
> which is a vector of two values.  What I actually want is a 
> matrix with
> two columns, and one row per element of the list.  Obviously I have
> tried as.matrix(), and as.vector() but I didn't expect the latter to
> work.
> 
> I feel so lame asking this.  Any suggestions?
> 
> Mick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From sdavis2 at mail.nih.gov  Thu Feb  3 16:39:14 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 3 Feb 2005 10:39:14 -0500
Subject: [R] How to convert a list to a matrix
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950121BA04@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E950121BA04@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <C229E292-75F9-11D9-AFCC-000D933565E8@mail.nih.gov>

Will do.call('rbind',yourlist) do what you want?

Sean

On Feb 3, 2005, at 10:08 AM, michael watson ((IAH-C)) wrote:

> Hi
>
> Sorry to ask such a basic question.  I have a list, each element of
> which is a vector of two values.  What I actually want is a matrix with
> two columns, and one row per element of the list.  Obviously I have
> tried as.matrix(), and as.vector() but I didn't expect the latter to
> work.
>
> I feel so lame asking this.  Any suggestions?
>
> Mick
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From jeaneid at chass.utoronto.ca  Thu Feb  3 16:50:09 2005
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Thu, 3 Feb 2005 10:50:09 -0500
Subject: [R] How to convert a list to a matrix
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950121BA04@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <Pine.SGI.4.40.0502031049300.9615676-100000@origin.chass.utoronto.ca>

list1 <- list(x=c(1,2), y=c(3,4), z=c(4,5))
matrix(unlist(list1), nrow=length(list1), byrow=T)

HTH

On Thu, 3 Feb 2005, michael watson (IAH-C) wrote:

> Hi
>
> Sorry to ask such a basic question.  I have a list, each element of
> which is a vector of two values.  What I actually want is a matrix with
> two columns, and one row per element of the list.  Obviously I have
> tried as.matrix(), and as.vector() but I didn't expect the latter to
> work.
>
> I feel so lame asking this.  Any suggestions?
>
> Mick
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From francoisromain at free.fr  Thu Feb  3 16:42:23 2005
From: francoisromain at free.fr (Romain Francois)
Date: Thu, 03 Feb 2005 16:42:23 +0100
Subject: [R] How to convert a list to a matrix
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950121BA04@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E950121BA04@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <4202465F.70801@free.fr>

Hello,

try the following :

> l <- list(NULL)
> l[[1]] <- 1:2
> l[[2]] <- 2:3
> l[[3]] <- 4:5
> l
[[1]]
[1] 1 2

[[2]]
[1] 2 3

[[3]]
[1] 4 5

> matrix(unlist(l),ncol=2)
     [,1] [,2]
[1,]    1    3
[2,]    2    4
[3,]    2    5


Is that what you want ?

Romain
Le 03.02.2005 16:08, michael watson (IAH-C) a ?crit :

>Hi
>
>Sorry to ask such a basic question.  I have a list, each element of
>which is a vector of two values.  What I actually want is a matrix with
>two columns, and one row per element of the list.  Obviously I have
>tried as.matrix(), and as.vector() but I didn't expect the latter to
>work.
>
>I feel so lame asking this.  Any suggestions?
>
>Mick
>
>  
>

-- 
Romain FRANCOIS : francoisromain at free.fr
page web : http://addictedtor.free.fr/  (en construction)
06 18 39 14 69 / 01 46 80 65 60
_______________________________________________________
Etudiant en 3eme ann?e
Institut de Statistique de l'Universit? de Paris (ISUP)
Fili?re Industrie et Services
http://www.isup.cicrp.jussieu.fr/



From f.harrell at vanderbilt.edu  Thu Feb  3 16:47:42 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 03 Feb 2005 09:47:42 -0600
Subject: [R] Reading Dates in a csv File
In-Reply-To: <200502031336.j13Dagkd012373@ms-smtp-03-eri0.southeast.rr.com>
References: <200502031336.j13Dagkd012373@ms-smtp-03-eri0.southeast.rr.com>
Message-ID: <4202479E.1070904@vanderbilt.edu>

Charles and Kimberly Maner wrote:
> Hi all.  I'm reading in a flat, comma-delimited flat file using read.csv.
> It works marvelously for the most part.  I am using the colClasses argument
> to, basically, create numeric, factor and character classes for the columns
> I'm reading in.  However, a couple of the fields in the file are date
> fields.  I'm fully aware that POSIXct can be used as a class, however the
> field must obey, (I think), the standard/default POSIXct format.  Hence the
> following question:  Does anyone have a method they can share to read in a
> non-standard formatted date to convert to POSIXct?  I can read it in then
> convert it, but that's a two pass approach and not as elegant as a single
> pass through read.csv.  I've read, from the documentation, that "[o]therwise
> there needs to be an as method (from package methods) for conversion from
> "character" to the specified formal class" but I do not know and have not
> figured out how to do that.
> 
> Any suggestion(s) would be greatly appreciated.
> 
> 
> Thanks,
> Charles

The csv.get function in the Hmisc package may do most of what you want.

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From deb37 at columbia.edu  Thu Feb  3 17:42:28 2005
From: deb37 at columbia.edu (Daniel E. Bunker)
Date: Thu, 03 Feb 2005 11:42:28 -0500
Subject: [R] Efficient selection and alteration of dataframe records
Message-ID: <42025474.1030508@columbia.edu>

Hi All,

I am writing a simulation that examines the effects of species 
extinctions on ecological communties by sequentially removing 
individuals of a given species (sometimes using weighted probabilities) 
and replacing the lost individuals with species identities randomly 
sampled from the remaining individuals. Thus I  use two dataframes. One 
contains all the individuals and their species identities (plotdf).  The 
other contains each species and their associated weights (traitdf).

While I have code that works, it runs slowly.  I suspect there is a more 
efficient way.

First, I 'sample' one species from the species file (traitdf), then I 
use that result to 'subset' the individuals dataframe (plotdf) into two 
new files: individuals of the extincted species (plotdf.del) and 
retained individuals (plotdf.old). 

I then use a 'for' loop to run through each record in plotdf.del and 
randomly sample a new species identity from plotdf.old.  (Note that I 
also need one species specific variable from the traitdf dataframe, 
which I have been attaching using 'merge.') When all are replaced, I 
simply 'rbind' plotdf.old and plotdf.del back together.  I then delete 
another species, etc, etc.

My guess is that there is a way to replace the lost individuals using a 
'sample' that simply excludes the lost individuals (records).  This 
would avoid splitting the data frame and 'rbind'ing it back together.  
If I could also inlcude a second variable from the 'sample'd records, 
this would eliminate the need for the 'merge'.

I am running R2.0.0 on windows 2000. 

Simplified code is below.

Any suggestions would be greatly appreciated.

Thanks for your time, Dan

plotdf=data.frame(
    tag=1:100,
    
pspp=c(rep("Sp1",40),rep("Sp2",30),rep("Sp3",20),rep("Sp4",5),rep("Sp5",5)),
    dim=runif(100)*100)
plotdf[1,]

abun.table=as.data.frame(table(plotdf$pspp))

#2.1 calculate Smax (count of species)
Smax=length(abun.table$Freq[abun.table$Freq>0])
Smax

traitdf=data.frame(
    tspp=c("Sp1","Sp2","Sp3","Sp4","Sp5"),
    width=runif(5),
    abun=abun.table$Freq)
traitdf[1,]

rm(abun.table)
   
#3. merge plotdf and traitdf
plotdft=merge(plotdf, traitdf, by.x ="pspp", by.y="tspp")

#4 define summary dataframe sumdf
sumdf=data.frame(s.n=NA, s.S=NA, s.crop=NA)

    #reset all data to raw data.
    #b. calculate crop in plotdft with all species present
    plotdft$crop=plotdft$width*exp(-2.0+2.42*(log(plotdft$dim)))
    #c. sum crop
    sumcrop=sum(plotdft$crop)
    #d. write n, S, crop to sumdf
    sumdflength=length(sumdf$s.n)       
    sumdf[sumdflength+1,1]=1;
    sumdf[sumdflength+1,2]=Smax;
    sumdf[sumdflength+1,3]=sumcrop;

    #6. SPECIES DELETION LOOP. This is the species deletion loop.
    #a. repeat from n=1:Smax-1 (S=Smax-n+1)
    for(n in 1:(Smax-1)) {
        S=Smax-n+1;

        #b. remove and replace one species
        #1. sample one species based on weight (e.g., abundance)
        #delsp = sample(traitdf$tspp, size=1);delsp
        delsp = sample(traitdf$tspp, size=1, prob=traitdf[,3]);

        #2. select traitdf records that match delsp
        traitdf.del = subset(traitdf, tspp==delsp);traitdf.del[1,]

        #3. and delete that species from trait data
        traitdf = subset(traitdf, tspp!=delsp[1]);

        #4. split that species from plot data into new df
        plotdf.old = subset(plotdf, plotdf$pspp!=delsp);plotdf.old[1,]
        plotdf.del = subset(plotdf, plotdf$pspp==delsp);plotdf.del[1,]

            #5. replace delsp params with params randomly selected from 
remaining spp:
            for (x in 1:length(plotdf.del$pspp)){
                newsp = sample(plotdf.old$pspp, size=1);#print(newsp[1])
                plotdf.del$pspp[x]=newsp[1]
            }
            #6. rbind plotdf and splitdf into plotdf,
            plotdf=rbind(plotdf.old,plotdf.del);plotdf[1,]

    #b. calculate standing crop,etc
        #1. merge plotdf and traitdf
        plotdft=merge(plotdf, traitdf, by.x ="pspp", by.y="tspp")

        #2. calculate crop in plotdft
        plotdft$crop=plotdft$width*exp(-2.0+2.42*log(plotdft$dim))

        #3. sum crop
        sumcrop=sum(plotdft$crop)

        #4. calculate S
        abun.table=as.data.frame(table(plotdf$pspp))
        S=length(abun.table$Freq[abun.table$Freq>0])

    #c. write  n, S, crop to sumdf
        sumdflength=length(sumdf$s.n)
        sumdf[sumdflength+1,1]=n+1;
        sumdf[sumdflength+1,2]=S;
        sumdf[sumdflength+1,3]=sumcrop;   
    }#d. REPEAT SPECIES DELETION LOOP
    #housekeeping
    rm(delsp, plotdf, plotdf.del, plotdf.old, plotdft, traitdf.del)
    gc()

#8. plot results, fit line
print(sumdf)
traitdf

plot(sumdf$s.S, sumdf$s.crop)






-- 

Daniel E. Bunker
Associate Coordinator - BioMERGE
Post-Doctoral Research Scientist
Columbia University
Department of Ecology, Evolution and Environmental Biology
1020 Schermerhorn Extension
1200 Amsterdam Avenue
New York, NY 10027-5557

212-854-9881
212-854-8188 fax
deb37 at columbia.edu



From rwilson+ at pitt.edu  Thu Feb  3 17:54:17 2005
From: rwilson+ at pitt.edu (roy wilson)
Date: Thu, 03 Feb 2005 11:54:17 -0500
Subject: [R] If this is should be posted elsewhere, please advise
Message-ID: <42025739.2040006@pitt.edu>

Hi,

I am puzzled by the relationship between the p-values asociated with the 
coefficients of a univariate logistic regression involving categorical 
variables and the p-value I get from Fisher's exact test of the 
associated 2 x 2 contingency table.

(1) The 2-sided p-value for the table is ~ 0.0015, whereas the p-value 
for the independent is 0.101 and the p-value for the intercept is ~0.56.

(2) I know the the coefficient for the independent is the ln(OR) 
(assuming I recall correctly). Most accounts I've seen ignore the 
intercept, say it's hard to interpret, or give an interpretation (with 
respect to the dataset as a single group) that I can't square with the 
interpretation of beta[1].

Thanks for help for redirection.



-- 
Roy Wilson
Learning Research Development Center
University of Pittsburgh
webpage: www.pitt.edu/~rwilson
email: rwilson at pitt.edu



From francoisromain at free.fr  Thu Feb  3 17:56:16 2005
From: francoisromain at free.fr (Romain Francois)
Date: Thu, 03 Feb 2005 17:56:16 +0100
Subject: [R] How to convert a list to a matrix
In-Reply-To: <4202465F.70801@free.fr>
References: <8975119BCD0AC5419D61A9CF1A923E950121BA04@iahce2knas1.iah.bbsrc.reserved>
	<4202465F.70801@free.fr>
Message-ID: <420257B0.3020006@free.fr>

Le 03.02.2005 16:42, Romain Francois a ?crit :

> Hello,
>
> try the following :
>
>> l <- list(NULL)
>> l[[1]] <- 1:2
>> l[[2]] <- 2:3
>> l[[3]] <- 4:5
>> l
>
> [[1]]
> [1] 1 2
>
> [[2]]
> [1] 2 3
>
> [[3]]
> [1] 4 5
>
>> matrix(unlist(l),ncol=2)
>
Sorry, i forgot the byrow argument, the correct comand was :

matrix(unlist(l),ncol=2,byrow=T)

I need some coffee. :-)

Romain

>     [,1] [,2]
> [1,]    1    3
> [2,]    2    4
> [3,]    2    5
>
>
> Is that what you want ?
>
> Romain
> Le 03.02.2005 16:08, michael watson (IAH-C) a ?crit :
>
>> Hi
>>
>> Sorry to ask such a basic question.  I have a list, each element of
>> which is a vector of two values.  What I actually want is a matrix with
>> two columns, and one row per element of the list.  Obviously I have
>> tried as.matrix(), and as.vector() but I didn't expect the latter to
>> work.
>>
>> I feel so lame asking this.  Any suggestions?
>>
>> Mick
>>
>>  
>>
>


-- 
Romain FRANCOIS : francoisromain at free.fr
page web : http://addictedtor.free.fr/  (en construction)
06 18 39 14 69 / 01 46 80 65 60
_______________________________________________________
Etudiant en 3eme ann?e
Institut de Statistique de l'Universit? de Paris (ISUP)
Fili?re Industrie et Services
http://www.isup.cicrp.jussieu.fr/



From combe at b3e.jussieu.fr  Thu Feb  3 18:10:24 2005
From: combe at b3e.jussieu.fr (combe@b3e.jussieu.fr)
Date: Thu,  3 Feb 2005 18:10:24 +0100
Subject: [R] he^lp on "stack" function
Message-ID: <1107450624.42025b003ceed@webmail.b3e.jussieu.fr>




Excuse me.
I don't understant how to use the function "stack".
I'm not sure to know the good syntax.

I've got a data witch column names are the time when the mesures was done and
an
other who indicates the age of the subjetcs.
I would like to do a new matrix with patients older than a value and the new
matrix will contain in the first column the time of the mesures and in the
second th evalue of the mesure.

Could you help me please ?



From p.dalgaard at biostat.ku.dk  Thu Feb  3 18:23:15 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 03 Feb 2005 18:23:15 +0100
Subject: [R] Logistic regression coef. Was: If this is should be posted
	elsewhere, please advise
In-Reply-To: <42025739.2040006@pitt.edu>
References: <42025739.2040006@pitt.edu>
Message-ID: <x2hdktv9q4.fsf@biostat.ku.dk>


Well, in principle there are general stats lists and newsgroups, and
this is not specifically an R question. However, people around here
have been known to contribute information about statistical substance
from time to time. You're still not excused for not using a meaningful
subject line though...

roy wilson <rwilson+ at pitt.edu> writes:

> Hi,
> 
> I am puzzled by the relationship between the p-values asociated with
> the coefficients of a univariate logistic regression involving
> categorical variables and the p-value I get from Fisher's exact test
> of the associated 2 x 2 contingency table.
> 
> (1) The 2-sided p-value for the table is ~ 0.0015, whereas the p-value
> for the independent is 0.101 and the p-value for the intercept is
> ~0.56.

The immediate conjecture is that you have become yet another victim of
the Hauck-Donner effect (an exact reference was given a week or so
ago, so use the list archives). The Wald tests given by dividing
coefficients with their formal s.e. can be badly misleading if you are
far from satisfying the requirements for asymptotic theory. This is
especially bad in cases where the OR is 0 or infinite so that the
glm() fit is divergent. 

You might try drop1(yourfit,test="Chisq") and see if the likelihod
ratio test for the independent is not at least somewhat closer to the
Fisher test.

> 
> (2) I know the the coefficient for the independent is the ln(OR)
> (assuming I recall correctly). Most accounts I've seen ignore the
> intercept, say it's hard to interpret, or give an interpretation (with
> respect to the dataset as a single group) that I can't square with the
> interpretation of beta[1].

In R it is (normally) the log-odds of the baseline group. That is,
provided you are using treatment contrasts.

> Thanks for help for redirection.
> 
> 
> 
> -- 
> Roy Wilson
> Learning Research Development Center
> University of Pittsburgh
> webpage: www.pitt.edu/~rwilson
> email: rwilson at pitt.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Rau at demogr.mpg.de  Thu Feb  3 19:15:54 2005
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Thu, 3 Feb 2005 19:15:54 +0100
Subject: [R] Surprising Behavior of 'tapply'
Message-ID: <8B08A3A1EA7AAC41BE24C750338754E61506FA@HERMES.demogr.mpg.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050203/67d8fff9/attachment.pl

From elvis at xlsolutions-corp.com  Thu Feb  3 19:16:47 2005
From: elvis at xlsolutions-corp.com (elvis@xlsolutions-corp.com)
Date: Thu,  3 Feb 2005 11:16:47 -0700
Subject: [R] Course***R/S System: Advanced Programming,
	San Francisco *** March 31st-April1st
Message-ID: <20050203181647.10337.qmail@webmail01.mesa1.secureserver.net>

XSolutions Corp (www.xlsolutions-corp.com) is proud to announce
our "Advanced R/Splus programming" course taught by R Development
Core Team Guru!

www.xlsolutions-corp.com/Radv.htm

*********San Francisco ---------->  March 31st-April 1st, 2005

*********Boston, MA    ---------->  TBD

Early-bird discount ends Feb 25th!
Ask for group discount and reserve your seat Now  (payment due after the
class)

Email Sue Turner: sue at xlsolutions-corp.com
Phone: 206-686-1578


Course Outline:

- Overview of R/S fundamentals: Syntax and Semantics
- Class and Inheritance in R/S-Plus
- Concepts, Construction and good use of language objects
- Coercion and efficiency
- Object-oriented programming in R and S-Plus
- Advanced manipulation tools: Parse, Deparse, Substitute, etc.
- How to fully take advantage of Vectorization
- Generic and Method Functions; S4 (S-Plus 6)
- Search path, databases and frames Visibility
- Working with large objects
- Handling Properly Recursion and iterative calculations
- Managing loops; For (S-Plus) and for() loops
- Consequences of Lazy Evaluation
- Efficient Code practices for large computations
- Memory management and Resource monitoring
- Writing R/S-Plus functions to call compiled code
- Writing and debugging compiled code for R/S-Plus system
- Connecting R/S-Plus to External Data Sources
- Understanding the structure of model fitting functions in R/S-Plus
- Designing and Packaging efficiently a new model function

It'll also deal with lots of S-Plus efficiency issues and any special
topics
from participants is welcome.

Please let us know if you and your colleagues are interested in this
class
to take advantage of group discount. Over half of the seats in this
class
are currently reserved.  Register now to secure your seat in this
course!

Cheers,

Elvis Miller, PhD
Manager Training.
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com



From gerifalte28 at hotmail.com  Thu Feb  3 19:38:01 2005
From: gerifalte28 at hotmail.com (F Z)
Date: Thu, 03 Feb 2005 18:38:01 +0000
Subject: [R] he^lp on "stack" function
In-Reply-To: <1107450624.42025b003ceed@webmail.b3e.jussieu.fr>
Message-ID: <BAY103-F956117F5A31774C78C507A67F0@phx.gbl>

In the future please be a little more specific with your questions. i.e. 
give us an example of your data  the way it looks now and the way you want 
it to look.

I think what you want is something like:

time<-seq(as.Date("2005/1/1"), as.Date("2005/1/10"), "days")
age<-round(runif(10,30,50))
measure<-rnorm(10)
mydat<-data.frame(time=time,age=age,measure=measure) #creates a data frame 
with your 3 #variables

         time age    measure
1  2005-01-01  48 -1.2873181
2  2005-01-02  50  0.5776001
3  2005-01-03  44  0.1892008
4  2005-01-04  38  0.4981651
5  2005-01-05  49 -1.7591745
6  2005-01-06  37 -0.8898464
7  2005-01-07  30  0.7528217
8  2005-01-08  46 -1.2421763
9  2005-01-09  39 -0.9333667
10 2005-01-10  32  1.2086509

#suppose you only want patients older than 40
Older40<-mydat[age>=40,c(1,3)]
        time    measure
1 2005-01-01 -1.2873181
2 2005-01-02  0.5776001
3 2005-01-03  0.1892008
5 2005-01-05 -1.7591745
8 2005-01-08 -1.2421763

Is this what you wanted to do?

Francisco


>From: "" <combe at b3e.jussieu.fr>
>To: "" <R-help at stat.math.ethz.ch>
>Subject: [R] he^lp on "stack" function
>Date: Thu,  3 Feb 2005 18:10:24 +0100
>
>
>
>
>Excuse me.
>I don't understant how to use the function "stack".
>I'm not sure to know the good syntax.
>
>I've got a data witch column names are the time when the mesures was done 
>and
>an
>other who indicates the age of the subjetcs.
>I would like to do a new matrix with patients older than a value and the 
>new
>matrix will contain in the first column the time of the mesures and in the
>second th evalue of the mesure.
>
>Could you help me please ?
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From carlos_ortegafernandez at yahoo.es  Thu Feb  3 19:56:06 2005
From: carlos_ortegafernandez at yahoo.es (Carlos Ortega)
Date: Thu, 3 Feb 2005 19:56:06 +0100 (CET)
Subject: [R] Surprising Behavior of 'tapply'
In-Reply-To: <8B08A3A1EA7AAC41BE24C750338754E61506FA@HERMES.demogr.mpg.de>
Message-ID: <20050203185606.66918.qmail@web25204.mail.ukl.yahoo.com>

Hi,

That is something strange, I could not replicate it...

Regards,
Carlos.

+++++++++++++++++++++++++++++++++++
> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    0.1            
year     2004           
month    11             
day      15             
language R              
> sex <- rep(c("F", "M"), 5)
> income <-  c(rep("low", 5), rep("high", 5))
> count <- 1:10
> mydf <- as.data.frame(cbind(sex, income, count))
> mydf$count = as.numeric(as.character(mydf$count))
> tapply(mydf$count, list(mydf$sex, mydf$income),
FUN=sum)
  high low
F   16   9
M   24   6
++++++++++++++++++++++++++++++++++++++++++++


 --- "Rau, Roland" <Rau at demogr.mpg.de> escribi?: 
> Dear all,
> 
> I wanted to make a two-way-table of two variables
> with a counting
> variable stored in another column of a dataframe. In
> version 1.9.1, the
> behavior is as expected as shown in the simplified
> example code.
> 
> > sex <- rep(c("F", "M"), 5)
> > income <-  c(rep("low", 5), rep("high", 5))
> > count <- 1:10
> > mydf <- as.data.frame(cbind(sex, income, count))
> > mydf$count = as.numeric(as.character(mydf$count))
> > tapply(mydf$count, list(mydf$sex, mydf$income),
> FUN=sum)
>   high low
> F   16   9
> M   24   6
> > version
>          _              
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    1              
> minor    9.1            
> year     2004           
> month    06             
> day      21             
> language R              
> > 
> 
> In version 2.0.1, however, I get the following
> output:
> 
> > sex <- rep(c("F", "M"), 5)
> > income <-  c(rep("low", 5), rep("high", 5))
> > count <- 1:10
> > mydf <- as.data.frame(cbind(sex, income, count))
> > mydf$count = as.numeric(as.character(mydf$count))
> > tapply(mydf$count, list(mydf$sex, mydf$income),
> FUN=sum)
> Error in get(x, envir, mode, inherits) : variable
> "FUN" was not found
> > version
>          _              
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    2              
> minor    0.1            
> year     2004           
> month    11             
> day      15             
> language R              
> > 
> 
> Was this change in behavior intended with the
> changes in tapply from
> R1.9.1 to R2.0.1?
> Is the R-help-list appropriate or rather R-Devel?
> 
> Thanks,
> Roland
> 
> 
> 
> +++++
> This mail has been sent through the MPI for
> Demographic Rese...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From combe at b3e.jussieu.fr  Thu Feb  3 20:19:23 2005
From: combe at b3e.jussieu.fr (combe@b3e.jussieu.fr)
Date: Thu,  3 Feb 2005 20:19:23 +0100
Subject: [R] help on "stack" function
Message-ID: <1107458363.4202793b0c1ad@webmail.b3e.jussieu.fr>

So in fact :

I have a data wich colnames are :
"id","VEMS5A","VEMS6A","VEMS7A","VEMS8A","VEMS9A","VEMS10A","VEMS11A","VEMS12A","VEMS13A","VEMS14A","VEMS15A","VEMS16A","VEMS17A","VEMS18A","cohort")

With "VEMS5A" is the value when the subject (id) was 5 year old,... "VEMS18A" is
the value when the subject(id) was 18 year old


I want to make a new data wich colnames will be :  "id","age", and "VEMS"
Where "age" will be the age when the value in "VEMS" was measured for the suject
"id"...

So I need to copy all the values of the columns "VEMS5A",... "VEMS18A" in "VEMS"
and in "id" the "id" corresponding and in "age" the values between 5 and 18
corresponding to the age of the subject when the mesure was done. And this only
when the value in "cohort" was 1.


-----Message d'origine-----
De : F Z [mailto:gerifalte28 at hotmail.com] 
Envoy? : jeudi 3 f?vrier 2005 19:38
? : combe at b3e.jussieu.fr; R-help at stat.math.ethz.ch
Objet : RE: [R] he^lp on "stack" function


In the future please be a little more specific with your questions. i.e. 
give us an example of your data  the way it looks now and the way you want 
it to look.

I think what you want is something like:

time<-seq(as.Date("2005/1/1"), as.Date("2005/1/10"), "days")
age<-round(runif(10,30,50))
measure<-rnorm(10)
mydat<-data.frame(time=time,age=age,measure=measure) #creates a data frame 
with your 3 #variables

         time age    measure
1  2005-01-01  48 -1.2873181
2  2005-01-02  50  0.5776001
3  2005-01-03  44  0.1892008
4  2005-01-04  38  0.4981651
5  2005-01-05  49 -1.7591745
6  2005-01-06  37 -0.8898464
7  2005-01-07  30  0.7528217
8  2005-01-08  46 -1.2421763
9  2005-01-09  39 -0.9333667
10 2005-01-10  32  1.2086509

#suppose you only want patients older than 40 Older40<-mydat[age>=40,c(1,3)]
        time    measure
1 2005-01-01 -1.2873181
2 2005-01-02  0.5776001
3 2005-01-03  0.1892008
5 2005-01-05 -1.7591745
8 2005-01-08 -1.2421763

Is this what you wanted to do?

Francisco


>From: "" <combe at b3e.jussieu.fr>
>To: "" <R-help at stat.math.ethz.ch>
>Subject: [R] he^lp on "stack" function
>Date: Thu,  3 Feb 2005 18:10:24 +0100
>
>
>
>
>Excuse me.
>I don't understant how to use the function "stack".
>I'm not sure to know the good syntax.
>
>I've got a data witch column names are the time when the mesures was 
>done
>and
>an
>other who indicates the age of the subjetcs.
>I would like to do a new matrix with patients older than a value and the 
>new
>matrix will contain in the first column the time of the mesures and in the
>second th evalue of the mesure.
>
>Could you help me please ?
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list 
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
>http://www.R-project.org/posting-guide.html



From dsonneborn at ucdavis.edu  Thu Feb  3 20:52:42 2005
From: dsonneborn at ucdavis.edu (Dean Sonneborn)
Date: Thu, 03 Feb 2005 11:52:42 -0800
Subject: [R] two issues
Message-ID: <6.1.0.6.2.20050203115211.032c3d90@yellow.ucdavis.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050203/293b8640/attachment.pl

From tghoward at gw.dec.state.ny.us  Thu Feb  3 20:57:58 2005
From: tghoward at gw.dec.state.ny.us (Tim Howard)
Date: Thu, 03 Feb 2005 14:57:58 -0500
Subject: [R] subset data.frame with value != in all columns
Message-ID: <s2023c17.034@gwsmtp.DEC.STATE.NY.US>

I am trying to extract rows from a data.frame based on the
presence/absence of a single value in any column.  I've figured out how
to do get the positive matches, but the remainder (rows without this
value) eludes me.  Mining the help pages and archives brought me,
frustratingly,  very close, as you'll see below. 

My goal: two data frames, one with -99 in at least one column in each
row, one with no occurrences of -99. I want to preserve rownames in
each.

My questions: 
Is there a cleaner way to extract all rows containing a specified
value?
How can I extract all rows that don't have this value in any col?

#create dummy dataset
x <- data.frame(
c1=c(-99,-99,-99,4:10),
c2=1:10,
c3=c(1:3,-99,5:10),
c4=c(10:1),
c5=c(1:9,-99))

#extract data.frame of rows with -99 in them
for(i in 1:ncol(x))
{
y<-subset(x, x[,i]==-99, drop=FALSE);
ifelse(i==1, z<-y, z <- rbind(z,y));
}

#various attempts to get rows not containing "-99":

# this attempt was to create, in "list", the exclusion formula for each
column.
# Here, I couldn't get subset to recognize "list" as the correct type.
# e.g. it works if I paste the value of list in the subset command
{
for(i in 1:ncol(x)){
if(i==1)
list<-paste("x[",i,"]!=-99", sep="")
else
list<-paste(list," ", " & x[",i,"]!=-99", sep="")
}
y<-subset(x, list, drop=FALSE);
}

# this will do it for one col, but if I index more
# it returns all rows
y <- x[!(x[,3] %in% -99),]

# this also works for one col
y<-x[x[,1]!=-99,]

# but if I index more, I get extra rows of NAs
y<-x[x[,1:5]!=-99,]

Thanks in advance.
Tim Howard

platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    0.1            
year     2004           
month    11             
day      15             
language R



From sundar.dorai-raj at pdf.com  Thu Feb  3 21:09:17 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 03 Feb 2005 14:09:17 -0600
Subject: [R] subset data.frame with value != in all columns
In-Reply-To: <s2023c17.034@gwsmtp.DEC.STATE.NY.US>
References: <s2023c17.034@gwsmtp.DEC.STATE.NY.US>
Message-ID: <420284ED.60407@pdf.com>



Tim Howard wrote:
> I am trying to extract rows from a data.frame based on the
> presence/absence of a single value in any column.  I've figured out how
> to do get the positive matches, but the remainder (rows without this
> value) eludes me.  Mining the help pages and archives brought me,
> frustratingly,  very close, as you'll see below. 
> 
> My goal: two data frames, one with -99 in at least one column in each
> row, one with no occurrences of -99. I want to preserve rownames in
> each.
> 
> My questions: 
> Is there a cleaner way to extract all rows containing a specified
> value?
> How can I extract all rows that don't have this value in any col?
> 
> #create dummy dataset
> x <- data.frame(
> c1=c(-99,-99,-99,4:10),
> c2=1:10,
> c3=c(1:3,-99,5:10),
> c4=c(10:1),
> c5=c(1:9,-99))
> 
<snip>

Tim,

How about using ?apply. Provided all your columns are numeric, you can do:

drop <- apply(x, 1, function(row) all(row != -99))
x[drop, ]

--sundar



From ccleland at optonline.net  Thu Feb  3 21:10:31 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Thu, 03 Feb 2005 15:10:31 -0500
Subject: [R] subset data.frame with value != in all columns
In-Reply-To: <s2023c17.034@gwsmtp.DEC.STATE.NY.US>
References: <s2023c17.034@gwsmtp.DEC.STATE.NY.US>
Message-ID: <42028537.60009@optonline.net>

How about this?

#extract data.frame of rows with -99 in them

subset(x, apply(x, 1, function(x){any(x == -99)}))

#extract data.frame of rows not containing -99 in them

subset(x, apply(x, 1, function(x){all(x != -99)}))

hope this helps,

Chuck Cleland

Tim Howard wrote:
> I am trying to extract rows from a data.frame based on the
> presence/absence of a single value in any column.  I've figured out how
> to do get the positive matches, but the remainder (rows without this
> value) eludes me.  Mining the help pages and archives brought me,
> frustratingly,  very close, as you'll see below. 
> 
> My goal: two data frames, one with -99 in at least one column in each
> row, one with no occurrences of -99. I want to preserve rownames in
> each.
> 
> My questions: 
> Is there a cleaner way to extract all rows containing a specified
> value?
> How can I extract all rows that don't have this value in any col?
> 
> #create dummy dataset
> x <- data.frame(
> c1=c(-99,-99,-99,4:10),
> c2=1:10,
> c3=c(1:3,-99,5:10),
> c4=c(10:1),
> c5=c(1:9,-99))
> 
> #extract data.frame of rows with -99 in them
> for(i in 1:ncol(x))
> {
> y<-subset(x, x[,i]==-99, drop=FALSE);
> ifelse(i==1, z<-y, z <- rbind(z,y));
> }
> 
> #various attempts to get rows not containing "-99":
> 
> # this attempt was to create, in "list", the exclusion formula for each
> column.
> # Here, I couldn't get subset to recognize "list" as the correct type.
> # e.g. it works if I paste the value of list in the subset command
> {
> for(i in 1:ncol(x)){
> if(i==1)
> list<-paste("x[",i,"]!=-99", sep="")
> else
> list<-paste(list," ", " & x[",i,"]!=-99", sep="")
> }
> y<-subset(x, list, drop=FALSE);
> }
> 
> # this will do it for one col, but if I index more
> # it returns all rows
> y <- x[!(x[,3] %in% -99),]
> 
> # this also works for one col
> y<-x[x[,1]!=-99,]
> 
> # but if I index more, I get extra rows of NAs
> y<-x[x[,1:5]!=-99,]
> 
> Thanks in advance.
> Tim Howard
> 
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    2              
> minor    0.1            
> year     2004           
> month    11             
> day      15             
> language R
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From deepayan at stat.wisc.edu  Thu Feb  3 21:14:21 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 3 Feb 2005 14:14:21 -0600
Subject: [R] two issues
In-Reply-To: <6.1.0.6.2.20050203115211.032c3d90@yellow.ucdavis.edu>
References: <6.1.0.6.2.20050203115211.032c3d90@yellow.ucdavis.edu>
Message-ID: <200502031414.21834.deepayan@stat.wisc.edu>

On Thursday 03 February 2005 13:52, Dean Sonneborn wrote:
> I'm working on a graphic but have run into a road block about two
> issues.  I don't have a color printer so I want to produce the
> graphic in black and white. I'm currently using this
> statement  trellis.device(bg="white")  

Doesn't that give you a warning? If not, you are using an older version 
of R (earlier than 2.0.0). In the latest version, ?trellis.device has a 
long rant on this issue.

> but the body of the graphic 
> contains color. What is the code to create the whole thing in black
> and white. 

If you want black and white, simply use 

trellis.device(color = FALSE)

> The other issue might be a bit more tricky. I'm using this 
> statement: auto.key=TRUE  and in the body of the graphic I'm getting
> two different symbols and on the screen I see two symbols in the key
> but when I print it the key only contains one symbol for both!

auto.key only works if you use the default plotting symbols, it doesn't 
know that you have specified pch=1:2. The easiest way to handle this 
would to replace 'pch=1:2' by 

'par.settings = list(superpose.symbol = list(pch = 1:2))'

(which may not work if your version of R is too old).

Deepayan

> here's the whole R code statement:
> dotplot(chemical~adjlogmle| convert*tdnew, data=allrisk ,
> group=rodentx, main="Interspecies Conversion", pch=1:2,
> auto.key=TRUE, scales = list(tick.number=10), fontfamily =
> "HersheySans")
>
> Thanks,
>
> Dean Sonneborn M.S.
> Public Health Sciences *
> University of California, Davis
> 916 734-6656
>
> * formerly Epidemiology & Preventive Medicine



From matthew_wiener at merck.com  Thu Feb  3 21:12:33 2005
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Thu, 3 Feb 2005 15:12:33 -0500
Subject: [R] two issues (black and white trellis graphics)
Message-ID: <45AAE6FD142DCB43A38C00A11FF5DF3E04993EF4@uswsmx03.merck.com>

On the first issue, there's a recent post you can find in the archives.
It's from Deepayan Sarkar on January 2 this year.  It would probably pop up
on a search for "black white lattice" or something similar.

The key part of his answer:

I'd do something like this as part of the initialization:

<<...>>
library(lattice)
ltheme <- canonical.theme(color = FALSE)     ## in-built B&W theme
ltheme$strip.background$col <- "transparent" ## change strip bg
lattice.options(default.theme = ltheme)      ## set as default

Hope this helps,

Matt Wiener


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dean Sonneborn
Sent: Thursday, February 03, 2005 2:53 PM
To: R-help at stat.math.ethz.ch
Subject: [R] two issues


I'm working on a graphic but have run into a road block about two 
issues.  I don't have a color printer so I want to produce the graphic in 
black and white. I'm currently using this 
statement  trellis.device(bg="white")  but the body of the graphic contains 
color. What is the code to create the whole thing in black and white. The 
other issue might be a bit more tricky. I'm using this statement: 
auto.key=TRUE  and in the body of the graphic I'm getting two different 
symbols and on the screen I see two symbols in the key but when I print it 
the key only contains one symbol for both!

here's the whole R code statement:
dotplot(chemical~adjlogmle| convert*tdnew, data=allrisk , group=rodentx, 
main="Interspecies Conversion", pch=1:2, auto.key=TRUE, scales = 
list(tick.number=10), fontfamily = "HersheySans")

Thanks,

Dean Sonneborn M.S.
Public Health Sciences *
University of California, Davis
916 734-6656

* formerly Epidemiology & Preventive Medicine

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From cliff at ms.washington.edu  Thu Feb  3 21:24:05 2005
From: cliff at ms.washington.edu (Cliff Lunneborg)
Date: Thu, 3 Feb 2005 12:24:05 -0800
Subject: Re; [R] nonparametric manova 
Message-ID: <014201c50a2e$4fb088f0$6401a8c0@C56909A>

Vera Hofer asks:

" Dear colleagues,

has anyone an idea how to carry out a nonparametric manova for comparing
3 groups?
Thank you for your help.

Vera"

One approach would be to carry out a permutation test, randomly
reassigning the vector observations among groups under the null
hypothesis that the three multivariate distributions sampled are
identical. The 'work' could be carried out using the boot function (in
package boot) but would require writing a not very involved function to
compute the manova test statistic.


**********************************************************
Cliff Lunneborg, Professor Emeritus, Statistics &
Psychology, University of Washington, Seattle
cliff at ms.washington.edu



From tghoward at gw.dec.state.ny.us  Thu Feb  3 21:19:55 2005
From: tghoward at gw.dec.state.ny.us (Tim Howard)
Date: Thu, 03 Feb 2005 15:19:55 -0500
Subject: [R] subset data.frame with value != in all columns
Message-ID: <s2024132.055@gwsmtp.DEC.STATE.NY.US>

apply, of course, does the trick exceptionally well. Thank you,
everyone, for the help.

tim

>>> Chuck Cleland <ccleland at optonline.net> 02/03/05 03:10PM >>>
How about this?

#extract data.frame of rows with -99 in them

subset(x, apply(x, 1, function(x){any(x == -99)}))

#extract data.frame of rows not containing -99 in them

subset(x, apply(x, 1, function(x){all(x != -99)}))

hope this helps,

Chuck Cleland

Tim Howard wrote:
> I am trying to extract rows from a data.frame based on the
> presence/absence of a single value in any column.  I've figured out
how
> to do get the positive matches, but the remainder (rows without this
> value) eludes me.  Mining the help pages and archives brought me,
> frustratingly,  very close, as you'll see below. 
> 
> My goal: two data frames, one with -99 in at least one column in
each
> row, one with no occurrences of -99. I want to preserve rownames in
> each.
> 
> My questions: 
> Is there a cleaner way to extract all rows containing a specified
> value?
> How can I extract all rows that don't have this value in any col?
> 
> #create dummy dataset
> x <- data.frame(
> c1=c(-99,-99,-99,4:10),
> c2=1:10,
> c3=c(1:3,-99,5:10),
> c4=c(10:1),
> c5=c(1:9,-99))
> 
> #extract data.frame of rows with -99 in them
> for(i in 1:ncol(x))
> {
> y<-subset(x, x[,i]==-99, drop=FALSE);
> ifelse(i==1, z<-y, z <- rbind(z,y));
> }
> 
> #various attempts to get rows not containing "-99":
> 
> # this attempt was to create, in "list", the exclusion formula for
each
> column.
> # Here, I couldn't get subset to recognize "list" as the correct
type.
> # e.g. it works if I paste the value of list in the subset command
> {
> for(i in 1:ncol(x)){
> if(i==1)
> list<-paste("x[",i,"]!=-99", sep="")
> else
> list<-paste(list," ", " & x[",i,"]!=-99", sep="")
> }
> y<-subset(x, list, drop=FALSE);
> }
> 
> # this will do it for one col, but if I index more
> # it returns all rows
> y <- x[!(x[,3] %in% -99),]
> 
> # this also works for one col
> y<-x[x[,1]!=-99,]
> 
> # but if I index more, I get extra rows of NAs
> y<-x[x[,1:5]!=-99,]
> 
> Thanks in advance.
> Tim Howard
> 
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    2              
> minor    0.1            
> year     2004           
> month    11             
> day      15             
> language R
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html 
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From ggrothendieck at myway.com  Thu Feb  3 21:12:40 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 3 Feb 2005 20:12:40 +0000 (UTC)
Subject: [R] help on "stack" function
References: <1107458363.4202793b0c1ad@webmail.b3e.jussieu.fr>
Message-ID: <loom.20050203T211142-129@post.gmane.org>


Check out ?reshape . 

 <combe <at> b3e.jussieu.fr> writes:

: 
: So in fact :
: 
: I have a data wich colnames are :
: "id","VEMS5A","VEMS6A","VEMS7A","VEMS8A","VEMS9A","VEMS10A","VEMS11A","VEMS12
A","VEMS13A","VEMS14A","VEMS15A","VEMS16A","VEMS17A","VEMS18A","cohort")
: 
: With "VEMS5A" is the value when the subject (id) was 5 year 
old,... "VEMS18A" is
: the value when the subject(id) was 18 year old
: 
: I want to make a new data wich colnames will be :  "id","age", and "VEMS"
: Where "age" will be the age when the value in "VEMS" was measured for the 
suject
: "id"...
: 
: So I need to copy all the values of the columns "VEMS5A",... "VEMS18A" 
in "VEMS"
: and in "id" the "id" corresponding and in "age" the values between 5 and 18
: corresponding to the age of the subject when the mesure was done. And this 
only
: when the value in "cohort" was 1.
: 
: -----Message d'origine-----
: De : F Z [mailto:gerifalte28 <at> hotmail.com] 
: Envoy : jeudi 3 fvrier 2005 19:38
:  : combe <at> b3e.jussieu.fr; R-help <at> stat.math.ethz.ch
: Objet : RE: [R] he^lp on "stack" function
: 
: In the future please be a little more specific with your questions. i.e. 
: give us an example of your data  the way it looks now and the way you want 
: it to look.
: 
: I think what you want is something like:
: 
: time<-seq(as.Date("2005/1/1"), as.Date("2005/1/10"), "days")
: age<-round(runif(10,30,50))
: measure<-rnorm(10)
: mydat<-data.frame(time=time,age=age,measure=measure) #creates a data frame 
: with your 3 #variables
: 
:          time age    measure
: 1  2005-01-01  48 -1.2873181
: 2  2005-01-02  50  0.5776001
: 3  2005-01-03  44  0.1892008
: 4  2005-01-04  38  0.4981651
: 5  2005-01-05  49 -1.7591745
: 6  2005-01-06  37 -0.8898464
: 7  2005-01-07  30  0.7528217
: 8  2005-01-08  46 -1.2421763
: 9  2005-01-09  39 -0.9333667
: 10 2005-01-10  32  1.2086509
: 
: #suppose you only want patients older than 40 Older40<-mydat[age>=40,c(1,3)]
:         time    measure
: 1 2005-01-01 -1.2873181
: 2 2005-01-02  0.5776001
: 3 2005-01-03  0.1892008
: 5 2005-01-05 -1.7591745
: 8 2005-01-08 -1.2421763
: 
: Is this what you wanted to do?
: 
: Francisco
: 
: >From: "" <combe <at> b3e.jussieu.fr>
: >To: "" <R-help <at> stat.math.ethz.ch>
: >Subject: [R] he^lp on "stack" function
: >Date: Thu,  3 Feb 2005 18:10:24 +0100
: >
: >
: >
: >
: >Excuse me.
: >I don't understant how to use the function "stack".
: >I'm not sure to know the good syntax.
: >
: >I've got a data witch column names are the time when the mesures was 
: >done
: >and
: >an
: >other who indicates the age of the subjetcs.
: >I would like to do a new matrix with patients older than a value and the 
: >new
: >matrix will contain in the first column the time of the mesures and in the
: >second th evalue of the mesure.
: >
: >Could you help me please ?
: >
: >______________________________________________
: >R-help <at> stat.math.ethz.ch mailing list 
: >https://stat.ethz.ch/mailman/listinfo/r-help
: >PLEASE do read the posting guide!
: >http://www.R-project.org/posting-guide.html
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://stat.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
: 
:



From ggrothendieck at myway.com  Thu Feb  3 21:07:43 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 3 Feb 2005 20:07:43 +0000 (UTC)
Subject: [R] Surprising Behavior of 'tapply'
References: <8B08A3A1EA7AAC41BE24C750338754E61506FA@HERMES.demogr.mpg.de>
	<20050203185606.66918.qmail@web25204.mail.ukl.yahoo.com>
Message-ID: <loom.20050203T205740-623@post.gmane.org>


I tried it on Windows XP with R 2.1.0 and could not replicate it either.
Suggest you start up a fresh session and try it again.

By the way, you could consider this:

xtabs(count ~., data.frame(sex = sex, income = income))



Carlos Ortega <carlos_ortegafernandez <at> yahoo.es> writes:

: 
: Hi,
: 
: That is something strange, I could not replicate it...
: 
: Regards,
: Carlos.
: 
: +++++++++++++++++++++++++++++++++++
: > version
:          _              
: platform i386-pc-mingw32
: arch     i386           
: os       mingw32        
: system   i386, mingw32  
: status                  
: major    2              
: minor    0.1            
: year     2004           
: month    11             
: day      15             
: language R              
: > sex <- rep(c("F", "M"), 5)
: > income <-  c(rep("low", 5), rep("high", 5))
: > count <- 1:10
: > mydf <- as.data.frame(cbind(sex, income, count))
: > mydf$count = as.numeric(as.character(mydf$count))
: > tapply(mydf$count, list(mydf$sex, mydf$income),
: FUN=sum)
:   high low
: F   16   9
: M   24   6
: ++++++++++++++++++++++++++++++++++++++++++++
: 
:  --- "Rau, Roland" <Rau <at> demogr.mpg.de> escribi?: 
: > Dear all,
: > 
: > I wanted to make a two-way-table of two variables
: > with a counting
: > variable stored in another column of a dataframe. In
: > version 1.9.1, the
: > behavior is as expected as shown in the simplified
: > example code.
: > 
: > > sex <- rep(c("F", "M"), 5)
: > > income <-  c(rep("low", 5), rep("high", 5))
: > > count <- 1:10
: > > mydf <- as.data.frame(cbind(sex, income, count))
: > > mydf$count = as.numeric(as.character(mydf$count))
: > > tapply(mydf$count, list(mydf$sex, mydf$income),
: > FUN=sum)
: >   high low
: > F   16   9
: > M   24   6
: > > version
: >          _              
: > platform i386-pc-mingw32
: > arch     i386           
: > os       mingw32        
: > system   i386, mingw32  
: > status                  
: > major    1              
: > minor    9.1            
: > year     2004           
: > month    06             
: > day      21             
: > language R              
: > > 
: > 
: > In version 2.0.1, however, I get the following
: > output:
: > 
: > > sex <- rep(c("F", "M"), 5)
: > > income <-  c(rep("low", 5), rep("high", 5))
: > > count <- 1:10
: > > mydf <- as.data.frame(cbind(sex, income, count))
: > > mydf$count = as.numeric(as.character(mydf$count))
: > > tapply(mydf$count, list(mydf$sex, mydf$income),
: > FUN=sum)
: > Error in get(x, envir, mode, inherits) : variable
: > "FUN" was not found
: > > version
: >          _              
: > platform i386-pc-mingw32
: > arch     i386           
: > os       mingw32        
: > system   i386, mingw32  
: > status                  
: > major    2              
: > minor    0.1            
: > year     2004           
: > month    11             
: > day      15             
: > language R              
: > > 
: > 
: > Was this change in behavior intended with the
: > changes in tapply from
: > R1.9.1 to R2.0.1?
: > Is the R-help-list appropriate or rather R-Devel?
: > 
: > Thanks,
: > Roland
: > 
: > 
: > 
: > +++++
: > This mail has been sent through the MPI for
: > Demographic Rese...{{dropped}}
: > 
: > ______________________________________________
: > R-help <at> stat.math.ethz.ch mailing list
: > https://stat.ethz.ch/mailman/listinfo/r-help
: > PLEASE do read the posting guide!
: > http://www.R-project.org/posting-guide.html
: >
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://stat.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
: 
:



From ecklundcj at gmail.com  Thu Feb  3 21:20:01 2005
From: ecklundcj at gmail.com (Christofer Ecklund)
Date: Thu, 3 Feb 2005 10:20:01 -1000
Subject: [R] Uninstalling R from Mac OS X
Message-ID: <3d2917b9622b29217e404fea0000e4e3@gmail.com>

Hello,
Could you tell me how to uninstall R from Mac OS X (10.3)?  I am not 
too familiar with terminal or the command line so a GUI removal would 
be better, however I would like to remove every trace of the program 
from my system.  Usually with a package installer there is some sort of 
uninstaller included, but I could not find it.  I have read the 
documentation but could not identify a GUI uninstaller.  Please let me 
know.

Thank you,
Chris



From MSchwartz at MedAnalytics.com  Thu Feb  3 21:24:18 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 03 Feb 2005 14:24:18 -0600
Subject: [R] subset data.frame with value != in all columns
In-Reply-To: <s2023c17.034@gwsmtp.DEC.STATE.NY.US>
References: <s2023c17.034@gwsmtp.DEC.STATE.NY.US>
Message-ID: <1107462259.25711.3.camel@horizons.localdomain>

On Thu, 2005-02-03 at 14:57 -0500, Tim Howard wrote: 
> I am trying to extract rows from a data.frame based on the
> presence/absence of a single value in any column.  I've figured out how
> to do get the positive matches, but the remainder (rows without this
> value) eludes me.  Mining the help pages and archives brought me,
> frustratingly,  very close, as you'll see below. 
> 
> My goal: two data frames, one with -99 in at least one column in each
> row, one with no occurrences of -99. I want to preserve rownames in
> each.
> 
> My questions: 
> Is there a cleaner way to extract all rows containing a specified
> value?
> How can I extract all rows that don't have this value in any col?
> 
> #create dummy dataset
> x <- data.frame(
> c1=c(-99,-99,-99,4:10),
> c2=1:10,
> c3=c(1:3,-99,5:10),
> c4=c(10:1),
> c5=c(1:9,-99))
> 
> #extract data.frame of rows with -99 in them
> for(i in 1:ncol(x))
> {
> y<-subset(x, x[,i]==-99, drop=FALSE);
> ifelse(i==1, z<-y, z <- rbind(z,y));
> }
> 
> #various attempts to get rows not containing "-99":
> 
> # this attempt was to create, in "list", the exclusion formula for each
> column.
> # Here, I couldn't get subset to recognize "list" as the correct type.
> # e.g. it works if I paste the value of list in the subset command
> {
> for(i in 1:ncol(x)){
> if(i==1)
> list<-paste("x[",i,"]!=-99", sep="")
> else
> list<-paste(list," ", " & x[",i,"]!=-99", sep="")
> }
> y<-subset(x, list, drop=FALSE);
> }
> 
> # this will do it for one col, but if I index more
> # it returns all rows
> y <- x[!(x[,3] %in% -99),]
> 
> # this also works for one col
> y<-x[x[,1]!=-99,]
> 
> # but if I index more, I get extra rows of NAs
> y<-x[x[,1:5]!=-99,]
> 
> Thanks in advance.
> Tim Howard


How about this, presuming that your data frame is all numeric:

For rows containing -99:

> x[unique(which(x == -99, arr.ind = TRUE)[, 1]), ]
    c1 c2  c3 c4  c5
1  -99  1   1 10   1
2  -99  2   2  9   2
3  -99  3   3  8   3
4    4  4 -99  7   4
10  10 10  10  1 -99


For rows not containing -99:

> x[-unique(which(x == -99, arr.ind = TRUE)[, 1]), ]
  c1 c2 c3 c4 c5
5  5  5  5  6  5
6  6  6  6  5  6
7  7  7  7  4  7
8  8  8  8  3  8
9  9  9  9  2  9


What I have done here is to use which(), setting arr.ind = TRUE. This
returns the row, column indices for the matches to the boolean
statement. The first column returned by which() in this case are the row
numbers matching the statement, so I take the first column only.

Since it is possible that more than one element in a row can match the
boolean, I then use unique() to get the singular row values.

Thus, I can use the returned row indices above to subset the data frame.

HTH,

Marc Schwartz



From Achim.Zeileis at wu-wien.ac.at  Thu Feb  3 21:34:21 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 3 Feb 2005 21:34:21 +0100
Subject: [R] subset data.frame with value != in all columns
In-Reply-To: <s2023c17.034@gwsmtp.DEC.STATE.NY.US>
References: <s2023c17.034@gwsmtp.DEC.STATE.NY.US>
Message-ID: <20050203213421.584bc555.Achim.Zeileis@wu-wien.ac.at>

On Thu, 03 Feb 2005 14:57:58 -0500 Tim Howard wrote:

> I am trying to extract rows from a data.frame based on the
> presence/absence of a single value in any column.  I've figured out
> how to do get the positive matches, but the remainder (rows without
> this value) eludes me.  Mining the help pages and archives brought me,
> frustratingly,  very close, as you'll see below. 
> 
> My goal: two data frames, one with -99 in at least one column in each
> row, one with no occurrences of -99. I want to preserve rownames in
> each.
> 
> My questions: 
> Is there a cleaner way to extract all rows containing a specified
> value?
> How can I extract all rows that don't have this value in any col?
> 
> #create dummy dataset
> x <- data.frame(
> c1=c(-99,-99,-99,4:10),
> c2=1:10,
> c3=c(1:3,-99,5:10),
> c4=c(10:1),
> c5=c(1:9,-99))

Assuming that there are no NAs (in fact, the -99 looks like it
might be coding NAs...), the following works:
R> is.na(x) <- x == -99
R> na.omit(x)
  c1 c2 c3 c4 c5
5  5  5  5  6  5
6  6  6  6  5  6
7  7  7  7  4  7
8  8  8  8  3  8
9  9  9  9  2  9

hth,
Z

> #extract data.frame of rows with -99 in them
> for(i in 1:ncol(x))
> {
> y<-subset(x, x[,i]==-99, drop=FALSE);
> ifelse(i==1, z<-y, z <- rbind(z,y));
> }
> 
> #various attempts to get rows not containing "-99":
> 
> # this attempt was to create, in "list", the exclusion formula for
> # each
> column.
> # Here, I couldn't get subset to recognize "list" as the correct type.
> # e.g. it works if I paste the value of list in the subset command
> {
> for(i in 1:ncol(x)){
> if(i==1)
> list<-paste("x[",i,"]!=-99", sep="")
> else
> list<-paste(list," ", " & x[",i,"]!=-99", sep="")
> }
> y<-subset(x, list, drop=FALSE);
> }
> 
> # this will do it for one col, but if I index more
> # it returns all rows
> y <- x[!(x[,3] %in% -99),]
> 
> # this also works for one col
> y<-x[x[,1]!=-99,]
> 
> # but if I index more, I get extra rows of NAs
> y<-x[x[,1:5]!=-99,]
> 
> Thanks in advance.
> Tim Howard
> 
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    2              
> minor    0.1            
> year     2004           
> month    11             
> day      15             
> language R
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From ggrothendieck at myway.com  Thu Feb  3 21:46:02 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 3 Feb 2005 20:46:02 +0000 (UTC)
Subject: [R] subset data.frame with value != in all columns
References: <s2024132.055@gwsmtp.DEC.STATE.NY.US>
Message-ID: <loom.20050203T214336-874@post.gmane.org>


Do the -99 entries really mean NA?  In that case, I think it
would be clearer to recode your data frame with NAs and then select
out the complete or incomplete rows:

x[x == -99] <- NA

x[compete.cases(x),]   # or na.omit(x)
x[!complete.cases(x),]


Tim Howard <tghoward <at> gw.dec.state.ny.us> writes:

: 
: apply, of course, does the trick exceptionally well. Thank you,
: everyone, for the help.
: 
: tim
: 
: >>> Chuck Cleland <ccleland <at> optonline.net> 02/03/05 03:10PM >>>
: How about this?
: 
: #extract data.frame of rows with -99 in them
: 
: subset(x, apply(x, 1, function(x){any(x == -99)}))
: 
: #extract data.frame of rows not containing -99 in them
: 
: subset(x, apply(x, 1, function(x){all(x != -99)}))
: 
: hope this helps,
: 
: Chuck Cleland
: 
: Tim Howard wrote:
: > I am trying to extract rows from a data.frame based on the
: > presence/absence of a single value in any column.  I've figured out
: how
: > to do get the positive matches, but the remainder (rows without this
: > value) eludes me.  Mining the help pages and archives brought me,
: > frustratingly,  very close, as you'll see below. 
: > 
: > My goal: two data frames, one with -99 in at least one column in
: each
: > row, one with no occurrences of -99. I want to preserve rownames in
: > each.
: > 
: > My questions: 
: > Is there a cleaner way to extract all rows containing a specified
: > value?
: > How can I extract all rows that don't have this value in any col?
: > 
: > #create dummy dataset
: > x <- data.frame(
: > c1=c(-99,-99,-99,4:10),
: > c2=1:10,
: > c3=c(1:3,-99,5:10),
: > c4=c(10:1),
: > c5=c(1:9,-99))
: > 
: > #extract data.frame of rows with -99 in them
: > for(i in 1:ncol(x))
: > {
: > y<-subset(x, x[,i]==-99, drop=FALSE);
: > ifelse(i==1, z<-y, z <- rbind(z,y));
: > }
: > 
: > #various attempts to get rows not containing "-99":
: > 
: > # this attempt was to create, in "list", the exclusion formula for
: each
: > column.
: > # Here, I couldn't get subset to recognize "list" as the correct
: type.
: > # e.g. it works if I paste the value of list in the subset command
: > {
: > for(i in 1:ncol(x)){
: > if(i==1)
: > list<-paste("x[",i,"]!=-99", sep="")
: > else
: > list<-paste(list," ", " & x[",i,"]!=-99", sep="")
: > }
: > y<-subset(x, list, drop=FALSE);
: > }
: > 
: > # this will do it for one col, but if I index more
: > # it returns all rows
: > y <- x[!(x[,3] %in% -99),]
: > 
: > # this also works for one col
: > y<-x[x[,1]!=-99,]
: > 
: > # but if I index more, I get extra rows of NAs
: > y<-x[x[,1:5]!=-99,]
: > 
: > Thanks in advance.
: > Tim Howard
: > 
: > platform i386-pc-mingw32
: > arch     i386           
: > os       mingw32        
: > system   i386, mingw32  
: > status                  
: > major    2              
: > minor    0.1            
: > year     2004           
: > month    11             
: > day      15             
: > language R
: > 
: > ______________________________________________
: > R-help <at> stat.math.ethz.ch mailing list
: > https://stat.ethz.ch/mailman/listinfo/r-help 
: > PLEASE do read the posting guide!
: http://www.R-project.org/posting-guide.html 
: > 
:



From p.dalgaard at biostat.ku.dk  Thu Feb  3 22:21:43 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 03 Feb 2005 22:21:43 +0100
Subject: [R] subset data.frame with value != in all columns
In-Reply-To: <s2023c17.034@gwsmtp.DEC.STATE.NY.US>
References: <s2023c17.034@gwsmtp.DEC.STATE.NY.US>
Message-ID: <x2d5vhuyoo.fsf@biostat.ku.dk>

"Tim Howard" <tghoward at gw.dec.state.ny.us> writes:

> I am trying to extract rows from a data.frame based on the
> presence/absence of a single value in any column.  I've figured out how
> to do get the positive matches, but the remainder (rows without this
> value) eludes me.  Mining the help pages and archives brought me,
> frustratingly,  very close, as you'll see below. 
> 
> My goal: two data frames, one with -99 in at least one column in each
> row, one with no occurrences of -99. I want to preserve rownames in
> each.
> 
> My questions: 
> Is there a cleaner way to extract all rows containing a specified
> value?
> How can I extract all rows that don't have this value in any col?
> 
> #create dummy dataset
> x <- data.frame(
> c1=c(-99,-99,-99,4:10),
> c2=1:10,
> c3=c(1:3,-99,5:10),
> c4=c(10:1),
> c5=c(1:9,-99))

> subset(x,apply(x==-99,1,any))
    c1 c2  c3 c4  c5
1  -99  1   1 10   1
2  -99  2   2  9   2
3  -99  3   3  8   3
4    4  4 -99  7   4
10  10 10  10  1 -99
> subset(x,!apply(x==-99,1,any))
  c1 c2 c3 c4 c5
5  5  5  5  6  5
6  6  6  6  5  6
7  7  7  7  4  7
8  8  8  8  3  8
9  9  9  9  2  9


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Woodall.George at epamail.epa.gov  Thu Feb  3 22:23:45 2005
From: Woodall.George at epamail.epa.gov (Woodall.George@epamail.epa.gov)
Date: Thu, 03 Feb 2005 16:23:45 -0500
Subject: [R] Is error hardware or workstation specific
Message-ID: <OF47706255.3FAE98DB-ON85256F9D.00748C1B-85256F9D.0075881D@epamail.epa.gov>

I am working on a Dell Latitude D800 laptop computer with an Intel
processor running Windows XP Professional.  The problem I describe below
has occurred while running R versions 1.9.1 and 2.0.1.  The error occurs
while using a program to perform categorical regression that was
originally written in S-plus but was recently converted to R.  Using the
same data set which gives me the error, one of my colleagues who is
running the same set-up, but on a Dell Desktop computer does not get the
error.  The error I am getting is listed below.

Error in seq(length(missing.row)) : Object "missing.row" not found

Any suggestions?

George Woodall



From ggrothendieck at myway.com  Thu Feb  3 22:29:45 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 3 Feb 2005 21:29:45 +0000 (UTC)
Subject: [R] Efficient selection and alteration of dataframe records
References: <42025474.1030508@columbia.edu>
Message-ID: <loom.20050203T222731-732@post.gmane.org>


I did not attempt to follow your code or discussion but you could
try these:

1. try to pin down what part of your code is taking the time
2. try to eliminate the loop, if possible
3. use matrices rather than data frames -- matrices are faster


Daniel E. Bunker <deb37 <at> columbia.edu> writes:

: 
: Hi All,
: 
: I am writing a simulation that examines the effects of species 
: extinctions on ecological communties by sequentially removing 
: individuals of a given species (sometimes using weighted probabilities) 
: and replacing the lost individuals with species identities randomly 
: sampled from the remaining individuals. Thus I  use two dataframes. One 
: contains all the individuals and their species identities (plotdf).  The 
: other contains each species and their associated weights (traitdf).
: 
: While I have code that works, it runs slowly.  I suspect there is a more 
: efficient way.
: 
: First, I 'sample' one species from the species file (traitdf), then I 
: use that result to 'subset' the individuals dataframe (plotdf) into two 
: new files: individuals of the extincted species (plotdf.del) and 
: retained individuals (plotdf.old). 
: 
: I then use a 'for' loop to run through each record in plotdf.del and 
: randomly sample a new species identity from plotdf.old.  (Note that I 
: also need one species specific variable from the traitdf dataframe, 
: which I have been attaching using 'merge.') When all are replaced, I 
: simply 'rbind' plotdf.old and plotdf.del back together.  I then delete 
: another species, etc, etc.
: 
: My guess is that there is a way to replace the lost individuals using a 
: 'sample' that simply excludes the lost individuals (records).  This 
: would avoid splitting the data frame and 'rbind'ing it back together.  
: If I could also inlcude a second variable from the 'sample'd records, 
: this would eliminate the need for the 'merge'.
: 
: I am running R2.0.0 on windows 2000. 
: 
: Simplified code is below.
: 
: Any suggestions would be greatly appreciated.
: 
: Thanks for your time, Dan
: 
: plotdf=data.frame(
:     tag=1:100,
: 
: pspp=c(rep("Sp1",40),rep("Sp2",30),rep("Sp3",20),rep("Sp4",5),rep("Sp5",5)),
:     dim=runif(100)*100)
: plotdf[1,]
: 
: abun.table=as.data.frame(table(plotdf$pspp))
: 
: #2.1 calculate Smax (count of species)
: Smax=length(abun.table$Freq[abun.table$Freq>0])
: Smax
: 
: traitdf=data.frame(
:     tspp=c("Sp1","Sp2","Sp3","Sp4","Sp5"),
:     width=runif(5),
:     abun=abun.table$Freq)
: traitdf[1,]
: 
: rm(abun.table)
: 
: #3. merge plotdf and traitdf
: plotdft=merge(plotdf, traitdf, by.x ="pspp", by.y="tspp")
: 
: #4 define summary dataframe sumdf
: sumdf=data.frame(s.n=NA, s.S=NA, s.crop=NA)
: 
:     #reset all data to raw data.
:     #b. calculate crop in plotdft with all species present
:     plotdft$crop=plotdft$width*exp(-2.0+2.42*(log(plotdft$dim)))
:     #c. sum crop
:     sumcrop=sum(plotdft$crop)
:     #d. write n, S, crop to sumdf
:     sumdflength=length(sumdf$s.n)       
:     sumdf[sumdflength+1,1]=1;
:     sumdf[sumdflength+1,2]=Smax;
:     sumdf[sumdflength+1,3]=sumcrop;
: 
:     #6. SPECIES DELETION LOOP. This is the species deletion loop.
:     #a. repeat from n=1:Smax-1 (S=Smax-n+1)
:     for(n in 1:(Smax-1)) {
:         S=Smax-n+1;
: 
:         #b. remove and replace one species
:         #1. sample one species based on weight (e.g., abundance)
:         #delsp = sample(traitdf$tspp, size=1);delsp
:         delsp = sample(traitdf$tspp, size=1, prob=traitdf[,3]);
: 
:         #2. select traitdf records that match delsp
:         traitdf.del = subset(traitdf, tspp==delsp);traitdf.del[1,]
: 
:         #3. and delete that species from trait data
:         traitdf = subset(traitdf, tspp!=delsp[1]);
: 
:         #4. split that species from plot data into new df
:         plotdf.old = subset(plotdf, plotdf$pspp!=delsp);plotdf.old[1,]
:         plotdf.del = subset(plotdf, plotdf$pspp==delsp);plotdf.del[1,]
: 
:             #5. replace delsp params with params randomly selected from 
: remaining spp:
:             for (x in 1:length(plotdf.del$pspp)){
:                 newsp = sample(plotdf.old$pspp, size=1);#print(newsp[1])
:                 plotdf.del$pspp[x]=newsp[1]
:             }
:             #6. rbind plotdf and splitdf into plotdf,
:             plotdf=rbind(plotdf.old,plotdf.del);plotdf[1,]
: 
:     #b. calculate standing crop,etc
:         #1. merge plotdf and traitdf
:         plotdft=merge(plotdf, traitdf, by.x ="pspp", by.y="tspp")
: 
:         #2. calculate crop in plotdft
:         plotdft$crop=plotdft$width*exp(-2.0+2.42*log(plotdft$dim))
: 
:         #3. sum crop
:         sumcrop=sum(plotdft$crop)
: 
:         #4. calculate S
:         abun.table=as.data.frame(table(plotdf$pspp))
:         S=length(abun.table$Freq[abun.table$Freq>0])
: 
:     #c. write  n, S, crop to sumdf
:         sumdflength=length(sumdf$s.n)
:         sumdf[sumdflength+1,1]=n+1;
:         sumdf[sumdflength+1,2]=S;
:         sumdf[sumdflength+1,3]=sumcrop;   
:     }#d. REPEAT SPECIES DELETION LOOP
:     #housekeeping
:     rm(delsp, plotdf, plotdf.del, plotdf.old, plotdft, traitdf.del)
:     gc()
: 
: #8. plot results, fit line
: print(sumdf)
: traitdf
: 
: plot(sumdf$s.S, sumdf$s.crop)
:



From danlipsitt at gmail.com  Thu Feb  3 23:51:51 2005
From: danlipsitt at gmail.com (Dan Lipsitt)
Date: Thu, 3 Feb 2005 17:51:51 -0500
Subject: [R] filling a string buffer in a C routine
Message-ID: <b3a7efa9050203145128c07f0c@mail.gmail.com>

I am trying to write a C function that reads lines from a file and
passes them back to R.

Here is a simplified version:

--- C code ---

#include <R.h>
void read_file(char **filename, char **buf, char **buflen) {
  FILE *infile;

  infile = fopen(*filename, "r");
  fgets(*buf, *buflen, infile);
  fclose(infile);
}

--- R code ---

buffer = "xxxxxxxxxx"            # kludge!

read.file <- function(filename) 
  .C("read_climate",
     as.character(filename),
     buf=buffer,
     buflen=as.integer(10))$buf

--- end code ---

This works okay, but the only way I could find to allocate a string
buffer of the size I want is to use a string literal as in the line
marked "kludge" above. It is impractical for large buffers, not to
mention uuuggly. I tried

 buffer = paste(rep('x', 10), sep="")

but it doesn't work. So my question is, how do I do one of the following:

- Allocate a character buffer of a desired size to pass to my C routine.
- Have the C routine allocate the buffer without causing a memory leak.
- Use .Call() instead

Thanks,
Dan



From yc176 at yahoo.com  Thu Feb  3 23:59:33 2005
From: yc176 at yahoo.com (Yong Chao)
Date: Thu, 3 Feb 2005 14:59:33 -0800 (PST)
Subject: [R] normality test on truncated data
Message-ID: <20050203225933.35608.qmail@web51308.mail.yahoo.com>

I tried to use shapiro.test or ks.test to check the
normality of some data, the problem is, the
distribution function is a mixture of a Gaussian and
some other distributions at the tails. The hypothesis
is that if the tails are excluded, the distribution is
perfect Gaussian, and I want to test that.

But I cannot simply cut the tails off and do a
normality test on the truncated data, as shown in the
following example, this will fail.


So that question is: how can I test whether the middle
chunk of the distribution is Gaussian?

Thanks!

Yong

> r<-rnorm(1000)
> r.trunc<-r[which(abs(r)<1.5)]
> shapiro.test(r.trunc)

        Shapiro-Wilk normality test

data:  r.trunc 
W = 0.9855, p-value = 1.237e-07

> ks.test(r.trunc, "pnorm")

        One-sample Kolmogorov-Smirnov test

data:  r.trunc 
D = 0.0873, p-value = 3.116e-06
alternative hypothesis: two.sided 

>



From sfalcon at fhcrc.org  Fri Feb  4 00:18:40 2005
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Thu, 3 Feb 2005 15:18:40 -0800
Subject: [R] installing R on Mac OS X
In-Reply-To: <b932ca9de8e5c5e328f62ae218df9854@ing.unitn.it>
References: <b932ca9de8e5c5e328f62ae218df9854@ing.unitn.it>
Message-ID: <20050203231840.GM8908@gerbil.fhcrc.org>

On Mon, Jan 31, 2005 at 07:52:26PM +0100, silvia simoni wrote:
> I have a problem in installing R-2.0.1, downloaded from the R web site, 
> on mac o sx version 10.3.7.
> when i launch the command ./configure i get the following error message:

Did you read the OS X FAQ on www.r-project.org?  There is a special way
you must call configure for this platform.

+ seth



From spencer.graves at pdf.com  Fri Feb  4 00:29:54 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 03 Feb 2005 15:29:54 -0800
Subject: [R] normality test on truncated data
In-Reply-To: <20050203225933.35608.qmail@web51308.mail.yahoo.com>
References: <20050203225933.35608.qmail@web51308.mail.yahoo.com>
Message-ID: <4202B3F2.9090706@pdf.com>

          Have you considered Monte Carlo? 

      Invent a test, then Monte Carlo it to get a p-value. 

      Something similar to what you are asking is provided by Monte 
Carlo confidence bounds on a QQ plot, discussed on this list last June.  
To find it, go to www.r-project.org -> Search -> "R site search" -> 
"confidence bounds on QQ plot".  I got 9 hits from this just now.  The 
first was an answer to a question I asked then on this issue, giving two 
references. 

      hope this helps. 
      spencer graves

Yong Chao wrote:

>I tried to use shapiro.test or ks.test to check the
>normality of some data, the problem is, the
>distribution function is a mixture of a Gaussian and
>some other distributions at the tails. The hypothesis
>is that if the tails are excluded, the distribution is
>perfect Gaussian, and I want to test that.
>
>But I cannot simply cut the tails off and do a
>normality test on the truncated data, as shown in the
>following example, this will fail.
>
>
>So that question is: how can I test whether the middle
>chunk of the distribution is Gaussian?
>
>Thanks!
>
>Yong
>
>  
>
>>r<-rnorm(1000)
>>r.trunc<-r[which(abs(r)<1.5)]
>>shapiro.test(r.trunc)
>>    
>>
>
>        Shapiro-Wilk normality test
>
>data:  r.trunc 
>W = 0.9855, p-value = 1.237e-07
>
>  
>
>>ks.test(r.trunc, "pnorm")
>>    
>>
>
>        One-sample Kolmogorov-Smirnov test
>
>data:  r.trunc 
>D = 0.0873, p-value = 3.116e-06
>alternative hypothesis: two.sided 
>
>  
>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From jporzak at loyaltymatrix.com  Fri Feb  4 01:34:17 2005
From: jporzak at loyaltymatrix.com (Jim Porzak)
Date: Thu, 3 Feb 2005 16:34:17 -0800
Subject: [R] Opening for a Statistics Practitioner in San Francisco
Message-ID: <004c01c50a51$42d018d0$6801a8c0@loyaltymatrix.local>

Statistics Practitioner Fluent in R - San Francisco CA

Loyalty Matrix Inc., in downtown San Francisco, is expanding our team. We
are a young, dynamic and growing team of multidisciplinary marketing and
technical professionals. We deliver value to our clients by discovering
actionable tactical and strategic insights in actual customer data augmented
with demographics and research.

You will work in the Client Services team doing EDA, basic statistics, data
mining & modeling. You will also assist R&D to develop and integrate new
methods into our proprietary customer intelligence platform
MatrixOptimizerR.

You will have a degree in statistics, be fluent in R and the Microsoft
Office suite (especially Excel, Word and PowerPoint). SQL query skills are
very helpful as is real-world business and marketing experience.

The successful candidate will demonstrate creative ability to solve
practical problems, juggle multiple projects, work in a multidisciplinary
team and have fun.

For more information on us see www.LoyaltyMatrix.com and
R.LoyaltyMatrix.com.

If interested, reply to techjobs at loyaltymatrix.com by February 18, 2005.

In addition to your resume, please include a cover letter stating how your
training, experience and skills would specifically contribute to our
clients' success.

We look forward to hearing from you.


Best,
Jim Porzak
Director of Analytics
Loyalty Matrix, Inc.
R.LoyaltyMatrix.com
www.LoyaltyMatrix.com
(415) 296-1141 x 210



From roebuck at odin.mdacc.tmc.edu  Fri Feb  4 01:52:21 2005
From: roebuck at odin.mdacc.tmc.edu (Paul Roebuck)
Date: Thu, 3 Feb 2005 18:52:21 -0600 (CST)
Subject: [R] Uninstalling R from Mac OS X
In-Reply-To: <3d2917b9622b29217e404fea0000e4e3@gmail.com>
References: <3d2917b9622b29217e404fea0000e4e3@gmail.com>
Message-ID: <Pine.OSF.4.58.0502031841490.155686@odin.mdacc.tmc.edu>

On Thu, 3 Feb 2005, Christofer Ecklund wrote:

> Could you tell me how to uninstall R from Mac OS X (10.3)?  I am not
> too familiar with terminal or the command line so a GUI removal would
> be better, however I would like to remove every trace of the program
> from my system.  Usually with a package installer there is some sort of
> uninstaller included, but I could not find it.  I have read the
> documentation but could not identify a GUI uninstaller.

No, most OS X packages do not provide an uninstaller.
You can find out what got installed via installer by
viewing the bill of materials files as such:

$ lsbom /Library/Receipts/R-GUI.pkg/Contents/Archive.bom
$ lsbom /Library/Receipts/R\ framework.pkg/Contents/Archive.bom

Toss the following in the trash from the Finder:
	/Applications/R.app
	/Library/Frameworks/R.framework
	~/Library/R/  (if it exists)

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)



From mwgrant2001 at yahoo.com  Fri Feb  4 02:37:19 2005
From: mwgrant2001 at yahoo.com (Michael Grant)
Date: Thu, 3 Feb 2005 17:37:19 -0800 (PST)
Subject: [R] Off topic help-- But interesting behavior on XP Professional
In-Reply-To: <4202B3F2.9090706@pdf.com>
Message-ID: <20050204013719.14642.qmail@web52004.mail.yahoo.com>

Please excuse the off topic question, but some of you
certainly have very significant computer savvy beyond
R and statistics and I would like to tap that
knowledge. I do not believe this to be an R problem,
but to date I believe that R is the only application
on my machine that has been impacted. That may be
because it is the only application that I use every
day.

In this email I am asking R-Windows-Novell list
readers about any similar experience. I am running R
on several machines at work and at home. Each machine
has a local installation. On one machine--Windows XP
Professional hooked into a Novell network (at
work)--running R 2.0.0, I have an occasional entire R
directory disappear. Actually, if I search the drive
it turns up in another directory. This morning it was
the 'modules' folder. It turned up in a subfolder of a
folder located in the same folder as 'rw2000'. Earlier
last month I found one of my roving directories
elsewhere in a totally 'non-R' region of the C: drive.

My directory structure reflects that I typically
manage transitions between R versions by having a 'R'
super folder containing different versions of R and
multiple project folders. Currently however, the
computer is relatively new and only 2.0.0 is
installed. My R directory structure looks something
like this:

               R
               |
       ------------------------ ...
       |       |         |
       misc  projects  rw2000    ....
              ...        | 
                    --------------- ...
                    ...   |     |
                         bin   modules  ...     .]
                         
After I booted up this morning, I opened R and went to
install a package from CRAN by way of the drop down
mwnu. It bombed and indicated that module/internet.dll
couldn't be found. When I 'searched' the C: drive, the
'modules folder turned up in a subdirectory of the
'projects' folder. Now I do not normally poke around
in rw2000 and had not done so since installation. That
is, I do not believe I moved the folder via a 'drag'. 
 

Until this evening I considered this to only be
nuisance but, comparing directories at home and work,
I now suspect this has been occurring for sometime.
That is, tomorrow morning at work I will be looking
for other folders which also appear to be missing
though they have not been missed by R to date.

If you have had or are aware of similar experience
could you please let me know--privately or on the list
if there are bigger issues. R is 'non-standard'
software at my firm and hence when I approach the MIS
folks I would like to have things in order as much as
possible. Given my paranoid nature I suspect some
nefarious combination of XP and the network.

If you don't have any ideas, que sera, sera. I'll slog
through whatever lies ahead and never, ever, give up
my R. 

Best regards,
Michael Grant



From p.murrell at auckland.ac.nz  Fri Feb  4 03:35:29 2005
From: p.murrell at auckland.ac.nz (p.murrell@auckland.ac.nz)
Date: Fri, 4 Feb 2005 15:35:29 +1300 (NZDT)
Subject: [R] graphics examples
Message-ID: <1356.130.216.146.11.1107484529.squirrel@stat11.stat.auckland.ac.nz>

Hi

I have put up some web pages containing a number of plots (and diagrams)
produced using R (they correspond to the figures for a book that I am
working on about R graphics), with the relevant R code provided for each
plot (or diagram), at
http://www.stat.auckland.ac.nz/~paul/RGraphics/rgraphics.html

Hope these are of some help/use;  comments/suggestions welcome.

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From petr.pikal at precheza.cz  Fri Feb  4 07:02:37 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 04 Feb 2005 07:02:37 +0100
Subject: [R] subset data.frame with value != in all columns
In-Reply-To: <s2023c17.034@gwsmtp.DEC.STATE.NY.US>
Message-ID: <42031E0D.30693.330FE6@localhost>

Hi

maybe there are other options but

x[rowSums(x==-99)>=1,]

seems to extract all rows which have at least one -99 in them

Cheers
Petr


On 3 Feb 2005 at 14:57, Tim Howard wrote:

> I am trying to extract rows from a data.frame based on the
> presence/absence of a single value in any column.  I've figured out
> how to do get the positive matches, but the remainder (rows without
> this value) eludes me.  Mining the help pages and archives brought me,
> frustratingly,  very close, as you'll see below. 
> 
> My goal: two data frames, one with -99 in at least one column in each
> row, one with no occurrences of -99. I want to preserve rownames in
> each.
> 
> My questions: 
> Is there a cleaner way to extract all rows containing a specified
> value? How can I extract all rows that don't have this value in any
> col?
> 
> #create dummy dataset
> x <- data.frame(
> c1=c(-99,-99,-99,4:10),
> c2=1:10,
> c3=c(1:3,-99,5:10),
> c4=c(10:1),
> c5=c(1:9,-99))
> 
> #extract data.frame of rows with -99 in them
> for(i in 1:ncol(x))
> {
> y<-subset(x, x[,i]==-99, drop=FALSE);
> ifelse(i==1, z<-y, z <- rbind(z,y));
> }
> 
> #various attempts to get rows not containing "-99":
> 
> # this attempt was to create, in "list", the exclusion formula for
> # each
> column.
> # Here, I couldn't get subset to recognize "list" as the correct type.
> # e.g. it works if I paste the value of list in the subset command
> {
> for(i in 1:ncol(x)){
> if(i==1)
> list<-paste("x[",i,"]!=-99", sep="")
> else
> list<-paste(list," ", " & x[",i,"]!=-99", sep="")
> }
> y<-subset(x, list, drop=FALSE);
> }
> 
> # this will do it for one col, but if I index more
> # it returns all rows
> y <- x[!(x[,3] %in% -99),]
> 
> # this also works for one col
> y<-x[x[,1]!=-99,]
> 
> # but if I index more, I get extra rows of NAs
> y<-x[x[,1:5]!=-99,]
> 
> Thanks in advance.
> Tim Howard
> 
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    2              
> minor    0.1            
> year     2004           
> month    11             
> day      15             
> language R
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From nawaaz at inktomi.com  Fri Feb  4 07:40:17 2005
From: nawaaz at inktomi.com (Nawaaz Ahmed)
Date: Thu, 3 Feb 2005 22:40:17 -0800
Subject: [R] Handling large data sets via scan()
Message-ID: <6cd05c06.5c066cd0@inktomi.com>

I'm trying to read in datasets with roughly 150,000 rows and 600
features. I wrote a function using scan() to read it in (I have a 4GB
linux machine) and it works like a charm.  Unfortunately, converting the
scanned list into a datafame using as.data.frame() causes the memory
usage to explode (it can go from 300MB for the scanned list to 1.4GB for
a data.frame of 30000 rows) and it fails claiming it cannot allocate
memory (though it is still not close to the 3GB limit per process on my
linux box - the message is "unable to allocate vector of size 522K"). 

So I have three questions --

1) Why is it failing even though there seems to be enough memory available?

2) Why is converting it into a data.frame causing the memory usage to
explode? Am I using as.data.frame() wrongly? Should I be using some
other command?

3) All the model fitting packages seem to want to use data.frames as
their input. If I cannot convert my list into a data.frame what can I
do? Is there any way of getting around this?

Much thanks!
Nawaaz



From dieter.menne at menne-biomed.de  Fri Feb  4 08:11:02 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Fri, 4 Feb 2005 07:11:02 +0000 (UTC)
Subject: [R] publishing random effects from lme
References: <42009628.8060001@uni-jena.de>
Message-ID: <loom.20050204T080339-574@post.gmane.org>

> Suppose I have a linear mixed-effects model (from the package nlme) with 
> nested random effects (see below); how would I present the results from 
 the random effects part in a publication?
> 

Have you tried anova(lme(....))?

Your asin(sqrt()) looks a bit like these are percentages of counts. The method 
is still quoted in old books, but has fallen a bit out of favor. Have you 
thought of some glm model instead (http://www.stats.ox.ac.uk/pub/MASS4/)? 

Dieter Menne



From ripley at stats.ox.ac.uk  Fri Feb  4 08:22:07 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 4 Feb 2005 07:22:07 +0000 (GMT)
Subject: [R] filling a string buffer in a C routine
In-Reply-To: <b3a7efa9050203145128c07f0c@mail.gmail.com>
References: <b3a7efa9050203145128c07f0c@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0502040716220.14847@gannet.stats>

On Thu, 3 Feb 2005, Dan Lipsitt wrote:

> I am trying to write a C function that reads lines from a file and
> passes them back to R.

Please take a closer look at the posting guide, which clearly indicates 
this was the wrong list:

   Questions likely to prompt discussion unintelligible to non-programmers
   should go to to R-devel. For example, questions involving C, C++, etc.
   code should go to R-devel.

!!

> Here is a simplified version:
>
> --- C code ---
>
> #include <R.h>
> void read_file(char **filename, char **buf, char **buflen) {
>  FILE *infile;
>
>  infile = fopen(*filename, "r");
>  fgets(*buf, *buflen, infile);
>  fclose(infile);
> }
>
> --- R code ---
>
> buffer = "xxxxxxxxxx"            # kludge!
>
> read.file <- function(filename)
>  .C("read_climate",
>     as.character(filename),
>     buf=buffer,
>     buflen=as.integer(10))$buf
>
> --- end code ---
>
> This works okay, but the only way I could find to allocate a string
> buffer of the size I want is to use a string literal as in the line
> marked "kludge" above. It is impractical for large buffers, not to
> mention uuuggly. I tried
>
> buffer = paste(rep('x', 10), sep="")

Did you look at the result?  You need 'collapse' not 'sep'.

> but it doesn't work. So my question is, how do I do one of the following:
>
> - Allocate a character buffer of a desired size to pass to my C routine.

Use paste(collapse=)

> - Have the C routine allocate the buffer without causing a memory leak.

Use R's allocation routines discussed in `Writing R Extensions'.

> - Use .Call() instead

Lots of examples in the R sources and package sources.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Tom.Mulholland at dpi.wa.gov.au  Fri Feb  4 08:26:49 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Fri, 4 Feb 2005 15:26:49 +0800
Subject: [R] Handling large data sets via scan()
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA82@afhex01.dpi.wa.gov.au>

I'm sure others with more experience will answer this, but for what it is worth my experience suggests that memory issues are more often with the user and not the machine. I don't use Linux so I can't make specific comments about the capacity of your machine. However it appears that there is often a need for a copy of an object to be in memory while you are working on creating a new version. So if you can get a data.frame to be 1.4Gb it wouldn't leave much space if there needed to be an original and a copy for any reason. (I speculate that this may be the case rather than asserting it is the case.)

>From a practical point of view I assume that when you say you have 600 features that you are not going to use each and every one in the models that you may generate. So is it practical to limit the features to those that you wish to use before creating a data.frame?

In short if you really do need to work this way I suggest that you read as many of the frequent posts on memory issues until you are either fully conversant with memory issues with the machine you have or you have found one of the many suggestions to work around this issue, such as working with a database and sql. Using "large dataset" as a query on Jonathon Baron's website gave over 400 hits. http://finzi.psych.upenn.edu/nmz.html

Tom

> -----Original Message-----
> From: Nawaaz Ahmed [mailto:nawaaz at inktomi.com]
> Sent: Friday, 4 February 2005 2:40 PM
> To: R-help at stat.math.ethz.ch
> Cc: nawaaz at yahoo-inc.com
> Subject: [R] Handling large data sets via scan()
> 
> 
> I'm trying to read in datasets with roughly 150,000 rows and 600
> features. I wrote a function using scan() to read it in (I have a 4GB
> linux machine) and it works like a charm.  Unfortunately, 
> converting the
> scanned list into a datafame using as.data.frame() causes the memory
> usage to explode (it can go from 300MB for the scanned list 
> to 1.4GB for
> a data.frame of 30000 rows) and it fails claiming it cannot allocate
> memory (though it is still not close to the 3GB limit per 
> process on my
> linux box - the message is "unable to allocate vector of size 522K"). 
> 
> So I have three questions --
> 
> 1) Why is it failing even though there seems to be enough 
> memory available?
> 
> 2) Why is converting it into a data.frame causing the memory usage to
> explode? Am I using as.data.frame() wrongly? Should I be using some
> other command?
> 
> 3) All the model fitting packages seem to want to use data.frames as
> their input. If I cannot convert my list into a data.frame what can I
> do? Is there any way of getting around this?
> 
> Much thanks!
> Nawaaz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri Feb  4 08:53:59 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 04 Feb 2005 08:53:59 +0100
Subject: [R] Is error hardware or workstation specific
In-Reply-To: <OF47706255.3FAE98DB-ON85256F9D.00748C1B-85256F9D.0075881D@epamail.epa.gov>
References: <OF47706255.3FAE98DB-ON85256F9D.00748C1B-85256F9D.0075881D@epamail.epa.gov>
Message-ID: <42032A17.7020309@statistik.uni-dortmund.de>

Woodall.George at epamail.epa.gov wrote:
> I am working on a Dell Latitude D800 laptop computer with an Intel
> processor running Windows XP Professional.  The problem I describe below
> has occurred while running R versions 1.9.1 and 2.0.1.  The error occurs
> while using a program to perform categorical regression that was
> originally written in S-plus but was recently converted to R.  Using the
> same data set which gives me the error, one of my colleagues who is
> running the same set-up, but on a Dell Desktop computer does not get the
> error.  The error I am getting is listed below.
> 
> Error in seq(length(missing.row)) : Object "missing.row" not found
>
> Any suggestions?

Please a) use traceback() to see where the error happens, b) specify an 
example, c) tell us at least which functions/packages you are using!

Uwe Ligges



> George Woodall
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Christoph.Scherber at uni-jena.de  Fri Feb  4 10:09:28 2005
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Fri, 04 Feb 2005 10:09:28 +0100
Subject: [R] publishing random effects from lme
In-Reply-To: <loom.20050204T080339-574@post.gmane.org>
References: <42009628.8060001@uni-jena.de>
	<loom.20050204T080339-574@post.gmane.org>
Message-ID: <42033BC8.9060304@uni-jena.de>

Hi Dieter,

Yes, I?ve tried both options. The anova(lme(...)) gives me good results 
for the fixed effects part, but what I?m specifically interested in is 
what to do with the random effects.

I have tried glmmPQL (generalized linear mixed-effects models), which 
did in fact greatly help account for heteroscedasticity, but I can?t do 
model simplification with these models (and they?re still heavily 
debated, as I read from previous postings to "R Help".

How would you deal with the random effects part of the models when 
publishing results from lme?

Thanks for your help!
Christoph







###
Here are my original questions once again (with an example below):

1) What is the total variance of the random effects at each level?
(2) How can I test the significance of the variance components?
(3) Is there something like an "r squared" for the whole model which I 
can state?  ##it seems, there isn?t (as I learned from a previous posting

The data come from an experiment on plant performance with and without 
insecticide, with and without grasses present, and across different 
levels of plant diversity ("div").

Thanks for your help!
Christoph.

lme(asin(sqrt(response)) ~ treatment + logb(div + 1, 2) + grass,
random =  ~ 1 | plotcode/treatment, na.action = na.exclude, method = "ML")

Linear mixed-effects model fit by maximum likelihood

Data: NULL
       AIC      BIC  logLik
 -290.4181 -268.719 152.209

Random effects:
Formula:  ~ 1 | plotcode
       (Intercept)
StdDev:  0.04176364

 Formula:  ~ 1 | treatment %in% plotcode
      (Intercept)   Residual
StdDev:  0.08660458 0.00833387

Fixed effects: asin(sqrt(response)) ~ treatment + logb(div + 1, 2) + grass
                   Value  Std.Error DF   t-value p-value
   (Intercept)  0.1858065 0.01858581 81  9.997225  <.0001
     treatment  0.0201384 0.00687832 81  2.927803  0.0044
logb(div + 1, 2) -0.0203301 0.00690074 79 -2.946073  0.0042
         grass  0.0428934 0.01802506 79  2.379656  0.0197

Standardized Within-Group Residuals:
      Min          Q1         Med         Q3       Max
-0.2033155 -0.05739679 -0.00943737 0.04045958 0.3637217

Number of Observations: 164
Number of Groups:
plotcode ansatz %in% plotcode
     82                  164






Dieter Menne wrote:

>>Suppose I have a linear mixed-effects model (from the package nlme) with 
>>nested random effects (see below); how would I present the results from 
>>    
>>
> the random effects part in a publication?
>  
>
>
>Have you tried anova(lme(....))?
>
>Your asin(sqrt()) looks a bit like these are percentages of counts. The method 
>is still quoted in old books, but has fallen a bit out of favor. Have you 
>thought of some glm model instead (http://www.stats.ox.ac.uk/pub/MASS4/)? 
>
>Dieter Menne
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From christoph.lehmann at gmx.ch  Fri Feb  4 10:28:34 2005
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Fri, 04 Feb 2005 10:28:34 +0100
Subject: [R] Handling large data sets via scan()
In-Reply-To: <6cd05c06.5c066cd0@inktomi.com>
References: <6cd05c06.5c066cd0@inktomi.com>
Message-ID: <42034042.9080906@gmx.ch>

does it solve to a part your problem, if you use read.table() instead of 
scan, since it imports data directly to a data.frame?

let me know, if it helps

Nawaaz Ahmed wrote:
> I'm trying to read in datasets with roughly 150,000 rows and 600
> features. I wrote a function using scan() to read it in (I have a 4GB
> linux machine) and it works like a charm.  Unfortunately, converting the
> scanned list into a datafame using as.data.frame() causes the memory
> usage to explode (it can go from 300MB for the scanned list to 1.4GB for
> a data.frame of 30000 rows) and it fails claiming it cannot allocate
> memory (though it is still not close to the 3GB limit per process on my
> linux box - the message is "unable to allocate vector of size 522K"). 
> 
> So I have three questions --
> 
> 1) Why is it failing even though there seems to be enough memory available?
> 
> 2) Why is converting it into a data.frame causing the memory usage to
> explode? Am I using as.data.frame() wrongly? Should I be using some
> other command?
> 
> 3) All the model fitting packages seem to want to use data.frames as
> their input. If I cannot convert my list into a data.frame what can I
> do? Is there any way of getting around this?
> 
> Much thanks!
> Nawaaz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From dimitris.rizopoulos at med.kuleuven.ac.be  Fri Feb  4 10:45:11 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Fri, 4 Feb 2005 10:45:11 +0100
Subject: [R] publishing random effects from lme
References: <42009628.8060001@uni-jena.de><loom.20050204T080339-574@post.gmane.org>
	<42033BC8.9060304@uni-jena.de>
Message-ID: <00ba01c50a9e$385b8680$0540210a@www.domain>

If you have heteroscedasticity problems, the nlme package has many 
varFunctions (e.g., varPower, varIdent, etc.) that could assist you in 
fitting it. The usage of GLMMs is mainly for discrete and count data 
that you cannot fit with lme.

Testing between competing lme models should be done via LRTs and the 
anova.lme() function. However, take care of the fitting procedure 
(REML vs ML), especially in case you also change the fixed-effects. 
The latter has been recently discussed on the list.

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Christoph Scherber" <Christoph.Scherber at uni-jena.de>
To: "Dieter Menne" <dieter.menne at menne-biomed.de>
Cc: <r-help at stat.math.ethz.ch>
Sent: Friday, February 04, 2005 10:09 AM
Subject: Re: [R] publishing random effects from lme


> Hi Dieter,
>
> Yes, I?ve tried both options. The anova(lme(...)) gives me good 
> results for the fixed effects part, but what I?m specifically 
> interested in is what to do with the random effects.
>
> I have tried glmmPQL (generalized linear mixed-effects models), 
> which did in fact greatly help account for heteroscedasticity, but I 
> can?t do model simplification with these models (and they?re still 
> heavily debated, as I read from previous postings to "R Help".
>
> How would you deal with the random effects part of the models when 
> publishing results from lme?
>
> Thanks for your help!
> Christoph
>
>
>
>
>
>
>
> ###
> Here are my original questions once again (with an example below):
>
> 1) What is the total variance of the random effects at each level?
> (2) How can I test the significance of the variance components?
> (3) Is there something like an "r squared" for the whole model which 
> I can state?  ##it seems, there isn?t (as I learned from a previous 
> posting
>
> The data come from an experiment on plant performance with and 
> without insecticide, with and without grasses present, and across 
> different levels of plant diversity ("div").
>
> Thanks for your help!
> Christoph.
>
> lme(asin(sqrt(response)) ~ treatment + logb(div + 1, 2) + grass,
> random =  ~ 1 | plotcode/treatment, na.action = na.exclude, method = 
> "ML")
>
> Linear mixed-effects model fit by maximum likelihood
>
> Data: NULL
>       AIC      BIC  logLik
> -290.4181 -268.719 152.209
>
> Random effects:
> Formula:  ~ 1 | plotcode
>       (Intercept)
> StdDev:  0.04176364
>
> Formula:  ~ 1 | treatment %in% plotcode
>      (Intercept)   Residual
> StdDev:  0.08660458 0.00833387
>
> Fixed effects: asin(sqrt(response)) ~ treatment + logb(div + 1, 2) + 
> grass
>                   Value  Std.Error DF   t-value p-value
>   (Intercept)  0.1858065 0.01858581 81  9.997225  <.0001
>     treatment  0.0201384 0.00687832 81  2.927803  0.0044
> logb(div + 1, 2) -0.0203301 0.00690074 79 -2.946073  0.0042
>         grass  0.0428934 0.01802506 79  2.379656  0.0197
>
> Standardized Within-Group Residuals:
>      Min          Q1         Med         Q3       Max
> -0.2033155 -0.05739679 -0.00943737 0.04045958 0.3637217
>
> Number of Observations: 164
> Number of Groups:
> plotcode ansatz %in% plotcode
>     82                  164
>
>
>
>
>
>
> Dieter Menne wrote:
>
>>>Suppose I have a linear mixed-effects model (from the package nlme) 
>>>with nested random effects (see below); how would I present the 
>>>results from
>> the random effects part in a publication?
>>
>>
>>Have you tried anova(lme(....))?
>>
>>Your asin(sqrt()) looks a bit like these are percentages of counts. 
>>The method is still quoted in old books, but has fallen a bit out of 
>>favor. Have you thought of some glm model instead 
>>(http://www.stats.ox.ac.uk/pub/MASS4/)?
>>Dieter Menne
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From h.andersson at nioo.knaw.nl  Fri Feb  4 10:45:53 2005
From: h.andersson at nioo.knaw.nl (Henrik Andersson)
Date: Fri, 04 Feb 2005 10:45:53 +0100
Subject: [R] Output from function to a tcltk window
Message-ID: <ctvg70$ugs$1@sea.gmane.org>

I would like to display output to a tcltk window from e.g. a call to 
summary().



I tried to get something else than oneliners into a text window of the 
kind found at:

http://bioinf.wehi.edu.au/~wettenhall/RTclTkExamples/TextWindows.html

But without success.

Henrik


-------------------------------------------------------------

Henrik Andersson
Netherlands Institute of Ecology -
Centre for Estuarine and Marine Ecology
P.O. Box 140
4400 AC Yerseke
Phone: +31 113 577473
h.andersson at nioo.knaw.nl

http://www.nioo.knaw.nl/ppages/handersson



From murdoch at stats.uwo.ca  Fri Feb  4 10:50:22 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 04 Feb 2005 09:50:22 +0000
Subject: [R] Is anyone using the MiniR distribution?
Message-ID: <btg601p2sr5cdsj8fc49qa4t1ssfsqpt6d@4ax.com>

I currently build two different versions of each Windows binary:  the
rwXXXX.exe full installation program (with the next release looking to
be around 25 Megabytes), and a series of 8 diskette-sized files named
miniR*.

The miniR files only include a minimal installation of R, and are
rarely tested.  Rather than building something that may not even work,
I'd like to stop building them.  

Would this be a problem for anyone? 

Duncan Murdoch



From Rau at demogr.mpg.de  Fri Feb  4 10:58:43 2005
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Fri, 4 Feb 2005 10:58:43 +0100
Subject: [R] Surprising Behavior of 'tapply'
Message-ID: <8B08A3A1EA7AAC41BE24C750338754E61506FB@HERMES.demogr.mpg.de>

Dear helpers,

thank you very much for your advice.

After starting a new R-session this morning, I was also unable to replicate the problem, although the old session showed still the same problem.
One suggestion was that I maybe redefined some functions, but this was not the case. I only loaded one additional package (Hmisc) but I did this now as well and it did not cause any problems.
Another suggestion (alternative) was to use 'xtabs'. This works also nicely, but I made some timings with my dataset (moderate size of 6MB) and I assume that for really large datasets 'tapply' is probably faster than 'xtabs':

> system.time(tapply(austria$COUNT, list(austria$sescat, austria$STATUS, austria$SEX), sum))
[1] 0.05 0.00 0.04   NA   NA
> system.time(xtabs(austria$COUNT ~., data.frame(ses = austria$sescat, status =austria$STATUS, sex=austria$SEX)))
[1] 0.86 0.00 0.86   NA   NA
> 

(I did the timings several times and was also using gc() ).

Thanks again (in chronological order) to Bert Gunter, Carlos Ortega, James Holtman, and Gabor Grothendieck.

Best,
Roland



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor Grothendieck
Sent: Thursday, February 03, 2005 9:08 PM
To: r-help at stat.math.ethz.ch
Subject: Re: [R] Surprising Behavior of 'tapply'


I tried it on Windows XP with R 2.1.0 and could not replicate it either.
Suggest you start up a fresh session and try it again.

By the way, you could consider this:

xtabs(count ~., data.frame(sex = sex, income = income))



Carlos Ortega <carlos_ortegafernandez <at> yahoo.es> writes:

: 
: Hi,
: 
: That is something strange, I could not replicate it...
: 
: Regards,
: Carlos.
: 
: +++++++++++++++++++++++++++++++++++
: > version
:          _              
: platform i386-pc-mingw32
: arch     i386           
: os       mingw32        
: system   i386, mingw32  
: status                  
: major    2              
: minor    0.1            
: year     2004           
: month    11             
: day      15             
: language R              
: > sex <- rep(c("F", "M"), 5)
: > income <-  c(rep("low", 5), rep("high", 5))
: > count <- 1:10
: > mydf <- as.data.frame(cbind(sex, income, count))
: > mydf$count = as.numeric(as.character(mydf$count))
: > tapply(mydf$count, list(mydf$sex, mydf$income),
: FUN=sum)
:   high low
: F   16   9
: M   24   6
: ++++++++++++++++++++++++++++++++++++++++++++
: 
:  --- "Rau, Roland" <Rau <at> demogr.mpg.de> escribi?: 
: > Dear all,
: > 
: > I wanted to make a two-way-table of two variables
: > with a counting
: > variable stored in another column of a dataframe. In
: > version 1.9.1, the
: > behavior is as expected as shown in the simplified
: > example code.
: > 
: > > sex <- rep(c("F", "M"), 5)
: > > income <-  c(rep("low", 5), rep("high", 5))
: > > count <- 1:10
: > > mydf <- as.data.frame(cbind(sex, income, count))
: > > mydf$count = as.numeric(as.character(mydf$count))
: > > tapply(mydf$count, list(mydf$sex, mydf$income),
: > FUN=sum)
: >   high low
: > F   16   9
: > M   24   6
: > > version
: >          _              
: > platform i386-pc-mingw32
: > arch     i386           
: > os       mingw32        
: > system   i386, mingw32  
: > status                  
: > major    1              
: > minor    9.1            
: > year     2004           
: > month    06             
: > day      21             
: > language R              
: > > 
: > 
: > In version 2.0.1, however, I get the following
: > output:
: > 
: > > sex <- rep(c("F", "M"), 5)
: > > income <-  c(rep("low", 5), rep("high", 5))
: > > count <- 1:10
: > > mydf <- as.data.frame(cbind(sex, income, count))
: > > mydf$count = as.numeric(as.character(mydf$count))
: > > tapply(mydf$count, list(mydf$sex, mydf$income),
: > FUN=sum)
: > Error in get(x, envir, mode, inherits) : variable
: > "FUN" was not found
: > > version
: >          _              
: > platform i386-pc-mingw32
: > arch     i386           
: > os       mingw32        
: > system   i386, mingw32  
: > status                  
: > major    2              
: > minor    0.1            
: > year     2004           
: > month    11             
: > day      15             
: > language R              
: > > 
: > 
: > Was this change in behavior intended with the
: > changes in tapply from
: > R1.9.1 to R2.0.1?
: > Is the R-help-list appropriate or rather R-Devel?
: > 
: > Thanks,
: > Roland
: > 
: > 
: > 
: > +++++
: > This mail has been sent through the MPI for
: > Demographic Rese...{{dropped}}
: > 
: > ______________________________________________
: > R-help <at> stat.math.ethz.ch mailing list
: > https://stat.ethz.ch/mailman/listinfo/r-help
: > PLEASE do read the posting guide!
: > http://www.R-project.org/posting-guide.html
: >
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://stat.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
: 
:

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


+++++
This mail has been sent through the MPI for Demographic Rese...{{dropped}}



From p.dalgaard at biostat.ku.dk  Fri Feb  4 11:21:22 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 04 Feb 2005 11:21:22 +0100
Subject: [R] Output from function to a tcltk window
In-Reply-To: <ctvg70$ugs$1@sea.gmane.org>
References: <ctvg70$ugs$1@sea.gmane.org>
Message-ID: <x2mzukpqvx.fsf@biostat.ku.dk>

Henrik Andersson <h.andersson at nioo.knaw.nl> writes:

> I would like to display output to a tcltk window from e.g. a call to
> summary().
> 
> 
> 
> I tried to get something else than oneliners into a text window of the
> kind found at:
> 
> http://bioinf.wehi.edu.au/~wettenhall/RTclTkExamples/TextWindows.html
> 
> But without success.

(Rcmdr must be doing this sort of thing already?)

I'd try this:

1) str <- paste(capture.output(summary(myfit),collapse="\n"))

2) clone the tkfaq demo (or one of James W.'s examples), but replace
   the line

   tkinsert(txt, "end", tkcmd("read", chn))

with 

   tkinsert(txt, "end", str)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From vikas at mail.jnu.ac.in  Fri Feb  4 12:09:51 2005
From: vikas at mail.jnu.ac.in (Vikas Rawal)
Date: Fri, 04 Feb 2005 16:39:51 +0530
Subject: [R] Installing R packages in windows
Message-ID: <1107515391.cf8bda00vikas@mail.jnu.ac.in>

I need to install a selected set of packages on a number of machines (in a computer lab). Some of these machines are not connected to internet. Is it possible to download all the packages and make a kind of repository on a CD, and then install.packages from the CD?

Vikas

==============================================

 This Mail was Scanned for Virus and found Virus free



From manuel_gutierrez_lopez at yahoo.es  Fri Feb  4 12:06:43 2005
From: manuel_gutierrez_lopez at yahoo.es (Manuel Gutierrez)
Date: Fri, 4 Feb 2005 12:06:43 +0100 (CET)
Subject: [R] Rare Cases and SOM
Message-ID: <20050204110643.58244.qmail@web25104.mail.ukl.yahoo.com>

I am trying to understand how the SOM algorithm works
using library(class) SOM function.
I have a 1000*10 matrix and I want to be able to
summarize the different types of 10-element vectors.
In my real world case it is likely that most of the
1000 values are of one kind the rest of other (this is
an oversimplification).
Say for example:

InputA<-matrix(cos(1:10),nrow=900,ncol=10,byrow=TRUE)
InputB<-matrix(sin(5:14),nrow=100,ncol=10,byrow=TRUE)
Input<-rbind(InputA,InputB)

I though that a small grid of 3*3 would be enough to
extract the patterns in such simple matrix :
GridWidth<-3
GridLength<-3
gr <- somgrid(xdim=GridWidth,ydim=GridLength,topo =
"hexagonal")
test.som <- SOM(Input, gr)
par(mfrow=c(GridLength,GridWidth))
for(i in 1:(GridWidth*GridLength))
plot(test.som$codes[i,],type="l")

Only when I use a larger grid (say for example 7*3 ) I
get some of the representatives for the sin pattern.
This must have something to do with the initialization
of the grid, as the sin is so rare it is unlikely that
I get it as a reference vector. Afterwards, because
the selection for the training is also random it is
also unlikely they are picked.
I've been trying to modify some of the other
parameters for the SOM also, but I would appreciatte
some input to keep me going until I receive the
reference books from my bookstore.

Are my suspictions right?
Should I be using the SOM for my study or should I
look somewhere else?
NOTE: I have no prior knowledge of whether the
datasets I want to analyse will have rare cases or not
or where they will be located.
Thanks,
Manuel



From Christoph.Scherber at uni-jena.de  Fri Feb  4 13:28:46 2005
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Fri, 04 Feb 2005 13:28:46 +0100
Subject: [R] integration function
Message-ID: <42036A7E.2050005@uni-jena.de>

Dear R users,

I have tried to write a function which gives the step-wise integral for 
an exponential function (moving from -3 to 3 in steps of 0.1, where the 
output for every step shall be the integral under the curve of y against x.

However, something seems to be wrong with this function; can anyone 
please help me?

x<-seq(-3,3,0.1)
y<-exp(x)


integral<-function(z,a,b,step){
for(i in (1:((b-a)/step))){
    c<-0
    c[i]<-integrate(z,lower=a+(i-1)*step,upper=a+i*step)
    print(c$integral)
}}

integral(y,-3,3,0.1)

Best regards
Christoph



From jfox at mcmaster.ca  Fri Feb  4 13:40:29 2005
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 4 Feb 2005 07:40:29 -0500
Subject: [R] Output from function to a tcltk window
In-Reply-To: <x2mzukpqvx.fsf@biostat.ku.dk>
Message-ID: <20050204124022.QJDI1836.tomts16-srv.bellnexxia.net@JohnDesktop8300>

Dear Peter and Henrik,

What the Rcmdr does may be overkill for Henrik's application, since it also
intercepts error and warning messages, and tries to take the behaviour of
the R console. The relevant functions are in the file Commander.R in the
source package; the principal one is:

doItAndPrint <- function(command, log=TRUE) {
    messages.connection <- textConnection(".messages", open="w")
    sink(messages.connection, type="message")
    output.connection <- textConnection(".Output", open="w")
    sink(output.connection, type="output")
    on.exit({
        sink(type="message")
        if (!.console.output) sink(type="output") # if .console.output,
output connection already closed
        close(messages.connection)
        close(output.connection)
        })
    if (log) logger(command)
    result <-  try(eval(parse(text=command), envir=.GlobalEnv), silent=TRUE)
    if (class(result)[1] ==  "try-error"){
        tkmessageBox(message=paste("Error:",
            strsplit(result, ":")[[1]][2]), icon="error")
        if (.console.output) sink(type="output")
        tkfocus(.commander)
        return()
        }
    if (isS4object(result)) show(result) else print(result)
    if (.Output[length(.Output)] == "NULL") .Output <-
.Output[-length(.Output)] # suppress "NULL" line at end of output
    if (length(.Output) != 0) {  # is there output to print?
        if (.console.output) {
            out <- .Output
            sink(type="output")
            for (line in out) cat(paste(line, "\n", sep=""))
            }
        else{
            for (line in .Output) tkinsert(.output, "end", paste(line, "\n",
sep=""))
            tkyview.moveto(.output, 1)
            }
        }
    else if (.console.output) sink(type="output")
    checkWarnings(.messages)  # errors already intercepted, display any
warnings
    result
    }

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Peter Dalgaard
> Sent: Friday, February 04, 2005 5:21 AM
> To: Henrik Andersson
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Output from function to a tcltk window
> 
> Henrik Andersson <h.andersson at nioo.knaw.nl> writes:
> 
> > I would like to display output to a tcltk window from e.g. 
> a call to 
> > summary().
> > 
> > 
> > 
> > I tried to get something else than oneliners into a text 
> window of the 
> > kind found at:
> > 
> > 
> http://bioinf.wehi.edu.au/~wettenhall/RTclTkExamples/TextWindows.html
> > 
> > But without success.
> 
> (Rcmdr must be doing this sort of thing already?)
> 
> I'd try this:
> 
> 1) str <- paste(capture.output(summary(myfit),collapse="\n"))
> 
> 2) clone the tkfaq demo (or one of James W.'s examples), but replace
>    the line
> 
>    tkinsert(txt, "end", tkcmd("read", chn))
> 
> with 
> 
>    tkinsert(txt, "end", str)
> 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: 
> (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: 
> (+45) 35327907
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri Feb  4 13:55:42 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 04 Feb 2005 13:55:42 +0100
Subject: [R] Installing R packages in windows
In-Reply-To: <1107515391.cf8bda00vikas@mail.jnu.ac.in>
References: <1107515391.cf8bda00vikas@mail.jnu.ac.in>
Message-ID: <420370CE.6060800@statistik.uni-dortmund.de>

Vikas Rawal wrote:

> I need to install a selected set of packages on a number of machines (in a computer lab). Some of these machines are not connected to internet. Is it possible to download all the packages and make a kind of repository on a CD, and then install.packages from the CD?

Yes, just download the packages and install.packages with CRAN=NULL ...

Instead, you might want to mount the installed packages from a network 
volume instead, adding a second library path for R. So you only need to 
install stuff once.


Uwe Ligges



> Vikas
> 
> ==============================================
> 
>  This Mail was Scanned for Virus and found Virus free
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri Feb  4 13:56:15 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 04 Feb 2005 13:56:15 +0100
Subject: [R] integration function
In-Reply-To: <42036A7E.2050005@uni-jena.de>
References: <42036A7E.2050005@uni-jena.de>
Message-ID: <420370EF.4070302@statistik.uni-dortmund.de>

Christoph Scherber wrote:

> Dear R users,
> 
> I have tried to write a function which gives the step-wise integral for 
> an exponential function (moving from -3 to 3 in steps of 0.1, where the 
> output for every step shall be the integral under the curve of y against x.
> 
> However, something seems to be wrong with this function; can anyone 
> please help me?
> 
> x<-seq(-3,3,0.1)
> y<-exp(x)
> 
> 
> integral<-function(z,a,b,step){
> for(i in (1:((b-a)/step))){
>    c<-0
>    c[i]<-integrate(z,lower=a+(i-1)*step,upper=a+i*step)

integrate() expects a function, you specify a vector of values ....

Uwe Liges




>    print(c$integral)
> }}
> 
> integral(y,-3,3,0.1)
> 
> Best regards
> Christoph
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From dimitris.rizopoulos at med.kuleuven.ac.be  Fri Feb  4 13:59:21 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Fri, 4 Feb 2005 13:59:21 +0100
Subject: [R] integration function
References: <42036A7E.2050005@uni-jena.de>
Message-ID: <003101c50ab9$580c0f20$0540210a@www.domain>

The syntax you have you used is not correct. integrate() needs as 
first argument a function! see ?integrate for more info.

a possible solution could be:

x <- seq(-3, 3, 0.1)
y <- exp(x)
######
integral <- function(z, a, b, step.){
    cc <- numeric(n <- (b-a)/step.)
    f <- function(x) exp(x)
    for(i in 1:n) cc[i] <- integrate(f, lower=a+(i-1)*step., 
upper=a+i*step.)$val
    cc
}
integral(y, -3, 3, 0.1)

However, since exp has known integral, you do not need to integrate:

exp(x[-1])-exp(x[seq(1, length(x)-1)]))


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Christoph Scherber" <Christoph.Scherber at uni-jena.de>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, February 04, 2005 1:28 PM
Subject: [R] integration function


> Dear R users,
>
> I have tried to write a function which gives the step-wise integral 
> for an exponential function (moving from -3 to 3 in steps of 0.1, 
> where the output for every step shall be the integral under the 
> curve of y against x.
>
> However, something seems to be wrong with this function; can anyone 
> please help me?
>
> x<-seq(-3,3,0.1)
> y<-exp(x)
>
>
> integral<-function(z,a,b,step){
> for(i in (1:((b-a)/step))){
>    c<-0
>    c[i]<-integrate(z,lower=a+(i-1)*step,upper=a+i*step)
>    print(c$integral)
> }}
>
> integral(y,-3,3,0.1)
>
> Best regards
> Christoph
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From rpeng at jhsph.edu  Fri Feb  4 14:36:15 2005
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Fri, 04 Feb 2005 08:36:15 -0500
Subject: [R] Handling large data sets via scan()
In-Reply-To: <6cd05c06.5c066cd0@inktomi.com>
References: <6cd05c06.5c066cd0@inktomi.com>
Message-ID: <42037A4F.40204@jhsph.edu>

I can usually read in large tables by very careful usage of 
read.table() without having to resort to scan().  In particular, using 
the `colClasses', `nrows', and `comment.char' arguments correctly can 
greatly reduce memory usage (and increase speed) when reading in data.

Converting from a list to a data frame likely requires at least two 
copies of the data being stored in memory.  Also, are you using a 
64-bit operating system?

-roger

Nawaaz Ahmed wrote:
> I'm trying to read in datasets with roughly 150,000 rows and 600
> features. I wrote a function using scan() to read it in (I have a 4GB
> linux machine) and it works like a charm.  Unfortunately, converting the
> scanned list into a datafame using as.data.frame() causes the memory
> usage to explode (it can go from 300MB for the scanned list to 1.4GB for
> a data.frame of 30000 rows) and it fails claiming it cannot allocate
> memory (though it is still not close to the 3GB limit per process on my
> linux box - the message is "unable to allocate vector of size 522K"). 
> 
> So I have three questions --
> 
> 1) Why is it failing even though there seems to be enough memory available?
> 
> 2) Why is converting it into a data.frame causing the memory usage to
> explode? Am I using as.data.frame() wrongly? Should I be using some
> other command?
> 
> 3) All the model fitting packages seem to want to use data.frames as
> their input. If I cannot convert my list into a data.frame what can I
> do? Is there any way of getting around this?
> 
> Much thanks!
> Nawaaz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From maechler at stat.math.ethz.ch  Fri Feb  4 14:46:03 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 4 Feb 2005 14:46:03 +0100
Subject: [R] Is anyone using the MiniR distribution?
In-Reply-To: <btg601p2sr5cdsj8fc49qa4t1ssfsqpt6d@4ax.com>
References: <btg601p2sr5cdsj8fc49qa4t1ssfsqpt6d@4ax.com>
Message-ID: <16899.31899.47067.432764@stat.math.ethz.ch>


>>>>> "Duncan" == Duncan Murdoch <murdoch at stats.uwo.ca>
>>>>>     on Fri, 04 Feb 2005 09:50:22 +0000 writes:

    Duncan> I currently build two different versions of each
    Duncan> Windows binary: the rwXXXX.exe full installation
    Duncan> program (with the next release looking to be around
    Duncan> 25 Megabytes), and a series of 8 diskette-sized
    Duncan> files named miniR*.

    Duncan> The miniR files only include a minimal installation
    Duncan> of R, and are rarely tested.  Rather than building
    Duncan> something that may not even work, I'd like to stop
    Duncan> building them.

    Duncan> Would this be a problem for anyone?


People for which this is a problem will probably not be
subscribed to R-help because they'll be living in places /
situations with bad / expensive internet connection.

(Sorry to be not really constructive here).

Martin



From William.Simpson at drdc-rddc.gc.ca  Fri Feb  4 15:03:07 2005
From: William.Simpson at drdc-rddc.gc.ca (Bill Simpson)
Date: Fri, 4 Feb 2005 09:03:07 -0500 (EST)
Subject: [R] How to read in .jpeg files
Message-ID: <Pine.LNX.4.44.0502040900540.22803-100000@localhost.localdomain>

In case others are looking for a simple way to read in .jpeg files as 
ordinary matrices, here is my solution. I am only interested in greyscale 
images, so you will have to alter the following if you want colour.

Most .jpegs are colour, so first step is to open the file with ImageMagick 
"display" and save as greyscale.

Then convert (using IM "convert") .jpg to .pgm (grey scale):
convert -compress none groundhogbw.jpg groundhogbw.pgm
The "compress none" is needed to make it stored as plain, not raw .pgm

- using an ordinary editor you will see the top of the file is like this:
P2       # "magic number" identifies file as plain .pgm
350 383  # width, height
255      # max grey level
- the rest of the numbers are the grey levels left-right, top-bottom.
- use scan() to read file in as a vector, then use matrix() to convert to 
matrix

dims<-scan("groundhogbw.pgm",skip=1,nlines=1) #skip magic number and read dims
x<-matrix(scan("groundhogbw.pgm",skip=3),ncol=dims[2],nrow=dims[1])
for(i in 1:dims[1]) x[i,]<-rev(x[i,]) #flip the image vertically
image(x,col=gray(0:255/255),axes=F)

Bill



From beep.boop at aol.hu  Fri Feb  4 15:05:17 2005
From: beep.boop at aol.hu (beep.boop@aol.hu)
Date: Fri, 04 Feb 2005 15:05:17 +0100
Subject: [R] Opening for a Statistics Practitioner in San Francisco
In-Reply-To: <004c01c50a51$42d018d0$6801a8c0@loyaltymatrix.local>
References: <004c01c50a51$42d018d0$6801a8c0@loyaltymatrix.local>
Message-ID: <jvv601lh1io0vd6t4i1rhp955p8o6r8u7i@4ax.com>

On Thu, 3 Feb 2005 16:34:17 -0800, you wrote:


|=[:o)  You will have a degree in statistics, be fluent in R and the Microsoft
|=[:o)  Office suite (especially Excel, Word and PowerPoint). SQL query skills are
|=[:o)  very helpful as is real-world business and marketing experience.
|=[:o)  


> fortune(59)

Let's not kid ourselves: the most widely used piece of software for statistics
is Excel.
   -- Brian D. Ripley (`Statistical Methods Need Software: A View of
      Statistical Computing')
      Opening lecture RSS 2002, Plymouth (September 2002)



From wagner at itp.phys.ethz.ch  Fri Feb  4 15:10:34 2005
From: wagner at itp.phys.ethz.ch (Urs Wagner)
Date: Fri, 04 Feb 2005 14:10:34 +0000
Subject: [R] sink to file
Message-ID: <4203825A.6010506@itp.phys.ethz.ch>

Hello

I would like to use the source(command)  and write the output into a file.
I am using

outputfile=file("output.txt", open="wt")
sink(outputfile, type="output")
source("input.R", echo=TRUE)

Unfortunately the result has prompted commands. How can I avoid the 
prompted commands data(iris), ...?

Thanks

 > data(iris)
 > dataset = iris
 > options(width = 50)
 > summary(dataset)
  Sepal.Length    Sepal.Width     Petal.Length 
 Min.   :4.300   Min.   :2.000   Min.   :1.000 
 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600 
 Median :5.800   Median :3.000   Median :4.350 
 Mean   :5.843   Mean   :3.057   Mean   :3.758 
 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100 
 Max.   :7.900   Max.   :4.400   Max.   :6.900 
  Petal.Width          Species 
 Min.   :0.100   setosa    :50 
 1st Qu.:0.300   versicolor:50 
 Median :1.300   virginica :50 
 Mean   :1.199                 
 3rd Qu.:1.800                 
 Max.   :2.500



From tonybao at mac.com  Fri Feb  4 15:10:57 2005
From: tonybao at mac.com (Tony Han Bao)
Date: Fri, 4 Feb 2005 14:10:57 +0000
Subject: [R] how to generate a function from a linear model 
Message-ID: <0ea92bd4082ea9cd51b10dc69bb68468@mac.com>

Hi All,

I am trying to generate a function from a linear model. I think there 
should be build-in function that perform this action but I've had no 
luck finding it.

For example, I have a model created using lm().

	model <- lm(sat.d~1+sat.n+I(sat.n^2))

What I would like to have is a function (similar to the one generated 
by splinefun()) so that I can use it on different data-sets.

Thanks in advance for the help.

Tony Han Bao
tonybao at mac.com



From andikumagenge at businessdecision.com  Fri Feb  4 15:22:35 2005
From: andikumagenge at businessdecision.com (NDIKUMAGENGE Alice)
Date: Fri, 4 Feb 2005 15:22:35 +0100
Subject: [R] Bayesian Network
Message-ID: <6CC4DC1EC1F92D4B8A5FF590362E756502FA425B@neptune.betd.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050204/32526197/attachment.pl

From sway at tanox.com  Fri Feb  4 15:16:03 2005
From: sway at tanox.com (Shawn Way)
Date: Fri, 4 Feb 2005 08:16:03 -0600
Subject: [R] QCC and PlotMath question
Message-ID: <2DBF8A8E1A1AEE4AB3618AC4D6BF308807209D@houston.tanox.net>

For some reason, using the qcc package, I'm unable to use the plotmath
notation in the title.  Can anyone see what I'm doing wrong?

library(qcc)
a <- rnorm(100)
qcc(a,type="xbar.one",title=expression(bar(X)),ylab=expression(CFU/ft^3)
)

This seems to not let the expression be evaluated, so I tried:

qcc(a,type="xbar.one",title=eval(expression(bar(X))),ylab=expression(CFU
/ft^3))

And get the following error:
Error in eval(expr, envir, enclos) : couldn't find function "bar"

Any thoughts?

-----------------------------------------------------------------
"Policies are many, Principles are few, Policies will change, Principles
never do." 
-John C. Maxwell


Shawn Way, PE



From rpeng at jhsph.edu  Fri Feb  4 15:31:11 2005
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Fri, 04 Feb 2005 09:31:11 -0500
Subject: [R] how to generate a function from a linear model
In-Reply-To: <0ea92bd4082ea9cd51b10dc69bb68468@mac.com>
References: <0ea92bd4082ea9cd51b10dc69bb68468@mac.com>
Message-ID: <4203872F.40005@jhsph.edu>

I don't think there's an automatic way to do this but you might try 
something like:

model <- lm(sat.d~1+sat.n+I(sat.n^2))

f <- function(x) { predict(model, data.frame(sat.n = x)) }

-roger

Tony Han Bao wrote:
> Hi All,
> 
> I am trying to generate a function from a linear model. I think there 
> should be build-in function that perform this action but I've had no 
> luck finding it.
> 
> For example, I have a model created using lm().
> 
>     model <- lm(sat.d~1+sat.n+I(sat.n^2))
> 
> What I would like to have is a function (similar to the one generated by 
> splinefun()) so that I can use it on different data-sets.
> 
> Thanks in advance for the help.
> 
> Tony Han Bao
> tonybao at mac.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From William.Simpson at drdc-rddc.gc.ca  Fri Feb  4 15:33:32 2005
From: William.Simpson at drdc-rddc.gc.ca (Bill Simpson)
Date: Fri, 4 Feb 2005 09:33:32 -0500 (EST)
Subject: [R] How to read in .jpeg files
In-Reply-To: <Pine.LNX.4.44.0502040900540.22803-100000@localhost.localdomain>
Message-ID: <Pine.LNX.4.44.0502040931580.22898-100000@localhost.localdomain>

> for(i in 1:dims[1]) x[i,]<-rev(x[i,]) #flip the image vertically

Courtesy of Rolf Turner, here is  a much better way to flip vertically:

x <- x[,ncol(x):1] 

Bill



From ligges at statistik.uni-dortmund.de  Fri Feb  4 15:39:39 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 04 Feb 2005 15:39:39 +0100
Subject: [R] Bayesian Network
In-Reply-To: <6CC4DC1EC1F92D4B8A5FF590362E756502FA425B@neptune.betd.fr>
References: <6CC4DC1EC1F92D4B8A5FF590362E756502FA425B@neptune.betd.fr>
Message-ID: <4203892B.1000300@statistik.uni-dortmund.de>

NDIKUMAGENGE Alice wrote:

> Hello,
>  
> I would like to use Bayesian Networks with R.
> I have already installed the package called deal which has succefully unpacked (package 'deal' successfully unpacked and MD5 sums checked)
> .
> But when I try to write " <- network (df)
> I have that kind of error message (be low)!
>


Quite certainly you forgot to load the package:

library(deal)

Uwe Ligges



> rats <- network(rats.df)
> Error: couldn't find function "network"
> 
> Thank u for your help
>  
> Alice
> 
>  
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Fri Feb  4 15:38:01 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 4 Feb 2005 09:38:01 -0500
Subject: [R] Bayesian Network
Message-ID: <3A822319EB35174CA3714066D590DCD50994E654@usrymx25.merck.com>

I guess you are using R on Windows, and install the binary version of the
package.  (Please tell us, as the Posting Guide asks, rather than leave us
guessing.)

Did you load the package with `library(deal)' before using the functions?

Andy

> From: NDIKUMAGENGE Alice
> 
> 
> Hello,
>  
> I would like to use Bayesian Networks with R.
> I have already installed the package called deal which has 
> succefully unpacked (package 'deal' successfully unpacked and 
> MD5 sums checked)
> .
> But when I try to write " <- network (df)
> I have that kind of error message (be low)!
>  
> rats <- network(rats.df)
> Error: couldn't find function "network"
> 
> Thank u for your help
>  
> Alice
> 
>  
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From andy_liaw at merck.com  Fri Feb  4 15:42:58 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 4 Feb 2005 09:42:58 -0500
Subject: [R] how to generate a function from a linear model
Message-ID: <3A822319EB35174CA3714066D590DCD50994E655@usrymx25.merck.com>

predict() can do that for you without giving you the explicit form of the
prediction function.  I believe Prof. Harrell has facilities in his
Design/Hmisc packages for producing functions from fitted models.

Andy

> From: Tony Han Bao
> 
> Hi All,
> 
> I am trying to generate a function from a linear model. I think there 
> should be build-in function that perform this action but I've had no 
> luck finding it.
> 
> For example, I have a model created using lm().
> 
> 	model <- lm(sat.d~1+sat.n+I(sat.n^2))
> 
> What I would like to have is a function (similar to the one generated 
> by splinefun()) so that I can use it on different data-sets.
> 
> Thanks in advance for the help.
> 
> Tony Han Bao
> tonybao at mac.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From murdoch at stats.uwo.ca  Fri Feb  4 15:45:35 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 04 Feb 2005 14:45:35 +0000
Subject: [R] Is anyone using the MiniR distribution?
In-Reply-To: <16899.31899.47067.432764@stat.math.ethz.ch>
References: <btg601p2sr5cdsj8fc49qa4t1ssfsqpt6d@4ax.com>
	<16899.31899.47067.432764@stat.math.ethz.ch>
Message-ID: <i22701hrulp6s3oggmrcuq5m41u21ip6g6@4ax.com>

On Fri, 4 Feb 2005 14:46:03 +0100, Martin Maechler
<maechler at stat.math.ethz.ch> wrote :

>
>>>>>> "Duncan" == Duncan Murdoch <murdoch at stats.uwo.ca>
>>>>>>     on Fri, 04 Feb 2005 09:50:22 +0000 writes:

>    Duncan> The miniR files only include a minimal installation
>    Duncan> of R, and are rarely tested.  Rather than building
>    Duncan> something that may not even work, I'd like to stop
>    Duncan> building them.
>
>    Duncan> Would this be a problem for anyone?
>
>
>People for which this is a problem will probably not be
>subscribed to R-help because they'll be living in places /
>situations with bad / expensive internet connection.
>
>(Sorry to be not really constructive here).

That's a good point; I'll put a copy of my question in the miniR
directory on CRAN.  It's not a big problem to build it, but I don't
want to test it if it's not being used.  

Duncan Murdoch



From ligges at statistik.uni-dortmund.de  Fri Feb  4 16:08:46 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 04 Feb 2005 16:08:46 +0100
Subject: [R] QCC and PlotMath question
In-Reply-To: <2DBF8A8E1A1AEE4AB3618AC4D6BF308807209D@houston.tanox.net>
References: <2DBF8A8E1A1AEE4AB3618AC4D6BF308807209D@houston.tanox.net>
Message-ID: <42038FFE.3070804@statistik.uni-dortmund.de>

Shawn Way wrote:

> For some reason, using the qcc package, I'm unable to use the plotmath
> notation in the title.  Can anyone see what I'm doing wrong?
> 
> library(qcc)
> a <- rnorm(100)
> qcc(a,type="xbar.one",title=expression(bar(X)),ylab=expression(CFU/ft^3)
> )
> 
> This seems to not let the expression be evaluated, so I tried:
> 
> qcc(a,type="xbar.one",title=eval(expression(bar(X))),ylab=expression(CFU
> /ft^3))
> 
> And get the following error:
> Error in eval(expr, envir, enclos) : couldn't find function "bar"
> 
> Any thoughts?

Add the title by a separate call:

   qcc(a, type="xbar.one", title="", ylab=expression(CFU/ft^3))
   title(expression(bar(X)))

Uwe Ligges

> -----------------------------------------------------------------
> "Policies are many, Principles are few, Policies will change, Principles
> never do." 
> -John C. Maxwell
> 
> 
> Shawn Way, PE
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri Feb  4 16:39:40 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 04 Feb 2005 16:39:40 +0100
Subject: [R] sink to file
In-Reply-To: <4203825A.6010506@itp.phys.ethz.ch>
References: <4203825A.6010506@itp.phys.ethz.ch>
Message-ID: <4203973C.5030201@statistik.uni-dortmund.de>

Urs Wagner wrote:

> Hello
> 
> I would like to use the source(command)  and write the output into a file.
> I am using
> 
> outputfile=file("output.txt", open="wt")
> sink(outputfile, type="output")
> source("input.R", echo=TRUE)
> 
> Unfortunately the result has prompted commands. How can I avoid the 
> prompted commands data(iris), ...?

By *not* specifying echo=TRUE in source, but print()-ing the summary below.

Uwe Ligges


> Thanks
> 
>  > data(iris)
>  > dataset = iris
>  > options(width = 50)
>  > summary(dataset)
>  Sepal.Length    Sepal.Width     Petal.Length Min.   :4.300   Min.   
> :2.000   Min.   :1.000 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600 
> Median :5.800   Median :3.000   Median :4.350 Mean   :5.843   Mean   
> :3.057   Mean   :3.758 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100 
> Max.   :7.900   Max.   :4.400   Max.   :6.900  Petal.Width          
> Species Min.   :0.100   setosa    :50 1st Qu.:0.300   versicolor:50 
> Median :1.300   virginica :50 Mean   :1.199                 3rd 
> Qu.:1.800                 Max.   :2.500
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From Alan.Jackson at shell.com  Fri Feb  4 16:54:58 2005
From: Alan.Jackson at shell.com (Jackson, Alan AK SIEP-EPT-RXR)
Date: Fri, 4 Feb 2005 09:54:58 -0600
Subject: [R] graphics examples
Message-ID: <A3B4E948E728054CAC7A6D9C55EAB6A3F8420E@houic-s-348.americas.shell.com>

Very nice! I can't wait to buy the book.

I have some plots I am working on that are surprisingly difficult to do :
http://www.oplnk.net/~ajackson/weather/Temperature_2000.png
and others in that directory for an example.
The challenge was coloring in the polygons which were, in some cases, defined by the intersection of four curves, and also required interpolating the bounding curves to those intersection points. I'll post the code on the website tonight.


Alan Jackson
Staff Geophysicist
Shell International Exploration and Production Inc.
3737 Bellaire Blvd, P O Box 481, Houston, Texas 77001-0481, USA

Tel: +0117132457355 none Other Tel: +011-713-245-7355
Email: Alan.Jackson at shell.com
Internet: http://www.shell.com/eandp-en


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of
p.murrell at auckland.ac.nz
Sent: Thursday, February 03, 2005 8:35 PM
To: r-help at stat.math.ethz.ch
Subject: [R] graphics examples


Hi

I have put up some web pages containing a number of plots (and diagrams)
produced using R (they correspond to the figures for a book that I am
working on about R graphics), with the relevant R code provided for each
plot (or diagram), at
http://www.stat.auckland.ac.nz/~paul/RGraphics/rgraphics.html

Hope these are of some help/use;  comments/suggestions welcome.

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From chabotd at globetrotter.net  Fri Feb  4 17:00:46 2005
From: chabotd at globetrotter.net (Denis Chabot)
Date: Fri, 4 Feb 2005 17:00:46 +0100
Subject: [R] 2 small problems: integer division and the nature of NA
Message-ID: <EE306E0B-76C5-11D9-99C3-00050279D82B@globetrotter.net>

Hi,

I'm wondering why

48 %/% 2 gives 24
but
4.8 %/% 0.2 gives 23...
I'm not trying to round up here, but to find out how many times 
something fits into something else, and the answer should have been the 
same for both examples, no?

On a different topic, I like the behavior of NAs better in R than in 
SAS (at least they are not considered the smallest value for a 
variable), but at the same time I am surprised that the sum of NAs is 0 
instead of NA.

The sum of a vector having at least one NA but also valid data gives NA 
if we do not specify na.rm=T. But with na.rm=T, we are telling sum to 
give the sum of valid data, ignoring NAs that do not tell us anything 
about the value of a variable. I found out while getting the sum of 
small subsets of my data (such as when subsetting by several 
variables), sometimes a "cell" only contained NAs for my response 
variable. I would have expected the sum to be NA in such cases, as I do 
not have a single data point telling me the value of my response here. 
But R tells me the sum was zero in that cell! Was this behavior 
considered "desirable" when sum was built? If not, any hope it will be 
fixed?

Sincerely,

Denis Chabot



From depire at inrets.fr  Fri Feb  4 17:02:09 2005
From: depire at inrets.fr (Depire Alexandre)
Date: Fri, 4 Feb 2005 17:02:09 +0100
Subject: [R] Compilation of R (linux) package on windows
Message-ID: <200502041702.09393.depire@inrets.fr>

Hello,
I develop some R package on Linux machine with C subroutines.
The programs in C are well compiled on Linux machine and so I have some ".so" 
files.

Now, I want to do the same work on windows, so I install R (the last version) 
on windows, with Active Perl and djgpp, which is, as I know, the gcc version 
for windows (to compile C program), but unfortunately when I run "R CMD SHLIB 
inv.c, ", I have an error.
I think it's a problem with my choice of compiler C, could somebody give to me 
the name of good compiler to do that ?

-- 
----------------
Alexandre DEPIRE
INRETS / GARIG



From osklyar at ebi.ac.uk  Fri Feb  4 17:10:45 2005
From: osklyar at ebi.ac.uk (Oleg Sklyar)
Date: Fri, 04 Feb 2005 16:10:45 +0000
Subject: [R] Keeping the data of C structure in R variables?..
Message-ID: <42039E85.2040906@ebi.ac.uk>

Dear all,

does anybody know if there is a way to implement the following idea:

if for example I have a C/C++ structure of form:

struct {
    int size;
    char * data;
} SData;

in C code I could create some implementation that would create this 
structure by pointer and fill in the data, so I would have a variable 
something like

SData* myData;

Now what I need is to pass this data to a certain SEXP structure and 
keep it completely in R, thus setting myData = NULL and _unloading the C 
library_; then later I want to create another variable, in another C 
call, SData* myOldData and reload it with values from R. Is there a way 
to do that, keeping also in mind that char* data is generally binary data.

Would be greatful for any suggestions.

Regards
Oleg



From ligges at statistik.uni-dortmund.de  Fri Feb  4 17:32:06 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 04 Feb 2005 17:32:06 +0100
Subject: [R] Compilation of R (linux) package on windows
In-Reply-To: <200502041702.09393.depire@inrets.fr>
References: <200502041702.09393.depire@inrets.fr>
Message-ID: <4203A386.3030000@statistik.uni-dortmund.de>

Depire Alexandre wrote:

> Hello,
> I develop some R package on Linux machine with C subroutines.
> The programs in C are well compiled on Linux machine and so I have some ".so" 
> files.
> 
> Now, I want to do the same work on windows, so I install R (the last version) 
> on windows, with Active Perl and djgpp, which is, as I know, the gcc version 
> for windows (to compile C program), but unfortunately when I run "R CMD SHLIB 
> inv.c, ", I have an error.
> I think it's a problem with my choice of compiler C, could somebody give to me 
> the name of good compiler to do that ?
> 

Please read the R for Windows FAQ 3.1 "Can I install packages into 
libraries in this version?".
It points you to README.packages, 
http://www.murdoch-sutherland.com/Rtools/ , and tells you "Note that 
this is rather tricky; please do ensure that you have followed the 
instructions exactly."

Uwe Ligges



From ligges at statistik.uni-dortmund.de  Fri Feb  4 17:40:06 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 04 Feb 2005 17:40:06 +0100
Subject: [R] 2 small problems: integer division and the nature of NA
In-Reply-To: <EE306E0B-76C5-11D9-99C3-00050279D82B@globetrotter.net>
References: <EE306E0B-76C5-11D9-99C3-00050279D82B@globetrotter.net>
Message-ID: <4203A566.90508@statistik.uni-dortmund.de>

Denis Chabot wrote:

> Hi,
> 
> I'm wondering why
> 
> 48 %/% 2 gives 24
> but
> 4.8 %/% 0.2 gives 23...
> I'm not trying to round up here, but to find out how many times 
> something fits into something else, and the answer should have been the 
> same for both examples, no?

No. Not from the perspective of a digital computer who cannot represent 
all real numbers exactly (well, only a very small subset, since we are 
using floating point arithmetics) ...


> On a different topic, I like the behavior of NAs better in R than in SAS 
> (at least they are not considered the smallest value for a variable), 
> but at the same time I am surprised that the sum of NAs is 0 instead of NA.

It *is* NA:
   sum(c(NA, NA)) # [1] NA
   sum(c(NA, 1))  # [1] NA

> The sum of a vector having at least one NA but also valid data gives NA 
> if we do not specify na.rm=T. But with na.rm=T, we are telling sum to 
> give the sum of valid data, ignoring NAs that do not tell us anything 
> about the value of a variable. I found out while getting the sum of 
> small subsets of my data (such as when subsetting by several variables), 
> sometimes a "cell" only contained NAs for my response variable. I would 
> have expected the sum to be NA in such cases, as I do not have a single 
> data point telling me the value of my response here. But R tells me the 
> sum was zero in that cell! Was this behavior considered "desirable" when 
> sum was built? If not, any hope it will be fixed?

???? I don't get your point!
If you *remove* NAs as in
   sum(c(NA, NA), na.rm=TRUE) # [1] 0
   sum(c(NA, 1), na.rm=TRUE)  # [1] 1
you are summing up not that much.... so what do you expect in the cases 
above?

Please read the docs on NA handling.

Uwe Ligges



> Sincerely,
> 
> Denis Chabot
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Fri Feb  4 17:43:02 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 04 Feb 2005 08:43:02 -0800
Subject: [R] 2 small problems: integer division and the nature of NA
In-Reply-To: <EE306E0B-76C5-11D9-99C3-00050279D82B@globetrotter.net>
References: <EE306E0B-76C5-11D9-99C3-00050279D82B@globetrotter.net>
Message-ID: <4203A616.9030500@pdf.com>

      It's the difference between integers and reals:  48 and 24 are 
integers;  4.8 and 0.2 are floating point numbers.  Consider: 

 > (4.8+.Machine$double.eps) %/% (0.2-.Machine$double.eps)
[1] 24
 > (4.8-.Machine$double.eps) %/% (0.2+.Machine$double.eps)
[1] 23
 >
      Does this help?  spencer graves

Denis Chabot wrote:

> Hi,
>
> I'm wondering why
>
> 48 %/% 2 gives 24
> but
> 4.8 %/% 0.2 gives 23...
> I'm not trying to round up here, but to find out how many times 
> something fits into something else, and the answer should have been 
> the same for both examples, no?
>
> On a different topic, I like the behavior of NAs better in R than in 
> SAS (at least they are not considered the smallest value for a 
> variable), but at the same time I am surprised that the sum of NAs is 
> 0 instead of NA.
>
> The sum of a vector having at least one NA but also valid data gives 
> NA if we do not specify na.rm=T. But with na.rm=T, we are telling sum 
> to give the sum of valid data, ignoring NAs that do not tell us 
> anything about the value of a variable. I found out while getting the 
> sum of small subsets of my data (such as when subsetting by several 
> variables), sometimes a "cell" only contained NAs for my response 
> variable. I would have expected the sum to be NA in such cases, as I 
> do not have a single data point telling me the value of my response 
> here. But R tells me the sum was zero in that cell! Was this behavior 
> considered "desirable" when sum was built? If not, any hope it will be 
> fixed?
>
> Sincerely,
>
> Denis Chabot
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Fri Feb  4 17:42:03 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 04 Feb 2005 17:42:03 +0100
Subject: [R] 2 small problems: integer division and the nature of NA
In-Reply-To: <EE306E0B-76C5-11D9-99C3-00050279D82B@globetrotter.net>
References: <EE306E0B-76C5-11D9-99C3-00050279D82B@globetrotter.net>
Message-ID: <x2wttonup0.fsf@biostat.ku.dk>

Denis Chabot <chabotd at globetrotter.net> writes:

> Hi,
> 
> I'm wondering why
> 
> 48 %/% 2 gives 24
> but
> 4.8 %/% 0.2 gives 23...
> I'm not trying to round up here, but to find out how many times
> something fits into something else, and the answer should have been
> the same for both examples, no?

Well, you can't trust floating point numbers to give you an exact
result:

> 4.8 / 0.2 - 24
[1] -3.552714e-15

and even

> (48/10) / (2/10) - 24
[1] -3.552714e-15

the basic issue being that tenths are not exactly representable in
binary floating point. I think very few people even expected you to
use integer division on non-integers, but I note that the claim on the
help page actually holds:

> 0.2 * 4.8 %/% 0.2  + 4.8 %% 0.2 == 4.8
[1] TRUE

 
> On a different topic, I like the behavior of NAs better in R than in
> SAS (at least they are not considered the smallest value for a
> variable), but at the same time I am surprised that the sum of NAs is
> 0 instead of NA.
> 
> The sum of a vector having at least one NA but also valid data gives
> NA if we do not specify na.rm=T. But with na.rm=T, we are telling sum
> to give the sum of valid data, ignoring NAs that do not tell us
> anything about the value of a variable. I found out while getting the
> sum of small subsets of my data (such as when subsetting by several
> variables), sometimes a "cell" only contained NAs for my response
> variable. I would have expected the sum to be NA in such cases, as I
> do not have a single data point telling me the value of my response
> here. But R tells me the sum was zero in that cell! Was this behavior
> considered "desirable" when sum was built? If not, any hope it will be
> fixed?

Yes it was, and no there isn't. In math, the sum over an empty index
set is zero, which has some nice consistency properties (the sum over
a disjoint union of sets is the sum of the sums over each set, for
instance. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From reid_huntsinger at merck.com  Fri Feb  4 17:45:58 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Fri, 4 Feb 2005 11:45:58 -0500
Subject: [R] Keeping the data of C structure in R variables?..
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A92AB@uswpmx00.merck.com>

I think you should have a look at external pointers (type EXTPTRSXP). They
are used in the R source . See, for example, memory.c. Also see the
developer page notes on weak references, finalizers, etc, which you'll need
to be familiar with. 

This is really an R-devel question!

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Oleg Sklyar
Sent: Friday, February 04, 2005 11:11 AM
To: R-help at stat.math.ethz.ch
Subject: [R] Keeping the data of C structure in R variables?..


Dear all,

does anybody know if there is a way to implement the following idea:

if for example I have a C/C++ structure of form:

struct {
    int size;
    char * data;
} SData;

in C code I could create some implementation that would create this 
structure by pointer and fill in the data, so I would have a variable 
something like

SData* myData;

Now what I need is to pass this data to a certain SEXP structure and 
keep it completely in R, thus setting myData = NULL and _unloading the C 
library_; then later I want to create another variable, in another C 
call, SData* myOldData and reload it with values from R. Is there a way 
to do that, keeping also in mind that char* data is generally binary data.

Would be greatful for any suggestions.

Regards
Oleg

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From reid_huntsinger at merck.com  Fri Feb  4 17:50:03 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Fri, 4 Feb 2005 11:50:03 -0500
Subject: [R] 2 small problems: integer division and the nature of NA
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A92AC@uswpmx00.merck.com>

It's convention in mathematics that the empty sum is 0. You can think of
this as a generalization of 0*x = 0. 

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Denis Chabot
Sent: Friday, February 04, 2005 11:01 AM
To: r-help at stat.math.ethz.ch
Subject: [R] 2 small problems: integer division and the nature of NA


Hi,

I'm wondering why

48 %/% 2 gives 24
but
4.8 %/% 0.2 gives 23...
I'm not trying to round up here, but to find out how many times 
something fits into something else, and the answer should have been the 
same for both examples, no?

On a different topic, I like the behavior of NAs better in R than in 
SAS (at least they are not considered the smallest value for a 
variable), but at the same time I am surprised that the sum of NAs is 0 
instead of NA.

The sum of a vector having at least one NA but also valid data gives NA 
if we do not specify na.rm=T. But with na.rm=T, we are telling sum to 
give the sum of valid data, ignoring NAs that do not tell us anything 
about the value of a variable. I found out while getting the sum of 
small subsets of my data (such as when subsetting by several 
variables), sometimes a "cell" only contained NAs for my response 
variable. I would have expected the sum to be NA in such cases, as I do 
not have a single data point telling me the value of my response here. 
But R tells me the sum was zero in that cell! Was this behavior 
considered "desirable" when sum was built? If not, any hope it will be 
fixed?

Sincerely,

Denis Chabot

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From helprhelp at gmail.com  Fri Feb  4 18:01:06 2005
From: helprhelp at gmail.com (WeiWei Shi)
Date: Fri, 4 Feb 2005 11:01:06 -0600
Subject: [R] genetic algorithm
Message-ID: <cdf8178305020409011c940d51@mail.gmail.com>

Hi, 
I am doing some research on feature selection for classfication
problem using genetic algorithm in a wrapper approach. I am wondering
if there is some package which is already built for this purpose. I
was advised before about dprep package but I don't think it used GA
there (if I am wrong, please correct me!)

Thanks,

Ed



From ripley at stats.ox.ac.uk  Fri Feb  4 18:37:08 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 4 Feb 2005 17:37:08 +0000 (GMT)
Subject: [R] Compilation of R (linux) package on windows
In-Reply-To: <4203A386.3030000@statistik.uni-dortmund.de>
References: <200502041702.09393.depire@inrets.fr>
	<4203A386.3030000@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.61.0502041733030.28939@gannet.stats>

On Fri, 4 Feb 2005, Uwe Ligges wrote:

> Depire Alexandre wrote:
>
>> Hello,
>> I develop some R package on Linux machine with C subroutines.
>> The programs in C are well compiled on Linux machine and so I have some 
>> ".so" files.
>> 
>> Now, I want to do the same work on windows, so I install R (the last 
>> version) on windows, with Active Perl and djgpp, which is, as I know, the 
>> gcc version for windows (to compile C program), but unfortunately when I 
>> run "R CMD SHLIB inv.c, ", I have an error.
>> I think it's a problem with my choice of compiler C, could somebody give to 
>> me the name of good compiler to do that ?
>> 
>
> Please read the R for Windows FAQ 3.1 "Can I install packages into libraries 
> in this version?".
> It points you to README.packages, http://www.murdoch-sutherland.com/Rtools/ , 
> and tells you "Note that this is rather tricky; please do ensure that you 
> have followed the instructions exactly."

To reinforce that, djgpp is a DOS (extender) and not a Windows compiler.
You need a native Windows compiler, from www.mingw.org, and currently we 
suggest the release candidate of MinGW-3.2.0 (which postdates the details 
in the last release of R, 2.0.1).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From depire at inrets.fr  Fri Feb  4 19:01:33 2005
From: depire at inrets.fr (Depire Alexandre)
Date: Fri, 4 Feb 2005 19:01:33 +0100
Subject: [R] Compilation of R (linux) package on windows
In-Reply-To: <Pine.LNX.4.61.0502041733030.28939@gannet.stats>
References: <200502041702.09393.depire@inrets.fr>
	<4203A386.3030000@statistik.uni-dortmund.de>
	<Pine.LNX.4.61.0502041733030.28939@gannet.stats>
Message-ID: <200502041901.33620.depire@inrets.fr>

On windows, I install the last version of MinGW, I change path environment 
variable,
but when on command windows, I try to compute "R CMD SHLIB inv.c" I have the 
following error:
'make' is unknown.

I have "mingw32-make.exe", but R don't use it, ?? I think it is'nt normal, but 
I don't know how change the name of it in R.



Le vendredi 4 F?vrier 2005 18:37, Prof Brian Ripley a ?crit :
> On Fri, 4 Feb 2005, Uwe Ligges wrote:
> > Depire Alexandre wrote:
> >> Hello,
> >> I develop some R package on Linux machine with C subroutines.
> >> The programs in C are well compiled on Linux machine and so I have some
> >> ".so" files.
> >>
> >> Now, I want to do the same work on windows, so I install R (the last
> >> version) on windows, with Active Perl and djgpp, which is, as I know,
> >> the gcc version for windows (to compile C program), but unfortunately
> >> when I run "R CMD SHLIB inv.c, ", I have an error.
> >> I think it's a problem with my choice of compiler C, could somebody give
> >> to me the name of good compiler to do that ?
> >
> > Please read the R for Windows FAQ 3.1 "Can I install packages into
> > libraries in this version?".
> > It points you to README.packages,
> > http://www.murdoch-sutherland.com/Rtools/ , and tells you "Note that this
> > is rather tricky; please do ensure that you have followed the
> > instructions exactly."
>
> To reinforce that, djgpp is a DOS (extender) and not a Windows compiler.
> You need a native Windows compiler, from www.mingw.org, and currently we
> suggest the release candidate of MinGW-3.2.0 (which postdates the details
> in the last release of R, 2.0.1).

-- 
----------------
Alexandre DEPIRE
INRETS / GARIG



From depire at inrets.fr  Fri Feb  4 19:03:22 2005
From: depire at inrets.fr (Depire Alexandre)
Date: Fri, 4 Feb 2005 19:03:22 +0100
Subject: [R] Compilation of R (linux) package on windows
In-Reply-To: <Pine.LNX.4.61.0502041733030.28939@gannet.stats>
References: <200502041702.09393.depire@inrets.fr>
	<4203A386.3030000@statistik.uni-dortmund.de>
	<Pine.LNX.4.61.0502041733030.28939@gannet.stats>
Message-ID: <200502041903.22582.depire@inrets.fr>

Is it easier to compute .dll on linux, via cross-compiler ?

Le vendredi 4 F?vrier 2005 18:37, Prof Brian Ripley a ?crit :
> On Fri, 4 Feb 2005, Uwe Ligges wrote:
> > Depire Alexandre wrote:
> >> Hello,
> >> I develop some R package on Linux machine with C subroutines.
> >> The programs in C are well compiled on Linux machine and so I have some
> >> ".so" files.
> >>
> >> Now, I want to do the same work on windows, so I install R (the last
> >> version) on windows, with Active Perl and djgpp, which is, as I know,
> >> the gcc version for windows (to compile C program), but unfortunately
> >> when I run "R CMD SHLIB inv.c, ", I have an error.
> >> I think it's a problem with my choice of compiler C, could somebody give
> >> to me the name of good compiler to do that ?
> >
> > Please read the R for Windows FAQ 3.1 "Can I install packages into
> > libraries in this version?".
> > It points you to README.packages,
> > http://www.murdoch-sutherland.com/Rtools/ , and tells you "Note that this
> > is rather tricky; please do ensure that you have followed the
> > instructions exactly."
>
> To reinforce that, djgpp is a DOS (extender) and not a Windows compiler.
> You need a native Windows compiler, from www.mingw.org, and currently we
> suggest the release candidate of MinGW-3.2.0 (which postdates the details
> in the last release of R, 2.0.1).

-- 
----------------
Alexandre DEPIRE
INRETS / GARIG



From andy_liaw at merck.com  Fri Feb  4 19:12:44 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 4 Feb 2005 13:12:44 -0500
Subject: [R] Compilation of R (linux) package on windows
Message-ID: <3A822319EB35174CA3714066D590DCD50994E65A@usrymx25.merck.com>

> From: Depire Alexandre
> 
> On windows, I install the last version of MinGW, I change 
> path environment 
> variable,
> but when on command windows, I try to compute "R CMD SHLIB 
> inv.c" I have the 
> following error:
> 'make' is unknown.
> 
> I have "mingw32-make.exe", but R don't use it, ?? I think it 
> is'nt normal, but 
> I don't know how change the name of it in R.
> 

Confusion is the price you pay for not following the directions given.  You
need to download and install the  tools in Rtools.zip as mentioned in
README.packages, and have it the the appropriate position in the PATH, as
also mentioned in README.packages.

Andy

 
> 
> Le vendredi 4 F?vrier 2005 18:37, Prof Brian Ripley a ?crit :
> > On Fri, 4 Feb 2005, Uwe Ligges wrote:
> > > Depire Alexandre wrote:
> > >> Hello,
> > >> I develop some R package on Linux machine with C subroutines.
> > >> The programs in C are well compiled on Linux machine and 
> so I have some
> > >> ".so" files.
> > >>
> > >> Now, I want to do the same work on windows, so I install 
> R (the last
> > >> version) on windows, with Active Perl and djgpp, which 
> is, as I know,
> > >> the gcc version for windows (to compile C program), but 
> unfortunately
> > >> when I run "R CMD SHLIB inv.c, ", I have an error.
> > >> I think it's a problem with my choice of compiler C, 
> could somebody give
> > >> to me the name of good compiler to do that ?
> > >
> > > Please read the R for Windows FAQ 3.1 "Can I install packages into
> > > libraries in this version?".
> > > It points you to README.packages,
> > > http://www.murdoch-sutherland.com/Rtools/ , and tells you 
> "Note that this
> > > is rather tricky; please do ensure that you have followed the
> > > instructions exactly."
> >
> > To reinforce that, djgpp is a DOS (extender) and not a 
> Windows compiler.
> > You need a native Windows compiler, from www.mingw.org, and 
> currently we
> > suggest the release candidate of MinGW-3.2.0 (which 
> postdates the details
> > in the last release of R, 2.0.1).
> 
> -- 
> ----------------
> Alexandre DEPIRE
> INRETS / GARIG
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From tlumley at u.washington.edu  Fri Feb  4 19:17:40 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 4 Feb 2005 10:17:40 -0800 (PST)
Subject: [R] sink to file
In-Reply-To: <4203973C.5030201@statistik.uni-dortmund.de>
References: <4203825A.6010506@itp.phys.ethz.ch>
	<4203973C.5030201@statistik.uni-dortmund.de>
Message-ID: <Pine.A41.4.61b.0502041017000.351236@homer11.u.washington.edu>

On Fri, 4 Feb 2005, Uwe Ligges wrote:

> Urs Wagner wrote:
>
>> Hello
>> 
>> I would like to use the source(command)  and write the output into a file.
>> I am using
>> 
>> outputfile=file("output.txt", open="wt")
>> sink(outputfile, type="output")
>> source("input.R", echo=TRUE)
>> 
>> Unfortunately the result has prompted commands. How can I avoid the 
>> prompted commands data(iris), ...?
>
> By *not* specifying echo=TRUE in source, but print()-ing the summary below.
>

There is also a print.eval= argument to source(), so that printing of the 
output can be controlled independently of echoing the input.

 	-thomas



From dr.mike at ntlworld.com  Fri Feb  4 19:20:42 2005
From: dr.mike at ntlworld.com (dr mike)
Date: Fri, 4 Feb 2005 18:20:42 -0000
Subject: [R] genetic algorithm
In-Reply-To: <cdf8178305020409011c940d51@mail.gmail.com>
Message-ID: <20050204182100.LTAA769.aamta05-winn.mailhost.ntl.com@c400>

To my knowledge, two packages have an implementation of an evolutionary, or
genentic, algorithm. Gafit is a curve fitting package and rgenoud for
function minimisation (combined with, iirc, a derivative-based Quasi-Newton
approach for unconstrained problems). One thing, in the S-Plus robust
library, the robust regression package lmRob has an option to use a genetic
algorithm in the resampling scheme to obtain initial S-estimates.

Regards

Mike

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of WeiWei Shi
Sent: 04 February 2005 17:01
To: R-help at stat.math.ethz.ch
Subject: [R] genetic algorithm

Hi,
I am doing some research on feature selection for classfication problem
using genetic algorithm in a wrapper approach. I am wondering if there is
some package which is already built for this purpose. I was advised before
about dprep package but I don't think it used GA there (if I am wrong,
please correct me!)

Thanks,

Ed

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri Feb  4 19:26:17 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 04 Feb 2005 19:26:17 +0100
Subject: [R] Compilation of R (linux) package on windows
In-Reply-To: <200502041903.22582.depire@inrets.fr>
References: <200502041702.09393.depire@inrets.fr>	<4203A386.3030000@statistik.uni-dortmund.de>	<Pine.LNX.4.61.0502041733030.28939@gannet.stats>
	<200502041903.22582.depire@inrets.fr>
Message-ID: <4203BE49.8030401@statistik.uni-dortmund.de>

Depire Alexandre wrote:

> Is it easier to compute .dll on linux, via cross-compiler ?

The Windows way described in README.packages is not hard, you just have 
to follow the advices. Once your system has been set up, it's the same 
as native compiling on Linux.

Uwe Ligges


> Le vendredi 4 F?vrier 2005 18:37, Prof Brian Ripley a ?crit :
> 
>>On Fri, 4 Feb 2005, Uwe Ligges wrote:
>>
>>>Depire Alexandre wrote:
>>>
>>>>Hello,
>>>>I develop some R package on Linux machine with C subroutines.
>>>>The programs in C are well compiled on Linux machine and so I have some
>>>>".so" files.
>>>>
>>>>Now, I want to do the same work on windows, so I install R (the last
>>>>version) on windows, with Active Perl and djgpp, which is, as I know,
>>>>the gcc version for windows (to compile C program), but unfortunately
>>>>when I run "R CMD SHLIB inv.c, ", I have an error.
>>>>I think it's a problem with my choice of compiler C, could somebody give
>>>>to me the name of good compiler to do that ?
>>>
>>>Please read the R for Windows FAQ 3.1 "Can I install packages into
>>>libraries in this version?".
>>>It points you to README.packages,
>>>http://www.murdoch-sutherland.com/Rtools/ , and tells you "Note that this
>>>is rather tricky; please do ensure that you have followed the
>>>instructions exactly."
>>
>>To reinforce that, djgpp is a DOS (extender) and not a Windows compiler.
>>You need a native Windows compiler, from www.mingw.org, and currently we
>>suggest the release candidate of MinGW-3.2.0 (which postdates the details
>>in the last release of R, 2.0.1).
> 
>



From ligges at statistik.uni-dortmund.de  Fri Feb  4 19:27:53 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 04 Feb 2005 19:27:53 +0100
Subject: [R] Compilation of R (linux) package on windows
In-Reply-To: <200502041901.33620.depire@inrets.fr>
References: <200502041702.09393.depire@inrets.fr>	<4203A386.3030000@statistik.uni-dortmund.de>	<Pine.LNX.4.61.0502041733030.28939@gannet.stats>
	<200502041901.33620.depire@inrets.fr>
Message-ID: <4203BEA9.6040300@statistik.uni-dortmund.de>

Depire Alexandre wrote:

> On windows, I install the last version of MinGW, I change path environment 
> variable,
> but when on command windows, I try to compute "R CMD SHLIB inv.c" I have the 
> following error:
> 'make' is unknown.
> 
> I have "mingw32-make.exe", but R don't use it, ?? I think it is'nt normal, but 
> I don't know how change the name of it in R.


I already quoted:
"please do ensure that you have followed the instructions exactly."

But you haven't!!!!

At least you have the tools from 
http://www.murdoch-sutherland.com/Rtools/ either not downloaded or not 
in your path.

Uwe Ligges



> 
> 
> Le vendredi 4 F?vrier 2005 18:37, Prof Brian Ripley a ?crit :
> 
>>On Fri, 4 Feb 2005, Uwe Ligges wrote:
>>
>>>Depire Alexandre wrote:
>>>
>>>>Hello,
>>>>I develop some R package on Linux machine with C subroutines.
>>>>The programs in C are well compiled on Linux machine and so I have some
>>>>".so" files.
>>>>
>>>>Now, I want to do the same work on windows, so I install R (the last
>>>>version) on windows, with Active Perl and djgpp, which is, as I know,
>>>>the gcc version for windows (to compile C program), but unfortunately
>>>>when I run "R CMD SHLIB inv.c, ", I have an error.
>>>>I think it's a problem with my choice of compiler C, could somebody give
>>>>to me the name of good compiler to do that ?
>>>
>>>Please read the R for Windows FAQ 3.1 "Can I install packages into
>>>libraries in this version?".
>>>It points you to README.packages,
>>>http://www.murdoch-sutherland.com/Rtools/ , and tells you "Note that this
>>>is rather tricky; please do ensure that you have followed the
>>>instructions exactly."
>>
>>To reinforce that, djgpp is a DOS (extender) and not a Windows compiler.
>>You need a native Windows compiler, from www.mingw.org, and currently we
>>suggest the release candidate of MinGW-3.2.0 (which postdates the details
>>in the last release of R, 2.0.1).
> 
>



From HDoran at air.org  Fri Feb  4 19:36:28 2005
From: HDoran at air.org (Doran, Harold)
Date: Fri, 4 Feb 2005 13:36:28 -0500
Subject: [R] Building a Matrix
Message-ID: <88EAF3512A55DF46B06B1954AEF73F74079A436E@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050204/cc13aaaf/attachment.pl

From andy_liaw at merck.com  Fri Feb  4 20:05:29 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 4 Feb 2005 14:05:29 -0500
Subject: [R] Building a Matrix
Message-ID: <3A822319EB35174CA3714066D590DCD50994E65B@usrymx25.merck.com>

Is this what you want?

> mat <- matrix(0, 4, 4)
> mat[col(mat) > 2 & row(mat) > 2] <- 100
> mat
     [,1] [,2] [,3] [,4]
[1,]    0    0    0    0
[2,]    0    0    0    0
[3,]    0    0  100  100
[4,]    0    0  100  100

Andy

> From: Doran, Harold
> 
> Dear List:
> 
> I am having some difficulty constructing a matrix that must take a
> specific form. The matrix must be have a lower block of 
> non-zero values
> and the rest must all be zero. For example, if I am building an n X n
> matrix, then the first n/2 rows need to be zero and the first n/2
> columns must remain as zero with all other elements having a non-zero
> value that I specify.
> 
> For example, assume I start with the following 4 x 4 matrix:
> 
> vl.mat <- matrix(0,4,4)
> 
>      [,1] [,2] [,3] [,4]
> [1,]    0    0    0    0
> [2,]    0    0    0    0
> [3,]    0    0    0    0
> [4,]    0    0    0    0
> 
> I need for the for the first two columns (4/2) to remain as 
> zero and the
> first two rows (4/2) to remain as zeros. But I need for the 
> bottom block
> to include some values that I specify.
> 
>      [,1] [,2] [,3] [,4]
> [1,]    0    0    0    0
> [2,]    0    0    0    0
> [3,]    0    0  100  100
> [4,]    0    0  100  100
> 
> I know that if I use the following I can build a matrix with values
> along the diagonals where I need them, but I need for the 
> lower block of
> off-diagonals to also be the same value.
> 
> vl.mat <- matrix(0,4,4)
> vl.mat[(col(vl.mat)%%4 == row(vl.mat)%%4)&col(vl.mat)%%4 !=1
> &col(vl.mat)%%4 !=2  ] <- 100
> 
> Can anyone offer a suggestion? Thank you.
> 
> -Harold
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From SoukupM at cder.fda.gov  Fri Feb  4 20:07:09 2005
From: SoukupM at cder.fda.gov (Soukup, Matt)
Date: Fri, 4 Feb 2005 14:07:09 -0500
Subject: [R] (no subject)
Message-ID: <EE8F300994F94144856B1EC54D0891F8B92ECE@cdsx06.cder.fda.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050204/07104710/attachment.pl

From spencer.graves at pdf.com  Fri Feb  4 20:13:59 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 04 Feb 2005 11:13:59 -0800
Subject: [R] Building a Matrix
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F74079A436E@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F74079A436E@dc1ex2.air.org>
Message-ID: <4203C977.5040600@pdf.com>

      Does the following do what you want: 

 > vl.mat <- matrix(0,4,4)
 > i34 <- 3:4
 > vl.mat[i34,i34] <- 100
 > vl.mat
     [,1] [,2] [,3] [,4]
[1,]    0    0    0    0
[2,]    0    0    0    0
[3,]    0    0  100  100
[4,]    0    0  100  100
 >
      spencer graves

Doran, Harold wrote:

>Dear List:
>
>I am having some difficulty constructing a matrix that must take a
>specific form. The matrix must be have a lower block of non-zero values
>and the rest must all be zero. For example, if I am building an n X n
>matrix, then the first n/2 rows need to be zero and the first n/2
>columns must remain as zero with all other elements having a non-zero
>value that I specify.
>
>For example, assume I start with the following 4 x 4 matrix:
>
>vl.mat <- matrix(0,4,4)
>
>     [,1] [,2] [,3] [,4]
>[1,]    0    0    0    0
>[2,]    0    0    0    0
>[3,]    0    0    0    0
>[4,]    0    0    0    0
>
>I need for the for the first two columns (4/2) to remain as zero and the
>first two rows (4/2) to remain as zeros. But I need for the bottom block
>to include some values that I specify.
>
>     [,1] [,2] [,3] [,4]
>[1,]    0    0    0    0
>[2,]    0    0    0    0
>[3,]    0    0  100  100
>[4,]    0    0  100  100
>
>I know that if I use the following I can build a matrix with values
>along the diagonals where I need them, but I need for the lower block of
>off-diagonals to also be the same value.
>
>vl.mat <- matrix(0,4,4)
>vl.mat[(col(vl.mat)%%4 == row(vl.mat)%%4)&col(vl.mat)%%4 !=1
>&col(vl.mat)%%4 !=2  ] <- 100
>
>Can anyone offer a suggestion? Thank you.
>
>-Harold
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From pburns at pburns.seanet.com  Fri Feb  4 20:28:44 2005
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Fri, 04 Feb 2005 19:28:44 +0000
Subject: [R] Building a Matrix
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F74079A436E@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F74079A436E@dc1ex2.air.org>
Message-ID: <4203CCEC.6010609@pburns.seanet.com>

Unless I'm missing something, all you need to do is create the
large matrix and then replace the submatrix via subscripting
the rows and columns:

ans <- matrix(0, n, n)
sub.seq <- floor(n/2 + 1):n
ans[sub.seq, sub.seq] <- submatrix


Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Doran, Harold wrote:

>Dear List:
>
>I am having some difficulty constructing a matrix that must take a
>specific form. The matrix must be have a lower block of non-zero values
>and the rest must all be zero. For example, if I am building an n X n
>matrix, then the first n/2 rows need to be zero and the first n/2
>columns must remain as zero with all other elements having a non-zero
>value that I specify.
>
>For example, assume I start with the following 4 x 4 matrix:
>
>vl.mat <- matrix(0,4,4)
>
>     [,1] [,2] [,3] [,4]
>[1,]    0    0    0    0
>[2,]    0    0    0    0
>[3,]    0    0    0    0
>[4,]    0    0    0    0
>
>I need for the for the first two columns (4/2) to remain as zero and the
>first two rows (4/2) to remain as zeros. But I need for the bottom block
>to include some values that I specify.
>
>     [,1] [,2] [,3] [,4]
>[1,]    0    0    0    0
>[2,]    0    0    0    0
>[3,]    0    0  100  100
>[4,]    0    0  100  100
>
>I know that if I use the following I can build a matrix with values
>along the diagonals where I need them, but I need for the lower block of
>off-diagonals to also be the same value.
>
>vl.mat <- matrix(0,4,4)
>vl.mat[(col(vl.mat)%%4 == row(vl.mat)%%4)&col(vl.mat)%%4 !=1
>&col(vl.mat)%%4 !=2  ] <- 100
>
>Can anyone offer a suggestion? Thank you.
>
>-Harold
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>  
>



From jbdunsmo at utmb.edu  Fri Feb  4 20:37:33 2005
From: jbdunsmo at utmb.edu (jbdunsmo@utmb.edu)
Date: Fri, 4 Feb 2005 13:37:33 -0600
Subject: [R] simple example of C interface to R
Message-ID: <20050204193733.GO8123@g20458.utmb.edu>

i'd like to use the C interface to R in a program i'm writing.  as a
starting point, i'm trying to create a very simple C program that uses
R.  i've read the R documentation on this, but i'm having trouble
figuring out where SEXP is defined and how to use it.

i noticed someone else on this list also tried to use the C interface,
but they ran into similar problems:
http://maths.newcastle.edu.au/~rking/R/help/03b/1942.html

could someone show me a simple example of how to use the R interface
to C?

thank you,
jason dunsmore



From nair at sdsc.edu  Fri Feb  4 20:41:35 2005
From: nair at sdsc.edu (T. Murlidharan Nair)
Date: Fri, 04 Feb 2005 11:41:35 -0800
Subject: [R] proportional chance criteria
Message-ID: <4203CFEF.3020701@sdsc.edu>

Is there an R function that I can use to calculate the p-value from the
Z statistics computed for the relationship between chance and observed
proportions in predictions. More sprcifically I am refering to proportional
chance criteria (Cpro). Details are in Huberty's book on Applied 
discriminant
analysis, unfortunately our library has misplaced the book.   I got some 
details from the
following page 
 http://marketing.byu.edu/htmlpages/tutorials/discriminant.htm
but I still need to compute the p-val from their Z-statistic.
Thanks ../Murli



From ggrothendieck at myway.com  Fri Feb  4 20:48:44 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 4 Feb 2005 19:48:44 +0000 (UTC)
Subject: [R] 2 small problems: integer division and the nature of NA
References: <EE306E0B-76C5-11D9-99C3-00050279D82B@globetrotter.net>
Message-ID: <loom.20050204T204346-668@post.gmane.org>

Denis Chabot <chabotd <at> globetrotter.net> writes:
: The sum of a vector having at least one NA but also valid data gives NA 
: if we do not specify na.rm=T. But with na.rm=T, we are telling sum to 
: give the sum of valid data, ignoring NAs that do not tell us anything 
: about the value of a variable. I found out while getting the sum of 
: small subsets of my data (such as when subsetting by several 
: variables), sometimes a "cell" only contained NAs for my response 
: variable. I would have expected the sum to be NA in such cases, as I do 
: not have a single data point telling me the value of my response here. 
: But R tells me the sum was zero in that cell! Was this behavior 
: considered "desirable" when sum was built? If not, any hope it will be 
: fixed?

Think of it this way: If u and v are index vectors then its desirable that

	sum(x[u]) + sum(x[v]) == sum(x[c(u,v)])

hold for zero length index vectors too in which case
sum(numeric()) should be zero, not NA.

If you want a short expression that gives NA for zero length x try this:

        sum(x) + if (length(x)) 0 else NA

or define your own function, sum0, say.



From andy_liaw at merck.com  Fri Feb  4 20:56:02 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 4 Feb 2005 14:56:02 -0500
Subject: [R] simple example of C interface to R
Message-ID: <3A822319EB35174CA3714066D590DCD50994E65D@usrymx25.merck.com>

See if this helps:
http://www.ci.tuwien.ac.at/Conferences/useR-2004/Keynotes/Dalgaard.pdf

Andy

> From: jbdunsmo at utmb.edu
> 
> 
> i'd like to use the C interface to R in a program i'm writing.  as a
> starting point, i'm trying to create a very simple C program that uses
> R.  i've read the R documentation on this, but i'm having trouble
> figuring out where SEXP is defined and how to use it.
> 
> i noticed someone else on this list also tried to use the C interface,
> but they ran into similar problems:
> http://maths.newcastle.edu.au/~rking/R/help/03b/1942.html
> 
> could someone show me a simple example of how to use the R interface
> to C?
> 
> thank you,
> jason dunsmore
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From rdsp5 at hotmail.com  Fri Feb  4 21:05:04 2005
From: rdsp5 at hotmail.com (Ruben Solares)
Date: Fri, 4 Feb 2005 20:05:04 +0000 (GMT)
Subject: [R] Birthday Calendar
Message-ID: <29059401.1107547504165.JavaMail.Administrator@win01>

Hi 

Please click (or copy and paste) the link below and enter your birthday into my calendar. It's quick, easy and you'll be helping me out:-). 

http://www.BirthdayAlarm.com/bd1/35687247a683219066b454557572c994417339d904 

Ruben



From arrayprofile at yahoo.com  Fri Feb  4 21:01:33 2005
From: arrayprofile at yahoo.com (array chip)
Date: Fri, 4 Feb 2005 12:01:33 -0800 (PST)
Subject: [R] no. at risk in survfit()
Message-ID: <20050204200133.91440.qmail@web40812.mail.yahoo.com>

Hi,

when I generated a survfit() object, I can get number
of patients at risk at various time points by using
summary():

fit<-survfit(Surv(time,status)~class,data=mtdata)
summary(fit)

                    class=1
 time n.risk n.event survival std.err lower 95% CI
upper 95% CI 
  9.9     78       1    0.987  0.0127  0.963     1
 41.5     77       1    0.974  0.0179  0.940     1
 54.0     76       1    0.962  0.0218  0.920     1
 99.1     38       1    0.936  0.0328  0.874     1

                   class=2
  time n.risk n.event survival std.err lower 95% CI
upper 95% CI 
   6.9    102       1    0.990 0.00976 0.971   1.000
   8.0    101       1    0.980 0.01373 0.954   1.000
  14.4    100       1    0.971 0.01673 0.938   1.000
  16.1     99       1    0.961 0.01922 0.924   0.999
  16.6     98       1    0.951 0.02138 0.910   0.994
  18.7     97       1    0.941 0.02330 0.897   0.988
   :
   :
   :

I have many censoring observations in the dataset, and
I would like to know the number of patients at risk
(n.risk in the above output) for certain time points,
for example at 60, 72, etc, which is not available
from the above printout for class=1. Is there anyway I
can get them?

Thanks



From Roger.Bivand at nhh.no  Fri Feb  4 21:09:37 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 4 Feb 2005 21:09:37 +0100 (CET)
Subject: [R] simple example of C interface to R
In-Reply-To: <20050204193733.GO8123@g20458.utmb.edu>
Message-ID: <Pine.LNX.4.44.0502042106000.6223-100000@reclus.nhh.no>

On Fri, 4 Feb 2005 jbdunsmo at utmb.edu wrote:

> i'd like to use the C interface to R in a program i'm writing.  as a
> starting point, i'm trying to create a very simple C program that uses
> R.  i've read the R documentation on this, but i'm having trouble
> figuring out where SEXP is defined and how to use it.
> 
> i noticed someone else on this list also tried to use the C interface,
> but they ran into similar problems:
> http://maths.newcastle.edu.au/~rking/R/help/03b/1942.html
> 
> could someone show me a simple example of how to use the R interface
> to C?
> 

Well, it is documented in the Writing R Extensions manual:

http://cran.r-project.org/doc/manuals/R-exts.html#System-and-foreign-language-interfaces

and in the source code of many contributed packages - they may give you 
good examples - look in their src/ directories. But DO read the manual, 
please, that's what it is for!

> thank you,
> jason dunsmore
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From f.harrell at vanderbilt.edu  Fri Feb  4 21:16:14 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 04 Feb 2005 15:16:14 -0500
Subject: [R] no. at risk in survfit()
In-Reply-To: <20050204200133.91440.qmail@web40812.mail.yahoo.com>
References: <20050204200133.91440.qmail@web40812.mail.yahoo.com>
Message-ID: <4203D80E.8010605@vanderbilt.edu>

array chip wrote:
> Hi,
> 
> when I generated a survfit() object, I can get number
> of patients at risk at various time points by using
> summary():
> 
> fit<-survfit(Surv(time,status)~class,data=mtdata)
> summary(fit)
> 
>                     class=1
>  time n.risk n.event survival std.err lower 95% CI
> upper 95% CI 
>   9.9     78       1    0.987  0.0127  0.963     1
>  41.5     77       1    0.974  0.0179  0.940     1
>  54.0     76       1    0.962  0.0218  0.920     1
>  99.1     38       1    0.936  0.0328  0.874     1
> 
>                    class=2
>   time n.risk n.event survival std.err lower 95% CI
> upper 95% CI 
>    6.9    102       1    0.990 0.00976 0.971   1.000
>    8.0    101       1    0.980 0.01373 0.954   1.000
>   14.4    100       1    0.971 0.01673 0.938   1.000
>   16.1     99       1    0.961 0.01922 0.924   0.999
>   16.6     98       1    0.951 0.02138 0.910   0.994
>   18.7     97       1    0.941 0.02330 0.897   0.988
>    :
>    :
>    :
> 
> I have many censoring observations in the dataset, and
> I would like to know the number of patients at risk
> (n.risk in the above output) for certain time points,
> for example at 60, 72, etc, which is not available
> from the above printout for class=1. Is there anyway I
> can get them?
> 
> Thanks

The Design package's survplot function can print n.risk over equally 
spaced time points.  You might see an easy way to print this by looking 
at the code.  -Frank

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From maustin at amgen.com  Fri Feb  4 21:21:46 2005
From: maustin at amgen.com (Austin, Matt)
Date: Fri, 4 Feb 2005 12:21:46 -0800 
Subject: [R] no. at risk in survfit()
Message-ID: <E7D5AB4811D20B489622AABA9C53859104E0DE86@teal-exch.amgen.com>

Have you looked at the times argument to the summary method?

--Matt


Matt Austin
Statistician

Amgen 
One Amgen Center Drive
M/S 24-2-C
Thousand Oaks CA 93021
(805) 447 - 7431


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of array chip
> Sent: Friday, February 04, 2005 12:2 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] no. at risk in survfit()
> 
> 
> Hi,
> 
> when I generated a survfit() object, I can get number
> of patients at risk at various time points by using
> summary():
> 
> fit<-survfit(Surv(time,status)~class,data=mtdata)
> summary(fit)
> 
>                     class=1
>  time n.risk n.event survival std.err lower 95% CI
> upper 95% CI 
>   9.9     78       1    0.987  0.0127  0.963     1
>  41.5     77       1    0.974  0.0179  0.940     1
>  54.0     76       1    0.962  0.0218  0.920     1
>  99.1     38       1    0.936  0.0328  0.874     1
> 
>                    class=2
>   time n.risk n.event survival std.err lower 95% CI
> upper 95% CI 
>    6.9    102       1    0.990 0.00976 0.971   1.000
>    8.0    101       1    0.980 0.01373 0.954   1.000
>   14.4    100       1    0.971 0.01673 0.938   1.000
>   16.1     99       1    0.961 0.01922 0.924   0.999
>   16.6     98       1    0.951 0.02138 0.910   0.994
>   18.7     97       1    0.941 0.02330 0.897   0.988
>    :
>    :
>    :
> 
> I have many censoring observations in the dataset, and
> I would like to know the number of patients at risk
> (n.risk in the above output) for certain time points,
> for example at 60, 72, etc, which is not available
> from the above printout for class=1. Is there anyway I
> can get them?
> 
> Thanks
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From sblay at sfu.ca  Fri Feb  4 21:26:28 2005
From: sblay at sfu.ca (S. Blay)
Date: Fri, 4 Feb 2005 12:26:28 -0800
Subject: [R] R package with C code on Windows
Message-ID: <20050204202628.GA14808@sfu.ca>

Dear R helpers,

MyPkg passes R CMD check on Linux machines.
However, when I 'R CMD check myPkg' on Windows,
the libs subdirectory is not being created.

If I install the package and then create the libs 
subdirectory manually and copy the dll files to it, 
the package seems to work fine 
(but that's not good enough for submitting it to CRAN).

Any advice will be appreciated, 

Thanks,

Sigal



From ivo_welch-rstat8303 at mailblocks.com  Fri Feb  4 21:33:52 2005
From: ivo_welch-rstat8303 at mailblocks.com (ivo_welch-rstat8303@mailblocks.com)
Date: Fri, 04 Feb 2005 12:33:52 -0800
Subject: [R] arrow head styles?
Message-ID: <200502042034.j14KY3UI011043@hypatia.math.ethz.ch>



dear R wizards:  is it possible to specify different arrow head styles? 
  E.g., a solid arrow head?  Or a bent arrow head?  Or a longer or 
shorter arrow head?  (perhaps through an add in?)  I guess I could 
write this myself, but since arrows is built-in, I was hoping it had 
some flexibility hidden in it that I did not see in "?arrows".

sincerely,  /iaw

---
ivo welch



From jbdunsmo at utmb.edu  Fri Feb  4 22:39:41 2005
From: jbdunsmo at utmb.edu (jbdunsmo@utmb.edu)
Date: Fri, 4 Feb 2005 15:39:41 -0600
Subject: [R] simple example of C interface to R
In-Reply-To: <Pine.LNX.4.44.0502042106000.6223-100000@reclus.nhh.no>
References: <20050204193733.GO8123@g20458.utmb.edu>
	<Pine.LNX.4.44.0502042106000.6223-100000@reclus.nhh.no>
Message-ID: <20050204213941.GP8123@g20458.utmb.edu>

On Fri, Feb 04, 2005 at 09:09:37PM +0100, Roger Bivand wrote:
>
> Well, it is documented in the Writing R Extensions manual:
> 
> http://cran.r-project.org/doc/manuals/R-exts.html#System-and-foreign-language-interfaces
> 

thanks.  reading through that a second time made all the difference.
i still think a complete example (a very simple C program that uses R
and compiles without errors) would be good to have in the
documentation.

jason



From tuechler at gmx.at  Fri Feb  4 23:17:53 2005
From: tuechler at gmx.at (Heinz Tuechler)
Date: Fri, 04 Feb 2005 23:17:53 +0100
Subject: [R] How to access results of survival analysis
Message-ID: <3.0.6.32.20050204231753.007aba90@pop.gmx.net>

Hello,

it seems that the main results of survival analysis with package survival
are shown only as side effects of the print method.

If I compute e.g. a Kaplan-Meier estimate by 
> km.survdur<-survfit(s.survdur) 
then I can simply print the results by 
> km.survdur
Call: survfit(formula = s.survdur)

      n  events  median 0.95LCL 0.95UCL 
  100.0    58.0    46.8    41.0    79.3 

Is there a simple method to access these results, e.g. if I want to print
only the median with the confidence limits?
Regarding the results of a Cox-PH-model I face the same situation. The
printed results are:
> cx.survdur.ipss_mds.sex
Call:
coxph(formula = s.survdur ~ x1 + x2, method = "efron")

       coef exp(coef) se(coef)     z      p
x1   0.6424      1.90    0.206 3.123 0.0018
x2.L 0.0616      1.06    0.263 0.234 0.8100

Likelihood ratio test=9.56  on 2 df, p=0.0084  n=58 (42 observations
deleted due to missing)

Is there a simple method to copy e.g. the coefficients and p-values in a
new object?

I am working with:
R : Copyright 2004, The R Foundation for Statistical Computing
Version 2.0.1  (2004-11-15), ISBN 3-900051-07-0
Survival package version: survival_2.16
Operating System: Windows 98SE

Thanks,
Heinz T?chler



From vikas at mail.jnu.ac.in  Sat Feb  5 01:43:04 2005
From: vikas at mail.jnu.ac.in (Vikas Rawal)
Date: Sat, 05 Feb 2005 06:13:04 +0530
Subject: [R] Installing R packages in windows
Message-ID: <1107564184.ce3d71e0vikas@mail.jnu.ac.in>


Thank you. That is useful. But is it possible to download all the packages in one go, or would one have download each one by one?

Vikas

-----Original Message-----
From: Uwe Ligges <ligges at statistik.uni-dortmund.de>
To: Vikas Rawal <vikas at mail.jnu.ac.in>
Date: Fri, 04 Feb 2005 13:55:42 +0100
Subject: Re: [R] Installing R packages in windows

Vikas Rawal wrote:

> I need to install a selected set of packages on a number of machines (in a computer lab). Some of these machines are not connected to internet. Is it possible to download all the packages and make a kind of repository on a CD, and then install.packages from the CD?

Yes, just download the packages and install.packages with CRAN=NULL ...

Instead, you might want to mount the installed packages from a network 
volume instead, adding a second library path for R. So you only need to 
install stuff once.


Uwe Ligges



> Vikas
> 
> ==============================================
> 
>  This Mail was Scanned for Virus and found Virus free
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Sat Feb  5 01:52:45 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 4 Feb 2005 19:52:45 -0500
Subject: [R] Installing R packages in windows
Message-ID: <3A822319EB35174CA3714066D590DCD50994E661@usrymx25.merck.com>

A half-way decent ftp client would allow you to get all files in a
directory, so that ought to be quite easy.

Andy

> From: Vikas Rawal
> 
> Thank you. That is useful. But is it possible to download all 
> the packages in one go, or would one have download each one by one?
> 
> Vikas
> 
> -----Original Message-----
> From: Uwe Ligges <ligges at statistik.uni-dortmund.de>
> To: Vikas Rawal <vikas at mail.jnu.ac.in>
> Date: Fri, 04 Feb 2005 13:55:42 +0100
> Subject: Re: [R] Installing R packages in windows
> 
> Vikas Rawal wrote:
> 
> > I need to install a selected set of packages on a number of 
> machines (in a computer lab). Some of these machines are not 
> connected to internet. Is it possible to download all the 
> packages and make a kind of repository on a CD, and then 
> install.packages from the CD?
> 
> Yes, just download the packages and install.packages with 
> CRAN=NULL ...
> 
> Instead, you might want to mount the installed packages from 
> a network 
> volume instead, adding a second library path for R. So you 
> only need to 
> install stuff once.
> 
> 
> Uwe Ligges
> 
> 
> 
> > Vikas
> > 
> > ==============================================
> > 
> >  This Mail was Scanned for Virus and found Virus free
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ggrothendieck at myway.com  Sat Feb  5 02:41:27 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 5 Feb 2005 01:41:27 +0000 (UTC)
Subject: [R] Building a Matrix
References: <88EAF3512A55DF46B06B1954AEF73F74079A436E@dc1ex2.air.org>
Message-ID: <loom.20050205T023919-65@post.gmane.org>

Doran, Harold <HDoran <at> air.org> writes:

: 
: Dear List:
: 
: I am having some difficulty constructing a matrix that must take a
: specific form. The matrix must be have a lower block of non-zero values
: and the rest must all be zero. For example, if I am building an n X n
: matrix, then the first n/2 rows need to be zero and the first n/2
: columns must remain as zero with all other elements having a non-zero
: value that I specify.
: 
: For example, assume I start with the following 4 x 4 matrix:
: 
: vl.mat <- matrix(0,4,4)
: 
:      [,1] [,2] [,3] [,4]
: [1,]    0    0    0    0
: [2,]    0    0    0    0
: [3,]    0    0    0    0
: [4,]    0    0    0    0
: 
: I need for the for the first two columns (4/2) to remain as zero and the
: first two rows (4/2) to remain as zeros. But I need for the bottom block
: to include some values that I specify.
: 
:      [,1] [,2] [,3] [,4]
: [1,]    0    0    0    0
: [2,]    0    0    0    0
: [3,]    0    0  100  100
: [4,]    0    0  100  100
: 
: I know that if I use the following I can build a matrix with values
: along the diagonals where I need them, but I need for the lower block of
: off-diagonals to also be the same value.
: 
: vl.mat <- matrix(0,4,4)
: vl.mat[(col(vl.mat)%%4 == row(vl.mat)%%4)&col(vl.mat)%%4 !=1
: &col(vl.mat)%%4 !=2  ] <- 100
: 

If A is the 2x2 submatrix that is to go in the lower right hand
corner then:

kronecker(diag(0:1), A)



From tghoward at gw.dec.state.ny.us  Fri Feb  4 17:17:40 2005
From: tghoward at gw.dec.state.ny.us (Tim Howard)
Date: Fri, 04 Feb 2005 11:17:40 -0500
Subject: [R] subset data.frame with value != in all columns
Message-ID: <s20359f0.085@gwsmtp.DEC.STATE.NY.US>

Because I'll be doing this on big datasets and time is important, I
thought I'd time all the different approaches that were suggested on a
small dataframe. The results were very instructive so I thought I'd pass
them on. I also discovered that my numeric columns (e.g. -9999.000)
weren't found by apply() but were found by which() and the simple
replace. Was it apply's fault or something else?

Note how much faster unique(which()) is; wow! Thanks to Marc Schwartz
for this blazing solution.

> nrow(in.df)
[1] 40000
#extract rows with no -9999
> system.time(x <- subset(in.df, apply(in.df, 1,
function(in.df){all(in.df != -9999)})))
[1] 3.25 0.00 3.25   NA   NA
> system.time(y<- in.df[-unique(which(in.df == -9999, arr.ind = TRUE)[,
1]), ])
[1] 0.17 0.00 0.17   NA   NA
> system.time({is.na(in.df) <-in.df == -9999; z <- na.omit(in.df)})
[1] 0.25 0.02 0.26   NA   NA

> nrow(x);nrow(y);nrow(z)
[1] 39990
[1] 39626
[1] 39626

#extract rows with -9999
> system.time(d<-subset(in.df, apply(in.df, 1,
function(in.df){any(in.df == -9999)})))
[1] 3.40 0.00 3.45   NA   NA
> system.time(e<-in.df[unique(which(in.df == -9999, arr.ind = TRUE)[,
1]), ])
[1] 0.11 0.00 0.11   NA   NA

> nrow(d); nrow(e)
[1] 10
[1] 374

Tim Howard


>>> Marc Schwartz <MSchwartz at MedAnalytics.com> 02/03/05 03:24PM >>>
On Thu, 2005-02-03 at 14:57 -0500, Tim Howard wrote: 
  ... snip...
> My questions: 
> Is there a cleaner way to extract all rows containing a specified
> value?
> How can I extract all rows that don't have this value in any col?
> 
> #create dummy dataset
> x <- data.frame(
> c1=c(-99,-99,-99,4:10),
> c2=1:10,
> c3=c(1:3,-99,5:10),
> c4=c(10:1),
> c5=c(1:9,-99))
> 
..snip...

How about this, presuming that your data frame is all numeric:

For rows containing -99:

> x[unique(which(x == -99, arr.ind = TRUE)[, 1]), ]
    c1 c2  c3 c4  c5
1  -99  1   1 10   1
2  -99  2   2  9   2
3  -99  3   3  8   3
4    4  4 -99  7   4
10  10 10  10  1 -99


For rows not containing -99:

> x[-unique(which(x == -99, arr.ind = TRUE)[, 1]), ]
  c1 c2 c3 c4 c5
5  5  5  5  6  5
6  6  6  6  5  6
7  7  7  7  4  7
8  8  8  8  3  8
9  9  9  9  2  9


What I have done here is to use which(), setting arr.ind = TRUE. This
returns the row, column indices for the matches to the boolean
statement. The first column returned by which() in this case are the
row
numbers matching the statement, so I take the first column only.

Since it is possible that more than one element in a row can match the
boolean, I then use unique() to get the singular row values.

Thus, I can use the returned row indices above to subset the data
frame.

HTH,

Marc Schwartz



From ggrothendieck at myway.com  Sat Feb  5 05:36:14 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 5 Feb 2005 04:36:14 +0000 (UTC)
Subject: [R] interval partition problem [was: (no subject)]
References: <EE8F300994F94144856B1EC54D0891F8B92ECE@cdsx06.cder.fda.gov>
Message-ID: <loom.20050205T053030-627@post.gmane.org>

Soukup, Matt <SoukupM <at> cder.fda.gov> writes:

: 
: Hi.
: 
: I have a problem that I can't seem to find an optimal way of solving other
: than by doing things manually. I'm trying to subset a data frame by the
: number of observations that occurred at a given row but want to take into
: account the number of observations of preceding rows. Here's an example.
: 
: I'm looking at intervals of data [10,20), [10, 30), ....., [10,120) which
: contain a certain number of observations for treatment A and treatment B. An
: example is given by the following code.
: 
: >int <- as.factor(paste("[", rep(10, 11), ",", seq(20,120, by=10), ")"))
: >nsamA <- c(62, 83, 118, 151, 180, 201, 212, 215, 216, 217, 218)
: >nsamB <- c(65, 90, 128, 163, 190, 199, 209, 214, 215, 216, 218)
: 
: >df0 <- data.frame(int, nsamA, nsamB)
: >df0
: 
: Since the interval [10, s) with n_s samples is nested in [10, t)with n_t
: sample for s < t, we know n_s - n_t samples exist in the interval [s, t). If
: this sample size of the difference is small I want to exclude the interval
: [10,s). This can be done comparing adjacent preceding rows using the
: following.
: 
: > df0$itagA <- ifelse(c(10, diff(nsamA)) <= 4, 1, 0)
: >df0$itagB <- ifelse(c(10, diff(nsamB)) <= 4, 1, 0)
: >df0
: ># Subset df0 on the tag results
: > df1 <- df0[df0$itagA != 1 & df0$itagB != 1,]
: > df1
: 
: This works fine, but here is my problem. This simply looks at only the
: immediate preceding row and not at rows further "down the line". What I
: would like to do is include the next interval that includes 5 or more
: samples from each group since earlier intervals are nested in the latter
: intervals. In the example given this would include the final interval [10,
: 120) as this contains more than 4 samples for each treatment. I can do this
: by hand using something like
: 
: > df0[c(1:7,11),]
: 
: But this is not an attractive solution as it requires me to actually look at
: the data set each time and determine the row numbers. This works for this
: case, but I have many intervals (rows of data) to look at and this would be
: cumbersome. I've considered using diff with different lag arguments, but
: this still doesn't seem to work. I also want to note that I need to keep the
: int factor (as used in the example above) as this is used throughout my
: analysis (i.e. this is a true factor variable and not simply denoting an
: interval). I'd be grateful for any possible suggestions as I'm stumped at
: this moment. 
: 


Delete the rows one by one and then recalculate diff
after each deletion (rather than diff'ing all at once 
and then deleting all at once).  Also, assuming you want 
every interval to be covered, force the last interval to 
end at the last row.

Assume too.few(df0, i) is a function, not shown here, which 
returns TRUE if there are too few As or Bs in row i minus row 
i-1 of df0 and otherwise FALSE. Then:

last.row <- df0[nrow(df0),]
i <- 1
while(i < nrow(df0)) if (too.few(df0, i)) df0 <- df0[-i,] else i <- i + 1
df0[nrow(df0),] <- last.row


P.S.

Please start a new thread rather than replying to an existing thread
and please use a meaningful subject.



From ggrothendieck at myway.com  Sat Feb  5 05:49:43 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 5 Feb 2005 04:49:43 +0000 (UTC)
Subject: [R] interval partition problem [was: (no subject)]
References: <EE8F300994F94144856B1EC54D0891F8B92ECE@cdsx06.cder.fda.gov>
	<loom.20050205T053030-627@post.gmane.org>
Message-ID: <loom.20050205T054845-633@post.gmane.org>

Gabor Grothendieck <ggrothendieck <at> myway.com> writes:

: 
: Soukup, Matt <SoukupM <at> cder.fda.gov> writes:
: 
: : 
: : Hi.
: : 
: : I have a problem that I can't seem to find an optimal way of solving other
: : than by doing things manually. I'm trying to subset a data frame by the
: : number of observations that occurred at a given row but want to take into
: : account the number of observations of preceding rows. Here's an example.
: : 
: : I'm looking at intervals of data [10,20), [10, 30), ....., [10,120) which
: : contain a certain number of observations for treatment A and treatment B. 
An
: : example is given by the following code.
: : 
: : >int <- as.factor(paste("[", rep(10, 11), ",", seq(20,120, by=10), ")"))
: : >nsamA <- c(62, 83, 118, 151, 180, 201, 212, 215, 216, 217, 218)
: : >nsamB <- c(65, 90, 128, 163, 190, 199, 209, 214, 215, 216, 218)
: : 
: : >df0 <- data.frame(int, nsamA, nsamB)
: : >df0
: : 
: : Since the interval [10, s) with n_s samples is nested in [10, t)with n_t
: : sample for s < t, we know n_s - n_t samples exist in the interval [s, t). 
If
: : this sample size of the difference is small I want to exclude the interval
: : [10,s). This can be done comparing adjacent preceding rows using the
: : following.
: : 
: : > df0$itagA <- ifelse(c(10, diff(nsamA)) <= 4, 1, 0)
: : >df0$itagB <- ifelse(c(10, diff(nsamB)) <= 4, 1, 0)
: : >df0
: : ># Subset df0 on the tag results
: : > df1 <- df0[df0$itagA != 1 & df0$itagB != 1,]
: : > df1
: : 
: : This works fine, but here is my problem. This simply looks at only the
: : immediate preceding row and not at rows further "down the line". What I
: : would like to do is include the next interval that includes 5 or more
: : samples from each group since earlier intervals are nested in the latter
: : intervals. In the example given this would include the final interval [10,
: : 120) as this contains more than 4 samples for each treatment. I can do this
: : by hand using something like
: : 
: : > df0[c(1:7,11),]
: : 
: : But this is not an attractive solution as it requires me to actually look 
at
: : the data set each time and determine the row numbers. This works for this
: : case, but I have many intervals (rows of data) to look at and this would be
: : cumbersome. I've considered using diff with different lag arguments, but
: : this still doesn't seem to work. I also want to note that I need to keep 
the
: : int factor (as used in the example above) as this is used throughout my
: : analysis (i.e. this is a true factor variable and not simply denoting an
: : interval). I'd be grateful for any possible suggestions as I'm stumped at
: : this moment. 
: : 
: 
: Delete the rows one by one and then recalculate diff
: after each deletion (rather than diff'ing all at once 
: and then deleting all at once).  Also, assuming you want 
: every interval to be covered, force the last interval to 
: end at the last row.
: 
: Assume too.few(df0, i) is a function, not shown here, which 
: returns TRUE if there are too few As or Bs in row i minus row 
: i-1 of df0 and otherwise FALSE. Then:
: 
: last.row <- df0[nrow(df0),]
: i <- 1
: while(i < nrow(df0)) if (too.few(df0, i)) df0 <- df0[-i,] else i <- i + 1

That should be i <= nrow(df0)

: df0[nrow(df0),] <- last.row
: 
: P.S.
: 
: Please start a new thread rather than replying to an existing thread
: and please use a meaningful subject.
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://stat.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
: 
:



From fhilal at yahoo.com  Sat Feb  5 06:12:43 2005
From: fhilal at yahoo.com (Fairouz Makhlouf)
Date: Fri, 4 Feb 2005 21:12:43 -0800 (PST)
Subject: [R] question about ldahist function
Message-ID: <20050205051244.12585.qmail@web14022.mail.yahoo.com>

Hi, 
When I am using ldahist function I would like to
specify different colors for each of the groups in the
data I am using. Is it possible? If not does anybody
know of another function to plot multiple histograms
on one plot for different groups.

Thanks

=====
Thanks
Fairouz Makhlouf



From phgrosjean at sciviews.org  Sat Feb  5 09:53:50 2005
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Sat, 05 Feb 2005 09:53:50 +0100
Subject: [R] genetic algorithm
In-Reply-To: <cdf8178305020409011c940d51@mail.gmail.com>
References: <cdf8178305020409011c940d51@mail.gmail.com>
Message-ID: <4204899E.5050102@sciviews.org>

After a little search, I found on CRAN:

- gafit: genetic algorithm for curve fitting,
- rgenout: R version of genetic optimization using derivatives
- subselect: Selecting variable subsets, with function genetic(): 
genetic algorithm searching for an optimal k-variable subset

I don't know if it fits your needs.
Best,

Philippe

..............................................<?}))><........
  ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
  ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
  ) ) ) ) )   Mons-Hainaut University, Pentagone (3D08)
( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
  ) ) ) ) )   8, av du Champ de Mars, 7000 Mons, Belgium
( ( ( ( (
  ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.30.54
( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
  ) ) ) ) )
( ( ( ( (    web:   http://www.umh.ac.be/~econum
  ) ) ) ) )          http://www.sciviews.org
( ( ( ( (
..............................................................

WeiWei Shi wrote:
> Hi, 
> I am doing some research on feature selection for classfication
> problem using genetic algorithm in a wrapper approach. I am wondering
> if there is some package which is already built for this purpose. I
> was advised before about dprep package but I don't think it used GA
> there (if I am wrong, please correct me!)
> 
> Thanks,
> 
> Ed
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From ripley at stats.ox.ac.uk  Sat Feb  5 10:23:35 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 5 Feb 2005 09:23:35 +0000 (GMT)
Subject: [R] Internationalization and localization of R
Message-ID: <Pine.LNX.4.61.0502050858220.11223@gannet.stats>

This pre-announcement is being sent for information to both R-help and 
BioC.  Please use R-devel at r-project.org for any follow-up discussion.

-----------------------------------------------------------------------

We are about at the end of a several-week process of adding support for
non-Latin character sets and non-Western-European languages to R.

- For Linux users, R works in the UTF-8 locales that are rapidly becoming
   the standard in the latest distributions: for example if in Fedora Core
   3 I select English (United Kingdom) I get locale en_GB.utf8.  This is
   also true of those commercial Unixes which support such locales.

- For Linux/Unix users, R works in many non-English locales with other
   encodings such as EUC-JP.  The only one that we are having problems with
   is vi_VN.tcvn (but the Vietnamese UTF-8 locale appears to work).

- For Windows users, R works in the double-byte `East Asian' locales as
   well as many others, if support is selected at installation time.
   (See the current rw-FAQ for more discussion of this: this is supported
   in the current automated builds of r-devel on CRAN.)

- There are built-in facilities to convert between encodings, so that e.g.
   Japanese in SHIFT-JIS can be read by R running under UTF-8 or EUC-JP.

- There are mechanisms to translate both C and R error messages from both
   base R and for packages, and also the menus etc of the Windows and
   MacOS X GUIs.

The purpose of this pre-announcement is to alert developers that these 
changes will be coming in 2.1.0 in a couple of months.

- Package writers who would like to translate messages in their packages,
   or to prepare their packages for translation by others should consult

   http://developer.r-project.org/210update.txt

   and the latest version of `Writing R Extensions' in the R-devel sources.

   All the standard packages and a few of the recommended ones have been
   prepared and updates will be appearing on CRAN shortly.

- People might like to start organizing translation teams for their own
   languages, and early experience from such a team would be helpful in
   polishing the instructions and mechanisms.  There is a document for
   translators at

   http://developer.r-project.org/Translations.html

   If translations are available by early April they can be shipped with
   2.1.0 (although there are planned to be mechanisms to add them to an R
   installation).

- We will be looking for testers in the alpha/beta period in about a
   month's time, and would-be users of R in unusual languages (e.g.
   Vietnamese) would be especially helpful as testers.  This is
   particularly important for Windows 95/98/ME to which we only have
   very limited access, to English versions.

-----------------------------------------------------------------------

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Sat Feb  5 11:07:58 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 5 Feb 2005 11:07:58 +0100
Subject: [R] arrow head styles?
In-Reply-To: <200502042034.j14KY3UI011043@hypatia.math.ethz.ch>
References: <200502042034.j14KY3UI011043@hypatia.math.ethz.ch>
Message-ID: <16900.39678.409986.775144@stat.math.ethz.ch>

>>>>> "IvoW" ==   <ivo_welch-rstat8303 at mailblocks.com>
>>>>>     on Fri, 04 Feb 2005 12:33:52 -0800 writes:

    IvoW> dear R wizards: is it possible to specify different
    IvoW> arrow head styles?  E.g., a solid arrow head?  Or a
    IvoW> bent arrow head?  Or a longer or shorter arrow head?
    IvoW> (perhaps through an add in?)  I guess I could write
    IvoW> this myself, but since arrows is built-in, I was
    IvoW> hoping it had some flexibility hidden in it that I did
    IvoW> not see in "?arrows".

The are several "extensions" to the arrows() function around.

package 'sfsmisc'  has  p.arrows()
package 'IDPmisc'  has  Arrows()

Did you try to search for "arrows" on Jonathan Baron's search
site  http://search.R-project.org/ ?



From kimalioud at tsamail.co.za  Sat Feb  5 11:03:15 2005
From: kimalioud at tsamail.co.za (Kima)
Date: 5 Feb 2005 10:03:15 -0000
Subject: [R] FROM RUSSIA WITH LOVE
Message-ID: <20050205100315.24654.qmail@nfly20.registerfly.com>


Dear Friend,
I have a profiling amount in an excess of US$423M,which I seek your partnership in accommodating for me. You will be rewarded with 40% of the total sum for your partnership. Can you be my partner on this?
INTRODUCTION OF MY SELF:
I am Ms.KIMAEVE LIOUDMILA, a personal secretary to Mikhail Khodorkovsky the richest man in Russia and owner of the following companies:Chairman CEO: YUKOS OIL (Russian Most Largest Oil Company) Chairman CEO:Menatep SBP Bank (A well reputable financial institution with its branches all over the world).
SOURCE OF FUNDS:
The documents of the above funds in question was handed over to me to be used in payment of an American oil merchant for his last oil deal with my boss Mikhail Khodorkovsky. Already the funds have been deposited with a bank,where the final crediting is expected to be carried out. While I was on the process, My Boss got arrested for his involvement on politics in financing the  leading and opposingpolitical parties (the Union of Right Forces,led by Boris Nemtsov, and Yabloko, a liberal/social democratic party led by Gregor Yavlinsky) which poses treat to President Vladimir
Putin second  tenure as Russian president. You can catch more of the story on this
http://newsfromrussia.com/main/2003/11/13/51215.html
YOUR ROLE:
All I need from you is to stand as the beneficiary of the above quoted sum and I will arrange for the documentation which will enable the paying bank transfer the sum to you. I have decided to use this sum to relocate to American continent and never to be connected to any of Mikhail Khodorkovsky conglomerates. The transaction has to be concluded in 2 weeks before Mikhail Khodorkovsky  is out on bail.
As soon as I get your willingness to comply through my most private email address:kima at bizemail.com
I will give you more details.
Thank you very much
Regards
KIMAEVE LIOUDMILA



From secchi at sssup.it  Sat Feb  5 11:43:02 2005
From: secchi at sssup.it (Angelo Secchi)
Date: Sat, 5 Feb 2005 11:43:02 +0100
Subject: [R] Std Err on Concentration measures
Message-ID: <20050205114302.488cd00d.secchi@sssup.it>


Hi,
I'm using the ineq package to calculate some concentration measures (Gini, Herfindal, ...) and I was wondering if there's around also a function to calculate standard error on these measures. If not, is anybody aware of where I can find a reference on this point?
Thanks.

-- 
========================================================
 Angelo Secchi                     PGP Key ID:EA280337



From ales.ziberna at guest.arnes.si  Sat Feb  5 11:51:18 2005
From: ales.ziberna at guest.arnes.si (=?windows-1250?Q?Ale=9A_=8Eiberna?=)
Date: Sat, 5 Feb 2005 11:51:18 +0100
Subject: [R] Problems compiling (configure) R on Ubuntu linux (debian)
Message-ID: <022501c50b70$a55d8760$1209f9c2@ales>

Hello!

I would first like to appologice if this question does not fit on this 
mailing-list.

I am new to Linux and I tried to compile R on my Ubuntu Warty linux. I 
followed the instructions in "R Installation and Administration" manual. It 
seams that there was a problem with "configure", since when running "make" 
afterward gave me this reply:
    make: *** No targets specified and no makefile found.  Stop.

I have copied the content of the "config.log" at the end of the file. If 
someone would give me some instructions on how to read this file, I would be 
very grateful.

Thank you in advance!
Ales Ziberna

"confing.log"
This file contains any messages produced by compilers while
running configure, to aid debugging if configure makes a mistake.

It was created by R configure 2.0.1, which was
generated by GNU Autoconf 2.59.  Invocation command line was

  $ ./configure

## --------- ##
## Platform. ##
## --------- ##

hostname = ubuntu
uname -m = i686
uname -r = 2.6.8.1-3-386
uname -s = Linux
uname -v = #1 Tue Oct 12 12:41:57 BST 2004

/usr/bin/uname -p = unknown
/bin/uname -X     = unknown

/bin/arch              = i686
/usr/bin/arch -k       = unknown
/usr/convex/getsysinfo = unknown
hostinfo               = unknown
/bin/machine           = unknown
/usr/bin/oslevel       = unknown
/bin/universe          = unknown

PATH: /usr/local/sbin
PATH: /usr/local/bin
PATH: /usr/sbin
PATH: /usr/bin
PATH: /sbin
PATH: /bin
PATH: /usr/X11R6/bin


## ----------- ##
## Core tests. ##
## ----------- ##

configure:1732: checking for a BSD-compatible install
configure:1787: result: /usr/bin/install -c
configure:1798: checking whether build environment is sane
configure:1841: result: yes
configure:1856: checking whether make sets $(MAKE)
configure:1876: result: yes
configure:1908: checking for working aclocal-1.4
configure:1919: result: missing
configure:1923: checking for working autoconf
configure:1934: result: missing
configure:1938: checking for working automake-1.4
configure:1949: result: missing
configure:1953: checking for working autoheader
configure:1964: result: missing
configure:1968: checking for working makeinfo
configure:1979: result: missing
configure:1999: checking build system type
configure:2017: result: i686-pc-linux-gnu
configure:2025: checking host system type
configure:2039: result: i686-pc-linux-gnu
configure:2498: checking for pwd
configure:2516: found /bin/pwd
configure:2529: result: /bin/pwd
configure:2536: checking whether builddir is srcdir
configure:2544: result: yes
configure:2553: checking for gawk
configure:2582: result: no
configure:2553: checking for mawk
configure:2569: found /usr/bin/mawk
configure:2579: result: mawk
configure:2589: checking for egrep
configure:2599: result: grep -E
configure:2604: checking whether ln -s works
configure:2608: result: yes
configure:2656: checking for ranlib
configure:2672: found /usr/bin/ranlib
configure:2683: result: ranlib
configure:2699: checking for bison
configure:2728: result: no
configure:2699: checking for byacc
configure:2728: result: no
configure:2740: checking for ar
configure:2756: found /usr/bin/ar
configure:2766: result: ar
configure:2799: checking for javac
configure:2832: result: no
configure:2841: checking for sed
configure:2860: found /bin/sed
configure:2872: result: /bin/sed
configure:2891: checking for less
configure:2909: found /usr/bin/less
configure:2921: result: /usr/bin/less
configure:2943: checking for perl
configure:2961: found /usr/bin/perl
configure:2973: result: /usr/bin/perl
configure:2984: checking whether perl version is at least 5.004
configure:2995: result: yes
configure:3068: checking for dvips
configure:3101: result: no
configure:3113: checking for tex
configure:3146: result: no
configure:3158: checking for latex
configure:3191: result: no
configure:3205: WARNING: you cannot build DVI versions of the R manuals
configure:3212: checking for makeindex
configure:3245: result: no
configure:3257: checking for pdftex
configure:3290: result: no
configure:3302: checking for pdflatex
configure:3335: result: no
configure:3349: WARNING: you cannot build PDF versions of the R manuals
configure:3356: checking for /home/ales/Programi/R/R-2.0.1/tools/missing
configure:3389: result: no
configure:3356: checking for makeinfo
configure:3389: result: no
configure:3356: checking for makeinfo
configure:3389: result: no
configure:3456: checking for unzip
configure:3474: found /usr/bin/unzip
configure:3486: result: /usr/bin/unzip
configure:3501: checking for zip
configure:3519: found /usr/bin/zip
configure:3531: result: /usr/bin/zip
configure:3546: checking for gzip
configure:3564: found /bin/gzip
configure:3576: result: /bin/gzip
configure:3593: checking for netscape
configure:3626: result: no
configure:3593: checking for mozilla
configure:3611: found /usr/bin/mozilla
configure:3623: result: /usr/bin/mozilla
configure:3639: result: using default browser ... /usr/bin/mozilla
configure:3649: checking for acroread
configure:3682: result: no
configure:3649: checking for acroread4
configure:3682: result: no
configure:3649: checking for xpdf
configure:3667: found /usr/bin/xpdf
configure:3679: result: /usr/bin/xpdf
configure:3743: checking for gcc
configure:3759: found /usr/bin/gcc
configure:3769: result: gcc
configure:4013: checking for C compiler version
configure:4016: gcc --version </dev/null >&5
gcc (GCC) 3.3.4 (Debian 1:3.3.4-9ubuntu5)
Copyright (C) 2003 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

configure:4019: $? = 0
configure:4021: gcc -v </dev/null >&5
Reading specs from /usr/lib/gcc-lib/i486-linux/3.3.4/specs
Configured with: 
../src/configure -v --enable-languages=c,c++,java,f77,pascal,objc,ada,treelang 
 --prefix=/usr --mandir=/usr/share/man --infodir=/usr/share/info --with-gxx-include-dir=/usr/include/c++/3.3 
 --enable-shared --with-system-zlib --enable-nls --without-included-gettext  
--enable-__cxa_atexit --enable-clocale=gnu --enable-debug --enable-java-gc=boehm 
 --enable-java-awt=xlib --enable-objc-gc i486-linux
Thread model: posix
gcc version 3.3.4 (Debian 1:3.3.4-9ubuntu5)
configure:4024: $? = 0
configure:4026: gcc -V </dev/null >&5
gcc: `-V' option must have argument
configure:4029: $? = 1
configure:4052: checking for C compiler default output file name
configure:4055: gcc  -I/usr/local/include -L/usr/local/lib conftest.c  >&5
configure:4058: $? = 0
configure:4104: result: a.out
configure:4109: checking whether the C compiler works
configure:4115: ./a.out
configure:4118: $? = 0
configure:4135: result: yes
configure:4142: checking whether we are cross compiling
configure:4144: result: no
configure:4147: checking for suffix of executables
configure:4149: gcc -o conftest  -I/usr/local/include -L/usr/local/lib 
conftest.c  >&5
configure:4152: $? = 0
configure:4177: result:
configure:4183: checking for suffix of object files
configure:4204: gcc -c  -I/usr/local/include conftest.c >&5
configure:4207: $? = 0
configure:4229: result: o
configure:4233: checking whether we are using the GNU C compiler
configure:4257: gcc -c  -I/usr/local/include conftest.c >&5
configure:4263: $? = 0
configure:4267: test -z
    || test ! -s conftest.err
configure:4270: $? = 0
configure:4273: test -s conftest.o
configure:4276: $? = 0
configure:4289: result: yes
configure:4295: checking whether gcc accepts -g
configure:4316: gcc -c -g -I/usr/local/include conftest.c >&5
configure:4322: $? = 0
configure:4326: test -z
    || test ! -s conftest.err
configure:4329: $? = 0
configure:4332: test -s conftest.o
configure:4335: $? = 0
configure:4346: result: yes
configure:4363: checking for gcc option to accept ANSI C
configure:4433: gcc  -c -g -O2 -I/usr/local/include conftest.c >&5
configure:4439: $? = 0
configure:4443: test -z
    || test ! -s conftest.err
configure:4446: $? = 0
configure:4449: test -s conftest.o
configure:4452: $? = 0
configure:4470: result: none needed
configure:4488: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
conftest.c:2: error: parse error before "me"
configure:4494: $? = 1
configure: failed program was:
| #ifndef __cplusplus
|   choke me
| #endif
configure:4634: checking how to run the C preprocessor
configure:4669: gcc -E -I/usr/local/include conftest.c
configure:4675: $? = 0
configure:4707: gcc -E -I/usr/local/include conftest.c
conftest.c:16:28: ac_nonexistent.h: No such file or directory
configure:4713: $? = 1
configure: failed program was:
| /* confdefs.h.  */
|
| #define PACKAGE_NAME "R"
| #define PACKAGE_TARNAME "R"
| #define PACKAGE_VERSION "2.0.1"
| #define PACKAGE_STRING "R 2.0.1"
| #define PACKAGE_BUGREPORT "r-bugs at R-project.org"
| #define PACKAGE "R"
| #define VERSION "2.0.1"
| #define R_PLATFORM "i686-pc-linux-gnu"
| #define R_CPU "i686"
| #define R_VENDOR "pc"
| #define R_OS "linux-gnu"
| #define Unix 1
| /* end confdefs.h.  */
| #include <ac_nonexistent.h>
configure:4752: result: gcc -E
configure:4776: gcc -E -I/usr/local/include conftest.c
configure:4782: $? = 0
configure:4814: gcc -E -I/usr/local/include conftest.c
conftest.c:16:28: ac_nonexistent.h: No such file or directory
configure:4820: $? = 1
configure: failed program was:
| /* confdefs.h.  */
|
| #define PACKAGE_NAME "R"
| #define PACKAGE_TARNAME "R"
| #define PACKAGE_VERSION "2.0.1"
| #define PACKAGE_STRING "R 2.0.1"
| #define PACKAGE_BUGREPORT "r-bugs at R-project.org"
| #define PACKAGE "R"
| #define VERSION "2.0.1"
| #define R_PLATFORM "i686-pc-linux-gnu"
| #define R_CPU "i686"
| #define R_VENDOR "pc"
| #define R_OS "linux-gnu"
| #define Unix 1
| /* end confdefs.h.  */
| #include <ac_nonexistent.h>
configure:4865: checking whether gcc needs -traditional
configure:4907: result: no
configure:4919: checking how to run the C preprocessor
configure:5037: result: gcc -E
configure:5061: gcc -E -I/usr/local/include conftest.c
configure:5067: $? = 0
configure:5099: gcc -E -I/usr/local/include conftest.c
conftest.c:16:28: ac_nonexistent.h: No such file or directory
configure:5105: $? = 1
configure: failed program was:
| /* confdefs.h.  */
|
| #define PACKAGE_NAME "R"
| #define PACKAGE_TARNAME "R"
| #define PACKAGE_VERSION "2.0.1"
| #define PACKAGE_STRING "R 2.0.1"
| #define PACKAGE_BUGREPORT "r-bugs at R-project.org"
| #define PACKAGE "R"
| #define VERSION "2.0.1"
| #define R_PLATFORM "i686-pc-linux-gnu"
| #define R_CPU "i686"
| #define R_VENDOR "pc"
| #define R_OS "linux-gnu"
| #define Unix 1
| /* end confdefs.h.  */
| #include <ac_nonexistent.h>
configure:5248: checking for g77
configure:5277: result: no
configure:5248: checking for f77
configure:5277: result: no
configure:5248: checking for xlf
configure:5277: result: no
configure:5248: checking for frt
configure:5277: result: no
configure:5248: checking for pgf77
configure:5277: result: no
configure:5248: checking for fl32
configure:5277: result: no
configure:5248: checking for af77
configure:5277: result: no
configure:5248: checking for fort77
configure:5277: result: no
configure:5248: checking for f90
configure:5277: result: no
configure:5248: checking for xlf90
configure:5277: result: no
configure:5248: checking for pgf90
configure:5277: result: no
configure:5248: checking for epcf90
configure:5277: result: no
configure:5248: checking for f95
configure:5277: result: no
configure:5248: checking for fort
configure:5277: result: no
configure:5248: checking for xlf95
configure:5277: result: no
configure:5248: checking for lf95
configure:5277: result: no
configure:5248: checking for g95
configure:5277: result: no
configure:5248: checking for fc
configure:5264: found /usr/bin/fc
configure:5274: result: fc
configure:5422: checking for Fortran 77 compiler version
configure:5425: fc --version </dev/null >&5
./configure: line 1: fc: --: invalid option
fc: usage: fc [-e ename] [-nlr] [first] [last] or fc -s [pat=rep] [cmd]
configure:5428: $? = 2
configure:5430: fc -v </dev/null >&5
./configure: line 1: fc: -v: invalid option
fc: usage: fc [-e ename] [-nlr] [first] [last] or fc -s [pat=rep] [cmd]
configure:5433: $? = 2
configure:5435: fc -V </dev/null >&5
./configure: line 1: fc: -V: invalid option
fc: usage: fc [-e ename] [-nlr] [first] [last] or fc -s [pat=rep] [cmd]
configure:5438: $? = 2
configure:5446: checking whether we are using the GNU Fortran 77 compiler
configure:5460: fc -c  conftest.F >&5
./configure: line 1: fc: -c: invalid option
fc: usage: fc [-e ename] [-nlr] [first] [last] or fc -s [pat=rep] [cmd]
configure:5466: $? = 2
configure: failed program was:
|       program main
| #ifndef __GNUC__
|        choke me
| #endif
|
|       end
configure:5492: result: no
configure:5498: checking whether fc accepts -g
configure:5510: fc -c -g conftest.f >&5
./configure: line 1: fc: -c: invalid option
fc: usage: fc [-e ename] [-nlr] [first] [last] or fc -s [pat=rep] [cmd]
configure:5516: $? = 2
configure: failed program was:
|       program main
|
|       end
configure:5541: result: no
configure:5633: checking for g++
configure:5649: found /usr/bin/g++
configure:5659: result: g++
configure:5675: checking for C++ compiler version
configure:5678: g++ --version </dev/null >&5
g++ (GCC) 3.3.4 (Debian 1:3.3.4-9ubuntu5)
Copyright (C) 2003 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

configure:5681: $? = 0
configure:5683: g++ -v </dev/null >&5
Reading specs from /usr/lib/gcc-lib/i486-linux/3.3.4/specs
Configured with: 
../src/configure -v --enable-languages=c,c++,java,f77,pascal,objc,ada,treelang 
 --prefix=/usr --mandir=/usr/share/man --infodir=/usr/share/info --with-gxx-include-dir=/usr/include/c++/3.3 
 --enable-shared --with-system-zlib --enable-nls --without-included-gettext  
--enable-__cxa_atexit --enable-clocale=gnu --enable-debug --enable-java-gc=boehm 
 --enable-java-awt=xlib --enable-objc-gc i486-linux
Thread model: posix
gcc version 3.3.4 (Debian 1:3.3.4-9ubuntu5)
configure:5686: $? = 0
configure:5688: g++ -V </dev/null >&5
g++: `-V' option must have argument
configure:5691: $? = 1
configure:5694: checking whether we are using the GNU C++ compiler
configure:5718: g++ -c  -I/usr/local/include conftest.cc >&5
configure:5724: $? = 0
configure:5728: test -z
    || test ! -s conftest.err
configure:5731: $? = 0
configure:5734: test -s conftest.o
configure:5737: $? = 0
configure:5750: result: yes
configure:5756: checking whether g++ accepts -g
configure:5777: g++ -c -g -I/usr/local/include conftest.cc >&5
configure:5783: $? = 0
configure:5787: test -z
    || test ! -s conftest.err
configure:5790: $? = 0
configure:5793: test -s conftest.o
configure:5796: $? = 0
configure:5807: result: yes
configure:5849: g++ -c -g -O2 -I/usr/local/include conftest.cc >&5
configure:5855: $? = 0
configure:5859: test -z
    || test ! -s conftest.err
configure:5862: $? = 0
configure:5865: test -s conftest.o
configure:5868: $? = 0
configure:5894: g++ -c -g -O2 -I/usr/local/include conftest.cc >&5
conftest.cc: In function `int main()':
conftest.cc:20: error: `exit' undeclared (first use this function)
conftest.cc:20: error: (Each undeclared identifier is reported only once for
   each function it appears in.)
configure:5900: $? = 1
configure: failed program was:
| /* confdefs.h.  */
|
| #define PACKAGE_NAME "R"
| #define PACKAGE_TARNAME "R"
| #define PACKAGE_VERSION "2.0.1"
| #define PACKAGE_STRING "R 2.0.1"
| #define PACKAGE_BUGREPORT "r-bugs at R-project.org"
| #define PACKAGE "R"
| #define VERSION "2.0.1"
| #define R_PLATFORM "i686-pc-linux-gnu"
| #define R_CPU "i686"
| #define R_VENDOR "pc"
| #define R_OS "linux-gnu"
| #define Unix 1
| /* end confdefs.h.  */
|
| int
| main ()
| {
| exit (42);
|   ;
|   return 0;
| }
configure:5849: g++ -c -g -O2 -I/usr/local/include conftest.cc >&5
configure:5855: $? = 0
configure:5859: test -z
    || test ! -s conftest.err
configure:5862: $? = 0
configure:5865: test -s conftest.o
configure:5868: $? = 0
configure:5894: g++ -c -g -O2 -I/usr/local/include conftest.cc >&5
configure:5900: $? = 0
configure:5904: test -z
    || test ! -s conftest.err
configure:5907: $? = 0
configure:5910: test -s conftest.o
configure:5913: $? = 0
configure:5941: checking how to run the C++ preprocessor
configure:5972: g++ -E -I/usr/local/include conftest.cc
configure:5978: $? = 0
configure:6010: g++ -E -I/usr/local/include conftest.cc
conftest.cc:19:28: ac_nonexistent.h: No such file or directory
configure:6016: $? = 1
configure: failed program was:
| /* confdefs.h.  */
|
| #define PACKAGE_NAME "R"
| #define PACKAGE_TARNAME "R"
| #define PACKAGE_VERSION "2.0.1"
| #define PACKAGE_STRING "R 2.0.1"
| #define PACKAGE_BUGREPORT "r-bugs at R-project.org"
| #define PACKAGE "R"
| #define VERSION "2.0.1"
| #define R_PLATFORM "i686-pc-linux-gnu"
| #define R_CPU "i686"
| #define R_VENDOR "pc"
| #define R_OS "linux-gnu"
| #define Unix 1
| #ifdef __cplusplus
| extern "C" void std::exit (int) throw (); using std::exit;
| #endif
| /* end confdefs.h.  */
| #include <ac_nonexistent.h>
configure:6055: result: g++ -E
configure:6079: g++ -E -I/usr/local/include conftest.cc
configure:6085: $? = 0
configure:6117: g++ -E -I/usr/local/include conftest.cc
conftest.cc:19:28: ac_nonexistent.h: No such file or directory
configure:6123: $? = 1
configure: failed program was:
| /* confdefs.h.  */
|
| #define PACKAGE_NAME "R"
| #define PACKAGE_TARNAME "R"
| #define PACKAGE_VERSION "2.0.1"
| #define PACKAGE_STRING "R 2.0.1"
| #define PACKAGE_BUGREPORT "r-bugs at R-project.org"
| #define PACKAGE "R"
| #define VERSION "2.0.1"
| #define R_PLATFORM "i686-pc-linux-gnu"
| #define R_CPU "i686"
| #define R_VENDOR "pc"
| #define R_OS "linux-gnu"
| #define Unix 1
| #ifdef __cplusplus
| extern "C" void std::exit (int) throw (); using std::exit;
| #endif
| /* end confdefs.h.  */
| #include <ac_nonexistent.h>
configure:6243: checking for a sed that does not truncate output
configure:6297: result: /bin/sed
configure:6311: checking for ld used by gcc
configure:6378: result: /usr/bin/ld
configure:6387: checking if the linker (/usr/bin/ld) is GNU ld
configure:6402: result: yes
configure:6407: checking for /usr/bin/ld option to reload object files
configure:6414: result: -r
configure:6423: checking for BSD-compatible nm
configure:6465: result: /usr/bin/nm -B
configure:6469: checking how to recognise dependent libraries
configure:6652: result: pass_all
configure:6860: checking for ANSI C header files
configure:6885: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:6891: $? = 0
configure:6895: test -z
    || test ! -s conftest.err
configure:6898: $? = 0
configure:6901: test -s conftest.o
configure:6904: $? = 0
configure:6990: gcc -o conftest -g -O2 -I/usr/local/include -L/usr/local/lib 
conftest.c  >&5
configure:6993: $? = 0
configure:6995: ./conftest
configure:6998: $? = 0
configure:7013: result: yes
configure:7037: checking for sys/types.h
configure:7053: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:7059: $? = 0
configure:7063: test -z
    || test ! -s conftest.err
configure:7066: $? = 0
configure:7069: test -s conftest.o
configure:7072: $? = 0
configure:7083: result: yes
configure:7037: checking for sys/stat.h
configure:7053: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:7059: $? = 0
configure:7063: test -z
    || test ! -s conftest.err
configure:7066: $? = 0
configure:7069: test -s conftest.o
configure:7072: $? = 0
configure:7083: result: yes
configure:7037: checking for stdlib.h
configure:7053: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:7059: $? = 0
configure:7063: test -z
    || test ! -s conftest.err
configure:7066: $? = 0
configure:7069: test -s conftest.o
configure:7072: $? = 0
configure:7083: result: yes
configure:7037: checking for string.h
configure:7053: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:7059: $? = 0
configure:7063: test -z
    || test ! -s conftest.err
configure:7066: $? = 0
configure:7069: test -s conftest.o
configure:7072: $? = 0
configure:7083: result: yes
configure:7037: checking for memory.h
configure:7053: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:7059: $? = 0
configure:7063: test -z
    || test ! -s conftest.err
configure:7066: $? = 0
configure:7069: test -s conftest.o
configure:7072: $? = 0
configure:7083: result: yes
configure:7037: checking for strings.h
configure:7053: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:7059: $? = 0
configure:7063: test -z
    || test ! -s conftest.err
configure:7066: $? = 0
configure:7069: test -s conftest.o
configure:7072: $? = 0
configure:7083: result: yes
configure:7037: checking for inttypes.h
configure:7053: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:7059: $? = 0
configure:7063: test -z
    || test ! -s conftest.err
configure:7066: $? = 0
configure:7069: test -s conftest.o
configure:7072: $? = 0
configure:7083: result: yes
configure:7037: checking for stdint.h
configure:7053: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:7059: $? = 0
configure:7063: test -z
    || test ! -s conftest.err
configure:7066: $? = 0
configure:7069: test -s conftest.o
configure:7072: $? = 0
configure:7083: result: yes
configure:7037: checking for unistd.h
configure:7053: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:7059: $? = 0
configure:7063: test -z
    || test ! -s conftest.err
configure:7066: $? = 0
configure:7069: test -s conftest.o
configure:7072: $? = 0
configure:7083: result: yes
configure:7109: checking dlfcn.h usability
configure:7121: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:7127: $? = 0
configure:7131: test -z
    || test ! -s conftest.err
configure:7134: $? = 0
configure:7137: test -s conftest.o
configure:7140: $? = 0
configure:7150: result: yes
configure:7154: checking dlfcn.h presence
configure:7164: gcc -E -I/usr/local/include conftest.c
configure:7170: $? = 0
configure:7190: result: yes
configure:7225: checking for dlfcn.h
configure:7232: result: yes
configure:7251: checking the maximum length of command line arguments
configure:7316: result: 32768
configure:7327: checking command to parse /usr/bin/nm -B output from gcc 
object
configure:7416: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:7419: $? = 0
configure:7423: /usr/bin/nm -B conftest.o \| sed -n -e 
s/^.*[  ]\([ABCDGIRSTW][ABCDGIRSTW]*\)[  ][  ]*\(\)\([_A-Za-z][_A-Za-z0-9]*\)$/\1 
\2\3 \3/p' \> conftest.nm
configure:7426: $? = 0
configure:7478: gcc -o conftest -g -O2 -I/usr/local/include -L/usr/local/lib 
conftest.c conftstm.o >&5
configure:7481: $? = 0
configure:7519: result: ok
configure:7523: checking for objdir
configure:7538: result: .libs
configure:7708: checking for ranlib
configure:7735: result: ranlib
configure:7788: checking for strip
configure:7804: found /usr/bin/strip
configure:7815: result: strip
configure:8077: checking if gcc static flag  works
configure:8100: result: yes
configure:8118: checking if gcc supports -fno-rtti -fno-exceptions
configure:8139: gcc -c -g -O2 -I/usr/local/include -fno-rtti -fno-exceptions 
conftest.c >&5
cc1: warning: "-fno-rtti" is valid for C++ but not for C/ObjC
configure:8143: $? = 0
configure:8151: result: no
configure:8166: checking for gcc option to produce PIC
configure:8343: result: -fPIC
configure:8351: checking if gcc PIC flag -fPIC works
configure:8372: gcc -c -g -O2 -I/usr/local/include -fPIC -DPIC conftest.c 
 >&5
configure:8376: $? = 0
configure:8384: result: yes
configure:8408: checking if gcc supports -c -o file.o
configure:8432: gcc -c -g -O2 -I/usr/local/include -o out/conftest2.o 
conftest.c >&5
configure:8436: $? = 0
configure:8453: result: yes
configure:8479: checking whether the gcc linker (/usr/bin/ld) supports 
shared libraries
configure:9327: result: yes
configure:9353: checking whether -lc should be explicitly linked in
configure:9358: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:9361: $? = 0
configure:9375: gcc -shared conftest.o  -v -Wl,-soname -Wl,conftest -o 
conftest 2\>\&1 \| grep  -lc  \>/dev/null 2\>\&1
configure:9378: $? = 0
configure:9390: result: no
configure:9398: checking dynamic linker characteristics
configure:9938: result: GNU/Linux ld.so
configure:9942: checking how to hardcode library paths into programs
configure:9967: result: immediate
configure:9981: checking whether stripping libraries is possible
configure:9986: result: yes
configure:10816: checking if libtool supports shared libraries
configure:10818: result: yes
configure:10821: checking whether to build shared libraries
configure:10879: result: yes
configure:10882: checking whether to build static libraries
configure:10886: result: no
configure:10978: creating libtool
configure:11525: checking for ld used by g++
configure:11592: result: /usr/bin/ld
configure:11601: checking if the linker (/usr/bin/ld) is GNU ld
configure:11616: result: yes
configure:11667: checking whether the g++ linker (/usr/bin/ld) supports 
shared libraries
configure:12495: result: yes
configure:12513: g++ -c -g -O2 -I/usr/local/include conftest.cc >&5
configure:12516: $? = 0
configure:12612: checking for g++ option to produce PIC
configure:12864: result: -fPIC
configure:12872: checking if g++ PIC flag -fPIC works
configure:12893: g++ -c -g -O2 -I/usr/local/include -fPIC -DPIC conftest.cc 
 >&5
configure:12897: $? = 0
configure:12905: result: yes
configure:12929: checking if g++ supports -c -o file.o
configure:12953: g++ -c -g -O2 -I/usr/local/include -o out/conftest2.o 
conftest.cc >&5
configure:12957: $? = 0
configure:12974: result: yes
configure:13000: checking whether the g++ linker (/usr/bin/ld) supports 
shared libraries
configure:13025: result: yes
configure:13096: checking dynamic linker characteristics
configure:13636: result: GNU/Linux ld.so
configure:13640: checking how to hardcode library paths into programs
configure:13665: result: immediate
configure:13679: checking whether stripping libraries is possible
configure:13684: result: yes
configure:14991: checking if libtool supports shared libraries
configure:14993: result: yes
configure:14996: checking whether to build shared libraries
configure:15014: result: yes
configure:15017: checking whether to build static libraries
configure:15021: result: no
configure:15033: checking for fc option to produce PIC
configure:15210: result: -fPIC
configure:15218: checking if fc PIC flag -fPIC works
configure:15239: fc -c  -fPIC conftest.f >&5
./configure: line 1: fc: -c: invalid option
fc: usage: fc [-e ename] [-nlr] [first] [last] or fc -s [pat=rep] [cmd]
configure:15243: $? = 2
configure:15251: result: no
configure:15275: checking if fc supports -c -o file.o
configure:15299: fc -c  -o out/conftest2.o conftest.f >&5
./configure: line 1: fc: -c: invalid option
fc: usage: fc [-e ename] [-nlr] [first] [last] or fc -s [pat=rep] [cmd]
configure:15303: $? = 2
configure:15320: result: no
configure:15346: checking whether the fc linker (/usr/bin/ld) supports 
shared libraries
configure:16174: result: yes
configure:16245: checking dynamic linker characteristics
configure:16785: result: GNU/Linux ld.so
configure:16789: checking how to hardcode library paths into programs
configure:16814: result: immediate
configure:16828: checking whether stripping libraries is possible
configure:16833: result: yes
configure:20949: WARNING: you cannot build info or html versions of the R 
manuals
configure:21149: checking for cos in -lm
configure:21179: gcc -o 
conftest -g -O2 -I/usr/local/include -L/usr/local/lib conftest.c -lm   >&5
conftest.c:37: warning: conflicting types for built-in function `cos'
configure:21185: $? = 0
configure:21189: test -z
    || test ! -s conftest.err
configure:21192: $? = 0
configure:21195: test -s conftest
configure:21198: $? = 0
configure:21211: result: yes
configure:21225: checking for sin in -lm
configure:21255: gcc -o 
conftest -g -O2 -I/usr/local/include -L/usr/local/lib conftest.c -lm   >&5
conftest.c:37: warning: conflicting types for built-in function `sin'
configure:21261: $? = 0
configure:21265: test -z
    || test ! -s conftest.err
configure:21268: $? = 0
configure:21271: test -s conftest
configure:21274: $? = 0
configure:21287: result: yes
configure:21299: checking for main in -lncurses
configure:21323: gcc -o 
conftest -g -O2 -I/usr/local/include -L/usr/local/lib 
onftest.c -lncurses  -lm  >&5
/usr/bin/ld: cannot find -lncurses
collect2: ld returned 1 exit status
configure:21329: $? = 1
configure: failed program was:
| /* confdefs.h.  */
|
| #define PACKAGE_NAME "R"
| #define PACKAGE_TARNAME "R"
| #define PACKAGE_VERSION "2.0.1"
| #define PACKAGE_STRING "R 2.0.1"
| #define PACKAGE_BUGREPORT "r-bugs at R-project.org"
| #define PACKAGE "R"
| #define VERSION "2.0.1"
| #define R_PLATFORM "i686-pc-linux-gnu"
| #define R_CPU "i686"
| #define R_VENDOR "pc"
| #define R_OS "linux-gnu"
| #define Unix 1
| #ifdef __cplusplus
| extern "C" void std::exit (int) throw (); using std::exit;
| #endif
| #define STDC_HEADERS 1
| #define HAVE_SYS_TYPES_H 1
| #define HAVE_SYS_STAT_H 1
| #define HAVE_STDLIB_H 1
| #define HAVE_STRING_H 1
| #define HAVE_MEMORY_H 1
| #define HAVE_STRINGS_H 1
| #define HAVE_INTTYPES_H 1
| #define HAVE_STDINT_H 1
| #define HAVE_UNISTD_H 1
| #define HAVE_DLFCN_H 1
| #define HAVE_LIBM 1
| /* end confdefs.h.  */
|
|
| int
| main ()
| {
| main ();
|   ;
|   return 0;
| }
configure:21355: result: no
configure:21366: checking for main in -ltermcap
configure:21390: gcc -o 
conftest -g -O2 -I/usr/local/include -L/usr/local/lib 
onftest.c -ltermcap  -lm  >&5
/usr/bin/ld: cannot find -ltermcap
collect2: ld returned 1 exit status
configure:21396: $? = 1
configure: failed program was:
| /* confdefs.h.  */
|
| #define PACKAGE_NAME "R"
| #define PACKAGE_TARNAME "R"
| #define PACKAGE_VERSION "2.0.1"
| #define PACKAGE_STRING "R 2.0.1"
| #define PACKAGE_BUGREPORT "r-bugs at R-project.org"
| #define PACKAGE "R"
| #define VERSION "2.0.1"
| #define R_PLATFORM "i686-pc-linux-gnu"
| #define R_CPU "i686"
| #define R_VENDOR "pc"
| #define R_OS "linux-gnu"
| #define Unix 1
| #ifdef __cplusplus
| extern "C" void std::exit (int) throw (); using std::exit;
| #endif
| #define STDC_HEADERS 1
| #define HAVE_SYS_TYPES_H 1
| #define HAVE_SYS_STAT_H 1
| #define HAVE_STDLIB_H 1
| #define HAVE_STRING_H 1
| #define HAVE_MEMORY_H 1
| #define HAVE_STRINGS_H 1
| #define HAVE_INTTYPES_H 1
| #define HAVE_STDINT_H 1
| #define HAVE_UNISTD_H 1
| #define HAVE_DLFCN_H 1
| #define HAVE_LIBM 1
| /* end confdefs.h.  */
|
|
| int
| main ()
| {
| main ();
|   ;
|   return 0;
| }
configure:21422: result: no
configure:21433: checking for main in -ltermlib
configure:21457: gcc -o 
conftest -g -O2 -I/usr/local/include -L/usr/local/lib 
onftest.c -ltermlib  -lm  >&5
/usr/bin/ld: cannot find -ltermlib
collect2: ld returned 1 exit status
configure:21463: $? = 1
configure: failed program was:
| /* confdefs.h.  */
|
| #define PACKAGE_NAME "R"
| #define PACKAGE_TARNAME "R"
| #define PACKAGE_VERSION "2.0.1"
| #define PACKAGE_STRING "R 2.0.1"
| #define PACKAGE_BUGREPORT "r-bugs at R-project.org"
| #define PACKAGE "R"
| #define VERSION "2.0.1"
| #define R_PLATFORM "i686-pc-linux-gnu"
| #define R_CPU "i686"
| #define R_VENDOR "pc"
| #define R_OS "linux-gnu"
| #define Unix 1
| #ifdef __cplusplus
| extern "C" void std::exit (int) throw (); using std::exit;
| #endif
| #define STDC_HEADERS 1
| #define HAVE_SYS_TYPES_H 1
| #define HAVE_SYS_STAT_H 1
| #define HAVE_STDLIB_H 1
| #define HAVE_STRING_H 1
| #define HAVE_MEMORY_H 1
| #define HAVE_STRINGS_H 1
| #define HAVE_INTTYPES_H 1
| #define HAVE_STDINT_H 1
| #define HAVE_UNISTD_H 1
| #define HAVE_DLFCN_H 1
| #define HAVE_LIBM 1
| /* end confdefs.h.  */
|
|
| int
| main ()
| {
| main ();
|   ;
|   return 0;
| }
configure:21489: result: no
configure:21598: checking for dlopen in -ldl
configure:21628: gcc -o 
conftest -g -O2 -I/usr/local/include -L/usr/local/lib conftest.c -ldl  -lm 
 >&5
configure:21634: $? = 0
configure:21638: test -z
    || test ! -s conftest.err
configure:21641: $? = 0
configure:21644: test -s conftest
configure:21647: $? = 0
configure:21660: result: yes
configure:21677: checking for rl_callback_read_char in -lreadline
configure:21707: gcc -o 
conftest -g -O2 -I/usr/local/include -L/usr/local/lib 
onftest.c -lreadline  -ldl -lm  >&5
/usr/bin/ld: cannot find -lreadline
collect2: ld returned 1 exit status
configure:21713: $? = 1
configure: failed program was:
| /* confdefs.h.  */
|
| #define PACKAGE_NAME "R"
| #define PACKAGE_TARNAME "R"
| #define PACKAGE_VERSION "2.0.1"
| #define PACKAGE_STRING "R 2.0.1"
| #define PACKAGE_BUGREPORT "r-bugs at R-project.org"
| #define PACKAGE "R"
| #define VERSION "2.0.1"
| #define R_PLATFORM "i686-pc-linux-gnu"
| #define R_CPU "i686"
| #define R_VENDOR "pc"
| #define R_OS "linux-gnu"
| #define Unix 1
| #ifdef __cplusplus
| extern "C" void std::exit (int) throw (); using std::exit;
| #endif
| #define STDC_HEADERS 1
| #define HAVE_SYS_TYPES_H 1
| #define HAVE_SYS_STAT_H 1
| #define HAVE_STDLIB_H 1
| #define HAVE_STRING_H 1
| #define HAVE_MEMORY_H 1
| #define HAVE_STRINGS_H 1
| #define HAVE_INTTYPES_H 1
| #define HAVE_STDINT_H 1
| #define HAVE_UNISTD_H 1
| #define HAVE_DLFCN_H 1
| #define HAVE_LIBM 1
| #define HAVE_LIBDL 1
| /* end confdefs.h.  */
|
| /* Override any gcc2 internal prototype to avoid an error.  */
| #ifdef __cplusplus
| extern "C"
| #endif
| /* We use char because int might match the return type of a gcc2
|    builtin and then its argument prototype would still apply.  */
| char rl_callback_read_char ();
| int
| main ()
| {
| rl_callback_read_char ();
|   ;
|   return 0;
| }
configure:21739: result: no
configure:21755: checking for ANSI C header files
configure:21908: result: yes
configure:21918: checking whether time.h and sys/time.h may both be included
configure:21943: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:21949: $? = 0
configure:21953: test -z
    || test ! -s conftest.err
configure:21956: $? = 0
configure:21959: test -s conftest.o
configure:21962: $? = 0
configure:21973: result: yes
configure:21991: checking for dirent.h that defines DIR
configure:22015: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:22021: $? = 0
configure:22025: test -z
    || test ! -s conftest.err
configure:22028: $? = 0
configure:22031: test -s conftest.o
configure:22034: $? = 0
configure:22045: result: yes
configure:22058: checking for library containing opendir
configure:22088: gcc -o 
conftest -g -O2 -I/usr/local/include -L/usr/local/lib conftest.c -ldl -lm 
 >&5
configure:22094: $? = 0
configure:22098: test -z
    || test ! -s conftest.err
configure:22101: $? = 0
configure:22104: test -s conftest
configure:22107: $? = 0
configure:22177: result: none required
configure:22313: checking for sys/wait.h that is POSIX.1 compatible
configure:22344: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:22350: $? = 0
configure:22354: test -z
    || test ! -s conftest.err
configure:22357: $? = 0
configure:22360: test -s conftest.o
configure:22363: $? = 0
configure:22374: result: yes
configure:22432: checking arpa/inet.h usability
configure:22444: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:22450: $? = 0
configure:22454: test -z
    || test ! -s conftest.err
configure:22457: $? = 0
configure:22460: test -s conftest.o
configure:22463: $? = 0
configure:22473: result: yes
configure:22477: checking arpa/inet.h presence
configure:22487: gcc -E -I/usr/local/include conftest.c
configure:22493: $? = 0
configure:22513: result: yes
configure:22548: checking for arpa/inet.h
configure:22555: result: yes
configure:22432: checking dl.h usability
configure:22444: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
conftest.c:71:16: dl.h: No such file or directory
configure:22450: $? = 1
configure: failed program was:
| /* confdefs.h.  */
|
| #define PACKAGE_NAME "R"
| #define PACKAGE_TARNAME "R"
| #define PACKAGE_VERSION "2.0.1"
| #define PACKAGE_STRING "R 2.0.1"
| #define PACKAGE_BUGREPORT "r-bugs at R-project.org"
| #define PACKAGE "R"
| #define VERSION "2.0.1"
| #define R_PLATFORM "i686-pc-linux-gnu"
| #define R_CPU "i686"
| #define R_VENDOR "pc"
| #define R_OS "linux-gnu"
| #define Unix 1
| #ifdef __cplusplus
| extern "C" void std::exit (int) throw (); using std::exit;
| #endif
| #define STDC_HEADERS 1
| #define HAVE_SYS_TYPES_H 1
| #define HAVE_SYS_STAT_H 1
| #define HAVE_STDLIB_H 1
| #define HAVE_STRING_H 1
| #define HAVE_MEMORY_H 1
| #define HAVE_STRINGS_H 1
| #define HAVE_INTTYPES_H 1
| #define HAVE_STDINT_H 1
| #define HAVE_UNISTD_H 1
| #define HAVE_DLFCN_H 1
| #define HAVE_LIBM 1
| #define HAVE_LIBDL 1
| #define STDC_HEADERS 1
| #define TIME_WITH_SYS_TIME 1
| #define HAVE_DIRENT_H 1
| #define HAVE_SYS_WAIT_H 1
| #define HAVE_ARPA_INET_H 1
| /* end confdefs.h.  */
| #include <stdio.h>
| #if HAVE_SYS_TYPES_H
| # include <sys/types.h>
| #endif
| #if HAVE_SYS_STAT_H
| # include <sys/stat.h>
| #endif
| #if STDC_HEADERS
| # include <stdlib.h>
| # include <stddef.h>
| #else
| # if HAVE_STDLIB_H
| #  include <stdlib.h>
| # endif
| #endif
| #if HAVE_STRING_H
| # if !STDC_HEADERS && HAVE_MEMORY_H
| #  include <memory.h>
| # endif
| # include <string.h>
| #endif
| #if HAVE_STRINGS_H
| # include <strings.h>
| #endif
| #if HAVE_INTTYPES_H
| # include <inttypes.h>
| #else
| # if HAVE_STDINT_H
| #  include <stdint.h>
| # endif
| #endif
| #if HAVE_UNISTD_H
| # include <unistd.h>
| #endif
| #include <dl.h>
configure:22473: result: no
configure:22477: checking dl.h presence
configure:22487: gcc -E -I/usr/local/include conftest.c
conftest.c:37:16: dl.h: No such file or directory
configure:22493: $? = 1
configure: failed program was:
| /* confdefs.h.  */
|
| #define PACKAGE_NAME "R"
| #define PACKAGE_TARNAME "R"
| #define PACKAGE_VERSION "2.0.1"
| #define PACKAGE_STRING "R 2.0.1"
| #define PACKAGE_BUGREPORT "r-bugs at R-project.org"
| #define PACKAGE "R"
| #define VERSION "2.0.1"
| #define R_PLATFORM "i686-pc-linux-gnu"
| #define R_CPU "i686"
| #define R_VENDOR "pc"
| #define R_OS "linux-gnu"
| #define Unix 1
| #ifdef __cplusplus
| extern "C" void std::exit (int) throw (); using std::exit;
| #endif
| #define STDC_HEADERS 1
| #define HAVE_SYS_TYPES_H 1
| #define HAVE_SYS_STAT_H 1
| #define HAVE_STDLIB_H 1
| #define HAVE_STRING_H 1
| #define HAVE_MEMORY_H 1
| #define HAVE_STRINGS_H 1
| #define HAVE_INTTYPES_H 1
| #define HAVE_STDINT_H 1
| #define HAVE_UNISTD_H 1
| #define HAVE_DLFCN_H 1
| #define HAVE_LIBM 1
| #define HAVE_LIBDL 1
| #define STDC_HEADERS 1
| #define TIME_WITH_SYS_TIME 1
| #define HAVE_DIRENT_H 1
| #define HAVE_SYS_WAIT_H 1
| #define HAVE_ARPA_INET_H 1
| /* end confdefs.h.  */
| #include <dl.h>
configure:22513: result: no
configure:22548: checking for dl.h
configure:22555: result: no
configure:22423: checking for dlfcn.h
configure:22428: result: yes
configure:22432: checking elf.h usability
configure:22444: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:22450: $? = 0
configure:22454: test -z
    || test ! -s conftest.err
configure:22457: $? = 0
configure:22460: test -s conftest.o
configure:22463: $? = 0
configure:22473: result: yes
configure:22477: checking elf.h presence
configure:22487: gcc -E -I/usr/local/include conftest.c
configure:22493: $? = 0
configure:22513: result: yes
configure:22548: checking for elf.h
configure:22555: result: yes
configure:22432: checking fcntl.h usability
configure:22444: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:22450: $? = 0
configure:22454: test -z
    || test ! -s conftest.err
configure:22457: $? = 0
configure:22460: test -s conftest.o
configure:22463: $? = 0
configure:22473: result: yes
configure:22477: checking fcntl.h presence
configure:22487: gcc -E -I/usr/local/include conftest.c
configure:22493: $? = 0
configure:22513: result: yes
configure:22548: checking for fcntl.h
configure:22555: result: yes
configure:22432: checking floatingpoint.h usability
configure:22444: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
conftest.c:74:27: floatingpoint.h: No such file or directory
configure:22450: $? = 1
configure: failed program was:
| /* confdefs.h.  */
|
| #define PACKAGE_NAME "R"
| #define PACKAGE_TARNAME "R"
| #define PACKAGE_VERSION "2.0.1"
| #define PACKAGE_STRING "R 2.0.1"
| #define PACKAGE_BUGREPORT "r-bugs at R-project.org"
| #define PACKAGE "R"
| #define VERSION "2.0.1"
| #define R_PLATFORM "i686-pc-linux-gnu"
| #define R_CPU "i686"
| #define R_VENDOR "pc"
| #define R_OS "linux-gnu"
| #define Unix 1
| #ifdef __cplusplus
| extern "C" void std::exit (int) throw (); using std::exit;
| #endif
| #define STDC_HEADERS 1
| #define HAVE_SYS_TYPES_H 1
| #define HAVE_SYS_STAT_H 1
| #define HAVE_STDLIB_H 1
| #define HAVE_STRING_H 1
| #define HAVE_MEMORY_H 1
| #define HAVE_STRINGS_H 1
| #define HAVE_INTTYPES_H 1
| #define HAVE_STDINT_H 1
| #define HAVE_UNISTD_H 1
| #define HAVE_DLFCN_H 1
| #define HAVE_LIBM 1
| #define HAVE_LIBDL 1
| #define STDC_HEADERS 1
| #define TIME_WITH_SYS_TIME 1
| #define HAVE_DIRENT_H 1
| #define HAVE_SYS_WAIT_H 1
| #define HAVE_ARPA_INET_H 1
| #define HAVE_DLFCN_H 1
| #define HAVE_ELF_H 1
| #define HAVE_FCNTL_H 1
| /* end confdefs.h.  */
| #include <stdio.h>
| #if HAVE_SYS_TYPES_H
| # include <sys/types.h>
| #endif
| #if HAVE_SYS_STAT_H
| # include <sys/stat.h>
| #endif
| #if STDC_HEADERS
| # include <stdlib.h>
| # include <stddef.h>
| #else
| # if HAVE_STDLIB_H
| #  include <stdlib.h>
| # endif
| #endif
| #if HAVE_STRING_H
| # if !STDC_HEADERS && HAVE_MEMORY_H
| #  include <memory.h>
| # endif
| # include <string.h>
| #endif
| #if HAVE_STRINGS_H
| # include <strings.h>
| #endif
| #if HAVE_INTTYPES_H
| # include <inttypes.h>
| #else
| # if HAVE_STDINT_H
| #  include <stdint.h>
| # endif
| #endif
| #if HAVE_UNISTD_H
| # include <unistd.h>
| #endif
| #include <floatingpoint.h>
configure:22473: result: no
configure:22477: checking floatingpoint.h presence
configure:22487: gcc -E -I/usr/local/include conftest.c
conftest.c:40:27: floatingpoint.h: No such file or directory
configure:22493: $? = 1
configure: failed program was:
| /* confdefs.h.  */
|
| #define PACKAGE_NAME "R"
| #define PACKAGE_TARNAME "R"
| #define PACKAGE_VERSION "2.0.1"
| #define PACKAGE_STRING "R 2.0.1"
| #define PACKAGE_BUGREPORT "r-bugs at R-project.org"
| #define PACKAGE "R"
| #define VERSION "2.0.1"
| #define R_PLATFORM "i686-pc-linux-gnu"
| #define R_CPU "i686"
| #define R_VENDOR "pc"
| #define R_OS "linux-gnu"
| #define Unix 1
| #ifdef __cplusplus
| extern "C" void std::exit (int) throw (); using std::exit;
| #endif
| #define STDC_HEADERS 1
| #define HAVE_SYS_TYPES_H 1
| #define HAVE_SYS_STAT_H 1
| #define HAVE_STDLIB_H 1
| #define HAVE_STRING_H 1
| #define HAVE_MEMORY_H 1
| #define HAVE_STRINGS_H 1
| #define HAVE_INTTYPES_H 1
| #define HAVE_STDINT_H 1
| #define HAVE_UNISTD_H 1
| #define HAVE_DLFCN_H 1
| #define HAVE_LIBM 1
| #define HAVE_LIBDL 1
| #define STDC_HEADERS 1
| #define TIME_WITH_SYS_TIME 1
| #define HAVE_DIRENT_H 1
| #define HAVE_SYS_WAIT_H 1
| #define HAVE_ARPA_INET_H 1
| #define HAVE_DLFCN_H 1
| #define HAVE_ELF_H 1
| #define HAVE_FCNTL_H 1
| /* end confdefs.h.  */
| #include <floatingpoint.h>
configure:22513: result: no
configure:22548: checking for floatingpoint.h
configure:22555: result: no
configure:22432: checking fpu_control.h usability
configure:22444: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:22450: $? = 0
configure:22454: test -z
    || test ! -s conftest.err
configure:22457: $? = 0
configure:22460: test -s conftest.o
configure:22463: $? = 0
configure:22473: result: yes
configure:22477: checking fpu_control.h presence
configure:22487: gcc -E -I/usr/local/include conftest.c
configure:22493: $? = 0
configure:22513: result: yes
configure:22548: checking for fpu_control.h
configure:22555: result: yes
configure:22432: checking grp.h usability
configure:22444: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:22450: $? = 0
configure:22454: test -z
    || test ! -s conftest.err
configure:22457: $? = 0
configure:22460: test -s conftest.o
configure:22463: $? = 0
configure:22473: result: yes
configure:22477: checking grp.h presence
configure:22487: gcc -E -I/usr/local/include conftest.c
configure:22493: $? = 0
configure:22513: result: yes
configure:22548: checking for grp.h
configure:22555: result: yes
configure:22432: checking ieee754.h usability
configure:22444: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:22450: $? = 0
configure:22454: test -z
    || test ! -s conftest.err
configure:22457: $? = 0
configure:22460: test -s conftest.o
configure:22463: $? = 0
configure:22473: result: yes
configure:22477: checking ieee754.h presence
configure:22487: gcc -E -I/usr/local/include conftest.c
configure:22493: $? = 0
configure:22513: result: yes
configure:22548: checking for ieee754.h
configure:22555: result: yes
configure:22432: checking ieeefp.h usability
configure:22444: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
conftest.c:77:20: ieeefp.h: No such file or directory
configure:22450: $? = 1
configure: failed program was:
| /* confdefs.h.  */
|
| #define PACKAGE_NAME "R"
| #define PACKAGE_TARNAME "R"
| #define PACKAGE_VERSION "2.0.1"
| #define PACKAGE_STRING "R 2.0.1"
| #define PACKAGE_BUGREPORT "r-bugs at R-project.org"
| #define PACKAGE "R"
| #define VERSION "2.0.1"
| #define R_PLATFORM "i686-pc-linux-gnu"
| #define R_CPU "i686"
| #define R_VENDOR "pc"
| #define R_OS "linux-gnu"
| #define Unix 1
| #ifdef __cplusplus
| extern "C" void std::exit (int) throw (); using std::exit;
| #endif
| #define STDC_HEADERS 1
| #define HAVE_SYS_TYPES_H 1
| #define HAVE_SYS_STAT_H 1
| #define HAVE_STDLIB_H 1
| #define HAVE_STRING_H 1
| #define HAVE_MEMORY_H 1
| #define HAVE_STRINGS_H 1
| #define HAVE_INTTYPES_H 1
| #define HAVE_STDINT_H 1
| #define HAVE_UNISTD_H 1
| #define HAVE_DLFCN_H 1
| #define HAVE_LIBM 1
| #define HAVE_LIBDL 1
| #define STDC_HEADERS 1
| #define TIME_WITH_SYS_TIME 1
| #define HAVE_DIRENT_H 1
| #define HAVE_SYS_WAIT_H 1
| #define HAVE_ARPA_INET_H 1
| #define HAVE_DLFCN_H 1
| #define HAVE_ELF_H 1
| #define HAVE_FCNTL_H 1
| #define HAVE_FPU_CONTROL_H 1
| #define HAVE_GRP_H 1
| #define HAVE_IEEE754_H 1
| /* end confdefs.h.  */
| #include <stdio.h>
| #if HAVE_SYS_TYPES_H
| # include <sys/types.h>
| #endif
| #if HAVE_SYS_STAT_H
| # include <sys/stat.h>
| #endif
| #if STDC_HEADERS
| # include <stdlib.h>
| # include <stddef.h>
| #else
| # if HAVE_STDLIB_H
| #  include <stdlib.h>
| # endif
| #endif
| #if HAVE_STRING_H
| # if !STDC_HEADERS && HAVE_MEMORY_H
| #  include <memory.h>
| # endif
| # include <string.h>
| #endif
| #if HAVE_STRINGS_H
| # include <strings.h>
| #endif
| #if HAVE_INTTYPES_H
| # include <inttypes.h>
| #else
| # if HAVE_STDINT_H
| #  include <stdint.h>
| # endif
| #endif
| #if HAVE_UNISTD_H
| # include <unistd.h>
| #endif
| #include <ieeefp.h>
configure:22473: result: no
configure:22477: checking ieeefp.h presence
configure:22487: gcc -E -I/usr/local/include conftest.c
conftest.c:43:20: ieeefp.h: No such file or directory
configure:22493: $? = 1
configure: failed program was:
| /* confdefs.h.  */
|
| #define PACKAGE_NAME "R"
| #define PACKAGE_TARNAME "R"
| #define PACKAGE_VERSION "2.0.1"
| #define PACKAGE_STRING "R 2.0.1"
| #define PACKAGE_BUGREPORT "r-bugs at R-project.org"
| #define PACKAGE "R"
| #define VERSION "2.0.1"
| #define R_PLATFORM "i686-pc-linux-gnu"
| #define R_CPU "i686"
| #define R_VENDOR "pc"
| #define R_OS "linux-gnu"
| #define Unix 1
| #ifdef __cplusplus
| extern "C" void std::exit (int) throw (); using std::exit;
| #endif
| #define STDC_HEADERS 1
| #define HAVE_SYS_TYPES_H 1
| #define HAVE_SYS_STAT_H 1
| #define HAVE_STDLIB_H 1
| #define HAVE_STRING_H 1
| #define HAVE_MEMORY_H 1
| #define HAVE_STRINGS_H 1
| #define HAVE_INTTYPES_H 1
| #define HAVE_STDINT_H 1
| #define HAVE_UNISTD_H 1
| #define HAVE_DLFCN_H 1
| #define HAVE_LIBM 1
| #define HAVE_LIBDL 1
| #define STDC_HEADERS 1
| #define TIME_WITH_SYS_TIME 1
| #define HAVE_DIRENT_H 1
| #define HAVE_SYS_WAIT_H 1
| #define HAVE_ARPA_INET_H 1
| #define HAVE_DLFCN_H 1
| #define HAVE_ELF_H 1
| #define HAVE_FCNTL_H 1
| #define HAVE_FPU_CONTROL_H 1
| #define HAVE_GRP_H 1
| #define HAVE_IEEE754_H 1
| /* end confdefs.h.  */
| #include <ieeefp.h>
configure:22513: result: no
configure:22548: checking for ieeefp.h
configure:22555: result: no
configure:22432: checking limits.h usability
configure:22444: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:22450: $? = 0
configure:22454: test -z
    || test ! -s conftest.err
configure:22457: $? = 0
configure:22460: test -s conftest.o
configure:22463: $? = 0
configure:22473: result: yes
configure:22477: checking limits.h presence
configure:22487: gcc -E -I/usr/local/include conftest.c
configure:22493: $? = 0
configure:22513: result: yes
configure:22548: checking for limits.h
configure:22555: result: yes
configure:22432: checking locale.h usability
configure:22444: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:22450: $? = 0
configure:22454: test -z
    || test ! -s conftest.err
configure:22457: $? = 0
configure:22460: test -s conftest.o
configure:22463: $? = 0
configure:22473: result: yes
configure:22477: checking locale.h presence
configure:22487: gcc -E -I/usr/local/include conftest.c
configure:22493: $? = 0
configure:22513: result: yes
configure:22548: checking for locale.h
configure:22555: result: yes
configure:22432: checking netdb.h usability
configure:22444: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:22450: $? = 0
configure:22454: test -z
    || test ! -s conftest.err
configure:22457: $? = 0
configure:22460: test -s conftest.o
configure:22463: $? = 0
configure:22473: result: yes
configure:22477: checking netdb.h presence
configure:22487: gcc -E -I/usr/local/include conftest.c
configure:22493: $? = 0
configure:22513: result: yes
configure:22548: checking for netdb.h
configure:22555: result: yes
configure:22432: checking netinet/in.h usability
configure:22444: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:22450: $? = 0
configure:22454: test -z
    || test ! -s conftest.err
configure:22457: $? = 0
configure:22460: test -s conftest.o
configure:22463: $? = 0
configure:22473: result: yes
configure:22477: checking netinet/in.h presence
configure:22487: gcc -E -I/usr/local/include conftest.c
configure:22493: $? = 0
configure:22513: result: yes
configure:22548: checking for netinet/in.h
configure:22555: result: yes
configure:22432: checking pwd.h usability
configure:22444: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:22450: $? = 0
configure:22454: test -z
    || test ! -s conftest.err
configure:22457: $? = 0
configure:22460: test -s conftest.o
configure:22463: $? = 0
configure:22473: result: yes
configure:22477: checking pwd.h presence
configure:22487: gcc -E -I/usr/local/include conftest.c
configure:22493: $? = 0
configure:22513: result: yes
configure:22548: checking for pwd.h
configure:22555: result: yes
configure:22432: checking readline/history.h usability
configure:22444: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
conftest.c:82:30: readline/history.h: No such file or directory
configure:22450: $? = 1
configure: failed program was:
| /* confdefs.h.  */
|
| #define PACKAGE_NAME "R"
| #define PACKAGE_TARNAME "R"
| #define PACKAGE_VERSION "2.0.1"
| #define PACKAGE_STRING "R 2.0.1"
| #define PACKAGE_BUGREPORT "r-bugs at R-project.org"
| #define PACKAGE "R"
| #define VERSION "2.0.1"
| #define R_PLATFORM "i686-pc-linux-gnu"
| #define R_CPU "i686"
| #define R_VENDOR "pc"
| #define R_OS "linux-gnu"
| #define Unix 1
| #ifdef __cplusplus
| extern "C" void std::exit (int) throw (); using std::exit;
| #endif
| #define STDC_HEADERS 1
| #define HAVE_SYS_TYPES_H 1
| #define HAVE_SYS_STAT_H 1
| #define HAVE_STDLIB_H 1
| #define HAVE_STRING_H 1
| #define HAVE_MEMORY_H 1
| #define HAVE_STRINGS_H 1
| #define HAVE_INTTYPES_H 1
| #define HAVE_STDINT_H 1
| #define HAVE_UNISTD_H 1
| #define HAVE_DLFCN_H 1
| #define HAVE_LIBM 1
| #define HAVE_LIBDL 1
| #define STDC_HEADERS 1
| #define TIME_WITH_SYS_TIME 1
| #define HAVE_DIRENT_H 1
| #define HAVE_SYS_WAIT_H 1
| #define HAVE_ARPA_INET_H 1
| #define HAVE_DLFCN_H 1
| #define HAVE_ELF_H 1
| #define HAVE_FCNTL_H 1
| #define HAVE_FPU_CONTROL_H 1
| #define HAVE_GRP_H 1
| #define HAVE_IEEE754_H 1
| #define HAVE_LIMITS_H 1
| #define HAVE_LOCALE_H 1
| #define HAVE_NETDB_H 1
| #define HAVE_NETINET_IN_H 1
| #define HAVE_PWD_H 1
| /* end confdefs.h.  */
| #include <stdio.h>
| #if HAVE_SYS_TYPES_H
| # include <sys/types.h>
| #endif
| #if HAVE_SYS_STAT_H
| # include <sys/stat.h>
| #endif
| #if STDC_HEADERS
| # include <stdlib.h>
| # include <stddef.h>
| #else
| # if HAVE_STDLIB_H
| #  include <stdlib.h>
| # endif
| #endif
| #if HAVE_STRING_H
| # if !STDC_HEADERS && HAVE_MEMORY_H
| #  include <memory.h>
| # endif
| # include <string.h>
| #endif
| #if HAVE_STRINGS_H
| # include <strings.h>
| #endif
| #if HAVE_INTTYPES_H
| # include <inttypes.h>
| #else
| # if HAVE_STDINT_H
| #  include <stdint.h>
| # endif
| #endif
| #if HAVE_UNISTD_H
| # include <unistd.h>
| #endif
| #include <readline/history.h>
configure:22473: result: no
configure:22477: checking readline/history.h presence
configure:22487: gcc -E -I/usr/local/include conftest.c
conftest.c:48:30: readline/history.h: No such file or directory
configure:22493: $? = 1
configure: failed program was:
| /* confdefs.h.  */
|
| #define PACKAGE_NAME "R"
| #define PACKAGE_TARNAME "R"
| #define PACKAGE_VERSION "2.0.1"
| #define PACKAGE_STRING "R 2.0.1"
| #define PACKAGE_BUGREPORT "r-bugs at R-project.org"
| #define PACKAGE "R"
| #define VERSION "2.0.1"
| #define R_PLATFORM "i686-pc-linux-gnu"
| #define R_CPU "i686"
| #define R_VENDOR "pc"
| #define R_OS "linux-gnu"
| #define Unix 1
| #ifdef __cplusplus
| extern "C" void std::exit (int) throw (); using std::exit;
| #endif
| #define STDC_HEADERS 1
| #define HAVE_SYS_TYPES_H 1
| #define HAVE_SYS_STAT_H 1
| #define HAVE_STDLIB_H 1
| #define HAVE_STRING_H 1
| #define HAVE_MEMORY_H 1
| #define HAVE_STRINGS_H 1
| #define HAVE_INTTYPES_H 1
| #define HAVE_STDINT_H 1
| #define HAVE_UNISTD_H 1
| #define HAVE_DLFCN_H 1
| #define HAVE_LIBM 1
| #define HAVE_LIBDL 1
| #define STDC_HEADERS 1
| #define TIME_WITH_SYS_TIME 1
| #define HAVE_DIRENT_H 1
| #define HAVE_SYS_WAIT_H 1
| #define HAVE_ARPA_INET_H 1
| #define HAVE_DLFCN_H 1
| #define HAVE_ELF_H 1
| #define HAVE_FCNTL_H 1
| #define HAVE_FPU_CONTROL_H 1
| #define HAVE_GRP_H 1
| #define HAVE_IEEE754_H 1
| #define HAVE_LIMITS_H 1
| #define HAVE_LOCALE_H 1
| #define HAVE_NETDB_H 1
| #define HAVE_NETINET_IN_H 1
| #define HAVE_PWD_H 1
| /* end confdefs.h.  */
| #include <readline/history.h>
configure:22513: result: no
configure:22548: checking for readline/history.h
configure:22555: result: no
configure:22432: checking readline/readline.h usability
configure:22444: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
conftest.c:82:31: readline/readline.h: No such file or directory
configure:22450: $? = 1
configure: failed program was:
| /* confdefs.h.  */
|
| #define PACKAGE_NAME "R"
| #define PACKAGE_TARNAME "R"
| #define PACKAGE_VERSION "2.0.1"
| #define PACKAGE_STRING "R 2.0.1"
| #define PACKAGE_BUGREPORT "r-bugs at R-project.org"
| #define PACKAGE "R"
| #define VERSION "2.0.1"
| #define R_PLATFORM "i686-pc-linux-gnu"
| #define R_CPU "i686"
| #define R_VENDOR "pc"
| #define R_OS "linux-gnu"
| #define Unix 1
| #ifdef __cplusplus
| extern "C" void std::exit (int) throw (); using std::exit;
| #endif
| #define STDC_HEADERS 1
| #define HAVE_SYS_TYPES_H 1
| #define HAVE_SYS_STAT_H 1
| #define HAVE_STDLIB_H 1
| #define HAVE_STRING_H 1
| #define HAVE_MEMORY_H 1
| #define HAVE_STRINGS_H 1
| #define HAVE_INTTYPES_H 1
| #define HAVE_STDINT_H 1
| #define HAVE_UNISTD_H 1
| #define HAVE_DLFCN_H 1
| #define HAVE_LIBM 1
| #define HAVE_LIBDL 1
| #define STDC_HEADERS 1
| #define TIME_WITH_SYS_TIME 1
| #define HAVE_DIRENT_H 1
| #define HAVE_SYS_WAIT_H 1
| #define HAVE_ARPA_INET_H 1
| #define HAVE_DLFCN_H 1
| #define HAVE_ELF_H 1
| #define HAVE_FCNTL_H 1
| #define HAVE_FPU_CONTROL_H 1
| #define HAVE_GRP_H 1
| #define HAVE_IEEE754_H 1
| #define HAVE_LIMITS_H 1
| #define HAVE_LOCALE_H 1
| #define HAVE_NETDB_H 1
| #define HAVE_NETINET_IN_H 1
| #define HAVE_PWD_H 1
| /* end confdefs.h.  */
| #include <stdio.h>
| #if HAVE_SYS_TYPES_H
| # include <sys/types.h>
| #endif
| #if HAVE_SYS_STAT_H
| # include <sys/stat.h>
| #endif
| #if STDC_HEADERS
| # include <stdlib.h>
| # include <stddef.h>
| #else
| # if HAVE_STDLIB_H
| #  include <stdlib.h>
| # endif
| #endif
| #if HAVE_STRING_H
| # if !STDC_HEADERS && HAVE_MEMORY_H
| #  include <memory.h>
| # endif
| # include <string.h>
| #endif
| #if HAVE_STRINGS_H
| # include <strings.h>
| #endif
| #if HAVE_INTTYPES_H
| # include <inttypes.h>
| #else
| # if HAVE_STDINT_H
| #  include <stdint.h>
| # endif
| #endif
| #if HAVE_UNISTD_H
| # include <unistd.h>
| #endif
| #include <readline/readline.h>
configure:22473: result: no
configure:22477: checking readline/readline.h presence
configure:22487: gcc -E -I/usr/local/include conftest.c
conftest.c:48:31: readline/readline.h: No such file or directory
configure:22493: $? = 1
configure: failed program was:
| /* confdefs.h.  */
|
| #define PACKAGE_NAME "R"
| #define PACKAGE_TARNAME "R"
| #define PACKAGE_VERSION "2.0.1"
| #define PACKAGE_STRING "R 2.0.1"
| #define PACKAGE_BUGREPORT "r-bugs at R-project.org"
| #define PACKAGE "R"
| #define VERSION "2.0.1"
| #define R_PLATFORM "i686-pc-linux-gnu"
| #define R_CPU "i686"
| #define R_VENDOR "pc"
| #define R_OS "linux-gnu"
| #define Unix 1
| #ifdef __cplusplus
| extern "C" void std::exit (int) throw (); using std::exit;
| #endif
| #define STDC_HEADERS 1
| #define HAVE_SYS_TYPES_H 1
| #define HAVE_SYS_STAT_H 1
| #define HAVE_STDLIB_H 1
| #define HAVE_STRING_H 1
| #define HAVE_MEMORY_H 1
| #define HAVE_STRINGS_H 1
| #define HAVE_INTTYPES_H 1
| #define HAVE_STDINT_H 1
| #define HAVE_UNISTD_H 1
| #define HAVE_DLFCN_H 1
| #define HAVE_LIBM 1
| #define HAVE_LIBDL 1
| #define STDC_HEADERS 1
| #define TIME_WITH_SYS_TIME 1
| #define HAVE_DIRENT_H 1
| #define HAVE_SYS_WAIT_H 1
| #define HAVE_ARPA_INET_H 1
| #define HAVE_DLFCN_H 1
| #define HAVE_ELF_H 1
| #define HAVE_FCNTL_H 1
| #define HAVE_FPU_CONTROL_H 1
| #define HAVE_GRP_H 1
| #define HAVE_IEEE754_H 1
| #define HAVE_LIMITS_H 1
| #define HAVE_LOCALE_H 1
| #define HAVE_NETDB_H 1
| #define HAVE_NETINET_IN_H 1
| #define HAVE_PWD_H 1
| /* end confdefs.h.  */
| #include <readline/readline.h>
configure:22513: result: no
configure:22548: checking for readline/readline.h
configure:22555: result: no
configure:22423: checking for strings.h
configure:22428: result: yes
configure:22432: checking sys/param.h usability
configure:22444: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:22450: $? = 0
configure:22454: test -z
    || test ! -s conftest.err
configure:22457: $? = 0
configure:22460: test -s conftest.o
configure:22463: $? = 0
configure:22473: result: yes
configure:22477: checking sys/param.h presence
configure:22487: gcc -E -I/usr/local/include conftest.c
configure:22493: $? = 0
configure:22513: result: yes
configure:22548: checking for sys/param.h
configure:22555: result: yes
configure:22432: checking sys/select.h usability
configure:22444: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:22450: $? = 0
configure:22454: test -z
    || test ! -s conftest.err
configure:22457: $? = 0
configure:22460: test -s conftest.o
configure:22463: $? = 0
configure:22473: result: yes
configure:22477: checking sys/select.h presence
configure:22487: gcc -E -I/usr/local/include conftest.c
configure:22493: $? = 0
configure:22513: result: yes
configure:22548: checking for sys/select.h
configure:22555: result: yes
configure:22432: checking sys/socket.h usability
configure:22444: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:22450: $? = 0
configure:22454: test -z
    || test ! -s conftest.err
configure:22457: $? = 0
configure:22460: test -s conftest.o
configure:22463: $? = 0
configure:22473: result: yes
configure:22477: checking sys/socket.h presence
configure:22487: gcc -E -I/usr/local/include conftest.c
configure:22493: $? = 0
configure:22513: result: yes
configure:22548: checking for sys/socket.h
configure:22555: result: yes
configure:22423: checking for sys/stat.h
configure:22428: result: yes
configure:22432: checking sys/time.h usability
configure:22444: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:22450: $? = 0
configure:22454: test -z
    || test ! -s conftest.err
configure:22457: $? = 0
configure:22460: test -s conftest.o
configure:22463: $? = 0
configure:22473: result: yes
configure:22477: checking sys/time.h presence
configure:22487: gcc -E -I/usr/local/include conftest.c
configure:22493: $? = 0
configure:22513: result: yes
configure:22548: checking for sys/time.h
configure:22555: result: yes
configure:22432: checking sys/times.h usability
configure:22444: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:22450: $? = 0
configure:22454: test -z
    || test ! -s conftest.err
configure:22457: $? = 0
configure:22460: test -s conftest.o
configure:22463: $? = 0
configure:22473: result: yes
configure:22477: checking sys/times.h presence
configure:22487: gcc -E -I/usr/local/include conftest.c
configure:22493: $? = 0
configure:22513: result: yes
configure:22548: checking for sys/times.h
configure:22555: result: yes
configure:22432: checking sys/utsname.h usability
configure:22444: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:22450: $? = 0
configure:22454: test -z
    || test ! -s conftest.err
configure:22457: $? = 0
configure:22460: test -s conftest.o
configure:22463: $? = 0
configure:22473: result: yes
configure:22477: checking sys/utsname.h presence
configure:22487: gcc -E -I/usr/local/include conftest.c
configure:22493: $? = 0
configure:22513: result: yes
configure:22548: checking for sys/utsname.h
configure:22555: result: yes
configure:22423: checking for unistd.h
configure:22428: result: yes
configure:22432: checking wchar.h usability
configure:22444: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:22450: $? = 0
configure:22454: test -z
    || test ! -s conftest.err
configure:22457: $? = 0
configure:22460: test -s conftest.o
configure:22463: $? = 0
configure:22473: result: yes
configure:22477: checking wchar.h presence
configure:22487: gcc -E -I/usr/local/include conftest.c
configure:22493: $? = 0
configure:22513: result: yes
configure:22548: checking for wchar.h
configure:22555: result: yes
configure:22588: checking errno.h usability
configure:22600: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:22606: $? = 0
configure:22610: test -z
    || test ! -s conftest.err
configure:22613: $? = 0
configure:22616: test -s conftest.o
configure:22619: $? = 0
configure:22629: result: yes
configure:22633: checking errno.h presence
configure:22643: gcc -E -I/usr/local/include conftest.c
configure:22649: $? = 0
configure:22669: result: yes
configure:22704: checking for errno.h
configure:22711: result: yes
configure:22588: checking stdarg.h usability
configure:22600: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:22606: $? = 0
configure:22610: test -z
    || test ! -s conftest.err
configure:22613: $? = 0
configure:22616: test -s conftest.o
configure:22619: $? = 0
configure:22629: result: yes
configure:22633: checking stdarg.h presence
configure:22643: gcc -E -I/usr/local/include conftest.c
configure:22649: $? = 0
configure:22669: result: yes
configure:22704: checking for stdarg.h
configure:22711: result: yes
configure:22579: checking for string.h
configure:22584: result: yes
configure:22740: checking whether setjmp.h is POSIX.1 compatible
configure:22763: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:22769: $? = 0
configure:22773: test -z
    || test ! -s conftest.err
configure:22776: $? = 0
configure:22779: test -s conftest.o
configure:22782: $? = 0
configure:22793: result: yes
configure:22803: checking for GNU C library with version >= 2
configure:22830: result: yes
configure:22843: checking return type of signal handlers
configure:22874: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:22880: $? = 0
configure:22884: test -z
    || test ! -s conftest.err
configure:22887: $? = 0
configure:22890: test -s conftest.o
configure:22893: $? = 0
configure:22904: result: void
configure:22912: checking for pid_t
configure:22936: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:22942: $? = 0
configure:22946: test -z
    || test ! -s conftest.err
configure:22949: $? = 0
configure:22952: test -s conftest.o
configure:22955: $? = 0
configure:22966: result: yes
configure:22978: checking for size_t
configure:23002: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:23008: $? = 0
configure:23012: test -z
    || test ! -s conftest.err
configure:23015: $? = 0
configure:23018: test -s conftest.o
configure:23021: $? = 0
configure:23032: result: yes
configure:23044: checking whether SIZE_MAX is declared
configure:23082: gcc -o 
conftest -g -O2 -I/usr/local/include -L/usr/local/lib conftest.c -ldl -lm 
 >&5
configure:23085: $? = 0
configure:23087: ./conftest
configure:23090: $? = 0
configure:23104: result: yes
configure:23114: checking for blkcnt_t
configure:23138: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:23144: $? = 0
configure:23148: test -z
    || test ! -s conftest.err
configure:23151: $? = 0
configure:23154: test -s conftest.o
configure:23157: $? = 0
configure:23168: result: yes
configure:23182: checking for type of socket length
configure:23213: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:23219: $? = 0
configure:23223: test -z
    || test ! -s conftest.err
configure:23226: $? = 0
configure:23229: test -s conftest.o
configure:23232: $? = 0
configure:23255: result: socklen_t *
configure:23271: checking whether byte ordering is bigendian
configure:23298: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:23304: $? = 0
configure:23308: test -z
    || test ! -s conftest.err
configure:23311: $? = 0
configure:23314: test -s conftest.o
configure:23317: $? = 0
configure:23341: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
conftest.c: In function `main':
conftest.c:73: error: `not' undeclared (first use in this function)
conftest.c:73: error: (Each undeclared identifier is reported only once
conftest.c:73: error: for each function it appears in.)
conftest.c:73: error: parse error before "big"
configure:23347: $? = 1
configure: failed program was:
| /* confdefs.h.  */
|
| #define PACKAGE_NAME "R"
| #define PACKAGE_TARNAME "R"
| #define PACKAGE_VERSION "2.0.1"
| #define PACKAGE_STRING "R 2.0.1"
| #define PACKAGE_BUGREPORT "r-bugs at R-project.org"
| #define PACKAGE "R"
| #define VERSION "2.0.1"
| #define R_PLATFORM "i686-pc-linux-gnu"
| #define R_CPU "i686"
| #define R_VENDOR "pc"
| #define R_OS "linux-gnu"
| #define Unix 1
| #ifdef __cplusplus
| extern "C" void std::exit (int) throw (); using std::exit;
| #endif
| #define STDC_HEADERS 1
| #define HAVE_SYS_TYPES_H 1
| #define HAVE_SYS_STAT_H 1
| #define HAVE_STDLIB_H 1
| #define HAVE_STRING_H 1
| #define HAVE_MEMORY_H 1
| #define HAVE_STRINGS_H 1
| #define HAVE_INTTYPES_H 1
| #define HAVE_STDINT_H 1
| #define HAVE_UNISTD_H 1
| #define HAVE_DLFCN_H 1
| #define HAVE_LIBM 1
| #define HAVE_LIBDL 1
| #define STDC_HEADERS 1
| #define TIME_WITH_SYS_TIME 1
| #define HAVE_DIRENT_H 1
| #define HAVE_SYS_WAIT_H 1
| #define HAVE_ARPA_INET_H 1
| #define HAVE_DLFCN_H 1
| #define HAVE_ELF_H 1
| #define HAVE_FCNTL_H 1
| #define HAVE_FPU_CONTROL_H 1
| #define HAVE_GRP_H 1
| #define HAVE_IEEE754_H 1
| #define HAVE_LIMITS_H 1
| #define HAVE_LOCALE_H 1
| #define HAVE_NETDB_H 1
| #define HAVE_NETINET_IN_H 1
| #define HAVE_PWD_H 1
| #define HAVE_STRINGS_H 1
| #define HAVE_SYS_PARAM_H 1
| #define HAVE_SYS_SELECT_H 1
| #define HAVE_SYS_SOCKET_H 1
| #define HAVE_SYS_STAT_H 1
| #define HAVE_SYS_TIME_H 1
| #define HAVE_SYS_TIMES_H 1
| #define HAVE_SYS_UTSNAME_H 1
| #define HAVE_UNISTD_H 1
| #define HAVE_WCHAR_H 1
| #define HAVE_ERRNO_H 1
| #define HAVE_STDARG_H 1
| #define HAVE_STRING_H 1
| #define HAVE_POSIX_SETJMP 1
| #define HAVE_GLIBC2 1
| #define RETSIGTYPE void
| #define HAVE_DECL_SIZE_MAX 1
| #define SOCKLEN_T socklen_t
| /* end confdefs.h.  */
| #include <sys/types.h>
| #include <sys/param.h>
|
| int
| main ()
| {
| #if BYTE_ORDER != BIG_ENDIAN
|  not big endian
| #endif
|
|   ;
|   return 0;
| }
configure:23482: result: no
configure:23501: checking for an ANSI C-conforming const
configure:23568: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:23574: $? = 0
configure:23578: test -z
    || test ! -s conftest.err
configure:23581: $? = 0
configure:23584: test -s conftest.o
configure:23587: $? = 0
configure:23598: result: yes
configure:23609: checking for inline
configure:23629: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:23635: $? = 0
configure:23639: test -z
    || test ! -s conftest.err
configure:23642: $? = 0
configure:23645: test -s conftest.o
configure:23648: $? = 0
configure:23660: result: inline
configure:23679: checking for int
configure:23703: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:23709: $? = 0
configure:23713: test -z
    || test ! -s conftest.err
configure:23716: $? = 0
configure:23719: test -s conftest.o
configure:23722: $? = 0
configure:23733: result: yes
configure:23736: checking size of int
configure:24055: gcc -o 
conftest -g -O2 -I/usr/local/include -L/usr/local/lib conftest.c -ldl -lm 
 >&5
configure:24058: $? = 0
configure:24060: ./conftest
configure:24063: $? = 0
configure:24086: result: 4
configure:24105: checking for long
configure:24129: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:24135: $? = 0
configure:24139: test -z
    || test ! -s conftest.err
configure:24142: $? = 0
configure:24145: test -s conftest.o
configure:24148: $? = 0
configure:24159: result: yes
configure:24162: checking size of long
configure:24481: gcc -o 
conftest -g -O2 -I/usr/local/include -L/usr/local/lib conftest.c -ldl -lm 
 >&5
configure:24484: $? = 0
configure:24486: ./conftest
configure:24489: $? = 0
configure:24512: result: 4
configure:24519: checking for long long
configure:24543: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:24549: $? = 0
configure:24553: test -z
    || test ! -s conftest.err
configure:24556: $? = 0
configure:24559: test -s conftest.o
configure:24562: $? = 0
configure:24573: result: yes
configure:24576: checking size of long long
configure:24895: gcc -o 
conftest -g -O2 -I/usr/local/include -L/usr/local/lib conftest.c -ldl -lm 
 >&5
configure:24898: $? = 0
configure:24900: ./conftest
configure:24903: $? = 0
configure:24926: result: 8
configure:24933: checking for long double
configure:24957: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
configure:24963: $? = 0
configure:24967: test -z
    || test ! -s conftest.err
configure:24970: $? = 0
configure:24973: test -s conftest.o
configure:24976: $? = 0
configure:24987: result: yes
configure:24990: checking size of long double
configure:25309: gcc -o 
conftest -g -O2 -I/usr/local/include -L/usr/local/lib conftest.c -ldl -lm 
 >&5
configure:25312: $? = 0
configure:25314: ./conftest
configure:25317: $? = 0
configure:25340: result: 12
configure:25348: checking whether we can compute C Make dependencies
configure:25376: result: yes, using gcc -MM
configure:25380: checking whether gcc supports -c -o FILE.lo
configure:25388: gcc -g -O2 -c conftest.c -o TMP/conftest.lo 1>&5
configure:25391: $? = 0
configure:25394: gcc -g -O2 -c conftest.c -o TMP/conftest.lo 1>&5
configure:25397: $? = 0
configure:25405: result: yes
configure:25454: checking how to get verbose linking output from fc
configure:25465: fc -c  conftest.f >&5
./configure: line 1: fc: -c: invalid option
fc: usage: fc [-e ename] [-nlr] [first] [last] or fc -s [pat=rep] [cmd]
configure:25471: $? = 2
configure: failed program was:
|       program main
|
|       end
configure:25550: WARNING: compilation failed
configure:25556: result:
configure:25558: checking for Fortran libraries of fc
configure:25585: fc -o conftest   -L/usr/local/lib conftest.f  >&5
./configure: line 1: fc: -o: invalid option
fc: usage: fc [-e ename] [-nlr] [first] [last] or fc -s [pat=rep] [cmd]
configure:25740: result:
configure:25779: checking for dummy main to link with Fortran libraries
configure:25818: gcc -o 
conftest -g -O2 -I/usr/local/include -L/usr/local/lib conftest.c -ldl -lm 
 >&5
configure:25824: $? = 0
configure:25828: test -z
    || test ! -s conftest.err
configure:25831: $? = 0
configure:25834: test -s conftest
configure:25837: $? = 0
configure:25915: result: none
configure:25952: checking for Fortran name-mangling scheme
configure:25966: fc -c  conftest.f >&5
./configure: line 1: fc: -c: invalid option
fc: usage: fc [-e ename] [-nlr] [first] [last] or fc -s [pat=rep] [cmd]
configure:25972: $? = 2
configure: failed program was:
|       subroutine foobar()
|       return
|       end
|       subroutine foo_bar()
|       return
|       end
configure:26180: error: cannot compile a simple Fortran program
See `config.log' for more details.

## ---------------- ##
## Cache variables. ##
## ---------------- ##

ac_cv_build=i686-pc-linux-gnu
ac_cv_build_alias=i686-pc-linux-gnu
ac_cv_c_bigendian=no
ac_cv_c_compiler_gnu=yes
ac_cv_c_const=yes
ac_cv_cxx_compiler_gnu=yes
ac_cv_env_BLAS_LIBS_set=
ac_cv_env_BLAS_LIBS_value=
ac_cv_env_CC_set=
ac_cv_env_CC_value=
ac_cv_env_CFLAGS_set=
ac_cv_env_CFLAGS_value=
ac_cv_env_CPICFLAGS_set=
ac_cv_env_CPICFLAGS_value=
ac_cv_env_CPPFLAGS_set=
ac_cv_env_CPPFLAGS_value=
ac_cv_env_CPP_set=
ac_cv_env_CPP_value=
ac_cv_env_CXXCPP_set=
ac_cv_env_CXXCPP_value=
ac_cv_env_CXXFLAGS_set=
ac_cv_env_CXXFLAGS_value=
ac_cv_env_CXXPICFLAGS_set=
ac_cv_env_CXXPICFLAGS_value=
ac_cv_env_CXX_set=
ac_cv_env_CXX_value=
ac_cv_env_DYLIB_LDFLAGS_set=
ac_cv_env_DYLIB_LDFLAGS_value=
ac_cv_env_DYLIB_LD_set=
ac_cv_env_DYLIB_LD_value=
ac_cv_env_F2C_set=
ac_cv_env_F2C_value=
ac_cv_env_F77_set=
ac_cv_env_F77_value=
ac_cv_env_FFLAGS_set=
ac_cv_env_FFLAGS_value=
ac_cv_env_FPICFLAGS_set=
ac_cv_env_FPICFLAGS_value=
ac_cv_env_LAPACK_LIBS_set=
ac_cv_env_LAPACK_LIBS_value=
ac_cv_env_LDFLAGS_set=
ac_cv_env_LDFLAGS_value=
ac_cv_env_MAIN_CFLAGS_set=
ac_cv_env_MAIN_CFLAGS_value=
ac_cv_env_MAIN_FFLAGS_set=
ac_cv_env_MAIN_FFLAGS_value=
ac_cv_env_MAIN_LDFLAGS_set=
ac_cv_env_MAIN_LDFLAGS_value=
ac_cv_env_MAIN_LD_set=
ac_cv_env_MAIN_LD_value=
ac_cv_env_MAKE_set=
ac_cv_env_MAKE_value=
ac_cv_env_R_BATCHSAVE_set=
ac_cv_env_R_BATCHSAVE_value=
ac_cv_env_R_BROWSER_set=
ac_cv_env_R_BROWSER_value=
ac_cv_env_R_PAPERSIZE_set=
ac_cv_env_R_PAPERSIZE_value=
ac_cv_env_R_PRINTCMD_set=
ac_cv_env_R_PRINTCMD_value=
ac_cv_env_SHLIB_CFLAGS_set=
ac_cv_env_SHLIB_CFLAGS_value=
ac_cv_env_SHLIB_CXXLDFLAGS_set=
ac_cv_env_SHLIB_CXXLDFLAGS_value=
ac_cv_env_SHLIB_CXXLD_set=
ac_cv_env_SHLIB_CXXLD_value=
ac_cv_env_SHLIB_FFLAGS_set=
ac_cv_env_SHLIB_FFLAGS_value=
ac_cv_env_SHLIB_LDFLAGS_set=
ac_cv_env_SHLIB_LDFLAGS_value=
ac_cv_env_SHLIB_LD_set=
ac_cv_env_SHLIB_LD_value=
ac_cv_env_TCLTK_CPPFLAGS_set=
ac_cv_env_TCLTK_CPPFLAGS_value=
ac_cv_env_TCLTK_LIBS_set=
ac_cv_env_TCLTK_LIBS_value=
ac_cv_env_build_alias_set=
ac_cv_env_build_alias_value=
ac_cv_env_host_alias_set=
ac_cv_env_host_alias_value=
ac_cv_env_target_alias_set=
ac_cv_env_target_alias_value=
ac_cv_exeext=
ac_cv_f77_compiler_gnu=no
ac_cv_f77_dummy_main=none
ac_cv_f77_libs=
ac_cv_fortran_dummy_main=none
ac_cv_header_arpa_inet_h=yes
ac_cv_header_dirent_dirent_h=yes
ac_cv_header_dl_h=no
ac_cv_header_dlfcn_h=yes
ac_cv_header_elf_h=yes
ac_cv_header_errno_h=yes
ac_cv_header_fcntl_h=yes
ac_cv_header_floatingpoint_h=no
ac_cv_header_fpu_control_h=yes
ac_cv_header_grp_h=yes
ac_cv_header_ieee754_h=yes
ac_cv_header_ieeefp_h=no
ac_cv_header_inttypes_h=yes
ac_cv_header_limits_h=yes
ac_cv_header_locale_h=yes
ac_cv_header_memory_h=yes
ac_cv_header_netdb_h=yes
ac_cv_header_netinet_in_h=yes
ac_cv_header_pwd_h=yes
ac_cv_header_readline_history_h=no
ac_cv_header_readline_readline_h=no
ac_cv_header_stdarg_h=yes
ac_cv_header_stdc=yes
ac_cv_header_stdint_h=yes
ac_cv_header_stdlib_h=yes
ac_cv_header_string_h=yes
ac_cv_header_strings_h=yes
ac_cv_header_sys_param_h=yes
ac_cv_header_sys_select_h=yes
ac_cv_header_sys_socket_h=yes
ac_cv_header_sys_stat_h=yes
ac_cv_header_sys_time_h=yes
ac_cv_header_sys_times_h=yes
ac_cv_header_sys_types_h=yes
ac_cv_header_sys_utsname_h=yes
ac_cv_header_sys_wait_h=yes
ac_cv_header_time=yes
ac_cv_header_unistd_h=yes
ac_cv_header_wchar_h=yes
ac_cv_host=i686-pc-linux-gnu
ac_cv_host_alias=i686-pc-linux-gnu
ac_cv_lib_dl_dlopen=yes
ac_cv_lib_m_cos=yes
ac_cv_lib_m_sin=yes
ac_cv_lib_ncurses_main=no
ac_cv_lib_readline_rl_callback_read_char=no
ac_cv_lib_termcap_main=no
ac_cv_lib_termlib_main=no
ac_cv_objext=o
ac_cv_path_GETWD=/bin/pwd
ac_cv_path_PAGER=/usr/bin/less
ac_cv_path_PERL=/usr/bin/perl
ac_cv_path_R_BROWSER=/usr/bin/mozilla
ac_cv_path_R_GZIPCMD=/bin/gzip
ac_cv_path_R_PDFVIEWER=/usr/bin/xpdf
ac_cv_path_R_UNZIPCMD=/usr/bin/unzip
ac_cv_path_R_ZIPCMD=/usr/bin/zip
ac_cv_path_SED=/bin/sed
ac_cv_path_install='/usr/bin/install -c'
ac_cv_prog_AR=ar
ac_cv_prog_AWK=mawk
ac_cv_prog_CPP='gcc -E'
ac_cv_prog_CXXCPP='g++ -E'
ac_cv_prog_F77=fc
ac_cv_prog_ac_ct_CC=gcc
ac_cv_prog_ac_ct_CXX=g++
ac_cv_prog_ac_ct_RANLIB=ranlib
ac_cv_prog_ac_ct_STRIP=strip
ac_cv_prog_cc_g=yes
ac_cv_prog_cc_stdc=
ac_cv_prog_cxx_g=yes
ac_cv_prog_egrep='grep -E'
ac_cv_prog_f77_g=no
ac_cv_prog_gcc_traditional=no
ac_cv_prog_make_make_set=yes
ac_cv_search_opendir='none required'
ac_cv_sizeof_int=4
ac_cv_sizeof_long=4
ac_cv_sizeof_long_double=12
ac_cv_sizeof_long_long=8
ac_cv_type_blkcnt_t=yes
ac_cv_type_int=yes
ac_cv_type_long=yes
ac_cv_type_long_double=yes
ac_cv_type_long_long=yes
ac_cv_type_pid_t=yes
ac_cv_type_signal=void
ac_cv_type_size_t=yes
lt_cv_deplibs_check_method=pass_all
lt_cv_file_magic_cmd='$MAGIC_CMD'
lt_cv_file_magic_test_file='/lib/libc.so.6 /lib/libc-2.3.2.so'
lt_cv_ld_reload_flag=-r
lt_cv_objdir=.libs
lt_cv_path_LD=/usr/bin/ld
lt_cv_path_LDCXX=/usr/bin/ld
lt_cv_path_NM='/usr/bin/nm -B'
lt_cv_path_SED=/bin/sed
lt_cv_prog_compiler_c_o=yes
lt_cv_prog_compiler_c_o_CXX=yes
lt_cv_prog_compiler_c_o_F77=no
lt_cv_prog_compiler_rtti_exceptions=no
lt_cv_prog_gnu_ld=yes
lt_cv_prog_gnu_ldcxx=yes
lt_cv_sys_global_symbol_pipe='sed -n -e 
\''s/^.*[  ]\([ABCDGIRSTW][ABCDGIRSTW]*\)[  ][  ]*\(\)\([_A-Za-z][_A-Za-z0-9]*\)$/\1 
\2\3 \3/p'\'''
lt_cv_sys_global_symbol_to_c_name_address='sed -n -e '\''s/^: \([^ ]*\) $/ 
{\"\1\", (lt_ptr) 0},/p'\'' -e '\''s/^[BCDEGRST] \([^ ]*\) \([^ ]*\)$/ 
{"\2", (lt_ptr) \&\2},/p'\'''
lt_cv_sys_global_symbol_to_cdecl='sed -n -e '\''s/^. .* \(.*\)$/extern int 
\1;/p'\'''
lt_cv_sys_max_cmd_len=32768
lt_lt_cv_prog_compiler_c_o='"yes"'
lt_lt_cv_prog_compiler_c_o_CXX='"yes"'
lt_lt_cv_prog_compiler_c_o_F77='"no"'
lt_lt_cv_sys_global_symbol_pipe='"sed -n -e 
\''s/^.*[  ]\\([ABCDGIRSTW][ABCDGIRSTW]*\\)[  ][  ]*\\(\\)\\([_A-Za-z][_A-Za-z0-9]*\\)\$/\\1 
\\2\\3 \\3/p'\''"'
lt_lt_cv_sys_global_symbol_to_c_name_address='"sed -n -e '\''s/^: 
\\([^ ]*\\) \$/  {\\\"\\1\\\", (lt_ptr) 0},/p'\'' -e '\''s/^[BCDEGRST] 
\\([^ ]*\\) \\([^ ]*\\)\$/  {\"\\2\", (lt_ptr) \\&\\2},/p'\''"'
lt_lt_cv_sys_global_symbol_to_cdecl='"sed -n -e '\''s/^. .* 
\\(.*\\)\$/extern int \\1;/p'\''"'
r_cv_c_inline=inline
r_cv_header_glibc2=yes
r_cv_header_setjmp_posix=yes
r_cv_prog_cc_c_o_lo=yes
r_cv_prog_cc_m='gcc -MM'
r_cv_prog_perl_v5=yes
r_cv_size_max=yes
r_cv_type_socklen=socklen_t

## ----------------- ##
## Output variables. ##
## ----------------- ##

ACLOCAL='/home/ales/Programi/R/R-2.0.1/tools/missing aclocal-1.4'
ALLOCA=''
AR='ar'
ARFLAGS='rc'
AUTOCONF='/home/ales/Programi/R/R-2.0.1/tools/missing autoconf'
AUTOHEADER='/home/ales/Programi/R/R-2.0.1/tools/missing autoheader'
AUTOMAKE='/home/ales/Programi/R/R-2.0.1/tools/missing automake-1.4'
AWK='mawk'
BITMAP_LIBS=''
BLAS_LIBS=''
BUILDDIR_IS_SRCDIR='yes'
BUILD_AQUA_FALSE=''
BUILD_AQUA_TRUE=''
BUILD_BZLIB_FALSE=''
BUILD_BZLIB_TRUE=''
BUILD_DLFCN_DARWIN_FALSE=''
BUILD_DLFCN_DARWIN_TRUE='#'
BUILD_PCRE_FALSE=''
BUILD_PCRE_TRUE=''
BUILD_XDR_FALSE=''
BUILD_XDR_TRUE=''
BUILD_ZLIB_FALSE=''
BUILD_ZLIB_TRUE=''
CC='gcc'
CFLAGS='-g -O2'
COMPILE_DOUBLE_COMPLEX_FALSE=''
COMPILE_DOUBLE_COMPLEX_TRUE=''
CPICFLAGS=''
CPP='gcc -E'
CPPFLAGS='-I/usr/local/include'
CXX='g++'
CXXCPP='g++ -E'
CXXFLAGS='-g -O2'
CXXPICFLAGS=''
DEFS=''
DVIPS='false'
DYLIB_EXT=''
DYLIB_LD=''
DYLIB_LDFLAGS=''
ECHO='echo'
ECHO_C=''
ECHO_N='-n'
ECHO_T=''
EGREP='grep -E'
EXEEXT=''
F2C=''
F2CFLAGS=''
F77='fc'
FALSE=''
FFLAGS=''
FLIBS=''
FPICFLAGS=''
GETWD='/bin/pwd'
GNOMEGNORBA_LIBS=''
GNOMEUI_LIBS=''
GNOME_APPLETS_LIBS=''
GNOME_CAPPLET_LIBS=''
GNOME_CONFIG=''
GNOME_IF_FILES=''
GNOME_INCLUDEDIR=''
GNOME_LIBDIR=''
GNOME_LIBS=''
GNORBA_CFLAGS=''
GNORBA_LIBS=''
GTKXMHTML_LIBS=''
HAVE_DOUBLE_COMPLEX=''
HAVE_GNOME=''
HAVE_GNORBA_FALSE=''
HAVE_GNORBA_TRUE=''
HAVE_ORBIT_FALSE=''
HAVE_ORBIT_TRUE=''
INSTALL_DATA='${INSTALL} -m 644'
INSTALL_INFO='$(PERL) $(top_srcdir)/tools/install-info.pl'
INSTALL_PROGRAM='${INSTALL}'
INSTALL_SCRIPT='${INSTALL}'
JAVAC=''
LAPACK_LDFLAGS=''
LAPACK_LIBS=''
LATEX='false'
LDFLAGS='-L/usr/local/lib'
LIBGLADE_CFLAGS=''
LIBGLADE_CONFIG=''
LIBGLADE_LIBS=''
LIBM='-lm'
LIBOBJS=''
LIBR=''
LIBR_LDFLAGS=''
LIBS='-ldl -lm '
LIBTOOL='$(SHELL) $(top_builddir)/libtool'
LIBTOOL_DEPS='tools/ltmain.sh'
LN_S='ln -s'
LTLIBOBJS=''
MAINTAINER_MODE_FALSE=''
MAINTAINER_MODE_TRUE='#'
MAIN_CFLAGS=''
MAIN_FFLAGS=''
MAIN_LD=''
MAIN_LDFLAGS=''
MAKE='make'
MAKEINDEX='false'
MAKEINFO='false'
MAKEINFO_CMD=''
NO_PERL5='false'
OBJEXT='o'
ORBIT_CFLAGS=''
ORBIT_CONFIG=''
ORBIT_IDL=''
ORBIT_LIBS=''
PACKAGE='R'
PACKAGE_BUGREPORT='r-bugs at R-project.org'
PACKAGE_NAME='R'
PACKAGE_STRING='R 2.0.1'
PACKAGE_TARNAME='R'
PACKAGE_VERSION='2.0.1'
PAGER='/usr/bin/less'
PAPERCONF=''
PATH_SEPARATOR=':'
PDFLATEX='false'
PDFTEX='false'
PERL='/usr/bin/perl'
RANLIB='ranlib'
RLAPACK_LDFLAGS=''
RMATH_HAVE_EXPM1=''
RMATH_HAVE_LOG1P=''
RMATH_HAVE_WORKING_LOG1P=''
R_BATCHSAVE=''
R_BROWSER='/usr/bin/mozilla'
R_GUIS=''
R_GZIPCMD='/bin/gzip'
R_LD_LIBRARY_PATH='/usr/local/lib'
R_OSTYPE='unix'
R_PAPERSIZE=''
R_PDFVIEWER='/usr/bin/xpdf'
R_PLATFORM='i686-pc-linux-gnu'
R_PRINTCMD=''
R_PROFILING=''
R_RD4DVI='ae'
R_RD4PDF='times,hyper'
R_UNZIPCMD='/usr/bin/unzip'
R_XTRA_CFLAGS=''
R_XTRA_CPPFLAGS=''
R_XTRA_CXXFLAGS=''
R_XTRA_FFLAGS=''
R_XTRA_LIBS=''
R_ZIPCMD='/usr/bin/zip'
SED='/bin/sed'
SET_MAKE=''
SHELL='/bin/sh'
SHLIB_CFLAGS=''
SHLIB_CXXFLAGS=''
SHLIB_CXXLD=''
SHLIB_CXXLDFLAGS=''
SHLIB_EXT=''
SHLIB_FFLAGS=''
SHLIB_LD=''
SHLIB_LDFLAGS=''
SHLIB_LIBADD=''
STRIP='strip'
TAR='tar'
TCLTK_CPPFLAGS=''
TCLTK_LIBS=''
TCL_CONFIG=''
TEX='false'
TK_CONFIG=''
USE_EXPORTFILES_FALSE=''
USE_EXPORTFILES_TRUE=''
USE_EXTERNAL_BLAS_FALSE=''
USE_EXTERNAL_BLAS_TRUE=''
USE_EXTERNAL_LAPACK_FALSE=''
USE_EXTERNAL_LAPACK_TRUE=''
USE_LIBTOOL_FALSE=''
USE_LIBTOOL_TRUE='#'
USE_MMAP_ZLIB_FALSE=''
USE_MMAP_ZLIB_TRUE=''
USE_RECOMMENDED_PACKAGES_FALSE=''
USE_RECOMMENDED_PACKAGES_TRUE=''
USING_G77_FALSE=''
USING_G77_TRUE='#'
VERSION='2.0.1'
WANT_R_FRAMEWORK_FALSE=''
WANT_R_FRAMEWORK_TRUE='#'
WANT_R_SHLIB_FALSE=''
WANT_R_SHLIB_TRUE='#'
XMKMF=''
X_CFLAGS=''
X_EXTRA_LIBS=''
X_LIBS=''
X_PRE_LIBS=''
YACC='yacc'
ZVT_LIBS=''
ac_ct_AR=''
ac_ct_CC='gcc'
ac_ct_CXX='g++'
ac_ct_F77=''
ac_ct_RANLIB='ranlib'
ac_ct_STRIP='strip'
bindir='${exec_prefix}/bin'
build='i686-pc-linux-gnu'
build_alias=''
build_cpu='i686'
build_os='linux-gnu'
build_vendor='pc'
config_opts=''
datadir='${prefix}/share'
exec_prefix='NONE'
host='i686-pc-linux-gnu'
host_alias=''
host_cpu='i686'
host_os='linux-gnu'
host_vendor='pc'
includedir='${prefix}/include'
infodir='${prefix}/info'
libdir='${exec_prefix}/lib'
libexecdir='${exec_prefix}/libexec'
localstatedir='${prefix}/var'
mandir='${prefix}/man'
oldincludedir='/usr/include'
prefix='NONE'
program_transform_name='s,x,x,'
sbindir='${exec_prefix}/sbin'
sharedstatedir='${prefix}/com'
shlibpath_var='LD_LIBRARY_PATH'
sysconfdir='${prefix}/etc'
target_alias=''
use_aqua=''
use_tcltk=''

## ------------- ##
## Output files. ##
## ------------- ##

r_cc_rules_frag='Makefrag.cc'
r_cxx_rules_frag=''
r_f77_rules_frag=''

## ----------- ##
## confdefs.h. ##
## ----------- ##

#define HAVE_ARPA_INET_H 1
#define HAVE_DECL_SIZE_MAX 1
#define HAVE_DIRENT_H 1
#define HAVE_DLFCN_H 1
#define HAVE_DLFCN_H 1
#define HAVE_ELF_H 1
#define HAVE_ERRNO_H 1
#define HAVE_FCNTL_H 1
#define HAVE_FPU_CONTROL_H 1
#define HAVE_GLIBC2 1
#define HAVE_GRP_H 1
#define HAVE_IEEE754_H 1
#define HAVE_INTTYPES_H 1
#define HAVE_LIBDL 1
#define HAVE_LIBM 1
#define HAVE_LIMITS_H 1
#define HAVE_LOCALE_H 1
#define HAVE_MEMORY_H 1
#define HAVE_NETDB_H 1
#define HAVE_NETINET_IN_H 1
#define HAVE_POSIX_SETJMP 1
#define HAVE_PWD_H 1
#define HAVE_STDARG_H 1
#define HAVE_STDINT_H 1
#define HAVE_STDLIB_H 1
#define HAVE_STRINGS_H 1
#define HAVE_STRINGS_H 1
#define HAVE_STRING_H 1
#define HAVE_STRING_H 1
#define HAVE_SYS_PARAM_H 1
#define HAVE_SYS_SELECT_H 1
#define HAVE_SYS_SOCKET_H 1
#define HAVE_SYS_STAT_H 1
#define HAVE_SYS_STAT_H 1
#define HAVE_SYS_TIMES_H 1
#define HAVE_SYS_TIME_H 1
#define HAVE_SYS_TYPES_H 1
#define HAVE_SYS_UTSNAME_H 1
#define HAVE_SYS_WAIT_H 1
#define HAVE_UNISTD_H 1
#define HAVE_UNISTD_H 1
#define HAVE_WCHAR_H 1
#define INT_32_BITS 1
#define PACKAGE "R"
#define PACKAGE_BUGREPORT "r-bugs at R-project.org"
#define PACKAGE_NAME "R"
#define PACKAGE_STRING "R 2.0.1"
#define PACKAGE_TARNAME "R"
#define PACKAGE_VERSION "2.0.1"
#define RETSIGTYPE void
#define R_CPU "i686"
#define R_INLINE inline
#define R_OS "linux-gnu"
#define R_PLATFORM "i686-pc-linux-gnu"
#define R_VENDOR "pc"
#define SIZEOF_INT 4
#define SIZEOF_LONG 4
#define SIZEOF_LONG_DOUBLE 12
#define SIZEOF_LONG_LONG 8
#define SOCKLEN_T socklen_t
#define STDC_HEADERS 1
#define STDC_HEADERS 1
#define TIME_WITH_SYS_TIME 1
#define Unix 1
#define VERSION "2.0.1"
#endif
#ifdef __cplusplus
extern "C" void std::exit (int) throw (); using std::exit;

configure: exit 1



From tfliao at uiuc.edu  Sat Feb  5 12:20:42 2005
From: tfliao at uiuc.edu (Tim F Liao)
Date: Sat, 5 Feb 2005 05:20:42 -0600
Subject: [R] Std Err on Concentration measures
Message-ID: <c6465e39.c9d31ac9.81e7600@expms6.cites.uiuc.edu>

David Giles has a paper on calculating standard errors for the
Gini: http://web.uvic.ca/econ/ewp0202.pdf

Tim

---- Original message ----
>Date: Sat, 5 Feb 2005 11:43:02 +0100
>From: Angelo Secchi <secchi at sssup.it>  
>Subject: [R] Std Err on Concentration measures  
>To: r-help at stat.math.ethz.ch
>
>
>Hi,
>I'm using the ineq package to calculate some concentration
measures (Gini, Herfindal, ...) and I was wondering if there's
around also a function to calculate standard error on these
measures. If not, is anybody aware of where I can find a
reference on this point?
>Thanks.
>
>-- 
>========================================================
> Angelo Secchi                     PGP Key ID:EA280337
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Sat Feb  5 12:18:37 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 05 Feb 2005 12:18:37 +0100
Subject: [R] Installing R packages in windows
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E661@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E661@usrymx25.merck.com>
Message-ID: <4204AB8D.40509@statistik.uni-dortmund.de>

Liaw, Andy wrote:

> A half-way decent ftp client would allow you to get all files in a
> directory, so that ought to be quite easy.

... or wget ... or in R:
download.packages(CRAN.packages()[,1], destdir = ......)

Uwe Ligges



> Andy
> 
> 
>>From: Vikas Rawal
>>
>>Thank you. That is useful. But is it possible to download all 
>>the packages in one go, or would one have download each one by one?
>>
>>Vikas
>>
>>-----Original Message-----
>>From: Uwe Ligges <ligges at statistik.uni-dortmund.de>
>>To: Vikas Rawal <vikas at mail.jnu.ac.in>
>>Date: Fri, 04 Feb 2005 13:55:42 +0100
>>Subject: Re: [R] Installing R packages in windows
>>
>>Vikas Rawal wrote:
>>
>>
>>>I need to install a selected set of packages on a number of 
>>
>>machines (in a computer lab). Some of these machines are not 
>>connected to internet. Is it possible to download all the 
>>packages and make a kind of repository on a CD, and then 
>>install.packages from the CD?
>>
>>Yes, just download the packages and install.packages with 
>>CRAN=NULL ...
>>
>>Instead, you might want to mount the installed packages from 
>>a network 
>>volume instead, adding a second library path for R. So you 
>>only need to 
>>install stuff once.
>>
>>
>>Uwe Ligges
>>
>>
>>
>>
>>>Vikas
>>>
>>>==============================================
>>>
>>> This Mail was Scanned for Virus and found Virus free
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! 
>>
>>http://www.R-project.org/posting-guide.html
>>
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>
> 
> 
> 
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachment...{{dropped}}



From kubovy at virginia.edu  Sat Feb  5 13:44:45 2005
From: kubovy at virginia.edu (Michael Kubovy)
Date: Sat, 5 Feb 2005 07:44:45 -0500
Subject: [R] Problem installing Hmisc 
Message-ID: <073c38f94fb534d6d29a4e3ab9117152@virginia.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050205/b86b9fa3/attachment.pl

From ggrothendieck at myway.com  Sat Feb  5 14:20:26 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 5 Feb 2005 13:20:26 +0000 (UTC)
Subject: [R] Rare Cases and SOM
References: <20050204110643.58244.qmail@web25104.mail.ukl.yahoo.com>
Message-ID: <loom.20050205T141353-659@post.gmane.org>

Manuel Gutierrez <manuel_gutierrez_lopez <at> yahoo.es> writes:

: 
: I am trying to understand how the SOM algorithm works
: using library(class) SOM function.
: I have a 1000*10 matrix and I want to be able to
: summarize the different types of 10-element vectors.
: In my real world case it is likely that most of the
: 1000 values are of one kind the rest of other (this is
: an oversimplification).
: Say for example:
: 
: InputA<-matrix(cos(1:10),nrow=900,ncol=10,byrow=TRUE)
: InputB<-matrix(sin(5:14),nrow=100,ncol=10,byrow=TRUE)
: Input<-rbind(InputA,InputB)
: 
: I though that a small grid of 3*3 would be enough to
: extract the patterns in such simple matrix :
: GridWidth<-3
: GridLength<-3
: gr <- somgrid(xdim=GridWidth,ydim=GridLength,topo =
: "hexagonal")
: test.som <- SOM(Input, gr)
: par(mfrow=c(GridLength,GridWidth))
: for(i in 1:(GridWidth*GridLength))
: plot(test.som$codes[i,],type="l")
: 
: Only when I use a larger grid (say for example 7*3 ) I
: get some of the representatives for the sin pattern.
: This must have something to do with the initialization
: of the grid, as the sin is so rare it is unlikely that
: I get it as a reference vector. Afterwards, because
: the selection for the training is also random it is
: also unlikely they are picked.
: I've been trying to modify some of the other
: parameters for the SOM also, but I would appreciatte
: some input to keep me going until I receive the
: reference books from my bookstore.
: 
: Are my suspictions right?
: Should I be using the SOM for my study or should I
: look somewhere else?
: NOTE: I have no prior knowledge of whether the
: datasets I want to analyse will have rare cases or not
: or where they will be located.

I don't have a direct answer to your question as I have not
used that package but I have used randomForest and it does have 
stratified sampling facitilities so that you can be sure that a rare case
is represented.  Check out the sampsize= argument.  Also there
is an article in RNews on randomForest and search this list where
you can find some relevant comments by the author of randomForest.



From p.dalgaard at biostat.ku.dk  Sat Feb  5 14:25:00 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 05 Feb 2005 14:25:00 +0100
Subject: [R] Problems compiling (configure) R on Ubuntu linux (debian)
In-Reply-To: <022501c50b70$a55d8760$1209f9c2@ales>
References: <022501c50b70$a55d8760$1209f9c2@ales>
Message-ID: <x2is57f8b7.fsf@biostat.ku.dk>

=?windows-1250?Q?Ale=9A_=8Eiberna?= <ales.ziberna at guest.arnes.si> writes:

> Hello!
> 
> I would first like to appologice if this question does not fit on this 
> mailing-list.
> 
> I am new to Linux and I tried to compile R on my Ubuntu Warty linux. I 
> followed the instructions in "R Installation and Administration" manual. It 
> seams that there was a problem with "configure", since when running "make" 
> afterward gave me this reply:
>     make: *** No targets specified and no makefile found.  Stop.

Presumably configure told you something before that?

> I have copied the content of the "config.log" at the end of the file. If 
> someone would give me some instructions on how to read this file, I would be 
> very grateful.
> 
> Thank you in advance!
> Ales Ziberna

So you thought that sending 2800 lines  to 2000+ people probably
wouldn't bother anyone?

You seem to be lacking a Fortran compiler as configure is picking up
"fc" (which is bash's "fix command"), but we could have found that out
based on more concentrated output than the entire config.log.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ligges at statistik.uni-dortmund.de  Sat Feb  5 15:19:43 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 05 Feb 2005 15:19:43 +0100
Subject: [R] How to access results of survival analysis
In-Reply-To: <3.0.6.32.20050204231753.007aba90@pop.gmx.net>
References: <3.0.6.32.20050204231753.007aba90@pop.gmx.net>
Message-ID: <4204D5FF.3040509@statistik.uni-dortmund.de>

Heinz Tuechler wrote:
> Hello,
> 
> it seems that the main results of survival analysis with package survival
> are shown only as side effects of the print method.
> 
> If I compute e.g. a Kaplan-Meier estimate by 
> 
>>km.survdur<-survfit(s.survdur) 
> 
> then I can simply print the results by 
> 
>>km.survdur
> 
> Call: survfit(formula = s.survdur)
> 
>       n  events  median 0.95LCL 0.95UCL 
>   100.0    58.0    46.8    41.0    79.3 
> 
> Is there a simple method to access these results, e.g. if I want to print
> only the median with the confidence limits?
> Regarding the results of a Cox-PH-model I face the same situation. The
> printed results are:
> 
>>cx.survdur.ipss_mds.sex
> 
> Call:
> coxph(formula = s.survdur ~ x1 + x2, method = "efron")
> 
>        coef exp(coef) se(coef)     z      p
> x1   0.6424      1.90    0.206 3.123 0.0018
> x2.L 0.0616      1.06    0.263 0.234 0.8100
> 
> Likelihood ratio test=9.56  on 2 df, p=0.0084  n=58 (42 observations
> deleted due to missing)
> 
> Is there a simple method to copy e.g. the coefficients and p-values in a
> new object?
> 
> I am working with:
> R : Copyright 2004, The R Foundation for Statistical Computing
> Version 2.0.1  (2004-11-15), ISBN 3-900051-07-0
> Survival package version: survival_2.16
> Operating System: Windows 98SE
> 
> Thanks,
> Heinz T?chler
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

No, the print methods do not return those values.
But you can copy code for calculation of the required values from those 
print methods into your own functions...

Uwe Ligges



From ligges at statistik.uni-dortmund.de  Sat Feb  5 15:30:48 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 05 Feb 2005 15:30:48 +0100
Subject: [R] Problem installing Hmisc
In-Reply-To: <073c38f94fb534d6d29a4e3ab9117152@virginia.edu>
References: <073c38f94fb534d6d29a4e3ab9117152@virginia.edu>
Message-ID: <4204D898.7080105@statistik.uni-dortmund.de>

Michael Kubovy wrote:

> What am I doing wrong?
> 
>  >  install.packages("Hmisc")
> trying URL `http://cran.r-project.org/src/contrib/PACKAGES'
> Content type `text/plain; charset=iso-8859-1' length 47565 bytes
> opened URL
> ==================================================
> downloaded 46Kb
> 
> trying URL `http://cran.r-project.org/src/contrib/Hmisc_3.0-1.tar.gz'
> Content type `application/x-tar' length 453567 bytes
> opened URL
> ==================================================
> downloaded 442Kb

No further messages here? E.g. something like
* Installing *source* package 'Hmisc' ...
** libs
?


Have you installed other packages with/without success before?

What happens when trying to install from the command line using
   R CMD INSTALL
?


Uwe Ligges



> Delete downloaded files (y/N)?
> 
> The packages are in /tmp/Rtmp1911/Rinstdir77a4044d
> Warning message:
> Installation of package Hmisc had non-zero exit status in: 
> install.packages("Hmisc")
> _____________________________
> Professor Michael Kubovy
> University of Virginia
> Department of Psychology
> USPS: 	P.O.Box 400400	Charlottesville, VA 22904-4400
> Parcels:	Room 102		Gilmer Hall
> 		McCormick Road	Charlottesville, VA 22903
> Office:	B011	+1-434-982-4729
> Lab:		B019	+1-434-982-4751
> Fax:		+1-434-982-4766
> WWW:	http://www.people.virginia.edu/~mk9y/
> 	[[alternative text/enriched version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From chabotd at globetrotter.net  Sat Feb  5 16:31:33 2005
From: chabotd at globetrotter.net (Denis Chabot)
Date: Sat, 5 Feb 2005 16:31:33 +0100
Subject: =?ISO-8859-1?Q?R=E9p_:_[R]_2_small_problems:_integer_division_an?=
	=?ISO-8859-1?Q?d_the_nature_of_NA?=
Message-ID: <59fecfdc253019ffa05efc7ec6f1f601@globetrotter.net>

Thanks to the many R users who convinced me that the sum of NAs should 
be zero and gave me a solution if I did not want it to be zero.

Thank you also for the explanations of rounding errors with floating 
point arithmetics. I did not expect it. This small error was a real 
problem for me as I was trying to find a way to recode numeric values 
into intervals. Because I wanted to retain numeric values as a result, 
I tried not to use cut or cut2. Hence to convert a range of 
temperatures into 0.2 degree intervals I had written:

(lets first make a fake temperature variable k for testing)
k <- seq(-5,5,0.1)
k1 <- ifelse(k<0,-0.2*(abs(k) %/% 0.2) - 0.1, 0.2 *(k %/% 0.2) + 0.1)

Note that this works well to quickly recode a numeric variable that 
only takes integer values. But it produces the problem that prompted my 
call for help when there are decimals: some values end up in a 
different class than what you'd expect.

Considering your answers, I found 3 solutions:

k2 <- ifelse(k<0,-0.2*(abs(round(10*k)) %/% 2) - 0.1, 0.2 *(round(10*k) 
%/% 2) + 0.1)

k3 <- (-0.1+min(k)) + 0.2 * as.numeric(cut(k, 
seq(min(k),max(k)+0.2,0.2), right=F, labels=F))

k4 <- cut2(k, seq(min(k), max(k)+0.2, 0.2), levels.mean=T)
k5 <- as.numeric(levels(k7))[k7]

I could "round" to 1 decimal to be even more exact but this is good 
enough. If it can be more elegant, please let me know!

Denis
> Subject: [R] 2 small problems: integer division and the nature of NA
>
>
> Hi,
>
> I'm wondering why
>
> 48 %/% 2 gives 24
> but
> 4.8 %/% 0.2 gives 23...
> I'm not trying to round up here, but to find out how many times
> something fits into something else, and the answer should have been the
> same for both examples, no?
>
> On a different topic, I like the behavior of NAs better in R than in
> SAS (at least they are not considered the smallest value for a
> variable), but at the same time I am surprised that the sum of NAs is 0
> instead of NA.
>
> The sum of a vector having at least one NA but also valid data gives NA
> if we do not specify na.rm=T. But with na.rm=T, we are telling sum to
> give the sum of valid data, ignoring NAs that do not tell us anything
> about the value of a variable. I found out while getting the sum of
> small subsets of my data (such as when subsetting by several
> variables), sometimes a "cell" only contained NAs for my response
> variable. I would have expected the sum to be NA in such cases, as I do
> not have a single data point telling me the value of my response here.
> But R tells me the sum was zero in that cell! Was this behavior
> considered "desirable" when sum was built? If not, any hope it will be
> fixed?
>
> Sincerely,
>
> Denis Chabot
>



From edd at debian.org  Sat Feb  5 16:44:46 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sat, 5 Feb 2005 09:44:46 -0600
Subject: [R] Problems compiling (configure) R on Ubuntu linux (debian)
In-Reply-To: <x2is57f8b7.fsf@biostat.ku.dk>
References: <022501c50b70$a55d8760$1209f9c2@ales>
	<x2is57f8b7.fsf@biostat.ku.dk>
Message-ID: <16900.59886.67300.348574@basebud.nulle.part>


On 5 February 2005 at 14:25, Peter Dalgaard wrote:
| =?windows-1250?Q?Ale=9A_=8Eiberna?= <ales.ziberna at guest.arnes.si> writes:
| > I am new to Linux and I tried to compile R on my Ubuntu Warty linux. I 

There is really no need to do this, especially when you're new to Linux. The
command

       $ apt-get install r-base

(or is graphical equivalent via aptitude et al) is your friend. You may have
to add Debian archives to the list of archives search when you do 'apt-get
update'. I'm sure the Ubunto docs explain how to do that.

Dirk, whose free Ubunto cdrom is still unused for lack of time

-- 
Better to have an approximate answer to the right question than a precise 
answer to the wrong question.  --  John Tukey as quoted by John Chambers



From tuechler at gmx.at  Sat Feb  5 16:44:30 2005
From: tuechler at gmx.at (Heinz Tuechler)
Date: Sat, 05 Feb 2005 16:44:30 +0100
Subject: [R] How to access results of survival analysis
In-Reply-To: <015f01c50b5c$37936330$0a09f9c2@ales>
References: <3.0.6.32.20050204231753.007aba90@pop.gmx.net>
Message-ID: <3.0.6.32.20050205164430.007a89c0@pop.gmx.net>

Hello Ales,

thank you for your hint regarding names(summary(fit)). Summary(fit) is a
list containig all important results of the Cox-model.
So it helps a lot!
Regarding the results of a Kaplan-Meier estimate summary does not help,
because it does not contain the main results.

Thanks again,
Heinz T?chler

ps. If I understood it right, you answered only to me and not to the list.
So I assumed that you prefer not to put your answer on the list and
therefore I answer to you off-list.

At 09:24 05.02.2005 +0100, Ales Ziberna wrote:
>Hi!
>
>I don't now how to reach the median directly. However I can help you whit 
>cox-PH coefficients.
>
>if fit is the resoult of the coxph, then
>
>fit$coefficients should give you the coefficients. To see whatother things 
>you can acces in similar way, see
>names(fit)
>or
>names(summary(fit)).
>
>I hope this helps at least a little!
>
>Ales Ziberna
>
>
>----- Original Message ----- 
>From: "Heinz Tuechler" <tuechler at gmx.at>
>To: <r-help at stat.math.ethz.ch>
>Sent: Friday, February 04, 2005 11:17 PM
>Subject: [R] How to access results of survival analysis
>
>
>Hello,
>
>it seems that the main results of survival analysis with package survival
>are shown only as side effects of the print method.
>
>If I compute e.g. a Kaplan-Meier estimate by
>> km.survdur<-survfit(s.survdur)
>then I can simply print the results by
>> km.survdur
>Call: survfit(formula = s.survdur)
>
>      n  events  median 0.95LCL 0.95UCL
>  100.0    58.0    46.8    41.0    79.3
>
>Is there a simple method to access these results, e.g. if I want to print
>only the median with the confidence limits?
>Regarding the results of a Cox-PH-model I face the same situation. The
>printed results are:
>> cx.survdur.ipss_mds.sex
>Call:
>coxph(formula = s.survdur ~ x1 + x2, method = "efron")
>
>       coef exp(coef) se(coef)     z      p
>x1   0.6424      1.90    0.206 3.123 0.0018
>x2.L 0.0616      1.06    0.263 0.234 0.8100
>
>Likelihood ratio test=9.56  on 2 df, p=0.0084  n=58 (42 observations
>deleted due to missing)
>
>Is there a simple method to copy e.g. the coefficients and p-values in a
>new object?
>
>I am working with:
>R : Copyright 2004, The R Foundation for Statistical Computing
>Version 2.0.1  (2004-11-15), ISBN 3-900051-07-0
>Survival package version: survival_2.16
>Operating System: Windows 98SE
>
>Thanks,
>Heinz T?chler
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html
>
>
>



From merser at image.dk  Sat Feb  5 18:04:01 2005
From: merser at image.dk (=?iso-8859-1?Q?S=F8ren_Merser?=)
Date: Sat, 5 Feb 2005 18:04:01 +0100
Subject: [R] how to make an empty screen
Message-ID: <20050205170318.A17EF5EE038@pfepb.post.tele.dk>

Hi

I want to plot control charts for several events grouped by employees.
As every employees doesn't encounter every event, a variable number of
control charts is produced in turn.
Now what I need is a way to get a new empty screen when the printout for one
employee has finished as the title lines is somhow mixed between the
changing of employees

The screen setup: 
	par(mfrow=c(4,4))
And I make the title like this:
	title(main=event, outer=T)

Regards soren



From tuechler at gmx.at  Sat Feb  5 17:28:27 2005
From: tuechler at gmx.at (Heinz Tuechler)
Date: Sat, 05 Feb 2005 17:28:27 +0100
Subject: [R] How to access results of survival analysis
In-Reply-To: <4204D5FF.3040509@statistik.uni-dortmund.de>
References: <3.0.6.32.20050204231753.007aba90@pop.gmx.net>
	<3.0.6.32.20050204231753.007aba90@pop.gmx.net>
Message-ID: <3.0.6.32.20050205172827.00797290@pop.gmx.net>

At 15:19 05.02.2005 +0100, Uwe Ligges wrote:
>Heinz Tuechler wrote:
>> Hello,
>> 
>> it seems that the main results of survival analysis with package survival
>> are shown only as side effects of the print method.
>> 
>> If I compute e.g. a Kaplan-Meier estimate by 
>> 
>>>km.survdur<-survfit(s.survdur) 
>> 
>> then I can simply print the results by 
>> 
>>>km.survdur
>> 
>> Call: survfit(formula = s.survdur)
>> 
>>       n  events  median 0.95LCL 0.95UCL 
>>   100.0    58.0    46.8    41.0    79.3 
>> 
>> Is there a simple method to access these results, e.g. if I want to print
>> only the median with the confidence limits?

...
>
>No, the print methods do not return those values.
>But you can copy code for calculation of the required values from those 
>print methods into your own functions...
>
>Uwe Ligges
>
>
Thank you for your answer. I assume, you suggest to use
capture.output(print(...)). Without your response I would have believed
that I had missed an important possibility of R.

Regarding the Cox-Model Ales Ziberna gave me a useful hint to use summary()
which returns a list.

Thanks to both of you,

Heinz T?chler



From zvalim at gmail.com  Sat Feb  5 19:01:45 2005
From: zvalim at gmail.com (Uri)
Date: Sat, 5 Feb 2005 12:01:45 -0600
Subject: [R] Compressed communication with DB using RMySQL?
Message-ID: <656a77fc05020510011eb53722@mail.gmail.com>

Dear All,
I have a setup where R pulls entries from a MySQL db server.  I wanted
to know whether the R interface can pull encrypted / compressed data
from MySQLD.  MySQL supports compressed communication on the server
side, but I couldn't find any references to such options on the client
(RMySQL) side.

Best,
Uri Hasson.
---
Uri Hasson.
Brain Research Imaging Center
The University of Chicago.



From ligges at statistik.uni-dortmund.de  Sat Feb  5 19:09:39 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 05 Feb 2005 19:09:39 +0100
Subject: [R] How to access results of survival analysis
In-Reply-To: <3.0.6.32.20050205172827.00797290@pop.gmx.net>
References: <3.0.6.32.20050204231753.007aba90@pop.gmx.net>
	<3.0.6.32.20050204231753.007aba90@pop.gmx.net>
	<3.0.6.32.20050205172827.00797290@pop.gmx.net>
Message-ID: <42050BE3.40609@statistik.uni-dortmund.de>

Heinz Tuechler wrote:

> At 15:19 05.02.2005 +0100, Uwe Ligges wrote:
> 
>>Heinz Tuechler wrote:
>>
>>>Hello,
>>>
>>>it seems that the main results of survival analysis with package survival
>>>are shown only as side effects of the print method.
>>>
>>>If I compute e.g. a Kaplan-Meier estimate by 
>>>
>>>
>>>>km.survdur<-survfit(s.survdur) 
>>>
>>>then I can simply print the results by 
>>>
>>>
>>>>km.survdur
>>>
>>>Call: survfit(formula = s.survdur)
>>>
>>>      n  events  median 0.95LCL 0.95UCL 
>>>  100.0    58.0    46.8    41.0    79.3 
>>>
>>>Is there a simple method to access these results, e.g. if I want to print
>>>only the median with the confidence limits?
> 
> 
> ...
> 
>>No, the print methods do not return those values.
>>But you can copy code for calculation of the required values from those 
>>print methods into your own functions...
>>
>>Uwe Ligges
>>
>>
> 
> Thank you for your answer. I assume, you suggest to use
> capture.output(print(...)). 

No, I suggested to copy the code from survival:::print.survfit and Co. 
that calculates the values you are looking for...

Uwe

 > Without your response I would have believed
> that I had missed an important possibility of R.
> 
> Regarding the Cox-Model Ales Ziberna gave me a useful hint to use summary()
> which returns a list.
> 
> Thanks to both of you,
> 
> Heinz T?chler



From tlumley at u.washington.edu  Sat Feb  5 19:28:20 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Sat, 5 Feb 2005 10:28:20 -0800 (PST)
Subject: [R] Problem installing Hmisc 
In-Reply-To: <073c38f94fb534d6d29a4e3ab9117152@virginia.edu>
References: <073c38f94fb534d6d29a4e3ab9117152@virginia.edu>
Message-ID: <Pine.A41.4.61b.0502051025060.206006@homer03.u.washington.edu>

On Sat, 5 Feb 2005, Michael Kubovy wrote:

> What am I doing wrong?

You can look at the console log (eg start Console.app in 
/Applications/Utilities) to see what the problem is, but the fact that 
Hmisc is not available as a binary package probably means that it does not 
compile for the Mac (at least without some modification)

 	-thomas

> >  install.packages("Hmisc")
> trying URL `http://cran.r-project.org/src/contrib/PACKAGES'
> Content type `text/plain; charset=iso-8859-1' length 47565 bytes
> opened URL
> ==================================================
> downloaded 46Kb
>
> trying URL `http://cran.r-project.org/src/contrib/Hmisc_3.0-1.tar.gz'
> Content type `application/x-tar' length 453567 bytes
> opened URL
> ==================================================
> downloaded 442Kb
>
> Delete downloaded files (y/N)?
>
> The packages are in /tmp/Rtmp1911/Rinstdir77a4044d
> Warning message:
> Installation of package Hmisc had non-zero exit status in:
> install.packages("Hmisc")
> _____________________________
> Professor Michael Kubovy
> University of Virginia
> Department of Psychology
> USPS: 	P.O.Box 400400	Charlottesville, VA 22904-4400
> Parcels:	Room 102		Gilmer Hall
> 		McCormick Road	Charlottesville, VA 22903
> Office:	B011	+1-434-982-4729
> Lab:		B019	+1-434-982-4751
> Fax:		+1-434-982-4766
> WWW:	http://www.people.virginia.edu/~mk9y/
> 	[[alternative text/enriched version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From dj at research.bell-labs.com  Sat Feb  5 19:55:50 2005
From: dj at research.bell-labs.com (David James)
Date: Sat, 5 Feb 2005 13:55:50 -0500
Subject: [R] Compressed communication with DB using RMySQL?
In-Reply-To: <656a77fc05020510011eb53722@mail.gmail.com>;
	from zvalim@gmail.com on Sat, Feb 05, 2005 at 12:01:45PM -0600
References: <656a77fc05020510011eb53722@mail.gmail.com>
Message-ID: <20050205135549.A25408@jessie.research.bell-labs.com>

Uri wrote:
> Dear All,
> I have a setup where R pulls entries from a MySQL db server.  I wanted
> to know whether the R interface can pull encrypted / compressed data
> from MySQLD.  MySQL supports compressed communication on the server
> side, but I couldn't find any references to such options on the client
> (RMySQL) side.

Right, this is not properly documented (buried in ?mysqlNewConnection).
You can simply add the argumet client.flag = 32 to dbConnect(), e.g.,

   con <- dbConnect(MySQL(), ..., client.flag=32)

The number 32 comes from the mysql_com.h version 4.1.17 header file
#define CLIENT_COMPRESS		32	/* Can use compression protocol */

I'd be curious to know whether you find compressed connections 
significantly faster over raw connections.

Hope this helps,

--
David

> 
> Best,
> Uri Hasson.
> ---
> Uri Hasson.
> Brain Research Imaging Center
> The University of Chicago.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jeaneid at chass.utoronto.ca  Sat Feb  5 21:05:17 2005
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Sat, 5 Feb 2005 15:05:17 -0500
Subject: [R] loess problems
Message-ID: <Pine.SGI.4.40.0502051428001.10179079-100000@origin.chass.utoronto.ca>

I have a problem either understanding what loess is doing or that loess
has a problem itself.

As the x-axis variables become more concentrated on a particular point,the
estimated loess tends to zero????. the examples below show what i am
talking about,  why is that? my intution tells me
that it should tend to the mean of the variable which is been smoothed.

Here's a worked up example

x <- c(seq(0,100), rep(100,1000))
y <- rnorm(length(x), mean=10, sd=2)
scatter.smooth(x,y)



Although it does give warnings, I don't understand why it is giving the
estimate as zero.


another example would be

x <- seq(0,100)
y <- rnorm(length(x), mean=50, sd=2)
scatter.smooth(x,y, span=1/length(x))


shoudn't this give just the points at which the smoothing algorithm is
applied?



thank you



From umalvarez at fata.unam.mx  Sat Feb  5 21:28:39 2005
From: umalvarez at fata.unam.mx (Ulises M. Alvarez)
Date: Sat, 05 Feb 2005 14:28:39 -0600
Subject: [Fwd: Re: [R] Problems compiling (configure) R on Ubuntu linux
	(debian)]
Message-ID: <42052C77.5020605@fata.unam.mx>

-------- Original Message --------
Subject: Re: [R] Problems compiling (configure) R on Ubuntu linux (debian)
Date: Sat, 5 Feb 2005 12:47:42 -0600
From: Dirk Eddelbuettel <edd at debian.org>
To: Ulises M. Alvarez <umalvarez at fata.unam.mx>
References: <022501c50b70$a55d8760$1209f9c2 at ales> 
<x2is57f8b7.fsf at biostat.ku.dk> 
<16900.59886.67300.348574 at basebud.nulle.part> 
<42051287.1050304 at fata.unam.mx>


Ulises,

Thanks -- but I am not the Ubuntu user who asked.  Could you resend this to
him, and the list?

On 5 February 2005 at 12:37, Ulises M. Alvarez wrote:
| Hello!
|
| I agree that the easiest way to get R in Ubuntu is with apt-get or with
| the GUI-based synaptic. Just make yourself sure that both the 'universe'
| and 'multiverse' repositories are available in your
| /etc/apt/sources.list, something like:
|
| ### /etc/apt/sources.list (partial view)
| deb http://archive.ubuntu.com/ubuntu warty universe multiverse
| deb-src http://archive.ubuntu.com/ubuntu warty universe multiverse
| ###
|
| Then, all you have to type is:
|
| sudo aptitude update
| sudo aptitude install r-base r-base-core r-base-latex r-cran-hmisc

Or apt-get, or wajig, or ...

| On the other hand, if you insist on compile R for your Ubuntu box, I'll
| recommend you to install (all in a long line):
|
| sudo aptitude install build-essential g77 libmagick++6-dev libpng3-dev
| libtiff-tools libtk-img libtiff4-dev tcl8.4 tcl8.4-dev tk8.4 tk8.4-dev
| a2ps libedit-dev libreadline4-dev tclreadline tetex-base tetex-bin
| tetex-extra

'apt-get build-dep r-base' is much easier, and possibly more complete :-)

That was a helpful post, though. Please send it to the list.

Cheers, Dirk

|
| And that's all I can remember now. Beware that this may take time,
| depending on your Internet connection.
|
| Let me know if you need more help.
|
|
| Dirk Eddelbuettel wrote:
| > On 5 February 2005 at 14:25, Peter Dalgaard wrote:
| > | =?windows-1250?Q?Ale=9A_=8Eiberna?= <ales.ziberna at guest.arnes.si> 
writes:
| > | > I am new to Linux and I tried to compile R on my Ubuntu Warty 
linux. I
| >
| > There is really no need to do this, especially when you're new to 
Linux. The
| > command
| >
| >        $ apt-get install r-base
| >
| > (or is graphical equivalent via aptitude et al) is your friend. You 
may have
| > to add Debian archives to the list of archives search when you do 
'apt-get
| > update'. I'm sure the Ubunto docs explain how to do that.
| >
| > Dirk, whose free Ubunto cdrom is still unused for lack of time
| >
|
| --
| U.M.A.
| http://sophie.fata.unam.mx/

-- 
Better to have an approximate answer to the right question than a precise
answer to the wrong question.  --  John Tukey as quoted by John Chambers


-- 
U.M.A.
http://sophie.fata.unam.mx/



From andy_liaw at merck.com  Sat Feb  5 22:17:02 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sat, 5 Feb 2005 16:17:02 -0500
Subject: [R] loess problems
Message-ID: <3A822319EB35174CA3714066D590DCD50994E664@usrymx25.merck.com>

The problem is that 90% of your data sit on the boundary.  Loess is a
nearest neighbor smoother (using (100 x span) % of the data to estimate at
each point).  If you call loess() directly with span=2/3 (the default in
scatter.smooth), or something smaller than about 0.91, you'll see that it
has trouble.   

Strangely, if you set span=.8, scatter.smooth() will also complain, but not
at the default span...  (Re-generating the data yet again does trigger the
warnings, so seems like it does catches things some of the time.)

For your second example, I think loess becomes undefined when the span is
set too small (and 1/n is surely too small):  You are asking the algorithm
to take the nearest 1/n of the data to do the smooth.  You would think that
should just mean _the_ nearest data point, but the problem is:

> n <- 100
> 1 / n < 1
[1] TRUE

so you're asking the algoithm to take fewer than 1 data point to estimate at
each point.  The warnings you see for that example is pointing you in the
right direction.

Andy


> From: Jean Eid
> 
> I have a problem either understanding what loess is doing or 
> that loess
> has a problem itself.
> 
> As the x-axis variables become more concentrated on a 
> particular point,the
> estimated loess tends to zero????. the examples below show what i am
> talking about,  why is that? my intution tells me
> that it should tend to the mean of the variable which is been 
> smoothed.
> 
> Here's a worked up example
> 
> x <- c(seq(0,100), rep(100,1000))
> y <- rnorm(length(x), mean=10, sd=2)
> scatter.smooth(x,y)
> 
> 
> 
> Although it does give warnings, I don't understand why it is 
> giving the
> estimate as zero.
> 
> 
> another example would be
> 
> x <- seq(0,100)
> y <- rnorm(length(x), mean=50, sd=2)
> scatter.smooth(x,y, span=1/length(x))
> 
> 
> shoudn't this give just the points at which the smoothing algorithm is
> applied?
> 
> 
> 
> thank you
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From kubovy at virginia.edu  Sat Feb  5 22:20:44 2005
From: kubovy at virginia.edu (Michael Kubovy)
Date: Sat, 5 Feb 2005 16:20:44 -0500
Subject: [R] Problem installing Hmisc (more info)
Message-ID: <4e13cec2c11f35ef6ddafca643d03004@virginia.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050205/069731cf/attachment.pl

From ripley at stats.ox.ac.uk  Sat Feb  5 22:59:42 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 5 Feb 2005 21:59:42 +0000 (GMT)
Subject: [R] Problem installing Hmisc (more info)
In-Reply-To: <4e13cec2c11f35ef6ddafca643d03004@virginia.edu>
References: <4e13cec2c11f35ef6ddafca643d03004@virginia.edu>
Message-ID: <Pine.LNX.4.61.0502052148340.31181@gannet.stats>

On Sat, 5 Feb 2005, Michael Kubovy wrote:

> Frank Harrell suggested I re-post with information about the version of
> R

Thanks, that starts to make sense.  You appear to have g77 installed, but 
not in the place the person who prepared the binary install of R has it.
Where do you have it installed?  ('whereis g77' or 'which g77' should tell 
you.)  Then you need to alter FLIBS in R_HOME/etc/Makeconf to point to 
it.  (You can remove -lfrtbegin from FLIBS: it is not needed: your R_HOME 
looks to be /Library/Frameworks/R.framework/Versions/2.0.1.)

However, I believe there is a more fundamental problem: because libg2c is 
a static library on current MacOS X, most packages using Fortran cannot be 
compiled there.  That's presumably the case with Hmisc, as the automated 
package builder is not providing a binary build.  (There is supposedly a 
check directory on CRAN, but it is not there at present.)


>
> Heres's the information:
>>>  version
>>          _
>> platform powerpc-apple-darwin6.8
>> arch     powerpc
>> os       darwin6.8
>> system   powerpc, darwin6.8
>> status
>> major    2
>> minor    0.1
>> year     2004
>> month    11
>> day      15
>> language R
>
> Here's what happens when I try to install:
>>>  install.packages("Hmisc")
>> trying URL `http://cran.r-project.org/src/contrib/PACKAGES'
>> Content type `text/plain; charset=iso-8859-1' length 47565 bytes
>> opened URL
>> ==================================================
>> downloaded 46Kb
>>
>> trying URL `http://cran.r-project.org/src/contrib/Hmisc_3.0-1.tar.gz'
>> Content type `application/x-tar' length 453567 bytes
>> opened URL
>> ==================================================
>> downloaded 442Kb
>>
>> Delete downloaded files (y/N)?
>>
>> The packages are in /tmp/Rtmp1911/Rinstdirfc4111
>> Warning message:
>> Installation of package Hmisc had non-zero exit status in:
>> install.packages("Hmisc")
>
> Here's what the Console had to say:
>> * Installing *source* package 'Hmisc' ...
>> ** libs
>> g77   -fno-common  -g -O2 -c cidxcn.f -o cidxcn.o
>> g77   -fno-common  -g -O2 -c cidxcp.f -o cidxcp.o
>> g77   -fno-common  -g -O2 -c hoeffd.f -o hoeffd.o
>> g77   -fno-common  -g -O2 -c jacklins.f -o jacklins.o
>> g77   -fno-common  -g -O2 -c largrec.f -o largrec.o
>> gcc -no-cpp-precomp
>> -I/Library/Frameworks/R.framework/Resources/include
>> -I/usr/local/include   -fno-common  -g -O2 -c ranksort.c -o ranksort.o
>> g77   -fno-common  -g -O2 -c rcorr.f -o rcorr.o
>> g77   -fno-common  -g -O2 -c wclosest.f -o wclosest.o
>> gcc -bundle -flat_namespace -undefined suppress -L/usr/local/lib -o
>> Hmisc.so cidxcn.o cidxcp.o hoeffd.o jacklins.o largrec.o ranksort.o
>> rcorr.o wclosest.o  -L/usr/local/lib
>> -L/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2
>> -L/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2/../../.. -lfrtbegin
>> -lg2c -lSystem -lcc_dynamic -framework R
>> ld: warning -L: directory name
>> (/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2) does not exist
>> ld: warning -L: directory name
>> (/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2/../../..) does not
>> exist
>> ld: can't locate file for: -lfrtbegin
>> make: *** [Hmisc.so] Error 1
>> ERROR: compilation failed for package 'Hmisc'
>> ** Removing
>> '/Library/Frameworks/R.framework/Versions/2.0.1/Resources/library/
>> Hmisc'
>> ** Restoring previous
>> '/Library/Frameworks/R.framework/Versions/2.0.1/Resources/library/
>> Hmisc'
>
>
> _____________________________
> Professor Michael Kubovy
> University of Virginia
> Department of Psychology
> USPS: 	P.O.Box 400400	Charlottesville, VA 22904-4400
> Parcels:	Room 102		Gilmer Hall
> 		McCormick Road	Charlottesville, VA 22903
> Office:	B011	+1-434-982-4729
> Lab:		B019	+1-434-982-4751
> Fax:		+1-434-982-4766
> WWW:	http://www.people.virginia.edu/~mk9y/
> 	[[alternative text/enriched version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From nw.galwey at ukonline.co.uk  Sat Feb  5 23:51:02 2005
From: nw.galwey at ukonline.co.uk (Nicholas Galwey)
Date: Sat, 5 Feb 2005 22:51:02 -0000
Subject: [R] Labelling and formatting of graphics
Message-ID: <E1CxYlc-0000ax-00@smarthost2.mail.uk.easynet.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050205/e16f5f99/attachment.pl

From yuleih at umich.edu  Sat Feb  5 23:51:41 2005
From: yuleih at umich.edu (Yulei He)
Date: Sat, 5 Feb 2005 17:51:41 -0500 (EST)
Subject: [R] plot smooth density estimates for bivariate data
Message-ID: <Pine.SOL.4.58.0502051749320.18029@asteroids.gpcc.itd.umich.edu>

Hi, there.

Suppose I have a bivarariate data matrix y1 and y2. I want to plot a 3-D
picture of the estimated density f(y1, y2) against y1 and y2? How can I do
that? Do I use persp() or density()?

Thanks for your help.

Yulei


$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
Yulei He
1586 Murfin Ave. Apt 37
Ann Arbor, MI 48105-3135
yuleih at umich.edu
734-647-0305(H)
734-763-0421(O)
734-763-0427(O)
734-764-8263(fax)
$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$



From nair at sdsc.edu  Sat Feb  5 23:49:56 2005
From: nair at sdsc.edu (Murli Nair)
Date: Sat, 05 Feb 2005 14:49:56 -0800
Subject: [R] MC using hclus
Message-ID: <42054D94.781A327D@sdsc.edu>

Has anyone used hclus to determine p-values of a subset of
features that give good class separation after clustering ? What I mean
is sampling a set of features and checking for class separation by
clustering
and then determining if the features that give the actual class
separation are
not random.
Cheers ../Murli



From kubovy at virginia.edu  Sun Feb  6 02:45:32 2005
From: kubovy at virginia.edu (Michael Kubovy)
Date: Sat, 5 Feb 2005 20:45:32 -0500
Subject: [R] Problem installing Hmisc (more info)
In-Reply-To: <Pine.LNX.4.61.0502052148340.31181@gannet.stats>
References: <4e13cec2c11f35ef6ddafca643d03004@virginia.edu>
	<Pine.LNX.4.61.0502052148340.31181@gannet.stats>
Message-ID: <1fc4dcc79604351158593575bd796816@virginia.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050205/97ae52db/attachment.pl

From statistician at jindan.homedns.org  Sun Feb  6 03:25:49 2005
From: statistician at jindan.homedns.org (Jindan Zhou)
Date: Sat, 5 Feb 2005 20:25:49 -0600
Subject: [R] Problem installing Hmisc (more info)
In-Reply-To: <4e13cec2c11f35ef6ddafca643d03004@virginia.edu>
References: <4e13cec2c11f35ef6ddafca643d03004@virginia.edu>
Message-ID: <Pine.CYG.4.58.0502052024470.1092@shenzhen>

I had problem installing Hmisc and Dedign a couple of weeks ago,
repetitively.
It failed at the stage compiling the last help file (summary, I
remember), after long time sucking up system resource. What I did was
updated some development tools, such as glibc, gcc, automake, etc., I
can't tell which one worked for me, but eventually I managed to
compile those two packages with success. If you really want to know what
exactly development tools I have updated, I'd love to take a look at my
log files.

Hope this helps;-)

My information:
  R.version
          _
         platform i586-pc-linux-gnu
         arch     i586
         os       linux-gnu
         system   i586, linux-gnu
         status
         major    2
         minor    0.1
         year     2004
         month    11
         day      15
         language R

Jindan




On Sat, 5 Feb 2005, Michael Kubovy wrote:

> Frank Harrell suggested I re-post with information about the version of
> R
>
> Heres's the information:
> > >  version
> >          _
> > platform powerpc-apple-darwin6.8
> > arch     powerpc
> > os       darwin6.8
> > system   powerpc, darwin6.8
> > status
> > major    2
> > minor    0.1
> > year     2004
> > month    11
> > day      15
> > language R
>
> Here's what happens when I try to install:
> > >  install.packages("Hmisc")
> > trying URL `http://cran.r-project.org/src/contrib/PACKAGES'
> > Content type `text/plain; charset=iso-8859-1' length 47565 bytes
> > opened URL
> > ==================================================
> > downloaded 46Kb
> >
> > trying URL `http://cran.r-project.org/src/contrib/Hmisc_3.0-1.tar.gz'
> > Content type `application/x-tar' length 453567 bytes
> > opened URL
> > ==================================================
> > downloaded 442Kb
> >
> > Delete downloaded files (y/N)?
> >
> > The packages are in /tmp/Rtmp1911/Rinstdirfc4111
> > Warning message:
> > Installation of package Hmisc had non-zero exit status in:
> > install.packages("Hmisc")
>
> Here's what the Console had to say:
> > * Installing *source* package 'Hmisc' ...
> > ** libs
> > g77   -fno-common  -g -O2 -c cidxcn.f -o cidxcn.o
> > g77   -fno-common  -g -O2 -c cidxcp.f -o cidxcp.o
> > g77   -fno-common  -g -O2 -c hoeffd.f -o hoeffd.o
> > g77   -fno-common  -g -O2 -c jacklins.f -o jacklins.o
> > g77   -fno-common  -g -O2 -c largrec.f -o largrec.o
> > gcc -no-cpp-precomp
> > -I/Library/Frameworks/R.framework/Resources/include
> > -I/usr/local/include   -fno-common  -g -O2 -c ranksort.c -o ranksort.o
> > g77   -fno-common  -g -O2 -c rcorr.f -o rcorr.o
> > g77   -fno-common  -g -O2 -c wclosest.f -o wclosest.o
> > gcc -bundle -flat_namespace -undefined suppress -L/usr/local/lib -o
> > Hmisc.so cidxcn.o cidxcp.o hoeffd.o jacklins.o largrec.o ranksort.o
> > rcorr.o wclosest.o  -L/usr/local/lib
> > -L/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2
> > -L/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2/../../.. -lfrtbegin
> > -lg2c -lSystem -lcc_dynamic -framework R
> > ld: warning -L: directory name
> > (/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2) does not exist
> > ld: warning -L: directory name
> > (/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2/../../..) does not
> > exist
> > ld: can't locate file for: -lfrtbegin
> > make: *** [Hmisc.so] Error 1
> > ERROR: compilation failed for package 'Hmisc'
> > ** Removing
> > '/Library/Frameworks/R.framework/Versions/2.0.1/Resources/library/
> > Hmisc'
> > ** Restoring previous
> > '/Library/Frameworks/R.framework/Versions/2.0.1/Resources/library/
> > Hmisc'
>
>
> _____________________________
> Professor Michael Kubovy
> University of Virginia
> Department of Psychology
> USPS: 	P.O.Box 400400	Charlottesville, VA 22904-4400
> Parcels:	Room 102		Gilmer Hall
> 		McCormick Road	Charlottesville, VA 22903
> Office:	B011	+1-434-982-4729
> Lab:		B019	+1-434-982-4751
> Fax:		+1-434-982-4766
> WWW:	http://www.people.virginia.edu/~mk9y/
> 	[[alternative text/enriched version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Sun Feb  6 08:50:39 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 6 Feb 2005 07:50:39 +0000 (GMT)
Subject: [R] Problem installing Hmisc (more info)
In-Reply-To: <1fc4dcc79604351158593575bd796816@virginia.edu>
References: <4e13cec2c11f35ef6ddafca643d03004@virginia.edu>
	<Pine.LNX.4.61.0502052148340.31181@gannet.stats>
	<1fc4dcc79604351158593575bd796816@virginia.edu>
Message-ID: <Pine.LNX.4.61.0502060749200.11527@gannet.stats>

On Sat, 5 Feb 2005, Michael Kubovy wrote:

> Following Brian Ripley's advice:
>
>> $ which g77
>> /sw/bin/g77

So I think you have fink.

> I then found Makeconf, in 
> /Library/Frameworks/R.framework/Versions/2.0.1/Resources/etc
>
> I edited it according to Brian's advice. It had:

You didn't.  You need to replace 
L/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2 appropriately.

>
>> FLIBS =  -L/usr/local/lib 
>> -L/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2 
>> -L/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2/../../.. -lfrtbegin 
>> -lg2c -lSystem
> which now is:
>> FLIBS =  -L/usr/local/lib 
>> -L/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2 
>> -L/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2/../../.. -L/sw/bin -lg2c 
>> -lSystem
>
> Nevertheless the install failed just as below. But the console complaint was 
> slightly different from before:
>> WARNING: ignoring environment value of R_HOME
>> * Installing *source* package 'Hmisc' ...
>> ** libs
>> g77   -fno-common  -g -O2 -c cidxcn.f -o cidxcn.o
>> g77   -fno-common  -g -O2 -c cidxcp.f -o cidxcp.o
>> g77   -fno-common  -g -O2 -c hoeffd.f -o hoeffd.o
>> g77   -fno-common  -g -O2 -c jacklins.f -o jacklins.o
>> g77   -fno-common  -g -O2 -c largrec.f -o largrec.o
>> gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include 
>> -I/usr/local/include   -fno-common  -g -O2 -c ranksort.c -o ranksort.o
>> g77   -fno-common  -g -O2 -c rcorr.f -o rcorr.o
>> g77   -fno-common  -g -O2 -c wclosest.f -o wclosest.o
>> gcc -bundle -flat_namespace -undefined suppress -L/usr/local/lib -o 
>> Hmisc.so cidxcn.o cidxcp.o hoeffd.o jacklins.o largrec.o ranksort.o rcorr.o 
>> wclosest.o  -L/usr/local/lib 
>> -L/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2 
>> -L/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2/../../.. -L/sw/bin -lg2c 
>> -lSystem -lcc_dynamic -framework R
>> ld: warning -L: directory name 
>> (/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2) does not exist
>> ld: warning -L: directory name 
>> (/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2/../../..) does not exist
>> ld: can't locate file for: -lg2c
>> make: *** [Hmisc.so] Error 1
>> ERROR: compilation failed for package 'Hmisc'
>> ** Removing 
>> '/Library/Frameworks/R.framework/Versions/2.0.1/Resources/library/Hmisc'
>> ** Restoring previous 
>> '/Library/Frameworks/R.framework/Versions/2.0.1/Resources/library/Hmisc'
>
>> On Feb 5, 2005, at 4:59 PM, Prof Brian Ripley wrote:
>> 
>>> On Sat, 5 Feb 2005, Michael Kubovy wrote:
>>> 
>>>> Frank Harrell suggested I re-post with information about the version of
>>>> R
>>> 
>>> Thanks, that starts to make sense.  You appear to have g77 installed, but 
>>> not in the place the person who prepared the binary install of R has it.
>>> Where do you have it installed?  ('whereis g77' or 'which g77' should tell 
>>> you.)  Then you need to alter FLIBS in R_HOME/etc/Makeconf to point to it. 
>>> (You can remove -lfrtbegin from FLIBS: it is not needed: your R_HOME looks 
>>> to be /Library/Frameworks/R.framework/Versions/2.0.1.)
>>> 
>>> However, I believe there is a more fundamental problem: because libg2c is 
>>> a static library on current MacOS X, most packages using Fortran cannot be 
>>> compiled there.  That's presumably the case with Hmisc, as the automated 
>>> package builder is not providing a binary build.  (There is supposedly a 
>>> check directory on CRAN, but it is not there at present.)
>>> 
>>>> 
>>>> Heres's the information:
>>>>>>  version
>>>>>          _
>>>>> platform powerpc-apple-darwin6.8
>>>>> arch     powerpc
>>>>> os       darwin6.8
>>>>> system   powerpc, darwin6.8
>>>>> status
>>>>> major    2
>>>>> minor    0.1
>>>>> year     2004
>>>>> month    11
>>>>> day      15
>>>>> language R
>>>> 
>>>> Here's what happens when I try to install:
>>>>>>  install.packages("Hmisc")
>>>>> trying URL `http://cran.r-project.org/src/contrib/PACKAGES'
>>>>> Content type `text/plain; charset=iso-8859-1' length 47565 bytes
>>>>> opened URL
>>>>> ==================================================
>>>>> downloaded 46Kb
>>>>> 
>>>>> trying URL `http://cran.r-project.org/src/contrib/Hmisc_3.0-1.tar.gz'
>>>>> Content type `application/x-tar' length 453567 bytes
>>>>> opened URL
>>>>> ==================================================
>>>>> downloaded 442Kb
>>>>> 
>>>>> Delete downloaded files (y/N)?
>>>>> 
>>>>> The packages are in /tmp/Rtmp1911/Rinstdirfc4111
>>>>> Warning message:
>>>>> Installation of package Hmisc had non-zero exit status in:
>>>>> install.packages("Hmisc")
>>>> 
>>>> Here's what the Console had to say:
>>>>> * Installing *source* package 'Hmisc' ...
>>>>> ** libs
>>>>> g77   -fno-common  -g -O2 -c cidxcn.f -o cidxcn.o
>>>>> g77   -fno-common  -g -O2 -c cidxcp.f -o cidxcp.o
>>>>> g77   -fno-common  -g -O2 -c hoeffd.f -o hoeffd.o
>>>>> g77   -fno-common  -g -O2 -c jacklins.f -o jacklins.o
>>>>> g77   -fno-common  -g -O2 -c largrec.f -o largrec.o
>>>>> gcc -no-cpp-precomp
>>>>> -I/Library/Frameworks/R.framework/Resources/include
>>>>> -I/usr/local/include   -fno-common  -g -O2 -c ranksort.c -o ranksort.o
>>>>> g77   -fno-common  -g -O2 -c rcorr.f -o rcorr.o
>>>>> g77   -fno-common  -g -O2 -c wclosest.f -o wclosest.o
>>>>> gcc -bundle -flat_namespace -undefined suppress -L/usr/local/lib -o
>>>>> Hmisc.so cidxcn.o cidxcp.o hoeffd.o jacklins.o largrec.o ranksort.o
>>>>> rcorr.o wclosest.o  -L/usr/local/lib
>>>>> -L/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2
>>>>> -L/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2/../../.. -lfrtbegin
>>>>> -lg2c -lSystem -lcc_dynamic -framework R
>>>>> ld: warning -L: directory name
>>>>> (/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2) does not exist
>>>>> ld: warning -L: directory name
>>>>> (/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2/../../..) does not
>>>>> exist
>>>>> ld: can't locate file for: -lfrtbegin
>>>>> make: *** [Hmisc.so] Error 1
>>>>> ERROR: compilation failed for package 'Hmisc'
>>>>> ** Removing
>>>>> '/Library/Frameworks/R.framework/Versions/2.0.1/Resources/library/
>>>>> Hmisc'
>>>>> ** Restoring previous
>>>>> '/Library/Frameworks/R.framework/Versions/2.0.1/Resources/library/
>>>>> Hmisc'
>
> _____________________________
> Professor Michael Kubovy
> University of Virginia
> Department of Psychology
> USPS: 	P.O.Box 400400	Charlottesville, VA 22904-4400
> Parcels:	Room 102		Gilmer Hall
> 		McCormick Road	Charlottesville, VA 22903
> Office:	B011	+1-434-982-4729
> Lab:		B019	+1-434-982-4751
> Fax:		+1-434-982-4766
> WWW:	http://www.people.virginia.edu/~mk9y/
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jari.oksanen at oulu.fi  Sun Feb  6 08:55:42 2005
From: jari.oksanen at oulu.fi (Jari Oksanen)
Date: Sun, 06 Feb 2005 09:55:42 +0200
Subject: [R] Problem installing Hmisc (more info)
In-Reply-To: <Pine.LNX.4.61.0502052148340.31181@gannet.stats>
References: <4e13cec2c11f35ef6ddafca643d03004@virginia.edu>
	<Pine.LNX.4.61.0502052148340.31181@gannet.stats>
Message-ID: <8ff290a6653ad5844332a9e9a4c6a5b3@oulu.fi>


On 5 Feb 2005, at 23:59, Prof Brian Ripley wrote:

> On Sat, 5 Feb 2005, Michael Kubovy wrote:
>
>> Frank Harrell suggested I re-post with information about the version 
>> of
>> R
>
> Thanks, that starts to make sense.  You appear to have g77 installed, 
> but not in the place the person who prepared the binary install of R 
> has it.
> Where do you have it installed?  ('whereis g77' or 'which g77' should 
> tell you.)  Then you need to alter FLIBS in R_HOME/etc/Makeconf to 
> point to it.  (You can remove -lfrtbegin from FLIBS: it is not needed: 
> your R_HOME looks to be 
> /Library/Frameworks/R.framework/Versions/2.0.1.)
>
> However, I believe there is a more fundamental problem: because libg2c 
> is a static library on current MacOS X, most packages using Fortran 
> cannot be compiled there.  That's presumably the case with Hmisc, as 
> the automated package builder is not providing a binary build.  (There 
> is supposedly a check directory on CRAN, but it is not there at 
> present.)
>
This must be a problem specific to a certain installation. I regularly 
build Fortran files into MacOS X binaries. The specification in this 
machine is:

pomme:~ jarioksanen$ uname -a
Darwin pomme.local 7.7.0 Darwin Kernel Version 7.7.0: Sun Nov  7 
16:06:51 PST 2004; root:xnu/xnu-517.9.5.obj~1/RELEASE_PPC  Power 
Macintosh powerpc
pomme:~ jarioksanen$ locate libg2c
/usr/local/lib/libg2c.0.0.0.dylib
/usr/local/lib/libg2c.0.dylib
/usr/local/lib/libg2c.a
/usr/local/lib/libg2c.dylib
/usr/local/lib/libg2c.la

So there are both static and dynamic libg2c's. I don't know of any 
package management system for Mac, so I don't know who installed these 
files, but probably it was g77. Probably I got these from 
http://hpc.sourceforge.net/, though (I try avoid Fink which is a 
constant source of trouble).

Similarly, the g2c is installed in /usr/local:

pomme:~ jarioksanen$ which g77
/usr/local/bin/g77

I got my g77 from a place pointed to in R-MacOS X FAQs.

I have often seen problems in MacOS with hardcoded paths which assume 
certain locations for files. Latest problem was that 'rgl' library 
assumes that 'libpng' to be in a different place that I had it. For 
instance, Darwin's Fink installs stuff in a unique place called /sw. 
Perhaps that's the problem? However, ideally MacOS software should work 
'jus anywhere' (and 'just work') like they say in ads.

cheers, jari oksanen
--
Jari Oksanen, Oulu, Finland



From ripley at stats.ox.ac.uk  Sun Feb  6 09:08:57 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 6 Feb 2005 08:08:57 +0000 (GMT)
Subject: [R] Problem installing Hmisc (more info)
In-Reply-To: <8ff290a6653ad5844332a9e9a4c6a5b3@oulu.fi>
References: <4e13cec2c11f35ef6ddafca643d03004@virginia.edu>
	<Pine.LNX.4.61.0502052148340.31181@gannet.stats>
	<8ff290a6653ad5844332a9e9a4c6a5b3@oulu.fi>
Message-ID: <Pine.LNX.4.61.0502060806320.11813@gannet.stats>

On Sun, 6 Feb 2005, Jari Oksanen wrote:

>
> On 5 Feb 2005, at 23:59, Prof Brian Ripley wrote:
>
>> On Sat, 5 Feb 2005, Michael Kubovy wrote:
>> 
>>> Frank Harrell suggested I re-post with information about the version of
>>> R
>> 
>> Thanks, that starts to make sense.  You appear to have g77 installed, but 
>> not in the place the person who prepared the binary install of R has it.
>> Where do you have it installed?  ('whereis g77' or 'which g77' should tell 
>> you.)  Then you need to alter FLIBS in R_HOME/etc/Makeconf to point to it. 
>> (You can remove -lfrtbegin from FLIBS: it is not needed: your R_HOME looks 
>> to be /Library/Frameworks/R.framework/Versions/2.0.1.)
>> 
>> However, I believe there is a more fundamental problem: because libg2c is a 
>> static library on current MacOS X, most packages using Fortran cannot be 
>> compiled there.  That's presumably the case with Hmisc, as the automated 
>> package builder is not providing a binary build.  (There is supposedly a 
>> check directory on CRAN, but it is not there at present.)
>> 
> This must be a problem specific to a certain installation. I regularly build 
> Fortran files into MacOS X binaries. The specification in this machine is:
>
> pomme:~ jarioksanen$ uname -a
> Darwin pomme.local 7.7.0 Darwin Kernel Version 7.7.0: Sun Nov  7 16:06:51 PST 
> 2004; root:xnu/xnu-517.9.5.obj~1/RELEASE_PPC  Power Macintosh powerpc
> pomme:~ jarioksanen$ locate libg2c
> /usr/local/lib/libg2c.0.0.0.dylib
> /usr/local/lib/libg2c.0.dylib
> /usr/local/lib/libg2c.a
> /usr/local/lib/libg2c.dylib
> /usr/local/lib/libg2c.la

That's not the same Darwin version ... this has certainly been a problem 
several times in the recent past.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From francoisromain at free.fr  Sun Feb  6 10:37:20 2005
From: francoisromain at free.fr (Romain Francois)
Date: Sun, 06 Feb 2005 10:37:20 +0100
Subject: [R] plot smooth density estimates for bivariate data
In-Reply-To: <Pine.SOL.4.58.0502051749320.18029@asteroids.gpcc.itd.umich.edu>
References: <Pine.SOL.4.58.0502051749320.18029@asteroids.gpcc.itd.umich.edu>
Message-ID: <4205E550.2070202@free.fr>

Hello,

try to look at those pages :

library(MASS)
?kde2d
?contour
?image
?persp


x11(height=6,width=12)
par(mfrow=c(1,2))
X <- rnorm(200)
Y <- rnorm(200)
d <- kde2d(X,Y)
persp(d,phi=30,theta=30)

image(d,col=terrain.colors(25))
contour(d,add=T)



Romain.

Le 05.02.2005 23:51, Yulei He a ?crit :

>Hi, there.
>
>Suppose I have a bivarariate data matrix y1 and y2. I want to plot a 3-D
>picture of the estimated density f(y1, y2) against y1 and y2? How can I do
>that? Do I use persp() or density()?
>
>  
>
density is for 1d estimation of density.

>Thanks for your help.
>
>Yulei
>
>
>$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
>Yulei He
>1586 Murfin Ave. Apt 37
>Ann Arbor, MI 48105-3135
>yuleih at umich.edu
>734-647-0305(H)
>734-763-0421(O)
>734-763-0427(O)
>734-764-8263(fax)
>$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
>
>  
>

-- 
Romain FRANCOIS : francoisromain at free.fr
page web : http://addictedtor.free.fr/  (en construction)
06 18 39 14 69 / 01 46 80 65 60
_______________________________________________________
Etudiant en 3eme ann?e
Institut de Statistique de l'Universit? de Paris (ISUP)
Fili?re Industrie et Services
http://www.isup.cicrp.jussieu.fr/



From ales.ziberna at guest.arnes.si  Sun Feb  6 10:49:51 2005
From: ales.ziberna at guest.arnes.si (=?ISO-8859-1?Q?Ales_Ziberna?=)
Date: Sun, 6 Feb 2005 10:49:51 +0100
Subject: [R] Problems compiling (configure) R on Ubuntu linux (debian)
References: <022501c50b70$a55d8760$1209f9c2@ales><x2is57f8b7.fsf@biostat.ku.dk>
	<16900.59886.67300.348574@basebud.nulle.part>
Message-ID: <011401c50c31$39bdad70$1109f9c2@ales>

Thanks to everybody for their valuble suggestions.

Unfortunaty I currently can not use apt-get or similar, since I can not 
connect to the net using Ubuntu (modem problems).

Thanks again to everybody!

Ales Ziberna




----- Original Message ----- 
From: "Dirk Eddelbuettel" <edd at debian.org>
To: "Peter Dalgaard" <p.dalgaard at biostat.ku.dk>
Cc: "Ales Ziberna" <ales.ziberna at guest.arnes.si>; "R-help" 
<r-help at stat.math.ethz.ch>
Sent: Saturday, February 05, 2005 4:44 PM
Subject: Re: [R] Problems compiling (configure) R on Ubuntu linux (debian)


>
> On 5 February 2005 at 14:25, Peter Dalgaard wrote:
> | =?windows-1250?Q?Ale=9A_=8Eiberna?= <ales.ziberna at guest.arnes.si> 
> writes:
> | > I am new to Linux and I tried to compile R on my Ubuntu Warty linux. I
>
> There is really no need to do this, especially when you're new to Linux. 
> The
> command
>
>       $ apt-get install r-base
>
> (or is graphical equivalent via aptitude et al) is your friend. You may 
> have
> to add Debian archives to the list of archives search when you do 'apt-get
> update'. I'm sure the Ubunto docs explain how to do that.
>
> Dirk, whose free Ubunto cdrom is still unused for lack of time
>
> -- 
> Better to have an approximate answer to the right question than a precise
> answer to the wrong question.  --  John Tukey as quoted by John Chambers
>
>



From adi at roda.ro  Sun Feb  6 13:02:37 2005
From: adi at roda.ro (Adrian Dusa)
Date: Sun, 6 Feb 2005 12:02:37 +0000 (UTC)
Subject: [R] file 'attributes'
Message-ID: <loom.20050206T130025-760@post.gmane.org>

Dear R-list,

I have many files on many CDs (probably same as many of you) and I would like 
to create a database containing only a few columns:
    - the name of the file
    - its extension
    - space occupied
    - date when it was created
    - its path
    - CD number (this should be typed in manually)

Is it possible to read a CD/structure of folders in such a way?
Thank you for any suggestion,
Adrian

-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd.
Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
              +40 21 3120210 / int.101



From andy_liaw at merck.com  Sun Feb  6 14:44:01 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sun, 6 Feb 2005 08:44:01 -0500
Subject: [R] file 'attributes'
Message-ID: <3A822319EB35174CA3714066D590DCD50994E665@usrymx25.merck.com>

file.info() should help with some of those items.

Andy

> From: Adrian Dusa
> 
> Dear R-list,
> 
> I have many files on many CDs (probably same as many of you) 
> and I would like 
> to create a database containing only a few columns:
>     - the name of the file
>     - its extension
>     - space occupied
>     - date when it was created
>     - its path
>     - CD number (this should be typed in manually)
> 
> Is it possible to read a CD/structure of folders in such a way?
> Thank you for any suggestion,
> Adrian
> 
> -- 
> Adrian Dusa
> Romanian Social Data Archive
> 1, Schitu Magureanu Bd.
> Bucharest sector 5
> Romania
> Tel./Fax: +40 21 3126618 \
>               +40 21 3120210 / int.101
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From edd at debian.org  Sun Feb  6 14:48:53 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 6 Feb 2005 07:48:53 -0600
Subject: [R] Problems compiling (configure) R on Ubuntu linux (debian)
In-Reply-To: <011401c50c31$39bdad70$1109f9c2@ales>
References: <022501c50b70$a55d8760$1209f9c2@ales>
	<x2is57f8b7.fsf@biostat.ku.dk>
	<16900.59886.67300.348574@basebud.nulle.part>
	<011401c50c31$39bdad70$1109f9c2@ales>
Message-ID: <16902.8261.144639.605121@basebud.nulle.part>


On 6 February 2005 at 10:49, Ales Ziberna wrote:
| Thanks to everybody for their valuble suggestions.
| 
| Unfortunaty I currently can not use apt-get or similar, since I can not 
| connect to the net using Ubuntu (modem problems).

So download r-base*deb under Windows, reboot, mount the Windows partition, cd
to it, dpkg -i r-base*.deb.

Dirk

-- 
Better to have an approximate answer to the right question than a precise 
answer to the wrong question.  --  John Tukey as quoted by John Chambers



From adi at roda.ro  Sun Feb  6 15:22:18 2005
From: adi at roda.ro (Adrian Dusa)
Date: Sun, 6 Feb 2005 14:22:18 +0000 (UTC)
Subject: [R] further issues with install.packages
Message-ID: <loom.20050206T151737-691@post.gmane.org>

Hi again,

I run R under SuSE 9.2 Professional (installed via rpm) and I am trying to 
install some packages from CRAN. The trouble is, after successful 
installations, my destdir directory is deleted...!

My command:
install.packages("Rcmdr", "/usr/lib/R/library",  
CRAN="http://cran.r-project.org", destdir="/home/adi/Kituri/R.packages", 
dependencies=TRUE)

After each package I get:
WARNING: UTF-8 locales are not currently supported

Package rgl gives:
ERROR: compilation failed for package 'rgl'
** Removing '/usr/lib/R/library/rgl'
** Restoring previous '/usr/lib/R/library/rgl'
Warning message:
Installation of package rgl had non-zero exit status in: 
install.packages("rgl", "/usr/lib/R/library", CRAN = 
"http://cran.r-project.org",

Trying to run Rcmdr I get:
library(Rcmdr)
Loading required package: tcltk
Error in fun(...) : this isn't a Tk applicationcouldn't connect to display 
"homelinux.roda.local:0.0"
Error: .onLoad failed in loadNamespace for 'tcltk'
Error: package 'tcltk' could not be loaded

I have the tcl and tk packages installed under SuSE.

The command whereis g77 gives:
g77: /usr/bin/g77 /usr/share/man/man1/g77.1.gz

R.version
         _
platform i686-pc-linux-gnu
arch     i686
os       linux-gnu
system   i686, linux-gnu
status
major    2
minor    0.1
year     2004
month    11
day      15
language R

Could you please advice?
Thank you,
Adrian

-- 
Adrian Dusa
Arhiva Romana de Date Sociale
Bd. Schitu Magureanu nr.1
Tel./Fax: +40 21 3126618 \
              +40 21 3120210 / int.101



From adi at roda.ro  Sun Feb  6 15:35:22 2005
From: adi at roda.ro (Adrian Dusa)
Date: Sun, 6 Feb 2005 14:35:22 +0000 (UTC)
Subject: [R] file 'attributes'
References: <3A822319EB35174CA3714066D590DCD50994E665@usrymx25.merck.com>
Message-ID: <loom.20050206T152307-723@post.gmane.org>

Liaw, Andy <andy_liaw <at> merck.com> writes:

> 
> file.info() should help with some of those items.
> 
> Andy
> 
> > From: Adrian Dusa
> > 
> > Dear R-list,
> > 
> > I have many files on many CDs (probably same as many of you) 
> > and I would like 
> > to create a database containing only a few columns:
> >     - the name of the file
> >     - its extension
> >     - space occupied
> >     - date when it was created
> >     - its path
> >     - CD number (this should be typed in manually)
> > 
> > Is it possible to read a CD/structure of folders in such a way?
> > Thank you for any suggestion,
> > Adrian
> > 
> > -- 
> > Adrian Dusa
> > Romanian Social Data Archive
> > 1, Schitu Magureanu Bd.
> > Bucharest sector 5
> > Romania
> > Tel./Fax: +40 21 3126618 \
> >               +40 21 3120210 / int.101
> > 


Brilliant, Andy. This is exactly what I needed. Now, if I could find a way to
scan a structure of folders, I could store the information about every file in
every folder in a table or a database...

Using ?file.info, I found another bunch of useful functions, like file.path,
list.files etc. It looks very promising.

I know there is a command in Linux called 'file', which determines the file type
 (a substitute for the extension). I wonder if I could use that command via R.

Thanks again,
Adrian



From ligges at statistik.uni-dortmund.de  Sun Feb  6 17:42:30 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 06 Feb 2005 17:42:30 +0100
Subject: [R] Labelling and formatting of graphics
In-Reply-To: <E1CxYlc-0000ax-00@smarthost2.mail.uk.easynet.net>
References: <E1CxYlc-0000ax-00@smarthost2.mail.uk.easynet.net>
Message-ID: <420648F6.1030806@statistik.uni-dortmund.de>

Nicholas Galwey wrote:

> In the output of the code below, I want to do the following:
> 
>  
> 
> -          get hats over some of the betas

See ?plotmath:
  expression(hat(beta))


> -          get the polygons stippled, not coloured grey


See ?polygon:
   polygon(selx, sely, density = 10)


> -          remove the tick marks at the ends of the axes.   If I put tick =
> false, the whole axis disappears.

Well, omit the axis at first, e.g. by yaxt="n", and add it using axis() 
specifying only those ticks you are going to see.


[lot of stuff removed here]

 > Moreover, in the output of the code below, I want to:
 >
 > -     get values placed by every tickmark, not just every alternate
 > tickmark

Reduce the font size (see ?par, cex.axis) or arrange the text 
perpendicular to the axis (see ?par, las).
It will be plotted if sufficient amount of space is available.


 > -     give the values to fewer places of decimals.

use round() in your call to axis().


Uwe Ligges



> Best wishes,
> 
> Nick Galwey
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From tom_hoary at web.de  Sun Feb  6 18:00:58 2005
From: tom_hoary at web.de (Thomas =?iso-8859-1?q?Sch=F6nhoff?=)
Date: Sun, 6 Feb 2005 18:00:58 +0100
Subject: [R] further issues with install.packages
In-Reply-To: <loom.20050206T151737-691@post.gmane.org>
References: <loom.20050206T151737-691@post.gmane.org>
Message-ID: <200502061800.59274.tom_hoary@web.de>

Hi Adrian,

Am Sonntag, 6. Februar 2005 15:22 schrieb Adrian Dusa:
> Hi again,
>
> I run R under SuSE 9.2 Professional (installed via rpm) and I am
> trying to install some packages from CRAN. The trouble is, after
> successful installations, my destdir directory is deleted...!

Rather strange? What about downloading all CRAN-packages in a selected 
directory, choosing in YAST this dir as an installation-resource? Try 
to install these packages again.
Does this work?


>
> My command:
> install.packages("Rcmdr", "/usr/lib/R/library",
> CRAN="http://cran.r-project.org",
> destdir="/home/adi/Kituri/R.packages", dependencies=TRUE)
>
> After each package I get:
> WARNING: UTF-8 locales are not currently supported
>
> Package rgl gives:
> ERROR: compilation failed for package 'rgl'
> ** Removing '/usr/lib/R/library/rgl'
> ** Restoring previous '/usr/lib/R/library/rgl'
> Warning message:
> Installation of package rgl had non-zero exit status in:
> install.packages("rgl", "/usr/lib/R/library", CRAN =
> "http://cran.r-project.org",

Sure that your current installations meets all required dependencies? 
Related READMEs of the packages will tell you! (look especially for 
versioning information of the required apps)

>
> I have the tcl and tk packages installed under SuSE.
>
> The command whereis g77 gives:
> g77: /usr/bin/g77 /usr/share/man/man1/g77.1.gz

Not quite sure if you have the required versions of these progs? I am 
running Suse-9.2, so far this seem to work smoothly....


regards
Thomas



From tuechler at gmx.at  Sun Feb  6 17:58:12 2005
From: tuechler at gmx.at (Heinz Tuechler)
Date: Sun, 06 Feb 2005 17:58:12 +0100
Subject: [R] How to access results of survival analysis
In-Reply-To: <42050BE3.40609@statistik.uni-dortmund.de>
References: <3.0.6.32.20050205172827.00797290@pop.gmx.net>
	<3.0.6.32.20050204231753.007aba90@pop.gmx.net>
	<3.0.6.32.20050204231753.007aba90@pop.gmx.net>
	<3.0.6.32.20050205172827.00797290@pop.gmx.net>
Message-ID: <3.0.6.32.20050206175812.0079e7b0@pop.gmx.net>

To sum up:
Starting from the question, how to access the results of survival analysis
Uwe Liggens suggested to ...copy code for calculation of the required
values from those print methods into your own functions...

Since I am new to R I choose the easiest way I knew and changed a few lines
in the print.survfit method. I introduced a parameter "ret.res=FALSE" set
to false to preserve the normal behaviour of print.
The second last line "invisible(x)" I changed to:

if (ret.res)
	invisible(list(x,x1))
else
	invisible(x)

Finally I source the changed function.
Of course this is only a temporary workaround, but it seems to work.

Thanks,
Heinz T?chler

At 19:09 05.02.2005 +0100, Uwe Ligges wrote:
>Heinz Tuechler wrote:
>
>> At 15:19 05.02.2005 +0100, Uwe Ligges wrote:
>> 
>>>Heinz Tuechler wrote:
>>>
>>>>Hello,
>>>>
>>>>it seems that the main results of survival analysis with package survival
>>>>are shown only as side effects of the print method.
>>>>
>>>>If I compute e.g. a Kaplan-Meier estimate by 
>>>>
>>>>
>>>>>km.survdur<-survfit(s.survdur) 
>>>>
>>>>then I can simply print the results by 
>>>>
>>>>
>>>>>km.survdur
>>>>
>>>>Call: survfit(formula = s.survdur)
>>>>
>>>>      n  events  median 0.95LCL 0.95UCL 
>>>>  100.0    58.0    46.8    41.0    79.3 
>>>>
>>>>Is there a simple method to access these results, e.g. if I want to print
>>>>only the median with the confidence limits?
>> 
>> 
>> ...
>> 
>>>No, the print methods do not return those values.
>>>But you can copy code for calculation of the required values from those 
>>>print methods into your own functions...
>>>
>>>Uwe Ligges
>>>
>>>
>> 
>> Thank you for your answer. I assume, you suggest to use
>> capture.output(print(...)). 
>
>No, I suggested to copy the code from survival:::print.survfit and Co. 
>that calculates the values you are looking for...
>
>Uwe
>
> > Without your response I would have believed
>> that I had missed an important possibility of R.
>> 
>> Regarding the Cox-Model Ales Ziberna gave me a useful hint to use summary()
>> which returns a list.
>> 
>> Thanks to both of you,
>> 
>> Heinz T?chler
>
>



From cg.pettersson at evp.slu.se  Sun Feb  6 19:37:07 2005
From: cg.pettersson at evp.slu.se (CG Pettersson)
Date: Sun, 06 Feb 2005 19:37:07 +0100
Subject: [R] Plotting from lme() objects
Message-ID: <200502061837.j16Ib7CT003599@mail1.slu.se>

Hello all!

R2.0.1, W2k

I use lme() from nlme to make mean estimates from series of field
experiments where not all treatments (like "variety") are present in
all experiments.

A typical call is:

yield.lme <- lme(Yield ~ Variety, data = data,
                            na.action = na.omit,
                            random = ~ 1 | Experiment/Block)

This works well, even when observations are lacking. I have checked
against the accepted method for doing this in Sweden, which is 
PROC MIXED in SAS, and the fitted fixed effects are more or less
identical. I use estimable() from gmodels (gregmisc package) to 
extract estimates, standard errors and such.  I use matrices with the 
variety names as row names, it works smooth.

What I am unable to, as yet, is to make nice plots of the estimates
for 
a given set of varieties. To use only the fixed call directly on the 
dataset works, but produces the wrong graph, as the structure is not 
used.

Pinheiro & Bates have a lot of graphics on lme objects, but they try 
to illustrate more sophisticated relations than my need. I?ve looked 
through gplots and the graphic parts of nlme without any hits.
Probably, 
my difficulties are just due to my own lack of skill. Some standard 
plotting facility plotting directly from the lme object ought to work,
but 
I don?t understand how. Another possiblility is to plot from the 
estimable() output.

How should I do this? All suggestions are welcome.

Cheers
/CG

CG Pettersson, MSci, PhD Stud.
Swedish University of Agricultural Sciences
Dep. of Ecology and Crop Production. Box 7043
SE-750 07 Uppsala



From francoisromain at free.fr  Sun Feb  6 20:15:53 2005
From: francoisromain at free.fr (Romain Francois)
Date: Sun, 06 Feb 2005 20:15:53 +0100
Subject: [R] file 'attributes'
In-Reply-To: <loom.20050206T152307-723@post.gmane.org>
References: <3A822319EB35174CA3714066D590DCD50994E665@usrymx25.merck.com>
	<loom.20050206T152307-723@post.gmane.org>
Message-ID: <42066CE9.8000200@free.fr>

Le 06.02.2005 15:35, Adrian Dusa a ?crit :

>Liaw, Andy <andy_liaw <at> merck.com> writes:
>
>  
>
>>file.info() should help with some of those items.
>>
>>Andy
>>
>>    
>>
>>>From: Adrian Dusa
>>>
>>>Dear R-list,
>>>
>>>I have many files on many CDs (probably same as many of you) 
>>>and I would like 
>>>to create a database containing only a few columns:
>>>    - the name of the file
>>>    - its extension
>>>    - space occupied
>>>    - date when it was created
>>>    - its path
>>>    - CD number (this should be typed in manually)
>>>
>>>Is it possible to read a CD/structure of folders in such a way?
>>>Thank you for any suggestion,
>>>Adrian
>>>
>>>-- 
>>>Adrian Dusa
>>>Romanian Social Data Archive
>>>1, Schitu Magureanu Bd.
>>>Bucharest sector 5
>>>Romania
>>>Tel./Fax: +40 21 3126618 \
>>>              +40 21 3120210 / int.101
>>>
>>>      
>>>
>
>
>Brilliant, Andy. This is exactly what I needed. Now, if I could find a way to
>scan a structure of folders, I could store the information about every file in
>every folder in a table or a database...
>
>Using ?file.info, I found another bunch of useful functions, like file.path,
>list.files etc. It looks very promising.
>
>I know there is a command in Linux called 'file', which determines the file type
> (a substitute for the extension). I wonder if I could use that command via R.
>
>  
>
Within a R session

system("file")


Take a look at ?system

Romain.

>Thanks again,
>Adrian
>
>  
>
-- 
Romain FRANCOIS : francoisromain at free.fr
page web : http://addictedtor.free.fr/  (en construction)
06 18 39 14 69 / 01 46 80 65 60
_______________________________________________________
Etudiant en 3eme ann?e
Institut de Statistique de l'Universit? de Paris (ISUP)
Fili?re Industrie et Services
http://www.isup.cicrp.jussieu.fr/



From tura at centroin.com.br  Sun Feb  6 20:48:28 2005
From: tura at centroin.com.br (Bernardo Rangel Tura)
Date: Sun, 06 Feb 2005 17:48:28 -0200
Subject: [R] Lecture
In-Reply-To: <loom.20050206T152307-723@post.gmane.org>
References: <3A822319EB35174CA3714066D590DCD50994E665@usrymx25.merck.com>
	<loom.20050206T152307-723@post.gmane.org>
Message-ID: <6.1.2.0.2.20050206174351.03dcadb0@centroin.com.br>

Hi R-masters!

>I you receive an invitation to make a lecture about R in Medical School of 
>Federal University of Rio de Janeiro.

>  The goal of the lecture is to stimulate the use of R instead of others 
> statiscal software in the Medical School.



>  It would like to hear tour suggestions on which topics to present in the 
> lecture.

Thanks in advance

Bernardo Rangel Tura, MD, MSc
National Institute of Cardiology Laranjeiras
Rio de Janeiro Brazil 


-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From miguel at aei.ca  Sun Feb  6 21:28:27 2005
From: miguel at aei.ca (miguel)
Date: Sun, 06 Feb 2005 15:28:27 -0500
Subject: [R] Matching rows between 2 matrix of different sizes
Message-ID: <42067DEB.7020201@aei.ca>

Hi, I would like is to add b's columns when a's first col. matches with 
b's first col. (a+b if a[i,1] == b[i,1]):

a <- matrix(c(11,22,33,4:9),nrow=3)
a
     [,1] [,2] [,3]
[1,]   11    4    7
[2,]   22    5    8
[3,]   33    6    9

b <- matrix(c(11,22,3:4),nrow=2)
b
     [,1] [,2]
[1,]   11    3
[2,]   22    4

The desired answer :
     [,1] [,2] [,3] [,4]
[1,]   11    4    7    3
[2,]   22    5    8    4
[3,]   33    6    9   NA

Thanks,



From ligges at statistik.uni-dortmund.de  Sun Feb  6 22:16:10 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 06 Feb 2005 22:16:10 +0100
Subject: [R] Lecture
In-Reply-To: <6.1.2.0.2.20050206174351.03dcadb0@centroin.com.br>
References: <3A822319EB35174CA3714066D590DCD50994E665@usrymx25.merck.com>	<loom.20050206T152307-723@post.gmane.org>
	<6.1.2.0.2.20050206174351.03dcadb0@centroin.com.br>
Message-ID: <4206891A.5010005@statistik.uni-dortmund.de>

Bernardo Rangel Tura wrote:
> Hi R-masters!
> 
>> I you receive an invitation to make a lecture about R in Medical 
>> School of Federal University of Rio de Janeiro.
> 
> 
>>  The goal of the lecture is to stimulate the use of R instead of 
>> others statiscal software in the Medical School.
> 
> 
> 
> 
>>  It would like to hear tour suggestions on which topics to present in 
>> the lecture.

I'd rather focus on R. ;-)


A subset of typical stuff for advertisements:
- The language and its capabilities
  + from the language point of view
  + huge number of statistical methods implemented
  + outstanding graphical capabilities
- GPL/Open Source
- packages
- for a medical school Bioconductor might be of high relevance

Uwe Ligges


> 
> Thanks in advance
> 
> Bernardo Rangel Tura, MD, MSc
> National Institute of Cardiology Laranjeiras
> Rio de Janeiro Brazil
>



From ligges at statistik.uni-dortmund.de  Sun Feb  6 22:20:58 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 06 Feb 2005 22:20:58 +0100
Subject: [R] Matching rows between 2 matrix of different sizes
In-Reply-To: <42067DEB.7020201@aei.ca>
References: <42067DEB.7020201@aei.ca>
Message-ID: <42068A3A.9030302@statistik.uni-dortmund.de>

miguel wrote:

> Hi, I would like is to add b's columns when a's first col. matches with 
> b's first col. (a+b if a[i,1] == b[i,1]):
> 
> a <- matrix(c(11,22,33,4:9),nrow=3)
> a
>     [,1] [,2] [,3]
> [1,]   11    4    7
> [2,]   22    5    8
> [3,]   33    6    9
> 
> b <- matrix(c(11,22,3:4),nrow=2)
> b
>     [,1] [,2]
> [1,]   11    3
> [2,]   22    4
> 
> The desired answer :
>     [,1] [,2] [,3] [,4]
> [1,]   11    4    7    3
> [2,]   22    5    8    4
> [3,]   33    6    9   NA


See ?merge (well, works on data.frames):
    merge(a, b, by=1, all=TRUE)

Uwe Ligges


> Thanks,
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From katchmalik at gmail.com  Mon Feb  7 01:33:10 2005
From: katchmalik at gmail.com (Shlomo Katchmalik)
Date: Sun, 6 Feb 2005 19:33:10 -0500
Subject: [R] RODBC working in Rgui but not Rterm
Message-ID: <38a7b0c80502061633634f98a9@mail.gmail.com>

Hello Users:

I'm using R version 2.0.1, and having problems with RODBC.  Everything
works fine when I use Rgui, but when I try to use Rterm and issue the
commands
  library(RODBC)
  con <- odbcConnect("MySQL", "test")

I get the following error:
Error in sqlQuery(con, str) : first argument is not an open RODBC channel       
In addition: Warning messages:                                                  
1: [RODBC] ERROR: state IM008, code 0, message [MySQL][ODBC 3.51 Driver]Invalid 
window handle for connection completion argument.                               
2: ODBC connection failed in: odbcDriverConnect(st, case = case, believeNRows = 
believeNRows)

Does anybody know how I can fix this problem?

Thanks,
Shlomo.



From xuanmars at hotmail.com  Mon Feb  7 01:44:47 2005
From: xuanmars at hotmail.com (xuan ...)
Date: Mon, 07 Feb 2005 08:44:47 +0800
Subject: [R] query about the nlm function
Message-ID: <BAY22-F35DF8D86F38B07B1306BB3C0730@phx.gbl>

Hi, I am a beginner to the R program and am currently having some problems 
with the nlm function. I have already checked that my function is a positive 
quadratic and since it has only one turning point (i plotted the graph to 
check) there should be no problems in terms of finding the minimum point. 
However, I keep getting error messages saying :
Error in nlm(poi.loglike, p = 0.3) : invalid function value in 'nlm' 
optimizer

Why does this error message appear and what are the usual causes of this 
error?
My primary aim is to calculate the negative poisson log likelihood function 
and by using the nlm function, to find the  maximum likelihood estimate.

Thanks to all who take their time to read and hopefully enlighten me with 
regards to this simple question.



From katchmalik at gmail.com  Mon Feb  7 02:10:20 2005
From: katchmalik at gmail.com (Shlomo Katchmalik)
Date: Sun, 6 Feb 2005 20:10:20 -0500
Subject: [R] RODBC working in Rgui but not Rterm
In-Reply-To: <86EED55AE819274B8308B62A225457D8016B870A@exch1.qsa.local>
References: <86EED55AE819274B8308B62A225457D8016B870A@exch1.qsa.local>
Message-ID: <38a7b0c80502061710398dae08@mail.gmail.com>

Thanks for your response.  I'm not aware of passing any arguments to
either Rgui or Rterm.  I have the R\bin subdirectory in my path, so to
run the programs, I simply type "rgui" or "rterm" from the DOS
(actually TakeCommand) prompt.

---------- Forwarded message ----------
From: Andrew Ward <Andrew.Ward at qsa.qld.edu.au>
Date: Mon, 7 Feb 2005 10:49:56 +1000
Subject: RE: [R] RODBC working in Rgui but not Rterm
To: Shlomo Katchmalik <katchmalik at gmail.com>


I wonder if you provide any arguments in Rgui
that aren't given with Rterm? For instance,
if in Rgui you get prompted for a username
or password, then these ought to appear in
the called to odbcConnect. Can you let us
know if you've already tried this?

Regards,

Andrew C. Ward,                andrew.ward at qsa.qld.edu.au
Senior Analyst (Quantitative), Tel: +61 7 3864 0439
Queensland Studies Authority,  Fax: +61 7 3229 3318
295 Ann Street,
Brisbane Qld 4000, Australia

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Shlomo Katchmalik
Sent: Monday, 7 February 2005 10:33 AM
To: r-help at stat.math.ethz.ch
Subject: [R] RODBC working in Rgui but not Rterm

Hello Users:

I'm using R version 2.0.1, and having problems with RODBC.  Everything
works fine when I use Rgui, but when I try to use Rterm and issue the
commands
  library(RODBC)
  con <- odbcConnect("MySQL", "test")

I get the following error:
Error in sqlQuery(con, str) : first argument is not an open RODBC channel
In addition: Warning messages:
1: [RODBC] ERROR: state IM008, code 0, message [MySQL][ODBC 3.51 Driver]Invalid
window handle for connection completion argument.
2: ODBC connection failed in: odbcDriverConnect(st, case = case, believeNRows =
believeNRows)

Does anybody know how I can fix this problem?

Thanks,
Shlomo.



From jinss at hkusua.hku.hk  Mon Feb  7 03:23:21 2005
From: jinss at hkusua.hku.hk (Jin Shusong)
Date: Mon, 7 Feb 2005 10:23:21 +0800
Subject: [R] Can R output figures as metapost file
Message-ID: <20050207022321.GA5901@S127.localdomain>

Dear R users,

I'd like to ask that can R output the figures in metapost
format so that sometimes I can modify the figures.

Thanks a lot.


  Jin



From baron at psych.upenn.edu  Mon Feb  7 03:34:57 2005
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Sun, 6 Feb 2005 21:34:57 -0500
Subject: [R] Can R output figures as metapost file
In-Reply-To: <20050207022321.GA5901@S127.localdomain>
References: <20050207022321.GA5901@S127.localdomain>
Message-ID: <20050207023457.GA26362@psych>

On 02/07/05 10:23, Jin Shusong wrote:
 Dear R users,
 
 I'd like to ask that can R output the figures in metapost
 format so that sometimes I can modify the figures.

At least in Linux, you can output xfig format, and current
versions of xfig will let you modify the figure, or save it as
metapost.

J
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron
R search page: http://finzi.psych.upenn.edu/



From rountree at cs.uga.edu  Mon Feb  7 03:41:17 2005
From: rountree at cs.uga.edu (Barry Rountree)
Date: Sun, 6 Feb 2005 21:41:17 -0500
Subject: [R] Pass along a "Thank you!" to Timur Elzhov
Message-ID: <200502062141.59778.rountree@cs.uga.edu>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Re msg:  Sat Jul 5 21:13:52 MEST 2003 [R] Second Y axis

You replied to an R question a year and a half ago.  I had exactly the same 
question, and you've made my evening much nicer.  I appreciate it.

Best,

Barry Rountree
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.2.4 (GNU/Linux)

iD8DBQFCBtVNhlM7FvEcdO8RAk8hAJ9U1XXqxEdtzkQpT63JUf9XHGaAWACfSFup
gaMyVbAjrRNriD7W9yom0UI=
=6tcw
-----END PGP SIGNATURE-----



From katchmalik at gmail.com  Mon Feb  7 05:03:26 2005
From: katchmalik at gmail.com (Shlomo Katchmalik)
Date: Sun, 6 Feb 2005 23:03:26 -0500
Subject: [R] RODBC working in Rgui but not Rterm
In-Reply-To: <86EED55AE819274B8308B62A225457D8016B870C@exch1.qsa.local>
References: <86EED55AE819274B8308B62A225457D8016B870C@exch1.qsa.local>
Message-ID: <38a7b0c8050206200374e4c67c@mail.gmail.com>

Oh, I see what you mean now.  Sorry for the misunderstanding.

I have my local MySQL database set with a very lax level of security,
so I don't need to use a password.  All I have needed in the past has
been to supply the uid, and everything has worked just fine, both for
Rgui and Rterm.  However, I recently reinstalled all of my software
and databases on a new machine, and since then, I haven't been able to
get RODBC to work from Rterm.

If I use only odbcConnect("MySQL"), I would previously get the dialog
box that you mention.  Now, however, I do not get that dialog box.

On Mon, 7 Feb 2005 12:32:08 +1000, Andrew Ward
<Andrew.Ward at qsa.qld.edu.au> wrote:
> I wasn't very clear, I'm afraid. When I call odbcConnect
> from within Rgui, a dialog box pops up asking me to enter
> a username and password (see ?odbcConnect). I wondered if
> you need to provide these in Rterm in your call to
> odbcConnect:
>         library(RODBC)
>         con <- odbcConnect(dsn="MySQL", uid="test", pwd="")
> 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Shlomo Katchmalik
> Sent: Monday, 7 February 2005 11:10 AM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] RODBC working in Rgui but not Rterm
> 
> Thanks for your response.  I'm not aware of passing any arguments to
> either Rgui or Rterm.  I have the R\bin subdirectory in my path, so to
> run the programs, I simply type "rgui" or "rterm" from the DOS
> (actually TakeCommand) prompt.
> 
> ---------- Forwarded message ----------
> From: Andrew Ward <Andrew.Ward at qsa.qld.edu.au>
> Date: Mon, 7 Feb 2005 10:49:56 +1000
> Subject: RE: [R] RODBC working in Rgui but not Rterm
> To: Shlomo Katchmalik <katchmalik at gmail.com>
> 
> I wonder if you provide any arguments in Rgui
> that aren't given with Rterm? For instance,
> if in Rgui you get prompted for a username
> or password, then these ought to appear in
> the called to odbcConnect. Can you let us
> know if you've already tried this?
> 
> Regards,
> 
> Andrew C. Ward,                andrew.ward at qsa.qld.edu.au
> Senior Analyst (Quantitative), Tel: +61 7 3864 0439
> Queensland Studies Authority,  Fax: +61 7 3229 3318
> 295 Ann Street,
> Brisbane Qld 4000, Australia
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Shlomo Katchmalik
> Sent: Monday, 7 February 2005 10:33 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] RODBC working in Rgui but not Rterm
> 
> Hello Users:
> 
> I'm using R version 2.0.1, and having problems with RODBC.  Everything
> works fine when I use Rgui, but when I try to use Rterm and issue the
> commands
>   library(RODBC)
>   con <- odbcConnect("MySQL", "test")
> 
> I get the following error:
> Error in sqlQuery(con, str) : first argument is not an open RODBC channel
> In addition: Warning messages:
> 1: [RODBC] ERROR: state IM008, code 0, message [MySQL][ODBC 3.51 Driver]Invalid
> window handle for connection completion argument.
> 2: ODBC connection failed in: odbcDriverConnect(st, case = case, believeNRows =
> believeNRows)
> 
> Does anybody know how I can fix this problem?
> 
> Thanks,
> Shlomo.
>
>



From adrian at maths.uwa.edu.au  Mon Feb  7 04:56:43 2005
From: adrian at maths.uwa.edu.au (Adrian Baddeley)
Date: Mon, 7 Feb 2005 11:56:43 +0800
Subject: [R] Environment of a formula
Message-ID: <16902.59131.805654.740605@maths.uwa.edu.au>

Wise and merciful R-helpers:

I want to equip a data frame with an attribute 
which specifies how to plot some of the columns. 

Up to now we have been doing this by giving the data frame 
a `formula' attribute, that can be passed to plot.formula.

For example
      dat <- data.frame(x=1:100,y=runif(100),z=100:1)
      attr(dat, "plotme") <- (z ~ x)
      ......
      ......
      if(missing(desiredformula))
	desiredformula <- attr(dat, "plotme")
      plot(desiredformula, data=dat)

We just got bitten by the fact that a formula object has a `.Environment'
attribute, which may be huge, depending on the environment
in which the formula was created. In the example above there is
no upper limit on the size of the object 'dat' !!!!
That is, environment(attr(dat, "plotme")) could be huge.

What is the recommended/safe way to avoid this?
I don't need the formula to have an environment at all; I'm just using
the formula structure to represent the format of the plot.

It appears that we can't set the environment to NULL;
should we set it to the Global environment e.g. using as.formula?

Or is it wiser to save the formula as a character string 
in the object 'dat' and only convert it back to a formula 
at the last possible moment?

thanks

Adrian Baddeley



From ggrothendieck at myway.com  Mon Feb  7 05:39:40 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 7 Feb 2005 04:39:40 +0000 (UTC)
Subject: [R] Environment of a formula
References: <16902.59131.805654.740605@maths.uwa.edu.au>
Message-ID: <loom.20050207T053403-878@post.gmane.org>

Adrian Baddeley <adrian <at> maths.uwa.edu.au> writes:

: I want to equip a data frame with an attribute 
: which specifies how to plot some of the columns. 
: 
: Up to now we have been doing this by giving the data frame 
: a `formula' attribute, that can be passed to plot.formula.
: 
: For example
:       dat <- data.frame(x=1:100,y=runif(100),z=100:1)
:       attr(dat, "plotme") <- (z ~ x)
:       ......
:       ......
:       if(missing(desiredformula))
: 	desiredformula <- attr(dat, "plotme")
:       plot(desiredformula, data=dat)
: 
: We just got bitten by the fact that a formula object has a `.Environment'
: attribute, which may be huge, depending on the environment
: in which the formula was created. In the example above there is
: no upper limit on the size of the object 'dat' !!!!
: That is, environment(attr(dat, "plotme")) could be huge.

Do you mean that if fo is the formula then ls(environment(fo))
has many large components?   I don't understand why that would
be a problem.

: 
: What is the recommended/safe way to avoid this?
: I don't need the formula to have an environment at all; I'm just using
: the formula structure to represent the format of the plot.
: 
: It appears that we can't set the environment to NULL;
: should we set it to the Global environment e.g. using as.formula?

It works for me (R 2.1.0 Windows):

R> fo <- y~x
R> environment(fo)
<environment: R_GlobalEnv>
R> environment(fo) <- NULL
R> environment(fo)
NULL

: 
: Or is it wiser to save the formula as a character string 
: in the object 'dat' and only convert it back to a formula 
: at the last possible moment?



From jfox at mcmaster.ca  Mon Feb  7 06:41:49 2005
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 7 Feb 2005 00:41:49 -0500
Subject: [R] RODBC working in Rgui but not Rterm
In-Reply-To: <38a7b0c8050206200374e4c67c@mail.gmail.com>
Message-ID: <20050207054140.VNXL1836.tomts16-srv.bellnexxia.net@JohnDesktop8300>

Dear Shlomo,

Just last week I experienced a similar problem with RODBC and MySQL under
Windows, though the problem extended to both Rterm *and* Rgui. After a fair
amount of detective work, I discovered that the source of the difficulty was
a newly downloaded MySQL ODBC Windows driver. When I instead installed a
driver downloaded last December, everything worked fine. I've just confirmed
that I can access MySQL from both Rterm and Rgui. I'm using R 2.0.1 under
Windows XP.

Since your symptoms are slightly different from mine, the ODBC driver may
not be the source of the difficulty, but I thought that it might be
worthwhile to mention it.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Shlomo 
> Katchmalik
> Sent: Sunday, February 06, 2005 11:03 PM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] RODBC working in Rgui but not Rterm
> 
> Oh, I see what you mean now.  Sorry for the misunderstanding.
> 
> I have my local MySQL database set with a very lax level of 
> security, so I don't need to use a password.  All I have 
> needed in the past has been to supply the uid, and everything 
> has worked just fine, both for Rgui and Rterm.  However, I 
> recently reinstalled all of my software and databases on a 
> new machine, and since then, I haven't been able to get RODBC 
> to work from Rterm.
> 
> If I use only odbcConnect("MySQL"), I would previously get 
> the dialog box that you mention.  Now, however, I do not get 
> that dialog box.
> 
> On Mon, 7 Feb 2005 12:32:08 +1000, Andrew Ward 
> <Andrew.Ward at qsa.qld.edu.au> wrote:
> > I wasn't very clear, I'm afraid. When I call odbcConnect 
> from within 
> > Rgui, a dialog box pops up asking me to enter a username 
> and password 
> > (see ?odbcConnect). I wondered if you need to provide these 
> in Rterm 
> > in your call to
> > odbcConnect:
> >         library(RODBC)
> >         con <- odbcConnect(dsn="MySQL", uid="test", pwd="")
> > 
> > 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Shlomo 
> > Katchmalik
> > Sent: Monday, 7 February 2005 11:10 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: Re: [R] RODBC working in Rgui but not Rterm
> > 
> > Thanks for your response.  I'm not aware of passing any 
> arguments to 
> > either Rgui or Rterm.  I have the R\bin subdirectory in my 
> path, so to 
> > run the programs, I simply type "rgui" or "rterm" from the DOS 
> > (actually TakeCommand) prompt.
> > 
> > ---------- Forwarded message ----------
> > From: Andrew Ward <Andrew.Ward at qsa.qld.edu.au>
> > Date: Mon, 7 Feb 2005 10:49:56 +1000
> > Subject: RE: [R] RODBC working in Rgui but not Rterm
> > To: Shlomo Katchmalik <katchmalik at gmail.com>
> > 
> > I wonder if you provide any arguments in Rgui that aren't 
> given with 
> > Rterm? For instance, if in Rgui you get prompted for a username or 
> > password, then these ought to appear in the called to 
> odbcConnect. Can 
> > you let us know if you've already tried this?
> > 
> > Regards,
> > 
> > Andrew C. Ward,                andrew.ward at qsa.qld.edu.au
> > Senior Analyst (Quantitative), Tel: +61 7 3864 0439 
> Queensland Studies 
> > Authority,  Fax: +61 7 3229 3318
> > 295 Ann Street,
> > Brisbane Qld 4000, Australia
> > 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Shlomo 
> > Katchmalik
> > Sent: Monday, 7 February 2005 10:33 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] RODBC working in Rgui but not Rterm
> > 
> > Hello Users:
> > 
> > I'm using R version 2.0.1, and having problems with RODBC.  
> Everything 
> > works fine when I use Rgui, but when I try to use Rterm and 
> issue the 
> > commands
> >   library(RODBC)
> >   con <- odbcConnect("MySQL", "test")
> > 
> > I get the following error:
> > Error in sqlQuery(con, str) : first argument is not an open RODBC 
> > channel In addition: Warning messages:
> > 1: [RODBC] ERROR: state IM008, code 0, message [MySQL][ODBC 3.51 
> > Driver]Invalid window handle for connection completion argument.
> > 2: ODBC connection failed in: odbcDriverConnect(st, case = case, 
> > believeNRows =
> > believeNRows)
> > 
> > Does anybody know how I can fix this problem?
> > 
> > Thanks,
> > Shlomo.
> >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ales.ziberna at guest.arnes.si  Mon Feb  7 07:44:16 2005
From: ales.ziberna at guest.arnes.si (=?iso-8859-1?Q?Ales_Ziberna?=)
Date: Mon, 7 Feb 2005 07:44:16 +0100
Subject: [R] Problems compiling (configure) R on Ubuntu linux (debian)
References: <022501c50b70$a55d8760$1209f9c2@ales><x2is57f8b7.fsf@biostat.ku.dk><16900.59886.67300.348574@basebud.nulle.part><011401c50c31$39bdad70$1109f9c2@ales>
	<16902.8261.144639.605121@basebud.nulle.part>
Message-ID: <017701c50ce1$0416b8a0$0309f9c2@ales>

I tried eactly that, however there are numerus dependecies. This seams to be 
the problem.

I have till now manage to install (via *.deb file) R1.9.1 thanks to 
following all depnedecies found using the following site (un unofficial site 
for searching Ubuntu packages):
http://higgs.djpig.de/ubuntu/www/

However, R 2.0.1 requires some different packages or newer versions and 
therfore will not work!

Thank you anyways for your answer!

Ales Ziberna


----- Original Message ----- 
From: "Dirk Eddelbuettel" <edd at debian.org>
To: "Ales Ziberna" <ales.ziberna at guest.arnes.si>
Cc: "R-help" <r-help at stat.math.ethz.ch>
Sent: Sunday, February 06, 2005 2:48 PM
Subject: Re: [R] Problems compiling (configure) R on Ubuntu linux (debian)


>
> On 6 February 2005 at 10:49, Ales Ziberna wrote:
> | Thanks to everybody for their valuble suggestions.
> |
> | Unfortunaty I currently can not use apt-get or similar, since I can not
> | connect to the net using Ubuntu (modem problems).
>
> So download r-base*deb under Windows, reboot, mount the Windows partition, 
> cd
> to it, dpkg -i r-base*.deb.
>
> Dirk
>
> -- 
> Better to have an approximate answer to the right question than a precise
> answer to the wrong question.  --  John Tukey as quoted by John Chambers
>
>



From petr.pikal at precheza.cz  Mon Feb  7 08:12:57 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 07 Feb 2005 08:12:57 +0100
Subject: [R] subset data.frame with value != in all columns
In-Reply-To: <s20359f0.085@gwsmtp.DEC.STATE.NY.US>
Message-ID: <42072309.25872.6A8B15@localhost>

Hi Tim

I can not say much about apply, but the code with unique(which()) 
gives you reordered rows in case of -9999 selection

try

set.seed(1)
in.df <- data.frame(
c1=rnorm(40000),
c2=rnorm(40000),
c3=rnorm(40000),
c4=rnorm(40000),
c5=rnorm(40000))
in.df[in.df>3] <- (-9999)

system.time(e <- in.df[unique(which(in.df == -9999, arr.ind = 
TRUE)[,1]), ])
system.time(e1 <- in.df[(rowSums(in.df == -9999)) != 0,])

all.equal(e,e1)

So if you mind you need to do reordering.

ooo<-order(as.numeric(rownames(e)))
all.equal(e[ooo,],e1)

Cheers
Petr

On 4 Feb 2005 at 11:17, Tim Howard wrote:

> Because I'll be doing this on big datasets and time is important, I
> thought I'd time all the different approaches that were suggested on a
> small dataframe. The results were very instructive so I thought I'd
> pass them on. I also discovered that my numeric columns (e.g.
> -9999.000) weren't found by apply() but were found by which() and the
> simple replace. Was it apply's fault or something else?
> 
> Note how much faster unique(which()) is; wow! Thanks to Marc Schwartz
> for this blazing solution.
> 
> > nrow(in.df)
> [1] 40000
> #extract rows with no -9999
> > system.time(x <- subset(in.df, apply(in.df, 1,
> function(in.df){all(in.df != -9999)})))
> [1] 3.25 0.00 3.25   NA   NA
> > system.time(y<- in.df[-unique(which(in.df == -9999, arr.ind =
> > TRUE)[,
> 1]), ])
> [1] 0.17 0.00 0.17   NA   NA
> > system.time({is.na(in.df) <-in.df == -9999; z <- na.omit(in.df)})
> [1] 0.25 0.02 0.26   NA   NA
> 
> > nrow(x);nrow(y);nrow(z)
> [1] 39990
> [1] 39626
> [1] 39626
> 
> #extract rows with -9999
> > system.time(d<-subset(in.df, apply(in.df, 1,
> function(in.df){any(in.df == -9999)})))
> [1] 3.40 0.00 3.45   NA   NA
> > system.time(e<-in.df[unique(which(in.df == -9999, arr.ind = TRUE)[,
> 1]), ])
> [1] 0.11 0.00 0.11   NA   NA
> 
> > nrow(d); nrow(e)
> [1] 10
> [1] 374
> 
> Tim Howard
> 
> 
> >>> Marc Schwartz <MSchwartz at MedAnalytics.com> 02/03/05 03:24PM >>>
> On Thu, 2005-02-03 at 14:57 -0500, Tim Howard wrote: 
>   ... snip...
> > My questions: 
> > Is there a cleaner way to extract all rows containing a specified
> > value? How can I extract all rows that don't have this value in any
> > col?
> > 
> > #create dummy dataset
> > x <- data.frame(
> > c1=c(-99,-99,-99,4:10),
> > c2=1:10,
> > c3=c(1:3,-99,5:10),
> > c4=c(10:1),
> > c5=c(1:9,-99))
> > 
> ..snip...
> 
> How about this, presuming that your data frame is all numeric:
> 
> For rows containing -99:
> 
> > x[unique(which(x == -99, arr.ind = TRUE)[, 1]), ]
>     c1 c2  c3 c4  c5
> 1  -99  1   1 10   1
> 2  -99  2   2  9   2
> 3  -99  3   3  8   3
> 4    4  4 -99  7   4
> 10  10 10  10  1 -99
> 
> 
> For rows not containing -99:
> 
> > x[-unique(which(x == -99, arr.ind = TRUE)[, 1]), ]
>   c1 c2 c3 c4 c5
> 5  5  5  5  6  5
> 6  6  6  6  5  6
> 7  7  7  7  4  7
> 8  8  8  8  3  8
> 9  9  9  9  2  9
> 
> 
> What I have done here is to use which(), setting arr.ind = TRUE. This
> returns the row, column indices for the matches to the boolean
> statement. The first column returned by which() in this case are the
> row numbers matching the statement, so I take the first column only.
> 
> Since it is possible that more than one element in a row can match the
> boolean, I then use unique() to get the singular row values.
> 
> Thus, I can use the returned row indices above to subset the data
> frame.
> 
> HTH,
> 
> Marc Schwartz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From markus.jantti at iki.fi  Mon Feb  7 09:09:25 2005
From: markus.jantti at iki.fi (Markus Jntti)
Date: Mon, 07 Feb 2005 10:09:25 +0200
Subject: [R] Std Err on Concentration measures
In-Reply-To: <20050205114302.488cd00d.secchi@sssup.it>
References: <20050205114302.488cd00d.secchi@sssup.it>
Message-ID: <42072235.4080007@iki.fi>

Angelo Secchi wrote:
> Hi,
> I'm using the ineq package to calculate some concentration measures
(Gini, Herfindal, ...) and I was wondering if there's around also a
function to calculate standard error on these measures. If not, is anybody
aware of where I can find a reference on this point?
> Thanks.
>

There are several suggestions for estimating the variance of the Gini
coefficient:a literature search should reveal some suggestions.
If the sampling design is complex, you may be best off using a
resampling method, such as bootstrap.

Here are a few references:

@Article{nayakandgastwirth1989,
   Author         = {Tapan K Nayak and Joseph L Gastwirth},
   Title          = {The use of diversity analysis to assess the relative
                    influence of factors affecting the income
distribution},
   Journal        = {Journal of Business \& Economic Statistics},
   Volume         = {7},
   Pages          = {453--460},
   subject        = {Asymptotic distribution Diversity measure Gini index
U
                    statistic Utility function},
   year           = 1989,
   month          = oct,
}

@Article{sandstromwretmanandwalden1988a,
   Author         = {Arne Sandstr?m and Jan Wretman and Bertil Walden},
   Title          = {Variance estimators of the Gini coefficient -
                    probability sampling},
   Journal        = {Journal of Business \& Economic Statistics},
   Volume         = {6},
   Pages          = {113--119},
   subject        = {Simulations},
   year           = 1988,
   month          = jan,
}

@Article{davidsonandduclos1997,
   Author         = {Russell Davidson and Jean-Yves Duclos},
   Title          = {Statistical inference for the measurement of the
                    incidence of taxes and transfers},
   Journal        = {Econometrica},
   Volume         = {65},
   Number         = {6},
   Pages          = {1453--1465},
   month          = {November},
   year           = 1997,
}

cheers,

markus
-- 
Markus Jantti
Abo Akademi University
markus.jantti at iki.fi
http://www.iki.fi/~mjantti
###########################################

This message has been scanned by F-Secure Anti-Virus for Mic...{{dropped}}



From r.hankin at soc.soton.ac.uk  Mon Feb  7 09:14:42 2005
From: r.hankin at soc.soton.ac.uk (Robin Hankin)
Date: Mon, 7 Feb 2005 08:14:42 +0000
Subject: [R] proportional matrix rows
Message-ID: <e1873c31b3836a68eb0e9532b87d6733@soc.soton.ac.uk>

Hi

I have a two-column integer matrix like this:


R> jj

       [,1] [,2]
  [1,]   -1    1
  [2,]   -2    2
  [3,]   -7    6
  [4,]   -8    7
  [5,]   -6    5
  [6,]   -9    8
  [7,]   -5    4
  [8,]    3   -3
  [9,]  -10    9
[10,]   -4    3

I want a diagnostic that detects whether a row is a multiple of
the first row or not.  In this case, this would be rows 1,2, and 8.

How to do this nicely?  Sometimes the first row has a zero, so the
method would have to work on

      [,1] [,2]
[1,]    0    1
[2,]    1    1
[3,]    0    8
[4,]    0   -4
[5,]    0    0
[6,]    4    0
 >

in which case rows 1,3,4,5 are multiples.  It'd be nice to have
a solution that works for any number of columns.

--
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From ripley at stats.ox.ac.uk  Mon Feb  7 09:24:37 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 7 Feb 2005 08:24:37 +0000 (GMT)
Subject: [R] RODBC working in Rgui but not Rterm
In-Reply-To: <20050207054140.VNXL1836.tomts16-srv.bellnexxia.net@JohnDesktop8300>
References: <20050207054140.VNXL1836.tomts16-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <Pine.LNX.4.61.0502070740500.32227@gannet.stats>

On Mon, 7 Feb 2005, John Fox wrote:

> Dear Shlomo,
>
> Just last week I experienced a similar problem with RODBC and MySQL under
> Windows, though the problem extended to both Rterm *and* Rgui. After a fair
> amount of detective work, I discovered that the source of the difficulty was
> a newly downloaded MySQL ODBC Windows driver. When I instead installed a
> driver downloaded last December, everything worked fine. I've just confirmed
> that I can access MySQL from both Rterm and Rgui. I'm using R 2.0.1 under
> Windows XP.
>
> Since your symptoms are slightly different from mine, the ODBC driver may
> not be the source of the difficulty, but I thought that it might be
> worthwhile to mention it.

There are two problems here

1) The latest MyODBC (3.51-10) has a problem that 3.51-07 did not have, as 
I found yesterday on both Linux and Windows (I am preparing an RODBC 
update).

I've now read the sources, and it has a logic bug: it throws the error

    state IM008 Invalid window handle for connection completion argument

whether or not any completion is needed.  RODBC 1.1-3 will have a 
workaround for this.

2) There _is_ a documented difference between RGui and Rterm: see 
?odbcConnect:

      Under the Windows GUI, specifying an incomplete 'connection', for
      example the default '""', will bring up a dialog box to complete
      the information required.  (This does not work from 'Rterm.exe'
      unless a driver is specified, a Windows restriction.)

As a result of these two, MyODBC 3.51-10 ought to work under RGui and not 
under Rterm.

Moral: if you can, downgrade your MyODBC driver, or wait for RODBC_1.1-3

Brian

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Mon Feb  7 09:35:09 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 7 Feb 2005 09:35:09 +0100
Subject: [R] Environment of a formula
In-Reply-To: <loom.20050207T053403-878@post.gmane.org>
References: <16902.59131.805654.740605@maths.uwa.edu.au>
	<loom.20050207T053403-878@post.gmane.org>
Message-ID: <16903.10301.31147.368621@stat.math.ethz.ch>

>>>>> "Gabor" == Gabor Grothendieck <ggrothendieck at myway.com>
>>>>>     on Mon, 7 Feb 2005 04:39:40 +0000 (UTC) writes:

     Adrian Baddeley <adrian <at> maths.uwa.edu.au> writes:
     : I want to equip a data frame with an attribute 
     : which specifies how to plot some of the columns. 
     : 
     : Up to now we have been doing this by giving the data frame 
     : a `formula' attribute, that can be passed to plot.formula.
     : 
     : For example
     :       dat <- data.frame(x=1:100,y=runif(100),z=100:1)
     :       attr(dat, "plotme") <- (z ~ x)
     :       ......
     :       ......
     :       if(missing(desiredformula))
     : 	desiredformula <- attr(dat, "plotme")
     :       plot(desiredformula, data=dat)
     : 
     : We just got bitten by the fact that a formula object has a `.Environment'
     : attribute, which may be huge, depending on the environment
     : in which the formula was created. In the example above there is
     : no upper limit on the size of the object 'dat' !!!!
     : That is, environment(attr(dat, "plotme")) could be huge.

    Gabor> Do you mean that if fo is the formula then ls(environment(fo))
    Gabor> has many large components?   I don't understand why that would
    Gabor> be a problem.

maybe Adrian save()s that data.frame susequently?
Then, I assume the environment will copied.
In all(?) other circumstances that should only be a pointer and
not really use much memory.

Martin



From murdoch at stats.uwo.ca  Mon Feb  7 09:38:41 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 07 Feb 2005 08:38:41 +0000
Subject: [R] Environment of a formula
In-Reply-To: <loom.20050207T053403-878@post.gmane.org>
References: <16902.59131.805654.740605@maths.uwa.edu.au>
	<loom.20050207T053403-878@post.gmane.org>
Message-ID: <2k9e01h7m26rt8qh7316lrjjtmb6e00v0t@4ax.com>

On Mon, 7 Feb 2005 04:39:40 +0000 (UTC), Gabor Grothendieck
<ggrothendieck at myway.com> wrote :

>Adrian Baddeley <adrian <at> maths.uwa.edu.au> writes:
>
>: I want to equip a data frame with an attribute 
>: which specifies how to plot some of the columns. 
>: 
>: Up to now we have been doing this by giving the data frame 
>: a `formula' attribute, that can be passed to plot.formula.
>: 
>: For example
>:       dat <- data.frame(x=1:100,y=runif(100),z=100:1)
>:       attr(dat, "plotme") <- (z ~ x)
>:       ......
>:       ......
>:       if(missing(desiredformula))
>: 	desiredformula <- attr(dat, "plotme")
>:       plot(desiredformula, data=dat)
>: 
>: We just got bitten by the fact that a formula object has a `.Environment'
>: attribute, which may be huge, depending on the environment
>: in which the formula was created. In the example above there is
>: no upper limit on the size of the object 'dat' !!!!
>: That is, environment(attr(dat, "plotme")) could be huge.
>
>Do you mean that if fo is the formula then ls(environment(fo))
>has many large components?   I don't understand why that would
>be a problem.

It can be a problem when you save a workspace.  For example, the
.Rdata file produced by this session is tiny:

> makef <- function() {
+     y ~ x
+ }
> fo <- makef()
> save.image()

while the file produced by this session is around 8 megabytes:

> makef <- function() {
+     z <- rnorm(1000000)
+     y ~ x
+ }
> fo <- makef()
> save.image()

You might assume that since z is local to the makef() call, and never
referenced by the formula, it wouldn't be saved:  but you'd be wrong.

>: It appears that we can't set the environment to NULL;
>: should we set it to the Global environment e.g. using as.formula?
>
>It works for me (R 2.1.0 Windows):
>
>R> fo <- y~x
>R> environment(fo)
><environment: R_GlobalEnv>
>R> environment(fo) <- NULL
>R> environment(fo)
>NULL

It works for me too, so I'm not sure what problem Adrian was having.  

Duncan Murdoch



From dle at aber.ac.uk  Mon Feb  7 09:41:47 2005
From: dle at aber.ac.uk (David Enot)
Date: Mon, 7 Feb 2005 08:41:47 +0000
Subject: [R] Rmatlab
Message-ID: <447de42c8ea280753d6e744ed0b1ef68@aber.ac.uk>


  Dear all,

  I came across recently to the Rmatlab package 
(http://www.omegahat.org/RMatlab/) that allows R and Matlab to "talk" 
to each other. I made several attempts to install it on my Mac but 
without any success as some headers seem to miss. I am just wondering 
if anyone has already tried and successfully used RMatlab. According to 
the website, this should run on unix machines and I was surprised that 
it couldn't be true for Mac as the OS X, Linux and Unix Matlab 
architectures are pretty much similar. Any inputs would be great!

   Thanks in advance for your help.

   David


PS: I am currently using Matlab v 13 and R2.0.1



From nathaliebouez at yahoo.fr  Mon Feb  7 10:51:12 2005
From: nathaliebouez at yahoo.fr (nathalie bouez)
Date: Mon, 7 Feb 2005 10:51:12 +0100 (CET)
Subject: [R] questions sur R
Message-ID: <20050207095112.76601.qmail@web60906.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050207/3aeee6cb/attachment.pl

From jeff.pr2 at added-insight.net  Mon Feb  7 10:44:01 2005
From: jeff.pr2 at added-insight.net (jeff.pr2@added-insight.net)
Date: Mon,  7 Feb 2005 10:44:01 +0100
Subject: [R] logit link + alternatives
Message-ID: <6575933$1107769292420737cc201892.59611107@config19.schlund.de>


Help needed with lm function:

Dear R's,
Could anyone tell me how to replace the link function (probit logit,
loglog etc.) in lm
with an abitrary user-defined function? The task is to perform ML
Estimation of betas
for a dichotome target variable.

Maybe there is already a package for this (I did not find one).
Any hints or a code excerpt would be welcome!
Thank you -Jeff

jeff.pr2 (at) added-insight (dot) net



From ligges at statistik.uni-dortmund.de  Mon Feb  7 11:20:15 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 07 Feb 2005 11:20:15 +0100
Subject: [R] questions sur R
In-Reply-To: <20050207095112.76601.qmail@web60906.mail.yahoo.com>
References: <20050207095112.76601.qmail@web60906.mail.yahoo.com>
Message-ID: <420740DF.4080507@statistik.uni-dortmund.de>

nathalie bouez wrote:

> bonjour,
>  
> Je suis actuellement en derniere annee d'ecole d'ingenieur en informatique et statistiques et je dois r?aliser mon projet de fin d'?tudes sur le logiciel R.
> En fait, je dois r?aliser un scoring sous R puis le meme sous SAS et comparer les resultats. Mon fichier se prete ? une regression logistique. J'ai donc utilis? la fonction glm sous R et Catmod sous SAS seumement, je ne parviens pas au meme resultats, les estimateurs different d'une methode a l'autre. Avez vous une idee d'ou peut provenir cette difference?
> Je vous remercie d'avance


1. This list is in english...
2. R is not SAS.
3. Quite probably you are applying different methods in SAS and in R, 
please read the corresponding documentation.
4. You have to specify a reproducible example, if you want some more 
precise answers. Just saying glm() is different from some (not fruther 
mentioned) procedure in SAS isn't that helpful ...

Please read the posting guide (see last line of this message).

Uwe Ligges








> Cordialement,
> 
> 
> 		
> ---------------------------------
> 
> mails !
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From dimitris.rizopoulos at med.kuleuven.ac.be  Mon Feb  7 11:27:38 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Mon, 7 Feb 2005 11:27:38 +0100
Subject: [R] logit link + alternatives
References: <6575933$1107769292420737cc201892.59611107@config19.schlund.de>
Message-ID: <017c01c50cff$a57e7890$0540210a@www.domain>

see at ?glm and ?family

and use, e.g., `family=binomial(link="probit")'.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: <jeff.pr2 at added-insight.net>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, February 07, 2005 10:44 AM
Subject: [R] logit link + alternatives


>
> Help needed with lm function:
>
> Dear R's,
> Could anyone tell me how to replace the link function (probit logit,
> loglog etc.) in lm
> with an abitrary user-defined function? The task is to perform ML
> Estimation of betas
> for a dichotome target variable.
>
> Maybe there is already a package for this (I did not find one).
> Any hints or a code excerpt would be welcome!
> Thank you -Jeff
>
> jeff.pr2 (at) added-insight (dot) net
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From plummer at iarc.fr  Mon Feb  7 11:59:27 2005
From: plummer at iarc.fr (Martyn Plummer)
Date: Mon, 07 Feb 2005 11:59:27 +0100
Subject: [R] logit link + alternatives
In-Reply-To: <017c01c50cff$a57e7890$0540210a@www.domain>
References: <6575933$1107769292420737cc201892.59611107@config19.schlund.de>
	<017c01c50cff$a57e7890$0540210a@www.domain>
Message-ID: <1107773967.3421.21.camel@seurat>

I am not sure that fully answers Jeff's question.  If the available link
functions (even with the quasi family) are not sufficient for your
needs, then you need to make your own constructor for a "family" object
- which provides the necessary information to the glm engine - and use
this as the family argument in your call to glm.  Specifically, you need
to make a copy of the binomial() function and replace the call to
make.link().

Martyn

On Mon, 2005-02-07 at 11:27 +0100, Dimitris Rizopoulos wrote:
> see at ?glm and ?family
> 
> and use, e.g., `family=binomial(link="probit")'.
> 
> Best,
> Dimitris

> ----- Original Message ----- 
> From: <jeff.pr2 at added-insight.net>
> To: <r-help at stat.math.ethz.ch>
> Sent: Monday, February 07, 2005 10:44 AM
> Subject: [R] logit link + alternatives
> 
> 
> >
> > Help needed with lm function:
> >
> > Dear R's,
> > Could anyone tell me how to replace the link function (probit logit,
> > loglog etc.) in lm
> > with an abitrary user-defined function? The task is to perform ML
> > Estimation of betas
> > for a dichotome target variable.
> >
> > Maybe there is already a package for this (I did not find one).
> > Any hints or a code excerpt would be welcome!
> > Thank you -Jeff
> >
> > jeff.pr2 (at) added-insight (dot) net
> >



From pierre.bady at univ-lyon1.fr  Mon Feb  7 12:31:46 2005
From: pierre.bady at univ-lyon1.fr (Pierre BADY)
Date: Mon, 07 Feb 2005 12:31:46 +0100
Subject: [R] questions sur R
In-Reply-To: <20050207095112.76601.qmail@web60906.mail.yahoo.com>
Message-ID: <5.1.0.14.2.20050207123113.00ba7400@pop.univ-lyon1.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050207/ad1cc025/attachment.pl

From gb at tal.stat.umu.se  Mon Feb  7 12:32:13 2005
From: gb at tal.stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Mon, 7 Feb 2005 12:32:13 +0100
Subject: [R] Sweave and connections
Message-ID: <20050207113213.GA15742@tal.stat.umu.se>

I'm learning to use Sweave (it's really great!) and everything works fine,
except that occasionally I get, after having sweaved many times, 
----------------------------------------------------------
 > Sweave("report.Snw")
Writing to file report.tex
Processing code chunks ...
 1 : term hide

Error:  chunk 1 
Error in file(file, encoding = encoding) : 
        All connections are in use
----------------------------------------------------------
This is no big problem, since it helps to close and reopen the R session,
but I wonder if I can avoid this, by "closing connections" or something
similar. (I really have no idea of what is going on!)

This is

R : Version 2.1.0 Under development (unstable) (2005-01-19)

on Debian.

-- 
 G?ran Brostr?m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume?, Sweden             e-mail: gb at stat.umu.se



From adrian at maths.uwa.edu.au  Mon Feb  7 12:46:09 2005
From: adrian at maths.uwa.edu.au (Adrian Baddeley)
Date: Mon, 7 Feb 2005 19:46:09 +0800
Subject: [R] Environment of a formula
In-Reply-To: <200502071119.j17B6q1Q008113@hypatia.math.ethz.ch>
References: <200502071119.j17B6q1Q008113@hypatia.math.ethz.ch>
Message-ID: <16903.21761.287768.994761@maths.uwa.edu.au>


> From: Martin Maechler <maechler at stat.math.ethz.ch>

> maybe Adrian save()s that data.frame susequently?
> Then, I assume the environment will copied.

> In all(?) other circumstances that should only be a pointer and
> not really use much memory.

Yes, sorry for the garbled message,
it is only when the object containing the formula is save()d
that we have a problem with huge file size. 

A



From Friedrich.Leisch at tuwien.ac.at  Mon Feb  7 13:04:37 2005
From: Friedrich.Leisch at tuwien.ac.at (Friedrich.Leisch@tuwien.ac.at)
Date: Mon, 7 Feb 2005 13:04:37 +0100
Subject: [R] Sweave and connections
In-Reply-To: <20050207113213.GA15742@tal.stat.umu.se>
References: <20050207113213.GA15742@tal.stat.umu.se>
Message-ID: <16903.22869.148638.597270@galadriel.ci.tuwien.ac.at>

>>>>> On Mon, 7 Feb 2005 12:32:13 +0100,
>>>>> G?ran Brostr?m (GB) wrote:

  > I'm learning to use Sweave (it's really great!) and everything works fine,
  > except that occasionally I get, after having sweaved many times, 
  > ----------------------------------------------------------
  >> Sweave("report.Snw")
  > Writing to file report.tex
  > Processing code chunks ...
  >  1 : term hide

  > Error:  chunk 1 
  > Error in file(file, encoding = encoding) : 
  >         All connections are in use
  > ----------------------------------------------------------
  > This is no big problem, since it helps to close and reopen the R session,
  > but I wonder if I can avoid this, by "closing connections" or something
  > similar. (I really have no idea of what is going on!)

This should happen only if you have errors in the Sweave document:
then Sweave may have no chance to close the output file properly.

showConnections() lists all open connections, and you may guess what
closeAllConnections() does :-)

HTH,
Fritz

-- 
-------------------------------------------------------------------
                        Friedrich Leisch 
Institut f?r Statistik                     Tel: (+43 1) 58801 10715
Technische Universit?t Wien                Fax: (+43 1) 58801 10798
Wiedner Hauptstra?e 8-10/1071
A-1040 Wien, Austria             http://www.ci.tuwien.ac.at/~leisch



From ggrothendieck at myway.com  Mon Feb  7 14:01:03 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 7 Feb 2005 13:01:03 +0000 (UTC)
Subject: [R] proportional matrix rows
References: <e1873c31b3836a68eb0e9532b87d6733@soc.soton.ac.uk>
Message-ID: <loom.20050207T135808-319@post.gmane.org>

Robin Hankin <r.hankin <at> soc.soton.ac.uk> writes:

: 
: Hi
: 
: I have a two-column integer matrix like this:
: 
: R> jj
: 
:        [,1] [,2]
:   [1,]   -1    1
:   [2,]   -2    2
:   [3,]   -7    6
:   [4,]   -8    7
:   [5,]   -6    5
:   [6,]   -9    8
:   [7,]   -5    4
:   [8,]    3   -3
:   [9,]  -10    9
: [10,]   -4    3
: 
: I want a diagnostic that detects whether a row is a multiple of
: the first row or not.  In this case, this would be rows 1,2, and 8.
: 
: How to do this nicely?  Sometimes the first row has a zero, so the
: method would have to work on
: 
:       [,1] [,2]
: [1,]    0    1
: [2,]    1    1
: [3,]    0    8
: [4,]    0   -4
: [5,]    0    0
: [6,]    4    0
:  >
: 
: in which case rows 1,3,4,5 are multiples.  It'd be nice to have
: a solution that works for any number of columns.


If rowi is a multiple of row1 then 
crossprod(cbind(row1, rowi)) is singular so:

apply(mat, 1, function(x) det(crossprod(cbind(x, mat[1,])))) == 0

If your matrix only has two columns then crossprod(x) is singular
iff x is so you can eliminate the crossprod.



From Ted.Harding at nessie.mcc.ac.uk  Mon Feb  7 14:01:15 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 07 Feb 2005 13:01:15 -0000 (GMT)
Subject: [R] logit link + alternatives
In-Reply-To: <6575933$1107769292420737cc201892.59611107@config19.schlund.de>
Message-ID: <XFMail.050207125707.Ted.Harding@nessie.mcc.ac.uk>

On 07-Feb-05 jeff.pr2 at added-insight.net wrote:
> 
> Help needed with lm function:
> 
> Dear R's,
> Could anyone tell me how to replace the link function (probit logit,
> loglog etc.) in lm
> with an abitrary user-defined function? The task is to perform ML
> Estimation of betas
> for a dichotome target variable.
> 
> Maybe there is already a package for this (I did not find one).
> Any hints or a code excerpt would be welcome!
> Thank you -Jeff

I asked a similar question last year (2 April 2004) since I wanted
a "cauchy" link in a binary response model (the data suggested
heavy tails). I thought in the first place that I saw a fairly
straightforward way to do it, but Brian Ripley's informed response
put me off, once I had looked into the details of what would be
involved (his reply which includes my original mail follows):

# On Fri, 2 Apr 2004 Ted.Harding at nessie.mcc.ac.uk wrote:
# 
# > I am interested in extending the repertoire of link functions
# > in glm(Y~X, family=binomial(link=...)) to include a "tan" link:
# > 
# >    eta = (4/pi)*tan(mu)
# > 
# > i.e. this link bears the same relation to the Cauchy distribution
# > as the probit link bears to the Gaussian. I'm interested in sage
# > advice about this from people who know their way aroung glm.
# > 
# > From the surface, it looks as though it might just be a matter
# > of re-writing 'make.link' in the obvious sort of way so as to
# > incorporate "tan", but I fear traps ...
# 
# How are you going to do that?  If you edit make.link and have your
# own local copy, the namespace scoping will ensure that the system
# copy gets used, and the code in binomial() will ensure that even
# that does not get  called except for the pre-coded list of links.
# 
# > What am I missing?
# 
# You need a local, modified, copy of binomial, too, AFAICS.

As I say, the implied details put me off for a while, but in
this particular case Thomas W Yee came up with a ready-made
solution (23 April 2004):

# my VGAM package at www.stat.auckland.ac.nz/~yee
# now has the tan link for binomialff().
# It is tan(pi*(mu-0.5)).

(See his full mail in the R-help archives for April 2004
for several important details regarding this implementation).

So: it would seem to be quite possible to write yor own link
function, but it would take quite a bit of work and would
involves re-writing at least the codes for 'make.link'
and for 'binomial', and being careful about how you use them.

Hoping this helps,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 07-Feb-05                                       Time: 12:57:07
------------------------------ XFMail ------------------------------



From wagner at itp.phys.ethz.ch  Mon Feb  7 14:31:06 2005
From: wagner at itp.phys.ethz.ch (Urs Wagner)
Date: Mon, 07 Feb 2005 13:31:06 +0000
Subject: [R] DLL hangs
Message-ID: <42076D9A.1030608@itp.phys.ethz.ch>

Hello

I built a dll with cggwin. When  I want to load it with RGUI the RGUI 
hangs. How
can I troubleshoot it?

Thanks

Urs



From rkoenker at uiuc.edu  Mon Feb  7 14:40:35 2005
From: rkoenker at uiuc.edu (roger koenker)
Date: Mon, 7 Feb 2005 07:40:35 -0600
Subject: [R] logit link + alternatives
In-Reply-To: <XFMail.050207125707.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050207125707.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <f6351eb93edbef01827ee0e689957e81@uiuc.edu>

Just for the record --  NEWS for 2.1.0 includes:

o	binomial() has a new "cauchit" link (suggested by Roger Koenker).

the MASS polr for ordered response is also now adapted for the Cauchit 
case.


url:	www.econ.uiuc.edu/~roger        	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820

On Feb 7, 2005, at 7:01 AM, (Ted Harding) wrote:

> On 07-Feb-05 jeff.pr2 at added-insight.net wrote:
>>
>> Help needed with lm function:
>>
>> Dear R's,
>> Could anyone tell me how to replace the link function (probit logit,
>> loglog etc.) in lm
>> with an abitrary user-defined function? The task is to perform ML
>> Estimation of betas
>> for a dichotome target variable.
>>
>> Maybe there is already a package for this (I did not find one).
>> Any hints or a code excerpt would be welcome!
>> Thank you -Jeff
>
> I asked a similar question last year (2 April 2004) since I wanted
> a "cauchy" link in a binary response model (the data suggested
> heavy tails). I thought in the first place that I saw a fairly
> straightforward way to do it, but Brian Ripley's informed response
> put me off, once I had looked into the details of what would be
> involved (his reply which includes my original mail follows):
>
> # On Fri, 2 Apr 2004 Ted.Harding at nessie.mcc.ac.uk wrote:
> #
> # > I am interested in extending the repertoire of link functions
> # > in glm(Y~X, family=binomial(link=...)) to include a "tan" link:
> # >
> # >    eta = (4/pi)*tan(mu)
> # >
> # > i.e. this link bears the same relation to the Cauchy distribution
> # > as the probit link bears to the Gaussian. I'm interested in sage
> # > advice about this from people who know their way aroung glm.
> # >
> # > From the surface, it looks as though it might just be a matter
> # > of re-writing 'make.link' in the obvious sort of way so as to
> # > incorporate "tan", but I fear traps ...
> #
> # How are you going to do that?  If you edit make.link and have your
> # own local copy, the namespace scoping will ensure that the system
> # copy gets used, and the code in binomial() will ensure that even
> # that does not get  called except for the pre-coded list of links.
> #
> # > What am I missing?
> #
> # You need a local, modified, copy of binomial, too, AFAICS.
>
> As I say, the implied details put me off for a while, but in
> this particular case Thomas W Yee came up with a ready-made
> solution (23 April 2004):
>
> # my VGAM package at www.stat.auckland.ac.nz/~yee
> # now has the tan link for binomialff().
> # It is tan(pi*(mu-0.5)).
>
> (See his full mail in the R-help archives for April 2004
> for several important details regarding this implementation).
>
> So: it would seem to be quite possible to write yor own link
> function, but it would take quite a bit of work and would
> involves re-writing at least the codes for 'make.link'
> and for 'binomial', and being careful about how you use them.
>
> Hoping this helps,
> Ted.
>
>
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 07-Feb-05                                       Time: 12:57:07
> ------------------------------ XFMail ------------------------------
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Mon Feb  7 14:43:45 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 07 Feb 2005 14:43:45 +0100
Subject: [R] DLL hangs
In-Reply-To: <42076D9A.1030608@itp.phys.ethz.ch>
References: <42076D9A.1030608@itp.phys.ethz.ch>
Message-ID: <42077091.6080006@statistik.uni-dortmund.de>

Urs Wagner wrote:

> Hello
> 
> I built a dll with cggwin.
 > When  I want to load it with RGUI the RGUI
> hangs. How can I troubleshoot it?

Not using cygwin, which is unsupported.
Please read the R for Windows FAQ, in particular Chapter 7 "Building 
from Source" and follow the advices given therein - and given in therein 
cited documentation such as README.packages.

Uwe Ligges


> 
> Thanks
> 
> Urs
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From katchmalik at gmail.com  Mon Feb  7 14:44:57 2005
From: katchmalik at gmail.com (Shlomo Katchmalik)
Date: Mon, 7 Feb 2005 08:44:57 -0500
Subject: [R] RODBC working in Rgui but not Rterm
In-Reply-To: <Pine.LNX.4.61.0502070740500.32227@gannet.stats>
References: <20050207054140.VNXL1836.tomts16-srv.bellnexxia.net@JohnDesktop8300>
	<Pine.LNX.4.61.0502070740500.32227@gannet.stats>
Message-ID: <38a7b0c805020705442e4d24ac@mail.gmail.com>

Thank you all very much for your help!
Shlomo.

> There are two problems here
> 
> 1) The latest MyODBC (3.51-10) has a problem that 3.51-07 did not have, as
> I found yesterday on both Linux and Windows (I am preparing an RODBC
> update).
> 
> I've now read the sources, and it has a logic bug: it throws the error
> 
>     state IM008 Invalid window handle for connection completion argument
> 
> whether or not any completion is needed.  RODBC 1.1-3 will have a
> workaround for this.
> 
> 2) There _is_ a documented difference between RGui and Rterm: see
> ?odbcConnect:
> 
>       Under the Windows GUI, specifying an incomplete 'connection', for
>       example the default '""', will bring up a dialog box to complete
>       the information required.  (This does not work from 'Rterm.exe'
>       unless a driver is specified, a Windows restriction.)
> 
> As a result of these two, MyODBC 3.51-10 ought to work under RGui and not
> under Rterm.
> 
> Moral: if you can, downgrade your MyODBC driver, or wait for RODBC_1.1-3
> 
> Brian
> 
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tghoward at gw.dec.state.ny.us  Mon Feb  7 14:47:48 2005
From: tghoward at gw.dec.state.ny.us (Tim Howard)
Date: Mon, 07 Feb 2005 08:47:48 -0500
Subject: [R] subset data.frame with value != in all columns
Message-ID: <s2072b4e.021@gwsmtp.DEC.STATE.NY.US>

Petr,
   Thank you!  Yes, rowSums appears to be even a little bit faster than
unique(which()), and it also maintains the original order. I do want
original order maintained, but I first apply a function to one of my
data.frames (that without any -9999s ... yes, these do represent nulls,
as someone asked earlier) and rbind these two dataframes back together,
so I need to sort (by rownames) after the rbind (there doesn't seem to
be a sortby option in rbind). 
   I apologize for not jumping on rowSums earlier, I hadn't caught on
that it was summing counts of occurrence of the search value, not
summing the search value itself.
   Thanks again, this is very instructive and *very* helpful.
humbly,
Tim

>>> "Petr Pikal" <petr.pikal at precheza.cz> 02/07/05 02:12AM >>>
Hi Tim

I can not say much about apply, but the code with unique(which()) 
gives you reordered rows in case of -9999 selection

try

set.seed(1)
in.df <- data.frame(
c1=rnorm(40000),
c2=rnorm(40000),
c3=rnorm(40000),
c4=rnorm(40000),
c5=rnorm(40000))
in.df[in.df>3] <- (-9999)

system.time(e <- in.df[unique(which(in.df == -9999, arr.ind = 
TRUE)[,1]), ])
system.time(e1 <- in.df[(rowSums(in.df == -9999)) != 0,])

all.equal(e,e1)

So if you mind you need to do reordering.

ooo<-order(as.numeric(rownames(e)))
all.equal(e[ooo,],e1)

Cheers
Petr

On 4 Feb 2005 at 11:17, Tim Howard wrote:

> Because I'll be doing this on big datasets and time is important, I
> thought I'd time all the different approaches that were suggested on
a
> small dataframe. The results were very instructive so I thought I'd
> pass them on. I also discovered that my numeric columns (e.g.
> -9999.000) weren't found by apply() but were found by which() and
the
> simple replace. Was it apply's fault or something else?
> 
> Note how much faster unique(which()) is; wow! Thanks to Marc
Schwartz
> for this blazing solution.
> 
> > nrow(in.df)
> [1] 40000
> #extract rows with no -9999
> > system.time(x <- subset(in.df, apply(in.df, 1,
> function(in.df){all(in.df != -9999)})))
> [1] 3.25 0.00 3.25   NA   NA
> > system.time(y<- in.df[-unique(which(in.df == -9999, arr.ind =
> > TRUE)[,
> 1]), ])
> [1] 0.17 0.00 0.17   NA   NA
> > system.time({is.na(in.df) <-in.df == -9999; z <- na.omit(in.df)})
> [1] 0.25 0.02 0.26   NA   NA
> 
> > nrow(x);nrow(y);nrow(z)
> [1] 39990
> [1] 39626
> [1] 39626
> 
> #extract rows with -9999
> > system.time(d<-subset(in.df, apply(in.df, 1,
> function(in.df){any(in.df == -9999)})))
> [1] 3.40 0.00 3.45   NA   NA
> > system.time(e<-in.df[unique(which(in.df == -9999, arr.ind =
TRUE)[,
> 1]), ])
> [1] 0.11 0.00 0.11   NA   NA
> 
> > nrow(d); nrow(e)
> [1] 10
> [1] 374
> 
> Tim Howard
> 
> 
> >>> Marc Schwartz <MSchwartz at MedAnalytics.com> 02/03/05 03:24PM >>>
> On Thu, 2005-02-03 at 14:57 -0500, Tim Howard wrote: 
>   ... snip...
> > My questions: 
> > Is there a cleaner way to extract all rows containing a specified
> > value? How can I extract all rows that don't have this value in
any
> > col?
> > 
> > #create dummy dataset
> > x <- data.frame(
> > c1=c(-99,-99,-99,4:10),
> > c2=1:10,
> > c3=c(1:3,-99,5:10),
> > c4=c(10:1),
> > c5=c(1:9,-99))
> > 
> ..snip...
> 
> How about this, presuming that your data frame is all numeric:
> 
> For rows containing -99:
> 
> > x[unique(which(x == -99, arr.ind = TRUE)[, 1]), ]
>     c1 c2  c3 c4  c5
> 1  -99  1   1 10   1
> 2  -99  2   2  9   2
> 3  -99  3   3  8   3
> 4    4  4 -99  7   4
> 10  10 10  10  1 -99
> 
> 
> For rows not containing -99:
> 
> > x[-unique(which(x == -99, arr.ind = TRUE)[, 1]), ]
>   c1 c2 c3 c4 c5
> 5  5  5  5  6  5
> 6  6  6  6  5  6
> 7  7  7  7  4  7
> 8  8  8  8  3  8
> 9  9  9  9  2  9
> 
> 
> What I have done here is to use which(), setting arr.ind = TRUE.
This
> returns the row, column indices for the matches to the boolean
> statement. The first column returned by which() in this case are the
> row numbers matching the statement, so I take the first column only.
> 
> Since it is possible that more than one element in a row can match
the
> boolean, I then use unique() to get the singular row values.
> 
> Thus, I can use the returned row indices above to subset the data
> frame.
> 
> HTH,
> 
> Marc Schwartz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html 

Petr Pikal
petr.pikal at precheza.cz



From ggrothendieck at myway.com  Mon Feb  7 14:50:08 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 7 Feb 2005 13:50:08 +0000 (UTC)
Subject: [R] logit link + alternatives
References: <XFMail.050207125707.Ted.Harding@nessie.mcc.ac.uk>
	<f6351eb93edbef01827ee0e689957e81@uiuc.edu>
Message-ID: <loom.20050207T144834-582@post.gmane.org>

roger koenker <rkoenker <at> uiuc.edu> writes:

: 
: Just for the record --  NEWS for 2.1.0 includes:
: 
: o	binomial() has a new "cauchit" link (suggested by Roger Koenker).
: 
: the MASS polr for ordered response is also now adapted for the Cauchit 
: case.

Do any of the data sets that come with R, MASS, etc. provide an example
where this model is appropriate?  If not, is there another publicly available
data set for it?

Thanks.



From andy_liaw at merck.com  Mon Feb  7 14:53:40 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 7 Feb 2005 08:53:40 -0500
Subject: [R] proportional matrix rows
Message-ID: <3A822319EB35174CA3714066D590DCD50994E666@usrymx25.merck.com>

This works for your first example:

f <- function(m) {
    k <- t(t(m) %/% m[1,])
    which(rowSums(k - k[, 1]) == 0)
}

> f(jj)
[1] 1 2 8

but not the second one.  The 0s are problematic...

Andy


> From: Robin Hankin
> 
> Hi
> 
> I have a two-column integer matrix like this:
> 
> 
> R> jj
> 
>        [,1] [,2]
>   [1,]   -1    1
>   [2,]   -2    2
>   [3,]   -7    6
>   [4,]   -8    7
>   [5,]   -6    5
>   [6,]   -9    8
>   [7,]   -5    4
>   [8,]    3   -3
>   [9,]  -10    9
> [10,]   -4    3
> 
> I want a diagnostic that detects whether a row is a multiple of
> the first row or not.  In this case, this would be rows 1,2, and 8.
> 
> How to do this nicely?  Sometimes the first row has a zero, so the
> method would have to work on
> 
>       [,1] [,2]
> [1,]    0    1
> [2,]    1    1
> [3,]    0    8
> [4,]    0   -4
> [5,]    0    0
> [6,]    4    0
>  >
> 
> in which case rows 1,3,4,5 are multiples.  It'd be nice to have
> a solution that works for any number of columns.
> 
> --
> Robin Hankin
> Uncertainty Analyst
> Southampton Oceanography Centre
> European Way, Southampton SO14 3ZH, UK
>   tel  023-8059-7743
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From fbilly at adi-gestion.com  Mon Feb  7 15:31:57 2005
From: fbilly at adi-gestion.com (fbilly@adi-gestion.com)
Date: Mon, 7 Feb 2005 15:31:57 +0100
Subject: [R] INFO : just one question
Message-ID: <OF2335D52D.13DD9240-ONC1256FA1.004F5E97-C1256FA1.004F82C0@adi-gestion.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050207/7ae0a179/attachment.pl

From murdoch at stats.uwo.ca  Mon Feb  7 15:37:34 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 07 Feb 2005 14:37:34 +0000
Subject: [R] DLL hangs
In-Reply-To: <42077091.6080006@statistik.uni-dortmund.de>
References: <42076D9A.1030608@itp.phys.ethz.ch>
	<42077091.6080006@statistik.uni-dortmund.de>
Message-ID: <mvue01pd063v7kggj6tmi7gnclqcf5iktm@4ax.com>

On Mon, 07 Feb 2005 14:43:45 +0100, Uwe Ligges
<ligges at statistik.uni-dortmund.de> wrote :

>Urs Wagner wrote:
>
>> Hello
>> 
>> I built a dll with cggwin.
> > When  I want to load it with RGUI the RGUI
>> hangs. How can I troubleshoot it?
>
>Not using cygwin, which is unsupported.
>Please read the R for Windows FAQ, in particular Chapter 7 "Building 
>from Source" and follow the advices given therein - and given in therein 
>cited documentation such as README.packages.

Urs:

That's probably the easiest approach.  

However, if there really is some reason why Cygwin is necessary for
you, then in principle what you are trying should be possible.  It is
definitely possible to write DLLs in compilers other than MinGW, such
as MSVC++, Delphi, various Fortrans, etc.

You can read some general advice for using foreign compilers on my web
page

 http://www.stats.uwo.ca/faculty/murdoch/software/compilingDLLs/

If you can work out what is going wrong with Cygwin, I'd be happy to
add a section of your findings.

Duncan Murdoch



From wagner at itp.phys.ethz.ch  Mon Feb  7 15:44:53 2005
From: wagner at itp.phys.ethz.ch (Urs Wagner)
Date: Mon, 07 Feb 2005 14:44:53 +0000
Subject: [R] DLL hangs
In-Reply-To: <42077091.6080006@statistik.uni-dortmund.de>
References: <42076D9A.1030608@itp.phys.ethz.ch>
	<42077091.6080006@statistik.uni-dortmund.de>
Message-ID: <42077EE5.1070604@itp.phys.ethz.ch>

I would like to use pipes. The mkfifo call is not in the mingw 
distribution that I am using.

Urs

Uwe Ligges wrote:

> Urs Wagner wrote:
>
>> Hello
>>
>> I built a dll with cggwin.
>
> > When  I want to load it with RGUI the RGUI
>
>> hangs. How can I troubleshoot it?
>
>
> Not using cygwin, which is unsupported.
> Please read the R for Windows FAQ, in particular Chapter 7 "Building 
> from Source" and follow the advices given therein - and given in 
> therein cited documentation such as README.packages.
>
> Uwe Ligges
>
>
>>
>> Thanks
>>
>> Urs
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
>



From Gregor.Gorjanc at bfro.uni-lj.si  Mon Feb  7 15:52:42 2005
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Mon, 7 Feb 2005 15:52:42 +0100
Subject: [R] Programming/scripting  with "expressions - variables"
Message-ID: <7FFEE688B57D7346BC6241C55900E730B6FEBF@pollux.bfro.uni-lj.si>

Hello to Rusers!

I am puzzled with R and I really do not know where to look
in for my problem. I am moving from SAS and I have difficulties
in translating SAS to R world. I hope I will get some hints 
or pointers so I can study from there on.

I would like to do something like this. In SAS I can write 
a macro as example bellow, which is afcourse a silly one but 
shows what I don't know how to do in R.

%macro test(data, colname, colvalue);
    data &data;
        ...
        &colname="&colvalue";
        other_&colname="other_&colvalue";
    run;
%mend;

And if I run it with this call:
%test(Gregor, Gorjanc, 25);

I get a table with name 'Gregor' and columns 'Gorjanc', 
and 'other_Gorjanc' with values:

Gorjanc other_Gorjanc
"25"    "other_25"

So can one show me the way to do the same thing in R? 

Thanks!

--
Lep pozdrav / With regards,
    Gregor GORJANC

---------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty       URI: http://www.bfro.uni-lj.si
Zootechnical Department    email: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                  tel: +386 (0)1 72 17 861
SI-1230 Domzale            fax: +386 (0)1 72 17 888
Slovenia



From macq at llnl.gov  Mon Feb  7 16:05:14 2005
From: macq at llnl.gov (Don MacQueen)
Date: Mon, 7 Feb 2005 07:05:14 -0800
Subject: [R] Problem installing Hmisc
In-Reply-To: <Pine.A41.4.61b.0502051025060.206006@homer03.u.washington.edu>
References: <073c38f94fb534d6d29a4e3ab9117152@virginia.edu>
	<Pine.A41.4.61b.0502051025060.206006@homer03.u.washington.edu>
Message-ID: <p06110401be2d3243bf1b@[128.115.153.6]>

At 10:28 AM -0800 2/5/05, Thomas Lumley wrote:
>On Sat, 5 Feb 2005, Michael Kubovy wrote:
>
>>What am I doing wrong?
>
>You can look at the console log (eg start Console.app in 
>/Applications/Utilities) to see what the problem is, but the fact 
>that Hmisc is not available as a binary package probably means that 
>it does not compile for the Mac (at least without some modification)
>
>	-thomas

I have Hmisc installed on a Mac without any modification, i.e., using 
install.packages().
R itself was installed from source code.

>  version
          _                       
platform powerpc-apple-darwin6.8.5
arch     powerpc                 
os       darwin6.8.5             
system   powerpc, darwin6.8.5    
status                           
major    2                       
minor    0.1                     
year     2004                    
month    11                      
day      15                      
language R                       
>
>  library(Hmisc)
Hmisc library by Frank E Harrell Jr

Type library(help='Hmisc'), ?Overview, or ?Hmisc.Overview')
to see overall documentation.

NOTE:Hmisc no longer redefines [.factor to drop unused levels when
subsetting.  To get the old behavior of Hmisc type dropUnusedLevels().

Attaching package 'Hmisc':


         The following object(s) are masked from package:stats :

          ecdf

>


<--- snip --->

>
>Thomas Lumley			Assoc. Professor, Biostatistics
>tlumley at u.washington.edu	University of Washington, Seattle
>



-Don
-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From james.holtman at convergys.com  Mon Feb  7 16:48:33 2005
From: james.holtman at convergys.com (james.holtman@convergys.com)
Date: Mon, 7 Feb 2005 10:48:33 -0500
Subject: [R] Programming/scripting  with "expressions - variables"
Message-ID: <OF5DAE4AE6.7F85DE86-ON85256FA1.0056B331@nd.convergys.com>





Here is one way.  It is the custom to return a value that will be assigned
to the variable, so I changed your 'macro' to a function that returns the
value and then assigns it to your variable:

> test <- function(name, value){
+     .result <- NULL # initialize to NULL
+     .result[name] <- value
+     .result[paste('other_', name, sep='')] <- paste("other_", value,
sep='')
+     .result
+ }
> Gregor <- test('Gorjanc', '25')
> Gregor    # print out the vector
      Gorjanc other_Gorjanc
         "25"    "other_25"
>
__________________________________________________________
James Holtman        "What is the problem you are trying to solve?"
Executive Technical Consultant  --  Office of Technology, Convergys
james.holtman at convergys.com
+1 (513) 723-2929


                                                                                                                                           
                      "Gorjanc Gregor"                                                                                                     
                      <Gregor.Gorjanc at bfro.        To:       <r-help at stat.math.ethz.ch>                                                    
                      uni-lj.si>                   cc:                                                                                     
                      Sent by:                     Subject:  [R] Programming/scripting  with "expressions - variables"                     
                      r-help-bounces at stat.m                                                                                                
                      ath.ethz.ch                                                                                                          
                                                                                                                                           
                                                                                                                                           
                      02/07/2005 09:52                                                                                                     
                                                                                                                                           
                                                                                                                                           




Hello to Rusers!

I am puzzled with R and I really do not know where to look
in for my problem. I am moving from SAS and I have difficulties
in translating SAS to R world. I hope I will get some hints
or pointers so I can study from there on.

I would like to do something like this. In SAS I can write
a macro as example bellow, which is afcourse a silly one but
shows what I don't know how to do in R.

%macro test(data, colname, colvalue);
    data &data;
        ...
        &colname="&colvalue";
        other_&colname="other_&colvalue";
    run;
%mend;

And if I run it with this call:
%test(Gregor, Gorjanc, 25);

I get a table with name 'Gregor' and columns 'Gorjanc',
and 'other_Gorjanc' with values:

Gorjanc other_Gorjanc
"25"    "other_25"

So can one show me the way to do the same thing in R?

Thanks!

--
Lep pozdrav / With regards,
    Gregor GORJANC

---------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty       URI: http://www.bfro.uni-lj.si
Zootechnical Department    email: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                  tel: +386 (0)1 72 17 861
SI-1230 Domzale            fax: +386 (0)1 72 17 888
Slovenia

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From val_lacombe at yahoo.fr  Mon Feb  7 16:56:05 2005
From: val_lacombe at yahoo.fr (lacombe valerie)
Date: Mon, 7 Feb 2005 16:56:05 +0100 (CET)
Subject: [R] R1.5.O
Message-ID: <20050207155605.36799.qmail@web60110.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050207/af2450d1/attachment.pl

From jlh599 at psu.edu  Mon Feb  7 16:58:13 2005
From: jlh599 at psu.edu (Jessica Higgs)
Date: Mon, 07 Feb 2005 10:58:13 -0500
Subject: [R] Creating a correlation Matrix
Message-ID: <5.2.0.9.2.20050207105459.026bea20@email.psu.edu>

Hi all:

I have a question on how to go about creating a correlation matrix.  I have 
a huge amount of data....21 variables for 3471 times. I want to see how 
each of the variables correlate to each other.  Any help would be 
appreciated, including which package and which functions I should use to do 
this.

Thanks,
Jessica Higgs

Masters Student
Department of Meteorology
Penn State University



From murdoch at stats.uwo.ca  Mon Feb  7 17:13:09 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 07 Feb 2005 16:13:09 +0000
Subject: [R] R1.5.O
In-Reply-To: <20050207155605.36799.qmail@web60110.mail.yahoo.com>
References: <20050207155605.36799.qmail@web60110.mail.yahoo.com>
Message-ID: <rj4f01t0gslhbqqg3nbm2o94sg8nhrppr1@4ax.com>

On Mon, 7 Feb 2005 16:56:05 +0100 (CET), lacombe valerie
<val_lacombe at yahoo.fr> wrote :

>How can I download the R1.5.O version for windows with its extend packages Splancs 1.5.0 and Spatial 1.5.0?

You will probably have trouble finding it.  You can download the
source code for it from

 http://cran.r-project.org/src/base/R-1/R-1.5.0.tgz

but I don't know where you would find a Windows binary.

You might try looking around on www.archive.org; they have one that
looks like 1.5.1 at 

http://web.archive.org/web/20020611092104/http://cran.r-project.org/bin/windows/base/SetupR.exe

for instance.

Duncan Murdoch



From Helene.Dryssens at eleves.polytech-lille.fr  Mon Feb  7 17:19:54 2005
From: Helene.Dryssens at eleves.polytech-lille.fr (Helene.Dryssens@eleves.polytech-lille.fr)
Date: Mon,  7 Feb 2005 17:19:54 +0100
Subject: [R] problem with logistic regression
Message-ID: <1107793194.4207952a37db5@webmail.polytech-lille.fr>

Hi,

we try to do a logistic regression with the function glm.
But we notice that this function don't give the same results as the SAS proc
catmod (differents estimate given).
We try to change the contrast on R system with:
> options(contrasts=c(unordered="contr.SAS",ordered="contr.poly"))

We also try with brlr and logistf functions.
Unfortunately, the estimate aren't still the same.

Please could someone help us.

Thank you

--------------------------------------------------
This mail sent through Polytech'Lille WebMail (IMP)



From roebuck at odin.mdacc.tmc.edu  Mon Feb  7 17:23:18 2005
From: roebuck at odin.mdacc.tmc.edu (Paul Roebuck)
Date: Mon, 7 Feb 2005 10:23:18 -0600 (CST)
Subject: [R] Rmatlab
In-Reply-To: <447de42c8ea280753d6e744ed0b1ef68@aber.ac.uk>
References: <447de42c8ea280753d6e744ed0b1ef68@aber.ac.uk>
Message-ID: <Pine.OSF.4.58.0502071018050.404227@odin.mdacc.tmc.edu>

On Mon, 7 Feb 2005, David Enot wrote:

> I came across recently to the Rmatlab package
> (http://www.omegahat.org/RMatlab/) that allows R and Matlab
> to "talk" to each other. I made several attempts to install
> it on my Mac but without any success as some headers seem
> to miss. I am just wondering if anyone has already tried
> and successfully used RMatlab. According to the website,
> this should run on unix machines and I was surprised that
> it couldn't be true for Mac as the OS X, Linux and Unix
> MATLAB architectures are pretty much similar.

Pretend we can't see your screen and include the error messages
in your post. Can we assume you are running 10.3.7 and have
XCode 1.5 loaded as well?

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)



From spencer.graves at pdf.com  Mon Feb  7 17:32:58 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 07 Feb 2005 08:32:58 -0800
Subject: [R] Creating a correlation Matrix
In-Reply-To: <5.2.0.9.2.20050207105459.026bea20@email.psu.edu>
References: <5.2.0.9.2.20050207105459.026bea20@email.psu.edu>
Message-ID: <4207983A.6060506@pdf.com>

      Have you considered "cor"?  The command 
'help.search("correlation")' suggests among other functions 
"var(stats)", the documentation for which also includes "cor". 

      If this is not adequate, "PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html ".  It may help you get more 
useful replies in the future. 
      spencer graves          

Jessica Higgs wrote:

> Hi all:
>
> I have a question on how to go about creating a correlation matrix.  I 
> have a huge amount of data....21 variables for 3471 times. I want to 
> see how each of the variables correlate to each other.  Any help would 
> be appreciated, including which package and which functions I should 
> use to do this.
>
> Thanks,
> Jessica Higgs
>
> Masters Student
> Department of Meteorology
> Penn State University
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From vito_ricci at yahoo.com  Mon Feb  7 17:36:09 2005
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Mon, 7 Feb 2005 17:36:09 +0100 (CET)
Subject: [R] R: Creating a correlation Matrix
Message-ID: <20050207163609.13216.qmail@web41214.mail.yahoo.com>

Hi,

see ?cor in base package to get correlation matrix for
your data. Maybe it could be usefull getting principal
components (give a look to: ? princomp (base)) to
reduce the number of variables.
Hoping I helped you.
Best regards,
Vito



You wrote:

Hi all:

I have a question on how to go about creating a
correlation matrix.  I have 
a huge amount of data....21 variables for 3471 times.
I want to see how 
each of the variables correlate to each other.  Any
help would be 
appreciated, including which package and which
functions I should use to do 
this.

Thanks,
Jessica Higgs

Masters Student
Department of Meteorology
Penn State University

=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box

Top 10 reasons to become a Statistician

     1. Deviation is considered normal
     2. We feel complete and sufficient
     3. We are 'mean' lovers
     4. Statisticians do it discretely and continuously
     5. We are right 95% of the time
     6. We can legally comment on someone's posterior distribution
     7. We may not be normal, but we are transformable
     8. We never have to say we are certain
     9. We are honestly significantly different
    10. No one wants our jobs


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/



From matthew_wiener at merck.com  Mon Feb  7 17:39:01 2005
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Mon, 7 Feb 2005 11:39:01 -0500
Subject: [R] Creating a correlation Matrix
Message-ID: <45AAE6FD142DCB43A38C00A11FF5DF3E04993F2C@uswsmx03.merck.com>

>From the help for cor (from the stats package):

"If x and y are matrices then the covariances (or correlations) between the
columns of x and the columns of y are computed."

So if you make a matrix with each column corresponding to one of your
variables, you can get what you're after.

For future reference, help.search("correlation") turns up the proper page as
the first suggestion.

Hope this helps,

Matt Wiener

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jessica Higgs
Sent: Monday, February 07, 2005 10:58 AM
To: R-help at stat.math.ethz.ch
Subject: [R] Creating a correlation Matrix


Hi all:

I have a question on how to go about creating a correlation matrix.  I have 
a huge amount of data....21 variables for 3471 times. I want to see how 
each of the variables correlate to each other.  Any help would be 
appreciated, including which package and which functions I should use to do 
this.

Thanks,
Jessica Higgs

Masters Student
Department of Meteorology
Penn State University

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Mon Feb  7 17:42:50 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 07 Feb 2005 17:42:50 +0100
Subject: [R] proportional matrix rows
In-Reply-To: <loom.20050207T135808-319@post.gmane.org>
References: <e1873c31b3836a68eb0e9532b87d6733@soc.soton.ac.uk>
	<loom.20050207T135808-319@post.gmane.org>
Message-ID: <x2u0ooxqwl.fsf@biostat.ku.dk>

Gabor Grothendieck <ggrothendieck at myway.com> writes:

> Robin Hankin <r.hankin <at> soc.soton.ac.uk> writes:
> 
> : 
> : Hi
> : 
> : I have a two-column integer matrix like this:
> : 
> : R> jj
> : 
> :        [,1] [,2]
> :   [1,]   -1    1
> :   [2,]   -2    2
> :   [3,]   -7    6
> :   [4,]   -8    7
> :   [5,]   -6    5
> :   [6,]   -9    8
> :   [7,]   -5    4
> :   [8,]    3   -3
> :   [9,]  -10    9
> : [10,]   -4    3
> : 
> : I want a diagnostic that detects whether a row is a multiple of
> : the first row or not.  In this case, this would be rows 1,2, and 8.
> : 
> : How to do this nicely?  Sometimes the first row has a zero, so the
> : method would have to work on
> : 
> :       [,1] [,2]
> : [1,]    0    1
> : [2,]    1    1
> : [3,]    0    8
> : [4,]    0   -4
> : [5,]    0    0
> : [6,]    4    0
> :  >
> : 
> : in which case rows 1,3,4,5 are multiples.  It'd be nice to have
> : a solution that works for any number of columns.
> 
> 
> If rowi is a multiple of row1 then 
> crossprod(cbind(row1, rowi)) is singular so:
> 
> apply(mat, 1, function(x) det(crossprod(cbind(x, mat[1,])))) == 0
> 
> If your matrix only has two columns then crossprod(x) is singular
> iff x is so you can eliminate the crossprod.

..and in that case it might be easier just to calculate the dot
product with the orthogonal of mat[1,]:

> z <- jj[1,]
> jj %*% c(z[1], -z[2])
      [,1]
 [1,]    0
 [2,]    0
 [3,]    1
 [4,]    1
 [5,]    1
 [6,]    1
 [7,]    1
 [8,]    0
 [9,]    1
[10,]    1

(Notice that the value "1" just comes from Robin being singularly bad
at selecting "arbitrary" rows that are not proportional to the first
one!)

This generalizes to more than two columns as

jj %*% (diag(ncol(jj)) - z%*%solve(crossprod(z))%*%t(z)) 

having rows that are identically zero (the multiplier matrix is
obviously rank-deficient so you can leave out one column, but you need
to guard against leaving in a column of zeroes, so it is hardly worth
it).

In all cases you probably need some sort of guard against round-off.
I.e. something like

> rowSums(zapsmall(jj %*% (diag(2) - z%*%solve(crossprod(z),t(z))))^2) == 0
 [1]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE

or

> rowSums((jj %*% (diag(2) - z%*%solve(crossprod(z),t(z))))^2) < 1e-7
 [1]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From v39k9 at unb.ca  Mon Feb  7 17:48:28 2005
From: v39k9 at unb.ca (Joe Nocera)
Date: Mon,  7 Feb 2005 12:48:28 -0400
Subject: [R] problem with logistic regression
In-Reply-To: <1107793194.4207952a37db5@webmail.polytech-lille.fr>
References: <1107793194.4207952a37db5@webmail.polytech-lille.fr>
Message-ID: <1107794908.42079bdc88bae@webmail.unb.ca>

CATMOD in SAS builds log-linear glms.  If you are truly trying to create logistic
regressions in SAS, use PROC LOGISTIC instead.  If you really meant that you are trying
to create log-linear models in R - then look up the usage of the function loglin (e.g.
loglin())

Cheers,
Joe

Quoting Helene.Dryssens at eleves.polytech-lille.fr:

> Hi,
> 
> we try to do a logistic regression with the function glm.
> But we notice that this function don't give the same results as the SAS proc
> catmod (differents estimate given).
> We try to change the contrast on R system with:
> > options(contrasts=c(unordered="contr.SAS",ordered="contr.poly"))
> 
> We also try with brlr and logistf functions.
> Unfortunately, the estimate aren't still the same.
> 
> Please could someone help us.
> 
> Thank you
> 
> --------------------------------------------------
> This mail sent through Polytech'Lille WebMail (IMP)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Joseph J. Nocera
Ph.D. Candidate
NB Coop. Fish & Wildlife Research Unit
Biology Department - Univ. New Brunswick
Fredericton, NB
Canada   E3B 6E1
tel: (902) 679-5733

"Why does it have to be spiders?  Why can't it be 'follow the butterflies'"?!
    - Ron Weasley, Harry Potter & The Chamber of Secrets



From francoisromain at free.fr  Mon Feb  7 17:54:11 2005
From: francoisromain at free.fr (Romain Francois)
Date: Mon, 07 Feb 2005 17:54:11 +0100
Subject: [R] Creating a correlation Matrix
In-Reply-To: <5.2.0.9.2.20050207105459.026bea20@email.psu.edu>
References: <5.2.0.9.2.20050207105459.026bea20@email.psu.edu>
Message-ID: <42079D33.3080609@free.fr>

Le 07.02.2005 16:58, Jessica Higgs a ?crit :

> Hi all:
>
> I have a question on how to go about creating a correlation matrix.  I 
> have a huge amount of data....21 variables for 3471 times. I want to 
> see how each of the variables correlate to each other.  Any help would 
> be appreciated, including which package and which functions I should 
> use to do this.
>
Hello,

that is not that huge and what is wrong with ?cor

A <- matrix(rnorm(21*3471),ncol=21) #you should replace A by your data
matcor <- cor(A) #that is your correlation matrix
image(matcor) # and how to watch it
heatmap(matcor) # an other way to watch it


Hope this helps.
Romain

-- 
Romain FRANCOIS : francoisromain at free.fr
page web : http://addictedtor.free.fr/  (en construction)
06 18 39 14 69 / 01 46 80 65 60
_______________________________________________________
Etudiant en 3eme ann?e
Institut de Statistique de l'Universit? de Paris (ISUP)
Fili?re Industrie et Services
http://www.isup.cicrp.jussieu.fr/



From eesteves at ualg.pt  Mon Feb  7 18:07:43 2005
From: eesteves at ualg.pt (eesteves@ualg.pt)
Date: Mon,  7 Feb 2005 17:07:43 +0000
Subject: [R] Help with multicomarisons after ANCOVA
Message-ID: <20050207170743.dv28mqjbk4kc44kk@wmail.ualg.pt>

Dear All,

I' ve used ANCOVA (through lm(Y~factor*x)) to study the influence/differences
between levels of factor upon the Y-x relationship. I found that both
intercepts and slopes differ among levels of the factor. I understand from the
results of summary(lm(Y~factor*x)) which of the intercepts/slopes per level
differ from "the reference level". I later used lm(Y~factor/(1+x)-1) to obtain
"explicit" coefficients per level.

My question is: Can I test for differences among all intercepts and /or slopes
per levels using simtest (from package multicomp) as summary
(simtest(Y~factor+x,type="tukey"))?

The outputs seem to contradict the results from the "explicit coefficients
approach"!

Thanks in advance,
Eduardo



From martin.chlond at btinternet.com  Mon Feb  7 18:24:39 2005
From: martin.chlond at btinternet.com (Martin Chlond)
Date: Mon, 7 Feb 2005 17:24:39 -0000
Subject: [R] proportional matrix rows
In-Reply-To: <e1873c31b3836a68eb0e9532b87d6733@soc.soton.ac.uk>
Message-ID: <000201c50d39$e76eea70$15ab8151@OWNER2TYZC0SV7>

Robin

The attached script works but it ain't pretty/

Martin
 

-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.

 
  

From jlh599 at psu.edu  Mon Feb  7 18:53:31 2005
From: jlh599 at psu.edu (Jessica Higgs)
Date: Mon, 07 Feb 2005 12:53:31 -0500
Subject: [R] Creating a correlation Matrix
In-Reply-To: <4207983A.6060506@pdf.com>
References: <5.2.0.9.2.20050207105459.026bea20@email.psu.edu>
	<5.2.0.9.2.20050207105459.026bea20@email.psu.edu>
Message-ID: <5.2.0.9.2.20050207125103.02670770@email.psu.edu>

I've tried using cor() by the following sequence:

C <- cor(x, y = NULL, use = "all.obs", method = c("pearson"))

where x is my matrix of 21 columns and 3471 rows.

and I get this error:

Error in cor(x, y = NULL, use = "all.obs", method = c("pearson")) :
         missing observations in cov/cor

any suggestions?

At 08:32 AM 2/7/2005 -0800, Spencer Graves wrote:
>      Have you considered "cor"?  The command 'help.search("correlation")' 
> suggests among other functions "var(stats)", the documentation for which 
> also includes "cor".
>      If this is not adequate, "PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html ".  It may help you get more 
> useful replies in the future.      spencer graves
>
>Jessica Higgs wrote:
>
>>Hi all:
>>
>>I have a question on how to go about creating a correlation matrix.  I 
>>have a huge amount of data....21 variables for 3471 times. I want to see 
>>how each of the variables correlate to each other.  Any help would be 
>>appreciated, including which package and which functions I should use to 
>>do this.
>>
>>Thanks,
>>Jessica Higgs
>>
>>Masters Student
>>Department of Meteorology
>>Penn State University
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From p.dalgaard at biostat.ku.dk  Mon Feb  7 19:01:23 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 07 Feb 2005 19:01:23 +0100
Subject: [R] problem with logistic regression
In-Reply-To: <1107793194.4207952a37db5@webmail.polytech-lille.fr>
References: <1107793194.4207952a37db5@webmail.polytech-lille.fr>
Message-ID: <x2k6pkxn9o.fsf@biostat.ku.dk>

Helene.Dryssens at eleves.polytech-lille.fr writes:

> Hi,
> 
> we try to do a logistic regression with the function glm.
> But we notice that this function don't give the same results as the SAS proc
> catmod (differents estimate given).
> We try to change the contrast on R system with:
> > options(contrasts=c(unordered="contr.SAS",ordered="contr.poly"))
> 
> We also try with brlr and logistf functions.
> Unfortunately, the estimate aren't still the same.

contr.SAS is something of a contradiction in terms since SAS does not
use the same type of contrast in all procedures. I believe PROC CATMOD
is using sum contrasts (contr.sum), whereas contr.SAS gives you
something similar to PROC GENMOD (last level set to zero).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From clini at stat.sfu.ca  Mon Feb  7 19:06:38 2005
From: clini at stat.sfu.ca (Chunfang Lin)
Date: Mon, 07 Feb 2005 10:06:38 -0800
Subject: [R] fortran into R
Message-ID: <200502071806.j17I6cO2016578@rm-rstar.sfu.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050207/a5d11ef4/attachment.pl

From gavin.simpson at ucl.ac.uk  Mon Feb  7 19:19:22 2005
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Mon, 07 Feb 2005 18:19:22 +0000
Subject: [R] Creating a correlation Matrix
In-Reply-To: <5.2.0.9.2.20050207125103.02670770@email.psu.edu>
References: <5.2.0.9.2.20050207105459.026bea20@email.psu.edu>	<5.2.0.9.2.20050207105459.026bea20@email.psu.edu>
	<5.2.0.9.2.20050207125103.02670770@email.psu.edu>
Message-ID: <4207B12A.9070500@ucl.ac.uk>

Jessica Higgs wrote:
> I've tried using cor() by the following sequence:
> 
> C <- cor(x, y = NULL, use = "all.obs", method = c("pearson"))
> 
> where x is my matrix of 21 columns and 3471 rows.
> 
> and I get this error:
> 
> Error in cor(x, y = NULL, use = "all.obs", method = c("pearson")) :
>         missing observations in cov/cor
> 
> any suggestions?

Reading ?cor would have given you a solution:

Quoting from ?cor

      If 'use' is '"all.obs"', then the presence of missing observations
      will produce an error. If 'use' is '"complete.obs"' then missing
      values are handled by casewise deletion.  Finally, if 'use' has
      the value '"pairwise.complete.obs"' then the correlation between
      each pair of variables is computed using all complete pairs of
      observations on those variables. This can result in covariance or
      correlation matrices which are not positive semidefinite.

You have missing values in your data and with your usage of cor and 
argument use = "all.obs" precludes this. Choose another value for use, 
noting the advice in the help pages for cor.

G


> At 08:32 AM 2/7/2005 -0800, Spencer Graves wrote:
> 
>>      Have you considered "cor"?  The command 
>> 'help.search("correlation")' suggests among other functions 
>> "var(stats)", the documentation for which also includes "cor".
>>      If this is not adequate, "PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html ".  It may help you get 
>> more useful replies in the future.      spencer graves
>>
>> Jessica Higgs wrote:
>>
>>> Hi all:
>>>
>>> I have a question on how to go about creating a correlation matrix.  
>>> I have a huge amount of data....21 variables for 3471 times. I want 
>>> to see how each of the variables correlate to each other.  Any help 
>>> would be appreciated, including which package and which functions I 
>>> should use to do this.
>>>
>>> Thanks,
>>> Jessica Higgs
>>>
>>> Masters Student
>>> Department of Meteorology
>>> Penn State University
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpsonATNOSPAMucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From francoisromain at free.fr  Mon Feb  7 19:20:03 2005
From: francoisromain at free.fr (Romain Francois)
Date: Mon, 07 Feb 2005 19:20:03 +0100
Subject: [R] Creating a correlation Matrix
In-Reply-To: <5.2.0.9.2.20050207125103.02670770@email.psu.edu>
References: <5.2.0.9.2.20050207105459.026bea20@email.psu.edu>	<5.2.0.9.2.20050207105459.026bea20@email.psu.edu>
	<5.2.0.9.2.20050207125103.02670770@email.psu.edu>
Message-ID: <4207B153.30604@free.fr>

Le 07.02.2005 18:53, Jessica Higgs a ?crit :

> I've tried using cor() by the following sequence:
>
> C <- cor(x, y = NULL, use = "all.obs", method = c("pearson"))
>
In ?cor you find that :

If 'use' is '"all.obs"', then the presence of missing observations will 
produce an error.

try use="complete" instead.

Romain.

> where x is my matrix of 21 columns and 3471 rows.
>
> and I get this error:
>
> Error in cor(x, y = NULL, use = "all.obs", method = c("pearson")) :
>         missing observations in cov/cor
>
> any suggestions?
>
> At 08:32 AM 2/7/2005 -0800, Spencer Graves wrote:
>
>>      Have you considered "cor"?  The command 
>> 'help.search("correlation")' suggests among other functions 
>> "var(stats)", the documentation for which also includes "cor".
>>      If this is not adequate, "PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html ".  It may help you get 
>> more useful replies in the future.      spencer graves
>>
>> Jessica Higgs wrote:
>>
>>> Hi all:
>>>
>>> I have a question on how to go about creating a correlation matrix.  
>>> I have a huge amount of data....21 variables for 3471 times. I want 
>>> to see how each of the variables correlate to each other.  Any help 
>>> would be appreciated, including which package and which functions I 
>>> should use to do this.
>>>
>>> Thanks,
>>> Jessica Higgs
>>>
>>> Masters Student
>>> Department of Meteorology
>>> Penn State University
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>


-- 
Romain FRANCOIS : francoisromain at free.fr
page web : http://addictedtor.free.fr/  (en construction)
06 18 39 14 69 / 01 46 80 65 60
_______________________________________________________
Etudiant en 3eme ann?e
Institut de Statistique de l'Universit? de Paris (ISUP)
Fili?re Industrie et Services
http://www.isup.cicrp.jussieu.fr/



From matthew_wiener at merck.com  Mon Feb  7 19:21:14 2005
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Mon, 7 Feb 2005 13:21:14 -0500
Subject: [R] Creating a correlation Matrix
Message-ID: <45AAE6FD142DCB43A38C00A11FF5DF3E04993F36@uswsmx03.merck.com>

It looks like you have missing observations.  With the "use" argument, you
can specify complete observations or pairwise-complete observations.

Hope this helps,

Matt Wiener

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jessica Higgs
Sent: Monday, February 07, 2005 12:54 PM
To: Spencer Graves; R-help at stat.math.ethz.ch
Subject: Re: [R] Creating a correlation Matrix


I've tried using cor() by the following sequence:

C <- cor(x, y = NULL, use = "all.obs", method = c("pearson"))

where x is my matrix of 21 columns and 3471 rows.

and I get this error:

Error in cor(x, y = NULL, use = "all.obs", method = c("pearson")) :
         missing observations in cov/cor

any suggestions?

At 08:32 AM 2/7/2005 -0800, Spencer Graves wrote:
>      Have you considered "cor"?  The command 'help.search("correlation")' 
> suggests among other functions "var(stats)", the documentation for which 
> also includes "cor".
>      If this is not adequate, "PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html ".  It may help you get more 
> useful replies in the future.      spencer graves
>
>Jessica Higgs wrote:
>
>>Hi all:
>>
>>I have a question on how to go about creating a correlation matrix.  I 
>>have a huge amount of data....21 variables for 3471 times. I want to see 
>>how each of the variables correlate to each other.  Any help would be 
>>appreciated, including which package and which functions I should use to 
>>do this.
>>
>>Thanks,
>>Jessica Higgs
>>
>>Masters Student
>>Department of Meteorology
>>Penn State University
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From tfliao at uiuc.edu  Mon Feb  7 19:28:09 2005
From: tfliao at uiuc.edu (Tim F Liao)
Date: Mon, 7 Feb 2005 12:28:09 -0600
Subject: [R] problem with logistic regression
Message-ID: <fa3bd17c.cb01e67f.81db000@expms6.cites.uiuc.edu>

Helene,

The problem is not with R, but with the specification of the
SAS procedure you used.  It would be simpler and more
comparable if you specify your model in SAS with Proc Logistic
or Proc Genmod.

See if they work out for you,

Tim

---- Original message ----
>Date: Mon,  7 Feb 2005 17:19:54 +0100
>From: Helene.Dryssens at eleves.polytech-lille.fr  
>Subject: [R] problem with logistic regression  
>To: r-help at stat.math.ethz.ch
>Cc: nathaliebouez at yahoo.fr
>
>Hi,
>
>we try to do a logistic regression with the function glm.
>But we notice that this function don't give the same results
as the SAS proc
>catmod (differents estimate given).
>We try to change the contrast on R system with:
>>
options(contrasts=c(unordered="contr.SAS",ordered="contr.poly"))
>
>We also try with brlr and logistf functions.
>Unfortunately, the estimate aren't still the same.
>
>Please could someone help us.
>
>Thank you
>
>--------------------------------------------------
>This mail sent through Polytech'Lille WebMail (IMP)
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From martin.chlond at btinternet.com  Mon Feb  7 19:28:59 2005
From: martin.chlond at btinternet.com (Martin Chlond)
Date: Mon, 7 Feb 2005 18:28:59 -0000
Subject: [R] proportional matrix rows - with attachment
In-Reply-To: <000201c50d39$e76eea70$15ab8151@OWNER2TYZC0SV7>
Message-ID: <000501c50d42$e43a5d40$15ab8151@OWNER2TYZC0SV7>

Sorry! Something went wrong with the attachment in my previous post.

Here it is.
-------------------------------


intmult <- function(X)

{
  # extract columns with non-zero top cell
  X1 <- X[,which(X[1,]!=0)]
  
  # look for integer multiples in reduced table
  m <- nrow(X1);n <- ncol(X1)
  D <- matrix(X1[1,],m,n,byrow=T)
  X2 <- X1/D
  ind <- which(rowSums(X2==rowMeans(X2))==n)
  
  # build new table of integer multiples
  X3 <- X[ind,]
  
  # remove rows that contain non-zero(s) below top zero
  ind <- which(X3[1,]==0)
  lind = length(ind)
  temp <- X3[,ind]==0
  ind2 <- which(rowSums(temp)==lind)
  X4 <- X3[ind2,]
}

-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From f.harrell at vanderbilt.edu  Mon Feb  7 19:49:00 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Mon, 07 Feb 2005 13:49:00 -0500
Subject: [R] Programming/scripting  with "expressions - variables"
In-Reply-To: <7FFEE688B57D7346BC6241C55900E730B6FEBF@pollux.bfro.uni-lj.si>
References: <7FFEE688B57D7346BC6241C55900E730B6FEBF@pollux.bfro.uni-lj.si>
Message-ID: <4207B81C.2090602@vanderbilt.edu>

Gorjanc Gregor wrote:
> Hello to Rusers!
> 
> I am puzzled with R and I really do not know where to look
> in for my problem. I am moving from SAS and I have difficulties
> in translating SAS to R world. I hope I will get some hints 
> or pointers so I can study from there on.
> 
> I would like to do something like this. In SAS I can write 
> a macro as example bellow, which is afcourse a silly one but 
> shows what I don't know how to do in R.
> 
> %macro test(data, colname, colvalue);
>     data &data;
>         ...
>         &colname="&colvalue";
>         other_&colname="other_&colvalue";
>     run;
> %mend;
> 
> And if I run it with this call:
> %test(Gregor, Gorjanc, 25);
> 
> I get a table with name 'Gregor' and columns 'Gorjanc', 
> and 'other_Gorjanc' with values:
> 
> Gorjanc other_Gorjanc
> "25"    "other_25"
> 
> So can one show me the way to do the same thing in R? 
> 
> Thanks!
> 
> --
> Lep pozdrav / With regards,
>     Gregor GORJANC
> 

Greetings, Gregor, from a fan of Slovenia.

Here is a start.  Multiple variables are often put together in data 
frames.  Your example did not include an input dataset name.  I took 
data to be an input data frame, and added two new variables.  I assume 
that colvalue and colname are character values.  If colvalue has length 
one it will be duplicated for each row of data.

test <- function(data, colname, colvalue) {
   data[[colname]] <- colvalue
   data[[paste('other',colname,sep='_')]] <- paste('other',colvalue,sep='_')
   data
}

Example usage:

data <- data.frame(x=1:5, y=11:15)
data2 <- test(data, 'a', 'b')
data <- test(data, 'a', 'b')  # over-write
test(data, 'a', 'b')          # create data but don't store
with(test(data, ....),{...})        # reference variables in temporary 
dataset

If we know what your ultimate goal is, there may be a much better 
approach than the test function.  In many cases, you do not need to 
create new variables at all.

Franc
-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From xmeadow at uchicago.edu  Mon Feb  7 20:25:48 2005
From: xmeadow at uchicago.edu (Xander Meadow)
Date: Mon, 07 Feb 2005 13:25:48 -0600
Subject: [R] install RMySQL
Message-ID: <1107804348.4305.6.camel@gregor.bsd.uchicago.edu>

Hi,

I've got a Dual G5 running 10.3.7 and I'm trying to install RMySQL. 
I've already got R up and running.  When I try the command

> install.packages("RMySQL")

It downloads a few things and then produces the following error:
---------------------------
Configuration error:
   Could not locate the library "libz" required by MySQL.
 
INSTRUCTIONS:
 
   The "libz" library is required by the MySQL client library
   in order to compress/uncompress connections between clients
   and the MySQL engine.
 
   Make sure you have "libz" installed properly and/or included
   in your $LD_LIBRARY_PATH.  Perhaps it is not in any of the
   standard directories (e.g., /usr/lib/, /usr/local/lib)?
 
Aborting the installation of RMySQL.
 
ERROR: configuration failed for package 'RMySQL'
----------------------------

However, if I check for libz I get:

> capabilities("libz")
libz
TRUE

I'm not sure what the problem is because R is telling me that it doesn't
know where libz is, but then it's also telling me it does know where
libz is.

Has anyone else seen this problem or know how I can get RMySQL installed
on my machine?

Just as an added note I know that install.packages works because

> install.packages("DBI")

worked without a problem.

Thanks again.

-Xander



From rpeng at jhsph.edu  Mon Feb  7 20:43:53 2005
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Mon, 07 Feb 2005 14:43:53 -0500
Subject: [R] install RMySQL
In-Reply-To: <1107804348.4305.6.camel@gregor.bsd.uchicago.edu>
References: <1107804348.4305.6.camel@gregor.bsd.uchicago.edu>
Message-ID: <4207C4F9.9080109@jhsph.edu>

R comes with it's own copy of zlib so even if it is compiled into R, 
there isn't necessarily a system-wide installation of the library. 
You may still need to install it.

-roger

Xander Meadow wrote:
> Hi,
> 
> I've got a Dual G5 running 10.3.7 and I'm trying to install RMySQL. 
> I've already got R up and running.  When I try the command
> 
> 
>>install.packages("RMySQL")
> 
> 
> It downloads a few things and then produces the following error:
> ---------------------------
> Configuration error:
>    Could not locate the library "libz" required by MySQL.
>  
> INSTRUCTIONS:
>  
>    The "libz" library is required by the MySQL client library
>    in order to compress/uncompress connections between clients
>    and the MySQL engine.
>  
>    Make sure you have "libz" installed properly and/or included
>    in your $LD_LIBRARY_PATH.  Perhaps it is not in any of the
>    standard directories (e.g., /usr/lib/, /usr/local/lib)?
>  
> Aborting the installation of RMySQL.
>  
> ERROR: configuration failed for package 'RMySQL'
> ----------------------------
> 
> However, if I check for libz I get:
> 
> 
>>capabilities("libz")
> 
> libz
> TRUE
> 
> I'm not sure what the problem is because R is telling me that it doesn't
> know where libz is, but then it's also telling me it does know where
> libz is.
> 
> Has anyone else seen this problem or know how I can get RMySQL installed
> on my machine?
> 
> Just as an added note I know that install.packages works because
> 
> 
>>install.packages("DBI")
> 
> 
> worked without a problem.
> 
> Thanks again.
> 
> -Xander
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From xmeadow at uchicago.edu  Mon Feb  7 20:52:54 2005
From: xmeadow at uchicago.edu (Xander Meadow)
Date: Mon, 07 Feb 2005 13:52:54 -0600
Subject: [R] install RMySQL
In-Reply-To: <4207C4F9.9080109@jhsph.edu>
References: <1107804348.4305.6.camel@gregor.bsd.uchicago.edu>
	<4207C4F9.9080109@jhsph.edu>
Message-ID: <1107805973.4305.11.camel@gregor.bsd.uchicago.edu>

Hi,

Thank you for responding.  One thing I forgot to mention (although I'm
guessing you figured it out anyway) is that I'm running Mac OS X.

I checked and I've got the file /usr/include/zlib.h installed on my
machine.  Is this enough for R or does it need a more robust
installation of zlib?  If this is enough, how do I let R know about the
zlib.h file. If it's not, where should I get the correct zlib from? 
Thank you for any and all responses.

-Xander

On Mon, 2005-02-07 at 13:43, Roger D. Peng wrote:
> R comes with it's own copy of zlib so even if it is compiled into R, 
> there isn't necessarily a system-wide installation of the library. 
> You may still need to install it.
> 
> -roger
> 
> Xander Meadow wrote:
> > Hi,
> > 
> > I've got a Dual G5 running 10.3.7 and I'm trying to install RMySQL. 
> > I've already got R up and running.  When I try the command
> > 
> > 
> >>install.packages("RMySQL")
> > 
> > 
> > It downloads a few things and then produces the following error:
> > ---------------------------
> > Configuration error:
> >    Could not locate the library "libz" required by MySQL.
> >  
> > INSTRUCTIONS:
> >  
> >    The "libz" library is required by the MySQL client library
> >    in order to compress/uncompress connections between clients
> >    and the MySQL engine.
> >  
> >    Make sure you have "libz" installed properly and/or included
> >    in your $LD_LIBRARY_PATH.  Perhaps it is not in any of the
> >    standard directories (e.g., /usr/lib/, /usr/local/lib)?
> >  
> > Aborting the installation of RMySQL.
> >  
> > ERROR: configuration failed for package 'RMySQL'
> > ----------------------------
> > 
> > However, if I check for libz I get:
> > 
> > 
> >>capabilities("libz")
> > 
> > libz
> > TRUE
> > 
> > I'm not sure what the problem is because R is telling me that it doesn't
> > know where libz is, but then it's also telling me it does know where
> > libz is.
> > 
> > Has anyone else seen this problem or know how I can get RMySQL installed
> > on my machine?
> > 
> > Just as an added note I know that install.packages works because
> > 
> > 
> >>install.packages("DBI")
> > 
> > 
> > worked without a problem.
> > 
> > Thanks again.
> > 
> > -Xander
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >



From andy_liaw at merck.com  Mon Feb  7 20:59:33 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 7 Feb 2005 14:59:33 -0500
Subject: [R] R on Beowulf cluster?
Message-ID: <3A822319EB35174CA3714066D590DCD50994E673@usrymx25.merck.com>

Dear R-help,

Has anyone tried running R on a Beowulf-type cluster?  I can get R to run in
batch (using R CMD BATCH) on a cluster, but am wondering if it is possible
to get an interactive R session on a compute node.  Right now, if I run:

  beorun --nolocal R

I just get the R start-up message and back to the shell prompt.  If I try

  bpsh 0 R

I can get R started (but the R prompt does not appear) and do computations,
but no access to an x11() device.  I'd very much appreciate any pointer.

Best,
Andy

Andy Liaw, PhD
Biometrics Research      PO Box 2000, RY33-300     
Merck Research Labs           Rahway, NJ 07065
andy_liaw <at> merck.com          732-594-0820



From elw at stderr.org  Mon Feb  7 21:02:56 2005
From: elw at stderr.org (elijah wright)
Date: Mon, 7 Feb 2005 14:02:56 -0600 (CST)
Subject: [R] R on Beowulf cluster?
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E673@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E673@usrymx25.merck.com>
Message-ID: <Pine.LNX.4.62.0502071401240.28164@illuminati.stderr.org>


look into the following knot of CRAN packages:

SNOW
Rmpi
Rpvm
rlecuyer
rsprng
SNOW-FT
RScaLAPACK

et cetera.

these provide some pretty interesting bits - like a suite of parallelized 
apply() functions.


--elijah


> Has anyone tried running R on a Beowulf-type cluster?  I can get R to run in
> batch (using R CMD BATCH) on a cluster, but am wondering if it is possible
> to get an interactive R session on a compute node.  Right now, if I run:
>
>  beorun --nolocal R
>
> I just get the R start-up message and back to the shell prompt.  If I try
>
>  bpsh 0 R
>
> I can get R started (but the R prompt does not appear) and do computations,
> but no access to an x11() device.  I'd very much appreciate any pointer.
>
> Best,
> Andy
>
> Andy Liaw, PhD
> Biometrics Research      PO Box 2000, RY33-300
> Merck Research Labs           Rahway, NJ 07065
> andy_liaw <at> merck.com          732-594-0820
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From andy_liaw at merck.com  Mon Feb  7 21:12:16 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 7 Feb 2005 15:12:16 -0500
Subject: [R] R on Beowulf cluster?
Message-ID: <3A822319EB35174CA3714066D590DCD50994E674@usrymx25.merck.com>

Hi Elijah,

Thanks for the reply.  I'm aware of almost all of those things you
mentioned, and have played with some of them.  I'm not looking to do
distributed computing within R at this point.  What I'm after, though, is
not really addressed by any of them.  I want a user to be able to start an
interactive R session on a compute node that the system determines
(dynamically) to be most available at the time.  That's what `beorun' does,
but I couldn't get it to run R interactively.

Cheers,
Andy

> From: elijah wright
> 
> look into the following knot of CRAN packages:
> 
> SNOW
> Rmpi
> Rpvm
> rlecuyer
> rsprng
> SNOW-FT
> RScaLAPACK
> 
> et cetera.
> 
> these provide some pretty interesting bits - like a suite of 
> parallelized 
> apply() functions.
> 
> 
> --elijah
> 
> 
> > Has anyone tried running R on a Beowulf-type cluster?  I 
> can get R to run in
> > batch (using R CMD BATCH) on a cluster, but am wondering if 
> it is possible
> > to get an interactive R session on a compute node.  Right 
> now, if I run:
> >
> >  beorun --nolocal R
> >
> > I just get the R start-up message and back to the shell 
> prompt.  If I try
> >
> >  bpsh 0 R
> >
> > I can get R started (but the R prompt does not appear) and 
> do computations,
> > but no access to an x11() device.  I'd very much appreciate 
> any pointer.
> >
> > Best,
> > Andy
> >
> > Andy Liaw, PhD
> > Biometrics Research      PO Box 2000, RY33-300
> > Merck Research Labs           Rahway, NJ 07065
> > andy_liaw <at> merck.com          732-594-0820
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> >
> 
>



From lraja at i-com.com  Mon Feb  7 21:21:08 2005
From: lraja at i-com.com (Latha Raja)
Date: Mon, 7 Feb 2005 15:21:08 -0500
Subject: [R] Need your help with my R plot
Message-ID: <E6A6A0A8A6FD0943B588100ECDD9DABA0E5BD7@mail-01.i-com.com>

Hi,

I am using R to plot the graph and the problem I am facing with my graph is that I have lots of points concentrated in one area and It is creating a visualization challenge.

Is there any commands in R I could use to solve this problem.  Even if there is no command, do you know how I could tackle this problem...(I want to separate these points so I could see each of them...)

Any help is much appreciated...

Thank you kindly

Latha



From eliothan at yahoo.com  Mon Feb  7 21:31:38 2005
From: eliothan at yahoo.com (Tae Sik Han)
Date: Mon, 7 Feb 2005 12:31:38 -0800 (PST)
Subject: [R] R-DCOM server problem.
Message-ID: <20050207203138.33201.qmail@web30505.mail.mud.yahoo.com>

R-DCOM server problem

My program (VB program) runs R-program through R DCOM
server. No problem to run statistical functions and my
own functions. However When I tried to load data using
ODBC I got an error message.
I need to set up any parameter for R-DCOM server for
ODBC ?  


library(DBI);
library(RODBC);
con = odbcConnect("odbc_db_string", uid = "uid",
pwd="pwd");
tlist = sqlQuery(con, "select tname from source_tab");

--- Error message ---
An unhandled exception of type
'System.Runtime.InteropServices.COMException' occurred
in matcher.exe
Additional information: Object is static; operation
not allowed


Tae Sik Han.
North Carolina State University.



From eliothan at yahoo.com  Mon Feb  7 21:34:24 2005
From: eliothan at yahoo.com (Tae Sik Han)
Date: Mon, 7 Feb 2005 12:34:24 -0800 (PST)
Subject: [R] ROracle problem.
Message-ID: <20050207203424.6753.qmail@web30506.mail.mud.yahoo.com>

ROracle ploblem

I can't install ROracle in the window XP. Is there any
way to install ROracle package on the windows OS ? I
think I can enhance data acquisition performance using
oracle native dirver instead of using ODBC.

Tae Sik Han.
North Carolina State University.



From rbb at nebiometrics.com  Mon Feb  7 21:39:13 2005
From: rbb at nebiometrics.com (Robert Burrows)
Date: Mon, 7 Feb 2005 15:39:13 -0500 (EST)
Subject: [R] Need your help with my R plot
In-Reply-To: <E6A6A0A8A6FD0943B588100ECDD9DABA0E5BD7@mail-01.i-com.com>
References: <E6A6A0A8A6FD0943B588100ECDD9DABA0E5BD7@mail-01.i-com.com>
Message-ID: <Pine.LNX.4.61.0502071537490.2907@neblt2>

On Mon, 7 Feb 2005, Latha Raja wrote:

> I am using R to plot the graph and the problem I am facing with my graph 
> is that I have lots of points concentrated in one area and It is 
> creating a visualization challenge.

How about 'plot(.... ,pch='.', ....)?

-- 
Robert Burrows, PhD
New England Biometrics
rbb at nebiometrics.com



From helprhelp at gmail.com  Mon Feb  7 21:53:49 2005
From: helprhelp at gmail.com (WeiWei Shi)
Date: Mon, 7 Feb 2005 14:53:49 -0600
Subject: [R] R or weka
Message-ID: <cdf81783050207125323e529c0@mail.gmail.com>

Hi, guys:

These days I keep using R and Weka to do data mining. I think my next
step is open the source codes so that I can "customrize them" and make
them better server my purpose. But now I kinda hesitate to do so b/c I
am really not sure which is better to start with. You know, both
require some time and I cannot clone myself to work on both:) If here
are some persons who used both of them, please provide some
suggestions from your experience.

Thanks,

Ed



From wolski at molgen.mpg.de  Mon Feb  7 22:06:00 2005
From: wolski at molgen.mpg.de (Witold Eryk Wolski)
Date: Mon, 07 Feb 2005 22:06:00 +0100
Subject: [R] Need your help with my R plot
In-Reply-To: <E6A6A0A8A6FD0943B588100ECDD9DABA0E5BD7@mail-01.i-com.com>
References: <E6A6A0A8A6FD0943B588100ECDD9DABA0E5BD7@mail-01.i-com.com>
Message-ID: <4207D838.2090405@molgen.mpg.de>

Latha Raja wrote:

>Hi,
>
>I am using R to plot the graph and the problem I am facing with my graph is that I have lots of points concentrated in one area and It is creating a visualization challenge.
>
>Is there any commands in R I could use to solve this problem.  Even if there is no command, do you know how I could tackle this problem...(I want to separate these points so I could see each of them...)
>
>Any help is much appreciated...
>
>Thank you kindly
>
>Latha
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>
Hi,

Take a look at the function:
ixyplot

in the package IDPmisc.

Eryk

-- 
Dipl. bio-chem. Witold Eryk Wolski
MPI-Moleculare Genetic
Ihnestrasse 63-73 14195 Berlin
tel: 0049-30-83875219                 __("<    _
http://www.molgen.mpg.de/~wolski      \__/    'v'
http://r4proteomics.sourceforge.net    ||    /   \
mail: witek96 at users.sourceforge.net    ^^     m m
      wolski at molgen.mpg.de



From lraja at i-com.com  Mon Feb  7 22:10:06 2005
From: lraja at i-com.com (Latha Raja)
Date: Mon, 7 Feb 2005 16:10:06 -0500
Subject: [R] THANK YOU:-)
Message-ID: <E6A6A0A8A6FD0943B588100ECDD9DABA0E5BD8@mail-01.i-com.com>

Thank you all for your help :-)  Much Much Aprreciated........

I tried with the "jitter" command and it worked :-)

Cheers,

Latha

>  -----Original Message-----
> From: 	Latha Raja  
> Sent:	Monday, February 07, 2005 3:21 PM
> To:	'r-help at stat.math.ethz.ch'
> Subject:	Need your help with my R plot
> Importance:	High
> 
> Hi,
> 
> I am using R to plot the graph and the problem I am facing with my graph is that I have lots of points concentrated in one area and It is creating a visualization challenge.
> 
> Is there any commands in R I could use to solve this problem.  Even if there is no command, do you know how I could tackle this problem...(I want to separate these points so I could see each of them...)
> 
> Any help is much appreciated...
> 
> Thank you kindly
> 
> Latha



From gunter.berton at gene.com  Mon Feb  7 22:54:19 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 7 Feb 2005 13:54:19 -0800
Subject: [R] THANK YOU:-)
In-Reply-To: <E6A6A0A8A6FD0943B588100ECDD9DABA0E5BD8@mail-01.i-com.com>
Message-ID: <200502072154.j17LsJxi020256@meitner.gene.com>

Glad that jitter() solved your problem. However, this would not be the case
for "largeish" bivariate displays in general. So may I add another
suggestion for those who might face similar problems: hexbin() in the hexbin
package on Bioconductor, which was specifically developed for the problem
you initially described. 

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Latha Raja
> Sent: Monday, February 07, 2005 1:10 PM
> To: wolski at molgen.mpg.de; Mike.Prager at noaa.gov; 
> james.holtman at convergys.com; rbb at nebiometrics.com; 
> PAlspach at hortresearch.co.nz; abunn at whrc.org
> Cc: r-help at stat.math.ethz.ch
> Subject: [R] THANK YOU:-)
> 
> Thank you all for your help :-)  Much Much Aprreciated........
> 
> I tried with the "jitter" command and it worked :-)
> 
> Cheers,
> 
> Latha
> 
> >  -----Original Message-----
> > From: 	Latha Raja  
> > Sent:	Monday, February 07, 2005 3:21 PM
> > To:	'r-help at stat.math.ethz.ch'
> > Subject:	Need your help with my R plot
> > Importance:	High
> > 
> > Hi,
> > 
> > I am using R to plot the graph and the problem I am facing 
> with my graph is that I have lots of points concentrated in 
> one area and It is creating a visualization challenge.
> > 
> > Is there any commands in R I could use to solve this 
> problem.  Even if there is no command, do you know how I 
> could tackle this problem...(I want to separate these points 
> so I could see each of them...)
> > 
> > Any help is much appreciated...
> > 
> > Thank you kindly
> > 
> > Latha
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From gerifalte28 at hotmail.com  Mon Feb  7 23:18:35 2005
From: gerifalte28 at hotmail.com (F Z)
Date: Mon, 07 Feb 2005 22:18:35 +0000
Subject: [R] Lecture
In-Reply-To: <4206891A.5010005@statistik.uni-dortmund.de>
Message-ID: <BAY103-F19F770456809CA6AD448BBA6730@phx.gbl>

Oi Bernardo

Maybe it would also be useful to show them some of the libraries available 
for medical research.  That list would also be useful for other people on 
this forum including myself ;)

In the past I have made demos running R and other commercial software side 
by side, to demonstrate that you can do the same things and even better 
using R (i.e. try examples using SAS Proc Mixed vs. lme, different plots, 
etc)

Cheers

Francisco

>From: Uwe Ligges <ligges at statistik.uni-dortmund.de>
>To: Bernardo Rangel Tura <tura at centroin.com.br>
>CC: r-help at stat.math.ethz.ch
>Subject: Re: [R] Lecture
>Date: Sun, 06 Feb 2005 22:16:10 +0100
>
>Bernardo Rangel Tura wrote:
>>Hi R-masters!
>>
>>>I you receive an invitation to make a lecture about R in Medical School 
>>>of Federal University of Rio de Janeiro.
>>
>>
>>>  The goal of the lecture is to stimulate the use of R instead of others 
>>>statiscal software in the Medical School.
>>
>>
>>
>>
>>>  It would like to hear tour suggestions on which topics to present in 
>>>the lecture.
>
>I'd rather focus on R. ;-)
>
>
>A subset of typical stuff for advertisements:
>- The language and its capabilities
>  + from the language point of view
>  + huge number of statistical methods implemented
>  + outstanding graphical capabilities
>- GPL/Open Source
>- packages
>- for a medical school Bioconductor might be of high relevance
>
>Uwe Ligges
>
>
>>
>>Thanks in advance
>>
>>Bernardo Rangel Tura, MD, MSc
>>National Institute of Cardiology Laranjeiras
>>Rio de Janeiro Brazil
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From h.brunschwig at utoronto.ca  Tue Feb  8 00:07:18 2005
From: h.brunschwig at utoronto.ca (h.brunschwig@utoronto.ca)
Date: Mon,  7 Feb 2005 18:07:18 -0500
Subject: [R] MLE: Question
Message-ID: <1107817638.4207f4a6c0edf@webmail.utoronto.ca>


Hi R users! 

I have a likelihood ratio statistic that depends on a parameter delta and I am
trying to get confidence intervals for this delta using the fact that the
likelihood ratio statistic is approx. chi-squared distributed.

For this I need to maximize the two likelihoods (for the ratio statistic) one of
which depends on delta too and I am trying to use the function "mle". But delta
is neither a parameter I would like to estimate nor a fixed value that I know
the value of (to give it to the "mle" function with fixed=list(delta=?)). 

Can I still use "mle" to find the likelihood of the data (just denoting delta as
a constant)?

Thank you.

Hadassa



From Gary.Whysong at asu.edu  Tue Feb  8 00:09:43 2005
From: Gary.Whysong at asu.edu (Gary Whysong)
Date: Mon, 07 Feb 2005 16:09:43 -0700
Subject: [R] Nonparametric Split-Plot
Message-ID: <4207F537.1080005@asu.edu>

Hello,

I have several data sets involving repeated measures over time of annual 
grass survival in plots following application of several levels of 
herbicide treatments. Since the data set contains quite a number of 
zeros, it doesn't fit the assumptions for parametric analysis.

I read in

Shah, D.A. and L. V. Madden. 2004. Nonparametric analysis of ordinal 
data in designed factorial experiments. Am. Phytopathological Soc. 94:1 
pp 33-43.

that free macros exist for this analysis in SAS and R. I've looked 
through the R packages and searched the R mailing list; however, I can't 
seem to find what I'm looking for. Perhaps, I'm looking in the wrong place.

I would appreciate it if someone would point me in the right direction.

Thanks,

Gary Whysong
Department of Applied Biological Sciences
Arizona State University East



From p.dalgaard at biostat.ku.dk  Tue Feb  8 00:50:10 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 08 Feb 2005 00:50:10 +0100
Subject: [R] MLE: Question
In-Reply-To: <1107817638.4207f4a6c0edf@webmail.utoronto.ca>
References: <1107817638.4207f4a6c0edf@webmail.utoronto.ca>
Message-ID: <x21xbsx74d.fsf@biostat.ku.dk>

h.brunschwig at utoronto.ca writes:

> Hi R users! 
> 
> I have a likelihood ratio statistic that depends on a parameter delta and I am
> trying to get confidence intervals for this delta using the fact that the
> likelihood ratio statistic is approx. chi-squared distributed.
> 
> For this I need to maximize the two likelihoods (for the ratio statistic) one of
> which depends on delta too and I am trying to use the function "mle". But delta
> is neither a parameter I would like to estimate nor a fixed value that I know
> the value of (to give it to the "mle" function with fixed=list(delta=?)). 
> 
> Can I still use "mle" to find the likelihood of the data (just denoting delta as
> a constant)?

Er, ... Why wouldn't delta be a parameter?? If you consider the
likelihood a function of delta and all the other parameters in the
model can you not maximize it and profile over delta?

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From dhkblaszyk at zeelandnet.nl  Tue Feb  8 01:15:06 2005
From: dhkblaszyk at zeelandnet.nl (dhkblaszyk@zeelandnet.nl)
Date: Tue, 8 Feb 2005 01:15:06 +0100
Subject: [R] Contour plot
Message-ID: <001201c50d73$3f7d3ee0$3ef4ee3e@1234abc>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050208/c9f7f54e/attachment.pl

From hold9309 at uidaho.edu  Tue Feb  8 01:46:13 2005
From: hold9309 at uidaho.edu (Zachary Holden)
Date: Mon, 07 Feb 2005 16:46:13 -0800
Subject: [R] Manova mult. comparisons?
Message-ID: <6f47f16f75af.6f75af6f47f1@uidaho.edu>

Hi R folks,

I'm using Manova to evlaluate the effects of a prescribed fire treatment on some forest stand structure data  MODEL <- manova(Y ~ FIRE, test = "Hotelling-Lawley") where Y is a 2 parameters describing tree distributions and fire has 4 levels, 0,1,2,3. 

Can anybody tell me how I can compare differences between individual treatments?
I can't find anything in ?multcomp or ?summary.manova. I've used Tukey and Bonferonni in the past for univariate analyses, but not MANOVA.

If anyone can point me to a help file I've missed or location for some useful code, I'd appreciate it. 

Thanks in advance,

Zack



From Achim.Zeileis at wu-wien.ac.at  Tue Feb  8 01:51:57 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 8 Feb 2005 01:51:57 +0100
Subject: [R] Contour plot
In-Reply-To: <001201c50d73$3f7d3ee0$3ef4ee3e@1234abc>
References: <001201c50d73$3f7d3ee0$3ef4ee3e@1234abc>
Message-ID: <20050208015157.5adb9224.Achim.Zeileis@wu-wien.ac.at>

On Tue, 8 Feb 2005 01:15:06 +0100 dhkblaszyk at zeelandnet.nl wrote:

> Hello,
> 
> I would like to make a contourplot of the following data;
> 
> > x <- 1:10
> > y <- 1:10
> > z <- 100:110
> 
> By doing >contour(x,y,z) I get the following error;
> 
> "Error in contour.default(x, y, z) : no proper `z' matrix specified"
> 
> How do I fix this??

x and y specify a grid and thus z must provide a value for each
combination of the x's and y's! For example:
  x <- y <- 1:10
  contour(x, y, outer(x, y))
Also look at 
  outer(x, y)
and read ?contour.

Z

> Kind regards, Datius Blaszijk
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From ckjmaner at carolina.rr.com  Tue Feb  8 05:07:56 2005
From: ckjmaner at carolina.rr.com (Charles and Kimberly Maner)
Date: Mon, 7 Feb 2005 23:07:56 -0500
Subject: [R] RE: Reading Dates in a csv File
Message-ID: <200502080406.j1846lee010669@ms-smtp-02-eri0.southeast.rr.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050207/9fbdf007/attachment.pl

From rsubscriber at slcmsr.net  Tue Feb  8 05:09:57 2005
From: rsubscriber at slcmsr.net (Christian Lederer)
Date: Tue, 8 Feb 2005 05:09:57 +0100 (CET)
Subject: [R] Strange parsing behavior of an else condition
Message-ID: <32967.217.89.8.143.1107835797.squirrel@mail.mytrium.com>

Dear R users,

can anybody explain the reason, why the first piece of code
below gives a parsing error, while the other two variations
work?

# Gives a parsing error
x <- 1
if (x < 0)
  {
    y <- 1
  }
else # Error occurs at this line
  {
    y <- -1
  }


# This works
x <- 1
{
  if (x < 0)
    {
      y <- 1
    }
  else
    {
      y <- -1
    }
}

# This works too
x <- 1
if (x < 0)
  {
    y <- 1
  } else
  {
    y <- -1
  }

Christian



From roebuck at odin.mdacc.tmc.edu  Tue Feb  8 06:31:36 2005
From: roebuck at odin.mdacc.tmc.edu (Paul Roebuck)
Date: Mon, 7 Feb 2005 23:31:36 -0600 (CST)
Subject: [R] Strange parsing behavior of an else condition
In-Reply-To: <32967.217.89.8.143.1107835797.squirrel@mail.mytrium.com>
References: <32967.217.89.8.143.1107835797.squirrel@mail.mytrium.com>
Message-ID: <Pine.OSF.4.58.0502072305590.456411@odin.mdacc.tmc.edu>

On Tue, 8 Feb 2005, Christian Lederer wrote:

> can anybody explain the reason, why the first piece of code
> below gives a parsing error, while the other two variations
> work?
>
> # Gives a parsing error
> x <- 1
> if (x < 0)
>   {
>     y <- 1
>   }
> else # Error occurs at this line
>   {
>     y <- -1
>   }
>
>
> # This works
> x <- 1
> {
>   if (x < 0)
>     {
>       y <- 1
>     }
>   else
>     {
>       y <- -1
>     }
> }
>
> # This works too
> x <- 1
> if (x < 0)
>   {
>     y <- 1
>   } else
>   {
>     y <- -1
>   }
>

help("if")

In the first case, the if expression is complete prior to
the else, hence the error. The second case completes since
the outer enclosing brace has not been processed. The third
case completes since the else is on the same line as brace
closing if expression and else expression not yet processed.

There may also be issues whether you're running interactively
or not. Recommend not using Whitesmith style brace formatting
with R or S-plus. IMO, the following formatting style works
best:

sety <- function(x) {
    if (x < 0) {
        y <- 1
    } else {
        y <- -1
    }
}

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)



From Tom.Mulholland at dpi.wa.gov.au  Tue Feb  8 06:46:59 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Tue, 8 Feb 2005 13:46:59 +0800
Subject: [R] RE: Reading Dates in a csv File
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA83@afhex01.dpi.wa.gov.au>

My first thought was that all it looked a bit complicated for something that should be straightforward.

I created a file called t.txt. I worked out the way I would have done it and then I tested to see which was fastest. One little hiccup is that the two objects are not identical and I though they would be. Of course I could have made a typo somewhere. But then there may be something I have not come across. Guess it's time to see what identical really means.

> system.time({
+ file <- read.csv("t.txt",header=F,
+                     col.names =c("c_field_1",
+                                 "n_field_2",
+                                 "d_field_3",
+                                 "d_field_4",
+                                 "n_field_5"),
+                      colClasses = c("character",
+                                   "numeric",
+                                   "character",
+                                   "character",
+                                   "numeric")
+ )
+ file$d_field_3 <- as.POSIXct(strptime(file$d_field_3,format="%m/%d/%Y" ))
+ file$d_field_4 <- as.POSIXct(strptime(file$d_field_4,format="%m/%d/%Y %I:%M:%S %p" ))
+  })
[1] 0.00 0.00 0.02   NA   NA
>  
> 
> 
> read_file <- function(file,nrows=-1) {
+ 
+    # create temp classes
+    setClass("t_class_",representation("character"))
+    setAs("character", "t_class_", function(from)
+ as.POSIXct(strptime(from,format="%m/%d/%Y")))
+ 
+    setClass("t_class2_", representation("character"))
+    setAs("character", "t_class2_", function(from)
+ as.POSIXct(strptime(from,format="%m/%d/%Y %I:%M:%S %p")))
+ 
+    # read the file
+    file <- read.csv(file,
+                     header=FALSE,
+                     comment.char = "",
+                     nrows=nrows,
+                     as.is=FALSE,
+                     col.names=c("c_field_1",
+                                 "n_field_2",
+                                 "d_field_3",
+                                 "d_field_4",
+                                 "n_field_5"),
+                      colClasses=c("character",
+                                   "numeric",
+                                   "t_class_",
+                                   "t_class2_",
+                                   "numeric")
+                      )
+ 
+    # remove them now that we are done with them
+    removeClass("t_class_")
+    removeClass("t_class2_")
+ 
+    return(file)
+ 
+ }
> system.time(file2 <- read_file("t.txt"))
[1] 0.14 0.00 0.16   NA   NA
> 
> identical(file, file2)
[1] FALSE
> 
> file
  c_field_1 n_field_2  d_field_3           d_field_4 n_field_5
1       MHK     76.53 2004-05-21 2004-05-04 16:00:00        60
2       MHK     76.53 2004-06-21 2004-05-05 16:00:00        60
3       MHK     76.53 2004-07-21 2004-05-06 16:00:00        65
4       MHK     76.53 2004-08-21 2004-05-07 16:00:00        65
5       MHK     76.53 2004-09-21 2004-05-08 16:00:00        70
> file2
  c_field_1 n_field_2  d_field_3           d_field_4 n_field_5
1       MHK     76.53 2004-05-21 2004-05-04 16:00:00        60
2       MHK     76.53 2004-06-21 2004-05-05 16:00:00        60
3       MHK     76.53 2004-07-21 2004-05-06 16:00:00        65
4       MHK     76.53 2004-08-21 2004-05-07 16:00:00        65
5       MHK     76.53 2004-09-21 2004-05-08 16:00:00        70
> str(file)
`data.frame':   5 obs. of  5 variables:
 $ c_field_1: chr  "MHK" "MHK" "MHK" "MHK" ...
 $ n_field_2: num  76.5 76.5 76.5 76.5 76.5
 $ d_field_3:`POSIXct', format: chr  "2004-05-21" "2004-06-21" "2004-07-21" "2004-08-21" ...
 $ d_field_4:`POSIXct', format: chr  "2004-05-04 16:00:00" "2004-05-05 16:00:00" "2004-05-06 16:00:00" "2004-05-07 16:00:00" ...
 $ n_field_5: num  60 60 65 65 70
> str(file2)
`data.frame':   5 obs. of  5 variables:
 $ c_field_1: chr  "MHK" "MHK" "MHK" "MHK" ...
 $ n_field_2: num  76.5 76.5 76.5 76.5 76.5
 $ d_field_3:`POSIXct', format: chr  "2004-05-21" "2004-06-21" "2004-07-21" "2004-08-21" ...
 $ d_field_4:`POSIXct', format: chr  "2004-05-04 16:00:00" "2004-05-05 16:00:00" "2004-05-06 16:00:00" "2004-05-07 16:00:00" ...
 $ n_field_5: num  60 60 65 65 70
> 

> -----Original Message-----
> From: Charles and Kimberly Maner [mailto:ckjmaner at carolina.rr.com]
> Sent: Tuesday, 8 February 2005 12:08 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] RE: Reading Dates in a csv File
> 
> 
> 
> Hi all.  Thanks for all of your help/suggestions.  I found an 
> old email in
> the R-help archives, pieced together a couple things and 
> arrived at the
> solution below.  As an additional followup, I thought I would 
> go ahead and
> post it should other readers come across this same situation. 
>  Here goes..
> 
> Raw data:
> MHK,76.53,05/21/2004,5/4/2004 4:00:00 PM,60
> MHK,76.53,06/21/2004,5/5/2004 4:00:00 PM,60
> MHK,76.53,07/21/2004,5/6/2004 4:00:00 PM,65
> MHK,76.53,08/21/2004,5/7/2004 4:00:00 PM,65
> MHK,76.53,09/21/2004,5/8/2004 4:00:00 PM,70
> 
> Code:
> read_file <- function(file,nrows=-1) {
> 
>    # create temp classes
>    setClass("t_class_",representation("character"))
>    setAs("character", "t_class_", function(from)
> as.POSIXct(strptime(from,format="%m/%d/%Y")))
>   
>    setClass("t_class2_", representation("character"))
>    setAs("character", "t_class2_", function(from)
> as.POSIXct(strptime(from,format="%m/%d/%Y %I:%M:%S %p")))
> 
>    # read the file
>    file <- read.csv(file,
>                     header=FALSE,
>                     comment.char = "",
>                     nrows=nrows,
>                     as.is=FALSE,
>                     col.names=c("c_field_1",
>                                 "n_field_2",
>                                 "d_field_3",
>                                 "d_field_4",
>                                 "n_field_5),
>                      colClasses=c("character",
>                                   "numeric",
>                                   "t_class_",
>                                   "t_class2_",
>                                   "numeric")
>                      )
> 
>    # remove them now that we are done with them
>    removeClass("t_class_")
>    removeClass("t_class2_")
> 
>    return(file)
> 
> }
> 
> If any of you folks know a better way and/or have 
> comments/enhancements to
> this code, feel free to post/email your critique.
> 
> 
> Thanks,
> Charles
> 
> 
> 
> 
> > _____________________________________________ 
> > From: 	Charles and Kimberly Maner 
> [mailto:ckjmaner at carolina.rr.com]
> > 
> > Sent:	Thursday, February 03, 2005 8:35 AM
> > To:	'r-help at stat.math.ethz.ch'
> > Subject:	Reading Dates in a csv File
> > 
> > 
> > Hi all.  I'm reading in a flat, comma-delimited flat file 
> using read.csv.
> > It works marvelously for the most part.  I am using the colClasses
> > argument to, basically, create numeric, factor and 
> character classes for
> > the columns I'm reading in.  However, a couple of the 
> fields in the file
> > are date fields.  I'm fully aware that POSIXct can be used 
> as a class,
> > however the field must obey, (I think), the standard/default POSIXct
> > format.  Hence the following question:  Does anyone have a 
> method they can
> > share to read in a non-standard formatted date to convert 
> to POSIXct?  I
> > can read it in then convert it, but that's a two pass 
> approach and not as
> > elegant as a single pass through read.csv.  I've read, from the
> > documentation, that "[o]therwise there needs to be an as 
> method (from
> > package methods) for conversion from "character" to the 
> specified formal
> > class" but I do not know and have not figured out how to do that.
> > 
> > Any suggestion(s) would be greatly appreciated.
> > 
> > 
> > Thanks,
> > Charles
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From blindglobe at gmail.com  Tue Feb  8 07:06:41 2005
From: blindglobe at gmail.com (A.J. Rossini)
Date: Tue, 8 Feb 2005 07:06:41 +0100
Subject: [R] R on Beowulf cluster?
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E673@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E673@usrymx25.merck.com>
Message-ID: <1abe3fa9050207220613a8ca25@mail.gmail.com>

That's a shell/R script issue.  I've come across it before, but I've
no longer got access to any clusters of that form (yet :-).

Depending on the jobs, it's not clear to me that you want a truly
interactive process on a subnode, but the idea is the same as the
chicanery we used to have a remote R process.

best,
-tony



On Mon, 7 Feb 2005 14:59:33 -0500, Liaw, Andy <andy_liaw at merck.com> wrote:
> Dear R-help,
> 
> Has anyone tried running R on a Beowulf-type cluster?  I can get R to run in
> batch (using R CMD BATCH) on a cluster, but am wondering if it is possible
> to get an interactive R session on a compute node.  Right now, if I run:
> 
>   beorun --nolocal R
> 
> I just get the R start-up message and back to the shell prompt.  If I try
> 
>   bpsh 0 R
> 
> I can get R started (but the R prompt does not appear) and do computations,
> but no access to an x11() device.  I'd very much appreciate any pointer.
> 
> Best,
> Andy
> 
> Andy Liaw, PhD
> Biometrics Research      PO Box 2000, RY33-300
> Merck Research Labs           Rahway, NJ 07065
> andy_liaw <at> merck.com          732-594-0820
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
best,
-tony

"Commit early,commit often, and commit in a repository from which we can easily
roll-back your mistakes" (AJR, 4Jan05).

A.J. Rossini
blindglobe at gmail.com



From mkondrin at hppi.troitsk.ru  Tue Feb  8 16:29:01 2005
From: mkondrin at hppi.troitsk.ru (mkondrin@hppi.troitsk.ru)
Date: Tue, 08 Feb 2005 18:29:01 +0300
Subject: [R] R on Beowulf cluster?
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E673@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E673@usrymx25.merck.com>
Message-ID: <4208DABD.2040104@hppi.troitsk.ru>

Hello!
Sorry, may be I will just say something trivial, but:

>I can get R started (but the R prompt does not appear) and do computations,
>but no access to an x11() device.  
>  
>
looks like you have started R with --slave switch and remote console has 
no DISPLAY environment variable set (or your X-server discards remote 
connections).
Hope this help.



From hellik at web.de  Tue Feb  8 07:39:42 2005
From: hellik at web.de (Helmut Kudrnovsky)
Date: Tue, 08 Feb 2005 07:39:42 +0100
Subject: [R] Data manipulation
Message-ID: <1703150541@web.de>

Content-Type: text/plain; charset="iso-8859-1"
Received-SPF: none (hypatia: domain of hellik at web.de does not designate permitted sender hosts)
X-Virus-Scanned: by amavisd-new at stat.math.ethz.ch
Content-Transfer-Encoding: 8bit
X-MIME-Autoconverted: from quoted-printable to 8bit by hypatia.math.ethz.ch id j186djX0017423
X-Spam-Checker-Version: SpamAssassin 3.0.2 (2004-11-16) on hypatia.math.ethz.ch
X-Spam-Level: 
X-Spam-Status: No, score=-1.0 required=5.0 tests=AWL,BAYES_50 autolearn=no version=3.0.2

Hi R-friends,

i have large dataset in the following structure:

BID;TERRCODE;ANMCODE
200310413290002;4;0
200310413290002;80;0
200310413290002;2;0
200310413290002;5;0
200310413290003;3;0
200310413290003;1;0
200310413290003;11;0
200310413290003;26;0
200310413290003;141;21
200310413290003;472;0
200310413290004;3;0
200310413290004;1;0
200310413290004;7;0
200310413290004;18;0
200310413290004;51;0
200310413290004;56;0
200310413290004;57;0
200310413290004;76;0
200310413290004;89;0
200310413290004;97;0
200310413290004;98;0
200310413290004;72;0
200310413290004;456;0
200310413290004;141;0
200310413290004;640;0
200310413290004;201;0
200310413290004;764;20
200310413290005;273;22
200310413290005;456;0
200310413290005;22;0
200310413290005;23;0
200310413290005;21;21
200310413290005;141;0
200310413290005;640;0
200310413290005;201;0
200310413290005;43;0
200310413290005;650;0
200310413290005;472;0
200310413290006;456;0
200310413290006;22;25
200310413290006;23;25
200310413290006;21;25
200310413290006;640;0
200310413290006;201;0
200310413290006;43;0
200310413290006;651;1
.
.
.

BID is the code of my sample-area
TERRCODE is the code for landscape characteristic for example: 640 ... sun exposed, .....
ANMCODE ist the value of the TERRCODE: for example 0 means "occuring", 1 means "often occuring", ......

Now my question: is it possible to get a table with the folllowing structure: 


BID (TERRCODE)4 .... (TERRCODE)21 ......
200310413290002 (ANMCODE)0 .... (ANMCODE)0 .......
200310413290003 0 .... 0 ......
200310413290004 0 .... 0 ......
200310413290005 0 .... 21 ......
200310413290006 0 ..... 25 ......
.
.

in this example (TERRCODE) and (ANMCODE) is only for explanation and not necessary for further analysis


greetings from the snowy tyrol

helli

platform i386-pc-mingw32
arch i386 
os mingw32 
system i386, mingw32 
status 
major 2 
minor 0.0 
year 2004 
month 10 
day 04 
language R

______________________________________________________________
Verschicken Sie romantische, coole und witzige Bilder per SMS!



From ligges at statistik.uni-dortmund.de  Tue Feb  8 08:40:49 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 08 Feb 2005 08:40:49 +0100
Subject: [R] ROracle problem.
In-Reply-To: <20050207203424.6753.qmail@web30506.mail.mud.yahoo.com>
References: <20050207203424.6753.qmail@web30506.mail.mud.yahoo.com>
Message-ID: <42086D01.9090107@statistik.uni-dortmund.de>

Tae Sik Han wrote:
> ROracle ploblem
> 
> I can't install ROracle in the window XP. Is there any
> way to install ROracle package on the windows OS ? 

You can both, install from sources as descibed in the docs, as well as 
downloading a binary version from  the maintainer's side:
http://stat.bell-labs.com/RS-DBI/download/index.html

Uwe Ligges

 > I
> think I can enhance data acquisition performance using
> oracle native dirver instead of using ODBC.
> 
> Tae Sik Han.
> North Carolina State University.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Tue Feb  8 08:44:44 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 08 Feb 2005 08:44:44 +0100
Subject: [R] Data manipulation
In-Reply-To: <1703150541@web.de>
References: <1703150541@web.de>
Message-ID: <42086DEC.9080203@statistik.uni-dortmund.de>

Helmut Kudrnovsky wrote:

> Content-Type: text/plain; charset="iso-8859-1"
> Received-SPF: none (hypatia: domain of hellik at web.de does not designate permitted sender hosts)
> X-Virus-Scanned: by amavisd-new at stat.math.ethz.ch
> Content-Transfer-Encoding: 8bit
> X-MIME-Autoconverted: from quoted-printable to 8bit by hypatia.math.ethz.ch id j186djX0017423
> X-Spam-Checker-Version: SpamAssassin 3.0.2 (2004-11-16) on hypatia.math.ethz.ch
> X-Spam-Level: 
> X-Spam-Status: No, score=-1.0 required=5.0 tests=AWL,BAYES_50 autolearn=no version=3.0.2
> 
> Hi R-friends,
> 
> i have large dataset in the following structure:
> 
> BID;TERRCODE;ANMCODE
> 200310413290002;4;0
> 200310413290002;80;0
> 200310413290002;2;0
> 200310413290002;5;0
> 200310413290003;3;0
> 200310413290003;1;0
> 200310413290003;11;0
> 200310413290003;26;0
> 200310413290003;141;21
> 200310413290003;472;0
> 200310413290004;3;0
> 200310413290004;1;0
> 200310413290004;7;0
> 200310413290004;18;0
> 200310413290004;51;0
> 200310413290004;56;0
> 200310413290004;57;0
> 200310413290004;76;0
> 200310413290004;89;0
> 200310413290004;97;0
> 200310413290004;98;0
> 200310413290004;72;0
> 200310413290004;456;0
> 200310413290004;141;0
> 200310413290004;640;0
> 200310413290004;201;0
> 200310413290004;764;20
> 200310413290005;273;22
> 200310413290005;456;0
> 200310413290005;22;0
> 200310413290005;23;0
> 200310413290005;21;21
> 200310413290005;141;0
> 200310413290005;640;0
> 200310413290005;201;0
> 200310413290005;43;0
> 200310413290005;650;0
> 200310413290005;472;0
> 200310413290006;456;0
> 200310413290006;22;25
> 200310413290006;23;25
> 200310413290006;21;25
> 200310413290006;640;0
> 200310413290006;201;0
> 200310413290006;43;0
> 200310413290006;651;1
> .
> .
> .
> 
> BID is the code of my sample-area
> TERRCODE is the code for landscape characteristic for example: 640 ... sun exposed, .....
> ANMCODE ist the value of the TERRCODE: for example 0 means "occuring", 1 means "often occuring", ......
> 
> Now my question: is it possible to get a table with the folllowing structure: 
> 
> 
> BID (TERRCODE)4 .... (TERRCODE)21 ......
> 200310413290002 (ANMCODE)0 .... (ANMCODE)0 .......
> 200310413290003 0 .... 0 ......
> 200310413290004 0 .... 0 ......
> 200310413290005 0 .... 21 ......
> 200310413290006 0 ..... 25 ......


Perhaps, if you explain us the formula you derive those lines from the 
data above. At least I don't understand it.

Uwe Ligges


> .
> 
> in this example (TERRCODE) and (ANMCODE) is only for explanation and not necessary for further analysis
> 
> 
> greetings from the snowy tyrol
> 
> helli
> 
> platform i386-pc-mingw32
> arch i386 
> os mingw32 
> system i386, mingw32 
> status 
> major 2 
> minor 0.0 
> year 2004 
> month 10 
> day 04 
> language R
> 
> ______________________________________________________________
> Verschicken Sie romantische, coole und witzige Bilder per SMS!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Helene.Dryssens at eleves.polytech-lille.fr  Tue Feb  8 08:44:40 2005
From: Helene.Dryssens at eleves.polytech-lille.fr (Helene.Dryssens@eleves.polytech-lille.fr)
Date: Tue,  8 Feb 2005 08:44:40 +0100
Subject: [R] problem with logistic regression
In-Reply-To: <x2k6pkxn9o.fsf@biostat.ku.dk>
References: <1107793194.4207952a37db5@webmail.polytech-lille.fr>
	<x2k6pkxn9o.fsf@biostat.ku.dk>
Message-ID: <1107848680.42086de81ea65@webmail.polytech-lille.fr>

You're right!!!
Thank you very much!!!!

Selon Peter Dalgaard <p.dalgaard at biostat.ku.dk>:

> Helene.Dryssens at eleves.polytech-lille.fr writes:
> 
> > Hi,
> > 
> > we try to do a logistic regression with the function glm.
> > But we notice that this function don't give the same results as the SAS
> proc
> > catmod (differents estimate given).
> > We try to change the contrast on R system with:
> > > options(contrasts=c(unordered="contr.SAS",ordered="contr.poly"))
> > 
> > We also try with brlr and logistf functions.
> > Unfortunately, the estimate aren't still the same.
> 
> contr.SAS is something of a contradiction in terms since SAS does not
> use the same type of contrast in all procedures. I believe PROC CATMOD
> is using sum contrasts (contr.sum), whereas contr.SAS gives you
> something similar to PROC GENMOD (last level set to zero).
> 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907
> 
>



From r.hankin at soc.soton.ac.uk  Tue Feb  8 09:48:44 2005
From: r.hankin at soc.soton.ac.uk (Robin Hankin)
Date: Tue, 8 Feb 2005 08:48:44 +0000
Subject: [R] proportional matrix rows
In-Reply-To: <x2u0ooxqwl.fsf@biostat.ku.dk>
References: <e1873c31b3836a68eb0e9532b87d6733@soc.soton.ac.uk>
	<loom.20050207T135808-319@post.gmane.org>
	<x2u0ooxqwl.fsf@biostat.ku.dk>
Message-ID: <0f1e3bb43e5390e4be816e22eb6d5fda@soc.soton.ac.uk>

Hi Peter, Gabor

On Feb 7, 2005, at 04:42 pm, Peter Dalgaard wrote:

> Gabor Grothendieck <ggrothendieck at myway.com> writes:
>
>> Robin Hankin <r.hankin <at> soc.soton.ac.uk> writes:

>> : I want a diagnostic that detects whether a row is a multiple of
>> : the first row or not.

> ..and in that case it might be easier just to calculate the dot
> product with the orthogonal of mat[1,]:
>
>> z <- jj[1,]
>> jj %*% c(z[1], -z[2])
>       [,1]
>  [1,]    0
>  [2,]    0
>  [3,]    1
>  [4,]    1
>  [5,]    1
>  [6,]    1
>  [7,]    1
>  [8,]    0
>  [9,]    1
> [10,]    1
>
> (Notice that the value "1" just comes from Robin being singularly bad
> at selecting "arbitrary" rows that are not proportional to the first
> one!)
>

Heh.  I spent some considerable time pondering this and realized that 
if one
views jj as a bunch of two-by two matrices  jj[c(a,b),]  then in my 
application,
many of these matrices are either singular or unimodular (that is, a 
matrix with a
determinant of one).

Your insight above is is VERY VERY helpful

Thank you.


--
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From pkhomski at wiwi.uni-bielefeld.de  Tue Feb  8 10:20:03 2005
From: pkhomski at wiwi.uni-bielefeld.de (Pavel Khomski)
Date: Tue, 08 Feb 2005 10:20:03 +0100
Subject: [R] lme4 --> GLMM
Message-ID: <42088443.2090206@wiwi.uni-bielefeld.de>

hello!

this is a question, how can i specify the random part in the GLMM-call 
(of the lme4 library) for compound matrices just in the the same way as
they defined in the lme-Call (of the nlme library).  For example

i would just need             
random=list(my.Subject=pdBlocked(list(pdIdent(~... , ...),pdIdent(~... , 
...))))

this specification , if i also attach   library(nlme) ,   is not 
accepted in the GLMM-call, though the simple form

                                                      
random=list(my.Subject=pdIdent(~...,...))

is accepted.

what is the analogous of pdBlocked  & Co  from nlme in  lme4?


thanks  for  replay



From Rau at demogr.mpg.de  Tue Feb  8 10:55:01 2005
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Tue, 8 Feb 2005 10:55:01 +0100
Subject: [R] simple example of C interface to R
Message-ID: <8B08A3A1EA7AAC41BE24C750338754E6150705@HERMES.demogr.mpg.de>

Hi, 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of jbdunsmo at utmb.edu
Sent: Friday, February 04, 2005 10:40 PM
To: Roger Bivand
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] simple example of C interface to R

On Fri, Feb 04, 2005 at 09:09:37PM +0100, Roger Bivand wrote:
>
> Well, it is documented in the Writing R Extensions manual:
> 
>
http://cran.r-project.org/doc/manuals/R-exts.html#System-and-foreign-lan
guage-interfaces
> 

thanks.  reading through that a second time made all the difference.
i still think a complete example (a very simple C program that uses R
and compiles without errors) would be good to have in the
documentation.

jason

do you know already the page of Roger D. Peng?
He has a document entitled "An Introduction to the .C Interface to R".
It is located at:
http://www.biostat.jhsph.edu/~rpeng/docs/interface.pdf

Does this help you?

Best,
Roland


+++++
This mail has been sent through the MPI for Demographic Rese...{{dropped}}



From B.Rowlingson at lancaster.ac.uk  Tue Feb  8 11:06:43 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 08 Feb 2005 10:06:43 +0000
Subject: [R] R on Beowulf cluster?
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E674@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E674@usrymx25.merck.com>
Message-ID: <42088F33.7030609@lancaster.ac.uk>

Liaw, Andy wrote:

> Thanks for the reply.  I'm aware of almost all of those things you
> mentioned, and have played with some of them.  I'm not looking to do
> distributed computing within R at this point.  What I'm after, though, is
> not really addressed by any of them.  I want a user to be able to start an
> interactive R session on a compute node that the system determines
> (dynamically) to be most available at the time.  That's what `beorun' does,
> but I couldn't get it to run R interactively.
> 

  As an alternative suggestion, have you thought of using OpenMOSIX 
instead of your Beowulf tools? With an OpenMOSIX cluster your process 
runs on whichever processor is best for it, and jumps around as CPU 
availability within the cluster changes. If you've got a cluster with 
slow and fast processors, jobs will run on the fast processors first, no 
matter where the user is logged in.

  You can also add machines to the cluster without having to install 
software on them - system calls are made back on the initial machine so 
the process always sees the files it had there. Its like plugging in a 
new CPU.

  We've been running an OpenMOSIX cluster for a few years now, and apart 
from the odd problem, it works really well. Older versions of R had 
trouble migrating, but anything above 1.7 quickly finds itself a nice 
zippy host and uses that.

  Note this is x86 processor only, since it's a set of patches to the 
linux kernel.

Baz



From Helene.Dryssens at eleves.polytech-lille.fr  Tue Feb  8 11:15:35 2005
From: Helene.Dryssens at eleves.polytech-lille.fr (Helene.Dryssens@eleves.polytech-lille.fr)
Date: Tue,  8 Feb 2005 11:15:35 +0100
Subject: [R] logistic regression
Message-ID: <1107857735.4208914793ba6@webmail.polytech-lille.fr>

Hi,

I'm using glm function to do logistic regression and now I want to know if it
exists a kind of R-squared with this function in order to check the model.

Thank you.



From uleopold at science.uva.nl  Tue Feb  8 11:22:57 2005
From: uleopold at science.uva.nl (Ulrich Leopold)
Date: 08 Feb 2005 11:22:57 +0100
Subject: [R] rename object
Message-ID: <1107858177.6088.12.camel@snowdon.science.uva.nl>

Dear list,

I would like to rename an object as follows:

SimLUall <- matrix(c(0,1,0,0, 1,0,0,0, 0,0,1,0, 0,0,0,1),nrow=4, ncol=4)

j <- 2

SimLUall2 <- SimLUall and j

The value of j should be assigned automatically to SimLUall.

How can I achieve this?

Regards, 
Ulrich



From ligges at statistik.uni-dortmund.de  Tue Feb  8 11:43:45 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 08 Feb 2005 11:43:45 +0100
Subject: [R] rename object
In-Reply-To: <1107858177.6088.12.camel@snowdon.science.uva.nl>
References: <1107858177.6088.12.camel@snowdon.science.uva.nl>
Message-ID: <420897E1.3050100@statistik.uni-dortmund.de>

Ulrich Leopold wrote:

> Dear list,
> 
> I would like to rename an object as follows:
> 
> SimLUall <- matrix(c(0,1,0,0, 1,0,0,0, 0,0,1,0, 0,0,0,1),nrow=4, ncol=4)
> 
> j <- 2
> 
> SimLUall2 <- SimLUall and j

Not absolutely sure what you are going to do, I guess either

  SimLUall2 <- SimLUall
  SimLUall <- j

or

  assign(paste("SimuLUall", j, sep=""), SimuLUall)


Uwe Ligges


> The value of j should be assigned automatically to SimLUall.
> 
> How can I achieve this?
> 
> Regards, 
> Ulrich
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From lecoutre at stat.ucl.ac.be  Tue Feb  8 11:36:29 2005
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Tue, 08 Feb 2005 11:36:29 +0100
Subject: [R] rename object
In-Reply-To: <1107858177.6088.12.camel@snowdon.science.uva.nl>
References: <1107858177.6088.12.camel@snowdon.science.uva.nl>
Message-ID: <6.0.1.1.2.20050208113259.032ceec0@stat4ux.stat.ucl.ac.be>

Hi Ulrich

See:
? assign   and
? paste

with something like:
 > assign(paste("SimLUall",j,sep=""),SimLUall)

Note though that if you are planning some simulations, many guRus would say 
that you should use a list.

nsimul <- 100
simOut <- vector(mode="list",length=nsimul)
names(simOut) <- paste("simu",1:nsimul,sep="")

for (i in 1:nsimul){
         ###
         simOut[i]<- resultsofsimulation.i
         ###
}

Eric

At 11:22 8/02/2005, Ulrich Leopold wrote:
>Dear list,
>
>I would like to rename an object as follows:
>
>SimLUall <- matrix(c(0,1,0,0, 1,0,0,0, 0,0,1,0, 0,0,0,1),nrow=4, ncol=4)
>
>j <- 2
>
>SimLUall2 <- SimLUall and j
>
>The value of j should be assigned automatically to SimLUall.
>
>How can I achieve this?
>
>Regards,
>Ulrich
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Eric Lecoutre
UCL /  Institut de Statistique
Voie du Roman Pays, 20
1348 Louvain-la-Neuve
Belgium

tel: (+32)(0)10473050
lecoutre at stat.ucl.ac.be
http://www.stat.ucl.ac.be/ISpersonnel/lecoutre

If the statistics are boring, then you've got the wrong numbers. -Edward 
Tufte



From maechler at stat.math.ethz.ch  Tue Feb  8 11:49:39 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 8 Feb 2005 11:49:39 +0100
Subject: lme4 "package" etc {Re: [R] lme4 --> GLMM}
In-Reply-To: <42088443.2090206@wiwi.uni-bielefeld.de>
References: <42088443.2090206@wiwi.uni-bielefeld.de>
Message-ID: <16904.39235.910508.75875@stat.math.ethz.ch>

>>>>> "Pavel" == Pavel Khomski <pkhomski at wiwi.uni-bielefeld.de>
>>>>>     on Tue, 08 Feb 2005 10:20:03 +0100 writes:


    Pavel> this is a question, how can i specify the random part
    Pavel> in the GLMM-call (of the lme4 library) for compound
    Pavel> matrices just in the the same way as they defined in
    Pavel> the lme-Call (of the nlme library).  

``twice in such a short paragraph -- yikes !!'' ... I'm getting
convulsive...

There is NO lme4 library nor an nlme one !
There's the lme4 *PACKAGE* and the nlme *PACKAGE* -- please --

(If the nlme package is built, it will rely on a nlme.so or
 nlme.dll (or nlme.dylib ...) *library* of compiled C code,
 and if packages are installed,
 they are installed into a library of packages; 
 typically into one of the libraries in .libPaths()
)

whooh.... Martin



From lecoutre at stat.ucl.ac.be  Tue Feb  8 12:02:29 2005
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Tue, 08 Feb 2005 12:02:29 +0100
Subject: lme4 "package" etc {Re: [R] lme4 --> GLMM} | fortunes
  package
In-Reply-To: <16904.39235.910508.75875@stat.math.ethz.ch>
References: <42088443.2090206@wiwi.uni-bielefeld.de>
	<16904.39235.910508.75875@stat.math.ethz.ch>
Message-ID: <6.0.1.1.2.20050208115901.031c7ec0@stat4ux.stat.ucl.ac.be>



I guess Martin is struggling to have the most entries in the next version 
of "fortunes" ;-)
Well... I hope so! Or we should really do something for new users - letting 
any R core team member getting convulsive is dangerous for the futuRe.

Best wishes,

Eric


>    Pavel> this is a question, how can i specify the random part
>     Pavel> in the GLMM-call (of the lme4 library) for compound
>     Pavel> matrices just in the the same way as they defined in
>     Pavel> the lme-Call (of the nlme library).
>
>``twice in such a short paragraph -- yikes !!'' ... I'm getting
>convulsive...
>
>There is NO lme4 library nor an nlme one !
>There's the lme4 *PACKAGE* and the nlme *PACKAGE* -- please --
>
>(If the nlme package is built, it will rely on a nlme.so or
>  nlme.dll (or nlme.dylib ...) *library* of compiled C code,
>  and if packages are installed,
>  they are installed into a library of packages;
>  typically into one of the libraries in .libPaths()
>)
>
>whooh.... Martin

Eric Lecoutre
UCL /  Institut de Statistique
Voie du Roman Pays, 20
1348 Louvain-la-Neuve
Belgium

tel: (+32)(0)10473050
lecoutre at stat.ucl.ac.be
http://www.stat.ucl.ac.be/ISpersonnel/lecoutre

If the statistics are boring, then you've got the wrong numbers. -Edward 
Tufte



From vito_ricci at yahoo.com  Tue Feb  8 12:20:16 2005
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Tue, 8 Feb 2005 12:20:16 +0100 (CET)
Subject: [R] Re:logistic regression
Message-ID: <20050208112016.13774.qmail@web41206.mail.yahoo.com>

Hi,

I don't know if a pseudo squared R for glm exists in
any R package, but I find some interesting functions
in S mailing list:

http://www.math.yorku.ca/Who/Faculty/Monette/S-news/0422.html

Here are some functions for calculating (pseudo-)R^2,
which may be of use to some.

Rsquared <- function(object)
{
# object is an lm, glm or gam object
# Rsquared() is implemented only for glm or gam
objects with
# one of the three link/variance combinations:
# (1) log and mu (which is canonical for Poisson),
# (2) logit and mu(1-mu) (which is canonical for
binomial), or
# (3) identity and constant (which is canonical for
gaussian).
# However these link/variance pairs may have been
passed to quasi()
# to allow for overdispersion.
UseMethod("Rsquared")
}

Rsquared.lm <-
function(o)
{
R2 <- summary.lm(o)$r.squared
names(R2) <- "Rsquared"
R2
}

Rsquared.glm <-
function(o)
{ def <- family(o)$family[-1] # character vector of
link and variance
typ <- matrix(c("Logit: log(mu/(1 - mu))", "Log:
log(mu)", "Identity: mu",
"Binomial: mu(1-mu)", "Identity: mu", "Constant: 1" ),
2, 3, byrow=T) #
# typ is matrix of supported link and variance
combinations
ml <- match(def[1],typ[1,], nomatch=-1)
mv <- pmatch(def[2],typ[2,], nomatch=-1)
if ( (ml != mv) || (ml <1 ) )
stop("Implemented only for canonical links for
gaussian, binomial or poisson
(with optional provision for overdispersion using
quasi)")
if (ml == 3)
return(Rsquared.lm(o)) # gaussian family
# Remainder of code is for binomial and poisson
families (perhaps with provision for overdispersion)
n <- length(o$residuals)
# number of observations
R2 <- (1 - exp((o$deviance - o$null.deviance)/n))/(1 -
exp( - o$null.deviance/n))
names(R2) <- "pseudo.Rsquared"
R2
}

http://www.math.yorku.ca/Who/Faculty/Monette/S-news/0408.html

Here are some functions for calculating (pseudo-)R^2,
which may be of use to some.

Rsquared <- function(o){
# o is an lm, glm or gam object
UseMethod("Rsquared")
}

Rsquared.lm _ function(o) {
R2 <- summary(o)$r.squared
names(R2) <- 'Rsquared'
R2
}

Rsquared.glm <- function(o) {
n <- length(o$residuals) # number of observations
R2 <- ( 1 - exp( (o$deviance - o$null.deviance)/n ) )
/ ( 1 - exp( -o$null.deviance/n ) )
names(R2) <- 'pseudo.Rsquared'
R2
}


I don't know if S code is completely running in R
environment.

See: 
http://www.econ.ucdavis.edu/faculty/cameron/research/je97preprint.pdf

Hoping I helped you!
you wrote:

Hi,

I'm using glm function to do logistic regression and
now I want to know if it exists a kind of R-squared
with this function in order to check the model.

Thank you.

=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box

Top 10 reasons to become a Statistician

     1. Deviation is considered normal
     2. We feel complete and sufficient
     3. We are 'mean' lovers
     4. Statisticians do it discretely and continuously
     5. We are right 95% of the time
     6. We can legally comment on someone's posterior distribution
     7. We may not be normal, but we are transformable
     8. We never have to say we are certain
     9. We are honestly significantly different
    10. No one wants our jobs


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/



From andy_liaw at merck.com  Tue Feb  8 12:25:00 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 8 Feb 2005 06:25:00 -0500
Subject: [R] R on Beowulf cluster?
Message-ID: <3A822319EB35174CA3714066D590DCD50994E679@usrymx25.merck.com>

> From: Barry Rowlingson
> 
> Liaw, Andy wrote:
> 
> > Thanks for the reply.  I'm aware of almost all of those things you
> > mentioned, and have played with some of them.  I'm not looking to do
> > distributed computing within R at this point.  What I'm 
> after, though, is
> > not really addressed by any of them.  I want a user to be 
> able to start an
> > interactive R session on a compute node that the system determines
> > (dynamically) to be most available at the time.  That's 
> what `beorun' does,
> > but I couldn't get it to run R interactively.
> > 
> 
>   As an alternative suggestion, have you thought of using OpenMOSIX 
> instead of your Beowulf tools? With an OpenMOSIX cluster your process 
> runs on whichever processor is best for it, and jumps around as CPU 
> availability within the cluster changes. If you've got a cluster with 
> slow and fast processors, jobs will run on the fast 
> processors first, no 
> matter where the user is logged in.
> 
>   You can also add machines to the cluster without having to install 
> software on them - system calls are made back on the initial 
> machine so 
> the process always sees the files it had there. Its like 
> plugging in a 
> new CPU.
> 
>   We've been running an OpenMOSIX cluster for a few years 
> now, and apart 
> from the odd problem, it works really well. Older versions of R had 
> trouble migrating, but anything above 1.7 quickly finds itself a nice 
> zippy host and uses that.
> 
>   Note this is x86 processor only, since it's a set of patches to the 
> linux kernel.
> 
> Baz

Baz,

Thanks for the suggestion.  oM would have been my first choice, too
(especially with Quantian).  The problem is that these are dual Opterons...

The Scyld Beowulf has many of the advantages that you mentioned: it's
trivial to bring up diskless compute nodes, and the head node can
dynamically determine which compute node has the most resource to run a job.
One thing that oM does that, AFAICT Scyld Beowulf doesn't do (at least not
automatically) is migrating from node to node, but one can argue whether
that's necessarily a good thing (and depend on how homogeneous the nodes
are).

I googled a bit more on the subject, and it seems like it's just about
impossible to get an interactive shell on a compute node on the Beowulf.  I
wonder if that also means no interactive R session...

BTW, a couple of people mentioned setting DISPLAY.  Yes, I did do that (both
using IP and host name), but alas, no luck.

Best,
Andy



From letudiant51 at voila.fr  Tue Feb  8 12:35:17 2005
From: letudiant51 at voila.fr (letudiant51@voila.fr)
Date: Tue,  8 Feb 2005 12:35:17 +0100 (CET)
Subject: [R] Memory game
Message-ID: <22134355.1107862517981.JavaMail.www@wwinf4103>

I am a student and for my R project I have to program a memory game, that is a game for the children with cards and you have to find pairs. 
Wouldn?t you have this program in your computer for make an example for me.
Thanks for helping me!!



From murdoch at stats.uwo.ca  Tue Feb  8 12:41:02 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 08 Feb 2005 11:41:02 +0000
Subject: [R] rename object
In-Reply-To: <6.0.1.1.2.20050208113259.032ceec0@stat4ux.stat.ucl.ac.be>
References: <1107858177.6088.12.camel@snowdon.science.uva.nl>
	<6.0.1.1.2.20050208113259.032ceec0@stat4ux.stat.ucl.ac.be>
Message-ID: <029h019kpvqsltthb1nb5f0mcqjb8smvbq@4ax.com>

On Tue, 08 Feb 2005 11:36:29 +0100, Eric Lecoutre
<lecoutre at stat.ucl.ac.be> wrote :

>Note though that if you are planning some simulations, many guRus would say 
>that you should use a list.
>
>nsimul <- 100
>simOut <- vector(mode="list",length=nsimul)
>names(simOut) <- paste("simu",1:nsimul,sep="")
>
>for (i in 1:nsimul){
>         ###
>         simOut[i]<- resultsofsimulation.i
>         ###
>}

Be careful with those brackets:  that should almost certtainly be
simOut[[i]], putting resultsofsimulation.i into the i'th position of
simOut.

Your syntax would put resultsofsimulation.i[[1]] into that place, and
that's not usually what one wants.

Duncan Murdoch



From bates at stat.wisc.edu  Tue Feb  8 12:43:32 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 08 Feb 2005 05:43:32 -0600
Subject: lme4 "package" etc {Re: [R] lme4 --> GLMM}
In-Reply-To: <16904.39235.910508.75875@stat.math.ethz.ch>
References: <42088443.2090206@wiwi.uni-bielefeld.de>
	<16904.39235.910508.75875@stat.math.ethz.ch>
Message-ID: <4208A5E4.8080709@stat.wisc.edu>

Martin Maechler wrote:
>>>>>>"Pavel" == Pavel Khomski <pkhomski at wiwi.uni-bielefeld.de>
>>>>>>    on Tue, 08 Feb 2005 10:20:03 +0100 writes:
> 
> 
> 
>     Pavel> this is a question, how can i specify the random part
>     Pavel> in the GLMM-call (of the lme4 library) for compound
>     Pavel> matrices just in the the same way as they defined in
>     Pavel> the lme-Call (of the nlme library).  

The GLMM function in the lme4 package allows you to specify crossed 
random effects within the random argument without the need for the 
pdBlocked and pdIdent constructions.  Simply ensure that your grouping 
factors are defined in such a way that each distinct group has a 
different level in the grouping factor (this is usually not a problem 
for crossed grouping factors but can be a problem with nested factors) 
and list them.  For example

   random = list(rows = ~ 1, columns = ~1)



From ccleland at optonline.net  Tue Feb  8 12:56:38 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Tue, 08 Feb 2005 06:56:38 -0500
Subject: [R] logistic regression
In-Reply-To: <1107857735.4208914793ba6@webmail.polytech-lille.fr>
References: <1107857735.4208914793ba6@webmail.polytech-lille.fr>
Message-ID: <4208A8F6.90401@optonline.net>

Terry Elrod posted functions for (pseudo-)R^2 on S-news back in 1998:

http://www.biostat.wustl.edu/archives/html/s-news/1998-02/msg00267.html

I am not sure that it performs well as a check of the model, so you 
might want to investigate that further.

Helene.Dryssens at eleves.polytech-lille.fr wrote:
> I'm using glm function to do logistic regression and now I want to know if it
> exists a kind of R-squared with this function in order to check the model.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From danbebber at yahoo.co.uk  Tue Feb  8 13:18:19 2005
From: danbebber at yahoo.co.uk (Dan Bebber)
Date: Tue, 8 Feb 2005 12:18:19 +0000 (GMT)
Subject: [R] logistic regression
In-Reply-To: <200502081138.j18BOo8s017782@hypatia.math.ethz.ch>
Message-ID: <20050208121819.7150.qmail@web26305.mail.ukl.yahoo.com>

Helene,

you should read up about AIC, deviance, and deviance
residuals, then look at the summary() for your model.

Dan
________________________________
Dr. Daniel P. Bebber
Department of Plant Sciences
University of Oxford
OX1 3RB
UK

Message: 78
Date: Tue,  8 Feb 2005 11:15:35 +0100
From: Helene.Dryssens at eleves.polytech-lille.fr
Subject: [R] logistic regression
To: r-help at stat.math.ethz.ch
Cc: nathaliebouez at yahoo.fr
Message-ID:
<1107857735.4208914793ba6 at webmail.polytech-lille.fr>
Content-Type: text/plain; charset=ISO-8859-1

Hi,

I'm using glm function to do logistic regression and
now I want to know 
if it
exists a kind of R-squared with this function in order
to check the 
model.

Thank you.



From Achim.Zeileis at wu-wien.ac.at  Tue Feb  8 13:26:56 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 8 Feb 2005 13:26:56 +0100
Subject: [R] Memory game
In-Reply-To: <22134355.1107862517981.JavaMail.www@wwinf4103>
References: <22134355.1107862517981.JavaMail.www@wwinf4103>
Message-ID: <20050208132656.185ea94b.Achim.Zeileis@wu-wien.ac.at>

On Tue,  8 Feb 2005 12:35:17 +0100 (CET) letudiant51 at voila.fr wrote:

> I am a student and for my R project I have to program a memory game,
> that is a game for the children with cards and you have to find pairs.
> Wouldn?t you have this program in your computer for make an example
> for me. Thanks for helping me!!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

PLEASE do! 

  "Basic statistics and classroom homework: R-help is not intended for
   these."



From ligges at statistik.uni-dortmund.de  Tue Feb  8 13:28:54 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 08 Feb 2005 13:28:54 +0100
Subject: [R] Memory game
In-Reply-To: <22134355.1107862517981.JavaMail.www@wwinf4103>
References: <22134355.1107862517981.JavaMail.www@wwinf4103>
Message-ID: <4208B086.3090909@statistik.uni-dortmund.de>

letudiant51 at voila.fr wrote:

> I am a student and for my R project I have to program a memory game, that is a game for the children with cards and you have to find pairs. 
> Wouldn?t you have this program in your computer for make an example for me.
> Thanks for helping me!!



!!! Generally, we don't do the homework for others !!!


The idea of homework is to think about a certain problem, but not to 
copy a solution.

Uwe Ligges



> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From blindglobe at gmail.com  Tue Feb  8 13:33:23 2005
From: blindglobe at gmail.com (A.J. Rossini)
Date: Tue, 8 Feb 2005 13:33:23 +0100
Subject: Packages and Libraries (was: Re: lme4 "package" etc {Re: [R] lme4 -->
	GLMM}
In-Reply-To: <16904.39235.910508.75875@stat.math.ethz.ch>
References: <42088443.2090206@wiwi.uni-bielefeld.de>
	<16904.39235.910508.75875@stat.math.ethz.ch>
Message-ID: <1abe3fa9050208043374d362df@mail.gmail.com>

For OBVIOUS reasons, is there any chance that we could introduce
"package()" and deprecate "library()"?

(well, I'll also ask if we could deprecate "=" for assignment, but
that's hopeless).

best,
-tony


On Tue, 8 Feb 2005 11:49:39 +0100, Martin Maechler
<maechler at stat.math.ethz.ch> wrote:
> >>>>> "Pavel" == Pavel Khomski <pkhomski at wiwi.uni-bielefeld.de>
> >>>>>     on Tue, 08 Feb 2005 10:20:03 +0100 writes:
> 
>    Pavel> this is a question, how can i specify the random part
>    Pavel> in the GLMM-call (of the lme4 library) for compound
>    Pavel> matrices just in the the same way as they defined in
>    Pavel> the lme-Call (of the nlme library).
> 
> ``twice in such a short paragraph -- yikes !!'' ... I'm getting
> convulsive...
> 
> There is NO lme4 library nor an nlme one !
> There's the lme4 *PACKAGE* and the nlme *PACKAGE* -- please --
> 
> (If the nlme package is built, it will rely on a nlme.so or
> nlme.dll (or nlme.dylib ...) *library* of compiled C code,
> and if packages are installed,
> they are installed into a library of packages;
> typically into one of the libraries in .libPaths()
> )
> 
> whooh.... Martin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
best,
-tony

"Commit early,commit often, and commit in a repository from which we can easily
roll-back your mistakes" (AJR, 4Jan05).

A.J. Rossini
blindglobe at gmail.com



From f.harrell at vanderbilt.edu  Tue Feb  8 13:40:49 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 08 Feb 2005 06:40:49 -0600
Subject: [R] Re:logistic regression
In-Reply-To: <20050208112016.13774.qmail@web41206.mail.yahoo.com>
References: <20050208112016.13774.qmail@web41206.mail.yahoo.com>
Message-ID: <4208B351.4080909@vanderbilt.edu>

Vito Ricci wrote:
> Hi,
> 
> I don't know if a pseudo squared R for glm exists in
> any R package, but I find some interesting functions
> in S mailing list:

It is included in lrm in the Design package.  But note that this is not 
for checking fit but rather for quantifying predictive discrimination.

.....

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From michael.watson at bbsrc.ac.uk  Tue Feb  8 14:25:10 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Tue, 8 Feb 2005 13:25:10 -0000
Subject: [R] Drawing maps of UK
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950172CDD4@iahce2knas1.iah.bbsrc.reserved>

Hi

I have downloaded the maps package from CRAN, along with a few other
map* packages that I could find - but there didn't seem to be data on
the UK.

What I actually want to do (finally) is draw some maps of data that I
have which is organised by postcode (sort of equivalent of US Zip code
for those who don't know).  I think this may be quite ambitious, but I
wanted to see just how far I could get with R.

I have seen examples on the list (e.g.
http://www.r-project.org/nocvs/mail/r-help/2000/4480.html) giving
examples of drawing maps of the UK at county level
(map("uk",fill=T,color=pop.colour)) but this didn't work on my system -
as I don't seem to have the correct data for the UK.

Thanks in advance for your help

Mick



From dirk.enzmann at jura.uni-hamburg.de  Tue Feb  8 14:51:11 2005
From: dirk.enzmann at jura.uni-hamburg.de (Dirk Enzmann)
Date: Tue, 08 Feb 2005 14:51:11 +0100
Subject: [R] Confidence intervals for rates (dependent events)
Message-ID: <4208C3CF.4010404@jura.uni-hamburg.de>

I need advice or opinions for the following problem:

In a sample a part of the respondents has experienced victimizing 
events. Only a part of these events have been reported to the police. 
The rate of events reported to the police is

number of experienced events / number of reported events.

Because the events cannot be assumed to be independent (some victims 
have a high rate of vicitimization because they are more prone to become 
a victim) the construction of a confidence interval for the rate of 
events reported becomes difficult (to me).

Question 1: If nevertheless a use a binomial test to construct a 
confidence interval (say: 0 of 13 events reported,

binom.test(0,13,0/13)

CI = 0 to 24.7 %), is it correct that the width of this interval is a 
lower bound and thus a conservative estimate?

Question 2: (a) My intuition tells me that multilevel modeling could be 
a solution to obtain a correct confidence interval by treating the 
events and the events reported as the first level and the victims as the 
second. Is this correct and how should I specify this model?

(b) Alternatively, could I treat the events (and events reported?) as 
coming from a negative binomial distribution and use this for 
constructing a confidence interval? How can this be done technically, 
for example by using nb.glm?

Thanks in advance,
Dirk


*************************************************
Dr. Dirk Enzmann
Institute of Criminal Sciences
Dept. of Criminology
Schlueterstr. 28
D-20146 Hamburg
Germany

phone: +49-040-42838.7498 (office)
        +49-040-42838.4591 (Billon)
fax:   +49-040-42838.2344
email: dirk.enzmann at jura.uni-hamburg.de
www: 
http://www2.jura.uni-hamburg.de/instkrim/kriminologie/Mitarbeiter/Enzmann/Enzmann.html



From p.dalgaard at biostat.ku.dk  Tue Feb  8 14:57:39 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 08 Feb 2005 14:57:39 +0100
Subject: Packages and Libraries (was: Re: lme4 "package" etc {Re: [R] lme4
	--> GLMM}
In-Reply-To: <1abe3fa9050208043374d362df@mail.gmail.com>
References: <42088443.2090206@wiwi.uni-bielefeld.de>
	<16904.39235.910508.75875@stat.math.ethz.ch>
	<1abe3fa9050208043374d362df@mail.gmail.com>
Message-ID: <x2ll9zyx0s.fsf@biostat.ku.dk>

"A.J. Rossini" <blindglobe at gmail.com> writes:

> For OBVIOUS reasons, is there any chance that we could introduce
> "package()" and deprecate "library()"?

usepackage() or usePackage() has been suggested, but someone got
ambitious and wanted it to be different from library(), and it sort of
didn't get any further. We still have some time before feature freeze
for 2.1.0 though.

> (well, I'll also ask if we could deprecate "=" for assignment, but
> that's hopeless).

You're not *forced* to use it...

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From screig at hotmail.com  Tue Feb  8 15:04:26 2005
From: screig at hotmail.com (Sean Creighton)
Date: Tue, 08 Feb 2005 14:04:26 +0000
Subject: [R] Copula as measure of correlation
Message-ID: <BAY18-F40A5EC8CDA2B444045EA45C6740@phx.gbl>


Hello

   Has anybody implemented a copula correlation in R in order to correlate 
between two non-normally distributed time series.

  Thanks

    Sean



From ozric at web.de  Tue Feb  8 15:07:53 2005
From: ozric at web.de (Christian Schulz)
Date: Tue, 08 Feb 2005 15:07:53 +0100
Subject: [R] R or weka
In-Reply-To: <cdf81783050207125323e529c0@mail.gmail.com>
References: <cdf81783050207125323e529c0@mail.gmail.com>
Message-ID: <4208C7B9.5080607@web.de>

Hi,

it's dependend of your task and the amount of data.
R-project have a lot more possibilities  manipulate the data and  many 
different plot possibilities.
Weka have the nice possibilty use the experimenter which could test 
several algorithms
on different dataset's for comparison, further you find 
association-algorithms which
doesn't exist in R-project if i'm correct.
Neverthless you could program  a R-project script which doing the same
like the experimenter in weka, but it's really more work for a beginner 
and a advanced user, too.

IMHO  if you want doing really data-mining ( i.e. more than ~ 200.000 
rows and 20 attributes)
with many recodings, i recgonzie for both software-packages
the best way is doing all recodings in a database ( i.e. MySQL,RMySQL)  
and  only mine (train , test , predict etc.)
in weka or R-project, because you get crazy when you didn't have a 
machine with a lot of Speed and  RAM.

hope it helps,
regards, Christian


WeiWei Shi wrote:

>Hi, guys:
>
>These days I keep using R and Weka to do data mining. I think my next
>step is open the source codes so that I can "customrize them" and make
>them better server my purpose. But now I kinda hesitate to do so b/c I
>am really not sure which is better to start with. You know, both
>require some time and I cannot clone myself to work on both:) If here
>are some persons who used both of them, please provide some
>suggestions from your experience.
>
>Thanks,
>
>Ed
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From cervino at ifir.edu.ar  Tue Feb  8 15:06:31 2005
From: cervino at ifir.edu.ar (=?ISO-8859-1?Q?Cervi=F1o?= Beresi Ulises)
Date: Tue, 08 Feb 2005 11:06:31 -0300
Subject: [R] Toying with neural networks
Message-ID: <1107871591.16680.23.camel@spoungebob.ifir.edu.ar>

Hello all, 

Ive been playing with nnet (package 'nnet') and Ive come across this
problem. nnet doesnt seems to like to have more than 1000 weights. If I
do:

> data(iris)
> names(iris)[5] <- "species"
> net <- nnet(species ~ ., data=iris, size=124, maxit=10)
# weights:  995
initial  value 309.342009
iter  10 value 21.668435
final  value 21.668435
stopped after 10 iterations
> table(iris$species[], predict(net, iris[], type="class"))

             setosa versicolor virginica
  setosa     50      0          0
  versicolor  0     46          4
  virginica   0      0         50

It works just fine, but if I do:

> net <- nnet(species ~ ., data=iris, size=125, maxit=10)
Error in nnet.default(x, y, w, softmax = TRUE, ...) :
        Too many (1003) weights

Ive only changed 'size' from 124 to 125 giving me more than 1000
weights.

Any ideas? Im I doing something wrong?

> version
         _
platform i386-pc-linux-gnu
arch     i386
os       linux-gnu
system   i386, linux-gnu
status
major    2
minor    0.1
year     2004
month    11
day      15
language R

-- 
Ulises Cervi?o
---
I place economy among the first and most important virtues, and public
debt as
the greatest of dangers to be feared.  To preserve our independence, we
must
not let our rulers load us with perpetual debt.  If we run into such
debts, we
must be taxed in our meat and drink, in our necessities and in our
comforts,
in our labor and in our amusements.  If we can prevent the government
from
wasting the labor of the people, under the pretense of caring for them,
they
will be happy.
                -- Thomas Jefferson



From pkhomski at wiwi.uni-bielefeld.de  Tue Feb  8 15:11:18 2005
From: pkhomski at wiwi.uni-bielefeld.de (Pavel Khomski)
Date: Tue, 08 Feb 2005 15:11:18 +0100
Subject: [R] 2: lme4 ---> GLMM
Message-ID: <4208C886.6060508@wiwi.uni-bielefeld.de>

Douglas Bates wrote:

>
> The GLMM function in the lme4 package allows you to specify crossed 
> random effects within the random argument without the need for the 
> pdBlocked and pdIdent constructions.  Simply ensure that your grouping 
> factors are defined in such a way that each distinct group has a 
> different level in the grouping factor (this is usually not a problem 
> for crossed grouping factors but can be a problem with nested factors) 
> and list them.  For example
>
>   random = list(rows = ~ 1, columns = ~1)
>
>
the reason is that i actually want to use just one group. factor with 
two and more blocks, each of them would have the simple diag. structure,
just as was possible with  like pdIdent(...,...,...) specification in 
nlme-package. i also wanted to give initial values via  value=....,
so i would really need to define  i.e.:

random=list(my.Subject=pdBlocked(pdIdent(value=labda1, 
form=~var.a1+...+var.am, nam=...), pdIdent(value=labda2, 
form=~var.b1+...+var.bn, nam=...),...))

or just, because "pdBlocked" is not accepted

random=list(my.Subject=list(pdIdent(value=labda1, 
form=~var.a1+...+var.am, nam=...), pdIdent(value=labda2, 
form=~var.b1+...+var.bn, nam=...),...))

or for last, if really were possible, alternatively without  having 
attached nlme:

random=list( form=~var.a1+...+var.am | my.Subject),  
form=~var.b1+...+var.bn | my.Subject , ... )


but each time i recieve

Error in switch(mode(x), "NULL" = structure(NULL, class = "formula"),  :
   invalid formula


so i don't know, how  it can go.
the reason for trying to use the lme4 is its sparse-matrix orientation 
and so comutationally more efficient.

thank you for hint in advance



From sdavis2 at mail.nih.gov  Tue Feb  8 15:17:26 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Tue, 8 Feb 2005 09:17:26 -0500
Subject: [R] Methods overview
Message-ID: <28A45457-79DC-11D9-813A-000D933565E8@mail.nih.gov>

Sorry for the very basic, general question.... I have read over the 
methods aspects of the R Language Reference manual and some of the help 
pages (methods, setGeneric, etc.) and remain a bit confused yet about R 
methods.  Looking through the contributed docs and mail archives didn't 
give me the VERY basic starting material that I feel I am lacking.  
Could folks share with me a few links or commands of interest to get 
started?  Also, just a general question, but are S4 methods now the 
standard?

Thanks,
Sean



From v39k9 at unb.ca  Tue Feb  8 15:55:23 2005
From: v39k9 at unb.ca (Joe Nocera)
Date: Tue,  8 Feb 2005 10:55:23 -0400
Subject: [R] Re:logistic regression
In-Reply-To: <4208B351.4080909@vanderbilt.edu>
References: <20050208112016.13774.qmail@web41206.mail.yahoo.com>
	<4208B351.4080909@vanderbilt.edu>
Message-ID: <1107874523.4208d2db83d81@webmail.unb.ca>

Helene - 

In addition to some of the excellent suggestions already posited (e.g. examining AIC,
pseudo R^2 in the Design package), you might want to consider another tool to assess
logistic regression model accuracy: the area-under-curve (AUC) from a
receiver-operating characteristic (ROC).

The ROC curve describes the relationship between the number of true positives observed
(sensitivity) to false positives, and also for negatives.  The AUC is the probability
that a model can correctly distinguish between the two.  This is an appealling
alternative to some of the known issues of citing only a pseudo-R^2 (like Nagelkerke's
for instance) to describe 'fit'.

Check out the ROC functions available at the Bioconductor website.  There was also some
code sent around on the list a few months back for calculating trapeziodal AUC, se's
from ROC, and comparing two ROC curves...search the archives if interested, or I can
probably dig them out for you offline...

Cheers,
Joe

Quoting Frank E Harrell Jr <f.harrell at vanderbilt.edu>:

> Vito Ricci wrote:
> > Hi,
> > 
> > I don't know if a pseudo squared R for glm exists in
> > any R package, but I find some interesting functions
> > in S mailing list:
> 
> It is included in lrm in the Design package.  But note that this is not 
> for checking fit but rather for quantifying predictive discrimination.
> 
> .....
> 
> -- 
> Frank E Harrell Jr   Professor and Chair           School of Medicine
>                       Department of Biostatistics   Vanderbilt University
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Joseph J. Nocera
Ph.D. Candidate
NB Coop. Fish & Wildlife Research Unit
Biology Department - Univ. New Brunswick
Fredericton, NB
Canada   E3B 6E1
tel: (902) 679-5733

"Why does it have to be spiders?  Why can't it be 'follow the butterflies'"?!
    - Ron Weasley, Harry Potter & The Chamber of Secrets



From br44114 at yahoo.com  Tue Feb  8 16:08:18 2005
From: br44114 at yahoo.com (bogdan romocea)
Date: Tue, 8 Feb 2005 07:08:18 -0800 (PST)
Subject: [R] have R informed of MySQL table updates
Message-ID: <20050208150819.38772.qmail@web50303.mail.yahoo.com>

Dear useRs,

I have a script (Python) that every once in a while appends data to a
MySQL table. Meanwhile, I have a running R session, and I want it to be
aware of such table updates. I could write a loop in R to periodically
check whether new data has become available; however, are you aware of
a way to make MySQL/Python talk directly to R? I'm interested in both
GNU/Linux and Windows approaches (if any).

Thank you,
b.



From rolf at math.unb.ca  Tue Feb  8 16:28:51 2005
From: rolf at math.unb.ca (Rolf Turner)
Date: Tue, 8 Feb 2005 11:28:51 -0400 (AST)
Subject: [R] Packages and Libraries
Message-ID: <200502081528.j18FSpEW021961@erdos.math.unb.ca>


A.J. Rossini wrote:

> For OBVIOUS reasons, is there any chance that we could introduce
> "package()" and deprecate "library()"?
> 
> (well, I'll also ask if we could deprecate "=" for assignment, but
> that's hopeless).

	Amen to both suggestions.

				cheers,

					Rolf Turner
					rolf at math.unb.ca



From rolf at math.unb.ca  Tue Feb  8 16:32:44 2005
From: rolf at math.unb.ca (Rolf Turner)
Date: Tue, 8 Feb 2005 11:32:44 -0400 (AST)
Subject: [R] Methods overview
Message-ID: <200502081532.j18FWiJG022356@erdos.math.unb.ca>

Sean Davis wrote (inter alia): 

>           Also, just a general question, but are S4 methods now the 
> standard?

	My God!  I hope not!!!

				cheers,

					Rolf Turner
					rolf at math.unb.ca



From dr.mike at ntlworld.com  Tue Feb  8 16:39:08 2005
From: dr.mike at ntlworld.com (dr mike)
Date: Tue, 8 Feb 2005 15:39:08 -0000
Subject: [R] Drawing maps of UK
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950172CDD4@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <20050208153934.HFIA13480.aamta04-winn.mailhost.ntl.com@c400>

The package blighty allows you to draw the UK coastline.

Regards

Mike

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of michael watson
(IAH-C)
Sent: 08 February 2005 13:25
To: r-help at stat.math.ethz.ch
Subject: [R] Drawing maps of UK

Hi

I have downloaded the maps package from CRAN, along with a few other
map* packages that I could find - but there didn't seem to be data on the
UK.

What I actually want to do (finally) is draw some maps of data that I have
which is organised by postcode (sort of equivalent of US Zip code for those
who don't know).  I think this may be quite ambitious, but I wanted to see
just how far I could get with R.

I have seen examples on the list (e.g.
http://www.r-project.org/nocvs/mail/r-help/2000/4480.html) giving examples
of drawing maps of the UK at county level
(map("uk",fill=T,color=pop.colour)) but this didn't work on my system - as I
don't seem to have the correct data for the UK.

Thanks in advance for your help

Mick

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From tmulholland at bigpond.com  Tue Feb  8 16:40:33 2005
From: tmulholland at bigpond.com (Tom Mulholland)
Date: Tue, 08 Feb 2005 23:40:33 +0800
Subject: [R] Toying with neural networks
In-Reply-To: <1107871591.16680.23.camel@spoungebob.ifir.edu.ar>
References: <1107871591.16680.23.camel@spoungebob.ifir.edu.ar>
Message-ID: <4208DD71.107@bigpond.com>

You are not reading the help.

from the help file

      ## Default S3 method:
      nnet(x, y, weights, size, Wts, mask,
           linout = FALSE, entropy = FALSE, softmax = FALSE,
           censored = FALSE, skip = FALSE, rang = 0.7, decay = 0,
           maxit = 100, Hess = FALSE, trace = TRUE, MaxNWts = 1000,
           abstol = 1.0e-4, reltol = 1.0e-8, ...)

Look at MaxNWts



Cervi?o Beresi Ulises wrote:
> Hello all, 
> 
> Ive been playing with nnet (package 'nnet') and Ive come across this
> problem. nnet doesnt seems to like to have more than 1000 weights. If I
> do:
> 
> 
>>data(iris)
>>names(iris)[5] <- "species"
>>net <- nnet(species ~ ., data=iris, size=124, maxit=10)
> 
> # weights:  995
> initial  value 309.342009
> iter  10 value 21.668435
> final  value 21.668435
> stopped after 10 iterations
> 
>>table(iris$species[], predict(net, iris[], type="class"))
> 
> 
>              setosa versicolor virginica
>   setosa     50      0          0
>   versicolor  0     46          4
>   virginica   0      0         50
> 
> It works just fine, but if I do:
> 
> 
>>net <- nnet(species ~ ., data=iris, size=125, maxit=10)
> 
> Error in nnet.default(x, y, w, softmax = TRUE, ...) :
>         Too many (1003) weights
> 
> Ive only changed 'size' from 124 to 125 giving me more than 1000
> weights.
> 
> Any ideas? Im I doing something wrong?
> 
> 
>>version
> 
>          _
> platform i386-pc-linux-gnu
> arch     i386
> os       linux-gnu
> system   i386, linux-gnu
> status
> major    2
> minor    0.1
> year     2004
> month    11
> day      15
> language R
>



From Robert.McGehee at geodecapital.com  Tue Feb  8 17:09:18 2005
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Tue, 8 Feb 2005 11:09:18 -0500
Subject: [R] Windows Printing and Line Widths
Message-ID: <67DCA285A2D7754280D3B8E88EB5480206741E5E@MSGBOSCLB2WIN.DMN1.FMR.COM>

Hi all,
I develop and print from both Windows and Linux, and am seeing some
printing inconsistencies first described about a year and a half ago by
Andy Liaw (see below). Specifically, the line widths on my windows plots
are about 5 times smaller than that on Linux, and my windows printouts
do not match what my screen looks like. However, if I print to a pdf
file first, then I can get accurate Windows reproduction of my screen. I
was thinking of writing a windows.print() wrapper that creates a
temporary pdf file and then prints that. However, I wanted to see if a
better solution now exists to get identical printouts on both Linux and
Windows (since Andy's original post), or any comments on what printers
this does or does not affect.

Thanks,
Robert

HP Laserjet 8150DN
HP Color Laserjet 4600DN
HP Laserjet 4050TN

platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    0.1            
year     2004           
month    11             
day      15             
language R      

---------------------------------
From: Prof Brian Ripley
Date: Mon Jun 23 2003 - 23:59:29 EDT


What printer driver are you using? 


I've just tried this and it works exactly as one would expect on my HP 
970CXi, as well as cut-and-paste into other applications. It also worked

printing to Acrobat Distiller (although all the lines were thinner there

than on-screen and on the 970CXi, the ratio was still 1:5). 


We've been here before, and had to abandon some optimizations because of
a 
bug in interpreting Windows metafiles in Word. 


On Mon, 23 Jun 2003, Sundar Dorai-Raj wrote: 


> Andy, 
> I've experienced the same thing. What's interesting is that printing 
> a plot (CTRL-P) with lwd = 25 makes lines on the hardcopy look like
lwd 
> = 5. I'm using R1.7.1 on Win2000Pro. 
> 
> Regards, 
> Sundar 
> 
> Liaw, Andy wrote: 
> > Dear R-help, 
> > 
> > Has anyone notice the problem that, on Windows (NT and XP), when
printing a 
> > graph using the "File -> Print..." menu in the graphics window to
print the 
> > graph, that line width seemed to be ignored in the printed output?
For 
> > example, if I make a plot with plot(1:10, type="l", lwd=5), it looks
right 
> > on screen, but when printed out using the menu, it looks like the
plot was 
> > made with lwd=1. I've had this problem for quite a while (at least
since 
> > 1.3.x) and still present in 1.7.1. Has anyone else seen this, or
just me? 
> > 
> > Best, 
> > Andy 
> > 
> > Andy Liaw, PhD 
> > Biometrics Research PO Box 2000, RY33-300 
> > Merck Research Labs Rahway, NJ 07065 
> > mailto:andy_liaw at merck.com <mailto:andy_liaw at merck.com> 732-594-0820

> > 
> > 
> > 
> >
------------------------------------------------------------------------
------ 
> > Notice: This e-mail message, together with any attachments, cont...
{{dropped}} 
> > 
> > ______________________________________________ 
> > R-help at stat.math.ethz.ch mailing list 
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help 
> > 
> 
> ______________________________________________ 
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help 
> 


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
<http://www.stats.ox.ac.uk/%7Eripley/> 
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


Robert McGehee
Geode Capital Management, LLC
53 State Street, 5th Floor | Boston, MA | 02109
Tel: 617/392-8396    Fax:617/476-6389
mailto:robert.mcgehee at geodecapital.com



This e-mail, and any attachments hereto, are intended for us...{{dropped}}



From Christoph.Scherber at uni-jena.de  Tue Feb  8 17:19:08 2005
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Tue, 08 Feb 2005 17:19:08 +0100
Subject: [R] panel plot with log-scaled x-axis
Message-ID: <4208E67C.90907@uni-jena.de>

Dear all,

Can anyone help me plotting a panel plot with log-scaled x axes?

My formula looks like this:

coplot(response~div|treatment+grass,
panel=function(x,y,...){panel.xyplot(x,y,...);panel.lmline(x,y,...)})

And I?d like to have "div" plotted in log scale.

Thanks very much for your help!

Best regards
Christoph



From gunter.berton at gene.com  Tue Feb  8 17:32:28 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 8 Feb 2005 08:32:28 -0800
Subject: [R] Methods overview
In-Reply-To: <28A45457-79DC-11D9-813A-000D933565E8@mail.nih.gov>
Message-ID: <200502081632.j18GWStj012478@meitner.gene.com>

V&R's S PROGRAMMING is an excellent reference with a thorough discussion and
examples of S3 methods. It is somewhat less thorough about S4 methods,
perhaps because the implementation was still in process at the time of the
book's writing. Maybe BDR's online complements have more, but I haven't
checked. Nevertheless, the book is still a good place to look for info on S4
methods. I consider it an indispensable reference for those who are even
moderately into S language programming.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Sean Davis
> Sent: Tuesday, February 08, 2005 6:17 AM
> To: r-help
> Subject: [R] Methods overview
> 
> Sorry for the very basic, general question.... I have read over the 
> methods aspects of the R Language Reference manual and some 
> of the help 
> pages (methods, setGeneric, etc.) and remain a bit confused 
> yet about R 
> methods.  Looking through the contributed docs and mail 
> archives didn't 
> give me the VERY basic starting material that I feel I am lacking.  
> Could folks share with me a few links or commands of interest to get 
> started?  Also, just a general question, but are S4 methods now the 
> standard?
> 
> Thanks,
> Sean
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Roger.Bivand at nhh.no  Tue Feb  8 17:38:35 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue, 8 Feb 2005 17:38:35 +0100 (CET)
Subject: [R] Drawing maps of UK
In-Reply-To: <20050208153934.HFIA13480.aamta04-winn.mailhost.ntl.com@c400>
Message-ID: <Pine.LNX.4.44.0502081730290.21214-100000@reclus.nhh.no>

On Tue, 8 Feb 2005, dr mike wrote:

> The package blighty allows you to draw the UK coastline.

Yes, everyone outside the US gets good, free data from the US government,
like coastlines, but has to jump through hoops to get administrative area
boundary files, that in the US are simply free for downloading. My best
guess is that http://edina.ac.uk/ukborders/ and an *.ac.uk address should
get you to files with the appropriate boundaries for academic use. From
there, you will have a choice of formats (probably e00, shapefile, or
MapInfo) and matching R packages for reading the formats (RArcInfo,
maptools/shapefiles, Rmap). It may be that Barry Rowlingson has a database
of county boundaries, but it is not unlikely in Europe that such data
cannot be redistributed, sadly.

Roger

> 
> Regards
> 
> Mike
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of michael watson
> (IAH-C)
> Sent: 08 February 2005 13:25
> To: r-help at stat.math.ethz.ch
> Subject: [R] Drawing maps of UK
> 
> Hi
> 
> I have downloaded the maps package from CRAN, along with a few other
> map* packages that I could find - but there didn't seem to be data on the
> UK.
> 
> What I actually want to do (finally) is draw some maps of data that I have
> which is organised by postcode (sort of equivalent of US Zip code for those
> who don't know).  I think this may be quite ambitious, but I wanted to see
> just how far I could get with R.
> 
> I have seen examples on the list (e.g.
> http://www.r-project.org/nocvs/mail/r-help/2000/4480.html) giving examples
> of drawing maps of the UK at county level
> (map("uk",fill=T,color=pop.colour)) but this didn't work on my system - as I
> don't seem to have the correct data for the UK.
> 
> Thanks in advance for your help
> 
> Mick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From sstoddar at life.uiuc.edu  Tue Feb  8 17:40:54 2005
From: sstoddar at life.uiuc.edu (Steven T.Stoddard)
Date: Tue, 8 Feb 2005 10:40:54 -0600
Subject: [R] Using GRASS/R interface with an xy location
Message-ID: <731a05d3df6c8a53f0fd054f13aba5a0@life.uiuc.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050208/910d3d7e/attachment.pl

From tuechler at gmx.at  Tue Feb  8 17:45:11 2005
From: tuechler at gmx.at (Heinz Tuechler)
Date: Tue, 08 Feb 2005 17:45:11 +0100
Subject: [R] How to get variable names in a function?
Message-ID: <3.0.6.32.20050208174511.007abc30@pop.gmx.net>

Hello,

applying a function to a list of variables I face the following problem:
Let's say I want to compute tables for several variables. I could write a
command for every single table, like
bravo<-c(1,1,2,3,5,5,5,);charly<-c(7,7,4,4,2,1)
table(bravo); table(charly)
> table(bravo); table(charly)
bravo
1 2 3 5 
2 1 1 3 
charly
1 2 4 7 
1 1 2 2 
The results are two tables with the names of the variables above each.
If I want to do the same thing by a function I find no way to get the
variable names above the tables. 
demofn<-function(varlist)
    {for (i in seq(along=varlist))
       {cat(deparse(varlist[i])) # < - - - - how to change this?
        print(table(varlist[i]))}}
> demofn(list(bravo, charly))
list(c(1, 1, 2, 3, 5, 5, 5))
1 2 3 5 
2 1 1 3 
list(c(7, 7, 4, 4, 2, 1))
1 2 4 7 
1 1 2 2 
> 

Thanks,
Heinz T?chler



From dhkblaszyk at zeelandnet.nl  Tue Feb  8 17:43:32 2005
From: dhkblaszyk at zeelandnet.nl (dhkblaszyk@zeelandnet.nl)
Date: Tue, 8 Feb 2005 17:43:32 +0100
Subject: Fw: [R] Contour plot
Message-ID: <001d01c50dfd$552ee1f0$3ef4ee3e@1234abc>

I understand that I need to have a (in this case) square matrix with all the
data. But the question now is;

- can the contourplot not interpolate the missing values

or alternatively

- I have fit a model to the z data (z = 100 + 0.5x + 0.5y). How can I make
from this model a "square" matrix z to make a contour plot?

Kind regards, Darius Blaszijk

----- Original Message -----
From: "Achim Zeileis" <Achim.Zeileis at wu-wien.ac.at>
To: <dhkblaszyk at zeelandnet.nl>
Cc: <r-help at stat.math.ethz.ch>
Sent: Tuesday, February 08, 2005 1:51 AM
Subject: Re: [R] Contour plot


> On Tue, 8 Feb 2005 01:15:06 +0100 dhkblaszyk at zeelandnet.nl wrote:
>
> > Hello,
> >
> > I would like to make a contourplot of the following data;
> >
> > > x <- 1:10
> > > y <- 1:10
> > > z <- 100:110
> >
> > By doing >contour(x,y,z) I get the following error;
> >
> > "Error in contour.default(x, y, z) : no proper `z' matrix specified"
> >
> > How do I fix this??
>
> x and y specify a grid and thus z must provide a value for each
> combination of the x's and y's! For example:
>   x <- y <- 1:10
>   contour(x, y, outer(x, y))
> Also look at
>   outer(x, y)
> and read ?contour.
>
> Z
>
> > Kind regards, Datius Blaszijk
> >
> > [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >



From chabotd at globetrotter.net  Tue Feb  8 17:46:35 2005
From: chabotd at globetrotter.net (Denis Chabot)
Date: Tue, 8 Feb 2005 17:46:35 +0100
Subject: =?ISO-8859-1?Q?R=E9p_:_[R]_Problem_installing_Hmisc?=
In-Reply-To: <200502081123.j18BNSqp017241@hypatia.math.ethz.ch>
References: <200502081123.j18BNSqp017241@hypatia.math.ethz.ch>
Message-ID: <ce77ced300cdc6fea895aba95c4f84c7@globetrotter.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050208/d81c417d/attachment.pl

From Roger.Bivand at nhh.no  Tue Feb  8 17:58:43 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue, 8 Feb 2005 17:58:43 +0100 (CET)
Subject: [R] Re: [STATSGRASS] Using GRASS/R interface with an xy location
In-Reply-To: <731a05d3df6c8a53f0fd054f13aba5a0@life.uiuc.edu>
Message-ID: <Pine.LNX.4.44.0502081754560.21214-100000@reclus.nhh.no>

On Tue, 8 Feb 2005, Steven T.Stoddard wrote:

> I am using GRASS 5.7 and R 2.0 for OS X.  I have a simple xy location I 
> am doing some theoretical work with and would like to import maps into 
> R using the interface in order to do analyses.  Yet when I attempt to 
> do this, I get the following error:
> 
>  >G<-gmeta()
> Error in gmeta() : region for current mapset is invalid
> line 11: <top:        100>
> run "g.region"
> 
> I traced the problem to the compiled C program that is called from the 
> gmeta R function (i.e. > .C("gmeta") gives me the same error). I 
> haven't been able to locate the source code on my volume.
> 
> If I use gmeta(interp = TRUE) there is no problem, until I want to 
> actually try and import data from my locations.

The current fix for 5.7 and 6.0.0beta is to use interp=TRUE. The cause of 
the problem seems to be the use of the 3D WINDOW, which the older code in 
the R-GRASS interface doesn't yet honour. I'm not at all sure what will 
happen if your x,y locations are sites; the interface knows about 
old-style sites, not vector sites. You may be obliged to use system("") to 
issue GRASS 5.7 commands to write out an ASCII file to read into R with 
read.table().

Roger


> 
> Any help is appreciated.
> 
> Steve
> 
> ---
> Steven T. Stoddard
> 
> Program in Ecology, Evolution and Conservation Biology
> University of Illinois at Urbana-Champaign
> 
> (217) 333-2235
> 302 Shelford Vivarium
> 606 E. Healy Ave
> Champaign, IL  61820
> http://www.life.uiuc.edu/~sstoddar
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From gregory.r.warnes at pfizer.com  Tue Feb  8 17:54:12 2005
From: gregory.r.warnes at pfizer.com (Warnes, Gregory R)
Date: Tue, 8 Feb 2005 11:54:12 -0500 
Subject: [R] parameter couldn't be set in high-level plot() function
Message-ID: <915D2D65A9986440A277AC5C98AA466F978A19@groamrexm02.amer.pfizer.com>

Thanks for your patch, I've modified the code of bandplot appropriately.
This change will be in the next release of the gregmisc bundle.

-G

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of R user
> Sent: Tuesday, January 25, 2005 1:02 PM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] parameter couldn't be set in high-level 
> plot() function
> 
> 
> Think the problem I had with the bandplot (gplots) function 
> is solved by
> changing the expand.dots = FALSE to expand.dots = TRUE.
> Don't understand actually why it says FALSE here, because 
> that means it
> does *not* pass extra arguments to plot.
> If I change it to TRUE, my main/xlab/ylab arguments are 
> passed just like
> I wanted.
> 
> fragment of bandplot[gplots]
> 
>     if (!add) {
>         m <- match.call(expand.dots = FALSE)
>         m$width <- m$add <- m$sd <- m$sd.col <- NULL
>         m$method <- m$n <- NULL
>         m[[1]] <- as.name("plot")
>         mf <- eval(m, parent.frame())
>     }
> 
> Jonne.
> 
> 
> On Mon, 2005-01-24 at 15:50 +0100, R user wrote:
> > 
> > Dear R users,
> > 
> > I am using function bandplot from the gplots package.
> > To my understanding (viewing the source of bandplot) it calls
> > function plot (add = FALSE) with the same parameters 
> (except for a few
> > removed).
> > 
> > I would like to give extra parameters 'xlab' and 'ylab' to function
> > bandplot, but, as can be seen below, that raises warnings (and the
> > labels do not show up at the end).
> > 
> > It does work to call title(... xlab="blah", ylab="foo") 
> after bandplot
> > (), but then I have two labels on top of each other, which 
> is even more
> > ugly.
> > 
> > Can anyone explain me why this goes wrong?
> > 
> > Thanks in advance,
> > Jonne.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> -- 
> R user <R-user at zutt.org>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}



From ripley at stats.ox.ac.uk  Tue Feb  8 18:09:54 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 8 Feb 2005 17:09:54 +0000 (GMT)
Subject: [R] Windows Printing and Line Widths
In-Reply-To: <67DCA285A2D7754280D3B8E88EB5480206741E5E@MSGBOSCLB2WIN.DMN1.FMR.COM>
References: <67DCA285A2D7754280D3B8E88EB5480206741E5E@MSGBOSCLB2WIN.DMN1.FMR.COM>
Message-ID: <Pine.LNX.4.61.0502081704330.14264@gannet.stats>

Those printers AFAIK support postscript.

How are you printing to them on Linux?  I suggest you use dev.print under 
Windows (it needs some setup, see ?postscript).  That makes more sense 
than going via PDF as the support is all already in R and it is AFAIK the 
printer's native mode.

We've seen far too many problems with HP Windows printer drivers on 8000 
and 4000 series printers.


On Tue, 8 Feb 2005, McGehee, Robert wrote:

> Hi all,
> I develop and print from both Windows and Linux, and am seeing some
> printing inconsistencies first described about a year and a half ago by
> Andy Liaw (see below). Specifically, the line widths on my windows plots
> are about 5 times smaller than that on Linux, and my windows printouts
> do not match what my screen looks like. However, if I print to a pdf
> file first, then I can get accurate Windows reproduction of my screen. I
> was thinking of writing a windows.print() wrapper that creates a
> temporary pdf file and then prints that. However, I wanted to see if a
> better solution now exists to get identical printouts on both Linux and
> Windows (since Andy's original post), or any comments on what printers
> this does or does not affect.
>
> Thanks,
> Robert
>
> HP Laserjet 8150DN
> HP Color Laserjet 4600DN
> HP Laserjet 4050TN
>
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    2
> minor    0.1
> year     2004
> month    11
> day      15
> language R
>
> ---------------------------------
> From: Prof Brian Ripley
> Date: Mon Jun 23 2003 - 23:59:29 EDT
>
>
> What printer driver are you using?
>
>
> I've just tried this and it works exactly as one would expect on my HP
> 970CXi, as well as cut-and-paste into other applications. It also worked
>
> printing to Acrobat Distiller (although all the lines were thinner there
>
> than on-screen and on the 970CXi, the ratio was still 1:5).
>
>
> We've been here before, and had to abandon some optimizations because of
> a
> bug in interpreting Windows metafiles in Word.
>
>
> On Mon, 23 Jun 2003, Sundar Dorai-Raj wrote:
>
>
>> Andy,
>> I've experienced the same thing. What's interesting is that printing
>> a plot (CTRL-P) with lwd = 25 makes lines on the hardcopy look like
> lwd
>> = 5. I'm using R1.7.1 on Win2000Pro.
>>
>> Regards,
>> Sundar
>>
>> Liaw, Andy wrote:
>>> Dear R-help,
>>>
>>> Has anyone notice the problem that, on Windows (NT and XP), when
> printing a
>>> graph using the "File -> Print..." menu in the graphics window to
> print the
>>> graph, that line width seemed to be ignored in the printed output?
> For
>>> example, if I make a plot with plot(1:10, type="l", lwd=5), it looks
> right
>>> on screen, but when printed out using the menu, it looks like the
> plot was
>>> made with lwd=1. I've had this problem for quite a while (at least
> since
>>> 1.3.x) and still present in 1.7.1. Has anyone else seen this, or
> just me?
>>>
>>> Best,
>>> Andy
>>>
>>> Andy Liaw, PhD
>>> Biometrics Research PO Box 2000, RY33-300
>>> Merck Research Labs Rahway, NJ 07065
>>> mailto:andy_liaw at merck.com <mailto:andy_liaw at merck.com> 732-594-0820
>
>>>
>>>
>>>
>>>
> ------------------------------------------------------------------------
> ------
>>> Notice: This e-mail message, together with any attachments, cont...
> {{dropped}}
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>
>
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> <http://www.stats.ox.ac.uk/%7Eripley/>
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
>
> Robert McGehee
> Geode Capital Management, LLC
> 53 State Street, 5th Floor | Boston, MA | 02109
> Tel: 617/392-8396    Fax:617/476-6389
> mailto:robert.mcgehee at geodecapital.com
>
>
>
> This e-mail, and any attachments hereto, are intended for us...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sstoddar at life.uiuc.edu  Tue Feb  8 18:13:41 2005
From: sstoddar at life.uiuc.edu (Steven T.Stoddard)
Date: Tue, 8 Feb 2005 11:13:41 -0600
Subject: [R] Re: [STATSGRASS] Using GRASS/R interface with an xy location &
	reverse(G) error
In-Reply-To: <Pine.LNX.4.44.0502081754560.21214-100000@reclus.nhh.no>
References: <Pine.LNX.4.44.0502081754560.21214-100000@reclus.nhh.no>
Message-ID: <de2edb75ef58f529d8cc2dfb48988ec2@life.uiuc.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050208/9415d902/attachment.pl

From jbdunsmo at utmb.edu  Tue Feb  8 18:13:49 2005
From: jbdunsmo at utmb.edu (jbdunsmo@utmb.edu)
Date: Tue, 8 Feb 2005 11:13:49 -0600
Subject: [R] simple example of C interface to R
In-Reply-To: <8B08A3A1EA7AAC41BE24C750338754E6150705@HERMES.demogr.mpg.de>
References: <8B08A3A1EA7AAC41BE24C750338754E6150705@HERMES.demogr.mpg.de>
Message-ID: <20050208171349.GK23399@g20458.utmb.edu>

On Tue, Feb 08, 2005 at 10:55:01AM +0100, Rau, Roland wrote:
>
> do you know already the page of Roger D. Peng?
> He has a document entitled "An Introduction to the .C Interface to R".
> It is located at:
> http://www.biostat.jhsph.edu/~rpeng/docs/interface.pdf
> 

thanks.  that's a nice tutorial, but what i'd really like to do is
write a program with R embedded in it (and not have to load the
interpreter).  i found some documentation for doing that here:
http://cran.r-project.org/doc/manuals/R-exts.html#Embedding-R-under-Unix_002dalikes

now i'm going through the tests/Embedding directory (included with the
R source distribution).

i've gotten Rtest.c to compile with:

    $ R CMD SHLIB Rtest.c
    gcc -I/usr/lib/R/include  -I/usr/local/include -D__NO_MATH_INLINES -mieee-fp  -fPIC  -march=pentium4 -O3 -pipe -fomit-frame-pointer -c Rtest.c -o Rtest.o
    gcc -shared -L/usr/local/lib -o Rtest.so Rtest.o   -L/usr/lib/R/bin -lR
    
then when i try to run Rtest.o, i get: 

    -/bin/bash: ./Rtest.o: cannot execute binary file
        
Rtest.c contains:

    #include "embeddedRCall.h"
    
    int
    main(int argc, char *argv[])
    { 
      eval_R_command("print", argc, argv);
      return(0);
    }
    
embeddedRCall.h contains:

    #ifndef EMBEDDED_R_CALL_H
    #define EMBEDDED_R_CALL_H
    
    #include <Rinternals.h>
    
    int eval_R_command(const char *funcName, int argc, char *argv[]);
    SEXP Test_tryEval(SEXP expression, int *errorOccurred);
    void init_R(int argc, char **argv);
    
    #endif
 
i also get the "cannot execute binary file" error when trying to
compile and run the following code from the documentation:

    int Rf_initEmbeddedR(int argc, char **argv)
    { /* This is already compiled into R */
      Rf_initialize_R(argc, argv);
      setup_Rmainloop();
      return(1);
    }
    
    int main(int ac, char **av)
    {
      /* do some setup */
      // Rf_initEmbeddedR(argc, argv);
      /* do some more setup */
    
      char *argv[]= {"REmbeddedPostgres", "--gui=none", "--silent"};
      Rf_initEmbeddedR(sizeof(argv)/sizeof(argv[0]), argv);
    
      /* submit some code to R, which is done interactively via
            run_Rmainloop();
      */
      end_Rmainloop();
      return 0;
    }
       

what am i doing wrong?



From Roger.Bivand at nhh.no  Tue Feb  8 18:18:34 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue, 8 Feb 2005 18:18:34 +0100 (CET)
Subject: [R] Re: [STATSGRASS] Using GRASS/R interface with an xy location & 
 reverse(G) error
In-Reply-To: <de2edb75ef58f529d8cc2dfb48988ec2@life.uiuc.edu>
Message-ID: <Pine.LNX.4.44.0502081816210.21214-100000@reclus.nhh.no>

On Tue, 8 Feb 2005, Steven T.Stoddard wrote:

> Ok.  The work-around I can grow accustomed to, or I might fall back on 
> GRASS5.4 if that works.
> 
> I wonder if another problem that cropped up is somehow related to the 
> first.  Using interp=TRUE I managed to import a raster map to R.  But 
> when trying to plot it, I get the following:
> 
>  > plot(G,tt$rb1)
> Error in reverse(G) : cannot allocate vector of length 1086556160

What does summary(G) say? It sounds as though the numbers of rows and/or 
columns have the wrong values.

Over 1G cells is a bit extreme?

Roger

> 
> ???
> 
> Thanks for the help,
> Steve
> 
> 
> ---
> Steven T. Stoddard
> 
> Program in Ecology, Evolution and Conservation Biology
> University of Illinois at Urbana-Champaign
> 
> (217) 333-2235
> 302 Shelford Vivarium
> 606 E. Healy Ave
> Champaign, IL  61820
> http://www.life.uiuc.edu/~sstoddar
> 
> 
> On Feb 8, 2005, at 10:58 AM, Roger Bivand wrote:
> 
> > On Tue, 8 Feb 2005, Steven T.Stoddard wrote:
> >
> >> I am using GRASS 5.7 and R 2.0 for OS X.  I have a simple xy location 
> >> I
> >> am doing some theoretical work with and would like to import maps into
> >> R using the interface in order to do analyses.  Yet when I attempt to
> >> do this, I get the following error:
> >>
> >>> G<-gmeta()
> >> Error in gmeta() : region for current mapset is invalid
> >> line 11: <top:        100>
> >> run "g.region"
> >>
> >> I traced the problem to the compiled C program that is called from the
> >> gmeta R function (i.e. > .C("gmeta") gives me the same error). I
> >> haven't been able to locate the source code on my volume.
> >>
> >> If I use gmeta(interp = TRUE) there is no problem, until I want to
> >> actually try and import data from my locations.
> >
> > The current fix for 5.7 and 6.0.0beta is to use interp=TRUE. The cause 
> > of
> > the problem seems to be the use of the 3D WINDOW, which the older code 
> > in
> > the R-GRASS interface doesn't yet honour. I'm not at all sure what will
> > happen if your x,y locations are sites; the interface knows about
> > old-style sites, not vector sites. You may be obliged to use 
> > system("") to
> > issue GRASS 5.7 commands to write out an ASCII file to read into R with
> > read.table().
> >
> > Roger
> >
> >
> >>
> >> Any help is appreciated.
> >>
> >> Steve
> >>
> >> ---
> >> Steven T. Stoddard
> >>
> >> Program in Ecology, Evolution and Conservation Biology
> >> University of Illinois at Urbana-Champaign
> >>
> >> (217) 333-2235
> >> 302 Shelford Vivarium
> >> 606 E. Healy Ave
> >> Champaign, IL  61820
> >> http://www.life.uiuc.edu/~sstoddar
> >>
> >>
> >
> > -- 
> > Roger Bivand
> > Economic Geography Section, Department of Economics, Norwegian School 
> > of
> > Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
> > Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
> > e-mail: Roger.Bivand at nhh.no
> >
> >
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From faceasec at uapar.edu  Tue Feb  8 18:23:28 2005
From: faceasec at uapar.edu (=?ISO-8859-1?Q?=22Lic=2E_Adri=E1n_Cecotto_-_FACEA=22?=)
Date: Tue, 08 Feb 2005 14:23:28 -0300
Subject: [R] consultation
Message-ID: <4208F590.9080703@uapar.edu>

R People
Is it possible to make financial calculus with are. anybody can help me 
on it?
TU
Adri?n

From baragoo at hotmail.com  Tue Feb  8 18:19:52 2005
From: baragoo at hotmail.com (Fernando Calle)
Date: Tue, 08 Feb 2005 17:19:52 +0000
Subject: [R] Question about R.
Message-ID: <BAY18-F31828B73494FF35098E654B8740@phx.gbl>



From efg at stowers-institute.org  Tue Feb  8 18:35:22 2005
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Tue, 8 Feb 2005 11:35:22 -0600
Subject: [R] Windows BMPs:  Why grey background?  How to display BMP in R?
Message-ID: <cuat62$404$1@sea.gmane.org>

"white" is supposed to be the default background for BMPs (according to
?bmp) but it doesn't work



> bmp("test.bmp", bg="white")

> plot(0)

> dev.off()

# results in grey background



# This seems to be a good enough workaround for now.

> bmp("test.bmp")

> par(bg="white")

> plot(0)

> dev.off()

# background is white



Should bmps have been fixed when jpegs with a similar grey background were
fixed for Windows?  http://tolstoy.newcastle.edu.au/R/devel/04/10/0914.html

>> jpeg("test.jpeg", bg="orangered")
>> plot(1:10, 1:10, col="green")
>> dev.off()

>There was a Windows-specific bug, for which I've just committed a fix



How can I display a bmp directly in R?



When plotting 100,000+ points (common with flow cytometry data) [or even the
10,000s of points with microarray data] the Windows metafile is not the way
to go (nor is postscript) because the files are too large and repainting
them takes too long.  Does  R provide a way to plot to a bmp and then
display that graphic immediately?  It's a pain to leave R just to view the
graphics.



JPGs work fine for real world images, but not that well for drawings.  BMPs
look like the only alternative when plotting a huge number of points so the
file size is reasonable.  (GIFs with drawings would be even better than
BMPs.  I guess I could try PNGs.)



With the GIF/LZW patent issue now in the past, why not have a GIF driver and
even support creation/display of some simple animated GIFs in R?



Earl F. Glynn
Scientific Programmer
Stowers Institute for Medical Research



From reid_huntsinger at merck.com  Tue Feb  8 18:39:32 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Tue, 8 Feb 2005 12:39:32 -0500
Subject: [R] How to get variable names in a function?
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A92C4@uswpmx00.merck.com>

Semantically, R is pass-by-value, so you don't really have the names, just
the values. In implementation, though, R *does* pass names, in part at least
in order to do "lazy evaluation". You can get them via "substitute" ; see
the help for that.

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Heinz Tuechler
Sent: Tuesday, February 08, 2005 11:45 AM
To: r-help at stat.math.ethz.ch
Subject: [R] How to get variable names in a function?


Hello,

applying a function to a list of variables I face the following problem:
Let's say I want to compute tables for several variables. I could write a
command for every single table, like
bravo<-c(1,1,2,3,5,5,5,);charly<-c(7,7,4,4,2,1)
table(bravo); table(charly)
> table(bravo); table(charly)
bravo
1 2 3 5 
2 1 1 3 
charly
1 2 4 7 
1 1 2 2 
The results are two tables with the names of the variables above each.
If I want to do the same thing by a function I find no way to get the
variable names above the tables. 
demofn<-function(varlist)
    {for (i in seq(along=varlist))
       {cat(deparse(varlist[i])) # < - - - - how to change this?
        print(table(varlist[i]))}}
> demofn(list(bravo, charly))
list(c(1, 1, 2, 3, 5, 5, 5))
1 2 3 5 
2 1 1 3 
list(c(7, 7, 4, 4, 2, 1))
1 2 4 7 
1 1 2 2 
> 

Thanks,
Heinz T?chler

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Tue Feb  8 18:51:48 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 8 Feb 2005 12:51:48 -0500
Subject: [R] How to get variable names in a function?
Message-ID: <3A822319EB35174CA3714066D590DCD50994E681@usrymx25.merck.com>

This might be easier for your purpose:

> lapply(list(bravo=bravo, charly=charly), table)
$bravo

1 2 3 5 
2 1 1 3 

$charly

1 2 4 7 
1 1 2 2 

Andy


> From: Heinz Tuechler
> 
> Hello,
> 
> applying a function to a list of variables I face the 
> following problem:
> Let's say I want to compute tables for several variables. I 
> could write a
> command for every single table, like
> bravo<-c(1,1,2,3,5,5,5,);charly<-c(7,7,4,4,2,1)
> table(bravo); table(charly)
> > table(bravo); table(charly)
> bravo
> 1 2 3 5 
> 2 1 1 3 
> charly
> 1 2 4 7 
> 1 1 2 2 
> The results are two tables with the names of the variables above each.
> If I want to do the same thing by a function I find no way to get the
> variable names above the tables. 
> demofn<-function(varlist)
>     {for (i in seq(along=varlist))
>        {cat(deparse(varlist[i])) # < - - - - how to change this?
>         print(table(varlist[i]))}}
> > demofn(list(bravo, charly))
> list(c(1, 1, 2, 3, 5, 5, 5))
> 1 2 3 5 
> 2 1 1 3 
> list(c(7, 7, 4, 4, 2, 1))
> 1 2 4 7 
> 1 1 2 2 
> > 
> 
> Thanks,
> Heinz T?chler
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From deepayan at stat.wisc.edu  Tue Feb  8 19:16:31 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Tue, 8 Feb 2005 12:16:31 -0600
Subject: [R] panel plot with log-scaled x-axis
In-Reply-To: <4208E67C.90907@uni-jena.de>
References: <4208E67C.90907@uni-jena.de>
Message-ID: <200502081216.31407.deepayan@stat.wisc.edu>

On Tuesday 08 February 2005 10:19, Christoph Scherber wrote:
> Dear all,
>
> Can anyone help me plotting a panel plot with log-scaled x axes?
>
> My formula looks like this:
>
> coplot(response~div|treatment+grass,
> panel=function(x,y,...){panel.xyplot(x,y,...);panel.lmline(x,y,...)})

Are you sure that actually works? 

> And I?d like to have "div" plotted in log scale.

How to do that would depend at least on whether you are using coplot or 
xyplot (neither may have good solutions implemented).

Deepayan



From gunter.berton at gene.com  Tue Feb  8 19:32:32 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 8 Feb 2005 10:32:32 -0800
Subject: [R] How to get variable names in a function?
In-Reply-To: <3.0.6.32.20050208174511.007abc30@pop.gmx.net>
Message-ID: <200502081832.j18IWWAK021954@compton.gene.com>

If you pass your function a NAMED list, then the following works:

demofn<-function(varlist)
{
	nm<-names(varlist)
    for (i in seq(along=varlist))
       {cat('\n',nm[i]) 
       print(table(varlist[[i]]))
    }
}
demofn(list(bravo=bravo, charly=charly))

If you don't pass a named list, then you need to restrict and know the form
of the expression that is the varlist argument in order to substitute() and
deparse it correctly to get the identifiers you want. For example, if you
knew that varlist were an expression of the form:
list(var1,var2,var3,...) 
then you could get the ith identifier vari via:

deparse((as.list(substitute(varlist))[-1])[[i]]) 

HOWEVER, this is probably inefficient and **clearly** clumsy, undesirable,
and almost certain to fail (so don't do this!). 

If the number of tables is small enough that you could simply list them as
arguments (as opposed to constructing the list of vectors to be tabled in
some way), then the function call could be of the form function(...) and the
... arguments could be processed as discussed in section 3.1 of V&R's S
PROGRAMMING. That is, your example call would be of the form:
demofn(bravo,charly), and you can forgo lists in the call altogether. This
strategy actually also works for long constructed lists of arguments using
do.call() -- see it's help file and V&R again for details.


-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Heinz Tuechler
> Sent: Tuesday, February 08, 2005 8:45 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] How to get variable names in a function?
> 
> Hello,
> 
> applying a function to a list of variables I face the 
> following problem:
> Let's say I want to compute tables for several variables. I 
> could write a
> command for every single table, like
> bravo<-c(1,1,2,3,5,5,5,);charly<-c(7,7,4,4,2,1)
> table(bravo); table(charly)
> > table(bravo); table(charly)
> bravo
> 1 2 3 5 
> 2 1 1 3 
> charly
> 1 2 4 7 
> 1 1 2 2 
> The results are two tables with the names of the variables above each.
> If I want to do the same thing by a function I find no way to get the
> variable names above the tables. 
> demofn<-function(varlist)
>     {for (i in seq(along=varlist))
>        {cat(deparse(varlist[i])) # < - - - - how to change this?
>         print(table(varlist[i]))}}
> > demofn(list(bravo, charly))
> list(c(1, 1, 2, 3, 5, 5, 5))
> 1 2 3 5 
> 2 1 1 3 
> list(c(7, 7, 4, 4, 2, 1))
> 1 2 4 7 
> 1 1 2 2 
> > 
> 
> Thanks,
> Heinz T?chler
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From munroe.9 at osu.edu  Tue Feb  8 19:43:24 2005
From: munroe.9 at osu.edu (Darla Munroe)
Date: Tue, 8 Feb 2005 13:43:24 -0500
Subject: [R] Plotting estimated betas, standard error
Message-ID: <200502081845.j18IjwxU009918@defang9.net.ohio-state.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050208/db2f2089/attachment.pl

From jerk_alert at hotmail.com  Tue Feb  8 19:47:19 2005
From: jerk_alert at hotmail.com (Ken Termiso)
Date: Tue, 08 Feb 2005 18:47:19 +0000
Subject: [R] Renaming columns in data.frame,
	inserting/removing columns from data.frame
Message-ID: <BAY101-F1133E64B50B47122D30F09E8740@phx.gbl>

Hello,

I'm hoping that there is an easier way to rename columns in a data frame 
other than by using the names() assignment, which requires you to type in 
all the column names at once for a data.frame, in the case that I simply 
want to rename a single column in a data frame.

Also, is there an easy way to move columns in a data frame around relative 
to the other columns?

Thanks in advance,
Ken



From murdoch at stats.uwo.ca  Tue Feb  8 20:29:24 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 08 Feb 2005 19:29:24 +0000
Subject: [R] Windows BMPs: Why grey background? How to display BMP in R?
In-Reply-To: <cuat62$404$1@sea.gmane.org>
References: <cuat62$404$1@sea.gmane.org>
Message-ID: <3j4i015lan8u9cu1eueuk8h5ghru2gt8hd@4ax.com>

On Tue, 8 Feb 2005 11:35:22 -0600, "Earl F. Glynn"
<efg at stowers-institute.org> wrote :

>"white" is supposed to be the default background for BMPs (according to
>?bmp) but it doesn't work
>
>
>
>> bmp("test.bmp", bg="white")
>
>> plot(0)
>
>> dev.off()
>
># results in grey background

In what version of R?  I just tried in 2.01 and R-patched, and it was
fine.  Maybe the problem is your viewer?

Duncan Murdoch



From ccleland at optonline.net  Tue Feb  8 20:31:42 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Tue, 08 Feb 2005 14:31:42 -0500
Subject: [R] Plotting estimated betas, standard error
In-Reply-To: <200502081845.j18IjwxU009918@defang9.net.ohio-state.edu>
References: <200502081845.j18IjwxU009918@defang9.net.ohio-state.edu>
Message-ID: <4209139E.8030600@optonline.net>

You could use Dotplot() in the Hmisc package.  For example:

mydata <- data.frame(BETA = runif(4, min=.50, max=.70), SIM = c("Sim A", 
"Sim B", "Sim C", "Sim D"))

mydata$CIL <- mydata$BETA - runif(4, min=.05, max=.20)
mydata$CIU <- mydata$BETA + runif(4, min=.05, max=.20)

Dotplot(SIM ~ Cbind(BETA, CIL, CIU), data=mydata, ylab="", xlab="Beta", 
main="Betas by Simulation")

hope this helps,

Chuck Cleland

Darla Munroe wrote:
> Hello,
> 
>  
> 
> I was wondering, is there a way to plot estimated betas and standard
> deviations that I've generated?
> 
>  
> 
> Specifically, I have the following:
> 
>  
> 
> A matrix of estimated betas (two different betas, 4 different simulations  -
> categories).
> 
> A matrix of estimated standard errors corresponding to each of those betas.
> 
>  
> 
> I would like to plot the estimated beta, and make a confidence interval for
> that beta similar to a boxplot, and I would like to plot the four different
> simulations in the same plot.
> 
>  
> 
> I apologize for my ignorance, I am new to R.  I just don't see anywhere to
> specify "whiskers" for your own specified confidence interval.
> 
>  
> 
> Thank you,
> 
> DM
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From sdavis2 at mail.nih.gov  Tue Feb  8 20:34:19 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Tue, 8 Feb 2005 14:34:19 -0500
Subject: [R] Renaming columns in data.frame,
	inserting/removing columns from data.frame
In-Reply-To: <BAY101-F1133E64B50B47122D30F09E8740@phx.gbl>
References: <BAY101-F1133E64B50B47122D30F09E8740@phx.gbl>
Message-ID: <6D4CCBC4-7A08-11D9-9137-000D933565E8@mail.nih.gov>


On Feb 8, 2005, at 1:47 PM, Ken Termiso wrote:

> Hello,
>
> I'm hoping that there is an easier way to rename columns in a data 
> frame other than by using the names() assignment, which requires you 
> to type in all the column names at once for a data.frame, in the case 
> that I simply want to rename a single column in a data frame.
>

If you want to change (or name) only the name in column 12, for example,

names(mydataframe)[12] <- 'new column 12 name'

  will do the trick.


> Also, is there an easy way to move columns in a data frame around 
> relative to the other columns?

You can do this by assigning your rearranged dataframe back to itself 
(carefully):

mydataframe <- mydataframe[c(5:10,1:4),]

Will take the columns 5 through 10 and put them before columns 1 
through 4.  I note "carefully" above because if mydataframe contains, 
for example, 12 columns, the command here would drop columns 11 and 12.

Hope this helps.

Sean



From wolski at molgen.mpg.de  Tue Feb  8 20:38:11 2005
From: wolski at molgen.mpg.de (Witold Eryk Wolski)
Date: Tue, 08 Feb 2005 20:38:11 +0100
Subject: [R] Renaming columns in data.frame,	inserting/removing columns
	from data.frame
In-Reply-To: <BAY101-F1133E64B50B47122D30F09E8740@phx.gbl>
References: <BAY101-F1133E64B50B47122D30F09E8740@phx.gbl>
Message-ID: <42091523.3080708@molgen.mpg.de>

Ken Termiso wrote:

> Hello,
>
> I'm hoping that there is an easier way to rename columns in a data 
> frame other than by using the names() assignment, which requires you 
> to type in all the column names at once for a data.frame, in the case 
> that I simply want to rename a single column in a data frame.
>

names(mydataframe)[column.index]<-"new.name"

> Also, is there an easy way to move columns in a data frame around 
> relative to the other columns?
>
?subset
or
mydataframe[c("column3","column2","column1")]

> Thanks in advance,
> Ken
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>


-- 
Dipl. bio-chem. Witold Eryk Wolski
MPI-Moleculare Genetic
Ihnestrasse 63-73 14195 Berlin
tel: 0049-30-83875219                 __("<    _
http://www.molgen.mpg.de/~wolski      \__/    'v'
http://r4proteomics.sourceforge.net    ||    /   \
mail: witek96 at users.sourceforge.net    ^^     m m
      wolski at molgen.mpg.de



From minhan.science at gmail.com  Tue Feb  8 20:38:18 2005
From: minhan.science at gmail.com (Min-Han Tan)
Date: Tue, 8 Feb 2005 14:38:18 -0500
Subject: [R] Compiling R as a shared library
Message-ID: <7902152a050208113864a9c903@mail.gmail.com>

Hi,

I am trying to compile R as a shared library (need to run RMAGEML,
which depends on SJava) on 64bit SUSE Linux 9.1 . I am using the
source code for R 2.0.1 (Nov 15 04)

>  ./configure R_PAPERSIZE=LETTER R_BROWSER=/opt/kde3/share/applications/kde/konqbrowser.desktop --enable-R-shlib

results in several errors (selected errors below), and make check also
yields errors (at bottom) The errors seem to centre around conftest,
with
configure:29275: test -z 
			 || test ! -s conftest.err
being fairly typical, occuring multiple times in config.log.

Any advice on trouble shooting this would be greatly appreciated. 

Min-Han


_______________


configure:1908: checking for working aclocal-1.4
configure:1919: result: missing
configure:1923: checking for working autoconf
configure:1930: result: found
configure:1938: checking for working automake-1.4
configure:1949: result: missing

configure:4488: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
conftest.c:2: error: parse error before "me"
configure:4494: $? = 1
configure: failed program was:
| #ifndef __cplusplus
|   choke me
| #endif

configure:4707: gcc -E -I/usr/local/include conftest.c
conftest.c:16:28: ac_nonexistent.h: No such file or directory
configure:4713: $? = 1
configure: failed program was:
| /* confdefs.h.  */
| 
| #define PACKAGE_NAME "R"
| #define PACKAGE_TARNAME "R"
| #define PACKAGE_VERSION "2.0.1"
| #define PACKAGE_STRING "R 2.0.1"
| #define PACKAGE_BUGREPORT "r-bugs at R-project.org"
| #define PACKAGE "R"
| #define VERSION "2.0.1"
| #define R_PLATFORM "x86_64-unknown-linux-gnu"
| #define R_CPU "x86_64"
| #define R_VENDOR "unknown"
| #define R_OS "linux-gnu"
| #define Unix 1
| /* end confdefs.h.  */
| #include <ac_nonexistent.h>


configure:4814: gcc -E -I/usr/local/include conftest.c
conftest.c:16:28: ac_nonexistent.h: No such file or directory
configure:4820: $? = 1
configure: failed program was:
| /* confdefs.h.  */
| 
| #define PACKAGE_NAME "R"
| #define PACKAGE_TARNAME "R"
| #define PACKAGE_VERSION "2.0.1"
| #define PACKAGE_STRING "R 2.0.1"
| #define PACKAGE_BUGREPORT "r-bugs at R-project.org"
| #define PACKAGE "R"
| #define VERSION "2.0.1"
| #define R_PLATFORM "x86_64-unknown-linux-gnu"
| #define R_CPU "x86_64"
| #define R_VENDOR "unknown"
| #define R_OS "linux-gnu"
| #define Unix 1
| /* end confdefs.h.  */
| #include <ac_nonexistent.h>


configure:5099: gcc -E -I/usr/local/include conftest.c
conftest.c:16:28: ac_nonexistent.h: No such file or directory
configure:5105: $? = 1
configure: failed program was:
| /* confdefs.h.  */
| 
| #define PACKAGE_NAME "R"
| #define PACKAGE_TARNAME "R"
| #define PACKAGE_VERSION "2.0.1"
| #define PACKAGE_STRING "R 2.0.1"
| #define PACKAGE_BUGREPORT "r-bugs at R-project.org"
| #define PACKAGE "R"
| #define VERSION "2.0.1"
| #define R_PLATFORM "x86_64-unknown-linux-gnu"
| #define R_CPU "x86_64"
| #define R_VENDOR "unknown"
| #define R_OS "linux-gnu"
| #define Unix 1
| /* end confdefs.h.  */
| #include <ac_nonexistent.h>

configure:29307: checking whether isfinite is declared
configure:29340: gcc -c -g -O2 -I/usr/local/include conftest.c >&5
conftest.c: In function `main':
conftest.c:122: error: `isfinite' undeclared (first use in this function)
conftest.c:122: error: (Each undeclared identifier is reported only once
conftest.c:122: error: for each function it appears in.)
configure:29346: $? = 1
configure: failed program was:
| /* confdefs.h.  */
| 
| #define PACKAGE_NAME "R"
| #define PACKAGE_TARNAME "R"
| #define PACKAGE_VERSION "2.0.1"
| #define PACKAGE_STRING "R 2.0.1"
| #define PACKAGE_BUGREPORT "r-bugs at R-project.org"
| #define PACKAGE "R"
| #define VERSION "2.0.1"
| #define R_PLATFORM "x86_64-unknown-linux-gnu"
| #define R_CPU "x86_64"
| #define R_VENDOR "unknown"
| #define R_OS "linux-gnu"
| #define Unix 1
| #ifdef __cplusplus
| extern "C" void std::exit (int) throw (); using std::exit;
| #endif
| #define STDC_HEADERS 1
| #define HAVE_SYS_TYPES_H 1
| #define HAVE_SYS_STAT_H 1
| #define HAVE_STDLIB_H 1
| #define HAVE_STRING_H 1
| #define HAVE_MEMORY_H 1
| #define HAVE_STRINGS_H 1
| #define HAVE_INTTYPES_H 1
| #define HAVE_STDINT_H 1
| #define HAVE_UNISTD_H 1
| #define HAVE_DLFCN_H 1
| #define HAVE_LIBM 1
| #define HAVE_LIBDL 1
| #define STDC_HEADERS 1
| #define TIME_WITH_SYS_TIME 1
| #define HAVE_DIRENT_H 1
| #define HAVE_SYS_WAIT_H 1
| #define HAVE_ARPA_INET_H 1
| #define HAVE_DLFCN_H 1
| #define HAVE_ELF_H 1
| #define HAVE_FCNTL_H 1
| #define HAVE_FPU_CONTROL_H 1
| #define HAVE_GRP_H 1
| #define HAVE_IEEE754_H 1
| #define HAVE_LIMITS_H 1
| #define HAVE_LOCALE_H 1
| #define HAVE_NETDB_H 1
| #define HAVE_NETINET_IN_H 1
| #define HAVE_PWD_H 1
| #define HAVE_STRINGS_H 1
| #define HAVE_SYS_PARAM_H 1
| #define HAVE_SYS_SELECT_H 1
| #define HAVE_SYS_SOCKET_H 1
| #define HAVE_SYS_STAT_H 1
| #define HAVE_SYS_TIME_H 1
| #define HAVE_SYS_TIMES_H 1
| #define HAVE_SYS_UTSNAME_H 1
| #define HAVE_UNISTD_H 1
| #define HAVE_WCHAR_H 1
| #define HAVE_ERRNO_H 1
| #define HAVE_STDARG_H 1
| #define HAVE_STRING_H 1
| #define HAVE_POSIX_SETJMP 1
| #define HAVE_GLIBC2 1
| #define RETSIGTYPE void
| #define HAVE_DECL_SIZE_MAX 1
| #define SOCKLEN_T socklen_t
| #define R_INLINE inline
| #define SIZEOF_INT 4
| #define INT_32_BITS 1
| #define SIZEOF_LONG 8
| #define SIZEOF_LONG_LONG 8
| #define SIZEOF_LONG_DOUBLE 16
| #define F77_FUNC(name,NAME) name ## _
| #define F77_FUNC_(name,NAME) name ## __
| #define HAVE_F77_UNDERSCORE 1
| #define HAVE_DOUBLE_COMPLEX 1
| #define SHLIB_EXT ".so"
| #define HAVE_OFF_T 1
| #define HAVE_ALLOCA_H 1
| #define HAVE_ALLOCA 1
| #define HAVE_ACCESS 1
| #define HAVE_CHDIR 1
| #define HAVE_EXPM1 1
| #define HAVE_FCNTL 1
| #define HAVE_FINITE 1
| #define HAVE_FSEEKO 1
| #define HAVE_FTELLO 1
| #define HAVE_FTRUNCATE 1
| #define HAVE_GETCWD 1
| #define HAVE_GETGRGID 1
| #define HAVE_GETPWUID 1
| #define HAVE_GETUID 1
| #define HAVE_HYPOT 1
| #define HAVE_ISASCII 1
| #define HAVE_LOG1P 1
| #define HAVE_MATHERR 1
| #define HAVE_MKFIFO 1
| #define HAVE_POPEN 1
| #define HAVE_PUTENV 1
| #define HAVE_RINT 1
| #define HAVE_SETENV 1
| #define HAVE_STRCOLL 1
| #define HAVE_STAT 1
| #define HAVE_STRPTIME 1
| #define HAVE_SYMLINK 1
| #define HAVE_SYSTEM 1
| #define HAVE_TIMES 1
| #define HAVE_UNSETENV 1
| /* end confdefs.h.  */
| #include <math.h>
| 
| #ifdef F77_DUMMY_MAIN
| 
| #  ifdef __cplusplus
|      extern "C"
| #  endif
|    int F77_DUMMY_MAIN() { return 1; }
| 
| #endif
| int
| main ()
| {
| #ifndef isfinite
|   char *p = (char *) isfinite;
| #endif
| 
|   ;
|   return 0;
| }

__________________

Make check errors
make[1]: Entering directory `/usr/R/R-2.0.1/tests'
make[2]: Entering directory `/usr/R/R-2.0.1/tests'
make[3]: Entering directory `/usr/R/R-2.0.1/tests/Examples'
make[4]: Entering directory `/usr/R/R-2.0.1/tests/Examples'
make[4]: `Makedeps' is up to date.
make[4]: Leaving directory `/usr/R/R-2.0.1/tests/Examples'
make[4]: Entering directory `/usr/R/R-2.0.1/tests/Examples'
running code in 'base-Ex.R' ...make[4]: *** [base-Ex.Rout] Error 1
make[4]: Leaving directory `/usr/R/R-2.0.1/tests/Examples'
make[3]: *** [test-Examples-Base] Error 2
make[3]: Leaving directory `/usr/R/R-2.0.1/tests/Examples'
make[2]: *** [test-Examples] Error 2
make[2]: Leaving directory `/usr/R/R-2.0.1/tests'
make[1]: *** [test-all-basics] Error 1
make[1]: Leaving directory `/usr/R/R-2.0.1/tests'
make: *** [check] Error 2



From albert.phillimore at imperial.ac.uk  Tue Feb  8 20:34:29 2005
From: albert.phillimore at imperial.ac.uk (Phillimore, Albert)
Date: Tue, 8 Feb 2005 19:34:29 -0000
Subject: [R] off-topic: bootstrapping ratios of variances to get CIs (or
	alternative approaches)
Message-ID: <37629CD96DEBCF42B807EAD22EA7EF780A1BCF@icex3.ic.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050208/dbaba4fa/attachment.pl

From francoisromain at free.fr  Tue Feb  8 20:57:09 2005
From: francoisromain at free.fr (Romain Francois)
Date: Tue, 08 Feb 2005 20:57:09 +0100
Subject: Fw: [R] Contour plot
In-Reply-To: <001d01c50dfd$552ee1f0$3ef4ee3e@1234abc>
References: <001d01c50dfd$552ee1f0$3ef4ee3e@1234abc>
Message-ID: <42091995.6050706@free.fr>

Le 08.02.2005 17:43, dhkblaszyk at zeelandnet.nl a ?crit :

>I understand that I need to have a (in this case) square matrix with all the
>data. But the question now is;
>
>- can the contourplot not interpolate the missing values
>
>  
>
I think it doesn't (see (1) below)

>or alternatively
>
>- I have fit a model to the z data (z = 100 + 0.5x + 0.5y). How can I make
>from this model a "square" matrix z to make a contour plot?
>
>  
>
Hello,

try something like that :

x <- y <- 1:10
z <- 100+0.5*outer(x,y,FUN="+")
contour(x,y,z)


(1) :
z[3:4,3:4] <- NA #putting some missing values
contour(x, y, z,nlevels=20)



Romain.

>>Hello,
>>
>>I would like to make a contourplot of the following data;
>>
>>    
>>
>>>>x <- 1:10
>>>>y <- 1:10
>>>>z <- 100:110
>>>>        
>>>>
>>>By doing >contour(x,y,z) I get the following error;
>>>
>>>"Error in contour.default(x, y, z) : no proper `z' matrix specified"
>>>
>>>How do I fix this??
>>>      
>>>
>>x and y specify a grid and thus z must provide a value for each
>>combination of the x's and y's! For example:
>>  x <- y <- 1:10
>>  contour(x, y, outer(x, y))
>>Also look at
>>  outer(x, y)
>>and read ?contour.
>>
>>Z
>>
>>    
>>
>>>Kind regards, Datius Blaszijk
>>>
>>>[[alternative HTML version deleted]]
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide!
>>>http://www.R-project.org/posting-guide.html
>>>
>>>      
>>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>


-- 
Romain FRANCOIS : francoisromain at free.fr
page web : http://addictedtor.free.fr/  (en construction)
06 18 39 14 69 / 01 46 80 65 60
_______________________________________________________
Etudiant en 3eme ann?e
Institut de Statistique de l'Universit? de Paris (ISUP)
Fili?re Industrie et Services
http://www.isup.cicrp.jussieu.fr/



From Mike.Prager at noaa.gov  Tue Feb  8 21:15:13 2005
From: Mike.Prager at noaa.gov (Mike Prager)
Date: Tue, 08 Feb 2005 15:15:13 -0500
Subject: [R] Renaming columns in data.frame, inserting/removing
	columns from data.frame
In-Reply-To: <BAY101-F1133E64B50B47122D30F09E8740@phx.gbl>
References: <BAY101-F1133E64B50B47122D30F09E8740@phx.gbl>
Message-ID: <6.1.2.0.2.20050208150739.01e49ec0@hermes.nos.noaa.gov>

At 2/8/2005 01:47 PM, Ken Termiso wrote:
>I'm hoping that there is an easier way to rename columns in a data frame 
>other than by using the names() assignment, which requires you to type in 
>all the column names at once for a data.frame, in the case that I simply 
>want to rename a single column in a data frame.

This is ugly but it works, and it avoids the use column indices (which I 
find error-prone):

 > aa = data.frame(a=1:6, b=5:10)       # a simple example
 > aa
   a  b
1 1  5
2 2  6
3 3  7
4 4  8
5 5  9
6 6 10

 > names(aa)[names(aa)=="a"] = "c"       # rename "a" to "c"
 > aa
   c  b
1 1  5
2 2  6
3 3  7
4 4  8
5 5  9
6 6 10

>Also, is there an easy way to move columns in a data frame around relative 
>to the other columns?

Ditto:

 > aa = data.frame(aa["b"],aa["c"])
 > aa
    b c
1  5 1
2  6 2
3  7 3
4  8 4
5  9 5
6 10 6


MHP


-- 
Michael Prager
NOAA Center for Coastal Fisheries and Habitat Research
Beaufort, North Carolina  28516  USA
http://shrimp.ccfhrb.noaa.gov/~mprager/

NOTE: Opinions expressed are personal, not official. No government
endorsement of any product is made or implied.



From Mike.Prager at noaa.gov  Tue Feb  8 21:43:25 2005
From: Mike.Prager at noaa.gov (Mike Prager)
Date: Tue, 08 Feb 2005 15:43:25 -0500
Subject: [R] Renaming columns in data.frame, inserting/removing
	columns from data.frame
Message-ID: <6.1.2.0.2.20050208154304.01e43d58@hermes.nos.noaa.gov>

At 2/8/2005 01:47 PM, Ken Termiso wrote:
>I'm hoping that there is an easier way to rename columns in a data frame 
>other than by using the names() assignment, which requires you to type in 
>all the column names at once for a data.frame, in the case that I simply 
>want to rename a single column in a data frame.

This is ugly but it works, and it avoids the use column indices (which I 
find error-prone):

 > aa = data.frame(a=1:6, b=5:10)       # a simple example
 > aa
   a  b
1 1  5
2 2  6
3 3  7
4 4  8
5 5  9
6 6 10

 > names(aa)[names(aa)=="a"] = "c"       # rename "a" to "c"
 > aa
   c  b
1 1  5
2 2  6
3 3  7
4 4  8
5 5  9
6 6 10

>Also, is there an easy way to move columns in a data frame around relative 
>to the other columns?

Ditto:

 > aa = data.frame(aa["b"],aa["c"])
 > aa
    b c
1  5 1
2  6 2
3  7 3
4  8 4
5  9 5
6 10 6


MHP


-- 
Michael Prager
NOAA Center for Coastal Fisheries and Habitat Research
Beaufort, North Carolina  28516  USA
http://shrimp.ccfhrb.noaa.gov/~mprager/

NOTE: Opinions expressed are personal, not official. No government
endorsement of any product is made or implied.



From efg at stowers-institute.org  Tue Feb  8 21:40:51 2005
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Tue, 8 Feb 2005 14:40:51 -0600
Subject: [R] Windows BMPs: Why grey background? How to display BMP in R?
References: <cuat62$404$1@sea.gmane.org>
	<3j4i015lan8u9cu1eueuk8h5ghru2gt8hd@4ax.com>
Message-ID: <cub81u$75p$1@sea.gmane.org>

"Duncan Murdoch" <murdoch at stats.uwo.ca> wrote in message
news:3j4i015lan8u9cu1eueuk8h5ghru2gt8hd at 4ax.com...
> On Tue, 8 Feb 2005 11:35:22 -0600, "Earl F. Glynn"
> <efg at stowers-institute.org> wrote :

> In what version of R?  I just tried in 2.01 and R-patched, and it was
> fine.  Maybe the problem is your viewer?

Sorry.

I thought I was using R 2.0.1 but I used a shortcut on my desktop that
started the older R 2.0.0 -- so I just rediscovered the problem that you've
already fixed.  With the correct shortcut to R 2.0.1, I am seeing a white
bmp background.  Sorry to have bothered you with this.

I don't see much information about the bmp driver (from ?bmp).  I noticed
the BMPs that are created have only 8-bit color depth, which means the
palette must be stored with the image.  But what is the palette R uses with
BMPs?  I always use true color (24-bit) bmps in Windows instead of 8-bit
bmps to avoid palette issues on older 256 color displays.

Microsoft recommended only using 240 of the 256 palette entries, since 16
were reserved for system colors for older 256 color display monitors (which
are no longer common) and the Windows palette manager.  I guess R's bmps
only have 240 colors that will work on all PCs.

So are bmps created under R restricted to a single, fixed 256 color palette?
(I don't see any color palette parameter to the bmp device.) Is there any
way to create an R bmp that is hicolor (15 or 16-bit color) or true color
(24-bit color) to avoid the palette issues of 8-bit bmps?

Thanks for any info about this.

efg



From ligges at statistik.uni-dortmund.de  Tue Feb  8 21:55:36 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 08 Feb 2005 21:55:36 +0100
Subject: [R] consultation
In-Reply-To: <4208F590.9080703@uapar.edu>
References: <4208F590.9080703@uapar.edu>
Message-ID: <42092748.9070406@statistik.uni-dortmund.de>

Lic. Adri?n Cecotto - FACEA wrote:

> R People
> Is it possible to make financial calculus with are. anybody can help me 
> on it?

Depends on what you mean with "financial calculus".
Please read the posting guide mentioned below and - if you have still 
questions - ask a precise question.

Uwe Ligges


> TU
> Adri?n
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From reid_huntsinger at merck.com  Tue Feb  8 22:19:10 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Tue, 8 Feb 2005 16:19:10 -0500
Subject: [R] simple example of C interface to R
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A92C7@uswpmx00.merck.com>

Rtest.o is just an object file, and the second call to gcc is really a call
to ld and creates Rtest.so, a shared library. Neither is executable.
Probably you want to compile Rtest.c into an executable; it does have the
"main" function after all. Perhaps 

gcc -L/usr/local/lib -o Rtest Rtest.o   -L/usr/lib/R/bin -lR

would work. 

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of jbdunsmo at utmb.edu
Sent: Tuesday, February 08, 2005 12:14 PM
To: Rau, Roland; r-help at stat.math.ethz.ch
Subject: Re: [R] simple example of C interface to R


On Tue, Feb 08, 2005 at 10:55:01AM +0100, Rau, Roland wrote:
>
> do you know already the page of Roger D. Peng?
> He has a document entitled "An Introduction to the .C Interface to R".
> It is located at:
> http://www.biostat.jhsph.edu/~rpeng/docs/interface.pdf
> 

thanks.  that's a nice tutorial, but what i'd really like to do is
write a program with R embedded in it (and not have to load the
interpreter).  i found some documentation for doing that here:
http://cran.r-project.org/doc/manuals/R-exts.html#Embedding-R-under-Unix_002
dalikes

now i'm going through the tests/Embedding directory (included with the
R source distribution).

i've gotten Rtest.c to compile with:

    $ R CMD SHLIB Rtest.c
    gcc -I/usr/lib/R/include  -I/usr/local/include -D__NO_MATH_INLINES
-mieee-fp  -fPIC  -march=pentium4 -O3 -pipe -fomit-frame-pointer -c Rtest.c
-o Rtest.o
    gcc -shared -L/usr/local/lib -o Rtest.so Rtest.o   -L/usr/lib/R/bin -lR
    
then when i try to run Rtest.o, i get: 

    -/bin/bash: ./Rtest.o: cannot execute binary file
        
Rtest.c contains:

    #include "embeddedRCall.h"
    
    int
    main(int argc, char *argv[])
    { 
      eval_R_command("print", argc, argv);
      return(0);
    }
    
embeddedRCall.h contains:

    #ifndef EMBEDDED_R_CALL_H
    #define EMBEDDED_R_CALL_H
    
    #include <Rinternals.h>
    
    int eval_R_command(const char *funcName, int argc, char *argv[]);
    SEXP Test_tryEval(SEXP expression, int *errorOccurred);
    void init_R(int argc, char **argv);
    
    #endif
 
i also get the "cannot execute binary file" error when trying to
compile and run the following code from the documentation:

    int Rf_initEmbeddedR(int argc, char **argv)
    { /* This is already compiled into R */
      Rf_initialize_R(argc, argv);
      setup_Rmainloop();
      return(1);
    }
    
    int main(int ac, char **av)
    {
      /* do some setup */
      // Rf_initEmbeddedR(argc, argv);
      /* do some more setup */
    
      char *argv[]= {"REmbeddedPostgres", "--gui=none", "--silent"};
      Rf_initEmbeddedR(sizeof(argv)/sizeof(argv[0]), argv);
    
      /* submit some code to R, which is done interactively via
            run_Rmainloop();
      */
      end_Rmainloop();
      return 0;
    }
       

what am i doing wrong?

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Tue Feb  8 22:33:02 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 8 Feb 2005 21:33:02 +0000 (GMT)
Subject: [R] Windows BMPs: Why grey background? How to display BMP in R?
In-Reply-To: <cub81u$75p$1@sea.gmane.org>
References: <cuat62$404$1@sea.gmane.org>
	<3j4i015lan8u9cu1eueuk8h5ghru2gt8hd@4ax.com>
	<cub81u$75p$1@sea.gmane.org>
Message-ID: <Pine.LNX.4.61.0502082130300.17539@gannet.stats>

The manual is the source code: we would have to read it to answer your 
questions so you may as well read it yourself.

That driver was written last century by someone no longer active in the R 
project.

On Tue, 8 Feb 2005, Earl F. Glynn wrote:

> "Duncan Murdoch" <murdoch at stats.uwo.ca> wrote in message
> news:3j4i015lan8u9cu1eueuk8h5ghru2gt8hd at 4ax.com...
>> On Tue, 8 Feb 2005 11:35:22 -0600, "Earl F. Glynn"
>> <efg at stowers-institute.org> wrote :
>
>> In what version of R?  I just tried in 2.01 and R-patched, and it was
>> fine.  Maybe the problem is your viewer?
>
> Sorry.
>
> I thought I was using R 2.0.1 but I used a shortcut on my desktop that
> started the older R 2.0.0 -- so I just rediscovered the problem that you've
> already fixed.  With the correct shortcut to R 2.0.1, I am seeing a white
> bmp background.  Sorry to have bothered you with this.
>
> I don't see much information about the bmp driver (from ?bmp).  I noticed
> the BMPs that are created have only 8-bit color depth, which means the
> palette must be stored with the image.  But what is the palette R uses with
> BMPs?  I always use true color (24-bit) bmps in Windows instead of 8-bit
> bmps to avoid palette issues on older 256 color displays.
>
> Microsoft recommended only using 240 of the 256 palette entries, since 16
> were reserved for system colors for older 256 color display monitors (which
> are no longer common) and the Windows palette manager.  I guess R's bmps
> only have 240 colors that will work on all PCs.
>
> So are bmps created under R restricted to a single, fixed 256 color palette?
> (I don't see any color palette parameter to the bmp device.) Is there any
> way to create an R bmp that is hicolor (15 or 16-bit color) or true color
> (24-bit color) to avoid the palette issues of 8-bit bmps?
>
> Thanks for any info about this.
>
> efg
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Feb  8 22:42:59 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 8 Feb 2005 21:42:59 +0000 (GMT)
Subject: [R] simple example of C interface to R
In-Reply-To: <D9A95B4B7B20354992E165EEADA31999056A92C7@uswpmx00.merck.com>
References: <D9A95B4B7B20354992E165EEADA31999056A92C7@uswpmx00.merck.com>
Message-ID: <Pine.LNX.4.61.0502082133490.17539@gannet.stats>

Did none of you notice the Makefile in that directory?

On Tue, 8 Feb 2005, Huntsinger, Reid wrote:

> Rtest.o is just an object file, and the second call to gcc is really a call
> to ld and creates Rtest.so, a shared library. Neither is executable.
> Probably you want to compile Rtest.c into an executable; it does have the
> "main" function after all. Perhaps
>
> gcc -L/usr/local/lib -o Rtest Rtest.o   -L/usr/lib/R/bin -lR

Should be -L`R RHOME`/lib for R>=2.0.0 (and that Makefile needs updating). 
>From ONEWS

     o   The dynamic libraries libR and libRlapack are now installed in
         R_HOME/lib rather than R_HOME/bin.

You may also need to set LD_LIBRARY_PATH: I do to include /usr/local/lib.


> would work.
>
> Reid Huntsinger
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of jbdunsmo at utmb.edu
> Sent: Tuesday, February 08, 2005 12:14 PM
> To: Rau, Roland; r-help at stat.math.ethz.ch
> Subject: Re: [R] simple example of C interface to R
>
>
> On Tue, Feb 08, 2005 at 10:55:01AM +0100, Rau, Roland wrote:
>>
>> do you know already the page of Roger D. Peng?
>> He has a document entitled "An Introduction to the .C Interface to R".
>> It is located at:
>> http://www.biostat.jhsph.edu/~rpeng/docs/interface.pdf
>>
>
> thanks.  that's a nice tutorial, but what i'd really like to do is
> write a program with R embedded in it (and not have to load the
> interpreter).  i found some documentation for doing that here:
> http://cran.r-project.org/doc/manuals/R-exts.html#Embedding-R-under-Unix_002
> dalikes
>
> now i'm going through the tests/Embedding directory (included with the
> R source distribution).
>
> i've gotten Rtest.c to compile with:
>
>    $ R CMD SHLIB Rtest.c
>    gcc -I/usr/lib/R/include  -I/usr/local/include -D__NO_MATH_INLINES
> -mieee-fp  -fPIC  -march=pentium4 -O3 -pipe -fomit-frame-pointer -c Rtest.c
> -o Rtest.o
>    gcc -shared -L/usr/local/lib -o Rtest.so Rtest.o   -L/usr/lib/R/bin -lR
>
> then when i try to run Rtest.o, i get:
>
>    -/bin/bash: ./Rtest.o: cannot execute binary file
>
> Rtest.c contains:
>
>    #include "embeddedRCall.h"
>
>    int
>    main(int argc, char *argv[])
>    {
>      eval_R_command("print", argc, argv);
>      return(0);
>    }
>
> embeddedRCall.h contains:
>
>    #ifndef EMBEDDED_R_CALL_H
>    #define EMBEDDED_R_CALL_H
>
>    #include <Rinternals.h>
>
>    int eval_R_command(const char *funcName, int argc, char *argv[]);
>    SEXP Test_tryEval(SEXP expression, int *errorOccurred);
>    void init_R(int argc, char **argv);
>
>    #endif
>
> i also get the "cannot execute binary file" error when trying to
> compile and run the following code from the documentation:
>
>    int Rf_initEmbeddedR(int argc, char **argv)
>    { /* This is already compiled into R */
>      Rf_initialize_R(argc, argv);
>      setup_Rmainloop();
>      return(1);
>    }
>
>    int main(int ac, char **av)
>    {
>      /* do some setup */
>      // Rf_initEmbeddedR(argc, argv);
>      /* do some more setup */
>
>      char *argv[]= {"REmbeddedPostgres", "--gui=none", "--silent"};
>      Rf_initEmbeddedR(sizeof(argv)/sizeof(argv[0]), argv);
>
>      /* submit some code to R, which is done interactively via
>            run_Rmainloop();
>      */
>      end_Rmainloop();
>      return 0;
>    }
>
>
> what am i doing wrong?
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From cznm4 at mizzou.edu  Tue Feb  8 22:52:40 2005
From: cznm4 at mizzou.edu (Chao Zhu)
Date: Tue, 8 Feb 2005 15:52:40 -0600
Subject: [R] batch jobs question
Message-ID: <005a01c50e28$837b6530$0923ce80@chao>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050208/f039f3b4/attachment.pl

From jbdunsmo at utmb.edu  Tue Feb  8 23:02:34 2005
From: jbdunsmo at utmb.edu (jbdunsmo@utmb.edu)
Date: Tue, 8 Feb 2005 16:02:34 -0600
Subject: [R] simple example of C interface to R
In-Reply-To: <D9A95B4B7B20354992E165EEADA31999056A92C7@uswpmx00.merck.com>
References: <D9A95B4B7B20354992E165EEADA31999056A92C7@uswpmx00.merck.com>
Message-ID: <20050208220234.GA25871@g20458.utmb.edu>

thanks for all the help.  i've tried everyone's suggestions, to no
avail...

On Tue, Feb 08, 2005 at 04:19:10PM -0500, Huntsinger, Reid wrote:
> Rtest.o is just an object file, and the second call to gcc is really a call
> to ld and creates Rtest.so, a shared library. Neither is executable.
> Probably you want to compile Rtest.c into an executable; it does have the
> "main" function after all. Perhaps 
> 
> gcc -L/usr/local/lib -o Rtest Rtest.o   -L/usr/lib/R/bin -lR
> 
> would work. 

that didn't work:

    $ gcc -L/usr/local/lib -o Rtest Rtest.o   -L/usr/lib/R/bin -lR
    Rtest.o(.text+0x2f): In function `main':
    : undefined reference to `eval_R_command'
    collect2: ld returned 1 exit status
    

On Tue, Feb 08, 2005 at 11:40:08AM -0800, Duncan Temple Lang wrote:
>
> The .o file is object code that is then used when creating an
> executable.   You cannot run it directly.
> You can use the Makefile in that tests/Embedding/ directory
> to build the Rtest application for you. 

    $ cd R-1.9.0/tests/Embedding/
    $ ./configure
    -/bin/bash: ./configure: No such file or directory
    $ make
    make: *** No targets specified and no makefile found.  Stop.
    $ aclocal
    aclocal-1.9: `configure.ac' or `configure.in' is required
    $ automake
    automake-1.9: `configure.ac' or `configure.in' is required
    $ autoconf
    autoconf-2.59: no input file

do i have to write my own configure script?


On Tue, Feb 08, 2005 at 09:42:59PM +0000, Prof Brian Ripley wrote:
> Did none of you notice the Makefile in that directory?
>
> Should be -L`R RHOME`/lib for R>=2.0.0 (and that Makefile needs
> updating). 
> From ONEWS
>
>     o   The dynamic libraries libR and libRlapack are now installed
>     in
>         R_HOME/lib rather than R_HOME/bin.
>
> You may also need to set LD_LIBRARY_PATH: I do to include
> /usr/local/lib.
>

    $ gcc -L`R RHOME`/lib -o Rtest Rtest.o -L/usr/lib/R/bin/ -lR
    Rtest.o(.text+0x2f): In function `main':
    : undefined reference to `eval_R_command'
    collect2: ld returned 1 exit status


it looks like someone's going to have to spell it out for me.



From rolf at math.unb.ca  Tue Feb  8 23:03:44 2005
From: rolf at math.unb.ca (Rolf Turner)
Date: Tue, 8 Feb 2005 18:03:44 -0400 (AST)
Subject: [R] batch jobs question
Message-ID: <200502082203.j18M3i4w011835@erdos.math.unb.ca>


> I'm doing some R batch jobs in Unix. 
> Something like
> R <prog1> output1 --save &
> R <prog2> output2 --save &
> 
> prog1 and prog2 are running at the same time and they are essentially
> same except it contains different parameter values.  I was wondering
> if two processes will affect each other? Hopefully they are two
> independent jobs.

If you are running these jobs in the same directory, then whichever
one finishes ***last*** will overwrite the workspace saved by the one
that finished first.  E.g. if there was nothing in .RData to start
with and prog1 creates an object `X'' and prog2 creates an object
``Y'', and prog2 finishes last, then .RData will have Y in it when
all is done, and will NOT have X in it.

You could either run the jobs in separate directories, or use save()
explicitly --- with separate filenames --- in the code in prog1 and
prog2 (rather than using the --save flag).

				cheers,

					Rolf Turner
					rolf at math.unb.ca



From hold9309 at uidaho.edu  Tue Feb  8 23:05:29 2005
From: hold9309 at uidaho.edu (Zachary Holden)
Date: Tue, 08 Feb 2005 14:05:29 -0800
Subject: [R] ?manova multiple comparisons
Message-ID: <b1954b21b3.b21b3b1954@uidaho.edu>

R wizards,

I'm trying to extract multiple comparisons from a MANOVA in R. I've exhausted other resources, and don't want to revert to using SAS for this... 

I'm doing simple MANOVA's in R, with a bivariate response variable (Y) and a treatment with four levels (1,2,3,4) 

model<- manova(Y~TREATMENT, test="Hotelling")

I'm unable to find guidance or code on how I can test for signficant differences between individual levels of my treatment, as you'd do with TukeyHSD in a univariate analysis situation. 

Can anyone help me out? 

Thanks in advance,
Zack



From ripley at stats.ox.ac.uk  Tue Feb  8 23:16:42 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 8 Feb 2005 22:16:42 +0000 (GMT)
Subject: [R] simple example of C interface to R
In-Reply-To: <20050208220234.GA25871@g20458.utmb.edu>
References: <D9A95B4B7B20354992E165EEADA31999056A92C7@uswpmx00.merck.com>
	<20050208220234.GA25871@g20458.utmb.edu>
Message-ID: <Pine.LNX.4.61.0502082212500.18839@gannet.stats>

On Tue, 8 Feb 2005 jbdunsmo at utmb.edu wrote:

> thanks for all the help.  i've tried everyone's suggestions, to no
> avail...

Not *my* suggestion, though.  Mine was to use the Makefile, which has

Rtest: Rtest.o embeddedRCall.o
         $(R_CMD_LINK) -o $@ Rtest.o embeddedRCall.o $(LIBR)

and just needs the correction I mentioned.


> On Tue, Feb 08, 2005 at 04:19:10PM -0500, Huntsinger, Reid wrote:
>> Rtest.o is just an object file, and the second call to gcc is really a call
>> to ld and creates Rtest.so, a shared library. Neither is executable.
>> Probably you want to compile Rtest.c into an executable; it does have the
>> "main" function after all. Perhaps
>>
>> gcc -L/usr/local/lib -o Rtest Rtest.o   -L/usr/lib/R/bin -lR
>>
>> would work.
>
> that didn't work:
>
>    $ gcc -L/usr/local/lib -o Rtest Rtest.o   -L/usr/lib/R/bin -lR
>    Rtest.o(.text+0x2f): In function `main':
>    : undefined reference to `eval_R_command'
>    collect2: ld returned 1 exit status
>
>
> On Tue, Feb 08, 2005 at 11:40:08AM -0800, Duncan Temple Lang wrote:
>>
>> The .o file is object code that is then used when creating an
>> executable.   You cannot run it directly.
>> You can use the Makefile in that tests/Embedding/ directory
>> to build the Rtest application for you.
>
>    $ cd R-1.9.0/tests/Embedding/
>    $ ./configure
>    -/bin/bash: ./configure: No such file or directory
>    $ make
>    make: *** No targets specified and no makefile found.  Stop.
>    $ aclocal
>    aclocal-1.9: `configure.ac' or `configure.in' is required
>    $ automake
>    automake-1.9: `configure.ac' or `configure.in' is required
>    $ autoconf
>    autoconf-2.59: no input file
>
> do i have to write my own configure script?
>
>
> On Tue, Feb 08, 2005 at 09:42:59PM +0000, Prof Brian Ripley wrote:
>> Did none of you notice the Makefile in that directory?
>>
>> Should be -L`R RHOME`/lib for R>=2.0.0 (and that Makefile needs
>> updating).
>> From ONEWS
>>
>>     o   The dynamic libraries libR and libRlapack are now installed
>>     in
>>         R_HOME/lib rather than R_HOME/bin.
>>
>> You may also need to set LD_LIBRARY_PATH: I do to include
>> /usr/local/lib.
>>
>
>    $ gcc -L`R RHOME`/lib -o Rtest Rtest.o -L/usr/lib/R/bin/ -lR
>    Rtest.o(.text+0x2f): In function `main':
>    : undefined reference to `eval_R_command'
>    collect2: ld returned 1 exit status
>
>
> it looks like someone's going to have to spell it out for me.
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jost at cict.fr  Tue Feb  8 23:17:54 2005
From: jost at cict.fr (Christian Jost)
Date: Tue, 8 Feb 2005 23:17:54 +0100
Subject: [R] testing exponential distributions
Message-ID: <a06002027be2ee880fa18@[130.120.104.141]>

Dear all,

I try to test behavioural data durations whether they follow an 
exponential distribution. My data have the particularity that there 
are no values below a certain threshold, and the only test adapted to 
this case I could find is the Shapiro-Wilk test for exponentiality 
(Shapiro & Wilk 1972, Technometrics 14, 355-370). But it requires to 
compute Shapiro and Wilks critical W values. Is there any R function 
that lets me compute these values? I checked out the shapiro.test 
source which seems to compute these values in swilk.c, but I could 
not find an R function wrapper that would compute either these 
critical values for a given alpha and n (number of values) or a 
function that comptues directly the p-value for a given value of the 
W statistic and n. Any idea where I could find that? Or are there 
specific packages that let me test for exponentiality (tried package 
"survival", but my stat level seems to be too low to understand 
application of this package beyond simple survival curve 
visualisation).

Thanks for any help, Christian.



From tuechler at gmx.at  Tue Feb  8 23:37:17 2005
From: tuechler at gmx.at (Heinz Tuechler)
Date: Tue, 08 Feb 2005 23:37:17 +0100
Subject: [R] How to get variable names in a function?
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E681@usrymx25.merck.co
 m>
Message-ID: <3.0.6.32.20050208233717.007b5370@pop.gmx.net>

At 12:51 08.02.2005 -0500, Liaw, Andy wrote:
>This might be easier for your purpose:
>
>> lapply(list(bravo=bravo, charly=charly), table)
>$bravo
>
>1 2 3 5 
>2 1 1 3 
>
>$charly
>
>1 2 4 7 
>1 1 2 2 
>
>Andy
>
Thank you for your answer. In the case of the example it is sufficient, but
I see that I did not choose it well.
What I search a solution for, is in general to use a variable and its name
within a function, e.g. to write a headline like "Results for
<variablename>" and to print the table(<variablename>) and maybe a test for
the same variable below it.
I solved this for one variable, but not too well for a list of variables.
In any case, thanks,
Heinz
>
>> From: Heinz Tuechler
>> 
>> Hello,
>> 
>> applying a function to a list of variables I face the 
>> following problem:
>> Let's say I want to compute tables for several variables. I 
>> could write a
>> command for every single table, like
>> bravo<-c(1,1,2,3,5,5,5,);charly<-c(7,7,4,4,2,1)
>> table(bravo); table(charly)
>> > table(bravo); table(charly)
>> bravo
>> 1 2 3 5 
>> 2 1 1 3 
>> charly
>> 1 2 4 7 
>> 1 1 2 2 
>> The results are two tables with the names of the variables above each.
>> If I want to do the same thing by a function I find no way to get the
>> variable names above the tables. 
>> demofn<-function(varlist)
>>     {for (i in seq(along=varlist))
>>        {cat(deparse(varlist[i])) # < - - - - how to change this?
>>         print(table(varlist[i]))}}
>> > demofn(list(bravo, charly))
>> list(c(1, 1, 2, 3, 5, 5, 5))
>> 1 2 3 5 
>> 2 1 1 3 
>> list(c(7, 7, 4, 4, 2, 1))
>> 1 2 4 7 
>> 1 1 2 2 
>> > 
>> 
>> Thanks,
>> Heinz T?chler
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>> 
>> 
>
>
>---------------------------------------------------------------------------
---
>Notice:  This e-mail message, together with any attachments, contains
information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New
Jersey, USA 08889), and/or its affiliates (which may be known outside the
United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as
Banyu) that may be confidential, proprietary copyrighted and/or legally
privileged. It is intended solely for the use of the individual or entity
named on this message.  If you are not the intended recipient, and have
received this message in error, please notify us immediately by reply
e-mail and then delete it from your system.
>---------------------------------------------------------------------------
---
>
>



From tuechler at gmx.at  Tue Feb  8 23:37:24 2005
From: tuechler at gmx.at (Heinz Tuechler)
Date: Tue, 08 Feb 2005 23:37:24 +0100
Subject: [R] How to get variable names in a function?
In-Reply-To: <200502081832.j18IWWAK021954@compton.gene.com>
References: <3.0.6.32.20050208174511.007abc30@pop.gmx.net>
Message-ID: <3.0.6.32.20050208233724.007b2100@pop.gmx.net>

At 10:32 08.02.2005 -0800, Berton Gunter wrote:
>If you pass your function a NAMED list, then the following works:
>
>demofn<-function(varlist)
>{
>	nm<-names(varlist)
>    for (i in seq(along=varlist))
>       {cat('\n',nm[i]) 
>       print(table(varlist[[i]]))
>    }
>}
>demofn(list(bravo=bravo, charly=charly))
>
>If you don't pass a named list, then you need to restrict and know the form
>of the expression that is the varlist argument in order to substitute() and
>deparse it correctly to get the identifiers you want. For example, if you
>knew that varlist were an expression of the form:
>list(var1,var2,var3,...) 
>then you could get the ith identifier vari via:
>
>deparse((as.list(substitute(varlist))[-1])[[i]]) 
>
>HOWEVER, this is probably inefficient and **clearly** clumsy, undesirable,
>and almost certain to fail (so don't do this!). 
>
>If the number of tables is small enough that you could simply list them as
>arguments (as opposed to constructing the list of vectors to be tabled in
>some way), then the function call could be of the form function(...) and the
>... arguments could be processed as discussed in section 3.1 of V&R's S
>PROGRAMMING. That is, your example call would be of the form:
>demofn(bravo,charly), and you can forgo lists in the call altogether. This
>strategy actually also works for long constructed lists of arguments using
>do.call() -- see it's help file and V&R again for details.
>
>
>-- Bert Gunter
>Genentech Non-Clinical Statistics
>South San Francisco, CA
> 
>"The business of the statistician is to catalyze the scientific learning
>process."  - George E. P. Box
> 
Thank you for your detailled answer. I see that it is not as easy as I
expected and I will think about a convenient way to do it.
I tried also the opposit approach i.e. to pass a vector of names to the
function like c("bravo", "charly") but until now I did not solve it either.
A third way worked, but seemed to me less generally applicable.
I stored all the names of the variables of a data.frame in a vector, made
the selection of the variables by match() and passed the indices of the
selected variables to the function. By this method I can then access each
variable by its index and also its name via the vector of the stored names.
It seemed to me a crude method of a beginner like me and I still hope to
find a better one.
Your suggestion with the named list may be the best, if I find a practical
way to produce this named list by some function from a simple list without
retyping each name. 

Thanks again,

Heinz
> 
>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch 
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Heinz Tuechler
>> Sent: Tuesday, February 08, 2005 8:45 AM
>> To: r-help at stat.math.ethz.ch
>> Subject: [R] How to get variable names in a function?
>> 
>> Hello,
>> 
>> applying a function to a list of variables I face the 
>> following problem:
>> Let's say I want to compute tables for several variables. I 
>> could write a
>> command for every single table, like
>> bravo<-c(1,1,2,3,5,5,5,);charly<-c(7,7,4,4,2,1)
>> table(bravo); table(charly)
>> > table(bravo); table(charly)
>> bravo
>> 1 2 3 5 
>> 2 1 1 3 
>> charly
>> 1 2 4 7 
>> 1 1 2 2 
>> The results are two tables with the names of the variables above each.
>> If I want to do the same thing by a function I find no way to get the
>> variable names above the tables. 
>> demofn<-function(varlist)
>>     {for (i in seq(along=varlist))
>>        {cat(deparse(varlist[i])) # < - - - - how to change this?
>>         print(table(varlist[i]))}}
>> > demofn(list(bravo, charly))
>> list(c(1, 1, 2, 3, 5, 5, 5))
>> 1 2 3 5 
>> 2 1 1 3 
>> list(c(7, 7, 4, 4, 2, 1))
>> 1 2 4 7 
>> 1 1 2 2 
>> > 
>> 
>> Thanks,
>> Heinz T?chler
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>> 
>
>
>



From ripley at stats.ox.ac.uk  Tue Feb  8 23:40:05 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 8 Feb 2005 22:40:05 +0000 (GMT)
Subject: [R] batch jobs question
In-Reply-To: <005a01c50e28$837b6530$0923ce80@chao>
References: <005a01c50e28$837b6530$0923ce80@chao>
Message-ID: <Pine.LNX.4.61.0502082237250.19222@gannet.stats>

On Tue, 8 Feb 2005, Chao Zhu wrote:

> I'm doing some R batch jobs in Unix.
> Something like
> R <prog1> output1 --save &
> R <prog2> output2 --save &

Consider R CMD BATCH instead: it redirects the error/warnings too, which I 
expect you would want to do.

> prog1 and prog2 are running at the same time and they are essentially 
> same except it contains different parameter values. I was wondering if 
> two processes will affect each other? Hopefully they are two independent 
> jobs.

They will save in the same file unless you run them from separate 
directories.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jm at 10squaredcorp.com  Tue Feb  8 21:08:02 2005
From: jm at 10squaredcorp.com (Jim Michael)
Date: Tue, 8 Feb 2005 20:08:02 +0000
Subject: [R] consultation
In-Reply-To: <4208F590.9080703@uapar.edu>
References: <4208F590.9080703@uapar.edu>
Message-ID: <200502082008.02252.jm@10squaredcorp.com>

Perhaps this is useful: 

QuantLib: http://www.quantlib.org/ 
Extensions: http://www.quantlib.org/extensions.shtml 
RQuantLib: http://dirk.eddelbuettel.com/code/rquantlib.html

Cheers,

Jim

On Tuesday 08 February 2005 17:23, Lic. Adri?n Cecotto - FACEA wrote:
> R People
> Is it possible to make financial calculus with are. anybody can
> help me on it?
> TU
> Adri?n



From f.harrell at vanderbilt.edu  Wed Feb  9 01:24:29 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 08 Feb 2005 18:24:29 -0600
Subject: [R] Re:logistic regression
In-Reply-To: <1107874523.4208d2db83d81@webmail.unb.ca>
References: <20050208112016.13774.qmail@web41206.mail.yahoo.com>	<4208B351.4080909@vanderbilt.edu>
	<1107874523.4208d2db83d81@webmail.unb.ca>
Message-ID: <4209583D.3000305@vanderbilt.edu>

Joe Nocera wrote:
> Helene - 
> 
> In addition to some of the excellent suggestions already posited (e.g. examining AIC,
> pseudo R^2 in the Design package), you might want to consider another tool to assess
> logistic regression model accuracy: the area-under-curve (AUC) from a
> receiver-operating characteristic (ROC).
> 
> The ROC curve describes the relationship between the number of true positives observed
> (sensitivity) to false positives, and also for negatives.  The AUC is the probability
> that a model can correctly distinguish between the two.  This is an appealling
> alternative to some of the known issues of citing only a pseudo-R^2 (like Nagelkerke's
> for instance) to describe 'fit'.

That's also standard output in Design's lrm function (C index).  But 
unlike R^2 comparing two models on the basis of ROC area is not very 
sensitive.  -Frank

> 
> Check out the ROC functions available at the Bioconductor website.  There was also some
> code sent around on the list a few months back for calculating trapeziodal AUC, se's
> from ROC, and comparing two ROC curves...search the archives if interested, or I can
> probably dig them out for you offline...
> 
> Cheers,
> Joe
> 
> Quoting Frank E Harrell Jr <f.harrell at vanderbilt.edu>:
> 
> 
>>Vito Ricci wrote:
>>
>>>Hi,
>>>
>>>I don't know if a pseudo squared R for glm exists in
>>>any R package, but I find some interesting functions
>>>in S mailing list:
>>
>>It is included in lrm in the Design package.  But note that this is not 
>>for checking fit but rather for quantifying predictive discrimination.
>>
>>.....
>>
>>-- 
>>Frank E Harrell Jr   Professor and Chair           School of Medicine
>>                      Department of Biostatistics   Vanderbilt University
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
> 
> 
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> Joseph J. Nocera
> Ph.D. Candidate
> NB Coop. Fish & Wildlife Research Unit
> Biology Department - Univ. New Brunswick
> Fredericton, NB
> Canada   E3B 6E1
> tel: (902) 679-5733
> 
> "Why does it have to be spiders?  Why can't it be 'follow the butterflies'"?!
>     - Ron Weasley, Harry Potter & The Chamber of Secrets
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From andy_liaw at merck.com  Wed Feb  9 01:26:57 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 8 Feb 2005 19:26:57 -0500
Subject: [R] How to get variable names in a function?
Message-ID: <3A822319EB35174CA3714066D590DCD50994E68D@usrymx25.merck.com>

Here's a (rather ugly) function that takes its arguments and turn them into
a list, using the names of the arguments and the names of the components:

listWithName <- function(...) {
    aname <- sapply(as.list(substitute({...})[-1]), deparse)
    res <- list(...)
    names(res) <- aname
    res
}

Here are a couple of examples:

> x <- runif(5)
> y <- "abc"
> listWithName(x, y)
$x
[1] 0.44241647 0.07540958 0.46405311 0.01684738 0.94370893

$y
[1] "abc"

> listWithName(x, 1:3, y)
$x
[1] 0.44241647 0.07540958 0.46405311 0.01684738 0.94370893

$"1:3"
[1] 1 2 3

$y
[1] "abc"

> listWithName(abc=x, y)
$x
[1] 0.44241647 0.07540958 0.46405311 0.01684738 0.94370893

$y
[1] "abc"

You can then use lapply() on the output.  lapply() will automatically attach
names to the components of its output.

Andy


> From: Heinz Tuechler
> 
> At 10:32 08.02.2005 -0800, Berton Gunter wrote:
> >If you pass your function a NAMED list, then the following works:
> >
> >demofn<-function(varlist)
> >{
> >	nm<-names(varlist)
> >    for (i in seq(along=varlist))
> >       {cat('\n',nm[i]) 
> >       print(table(varlist[[i]]))
> >    }
> >}
> >demofn(list(bravo=bravo, charly=charly))
> >
> >If you don't pass a named list, then you need to restrict 
> and know the form
> >of the expression that is the varlist argument in order to 
> substitute() and
> >deparse it correctly to get the identifiers you want. For 
> example, if you
> >knew that varlist were an expression of the form:
> >list(var1,var2,var3,...) 
> >then you could get the ith identifier vari via:
> >
> >deparse((as.list(substitute(varlist))[-1])[[i]]) 
> >
> >HOWEVER, this is probably inefficient and **clearly** 
> clumsy, undesirable,
> >and almost certain to fail (so don't do this!). 
> >
> >If the number of tables is small enough that you could 
> simply list them as
> >arguments (as opposed to constructing the list of vectors to 
> be tabled in
> >some way), then the function call could be of the form 
> function(...) and the
> >... arguments could be processed as discussed in section 3.1 
> of V&R's S
> >PROGRAMMING. That is, your example call would be of the form:
> >demofn(bravo,charly), and you can forgo lists in the call 
> altogether. This
> >strategy actually also works for long constructed lists of 
> arguments using
> >do.call() -- see it's help file and V&R again for details.
> >
> >
> >-- Bert Gunter
> >Genentech Non-Clinical Statistics
> >South San Francisco, CA
> > 
> >"The business of the statistician is to catalyze the 
> scientific learning
> >process."  - George E. P. Box
> > 
> Thank you for your detailled answer. I see that it is not as easy as I
> expected and I will think about a convenient way to do it.
> I tried also the opposit approach i.e. to pass a vector of 
> names to the
> function like c("bravo", "charly") but until now I did not 
> solve it either.
> A third way worked, but seemed to me less generally applicable.
> I stored all the names of the variables of a data.frame in a 
> vector, made
> the selection of the variables by match() and passed the 
> indices of the
> selected variables to the function. By this method I can then 
> access each
> variable by its index and also its name via the vector of the 
> stored names.
> It seemed to me a crude method of a beginner like me and I 
> still hope to
> find a better one.
> Your suggestion with the named list may be the best, if I 
> find a practical
> way to produce this named list by some function from a simple 
> list without
> retyping each name. 
> 
> Thanks again,
> 
> Heinz
> > 
> >
> >> -----Original Message-----
> >> From: r-help-bounces at stat.math.ethz.ch 
> >> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Heinz Tuechler
> >> Sent: Tuesday, February 08, 2005 8:45 AM
> >> To: r-help at stat.math.ethz.ch
> >> Subject: [R] How to get variable names in a function?
> >> 
> >> Hello,
> >> 
> >> applying a function to a list of variables I face the 
> >> following problem:
> >> Let's say I want to compute tables for several variables. I 
> >> could write a
> >> command for every single table, like
> >> bravo<-c(1,1,2,3,5,5,5,);charly<-c(7,7,4,4,2,1)
> >> table(bravo); table(charly)
> >> > table(bravo); table(charly)
> >> bravo
> >> 1 2 3 5 
> >> 2 1 1 3 
> >> charly
> >> 1 2 4 7 
> >> 1 1 2 2 
> >> The results are two tables with the names of the variables 
> above each.
> >> If I want to do the same thing by a function I find no way 
> to get the
> >> variable names above the tables. 
> >> demofn<-function(varlist)
> >>     {for (i in seq(along=varlist))
> >>        {cat(deparse(varlist[i])) # < - - - - how to change this?
> >>         print(table(varlist[i]))}}
> >> > demofn(list(bravo, charly))
> >> list(c(1, 1, 2, 3, 5, 5, 5))
> >> 1 2 3 5 
> >> 2 1 1 3 
> >> list(c(7, 7, 4, 4, 2, 1))
> >> 1 2 4 7 
> >> 1 1 2 2 
> >> > 
> >> 
> >> Thanks,
> >> Heinz T?chler
> >> 
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide! 
> >> http://www.R-project.org/posting-guide.html
> >> 
> >
> >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From Robert.Denham at nrm.qld.gov.au  Wed Feb  9 05:37:15 2005
From: Robert.Denham at nrm.qld.gov.au (Denham Robert)
Date: Wed, 9 Feb 2005 14:37:15 +1000
Subject: [R] Dates labels on axes in xyplot
Message-ID: <66CE7F4ACF0C74439F9F6E0A568293020BA6EE@INDMAIL.lands.resnet.qg>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050209/f716724f/attachment.pl

From edd at debian.org  Wed Feb  9 05:57:07 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 8 Feb 2005 22:57:07 -0600
Subject: [R] Dates labels on axes in xyplot
In-Reply-To: <66CE7F4ACF0C74439F9F6E0A568293020BA6EE@INDMAIL.lands.resnet.qg>
References: <66CE7F4ACF0C74439F9F6E0A568293020BA6EE@INDMAIL.lands.resnet.qg>
Message-ID: <16905.38947.121212.588113@basebud.nulle.part>


On 9 February 2005 at 14:37, Denham Robert wrote:
| I am a bit confused about how to get the format of the labels in xyplot
| to show as dates rather than the numeric value.
|  
| e.g., 
|  
| x <- as.Date(c("20/01/2001","20/02/2003","21/06/2004"),"%d/%m/%Y")
| y <- c(1,2,3)
|  
| plot(y~x)     #produces formatted labels
|  
| xyplot(y~x)  #doesn't give formatted x labels
|  
| xyplot(y~x,scales=list(x=list(format="%d/%m/%Y"))) #still doesn't.
|  
| How do I get the format for the xlabels ?

You need another as.POSIXct():

> x <- as.POSIXct(as.Date(c("20/01/2001","20/02/2003","21/06/2004"),"%d/%m/%Y"))
> y <- c(1,2,3)
> xyplot(y~x)

Dirk

-- 
Better to have an approximate answer to the right question than a precise 
answer to the wrong question.  --  John Tukey as quoted by John Chambers



From Tom.Mulholland at dpi.wa.gov.au  Wed Feb  9 06:24:23 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Wed, 9 Feb 2005 13:24:23 +0800
Subject: [R] Contour plot
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA86@afhex01.dpi.wa.gov.au>

x <- rep(seq(1,10),10)
y <- rep(seq(1,10),each = 10)
z <- matrix(100 + x*0.5 + y*0.5,ncol=10,byrow=T)
contour(z)

or back to where you were originally


x <- 1:10
y <- 1:10
# z <- 100:110
z <- 1:100
contour(matrix(z,ncol=10))

read the help on contour it states that 
 z: a matrix containing the values to be plotted ('NA's are
          allowed).  Note that 'x' can be used instead of 'z' for
          convenience.

So 

contour(matrix(runif(100),ncol=10))

works

I can't actually say why there is an x and a y


x <- log(1:10)
y <- log(1:10)
z <- 1:100
contour(matrix(z,ncol=10))

did not do what I expected it to do. I expected the contour lines to become compressed gradually.

> -----Original Message-----
> From: dhkblaszyk at zeelandnet.nl [mailto:dhkblaszyk at zeelandnet.nl]
> Sent: Wednesday, 9 February 2005 12:44 AM
> To: r-help at stat.math.ethz.ch
> Subject: Fw: [R] Contour plot
> 
> 
> I understand that I need to have a (in this case) square 
> matrix with all the
> data. But the question now is;
> 
> - can the contourplot not interpolate the missing values

What missing values? A 10 by 10 grid is 100 possibilities, you gave 10. Which 10?

x <- 1:10
y <- 1:10
z <- matrix(numeric(100),ncol=10)
diag(z) <- 1:10
contour(z)

or 


x <- 1:10
y <- 1:10
z <- matrix(numeric(100),ncol=10)
z[trunc(runif(10) * 100)] <- 1:10
contour(z)

are two possibilities

> 
> or alternatively
> 
> - I have fit a model to the z data (z = 100 + 0.5x + 0.5y). 
> How can I make
> from this model a "square" matrix z to make a contour plot?
> 
> Kind regards, Darius Blaszijk
> 
> ----- Original Message -----
> From: "Achim Zeileis" <Achim.Zeileis at wu-wien.ac.at>
> To: <dhkblaszyk at zeelandnet.nl>
> Cc: <r-help at stat.math.ethz.ch>
> Sent: Tuesday, February 08, 2005 1:51 AM
> Subject: Re: [R] Contour plot
> 
> 
> > On Tue, 8 Feb 2005 01:15:06 +0100 dhkblaszyk at zeelandnet.nl wrote:
> >
> > > Hello,
> > >
> > > I would like to make a contourplot of the following data;
> > >
> > > > x <- 1:10
> > > > y <- 1:10
> > > > z <- 100:110
> > >
> > > By doing >contour(x,y,z) I get the following error;
> > >
> > > "Error in contour.default(x, y, z) : no proper `z' matrix 
> specified"
> > >
> > > How do I fix this??
> >
> > x and y specify a grid and thus z must provide a value for each
> > combination of the x's and y's! For example:
> >   x <- y <- 1:10
> >   contour(x, y, outer(x, y))
> > Also look at
> >   outer(x, y)
> > and read ?contour.
> >
> > Z
> >
> > > Kind regards, Datius Blaszijk
> > >
> > > [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From renaud.lancelot at cirad.fr  Wed Feb  9 07:08:39 2005
From: renaud.lancelot at cirad.fr (Renaud Lancelot)
Date: Wed, 09 Feb 2005 09:08:39 +0300
Subject: [R] Dates labels on axes in xyplot
In-Reply-To: <16905.38947.121212.588113@basebud.nulle.part>
References: <66CE7F4ACF0C74439F9F6E0A568293020BA6EE@INDMAIL.lands.resnet.qg>
	<16905.38947.121212.588113@basebud.nulle.part>
Message-ID: <4209A8E7.60607@cirad.fr>

Dirk Eddelbuettel a ?crit :

> On 9 February 2005 at 14:37, Denham Robert wrote:
> | I am a bit confused about how to get the format of the labels in xyplot
> | to show as dates rather than the numeric value.
> |  
> | e.g., 
> |  
> | x <- as.Date(c("20/01/2001","20/02/2003","21/06/2004"),"%d/%m/%Y")
> | y <- c(1,2,3)
> |  
> | plot(y~x)     #produces formatted labels
> |  
> | xyplot(y~x)  #doesn't give formatted x labels
> |  
> | xyplot(y~x,scales=list(x=list(format="%d/%m/%Y"))) #still doesn't.
> |  
> | How do I get the format for the xlabels ?
> 
> You need another as.POSIXct():
> 
> 
>>x <- as.POSIXct(as.Date(c("20/01/2001","20/02/2003","21/06/2004"),"%d/%m/%Y"))
>>y <- c(1,2,3)
>>xyplot(y~x)
> 
> 
> Dirk
> 

Does not work for me (R 2.0.1, Windows, current version of lattice).

The following does:

x <- as.Date(c("20/01/2001", "20/02/2003", "21/06/2004"), "%d/%m/%Y")
y <- c(1,2,3)

library(lattice)

AT <- pretty(x)
LAB <- as.Date("1970-01-01") + AT
xyplot(y ~ x,
   scales = list(x = list(at = AT, labels = format(LAB, "%d/%m/%Y"))))

Best,

Renaud


-- 
Dr Renaud Lancelot, v?t?rinaire
C/0 Ambassade de France - SCAC
BP 834 Antananarivo 101 - Madagascar

e-mail: renaud.lancelot at cirad.fr
tel.:   +261 32 40 165 53 (cell)
         +261 20 22 665 36 ext. 225 (work)
         +261 20 22 494 37 (home)



From hellik at web.de  Wed Feb  9 07:33:06 2005
From: hellik at web.de (Helmut Kudrnovsky)
Date: Wed, 09 Feb 2005 07:33:06 +0100
Subject: [R] Data manipulation
Message-ID: <1705191734@web.de>

thanks a lot  for the information, reshape did the job

> datars <-reshape(data, timevar="TERRCODE", idvar="BID", direction="wide")

greetings
helli

BID	TERRCODE	ANMCODE
200310413120660	22	0
200310413120660	273	0
200310413120660	280	0
200310413120660	467	0
200310413120660	468	0
200310413127001	5	0
200310413127001	50	0
200310413127001	53	13
200310413127001	54	11
200310413127001	72	0
200310413127001	89	0
200310413127001	671	0
200310413225032	1	0
200310413225032	3	0
200310413225032	6	0
200310413225032	51	0
200310413225032	52	21
200310413225032	53	21
200310413225032	54	21
200310413225032	55	13
200310413225032	57	11
200310413225032	72	0
....

result:

BID	ANMCODE.1	ANMCODE.2	ANMCODE.3	ANMCODE.4	ANMCODE.5	ANMCODE.6	ANMCODE.7 ....	
200310413120660	NA	NA	NA	NA	NA	NA	NA	NA	NA	NA	NA	NA	
200310413127001	NA	NA	NA	NA	0	NA	NA	NA	NA	NA	NA	NA	
200310413225032	0	NA	0	NA	NA	0	NA	NA	NA	NA	NA	NA			
200310413225033	0	NA	0	NA	NA	0	NA	NA	NA	NA	NA	NA	
200310413225072	0	NA	0	NA	NA	NA	NA	NA	0	NA	NA	0			
200310413225073	0	NA	0	NA	NA	0	NA	NA	NA	NA	0	NA	
200310413225074	0	NA	0	NA	NA	0	NA	NA	NA	NA	NA	0
...



Eric Lecoutre <lecoutre at stat.ucl.ac.be> schrieb am 08.02.05 08:55:46:


Hi,

Have a look at:
? aggregate
? reshape

Eric


At 07:39 8/02/2005, you wrote:
>Content-Type: text/plain; charset="iso-8859-1"
>Received-SPF: none (hypatia: domain of hellik at web.de does not designate 
>permitted sender hosts)
>X-Virus-Scanned: by amavisd-new at stat.math.ethz.ch
>Content-Transfer-Encoding: 8bit
>X-MIME-Autoconverted: from quoted-printable to 8bit by 
>hypatia.math.ethz.ch id j186djX0017423
>X-Spam-Checker-Version: SpamAssassin 3.0.2 (2004-11-16) on 
>hypatia.math.ethz.ch
>X-Spam-Level:
>X-Spam-Status: No, score=-1.0 required=5.0 tests=AWL,BAYES_50 autolearn=no 
>version=3.0.2
>
>Hi R-friends,
>
>i have large dataset in the following structure:
>
>BID;TERRCODE;ANMCODE
>200310413290002;4;0
>200310413290002;80;0
>200310413290002;2;0
>200310413290002;5;0
>200310413290003;3;0
>200310413290003;1;0
>200310413290003;11;0
>200310413290003;26;0
>200310413290003;141;21
>200310413290003;472;0
>200310413290004;3;0
>200310413290004;1;0
>200310413290004;7;0
>200310413290004;18;0
>200310413290004;51;0
>200310413290004;56;0
>200310413290004;57;0
>200310413290004;76;0
>200310413290004;89;0
>200310413290004;97;0
>200310413290004;98;0
>200310413290004;72;0
>200310413290004;456;0
>200310413290004;141;0
>200310413290004;640;0
>200310413290004;201;0
>200310413290004;764;20
>200310413290005;273;22
>200310413290005;456;0
>200310413290005;22;0
>200310413290005;23;0
>200310413290005;21;21
>200310413290005;141;0
>200310413290005;640;0
>200310413290005;201;0
>200310413290005;43;0
>200310413290005;650;0
>200310413290005;472;0
>200310413290006;456;0
>200310413290006;22;25
>200310413290006;23;25
>200310413290006;21;25
>200310413290006;640;0
>200310413290006;201;0
>200310413290006;43;0
>200310413290006;651;1
>.
>.
>.
>
>BID is the code of my sample-area
>TERRCODE is the code for landscape characteristic for example: 640 ... sun 
>exposed, .....
>ANMCODE ist the value of the TERRCODE: for example 0 means "occuring", 1 
>means "often occuring", ......
>
>Now my question: is it possible to get a table with the folllowing structure:
>
>
>BID (TERRCODE)4 .... (TERRCODE)21 ......
>200310413290002 (ANMCODE)0 .... (ANMCODE)0 .......
>200310413290003 0 .... 0 ......
>200310413290004 0 .... 0 ......
>200310413290005 0 .... 21 ......
>200310413290006 0 ..... 25 ......
>.
>.
>
>in this example (TERRCODE) and (ANMCODE) is only for explanation and not 
>necessary for further analysis
>
>
>greetings from the snowy tyrol
>
>helli
>
>platform i386-pc-mingw32
>arch i386
>os mingw32
>system i386, mingw32
>status
>major 2
>minor 0.0
>year 2004
>month 10
>day 04
>language R
>
>______________________________________________________________
>Verschicken Sie romantische, coole und witzige Bilder per SMS!
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Eric Lecoutre
UCL /  Institut de Statistique
Voie du Roman Pays, 20
1348 Louvain-la-Neuve
Belgium

tel: (+32)(0)10473050
lecoutre at stat.ucl.ac.be
http://www.stat.ucl.ac.be/ISpersonnel/lecoutre

If the statistics are boring, then you've got the wrong numbers. -Edward 
Tufte



From tuechler at gmx.at  Wed Feb  9 08:18:16 2005
From: tuechler at gmx.at (Heinz Tuechler)
Date: Wed, 09 Feb 2005 08:18:16 +0100
Subject: [R] How to get variable names in a function?
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E68D@usrymx25.merck.co
 m>
Message-ID: <3.0.6.32.20050209081816.007af100@pop.gmx.net>

At 19:26 08.02.2005 -0500, Liaw, Andy wrote:
>Here's a (rather ugly) function that takes its arguments and turn them into
>a list, using the names of the arguments and the names of the components:
>
>listWithName <- function(...) {
>    aname <- sapply(as.list(substitute({...})[-1]), deparse)
>    res <- list(...)
>    names(res) <- aname
>    res
>}
>
>Here are a couple of examples:
>
>> x <- runif(5)
>> y <- "abc"
>> listWithName(x, y)
>$x
>[1] 0.44241647 0.07540958 0.46405311 0.01684738 0.94370893
>
>$y
>[1] "abc"
>
>> listWithName(x, 1:3, y)
>$x
>[1] 0.44241647 0.07540958 0.46405311 0.01684738 0.94370893
>
>$"1:3"
>[1] 1 2 3
>
>$y
>[1] "abc"
>
>> listWithName(abc=x, y)
>$x
>[1] 0.44241647 0.07540958 0.46405311 0.01684738 0.94370893
>
>$y
>[1] "abc"
>
>You can then use lapply() on the output.  lapply() will automatically attach
>names to the components of its output.
>
>Andy
>
Thank you very much, this looks promising to me. As soon as possible I will
try to apply it to my problem.

Heinz

>
>> From: Heinz Tuechler
>> 
>> At 10:32 08.02.2005 -0800, Berton Gunter wrote:
>> >If you pass your function a NAMED list, then the following works:
>> >
>> >demofn<-function(varlist)
>> >{
>> >	nm<-names(varlist)
>> >    for (i in seq(along=varlist))
>> >       {cat('\n',nm[i]) 
>> >       print(table(varlist[[i]]))
>> >    }
>> >}
>> >demofn(list(bravo=bravo, charly=charly))
>> >
>> >If you don't pass a named list, then you need to restrict 
>> and know the form
>> >of the expression that is the varlist argument in order to 
>> substitute() and
>> >deparse it correctly to get the identifiers you want. For 
>> example, if you
>> >knew that varlist were an expression of the form:
>> >list(var1,var2,var3,...) 
>> >then you could get the ith identifier vari via:
>> >
>> >deparse((as.list(substitute(varlist))[-1])[[i]]) 
>> >
>> >HOWEVER, this is probably inefficient and **clearly** 
>> clumsy, undesirable,
>> >and almost certain to fail (so don't do this!). 
>> >
>> >If the number of tables is small enough that you could 
>> simply list them as
>> >arguments (as opposed to constructing the list of vectors to 
>> be tabled in
>> >some way), then the function call could be of the form 
>> function(...) and the
>> >... arguments could be processed as discussed in section 3.1 
>> of V&R's S
>> >PROGRAMMING. That is, your example call would be of the form:
>> >demofn(bravo,charly), and you can forgo lists in the call 
>> altogether. This
>> >strategy actually also works for long constructed lists of 
>> arguments using
>> >do.call() -- see it's help file and V&R again for details.
>> >
>> >
>> >-- Bert Gunter
>> >Genentech Non-Clinical Statistics
>> >South San Francisco, CA
>> > 
>> >"The business of the statistician is to catalyze the 
>> scientific learning
>> >process."  - George E. P. Box
>> > 
>> Thank you for your detailled answer. I see that it is not as easy as I
>> expected and I will think about a convenient way to do it.
>> I tried also the opposit approach i.e. to pass a vector of 
>> names to the
>> function like c("bravo", "charly") but until now I did not 
>> solve it either.
>> A third way worked, but seemed to me less generally applicable.
>> I stored all the names of the variables of a data.frame in a 
>> vector, made
>> the selection of the variables by match() and passed the 
>> indices of the
>> selected variables to the function. By this method I can then 
>> access each
>> variable by its index and also its name via the vector of the 
>> stored names.
>> It seemed to me a crude method of a beginner like me and I 
>> still hope to
>> find a better one.
>> Your suggestion with the named list may be the best, if I 
>> find a practical
>> way to produce this named list by some function from a simple 
>> list without
>> retyping each name. 
>> 
>> Thanks again,
>> 
>> Heinz
>> > 
>> >
>> >> -----Original Message-----
>> >> From: r-help-bounces at stat.math.ethz.ch 
>> >> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
>> Heinz Tuechler
>> >> Sent: Tuesday, February 08, 2005 8:45 AM
>> >> To: r-help at stat.math.ethz.ch
>> >> Subject: [R] How to get variable names in a function?
>> >> 
>> >> Hello,
>> >> 
>> >> applying a function to a list of variables I face the 
>> >> following problem:
>> >> Let's say I want to compute tables for several variables. I 
>> >> could write a
>> >> command for every single table, like
>> >> bravo<-c(1,1,2,3,5,5,5,);charly<-c(7,7,4,4,2,1)
>> >> table(bravo); table(charly)
>> >> > table(bravo); table(charly)
>> >> bravo
>> >> 1 2 3 5 
>> >> 2 1 1 3 
>> >> charly
>> >> 1 2 4 7 
>> >> 1 1 2 2 
>> >> The results are two tables with the names of the variables 
>> above each.
>> >> If I want to do the same thing by a function I find no way 
>> to get the
>> >> variable names above the tables. 
>> >> demofn<-function(varlist)
>> >>     {for (i in seq(along=varlist))
>> >>        {cat(deparse(varlist[i])) # < - - - - how to change this?
>> >>         print(table(varlist[i]))}}
>> >> > demofn(list(bravo, charly))
>> >> list(c(1, 1, 2, 3, 5, 5, 5))
>> >> 1 2 3 5 
>> >> 2 1 1 3 
>> >> list(c(7, 7, 4, 4, 2, 1))
>> >> 1 2 4 7 
>> >> 1 1 2 2 
>> >> > 
>> >> 
>> >> Thanks,
>> >> Heinz T?chler
>> >> 
>> >> ______________________________________________
>> >> R-help at stat.math.ethz.ch mailing list
>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>> >> PLEASE do read the posting guide! 
>> >> http://www.R-project.org/posting-guide.html
>> >> 
>> >
>> >
>> >
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>> 
>> 
>
>
>---------------------------------------------------------------------------
---
>Notice:  This e-mail message, together with any attachments, contains
information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New
Jersey, USA 08889), and/or its affiliates (which may be known outside the
United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as
Banyu) that may be confidential, proprietary copyrighted and/or legally
privileged. It is intended solely for the use of the individual or entity
named on this message.  If you are not the intended recipient, and have
received this message in error, please notify us immediately by reply
e-mail and then delete it from your system.
>---------------------------------------------------------------------------
---
>
>



From Gregor.gawron at rmf.ch  Wed Feb  9 08:51:30 2005
From: Gregor.gawron at rmf.ch (Gregor.gawron@rmf.ch)
Date: Wed, 9 Feb 2005 08:51:30 +0100
Subject: [R] Copula as measure of correlation
Message-ID: <A3A7D687D463B240844F0718418B68BA4F9A04@michexmb01>

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Sean Creighton
Sent: Dienstag, 8. Februar 2005 15:04
To: r-help at stat.math.ethz.ch
Subject: [R] Copula as measure of correlation



Hello

   Has anybody implemented a copula correlation in R in order to
correlate 
between two non-normally distributed time series.

  Thanks

    Sean


Sean,
Ken Knoblauch once gave the follwing answer to a similar question:

In Jim Lindsey's repeated package, 

`gausscop'        Multivariate Gaussian Copula with Arbitrary  Marginals

Gregor

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


Any information in this communication is confidential and ma...{{dropped}}



From mjf at ansto.gov.au  Wed Feb  9 10:14:57 2005
From: mjf at ansto.gov.au (FISCHER, Matthew)
Date: Wed, 9 Feb 2005 20:14:57 +1100 
Subject: [R] Getting R via web to access a remote file
Message-ID: <283982AD9F3CD211B3AC00A0C983032F0F8F3F39@paradise.ansto.gov.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050209/0c85cbf2/attachment.pl

From michael.watson at bbsrc.ac.uk  Wed Feb  9 10:36:38 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Wed, 9 Feb 2005 09:36:38 -0000
Subject: FW: [R] Drawing maps of UK
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950172CE08@iahce2knas1.iah.bbsrc.reserved>


Hi Guys

Thanks for this!  As I am a beginner, I bet I'm running into some really
basic problems.  Using the example from the Map2poly function in
maptools:

try <- read.shape("euadmll.shp")
mappolys <-  Map2poly(try)
# this produces 14 warnings() of the type:
# 1: level 2 circularity at 62 in: .mtlbuild(.mtafters(r1), rD) # etc #
14: From next release, default hole handling will change in:
Map2poly(try) 
plot(mappolys)

Gives an error:
Error in plot.polylist(mappolys) : malformed plot order

I also get this with Map2ploy(..., raw=FALSE)

I have no idea what this means! :-(

Mick



From john.gavin at ubs.com  Wed Feb  9 10:37:01 2005
From: john.gavin at ubs.com (john.gavin@ubs.com)
Date: Wed, 9 Feb 2005 09:37:01 -0000
Subject: [R] HTML help index generation problem with R under Windows
Message-ID: <E0C02753EF9D9046AAADC63C498A00F90103F746@NLDNC009PEX1.ubsgs.ubsgroup.net>

Hi,

I recently encountered a similar problem to
http://www.mail-archive.com/r-help at stat.math.ethz.ch/msg31960.html

One conclusion from that thread seems to be
that if XP users need to update the index, 
after installing a package,
they have to install R into a directory 
for which they have got write access,
either a network or local drive.
This is in order to update the files
'C:Program Files\R\rw2001\doc\html\search\index.txt' and
'C:Program Files\R\rw2001\doc\html\packages.html'

Most R users in my institution do not have admin rights 
to their local drive, for security reasons,
and software has to be installed on the local drives, 
not on remote servers (presumably for networking efficiency).
If just these two files were moved to an area where 
the user has write permission, such as a remote server, 
is it possible to tell R to look for them there?

Otherwise, it seems that users 
who dont have admin permission (the majority), 
will not be able to update their 'index.txt' file,
after installing packages.
While this isnt a major issue, 
suggestions for a workaround would be welcome.

Regards,

John.

John Gavin <john.gavin at ubs.com>,
Quantitative Risk Models and Statistics,
UBS Investment Bank, 6th floor, 
100 Liverpool St., London EC2M 2RH, UK.
Phone +44 (0) 207 567 4289
Fax   +44 (0) 207 568 5352

Visit our website at http://www.ubs.com

This message contains confidential information and is intend...{{dropped}}



From Christoph.Scherber at uni-jena.de  Wed Feb  9 11:07:33 2005
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Wed, 09 Feb 2005 11:07:33 +0100
Subject: [R] panel plot with log-scaled x-axis
In-Reply-To: <200502081216.31407.deepayan@stat.wisc.edu>
References: <4208E67C.90907@uni-jena.de>
	<200502081216.31407.deepayan@stat.wisc.edu>
Message-ID: <4209E0E5.90502@uni-jena.de>

Hi,

I?ll try to rephrase my question using the following example:

xyplot(conc~uptake|Treatment+Type,data=CO2)

Would it be possible to make each of the x axes log-scaled (but with the 
same tick labels)?

Thanks very much!
Christoph








Deepayan Sarkar wrote:

>On Tuesday 08 February 2005 10:19, Christoph Scherber wrote:
>  
>
>>Dear all,
>>
>>Can anyone help me plotting a panel plot with log-scaled x axes?
>>
>>My formula looks like this:
>>
>>coplot(response~div|treatment+grass,
>>panel=function(x,y,...){panel.xyplot(x,y,...);panel.lmline(x,y,...)})
>>    
>>
>
>Are you sure that actually works? 
>
>  
>
>>And I?d like to have "div" plotted in log scale.
>>    
>>
>
>How to do that would depend at least on whether you are using coplot or 
>xyplot (neither may have good solutions implemented).
>
>Deepayan
>
>
>  
>



From ripley at stats.ox.ac.uk  Wed Feb  9 11:21:03 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 9 Feb 2005 10:21:03 +0000 (GMT)
Subject: [R] HTML help index generation problem with R under Windows
In-Reply-To: <E0C02753EF9D9046AAADC63C498A00F90103F746@NLDNC009PEX1.ubsgs.ubsgroup.net>
References: <E0C02753EF9D9046AAADC63C498A00F90103F746@NLDNC009PEX1.ubsgs.ubsgroup.net>
Message-ID: <Pine.LNX.4.61.0502091010140.27047@gannet.stats>

On Wed, 9 Feb 2005 john.gavin at ubs.com wrote:

> I recently encountered a similar problem to
> http://www.mail-archive.com/r-help at stat.math.ethz.ch/msg31960.html
>
> One conclusion from that thread seems to be
> that if XP users need to update the index,
> after installing a package,
> they have to install R into a directory
> for which they have got write access,
> either a network or local drive.

That's stated in the rw-FAQ Q3.3: please read the relevant FAQ before 
posting as the posting guide asks.

> This is in order to update the files
> 'C:Program Files\R\rw2001\doc\html\search\index.txt' and
> 'C:Program Files\R\rw2001\doc\html\packages.html'
>
> Most R users in my institution do not have admin rights
> to their local drive, for security reasons,
> and software has to be installed on the local drives,
> not on remote servers (presumably for networking efficiency).

That is not necessary, especially not in R-devel (2.1.0 to be) unless 
your network is very slow (slower than USB 1.1, for example).

> If just these two files were moved to an area where
> the user has write permission, such as a remote server,
> is it possible to tell R to look for them there?

No.  The relative location is hardcoded into the other HTML files and the 
search engine.

> Otherwise, it seems that users
> who dont have admin permission (the majority),
> will not be able to update their 'index.txt' file,
> after installing packages.
> While this isnt a major issue,
> suggestions for a workaround would be welcome.

Get packages installed by those who do have permission (which was the 
solution given by Uwe Ligges in that thread).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dhkblaszyk at zeelandnet.nl  Wed Feb  9 11:22:52 2005
From: dhkblaszyk at zeelandnet.nl (dhkblaszyk@zeelandnet.nl)
Date: Wed, 9 Feb 2005 11:22:52 +0100 (CET)
Subject: [R] Contour plot
Message-ID: <35589.127.0.0.1.1107944572.squirrel@127.0.0.1>

> x <- log(1:10)
> y <- log(1:10)
> z <- 1:100
> contour(matrix(z,ncol=10))

> did not do what I expected it to do. I expected the contour lines to
become compressed gradually.

I believe you want to do:

contour(x,y,matrix(z,ncol=10))

This will show the desired contour-plot.

Kind regards, Darius Blaszijk



From petr.pikal at precheza.cz  Wed Feb  9 11:23:44 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 09 Feb 2005 11:23:44 +0100
Subject: Fw: [R] Contour plot
In-Reply-To: <001d01c50dfd$552ee1f0$3ef4ee3e@1234abc>
Message-ID: <4209F2C0.1611.FD6EE8@localhost>

Hi Darius


On 8 Feb 2005 at 17:43, dhkblaszyk at zeelandnet.nl wrote:

> I understand that I need to have a (in this case) square matrix with
> all the data. But the question now is;
> 
> - can the contourplot not interpolate the missing values
> 
> or alternatively
> 
> - I have fit a model to the z data (z = 100 + 0.5x + 0.5y). How can I
> make from this model a "square" matrix z to make a contour plot?


Will

x<-1:10
y<-1:10
z <- outer(x,y,function(x,y) 100 + 0.5*x + 0.5*y)
contour(x,y,z)

work as you wish?
Cheers
Petr


> 
> Kind regards, Darius Blaszijk
> 
> ----- Original Message -----
> From: "Achim Zeileis" <Achim.Zeileis at wu-wien.ac.at>
> To: <dhkblaszyk at zeelandnet.nl>
> Cc: <r-help at stat.math.ethz.ch>
> Sent: Tuesday, February 08, 2005 1:51 AM
> Subject: Re: [R] Contour plot
> 
> 
> > On Tue, 8 Feb 2005 01:15:06 +0100 dhkblaszyk at zeelandnet.nl wrote:
> >
> > > Hello,
> > >
> > > I would like to make a contourplot of the following data;
> > >
> > > > x <- 1:10
> > > > y <- 1:10
> > > > z <- 100:110
> > >
> > > By doing >contour(x,y,z) I get the following error;
> > >
> > > "Error in contour.default(x, y, z) : no proper `z' matrix
> > > specified"
> > >
> > > How do I fix this??
> >
> > x and y specify a grid and thus z must provide a value for each
> > combination of the x's and y's! For example:
> >   x <- y <- 1:10
> >   contour(x, y, outer(x, y))
> > Also look at
> >   outer(x, y)
> > and read ?contour.
> >
> > Z
> >
> > > Kind regards, Datius Blaszijk
> > >
> > > [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From dimitris.rizopoulos at med.kuleuven.ac.be  Wed Feb  9 11:25:31 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Wed, 9 Feb 2005 11:25:31 +0100
Subject: [R] panel plot with log-scaled x-axis
References: <4208E67C.90907@uni-jena.de><200502081216.31407.deepayan@stat.wisc.edu>
	<4209E0E5.90502@uni-jena.de>
Message-ID: <013a01c50e91$aeb4d2e0$0540210a@www.domain>

I am not sure if this is what you want, but try:

library(lattice)
xyplot(conc~uptake|Treatment+Type, data=CO2, 
scales=list(x=list(log=TRUE)))

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Christoph Scherber" <Christoph.Scherber at uni-jena.de>
To: "Deepayan Sarkar" <deepayan at stat.wisc.edu>
Cc: <r-help at stat.math.ethz.ch>
Sent: Wednesday, February 09, 2005 11:07 AM
Subject: Re: [R] panel plot with log-scaled x-axis


> Hi,
>
> I?ll try to rephrase my question using the following example:
>
> xyplot(conc~uptake|Treatment+Type,data=CO2)
>
> Would it be possible to make each of the x axes log-scaled (but with 
> the same tick labels)?
>
> Thanks very much!
> Christoph
>
>
>
>
>
>
>
>
> Deepayan Sarkar wrote:
>
>>On Tuesday 08 February 2005 10:19, Christoph Scherber wrote:
>>
>>>Dear all,
>>>
>>>Can anyone help me plotting a panel plot with log-scaled x axes?
>>>
>>>My formula looks like this:
>>>
>>>coplot(response~div|treatment+grass,
>>>panel=function(x,y,...){panel.xyplot(x,y,...);panel.lmline(x,y,...)})
>>>
>>
>>Are you sure that actually works?
>>
>>>And I?d like to have "div" plotted in log scale.
>>>
>>
>>How to do that would depend at least on whether you are using coplot 
>>or xyplot (neither may have good solutions implemented).
>>
>>Deepayan
>>
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Roger.Bivand at nhh.no  Wed Feb  9 11:53:52 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 9 Feb 2005 11:53:52 +0100 (CET)
Subject: FW: [R] Drawing maps of UK
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950172CE08@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <Pine.LNX.4.44.0502091128370.21842-100000@reclus.nhh.no>

On Wed, 9 Feb 2005, michael watson (IAH-C) wrote:

> 
> Hi Guys
> 
> Thanks for this!  As I am a beginner, I bet I'm running into some really
> basic problems.  Using the example from the Map2poly function in
> maptools:
> 
> try <- read.shape("euadmll.shp")
> mappolys <-  Map2poly(try)
> # this produces 14 warnings() of the type:
> # 1: level 2 circularity at 62 in: .mtlbuild(.mtafters(r1), rD) # etc #
> 14: From next release, default hole handling will change in:
> Map2poly(try) 
> plot(mappolys)
> 
> Gives an error:
> Error in plot.polylist(mappolys) : malformed plot order
> 
> I also get this with Map2ploy(..., raw=FALSE)
> 
> I have no idea what this means! :-(

Basically, the original *.e00 file that Barry refered to had "topology", 
that is, polygons are built into rings on the fly from arcs, lines that 
are 2D coordinates. To draw a polygon, you need to build a ring getting 
the arcs in the right direction and order, otherwise you get spaghetti. 
Shapefiles just have rings, so can get pretty confused when the data they 
contain do not adhere to the ESRI standard (clockwise ring direction for 
islands, anti-clockwise for holes). Because most (unfortunately) 
shapefiles in the wild do not follow the specification, Map2poly() tries 
to work out whether ring directions seem plausible, and gives warnings 
when it gets confused. 

Finally. plot order is needed because R's polygon filling assumes that
polygons can be plotted one on top of the other, but this can lead to
wholly contained polygons (say Hamburg NUTS2 region)  being "painted over"
by its including polygon plotted later. Here the plot order heuristic
tries to see which plot order seems sensible, committing wholly contained
polygons last, or at least after the containing polygon if it is correctly
identified.

Downloading from: http://www.maths.lancs.ac.uk/~rowlings/Geog/Maps/

> library(maptools)
Loading required package: foreign 
> try <- read.shape("euadmll.shp")
Shapefile Type: Polygon   # of Shapes: 2556 
> mappolys <-  Map2poly(try, raw=FALSE)
There were 50 or more warnings (use warnings() to see the first 50)

One possible problem with the file is that the first shape seems to be the 
whole of Europe as a single polygon:

>  str(mappolys[[1]])
 num [1:202333, 1:2] 25.6 25.5 25.5 25.5 25.4 ...
 - attr(*, "nParts")= int 2003
 - attr(*, "pstart")=List of 2
  ..$ from: int [1:2003] 1 130 185 198 260 539 28921 29056 29138 29159 ...
  ..$ to  : int [1:2003] 128 183 196 258 537 28919 29054 29136 29157 29173 
...
 - attr(*, "bbox")= num [1:4] -24.5  27.6  31.6  71.2
 - attr(*, "ringDir")= num [1:2003] 1 1 1 1 1 1 1 1 1 1 ...
 - attr(*, "after")= int [1:2003] NA NA NA NA NA NA NA NA NA NA ...
 - attr(*, "plotOrder")= int [1:2003] 1 2 3 4 5 6 190 7 8 9 ...
 - attr(*, "shpID")= int NA

and this seems to be the problem. This does happen in e00 files - the
first object is the totality of the objects included, giving an outer
border, here with 2003 islands, lakes, etc. The plot order malformation is
then clear, everything is contained inside itself in the first shape. A
possible fix is then:

> length(mappolys)
[1] 2556
> mp1 <- subset(mappolys, 1:length(mappolys) %in% 2:length(mappolys))
> plot(mp1)

which works for me.

> names(try$att.data)
[1] "AREA"      "PERIMETER" "NUM"       "ID"        "COUNTRY"   "TOTALAREA"

suggests though that identifying which bits of the jigsaw-puzzle are which 
may not be easy, perhaps there is more to go on in the original e00.

Roger

> 
> Mick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From y.clough at NS1.uaoe.gwdg.de  Wed Feb  9 12:07:52 2005
From: y.clough at NS1.uaoe.gwdg.de (Yann Clough)
Date: Wed, 09 Feb 2005 12:07:52 +0100
Subject: [R] randomisation
Message-ID: <4209FD18.2857.90ABF3@localhost>

Dear useRs

I am looking for a way to randomise the values within a matrix:
the conditions are that the sums of the rows and the sums of the columns should 
remain the same as in the original matrix.

Any help would be appreciated
Cheers

Yann



From p.dalgaard at biostat.ku.dk  Wed Feb  9 13:58:42 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 09 Feb 2005 13:58:42 +0100
Subject: [R] randomisation
In-Reply-To: <4209FD18.2857.90ABF3@localhost>
References: <4209FD18.2857.90ABF3@localhost>
Message-ID: <x2mzudq48t.fsf@biostat.ku.dk>

"Yann Clough" <y.clough at NS1.uaoe.gwdg.de> writes:

> Dear useRs
> 
> I am looking for a way to randomise the values within a matrix:
> the conditions are that the sums of the rows and the sums of the columns should 
> remain the same as in the original matrix.
> 
> Any help would be appreciated

Er, explain further... Are you shuffling around constant numbers (as
in the 0/1 values in item analysis) or can they be integers of varying
magnitude (as in coloured-balls-in-urns), or ?

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From jfox at mcmaster.ca  Wed Feb  9 14:07:15 2005
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 9 Feb 2005 08:07:15 -0500
Subject: [R] randomisation
In-Reply-To: <4209FD18.2857.90ABF3@localhost>
Message-ID: <20050209130706.EVGH1899.tomts13-srv.bellnexxia.net@JohnDesktop8300>

Dear Yann,

Whether you can do this depends upon what you mean. If you mean randomly
permute the elements of the matrix preserving row and column sums, then this
generally won't be possible. Consider, e.g., matrix(1:4, 2, 2). 

If you mean generate a matrix with specified row and column sums but
randomly sampled elements, then it should be possible to do it. For example,
if all elements are positive, you can generate a matrix with unconstrained
random elements and then make them match the required row and column sums by
iterative proportional fitting. The loglin function in the MASS package will
do this: loglin(X, list(1, 2), fit=TRUE)$fit (where your original matrix
with random elements is X).

If there are negative elements, then you could add a constant, adjust as
above, and subtract the constant.

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Yann Clough
> Sent: Wednesday, February 09, 2005 6:08 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] randomisation
> 
> Dear useRs
> 
> I am looking for a way to randomise the values within a matrix:
> the conditions are that the sums of the rows and the sums of 
> the columns should remain the same as in the original matrix.
> 
> Any help would be appreciated
> Cheers
> 
> Yann
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From y.clough at NS1.uaoe.gwdg.de  Wed Feb  9 14:27:38 2005
From: y.clough at NS1.uaoe.gwdg.de (Yann Clough)
Date: Wed, 09 Feb 2005 14:27:38 +0100
Subject: [R] randomisation
In-Reply-To: <x2mzudq48t.fsf@biostat.ku.dk>
References: <4209FD18.2857.90ABF3@localhost>
Message-ID: <420A1DDA.16050.110A184@localhost>


   I  am working on an ecological problem and dealing with a matrix where
   rows correspond to samples, and columns correspond to species.

   The values in the matrix are recorded abundances of the organisms.

   I  want  to  create  a  series  of  randomised  datasets  where  total
   abundances per sample (rowSums) and per species (colSums) are equal to
   those in the dataset of my observations.

   Simple example of the kind of thing I have:

   matrix(c(1,0,2,10,1,3,5,6,7,1,0,0),nrow=4,  ncol=3,by=row)  # observed
   data

   rowSums(tempmatrix) #individuals per location,

   colSums(tempmatrix) #individuals per species

   example of a matrix which complies with the two restrictions:

   tempmatrix2=matrix(c(1,0,2,11,0,3,5,6,7,0,1,0),nrow=4, ncol=3,by=row)

   rowSums(tempmatrix2)

   colSums(tempmatrix2)

   hope this is clear

   Cheers

   Yann

   ***************************

   Yann Clough

   Fachgebiet Agraroekologie

   Waldweg 26

   D-37073 Goettingen

   Tel: 0551/39-2157

   email: y.clough at uaoe.gwdg.de

   www: http://wwwuser.gwdg.de/~uaoe/mitarbeiter/y_clough.htm


From peter.rabinovitch at alcatel.com  Wed Feb  9 14:43:21 2005
From: peter.rabinovitch at alcatel.com (Peter Rabinovitch)
Date: Wed, 9 Feb 2005 08:43:21 -0500
Subject: [R] randomisation
In-Reply-To: <420A1DDA.16050.110A184@localhost>
Message-ID: <200502091346.j19DkmbE006325@tm3.ca.alcatel.com>

This is not trivial. There is a really nice algorithm (should be simple to
code up) due to Persi Diaconis, and described in example 1.22 (page 1-14) of
Joseph Chang's notes on Markov chains available at
http://www.stat.yale.edu/~jtc5/251/mc.pdf




-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Yann Clough
Sent: Wednesday, February 09, 2005 8:28 AM
To: Peter Dalgaard; r-help at stat.math.ethz.ch
Subject: Re: [R] randomisation


   I  am working on an ecological problem and dealing with a matrix where
   rows correspond to samples, and columns correspond to species.

   The values in the matrix are recorded abundances of the organisms.

   I  want  to  create  a  series  of  randomised  datasets  where  total
   abundances per sample (rowSums) and per species (colSums) are equal to
   those in the dataset of my observations.

   Simple example of the kind of thing I have:

   matrix(c(1,0,2,10,1,3,5,6,7,1,0,0),nrow=4,  ncol=3,by=row)  # observed
   data

   rowSums(tempmatrix) #individuals per location,

   colSums(tempmatrix) #individuals per species

   example of a matrix which complies with the two restrictions:

   tempmatrix2=matrix(c(1,0,2,11,0,3,5,6,7,0,1,0),nrow=4, ncol=3,by=row)

   rowSums(tempmatrix2)

   colSums(tempmatrix2)

   hope this is clear

   Cheers

   Yann

   ***************************

   Yann Clough

   Fachgebiet Agraroekologie

   Waldweg 26

   D-37073 Goettingen

   Tel: 0551/39-2157

   email: y.clough at uaoe.gwdg.de

   www: http://wwwuser.gwdg.de/~uaoe/mitarbeiter/y_clough.htm
______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From rrsilva at ib.usp.br  Wed Feb  9 14:52:26 2005
From: rrsilva at ib.usp.br (Rogerio Rosa da Silva)
Date: Wed, 9 Feb 2005 11:52:26 -0200
Subject: [R] subset
Message-ID: <200502091152.26874.rrsilva@ib.usp.br>

Dear all,

I am trying to extract rows from a data.frame based on the
rowSums != 0.  I want to preserve rownames in the first column in the subset.

Does anyone know how to extract all species that don't have rowSums equal
to zero?  Here it is:

# dataset
x <- data.frame(
species=c("sp.1","sp.2","sp.3","sp.4"),
site1=c(2,3,0,0),
site2=c(0,0,0,0),
site3=c(0,1,0,6),
site4=c(0,0,0,0))

#I want extract the matrix:

 species site1 site2 site3 site4
  sp.1     2     0     0     0
  sp.2     3     0     1     0
  sp.4     0     0     6     0

#extract data.frame of rowSums with x[,2:4] != 0 

subset (x, apply (x,1,function(row) all(rowSums(x[,2:4] !=0)) ## don't work


Thanks in advance.

-- 
Rog?rio R. Silva
MZUSP http://www.mz.usp.br
Linux/Debian User # 354364
Linux counter http://counter.li.org



From dimitris.rizopoulos at med.kuleuven.ac.be  Wed Feb  9 14:51:48 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Wed, 9 Feb 2005 14:51:48 +0100
Subject: [R] panel plot with log-scaled x-axis
References: <4208E67C.90907@uni-jena.de><200502081216.31407.deepayan@stat.wisc.edu>
	<4209E0E5.90502@uni-jena.de>
	<013a01c50e91$aeb4d2e0$0540210a@www.domain>
	<420A0935.5050608@uni-jena.de>
Message-ID: <004201c50eae$8042ffa0$0540210a@www.domain>

then try this:

library(lattice)
xyplot(conc~uptake|Treatment+Type, data=CO2,
scales=list(x=list(log=TRUE, at=c(10^1, 10^1.2, 10^1.4, 10^1.6),
labels=letters[1:4])))


Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Christoph Scherber" <Christoph.Scherber at uni-jena.de>
To: "Dimitris Rizopoulos" <dimitris.rizopoulos at med.kuleuven.ac.be>
Sent: Wednesday, February 09, 2005 1:59 PM
Subject: Re: [R] panel plot with log-scaled x-axis


> Hi,
>
> Yes, that works great! But how can I now change the x axis tick 
> labels look better (eg., give a sequence that shall be plotted)?
>
> Regards
> Christoph
>
> Dimitris Rizopoulos wrote:
>
>> I am not sure if this is what you want, but try:
>>
>> library(lattice)
>> xyplot(conc~uptake|Treatment+Type, data=CO2, 
>> scales=list(x=list(log=TRUE)))
>>
>> I hope it helps.
>>
>> Best,
>> Dimitris
>>
>> ----
>> Dimitris Rizopoulos
>> Ph.D. Student
>> Biostatistical Centre
>> School of Public Health
>> Catholic University of Leuven
>>
>> Address: Kapucijnenvoer 35, Leuven, Belgium
>> Tel: +32/16/336899
>> Fax: +32/16/337015
>> Web: http://www.med.kuleuven.ac.be/biostat/
>>     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
>>
>>
>> ----- Original Message ----- From: "Christoph Scherber" 
>> <Christoph.Scherber at uni-jena.de>
>> To: "Deepayan Sarkar" <deepayan at stat.wisc.edu>
>> Cc: <r-help at stat.math.ethz.ch>
>> Sent: Wednesday, February 09, 2005 11:07 AM
>> Subject: Re: [R] panel plot with log-scaled x-axis
>>
>>
>>> Hi,
>>>
>>> I?ll try to rephrase my question using the following example:
>>>
>>> xyplot(conc~uptake|Treatment+Type,data=CO2)
>>>
>>> Would it be possible to make each of the x axes log-scaled (but 
>>> with the same tick labels)?
>>>
>>> Thanks very much!
>>> Christoph
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>> Deepayan Sarkar wrote:
>>>
>>>> On Tuesday 08 February 2005 10:19, Christoph Scherber wrote:
>>>>
>>>>> Dear all,
>>>>>
>>>>> Can anyone help me plotting a panel plot with log-scaled x axes?
>>>>>
>>>>> My formula looks like this:
>>>>>
>>>>> coplot(response~div|treatment+grass,
>>>>> panel=function(x,y,...){panel.xyplot(x,y,...);panel.lmline(x,y,...)})
>>>>>
>>>>
>>>> Are you sure that actually works?
>>>>
>>>>> And I?d like to have "div" plotted in log scale.
>>>>>
>>>>
>>>> How to do that would depend at least on whether you are using 
>>>> coplot or xyplot (neither may have good solutions implemented).
>>>>
>>>> Deepayan
>>>>
>>>>
>>>>
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>>
>>
>>
>



From jarioksa at sun3.oulu.fi  Wed Feb  9 15:00:31 2005
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: Wed, 09 Feb 2005 16:00:31 +0200
Subject: [R] randomisation
In-Reply-To: <420A1DDA.16050.110A184@localhost>
References: <4209FD18.2857.90ABF3@localhost> <420A1DDA.16050.110A184@localhost>
Message-ID: <1107957632.15373.72.camel@biol102145.oulu.fi>

On Wed, 2005-02-09 at 14:27 +0100, Yann Clough wrote:
>    I  am working on an ecological problem and dealing with a matrix where
>    rows correspond to samples, and columns correspond to species.
> 
>    The values in the matrix are recorded abundances of the organisms.
> 
>    I  want  to  create  a  series  of  randomised  datasets  where  total
>    abundances per sample (rowSums) and per species (colSums) are equal to
>    those in the dataset of my observations.
> 
>    Simple example of the kind of thing I have:
> 
>    matrix(c(1,0,2,10,1,3,5,6,7,1,0,0),nrow=4,  ncol=3,by=row)  # observed
>    data
> 
>    rowSums(tempmatrix) #individuals per location,
> 
>    colSums(tempmatrix) #individuals per species
> 
>    example of a matrix which complies with the two restrictions:
> 
>    tempmatrix2=matrix(c(1,0,2,11,0,3,5,6,7,0,1,0),nrow=4, ncol=3,by=row)
> 
>    rowSums(tempmatrix2)
> 
>    colSums(tempmatrix2)
> 
>    hope this is clear
> 
As already explained, this may not be possible as a simple permutation.
You seem to have something else on your mind: moving individuals freely
between species instead of permuting data which means redistributing the
abundances among species instead of permutation. 

For a traditional permutation, you may have a look at the labdsv package
(for ecological applications). This has function 'rndveg' which
"attempts to preserve either species occurrence distributions or plot-
level species richness." Preserving both may be impossible, but check
the function. The 'labdsv' source package is available at CRAN, and
Windows and MacOS X binaries through my web page
(http://cc.oulu.fi/~jarioksa/softhelp/softalist.html).


The Windows binary is not available at CRAN since the package fails R
CMD check in Windows (so you shouldn't check the package but just use
it). The Mac binary is not available at CRAN since the whole Mac binary
package system seems to be dysfunctional (there is nothing after Jan 19,
2005).

cheers, jari oksanen



From ggrothendieck at myway.com  Wed Feb  9 15:01:31 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 9 Feb 2005 14:01:31 +0000 (UTC)
Subject: [R] randomisation
References: <4209FD18.2857.90ABF3@localhost>
Message-ID: <loom.20050209T150112-910@post.gmane.org>

Yann Clough <y.clough <at> NS1.uaoe.gwdg.de> writes:

: 
: Dear useRs
: 
: I am looking for a way to randomise the values within a matrix:
: the conditions are that the sums of the rows and the sums of the columns 
should 
: remain the same as in the original matrix.
: 

?r2dtable



From francoisromain at free.fr  Wed Feb  9 15:05:39 2005
From: francoisromain at free.fr (francoisromain@free.fr)
Date: Wed,  9 Feb 2005 15:05:39 +0100
Subject: [R] randomisation
In-Reply-To: <4209FD18.2857.90ABF3@localhost>
References: <4209FD18.2857.90ABF3@localhost>
Message-ID: <1107957939.420a18b3adb87@imp5-q.free.fr>

Selon Yann Clough <y.clough at NS1.uaoe.gwdg.de>:

> Dear useRs
>
> I am looking for a way to randomise the values within a matrix:
> the conditions are that the sums of the rows and the sums of the columns
> should
> remain the same as in the original matrix.
>
> Any help would be appreciated
> Cheers
>
> Yann
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


Hello, your question makes me think about what does the chisq.test function with
simulate.p.value = T, maybe that could help to look what is done there.

The help page of chisq.test points to that reference wich seems quite like what
you are looking for :

Patefield, W. M. (1981) Algorithm AS159. An efficient method of generating r x c
tables with given row and column totals. Applied Statistics 30, 91-97.


Romain.



From ccleland at optonline.net  Wed Feb  9 15:11:51 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 09 Feb 2005 09:11:51 -0500
Subject: [R] subset
In-Reply-To: <200502091152.26874.rrsilva@ib.usp.br>
References: <200502091152.26874.rrsilva@ib.usp.br>
Message-ID: <420A1A27.6050307@optonline.net>

subset(x, rowSums(x[,-1], na.rm=TRUE) != 0)

Rogerio Rosa da Silva wrote:
> Dear all,
> 
> I am trying to extract rows from a data.frame based on the
> rowSums != 0.  I want to preserve rownames in the first column in the subset.
> 
> Does anyone know how to extract all species that don't have rowSums equal
> to zero?  Here it is:
> 
> # dataset
> x <- data.frame(
> species=c("sp.1","sp.2","sp.3","sp.4"),
> site1=c(2,3,0,0),
> site2=c(0,0,0,0),
> site3=c(0,1,0,6),
> site4=c(0,0,0,0))
> 
> #I want extract the matrix:
> 
>  species site1 site2 site3 site4
>   sp.1     2     0     0     0
>   sp.2     3     0     1     0
>   sp.4     0     0     6     0
> 
> #extract data.frame of rowSums with x[,2:4] != 0 
> 
> subset (x, apply (x,1,function(row) all(rowSums(x[,2:4] !=0)) ## don't work
> 
> 
> Thanks in advance.
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From dimitris.rizopoulos at med.kuleuven.ac.be  Wed Feb  9 15:13:25 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Wed, 9 Feb 2005 15:13:25 +0100
Subject: [R] subset
References: <200502091152.26874.rrsilva@ib.usp.br>
Message-ID: <006101c50eb1$84d4b560$0540210a@www.domain>

you have answer it yourself:

x[rowSums(x[,2:4])!=0,]


Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Rogerio Rosa da Silva" <rrsilva at ib.usp.br>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, February 09, 2005 2:52 PM
Subject: [R] subset


> Dear all,
>
> I am trying to extract rows from a data.frame based on the
> rowSums != 0. I want to preserve rownames in the first column in the 
> subset.
>
> Does anyone know how to extract all species that don't have rowSums 
> equal
> to zero?  Here it is:
>
> # dataset
> x <- data.frame(
> species=c("sp.1","sp.2","sp.3","sp.4"),
> site1=c(2,3,0,0),
> site2=c(0,0,0,0),
> site3=c(0,1,0,6),
> site4=c(0,0,0,0))
>
> #I want extract the matrix:
>
> species site1 site2 site3 site4
> sp.1 2 0 0 0
> sp.2 3 0 1 0
> sp.4 0 0 6 0
>
> #extract data.frame of rowSums with x[,2:4] != 0
>
> subset (x, apply (x,1,function(row) all(rowSums(x[,2:4] !=0)) ## 
> don't work
>
>
> Thanks in advance.
>
> -- 
> Rog?rio R. Silva
> MZUSP http://www.mz.usp.br
> Linux/Debian User # 354364
> Linux counter http://counter.li.org
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From andy_liaw at merck.com  Wed Feb  9 15:13:13 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 9 Feb 2005 09:13:13 -0500
Subject: [R] subset
Message-ID: <3A822319EB35174CA3714066D590DCD50994E695@usrymx25.merck.com>

Something like:

> x[rowSums(x[,-1]) > 0,]
  species site1 site2 site3 site4
1    sp.1     2     0     0     0
2    sp.2     3     0     1     0
4    sp.4     0     0     6     0

Andy

> From: Rogerio Rosa da Silva
> 
> Dear all,
> 
> I am trying to extract rows from a data.frame based on the
> rowSums != 0.  I want to preserve rownames in the first 
> column in the subset.
> 
> Does anyone know how to extract all species that don't have 
> rowSums equal
> to zero?  Here it is:
> 
> # dataset
> x <- data.frame(
> species=c("sp.1","sp.2","sp.3","sp.4"),
> site1=c(2,3,0,0),
> site2=c(0,0,0,0),
> site3=c(0,1,0,6),
> site4=c(0,0,0,0))
> 
> #I want extract the matrix:
> 
>  species site1 site2 site3 site4
>   sp.1     2     0     0     0
>   sp.2     3     0     1     0
>   sp.4     0     0     6     0
> 
> #extract data.frame of rowSums with x[,2:4] != 0 
> 
> subset (x, apply (x,1,function(row) all(rowSums(x[,2:4] !=0)) 
> ## don't work
> 
> 
> Thanks in advance.
> 
> -- 
> Rog?rio R. Silva
> MZUSP http://www.mz.usp.br
> Linux/Debian User # 354364
> Linux counter http://counter.li.org
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From jfox at mcmaster.ca  Wed Feb  9 15:44:34 2005
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 9 Feb 2005 09:44:34 -0500
Subject: [R] subset
In-Reply-To: <200502091152.26874.rrsilva@ib.usp.br>
Message-ID: <20050209144426.EWXG1919.tomts22-srv.bellnexxia.net@JohnDesktop8300>

Dear Rogerio,

x[rowSums(x[,2:5]) != 0,] should do what you want.

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Rogerio Rosa da Silva
> Sent: Wednesday, February 09, 2005 8:52 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] subset
> 
> Dear all,
> 
> I am trying to extract rows from a data.frame based on the 
> rowSums != 0.  I want to preserve rownames in the first 
> column in the subset.
> 
> Does anyone know how to extract all species that don't have 
> rowSums equal to zero?  Here it is:
> 
> # dataset
> x <- data.frame(
> species=c("sp.1","sp.2","sp.3","sp.4"),
> site1=c(2,3,0,0),
> site2=c(0,0,0,0),
> site3=c(0,1,0,6),
> site4=c(0,0,0,0))
> 
> #I want extract the matrix:
> 
>  species site1 site2 site3 site4
>   sp.1     2     0     0     0
>   sp.2     3     0     1     0
>   sp.4     0     0     6     0
> 
> #extract data.frame of rowSums with x[,2:4] != 0 
> 
> subset (x, apply (x,1,function(row) all(rowSums(x[,2:4] !=0)) 
> ## don't work
> 
> 
> Thanks in advance.
> 
> --
> Rog?rio R. Silva
> MZUSP http://www.mz.usp.br
> Linux/Debian User # 354364
> Linux counter http://counter.li.org
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From tomhopper at comcast.net  Wed Feb  9 16:02:47 2005
From: tomhopper at comcast.net (Thomas Hopper)
Date: Wed, 09 Feb 2005 10:02:47 -0500
Subject: [R] Histogram Bar Spacing or Border Width
Message-ID: <420A2617.9040008@comcast.net>

Is there any way to control the spacing between bars in a histogram, or 
change the border width (I'm assuming the hist() function, though 
alternatives are welcome)? I'm interested in changing the visual spacing 
between columns in a plotted histogram.

The general effect I'm looking for can be accomplished in barplots using 
the "width=" parameter, but I have not been able to find a way to adjust 
the apparent spacing in histograms.

Thank you,

Tom Hopper



From r.hankin at soc.soton.ac.uk  Wed Feb  9 16:01:52 2005
From: r.hankin at soc.soton.ac.uk (Robin Hankin)
Date: Wed, 9 Feb 2005 15:01:52 +0000
Subject: [R] dput() error
Message-ID: <2bb38f532df2ee4dadb2e32a20667a39@soc.soton.ac.uk>

Hi

I'm trying to debug one of my functions, which stopped with the 
following message:



Error in is.vector(x.star) : subscript out of bounds


So I put a "print(x.star)" statement in just before the is.vector bit 
and got


Error in print(x.star) : subscript out of bounds


So I put "dput(x.star)" in and got

Error in dput(x.star) : subscript out of bounds



what's going on here?  How best to proceed?



--
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From deepayan at stat.wisc.edu  Wed Feb  9 16:09:24 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Wed, 9 Feb 2005 09:09:24 -0600
Subject: [R] panel plot with log-scaled x-axis
In-Reply-To: <4209E0E5.90502@uni-jena.de>
References: <4208E67C.90907@uni-jena.de>
	<200502081216.31407.deepayan@stat.wisc.edu>
	<4209E0E5.90502@uni-jena.de>
Message-ID: <200502090909.24354.deepayan@stat.wisc.edu>

On Wednesday 09 February 2005 04:07, Christoph Scherber wrote:
> Hi,
>
> I?ll try to rephrase my question using the following example:
>
> xyplot(conc~uptake|Treatment+Type,data=CO2)
>
> Would it be possible to make each of the x axes log-scaled 

Yes, with 

xyplot(conc ~ uptake | Treatment + Type, data = CO2,
       scales = list(x = list(log = TRUE)))

> (but with  the same tick labels)?

No, but that isn't always the most appropriate thing anyway (the ticks 
may get too close). You can always specify the locations manually, as 
in

xyplot(conc ~ uptake | Treatment + Type, data = CO2,
       scales =
       list(x =
            list(log = TRUE,
                 at = c(10, 20, 30, 40))))

Ideally, the tick locations should be computed by (a variation of) 
'axTicks', but I've never gotten around to implementing that.

Deepayan



From xmeadow at uchicago.edu  Wed Feb  9 16:10:13 2005
From: xmeadow at uchicago.edu (Xander Meadow)
Date: Wed, 09 Feb 2005 09:10:13 -0600
Subject: [R] install RMySQL
In-Reply-To: <1107805973.4305.11.camel@gregor.bsd.uchicago.edu>
References: <1107804348.4305.6.camel@gregor.bsd.uchicago.edu>
	<4207C4F9.9080109@jhsph.edu>
	<1107805973.4305.11.camel@gregor.bsd.uchicago.edu>
Message-ID: <1107961813.9331.3.camel@gregor.bsd.uchicago.edu>

Hi,

I'm still trying to get RMySQL installed on my Mac.  When I try and
install from R (version 2.0.1) I'm told that the library "libz" can't be
located.  It suggests checking in /usr/lib to see if I have the
necessary libraries.  However, if I check /usr/lib I find I have the
following:

libz.1.1.3.dylib*
libz.1.dylib*
libz.dylib@

Since I have the libraries that RMySQL needs to install I can't figure
out why the installation is failing.  Does anyone have any thoughts on
how to get RMySQL installed on my Mac?  Thanks so much.

>Xander

On Mon, 2005-02-07 at 13:52, Xander Meadow wrote:
> Hi,
> 
> Thank you for responding.  One thing I forgot to mention (although I'm
> guessing you figured it out anyway) is that I'm running Mac OS X.
> 
> I checked and I've got the file /usr/include/zlib.h installed on my
> machine.  Is this enough for R or does it need a more robust
> installation of zlib?  If this is enough, how do I let R know about the
> zlib.h file. If it's not, where should I get the correct zlib from? 
> Thank you for any and all responses.
> 
> -Xander
> 
> On Mon, 2005-02-07 at 13:43, Roger D. Peng wrote:
> > R comes with it's own copy of zlib so even if it is compiled into R, 
> > there isn't necessarily a system-wide installation of the library. 
> > You may still need to install it.
> > 
> > -roger
> > 
> > Xander Meadow wrote:
> > > Hi,
> > > 
> > > I've got a Dual G5 running 10.3.7 and I'm trying to install RMySQL. 
> > > I've already got R up and running.  When I try the command
> > > 
> > > 
> > >>install.packages("RMySQL")
> > > 
> > > 
> > > It downloads a few things and then produces the following error:
> > > ---------------------------
> > > Configuration error:
> > >    Could not locate the library "libz" required by MySQL.
> > >  
> > > INSTRUCTIONS:
> > >  
> > >    The "libz" library is required by the MySQL client library
> > >    in order to compress/uncompress connections between clients
> > >    and the MySQL engine.
> > >  
> > >    Make sure you have "libz" installed properly and/or included
> > >    in your $LD_LIBRARY_PATH.  Perhaps it is not in any of the
> > >    standard directories (e.g., /usr/lib/, /usr/local/lib)?
> > >  
> > > Aborting the installation of RMySQL.
> > >  
> > > ERROR: configuration failed for package 'RMySQL'
> > > ----------------------------
> > > 
> > > However, if I check for libz I get:
> > > 
> > > 
> > >>capabilities("libz")
> > > 
> > > libz
> > > TRUE
> > > 
> > > I'm not sure what the problem is because R is telling me that it doesn't
> > > know where libz is, but then it's also telling me it does know where
> > > libz is.
> > > 
> > > Has anyone else seen this problem or know how I can get RMySQL installed
> > > on my machine?
> > > 
> > > Just as an added note I know that install.packages works because
> > > 
> > > 
> > >>install.packages("DBI")
> > > 
> > > 
> > > worked without a problem.
> > > 
> > > Thanks again.
> > > 
> > > -Xander
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> > >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From y.clough at NS1.uaoe.gwdg.de  Wed Feb  9 16:26:48 2005
From: y.clough at NS1.uaoe.gwdg.de (Yann Clough)
Date: Wed, 09 Feb 2005 16:26:48 +0100
Subject: [R] randomisation
In-Reply-To: <loom.20050209T150112-910@post.gmane.org>
Message-ID: <420A39C8.11751.17DBCAB@localhost>

Thanks to Gabor, Francois, Peter D., Jari, Peter R.and John

r2dtable is what I was looking for, it implements the Patefield algorithm.
Patefield, W. M. (1981) Algorithm AS159. An efficient method of generating r x c
tables with given row and column totals. Applied Statistics 30, 91 97.
This definitely made my day!
Cheers

Yann



From dhkblaszyk at zeelandnet.nl  Wed Feb  9 16:32:28 2005
From: dhkblaszyk at zeelandnet.nl (dhkblaszyk@zeelandnet.nl)
Date: Wed, 9 Feb 2005 16:32:28 +0100 (CET)
Subject: [Fwd: Re: Fw: [R] Contour plot]
Message-ID: <57292.127.0.0.1.1107963148.squirrel@127.0.0.1>

Petr,

It works perfectly! But I still have a question;

I have fit the following data;

x,y,z
1,10,11
2,11,15
3,12,21
4,13,29
5,14,39
6,15,51
7,16,65
8,17,81
9,18,99
10,19,119

>dat.lm <- lm(z~I(x^2)+y, data=dat)
>dat.lm

Call:
lm(formula = z ~ I(x^2) + y, data = dat)

Coefficients:
(Intercept)       I(x^2)            y
  1.841e-14    1.000e+00    1.000e+00

How do I create the "z" matrix from dat.lm?? Without having to type over
all the coefficients?

Kind regards, Darius Blaszijk

------------------------- Oorspronkelijk bericht -------------------------
Onderwerp: Re: Fw: [R] Contour plot
Van:       "Petr Pikal" <petr.pikal at precheza.cz>
Datum:     Wo, 9 februari, 2005 11:23 am
Aan:       dhkblaszyk at zeelandnet.nl
           r-help at stat.math.ethz.ch
--------------------------------------------------------------------------

Hi Darius


On 8 Feb 2005 at 17:43, dhkblaszyk at zeelandnet.nl wrote:

> I understand that I need to have a (in this case) square matrix with all
the data. But the question now is;
>
> - can the contourplot not interpolate the missing values
>
> or alternatively
>
> - I have fit a model to the z data (z = 100 + 0.5x + 0.5y). How can I
make from this model a "square" matrix z to make a contour plot?


Will

x<-1:10
y<-1:10
z <- outer(x,y,function(x,y) 100 + 0.5*x + 0.5*y)
contour(x,y,z)

work as you wish?
Cheers
Petr


>
> Kind regards, Darius Blaszijk
>
> ----- Original Message -----
> From: "Achim Zeileis" <Achim.Zeileis at wu-wien.ac.at>
> To: <dhkblaszyk at zeelandnet.nl>
> Cc: <r-help at stat.math.ethz.ch>
> Sent: Tuesday, February 08, 2005 1:51 AM
> Subject: Re: [R] Contour plot
>
>
> > On Tue, 8 Feb 2005 01:15:06 +0100 dhkblaszyk at zeelandnet.nl wrote:
> >
> > > Hello,
> > >
> > > I would like to make a contourplot of the following data;
> > >
> > > > x <- 1:10
> > > > y <- 1:10
> > > > z <- 100:110
> > >
> > > By doing >contour(x,y,z) I get the following error;
> > >
> > > "Error in contour.default(x, y, z) : no proper `z' matrix
> > > specified"
> > >
> > > How do I fix this??
> >
> > x and y specify a grid and thus z must provide a value for each
combination of the x's and y's! For example:
> >   x <- y <- 1:10
> >   contour(x, y, outer(x, y))
> > Also look at
> >   outer(x, y)
> > and read ?contour.
> >
> > Z
> >
> > > Kind regards, Datius Blaszijk
> > >
> > > [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From matthias.burger at epigenomics.com  Wed Feb  9 16:42:17 2005
From: matthias.burger at epigenomics.com (Matthias Burger)
Date: Wed, 09 Feb 2005 16:42:17 +0100
Subject: [R] behaviour of all(NULL == c("a", "b"))
Message-ID: <420A2F59.50602@epigenomics.com>


Hi all,

I'm a little surprised at
 > NULL == c("a", "b")
logical(0)
 > all(NULL == c("a", "b"))
[1] TRUE

Reading the documentation for all() this was not clear for me to be expected.
Originally the question came up when using
 > match.arg(NULL, c("a", "b"))
[1] "a"
where I had thought an error would occur.

So could someone please help me and explain what I have overlooked.
Should I realy have to use is.null() as a precondition check here when a NULL
argument could arise.

Regards,

   Matthias


PS: the same behaviour occurs in R 1.9.1 and R 2.1.0 devel: 2005-02-07
 > version
          _
platform i686-pc-linux-gnu
arch     i686
os       linux-gnu
system   i686, linux-gnu
status
major    2
minor    0.1
year     2004
month    11
day      15
language R

-- 
Matthias Burger                     Project Manager/ Biostatistician
Epigenomics AG    Kleine Praesidentenstr. 1    10178 Berlin, Germany
phone:+49-30-24345-371                          fax:+49-30-24345-555
http://www.epigenomics.com           matthias.burger at epigenomics.com



From kmw at mail.rockefeller.edu  Wed Feb  9 16:29:41 2005
From: kmw at mail.rockefeller.edu (Knut M. Wittkowski)
Date: Wed, 09 Feb 2005 10:29:41 -0500
Subject: [R] efficient R code
Message-ID: <5.1.0.14.0.20050209095729.020018e8@imap.rockefeller.edu>

Last Friday, Gregory Chaitin (http://www.umcs.maine.edu/~chaitin/lm.html) 
mentioned that there can be no proof that a given code is the shortest for 
a problem, even within a language. Still, the script below, a replacement 
of the "TDT", one of the most frequently used tests in genetics 
(http://mustat.rockefeller.edu under "downloads") may get close. It 
contains a few additional bytes for clarity, as in (2^1) for 2, but, 
otherwise, I don't think one could make this much shorter, especially the 
part that does the exact distribution.

I'm looking forward to comments on improving the programming efficiency for 
this problem. (The "return(...)" seems to be necessary in R only.)

Knut

#--------------------------------------------------------------------------
# asymp.SMN.pvalue(pP,qP,pX,qX,pQ,qQ)
# exact.SMN.pvalue(pP,qP,pX,qX,pQ,qQ)
#--------------------------------------------------------------------------
# pP,qP = number of PP,PQ children of PP~PQ parents
# pX,qX = number of PP,QQ children of PQ~PQ parents
# pQ,qQ = number of PQ,QQ children of PQ~QQ parents
#--------------------------------------------------------------------------

O3 <- function(X1,X2,X3,Op) matrix(outer(outer(X1,X2,Op),X3,Op))
b0 <- function(n) if (n==0) 1 else dbinom(0:n, n, .5)

Dst <- function(nP,   nX,   nQ   ) O3(b0(nP),(2^0)*b0(nX),b0(nQ),"*")
Est <- function(pP,qP,pX,qX,pQ,qQ) O3(pP-qP, (2^1)*(pX-qX),pQ-qQ,"+") # (1)
Var <- function(pP,qP,pX,qX,pQ,qQ) O3(pP+qP, (2^2)*(pX+qX),pQ+qQ,"+") # (2)

asymp.SMN.pvalue <- function(...)  1-pchisq( Est(...)^2/Var(...) ,1)  # (3)

exact.SMN.pvalue <- function(pP,qP,pX,qX,pQ,qQ)
{
         tb <- cbind(
                 Dst(nP<-pP+qP, nX<-pX+qX, nQ<-pQ+qQ),
                 Est(0:nP,nP:0, 0:nX,nX:0, 0:nQ,nQ:0)^2)

         return(1-sum(tb[tb[,2]<c(Est(pP,qP,pX,qX,pQ,qQ)^2),1]))
}
#--------------------------------------------------------------------------
# Note: The numbers in parentheses in inline comments refer to the equation
#       numbers in
#       Wittkowski KM, Liu X
#       A Statistically Valid Alternative to the TDT
#       Hum Hered 2002, 54:157-164; 2004, 58:60-61 (discussion)
#--------------------------------------------------------------------------


Knut M. Wittkowski, PhD,DSc
------------------------------------------
The Rockefeller University, GCRC
Experimental Design and Biostatistics
1230 York Ave #121B, Box 322, NY,NY 10021
+1(212)327-7175, +1(212)327-8450 (Fax)
kmw at rockefeller.edu
http://www.rucares.org/clinicalresearch/dept/biometry/



From parkhurs at ariel.ucs.indiana.edu  Wed Feb  9 16:50:35 2005
From: parkhurs at ariel.ucs.indiana.edu (David Parkhurst)
Date: Wed, 09 Feb 2005 10:50:35 -0500
Subject: [R] Black and white lattice plots
Message-ID: <420A314B.1070703@ariel.ucs.indiana.edu>

How can I get lattice plots (with xyplot, etc.) to be produced in black 
and white, rather than in color?  I?m using R 2.0.1 under windows XP.

(I'm not subscribed to the list, so would appreciate direct replies.)
Thanks for any help.

David Parkhurst



From ripley at stats.ox.ac.uk  Wed Feb  9 17:06:45 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 9 Feb 2005 16:06:45 +0000 (GMT)
Subject: [R] Histogram Bar Spacing or Border Width
In-Reply-To: <420A2617.9040008@comcast.net>
References: <420A2617.9040008@comcast.net>
Message-ID: <Pine.LNX.4.61.0502091601530.15867@gannet.stats>

A histogram is a density estimate (at least as defined in the Encyclopedia 
of Statistics Sciences, if not in many US Universities).  It is an area, 
not a series of unrelated bars, so it makes no sense to have spaces 
between the subareas.

Unfortunately, hist() will also produce barplots of counts.

On Wed, 9 Feb 2005, Thomas Hopper wrote:

> Is there any way to control the spacing between bars in a histogram, or 
> change the border width (I'm assuming the hist() function, though 
> alternatives are welcome)? I'm interested in changing the visual spacing 
> between columns in a plotted histogram.
>
> The general effect I'm looking for can be accomplished in barplots using the 
> "width=" parameter, but I have not been able to find a way to adjust the 
> apparent spacing in histograms.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From hold9309 at uidaho.edu  Wed Feb  9 17:12:06 2005
From: hold9309 at uidaho.edu (Zachary Holden)
Date: Wed, 09 Feb 2005 08:12:06 -0800
Subject: [R] Histogram Bar Spacing or Border Width
Message-ID: <148fa1147d79.147d79148fa1@uidaho.edu>

Tom,
I dealt with this once. 

 barplot.data<- c(2,3,4,5,6)
 x<- barplot(barplot.data, ylim = c(0,10), space= .9,.9,.9,.9)

use the space= to define the spacing between each of your barplot values. 
If you have groups of bars that you want together, with spaces between the groups,
you have to put 0's for each of the unspaced bars. 

Hope this helps,

Zack
----- Original Message -----
From: Thomas Hopper <tomhopper at comcast.net>
Date: Wednesday, February 9, 2005 7:02 am
Subject: [R] Histogram Bar Spacing or Border Width

> Is there any way to control the spacing between bars in a 
> histogram, or 
> change the border width (I'm assuming the hist() function, though 
> alternatives are welcome)? I'm interested in changing the visual 
> spacing 
> between columns in a plotted histogram.
> 
> The general effect I'm looking for can be accomplished in barplots 
> using 
> the "width=" parameter, but I have not been able to find a way to 
> adjust 
> the apparent spacing in histograms.
> 
> Thank you,
> 
> Tom Hopper
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html



From p.dalgaard at biostat.ku.dk  Wed Feb  9 17:09:10 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 09 Feb 2005 17:09:10 +0100
Subject: [R] dput() error
In-Reply-To: <2bb38f532df2ee4dadb2e32a20667a39@soc.soton.ac.uk>
References: <2bb38f532df2ee4dadb2e32a20667a39@soc.soton.ac.uk>
Message-ID: <x2sm45ogux.fsf@biostat.ku.dk>

Robin Hankin <r.hankin at soc.soton.ac.uk> writes:

> Hi
> 
> I'm trying to debug one of my functions, which stopped with the
> following message:
> 
> 
> 
> Error in is.vector(x.star) : subscript out of bounds
> 
> 
> So I put a "print(x.star)" statement in just before the is.vector bit
> and got
> 
> 
> Error in print(x.star) : subscript out of bounds
> 
> 
> So I put "dput(x.star)" in and got
> 
> Error in dput(x.star) : subscript out of bounds
> 
> 
> 
> what's going on here?  How best to proceed?

If x.star is an argument to your function, watch out for lazy
evaluation. It could be that *any* use of x.star triggers a faulty
computation in the caller. It might be informative to
print(substitute(x.star))

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From achilleas.psomas at wsl.ch  Wed Feb  9 17:15:23 2005
From: achilleas.psomas at wsl.ch (achilleas.psomas@wsl.ch)
Date: Wed,  9 Feb 2005 17:15:23 +0100
Subject: [R] Using %variable% object-column names in function
Message-ID: <1107965723.420a371b2bf55@webmail.wsl.ch>

Dear R-help..

I am rather new in R so i would appreciate your help in my problem..
I cant seem to be able to write a function that has arguments being  objects and
column names of these ojbects...
A simple example code that doesnt work is the following..


auto_plot <- function (object1,column1,object2,column2) {


plot(object1$column1,object2$column2)

}


I get the message:
Error in xy.coords(x, y, xlabel, ylabel, log) :
        x and y lengths differ

Maybe the solution is simple but i just couldnt find it..


Thanks a lot for your help..

Achilleas.



From p.dalgaard at biostat.ku.dk  Wed Feb  9 17:47:24 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 09 Feb 2005 17:47:24 +0100
Subject: [R] behaviour of all(NULL == c("a", "b"))
In-Reply-To: <420A2F59.50602@epigenomics.com>
References: <420A2F59.50602@epigenomics.com>
Message-ID: <x2k6phof37.fsf@biostat.ku.dk>

Matthias Burger <matthias.burger at epigenomics.com> writes:

> Hi all,
> 
> I'm a little surprised at
>  > NULL == c("a", "b")
> logical(0)
>  > all(NULL == c("a", "b"))
> [1] TRUE
> 
> Reading the documentation for all() this was not clear for me to be expected.

all() over a vector of length zero is TRUE for much the same reason
that prod(numeric(0)) is 1 which in turn is related to sums over empty
sets being zero. NULL has length zero so comparisons with it makes a
logical vector of length zero....

> Originally the question came up when using
>  > match.arg(NULL, c("a", "b"))
> [1] "a"
> where I had thought an error would occur.
> 
> So could someone please help me and explain what I have overlooked.
> Should I realy have to use is.null() as a precondition check here when a NULL
> argument could arise.
> 

Probably yes, unless you happen to like that behaviour (not entirely
unlikely in some cases).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From jrgold at insightful.com  Wed Feb  9 18:05:45 2005
From: jrgold at insightful.com (Jill Goldschneider 327)
Date: Wed, 9 Feb 2005 09:05:45 -0800 (PST)
Subject: [R] Job: Research Scientist Position at Insightful
Message-ID: <Pine.GSO.3.96.1050209090449.11554F-100000@octopus.insightful.com>

Insightful is searching for a candidate to fill
a technology position whose primary function is
to develop commercial software for mixed-effects
models and methods for analyzing longitudinal data,
survival data, and missing data.

Utilize your background in statistical algorithms,
data analysis, computational mathematics, numerical
analysis, multivariate calculus, linear algebra, and
commercial software development to create state-of-the-art
analytical software solutions for data analysis. Insightful
is an environment where you can make a difference through
innovation, drive, and passion.

Full details at:
http://www.insightful.com/company/jobdescription.asp?JobID=54

Please, email contact only to hr at insightful.com.

---

Jill R. Goldschneider, Ph.D.
Director of Research
Insightful Corporation
1700 Westlake Ave N., Suite 1700
Seattle, WA 98109



From achilleas.psomas at wsl.ch  Wed Feb  9 18:12:32 2005
From: achilleas.psomas at wsl.ch (achilleas.psomas@wsl.ch)
Date: Wed,  9 Feb 2005 18:12:32 +0100
Subject: [R] Using %variable% object-column names in function
In-Reply-To: <1107965723.420a371b2bf55@webmail.wsl.ch>
References: <1107965723.420a371b2bf55@webmail.wsl.ch>
Message-ID: <1107969152.420a4480bbff8@webmail.wsl.ch>

> Dear R-help..
>
> I am rather new in R so i would appreciate your help in my problem..
> I cant seem to be able to write a function that has arguments being  objects
> and
> column names of these ojbects...
> A simple example code that doesnt work is the following..
>
>
> auto_plot <- function (object1,column1,object2,column2) {
>
>
> plot(object1$column1,object2$column2)
>
> }
>
>
> I get the message:
> Error in xy.coords(x, y, xlabel, ylabel, log) :
>         x and y lengths differ
>
> Maybe the solution is simple but i just couldnt find it..
>
>
> Thanks a lot for your help..
>
> Achilleas.
>
>



From ripley at stats.ox.ac.uk  Wed Feb  9 18:13:23 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 9 Feb 2005 17:13:23 +0000 (GMT)
Subject: [R] behaviour of all(NULL == c("a", "b"))
In-Reply-To: <420A2F59.50602@epigenomics.com>
References: <420A2F59.50602@epigenomics.com>
Message-ID: <Pine.LNX.4.61.0502091652260.16434@gannet.stats>

On Wed, 9 Feb 2005, Matthias Burger wrote:

> I'm a little surprised at
>> NULL == c("a", "b")
> logical(0)

NULL is of length 0.
== coerces arguments and recycles as required, so you have done
as.character(NULL) == character(0)

>> all(NULL == c("a", "b"))
> [1] TRUE
>
> Reading the documentation for all() this was not clear for me to be expected.

all() of a zero-length vector is obviously true as all (zero) elements are 
true, standard in logic.

> Originally the question came up when using
>> match.arg(NULL, c("a", "b"))
> [1] "a"
> where I had thought an error would occur.

Did you read the help page?  You have misused the function (the first 
argument should be a character string), and it does not say what it does 
in the case of this particular misuse.  This behaviour is different from 
S, which also does not say.

> So could someone please help me and explain what I have overlooked.
> Should I realy have to use is.null() as a precondition check here when a NULL
> argument could arise.

Yes, as the help page says `character string' and NULL is not a character 
string.  The user should ensure the input is as documented.


[I suspect BTW that this undocumented behaviour is intentional, but it was 
introduced in about June 1998 and I cannot find a comment at that time.]


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From deepayan at stat.wisc.edu  Wed Feb  9 18:22:05 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Wed, 9 Feb 2005 11:22:05 -0600
Subject: [R] Black and white lattice plots
In-Reply-To: <420A314B.1070703@ariel.ucs.indiana.edu>
References: <420A314B.1070703@ariel.ucs.indiana.edu>
Message-ID: <200502091122.05101.deepayan@stat.wisc.edu>

On Wednesday 09 February 2005 09:50, David Parkhurst wrote:
> How can I get lattice plots (with xyplot, etc.) to be produced in
> black and white, rather than in color?  I?m using R 2.0.1 under
> windows XP.

This happens automatically for postscript output. Otherwise, start your 
device with

trellis.device(color = FALSE)

See ?trellis.device for details.

Deepayan



From tlumley at u.washington.edu  Wed Feb  9 18:21:26 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 9 Feb 2005 09:21:26 -0800 (PST)
Subject: [R] behaviour of all(NULL == c("a", "b"))
In-Reply-To: <420A2F59.50602@epigenomics.com>
References: <420A2F59.50602@epigenomics.com>
Message-ID: <Pine.A41.4.61b.0502090907370.224550@homer08.u.washington.edu>

On Wed, 9 Feb 2005, Matthias Burger wrote:

>
> Hi all,
>
> I'm a little surprised at
>> NULL == c("a", "b")
> logical(0)
>> all(NULL == c("a", "b"))
> [1] TRUE
>
> Reading the documentation for all() this was not clear for me to be expected.

This is related to the question about sum(numeric(0)) that came up a few 
days ago.

It is conventional in logic that "for all x in X: P(x)" is true when X is 
empty.  A reason why this is a useful convention is that it implies
all(c(x,y)) == all(x) && all(y) is still true when x or y happens to be 
empty.  The converse holds for any():  any(logical(0)) is FALSE. This 
surprises fewer people.

A fairly complete list is

all(NULL) is TRUE
any(NULL) is FALSE
sum(NULL) is 0
prod(NULL) is 1
min(NULL) is Inf
max(NULL) is -Inf

with the last two giving a warning

> Originally the question came up when using
>> match.arg(NULL, c("a", "b"))
> [1] "a"
> where I had thought an error would occur.

I would have assumed "a" would be returned, thinking of match.arg as a way 
to handle default arguments, but I agree it isn't documented, and looking 
at the code it isn't clear what the author thought.

On the other hand, it *is* documented that `arg' must be a character 
string, and NULL isn't a character string. Perhaps you should just use 
pmatch() directly, as the main advantage of match.arg() is its ability to 
look up default arguments, which you don't seem to be using.


 	-thomas



From ramasamy at cancer.org.uk  Wed Feb  9 18:23:22 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Wed, 09 Feb 2005 17:23:22 +0000
Subject: [Fwd: Re: Fw: [R] Contour plot]
In-Reply-To: <57292.127.0.0.1.1107963148.squirrel@127.0.0.1>
References: <57292.127.0.0.1.1107963148.squirrel@127.0.0.1>
Message-ID: <1107969802.5884.54.camel@dhcp-63.ccc.ox.ac.uk>

How about "data.lm$model" ?


On Wed, 2005-02-09 at 16:32 +0100, dhkblaszyk at zeelandnet.nl wrote:
> Petr,
> 
> It works perfectly! But I still have a question;
> 
> I have fit the following data;
> 
> x,y,z
> 1,10,11
> 2,11,15
> 3,12,21
> 4,13,29
> 5,14,39
> 6,15,51
> 7,16,65
> 8,17,81
> 9,18,99
> 10,19,119
> 
> >dat.lm <- lm(z~I(x^2)+y, data=dat)
> >dat.lm
> 
> Call:
> lm(formula = z ~ I(x^2) + y, data = dat)
> 
> Coefficients:
> (Intercept)       I(x^2)            y
>   1.841e-14    1.000e+00    1.000e+00
> 
> How do I create the "z" matrix from dat.lm?? Without having to type over
> all the coefficients?
> 
> Kind regards, Darius Blaszijk
> 
> ------------------------- Oorspronkelijk bericht -------------------------
> Onderwerp: Re: Fw: [R] Contour plot
> Van:       "Petr Pikal" <petr.pikal at precheza.cz>
> Datum:     Wo, 9 februari, 2005 11:23 am
> Aan:       dhkblaszyk at zeelandnet.nl
>            r-help at stat.math.ethz.ch
> --------------------------------------------------------------------------
> 
> Hi Darius
> 
> 
> On 8 Feb 2005 at 17:43, dhkblaszyk at zeelandnet.nl wrote:
> 
> > I understand that I need to have a (in this case) square matrix with all
> the data. But the question now is;
> >
> > - can the contourplot not interpolate the missing values
> >
> > or alternatively
> >
> > - I have fit a model to the z data (z = 100 + 0.5x + 0.5y). How can I
> make from this model a "square" matrix z to make a contour plot?
> 
> 
> Will
> 
> x<-1:10
> y<-1:10
> z <- outer(x,y,function(x,y) 100 + 0.5*x + 0.5*y)
> contour(x,y,z)
> 
> work as you wish?
> Cheers
> Petr
> 
> 
> >
> > Kind regards, Darius Blaszijk
> >
> > ----- Original Message -----
> > From: "Achim Zeileis" <Achim.Zeileis at wu-wien.ac.at>
> > To: <dhkblaszyk at zeelandnet.nl>
> > Cc: <r-help at stat.math.ethz.ch>
> > Sent: Tuesday, February 08, 2005 1:51 AM
> > Subject: Re: [R] Contour plot
> >
> >
> > > On Tue, 8 Feb 2005 01:15:06 +0100 dhkblaszyk at zeelandnet.nl wrote:
> > >
> > > > Hello,
> > > >
> > > > I would like to make a contourplot of the following data;
> > > >
> > > > > x <- 1:10
> > > > > y <- 1:10
> > > > > z <- 100:110
> > > >
> > > > By doing >contour(x,y,z) I get the following error;
> > > >
> > > > "Error in contour.default(x, y, z) : no proper `z' matrix
> > > > specified"
> > > >
> > > > How do I fix this??
> > >
> > > x and y specify a grid and thus z must provide a value for each
> combination of the x's and y's! For example:
> > >   x <- y <- 1:10
> > >   contour(x, y, outer(x, y))
> > > Also look at
> > >   outer(x, y)
> > > and read ?contour.
> > >
> > > Z
> > >
> > > > Kind regards, Datius Blaszijk
> > > >
> > > > [[alternative HTML version deleted]]
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide!
> > > > http://www.R-project.org/posting-guide.html
> > > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> 
> Petr Pikal
> petr.pikal at precheza.cz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From tlumley at u.washington.edu  Wed Feb  9 18:30:53 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 9 Feb 2005 09:30:53 -0800 (PST)
Subject: [R] efficient R code
In-Reply-To: <5.1.0.14.0.20050209095729.020018e8@imap.rockefeller.edu>
References: <5.1.0.14.0.20050209095729.020018e8@imap.rockefeller.edu>
Message-ID: <Pine.A41.4.61b.0502090923210.224550@homer08.u.washington.edu>

On Wed, 9 Feb 2005, Knut M. Wittkowski wrote:

> Last Friday, Gregory Chaitin (http://www.umcs.maine.edu/~chaitin/lm.html) 
> mentioned that there can be no proof that a given code is the shortest for a 
> problem, even within a language.

I presume he said something more like `for some problems there is no 
proof' or `there is no procedure for constructing a proof'

You can prove that function(x) x+1 is the shortest function for computing 
x+1 in R by listing all the shorter functions and verifying that none of 
them computes x+1, both of which tasks can be done fairly fast.  This 
approach doesn't generalise because you can't give a general way to 
perform the second step: checking that none of the smaller programs works.

I would expect that the TDT falls in the category of problems checkable by 
enumeration.


>				Still, the script below, a replacement of 
> the "TDT", one of the most frequently used tests in genetics 
> (http://mustat.rockefeller.edu under "downloads") may get close. It contains 
> a few additional bytes for clarity, as in (2^1) for 2, but, otherwise, I 
> don't think one could make this much shorter, especially the part that does 
> the exact distribution.
>
> I'm looking forward to comments on improving the programming efficiency for 
> this problem. (The "return(...)" seems to be necessary in R only.)
>

The return() should not be necessary in R.

 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From matthias.burger at epigenomics.com  Wed Feb  9 18:50:44 2005
From: matthias.burger at epigenomics.com (Matthias Burger)
Date: Wed, 09 Feb 2005 18:50:44 +0100
Subject: [R] behaviour of all(NULL == c("a", "b"))
In-Reply-To: <Pine.A41.4.61b.0502090907370.224550@homer08.u.washington.edu>
References: <420A2F59.50602@epigenomics.com>
	<Pine.A41.4.61b.0502090907370.224550@homer08.u.washington.edu>
Message-ID: <420A4D74.9080107@epigenomics.com>


thanks to Peter Daalgard, Brian Ripley, and Thomas Lumley for their quick, 
accurate and very helpful replies.

My understanding of NULL was incorrect, thanks for making this clear.
As for the question on match.arg I will use pmatch instead.

Best,

   Matthias



Thomas Lumley wrote:
> On Wed, 9 Feb 2005, Matthias Burger wrote:
> 
>>
>> Hi all,
>>
>> I'm a little surprised at
>>
>>> NULL == c("a", "b")
>>
>> logical(0)
>>
>>> all(NULL == c("a", "b"))
>>
>> [1] TRUE
>>
>> Reading the documentation for all() this was not clear for me to be 
>> expected.
> 
> 
> This is related to the question about sum(numeric(0)) that came up a few 
> days ago.
> 
> It is conventional in logic that "for all x in X: P(x)" is true when X 
> is empty.  A reason why this is a useful convention is that it implies
> all(c(x,y)) == all(x) && all(y) is still true when x or y happens to be 
> empty.  The converse holds for any():  any(logical(0)) is FALSE. This 
> surprises fewer people.
> 
> A fairly complete list is
> 
> all(NULL) is TRUE
> any(NULL) is FALSE
> sum(NULL) is 0
> prod(NULL) is 1
> min(NULL) is Inf
> max(NULL) is -Inf
> 
> with the last two giving a warning
> 
>> Originally the question came up when using
>>
>>> match.arg(NULL, c("a", "b"))
>>
>> [1] "a"
>> where I had thought an error would occur.
> 
> 
> I would have assumed "a" would be returned, thinking of match.arg as a 
> way to handle default arguments, but I agree it isn't documented, and 
> looking at the code it isn't clear what the author thought.
> 
> On the other hand, it *is* documented that `arg' must be a character 
> string, and NULL isn't a character string. Perhaps you should just use 
> pmatch() directly, as the main advantage of match.arg() is its ability 
> to look up default arguments, which you don't seem to be using.
> 
> 
>     -thomas

-- 
Matthias Burger                     Project Manager/ Biostatistician
Epigenomics AG    Kleine Praesidentenstr. 1    10178 Berlin, Germany
phone:+49-30-24345-371                          fax:+49-30-24345-555
http://www.epigenomics.com           matthias.burger at epigenomics.com



From Jan.Verbesselt at agr.kuleuven.ac.be  Wed Feb  9 19:18:32 2005
From: Jan.Verbesselt at agr.kuleuven.ac.be (Jan Verbesselt)
Date: Wed, 9 Feb 2005 19:18:32 +0100
Subject: [R] Plotting: Plot several axis at right hand side of the plot
Message-ID: <000001c50ed3$c3623a10$1145210a@agr.ad10.intern.kuleuven.ac.be>

Hi all,

I created a graph by plotting 4 time series on top of each other but don't
know how to add extra axes on the right hand side of the plot (or leftside).
For the first two time series it axis are plotted but the last two time
series don't have and axis. How can these be added and where?

e.g.
par(mfrow=c(1,1))
    plot(timeserie1)
    par(new=T)
    plot(timeserie2, type="b", col=3,yaxt="n", ylab="" )
    axis(4)
    mtext(side=4, line=2, "NDVI")
    par(new=T)
    plot(timeserie3, type="h", col=2,yaxt="n", ylab="")
    par(new=T)
    plot(timeserie4, type="b", col=4,yaxt="n", ylab="")

Thanks,
Jan


_______________________________________________________________________
ir. Jan Verbesselt 
Research Associate 
Lab of Geomatics and Forest Engineering K.U. Leuven
Vital Decosterstraat 102. B-3000 Leuven Belgium 
Tel:+32-16-329750   Fax: +32-16-329760
http://gloveg.kuleuven.ac.be/



From ligges at statistik.uni-dortmund.de  Wed Feb  9 19:32:07 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 09 Feb 2005 19:32:07 +0100
Subject: [R] Using %variable% object-column names in function
In-Reply-To: <1107965723.420a371b2bf55@webmail.wsl.ch>
References: <1107965723.420a371b2bf55@webmail.wsl.ch>
Message-ID: <420A5727.3080908@statistik.uni-dortmund.de>

achilleas.psomas at wsl.ch wrote:

> Dear R-help..
> 
> I am rather new in R so i would appreciate your help in my problem..
> I cant seem to be able to write a function that has arguments being  objects and
> column names of these ojbects...
> A simple example code that doesnt work is the following..
> 
> 
> auto_plot <- function (object1,column1,object2,column2) {
> 
> 
> plot(object1$column1,object2$column2)


    plot(object1[[column1]], object2[[column2]])

Uwe Ligges

> }
> 
> 
> I get the message:
> Error in xy.coords(x, y, xlabel, ylabel, log) :
>         x and y lengths differ
> 
> Maybe the solution is simple but i just couldnt find it..
> 
> 
> Thanks a lot for your help..
> 
> Achilleas.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From shuangge at uwalumni.com  Wed Feb  9 19:41:37 2005
From: shuangge at uwalumni.com (shuangge@uwalumni.com)
Date: Wed,  9 Feb 2005 12:41:37 -0600
Subject: [R] help with plot hclust tree
Message-ID: <20050209124137.oi2rni84s0sk40go@secure.uwalumni.com>

Hello,
I will really appreciate some help with the following question.

let's say cor.m is a correlation coefficient matrix.

tree<-hclust(as.dist(1-cor.m));
plot(tree);

my question is: is it possible to add the correlation coefficient information
to the plot?

thanks,
S. Ma



From rrsilva at ib.usp.br  Wed Feb  9 21:24:31 2005
From: rrsilva at ib.usp.br (=?iso-8859-1?q?Rog=E9rio_Rosa_da_Silva?=)
Date: Wed, 9 Feb 2005 18:24:31 -0200
Subject: [R] gl and different number of replications
Message-ID: <200502091824.31308.rrsilva@ib.usp.br>

I am trying to use the function gl (generate levels), and would like to make 
levels with different number of replications. Does anyone know how to 
generate different number of replications in each level?

Something like:

[1] Site1 Site1 Site1 Site1 Site1 Site1 Site1
[8] Site2 Site2 Site2 Site2 Site2

Thanks in advance
-- 
Rog?rio R. Silva
MZUSP http://www.mz.usp.br
Linux/Debian User # 354364
Linux counter http://counter.li.org



From dwilhelm at evafunds.com  Wed Feb  9 21:30:27 2005
From: dwilhelm at evafunds.com (Dale Ryon Wilhelm)
Date: Wed, 9 Feb 2005 12:30:27 -0800
Subject: [R] install issue | suse 9.2
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A584E49B@phost015.EVAFUNDS.intermedia.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050209/88a8dd89/attachment.pl

From gunter.berton at gene.com  Wed Feb  9 21:50:03 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 9 Feb 2005 12:50:03 -0800
Subject: [R] gl and different number of replications
In-Reply-To: <200502091824.31308.rrsilva@ib.usp.br>
Message-ID: <200502092050.j19Ko3I2001566@faraday.gene.com>

Can't do it. gl is based on rep(), which  easily does what you want:

factor(rep(c('Site1','Site2'),c(7,5)))

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Rog?rio Rosa da Silva
> Sent: Wednesday, February 09, 2005 12:25 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] gl and different number of replications
> 
> I am trying to use the function gl (generate levels), and 
> would like to make 
> levels with different number of replications. Does anyone know how to 
> generate different number of replications in each level?
> 
> Something like:
> 
> [1] Site1 Site1 Site1 Site1 Site1 Site1 Site1
> [8] Site2 Site2 Site2 Site2 Site2
> 
> Thanks in advance
> -- 
> Rog?rio R. Silva
> MZUSP http://www.mz.usp.br
> Linux/Debian User # 354364
> Linux counter http://counter.li.org
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Mike.Prager at noaa.gov  Wed Feb  9 22:03:48 2005
From: Mike.Prager at noaa.gov (Mike Prager)
Date: Wed, 09 Feb 2005 16:03:48 -0500
Subject: [R] Appending to list
Message-ID: <6.1.2.0.2.20050209155558.01eb4a38@hermes.nos.noaa.gov>

An R program needs to accumulate a list (length unknown until done) of 
objects.  How efficiently and repeatedly can one append new objects to the 
list?

I have been using something like

mylist[[length(mylist)+1]] = newobject

but I wonder if there is something better.



-- 
Michael Prager
NOAA Center for Coastal Fisheries and Habitat Research
Beaufort, North Carolina  28516  USA
http://shrimp.ccfhrb.noaa.gov/~mprager/

NOTE: Opinions expressed are personal, not official. No government
endorsement of any product is made or implied.



From tomhopper at comcast.net  Wed Feb  9 22:04:02 2005
From: tomhopper at comcast.net (Thomas Hopper)
Date: Wed, 9 Feb 2005 16:04:02 -0500
Subject: [R] Histogram Bar Spacing or Border Width
In-Reply-To: <Pine.LNX.4.61.0502091601530.15867@gannet.stats>
References: <420A2617.9040008@comcast.net>
	<Pine.LNX.4.61.0502091601530.15867@gannet.stats>
Message-ID: <9bc71d33307287fa0e6eff07ceb9e2c1@comcast.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050209/932deda7/attachment.pl

From tomhopper at comcast.net  Wed Feb  9 22:07:21 2005
From: tomhopper at comcast.net (Thomas Hopper)
Date: Wed, 9 Feb 2005 16:07:21 -0500
Subject: [R] Histogram Bar Spacing or Border Width
In-Reply-To: <148fa1147d79.147d79148fa1@uidaho.edu>
References: <148fa1147d79.147d79148fa1@uidaho.edu>
Message-ID: <73888b4b5762760901d36710f37e0216@comcast.net>

I see where you're going...yes, I could create my own histogram 
distribution from the data and plot it using barplot (in fact, I've 
already achieved the visual effect I want with barplot, so I could just 
expand on that). I'm hoping that I can find a way to do it with hist() 
to avoid duplicating the good work that has already gone in to that 
function.

Thanks,

Tom
On Feb 9, 2005, at 11:12 AM, Zachary Holden wrote:

> Tom,
> I dealt with this once.
>
>  barplot.data<- c(2,3,4,5,6)
>  x<- barplot(barplot.data, ylim = c(0,10), space= .9,.9,.9,.9)
>
> use the space= to define the spacing between each of your barplot 
> values.
> If you have groups of bars that you want together, with spaces between 
> the groups,
> you have to put 0's for each of the unspaced bars.
>
> Hope this helps,
>
> Zack
> ----- Original Message -----
> From: Thomas Hopper <tomhopper at comcast.net>
> Date: Wednesday, February 9, 2005 7:02 am
> Subject: [R] Histogram Bar Spacing or Border Width
>
>> Is there any way to control the spacing between bars in a
>> histogram, or
>> change the border width (I'm assuming the hist() function, though
>> alternatives are welcome)? I'm interested in changing the visual
>> spacing
>> between columns in a plotted histogram.
>>
>> The general effect I'm looking for can be accomplished in barplots
>> using
>> the "width=" parameter, but I have not been able to find a way to
>> adjust
>> the apparent spacing in histograms.
>>
>> Thank you,
>>
>> Tom Hopper
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-
>> guide.html
>
>
---
"Never confuse motion with action."
                                   -Benjamin Franklin



From ligges at statistik.uni-dortmund.de  Wed Feb  9 22:30:29 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 09 Feb 2005 22:30:29 +0100
Subject: [R] Plotting: Plot several axis at right hand side of the plot
In-Reply-To: <000001c50ed3$c3623a10$1145210a@agr.ad10.intern.kuleuven.ac.be>
References: <000001c50ed3$c3623a10$1145210a@agr.ad10.intern.kuleuven.ac.be>
Message-ID: <420A80F5.3000000@statistik.uni-dortmund.de>

Jan Verbesselt wrote:
> Hi all,
> 
> I created a graph by plotting 4 time series on top of each other but don't
> know how to add extra axes on the right hand side of the plot (or leftside).
> For the first two time series it axis are plotted but the last two time
> series don't have and axis. How can these be added and where?
> 
> e.g.
> par(mfrow=c(1,1))
>     plot(timeserie1)
>     par(new=T)
>     plot(timeserie2, type="b", col=3,yaxt="n", ylab="" )
>     axis(4)
>     mtext(side=4, line=2, "NDVI")
>     par(new=T)
>     plot(timeserie3, type="h", col=2,yaxt="n", ylab="")
>     par(new=T)
>     plot(timeserie4, type="b", col=4,yaxt="n", ylab="")


I'm confused! You want 4 different y axis for one plot? Well, the first 
question is where want you the third and fourth y-axis be added?

Isn't it a better idea - given all timeseries have a reasonable range of 
y-values - to plot them using the same y axis?

Uwe Ligges


> Thanks,
> Jan
> 
> 
> _______________________________________________________________________
> ir. Jan Verbesselt 
> Research Associate 
> Lab of Geomatics and Forest Engineering K.U. Leuven
> Vital Decosterstraat 102. B-3000 Leuven Belgium 
> Tel:+32-16-329750   Fax: +32-16-329760
> http://gloveg.kuleuven.ac.be/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Wed Feb  9 22:38:37 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 09 Feb 2005 22:38:37 +0100
Subject: [R] help with plot hclust tree
In-Reply-To: <20050209124137.oi2rni84s0sk40go@secure.uwalumni.com>
References: <20050209124137.oi2rni84s0sk40go@secure.uwalumni.com>
Message-ID: <420A82DD.6050001@statistik.uni-dortmund.de>

shuangge at uwalumni.com wrote:

> Hello,
> I will really appreciate some help with the following question.
> 
> let's say cor.m is a correlation coefficient matrix.
> 
> tree<-hclust(as.dist(1-cor.m));
> plot(tree);
> 
> my question is: is it possible to add the correlation coefficient information
> to the plot?

Which "correlation coefficient information"?
By means of some visualization trick or as labels?

Folks, please read the psoting guide, ask specific questions and give 
reproducible examples (is not of that importance in this case, but yet 
another example of non reproducible code).

Uwe Ligges


> thanks,
> S. Ma
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Wed Feb  9 22:48:03 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 9 Feb 2005 21:48:03 +0000 (GMT)
Subject: [R] Histogram Bar Spacing or Border Width
In-Reply-To: <9bc71d33307287fa0e6eff07ceb9e2c1@comcast.net>
References: <420A2617.9040008@comcast.net>
	<Pine.LNX.4.61.0502091601530.15867@gannet.stats>
	<9bc71d33307287fa0e6eff07ceb9e2c1@comcast.net>
Message-ID: <Pine.LNX.4.61.0502092145230.28649@gannet.stats>

A hint: hist() does no plotting.  That is done by the "histogram" method 
of plot(), which you can copy and modify to your own needs/desires.
Alternatively you can create myplot() and call

myplot(hist(..., plot=TRUE), ...)

On Wed, 9 Feb 2005, Thomas Hopper wrote:

> I understand the problem from a statistical perspective, and you make an 
> excellent point (as I have come to expect, reading this list). However, I'm 
> thinking about it from a visual/aesthetic perspective.
>
> Let me try this. Plot two histograms side-by-side:
>
>> x <- rnorm(10)
>> par(mfcol=c(1,2))
>> hist(x)
>> hist(x,col="gray",border="gray")
>
> Which one is easier to interpret? To my eyes, the default hist() is, but 
> given the statistical meaning of a histogram, I think I could argue that the 
> gray version is a more accurate representation of the data.
>
> The default histogram delimits the space between subareas to make the graph 
> easier to read. I'm just trying to tweak this to further improve it. I don't 
> want to separate the bars so much as control (and, I hope, improve) their 
> appearance.
>
> Thanks,
>
> Tom
>
> On Feb 9, 2005, at 11:06 AM, Prof Brian Ripley wrote:
>
>> A histogram is a density estimate (at least as defined in the Encyclopedia 
>> of Statistics Sciences, if not in many US Universities).  It is an area, 
>> not a series of unrelated bars, so it makes no sense to have spaces between 
>> the subareas.
>> 
>> Unfortunately, hist() will also produce barplots of counts.
>> 
>> On Wed, 9 Feb 2005, Thomas Hopper wrote:
>> 
>>> Is there any way to control the spacing between bars in a histogram, or 
>>> change the border width (I'm assuming the hist() function, though 
>>> alternatives are welcome)? I'm interested in changing the visual spacing 
>>> between columns in a plotted histogram.
>>> 
>>> The general effect I'm looking for can be accomplished in barplots using 
>>> the "width=" parameter, but I have not been able to find a way to adjust 
>>> the apparent spacing in histograms.
>> 
>> -- 
>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>> University of Oxford,             Tel:  +44 1865 272861 (self)
>> 1 South Parks Road,                     +44 1865 272866 (PA)
>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>> 
>> 
> ---
> In cyberspace, no one can hear you scream.
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at myway.com  Wed Feb  9 23:17:16 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 9 Feb 2005 22:17:16 +0000 (UTC)
Subject: [R] gl and different number of replications
References: <200502091824.31308.rrsilva@ib.usp.br>
Message-ID: <loom.20050209T231627-204@post.gmane.org>

Rogrio Rosa da Silva <rrsilva <at> ib.usp.br> writes:

: 
: I am trying to use the function gl (generate levels), and would like to make 
: levels with different number of replications. Does anyone know how to 
: generate different number of replications in each level?
: 
: Something like:
: 
: [1] Site1 Site1 Site1 Site1 Site1 Site1 Site1
: [8] Site2 Site2 Site2 Site2 Site2
: 

gl(2,7,12, lab = "Site")



From ggrothendieck at myway.com  Wed Feb  9 23:25:58 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 9 Feb 2005 22:25:58 +0000 (UTC)
Subject: [R] Appending to list
References: <6.1.2.0.2.20050209155558.01eb4a38@hermes.nos.noaa.gov>
Message-ID: <loom.20050209T232532-256@post.gmane.org>

Mike Prager <Mike.Prager <at> noaa.gov> writes:

: 
: An R program needs to accumulate a list (length unknown until done) of 
: objects.  How efficiently and repeatedly can one append new objects to the 
: list?
: 
: I have been using something like
: 
: mylist[[length(mylist)+1]] = newobject
: 
: but I wonder if there is something better.
: 

This was discussed last month:

https://www.stat.math.ethz.ch/pipermail/r-help/2005-January/062088.html



From jbdunsmo at utmb.edu  Wed Feb  9 23:50:13 2005
From: jbdunsmo at utmb.edu (jbdunsmo@utmb.edu)
Date: Wed, 9 Feb 2005 16:50:13 -0600
Subject: [R] simple example of C interface to R
In-Reply-To: <Pine.LNX.4.61.0502082212500.18839@gannet.stats>
References: <D9A95B4B7B20354992E165EEADA31999056A92C7@uswpmx00.merck.com>
	<20050208220234.GA25871@g20458.utmb.edu>
	<Pine.LNX.4.61.0502082212500.18839@gannet.stats>
Message-ID: <20050209225013.GB997@g20458.utmb.edu>

On Tue, Feb 08, 2005 at 10:16:42PM +0000, Prof Brian Ripley wrote:
> On Tue, 8 Feb 2005 jbdunsmo at utmb.edu wrote:
> 
> >thanks for all the help.  i've tried everyone's suggestions, to no
> >avail...
> 

i finally figured it out.  i needed to do the following: use the
configure script in the base directory of the R 2.0 source, build the
whole R package using make, update my version of R to 2.0, edit the
Makefile in the tests/Embedded directory:

change
	LIBR = -L`cd $(top_builddir) && $(GETWD)`/bin -lR
to
	LIBR = -L`R RHOME`/lib -lR
	
set LD_LIBRARY_PATH:

	export LD_LIBRARY_PATH="/usr/lib/R/lib"

and then execute make in the tests/Embedded directory.



From nair at sdsc.edu  Thu Feb 10 01:35:22 2005
From: nair at sdsc.edu (T. Murlidharan Nair)
Date: Wed, 09 Feb 2005 16:35:22 -0800
Subject: [R] cuttree
Message-ID: <420AAC4A.4070808@sdsc.edu>

How do I get the labels from a hclust object after I have cut
the tree using cutree ?
hc<-hclust(someData,"a")

branches<-cutree(hc)

If there are two branches, how do I obtain the labels associated with 
each of the branches separately ?
Thanks../Murli



From spencer.graves at pdf.com  Thu Feb 10 02:32:15 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 09 Feb 2005 17:32:15 -0800
Subject: [R] Re: Packages and Libraries
In-Reply-To: <x2ll9zyx0s.fsf@biostat.ku.dk>
References: <42088443.2090206@wiwi.uni-bielefeld.de>	<16904.39235.910508.75875@stat.math.ethz.ch>	<1abe3fa9050208043374d362df@mail.gmail.com>
	<x2ll9zyx0s.fsf@biostat.ku.dk>
Message-ID: <420AB99F.90704@pdf.com>

      The reasons to 'introduce "package()" and deprecate "library()"' 
may be OBVIOUS to you, but they completely escape me.  Could you please 
clarify why that's obvious?  I've seen many admonitions on this list 
that the term is "package" NOT "library", but I don't recall ever seeing 
any explanation of why the term "package" is more appropriate than 
"library". 

      I suspect there may be some rationale that "package" seems more 
appropriately descriptive.  However, is it so much more precise that it 
justifies creating a distinction between S-Plus and R? 

      Or is it your intention to make it harder for people who have to 
use S-Plus to also use R?  Or do you want to make it more difficult for 
people to write code that will work in both S-Plus and R or for people 
to migrate from S-Plus to R or vice versa?  Unless that is your intent, 
I'd like to know why you don't make the complementary change, namely 
globally replace "package" with "library" everywhere in the 
documentation -- or at least deprecate its future use. 

      R is a marvelous creation, a solid contribution to the advancement 
of science and through that human knowledge and eventually even the 
ability of people everywhere to live more comfortably, longer. 

      spencer graves

p.s.  I'm told that the French Royal Academy delayed the introduction in 
France of a product marketed by a US company.  The product couldn't be 
sold in France without a French language manual.  The translation could 
not be published until the French Royal Academy officially provided or 
blessed new French words for new technical terms.  Similarly, Le Monde 
Diplomatique recently carried an article proposing "A polyglot world to 
escape the English dictatorship."  (January 2005, pp. 22-23:  "Un monde 
polyglotte pour echapper a la dictature de l'anglais';  someone may wish 
to correct my translation.)  We could require all r-help subscribers to 
learn enough French, German, Spanish AND Portuguese to be able to read 
posts in those languages, but that might be counterproductive.   

Peter Dalgaard wrote:

>"A.J. Rossini" <blindglobe at gmail.com> writes:
>
>  
>
>>For OBVIOUS reasons, is there any chance that we could introduce
>>"package()" and deprecate "library()"?
>>    
>>
>
>usepackage() or usePackage() has been suggested, but someone got
>ambitious and wanted it to be different from library(), and it sort of
>didn't get any further. We still have some time before feature freeze
>for 2.1.0 though.
>
>  
>
>>(well, I'll also ask if we could deprecate "=" for assignment, but
>>that's hopeless).
>>    
>>
>
>You're not *forced* to use it...
>
>  
>



From br44114 at yahoo.com  Thu Feb 10 02:38:30 2005
From: br44114 at yahoo.com (bogdan romocea)
Date: Wed, 9 Feb 2005 17:38:30 -0800 (PST)
Subject: [R] question about sorting POSIXt vector
Message-ID: <20050210013830.8660.qmail@web50301.mail.yahoo.com>

Dear useRs,

How come the first attempt to sort a POSIXt vector fails (Error:
non-atomic type in greater), while the second succeeds? (Code inserted
below.) The documentation says that POSIXt is used to allow operations
such as subtraction, so I'd expect sorting to work. Is this perhaps an
OS issue? (I run R 2.0.1 on Win xp.)

Thank you,
b.

#------------code
test <- c("2005-02-08 18:49:15","2005-02-07 18:36:54",
	"2005-02-04 18:37:03","2005-02-06 18:29:04")
test <- strptime(test,format="%Y-%m-%d %H:%M:%S")
order(test,decreasing=F)	#doesn't work - why?
tst <- test + 0
order(tst,decreasing=F)	#works - how come?
print(tst)
#------------run
> test <- c("2005-02-08 18:49:15","2005-02-07 18:36:54",
+ "2005-02-04 18:37:03","2005-02-06 18:29:04")
> test <- strptime(test,format="%Y-%m-%d %H:%M:%S")
> order(test,decreasing=F)#doesn't work - why?
Error in order(test, decreasing = F) : non-atomic type in greater
> tst <- test + 0
> order(tst,decreasing=F)#works - how come?
[1] 3 4 2 1
> print(tst)
[1] "2005-02-08 18:49:15 Eastern Standard Time" "2005-02-07 18:36:54
Eastern Standard Time"
[3] "2005-02-04 18:37:03 Eastern Standard Time" "2005-02-06 18:29:04
Eastern Standard Time"
>



From ggrothendieck at myway.com  Thu Feb 10 02:57:43 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 10 Feb 2005 01:57:43 +0000 (UTC)
Subject: [R] question about sorting POSIXt vector
References: <20050210013830.8660.qmail@web50301.mail.yahoo.com>
Message-ID: <loom.20050210T025121-831@post.gmane.org>

bogdan romocea <br44114 <at> yahoo.com> writes:

: 
: Dear useRs,
: 
: How come the first attempt to sort a POSIXt vector fails (Error:
: non-atomic type in greater), while the second succeeds? (Code inserted
: below.) The documentation says that POSIXt is used to allow operations
: such as subtraction, so I'd expect sorting to work. Is this perhaps an
: OS issue? (I run R 2.0.1 on Win xp.)
: 
: Thank you,
: b.
: 
: #------------code
: test <- c("2005-02-08 18:49:15","2005-02-07 18:36:54",
: 	"2005-02-04 18:37:03","2005-02-06 18:29:04")
: test <- strptime(test,format="%Y-%m-%d %H:%M:%S")
: order(test,decreasing=F)	#doesn't work - why?
: tst <- test + 0
: order(tst,decreasing=F)	#works - how come?
: print(tst)
: #------------run
: > test <- c("2005-02-08 18:49:15","2005-02-07 18:36:54",
: + "2005-02-04 18:37:03","2005-02-06 18:29:04")
: > test <- strptime(test,format="%Y-%m-%d %H:%M:%S")
: > order(test,decreasing=F)#doesn't work - why?
: Error in order(test, decreasing = F) : non-atomic type in greater
: > tst <- test + 0
: > order(tst,decreasing=F)#works - how come?
: [1] 3 4 2 1
: > print(tst)
: [1] "2005-02-08 18:49:15 Eastern Standard Time" "2005-02-07 18:36:54
: Eastern Standard Time"
: [3] "2005-02-04 18:37:03 Eastern Standard Time" "2005-02-06 18:29:04
: Eastern Standard Time"
: >

Because test is was produced by strptime which returns a POSIXlt value
(which is stored as a list of values) while tst is set by an addition
which returns a POSIXct value (i.e. stored as numbers) and you can sort the
numbers but not the lists.

R> class(tst)
[1] "POSIXt"  "POSIXct"
R> unclass(tst)
[1] 1107906555 1107819414 1107560223 1107732544
attr(,"tzone")
[1] ""


R> class(test)
[1] "POSIXt"  "POSIXlt"
R> unclass(test)
$sec
[1] 15 54  3  4

$min
[1] 49 36 37 29

$hour
[1] 18 18 18 18

$mday
[1] 8 7 4 6

$mon
[1] 1 1 1 1

$year
[1] 105 105 105 105

$wday
[1] 2 1 5 0

$yday
[1] 38 37 34 36

$isdst
[1] 0 0 0 0



From tstalley at yahoo.com  Thu Feb 10 04:23:18 2005
From: tstalley at yahoo.com (Theresa Talley)
Date: Wed, 9 Feb 2005 19:23:18 -0800 (PST)
Subject: [R] installing package hier.part on Mac OSX
Message-ID: <20050210032318.4508.qmail@web41123.mail.yahoo.com>

Hi-
I've been trying to install the hier.part package on
my mac (OSX 10.3.7) and it is not working for some
reason. I am downloading the package source called :
hier.part_1.0.tar.gz.  When I try to auto install from
the cran site, I get this message: 
* Installing *source* package 'hier.part' ...
** libs
/Library/Frameworks/R.framework/Resources/bin/SHLIB:
line 1: make: command not found

And when I try to install from the zip file on my
computer, I get this message:
gzip: stdin: not in gzip format
tar: Child returned status 1
tar: Error exit delayed from previous errors
Error in file(file, "r") : unable to open connection
In addition: Warning messages: 
1: Installation of package hier.part had non-zero exit
status in: install.packages(c("hier.part"), lib =
"/Library/Frameworks/R.framework/Resources/library",  
2: tar returned non-zero exit code: 512 in: untar(pkg,
tmpDir) 
3: cannot open file `hier.part_1.0.tar/DESCRIPTION' 

I've successfully installed other packages (e.g.,
vegan, cluster) so am not sure if there is something
different about this one or if Im just being dopey. 

Any insight would be appreciated.
Thanks, theresa



From dataanalytics at rediffmail.com  Thu Feb 10 05:42:46 2005
From: dataanalytics at rediffmail.com (Arin Basu)
Date: 10 Feb 2005 04:42:46 -0000
Subject: [R] Looking for tools to run case crossover analysis
Message-ID: <20050210044246.5569.qmail@webmail17.rediffmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050210/69f1c5bc/attachment.pl

From blindglobe at gmail.com  Thu Feb 10 06:44:52 2005
From: blindglobe at gmail.com (A.J. Rossini)
Date: Thu, 10 Feb 2005 06:44:52 +0100
Subject: [R] Re: Packages and Libraries
In-Reply-To: <420AB99F.90704@pdf.com>
References: <42088443.2090206@wiwi.uni-bielefeld.de>
	<16904.39235.910508.75875@stat.math.ethz.ch>
	<1abe3fa9050208043374d362df@mail.gmail.com>
	<x2ll9zyx0s.fsf@biostat.ku.dk> <420AB99F.90704@pdf.com>
Message-ID: <1abe3fa9050209214422bd4dcb@mail.gmail.com>

Hm.  Everyone else got it (the reasons for the change).

The reason is that beginners often fail to get it (package vs.
library), and one of the contributing factors is the naming approach,
which isn't clear.  Certain people are rather sensitive to mistakes of
this nature, so we end up with 2-3 messages expressing frustration
because some people (who are sometimes asking otherwise reasonable
questions) aren't detail-oriented enough to suit.

Please read the rest of the thread, where the questions you ask are
covered (though whether on r-devel or r-help, I'm not sure, since my
mail is completely threaded).


On Wed, 09 Feb 2005 17:32:15 -0800, Spencer Graves
<spencer.graves at pdf.com> wrote:
>       The reasons to 'introduce "package()" and deprecate "library()"'
> may be OBVIOUS to you, but they completely escape me.  Could you please
> clarify why that's obvious?  I've seen many admonitions on this list
> that the term is "package" NOT "library", but I don't recall ever seeing
> any explanation of why the term "package" is more appropriate than
> "library".
> 
>       I suspect there may be some rationale that "package" seems more
> appropriately descriptive.  However, is it so much more precise that it
> justifies creating a distinction between S-Plus and R?
> 
>       Or is it your intention to make it harder for people who have to
> use S-Plus to also use R?  Or do you want to make it more difficult for
> people to write code that will work in both S-Plus and R or for people
> to migrate from S-Plus to R or vice versa?  Unless that is your intent,
> I'd like to know why you don't make the complementary change, namely
> globally replace "package" with "library" everywhere in the
> documentation -- or at least deprecate its future use.
> 
>       R is a marvelous creation, a solid contribution to the advancement
> of science and through that human knowledge and eventually even the
> ability of people everywhere to live more comfortably, longer.
> 
>       spencer graves
> 
> p.s.  I'm told that the French Royal Academy delayed the introduction in
> France of a product marketed by a US company.  The product couldn't be
> sold in France without a French language manual.  The translation could
> not be published until the French Royal Academy officially provided or
> blessed new French words for new technical terms.  Similarly, Le Monde
> Diplomatique recently carried an article proposing "A polyglot world to
> escape the English dictatorship."  (January 2005, pp. 22-23:  "Un monde
> polyglotte pour echapper a la dictature de l'anglais';  someone may wish
> to correct my translation.)  We could require all r-help subscribers to
> learn enough French, German, Spanish AND Portuguese to be able to read
> posts in those languages, but that might be counterproductive.
> 
> Peter Dalgaard wrote:
> 
> >"A.J. Rossini" <blindglobe at gmail.com> writes:
> >
> >
> >
> >>For OBVIOUS reasons, is there any chance that we could introduce
> >>"package()" and deprecate "library()"?
> >>
> >>
> >
> >usepackage() or usePackage() has been suggested, but someone got
> >ambitious and wanted it to be different from library(), and it sort of
> >didn't get any further. We still have some time before feature freeze
> >for 2.1.0 though.
> >
> >
> >
> >>(well, I'll also ask if we could deprecate "=" for assignment, but
> >>that's hopeless).
> >>
> >>
> >
> >You're not *forced* to use it...
> >
> >
> >
> 
> 


-- 
best,
-tony

"Commit early,commit often, and commit in a repository from which we can easily
roll-back your mistakes" (AJR, 4Jan05).

A.J. Rossini
blindglobe at gmail.com



From ales.ziberna at guest.arnes.si  Thu Feb 10 08:25:03 2005
From: ales.ziberna at guest.arnes.si (=?iso-8859-1?Q?Ales_Ziberna?=)
Date: Thu, 10 Feb 2005 08:25:03 +0100
Subject: [R] cuttree
References: <420AAC4A.4070808@sdsc.edu>
Message-ID: <008801c50f41$d5217e00$1a09f9c2@ales>

I am not sure if I understad you correctly. Are you inrested in getting a 
list of units that belond to each group? Then do:

branches<-cutree(hc,k=2)

group1<-which(branches==1)
group2<-which(branches==2)

Hope this helps!
Ales Ziberna

----- Original Message ----- 
From: "T. Murlidharan Nair" <nair at sdsc.edu>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, February 10, 2005 1:35 AM
Subject: [R] cuttree


> How do I get the labels from a hclust object after I have cut
> the tree using cutree ?
> hc<-hclust(someData,"a")
>
> branches<-cutree(hc)
>
> If there are two branches, how do I obtain the labels associated with each 
> of the branches separately ?
> Thanks../Murli
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>



From ripley at stats.ox.ac.uk  Thu Feb 10 08:41:56 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 10 Feb 2005 07:41:56 +0000 (GMT)
Subject: [R] Re: Packages and Libraries
In-Reply-To: <420AB99F.90704@pdf.com>
References: <42088443.2090206@wiwi.uni-bielefeld.de>
	<16904.39235.910508.75875@stat.math.ethz.ch>
	<1abe3fa9050208043374d362df@mail.gmail.com>
	<x2ll9zyx0s.fsf@biostat.ku.dk> <420AB99F.90704@pdf.com>
Message-ID: <Pine.LNX.4.61.0502100730160.10625@gannet.stats>

On Wed, 9 Feb 2005, Spencer Graves wrote:

>     The reasons to 'introduce "package()" and deprecate "library()"' may be 
> OBVIOUS to you, but they completely escape me.  Could you please clarify why 
> that's obvious?  I've seen many admonitions on this list that the term is 
> "package" NOT "library", but I don't recall ever seeing any explanation of 
> why the term "package" is more appropriate than "library". 
>     I suspect there may be some rationale that "package" seems more 
> appropriately descriptive.  However, is it so much more precise that it 
> justifies creating a distinction between S-Plus and R?

Note that S(-PLUS) does not use `library' for `package', it uses `library 
section', and that is in the 1988 Blue Book, the manual for the first 
version of S which was extensible in that way.  Only a few observant 
people used `library section', and when S4 introduced `chapter' (almost 
but not quite the same thing) few people adopted that either.

There is a need to change the programmatic interface to what 
library/require do, for example to take character string (only) arguments 
and to return a suitable classed object, as well as separate out 
library(help=). This will almost certainly be done (if/when it is done) so 
that library() remains for ever as a compatibility wrapper, but the new 
interface (usePackage(), use(), whatever) becomes the preferred one.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Feb 10 08:55:06 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 10 Feb 2005 07:55:06 +0000 (GMT)
Subject: [R] question about sorting POSIXt vector
In-Reply-To: <20050210013830.8660.qmail@web50301.mail.yahoo.com>
References: <20050210013830.8660.qmail@web50301.mail.yahoo.com>
Message-ID: <Pine.LNX.4.61.0502100742230.10625@gannet.stats>

You can order (which is not the same as sort) POSIXct vectors but not 
POSIXlt ones.

class() on your objects (or str(), but that only shows one class) would 
have been revealing.

It is somewhat fortuitous that you can order and sort POSIXct vectors, as 
you are ordering the underlying numeric representation.

On Wed, 9 Feb 2005, bogdan romocea wrote:

> Dear useRs,
>
> How come the first attempt to sort a POSIXt vector fails (Error:
> non-atomic type in greater), while the second succeeds? (Code inserted
> below.) The documentation says that POSIXt is used to allow operations
> such as subtraction, so I'd expect sorting to work.

I didn't know sorting was like subtraction. What I found was

      Logical comparisons and limited arithmetic are available for both
      classes.

Try the documentation on order() and sort(), which is clear that these 
apply only to certain sorts of vectors, and for DateTimeClasses which 
tells you POSIXlt is a list and POSIXct is a numeric vector.


> Is this perhaps an
> OS issue? (I run R 2.0.1 on Win xp.)
>
> Thank you,
> b.
>
> #------------code
> test <- c("2005-02-08 18:49:15","2005-02-07 18:36:54",
> 	"2005-02-04 18:37:03","2005-02-06 18:29:04")
> test <- strptime(test,format="%Y-%m-%d %H:%M:%S")
> order(test,decreasing=F)	#doesn't work - why?
> tst <- test + 0
> order(tst,decreasing=F)	#works - how come?
> print(tst)
> #------------run
>> test <- c("2005-02-08 18:49:15","2005-02-07 18:36:54",
> + "2005-02-04 18:37:03","2005-02-06 18:29:04")
>> test <- strptime(test,format="%Y-%m-%d %H:%M:%S")
>> order(test,decreasing=F)#doesn't work - why?
> Error in order(test, decreasing = F) : non-atomic type in greater
>> tst <- test + 0
>> order(tst,decreasing=F)#works - how come?
> [1] 3 4 2 1
>> print(tst)
> [1] "2005-02-08 18:49:15 Eastern Standard Time" "2005-02-07 18:36:54
> Eastern Standard Time"
> [3] "2005-02-04 18:37:03 Eastern Standard Time" "2005-02-06 18:29:04
> Eastern Standard Time"
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Feb 10 09:01:48 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 10 Feb 2005 08:01:48 +0000 (GMT)
Subject: [R] installing package hier.part on Mac OSX
In-Reply-To: <20050210032318.4508.qmail@web41123.mail.yahoo.com>
References: <20050210032318.4508.qmail@web41123.mail.yahoo.com>
Message-ID: <Pine.LNX.4.61.0502100755480.10625@gannet.stats>

For MacOS we have

Binary packages, foo.tgz
Source packages, foo.tar.gz

Neither are `zip files' (things created by zip, usually with extension .zip).

It looks like you have not installed a source package before, and you 
either do not have the development tools installed or they are not in your 
path.

That there is no binary version of a package available usually indicates a 
problem with it on MacOS X, at least on the autobuilder's version of 
MacOS.

On Wed, 9 Feb 2005, Theresa Talley wrote:

> Hi-
> I've been trying to install the hier.part package on
> my mac (OSX 10.3.7) and it is not working for some
> reason. I am downloading the package source called :
> hier.part_1.0.tar.gz.  When I try to auto install from
> the cran site, I get this message:
> * Installing *source* package 'hier.part' ...
> ** libs
> /Library/Frameworks/R.framework/Resources/bin/SHLIB:
> line 1: make: command not found
>
> And when I try to install from the zip file on my
> computer, I get this message:

What precisely did you do here?

> gzip: stdin: not in gzip format
> tar: Child returned status 1
> tar: Error exit delayed from previous errors
> Error in file(file, "r") : unable to open connection
> In addition: Warning messages:
> 1: Installation of package hier.part had non-zero exit
> status in: install.packages(c("hier.part"), lib =
> "/Library/Frameworks/R.framework/Resources/library",
> 2: tar returned non-zero exit code: 512 in: untar(pkg,
> tmpDir)
> 3: cannot open file `hier.part_1.0.tar/DESCRIPTION'
>
> I've successfully installed other packages (e.g.,
> vegan, cluster) so am not sure if there is something
> different about this one or if I?m just being dopey.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From petr.pikal at precheza.cz  Thu Feb 10 09:10:43 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 10 Feb 2005 09:10:43 +0100
Subject: [Fwd: Re: Fw: [R] Contour plot]
In-Reply-To: <57292.127.0.0.1.1107963148.squirrel@127.0.0.1>
Message-ID: <420B2513.29710.48422B@localhost>



On 9 Feb 2005 at 16:32, dhkblaszyk at zeelandnet.nl wrote:

> Petr,
> 
> It works perfectly! But I still have a question;
> 
> I have fit the following data;
> 
> x,y,z
> 1,10,11
> 2,11,15
> 3,12,21
> 4,13,29
> 5,14,39
> 6,15,51
> 7,16,65
> 8,17,81
> 9,18,99
> 10,19,119
> 
> >dat.lm <- lm(z~I(x^2)+y, data=dat)
> >dat.lm
> 
> Call:
> lm(formula = z ~ I(x^2) + y, data = dat)
> 
> Coefficients:
> (Intercept)       I(x^2)            y
>   1.841e-14    1.000e+00    1.000e+00
> 
> How do I create the "z" matrix from dat.lm?? Without having to type
> over all the coefficients?

Maybe
expand.grid() and predict()

# new data
x<-1:10
y<-20:60

new.df<-expand.grid(x,y) #make data.frame
names(new.df)<-c("x","y") #change to propper names

contour(x,y,matrix(predict(dat.lm, new.df),10,41))

Cheers
Petr



> 
> Kind regards, Darius Blaszijk
> 
> ------------------------- Oorspronkelijk bericht
> ------------------------- Onderwerp: Re: Fw: [R] Contour plot Van:    
>   "Petr Pikal" <petr.pikal at precheza.cz> Datum:     Wo, 9 februari,
> 2005 11:23 am Aan:       dhkblaszyk at zeelandnet.nl
>            r-help at stat.math.ethz.ch
> ----------------------------------------------------------------------
> ----
> 
> Hi Darius
> 
> 
> On 8 Feb 2005 at 17:43, dhkblaszyk at zeelandnet.nl wrote:
> 
> > I understand that I need to have a (in this case) square matrix with
> > all
> the data. But the question now is;
> >
> > - can the contourplot not interpolate the missing values
> >
> > or alternatively
> >
> > - I have fit a model to the z data (z = 100 + 0.5x + 0.5y). How can
> > I
> make from this model a "square" matrix z to make a contour plot?
> 
> 
> Will
> 
> x<-1:10
> y<-1:10
> z <- outer(x,y,function(x,y) 100 + 0.5*x + 0.5*y)
> contour(x,y,z)
> 
> work as you wish?
> Cheers
> Petr
> 
> 
> >
> > Kind regards, Darius Blaszijk
> >
> > ----- Original Message -----
> > From: "Achim Zeileis" <Achim.Zeileis at wu-wien.ac.at>
> > To: <dhkblaszyk at zeelandnet.nl>
> > Cc: <r-help at stat.math.ethz.ch>
> > Sent: Tuesday, February 08, 2005 1:51 AM
> > Subject: Re: [R] Contour plot
> >
> >
> > > On Tue, 8 Feb 2005 01:15:06 +0100 dhkblaszyk at zeelandnet.nl wrote:
> > >
> > > > Hello,
> > > >
> > > > I would like to make a contourplot of the following data;
> > > >
> > > > > x <- 1:10
> > > > > y <- 1:10
> > > > > z <- 100:110
> > > >
> > > > By doing >contour(x,y,z) I get the following error;
> > > >
> > > > "Error in contour.default(x, y, z) : no proper `z' matrix
> > > > specified"
> > > >
> > > > How do I fix this??
> > >
> > > x and y specify a grid and thus z must provide a value for each
> combination of the x's and y's! For example:
> > >   x <- y <- 1:10
> > >   contour(x, y, outer(x, y))
> > > Also look at
> > >   outer(x, y)
> > > and read ?contour.
> > >
> > > Z
> > >
> > > > Kind regards, Datius Blaszijk
> > > >
> > > > [[alternative HTML version deleted]]
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide!
> > > > http://www.R-project.org/posting-guide.html
> > > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> 
> Petr Pikal
> petr.pikal at precheza.cz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From petr.pikal at precheza.cz  Thu Feb 10 09:22:52 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 10 Feb 2005 09:22:52 +0100
Subject: [R] question about sorting POSIXt vector
In-Reply-To: <20050210013830.8660.qmail@web50301.mail.yahoo.com>
Message-ID: <420B27EC.16936.536231@localhost>

Hi Bogdan

the behaviour is result of ct and lt POSIX class

try

> str(test)
`POSIXlt', format: chr [1:4] "2005-02-08 18:49:15" "2005-02-07 
18:36:54" "2005-02-04 18:37:03" "2005-02-06 18:29:04"
> str(tst)
`POSIXct', format: chr [1:4] "2005-02-08 18:49:15" "2005-02-07 
18:36:54" "2005-02-04 18:37:03" "2005-02-06 18:29:04"
>

or

> length(tst)
[1] 4
> length(test)
[1] 9
>

and read help pages about POSIX how to convert

Cheers
Petr


On 9 Feb 2005 at 17:38, bogdan romocea wrote:

> Dear useRs,
> 
> How come the first attempt to sort a POSIXt vector fails (Error:
> non-atomic type in greater), while the second succeeds? (Code inserted
> below.) The documentation says that POSIXt is used to allow operations
> such as subtraction, so I'd expect sorting to work. Is this perhaps an
> OS issue? (I run R 2.0.1 on Win xp.)
> 
> Thank you,
> b.
> 
> #------------code
> test <- c("2005-02-08 18:49:15","2005-02-07 18:36:54",
>  "2005-02-04 18:37:03","2005-02-06 18:29:04")
> test <- strptime(test,format="%Y-%m-%d %H:%M:%S")
> order(test,decreasing=F)	#doesn't work - why?
> tst <- test + 0
> order(tst,decreasing=F)	#works - how come?
> print(tst)
> #------------run
> > test <- c("2005-02-08 18:49:15","2005-02-07 18:36:54",
> + "2005-02-04 18:37:03","2005-02-06 18:29:04")
> > test <- strptime(test,format="%Y-%m-%d %H:%M:%S")
> > order(test,decreasing=F)#doesn't work - why?
> Error in order(test, decreasing = F) : non-atomic type in greater >
> tst <- test + 0 > order(tst,decreasing=F)#works - how come? [1] 3 4 2
> 1 > print(tst) [1] "2005-02-08 18:49:15 Eastern Standard Time"
> "2005-02-07 18:36:54 Eastern Standard Time" [3] "2005-02-04 18:37:03
> Eastern Standard Time" "2005-02-06 18:29:04 Eastern Standard Time" >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From csoh at hsph.harvard.edu  Thu Feb 10 09:44:35 2005
From: csoh at hsph.harvard.edu (Chang-Heok Soh)
Date: Thu, 10 Feb 2005 03:44:35 -0500 (EST)
Subject: [R] Mean calculated from R1.9.1 different from R2.0.1
Message-ID: <Pine.GSO.4.53.0502100330210.390@hsph.harvard.edu>

Hello,
I ran my simulations on the Unix verson of R1.9.1 and the Windows version
of R2.0.1 on XP.  I kept getting different values for the mean of the same
column of the same matrix, and am perplexed.  I would appreciate if anyone
could help explain the difference?

Here is a sample code:

set.seed(7293)
z1v <- rnorm(1000, mean=68, sd=13)
z1v <- (z1v-mean(z1v))/sd(z1v)

Using R1.9.1 on Unix, I get:

mean(z1v)
[1] -4.88775e-15

Using R2.0.1 on Windows XP, I get:

mean(z1v)
[1] -3.7497e-16

The same problem occurs when I dump the data from one version and
then source it using the other version.

Thanks for the help.

Regards,
Chang-Heok



From jarioksa at sun3.oulu.fi  Thu Feb 10 10:05:37 2005
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: Thu, 10 Feb 2005 11:05:37 +0200
Subject: [R] installing package hier.part on Mac OSX
In-Reply-To: <Pine.LNX.4.61.0502100755480.10625@gannet.stats>
References: <20050210032318.4508.qmail@web41123.mail.yahoo.com>
	<Pine.LNX.4.61.0502100755480.10625@gannet.stats>
Message-ID: <1108026337.25394.20.camel@biol102145.oulu.fi>

On Thu, 2005-02-10 at 08:01 +0000, Prof Brian Ripley wrote:
> For MacOS we have
> 
> Binary packages, foo.tgz
> Source packages, foo.tar.gz
> 
> Neither are `zip files' (things created by zip, usually with extension .zip).
> 
> It looks like you have not installed a source package before, and you 
> either do not have the development tools installed or they are not in your 
> path.

In this case probably the tools are missing, starting from 'make'. You
should install the Development Tools / X-Code which come with the MacOS
X installation cd/dvd at least from version 10.3.x. Otherwise you can
get the development tools from http://developer.apple.com/. Moreover,
in this case you need to get a Fortran compiler which does not come with
MacOS. See R for Mac OS X FAQ, section "the Fortran compiler g77 gcc
3.3". 

> 
> That there is no binary version of a package available usually indicates a 
> problem with it on MacOS X, at least on the autobuilder's version of 
> MacOS.
> 
Well, 'hier.part' is younger than the latest entry in Mac binary
packages: there is nothing after Jan 19, 2005. The binary package builds
beautifully in MacOS X. However, it seems to require package 'gtools'
that I can't find in CRAN nor in BioConductor repositories. It seems
that this didn't prevent passing tests to be included at CRAN or
producing Windows binaries. 

Theresa, I can send you a Mac binary if you don't want to see the
trouble of installing X-Code and g77. However, it failed with missing
'gtools' upon loading.

cheers, jari oksanen
> On Wed, 9 Feb 2005, Theresa Talley wrote:
> 
> > Hi-
> > I've been trying to install the hier.part package on
> > my mac (OSX 10.3.7) and it is not working for some
> > reason. I am downloading the package source called :
> > hier.part_1.0.tar.gz.  When I try to auto install from
> > the cran site, I get this message:
> > * Installing *source* package 'hier.part' ...
> > ** libs
> > /Library/Frameworks/R.framework/Resources/bin/SHLIB:
> > line 1: make: command not found
> >
> > And when I try to install from the zip file on my
> > computer, I get this message:
> 
> What precisely did you do here?
> 
> > gzip: stdin: not in gzip format
> > tar: Child returned status 1
> > tar: Error exit delayed from previous errors
> > Error in file(file, "r") : unable to open connection
> > In addition: Warning messages:
> > 1: Installation of package hier.part had non-zero exit
> > status in: install.packages(c("hier.part"), lib =
> > "/Library/Frameworks/R.framework/Resources/library",
> > 2: tar returned non-zero exit code: 512 in: untar(pkg,
> > tmpDir)
> > 3: cannot open file `hier.part_1.0.tar/DESCRIPTION'
> >
> > I've successfully installed other packages (e.g.,
> > vegan, cluster) so am not sure if there is something
> > different about this one or if I?m just being dopey.
> 
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> ______________________________________________ R-help at stat.math.ethz.ch mailing list https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Thu Feb 10 10:22:58 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 10 Feb 2005 09:22:58 +0000 (GMT)
Subject: [R] installing package hier.part on Mac OSX
In-Reply-To: <1108026337.25394.20.camel@biol102145.oulu.fi>
References: <20050210032318.4508.qmail@web41123.mail.yahoo.com> 
	<Pine.LNX.4.61.0502100755480.10625@gannet.stats>
	<1108026337.25394.20.camel@biol102145.oulu.fi>
Message-ID: <Pine.LNX.4.61.0502100915580.12020@gannet.stats>

On Thu, 10 Feb 2005, Jari Oksanen wrote:

> On Thu, 2005-02-10 at 08:01 +0000, Prof Brian Ripley wrote:
>> For MacOS we have
>>
>> Binary packages, foo.tgz
>> Source packages, foo.tar.gz
>>
>> Neither are `zip files' (things created by zip, usually with extension .zip).
>>
>> It looks like you have not installed a source package before, and you
>> either do not have the development tools installed or they are not in your
>> path.
>
> In this case probably the tools are missing, starting from 'make'. You
> should install the Development Tools / X-Code which come with the MacOS
> X installation cd/dvd at least from version 10.3.x. Otherwise you can
> get the development tools from http://developer.apple.com/. Moreover,
> in this case you need to get a Fortran compiler which does not come with
> MacOS. See R for Mac OS X FAQ, section "the Fortran compiler g77 gcc
> 3.3".
>
>>
>> That there is no binary version of a package available usually indicates a
>> problem with it on MacOS X, at least on the autobuilder's version of
>> MacOS.
>>
> Well, 'hier.part' is younger than the latest entry in Mac binary
> packages: there is nothing after Jan 19, 2005. The binary package builds

As hier.part's tarball is dated 3 November 2004, this is obviously false
(and I had checked).

> beautifully in MacOS X. However, it seems to require package 'gtools'
> that I can't find in CRAN nor in BioConductor repositories. It seems
> that this didn't prevent passing tests to be included at CRAN or
> producing Windows binaries.
>
> Theresa, I can send you a Mac binary if you don't want to see the
> trouble of installing X-Code and g77. However, it failed with missing
> 'gtools' upon loading.

gtools is part of gregmisc, so you should have been able to find it.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From petr.pikal at precheza.cz  Thu Feb 10 10:45:06 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 10 Feb 2005 10:45:06 +0100
Subject: [R] Mean calculated from R1.9.1 different from R2.0.1
In-Reply-To: <Pine.GSO.4.53.0502100330210.390@hsph.harvard.edu>
Message-ID: <420B3B32.3815.9EADB4@localhost>

Hi

Isn't it due to rounding and floating point precision of different 
OSses?

Cheers
Petr



On 10 Feb 2005 at 3:44, Chang-Heok Soh wrote:

> Hello,
> I ran my simulations on the Unix verson of R1.9.1 and the Windows
> version of R2.0.1 on XP.  I kept getting different values for the mean
> of the same column of the same matrix, and am perplexed.  I would
> appreciate if anyone could help explain the difference?
> 
> Here is a sample code:
> 
> set.seed(7293)
> z1v <- rnorm(1000, mean=68, sd=13)
> z1v <- (z1v-mean(z1v))/sd(z1v)
> 
> Using R1.9.1 on Unix, I get:
> 
> mean(z1v)
> [1] -4.88775e-15
> 
> Using R2.0.1 on Windows XP, I get:
> 
> mean(z1v)
> [1] -3.7497e-16
> 
> The same problem occurs when I dump the data from one version and then
> source it using the other version.
> 
> Thanks for the help.
> 
> Regards,
> Chang-Heok
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From georg.hoermann at gmx.de  Thu Feb 10 11:01:11 2005
From: georg.hoermann at gmx.de (Georg Hoermann)
Date: Thu, 10 Feb 2005 11:01:11 +0100
Subject: [R] Annual cumulative sums from time series
Message-ID: <200502101101.11275.georg.hoermann@gmx.de>

Hello world,

I am actually transferring a course in data management for
students in biology, geography and agriculture 
from statistica to R - it works
surprisingly well. If anyone is interested in my scratch/notepad
(in German language), please see

www.hydrology.uni-kiel.de/~schorsch/statistik/statistik_datenauswertung.pdf

(pages 40-52)

The dataset is:

www.hydrology.uni-kiel.de/~schorsch/statistik/erle_stat.csv

It contains a 10 year dataset. So far for introduction, now
comes the problem:

we often need cumulative *annual* sums (sunshine, precipitation),
 i.e. the sum
must reset to 0 at the beginning of the year. I know
of cumsum(), but I do not now how to split the dataset automagically 
into annual pieces so I can cumsum() every year separately.
I have the strong hope that the solution is one of these
one-liners which leave the students with eyes wide open in surprise and 
makes them true believers in the power of the command-line 8-).

Thanks & Greetings
Georg


-- 
Georg Hoermann, Luebeck, Germany
Tel. 0451/47 70 32, 0172/431 57 15, Penguin #189476



From dimitris.rizopoulos at med.kuleuven.ac.be  Thu Feb 10 11:28:37 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Thu, 10 Feb 2005 11:28:37 +0100
Subject: [R] Annual cumulative sums from time series
References: <200502101101.11275.georg.hoermann@gmx.de>
Message-ID: <003501c50f5b$47d81030$0540210a@www.domain>

maybe something like this:

dat <- data.frame(years=rep(1994:2004, each=10), x=rnorm(110), 
y=rnorm(110))
lapply(split(dat[,-1], dat$years), cumsum)

is what you want.

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Georg Hoermann" <georg.hoermann at gmx.de>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, February 10, 2005 11:01 AM
Subject: [R] Annual cumulative sums from time series


> Hello world,
>
> I am actually transferring a course in data management for
> students in biology, geography and agriculture
> from statistica to R - it works
> surprisingly well. If anyone is interested in my scratch/notepad
> (in German language), please see
>
> www.hydrology.uni-kiel.de/~schorsch/statistik/statistik_datenauswertung.pdf
>
> (pages 40-52)
>
> The dataset is:
>
> www.hydrology.uni-kiel.de/~schorsch/statistik/erle_stat.csv
>
> It contains a 10 year dataset. So far for introduction, now
> comes the problem:
>
> we often need cumulative *annual* sums (sunshine, precipitation),
> i.e. the sum
> must reset to 0 at the beginning of the year. I know
> of cumsum(), but I do not now how to split the dataset automagically
> into annual pieces so I can cumsum() every year separately.
> I have the strong hope that the solution is one of these
> one-liners which leave the students with eyes wide open in surprise 
> and
> makes them true believers in the power of the command-line 8-).
>
> Thanks & Greetings
> Georg
>
>
> -- 
> Georg Hoermann, Luebeck, Germany
> Tel. 0451/47 70 32, 0172/431 57 15, Penguin #189476
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From petr.pikal at precheza.cz  Thu Feb 10 11:37:17 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 10 Feb 2005 11:37:17 +0100
Subject: [R] Annual cumulative sums from time series
In-Reply-To: <200502101101.11275.georg.hoermann@gmx.de>
Message-ID: <420B476D.7946.189B51@localhost>

Hi Georg

If you know which value belongs to which year you can use

tapply, by, aggregate family like:

years<-rep(c(2000,2001,2002,2003), each=10)
values<-(1:40)
test<-cbind(years,values)
test<-data.frame(test)
aggregate(test$values,list(y=test$years),sum)

Cheers
Petr



On 10 Feb 2005 at 11:01, Georg Hoermann wrote:

> Hello world,
> 
> I am actually transferring a course in data management for
> students in biology, geography and agriculture 
> from statistica to R - it works
> surprisingly well. If anyone is interested in my scratch/notepad
> (in German language), please see
> 
> www.hydrology.uni-kiel.de/~schorsch/statistik/statistik_datenauswertun
> g.pdf
> 
> (pages 40-52)
> 
> The dataset is:
> 
> www.hydrology.uni-kiel.de/~schorsch/statistik/erle_stat.csv
> 
> It contains a 10 year dataset. So far for introduction, now
> comes the problem:
> 
> we often need cumulative *annual* sums (sunshine, precipitation),
>  i.e. the sum
> must reset to 0 at the beginning of the year. I know
> of cumsum(), but I do not now how to split the dataset automagically
> into annual pieces so I can cumsum() every year separately. I have the
> strong hope that the solution is one of these one-liners which leave
> the students with eyes wide open in surprise and makes them true
> believers in the power of the command-line 8-).
> 
> Thanks & Greetings
> Georg
> 
> 
> -- 
> Georg Hoermann, Luebeck, Germany
> Tel. 0451/47 70 32, 0172/431 57 15, Penguin #189476
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From slusek at o2.pl  Thu Feb 10 11:40:23 2005
From: slusek at o2.pl (Wojtek Slusarski)
Date: Thu, 10 Feb 2005 11:40:23 +0100
Subject: [R] Writing output to a file in a loop
Message-ID: <420B3A17.5070603@o2.pl>

Hello,
My problem is, that I have to build hundreds of GARCH models to obtain 
volatility forecasts. I would like to run a loop, that would build those 
forecasts for me. There is no problem, with writing only the results of 
the forecasts, but I'd like to have stored results of the models in some 
file, that I could check later, what are the models like, to be able to 
compare if I should use GARCH(1,1) or GARCH(0,4) or any other, that 
suits the data best. The data file looks like that:

TICKER;DTYYYYMMDD;OPEN;HIGH;LOW;CLOSE;VOL
WIG20;19940414;1000,00;1000,00;1000,00;1000,00;71600
WIG20;19940418;1050,47;1050,47;1050,47;1050,47;99950
WIG20;19940419;1124,92;1124,92;1124,92;1124,92;138059
...

I wrote a script to do that, but when I use sink() to save the results 
in a text file, after running the script files are empty. I read in FAQ, 
that in a loop R only computes and prints only some warning messages, 
but I don't know how to do that in any other way, to ommit this problem.
Below I enclose the code:

############################################################

library(tseries)

wig20 <- read.csv("wig20.txt", sep=";", dec=",")
m <- 2321   	#upper bound of time series
niter <- 10        #length(wig20$CLOSE)- m
fcv <- 0


for (i in 1:niter){
	
	m <- m + 1
	r <- 100*diff(log(wig20$CLOSE[1:m]))
	y <- r - mean(r)
	fit <- garch(y, order = c(1,1))

	sink("garch21.txt", append = TRUE)
		summary(fit)
		logLik(fit)
    	sink()

	cv <- predict(fit, genuine=TRUE)
	fcv <- c(fcv,cv[length(cv)/2,1])

	postscript("garch21.ps",encoding="ISOLatin2",
			title = paste("Day: ",wig20$DTYYYYMMDD[m]),
			paper = "a4",
			family = "URWTimes",
			append = TRUE)
		
		plot(fit, ask=FALSE)
		
	dev.off()
	
}

sink("forecasts.txt")

     fcv

sink()

###################################################

The most wondering thing for me is that, plots are stored in the ps 
file, but only the ones from last iteration. In manuals there is:

append: logical; currently *disregarded*; just there for
           compatibility reasons.

What does *disregarded* actually mean and when I will be able to add a 
title before each set of plots, because now the option title is not 
adding anything to file.

I will be very thankfull for any help.

Best regards,
Wojtek Slusarski



From Roger.Bivand at nhh.no  Thu Feb 10 12:02:58 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 10 Feb 2005 12:02:58 +0100 (CET)
Subject: [R] Annual cumulative sums from time series
In-Reply-To: <200502101101.11275.georg.hoermann@gmx.de>
Message-ID: <Pine.LNX.4.44.0502101135590.23088-100000@reclus.nhh.no>

On Thu, 10 Feb 2005, Georg Hoermann wrote:

> Hello world,
> 
> I am actually transferring a course in data management for
> students in biology, geography and agriculture 
> from statistica to R - it works
> surprisingly well. If anyone is interested in my scratch/notepad
> (in German language), please see
> 
> www.hydrology.uni-kiel.de/~schorsch/statistik/statistik_datenauswertung.pdf
> 
> (pages 40-52)
> 
> The dataset is:
> 
> www.hydrology.uni-kiel.de/~schorsch/statistik/erle_stat.csv
> 
> It contains a 10 year dataset. So far for introduction, now
> comes the problem:
> 
> we often need cumulative *annual* sums (sunshine, precipitation),
>  i.e. the sum
> must reset to 0 at the beginning of the year. I know
> of cumsum(), but I do not now how to split the dataset automagically 
> into annual pieces so I can cumsum() every year separately.
> I have the strong hope that the solution is one of these
> one-liners which leave the students with eyes wide open in surprise and 
> makes them true believers in the power of the command-line 8-).
> 

> kiel <- read.csv(url("http://www.hydrology.uni-kiel.de/~schorsch/statistik/erle_stat.csv"))
> yrs <- factor(strftime(strptime(as.character(kiel$DATUM), "%d.%m.%Y"), 
+ "%Y")) # this is clumsy, there are probably better ways
> tpks1 <- unlist(tapply(kiel$Sonnen, yrs, cumsum))
> str(tpks1)
 Named num [1:3652] 0.12 0.24 0.24 0.24 5.52 ...
 - attr(*, "names")= chr [1:3652] "19891" "19892" "19893" "19894" ...
> kiel$Sonnen[1:5]
[1] 0.12 0.12 0.00 0.00 5.28

I don't know if you want the name attribute on your annual cumulative 
sums, but for now they help control what's going on. This looks OK:

> try1 <- tapply(kiel$Sonnen, yrs, cumsum)
> cols <- rainbow(length(try1))
> plot(x=c(1,366), y=c(0,1200), type="n")
> for (i in 1:length(try1)) lines(1:length(try1[[i]]), try1[[i]], 
+ col=cols[i])
> legend(c(0,70), c(700,1100), names(try1), col=cols, lwd=1)

for a quick impression.


> Thanks & Greetings
> Georg
> 
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From hb at maths.lth.se  Thu Feb 10 12:03:52 2005
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Thu, 10 Feb 2005 12:03:52 +0100
Subject: [R] Histogram Bar Spacing or Border Width
In-Reply-To: <Pine.LNX.4.61.0502092145230.28649@gannet.stats>
Message-ID: <006301c50f60$3550ec20$3ef1ba51@hblaptop>

See http://www.maths.lth.se/help/R/plot.histogram/ and note the note at the
top of the page.

Henrik Bengtsson

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Prof 
> Brian Ripley
> Sent: Wednesday, February 09, 2005 10:48 PM
> To: Thomas Hopper
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Histogram Bar Spacing or Border Width
> 
> 
> A hint: hist() does no plotting.  That is done by the 
> "histogram" method 
> of plot(), which you can copy and modify to your own 
> needs/desires. Alternatively you can create myplot() and call
> 
> myplot(hist(..., plot=TRUE), ...)
> 
> On Wed, 9 Feb 2005, Thomas Hopper wrote:
> 
> > I understand the problem from a statistical perspective, 
> and you make 
> > an
> > excellent point (as I have come to expect, reading this 
> list). However, I'm 
> > thinking about it from a visual/aesthetic perspective.
> >
> > Let me try this. Plot two histograms side-by-side:
> >
> >> x <- rnorm(10)
> >> par(mfcol=c(1,2))
> >> hist(x)
> >> hist(x,col="gray",border="gray")
> >
> > Which one is easier to interpret? To my eyes, the default 
> hist() is, 
> > but
> > given the statistical meaning of a histogram, I think I 
> could argue that the 
> > gray version is a more accurate representation of the data.
> >
> > The default histogram delimits the space between subareas 
> to make the 
> > graph
> > easier to read. I'm just trying to tweak this to further 
> improve it. I don't 
> > want to separate the bars so much as control (and, I hope, 
> improve) their 
> > appearance.
> >
> > Thanks,
> >
> > Tom
> >
> > On Feb 9, 2005, at 11:06 AM, Prof Brian Ripley wrote:
> >
> >> A histogram is a density estimate (at least as defined in the 
> >> Encyclopedia
> >> of Statistics Sciences, if not in many US Universities).  
> It is an area, 
> >> not a series of unrelated bars, so it makes no sense to 
> have spaces between 
> >> the subareas.
> >> 
> >> Unfortunately, hist() will also produce barplots of counts.
> >> 
> >> On Wed, 9 Feb 2005, Thomas Hopper wrote:
> >> 
> >>> Is there any way to control the spacing between bars in a 
> histogram, 
> >>> or
> >>> change the border width (I'm assuming the hist() function, though 
> >>> alternatives are welcome)? I'm interested in changing the 
> visual spacing 
> >>> between columns in a plotted histogram.
> >>> 
> >>> The general effect I'm looking for can be accomplished in 
> barplots 
> >>> using
> >>> the "width=" parameter, but I have not been able to find 
> a way to adjust 
> >>> the apparent spacing in histograms.
> >> 
> >> -- 
> >> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> >> Professor of Applied Statistics,  
> http://www.stats.ox.ac.uk/~ripley/
> >> University of Oxford,             Tel:  +44 1865 272861 (self)
> >> 1 South Parks Road,                     +44 1865 272866 (PA)
> >> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> >> 
> >> 
> > ---
> > In cyberspace, no one can hear you scream.
> >
> >
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From p.dalgaard at biostat.ku.dk  Thu Feb 10 12:09:58 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 10 Feb 2005 12:09:58 +0100
Subject: [R] Annual cumulative sums from time series
In-Reply-To: <003501c50f5b$47d81030$0540210a@www.domain>
References: <200502101101.11275.georg.hoermann@gmx.de>
	<003501c50f5b$47d81030$0540210a@www.domain>
Message-ID: <x2ll9wacxl.fsf@biostat.ku.dk>

"Dimitris Rizopoulos" <dimitris.rizopoulos at med.kuleuven.ac.be> writes:

> maybe something like this:
> 
> dat <- data.frame(years=rep(1994:2004, each=10), x=rnorm(110),

(I thought they abolished the 10 month year after the failure of the
French revolution. :-) )

> y=rnorm(110))
> lapply(split(dat[,-1], dat$years), cumsum)

or to get a matching data frame:

 yr <- dat$years
 cum <- dat[-1]
 split(cum,yr) <- lapply(split(cum,yr),cumsum)

(or do.call("cbind",  lapply(split(dat[,-1], dat$years), cumsum)), if
you can rely on data being in group order.)


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Roger.Bivand at nhh.no  Thu Feb 10 12:25:37 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 10 Feb 2005 12:25:37 +0100 (CET)
Subject: [R] Writing output to a file in a loop
In-Reply-To: <420B3A17.5070603@o2.pl>
Message-ID: <Pine.LNX.4.44.0502101219010.23088-100000@reclus.nhh.no>

On Thu, 10 Feb 2005, Wojtek Slusarski wrote:

> Hello,
> My problem is, that I have to build hundreds of GARCH models to obtain 
> volatility forecasts. I would like to run a loop, that would build those 
> forecasts for me. There is no problem, with writing only the results of 
> the forecasts, but I'd like to have stored results of the models in some 
> file, that I could check later, what are the models like, to be able to 
> compare if I should use GARCH(1,1) or GARCH(0,4) or any other, that 
> suits the data best. The data file looks like that:
> 
> TICKER;DTYYYYMMDD;OPEN;HIGH;LOW;CLOSE;VOL
> WIG20;19940414;1000,00;1000,00;1000,00;1000,00;71600
> WIG20;19940418;1050,47;1050,47;1050,47;1050,47;99950
> WIG20;19940419;1124,92;1124,92;1124,92;1124,92;138059
> ...
> 
> I wrote a script to do that, but when I use sink() to save the results 
> in a text file, after running the script files are empty. I read in FAQ, 
> that in a loop R only computes and prints only some warning messages, 
> but I don't know how to do that in any other way, to ommit this problem.
> Below I enclose the code:
> 
> ############################################################
> 
> library(tseries)
> 
> wig20 <- read.csv("wig20.txt", sep=";", dec=",")
> m <- 2321   	#upper bound of time series
> niter <- 10        #length(wig20$CLOSE)- m
> fcv <- 0
> 
> 
> for (i in 1:niter){
> 	
> 	m <- m + 1
> 	r <- 100*diff(log(wig20$CLOSE[1:m]))
> 	y <- r - mean(r)
> 	fit <- garch(y, order = c(1,1))
> 
> 	sink("garch21.txt", append = TRUE)
> 		summary(fit)
> 		logLik(fit)
>     	sink()
> 
> 	cv <- predict(fit, genuine=TRUE)
> 	fcv <- c(fcv,cv[length(cv)/2,1])
> 
> 	postscript("garch21.ps",encoding="ISOLatin2",
> 			title = paste("Day: ",wig20$DTYYYYMMDD[m]),
> 			paper = "a4",
> 			family = "URWTimes",
> 			append = TRUE)
> 		
> 		plot(fit, ask=FALSE)
> 		
> 	dev.off()
> 	
> }
> 
> sink("forecasts.txt")
> 
>      fcv
> 
> sink()
> 
> ###################################################

First, you should enclose the functions that output results in the loop in 
print(), so that they do print, in loops this has to be done and is a FAQ 
(often met with lattice graphics)

> 
> The most wondering thing for me is that, plots are stored in the ps 
> file, but only the ones from last iteration. In manuals there is:
> 
> append: logical; currently *disregarded*; just there for
>            compatibility reasons.
> 
> What does *disregarded* actually mean and when I will be able to add a 
> title before each set of plots, because now the option title is not 
> adding anything to file.
> 
Well disregarded here seems to mean exactly that. Why not open the 
postscript device before the loop starts, with onefile = TRUE, write to it 
in the loop, then dev.off() after the loop? You are getting the last 
graphic because that is what you are telling it to do, the others are 
being overwritten, because you are opening and closing the same file 
inside the loop, and append doesn't exist. 


> I will be very thankfull for any help.
> 
> Best regards,
> Wojtek Slusarski
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From iwhite at staffmail.ed.ac.uk  Thu Feb 10 13:23:03 2005
From: iwhite at staffmail.ed.ac.uk (I M S White)
Date: Thu, 10 Feb 2005 12:23:03 +0000 (GMT)
Subject: [R] Failure of update.packages()
Message-ID: <Pine.GSO.4.58.0502101214080.24015@holyrood.ed.ac.uk>

Can anyone explain why with latest version of R (2.0.1) on FC3, installed
from R-2.0.1-0.fdr.2.fc3.i386.rpm, update.packages() produces the message

/usr/lib/R/bin/Rcmd exec: INSTALL: not found.

Indeed /usr/lib/R/bin seems to lack various shell scripts (INSTALL,
REMOVE, etc).

======================================
I.White
University of Edinburgh
Ashworth Laboratories, West Mains Road
Edinburgh EH9 3JT
Tel: 0131 650 5490  Fax: 0131 650 6564
E-mail: iwhite at staffmail.ed.ac.uk



From p.campbell at econ.bbk.ac.uk  Thu Feb 10 13:28:00 2005
From: p.campbell at econ.bbk.ac.uk (Campbell)
Date: Thu, 10 Feb 2005 12:28:00 +0000
Subject: [R] Mean calculated from R1.9.1 different from R2.0.1
Message-ID: <s20b5364.077@markets.econ.bbk.ac.uk>

This may or may not be related, but looking at the random number
generators in Numerical Recipies In C book it would appear that random
number generators with identical seeds will give different results 
depending upon whther they are compliled with 64 bit or 32 bit 
settings.


Is this the case with the random number generators in R?

Phineas Campbell


 

>>> Chang-Heok Soh <csoh at hsph.harvard.edu> 02/10/05 8:44 AM >>>
Hello,
I ran my simulations on the Unix verson of R1.9.1 and the Windows
version
of R2.0.1 on XP.  I kept getting different values for the mean of the
same
column of the same matrix, and am perplexed.  I would appreciate if
anyone
could help explain the difference?

Here is a sample code:

set.seed(7293)
z1v <- rnorm(1000, mean=68, sd=13)
z1v <- (z1v-mean(z1v))/sd(z1v)

Using R1.9.1 on Unix, I get:

mean(z1v)
[1] -4.88775e-15

Using R2.0.1 on Windows XP, I get:

mean(z1v)
[1] -3.7497e-16

The same problem occurs when I dump the data from one version and
then source it using the other version.

Thanks for the help.

Regards,
Chang-Heok

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From slusek at o2.pl  Thu Feb 10 13:50:37 2005
From: slusek at o2.pl (Wojtek Slusarski)
Date: Thu, 10 Feb 2005 13:50:37 +0100
Subject: [R] Writing output to a file in a loop
In-Reply-To: <Pine.LNX.4.44.0502101219010.23088-100000@reclus.nhh.no>
References: <Pine.LNX.4.44.0502101219010.23088-100000@reclus.nhh.no>
Message-ID: <420B589D.3020502@o2.pl>

Roger Bivand wrote:

> First, you should enclose the functions that output results in the loop in 
> print(), so that they do print, in loops this has to be done and is a FAQ 
> (often met with lattice graphics)

I am sorry for that, that I didn't find that. Now it works great. If 
anyone needs that, that's what I have inside the loop:

sink("zz.txt", append = TRUE)

	print(out <- summary(fit))
	print(out <- logLik(fit))

sink()

>>The most wondering thing for me is that, plots are stored in the ps 
>>file, but only the ones from last iteration. In manuals there is:
>>
>>append: logical; currently *disregarded*; just there for
>>           compatibility reasons.
>>
>>What does *disregarded* actually mean and when I will be able to add a 
>>title before each set of plots, because now the option title is not 
>>adding anything to file.
>>
> 
> Well disregarded here seems to mean exactly that. Why not open the 
> postscript device before the loop starts, with onefile = TRUE, write to it 
> in the loop, then dev.off() after the loop? You are getting the last 
> graphic because that is what you are telling it to do, the others are 
> being overwritten, because you are opening and closing the same file 
> inside the loop, and append doesn't exist. 

Well, I don't know how to change the properties main, xlab and ylab in 
the GARCH object for plots, when I do plot(fit), so I made for each 
model separate file containing name of the model and date. Those plots 
are just for me, to have a look if the residuals from the model are OK. 
I am not going to print them.

Once again, thanks a lot.

Best regards,
Wojtek Slusarski



From p.dalgaard at biostat.ku.dk  Thu Feb 10 13:52:41 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 10 Feb 2005 13:52:41 +0100
Subject: [R] Failure of update.packages()
In-Reply-To: <Pine.GSO.4.58.0502101214080.24015@holyrood.ed.ac.uk>
References: <Pine.GSO.4.58.0502101214080.24015@holyrood.ed.ac.uk>
Message-ID: <x28y5wa86e.fsf@biostat.ku.dk>

I M S White <iwhite at staffmail.ed.ac.uk> writes:

> Can anyone explain why with latest version of R (2.0.1) on FC3, installed
> from R-2.0.1-0.fdr.2.fc3.i386.rpm, update.packages() produces the message
> 
> /usr/lib/R/bin/Rcmd exec: INSTALL: not found.
> 
> Indeed /usr/lib/R/bin seems to lack various shell scripts (INSTALL,
> REMOVE, etc).
> 
> ======================================
> I.White
> University of Edinburgh
> Ashworth Laboratories, West Mains Road
> Edinburgh EH9 3JT
> Tel: 0131 650 5490  Fax: 0131 650 6564
> E-mail: iwhite at staffmail.ed.ac.uk

You need to install the R-devel package too:

R-devel-2.0.1-0.fdr.2.fc3.i386.rpm 

The big idea is that this will suck in all the required compilers,
libraries, and include files via RPM dependencies, but users with
limited disk space may be content with the binaries of R+recommended
packages. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From maarranz at tol-project.org  Thu Feb 10 15:04:50 2005
From: maarranz at tol-project.org (Miguel A. Arranz)
Date: Thu, 10 Feb 2005 15:04:50 +0100
Subject: [R] Writing output to a file in a loop
In-Reply-To: <420B3A17.5070603@o2.pl>
References: <420B3A17.5070603@o2.pl>
Message-ID: <200502101504.50699.maarranz@tol-project.org>

That is correct, you have to specify with print or cat what you want to sink. 
This should work for you:
print(summary(fit))
print(logLik(fit))
print(fcv)

I hope this helps.


On Thursday 10 February 2005 11:40, Wojtek Slusarski wrote:
> Hello,
> My problem is, that I have to build hundreds of GARCH models to obtain
> volatility forecasts. I would like to run a loop, that would build those
> forecasts for me. There is no problem, with writing only the results of
> the forecasts, but I'd like to have stored results of the models in some
> file, that I could check later, what are the models like, to be able to
> compare if I should use GARCH(1,1) or GARCH(0,4) or any other, that
> suits the data best. The data file looks like that:
>
> TICKER;DTYYYYMMDD;OPEN;HIGH;LOW;CLOSE;VOL
> WIG20;19940414;1000,00;1000,00;1000,00;1000,00;71600
> WIG20;19940418;1050,47;1050,47;1050,47;1050,47;99950
> WIG20;19940419;1124,92;1124,92;1124,92;1124,92;138059
> ...
>
> I wrote a script to do that, but when I use sink() to save the results
> in a text file, after running the script files are empty. I read in FAQ,
> that in a loop R only computes and prints only some warning messages,
> but I don't know how to do that in any other way, to ommit this problem.
> Below I enclose the code:
>
> ############################################################
>
> library(tseries)
>
> wig20 <- read.csv("wig20.txt", sep=";", dec=",")
> m <- 2321    #upper bound of time series
> niter <- 10        #length(wig20$CLOSE)- m
> fcv <- 0
>
>
> for (i in 1:niter){
>
>  m <- m + 1
>  r <- 100*diff(log(wig20$CLOSE[1:m]))
>  y <- r - mean(r)
>  fit <- garch(y, order = c(1,1))
>
>  sink("garch21.txt", append = TRUE)
>   summary(fit)
>   logLik(fit)
>      sink()
>
>  cv <- predict(fit, genuine=TRUE)
>  fcv <- c(fcv,cv[length(cv)/2,1])
>
>  postscript("garch21.ps",encoding="ISOLatin2",
>    title = paste("Day: ",wig20$DTYYYYMMDD[m]),
>    paper = "a4",
>    family = "URWTimes",
>    append = TRUE)
>
>   plot(fit, ask=FALSE)
>
>  dev.off()
>
> }
>
> sink("forecasts.txt")
>
>      fcv
>
> sink()
>
> ###################################################
>
> The most wondering thing for me is that, plots are stored in the ps
> file, but only the ones from last iteration. In manuals there is:
>
> append: logical; currently *disregarded*; just there for
>            compatibility reasons.
>
> What does *disregarded* actually mean and when I will be able to add a
> title before each set of plots, because now the option title is not
> adding anything to file.
>
> I will be very thankfull for any help.
>
> Best regards,
> Wojtek Slusarski
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Thu Feb 10 14:13:53 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 10 Feb 2005 13:13:53 +0000 (GMT)
Subject: [R] Mean calculated from R1.9.1 different from R2.0.1
In-Reply-To: <s20b5364.077@markets.econ.bbk.ac.uk>
References: <s20b5364.077@markets.econ.bbk.ac.uk>
Message-ID: <Pine.LNX.4.61.0502101302450.25556@gannet.stats>

On Thu, 10 Feb 2005, Campbell wrote:

> This may or may not be related,

It is completely unrelated: see earlier replies in this thread.

> but looking at the random number
> generators in Numerical Recipies In C book it would appear that random
> number generators with identical seeds will give different results
> depending upon whther they are compliled with 64 bit or 32 bit
> settings.
>
>
> Is this the case with the random number generators in R?

No, as R cannot be `compliled with 64 bit or 32 bit settings', whatever 
that is supposed to mean.

If your concern is with 32- vs 64- bit ints, we do ensure that R is 
compiled with 32-bit ints, and we do check the operation of the basic 
random number generators.  On the other hand, all known 64-bit ports of R 
use IEC60559 doubles (8 bytes, effective 53-bit mantissa), as do all 
current 32-bit ports that I am aware of.

At least on machines with standard IEC60559 arithmetic you will get the 
same stream of random numbers on every implementation.  Now GCC does not
support fully IEC60559 on ix86 and some other chips, but lots of testing 
has shown that to be close enough to be true for practical purposes.
Lots of R tests do use specific starting seeds and do give the same 
answers of many different architectures up to rounding error (the issue 
here).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jarioksa at sun3.oulu.fi  Thu Feb 10 14:33:49 2005
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: Thu, 10 Feb 2005 15:33:49 +0200
Subject: [R] Failure of update.packages()
In-Reply-To: <x28y5wa86e.fsf@biostat.ku.dk>
References: <Pine.GSO.4.58.0502101214080.24015@holyrood.ed.ac.uk>
	<x28y5wa86e.fsf@biostat.ku.dk>
Message-ID: <1108042429.29936.7.camel@biol102145.oulu.fi>

On Thu, 2005-02-10 at 13:52 +0100, Peter Dalgaard wrote:
> I M S White <iwhite at staffmail.ed.ac.uk> writes:
> 
> > Can anyone explain why with latest version of R (2.0.1) on FC3, installed
> > from R-2.0.1-0.fdr.2.fc3.i386.rpm, update.packages() produces the message
> > 
> > /usr/lib/R/bin/Rcmd exec: INSTALL: not found.
> > 
> > Indeed /usr/lib/R/bin seems to lack various shell scripts (INSTALL,
> > REMOVE, etc).

> You need to install the R-devel package too:
> 1
> R-devel-2.0.1-0.fdr.2.fc3.i386.rpm 
> 
> The big idea is that this will suck in all the required compilers,
> libraries, and include files via RPM dependencies, but users with
> limited disk space may be content with the binaries of R+recommended
> packages. 
> 
This kind of problems were to be anticipated, weren't they? The great
divide between use-only and devel packages is a rpm packaging standard,
but not very useful in this case: it splits a 568K devel chip from a
15.4M chunk of base R. Moreover, you don't have a repository of binary
packages for Linux which means that not many people can use the 568K
saving in download times (saving in disk space is more considerable of
course). So are there plans for binary Linux packages for other distros
than Debian so that people could use the non-devel piece of R only?

cheers, jari oksanen
-- 
Jari Oksanen <jarioksa at sun3.oulu.fi>



From michael.watson at bbsrc.ac.uk  Thu Feb 10 14:35:56 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Thu, 10 Feb 2005 13:35:56 -0000
Subject: [R] Using a number as a name to access a list
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950121BA1B@iahce2knas1.iah.bbsrc.reserved>

Hi

Dumb question time again, for which I apologise.

I have a variable that contains the following numerical text "04010".
This is the name to access a list:

> as.list(KEGGPATHID2NAME)$"04010"
[1] "MAPK signaling pathway"

Marvellous!  Except I want to do that when "04010" is assigned to a
variable called path and I can't figure out how to do it!

> path <- "04010"
>
> # the original and best
> as.list(KEGGPATHID2NAME)$"04010"
[1] "MAPK signaling pathway"
>
> # clearly this doesn't, and shouldn't, work
> as.list(KEGGPATHID2NAME)$path
NULL
>
> # this produces a string...
> eval(paste("as.list(KEGGPATHID2NAME)$",path,sep=''))
[1] "as.list(KEGGPATHID2NAME)$04010"
>
> # as does this
> eval(paste('as.list(KEGGPATHID2NAME)$"',path,'"',sep=''))
[1] "as.list(KEGGPATHID2NAME)$\"04010\""
>

Whats really annoying is that when everyone mails me the answer, I'm
going to have known how obvious it is.... Thanks in advance

Mick
Village Idiot



From michael.watson at bbsrc.ac.uk  Thu Feb 10 14:40:33 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Thu, 10 Feb 2005 13:40:33 -0000
Subject: [R] RE: Using a number as a name to access a list
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950172CE55@iahce2knas1.iah.bbsrc.reserved>

The answer of course is parse()!

Thank you and good night!
M

-----Original Message-----
From: michael watson (IAH-C) 
Sent: 10 February 2005 13:36
To: r-help at stat.math.ethz.ch
Subject: Using a number as a name to access a list


Hi

Dumb question time again, for which I apologise.

I have a variable that contains the following numerical text "04010".
This is the name to access a list:

> as.list(KEGGPATHID2NAME)$"04010"
[1] "MAPK signaling pathway"

Marvellous!  Except I want to do that when "04010" is assigned to a
variable called path and I can't figure out how to do it!

> path <- "04010"
>
> # the original and best
> as.list(KEGGPATHID2NAME)$"04010"
[1] "MAPK signaling pathway"
>
> # clearly this doesn't, and shouldn't, work 
> as.list(KEGGPATHID2NAME)$path
NULL
>
> # this produces a string...
> eval(paste("as.list(KEGGPATHID2NAME)$",path,sep=''))
[1] "as.list(KEGGPATHID2NAME)$04010"
>
> # as does this
> eval(paste('as.list(KEGGPATHID2NAME)$"',path,'"',sep=''))
[1] "as.list(KEGGPATHID2NAME)$\"04010\""
>

Whats really annoying is that when everyone mails me the answer, I'm
going to have known how obvious it is.... Thanks in advance

Mick
Village Idiot



From plummer at iarc.fr  Thu Feb 10 14:55:51 2005
From: plummer at iarc.fr (plummer@iarc.fr)
Date: Thu, 10 Feb 2005 14:55:51 +0100
Subject: [R] Failure of update.packages()
In-Reply-To: <1108042429.29936.7.camel@biol102145.oulu.fi>
References: <Pine.GSO.4.58.0502101214080.24015@holyrood.ed.ac.uk>
	<x28y5wa86e.fsf@biostat.ku.dk>
	<1108042429.29936.7.camel@biol102145.oulu.fi>
Message-ID: <1108043751.420b67e79d808@postie.iarc.fr>

Quoting Jari Oksanen <jarioksa at sun3.oulu.fi>:

> On Thu, 2005-02-10 at 13:52 +0100, Peter Dalgaard wrote:
> > I M S White <iwhite at staffmail.ed.ac.uk> writes:
> >
> > > Can anyone explain why with latest version of R (2.0.1) on FC3, installed
> > > from R-2.0.1-0.fdr.2.fc3.i386.rpm, update.packages() produces the message
> > >
> > > /usr/lib/R/bin/Rcmd exec: INSTALL: not found.
> > >
> > > Indeed /usr/lib/R/bin seems to lack various shell scripts (INSTALL,
> > > REMOVE, etc).
>
> > You need to install the R-devel package too:
> > 1
> > R-devel-2.0.1-0.fdr.2.fc3.i386.rpm
> >
> > The big idea is that this will suck in all the required compilers,
> > libraries, and include files via RPM dependencies, but users with
> > limited disk space may be content with the binaries of R+recommended
> > packages.
> >
> This kind of problems were to be anticipated, weren't they? The great
> divide between use-only and devel packages is a rpm packaging standard,
> but not very useful in this case: it splits a 568K devel chip from a
> 15.4M chunk of base R. Moreover, you don't have a repository of binary
> packages for Linux which means that not many people can use the 568K
> saving in download times (saving in disk space is more considerable of
> course). So are there plans for binary Linux packages for other distros
> than Debian so that people could use the non-devel piece of R only?
>
> cheers, jari oksanen

The splitting is an experiment (and I said so when I announced it).
It does have unforseen consequences, like implicating me in maintaining a
repository of binary RPMs for CRAN packages, which I'm not particularly keen
on.

So I shall probably revert to a single RPM, and force the installation
requirements to be the same as the build requirements.  This was, in fact,
Peter's suggestion which shows that not everybody is as short-sighted as me.

Martyn



From dimitris.rizopoulos at med.kuleuven.ac.be  Thu Feb 10 15:03:03 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Thu, 10 Feb 2005 15:03:03 +0100
Subject: [R] Using a number as a name to access a list
References: <8975119BCD0AC5419D61A9CF1A923E950121BA1B@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <002f01c50f79$3d125c00$0540210a@www.domain>

lists are like vectors, i.e.,

x <- c("a" = 1, "b" = 2)
path <- "b"
x[path]

the same applies to lists:

lis <- list(a = 1:3)
path <- "a"
lis[path]

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "michael watson (IAH-C)" <michael.watson at bbsrc.ac.uk>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, February 10, 2005 2:35 PM
Subject: [R] Using a number as a name to access a list


> Hi
>
> Dumb question time again, for which I apologise.
>
> I have a variable that contains the following numerical text 
> "04010".
> This is the name to access a list:
>
>> as.list(KEGGPATHID2NAME)$"04010"
> [1] "MAPK signaling pathway"
>
> Marvellous!  Except I want to do that when "04010" is assigned to a
> variable called path and I can't figure out how to do it!
>
>> path <- "04010"
>>
>> # the original and best
>> as.list(KEGGPATHID2NAME)$"04010"
> [1] "MAPK signaling pathway"
>>
>> # clearly this doesn't, and shouldn't, work
>> as.list(KEGGPATHID2NAME)$path
> NULL
>>
>> # this produces a string...
>> eval(paste("as.list(KEGGPATHID2NAME)$",path,sep=''))
> [1] "as.list(KEGGPATHID2NAME)$04010"
>>
>> # as does this
>> eval(paste('as.list(KEGGPATHID2NAME)$"',path,'"',sep=''))
> [1] "as.list(KEGGPATHID2NAME)$\"04010\""
>>
>
> Whats really annoying is that when everyone mails me the answer, I'm
> going to have known how obvious it is.... Thanks in advance
>
> Mick
> Village Idiot
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From jtk at cmp.uea.ac.uk  Thu Feb 10 16:02:08 2005
From: jtk at cmp.uea.ac.uk (Jan T. Kim)
Date: Thu, 10 Feb 2005 15:02:08 +0000
Subject: [R] Using a number as a name to access a list
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950121BA1B@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E950121BA1B@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <20050210150208.GF25257@jtkpc.cmp.uea.ac.uk>

On Thu, Feb 10, 2005 at 01:35:56PM -0000, michael watson (IAH-C) wrote:

> I have a variable that contains the following numerical text "04010".
> This is the name to access a list:
> 
> > as.list(KEGGPATHID2NAME)$"04010"
> [1] "MAPK signaling pathway"
> 
> Marvellous!  Except I want to do that when "04010" is assigned to a
> variable called path and I can't figure out how to do it!
> 
> > path <- "04010"
> >
> > # the original and best
> > as.list(KEGGPATHID2NAME)$"04010"
> [1] "MAPK signaling pathway"
> >
> > # clearly this doesn't, and shouldn't, work
> > as.list(KEGGPATHID2NAME)$path
> NULL

$ allows only a literal character string or a symbol as the index, according
to the R language definition, but [[ indexing does the same as $ and can
be used for computed indexing. So

    as.list(KEGGPATHID2NAME)[[path]]

should do what you want.

Greetinx, Jan
-- 
 +- Jan T. Kim -------------------------------------------------------+
 |    *NEW*    email: jtk at cmp.uea.ac.uk                               |
 |    *NEW*    WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
 *-----=<  hierarchical systems are for files, not for humans  >=-----*



From martinol at ensam.inra.fr  Thu Feb 10 20:25:35 2005
From: martinol at ensam.inra.fr (Martin Olivier)
Date: Thu, 10 Feb 2005 20:25:35 +0100
Subject: [R] algorithms for matching and Hungarian method 
Message-ID: <420BB52F.3010702@ensam.inra.fr>

Hi all,

I would like to match two partitions. That is, if I have exactly the 
same objects grouped
together for the two partitions, the labels may be arbitrarly permuted. 
and so, i would like
to know the correspondances of the groups between the two clusterings.

In the e1701 pachage, it is possible to use the function  matchClasses() 
for this problem. The problem is that
for k greater than 10 (k number of classes), I have a memory problem. So 
I would like to know
if this function explicitly examine all k! possible matches, or if it 
uses the Hungarian
method (or an other optimal algorithm).  If not, do you know if I can 
find one.

Best regards



From Christoph.Scherber at uni-jena.de  Thu Feb 10 16:34:59 2005
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Thu, 10 Feb 2005 16:34:59 +0100
Subject: [R] xyplot() question
Message-ID: <420B7F23.5000509@uni-jena.de>

Dear R Users,

I have an xyplot() where different plotting symbols are used for 
subgroups (originally used within S-Plus, but hopefully it?s also 
applicable to R users).
 How can I fit separate regression lines for every subgroup? So far, I 
can only plot the overall fitted line.

The code looks like this:

trellis.device()
sps<-trellis.par.get("superpose.symbol")
sps$pch<-1:7
trellis.par.set("superpose.symbol",sps)

spl <- trellis.par.get("superpose.line")  
ps$lty <- 1:7 
trellis.par.set("superpose.line",spl)         
xyplot(a~b|factor+treatment,
groups=external,data=ownframe,layout=c(2,2),
panel=function(x,y,subscripts,...){panel.superpose(x,y,subscripts,...);panel.lmline(x,y)}) 


Thank you very much for your help!

Regards
Christioph



From deepayan at stat.wisc.edu  Thu Feb 10 16:56:46 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 10 Feb 2005 09:56:46 -0600
Subject: [R] xyplot() question
In-Reply-To: <420B7F23.5000509@uni-jena.de>
References: <420B7F23.5000509@uni-jena.de>
Message-ID: <200502100956.46848.deepayan@stat.wisc.edu>

On Thursday 10 February 2005 09:34, Christoph Scherber wrote:
> Dear R Users,
>
> I have an xyplot() where different plotting symbols are used for
> subgroups (originally used within S-Plus, but hopefully it?s also
> applicable to R users).
>  How can I fit separate regression lines for every subgroup? So far,
> I can only plot the overall fitted line.
>
> The code looks like this:
>
> trellis.device()
> sps<-trellis.par.get("superpose.symbol")
> sps$pch<-1:7
> trellis.par.set("superpose.symbol",sps)
>
> spl <- trellis.par.get("superpose.line")
> ps$lty <- 1:7
> trellis.par.set("superpose.line",spl)
> xyplot(a~b|factor+treatment,
> groups=external,data=ownframe,layout=c(2,2),
> panel=function(x,y,subscripts,...){panel.superpose(x,y,subscripts,...
>);panel.lmline(x,y)})

xyplot(a~b|factor+treatment,
       groups=external,data=ownframe,layout=c(2,2),
       panel.groups=function(x,y,...){ 
           panel.xyplot(x,y,...)
           panel.lmline(x,y,...)
       }) 

or

xyplot(a~b|factor+treatment,
       groups=external,data=ownframe,layout=c(2,2),
       type = c('p', 'r'))

Deepayan



From macq at llnl.gov  Thu Feb 10 17:09:52 2005
From: macq at llnl.gov (Don MacQueen)
Date: Thu, 10 Feb 2005 08:09:52 -0800
Subject: [R] install RMySQL
In-Reply-To: <1107961813.9331.3.camel@gregor.bsd.uchicago.edu>
References: <1107804348.4305.6.camel@gregor.bsd.uchicago.edu>
	<4207C4F9.9080109@jhsph.edu>
	<1107805973.4305.11.camel@gregor.bsd.uchicago.edu>
	<1107961813.9331.3.camel@gregor.bsd.uchicago.edu>
Message-ID: <p0611040cbe3131d5a8de@[128.115.153.6]>

I have a working installation of RMySQL in Mac OS X (10.3.7). I have 
libz in /usr/lib, as you do.
None of my personal installation notes mention doing anything special 
for /usr/lib/libz*.
My R was built from a source distribution; it is not a binary 
distribution. (version info below)

The only extra step I do is to define environment variables 
PKG_CPPFLAGS and PKG_LIBS,
like this:

mysql.home <- '/usr/local/mysql'
Sys.putenv('PKG_CPPFLAGS'=paste('-I',file.path(mysql.home,'include'),sep=''))
Sys.putenv('PKG_LIBS'=paste('-L',file.path(mysql.home,'lib'),' 
-lmysqlclient',sep=''))
install.packages('RMySQL')

You could try adding /usr/lib to PKG_LIBS.

-Don


>  version
          _                       
platform powerpc-apple-darwin6.8.5
arch     powerpc                 
os       darwin6.8.5             
system   powerpc, darwin6.8.5    
status                           
major    2                       
minor    0.1                     
year     2004                    
month    11                      
day      15                      
language R                       


RMySQL is 0.5-5


At 9:10 AM -0600 2/9/05, Xander  Meadow wrote:
>Hi,
>
>I'm still trying to get RMySQL installed on my Mac.  When I try and
>install from R (version 2.0.1) I'm told that the library "libz" can't be
>located.  It suggests checking in /usr/lib to see if I have the
>necessary libraries.  However, if I check /usr/lib I find I have the
>following:
>
>libz.1.1.3.dylib*
>libz.1.dylib*
>libz.dylib@
>
>Since I have the libraries that RMySQL needs to install I can't figure
>out why the installation is failing.  Does anyone have any thoughts on
>how to get RMySQL installed on my Mac?  Thanks so much.
>
>>Xander
>
>On Mon, 2005-02-07 at 13:52, Xander Meadow wrote:
>>  Hi,
>>
>>  Thank you for responding.  One thing I forgot to mention (although I'm
>>  guessing you figured it out anyway) is that I'm running Mac OS X.
>>
>>  I checked and I've got the file /usr/include/zlib.h installed on my
>>  machine.  Is this enough for R or does it need a more robust
>>  installation of zlib?  If this is enough, how do I let R know about the
>>  zlib.h file. If it's not, where should I get the correct zlib from?
>>  Thank you for any and all responses.
>>
>>  -Xander
>>
>>  On Mon, 2005-02-07 at 13:43, Roger D. Peng wrote:
>>  > R comes with it's own copy of zlib so even if it is compiled into R,
>>  > there isn't necessarily a system-wide installation of the library.
>>  > You may still need to install it.
>>  >
>>  > -roger
>>  >
>>  > Xander Meadow wrote:
>>  > > Hi,
>>  > >
>>  > > I've got a Dual G5 running 10.3.7 and I'm trying to install RMySQL.
>>  > > I've already got R up and running.  When I try the command
>>  > >
>>  > >
>>  > >>install.packages("RMySQL")
>>  > >
>>  > >
>>  > > It downloads a few things and then produces the following error:
>>  > > ---------------------------
>>  > > Configuration error:
>>  > >    Could not locate the library "libz" required by MySQL.
>>  > > 
>>  > > INSTRUCTIONS:
>>  > > 
>>  > >    The "libz" library is required by the MySQL client library
>>  > >    in order to compress/uncompress connections between clients
>>  > >    and the MySQL engine.
>>  > > 
>>  > >    Make sure you have "libz" installed properly and/or included
>>  > >    in your $LD_LIBRARY_PATH.  Perhaps it is not in any of the
>>  > >    standard directories (e.g., /usr/lib/, /usr/local/lib)?
>>  > > 
>>  > > Aborting the installation of RMySQL.
>>  > > 
>>  > > ERROR: configuration failed for package 'RMySQL'
>>  > > ----------------------------
>>  > >
>>  > > However, if I check for libz I get:
>>  > >
>>  > >
>>  > >>capabilities("libz")
>>  > >
>>  > > libz
>>  > > TRUE
>>  > >
>>  > > I'm not sure what the problem is because R is telling me that it doesn't
>>  > > know where libz is, but then it's also telling me it does know where
>>  > > libz is.
>>  > >
>>  > > Has anyone else seen this problem or know how I can get RMySQL installed
>>  > > on my machine?
>>  > >
>>  > > Just as an added note I know that install.packages works because
>  > > >
>>  > >
>>  > >>install.packages("DBI")
>>  > >
>>  > >
>>  > > worked without a problem.
>>  > >
>>  > > Thanks again.
>>  > >
>>  > > -Xander
>>  > >
>>  > > ______________________________________________
>>  > > R-help at stat.math.ethz.ch mailing list
>>  > > https://stat.ethz.ch/mailman/listinfo/r-help
>>  > > PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>  > >
>>
>>  ______________________________________________
>>  R-help at stat.math.ethz.ch mailing list
>>  https://stat.ethz.ch/mailman/listinfo/r-help
>>  PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From David.Brahm at geodecapital.com  Thu Feb 10 17:20:19 2005
From: David.Brahm at geodecapital.com (Brahm, David)
Date: Thu, 10 Feb 2005 11:20:19 -0500
Subject: [R] Annual cumulative sums from time series
Message-ID: <6AF7541D27821A4BAB515245C1A2FEED03776B8F@MSGBOSCLC2WIN.DMN1.FMR.COM>

Georg Hoermann [mailto:georg.hoermann at gmx.de] wrote:
> www.hydrology.uni-kiel.de/~schorsch/statistik/erle_stat.csv ...
contains a 10 year dataset.
> We often need cumulative *annual* sums (sunshine, precipitation), i.e.
the sum must reset to 0
> at the beginning of the year. I know of cumsum(), but I do not now how
to split the dataset
> automagically into annual pieces so I can cumsum() every year
separately.

Several replies have been given using tapply/aggregate/split; here is a
more primitive approach.
I do the cumsum once for the series, and call it "x".  Then I subtract
from x the value of x on 12/31
of the prior year:

R> y <- read.csv("erle_stat.csv", as.is=TRUE)
R> year <- substring(y$DATUM, 7)
R> x <- cumsum(y$Peff)
R> y$cumPeff <- x - c(0,x)[match(year, year)]

-- David Brahm (brahm at alum.mit.edu)



From tlumley at u.washington.edu  Thu Feb 10 17:24:11 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 10 Feb 2005 08:24:11 -0800 (PST)
Subject: [R] Looking for tools to run case crossover analysis
In-Reply-To: <20050210044246.5569.qmail@webmail17.rediffmail.com>
References: <20050210044246.5569.qmail@webmail17.rediffmail.com>
Message-ID: <Pine.A41.4.61b.0502100823460.120016@homer07.u.washington.edu>

On Wed, 10 Feb 2005, Arin Basu wrote:

> Hi All:
>
> I am interested in tools available in R that will enable me to run 
> analysis of case crossover studies.



Conditional logistic regression, the basic tool, is the survival package.

 	-thomas



From tlumley at u.washington.edu  Thu Feb 10 17:27:09 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 10 Feb 2005 08:27:09 -0800 (PST)
Subject: [R] RE: Using a number as a name to access a list
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950172CE55@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E950172CE55@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <Pine.A41.4.61b.0502100826300.120016@homer07.u.washington.edu>

On Thu, 10 Feb 2005, michael watson (IAH-C) wrote:

> The answer of course is parse()!

Nononono.  If the answer is parse() you should usually rethink the 
question.

The answer is as.list(WHATEVERITWASCALLED)[[path]]


 	-thomas

>
> Thank you and good night!
> M
>
> -----Original Message-----
> From: michael watson (IAH-C)
> Sent: 10 February 2005 13:36
> To: r-help at stat.math.ethz.ch
> Subject: Using a number as a name to access a list
>
>
> Hi
>
> Dumb question time again, for which I apologise.
>
> I have a variable that contains the following numerical text "04010".
> This is the name to access a list:
>
>> as.list(KEGGPATHID2NAME)$"04010"
> [1] "MAPK signaling pathway"
>
> Marvellous!  Except I want to do that when "04010" is assigned to a
> variable called path and I can't figure out how to do it!
>
>> path <- "04010"
>>
>> # the original and best
>> as.list(KEGGPATHID2NAME)$"04010"
> [1] "MAPK signaling pathway"
>>
>> # clearly this doesn't, and shouldn't, work
>> as.list(KEGGPATHID2NAME)$path
> NULL
>>
>> # this produces a string...
>> eval(paste("as.list(KEGGPATHID2NAME)$",path,sep=''))
> [1] "as.list(KEGGPATHID2NAME)$04010"
>>
>> # as does this
>> eval(paste('as.list(KEGGPATHID2NAME)$"',path,'"',sep=''))
> [1] "as.list(KEGGPATHID2NAME)$\"04010\""
>>
>
> Whats really annoying is that when everyone mails me the answer, I'm
> going to have known how obvious it is.... Thanks in advance
>
> Mick
> Village Idiot
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From davidhughjones at gmail.com  Thu Feb 10 18:15:29 2005
From: davidhughjones at gmail.com (David Hugh-Jones)
Date: Thu, 10 Feb 2005 17:15:29 +0000
Subject: [R] correcting for autocorrelation in models with panel data?
Message-ID: <f5d84806050210091556b65848@mail.gmail.com>

Hi 

I have some panel data for the 50 US states over about 25 years, and I
would like to test a simple model via OLS, using this data. I know how
to run OLS in R, and I think I can see how to  create Panel Corrected
Standard Errors using

http://jackman.stanford.edu/classes/350C/pcse.r

What I can't figure out is how to correct for autocorrelation over
time. I have found a lot of R stuff on time series models but they all
seem focused on predicting a single variable from its previous values.
Can anyone explain to me how to detect and get round autocorrelation?
Is there a package for panel data that I have missed?

I appreciate that this is probably just as much about my ignorance of
econometrics as about R itself!

Cheers
David



From p.dalgaard at biostat.ku.dk  Thu Feb 10 18:26:32 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 10 Feb 2005 18:26:32 +0100
Subject: [R] Failure of update.packages()
In-Reply-To: <1108043751.420b67e79d808@postie.iarc.fr>
References: <Pine.GSO.4.58.0502101214080.24015@holyrood.ed.ac.uk>
	<x28y5wa86e.fsf@biostat.ku.dk>
	<1108042429.29936.7.camel@biol102145.oulu.fi>
	<1108043751.420b67e79d808@postie.iarc.fr>
Message-ID: <x27jlgjph3.fsf@biostat.ku.dk>

plummer at iarc.fr writes:

> Quoting Jari Oksanen <jarioksa at sun3.oulu.fi>:
> 
> > On Thu, 2005-02-10 at 13:52 +0100, Peter Dalgaard wrote:
> > > I M S White <iwhite at staffmail.ed.ac.uk> writes:
> > >
> > > > Can anyone explain why with latest version of R (2.0.1) on FC3, installed
> > > > from R-2.0.1-0.fdr.2.fc3.i386.rpm, update.packages() produces the message
> > > >
> > > > /usr/lib/R/bin/Rcmd exec: INSTALL: not found.
> > > >
> > > > Indeed /usr/lib/R/bin seems to lack various shell scripts (INSTALL,
> > > > REMOVE, etc).
> >
> > > You need to install the R-devel package too:
> > > 1
> > > R-devel-2.0.1-0.fdr.2.fc3.i386.rpm
> > >
> > > The big idea is that this will suck in all the required compilers,
> > > libraries, and include files via RPM dependencies, but users with
> > > limited disk space may be content with the binaries of R+recommended
> > > packages.
> > >
> > This kind of problems were to be anticipated, weren't they? The great
> > divide between use-only and devel packages is a rpm packaging standard,
> > but not very useful in this case: it splits a 568K devel chip from a
> > 15.4M chunk of base R. Moreover, you don't have a repository of binary
> > packages for Linux which means that not many people can use the 568K
> > saving in download times (saving in disk space is more considerable of
> > course). So are there plans for binary Linux packages for other distros
> > than Debian so that people could use the non-devel piece of R only?
> >
> > cheers, jari oksanen
> 
> The splitting is an experiment (and I said so when I announced it).
> It does have unforseen consequences, like implicating me in maintaining a
> repository of binary RPMs for CRAN packages, which I'm not particularly keen
> on.
> 
> So I shall probably revert to a single RPM, and force the installation
> requirements to be the same as the build requirements.  This was, in fact,
> Peter's suggestion which shows that not everybody is as short-sighted as me.
> 
> Martyn

Hmm... Actually, you had sort of convinced me that the split might be
a good idea. Point being of course that it's not the 568K that gets
shaved off in R-devel, it's the 12M for gcc + the 5M for g77 + 28M for
perl + more, which are only needed for installing packages and are
therefore not dependencies of the main R RPM. Maintaining binary
package RPMs was never in the cards as I saw it. However, it then only
makes sense if a sizable proportion of R users are never going to
install packages. Otherwise you get cost of having to explain the
point repeatedly, at basically zero benefit.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From HDoran at air.org  Thu Feb 10 18:36:32 2005
From: HDoran at air.org (Doran, Harold)
Date: Thu, 10 Feb 2005 12:36:32 -0500
Subject: [R] correcting for autocorrelation in models with panel data?
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7407B1033A@dc1ex2.air.org>

In the nlme package you can find the gls() function to account for
autocorrelation over time using corAR1. Syntax might look something like
this:

fm1 <- gls(response ~ IV, long, correlation=corAR1(form=~1|ID),
method='ML')

You can also use weights() for heteroscedasticity.

-Harold

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of David Hugh-Jones
Sent: Thursday, February 10, 2005 12:15 PM
To: r-help at stat.math.ethz.ch
Subject: [R] correcting for autocorrelation in models with panel data?

Hi 

I have some panel data for the 50 US states over about 25 years, and I
would like to test a simple model via OLS, using this data. I know how
to run OLS in R, and I think I can see how to  create Panel Corrected
Standard Errors using

http://jackman.stanford.edu/classes/350C/pcse.r

What I can't figure out is how to correct for autocorrelation over time.
I have found a lot of R stuff on time series models but they all seem
focused on predicting a single variable from its previous values.
Can anyone explain to me how to detect and get round autocorrelation?
Is there a package for panel data that I have missed?

I appreciate that this is probably just as much about my ignorance of
econometrics as about R itself!

Cheers
David

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From pankajchopra00 at yahoo.com  Thu Feb 10 18:46:48 2005
From: pankajchopra00 at yahoo.com (Pankaj Chopra)
Date: Thu, 10 Feb 2005 09:46:48 -0800 (PST)
Subject: [R] Conversion of Affy IDs , LocusLink IDs etc...
Message-ID: <20050210174648.1097.qmail@web80903.mail.scd.yahoo.com>

Hi,

I was trying to compare differential expression data
from multiple studies ( 4 dataset downloaded from NCBI
GEO). The expression dataset are from Affy, as well as
cDNA (dual channel) platforms. Before I conduct
analysis, I have to map the gene IDs so that the
different genes are comparable.

Which packages, and what commands do I need for this?

thanks,

Pankaj



From jaana.volotinen at k-rauta.com  Thu Feb 10 18:58:40 2005
From: jaana.volotinen at k-rauta.com (Volotinen Jaana)
Date: Thu, 10 Feb 2005 19:58:40 +0200
Subject: [R] Automaattinen poissaolovastaus: Document
Message-ID: <6055D437681FA84C8292F9E677376D510946AB@keskobe01.keskomail.x>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050210/e3153e67/attachment.pl

From xmeadow at uchicago.edu  Thu Feb 10 19:08:21 2005
From: xmeadow at uchicago.edu (Xander Meadow)
Date: Thu, 10 Feb 2005 12:08:21 -0600
Subject: [R] install RMySQL
In-Reply-To: <p0611040cbe3131d5a8de@[128.115.153.6]>
References: <1107804348.4305.6.camel@gregor.bsd.uchicago.edu>
	<4207C4F9.9080109@jhsph.edu>
	<1107805973.4305.11.camel@gregor.bsd.uchicago.edu>
	<1107961813.9331.3.camel@gregor.bsd.uchicago.edu>
	<p0611040cbe3131d5a8de@[128.115.153.6]>
Message-ID: <1108058901.25717.5.camel@gregor.bsd.uchicago.edu>

Hi,

Thank you so much for the help.  After setting the environment variables
I was able to install RMySQL.  Thanks again.


>Xander

On Thu, 2005-02-10 at 10:09, Don MacQueen wrote:
> I have a working installation of RMySQL in Mac OS X (10.3.7). I have 
> libz in /usr/lib, as you do.
> None of my personal installation notes mention doing anything special 
> for /usr/lib/libz*.
> My R was built from a source distribution; it is not a binary 
> distribution. (version info below)
> 
> The only extra step I do is to define environment variables 
> PKG_CPPFLAGS and PKG_LIBS,
> like this:
> 
> mysql.home <- '/usr/local/mysql'
> Sys.putenv('PKG_CPPFLAGS'=paste('-I',file.path(mysql.home,'include'),sep=''))
> Sys.putenv('PKG_LIBS'=paste('-L',file.path(mysql.home,'lib'),' 
> -lmysqlclient',sep=''))
> install.packages('RMySQL')
> 
> You could try adding /usr/lib to PKG_LIBS.
> 
> -Don
> 
> 
> >  version
>           _                       
> platform powerpc-apple-darwin6.8.5
> arch     powerpc                 
> os       darwin6.8.5             
> system   powerpc, darwin6.8.5    
> status                           
> major    2                       
> minor    0.1                     
> year     2004                    
> month    11                      
> day      15                      
> language R                       
> 
> 
> RMySQL is 0.5-5
> 
> 
> At 9:10 AM -0600 2/9/05, Xander  Meadow wrote:
> >Hi,
> >
> >I'm still trying to get RMySQL installed on my Mac.  When I try and
> >install from R (version 2.0.1) I'm told that the library "libz" can't be
> >located.  It suggests checking in /usr/lib to see if I have the
> >necessary libraries.  However, if I check /usr/lib I find I have the
> >following:
> >
> >libz.1.1.3.dylib*
> >libz.1.dylib*
> >libz.dylib@
> >
> >Since I have the libraries that RMySQL needs to install I can't figure
> >out why the installation is failing.  Does anyone have any thoughts on
> >how to get RMySQL installed on my Mac?  Thanks so much.
> >
> >>Xander
> >
> >On Mon, 2005-02-07 at 13:52, Xander Meadow wrote:
> >>  Hi,
> >>
> >>  Thank you for responding.  One thing I forgot to mention (although I'm
> >>  guessing you figured it out anyway) is that I'm running Mac OS X.
> >>
> >>  I checked and I've got the file /usr/include/zlib.h installed on my
> >>  machine.  Is this enough for R or does it need a more robust
> >>  installation of zlib?  If this is enough, how do I let R know about the
> >>  zlib.h file. If it's not, where should I get the correct zlib from?
> >>  Thank you for any and all responses.
> >>
> >>  -Xander
> >>
> >>  On Mon, 2005-02-07 at 13:43, Roger D. Peng wrote:
> >>  > R comes with it's own copy of zlib so even if it is compiled into R,
> >>  > there isn't necessarily a system-wide installation of the library.
> >>  > You may still need to install it.
> >>  >
> >>  > -roger
> >>  >
> >>  > Xander Meadow wrote:
> >>  > > Hi,
> >>  > >
> >>  > > I've got a Dual G5 running 10.3.7 and I'm trying to install RMySQL.
> >>  > > I've already got R up and running.  When I try the command
> >>  > >
> >>  > >
> >>  > >>install.packages("RMySQL")
> >>  > >
> >>  > >
> >>  > > It downloads a few things and then produces the following error:
> >>  > > ---------------------------
> >>  > > Configuration error:
> >>  > >    Could not locate the library "libz" required by MySQL.
> >>  > > 
> >>  > > INSTRUCTIONS:
> >>  > > 
> >>  > >    The "libz" library is required by the MySQL client library
> >>  > >    in order to compress/uncompress connections between clients
> >>  > >    and the MySQL engine.
> >>  > > 
> >>  > >    Make sure you have "libz" installed properly and/or included
> >>  > >    in your $LD_LIBRARY_PATH.  Perhaps it is not in any of the
> >>  > >    standard directories (e.g., /usr/lib/, /usr/local/lib)?
> >>  > > 
> >>  > > Aborting the installation of RMySQL.
> >>  > > 
> >>  > > ERROR: configuration failed for package 'RMySQL'
> >>  > > ----------------------------
> >>  > >
> >>  > > However, if I check for libz I get:
> >>  > >
> >>  > >
> >>  > >>capabilities("libz")
> >>  > >
> >>  > > libz
> >>  > > TRUE
> >>  > >
> >>  > > I'm not sure what the problem is because R is telling me that it doesn't
> >>  > > know where libz is, but then it's also telling me it does know where
> >>  > > libz is.
> >>  > >
> >>  > > Has anyone else seen this problem or know how I can get RMySQL installed
> >>  > > on my machine?
> >>  > >
> >>  > > Just as an added note I know that install.packages works because
> >  > > >
> >>  > >
> >>  > >>install.packages("DBI")
> >>  > >
> >>  > >
> >>  > > worked without a problem.
> >>  > >
> >>  > > Thanks again.
> >>  > >
> >>  > > -Xander
> >>  > >
> >>  > > ______________________________________________
> >>  > > R-help at stat.math.ethz.ch mailing list
> >>  > > https://stat.ethz.ch/mailman/listinfo/r-help
> >>  > > PLEASE do read the posting guide! 
> >>http://www.R-project.org/posting-guide.html
> >>  > >
> >>
> >>  ______________________________________________
> >>  R-help at stat.math.ethz.ch mailing list
> >>  https://stat.ethz.ch/mailman/listinfo/r-help
> >>  PLEASE do read the posting guide! 
> >>http://www.R-project.org/posting-guide.html
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ligges at statistik.uni-dortmund.de  Thu Feb 10 19:29:20 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 10 Feb 2005 19:29:20 +0100
Subject: [R] Conversion of Affy IDs , LocusLink IDs etc...
In-Reply-To: <20050210174648.1097.qmail@web80903.mail.scd.yahoo.com>
References: <20050210174648.1097.qmail@web80903.mail.scd.yahoo.com>
Message-ID: <420BA800.1030100@statistik.uni-dortmund.de>

Pankaj Chopra wrote:

> Hi,
> 
> I was trying to compare differential expression data
> from multiple studies ( 4 dataset downloaded from NCBI
> GEO). The expression dataset are from Affy, as well as
> cDNA (dual channel) platforms. Before I conduct
> analysis, I have to map the gene IDs so that the
> different genes are comparable.
> 
> Which packages, and what commands do I need for this?


Take a look at the Bioconductor project:
http://www.bioconductor.org/

Uwe Ligges

> thanks,
> 
> Pankaj
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From manuel_gutierrez_lopez at yahoo.es  Thu Feb 10 19:36:25 2005
From: manuel_gutierrez_lopez at yahoo.es (Manuel Gutierrez)
Date: Thu, 10 Feb 2005 19:36:25 +0100 (CET)
Subject: [R] testing slopes different than a given value
Message-ID: <20050210183625.49369.qmail@web25108.mail.ukl.yahoo.com>

In a multiple linear regression with two independent
variables is there any function in R to test for the
coefficients being different than some given values?
Example:
x1<-rnorm(100)
x2<-rnorm(100)
y<-3+0.6*x1+0.3*x2 
lm(y~x1+x2)
Obtain a test for the coefficients for x1 being 
different than 0.6 and for x2 different than 0.3
Thanks
Manuel



From gunter.berton at gene.com  Thu Feb 10 19:41:17 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 10 Feb 2005 10:41:17 -0800
Subject: [R] Conversion of Affy IDs , LocusLink IDs etc...
In-Reply-To: <420BA800.1030100@statistik.uni-dortmund.de>
Message-ID: <200502101841.j1AIfH4G008543@compton.gene.com>

You may also be interested in the following, which point out potentially
severe problems with such metanalyses. 

http://www.cbs.dtu.dk/courses/norfa2004/Extras/5_1_Yves_TiG_meta-analysis_Ar
ticle.pdf

http://www.clinchem.org/cgi/content/full/47/8/1350

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Uwe Ligges
> Sent: Thursday, February 10, 2005 10:29 AM
> To: Pankaj Chopra
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Conversion of Affy IDs , LocusLink IDs etc...
> 
> Pankaj Chopra wrote:
> 
> > Hi,
> > 
> > I was trying to compare differential expression data
> > from multiple studies ( 4 dataset downloaded from NCBI
> > GEO). The expression dataset are from Affy, as well as
> > cDNA (dual channel) platforms. Before I conduct
> > analysis, I have to map the gene IDs so that the
> > different genes are comparable.
> > 
> > Which packages, and what commands do I need for this?
> 
> 
> Take a look at the Bioconductor project:
> http://www.bioconductor.org/
> 
> Uwe Ligges
> 
> > thanks,
> > 
> > Pankaj
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From davidhughjones at gmail.com  Thu Feb 10 20:12:11 2005
From: davidhughjones at gmail.com (David Hugh-Jones)
Date: Thu, 10 Feb 2005 19:12:11 +0000
Subject: [R] correcting for autocorrelation in models with panel data?
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F7407B1033A@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F7407B1033A@dc1ex2.air.org>
Message-ID: <f5d8480605021011122fab0c78@mail.gmail.com>

That's very helpful - I was on the point of giving up and going with
Stata! I will look into that in more detail. I assume that afterwards
it would be ok to apply the Beck and Katz procedure to get panel
corrected standard errors.

Cheers
David


On Thu, 10 Feb 2005 12:36:32 -0500, Doran, Harold <HDoran at air.org> wrote:
> In the nlme package you can find the gls() function to account for
> autocorrelation over time using corAR1. Syntax might look something like
> this:
> 
> fm1 <- gls(response ~ IV, long, correlation=corAR1(form=~1|ID),
> method='ML')
> 
> You can also use weights() for heteroscedasticity.
> 
> -Harold
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of David Hugh-Jones
> Sent: Thursday, February 10, 2005 12:15 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] correcting for autocorrelation in models with panel data?
> 
> Hi
> 
> I have some panel data for the 50 US states over about 25 years, and I
> would like to test a simple model via OLS, using this data. I know how
> to run OLS in R, and I think I can see how to  create Panel Corrected
> Standard Errors using
> 
> http://jackman.stanford.edu/classes/350C/pcse.r
> 
> What I can't figure out is how to correct for autocorrelation over time.
> I have found a lot of R stuff on time series models but they all seem
> focused on predicting a single variable from its previous values.
> Can anyone explain to me how to detect and get round autocorrelation?
> Is there a package for panel data that I have missed?
> 
> I appreciate that this is probably just as much about my ignorance of
> econometrics as about R itself!
> 
> Cheers
> David
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
>



From jerk_alert at hotmail.com  Thu Feb 10 20:23:33 2005
From: jerk_alert at hotmail.com (Ken Termiso)
Date: Thu, 10 Feb 2005 19:23:33 +0000
Subject: [R] Mac OSX - outputting history to a source file (plain text file)
Message-ID: <BAY101-F131F774C0BAD996B5971A8E8760@phx.gbl>

Hi,

I've got OSX 10.3 and R v2.0.1 and what I'd like to do is either take a 
history file and run it as a source file (after I modify it in a text 
editor) or simply output the command history to a plain text file that I 
could then modify and later run as source.

The .history files are somewhat serialized in that they are not 100% plain 
text, and using history(max.show = 25, reverse = FALSE) created a popup file 
with only "NA" in it.

Thanks in advance,
ken



From FeiChen at fairisaac.com  Thu Feb 10 20:32:38 2005
From: FeiChen at fairisaac.com (Chen, Fei)
Date: Thu, 10 Feb 2005 11:32:38 -0800
Subject: [R] Employment opportunities at Fair Isaac
Message-ID: <B1B25C5DF418644C8391261C189B58DA019DCDD9@SDOMSGMB00.corp.fairisaac.com>

Hi all,

We currently have 5 immediate openings in the Analytic Science/Product
Development organization of Fair Isaac in San Diego, CA, USA. Send me an
email if you are interested: FeiChen at fairisaac.com

Thanks much, and here's the blurb:
---------------------------------------------------------------

Position Overview:
Join a company that makes more than 25 billion decisions a year. By the
time you finish reading this job description Fair Isaac will have made
tens of thousands of decisions in credit risk assessment and fraud   
detection around the world. You will be a part of a talented team for
designing, developing, and deploying state-of-the-art, data-driven
predictive models to solve business problems using the latest
technologies
in neural networks, machine learning, statistical modeling, pattern
recognition, and artificial intelligence.

Job Responsibilities:
The main responsibilities include: analyzing and understanding large
amounts of historical data for data clean-up and filtering, pattern 
identification and feature extraction, feature (variable) selection,
experimenting with different types of algorithms and modeling
techniques,
analyzing performance, preparing model reports for communication with   
internal and external clients, participating in pre-sales, and providing
post implementation support.

Minimum Qualifications:
Master's or Ph.D. in science or engineering field. Industry or academic
experience in employing neural networks, machine learning techniques,
mathematical/statistical modeling, pattern recognition, or data
mining/data analysis on real world problems. Strong problem solving and

analytical ability. Excellent communication skills. A statistical
thinker
and quick learner. Team orientated attitude, as well as the ability to

work independently with minimum direction. Preferred experience with
Unix
and strong experience in C/C++ and Java.

Travel Percentage:  5



From jfox at mcmaster.ca  Thu Feb 10 20:45:16 2005
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 10 Feb 2005 14:45:16 -0500
Subject: [R] testing slopes different than a given value
In-Reply-To: <20050210183625.49369.qmail@web25108.mail.ukl.yahoo.com>
Message-ID: <20050210194507.IMYJ2034.tomts20-srv.bellnexxia.net@JohnDesktop8300>

Dear Manuel,

Take a look at the linear.hypothesis function in the car package.

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Manuel 
> Gutierrez
> Sent: Thursday, February 10, 2005 1:36 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] testing slopes different than a given value
> 
> In a multiple linear regression with two independent 
> variables is there any function in R to test for the 
> coefficients being different than some given values?
> Example:
> x1<-rnorm(100)
> x2<-rnorm(100)
> y<-3+0.6*x1+0.3*x2
> lm(y~x1+x2)
> Obtain a test for the coefficients for x1 being different 
> than 0.6 and for x2 different than 0.3 Thanks Manuel
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From jari.oksanen at oulu.fi  Thu Feb 10 21:13:04 2005
From: jari.oksanen at oulu.fi (Jari Oksanen)
Date: Thu, 10 Feb 2005 22:13:04 +0200
Subject: [R] Failure of update.packages()
In-Reply-To: <x27jlgjph3.fsf@biostat.ku.dk>
References: <Pine.GSO.4.58.0502101214080.24015@holyrood.ed.ac.uk>
	<x28y5wa86e.fsf@biostat.ku.dk>
	<1108042429.29936.7.camel@biol102145.oulu.fi>
	<1108043751.420b67e79d808@postie.iarc.fr>
	<x27jlgjph3.fsf@biostat.ku.dk>
Message-ID: <47f03c9c0818746b835fe339a7ccf44f@oulu.fi>


On 10 Feb 2005, at 19:26, Peter Dalgaard wrote:

> plummer at iarc.fr writes:
>
>> Quoting Jari Oksanen <jarioksa at sun3.oulu.fi>:
>>
>>> On Thu, 2005-02-10 at 13:52 +0100, Peter Dalgaard wrote:
>>>> I M S White <iwhite at staffmail.ed.ac.uk> writes:
>>>>
>>>>> Can anyone explain why with latest version of R (2.0.1) on FC3, 
>>>>> installed
>>>>> from R-2.0.1-0.fdr.2.fc3.i386.rpm, update.packages() produces the 
>>>>> message
>>>>>
>>>>> /usr/lib/R/bin/Rcmd exec: INSTALL: not found.
>>>>>
>>>>> Indeed /usr/lib/R/bin seems to lack various shell scripts (INSTALL,
>>>>> REMOVE, etc).
>>>
>>>> You need to install the R-devel package too:
>>>> 1
>>>> R-devel-2.0.1-0.fdr.2.fc3.i386.rpm
>>>>
>>>> The big idea is that this will suck in all the required compilers,
>>>> libraries, and include files via RPM dependencies, but users with
>>>> limited disk space may be content with the binaries of R+recommended
>>>> packages.
>>>>
>>> This kind of problems were to be anticipated, weren't they? The great
>>> divide between use-only and devel packages is a rpm packaging 
>>> standard,
>>> but not very useful in this case: it splits a 568K devel chip from a
>>> 15.4M chunk of base R. Moreover, you don't have a repository of 
>>> binary
>>> packages for Linux which means that not many people can use the 568K
>>> saving in download times (saving in disk space is more considerable 
>>> of
>>> course). So are there plans for binary Linux packages for other 
>>> distros
>>> than Debian so that people could use the non-devel piece of R only?
>>>
>>> cheers, jari oksanen
>>
>> The splitting is an experiment (and I said so when I announced it).
>> It does have unforseen consequences, like implicating me in 
>> maintaining a
>> repository of binary RPMs for CRAN packages, which I'm not 
>> particularly keen
>> on.
>>
>> So I shall probably revert to a single RPM, and force the installation
>> requirements to be the same as the build requirements.  This was, in 
>> fact,
>> Peter's suggestion which shows that not everybody is as short-sighted 
>> as me.
>>
>> Martyn
>
> Hmm... Actually, you had sort of convinced me that the split might be
> a good idea. Point being of course that it's not the 568K that gets
> shaved off in R-devel, it's the 12M for gcc + the 5M for g77 + 28M for
> perl + more, which are only needed for installing packages and are
> therefore not dependencies of the main R RPM. Maintaining binary
> package RPMs was never in the cards as I saw it. However, it then only
> makes sense if a sizable proportion of R users are never going to
> install packages. Otherwise you get cost of having to explain the
> point repeatedly, at basically zero benefit.

That's a good point. You could look at MacOS X standard installation to 
see what can be left out in a working installation. In default Mac, you 
don't have gcc (12M), nor g77, but you sure need perl for a sensible 
working machine, and tha't in default MacOS X installation. The price 
is that you need a possibility to install binary R packages. So not so 
much saving, but a bit more than what you get by shaving off R-devel.

cheers, jari oksanen
--
Jari Oksanen, Oulu, Finland



From georg.hoermann at gmx.de  Thu Feb 10 21:26:57 2005
From: georg.hoermann at gmx.de (Georg Hoermann)
Date: Thu, 10 Feb 2005 21:26:57 +0100
Subject: [R] Annual cumulative sums from time series - SOLVED + questions
In-Reply-To: <Pine.LNX.4.44.0502101135590.23088-100000@reclus.nhh.no>
References: <Pine.LNX.4.44.0502101135590.23088-100000@reclus.nhh.no>
Message-ID: <420BC391.4030808@gmx.de>

Roger Bivand wrote:
> On Thu, 10 Feb 2005, Georg Hoermann wrote:
> 
Hello world,

based on the code of Roger I have now two solutions:

the first one (one line for the whole dataset)
--------- cut here ------

erle <- 
read.csv(url("http://www.hydrology.uni-kiel.de/~schorsch/statistik/erle_stat.csv"))
jahre <- factor(substring(erle$DATUM, 7))
tpks1 <- unlist(tapply(erle$Sonnen, jahre, cumsum))
plot(tpks1, type="l")

--------- cut here -------

The second one plots one line for each year:

---- start - cut here ----
# read data in from Internet
erle <- 
read.csv(url("http://www.hydrology.uni-kiel.de/~schorsch/statistik/erle_stat.csv"))
# extract Year as a factor from variable DATUM
jahre <- factor(substring(erle$DATUM, 7))
try1 <- tapply(erle$Sonnen, jahre, cumsum)
# create colors for every year from rainbow color scheme
cols <- rainbow(length(try1))
plot(x=c(1,366), y=c(0,1200), type="n", xlab="Days", ylab="Cumulative 
sunshine (h)")
# draw the lines, one line for each year
for (i in 1:length(try1)) lines(1:length(try1[[i]]), try1[[i]], col=cols[i])

# ...and the legend
legend(c(0,100), c(400,1100), names(try1), col=cols, lwd=1, bty="n")

---- end cut here -----

For the second example, a mean sum for all years would also be a good 
idea...

Thanks for all solutions...

Merci & Gruss
Georg

-- 
Georg Hoermann, Dep. of Hydrology, Ecology, Kiel University, Germany
Tel. 0431-880-1207, icq - 348 340 729, 0172/4315715, Penguin #189476



From JAROSLAW.W.TUSZYNSKI at saic.com  Thu Feb 10 22:07:25 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Thu, 10 Feb 2005 16:07:25 -0500
Subject: [R] Problem with "R CMD Rd2dvi": Rd.sty not found
Message-ID: <9CC1B717EF3BD511AD98000103D63FC53FA754@us-arl-asg.mail.saic.com>


Hi,

I run into a problem with "R CMD Rd2dvi" command: it gives me "File `Rd.sty'
not found" error (See the output message on the bottom). I get the same
error when running "RCMD check". My system is: Windows 2000, R version
2.0.1, MiKTeX version 2.4. I do have a Rd.sty file in R_HOME/share/texm
directory.

I looked through newsgroups for any related discussions and found Arnab
Mukherji reporting the same problem on Dec 23 2003 (Subject: "[R] Rd.sty not
found -"). In reply Prof. Brian Ripley suggested the following:
	It should be looking in TEXINPUTS, as is the point of the following
in the check script
	$ENV{'TEXINPUTS'} = env_path(&file_path($R::Vars::R_HOME, "share",
"texmf"), $ENV{'TEXINPUTS'});
	So you could try adding to TEXINPUTS yourself, and you should
certainly check the current setting of the environment variable (unset is
fine). 
I am not sure if I understand it correctly but I created environmental
variable TEXINPUTS with path to R_HOME/share/texm. It did not help.

Any other suggestions? 
Should I use some other version of Latex?
Do I have to configure it in any special way?

Thanks for any suggestions,


C:\programs\R\rw2001\src\library\caMassClass\man>R CMD Rd2dvi
BaselineSubstracton.Rd
Converting Rd files to LaTeX ...
BaselineSubstraction.Rd
Creating dvi output from LaTeX ...
This is pdfTeX, Version 3.141592-1.20a-rc7.2 (MiKTeX 2.4)
output format initialized to PDF
(Rd2.tex
LaTeX2e <2003/12/01>
Babel <v3.8a> and hyphenation patterns for english, french, german, ngerman,
dumylang, nohyphenation, loaded.

(C:\Program Files\latex\tex\latex\base\book.cls
Document Class: book 2004/02/16 v1.4f Standard LaTeX document class
(C:\Program Files\latex\tex\latex\base\bk10.clo))

! LaTeX Error: File `Rd.sty' not found.

Type X to quit or <RETURN> to proceed,
or enter new name. (Default extension: sty)

Enter file name:
! Emergency stop.
<read *>

l.4 \usepackage
               {makeidx}
No pages of output.
Transcript written on Rd2.log.
Couldn't find input index file Rd2 nor Rd2.idx.
Usage: c:\Program Files\latex\miktex\bin\makeindex.exe [-ilqrcg] [-s sty]
[-o id] [-t log] [-p num] [idx0 idx1 ...]
This is pdfTeX, Version 3.141592-1.20a-rc7.2 (MiKTeX 2.4)
output format initialized to PDF
(Rd2.tex
LaTeX2e <2003/12/01>
Babel <v3.8a> and hyphenation patterns for english, french, german, ngerman,
dumylang, nohyphenation, loaded.

(C:\Program Files\latex\tex\latex\base\book.cls
Document Class: book 2004/02/16 v1.4f Standard LaTeX document class
(C:\Program Files\latex\tex\latex\base\bk10.clo))

! LaTeX Error: File `Rd.sty' not found.

Type X to quit or <RETURN> to proceed,
or enter new name. (Default extension: sty)

Enter file name:
! Emergency stop.
<read *>

l.4 \usepackage
               {makeidx}
No pages of output.
Transcript written on Rd2.log.
Saving output to 'BaselineSubstraction.dvi' ...
cp: cannot stat `.Rd2dvi/Rd2.dvi': No such file or directory
Done
xdvi.bat: not found

Jarek
====================================================\=======

 Jarek Tuszynski, PhD.                           o / \ 
 Science Applications International Corporation  <\__,|  
 (703) 676-4192                                   ">   \
 Jaroslaw.W.Tuszynski at saic.com                     `    \



From ggrothendieck at myway.com  Thu Feb 10 22:27:03 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 10 Feb 2005 21:27:03 +0000 (UTC)
Subject: [R] Problem with "R CMD Rd2dvi": Rd.sty not found
References: <9CC1B717EF3BD511AD98000103D63FC53FA754@us-arl-asg.mail.saic.com>
Message-ID: <loom.20050210T222445-608@post.gmane.org>

Tuszynski, Jaroslaw W. <JAROSLAW.W.TUSZYNSKI <at> saic.com> writes:

: 
: Hi,
: 
: I run into a problem with "R CMD Rd2dvi" command: it gives me "File `Rd.sty'
: not found" error (See the output message on the bottom). I get the same
: error when running "RCMD check". My system is: Windows 2000, R version
: 2.0.1, MiKTeX version 2.4. I do have a Rd.sty file in R_HOME/share/texm
: directory.
: 
: I looked through newsgroups for any related discussions and found Arnab
: Mukherji reporting the same problem on Dec 23 2003 (Subject: "[R] Rd.sty not
: found -"). In reply Prof. Brian Ripley suggested the following:
: 	It should be looking in TEXINPUTS, as is the point of the following
: in the check script
: 	$ENV{'TEXINPUTS'} = env_path(&file_path($R::Vars::R_HOME, "share",
: "texmf"), $ENV{'TEXINPUTS'});
: 	So you could try adding to TEXINPUTS yourself, and you should
: certainly check the current setting of the environment variable (unset is
: fine). 
: I am not sure if I understand it correctly but I created environmental
: variable TEXINPUTS with path to R_HOME/share/texm. It did not help.
: 
: Any other suggestions? 
: Should I use some other version of Latex?
: Do I have to configure it in any special way?
: 
: Thanks for any suggestions,
: 
: C:\programs\R\rw2001\src\library\caMassClass\man>R CMD Rd2dvi
: BaselineSubstracton.Rd
: Converting Rd files to LaTeX ...
: BaselineSubstraction.Rd
: Creating dvi output from LaTeX ...
: This is pdfTeX, Version 3.141592-1.20a-rc7.2 (MiKTeX 2.4)
: output format initialized to PDF
: (Rd2.tex
: LaTeX2e <2003/12/01>
: Babel <v3.8a> and hyphenation patterns for english, french, german, ngerman,
: dumylang, nohyphenation, loaded.
: 
: (C:\Program Files\latex\tex\latex\base\book.cls
: Document Class: book 2004/02/16 v1.4f Standard LaTeX document class
: (C:\Program Files\latex\tex\latex\base\bk10.clo))
: 
: ! LaTeX Error: File `Rd.sty' not found.
: 
: Type X to quit or <RETURN> to proceed,
: or enter new name. (Default extension: sty)
: 
: Enter file name:
: ! Emergency stop.
: <read *>
: 
: l.4 \usepackage
:                {makeidx}
: No pages of output.
: Transcript written on Rd2.log.
: Couldn't find input index file Rd2 nor Rd2.idx.
: Usage: c:\Program Files\latex\miktex\bin\makeindex.exe [-ilqrcg] [-s sty]
: [-o id] [-t log] [-p num] [idx0 idx1 ...]
: This is pdfTeX, Version 3.141592-1.20a-rc7.2 (MiKTeX 2.4)
: output format initialized to PDF
: (Rd2.tex
: LaTeX2e <2003/12/01>
: Babel <v3.8a> and hyphenation patterns for english, french, german, ngerman,
: dumylang, nohyphenation, loaded.
: 
: (C:\Program Files\latex\tex\latex\base\book.cls
: Document Class: book 2004/02/16 v1.4f Standard LaTeX document class
: (C:\Program Files\latex\tex\latex\base\bk10.clo))
: 
: ! LaTeX Error: File `Rd.sty' not found.
: 
: Type X to quit or <RETURN> to proceed,
: or enter new name. (Default extension: sty)
: 
: Enter file name:
: ! Emergency stop.
: <read *>
: 
: l.4 \usepackage
:                {makeidx}
: No pages of output.
: Transcript written on Rd2.log.
: Saving output to 'BaselineSubstraction.dvi' ...
: cp: cannot stat `.Rd2dvi/Rd2.dvi': No such file or directory
: Done
: xdvi.bat: not found
: 


There is some information on this at:

http://www.murdoch-sutherland.com/Rtools/miktex.html



From wib2004 at med.cornell.edu  Thu Feb 10 22:29:12 2005
From: wib2004 at med.cornell.edu (William Briggs)
Date: Thu, 10 Feb 2005 16:29:12 -0500
Subject: [R] rewrite of scatter.smooth to handle NAs
Message-ID: <420BD228.4080805@med.cornell.edu>


I rewrote scatter.smooth to handle missing values, but I have a question 
about a move I had to make.  Here's the code:

Mscatter.smooth<-function (x, y, span = 2/3, degree = 1, family = 
c("symmetric",
     "gaussian"), xlab = deparse(substitute(x)), ylab = 
deparse(substitute(y)),
     ylim = range(y, prediction$y), evaluation = 50, ...)
{
     if (inherits(x, "formula")) {
         if (length(x) < 3)
             stop("need response in formula")
         thiscall <- match.call()
         thiscall$x <- x[[3]]
         thiscall$y <- x[[2]]
         return(invisible(eval(thiscall, sys.parent())))
     }
##################
     plot(x, y, xlab = xlab, ylab = ylab, ...)
	i<-complete.cases(x,y)
	x<-x[i]
	y<-y[i]
##################
     prediction <- loess.smooth(x, y, span, degree, family, evaluation)
     lines(prediction)
     invisible()
}

The changes I made are between the '###########'.  The idea was only to 
pass complete cases on to loess.smooth and thus avoice the NA error 
you'd usually get by calling scatter.smooth with missing values.

I had to move the plot above the loess.smooth, whereas in the orginal 
scatter.smooth is was just below to take adavantage of prediction$y to 
set the ylim in the plotting function.

But if I try to plot after creating the complete cases, the names of the 
x and y variables disappear and instead the plotting labels on the x and 
y axis look like, for example, 'c(1,2,1.2,5.2, ...)', that is, the 
vector of values being plotted.

Why does that happen?

Matt
-- 
William (Matt) Briggs
Assistant Professor of Biostatistics
Division of General Internal Medicine
Weill Medical College of Cornell University
525 East 68th St., Box #46
New York, NY 10021
Tel: 212-628-0128; Fax: 212-746-8965

http://drclamp.wmbriggs.com



From dsonneborn at ucdavis.edu  Thu Feb 10 22:33:57 2005
From: dsonneborn at ucdavis.edu (Dean Sonneborn)
Date: Thu, 10 Feb 2005 13:33:57 -0800
Subject: [R] skip missing values in plots
Message-ID: <6.1.0.6.2.20050210133319.032cc850@yellow.ucdavis.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050210/e7693e81/attachment.pl

From nair at sdsc.edu  Thu Feb 10 22:52:25 2005
From: nair at sdsc.edu (T. Murlidharan Nair)
Date: Thu, 10 Feb 2005 13:52:25 -0800
Subject: [R] sample
Message-ID: <420BD799.40700@sdsc.edu>

I am trying to sample a subset from a matrix using sample.
The size of the matrix is 20X 1532. It works fine with this,
but when I transpose the matrix and try to sample it, it returns
null.  
pick.set<-sample(tissue.exp.t,5,replace=FALSE,prob=NULL)

Is there something that I am missing here ?  
Thanks ../Murli



From gunter.berton at gene.com  Thu Feb 10 22:52:33 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 10 Feb 2005 13:52:33 -0800
Subject: [R] rewrite of scatter.smooth to handle NAs
In-Reply-To: <420BD228.4080805@med.cornell.edu>
Message-ID: <200502102152.j1ALqXks002784@volta.gene.com>

Lazy evaluation. See V&R's S Programming for a good explanation. But, in
brief, the default values for xlab and ylab are not evaluated until they are
needed. If plot(..., xlab=...) appears after the assignments to x and y,
then x and y are no longer the original argument expressions but numeric
vectors.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of William Briggs
> Sent: Thursday, February 10, 2005 1:29 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] rewrite of scatter.smooth to handle NAs
> 
> 
> I rewrote scatter.smooth to handle missing values, but I have 
> a question 
> about a move I had to make.  Here's the code:
> 
> Mscatter.smooth<-function (x, y, span = 2/3, degree = 1, family = 
> c("symmetric",
>      "gaussian"), xlab = deparse(substitute(x)), ylab = 
> deparse(substitute(y)),
>      ylim = range(y, prediction$y), evaluation = 50, ...)
> {
>      if (inherits(x, "formula")) {
>          if (length(x) < 3)
>              stop("need response in formula")
>          thiscall <- match.call()
>          thiscall$x <- x[[3]]
>          thiscall$y <- x[[2]]
>          return(invisible(eval(thiscall, sys.parent())))
>      }
> ##################
>      plot(x, y, xlab = xlab, ylab = ylab, ...)
> 	i<-complete.cases(x,y)
> 	x<-x[i]
> 	y<-y[i]
> ##################
>      prediction <- loess.smooth(x, y, span, degree, family, 
> evaluation)
>      lines(prediction)
>      invisible()
> }
> 
> The changes I made are between the '###########'.  The idea 
> was only to 
> pass complete cases on to loess.smooth and thus avoice the NA error 
> you'd usually get by calling scatter.smooth with missing values.
> 
> I had to move the plot above the loess.smooth, whereas in the orginal 
> scatter.smooth is was just below to take adavantage of 
> prediction$y to 
> set the ylim in the plotting function.
> 
> But if I try to plot after creating the complete cases, the 
> names of the 
> x and y variables disappear and instead the plotting labels 
> on the x and 
> y axis look like, for example, 'c(1,2,1.2,5.2, ...)', that is, the 
> vector of values being plotted.
> 
> Why does that happen?
> 
> Matt
> -- 
> William (Matt) Briggs
> Assistant Professor of Biostatistics
> Division of General Internal Medicine
> Weill Medical College of Cornell University
> 525 East 68th St., Box #46
> New York, NY 10021
> Tel: 212-628-0128; Fax: 212-746-8965
> 
> http://drclamp.wmbriggs.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From thopper at cobasys.com  Thu Feb 10 23:08:39 2005
From: thopper at cobasys.com (Thomas Hopper)
Date: Thu, 10 Feb 2005 17:08:39 -0500
Subject: [R] Curious Behavior with Curve() and dnorm()
Message-ID: <420BDB67.5030304@cobasys.com>

I am attempting to wrap the histogram function in my own custom 
function, so that I can quickly generate some standard plots.

A part of what I want to do is to draw a normal curve over the histogram:

 > x <- rnorm(1000)
 > hist(x, freq=F)
 > curve(dnorm(x), lty=3, add=T)

(for normal use, x would be a vector of empirical values, but the 
rnorm() function works for testing)

That works just as you'd expect, but I've found something a bit strange.

If I try the following:

 > curve(dnorm(x, mean=mean(x), sd=sd(x)), lty=3, add=T)

I get a much flatter and broader curve (which looks like it probably has 
the same area as the first curve, though I haven't tested).

However, if I do

 > z <- sd(x)
 > curve(dnorm(x, mean=mean(x), sd=z), lty=1, add=T)

I get the curve you'd expect; it draws right over the first curve 
(curve(dnorm(x),...), above).

I haven't touched x between the call to curve() containing 
dnorm(...,sd=sd(x)) and the call to curve() containing dnorm(...,sd=z), 
and tests show that z == sd(x).

I get similar results if I manually type in the standard deviation of 
x--the expected curve is drawn--so the broader and flatter curve is only 
drawn when I call dnorm with sd=sd(x).

Is there a reason for this, or is there something odd going on with the 
call to curve()?

Regards,

Tom



From jrgonzalez at ico.scs.es  Thu Feb 10 23:17:43 2005
From: jrgonzalez at ico.scs.es (Gonzalez Ruiz, Juan Ramon)
Date: Thu, 10 Feb 2005 23:17:43 +0100
Subject: [R] rats in survival package
Message-ID: <5FF3F11444E3A9439191AA1EDCB69A17089320@icosrvmail01.ICO.SCS.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050210/a604ab06/attachment.pl

From davidhughjones at gmail.com  Thu Feb 10 23:16:13 2005
From: davidhughjones at gmail.com (David Hugh-Jones)
Date: Thu, 10 Feb 2005 22:16:13 +0000
Subject: [R] correcting for autocorrelation in models with panel data?
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F7407B1033A@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F7407B1033A@dc1ex2.air.org>
Message-ID: <f5d8480605021014162da42c13@mail.gmail.com>

Assuming I have years in YEAR and state ids in ID, I guess the
correlation ought to be

corAR1(form = ~ YEAR | ID)

?

Thanks a lot,
David




On Thu, 10 Feb 2005 12:36:32 -0500, Doran, Harold <HDoran at air.org> wrote:
> In the nlme package you can find the gls() function to account for
> autocorrelation over time using corAR1. Syntax might look something like
> this:
> 
> fm1 <- gls(response ~ IV, long, correlation=corAR1(form=~1|ID),
> method='ML')
> 
> You can also use weights() for heteroscedasticity.
> 
> -Harold
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of David Hugh-Jones
> Sent: Thursday, February 10, 2005 12:15 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] correcting for autocorrelation in models with panel data?
> 
> Hi
> 
> I have some panel data for the 50 US states over about 25 years, and I
> would like to test a simple model via OLS, using this data. I know how
> to run OLS in R, and I think I can see how to  create Panel Corrected
> Standard Errors using
> 
> http://jackman.stanford.edu/classes/350C/pcse.r
> 
> What I can't figure out is how to correct for autocorrelation over time.
> I have found a lot of R stuff on time series models but they all seem
> focused on predicting a single variable from its previous values.
> Can anyone explain to me how to detect and get round autocorrelation?
> Is there a package for panel data that I have missed?
> 
> I appreciate that this is probably just as much about my ignorance of
> econometrics as about R itself!
> 
> Cheers
> David
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
>



From ripley at stats.ox.ac.uk  Thu Feb 10 23:26:05 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 10 Feb 2005 22:26:05 +0000 (GMT)
Subject: [R] Problem with "R CMD Rd2dvi": Rd.sty not found
In-Reply-To: <9CC1B717EF3BD511AD98000103D63FC53FA754@us-arl-asg.mail.saic.com>
References: <9CC1B717EF3BD511AD98000103D63FC53FA754@us-arl-asg.mail.saic.com>
Message-ID: <Pine.LNX.4.61.0502102220560.31066@gannet.stats>

This is a problem with a recent-ish change to MikTeX to make it use 
non-standard paths: see the notes on Duncan Murdoch's site at

   http://www.murdoch-sutherland.com/Rtools/miktex.html

Fptex is a standard distribution e.g. from TexLive.


On Thu, 10 Feb 2005, Tuszynski, Jaroslaw W. wrote:

>
> Hi,
>
> I run into a problem with "R CMD Rd2dvi" command: it gives me "File `Rd.sty'
> not found" error (See the output message on the bottom). I get the same
> error when running "RCMD check". My system is: Windows 2000, R version
> 2.0.1, MiKTeX version 2.4. I do have a Rd.sty file in R_HOME/share/texm
> directory.
>
> I looked through newsgroups for any related discussions and found Arnab
> Mukherji reporting the same problem on Dec 23 2003 (Subject: "[R] Rd.sty not
> found -"). In reply Prof. Brian Ripley suggested the following:
> 	It should be looking in TEXINPUTS, as is the point of the following
> in the check script
> 	$ENV{'TEXINPUTS'} = env_path(&file_path($R::Vars::R_HOME, "share",
> "texmf"), $ENV{'TEXINPUTS'});
> 	So you could try adding to TEXINPUTS yourself, and you should
> certainly check the current setting of the environment variable (unset is
> fine).
> I am not sure if I understand it correctly but I created environmental
> variable TEXINPUTS with path to R_HOME/share/texm. It did not help.
>
> Any other suggestions?
> Should I use some other version of Latex?
> Do I have to configure it in any special way?


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Feb 10 23:32:45 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 10 Feb 2005 22:32:45 +0000 (GMT)
Subject: [R] rewrite of scatter.smooth to handle NAs
In-Reply-To: <420BD228.4080805@med.cornell.edu>
References: <420BD228.4080805@med.cornell.edu>
Message-ID: <Pine.LNX.4.61.0502102227510.31066@gannet.stats>

You need evaluate deparse(substitute(x)) _before_ you change x.
so have

 	xlab; ylab

early in the body of your function.

However, NEWS in R-devel says


     o	scatter.smooth() and loess.smooth() now handle missing values
 	in their inputs.

so why reinvent that wheel?


On Thu, 10 Feb 2005, William Briggs wrote:

>
> I rewrote scatter.smooth to handle missing values, but I have a question 
> about a move I had to make.  Here's the code:
>
> Mscatter.smooth<-function (x, y, span = 2/3, degree = 1, family = 
> c("symmetric",
>    "gaussian"), xlab = deparse(substitute(x)), ylab = 
> deparse(substitute(y)),
>    ylim = range(y, prediction$y), evaluation = 50, ...)
> {
>    if (inherits(x, "formula")) {
>        if (length(x) < 3)
>            stop("need response in formula")
>        thiscall <- match.call()
>        thiscall$x <- x[[3]]
>        thiscall$y <- x[[2]]
>        return(invisible(eval(thiscall, sys.parent())))
>    }
> ##################
>    plot(x, y, xlab = xlab, ylab = ylab, ...)
> 	i<-complete.cases(x,y)
> 	x<-x[i]
> 	y<-y[i]
> ##################
>    prediction <- loess.smooth(x, y, span, degree, family, evaluation)
>    lines(prediction)
>    invisible()
> }
>
> The changes I made are between the '###########'.  The idea was only to pass 
> complete cases on to loess.smooth and thus avoice the NA error you'd usually 
> get by calling scatter.smooth with missing values.
>
> I had to move the plot above the loess.smooth, whereas in the orginal 
> scatter.smooth is was just below to take adavantage of prediction$y to set 
> the ylim in the plotting function.
>
> But if I try to plot after creating the complete cases, the names of the x 
> and y variables disappear and instead the plotting labels on the x and y axis 
> look like, for example, 'c(1,2,1.2,5.2, ...)', that is, the vector of values 
> being plotted.
>
> Why does that happen?

Because it is documented that way!

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Feb 10 23:43:49 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 10 Feb 2005 22:43:49 +0000 (GMT)
Subject: [R] Curious Behavior with Curve() and dnorm()
In-Reply-To: <420BDB67.5030304@cobasys.com>
References: <420BDB67.5030304@cobasys.com>
Message-ID: <Pine.LNX.4.61.0502102234310.31066@gannet.stats>

For which x do you think sd(x) is evaluated?

Hint: the help page shows

     expr: an expression written as a function of 'x', or alternatively
           the name of a function which will be plotted.

and you have written dnorm(x, mean=mean(x), sd=sd(x)) as function of x.


On Thu, 10 Feb 2005, Thomas Hopper wrote:

> I am attempting to wrap the histogram function in my own custom function, so 
> that I can quickly generate some standard plots.
>
> A part of what I want to do is to draw a normal curve over the histogram:
>
>> x <- rnorm(1000)
>> hist(x, freq=F)
>> curve(dnorm(x), lty=3, add=T)
>
> (for normal use, x would be a vector of empirical values, but the rnorm() 
> function works for testing)
>
> That works just as you'd expect, but I've found something a bit strange.
>
> If I try the following:
>
>> curve(dnorm(x, mean=mean(x), sd=sd(x)), lty=3, add=T)
>
> I get a much flatter and broader curve (which looks like it probably has the 
> same area as the first curve, though I haven't tested).
>
> However, if I do
>
>> z <- sd(x)
>> curve(dnorm(x, mean=mean(x), sd=z), lty=1, add=T)
>
> I get the curve you'd expect; it draws right over the first curve 
> (curve(dnorm(x),...), above).
>
> I haven't touched x between the call to curve() containing 
> dnorm(...,sd=sd(x)) and the call to curve() containing dnorm(...,sd=z), and 
> tests show that z == sd(x).
>
> I get similar results if I manually type in the standard deviation of x--the 
> expected curve is drawn--so the broader and flatter curve is only drawn when 
> I call dnorm with sd=sd(x).
>
> Is there a reason for this, or is there something odd going on with the call 
> to curve()?

It's working as documented.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Thu Feb 10 23:40:33 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 10 Feb 2005 23:40:33 +0100
Subject: [R] Curious Behavior with Curve() and dnorm()
In-Reply-To: <420BDB67.5030304@cobasys.com>
References: <420BDB67.5030304@cobasys.com>
Message-ID: <x2k6pgavj2.fsf@biostat.ku.dk>

Thomas Hopper <thopper at cobasys.com> writes:

> I am attempting to wrap the histogram function in my own custom
> function, so that I can quickly generate some standard plots.
> 
> A part of what I want to do is to draw a normal curve over the histogram:
> 
>  > x <- rnorm(1000)
>  > hist(x, freq=F)
>  > curve(dnorm(x), lty=3, add=T)
> 
> (for normal use, x would be a vector of empirical values, but the
> rnorm() function works for testing)
> 
> That works just as you'd expect, but I've found something a bit strange.
> 
> If I try the following:
> 
>  > curve(dnorm(x, mean=mean(x), sd=sd(x)), lty=3, add=T)
> 
> I get a much flatter and broader curve (which looks like it probably
> has the same area as the first curve, though I haven't tested).
> 
> However, if I do
> 
>  > z <- sd(x)
>  > curve(dnorm(x, mean=mean(x), sd=z), lty=1, add=T)
> 
> I get the curve you'd expect; it draws right over the first curve
> (curve(dnorm(x),...), above).

I don't think that is guaranteed, actually.

Notice that curve plots the *expression* as a function of the argument
"x". So it takes a bunch of x values, evenly spread across the
abscissa collects them into a vector and plugs that in as "x" in

curve(dnorm(x, mean=mean(x), sd=sd(x)), lty=3, add=T)

I.e. the x that gets plugged into mean(x) and sd(x) has nothing to do
with your original data (except that they both fit in the same xlim)! 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Thu Feb 10 23:49:39 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 10 Feb 2005 22:49:39 +0000 (GMT)
Subject: [R] correcting for autocorrelation in models with panel data?
In-Reply-To: <f5d8480605021014162da42c13@mail.gmail.com>
References: <88EAF3512A55DF46B06B1954AEF73F7407B1033A@dc1ex2.air.org>
	<f5d8480605021014162da42c13@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0502102245350.31066@gannet.stats>

Are the years equally spaced and in time order?
If so, it probably doesn't matter, and if not you may want corCAR1 not 
corAR1.


On Thu, 10 Feb 2005, David Hugh-Jones wrote:

> Assuming I have years in YEAR and state ids in ID, I guess the
> correlation ought to be
>
> corAR1(form = ~ YEAR | ID)
>
> ?
>
> Thanks a lot,
> David
>
>
>
>
> On Thu, 10 Feb 2005 12:36:32 -0500, Doran, Harold <HDoran at air.org> wrote:
>> In the nlme package you can find the gls() function to account for
>> autocorrelation over time using corAR1. Syntax might look something like
>> this:
>>
>> fm1 <- gls(response ~ IV, long, correlation=corAR1(form=~1|ID),
>> method='ML')
>>
>> You can also use weights() for heteroscedasticity.
>>
>> -Harold
>>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of David Hugh-Jones
>> Sent: Thursday, February 10, 2005 12:15 PM
>> To: r-help at stat.math.ethz.ch
>> Subject: [R] correcting for autocorrelation in models with panel data?
>>
>> Hi
>>
>> I have some panel data for the 50 US states over about 25 years, and I
>> would like to test a simple model via OLS, using this data. I know how
>> to run OLS in R, and I think I can see how to  create Panel Corrected
>> Standard Errors using
>>
>> http://jackman.stanford.edu/classes/350C/pcse.r
>>
>> What I can't figure out is how to correct for autocorrelation over time.
>> I have found a lot of R stuff on time series models but they all seem
>> focused on predicting a single variable from its previous values.
>> Can anyone explain to me how to detect and get round autocorrelation?
>> Is there a package for panel data that I have missed?
>>
>> I appreciate that this is probably just as much about my ignorance of
>> econometrics as about R itself!
>>
>> Cheers
>> David

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From nair at sdsc.edu  Thu Feb 10 23:53:32 2005
From: nair at sdsc.edu (T. Murlidharan Nair)
Date: Thu, 10 Feb 2005 14:53:32 -0800
Subject: [R] sample
Message-ID: <420BE5EC.8090903@sdsc.edu>

Just to explain my previous mail, here is the output I get.  


 > dim(tissue.exp)
[1] 1532   20
 > pick<-sample(tissue.exp,5,replace=TRUE)
 > dim(pick)
[1] 1532    5
 > tissue.exp.t<-t(tissue.exp)
 > dim(tissue.exp.t)
[1]   20 1532
 > pick<-sample(tissue.exp.t,5,replace=TRUE)
 > dim(pick)
NULL

--------
Thanks ../Murli



From ggrothendieck at myway.com  Fri Feb 11 01:02:25 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 11 Feb 2005 00:02:25 +0000 (UTC)
Subject: [R] Problem with "R CMD Rd2dvi": Rd.sty not found
References: <9CC1B717EF3BD511AD98000103D63FC53FA754@us-arl-asg.mail.saic.com>
	<Pine.LNX.4.61.0502102220560.31066@gannet.stats>
Message-ID: <loom.20050211T005104-323@post.gmane.org>


Prof Brian Ripley <ripley <at> stats.ox.ac.uk> writes:

: 
: This is a problem with a recent-ish change to MikTeX to make it use 
: non-standard paths: see the notes on Duncan Murdoch's site at
: 
:    http://www.murdoch-sutherland.com/Rtools/miktex.html
: 
: Fptex is a standard distribution e.g. from TexLive.

I initially started with MikTeX but then had the texinput problem
while trying to generate vignettes for a package so
tried fptex but found that the latter was huge (the .bat file
downloads about half a gig of files) and Miktex seems more polished.  
The most important shortcoming is that fptex lacks texi2dvi and while
Miktex lacks this too it has a substitute, texify.exe, which can
be used instead whereas fptex does not.  The Miktex developer told
me that the underlying tex engine he is using is missing the texinput
part and as soon as that is corrected he will incorporate it
into miktex.  This was quite a while ago and the situation 
may have changed somewhat.  I think some people have had good expeiences
with fptex but they probably were not trying to develop packages with
vignettes and certain other tasks that involve texinput.  I think had 
I known all this I just would have stuck to miktex and done the extra
configuration and saved myself having to download the huge fptex 
distribution.



From ggrothendieck at myway.com  Fri Feb 11 01:09:31 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 11 Feb 2005 00:09:31 +0000 (UTC)
Subject: [R] rats in survival package
References: <5FF3F11444E3A9439191AA1EDCB69A17089320@icosrvmail01.ICO.SCS.local>
Message-ID: <loom.20050211T010530-730@post.gmane.org>

Gonzalez Ruiz, Juan Ramon <jrgonzalez <at> ico.scs.es> writes:

: 
: Dear R-listers,
: 
: Does anybody know what is the correct source of "rats" dataset in survival 
package?
: 
: The help gives the following information:
: 
: Rat data from survival5
: Description:
:      48 rats were injected with a carcinogen, and then randomized to
:      either drug or placebo.  The number of tumors ranges from 0 to 13;
:      all rats were  censored at 6 months after randomization.
: Usage:
:      data(rats)
: Format:
:        rat:          id
:        rx:           treatment,(1=drug, 0=control)
:        observation:  within rat
:        start:        entry time
:        stop:         exit time
:        status:       event status
: Source:
:      Gail, Sautner and Brown, Biometrics 36, 255-66, 1980
: 
: but "rats" data.frame contains the variables:
: 
: names(rats)
: [1] "litter" "rx"     "time"   "status"
: 
: It seems to be 150 rats grouped in 50 litters, and 1 mouse is treated at 
each one. So this dataset does not agree
: with the description above mentioned. 
: 
: I am very interesting in knowing the reference of this data.
: 
: PS/ BTW, the correct source of  rats injected with a carcinogen is Gail, 
Santner and Brown,...
: 

Not sure what your question is but the rats data set is also in
the mimR package and the ?rats help there may be of use.

library(mimR)
data(rats)
?rats



From jrclmilks at joimail.com  Fri Feb 11 03:02:21 2005
From: jrclmilks at joimail.com (Jim and Chana Milks)
Date: Thu, 10 Feb 2005 21:02:21 -0500
Subject: [R] Saving graphs in formats other than PDF?
Message-ID: <BE317C5D.8C8%jrclmilks@joimail.com>

I am running R 2.01 on Mac OS 10.3.7.  Whenever I save graphs, they are
saved as PDF files.  Are there any other file formats to which I could save
my graphs.  I would like to directly export my graphs to MS Word, if
possible.

Thank you in advance.

Sincerely,
Jim Milks
Graduate Student
Environmental Sciences Ph.D. Program
Wright State University
3640 Colonel Glenn Hwy
Dayton, OH 45435



From spencer.graves at pdf.com  Fri Feb 11 03:08:23 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 10 Feb 2005 18:08:23 -0800
Subject: [R] time series questions? 
Message-ID: <420C1397.50606@pdf.com>

      Two time series questions: 

FITTING TRANSFER FUNCTIONS WITH LAGS:  Consider the following toy example: 

 > dates <- paste(11:21, "/01/2005", sep="")
 > Dates <- as.Date(dates, "%d/%m/%Y")
 > set.seed(1)
 > DF <- data.frame(date=Dates, y=rnorm(11), x=rnorm(11, 3))
 > arima(DF$y, c(1,0,0), xreg=lag(DF$x, 1))
          ar1  intercept  lag(DF$x, 1)
      -0.3876    -1.1328        0.4280
s.e.   0.3556     0.6417        0.1945
sigma^2 estimated as 0.3807:  log likelihood = -10.38,  aic = 28.76
 > arima(DF$y, c(1,0,0), xreg=lag(DF$x, 2))
          ar1  intercept  lag(DF$x, 2)
      -0.3876    -1.1328        0.4280
s.e.   0.3556     0.6417        0.1945
sigma^2 estimated as 0.3807:  log likelihood = -10.38,  aic = 28.76

****I NAIVELY THOUGHT THAT "lag" WOULD DO SOMETHING HERE.  Evidently, it 
didn't. 
****The following seems to work: 

 > arima(DF$y, c(1,0,0), xreg=c(DF$x[-1], NA))
          ar1  intercept  c(DF$x[-1], NA)
      -0.3943    -0.2155           0.1185
s.e.   0.3454     0.8024           0.2464
sigma^2 estimated as 0.4889:  log likelihood = -10.7,  aic = 29.39
 > arima(DF$y, c(1,0,0), xreg=c(DF$x[-(1:2)], NA, NA))
          ar1  intercept  c(DF$x[-(1:2)], NA, NA)
      -0.2385    -0.6430                   0.2472
s.e.   0.3073     0.8592                   0.2485
sigma^2 estimated as 0.491:  log likelihood = -9.6,  aic = 27.2

      Is there a better way? 

ASSOCIATING A CALENDAR DATE WITH A 'ts' OBJECT

      In the previous example, I'd like to convert x and y into "ts" 
objects, retaining "Dates".  Is there a way to do this?  The following 
did not work: 

 > ts(DF$y, start=Dates[1])
Error in Math.difftime((end - start) * frequency + 1.01) :
    floor not defined for difftime objects

      Thanks,
      spencer graves



From cuiczhao at yahoo.com  Fri Feb 11 03:10:19 2005
From: cuiczhao at yahoo.com (Cuichang Zhao)
Date: Thu, 10 Feb 2005 18:10:19 -0800 (PST)
Subject: [R] table in R
Message-ID: <20050211021019.33477.qmail@web30704.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050210/021da436/attachment.pl

From spencer.graves at pdf.com  Fri Feb 11 03:18:19 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 10 Feb 2005 18:18:19 -0800
Subject: [R] time series questions (corrected)?
Message-ID: <420C15EB.4050209@pdf.com>

## The following corrects what I believe to have been an error in my 
previous post:
####
      Two time series questions:

I.  FITTING TRANSFER FUNCTIONS WITH LAGS:  Consider the following toy 
example:

> dates <- paste(11:21, "/01/2005", sep="")
> Dates <- as.Date(dates, "%d/%m/%Y")
> set.seed(1)
> DF <- data.frame(date=Dates, y=rnorm(11), x=rnorm(11, 3))
> arima(DF$y, c(1,0,0), xreg=lag(DF$x, 1))
          ar1  intercept  lag(DF$x, 1)
      -0.3876    -1.1328        0.4280
s.e.   0.3556     0.6417        0.1945
sigma^2 estimated as 0.3807:  log likelihood = -10.38,  aic = 28.76
> arima(DF$y, c(1,0,0), xreg=lag(DF$x, 2))
          ar1  intercept  lag(DF$x, 2)
      -0.3876    -1.1328        0.4280
s.e.   0.3556     0.6417        0.1945
sigma^2 estimated as 0.3807:  log likelihood = -10.38,  aic = 28.76

****I NAIVELY THOUGHT THAT "lag" WOULD DO SOMETHING HERE.  Evidently, it
didn't.
****The following seems to work:

 > arima(DF$y, c(1,0,0), xreg=c(NA, DF$x[-11]))
           ar1  intercept  c(NA, DF$x[-11])
       -0.3994     0.3837           -0.0215
s.e.   0.3107     0.7366            0.2259
sigma^2 estimated as 0.5356:  log likelihood = -11.15,  aic = 30.31
 > arima(DF$y, c(1,0,0), xreg=c(NA, NA, DF$x[-(10:11)]))
           ar1  intercept  c(NA, NA, DF$x[-(10:11)])
       -0.5703     0.9180                    -0.1794
s.e.   0.3374     0.6685                     0.2115
sigma^2 estimated as 0.4871:  log likelihood = -9.73,  aic = 27.46

      Is there a better way?

II.  ASSOCIATING A CALENDAR DATE WITH A 'ts' OBJECT

      In the previous example, I'd like to convert x and y into "ts"
objects, retaining "Dates".  Is there a way to do this?  The following
did not work:

> ts(DF$y, start=Dates[1])
Error in Math.difftime((end - start) * frequency + 1.01) :
    floor not defined for difftime objects

      Thanks,
      spencer graves



From phdhwang at gmail.com  Fri Feb 11 06:33:06 2005
From: phdhwang at gmail.com (Kum-Hoe Hwang)
Date: Fri, 11 Feb 2005 14:33:06 +0900
Subject: [R] How to solve error : "cannot allocate vector of size 1208235 Kb"
Message-ID: <b040cbb005021021337d7aa621@mail.gmail.com>

Howdy R gurus !

I am newbie to R
I use R 2.0.1 in Windows XP. When I run R
I got the follwoing memory error.
My physical memory size is 3 Gb.
My R got the memory problem when it reached to 
about 2 Gb.

Thanks in advance,


> library(spdep)
> sfr.lagsarlm <- lagsarlm(sfr.data$Bldgsqft ~ sfr.data$Ncounty + sfr.data$Nugb + sfr.data$Ngroup, data=sfr.data, listw=sfr.listw, method="eigen") 
Error: cannot allocate vector of size 1208235 Kb
> memory.size(max=FALSE)
[1] 17862584
> memory.limit(size=NA)
[1] 3145728000
> 


-- 
Kum-Hoe Hwang, Ph.D.

Kyonggi Research Institute, Korea (ROK)
(Urban Planning and GIS)
Phone : 82-31-250-3283
Email : phdhwang at gmail.com



From pcj1 at nyu.edu  Fri Feb 11 07:06:39 2005
From: pcj1 at nyu.edu (Peter Jaffe)
Date: Fri, 11 Feb 2005 01:06:39 -0500
Subject: [R] importing minitab datasets
Message-ID: <a031b01a200fb16a3c94e5076c468590@nyu.edu>

I'm having trouble using the read.mtp function in the foreign package 
to import datasets from minitab (.mtw) format. Specifically, each file 
I try to import fails to load any data beyond a header row stating the 
version of Minitab that saved the dataset. I get this error:

incomplete final line found by readtableHeader on 'income.mtw'

The dataset appears to be complete (ie, using the scan() function I can 
view the contents, and the file does not end after the header, but the 
formatting characters make it impossible to guess what the raw data 
elements are). I got the same result trying to read minitab files from 
two different sources. Any suggestions would be appreciated.

Thanks,

Peter



From ripley at stats.ox.ac.uk  Fri Feb 11 08:03:30 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 11 Feb 2005 07:03:30 +0000 (GMT)
Subject: [R] How to solve error : "cannot allocate vector of size 1208235
	Kb"
In-Reply-To: <b040cbb005021021337d7aa621@mail.gmail.com>
References: <b040cbb005021021337d7aa621@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0502110659330.4032@gannet.stats>

You need to read the rw-FAQ (as the posting guide asks) and make the R 
executable capable of using more than 2Gb (assuming that you have already 
enabled your OS).

On Fri, 11 Feb 2005, Kum-Hoe Hwang wrote:

> Howdy R gurus !
>
> I am newbie to R
> I use R 2.0.1 in Windows XP. When I run R
> I got the follwoing memory error.
> My physical memory size is 3 Gb.
> My R got the memory problem when it reached to
> about 2 Gb.
>
> Thanks in advance,
>
>
>> library(spdep)
>> sfr.lagsarlm <- lagsarlm(sfr.data$Bldgsqft ~ sfr.data$Ncounty + sfr.data$Nugb + sfr.data$Ngroup, data=sfr.data, listw=sfr.listw, method="eigen")
> Error: cannot allocate vector of size 1208235 Kb
>> memory.size(max=FALSE)
> [1] 17862584

That tells you the state after that command failed: the max=TRUE figure is
more interesting at that point.

>> memory.limit(size=NA)
> [1] 3145728000
>>
>
>
> -- 
> Kum-Hoe Hwang, Ph.D.
>
> Kyonggi Research Institute, Korea (ROK)
> (Urban Planning and GIS)
> Phone : 82-31-250-3283
> Email : phdhwang at gmail.com
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From petr.pikal at precheza.cz  Fri Feb 11 08:21:36 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 11 Feb 2005 08:21:36 +0100
Subject: [R] table in R
In-Reply-To: <20050211021019.33477.qmail@web30704.mail.mud.yahoo.com>
Message-ID: <420C6B10.24614.27574A@localhost>

Hi

On 10 Feb 2005 at 18:10, Cuichang Zhao wrote:

> Hello, 
> I want to build some tables in my project using R, does R have some
> tables form that I can use? if i use the internal R table, how can
> initial a table before I use it?

How do you want to use a table???

> 
> I want to my tables to have some columns, for example:
> (trial, x, y, goal) I want to put these columns into my tables, for i
> want to put data into each entry one by one. I have no idea about how
> big the table would be, so will it possible to use R's internal table?

Do you mean build in data editor, which can be used for editing 
existing objects???

> 
> Also, can someone please recommand me a good book or a good website
> about R.

If you installed R you can find useful documentation in .../doc 
directory. I can also recommend Introductory statistics with R or 
Modern applied statistics with S.

And of course all stuff you can find on CRAN.

BTW, try to read the last lines of the email about ***posting 
guide*** and of course the posting guide itself.

Cheers
Petr



> 
> 
> Thank you so much.
> 
> C-Ming
> 
> Feb 10, 2005
> 
> 
> ---------------------------------
> 
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From imosqueira at suk.azti.es  Fri Feb 11 09:29:03 2005
From: imosqueira at suk.azti.es (Iago Mosqueira)
Date: Fri, 11 Feb 2005 08:29:03 +0000
Subject: [R] Subsetting using dimnames on S4 array-based class
Message-ID: <1108069743.4734.121.camel@xurelo.azti.local>

Hello,

I am encountering some problems when overloading the "[" operator for a
new S4 class based on array. This is an example class definition:

setClass("foo",
	representation("array"),
	prototype(array(NA, dim=c(3,3)), 
        dimnames=list(age=1:3, year=10:12))
)

And this the corresponding setMethod with print estatements to see what
is being passed:

setMethod("[", signature(x="foo"),
    function(x, i="missing", j="missing", ..., drop="missing") {
        print(paste("i:", i))
        print(paste("j:", j))
     }
)


So I first create a new object and load it with some data:

> x <- new("foo")
> x[,] <- 1:9

And then apply subsetting without using the dimension names and see what
are the values of i and j inside the function:

> x[1:2,'10']
[1] "i: 1" "i: 2"
[1] "j: 10"


Both i and j hold exactly what was expected here. But if I use the
dimension names, the subsetting indices does not seem to be passed as I
expected:

> x[age=1:3, year=1:3]
[1] "i: missing"
[1] "j: missing"
> x[, year='10']
[1] "i: missing"
[1] "j: missing"

Subsetting with dimnames appears to work without trouble on an array,
which "foo" extends:

s<-array(1:9,dim=c(3,3),dimnames=list(age=1:3,year=1:3))
> s[1,2:3]
2 3 
4 7 
> s[age=1,year=2:3]
2 3 
4 7

Although dimnames seem to be in fact simply ignored:

> s[a=1,b=3]
[1] 7


System:
Linux Debian 3.0
R 2.0.0

Do I need to define my class differently for subsetting using dimnames
to work? Even if they are not really being checked, I would like to be
able to use subsetting in this way as it makes code more readable when
using arrays with many dimensions.

Many thanks,


Iago Mosqueira



From petr.pikal at precheza.cz  Fri Feb 11 08:32:46 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 11 Feb 2005 08:32:46 +0100
Subject: [R] importing minitab datasets
In-Reply-To: <a031b01a200fb16a3c94e5076c468590@nyu.edu>
Message-ID: <420C6DAE.24784.319116@localhost>

Hi Peter

On 11 Feb 2005 at 1:06, Peter Jaffe wrote:

> I'm having trouble using the read.mtp function in the foreign package
> to import datasets from minitab (.mtw) format. Specifically, each file
> I try to import fails to load any data beyond a header row stating the
> version of Minitab that saved the dataset. I get this error:
> 
> incomplete final line found by readtableHeader on 'income.mtw'
> 
> The dataset appears to be complete (ie, using the scan() function I
> can view the contents, and the file does not end after the header, but
> the formatting characters make it impossible to guess what the raw
> data elements are). I got the same result trying to read minitab files
> from two different sources. Any suggestions would be appreciated.
>

I did not do the transfer yet but quick look into help page:


file: 

character variable with the name of the file to read. The file must 
be in Minitab Portable Worksheet format. 
	   ^^^^^^^^^^^^^^^^^^^^^^

so first you need to save a minitab file in correct format. If you 
did it it would work as expected -> it will give you a list.


Cheers
Petr


 
> Thanks,
> 
> Peter
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From ligges at statistik.uni-dortmund.de  Fri Feb 11 08:44:46 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 11 Feb 2005 08:44:46 +0100
Subject: [R] Saving graphs in formats other than PDF?
In-Reply-To: <BE317C5D.8C8%jrclmilks@joimail.com>
References: <BE317C5D.8C8%jrclmilks@joimail.com>
Message-ID: <420C626E.4050401@statistik.uni-dortmund.de>

Jim and Chana Milks wrote:
> I am running R 2.01 on Mac OS 10.3.7.  Whenever I save graphs, they are
> saved as PDF files.  Are there any other file formats to which I could save
> my graphs.  I would like to directly export my graphs to MS Word, if
> possible.
> 
> Thank you in advance.
> 
> Sincerely,
> Jim Milks
> Graduate Student
> Environmental Sciences Ph.D. Program
> Wright State University
> 3640 Colonel Glenn Hwy
> Dayton, OH 45435
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


See ?Devices.

Uwe Ligges



From ligges at statistik.uni-dortmund.de  Fri Feb 11 08:54:22 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 11 Feb 2005 08:54:22 +0100
Subject: [R] sample
In-Reply-To: <420BD799.40700@sdsc.edu>
References: <420BD799.40700@sdsc.edu>
Message-ID: <420C64AE.3000500@statistik.uni-dortmund.de>

T. Murlidharan Nair wrote:

> I am trying to sample a subset from a matrix using sample.
> The size of the matrix is 20X 1532. It works fine with this,
> but when I transpose the matrix and try to sample it, it returns
> null.  pick.set<-sample(tissue.exp.t,5,replace=FALSE,prob=NULL)
 >
> Is there something that I am missing here ?  Thanks ../Murli

Yes: tissue.exp.t is not a matrix.
Check it with, e.g.,  str().

Uwe Ligges

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri Feb 11 09:06:59 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 11 Feb 2005 09:06:59 +0100
Subject: [R] skip missing values in plots
In-Reply-To: <6.1.0.6.2.20050210133319.032cc850@yellow.ucdavis.edu>
References: <6.1.0.6.2.20050210133319.032cc850@yellow.ucdavis.edu>
Message-ID: <420C67A3.3050602@statistik.uni-dortmund.de>

Dean Sonneborn wrote:

>          I really like these Trellis graphics but how do I get this code to 
> skip the missing?


What is missing here? Some precision in your question, perhaps?
We do not have logreg.csv, so please tell us what is going on, or even 
better specify a minimal reproducible example, as the posting guide 
suggests.


> logreg<-read.csv("logreg.csv", header=TRUE, sep=",", na.string=" ")
> attach(logreg)

Why do you attach()?

Uwe Ligges


> bwplot(yesno~bc_pcb_tot |varlist, data=logreg, main="Box Cox PCB 
> transformation",  auto.key=TRUE, fontfamily = "HersheySans" )
> 
> 
> Dean Sonneborn M.S.
> Public Health Sciences *
> University of California, Davis
> 916 734-6656
> 
> * formerly Epidemiology & Preventive Medicine
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From vito_ricci at yahoo.com  Fri Feb 11 09:29:57 2005
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Fri, 11 Feb 2005 09:29:57 +0100 (CET)
Subject: [R] Re: testing slopes different than a given value
Message-ID: <20050211082957.72354.qmail@web41202.mail.yahoo.com>

Hi,

We know that a regression coefficent fitted by sample
data (under usual linear model hypothesis) b_hat has
mean=b and se=se(b_hat); (b_hat-b)/s(b_hat) is
distributed as Student?s t distribution with df=n-2.
So you can test h0:b=b0 hA:b<>b0 using t test (for
large sample normal distribution is the same of a t
distribution):

x1<-rnorm(100)
x2<-rnorm(100)
e<-rnorm(100)
y<-3+0.6*x1+0.3*x2 +e
fm<-lm(y~x1+x2)

> summary(fm)

Call:
lm(formula = y ~ x1 + x2)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.17610 -0.65146 -0.09532  0.54848  2.41966 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  3.04924    0.09661  31.562  < 2e-16 ***
x1           0.55124    0.09930   5.551 2.47e-07 ***
x2           0.23477    0.10534   2.229   0.0281 *  
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.'
0.1 ` ' 1 

Residual standard error: 0.9492 on 97 degrees of
freedom
Multiple R-Squared: 0.2687,     Adjusted R-squared:
0.2536 
F-statistic: 17.82 on 2 and 97 DF,  p-value: 2.561e-07
> b<-coef(fm)
> b
(Intercept)          x1          x2 
  3.0492374   0.5512398   0.2347682
you get b_hat standard errors from summary(fm):

se<-c(0.09661,0.09930,0.10534)
> se
[1] 0.09661 0.09930 0.10534

ttest<-(b[2]-0.6)/se[2]

> ttest
        x1 
-0.4910391
> 1-pt(ttest,df=97) ##p-value, as df is high we can
use normal distribution
      x1 
0.687746 

we accept h0 :b1=0.6;

Hoping I helped you.
Best regards,
Vito

You wrote:
In a multiple linear regression with two independent
variables is there any function in R to test for the
coefficients being different than some given values?
Example:
x1<-rnorm(100)
x2<-rnorm(100)
y<-3+0.6*x1+0.3*x2 
fm<-lm(y~x1+x2)
Obtain a test for the coefficients for x1 being 
different than 0.6 and for x2 different than 0.3
Thanks
Manuel



=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box

Top 10 reasons to become a Statistician

     1. Deviation is considered normal
     2. We feel complete and sufficient
     3. We are 'mean' lovers
     4. Statisticians do it discretely and continuously
     5. We are right 95% of the time
     6. We can legally comment on someone's posterior distribution
     7. We may not be normal, but we are transformable
     8. We never have to say we are certain
     9. We are honestly significantly different
    10. No one wants our jobs


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/



From fengqiu1984 at yahoo.com  Fri Feb 11 10:01:06 2005
From: fengqiu1984 at yahoo.com (Feng Qiu)
Date: Fri, 11 Feb 2005 01:01:06 -0800 (PST)
Subject: [R] Help concerning Lasso::l1ce
Message-ID: <20050211090106.29573.qmail@web30504.mail.mud.yahoo.com>

Hi, 
First, when I try the example Prostate with bound 0.44
(as in the manual), I got a different result:

> l1c.P <- l1ce(lpsa ~ ., Prostate, bound=0.44)
> l1c.P
....
Coefficients:
(Intercept)      lcavol     lweight         age       
lbph         svi
  1.0435803   0.4740831   0.1953156   0.0000000  
0.0000000   0.3758199
        lcp     gleason       pgg45
  0.0000000   0.0000000   0.0000000
 The relative L1 bound was      : 0.44
The absolute L1 bound was      : 0.8113534
The Lagrangian for the bound is:  17.89198

And the sum of the absolute values of the coefficients
are larger than the bound!! Why?

Second, the manual says that if I set sweep.out to
NULL, the constant term (which is the intercept,
right?)
will be included in the bound, but when I do so, I get
an error "Matrix build from transformed variables has
a constant column". Why?
And how can I constrain the constant term?

Finally, could anyone explain to me the difference
between relative bound and absolute bound? Which is
the actually bound adopted?

    Im new to R and the lasso2 package, please help
me out, thank you very much.



From davidhughjones at gmail.com  Fri Feb 11 11:23:56 2005
From: davidhughjones at gmail.com (David Hugh-Jones)
Date: Fri, 11 Feb 2005 10:23:56 +0000
Subject: [R] correcting for autocorrelation in models with panel data?
In-Reply-To: <f5d8480605021014162da42c13@mail.gmail.com>
References: <88EAF3512A55DF46B06B1954AEF73F7407B1033A@dc1ex2.air.org>
	<f5d8480605021014162da42c13@mail.gmail.com>
Message-ID: <f5d8480605021102236a6a36c6@mail.gmail.com>

Another question - is there a way to use autocorrelation with OLS,
rather than GLS?
I am really blindly following Beck and Katz (1995) here, and they
recommend OLS rather than "Feasible Generalized Least Squares" for
panel data where the number of individuals is larger than the number
of time units, which is my case.

Cheers
David

> On Thu, 10 Feb 2005 12:36:32 -0500, Doran, Harold <HDoran at air.org> wrote:
> > In the nlme package you can find the gls() function to account for
> > autocorrelation over time using corAR1. Syntax might look something like
> > this:
> >
> > fm1 <- gls(response ~ IV, long, correlation=corAR1(form=~1|ID),
> > method='ML')
> >
> > You can also use weights() for heteroscedasticity.
> >
> > -Harold
> >
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of David Hugh-Jones
> > Sent: Thursday, February 10, 2005 12:15 PM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] correcting for autocorrelation in models with panel data?
> >
> > Hi
> >
> > I have some panel data for the 50 US states over about 25 years, and I
> > would like to test a simple model via OLS, using this data. I know how
> > to run OLS in R, and I think I can see how to  create Panel Corrected
> > Standard Errors using
> >
> > http://jackman.stanford.edu/classes/350C/pcse.r
> >
> > What I can't figure out is how to correct for autocorrelation over time.
> > I have found a lot of R stuff on time series models but they all seem
> > focused on predicting a single variable from its previous values.
> > Can anyone explain to me how to detect and get round autocorrelation?
> > Is there a package for panel data that I have missed?
> >
> > I appreciate that this is probably just as much about my ignorance of
> > econometrics as about R itself!
> >
> > Cheers
> > David
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
>



From ramasamy at cancer.org.uk  Fri Feb 11 11:24:04 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Fri, 11 Feb 2005 10:24:04 +0000
Subject: [R] sample
In-Reply-To: <420BE5EC.8090903@sdsc.edu>
References: <420BE5EC.8090903@sdsc.edu>
Message-ID: <1108117444.5855.6.camel@ndmpc126.orc.ox.ac.uk>

See below.

On Thu, 2005-02-10 at 14:53 -0800, T. Murlidharan Nair wrote:
> Just to explain my previous mail, here is the output I get.  
> 
> 
>  > dim(tissue.exp)
> [1] 1532   20

What is this object ? Try class(tissue.exp)

>  > pick<-sample(tissue.exp,5,replace=TRUE)
>  > dim(pick)
> [1] 1532    5

>From help(sample) :

    sample(x, size, replace = FALSE, prob = NULL)

    x: Either a (numeric, complex, character or logical) vector of
       more than one element from which to choose, or a positive
       integer.

x has to be a vector or positive integer integer. I am not sure how you
got "sample" to work on tissue.exp in the first place. Maybe you got
your own version of "sample".


>  > tissue.exp.t<-t(tissue.exp)
>  > dim(tissue.exp.t)
> [1]   20 1532
>  > pick<-sample(tissue.exp.t,5,replace=TRUE)
>  > dim(pick)
> NULL
> 
> --------
> Thanks ../Murli
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From michael.watson at bbsrc.ac.uk  Fri Feb 11 12:09:49 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Fri, 11 Feb 2005 11:09:49 -0000
Subject: [R] Setting x-axis labels in plot()
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950121BA2E@iahce2knas1.iah.bbsrc.reserved>

Hi
 
This draws the (kind of) plot I want:
 
plot(InsectSprays[,2:1])
 
But how do I make it label the x-axis with A, B, C, D etc instead of 1:6?
 
Mick



From HDoran at air.org  Fri Feb 11 12:12:30 2005
From: HDoran at air.org (Doran, Harold)
Date: Fri, 11 Feb 2005 06:12:30 -0500
Subject: [R] correcting for autocorrelation in models with panel data?
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7404044E73@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050211/b704082c/attachment.pl

From vikas at mail.jnu.ac.in  Fri Feb 11 12:10:27 2005
From: vikas at mail.jnu.ac.in (Vikas Rawal)
Date: Fri, 11 Feb 2005 16:40:27 +0530
Subject: [R] Emacs and ESS installation issues in Windows
In-Reply-To: <200502101115.j1AB67c8006167@hypatia.math.ethz.ch>
References: <200502101115.j1AB67c8006167@hypatia.math.ethz.ch>
Message-ID: <420C92A3.1000502@mail.jnu.ac.in>


I have been using emacs+ESS for running R and have generally managed to 
install it on both linux and windows (98 as well as XP) machines. Today 
I was trying to install it on a friends machine which had Windows XP. 
But for some reason, I was unable to start an R process from within 
Emacs/ESS.

I got two different kinds of error when I tried to start R from the ESS 
menu. These said:

Variable binding depth exceeds max-specpdl-size

and

Lisp nesting exceeds max-lisp-eval-depth

Emacs was installed at c:\emacs and ess-5.2.4 was at c:\ess-5.2.5.

.emacs correctly identified the path to ess/lisp, and the path to R was 
correctly specified in the path variable in the "control 
panel/system/environment variables".

Can somebody help me with what might have gone wrong?

BTW, I have also had another installation problems on some of the win98 
machines. It does not find rterm.exe even when the path to R/bin is 
specified in the path. Is there some other way of defining the path to 
R. There must be some lisp file which one can modify. Can somebody 
enlighten me on this as well.

Thanks,

Vikas Rawal



From Roger.Bivand at nhh.no  Fri Feb 11 13:02:53 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 11 Feb 2005 13:02:53 +0100 (CET)
Subject: [R] How to solve error : "cannot allocate vector of size 1208235
	Kb"
In-Reply-To: <b040cbb005021021337d7aa621@mail.gmail.com>
Message-ID: <Pine.LNX.4.44.0502111249560.4087-100000@reclus.nhh.no>

On Fri, 11 Feb 2005, Kum-Hoe Hwang wrote:

> Howdy R gurus !
> 
> I am newbie to R
> I use R 2.0.1 in Windows XP. When I run R
> I got the follwoing memory error.
> My physical memory size is 3 Gb.
> My R got the memory problem when it reached to 
> about 2 Gb.

This is not a question just about memory but also about methods. The
number of rows in sfr.data is sufficiently large (about 35000?) that the
NxN matrices being used need a lot of room. That is why the function
provides the "SparseM" method to cater for your situation (unless you have
chosen a representation of neighbour weights that is not similar to
symmetric). 

If you choose a symmetric or similar to symmetric representation, you can
use the "SparseM" method, which, as its name suggests, uses sparse
matrices. All of this is described in help(lagsarlm). 

It is also quite possible that the last 95% of your data add almost no new
information, so you could also consider choosing a subset for analysis, in
which case the "eigen" method using dense matrices should work.

> 
> Thanks in advance,
> 
> 
> > library(spdep)
> > sfr.lagsarlm <- lagsarlm(sfr.data$Bldgsqft ~ sfr.data$Ncounty + sfr.data$Nugb + sfr.data$Ngroup, data=sfr.data, listw=sfr.listw, method="eigen") 
> Error: cannot allocate vector of size 1208235 Kb
> > memory.size(max=FALSE)
> [1] 17862584
> > memory.limit(size=NA)
> [1] 3145728000
> > 
> 
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From Luisr at frs.fo  Fri Feb 11 13:02:45 2005
From: Luisr at frs.fo (Luis Ridao Cruz)
Date: Fri, 11 Feb 2005 12:02:45 +0000
Subject: [R] Double sort a data frame
Message-ID: <s20c9ef0.004@ffdata.setur.fo>

R-help,

Long ago I asked how to sort a data frame according to one of the
columns.
But Iwas wondering ho I could "double sort" , i.e, sort according to
one column and another one afterwards.
It is quite easy in Excel but  I am unable to implemented in R.


I have searched in the R help archives and found nothing.


Thank you in advance.



From ligges at statistik.uni-dortmund.de  Fri Feb 11 13:15:44 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 11 Feb 2005 13:15:44 +0100
Subject: [R] Setting x-axis labels in plot()
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950121BA2E@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E950121BA2E@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <420CA1F0.1090709@statistik.uni-dortmund.de>

michael watson (IAH-C) wrote:

> Hi
>  
> This draws the (kind of) plot I want:
>  
> plot(InsectSprays[,2:1])
>  
> But how do I make it label the x-axis with A, B, C, D etc instead of 1:6?

Set xaxt="n" in your call to plot and add the axis() separately 
specifying the labels.

Uwe Ligges

> Mick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jfox at mcmaster.ca  Fri Feb 11 14:01:50 2005
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 11 Feb 2005 08:01:50 -0500
Subject: [R] Re: testing slopes different than a given value
In-Reply-To: <20050211082957.72354.qmail@web41202.mail.yahoo.com>
Message-ID: <20050211130141.RDOJ2034.tomts20-srv.bellnexxia.net@JohnDesktop8300>

Dear Vito,

Since Manuel says that he wants to "obtain a test" and not "obtain two
tests," I assume that he's interested in the F-test for the hypothesis that
both coefficients are simultaneously equal to the specified values rather
than in the t-tests for the individual hypotheses.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Vito Ricci
> Sent: Friday, February 11, 2005 3:30 AM
> To: r-help at stat.math.ethz.ch
> Cc: manuel_gutierrez_lopez at yahoo.es
> Subject: [R] Re: testing slopes different than a given value
> 
> Hi,
> 
> We know that a regression coefficent fitted by sample data 
> (under usual linear model hypothesis) b_hat has mean=b and 
> se=se(b_hat); (b_hat-b)/s(b_hat) is distributed as Student's 
> t distribution with df=n-2.
> So you can test h0:b=b0 hA:b<>b0 using t test (for large 
> sample normal distribution is the same of a t
> distribution):
> 
> x1<-rnorm(100)
> x2<-rnorm(100)
> e<-rnorm(100)
> y<-3+0.6*x1+0.3*x2 +e
> fm<-lm(y~x1+x2)
> 
> > summary(fm)
> 
> Call:
> lm(formula = y ~ x1 + x2)
> 
> Residuals:
>      Min       1Q   Median       3Q      Max 
> -2.17610 -0.65146 -0.09532  0.54848  2.41966 
> 
> Coefficients:
>             Estimate Std. Error t value Pr(>|t|)    
> (Intercept)  3.04924    0.09661  31.562  < 2e-16 ***
> x1           0.55124    0.09930   5.551 2.47e-07 ***
> x2           0.23477    0.10534   2.229   0.0281 *  
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.'
> 0.1 ` ' 1 
> 
> Residual standard error: 0.9492 on 97 degrees of freedom
> Multiple R-Squared: 0.2687,     Adjusted R-squared:
> 0.2536
> F-statistic: 17.82 on 2 and 97 DF,  p-value: 2.561e-07
> > b<-coef(fm)
> > b
> (Intercept)          x1          x2 
>   3.0492374   0.5512398   0.2347682
> you get b_hat standard errors from summary(fm):
> 
> se<-c(0.09661,0.09930,0.10534)
> > se
> [1] 0.09661 0.09930 0.10534
> 
> ttest<-(b[2]-0.6)/se[2]
> 
> > ttest
>         x1
> -0.4910391
> > 1-pt(ttest,df=97) ##p-value, as df is high we can
> use normal distribution
>       x1
> 0.687746 
> 
> we accept h0 :b1=0.6;
> 
> Hoping I helped you.
> Best regards,
> Vito
> 
> You wrote:
> In a multiple linear regression with two independent 
> variables is there any function in R to test for the 
> coefficients being different than some given values?
> Example:
> x1<-rnorm(100)
> x2<-rnorm(100)
> y<-3+0.6*x1+0.3*x2
> fm<-lm(y~x1+x2)
> Obtain a test for the coefficients for x1 being different 
> than 0.6 and for x2 different than 0.3 Thanks Manuel
> 
> 
>



From jeaneid at chass.utoronto.ca  Fri Feb 11 14:13:04 2005
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Fri, 11 Feb 2005 08:13:04 -0500
Subject: [R] sample
In-Reply-To: <420BE5EC.8090903@sdsc.edu>
Message-ID: <Pine.SGI.4.40.0502110808410.11097040-100000@origin.chass.utoronto.ca>

that is because your first object is a data.frame but when you transposed
it you turned it into a matrix. so doing

> mat1 <- data.frame(matrix(rnorm(20*1532), ncol=20))
> mat2 <- t(mat1)
> dim(sample(mat1, 5, replace=T))
[1] 1532    5
> dim(sample(mat2, 5, replace=T))
NULL
> length(sample(mat2, 5, replace=T))
[1] 5
> dim(sample(as.data.frame(mat2), 5, replace=T))
[1] 20  5


It seems that all you want is to pick columns at random so why don't you
just pick columns

> mat1 <- matrix(rnorm(20*1532), ncol=20)
> dim(mat1[, sample(1:20, 5, replace=T)])
[1] 1532    5
> dim(t(mat1)[, sample(1:1532, 5, replace=T)])
[1] 20  5



Jean





On Thu, 10 Feb 2005, T. Murlidharan Nair wrote:

> Just to explain my previous mail, here is the output I get.
>
>
>  > dim(tissue.exp)
> [1] 1532   20
>  > pick<-sample(tissue.exp,5,replace=TRUE)
>  > dim(pick)
> [1] 1532    5
>  > tissue.exp.t<-t(tissue.exp)
>  > dim(tissue.exp.t)
> [1]   20 1532
>  > pick<-sample(tissue.exp.t,5,replace=TRUE)
>  > dim(pick)
> NULL
>
> --------
> Thanks ../Murli
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From jacques.veslot at cirad.fr  Fri Feb 11 14:00:35 2005
From: jacques.veslot at cirad.fr (Jacques VESLOT)
Date: Fri, 11 Feb 2005 17:00:35 +0400
Subject: [R] Double sort a data frame
In-Reply-To: <s20c9ef0.004@ffdata.setur.fo>
Message-ID: <HHEDKBCGCMDOHEDELFBCGEPJCIAA.jacques.veslot@cirad.fr>


> class(df)
[1] data.frame

> names(df)
[1] "x" "y" "z"

df <- df[order(df$x, df$y),]		# sort df by x and y with priority to x



-----Message d'origine-----
De : r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]De la part de Luis Ridao Cruz
Envoye : vendredi 11 fevrier 2005 16:03
A : r-help at stat.math.ethz.ch
Objet : [R] Double sort a data frame


R-help,

Long ago I asked how to sort a data frame according to one of the
columns.
But Iwas wondering ho I could "double sort" , i.e, sort according to
one column and another one afterwards.
It is quite easy in Excel but  I am unable to implemented in R.


I have searched in the R help archives and found nothing.


Thank you in advance.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From jeaneid at chass.utoronto.ca  Fri Feb 11 14:14:45 2005
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Fri, 11 Feb 2005 08:14:45 -0500
Subject: [R] sample
In-Reply-To: <1108117444.5855.6.camel@ndmpc126.orc.ox.ac.uk>
Message-ID: <Pine.SGI.4.40.0502110813411.11097040-100000@origin.chass.utoronto.ca>

It seems that sample picks columns when the object is turned into a
data.frame. I do not knoe why it is doing that....

Is this something that was meant and not documented or something?


Jean


On Fri, 11 Feb 2005, Adaikalavan Ramasamy wrote:

> See below.
>
> On Thu, 2005-02-10 at 14:53 -0800, T. Murlidharan Nair wrote:
> > Just to explain my previous mail, here is the output I get.
> >
> >
> >  > dim(tissue.exp)
> > [1] 1532   20
>
> What is this object ? Try class(tissue.exp)
>
> >  > pick<-sample(tissue.exp,5,replace=TRUE)
> >  > dim(pick)
> > [1] 1532    5
>
> >From help(sample) :
>
>     sample(x, size, replace = FALSE, prob = NULL)
>
>     x: Either a (numeric, complex, character or logical) vector of
>        more than one element from which to choose, or a positive
>        integer.
>
> x has to be a vector or positive integer integer. I am not sure how you
> got "sample" to work on tissue.exp in the first place. Maybe you got
> your own version of "sample".
>
>
> >  > tissue.exp.t<-t(tissue.exp)
> >  > dim(tissue.exp.t)
> > [1]   20 1532
> >  > pick<-sample(tissue.exp.t,5,replace=TRUE)
> >  > dim(pick)
> > NULL
> >
> > --------
> > Thanks ../Murli
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From jeaneid at chass.utoronto.ca  Fri Feb 11 14:18:05 2005
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Fri, 11 Feb 2005 08:18:05 -0500
Subject: [R] Double sort a data frame
In-Reply-To: <s20c9ef0.004@ffdata.setur.fo>
Message-ID: <Pine.SGI.4.40.0502110818000.11097040-100000@origin.chass.utoronto.ca>

?order

On Fri, 11 Feb 2005, Luis Ridao Cruz wrote:

> R-help,
>
> Long ago I asked how to sort a data frame according to one of the
> columns.
> But Iwas wondering ho I could "double sort" , i.e, sort according to
> one column and another one afterwards.
> It is quite easy in Excel but  I am unable to implemented in R.
>
>
> I have searched in the R help archives and found nothing.
>
>
> Thank you in advance.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From jgoebel at diw.de  Fri Feb 11 14:09:40 2005
From: jgoebel at diw.de (Jan Goebel)
Date: Fri, 11 Feb 2005 14:09:40 +0100
Subject: [R] Double sort a data frame
In-Reply-To: <s20c9ef0.004@ffdata.setur.fo>
References: <s20c9ef0.004@ffdata.setur.fo>
Message-ID: <20050211130940.GA12845@diw138134.diw-berlin.de>

Hi,
you can use order, like

DATA <- DATA[order(DATA$V1, DATA$V2), ]

best
jan

On Fri, 11 Feb 2005, Luis Ridao Cruz wrote:

> R-help,
> 
> Long ago I asked how to sort a data frame according to one of the
> columns.
> But Iwas wondering ho I could "double sort" , i.e, sort according to
> one column and another one afterwards.
> It is quite easy in Excel but  I am unable to implemented in R.
> 
> 
> I have searched in the R help archives and found nothing.
> 
> 
> Thank you in advance.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
+-----------------------------------------
 Jan Goebel 
 j g o e b e l @ d i w . d e

 DIW Berlin 
 German Socio-Economic Panel Study (GSOEP) 
 K?nigin-Luise-Str. 5
 D-14195 Berlin -- Germany --
 phone: 49 30 89789-377
+-----------------------------------------



From andy_liaw at merck.com  Fri Feb 11 14:15:16 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 11 Feb 2005 08:15:16 -0500
Subject: [R] Double sort a data frame
Message-ID: <3A822319EB35174CA3714066D590DCD50994E6B5@usrymx25.merck.com>

> From: Luis Ridao Cruz
> 
> R-help,
> 
> Long ago I asked how to sort a data frame according to one of the
> columns.
> But Iwas wondering ho I could "double sort" , i.e, sort according to
> one column and another one afterwards.
> It is quite easy in Excel but  I am unable to implemented in R.

Just as easy in R, if not easier.  Something like:

dat[order(dat$col1, dat$col2), ]
 
> I have searched in the R help archives and found nothing.

This has been asked (and answered) numerous times on this list.  There was a
function posted for Kevin Wright that implements a formula interface for
sorting data frames.

Andy

> Thank you in advance.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From pcj1 at nyu.edu  Fri Feb 11 07:06:39 2005
From: pcj1 at nyu.edu (Peter Jaffe)
Date: Fri, 11 Feb 2005 01:06:39 -0500
Subject: [R] importing minitab datasets
Message-ID: <a031b01a200fb16a3c94e5076c468590@nyu.edu>

I'm having trouble using the read.mtp function in the foreign package 
to import datasets from minitab (.mtw) format. Specifically, each file 
I try to import fails to load any data beyond a header row stating the 
version of Minitab that saved the dataset. I get this error:

incomplete final line found by readtableHeader on 'income.mtw'

The dataset appears to be complete (ie, using the scan() function I can 
view the contents, and the file does not end after the header, but the 
formatting characters make it impossible to guess what the raw data 
elements are). I got the same result trying to read minitab files from 
two different sources. Any suggestions would be appreciated.

Thanks,

Peter



From carsten.steinhoff at stud.uni-goettingen.de  Fri Feb 11 14:27:21 2005
From: carsten.steinhoff at stud.uni-goettingen.de (Carsten Steinhoff)
Date: Fri, 11 Feb 2005 14:27:21 +0100
Subject: [R] function table
Message-ID: <E1CzapQ-0006ld-93@s2.stud.uni-goettingen.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050211/54335dd1/attachment.pl

From Stephanie.Tomczak at eleves.polytech-lille.fr  Fri Feb 11 14:40:20 2005
From: Stephanie.Tomczak at eleves.polytech-lille.fr (Stephanie.Tomczak@eleves.polytech-lille.fr)
Date: Fri, 11 Feb 2005 14:40:20 +0100
Subject: [R] pb with package mix
Message-ID: <1108129220.420cb5c4a23d3@webmail.polytech-lille.fr>

hello,

We've got a problem with the mix package in order to impute the missing data.
After importing the data, the prelim function does not work (only the stlouis
data works).

We have done :

>library(mix)
>Manq <- read.table("C:/.../file.txt")
>attach(Manq)
>save(Manq,file="C:/../R/rw2001/library/mix/data/Manq.rda")
>data(Manq)
>Manq
 V1 V2 V3 V4
1  1  1  1  1
2  1  1  3  6
3  1  2  6  2
...
...
52  2  7  6  2

>Essaimanq <-prelim.mix(Manq,4)
  Error in as.integer.default(list(V1 = c(1, 1,  1,  1,  1,  1, 1,  1,  1,  1,:
     (list) object cannot be coerced to integer   

thank you



From christoph.lehmann at gmx.ch  Fri Feb 11 14:44:18 2005
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Fri, 11 Feb 2005 14:44:18 +0100
Subject: [R] label outliers in boxplot and/or bwplot
Message-ID: <420CB6B2.7070907@gmx.ch>

Hi

Is there a way to lable (e.g. observation-number) the outliers in a boxplot?

and in a bwplot?

thanks a lot

Christoph


P.S. identify() is not available with bwplot, is it?



From andy_liaw at merck.com  Fri Feb 11 14:45:25 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 11 Feb 2005 08:45:25 -0500
Subject: [R] sample
Message-ID: <3A822319EB35174CA3714066D590DCD50994E6B7@usrymx25.merck.com>

?sample says:

x   Either a (numeric, complex, character or logical) vector of more
    than one element from which to choose, or a positive integer 

so I guess it wasn't meant to be used on a data frame.  However, a data
frame is a list (where the variables are the components), and a list is a
vector, so the behavior is consistent:

> x <- list("a", 2, "c", 4, "e", "f")
> sample(x)
[[1]]
[1] "c"

[[2]]
[1] "a"

[[3]]
[1] 4

[[4]]
[1] 2

[[5]]
[1] "f"

[[6]]
[1] "e"

Andy

> From: Jean Eid
> 
> It seems that sample picks columns when the object is turned into a
> data.frame. I do not knoe why it is doing that....
> 
> Is this something that was meant and not documented or something?
> 
> 
> Jean
> 
> 
> On Fri, 11 Feb 2005, Adaikalavan Ramasamy wrote:
> 
> > See below.
> >
> > On Thu, 2005-02-10 at 14:53 -0800, T. Murlidharan Nair wrote:
> > > Just to explain my previous mail, here is the output I get.
> > >
> > >
> > >  > dim(tissue.exp)
> > > [1] 1532   20
> >
> > What is this object ? Try class(tissue.exp)
> >
> > >  > pick<-sample(tissue.exp,5,replace=TRUE)
> > >  > dim(pick)
> > > [1] 1532    5
> >
> > >From help(sample) :
> >
> >     sample(x, size, replace = FALSE, prob = NULL)
> >
> >     x: Either a (numeric, complex, character or logical) vector of
> >        more than one element from which to choose, or a positive
> >        integer.
> >
> > x has to be a vector or positive integer integer. I am not 
> sure how you
> > got "sample" to work on tissue.exp in the first place. Maybe you got
> > your own version of "sample".
> >
> >
> > >  > tissue.exp.t<-t(tissue.exp)
> > >  > dim(tissue.exp.t)
> > > [1]   20 1532
> > >  > pick<-sample(tissue.exp.t,5,replace=TRUE)
> > >  > dim(pick)
> > > NULL
> > >
> > > --------
> > > Thanks ../Murli
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> > >
> >
> > 
> ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> >
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From p.dalgaard at biostat.ku.dk  Fri Feb 11 14:56:45 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 11 Feb 2005 14:56:45 +0100
Subject: [R] Re: testing slopes different than a given value
In-Reply-To: <20050211130141.RDOJ2034.tomts20-srv.bellnexxia.net@JohnDesktop8300>
References: <20050211130141.RDOJ2034.tomts20-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <x2vf8z19pe.fsf@biostat.ku.dk>

"John Fox" <jfox at mcmaster.ca> writes:

> Dear Vito,
> 
> Since Manuel says that he wants to "obtain a test" and not "obtain two
> tests," I assume that he's interested in the F-test for the hypothesis that
> both coefficients are simultaneously equal to the specified values rather
> than in the t-tests for the individual hypotheses.
> 
> Regards,
>  John

...in which case one answer is this:

>  y<-3+0.6*x1+0.3*x2 + rnorm(100,sd=.1) # as meant, no?
>  fm<-lm(y~x1+x2)
>  anova(fm, lm(y~offset(0.6*x1+0.3*x2)))
Analysis of Variance Table

Model 1: y ~ x1 + x2
Model 2: y ~ offset(0.6 * x1 + 0.3 * x2)
  Res.Df      RSS Df Sum of Sq      F Pr(>F)
1     97  1.06118
2     99  1.06184 -2  -0.00066 0.0302 0.9703



-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From cepardot at cable.net.co  Fri Feb 11 15:34:22 2005
From: cepardot at cable.net.co (=?iso-8859-1?Q?Campo_El=EDas_PARDO?=)
Date: Fri, 11 Feb 2005 09:34:22 -0500
Subject: [R] Profiles graphics in a contingency table
Message-ID: <007601c51046$c7b10730$a74176c8@PARDOBERNAL>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050211/f1562cbf/attachment.pl

From Luisr at frs.fo  Fri Feb 11 15:39:56 2005
From: Luisr at frs.fo (Luis Ridao Cruz)
Date: Fri, 11 Feb 2005 14:39:56 +0000
Subject: [R] Double sort a data frame
Message-ID: <s20cc3c9.054@ffdata.setur.fo>

You are right Andy,

I just found on the archives soon after I tried searching for a second
time.

Sorry

Luis

>>> "Liaw, Andy" <andy_liaw at merck.com> 11/02/2005 13:15:16 >>>
> From: Luis Ridao Cruz
> 
> R-help,
> 
> Long ago I asked how to sort a data frame according to one of the
> columns.
> But Iwas wondering ho I could "double sort" , i.e, sort according to
> one column and another one afterwards.
> It is quite easy in Excel but  I am unable to implemented in R.

Just as easy in R, if not easier.  Something like:

dat[order(dat$col1, dat$col2), ]
 
> I have searched in the R help archives and found nothing.

This has been asked (and answered) numerous times on this list.  There
was a
function posted for Kevin Wright that implements a formula interface
for
sorting data frames.

Andy

> Thank you in advance.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html 
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From jgoebel at diw.de  Fri Feb 11 15:51:14 2005
From: jgoebel at diw.de (Jan Goebel)
Date: Fri, 11 Feb 2005 15:51:14 +0100
Subject: [R] pb with package mix
In-Reply-To: <1108129220.420cb5c4a23d3@webmail.polytech-lille.fr>
References: <1108129220.420cb5c4a23d3@webmail.polytech-lille.fr>
Message-ID: <20050211145114.GA14309@diw138134.diw-berlin.de>

Hi,

i guess that prelim.mix needs a matrix rather than a data frame.

best

jan

On Fri, 11 Feb 2005, Stephanie.Tomczak at eleves.polytech-lille.fr wrote:

> hello,
> 
> We've got a problem with the mix package in order to impute the missing data.
> After importing the data, the prelim function does not work (only the stlouis
> data works).
> 
> We have done :
> 
> >library(mix)
> >Manq <- read.table("C:/.../file.txt")
> >attach(Manq)
> >save(Manq,file="C:/../R/rw2001/library/mix/data/Manq.rda")
> >data(Manq)
> >Manq
>  V1 V2 V3 V4
> 1  1  1  1  1
> 2  1  1  3  6
> 3  1  2  6  2
> ...
> ...
> 52  2  7  6  2
> 
> >Essaimanq <-prelim.mix(Manq,4)
>   Error in as.integer.default(list(V1 = c(1, 1,  1,  1,  1,  1, 1,  1,  1,  1,:
>      (list) object cannot be coerced to integer   
> 
> thank you
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
+-----------------------------------------
 Jan Goebel 
 j g o e b e l @ d i w . d e

 DIW Berlin 
 German Socio-Economic Panel Study (GSOEP) 
 K?nigin-Luise-Str. 5
 D-14195 Berlin -- Germany --
 phone: 49 30 89789-377
+-----------------------------------------



From cepardot at cable.net.co  Fri Feb 11 16:06:26 2005
From: cepardot at cable.net.co (=?iso-8859-1?Q?Campo_El=EDas_PARDO?=)
Date: Fri, 11 Feb 2005 10:06:26 -0500
Subject: [R] Profiles graphics in a contingency table
Message-ID: <008d01c5104b$4263b730$a74176c8@PARDOBERNAL>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050211/09cb833b/attachment.pl

From manuel_gutierrez_lopez at yahoo.es  Fri Feb 11 16:11:33 2005
From: manuel_gutierrez_lopez at yahoo.es (Manuel Gutierrez)
Date: Fri, 11 Feb 2005 16:11:33 +0100 (CET)
Subject: [R] Re: testing slopes different than a given value
In-Reply-To: <x2vf8z19pe.fsf@biostat.ku.dk>
Message-ID: <20050211151133.88939.qmail@web25101.mail.ukl.yahoo.com>

Thanks to all,
Yes, I meant a single test for both coefficients.
Peter's reply is what I wanted. I've tried with
linear.hypothesis but I must confess that with my
limited statistical experience and without the car
book at hand, the nomenclature for the function was a
bit difficult to understand for me. A toy example of
linear.hypothesis for my case would be great.
Thanks,
Manuel
--- Peter Dalgaard <p.dalgaard at biostat.ku.dk>
escribi?: 
> "John Fox" <jfox at mcmaster.ca> writes:
> 
> > Dear Vito,
> > 
> > Since Manuel says that he wants to "obtain a test"
> and not "obtain two
> > tests," I assume that he's interested in the
> F-test for the hypothesis that
> > both coefficients are simultaneously equal to the
> specified values rather
> > than in the t-tests for the individual hypotheses.
> > 
> > Regards,
> >  John
> 
> ...in which case one answer is this:
> 
> >  y<-3+0.6*x1+0.3*x2 + rnorm(100,sd=.1) # as meant,
> no?
> >  fm<-lm(y~x1+x2)
> >  anova(fm, lm(y~offset(0.6*x1+0.3*x2)))
> Analysis of Variance Table
> 
> Model 1: y ~ x1 + x2
> Model 2: y ~ offset(0.6 * x1 + 0.3 * x2)
>   Res.Df      RSS Df Sum of Sq      F Pr(>F)
> 1     97  1.06118
> 2     99  1.06184 -2  -0.00066 0.0302 0.9703
> 
> 
> 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej
> 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N 
>  
>  (*) \(*) -- University of Copenhagen   Denmark     
> Ph: (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)            
> FAX: (+45) 35327907
>



From tlumley at u.washington.edu  Fri Feb 11 16:13:50 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 11 Feb 2005 07:13:50 -0800 (PST)
Subject: [R] Saving graphs in formats other than PDF?
In-Reply-To: <420C626E.4050401@statistik.uni-dortmund.de>
References: <BE317C5D.8C8%jrclmilks@joimail.com>
	<420C626E.4050401@statistik.uni-dortmund.de>
Message-ID: <Pine.A41.4.61b.0502110710460.44302@homer07.u.washington.edu>

On Fri, 11 Feb 2005, Uwe Ligges wrote:
> Jim and Chana Milks wrote:
>> I am running R 2.01 on Mac OS 10.3.7.  Whenever I save graphs, they are
>> saved as PDF files.  Are there any other file formats to which I could save
>> my graphs.  I would like to directly export my graphs to MS Word, if
>> possible.
>
>
> See ?Devices.
>

It's not quite that simple.  There was some discussion on r-sig-mac 
earlier today.  It turns out to be quite hard to get good quality graphics 
into MS Office documents on the Mac.   The natural thing to use is PDF, 
but Office rasterises this, and not very well.

For Powerpoint it appears that the best solution that doesn't require 
other software may be png(), but for written documents that may be less 
satisfactory.


 	-thomas



From davidhughjones at gmail.com  Fri Feb 11 16:15:06 2005
From: davidhughjones at gmail.com (David Hugh-Jones)
Date: Fri, 11 Feb 2005 15:15:06 +0000
Subject: [R] correcting for autocorrelation in models with panel data?
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F7404044E73@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F7404044E73@dc1ex2.air.org>
Message-ID: <f5d8480605021107154a6ba5f2@mail.gmail.com>

Specifically the Beck and Katz article points out that "Feasible GLS",
which involves a special method for correcting standard errors for
panel data, doesn't work when time period is less than number of
individuals. They suggest using OLS, then correcting the standard
errors. But their method does not correct for autocorrelation over
time - only for heteroskedasticity and correlation between different
units at time t.

Cheers
David


On Fri, 11 Feb 2005 06:12:30 -0500, Doran, Harold <HDoran at air.org> wrote:
> No, by definition the off-diagonal elements in the covariance matrix for an OLS are 0. Thus, OLS is a special case of a GLS. You can see this if you write out the formulae for an OLS solution and GLS solution.
> 
> The typical solution for the standard errors in an OLS are (X'X)^{-1}*s^2. This is the same as (X'V^{-1}X)^{-1} when V= s^2*I, I being the identity matrix, which is also the gls solution.  But in the gls solution the off diagonal elements of V are covariances, not 0. In the case of autocorrelation (AR1) the off-diagonal elements decay exponentially over time.
> 
> I'm not familiar with Beck and Katz or why they would recommend that OLS be used when the number of time units is smaller than the number of individuals in the data. But to me, this seems rather silly, isn't this often the case?
> 
> HTH
> Harold
> 
>         -----Original Message-----
>         From: David Hugh-Jones [mailto:davidhughjones at gmail.com]
>         Sent: Fri 2/11/2005 5:23 AM
>         To: Doran, Harold
>         Cc: r-help at lists.r-project.org
>         Subject: Re: [R] correcting for autocorrelation in models with panel data?
> 
>         Another question - is there a way to use autocorrelation with OLS,
>         rather than GLS?
>         I am really blindly following Beck and Katz (1995) here, and they
>         recommend OLS rather than "Feasible Generalized Least Squares" for
>         panel data where the number of individuals is larger than the number
>         of time units, which is my case.
> 
>         Cheers
>         David
> 
>         > On Thu, 10 Feb 2005 12:36:32 -0500, Doran, Harold <HDoran at air.org> wrote:
>         > > In the nlme package you can find the gls() function to account for
>         > > autocorrelation over time using corAR1. Syntax might look something like
>         > > this:
>         > >
>         > > fm1 <- gls(response ~ IV, long, correlation=corAR1(form=~1|ID),
>         > > method='ML')
>         > >
>         > > You can also use weights() for heteroscedasticity.
>         > >
>         > > -Harold
>         > >
>         > > -----Original Message-----
>         > > From: r-help-bounces at stat.math.ethz.ch
>         > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of David Hugh-Jones
>         > > Sent: Thursday, February 10, 2005 12:15 PM
>         > > To: r-help at stat.math.ethz.ch
>         > > Subject: [R] correcting for autocorrelation in models with panel data?
>         > >
>         > > Hi
>         > >
>         > > I have some panel data for the 50 US states over about 25 years, and I
>         > > would like to test a simple model via OLS, using this data. I know how
>         > > to run OLS in R, and I think I can see how to  create Panel Corrected
>         > > Standard Errors using
>         > >
>         > > http://jackman.stanford.edu/classes/350C/pcse.r
>         > >
>         > > What I can't figure out is how to correct for autocorrelation over time.
>         > > I have found a lot of R stuff on time series models but they all seem
>         > > focused on predicting a single variable from its previous values.
>         > > Can anyone explain to me how to detect and get round autocorrelation?
>         > > Is there a package for panel data that I have missed?
>         > >
>         > > I appreciate that this is probably just as much about my ignorance of
>         > > econometrics as about R itself!
>         > >
>         > > Cheers
>         > > David
>         > >
>         > > ______________________________________________
>         > > R-help at stat.math.ethz.ch mailing list
>         > > https://stat.ethz.ch/mailman/listinfo/r-help
>         > > PLEASE do read the posting guide!
>         > > http://www.R-project.org/posting-guide.html
>         > >
>         > >
>         >
> 
> 
>



From ripley at stats.ox.ac.uk  Fri Feb 11 16:40:22 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 11 Feb 2005 15:40:22 +0000 (GMT)
Subject: [R] pb with package mix
In-Reply-To: <1108129220.420cb5c4a23d3@webmail.polytech-lille.fr>
References: <1108129220.420cb5c4a23d3@webmail.polytech-lille.fr>
Message-ID: <Pine.LNX.4.61.0502111537400.3779@gannet.stats>

See ?prelim.mix which says

        x: data matrix containing missing values. The rows of x

and you gave a data *frame*.

It is not prelim.mix that `does not work'.

On Fri, 11 Feb 2005 Stephanie.Tomczak at eleves.polytech-lille.fr wrote:

> hello,
>
> We've got a problem with the mix package in order to impute the missing data.
> After importing the data, the prelim function does not work (only the stlouis
> data works).
>
> We have done :
>
>> library(mix)
>> Manq <- read.table("C:/.../file.txt")
>> attach(Manq)
>> save(Manq,file="C:/../R/rw2001/library/mix/data/Manq.rda")
>> data(Manq)

Don't do this -- it's a package, not a play area.

>> Manq
> V1 V2 V3 V4
> 1  1  1  1  1
> 2  1  1  3  6
> 3  1  2  6  2
> ...
> ...
> 52  2  7  6  2
>
>> Essaimanq <-prelim.mix(Manq,4)
>  Error in as.integer.default(list(V1 = c(1, 1,  1,  1,  1,  1, 1,  1,  1,  1,:
>     (list) object cannot be coerced to integer
>
> thank you


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jfox at mcmaster.ca  Fri Feb 11 17:05:38 2005
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 11 Feb 2005 11:05:38 -0500
Subject: [R] Re: testing slopes different than a given value
In-Reply-To: <x2vf8z19pe.fsf@biostat.ku.dk>
Message-ID: <20050211160529.GUKR1899.tomts13-srv.bellnexxia.net@JohnDesktop8300>

Dear Peter,

> -----Original Message-----
> From: Peter Dalgaard [mailto:p.dalgaard at biostat.ku.dk] 
> Sent: Friday, February 11, 2005 8:57 AM
> To: John Fox
> Cc: 'Vito Ricci'; manuel_gutierrez_lopez at yahoo.es; 
> r-help at stat.math.ethz.ch
> Subject: Re: [R] Re: testing slopes different than a given value
> 
> "John Fox" <jfox at mcmaster.ca> writes:
> 
> > Dear Vito,
> > 
> > Since Manuel says that he wants to "obtain a test" and not 
> "obtain two 
> > tests," I assume that he's interested in the F-test for the 
> hypothesis 
> > that both coefficients are simultaneously equal to the specified 
> > values rather than in the t-tests for the individual hypotheses.
> > 
> > Regards,
> >  John
> 
> ...in which case one answer is this:
> 
> >  y<-3+0.6*x1+0.3*x2 + rnorm(100,sd=.1) # as meant, no?
> >  fm<-lm(y~x1+x2)
> >  anova(fm, lm(y~offset(0.6*x1+0.3*x2)))
> Analysis of Variance Table
> 
> Model 1: y ~ x1 + x2
> Model 2: y ~ offset(0.6 * x1 + 0.3 * x2)
>   Res.Df      RSS Df Sum of Sq      F Pr(>F)
> 1     97  1.06118
> 2     99  1.06184 -2  -0.00066 0.0302 0.9703
> 

Indeed, or as I responded yesterday, to use the linear.hypothesis function
in the car package.

Regards,
 John

> 
> 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: 
> (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: 
> (+45) 35327907



From macq at llnl.gov  Fri Feb 11 17:07:13 2005
From: macq at llnl.gov (Don MacQueen)
Date: Fri, 11 Feb 2005 08:07:13 -0800
Subject: [R] Saving graphs in formats other than PDF?
In-Reply-To: <BE317C5D.8C8%jrclmilks@joimail.com>
References: <BE317C5D.8C8%jrclmilks@joimail.com>
Message-ID: <p06110403be3287adc8a9@[128.115.153.6]>

See the discussion yesterday on R-SIG-Mac with the subject:
       [R-SIG-Mac] How to put nice R graphics into powerpoint

-Don

At 9:02 PM -0500 2/10/05, Jim and Chana Milks wrote:
>I am running R 2.01 on Mac OS 10.3.7.  Whenever I save graphs, they are
>saved as PDF files.  Are there any other file formats to which I could save
>my graphs.  I would like to directly export my graphs to MS Word, if
>possible.
>
>Thank you in advance.
>
>Sincerely,
>Jim Milks
>Graduate Student
>Environmental Sciences Ph.D. Program
>Wright State University
>3640 Colonel Glenn Hwy
>Dayton, OH 45435
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From sundar.dorai-raj at pdf.com  Fri Feb 11 17:07:28 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 11 Feb 2005 10:07:28 -0600
Subject: [R] formula behaviour in model.matrix
Message-ID: <420CD840.6020605@pdf.com>

Hi all,

Perhaps somebody can explain the following behaviour to me.

Take the following data.frame.

z <- expand.grid(X = LETTERS[1:3], Y = letters[1:3])

Now, from ?formula we see:

<quote>
The '*' operator denotes factor crossing: 'a*b' interpreted as 'a+b+a:b'.
</quote>

So I would expect the following:

ncol(model.matrix(~X*Y, z)) # returns 1 + 2 + 2 + 2 * 2 = 9

and

ncol(model.matrix(~X + Y + X:Y, z)) # returns 1 + 2 + 2 + 2 * 2 = 9

are equivalent.

However, I did not expect this:

ncol(model.matrix(~X:Y, z)) # returns 1 + 3 * 3 = 10

Why isn't this 5? In other words, why doesn't "~X:Y" just denote the 
interaction term so that all you would get is an intercept plus the 
two-way interaction between X and Y (1 + 2 * 2 = 5 parameters)? Instead 
what is returned is the fully crossed effects (every level of X against 
every level of Y) plus an intercept. Is there something in the 
documentation I'm missing?

--sundar

P.S. This behaviour is identical in S-PLUS 6.2.



From Dorfman.Alan at bls.gov  Fri Feb 11 17:08:08 2005
From: Dorfman.Alan at bls.gov (Dorfman, Alan - BLS)
Date: Fri, 11 Feb 2005 11:08:08 -0500
Subject: [R] cook's distance in weighted regression
Message-ID: <70E1C0DB4F9B5E4F9CEDB8433F4A68B903D385E5@psbmail2.psb.bls.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050211/25621379/attachment.pl

From jfox at mcmaster.ca  Fri Feb 11 17:16:07 2005
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 11 Feb 2005 11:16:07 -0500
Subject: [R] Re: testing slopes different than a given value
In-Reply-To: <20050211151133.88939.qmail@web25101.mail.ukl.yahoo.com>
Message-ID: <20050211161608.BTRK1836.tomts16-srv.bellnexxia.net@JohnDesktop8300>

Dear Manuel,

Here's an example:

> x1<-rnorm(100)
> x2<-rnorm(100)
> y<-3+0.6*x1+0.3*x2 + rnorm(100,sd=.1)
> fm<-lm(y~x1+x2)
> anova(fm, lm(y~offset(0.6*x1+0.3*x2))) # Peter's solution
Analysis of Variance Table

Model 1: y ~ x1 + x2
Model 2: y ~ offset(0.6 * x1 + 0.3 * x2)
  Res.Df      RSS Df Sum of Sq      F Pr(>F)
1     97  1.00150                           
2     99  1.01732 -2  -0.01583 0.7664 0.4675
> hyp <- matrix(c(0, 1, 0,  0, 0, 1), 2, 3, byrow=TRUE)
> hyp # the hypothesis matrix
     [,1] [,2] [,3]
[1,]    0    1    0
[2,]    0    0    1
> linear.hypothesis(fm, hyp, c(0.6, 0.3))
F-Test 
SS = 0.01582501     SSE = 1.001495     F = 0.7663671  Df = 2 and 97     p =
0.4674909 
> 

My apologies for not including an example in my original response.

Regards,
 John

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Manuel 
> Gutierrez
> Sent: Friday, February 11, 2005 10:12 AM
> To: Peter Dalgaard; John Fox
> Cc: r-help at stat.math.ethz.ch; 'Vito Ricci'; 
> manuel_gutierrez_lopez at yahoo.es
> Subject: Re: [R] Re: testing slopes different than a given value
> 
> Thanks to all,
> Yes, I meant a single test for both coefficients.
> Peter's reply is what I wanted. I've tried with 
> linear.hypothesis but I must confess that with my limited 
> statistical experience and without the car book at hand, the 
> nomenclature for the function was a bit difficult to 
> understand for me. A toy example of linear.hypothesis for my 
> case would be great.


> Thanks,
> Manuel
> --- Peter Dalgaard <p.dalgaard at biostat.ku.dk>
> escribi?: 
> > "John Fox" <jfox at mcmaster.ca> writes:
> > 
> > > Dear Vito,
> > > 
> > > Since Manuel says that he wants to "obtain a test"
> > and not "obtain two
> > > tests," I assume that he's interested in the
> > F-test for the hypothesis that
> > > both coefficients are simultaneously equal to the
> > specified values rather
> > > than in the t-tests for the individual hypotheses.
> > > 
> > > Regards,
> > >  John
> > 
> > ...in which case one answer is this:
> > 
> > >  y<-3+0.6*x1+0.3*x2 + rnorm(100,sd=.1) # as meant,
> > no?
> > >  fm<-lm(y~x1+x2)
> > >  anova(fm, lm(y~offset(0.6*x1+0.3*x2)))
> > Analysis of Variance Table
> > 
> > Model 1: y ~ x1 + x2
> > Model 2: y ~ offset(0.6 * x1 + 0.3 * x2)
> >   Res.Df      RSS Df Sum of Sq      F Pr(>F)
> > 1     97  1.06118
> > 2     99  1.06184 -2  -0.00066 0.0302 0.9703
> > 
> > 
> > 
> > -- 
> >    O__  ---- Peter Dalgaard             Blegdamsvej
> > 3  
> >   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N 
> >  
> >  (*) \(*) -- University of Copenhagen   Denmark     
> > Ph: (+45) 35327918
> > ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)            
> > FAX: (+45) 35327907
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From tomhopper at comcast.net  Fri Feb 11 17:36:35 2005
From: tomhopper at comcast.net (Thomas Hopper)
Date: Fri, 11 Feb 2005 11:36:35 -0500
Subject: [R] Curious Behavior with Curve() and dnorm()
In-Reply-To: <x2k6pgavj2.fsf@biostat.ku.dk>
References: <420BDB67.5030304@cobasys.com> <x2k6pgavj2.fsf@biostat.ku.dk>
Message-ID: <420CDF13.2030505@comcast.net>

Okay, I see how I'm using dnorm() incorrectly (my thanks to you and 
Prof. Ripley). I'll work on correcting that.

The important issue resolved, I still don't understand why I get 
different results for dnorm() when supplying the same values, based on 
how those values were supplied. I've got three options, all of which 
give the same value, but which result in a different distribution from 
dnorm(): the direct output of the function sd(); a number typed 
manually; or a variable which was set by the output of the function 
sd()). Using sd() produces different results than using a variable set 
from sd().

Having identified this seeming quirk, it's not a problem for my work; it 
just seems inconsistent and I'm having trouble understanding it.

Thanks,

Tom

Peter Dalgaard wrote:

>Thomas Hopper <thopper at cobasys.com> writes:
>
>  
>
>>I am attempting to wrap the histogram function in my own custom
>>function, so that I can quickly generate some standard plots.
>>
>>A part of what I want to do is to draw a normal curve over the histogram:
>>
>> > x <- rnorm(1000)
>> > hist(x, freq=F)
>> > curve(dnorm(x), lty=3, add=T)
>>
>>(for normal use, x would be a vector of empirical values, but the
>>rnorm() function works for testing)
>>
>>That works just as you'd expect, but I've found something a bit strange.
>>
>>If I try the following:
>>
>> > curve(dnorm(x, mean=mean(x), sd=sd(x)), lty=3, add=T)
>>
>>I get a much flatter and broader curve (which looks like it probably
>>has the same area as the first curve, though I haven't tested).
>>
>>However, if I do
>>
>> > z <- sd(x)
>> > curve(dnorm(x, mean=mean(x), sd=z), lty=1, add=T)
>>
>>I get the curve you'd expect; it draws right over the first curve
>>(curve(dnorm(x),...), above).
>>    
>>
>
>I don't think that is guaranteed, actually.
>
>Notice that curve plots the *expression* as a function of the argument
>"x". So it takes a bunch of x values, evenly spread across the
>abscissa collects them into a vector and plugs that in as "x" in
>
>curve(dnorm(x, mean=mean(x), sd=sd(x)), lty=3, add=T)
>
>I.e. the x that gets plugged into mean(x) and sd(x) has nothing to do
>with your original data (except that they both fit in the same xlim)! 
>
>  
>



From spencer.graves at pdf.com  Fri Feb 11 17:35:05 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 11 Feb 2005 08:35:05 -0800
Subject: [R] table in R
In-Reply-To: <20050211021019.33477.qmail@web30704.mail.mud.yahoo.com>
References: <20050211021019.33477.qmail@web30704.mail.mud.yahoo.com>
Message-ID: <420CDEB9.3030603@pdf.com>

      Have you read sections 5 and 6 on arrays, matrices, lists and 
data.frames in "An Introduction to R" [accessible via help.start() from 
an R session]?  If you have, please try something and give us a very 
simple example of what you tried that didn't work, as requested in the 
posting guide. 

      hope this helps.  spencer graves

Cuichang Zhao wrote:

>Hello, 
>I want to build some tables in my project using R, does R have some tables form that I can use? if i use the internal R table, how can initial a table before I use it?
> 
>I want to my tables to have some columns, for example:
>(trial, x, y, goal) I want to put these columns into my tables, for i want to put data into each entry one by one. I have no idea about how big the table would be, so will it possible to use R's internal table?
> 
>Also, can someone please recommand me a good book or a good website about R.
> 
> 
>Thank you so much.
> 
>C-Ming
> 
>Feb 10, 2005
>
>		
>---------------------------------
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From nair at sdsc.edu  Fri Feb 11 17:47:07 2005
From: nair at sdsc.edu (T. Murlidharan Nair)
Date: Fri, 11 Feb 2005 08:47:07 -0800
Subject: [R] sample
References: <3A822319EB35174CA3714066D590DCD50994E6B7@usrymx25.merck.com>
Message-ID: <420CE18B.3070705@sdsc.edu>

Thanks to all for trying to help me with problem. After spending a long 
time, I eventually solved it
by writing a perl script and transposing the matrix and re-reading the 
file. When I did this I got an
error saying I had duplicate row names (which in fact was not true or 
may be something to with
the naming conventions in R).
The column names were  :
Tumor_VA_114-1, Tumor_VA_114-2,.....
But when I changed it to Tumor_VA_114_1, Tumor_VA_114_2 it worked fine.
I was not aware that - cannot be used to differentiate variables. Is 
this the case ?
Sorry, if I wasted any of your time.
Cheers ../Murli



Liaw, Andy wrote:

>?sample says:
>
>x   Either a (numeric, complex, character or logical) vector of more
>    than one element from which to choose, or a positive integer 
>
>so I guess it wasn't meant to be used on a data frame.  However, a data
>frame is a list (where the variables are the components), and a list is a
>vector, so the behavior is consistent:
>
>  
>
>>x <- list("a", 2, "c", 4, "e", "f")
>>sample(x)
>>    
>>
>[[1]]
>[1] "c"
>
>[[2]]
>[1] "a"
>
>[[3]]
>[1] 4
>
>[[4]]
>[1] 2
>
>[[5]]
>[1] "f"
>
>[[6]]
>[1] "e"
>
>Andy
>
>  
>
>>From: Jean Eid
>>
>>It seems that sample picks columns when the object is turned into a
>>data.frame. I do not knoe why it is doing that....
>>
>>Is this something that was meant and not documented or something?
>>
>>
>>Jean
>>
>>
>>On Fri, 11 Feb 2005, Adaikalavan Ramasamy wrote:
>>
>>    
>>
>>>See below.
>>>
>>>On Thu, 2005-02-10 at 14:53 -0800, T. Murlidharan Nair wrote:
>>>      
>>>
>>>>Just to explain my previous mail, here is the output I get.
>>>>
>>>>
>>>> > dim(tissue.exp)
>>>>[1] 1532   20
>>>>        
>>>>
>>>What is this object ? Try class(tissue.exp)
>>>
>>>      
>>>
>>>> > pick<-sample(tissue.exp,5,replace=TRUE)
>>>> > dim(pick)
>>>>[1] 1532    5
>>>>        
>>>>
>>>>From help(sample) :
>>>
>>>    sample(x, size, replace = FALSE, prob = NULL)
>>>
>>>    x: Either a (numeric, complex, character or logical) vector of
>>>       more than one element from which to choose, or a positive
>>>       integer.
>>>
>>>x has to be a vector or positive integer integer. I am not 
>>>      
>>>
>>sure how you
>>    
>>
>>>got "sample" to work on tissue.exp in the first place. Maybe you got
>>>your own version of "sample".
>>>
>>>
>>>      
>>>
>>>> > tissue.exp.t<-t(tissue.exp)
>>>> > dim(tissue.exp.t)
>>>>[1]   20 1532
>>>> > pick<-sample(tissue.exp.t,5,replace=TRUE)
>>>> > dim(pick)
>>>>NULL
>>>>
>>>>--------
>>>>Thanks ../Murli
>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide! 
>>>>        
>>>>
>>http://www.R-project.org/posting-guide.html
>>    
>>
>>>      
>>>
>>______________________________________________
>>    
>>
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! 
>>>      
>>>
>>http://www.R-project.org/posting-guide.html
>>    
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>
>>    
>>
>
>
>------------------------------------------------------------------------------
>Notice:  This e-mail message, together with any attachments, contains information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New Jersey, USA 08889), and/or its affiliates (which may be known outside the United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as Banyu) that may be confidential, proprietary copyrighted and/or legally privileged. It is intended solely for the use of the individual or entity named on this message.  If you are not the intended recipient, and have received this message in error, please notify us immediately by reply e-mail and then delete it from your system.
>------------------------------------------------------------------------------
>  
>

---



From ripley at stats.ox.ac.uk  Fri Feb 11 18:43:50 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 11 Feb 2005 17:43:50 +0000 (GMT)
Subject: [R] formula behaviour in model.matrix
In-Reply-To: <420CD840.6020605@pdf.com>
Message-ID: <Pine.GSO.4.31.0502111741530.21966-100000@toucan.stats>

See MASS4 pp.149-150 (and I don't know of a similarly detailed explanation
elsewhere, although there is a terser account in the White Book).

On Fri, 11 Feb 2005, Sundar Dorai-Raj wrote:

> Hi all,
>
> Perhaps somebody can explain the following behaviour to me.
>
> Take the following data.frame.
>
> z <- expand.grid(X = LETTERS[1:3], Y = letters[1:3])
>
> Now, from ?formula we see:
>
> <quote>
> The '*' operator denotes factor crossing: 'a*b' interpreted as 'a+b+a:b'.
> </quote>
>
> So I would expect the following:
>
> ncol(model.matrix(~X*Y, z)) # returns 1 + 2 + 2 + 2 * 2 = 9
>
> and
>
> ncol(model.matrix(~X + Y + X:Y, z)) # returns 1 + 2 + 2 + 2 * 2 = 9
>
> are equivalent.
>
> However, I did not expect this:
>
> ncol(model.matrix(~X:Y, z)) # returns 1 + 3 * 3 = 10
>
> Why isn't this 5? In other words, why doesn't "~X:Y" just denote the
> interaction term so that all you would get is an intercept plus the
> two-way interaction between X and Y (1 + 2 * 2 = 5 parameters)? Instead
> what is returned is the fully crossed effects (every level of X against
> every level of Y) plus an intercept. Is there something in the
> documentation I'm missing?
>
> --sundar
>
> P.S. This behaviour is identical in S-PLUS 6.2.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Feb 11 18:49:38 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 11 Feb 2005 17:49:38 +0000 (GMT)
Subject: [R] Curious Behavior with Curve() and dnorm()
In-Reply-To: <420CDF13.2030505@comcast.net>
Message-ID: <Pine.GSO.4.31.0502111745000.21966-100000@toucan.stats>

On Fri, 11 Feb 2005, Thomas Hopper wrote:

> Okay, I see how I'm using dnorm() incorrectly (my thanks to you and
> Prof. Ripley). I'll work on correcting that.
>
> The important issue resolved, I still don't understand why I get
> different results for dnorm() when supplying the same values, based on
> how those values were supplied. I've got three options, all of which
> give the same value, but which result in a different distribution from
> dnorm(): the direct output of the function sd(); a number typed
> manually; or a variable which was set by the output of the function
> sd()). Using sd() produces different results than using a variable set
> from sd().
>
> Having identified this seeming quirk, it's not a problem for my work; it
> just seems inconsistent and I'm having trouble understanding it.

You use sd(x) for two different x's, one your own and one inside curve.

Try:

Y <- rnorm(1000)
hist(Y, prob = TRUE)
curve(dnorm(x, mean(Y), sd(Y)), lty=3, add=T)
m <- mean(Y); z <- sd(Y)
curve(dnorm(x, m, z), lty=3, add=T)

and they are identical.

dnorm(x, mean=mean(x), sd=sd(x))  depends on x in three places, only one
of which you intended AFAICS.

>
> Thanks,
>
> Tom
>
> Peter Dalgaard wrote:
>
> >Thomas Hopper <thopper at cobasys.com> writes:
> >
> >
> >
> >>I am attempting to wrap the histogram function in my own custom
> >>function, so that I can quickly generate some standard plots.
> >>
> >>A part of what I want to do is to draw a normal curve over the histogram:
> >>
> >> > x <- rnorm(1000)
> >> > hist(x, freq=F)
> >> > curve(dnorm(x), lty=3, add=T)
> >>
> >>(for normal use, x would be a vector of empirical values, but the
> >>rnorm() function works for testing)
> >>
> >>That works just as you'd expect, but I've found something a bit strange.
> >>
> >>If I try the following:
> >>
> >> > curve(dnorm(x, mean=mean(x), sd=sd(x)), lty=3, add=T)
> >>
> >>I get a much flatter and broader curve (which looks like it probably
> >>has the same area as the first curve, though I haven't tested).
> >>
> >>However, if I do
> >>
> >> > z <- sd(x)
> >> > curve(dnorm(x, mean=mean(x), sd=z), lty=1, add=T)
> >>
> >>I get the curve you'd expect; it draws right over the first curve
> >>(curve(dnorm(x),...), above).
> >>
> >>
> >
> >I don't think that is guaranteed, actually.
> >
> >Notice that curve plots the *expression* as a function of the argument
> >"x". So it takes a bunch of x values, evenly spread across the
> >abscissa collects them into a vector and plugs that in as "x" in
> >
> >curve(dnorm(x, mean=mean(x), sd=sd(x)), lty=3, add=T)
> >
> >I.e. the x that gets plugged into mean(x) and sd(x) has nothing to do
> >with your original data (except that they both fit in the same xlim)!

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From nair at sdsc.edu  Fri Feb 11 19:31:49 2005
From: nair at sdsc.edu (T. Murlidharan Nair)
Date: Fri, 11 Feb 2005 10:31:49 -0800
Subject: [R] Random Matrix theory
Message-ID: <420CFA15.5090002@sdsc.edu>

Any package that implements Random matrix theory  application in R.
I am completely new to this subject, so just wanted to explore it.
Cheers ../Murli



From wcai11 at hotmail.com  Fri Feb 11 19:36:44 2005
From: wcai11 at hotmail.com (Weijie Cai)
Date: Fri, 11 Feb 2005 13:36:44 -0500
Subject: [R] two dimensional array of object elements
Message-ID: <BAY103-F14871DC8CAB87FADF84062D3770@phx.gbl>

Hi list,

I want to create a two (possibly three) dimensional array of objects. These 
objects are classes in object oriented style. I failed by using
a<-array(NA,c(m,n))
for (i in 1:m){
  for (j in 1:n){
    a[i,j]<-My.Obj
  }
}

The elements are still NA. Any suggestions?

Thanks



From mingan at math.unm.edu  Fri Feb 11 20:53:47 2005
From: mingan at math.unm.edu (mingan)
Date: Fri, 11 Feb 2005 12:53:47 -0700
Subject: [R] how ot get covariance matrix from survreg 
Message-ID: <420D0D4B.3050805@math.unm.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050211/4c16bc81/attachment.pl

From HDoran at air.org  Fri Feb 11 21:18:16 2005
From: HDoran at air.org (Doran, Harold)
Date: Fri, 11 Feb 2005 15:18:16 -0500
Subject: [R] how ot get covariance matrix from survreg 
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7407B10771@dc1ex2.air.org>

vcov(f) will work 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of mingan
Sent: Friday, February 11, 2005 2:54 PM
To: r-help at stat.math.ethz.ch
Subject: [R] how ot get covariance matrix from survreg 


 I have a question,  I generated some data and tried survreg to analyze
them, but here, I want to get the covariance matrix of the coefficients.


I know how to get corr from survreg, say,  summary(f,corr=T), but how to

get covariance matrix of the coefficient ?
can I put them in a matrix and output ?


thanks


below is what I did


<mailto:r-help at stat.math.ethz.ch>
n=100; beta=1.0

z=runif(n)
u=runif(n)
epi=log(u/(1-u))
d=rep(1,n)
t=z*beta+epi
f<-survreg(Surv(t,d)~z+epi, dist="logistic")

summary(f, corr=T)


below is the result

 > summary(f, corr=T)

.......
........

Correlation of Coefficients:
           (Intercept) z      epi  
z          -0.882                  
epi         0.167      -0.168      
Log(scale)  0.000       0.000  0.000



here I want covariance matrix

 thanks for your help

pls reply to me directly if you can


mingan yang











	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From maechler at stat.math.ethz.ch  Fri Feb 11 21:32:58 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 11 Feb 2005 21:32:58 +0100
Subject: [R] Help concerning Lasso::l1ce
In-Reply-To: <20050211090106.29573.qmail@web30504.mail.mud.yahoo.com>
References: <20050211090106.29573.qmail@web30504.mail.mud.yahoo.com>
Message-ID: <16909.5754.549792.395991@stat.math.ethz.ch>

>>>>> "Feng" == Feng Qiu <fengqiu1984 at yahoo.com>
>>>>>     on Fri, 11 Feb 2005 01:01:06 -0800 (PST) writes:

    Feng> Hi, 
    Feng> First, when I try the example Prostate with bound 0.44
    Feng> (as in the manual), I got a different result:
	                              ^^^^^^^^^
different than the manual?  I get the same result as you do.
and I see that example in the manual seems to first transform
the data so I wouldn't expect the same answer...

    Feng> And the sum of the absolute values of the coefficients
    Feng> are larger than the bound!! Why?

because they are (back) transformed from the so called
"constrained coefficients"

    Feng> Second, the manual says that if I set sweep.out to
    Feng> NULL, the constant term (which is the intercept,
    Feng> right?)
    Feng> will be included in the bound, but when I do so, I get
    Feng> an error "Matrix build from transformed variables has
    Feng> a constant column". 

because the intercept is a constant column in your matrix.
You need to additionally say "standardize = FALSE",
and things will work (see below).

    Feng> And how can I constrain the constant term?

as mentioned above. Here is the call and the "proof" that the
constrained coefficients follow the bound :

Input:
 
  library(lasso2)
  data(Prostate)
  l1c.P. <- l1ce(lpsa ~ ., Prostate, bound=0.44, sweep= NULL, standardize=FALSE)
  l1c.P.
  sum(l1c.P. $ constrained.coefficients)


Transcript :

 > l1c.P. <- l1ce(lpsa ~ ., Prostate, bound=0.44, sweep=NULL, standardize=FALSE)
 > l1c.P.
 Call:
 l1ce(formula = lpsa ~ ., data = Prostate, sweep.out = NULL, standardize = FALSE, 
     bound = 0.44)

 Coefficients:
 (Intercept)      lcavol     lweight         age        lbph         svi 
 0.000000000 0.559056488 0.393462929 0.000000000 0.042394403 0.198590534 
	 lcp     gleason       pgg45 
 0.000000000 0.015171790 0.005224389 

 The relative L1 bound was      : 0.44 
 The absolute L1 bound was      : 1.213901 
 The Lagrangian for the bound is:  5.340326 
 > sum(l1c.P. $ constrained.coefficients)
 [1] 1.213901
 > 

    Feng> Finally, could anyone explain to me the difference
    Feng> between relative bound and absolute bound? Which is
    Feng> the actually bound adopted?

I think you should read the reference mentioned
and/or also see how l1ce() computes its quantities.
Just type  ` l1ce '  to see the function definition (apart from
the few comments in the source code).


    Feng> I?m new to R and the lasso2 package, please help
    Feng> me out, thank you very much.

you're welcome.
Martin Maechler


BTW: I'll be gone for vacations, so please continue this topic
     on R-help  (and keep me out of CC: if you know how).



From ggrothendieck at myway.com  Fri Feb 11 21:15:51 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 11 Feb 2005 20:15:51 +0000 (UTC)
Subject: [R] two dimensional array of object elements
References: <BAY103-F14871DC8CAB87FADF84062D3770@phx.gbl>
Message-ID: <loom.20050211T211342-235@post.gmane.org>

Weijie Cai <wcai11 <at> hotmail.com> writes:

: 
: Hi list,
: 
: I want to create a two (possibly three) dimensional array of objects. These 
: objects are classes in object oriented style. I failed by using
: a<-array(NA,c(m,n))
: for (i in 1:m){
:   for (j in 1:n){
:     a[i,j]<-My.Obj
:   }
: }
: 

A list that has a dim attribute should do it:

R> x <- list(12, "abc", 3i, sin)
R> dim(x) <- c(2,2)
R> x
     [,1]  [,2]
[1,] 12    0+3i
[2,] "abc" ?   
R> x[[2,2]](pi/180)
[1] 0.01745241



From ggrothendieck at myway.com  Fri Feb 11 19:28:01 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 11 Feb 2005 18:28:01 +0000 (UTC)
Subject: [R] function table
References: <E1CzapQ-0006ld-93@s2.stud.uni-goettingen.de>
Message-ID: <loom.20050211T190249-459@post.gmane.org>

Carsten Steinhoff <carsten.steinhoff <at> stud.uni-goettingen.de> writes:

: 
: Hi,
: 
: my problem is the following:
: 
: I have a large database of insurance-damage data and want to model the
: frequency of these events. So to fit a distribution on my frequency-data I
: want to count the number of events in each month via the date of occurrence.
: Therefor I use this command which works very well:
: 
: count_table <- table(months(date_occ),years(date_occ)) 
: 
: But there is another column in my database called "weight". So I don't want
: to count EACH event by "1", some are e.g. only counted by "half an event".
: How could I modify my table function, that the output is not a simple
: counting but a counting of the weights?

I assume you are using chron since months and years is defined in
that package so assuming some dummy data:

library(chron)
set.seed(1)
x <- chron(1:1000)
w <- runif(1000)

We could do this:

result.matrix <- tapply(w, list(month = months(x), year = years(x)), sum)

# This gives a matrix but you could coerce it into a table
# if you like:

result.table <- as.table(result.matrix)

# and that could be coerced to a data frame

result.df <- as.data.frame(result.table)

Note that you cannot coerce the matrix directly to a data frame
since that won't give you the right form.

: 
: And - another question how can I directly access the columns of my table? I
: tried e.g. count_table$2004 (because one column is named 2004), or
: count_table[2004] ... nothing worked. The access via column - number
: (count_table[,9]) woks, but will be to unflexible for the future.

Tables cannot be indexed by name.  I think this is significant problem
for users since margining and conditioning a table often changes the
positions of dimensions so its very confusing to access them since
one has to manually keep track of the changing positions of variables.
There are some functions which allow one to specify the variable name
but that's a significant limitation since it means that every function
that has to access a table has to reinvent the mapping.

You could do it yourself by redefining [.table or slightly easier but
not as slick would be define a function which does the mapping
for you.  You might design it so it can be used like this:

   count_table[,m(2004)]  # m is specific to count_table

or maybe like this:

   m(count_table,,2004)  # more general m possible

or define a table-like S3 or S4 class which does it.

Another possibility is to use the data frame form.  For example,

   subset(result.df, subset = year == 1970)

If you are doing to move back and forth between tables and
data frames then you should check out ?xtabs .



From VanHezewijkB at AGR.GC.CA  Fri Feb 11 21:40:10 2005
From: VanHezewijkB at AGR.GC.CA (VanHezewijk, Brian)
Date: Fri, 11 Feb 2005 15:40:10 -0500
Subject: [R] multcomp analog for poisson errors?
Message-ID: <FF6B50242434404BA6A2E3BFDE22E36A246284@onncrxms5.agr.gc.ca>


Hello all,

Does anyone know if a package exists that conducts post-hoc multiple comparisons for generalized linear models? 

Thanks,

Brian Van Hezewijk 
Agriculture and Agri-Food Canada/Agriculture et Agroalimentaire Canada
Telephone/T?l?phone: 403-317-3404
Facsimile/T?l?copieur: 403-382-3156
P.O. Box 3000, 5403 - 1st Ave South
Lethbridge, Alberta
T1J 4B1
vanhezewijkb at agr.gc.ca



From tplate at acm.org  Fri Feb 11 21:51:02 2005
From: tplate at acm.org (Tony Plate)
Date: Fri, 11 Feb 2005 13:51:02 -0700
Subject: [R] two dimensional array of object elements
In-Reply-To: <BAY103-F14871DC8CAB87FADF84062D3770@phx.gbl>
References: <BAY103-F14871DC8CAB87FADF84062D3770@phx.gbl>
Message-ID: <6.2.1.2.2.20050211134920.06375560@mailhost.blackmesacapital.com>

Create your original matrix as a list datatype.  When assigning elements, 
be careful with the list structure, as the example indicates.

 > m <- 2; n <- 3
 > a <- array(list(),c(m,n))
 > a[1,2] <- list(b=1,c=2)
Error in "[<-"(`*tmp*`, 1, 2, value = list(b = 1, c = 2)) :
         number of items to replace is not a multiple of replacement length
 > a[1,2] <- list(list(b=1,c=2))
 >



At Friday 11:36 AM 2/11/2005, Weijie Cai wrote:
>Hi list,
>
>I want to create a two (possibly three) dimensional array of objects. 
>These objects are classes in object oriented style. I failed by using
>a<-array(NA,c(m,n))
>for (i in 1:m){
>  for (j in 1:n){
>    a[i,j]<-My.Obj
>  }
>}
>
>The elements are still NA. Any suggestions?
>
>Thanks
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Fri Feb 11 22:08:09 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 11 Feb 2005 21:08:09 +0000 (UTC)
Subject: [R] two dimensional array of object elements
References: <BAY103-F14871DC8CAB87FADF84062D3770@phx.gbl>
	<6.2.1.2.2.20050211134920.06375560@mailhost.blackmesacapital.com>
Message-ID: <loom.20050211T220648-77@post.gmane.org>

Tony Plate <tplate <at> acm.org> writes:

: 
: Create your original matrix as a list datatype.  When assigning elements, 
: be careful with the list structure, as the example indicates.
: 
:  > m <- 2; n <- 3
:  > a <- array(list(),c(m,n))
:  > a[1,2] <- list(b=1,c=2)
: Error in "[<-"(`*tmp*`, 1, 2, value = list(b = 1, c = 2)) :
:          number of items to replace is not a multiple of replacement length

Try:

a[[1,2]] <- list(b=1,c=2)


:  > a[1,2] <- list(list(b=1,c=2))
:  >
: 
: At Friday 11:36 AM 2/11/2005, Weijie Cai wrote:
: >Hi list,
: >
: >I want to create a two (possibly three) dimensional array of objects. 
: >These objects are classes in object oriented style. I failed by using
: >a<-array(NA,c(m,n))
: >for (i in 1:m){
: >  for (j in 1:n){
: >    a[i,j]<-My.Obj
: >  }
: >}
: >
: >The elements are still NA. Any suggestions?



From lindsayv at unbc.ca  Fri Feb 11 22:34:56 2005
From: lindsayv at unbc.ca (lindsayv@unbc.ca)
Date: Fri, 11 Feb 2005 13:34:56 -0800 (PST)
Subject: [R] NA's in if statement
Message-ID: <45260.142.207.68.159.1108157696.squirrel@webmail.unbc.ca>

Hello,

I am having trouble dealing with NA values in if statements such as:
i<-1
for(i in 1:length(dat$wdir)){
if (dat$wspd[i]==0){
dat$wdir[i]<-0
}
}

I get the following error due to the presence of NS values in the data:
Error in if (dat$wspd[i] == 0) { : missing value where TRUE/FALSE needed

Sorry if this is an old question but I was unable to resolve this from the
info on na.action etc. or old archives.

Thank you for any suggestions,

Vera Lindsay



From mallen at tntech.edu  Fri Feb 11 22:47:11 2005
From: mallen at tntech.edu (Michael R. Allen)
Date: Fri, 11 Feb 2005 15:47:11 -0600
Subject: [R] mode of a data set
Message-ID: <420D27DF.8070601@tntech.edu>

I was looking up how to find the mode of a data set in R and found some 
solutions through your service.  But,  I needed a program to find the 
mode or modes of a data set.  Here is what I came up with:

names(sort(table(x)))[sort(table(x))==max(sort(table(x)))]

Michael Allen
Department of Mathematics
Tennessee Technological University
Cookeville, TN 38505
USA



From michael.watson at bbsrc.ac.uk  Fri Feb 11 23:25:32 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Fri, 11 Feb 2005 22:25:32 -0000
Subject: [R] NA's in if statement
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950121BA41@iahce2knas1.iah.bbsrc.reserved>

Some careful use of is.na() may help.
Either by using it to remove NA values or as part of the if statement.
 
Or:
 
my.wdir <- na.omit(dat$wdir)
for(i in 1:length(my.wdir)) {
 
#etc

	-----Original Message----- 
	From: r-help-bounces at stat.math.ethz.ch on behalf of lindsayv at unbc.ca 
	Sent: Fri 2/11/2005 9:34 PM 
	To: r-help at stat.math.ethz.ch 
	Cc: 
	Subject: [R] NA's in if statement
	
	

	Hello,
	
	I am having trouble dealing with NA values in if statements such as:
	i<-1
	for(i in 1:length(dat$wdir)){
	if (dat$wspd[i]==0){
	dat$wdir[i]<-0
	}
	}
	
	I get the following error due to the presence of NS values in the data:
	Error in if (dat$wspd[i] == 0) { : missing value where TRUE/FALSE needed
	
	Sorry if this is an old question but I was unable to resolve this from the
	info on na.action etc. or old archives.
	
	Thank you for any suggestions,
	
	Vera Lindsay
	
	______________________________________________
	R-help at stat.math.ethz.ch mailing list
	https://stat.ethz.ch/mailman/listinfo/r-help
	PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Fri Feb 11 23:51:35 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 11 Feb 2005 14:51:35 -0800
Subject: [R] mode of a data set
In-Reply-To: <420D27DF.8070601@tntech.edu>
References: <420D27DF.8070601@tntech.edu>
Message-ID: <420D36F7.6040807@pdf.com>

      That'll work, provided x is discrete or categorical. 

      Otherwise, the mode is not a well defined concept, as has been 
discussed on this list several times.  I just got 17 hits for a search 
-> "R site search" -> "finding the mode of a distribution from data" 
from "www.r-project.org".  The seventh item on that list was a reply 
from Ted Harding on Sat Dec 13 2003 - 13:13:37 EST to a question subject 
"mode", elaborating on this point.  (The term "mode" is a difficult 
search here, because the R command "mode" returns the storage mode of 
the data, which is unrelated to your question.) 

      hope this helps.  spencer graves

Michael R. Allen wrote:

> I was looking up how to find the mode of a data set in R and found 
> some solutions through your service.  But,  I needed a program to find 
> the mode or modes of a data set.  Here is what I came up with:
>
> names(sort(table(x)))[sort(table(x))==max(sort(table(x)))]
>
> Michael Allen
> Department of Mathematics
> Tennessee Technological University
> Cookeville, TN 38505
> USA
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Fri Feb 11 23:59:01 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 11 Feb 2005 14:59:01 -0800
Subject: [R] NA's in if statement
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950121BA41@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E950121BA41@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <420D38B5.5030006@pdf.com>

      The following provides a sample use if "is.na" in an context 
similar to yours, that also uses vector arithmetic, so you don't need a 
loop: 

 > tst <- (-2):2
 > (tst2 <- 1+tst/(tst^2))
[1] 0.5 0.0 NaN 2.0 1.5
 > (sel <- (tst2==0))
[1] FALSE  TRUE    NA FALSE FALSE
 > sel[is.na(tst2)] <- FALSE
 > tst2[sel] <- 123
 > tst2
[1]   0.5 123.0   NaN   2.0   1.5

      hope this helps.  spencer graves

michael watson (IAH-C) wrote:

>Some careful use of is.na() may help.
>Either by using it to remove NA values or as part of the if statement.
> 
>Or:
> 
>my.wdir <- na.omit(dat$wdir)
>for(i in 1:length(my.wdir)) {
> 
>#etc
>
>	-----Original Message----- 
>	From: r-help-bounces at stat.math.ethz.ch on behalf of lindsayv at unbc.ca 
>	Sent: Fri 2/11/2005 9:34 PM 
>	To: r-help at stat.math.ethz.ch 
>	Cc: 
>	Subject: [R] NA's in if statement
>	
>	
>
>	Hello,
>	
>	I am having trouble dealing with NA values in if statements such as:
>	i<-1
>	for(i in 1:length(dat$wdir)){
>	if (dat$wspd[i]==0){
>	dat$wdir[i]<-0
>	}
>	}
>	
>	I get the following error due to the presence of NS values in the data:
>	Error in if (dat$wspd[i] == 0) { : missing value where TRUE/FALSE needed
>	
>	Sorry if this is an old question but I was unable to resolve this from the
>	info on na.action etc. or old archives.
>	
>	Thank you for any suggestions,
>	
>	Vera Lindsay
>	
>	______________________________________________
>	R-help at stat.math.ethz.ch mailing list
>	https://stat.ethz.ch/mailman/listinfo/r-help
>	PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From binabina at bellsouth.net  Sat Feb 12 01:53:21 2005
From: binabina at bellsouth.net (zubin)
Date: Fri, 11 Feb 2005 19:53:21 -0500
Subject: [R] random coefficients in R - simultaneity in panel data
Message-ID: <DMEEJOKJCIMNIJJMBNPEGEBCCBAA.binabina@bellsouth.net>

hello - using SAS proc mixed to estimate random coefficients - R has a
similar command.  However, i need to estimate random coefficients when there
exists Simultaneity bias in the cross sectional time series panels  -
specifically 

Simultaneity, endogeneity, and lagged dependent variables in panel data.

anyone can steer me to an R function or such that can help?




From ramasamy at cancer.org.uk  Sat Feb 12 02:36:50 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Sat, 12 Feb 2005 01:36:50 +0000
Subject: [R] sample
In-Reply-To: <420CE18B.3070705@sdsc.edu>
References: <3A822319EB35174CA3714066D590DCD50994E6B7@usrymx25.merck.com>
	<420CE18B.3070705@sdsc.edu>
Message-ID: <1108172210.6237.74.camel@dhcp-63.ccc.ox.ac.uk>

See comments below.


On Fri, 2005-02-11 at 08:47 -0800, T. Murlidharan Nair wrote:
> Thanks to all for trying to help me with problem. After spending a long 
> time, I eventually solved it
> by writing a perl script and transposing the matrix 

If you plan on doing your analysis mostly in R, it would be best to use
R directly. While perl is great for many things, I recommend caution
with numerical data (e.g. it will happily add a character and a number)

Besides, you may risk extra computing overhead and debugging nightmares.


> and re-reading the file. When I did this I got an
> error saying I had duplicate row names (which in fact was not true or 

How did you read the file ? e.g. read.delim, read.table, read.csv, ...

> may be something to with
> the naming conventions in R).
> The column names were  :

Wouldn't this become row names after you transpose it ? Nevermind, I
have tested both cases below.


> Tumor_VA_114-1, Tumor_VA_114-2,.....
> But when I changed it to Tumor_VA_114_1, Tumor_VA_114_2 it worked fine.
> I was not aware that - cannot be used to differentiate variables. Is 
> this the case ?

Not true. I successfully read in (tab delimited) files containing either
of the following contents in Redhat Fedora Core 3 and R-1.9.1 :

read.delim(file="file1.txt", row.names=1, header=TRUE)
read.delim(file="file2.txt", row.names=1, header=TRUE)

----------------- File 1 ---------------------
Index	Tumor_VA_114-1	Tumor_VA_114-2
A	10	100
B	20	200
----------------------------------------------

----------------- File 2 ---------------------
Index	Value
Tumor_VA_114-1	100
Tumor_VA_114-2	200
---------------------------------------------

Another possibility is that somehow you are using "-" as the field
separator. 

Maybe some other software that use in between corrupted the dimnames ?

We can only guess since you have not provided neither a simple
reproducible example, your operating system or working R version. Please
read the posting guide first.


> Sorry, if I wasted any of your time.
> Cheers ../Murli
> 
> 
> 
> Liaw, Andy wrote:
> 
> >?sample says:
> >
> >x   Either a (numeric, complex, character or logical) vector of more
> >    than one element from which to choose, or a positive integer 
> >
> >so I guess it wasn't meant to be used on a data frame.  However, a data
> >frame is a list (where the variables are the components), and a list is a
> >vector, so the behavior is consistent:
> >
> >  
> >
> >>x <- list("a", 2, "c", 4, "e", "f")
> >>sample(x)
> >>    
> >>
> >[[1]]
> >[1] "c"
> >
> >[[2]]
> >[1] "a"
> >
> >[[3]]
> >[1] 4
> >
> >[[4]]
> >[1] 2
> >
> >[[5]]
> >[1] "f"
> >
> >[[6]]
> >[1] "e"
> >
> >Andy
> >
> >  
> >
> >>From: Jean Eid
> >>
> >>It seems that sample picks columns when the object is turned into a
> >>data.frame. I do not knoe why it is doing that....
> >>
> >>Is this something that was meant and not documented or something?
> >>
> >>
> >>Jean
> >>
> >>
> >>On Fri, 11 Feb 2005, Adaikalavan Ramasamy wrote:
> >>
> >>    
> >>
> >>>See below.
> >>>
> >>>On Thu, 2005-02-10 at 14:53 -0800, T. Murlidharan Nair wrote:
> >>>      
> >>>
> >>>>Just to explain my previous mail, here is the output I get.
> >>>>
> >>>>
> >>>> > dim(tissue.exp)
> >>>>[1] 1532   20
> >>>>        
> >>>>
> >>>What is this object ? Try class(tissue.exp)
> >>>
> >>>      
> >>>
> >>>> > pick<-sample(tissue.exp,5,replace=TRUE)
> >>>> > dim(pick)
> >>>>[1] 1532    5
> >>>>        
> >>>>
> >>>>From help(sample) :
> >>>
> >>>    sample(x, size, replace = FALSE, prob = NULL)
> >>>
> >>>    x: Either a (numeric, complex, character or logical) vector of
> >>>       more than one element from which to choose, or a positive
> >>>       integer.
> >>>
> >>>x has to be a vector or positive integer integer. I am not 
> >>>      
> >>>
> >>sure how you
> >>    
> >>
> >>>got "sample" to work on tissue.exp in the first place. Maybe you got
> >>>your own version of "sample".
> >>>
> >>>
> >>>      
> >>>
> >>>> > tissue.exp.t<-t(tissue.exp)
> >>>> > dim(tissue.exp.t)
> >>>>[1]   20 1532
> >>>> > pick<-sample(tissue.exp.t,5,replace=TRUE)
> >>>> > dim(pick)
> >>>>NULL
> >>>>
> >>>>--------
> >>>>Thanks ../Murli
> >>>>
> >>>>______________________________________________
> >>>>R-help at stat.math.ethz.ch mailing list
> >>>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>PLEASE do read the posting guide! 
> >>>>        
> >>>>
> >>http://www.R-project.org/posting-guide.html
> >>    
> >>
> >>>      
> >>>
> >>______________________________________________
> >>    
> >>
> >>>R-help at stat.math.ethz.ch mailing list
> >>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>PLEASE do read the posting guide! 
> >>>      
> >>>
> >>http://www.R-project.org/posting-guide.html
> >>    
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide! 
> >>http://www.R-project.org/posting-guide.html
> >>
> >>
> >>    
> >>
> >
> >
> >------------------------------------------------------------------------------
> >Notice:  This e-mail message, together with any attachments, contains information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New Jersey, USA 08889), and/or its affiliates (which may be known outside the United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as Banyu) that may be confidential, proprietary copyrighted and/or legally privileged. It is intended solely for the use of the individual or entity named on this message.  If you are not the intended recipient, and have received this message in error, please notify us immediately by reply e-mail and then delete it from your system.
> >------------------------------------------------------------------------------
> >  
> >
> 
> ---
> 
> 
> 
>



From jfox at mcmaster.ca  Sat Feb 12 03:34:46 2005
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 11 Feb 2005 21:34:46 -0500
Subject: [R] cook's distance in weighted regression
In-Reply-To: <70E1C0DB4F9B5E4F9CEDB8433F4A68B903D385E5@psbmail2.psb.bls.gov>
Message-ID: <20050212023436.JFLE19622.tomts10-srv.bellnexxia.net@JohnDesktop8300>

Dear Alan,

You can use the (generic) cooks.distance function in R, which uses the
weighted residuals. See ?cooks.distance, and stats:::cooks.distance.lm for
the function definition (i.e., the method for a linear model).

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Dorfman, Alan - BLS
> Sent: Friday, February 11, 2005 11:08 AM
> To: 'r-help at stat.math.ethz.ch'
> Subject: [R] cook's distance in weighted regression
> 
> 
> I have a puzzle as to how R is computing Cook's distance in 
> weighted linear regression.
> 
> In
> this case cook's distance should be given not  as in OLS case by
> 
> h_ii*r_i^2/(1-hii)^2 divided by k*s^2                         
>            (1)
> (where  r is plain unadjusted residual, k is number of 
> parameters in model, etc. )
> 
> but rather by 
> 
> w_ii*h_ii*r_i^2/(1-hii)^2 divided by k*s^2,                   
>          (2)
> 
> i.e. has the weight in there. Apart from the division this is 
> sum of weighted squares of differences 
> 
> yhat_j - yhat_j[i]. (That is, it is the weighted sum of 
> squares of fits minus fits with ith point deleted.)
> 
> However, a little experimentation in R, using 
> ls.diag(fit)$cooks, suggests that in weighted case R gives 
> (1) times some constant. Does anybody know how that constant 
> is calculated?  What is the rationale for using equation (1) 
> (times a constant) in the weighted case anyway?  
> Thanks.
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Sat Feb 12 04:05:01 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 11 Feb 2005 19:05:01 -0800
Subject: [R] Random Matrix theory
In-Reply-To: <420CFA15.5090002@sdsc.edu>
References: <420CFA15.5090002@sdsc.edu>
Message-ID: <420D725D.5000104@pdf.com>

      What kind of random matrices?  The following will produce a random 
orthogonal matrix.  I'm not certain of its distribution, but I think 
it's likely uniform:  I generate a matrix from rnorm(.., sd) and compute 
its singular value decomposition.  I think I saw someplace that the 
result should be essentially a random orthogonal matrix, but I'm not 
sure of that. 

rortho <- function(k=3, sd=100){
  a <- array(rnorm(k^2, sd=sd), dim=c(k,k))
  s1 <-  sample(c(-1,1), 3, replace=TRUE)
  s2 <-  sample(c(-1,1), 3, replace=TRUE)
  (s1*rep(s2, each=3)*svd(a)$u)
}

      hope this helps. 
      spencer graves

T. Murlidharan Nair wrote:

> Any package that implements Random matrix theory  application in R.
> I am completely new to this subject, so just wanted to explore it.
> Cheers ../Murli
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From jfox at mcmaster.ca  Sat Feb 12 04:20:46 2005
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 11 Feb 2005 22:20:46 -0500
Subject: [R] label outliers in boxplot and/or bwplot
In-Reply-To: <420CB6B2.7070907@gmx.ch>
Message-ID: <20050212032037.JJXE1836.tomts16-srv.bellnexxia.net@JohnDesktop8300>

Dear Christoph,

(I don't believe that this question was answered; my apologies if it was.)

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Christoph Lehmann
> Sent: Friday, February 11, 2005 8:44 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] label outliers in boxplot and/or bwplot
> 
> Hi
> 
> Is there a way to lable (e.g. observation-number) the 
> outliers in a boxplot?
> 

You can use identify() with horizontal coordinates at 1 (and at successive
integers for parallel boxplots); e.g.,

x <- c(rnorm(98), 8, 10)
boxplot(x)
identify(rep(1, 100), x)

> and in a bwplot?
> 

Not to my knowledge.

I hope this helps.
 John

> thanks a lot
> 
> Christoph
> 
> 
> P.S. identify() is not available with bwplot, is it?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From deepayan at stat.wisc.edu  Sat Feb 12 04:43:13 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Fri, 11 Feb 2005 21:43:13 -0600
Subject: [R] label outliers in boxplot and/or bwplot
In-Reply-To: <20050212032037.JJXE1836.tomts16-srv.bellnexxia.net@JohnDesktop8300>
References: <20050212032037.JJXE1836.tomts16-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <200502112143.14016.deepayan@stat.wisc.edu>

On Friday 11 February 2005 21:20, John Fox wrote:
> Dear Christoph,
>
> (I don't believe that this question was answered; my apologies if it
> was.)
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
> > Christoph Lehmann
> > Sent: Friday, February 11, 2005 8:44 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] label outliers in boxplot and/or bwplot
> >
> > Hi
> >
> > Is there a way to lable (e.g. observation-number) the
> > outliers in a boxplot?
>
> You can use identify() with horizontal coordinates at 1 (and at
> successive integers for parallel boxplots); e.g.,
>
> x <- c(rnorm(98), 8, 10)
> boxplot(x)
> identify(rep(1, 100), x)
>
> > and in a bwplot?
>
> Not to my knowledge.

Actually, it turns out that the obvious analog works with bwplot too 
(though it's a bit weird that you can 'identify' points that are not 
actually plotted). See below.

> I hope this helps.
>  John
>
> > thanks a lot
> >
> > Christoph
> >
> >
> > P.S. identify() is not available with bwplot, is it?

There's panel.identify(). You could use it as part of your panel 
function, but in this case it might be more natural to do it after the 
fact, as in:

bwplot(x)
trellis.focus("panel", 1, 1)
panel.identify()
trellis.unfocus()

This has the added advantage that you don't have to specify 'x' and 'y' 
explicitly, they're taken from the corresponding panel data.

Deepayan



From ggrothendieck at myway.com  Sat Feb 12 07:23:25 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 12 Feb 2005 06:23:25 +0000 (UTC)
Subject: [R] Problem with "R CMD Rd2dvi": Rd.sty not found
References: <9CC1B717EF3BD511AD98000103D63FC53FA754@us-arl-asg.mail.saic.com>
	<Pine.LNX.4.61.0502102220560.31066@gannet.stats>
	<loom.20050211T005104-323@post.gmane.org>
Message-ID: <loom.20050212T072032-304@post.gmane.org>

Gabor Grothendieck <ggrothendieck <at> myway.com> writes:

: 
: Prof Brian Ripley <ripley <at> stats.ox.ac.uk> writes:
: 
: : 
: : This is a problem with a recent-ish change to MikTeX to make it use 
: : non-standard paths: see the notes on Duncan Murdoch's site at
: : 
: :    http://www.murdoch-sutherland.com/Rtools/miktex.html
: : 
: : Fptex is a standard distribution e.g. from TexLive.
: 
: I initially started with MikTeX but then had the texinput problem
: while trying to generate vignettes for a package so
: tried fptex but found that the latter was huge (the .bat file
: downloads about half a gig of files) and Miktex seems more polished.  
: The most important shortcoming is that fptex lacks texi2dvi and while
: Miktex lacks this too it has a substitute, texify.exe, which can
: be used instead whereas fptex does not.  The Miktex developer told
: me that the underlying tex engine he is using is missing the texinput
: part and as soon as that is corrected he will incorporate it
: into miktex.  This was quite a while ago and the situation 
: may have changed somewhat.  I think some people have had good expeiences
: with fptex but they probably were not trying to develop packages with
: vignettes and certain other tasks that involve texinput.  I think had 
: I known all this I just would have stuck to miktex and done the extra
: configuration and saved myself having to download the huge fptex 
: distribution.

Just a follow up on this.

I found out from Marc Paterno, off list, that the latest
MiKTeX distribution has texi2dvi.exe .  This should
simplify using MiKTeX with R.  (Also, one MiKTeX
gotcha is to be sure to read the 'Refreshing Format Files'
section of the MiKTeX manual after installing MiKTeX and
follow the directions there.)



From tiago17 at socrates.Berkeley.EDU  Sat Feb 12 09:17:03 2005
From: tiago17 at socrates.Berkeley.EDU (Tiago R Magalhaes)
Date: Sat, 12 Feb 2005 00:17:03 -0800
Subject: [R] data.frame into list by columns; merge and row.names
Message-ID: <p06100501be33022f1711@[192.168.0.103]>

Hi

a)
I want to make a list out of a data.frame, where each element of the 
list is a column of the data.frame.
I looked in the archives and saw a lot of postings but surprsingly 
none elucidated me. I also tried the split, aggregate help files and 
counldn't see any easy way to do this. I wouldn't be surprised if 
it's there, but I really didn't see it.

I solved the problem using a very convoluted way:

x <- data.frame(a=sample(10), b=sample(10), c=sample(10))
f <- factor(names(x), levels=names(x))
xx <- data.frame(f=f, t(x))
xlist.transpose <- split(xx, xx$f)
xlist <- lapply(xlist, function(x) x=t(x))

I am very convinced there's a much easier way, so if any of you 
people enlighten me I would appreciate

b)
In terms of my own personal use, it would be much better that merge 
when using 'row.names' as the by argument would output a data.frame 
with the merged row.names and not a column 'Row.names'.

Also it would be great if it would be possible to choose wich sorting 
would be the final one - right now the default is argument y, but 
sometimes argument x is much more useful
Of course these are minor points and can be dealt very easily after 
calling merge, but here it goes my comment anyway.



From ssk2031 at columbia.edu  Sat Feb 12 12:11:31 2005
From: ssk2031 at columbia.edu (Suresh Krishna)
Date: Sat, 12 Feb 2005 06:11:31 -0500
Subject: [R] comparing predicted sequence A'(t) to observed sequence A(t)
Message-ID: <420DE463.8080009@columbia.edu>


Hi,

I have a question that I have not been succesful in finding a definitive 
answer to; and I was hoping someone here could give me some pointers to 
the right place in the literature.

A. We have 4 sets of data, A(t), B(t), C(t), and D(t). Each of these 
consists of a series of counts obtained in sequential time-intervals: so 
  for example, A(t) would be something like:

Count A(t):  25,    28,    26,   34   ......
Time (ms):  0-10, 10-20, 20-30, 30-40 .......

Each count in the series A(t) is obtained by summing the total number of 
observed counts over multiple (say 50), independent repetitions of that 
time-series. These counts are generally known to be Poisson distributed, 
and the 4 processes A(t), B(t), C(t) and D(t) are independent of each other.

B. It appears on visual observation that the following relationship 
holds; and such a relationship would also be expected on mechanistic 
considerations.

A(t) = B(t) + C(t) - D(t)

We now want to test this hypothesis statistically.

Because successive counts in the sequence are likely to be correlated, 
isnt it true that none of these methods are valid ? Perhaps for other 
reasons as well ?

a)Doing a chi-squared test to see if the predicted curve for A(t) 
deviates significantly from the observed A(t); this also seems to not 
take the variability of the predicted curve into account.

b)Doing a regression of the predicted values of A(t) against the actual 
values of A(t) and checking for deviations of slope from 1 and intercept 
from 0 ? Here, in addition to lack of independence, the fact that 
X-values are not fixed (i.e. are variable) and the fact that X and Y are 
Poisson distributed counts should also be taken into account, right ?

I would be very grateful if someone could point me to methods to handle 
this kind of situation, or where to look for them. Is there something in 
the time-series literature, for instance ?

Thanks !!

Suresh



From andy_liaw at merck.com  Sat Feb 12 13:52:01 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sat, 12 Feb 2005 07:52:01 -0500
Subject: [R] data.frame into list by columns; merge and row.names
Message-ID: <3A822319EB35174CA3714066D590DCD50994E6C4@usrymx25.merck.com>

> From: Tiago R Magalhaes
> 
> Hi
> 
> a)
> I want to make a list out of a data.frame, where each element of the 
> list is a column of the data.frame.
> I looked in the archives and saw a lot of postings but surprsingly 
> none elucidated me. I also tried the split, aggregate help files and 
> counldn't see any easy way to do this. I wouldn't be surprised if 
> it's there, but I really didn't see it.
> 
> I solved the problem using a very convoluted way:
> 
> x <- data.frame(a=sample(10), b=sample(10), c=sample(10))
> f <- factor(names(x), levels=names(x))
> xx <- data.frame(f=f, t(x))
> xlist.transpose <- split(xx, xx$f)
> xlist <- lapply(xlist, function(x) x=t(x))
> 
> I am very convinced there's a much easier way, so if any of you 
> people enlighten me I would appreciate

1. Please make sure the code you show actually works.  The last line
doesn't.

2. I'm not sure what you want to do.  A data frame is already a list.  If
you want it to be just a list, just use as.list(x).  If you want a list
where each component is a data frame with one column, use lapply(x,
as.data.frame).

Andy
 
> b)
> In terms of my own personal use, it would be much better that merge 
> when using 'row.names' as the by argument would output a data.frame 
> with the merged row.names and not a column 'Row.names'.
> 
> Also it would be great if it would be possible to choose wich sorting 
> would be the final one - right now the default is argument y, but 
> sometimes argument x is much more useful
> Of course these are minor points and can be dealt very easily after 
> calling merge, but here it goes my comment anyway.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ggrothendieck at myway.com  Sat Feb 12 15:16:12 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 12 Feb 2005 14:16:12 +0000 (UTC)
Subject: [R] data.frame into list by columns; merge and row.names
References: <p06100501be33022f1711@[192.168.0.103]>
Message-ID: <loom.20050212T151222-336@post.gmane.org>

Tiago R Magalhaes <tiago17 <at> socrates.Berkeley.EDU> writes:

: 
: Hi
: 
: a)
: I want to make a list out of a data.frame, where each element of the 
: list is a column of the data.frame.
: I looked in the archives and saw a lot of postings but surprsingly 
: none elucidated me. I also tried the split, aggregate help files and 
: counldn't see any easy way to do this. I wouldn't be surprised if 
: it's there, but I really didn't see it.
: 
: I solved the problem using a very convoluted way:
: 
: x <- data.frame(a=sample(10), b=sample(10), c=sample(10))
: f <- factor(names(x), levels=names(x))
: xx <- data.frame(f=f, t(x))
: xlist.transpose <- split(xx, xx$f)
: xlist <- lapply(xlist, function(x) x=t(x))
: 
: I am very convinced there's a much easier way, so if any of you 
: people enlighten me I would appreciate

A data frame _is_ a list.  You could do this:

class(x) <- "list"  # x is from above

but you may not even need to do that and may be able to use
x directly depending on what you need to do next.

: 
: b)
: In terms of my own personal use, it would be much better that merge 
: when using 'row.names' as the by argument would output a data.frame 
: with the merged row.names and not a column 'Row.names'.
: 
: Also it would be great if it would be possible to choose wich sorting 
: would be the final one - right now the default is argument y, but 
: sometimes argument x is much more useful
: Of course these are minor points and can be dealt very easily after 
: calling merge, but here it goes my comment anyway.



From jfox at mcmaster.ca  Sat Feb 12 16:04:05 2005
From: jfox at mcmaster.ca (John Fox)
Date: Sat, 12 Feb 2005 10:04:05 -0500
Subject: [R] label outliers in boxplot and/or bwplot
In-Reply-To: <200502112143.14016.deepayan@stat.wisc.edu>
Message-ID: <20050212150356.MLAP1836.tomts16-srv.bellnexxia.net@JohnDesktop8300>

Dear Deepayan,

I wasn't aware of panel.identify(), which I too will find very useful. As
you say, it is strange that you can identify points that aren't plotted;
this is the case for my solution for boxplot() as well -- though the reason
it works there is more obvious (at least to me).

Thanks,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: Deepayan Sarkar [mailto:deepayan at stat.wisc.edu] 
> Sent: Friday, February 11, 2005 10:43 PM
> To: r-help at stat.math.ethz.ch
> Cc: John Fox; 'Christoph Lehmann'
> Subject: Re: [R] label outliers in boxplot and/or bwplot
> 
> On Friday 11 February 2005 21:20, John Fox wrote:
> > Dear Christoph,
> >
> > (I don't believe that this question was answered; my apologies if it
> > was.)
> >
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch 
> > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Christoph 
> > > Lehmann
> > > Sent: Friday, February 11, 2005 8:44 AM
> > > To: r-help at stat.math.ethz.ch
> > > Subject: [R] label outliers in boxplot and/or bwplot
> > >
> > > Hi
> > >
> > > Is there a way to lable (e.g. observation-number) the 
> outliers in a 
> > > boxplot?
> >
> > You can use identify() with horizontal coordinates at 1 (and at 
> > successive integers for parallel boxplots); e.g.,
> >
> > x <- c(rnorm(98), 8, 10)
> > boxplot(x)
> > identify(rep(1, 100), x)
> >
> > > and in a bwplot?
> >
> > Not to my knowledge.
> 
> Actually, it turns out that the obvious analog works with 
> bwplot too (though it's a bit weird that you can 'identify' 
> points that are not actually plotted). See below.
> 
> > I hope this helps.
> >  John
> >
> > > thanks a lot
> > >
> > > Christoph
> > >
> > >
> > > P.S. identify() is not available with bwplot, is it?
> 
> There's panel.identify(). You could use it as part of your 
> panel function, but in this case it might be more natural to 
> do it after the fact, as in:
> 
> bwplot(x)
> trellis.focus("panel", 1, 1)
> panel.identify()
> trellis.unfocus()
> 
> This has the added advantage that you don't have to specify 
> 'x' and 'y' 
> explicitly, they're taken from the corresponding panel data.
> 
> Deepayan



From spencer.graves at pdf.com  Sat Feb 12 16:35:59 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 12 Feb 2005 07:35:59 -0800
Subject: [R] comparing predicted sequence A'(t) to observed sequence A(t)
In-Reply-To: <420DE463.8080009@columbia.edu>
References: <420DE463.8080009@columbia.edu>
Message-ID: <420E225F.4050209@pdf.com>

      What do you mean by the following: 

      A(t) = B(t) + C(t) - D(t)? 

      Since you speak of regressing predicted against actual A(t), I 
gather this is not what you mean. 

      Another question:  Do you have numbers <-0 for either predicted or 
actual A(t)?  If yes but only a very few, I might replace the 0's by 0.5 
and any negatives by 0.25, take their logarithms, then try acf, pacf, 
ar, arima(..., xreg=A.pred), etc. 

      There are doubtless better methods.  However, if I had to have an 
answer today, I think I'd try this, then discuss implications and 
limitations.  If I needed a more sophisticated answer and I had a few 
weeks or months to work on it, I might develop some way to simulate a 
process that seemed to describe what I thought generated these numbers 
and compare simulated results with actual, under a variety of 
hypotheses, obtaining various kinds of p-values, etc. 

      hope this helps. 
      spencer graves

Suresh Krishna wrote:

>
> Hi,
>
> I have a question that I have not been succesful in finding a 
> definitive answer to; and I was hoping someone here could give me some 
> pointers to the right place in the literature.
>
> A. We have 4 sets of data, A(t), B(t), C(t), and D(t). Each of these 
> consists of a series of counts obtained in sequential time-intervals: 
> so  for example, A(t) would be something like:
>
> Count A(t):  25,    28,    26,   34   ......
> Time (ms):  0-10, 10-20, 20-30, 30-40 .......
>
> Each count in the series A(t) is obtained by summing the total number 
> of observed counts over multiple (say 50), independent repetitions of 
> that time-series. These counts are generally known to be Poisson 
> distributed, and the 4 processes A(t), B(t), C(t) and D(t) are 
> independent of each other.
>
> B. It appears on visual observation that the following relationship 
> holds; and such a relationship would also be expected on mechanistic 
> considerations.
>
> A(t) = B(t) + C(t) - D(t)
>
> We now want to test this hypothesis statistically.
>
> Because successive counts in the sequence are likely to be correlated, 
> isnt it true that none of these methods are valid ? Perhaps for other 
> reasons as well ?
>
> a)Doing a chi-squared test to see if the predicted curve for A(t) 
> deviates significantly from the observed A(t); this also seems to not 
> take the variability of the predicted curve into account.
>
> b)Doing a regression of the predicted values of A(t) against the 
> actual values of A(t) and checking for deviations of slope from 1 and 
> intercept from 0 ? Here, in addition to lack of independence, the fact 
> that X-values are not fixed (i.e. are variable) and the fact that X 
> and Y are Poisson distributed counts should also be taken into 
> account, right ?
>
> I would be very grateful if someone could point me to methods to 
> handle this kind of situation, or where to look for them. Is there 
> something in the time-series literature, for instance ?
>
> Thanks !!
>
> Suresh
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ssk2031 at columbia.edu  Sat Feb 12 16:41:22 2005
From: ssk2031 at columbia.edu (Suresh Krishna)
Date: Sat, 12 Feb 2005 10:41:22 -0500
Subject: [R] comparing predicted sequence A'(t) to observed sequence A(t)
In-Reply-To: <420E225F.4050209@pdf.com>
References: <420DE463.8080009@columbia.edu> <420E225F.4050209@pdf.com>
Message-ID: <420E23A2.3020905@columbia.edu>


Hi,

Thanks for your quick response !

By predicted A(t), I meant B(t) + C(t) - D(t). In other words, how well 
does B(t) + C(t) - D(t) approximate A(t) ?

And all the counts are non-negative.

Regards, Suresh

Spencer Graves wrote:
>      What do you mean by the following:
>      A(t) = B(t) + C(t) - D(t)?
>      Since you speak of regressing predicted against actual A(t), I 
> gather this is not what you mean.
>      Another question:  Do you have numbers <-0 for either predicted or 
> actual A(t)?  If yes but only a very few, I might replace the 0's by 0.5 
> and any negatives by 0.25, take their logarithms, then try acf, pacf, 
> ar, arima(..., xreg=A.pred), etc.
>      There are doubtless better methods.  However, if I had to have an 
> answer today, I think I'd try this, then discuss implications and 
> limitations.  If I needed a more sophisticated answer and I had a few 
> weeks or months to work on it, I might develop some way to simulate a 
> process that seemed to describe what I thought generated these numbers 
> and compare simulated results with actual, under a variety of 
> hypotheses, obtaining various kinds of p-values, etc.
>      hope this helps.      spencer graves
> 
> Suresh Krishna wrote:
> 
>>
>> Hi,
>>
>> I have a question that I have not been succesful in finding a 
>> definitive answer to; and I was hoping someone here could give me some 
>> pointers to the right place in the literature.
>>
>> A. We have 4 sets of data, A(t), B(t), C(t), and D(t). Each of these 
>> consists of a series of counts obtained in sequential time-intervals: 
>> so  for example, A(t) would be something like:
>>
>> Count A(t):  25,    28,    26,   34   ......
>> Time (ms):  0-10, 10-20, 20-30, 30-40 .......
>>
>> Each count in the series A(t) is obtained by summing the total number 
>> of observed counts over multiple (say 50), independent repetitions of 
>> that time-series. These counts are generally known to be Poisson 
>> distributed, and the 4 processes A(t), B(t), C(t) and D(t) are 
>> independent of each other.
>>
>> B. It appears on visual observation that the following relationship 
>> holds; and such a relationship would also be expected on mechanistic 
>> considerations.
>>
>> A(t) = B(t) + C(t) - D(t)
>>
>> We now want to test this hypothesis statistically.
>>
>> Because successive counts in the sequence are likely to be correlated, 
>> isnt it true that none of these methods are valid ? Perhaps for other 
>> reasons as well ?
>>
>> a)Doing a chi-squared test to see if the predicted curve for A(t) 
>> deviates significantly from the observed A(t); this also seems to not 
>> take the variability of the predicted curve into account.
>>
>> b)Doing a regression of the predicted values of A(t) against the 
>> actual values of A(t) and checking for deviations of slope from 1 and 
>> intercept from 0 ? Here, in addition to lack of independence, the fact 
>> that X-values are not fixed (i.e. are variable) and the fact that X 
>> and Y are Poisson distributed counts should also be taken into 
>> account, right ?
>>
>> I would be very grateful if someone could point me to methods to 
>> handle this kind of situation, or where to look for them. Is there 
>> something in the time-series literature, for instance ?
>>
>> Thanks !!
>>
>> Suresh
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
> 
> 
>



From nielssteenkrogh at zitelab.dk  Sat Feb 12 16:56:45 2005
From: nielssteenkrogh at zitelab.dk (Niels Steen Krogh)
Date: Sat, 12 Feb 2005 16:56:45 +0100
Subject: [R] Saving graphs in formats other than PDF? 
Message-ID: <20050212155208.M75218@zitelab.dk>

We use the snap-shot-function in acrobat-reader (version 6.0+). 

My output from R can be easily scaled in acrobat-reader, and then copy-pasted
into officedocuments via the snapshot-function. 

I haven't tried the snap-shot-function on mac - but I guess ITs the same as on
windoze. 

Hope this helps. 

Niels Steen Krogh
Konsulent
ZiteLab

Mail: ---------- nielssteenkrogh at zitelab.dk
Telefon: ------- +45 38 88 86 13
Mobil: --------- +45 22 67 37 38
Adresse: ------- Zitelab
---------------- Solsortvej 44
---------------- 2000 F.

ZiteLab
-Let's Empower Your Data with Webservices



From deepayan at stat.wisc.edu  Sat Feb 12 17:01:05 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Sat, 12 Feb 2005 10:01:05 -0600
Subject: [R] label outliers in boxplot and/or bwplot
In-Reply-To: <20050212150356.MLAP1836.tomts16-srv.bellnexxia.net@JohnDesktop8300>
References: <20050212150356.MLAP1836.tomts16-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <200502121001.05144.deepayan@stat.wisc.edu>

On Saturday 12 February 2005 09:04, John Fox wrote:
> Dear Deepayan,
>
> I wasn't aware of panel.identify(), which I too will find very
> useful. As you say, it is strange that you can identify points that
> aren't plotted; this is the case for my solution for boxplot() as
> well -- though the reason it works there is more obvious (at least to
> me).

It's the same reason really (although it might not have worked if 
panel.bwplot had different arguments). Say you have 

bwplot(gl(2, 5) ~ 1:10)

Then the panel function will get arguments 

x = 1, 2, 3, 4, 5, 6, 7, 8, 9,10
y = 1, 1, 1, 1, 1, 2, 2, 2, 2, 2

which are eventually supplied to panel.identify(), just as in your 
identify() example. The only difference is that panel.identify has 
reasonable defaults for x and y (namely the x and y passed to the 
corresponding panel), which happen to work in this case. This is 
possible because the trellis object is saved (by default) when it's 
printed, and so it's panel arguments can be recovered.

Deepayan



From ligges at statistik.uni-dortmund.de  Sat Feb 12 17:42:49 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 12 Feb 2005 17:42:49 +0100
Subject: [R] Profiles graphics in a contingency table
In-Reply-To: <008d01c5104b$4263b730$a74176c8@PARDOBERNAL>
References: <008d01c5104b$4263b730$a74176c8@PARDOBERNAL>
Message-ID: <420E3209.4010608@statistik.uni-dortmund.de>

Campo El?as PARDO wrote:

> Dear Users,
> 
> 
> How can I obtain a profiles graphic in a CT similar to Excel. 

Do you mean a mosaicplot? If not, what do you mean with "profiles 
graphic" and what does "CT" stand for?

[This is one of the very few unanswered (besides an enormous quantity of 
answered) mails within the last 24 hours. Now guess why.]

Uwe Ligges



> Campo Elias  PARDO
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From kjetil at acelerate.com  Sat Feb 12 15:46:57 2005
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Sat, 12 Feb 2005 10:46:57 -0400
Subject: [R] Profiles graphics in a contingency table
In-Reply-To: <008d01c5104b$4263b730$a74176c8@PARDOBERNAL>
References: <008d01c5104b$4263b730$a74176c8@PARDOBERNAL>
Message-ID: <420E16E1.5030704@acelerate.com>

Campo El?as PARDO wrote:

>Dear Users,
>
>
>How can I obtain a profiles graphic in a CT similar to Excel. 
>
>Campo Elias  PARDO
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>  
>
If you mean profiles as in correspondence analysis, just do:

( assuming your table is mytab)
N <- sum(mytab)
Nr <- rowSums(mytab)
Nc <- colSums(mytab)
perfRow <- mytab
for (i in 1:NROW(mytab)) }{
     perfRow[i,] <- perfRow[i,]/Nr[i] }

perfRow

and then you can just plot them, maybe using lines().

But have a look into ?mosaicplot
Kjetil

-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra





-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From rog at stanford.edu  Sat Feb 12 20:21:35 2005
From: rog at stanford.edu (Roger Levy)
Date: 12 Feb 2005 11:21:35 -0800
Subject: [R] question on indexing of a matrix
Message-ID: <253bw1iny8.fsf@joel.Stanford.EDU>

Hi,

I have a k-level factor F of length n that I would like to use to
extract from an n-by-k matrix M a vector V such that

  V[i] = M[i,as.numeric(F)[i]]

I don't currently understand how to do that in R -- can anyone explain
to me how to do so?

Thanks,

Roger Levy



From ramasamy at cancer.org.uk  Sat Feb 12 21:10:20 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Sat, 12 Feb 2005 20:10:20 +0000
Subject: [R] question on indexing of a matrix
In-Reply-To: <253bw1iny8.fsf@joel.Stanford.EDU>
References: <253bw1iny8.fsf@joel.Stanford.EDU>
Message-ID: <1108239020.7533.13.camel@dhcp-63.ccc.ox.ac.uk>

Extract what ? Can you please provide a simple example of how the input
looks like and the desired output. 


Here is an example of how to subscript a matrix columns from the first
level of a factor (or you can replace 1 with i for the ith level) :

f <- factor( rep( letters[1:4], 3) )
f
 [1] a b c d a b c d a b c d
Levels: a b c d

levels(f)
[1] "a" "b" "c" "d"

w <- which( f == levels(f)[1] )  # equivalent to which( f == "a" )
[1] 1 5 9


matrix <- matrix( rnorm(30), nc=10 )
matrix[ , w ]                    # returns the 1st, 5th and 9th columns 

Alternatively you can try matrix[ , which( as.numeric(f) == 1 ) ].


Note that matrix[ x, y ] subsets both the rows and columns of a matrix
which I do think is what you want to do.


Try http://cran.r-project.org/doc/manuals/R-intro.html#Array-indexing or
help("[") for how to subset a matrix or searching the mail archives.

Regards, Adai



On Sat, 2005-02-12 at 11:21 -0800, Roger Levy wrote:
> Hi,
> 
> I have a k-level factor F of length n that I would like to use to
> extract from an n-by-k matrix M a vector V such that
> 
>   V[i] = M[i,as.numeric(F)[i]]
> 
> I don't currently understand how to do that in R -- can anyone explain
> to me how to do so?
> 
> Thanks,
> 
> Roger Levy
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Roger.Bivand at nhh.no  Sat Feb 12 21:26:46 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Sat, 12 Feb 2005 21:26:46 +0100 (CET)
Subject: [R] question on indexing of a matrix
In-Reply-To: <253bw1iny8.fsf@joel.Stanford.EDU>
Message-ID: <Pine.LNX.4.44.0502122116490.5140-100000@reclus.nhh.no>

On 12 Feb 2005, Roger Levy wrote:

> Hi,
> 
> I have a k-level factor F of length n that I would like to use to
> extract from an n-by-k matrix M a vector V such that
> 
>   V[i] = M[i,as.numeric(F)[i]]
> 
> I don't currently understand how to do that in R -- can anyone explain
> to me how to do so?

This may be an answer:

> k <- 3
> n <- 10
> M <- matrix(1:(k*n), ncol=k, nrow=n)
> M
      [,1] [,2] [,3]
 [1,]    1   11   21
 [2,]    2   12   22
 [3,]    3   13   23
 [4,]    4   14   24
 [5,]    5   15   25
 [6,]    6   16   26
 [7,]    7   17   27
 [8,]    8   18   28
 [9,]    9   19   29
[10,]   10   20   30
> set.seed(1234)
> K <- factor(sample(1:3, n, replace=TRUE))
> K
 [1] 1 2 2 2 3 2 1 1 2 2
Levels: 1 2 3
> Kmm <- model.matrix(terms(~ K-1))
> Kmm
   K1 K2 K3
1   1  0  0
2   0  1  0
3   0  1  0
4   0  1  0
5   0  0  1
6   0  1  0
7   1  0  0
8   1  0  0
9   0  1  0
10  0  1  0
attr(,"assign")
[1] 1 1 1
attr(,"contrasts")
attr(,"contrasts")$K
[1] "contr.treatment"

> rowSums(M * Kmm)
 1  2  3  4  5  6  7  8  9 10 
 1 12 13 14 25 16  7  8 19 20 

My understanding is that you want to extract the K[i]th column from the 
[i]th row of M, so the model matrix of K should zero out the values you 
don't want. Using something like this in a function would mean checking K 
carefully for the number of levels actually present and their coding. 

> 
> Thanks,
> 
> Roger Levy
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From choudary.jagar at swosu.edu  Sat Feb 12 22:45:45 2005
From: choudary.jagar at swosu.edu (Jagarlamudi, Choudary)
Date: Sat, 12 Feb 2005 15:45:45 -0600
Subject: [R] Trimming Parallel Vectors.
Message-ID: <E03EBB50FF2C024781A6E4460AD58F0607C184@swosu-mbx01.admin.swosu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050212/27d9d23b/attachment.pl

From p.dalgaard at biostat.ku.dk  Sat Feb 12 22:50:51 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 12 Feb 2005 22:50:51 +0100
Subject: [R] question on indexing of a matrix
In-Reply-To: <Pine.LNX.4.44.0502122116490.5140-100000@reclus.nhh.no>
References: <Pine.LNX.4.44.0502122116490.5140-100000@reclus.nhh.no>
Message-ID: <x2mzu978hw.fsf@biostat.ku.dk>

Roger Bivand <Roger.Bivand at nhh.no> writes:

> On 12 Feb 2005, Roger Levy wrote:
> 
> > Hi,
> > 
> > I have a k-level factor F of length n that I would like to use to
> > extract from an n-by-k matrix M a vector V such that
> > 
> >   V[i] = M[i,as.numeric(F)[i]]
> > 
> > I don't currently understand how to do that in R -- can anyone explain
> > to me how to do so?
> 
> This may be an answer:
> 
> > k <- 3
> > n <- 10
> > M <- matrix(1:(k*n), ncol=k, nrow=n)
> > M
>       [,1] [,2] [,3]
>  [1,]    1   11   21
>  [2,]    2   12   22
>  [3,]    3   13   23
>  [4,]    4   14   24
>  [5,]    5   15   25
>  [6,]    6   16   26
>  [7,]    7   17   27
>  [8,]    8   18   28
>  [9,]    9   19   29
> [10,]   10   20   30
> > set.seed(1234)
> > K <- factor(sample(1:3, n, replace=TRUE))
> > K
>  [1] 1 2 2 2 3 2 1 1 2 2
> Levels: 1 2 3
> > Kmm <- model.matrix(terms(~ K-1))
> > Kmm
>    K1 K2 K3
> 1   1  0  0
> 2   0  1  0
> 3   0  1  0
> 4   0  1  0
> 5   0  0  1
> 6   0  1  0
> 7   1  0  0
> 8   1  0  0
> 9   0  1  0
> 10  0  1  0
> attr(,"assign")
> [1] 1 1 1
> attr(,"contrasts")
> attr(,"contrasts")$K
> [1] "contr.treatment"
> 
> > rowSums(M * Kmm)
>  1  2  3  4  5  6  7  8  9 10 
>  1 12 13 14 25 16  7  8 19 20 
> 
> My understanding is that you want to extract the K[i]th column from the 
> [i]th row of M, so the model matrix of K should zero out the values you 
> don't want. Using something like this in a function would mean checking K 
> carefully for the number of levels actually present and their coding. 

This looks simpler:

>  M[cbind(seq(along=K),as.numeric(K))]
 [1]  1 12 13 14 25 16  7  8 19 20


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From tomas.willebrand at szooek.slu.se  Sat Feb 12 23:55:53 2005
From: tomas.willebrand at szooek.slu.se (Tomas Willebrand)
Date: Sat, 12 Feb 2005 23:55:53 +0100
Subject: [R] Paste script in R stops before end of file
Message-ID: <200502122355.53284.tomas.willebrand@szooek.slu.se>

Hi!

I can only paste a limited amount of text into R (which is started in KDEs 
Konsole). The script I have problem with stops pasting after 4224 characters, 
and 78 lines. Although I can get Rs' attention at that point by just pressing 
Return.  The script mainly consists of a a large amount of vectors that I 
would like to join into a dataset for further analysis. Example of vector: 
AC <- 
c(1700,18000,18000,20000,19000,14500,24000,3421,3770,5045,11400,5775,4944,6765,13557,5200,7930,16400,14500,8900,10500,17000,18000,23500,28700,33200,26000,20000,27500,25100,20900,18500,27400,25000,12200,12200,8203,6958,8455,8828,7707,5217,5722)

There is no problem pasting the total script into different editors, and I can 
paste all lines into vi when it is run in the Konsole. I can also divide the 
script i 3 different parts that R would read through pasting. 

I use R 2.01 on a machine that have SuSE 9.2 with KDE3.3.2, kernel-smp 
2.6.8-24.11 installed and an Intel 3 GHz, 1056 MB RAM. I have set the coding 
to ISO8859-1.

What puzzles me is that I have no problem pasting in the whole script in one 
chunck using R 2.0.1 with SuSE 9.2 on a ThinkpadX40.

Is this likely to be a problem with my R installation? Any help is 
appriciated.

/Tomas



From bates at stat.wisc.edu  Sun Feb 13 00:27:26 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 12 Feb 2005 17:27:26 -0600
Subject: [R] Trimming Parallel Vectors.
In-Reply-To: <E03EBB50FF2C024781A6E4460AD58F0607C184@swosu-mbx01.admin.swosu.edu>
References: <E03EBB50FF2C024781A6E4460AD58F0607C184@swosu-mbx01.admin.swosu.edu>
Message-ID: <420E90DE.4020106@stat.wisc.edu>

Jagarlamudi, Choudary wrote:
> Good Day All !
> I have a 2-D vector of mode numeric and a parallel 1-D vector of mode numeric. 
> Here are my values.
> 2-D vector         1-D vector
> 80 75 85 80       80
> 70 80 90 80       80  
> 60 70 80 70       70
> 85 75 95 85       85
> 70 60 90 60       70
> My 1-D vector is the average across the rows of my 2-D vector.
> I process each column of the 2-D vector in a loop.
> When I trim my 2-D vector column for values less than 65, I have to trim the corrosponding average value.
> I have a copy of the 1-D vector to recreate the original 1-D vector for each loop iteration.
> My original vectors consist of 54,000 values and I am trying to avoid using loops as much as I can to gain speed.
> Any advice will be greatly appreciated.
>  
> Thanks in advance.
>  
> Choudary Jagarlamudi
> Instructor
> Southwestern Oklahoma State University
> STF 254
> 100 campus Drive
> Weatherford OK 73096
> Tel 580-774-7136

I'm not sure if this is what you had in mind but the rowMeans function 
produces the row-wise means of a matrix.  If you replace some of the 
values in the matrix by the missing value code NA then the means for 
those rows also end up as NA.

 > mat
      [,1] [,2] [,3] [,4]
[1,]   80   75   85   80
[2,]   70   80   90   80
[3,]   60   70   80   70
[4,]   85   75   95   85
[5,]   70   60   90   60
 > rowMeans(mat)
[1] 80 80 70 85 70
 > mat[mat < 65] <- NA
 > mat
      [,1] [,2] [,3] [,4]
[1,]   80   75   85   80
[2,]   70   80   90   80
[3,]   NA   70   80   70
[4,]   85   75   95   85
[5,]   70   NA   90   NA
 > rowMeans(mat)
[1] 80 80 NA 85 NA



From Gregor.Gorjanc at bfro.uni-lj.si  Sun Feb 13 03:04:18 2005
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Sun, 13 Feb 2005 03:04:18 +0100
Subject: [R] corrupt data frame: columns will be truncated or padded with
	NAs in: format.data.frame(x, digits = digits)
Message-ID: <7FFEE688B57D7346BC6241C55900E730B6FF06@pollux.bfro.uni-lj.si>

Hello R users!

I have written one function (look at the end), which will ease my 
work with analysis of data in another programme, for which I need 
sometimes a special data structure. However I encountered several
problems with a created data frame.

---------------------------------------------------------------

The data frame (produced from the example at the end) looks like 
the way I want and is:

   c1 c2 f2 f1       y1.A       y2.A       y1.B        y2.B
1   1  2  M  A -1.2776840 -1.4695219         NA          NA
3   3  6  M  A  0.1593941  0.7581128         NA          NA
5   5 10  M  A  1.1085950  0.8556062         NA          NA
7   7 14  F  A -1.8259281  3.0675536         NA          NA
9   9 18  F  A  0.8017311 -0.1056571         NA          NA
2   2  4  M  B       <NA>       <NA>  0.3577166  0.27310051
4   4  8  M  B       <NA>       <NA> -0.8021399 -1.10060507
6   6 12  F  B       <NA>       <NA> -0.4912098  0.04526153
8   8 16  F  B       <NA>       <NA> -1.2522998 -1.03796810
10 10 20  F  B       <NA>       <NA> -0.3446779  0.53854276
Warning message: 
corrupt data frame: columns will be truncated or padded with 
NAs in: format.data.frame(x, digits = digits)

Is this data frame really corrupted as R points out? 

---------------------------------------------------------------

Then I have a problem with this function if there is also a
factor column between other columns i.e. columns that are
being "divided" according to levels. For example this call

mt.by.factor(x=data, factor="f1", common=c("c1", "c2")))

gives me:

   c1 c2 f1        y1.A        y2.A f2.A         y1.B       y2.B f2.B
1   1  2  A -0.02040825 -0.28686293    2           NA         NA   NA
3   3  6  A -0.60497978  0.84527030    2           NA         NA   NA
5   5 10  A -0.74968516 -0.01094755    2           NA         NA   NA
7   7 14  A  0.07658122 -0.30101228    1           NA         NA   NA
9   9 18  A -0.68788670 -0.02177379    1           NA         NA   NA
2   2  4  B        <NA>        <NA> <NA>  0.003037107  0.4067418    2
4   4  8  B        <NA>        <NA> <NA> -0.035371363 -1.9397670    2
6   6 12  B        <NA>        <NA> <NA>  0.970424682 -1.3881620    1
8   8 16  B        <NA>        <NA> <NA> -1.169746470  0.7670071    1
10 10 20  B        <NA>        <NA> <NA>  1.238606959 -0.1831825    1
Warning message: 
corrupt data frame: columns will be truncated or padded with NAs in: 
format.data.frame(x, digits = digits)

Why are factor columns 'f2.A' and 'f2.B' now represented as integers?
It looks like that I lost somewhere the factor class but I do not
know why. It should have happened in this part of the function (the 
whole function is at the end). Can anyone help me with this?

    # - add all other columns but as a set for each level of a factor
    levels <- unique(X[factor])
    for (level in 1:length(unlist(levels))) {
        X[x[factor] == as.character(levels[level, ]),     
          paste(other, as.character(levels[level, ]), sep=".")] <- 
            x[x[factor] == as.character(levels[level, ]), other]    
    }

---------------------------------------------------------------

And another thing are NAs. If I compute means I get:

> mean(data1$y1.A)
[1] -0.2067784
> mean(data1$y1.A, na.rm=T)
[1] -0.2067784
> mean(data1$y1.B)
[1] NA
> mean(data1$y1.B, na.rm=T)
[1] -0.5065222

So <NA> and NA do not behave the same. Is this OK? It really
does not bother me, but I am just curious.

---------------------------------------------------------------

Here is the whole description of the function, the function and
example.

Thanks in advance.

# mt.by.factor.R
#-------------------------------------------------------------------------
# What: Create multiple trait data frame by given factor
# Time-stamp: <2005-02-12 02:28:00 ggorjan>
#-------------------------------------------------------------------------
# Quite often one wants to treat a trait for different levels e.g. sex,
# breed, ... as a different trait. This function eases preparation of data
# for such an analysis. 
#
# Input data frame with given variables is expanded in such a way, that 
# output represents a data frame with c + l + n * v columns, where c is a
# number of common columns for all levels of a factor, l is a factor 
# column, n is a number of levels in a factor and v number of variables
# that should be given for each level of a factor. Number of rows stays 
# the same.
#
#-------------------------------------------------------------------------

# Example
n=10                                                                    
(data <- data.frame(y1=rnorm(n=n),                                       
                   y2=rnorm(n=n),
                   f1=factor(rep(c("A", "B"), n/2)),                    
                   f2=factor(c(rep(c("M"), n/2), rep(c("F"), n/2))),    
                   c1=1:n,                                               
                   c2=2*(1:n)))          
                                                  
(data1 <-mt.by.factor(x=data, factor="f1", common=c("c1", "c2", "f2")))
(data1 <-mt.by.factor(x=data, factor="f1", common=c("c1", "c2")))

#
x <- data
factor <- "f1"
common <- c("c1", "c2")

# Function
mt.by.factor <- function(x, factor, common, sort=TRUE) {
    # Checks
    if (!is.data.frame(x)) {
        stop("`x' must be a data frame")
    }
    if (!is.factor(x[[factor]])) {                                  
        stop("`factor' must be a factor")
    }    
    # Sort
    if (sort) {
        x <- x[order(x[, factor]),]                        
    }                                                    
    # New data frame
    X <- x[common] # Common columns
    X[factor] <- x[factor] # Factor column
    # Other columns
    # - remove common and factor
    other <- names(x)
    for (i in 1:length(names(x[common]))) {
        other <- other[other != common[i]]
    }
    for (i in 1:length(names(x[factor]))) {
        other <- other[other != factor[i]]
    }
    # - add all other columns but as a set for each level of a factor
    levels <- unique(X[factor])
    for (level in 1:length(unlist(levels))) {
        X[x[factor] == as.character(levels[level, ]),     
          paste(other, as.character(levels[level, ]), sep=".")] <- 
            x[x[factor] == as.character(levels[level, ]), other]    
    }
    return(X)
}

#-------------------------------------------------------------------------
# mt.by.factor.R ends here


--
Lep pozdrav / With regards,
    Gregor GORJANC

---------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty       URI: http://www.bfro.uni-lj.si
Zootechnical Department    email: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                  tel: +386 (0)1 72 17 861
SI-1230 Domzale            fax: +386 (0)1 72 17 888
Slovenia



From skt at stat.psu.edu  Sun Feb 13 05:21:49 2005
From: skt at stat.psu.edu (Steven K Thompson)
Date: Sat, 12 Feb 2005 23:21:49 -0500
Subject: [R] missing X11 graphics title bar
Message-ID: <20050213042149.GC12378@linux18.stat.psu.edu>

I have on occasion had the problem of missing title bars on X11
graphics windows when using R, and would like to know what others have
found in terms of the occurance, source, or solution of this problem.
In searching for information on this I found only the brief thread
from last November which I have copied below.  Anyone who has
experienced this knows it brings a very unwelcome interruption to work
as one struggles to move or close the obstructing graphics windows
having no frames.  

I first experienced this problem with a Mac OSX system in which I was
using R under X11.  My R program called C functions and the problem
only occured when I was using graphics with fairly intensive
computations going on at the same time.

Recently, I've encountered this problem with a Linux system (Ubuntu)
using Gnome 2.8 with its default window manager Metacity.  On the
advice from the thread below, I changed window manager (to icewm) and
that eliminated the problem, except that the default Metacity seems
otherwise to work more efficiently and faster with Gnome.

I should note that I have been using Debian 3.0 (with gnome/icewm) for
almost three years with never any problem like this.

Thanks in advance for any information anyone may have on this issue.

Steve

From: Luke Tierney <luke_at_stat.uiowa.edu>
Date: Fri 12 Nov 2004 - 01:16:00 EST

On Thu, 11 Nov 2004, Martyn Plummer wrote:

> On Wed, 2004-11-10 at 09:19 +0100, Martyn Plummer wrote:
> > On Tue, 2004-11-09 at 19:47, Jonathan Baron wrote:
> > > On 11/09/04 20:37, Jari Oksanen wrote:
> > > >
> > > >On 9 Nov 2004, at 19:44, Jonathan Baron wrote:
> > > >
> > > >> The RPM for Fedora Core 2 seems to work just fine on Core 3.
> > > >>
> > > >> (The graphics window got smaller, but I'm sure there is a
> setting
> > > >> for that.)
> > > >>
> > > >That would be good news. I really don't know how the graphics
> window
> > > >became so big at some stage. (MacOS X is just cute here: tiny,
> sharp,
> > > >fast graphics window.)
> > >
> > > I have the opposite problem, a 1680x1050 display.
> > >
> > > >Has the options()printcmd reappeared, so that dev.print() works
> without
> > > >changing default options?
> > >
> > > I can't imagine how this would change. This is the same "old"
> > > RPM, not a new one. The option is there, and I don't think it
> > > ever disappeared. I can't test it. This is my laptop, which is
> > > not set up to print anything.
> >
> > My mistake. The default print command is determined at configure
> time.
> > But the RedHat RPMS are built in a sandbox that has only the
> minimal
> > configuration needed to build R. This doesn't include the lpr
> package so
> > the default print command is null. I will fix this in the next RPM
> > release, but right now I am upgrading to FC3.
>
> An RPM for Fedora Core 3 should be on a CRAN mirror near you by the
> weekend. This fixes the printcmd bug.
>
> The X11() window is the right size for me, but it doesn't have a
> title
> bar, which is a nuisance.
>
> Martyn
>

I've seen this with X11 a little on FC2 but much more with rgl (maybe
50% of the time with rgl vs 5-10% for X11()). Ross Ihaka tried
replacing the default metacity window manager with sawfish and found
that the problem seemed to go away. Ross also found a bug report for
metacity,

   https://bugzilla.redhat.com/bugzilla/show_bug.cgi?id=126571

that might be related. It might also be something we are not doing
quite right in opening a window that happens to bite metacity more
than other wms. If anyone has the time and energy to pursue this
please do.

Best,

luke

-- 
Luke Tierney
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
   Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu

______________________________________________
-- 
Steven K. Thompson
Department of Statistics
Pennsylvania State University
address for 2004-5 academic year:
132 Mesa Vista Street
Santa Fe, NM 87501
phone: 505 982 5466



From pensterfuzzer at yahoo.de  Sun Feb 13 10:43:19 2005
From: pensterfuzzer at yahoo.de (Werner Wernersen)
Date: Sun, 13 Feb 2005 10:43:19 +0100 (CET)
Subject: [R] Bubble Plot with Pie Charts
Message-ID: <20050213094319.77776.qmail@web25810.mail.ukl.yahoo.com>

Hi there!

I have seen that someone had this question some years
before, but there 
was only a rough answer to it and I was wondering if
actually someone 
was able to accomplish it or maybe even wrote a
function...

How do you make a bubble plot where you have pie
charts instead of bubbles?

I have a geographic map and need to plot
attribute-sized pie charts 
representing a second attribute at geographic
coordinates.

Thanks a lot for any help!

Regards,
  Bob



From p.dalgaard at biostat.ku.dk  Sun Feb 13 10:40:17 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 13 Feb 2005 10:40:17 +0100
Subject: [R] missing X11 graphics title bar
In-Reply-To: <20050213042149.GC12378@linux18.stat.psu.edu>
References: <20050213042149.GC12378@linux18.stat.psu.edu>
Message-ID: <x2sm404x32.fsf@biostat.ku.dk>

skt at stat.psu.edu (Steven K Thompson) writes:

> I have on occasion had the problem of missing title bars on X11
> graphics windows when using R, and would like to know what others have
> found in terms of the occurance, source, or solution of this problem.
> In searching for information on this I found only the brief thread
> from last November which I have copied below.  Anyone who has
> experienced this knows it brings a very unwelcome interruption to work
> as one struggles to move or close the obstructing graphics windows
> having no frames.  
> 
> I first experienced this problem with a Mac OSX system in which I was
> using R under X11.  My R program called C functions and the problem
> only occured when I was using graphics with fairly intensive
> computations going on at the same time.
> 
> Recently, I've encountered this problem with a Linux system (Ubuntu)
> using Gnome 2.8 with its default window manager Metacity.  On the
> advice from the thread below, I changed window manager (to icewm) and
> that eliminated the problem, except that the default Metacity seems
> otherwise to work more efficiently and faster with Gnome.
> 
> I should note that I have been using Debian 3.0 (with gnome/icewm) for
> almost three years with never any problem like this.
> 
> Thanks in advance for any information anyone may have on this issue.
> 
> Steve

I did a bit of poking around a month or so ago, and found that the
example programs from an Xlib tutorial (I believe it was from
http://tronche.com/gui/x/xlib-tutorial/xlib-tutorial.tar.gz) did the
same thing. So this has much more to do with Metacity than it does
with R. The mystery is of course that all the non-raw-Xlib
applications seem to get by quite nicely, even xterms, so there is
probably a trick but I was unable to find it.

One should probably relay this information to the Gnome/Metacity guys
and get them either to acknowledge the bug or divulge the workaround.
 
(BTW, alt-F7 and alt-F8 allow you to move and resize the frame even
without the title bar, and maximize followed by unmaximize does put
the decoration right).

> From: Luke Tierney <luke_at_stat.uiowa.edu>
> Date: Fri 12 Nov 2004 - 01:16:00 EST
> 
> On Thu, 11 Nov 2004, Martyn Plummer wrote:
> 
> > On Wed, 2004-11-10 at 09:19 +0100, Martyn Plummer wrote:
> > > On Tue, 2004-11-09 at 19:47, Jonathan Baron wrote:
> > > > On 11/09/04 20:37, Jari Oksanen wrote:
> > > > >
> > > > >On 9 Nov 2004, at 19:44, Jonathan Baron wrote:
> > > > >
> > > > >> The RPM for Fedora Core 2 seems to work just fine on Core 3.
> > > > >>
> > > > >> (The graphics window got smaller, but I'm sure there is a
> > setting
> > > > >> for that.)
> > > > >>
> > > > >That would be good news. I really don't know how the graphics
> > window
> > > > >became so big at some stage. (MacOS X is just cute here: tiny,
> > sharp,
> > > > >fast graphics window.)
> > > >
> > > > I have the opposite problem, a 1680x1050 display.
> > > >
> > > > >Has the options()printcmd reappeared, so that dev.print() works
> > without
> > > > >changing default options?
> > > >
> > > > I can't imagine how this would change. This is the same "old"
> > > > RPM, not a new one. The option is there, and I don't think it
> > > > ever disappeared. I can't test it. This is my laptop, which is
> > > > not set up to print anything.
> > >
> > > My mistake. The default print command is determined at configure
> > time.
> > > But the RedHat RPMS are built in a sandbox that has only the
> > minimal
> > > configuration needed to build R. This doesn't include the lpr
> > package so
> > > the default print command is null. I will fix this in the next RPM
> > > release, but right now I am upgrading to FC3.
> >
> > An RPM for Fedora Core 3 should be on a CRAN mirror near you by the
> > weekend. This fixes the printcmd bug.
> >
> > The X11() window is the right size for me, but it doesn't have a
> > title
> > bar, which is a nuisance.
> >
> > Martyn
> >
> 
> I've seen this with X11 a little on FC2 but much more with rgl (maybe
> 50% of the time with rgl vs 5-10% for X11()). Ross Ihaka tried
> replacing the default metacity window manager with sawfish and found
> that the problem seemed to go away. Ross also found a bug report for
> metacity,
> 
>    https://bugzilla.redhat.com/bugzilla/show_bug.cgi?id=126571
> 
> that might be related. It might also be something we are not doing
> quite right in opening a window that happens to bite metacity more
> than other wms. If anyone has the time and energy to pursue this
> please do.
> 
> Best,
> 
> luke


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From pensterfuzzer at yahoo.de  Sun Feb 13 12:18:44 2005
From: pensterfuzzer at yahoo.de (Werner Wernersen)
Date: Sun, 13 Feb 2005 12:18:44 +0100 (CET)
Subject: [R] Bubble Plot with Pie Charts
Message-ID: <20050213111844.85627.qmail@web25804.mail.ukl.yahoo.com>

OK, I used the paper "Integrating grid Graphics Output
with Base Graphics 
Output" by Paul Murrell (R News) which has an example
of implementing pie maps 
and made up a function from that. It looks actually
very nice!

One thing I couldn't figure out is if I can make pie
charts transparent, so that 
another chart below it would show through.

One more: Generally, how can I find out what values
for the parameters of gpar() 
are available?

Thanks again!

> Hi there!
> 
> I have seen that someone had this question some
years
> before, but there 
> was only a rough answer to it and I was wondering if
> actually someone 
> was able to accomplish it or maybe even wrote a
> function...
> 
> How do you make a bubble plot where you have pie
> charts instead of bubbles?
> 
> I have a geographic map and need to plot
> attribute-sized pie charts 
> representing a second attribute at geographic
> coordinates.
> 
> Thanks a lot for any help!
> 
> Regards,
>   Bob
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
> 
>



From pensterfuzzer at yahoo.de  Sun Feb 13 13:00:53 2005
From: pensterfuzzer at yahoo.de (Werner Wernersen)
Date: Sun, 13 Feb 2005 13:00:53 +0100 (CET)
Subject: [R] Transparent Pie Charts
Message-ID: <20050213120054.36006.qmail@web25808.mail.ukl.yahoo.com>

Hi again!

I put this question in another topics post before but
I fear it might drown there.

Is it possible to have transparent / alpha blended
colors for pie charts?
I am using the pies in a map of pies and those pies
are sometimes overlapping so 
it would be great to see if another pie lies beneath.

Thanks,
   Werner



From ligges at statistik.uni-dortmund.de  Sun Feb 13 14:54:20 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 13 Feb 2005 14:54:20 +0100
Subject: [R] Transparent Pie Charts
In-Reply-To: <20050213120054.36006.qmail@web25808.mail.ukl.yahoo.com>
References: <20050213120054.36006.qmail@web25808.mail.ukl.yahoo.com>
Message-ID: <420F5C0C.6000006@statistik.uni-dortmund.de>

Werner Wernersen wrote:
> Hi again!
> 
> I put this question in another topics post before but
> I fear it might drown there.
> 
> Is it possible to have transparent / alpha blended
> colors for pie charts?
> I am using the pies in a map of pies and those pies
> are sometimes overlapping so 
> it would be great to see if another pie lies beneath.
> 
> Thanks,
>    Werner
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


You have already found Paul Murrell's R News 3(2) article. You might 
also want to take a look at the page of his forthcoming book: 
http://www.stat.auckland.ac.nz/~paul/RGraphics/rgraphics.html

For transparency/alpha blending see the article "Fonts, Lines and 
Transparency in R Graphics" in the latest R News 4(2). Transparency 
works on many devices, alpha blending only on a few (pdf, for example).

Let's change Paul's code from R News 3(2) to allow transparency:


   x <- c(0.88, 1, 0.67, 0.34)
   y <- c(0.5, 0.4, 0.6, 0.55)
   set.seed(1234)
   z <- matrix(runif(4 * 2), ncol = 2)
   oldpar <- par(no.readonly = TRUE)
   plot(x, y, xlim = c(-0.2, 1.2), ylim = c(-0.2, 1.2), type = "n")
   vps <- baseViewports()
   par(new = TRUE)
   pushViewport(vps$inner, vps$figure, vps$plot)
   grid.segments(x0 = unit(c(rep(0, 4), x), rep(c("npc", "native"),
                           each = 4)),
                 x1 = unit(c(x, x), rep("native", 8)),
                 y0 = unit(c(y, rep(0, 4)), rep(c("native", "npc"),
                           each = 4)),
                 y1 = unit(c(y, y), rep("native", 8)),
                 gp = gpar(lty = "dashed", col = "grey"))
   maxpiesize <- unit(1, "inches")
   totals <- rowSums(z)
   sizemult <- totals/max(totals)
   for (i in 1:4) {
     pushViewport(viewport(x = unit(x[i], "native"),
                           y = unit(y[i], "native"),
                           width = sizemult[i] * maxpiesize,
                           height = sizemult[i] * maxpiesize))
     grid.rect(gp = gpar(col = "grey", lty = "dashed"))
     par(plt = gridPLT(), new = TRUE)
     pie(z[i, ], radius = 1, labels = rep("", 2),
         col = c("transparent", "lightblue"))
     popViewport()
   }
   popViewport(3)
   par(oldpar)




Uwe Ligges



From ligges at statistik.uni-dortmund.de  Sun Feb 13 14:58:27 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 13 Feb 2005 14:58:27 +0100
Subject: [R] Paste script in R stops before end of file
In-Reply-To: <200502122355.53284.tomas.willebrand@szooek.slu.se>
References: <200502122355.53284.tomas.willebrand@szooek.slu.se>
Message-ID: <420F5D03.7030001@statistik.uni-dortmund.de>

Tomas Willebrand wrote:

> Hi!
> 
> I can only paste a limited amount of text into R (which is started in KDEs 
> Konsole). The script I have problem with stops pasting after 4224 characters, 
> and 78 lines. Although I can get Rs' attention at that point by just pressing 
> Return.  The script mainly consists of a a large amount of vectors that I 
> would like to join into a dataset for further analysis. Example of vector: 
> AC <- 
> c(1700,18000,18000,20000,19000,14500,24000,3421,3770,5045,11400,5775,4944,6765,13557,5200,7930,16400,14500,8900,10500,17000,18000,23500,28700,33200,26000,20000,27500,25100,20900,18500,27400,25000,12200,12200,8203,6958,8455,8828,7707,5217,5722)
> 
> There is no problem pasting the total script into different editors, and I can 
> paste all lines into vi when it is run in the Konsole. I can also divide the 
> script i 3 different parts that R would read through pasting. 
> 
> I use R 2.01 on a machine that have SuSE 9.2 with KDE3.3.2, kernel-smp 
> 2.6.8-24.11 installed and an Intel 3 GHz, 1056 MB RAM. I have set the coding 
> to ISO8859-1.
> 
> What puzzles me is that I have no problem pasting in the whole script in one 
> chunck using R 2.0.1 with SuSE 9.2 on a ThinkpadX40.
> 
> Is this likely to be a problem with my R installation? Any help is 
> appriciated.
> 
> /Tomas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


Probably something with buffers.

Anyway, it's much easier to import data in a different way. If you 
already have them in R syntax, just source() the file containg the data...

Uwe Ligges



From ligges at statistik.uni-dortmund.de  Sun Feb 13 15:06:12 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 13 Feb 2005 15:06:12 +0100
Subject: [R] Bubble Plot with Pie Charts
In-Reply-To: <20050213111844.85627.qmail@web25804.mail.ukl.yahoo.com>
References: <20050213111844.85627.qmail@web25804.mail.ukl.yahoo.com>
Message-ID: <420F5ED4.7070905@statistik.uni-dortmund.de>

Werner Wernersen wrote:
> OK, I used the paper "Integrating grid Graphics Output
> with Base Graphics 
> Output" by Paul Murrell (R News) which has an example
> of implementing pie maps 
> and made up a function from that. It looks actually
> very nice!
> 
> One thing I couldn't figure out is if I can make pie
> charts transparent, so that 
> another chart below it would show through.

See my other response.

> One more: Generally, how can I find out what values
> for the parameters of gpar() 
> are available?

See ?gpar, and get.gpar() for the current setting.


Uwe Ligges

> Thanks again!
> 
> 
>>Hi there!
>>
>>I have seen that someone had this question some
> 
> years
> 
>>before, but there 
>>was only a rough answer to it and I was wondering if
>>actually someone 
>>was able to accomplish it or maybe even wrote a
>>function...
>>
>>How do you make a bubble plot where you have pie
>>charts instead of bubbles?
>>
>>I have a geographic map and need to plot
>>attribute-sized pie charts 
>>representing a second attribute at geographic
>>coordinates.
>>
>>Thanks a lot for any help!
>>
>>Regards,
>>  Bob
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
> 
> http://www.R-project.org/posting-guide.html
> 
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From mjenny at rumms.uni-mannheim.de  Sun Feb 13 15:45:11 2005
From: mjenny at rumms.uni-mannheim.de (mjenny@rumms.uni-mannheim.de)
Date: Sun, 13 Feb 2005 15:45:11 +0100
Subject: [R] combinations without repetition
Message-ID: <20050213154511.0jtwwsgggwco4kok@webmail.uni-mannheim.de>

The solution is probably simple but I need someone to point me to it.
How can I reduce the following 6 permutations to the 3 unique combinations
(1,1,0) (1,0,1) (0,1,1)?

permn(c(1,1,0))
[[1]]
[1] 1 1 0

[[2]]
[1] 1 0 1

[[3]]
[1] 0 1 1

[[4]]
[1] 0 1 1

[[5]]
[1] 1 0 1

[[6]]
[1] 1 1 0

Thanks a lot
Marcelo Jenny



From talebelm at gmail.com  Sun Feb 13 16:30:17 2005
From: talebelm at gmail.com (Taleb Elm)
Date: Sun, 13 Feb 2005 15:30:17 +0000
Subject: [R] Multi-item Scale Development Training
Message-ID: <2db3681b05021307306234a7f6@mail.gmail.com>

I apologize for cross-posting.

Could you recommend any courses on multi-item scale development in UK
or Europe? Also, could you recommend any resources on building
multi-item scales for management research?



From jfox at mcmaster.ca  Sun Feb 13 16:33:53 2005
From: jfox at mcmaster.ca (John Fox)
Date: Sun, 13 Feb 2005 10:33:53 -0500
Subject: [R] combinations without repetition
In-Reply-To: <20050213154511.0jtwwsgggwco4kok@webmail.uni-mannheim.de>
Message-ID: <20050213153342.TXKR1836.tomts16-srv.bellnexxia.net@JohnDesktop8300>

Dear Marcelo,

This is, I guess, permn() in the combinat package.

How about unique(matrix(unlist(permn(c(1,1,0))), ncol=3, byrow=TRUE)), which
leaves the permutations in the rows of a matrix?

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> mjenny at rumms.uni-mannheim.de
> Sent: Sunday, February 13, 2005 9:45 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] combinations without repetition
> 
> The solution is probably simple but I need someone to point me to it.
> How can I reduce the following 6 permutations to the 3 unique 
> combinations
> (1,1,0) (1,0,1) (0,1,1)?
> 
> permn(c(1,1,0))
> [[1]]
> [1] 1 1 0
> 
> [[2]]
> [1] 1 0 1
> 
> [[3]]
> [1] 0 1 1
> 
> [[4]]
> [1] 0 1 1
> 
> [[5]]
> [1] 1 0 1
> 
> [[6]]
> [1] 1 1 0
> 
> Thanks a lot
> Marcelo Jenny
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From rog at stanford.edu  Sun Feb 13 18:43:27 2005
From: rog at stanford.edu (Roger Levy)
Date: 13 Feb 2005 09:43:27 -0800
Subject: [R] question on indexing of a matrix
In-Reply-To: <x2mzu978hw.fsf@biostat.ku.dk>
References: <Pine.LNX.4.44.0502122116490.5140-100000@reclus.nhh.no>
	<x2mzu978hw.fsf@biostat.ku.dk>
Message-ID: <253bw0pd8g.fsf@joel.Stanford.EDU>

Peter Dalgaard <p.dalgaard at biostat.ku.dk> writes:

> Roger Bivand <Roger.Bivand at nhh.no> writes:
> 
> > On 12 Feb 2005, Roger Levy wrote:
> > 
> > > Hi,
> > > 
> > > I have a k-level factor F of length n that I would like to use to
> > > extract from an n-by-k matrix M a vector V such that
> > > 
> > >   V[i] = M[i,as.numeric(F)[i]]
> > > 
> > > I don't currently understand how to do that in R -- can anyone explain
> > > to me how to do so?
> > 
> > This may be an answer:
> > 
> > > k <- 3
> > > n <- 10
> > > M <- matrix(1:(k*n), ncol=k, nrow=n)
> > > M
> >       [,1] [,2] [,3]
> >  [1,]    1   11   21
> >  [2,]    2   12   22
> >  [3,]    3   13   23
> >  [4,]    4   14   24
> >  [5,]    5   15   25
> >  [6,]    6   16   26
> >  [7,]    7   17   27
> >  [8,]    8   18   28
> >  [9,]    9   19   29
> > [10,]   10   20   30
> > > set.seed(1234)
> > > K <- factor(sample(1:3, n, replace=TRUE))
> > > K
> >  [1] 1 2 2 2 3 2 1 1 2 2
> > Levels: 1 2 3
> > > Kmm <- model.matrix(terms(~ K-1))
> > > Kmm
> >    K1 K2 K3
> > 1   1  0  0
> > 2   0  1  0
> > 3   0  1  0
> > 4   0  1  0
> > 5   0  0  1
> > 6   0  1  0
> > 7   1  0  0
> > 8   1  0  0
> > 9   0  1  0
> > 10  0  1  0
> > attr(,"assign")
> > [1] 1 1 1
> > attr(,"contrasts")
> > attr(,"contrasts")$K
> > [1] "contr.treatment"
> > 
> > > rowSums(M * Kmm)
> >  1  2  3  4  5  6  7  8  9 10 
> >  1 12 13 14 25 16  7  8 19 20 
> > 
> > My understanding is that you want to extract the K[i]th column from the 
> > [i]th row of M, so the model matrix of K should zero out the values you 
> > don't want. Using something like this in a function would mean checking K 
> > carefully for the number of levels actually present and their coding. 
> 
> This looks simpler:
> 
> >  M[cbind(seq(along=K),as.numeric(K))]
>  [1]  1 12 13 14 25 16  7  8 19 20

Thank you!  This does the trick.

Best,

Roger



From jost at cict.fr  Sun Feb 13 19:55:19 2005
From: jost at cict.fr (Christian Jost)
Date: Sun, 13 Feb 2005 19:55:19 +0100
Subject: [R] Re: comparing predicted sequence A'(t) to observed sequence A(t)
In-Reply-To: <200502131118.j1DB6cFF022398@hypatia.math.ethz.ch>
References: <200502131118.j1DB6cFF022398@hypatia.math.ethz.ch>
Message-ID: <a06002000be354dc89600@[130.120.104.141]>

>From: Suresh Krishna <ssk2031 at columbia.edu>
>Subject: [R] comparing predicted sequence A'(t) to observed sequence
>	A(t)
>To: r-help at stat.math.ethz.ch
>Message-ID: <420DE463.8080009 at columbia.edu>
>Content-Type: text/plain; charset=ISO-8859-1; format=flowed
>
>
>Hi,
>
>I have a question that I have not been succesful in finding a definitive
>answer to; and I was hoping someone here could give me some pointers to
>the right place in the literature.
>
>A. We have 4 sets of data, A(t), B(t), C(t), and D(t). Each of these
>consists of a series of counts obtained in sequential time-intervals: so
>   for example, A(t) would be something like:
>
>Count A(t):  25,    28,    26,   34   ......
>Time (ms):  0-10, 10-20, 20-30, 30-40 .......
>
>Each count in the series A(t) is obtained by summing the total number of
>observed counts over multiple (say 50), independent repetitions of that
>time-series. These counts are generally known to be Poisson distributed,
>and the 4 processes A(t), B(t), C(t) and D(t) are independent of each other.
>
>B. It appears on visual observation that the following relationship
>holds; and such a relationship would also be expected on mechanistic
>considerations.
>
>A(t) = B(t) + C(t) - D(t)
>
>We now want to test this hypothesis statistically.
>
>Because successive counts in the sequence are likely to be correlated,
>isnt it true that none of these methods are valid ? Perhaps for other
>reasons as well ?
>
>a)Doing a chi-squared test to see if the predicted curve for A(t)
>deviates significantly from the observed A(t); this also seems to not
>take the variability of the predicted curve into account.
>
>b)Doing a regression of the predicted values of A(t) against the actual
>values of A(t) and checking for deviations of slope from 1 and intercept
>from 0 ? Here, in addition to lack of independence, the fact that
>X-values are not fixed (i.e. are variable) and the fact that X and Y are
>Poisson distributed counts should also be taken into account, right ?
>
>I would be very grateful if someone could point me to methods to handle
>this kind of situation, or where to look for them. Is there something in
>the time-series literature, for instance ?
>
>

This is a frequent problem I also encounter when wanting to compare 
two dynamic processes (e.g. temporal evolution of number of ants on 
two branches). To my knowledge there is no general statistical way to 
compare these two time series. But in your case you might try a 
repeated measure anova, e.g. to compare A(t) against B(t)+C(t)-D(t), 
put in a first column 'counts' the counts for A and then for B+C-D, 
in a second column 'time' the correspoding t, in a third column 
'series' mark the A measures by "A" and the B+C-D measures by "BCD", 
then run an anova
summary(aov(counts ~ series:time + Error(series)))

This works if there are replicates of conditions "A" and "BDC", but I 
am not a statistitian and am not sure whether it applies to your case 
(though, you seem to have repetitions, so you might use this 
information instead of only looking at the sums).
For a hands-on example with behavioural data of mice (with or without 
treatment, 4 training session for each mouse, does treatment affect 
training) see
http://cognition.ups-tlse.fr/_christian/M7P14M/TP7/TP-Anova.pdf
with the data in
http://cognition.ups-tlse.fr/_christian/M7P14M/TP7/tp-anova.rda
(well, its in french, but the R formulas should be understandable ;-)

Well, as I said, I am not a statistitian, there might be a logical 
flaw in applying repeated measures anova to time series, if anybody 
out there sees one please tell us ;-)

Best, Christian.
-- 
***********************************************************
http://cognition.ups-tlse.fr/vas-y.php?id=chj  jost at cict.fr
Christian Jost                                   (PhD, MdC)
Centre de Recherches sur la Cognition Animale
Universite Paul Sabatier, Bat IV R3
118 route de Narbonne
31062 Toulouse cedex 4, France
Tel: +33 5 61 55 64 37   Fax: +33 5 61 55 61 54



From tiago17 at socrates.Berkeley.EDU  Sun Feb 13 21:06:49 2005
From: tiago17 at socrates.Berkeley.EDU (Tiago R Magalhaes)
Date: Sun, 13 Feb 2005 12:06:49 -0800
Subject: [R] data.frame into list by columns; merge and row.names
Message-ID: <p0610050abe35625964c2@[192.168.0.103]>

thanks to Andy Liaw and james holtman that replied to my posting

this is very basic stuff, but for people strugling with these 
concepts - as I am... - here goes a more detailed explanation

#####
what I wanted was to create a list where every element would be a 
column of a given data.frame

the only small problem is that I cannot find a way to keep the column 
names - third paragraph of 3)

1)
x <- data.frame(a=sample(10), b=rep('a',10), c=sample(10))
xlistBycol <- as.list(x)

a data.frame is a list in which each column is a vector.
you can separate every vector/column of a database into elements of a 
list by using as.list
note that every element is a vector of the same type as in the data.frame
element 1 in xlistBycol is an integer, element 2 is a factor with 1 level ('a')

2)
x <- data.frame(a=sample(10), b=rep('a',10), c=sample(10))
xlistBycol <- lapply(x, as.data.frame)

the same concept as 1), but this time to each element/column of the 
data.frame, as.data.frame is applied
ther result of this call is a list in which every element is a 
data.frame with 1 column

if you want to keep the row.names you have to give row.names as an 
argument to as.data.frame:
xlistBycol <- lapply(x, as.data.frame, row.names(x))
Unfortunately so far I couldn't find an easy way to keep the column 
names of the data.frame in the corresponding elements of the list

3)
to better understand 2) it was useful for me to do this:
x <- data.frame(a=sample(10), b=sample(10))
xAddOne <- lapply(x, function(x) x <- x+1)

in this case lapply adds 1 to every element of the list/data.frame x 
and the result is the list xAddOne where every element is the 
corresponding element of x plus one

I hope this doesn't confuse people even more...




>  > From: Tiago R Magalhaes
>>
>>  Hi
>>
>>  a)
>>  I want to make a list out of a data.frame, where each element of the
>>  list is a column of the data.frame.
>>  I looked in the archives and saw a lot of postings but surprsingly
>>  none elucidated me. I also tried the split, aggregate help files and
>>  counldn't see any easy way to do this. I wouldn't be surprised if
>>  it's there, but I really didn't see it.
>>
>>  I solved the problem using a very convoluted way:
>>
>  > x <- data.frame(a=sample(10), b=sample(10), c=sample(10))
>>  f <- factor(names(x), levels=names(x))
>>  xx <- data.frame(f=f, t(x))
>>  xlist.transpose <- split(xx, xx$f)
>  > xlist <- lapply(xlist, function(x) x=t(x))
>>
>>  I am very convinced there's a much easier way, so if any of you
>>  people enlighten me I would appreciate
>
>1. Please make sure the code you show actually works.  The last line
>doesn't.
>
>2. I'm not sure what you want to do.  A data frame is already a list.  If
>you want it to be just a list, just use as.list(x).  If you want a list
>where each component is a data frame with one column, use lapply(x,
>as.data.frame).
>
>Andy



From spencer.graves at pdf.com  Sun Feb 13 23:09:00 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 13 Feb 2005 14:09:00 -0800
Subject: [R] Re: comparing predicted sequence A'(t) to observed sequence
	A(t)
In-Reply-To: <a06002000be354dc89600@[130.120.104.141]>
References: <200502131118.j1DB6cFF022398@hypatia.math.ethz.ch>
	<a06002000be354dc89600@[130.120.104.141]>
Message-ID: <420FCFFC.9080706@pdf.com>

      Regarding applying repeated measures to time series, the lme 
software in packages nlme and lme4 provide many options for doing that.  
The best discussion I know of this is Pinheiro and Bates (2000) 
Mixed-Effects Models in S and S-PLUS (Springer). 

      hope this helps.  spencer graves

Christian Jost wrote:

>> From: Suresh Krishna <ssk2031 at columbia.edu>
>> Subject: [R] comparing predicted sequence A'(t) to observed sequence
>>     A(t)
>> To: r-help at stat.math.ethz.ch
>> Message-ID: <420DE463.8080009 at columbia.edu>
>> Content-Type: text/plain; charset=ISO-8859-1; format=flowed
>>
>>
>> Hi,
>>
>> I have a question that I have not been succesful in finding a definitive
>> answer to; and I was hoping someone here could give me some pointers to
>> the right place in the literature.
>>
>> A. We have 4 sets of data, A(t), B(t), C(t), and D(t). Each of these
>> consists of a series of counts obtained in sequential time-intervals: so
>>   for example, A(t) would be something like:
>>
>> Count A(t):  25,    28,    26,   34   ......
>> Time (ms):  0-10, 10-20, 20-30, 30-40 .......
>>
>> Each count in the series A(t) is obtained by summing the total number of
>> observed counts over multiple (say 50), independent repetitions of that
>> time-series. These counts are generally known to be Poisson distributed,
>> and the 4 processes A(t), B(t), C(t) and D(t) are independent of each 
>> other.
>>
>> B. It appears on visual observation that the following relationship
>> holds; and such a relationship would also be expected on mechanistic
>> considerations.
>>
>> A(t) = B(t) + C(t) - D(t)
>>
>> We now want to test this hypothesis statistically.
>>
>> Because successive counts in the sequence are likely to be correlated,
>> isnt it true that none of these methods are valid ? Perhaps for other
>> reasons as well ?
>>
>> a)Doing a chi-squared test to see if the predicted curve for A(t)
>> deviates significantly from the observed A(t); this also seems to not
>> take the variability of the predicted curve into account.
>>
>> b)Doing a regression of the predicted values of A(t) against the actual
>> values of A(t) and checking for deviations of slope from 1 and intercept
>> from 0 ? Here, in addition to lack of independence, the fact that
>> X-values are not fixed (i.e. are variable) and the fact that X and Y are
>> Poisson distributed counts should also be taken into account, right ?
>>
>> I would be very grateful if someone could point me to methods to handle
>> this kind of situation, or where to look for them. Is there something in
>> the time-series literature, for instance ?
>>
>>
>
> This is a frequent problem I also encounter when wanting to compare 
> two dynamic processes (e.g. temporal evolution of number of ants on 
> two branches). To my knowledge there is no general statistical way to 
> compare these two time series. But in your case you might try a 
> repeated measure anova, e.g. to compare A(t) against B(t)+C(t)-D(t), 
> put in a first column 'counts' the counts for A and then for B+C-D, in 
> a second column 'time' the correspoding t, in a third column 'series' 
> mark the A measures by "A" and the B+C-D measures by "BCD", then run 
> an anova
> summary(aov(counts ~ series:time + Error(series)))
>
> This works if there are replicates of conditions "A" and "BDC", but I 
> am not a statistitian and am not sure whether it applies to your case 
> (though, you seem to have repetitions, so you might use this 
> information instead of only looking at the sums).
> For a hands-on example with behavioural data of mice (with or without 
> treatment, 4 training session for each mouse, does treatment affect 
> training) see
> http://cognition.ups-tlse.fr/_christian/M7P14M/TP7/TP-Anova.pdf
> with the data in
> http://cognition.ups-tlse.fr/_christian/M7P14M/TP7/tp-anova.rda
> (well, its in french, but the R formulas should be understandable ;-)
>
> Well, as I said, I am not a statistitian, there might be a logical 
> flaw in applying repeated measures anova to time series, if anybody 
> out there sees one please tell us ;-)
>
> Best, Christian.



From bullard at berkeley.edu  Sun Feb 13 23:55:57 2005
From: bullard at berkeley.edu (James Bullard)
Date: Sun, 13 Feb 2005 14:55:57 -0800
Subject: [R] row equality.
Message-ID: <420FDAFD.8010204@berkeley.edu>

I think that this is an easy one...

I have a matrix where each row is an (x,y,z) triplet. Given a potential 
(xnew,ynew,znew) triplet I want to know if the matrix already contains a 
row with the new values (the space already has that point). I can do it 
using a for loop, but I would like to know if there is anyway in which I 
can do it without the for loop.

I do it now like this (this algorithm appears to be correct, but there 
are probably much cleaner ways to write it.)

has.row <- function(m, r) {
  for (i in 1:length(y[,1])) {
    x <- ifelse(y[i,1:3] == r, 1, 0)
    if (sum(x) == 3) {
      return(TRUE)
    }
  }
  return(FALSE)
}



Thanks, jim



From bullard at berkeley.edu  Mon Feb 14 00:27:00 2005
From: bullard at berkeley.edu (James Bullard)
Date: Sun, 13 Feb 2005 15:27:00 -0800
Subject: [R] row equality.
In-Reply-To: <420FDAFD.8010204@berkeley.edu>
References: <420FDAFD.8010204@berkeley.edu>
Message-ID: <420FE244.2080808@berkeley.edu>

Sorry about that I realized that I posted code that doesnt work ... below

y should be replaced by the parameter m in the body of the function.

(I tried to clean it up to post, but that never works...)

has.row <- function(m, r) {
 for (i in 1:length(m[,1])) {
   x <- ifelse(m[i,1:3] == r, 1, 0)

   if (sum(x) == 3) {
     return(TRUE)
   }
 }
 return(FALSE)
}


thanks. Jim

James Bullard wrote:

> I think that this is an easy one...
>
> I have a matrix where each row is an (x,y,z) triplet. Given a 
> potential (xnew,ynew,znew) triplet I want to know if the matrix 
> already contains a row with the new values (the space already has that 
> point). I can do it using a for loop, but I would like to know if 
> there is anyway in which I can do it without the for loop.
>
> I do it now like this (this algorithm appears to be correct, but there 
> are probably much cleaner ways to write it.)
>
> has.row <- function(m, r) {
>  for (i in 1:length(y[,1])) {
>    x <- ifelse(y[i,1:3] == r, 1, 0)
>    if (sum(x) == 3) {
>      return(TRUE)
>    }
>  }
>  return(FALSE)
> }
>
>
>
> Thanks, jim
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>


-- 
James Bullard
bullard at berkeley.edu
760.267.0986



From andy_liaw at merck.com  Mon Feb 14 00:31:49 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sun, 13 Feb 2005 18:31:49 -0500
Subject: [R] row equality.
Message-ID: <3A822319EB35174CA3714066D590DCD50994E6CA@usrymx25.merck.com>

Try something like:

> x1 <- matrix(sample(1:5, 30, replace=TRUE), ncol=3)
> x2 <- x1[4,]
> which(colSums(abs(t(x1) - x2)) == 0)
[1] 4

Note:  If the data are not all integers, you probably should test whether
the absolute sum differences is less than some very small number, rather
than == 0.

This uses the recyling rule, which, if you don't know about, probably would
be good to find out.

Andy


> From: James Bullard
> 
> I think that this is an easy one...
> 
> I have a matrix where each row is an (x,y,z) triplet. Given a 
> potential 
> (xnew,ynew,znew) triplet I want to know if the matrix already 
> contains a 
> row with the new values (the space already has that point). I 
> can do it 
> using a for loop, but I would like to know if there is anyway 
> in which I 
> can do it without the for loop.
> 
> I do it now like this (this algorithm appears to be correct, 
> but there 
> are probably much cleaner ways to write it.)
> 
> has.row <- function(m, r) {
>   for (i in 1:length(y[,1])) {
>     x <- ifelse(y[i,1:3] == r, 1, 0)
>     if (sum(x) == 3) {
>       return(TRUE)
>     }
>   }
>   return(FALSE)
> }
> 
> 
> 
> Thanks, jim
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From tom_colson at ncsu.edu  Mon Feb 14 01:39:08 2005
From: tom_colson at ncsu.edu (Thomas Colson)
Date: Sun, 13 Feb 2005 19:39:08 -0500
Subject: [R] 64 Bit R Background Question
Message-ID: <200502140039.j1E0dDrT002388@uni08mr.unity.ncsu.edu>

Hi, 

I've collected quite a bit of elevation data (LIDAR elevation points) and am
looking for a suitable platform to do analysis and modeling on it. The data
is sitting in an Oracle database, one table, 200 million rows of x,y, and z.
I'm trying to figure out what hardware resources we need to reserve in order
to run 64 BIT R for this size data. 

Here's my question:
Is the 64 BIT version of R appropriate for this size? Or is attempting to
read all 200 million rows a pipe dream no matter what platform I'm using? 

I've read in the archives that some have gotten 64 BIT R to build "out of
box" on Intel Xeon processors...is there any differences in
compiling/performance between the Intel Xeon architecture vs AMD 64? 

Are there pure physical limitations of 64 BIT R that would make intalling it
on a grid (http://www.ncsu.edu/itd/hpc/Hardware/Hardware.php) a wasted
effort? We have the memory and drive space to do this.

Thanks for any advice


Tom Colson
Center for Earth Observation
North Carolina State University 
Raleigh, NC 27695
(919) 515 3434
(919) 673 8023
tom_colson at ncsu.edu

Online Calendar:
http://www4.ncsu.edu/~tpcolson



From THOMAS.VOLSCHO at huskymail.uconn.edu  Mon Feb 14 03:00:14 2005
From: THOMAS.VOLSCHO at huskymail.uconn.edu (Thomas W Volscho)
Date: Sun, 13 Feb 2005 21:00:14 -0500
Subject: [R] Update error in Fedora Core 3
Message-ID: <661bc3667868.667868661bc3@huskymail.uconn.edu>

Dear List,
I am new to Linux and I just installed Fedora Core 3 (workstation installation) on my PC--software came from a book called Red Hate Linux Fedora 3 Unleashed.  Installing R was relatively easy.

I ran into a problem, however.  After typing "Update.Packages()" I am able to download them, but I then get the following output:

/usr/lib/R/bin/Rcmd: line 45: exec: INSTALL: not found
/usr/lib/R/bin/Rcmd: line 45: exec: INSTALL: not found
/usr/lib/R/bin/Rcmd: line 45: exec: INSTALL: not found
/usr/lib/R/bin/Rcmd: line 45: exec: INSTALL: not found
/usr/lib/R/bin/Rcmd: line 45: exec: INSTALL: not found
/usr/lib/R/bin/Rcmd: line 45: exec: INSTALL: not found
/usr/lib/R/bin/Rcmd: line 45: exec: INSTALL: not found
/usr/lib/R/bin/Rcmd: line 45: exec: INSTALL: not found

After deleting downloaded files, I get:

Warning messages:
1: Installation of package KernSmooth had non-zero exit status in: install.packages(update[, "Package"], instlib, contriburl = contriburl,
2: Installation of package VR had non-zero exit status in: install.packages(update[, "Package"], instlib, contriburl = contriburl,
3: Installation of package boot had non-zero exit status in: install.packages(update[, "Package"], instlib, contriburl = contriburl,
4: Installation of package cluster had non-zero exit status in: install.packages(update[, "Package"], instlib, contriburl = contriburl,
5: Installation of package foreign had non-zero exit status in: install.packages(update[, "Package"], instlib, contriburl = contriburl,
6: Installation of package lattice had non-zero exit status in: install.packages(update[, "Package"], instlib, contriburl = contriburl,
7: Installation of package rpart had non-zero exit status in: install.packages(update[, "Package"], instlib, contriburl = contriburl,
8: Installation of package survival had non-zero exit status in: install.packages(update[, "Package"], instlib, contriburl = contriburl,

Does anyone know how to resolve this?  Am I using the wrong command to update?  I just switched from using R in Windows.  Thank you for your input.

-Tom  

************************************        
Thomas W. Volscho
Graduate Student
Dept. of Sociology U-2068
University of Connecticut
Storrs, CT 06269
Phone: (860) 486-3882
http://vm.uconn.edu/~twv00001



From Paul.Sorenson at vision-bio.com  Mon Feb 14 03:16:45 2005
From: Paul.Sorenson at vision-bio.com (Paul Sorenson)
Date: Mon, 14 Feb 2005 13:16:45 +1100
Subject: [R] graphics - current filename
Message-ID: <5E06BFED29594F4C9C5EBE230DE320C6068027E5@ewok.vsl.com.au>

I would like to query R for the current (or last used) filename for a graphics device.

Eg after png(filename="plot%02d.png") I would like something like the output of dev.cur() but with the %02d expanded to the current name.

Can anyone point me at where I can find this please?



From ramasamy at cancer.org.uk  Mon Feb 14 03:54:40 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 14 Feb 2005 02:54:40 +0000
Subject: [R] combinations without repetition
In-Reply-To: <20050213153342.TXKR1836.tomts16-srv.bellnexxia.net@JohnDesktop8300>
References: <20050213153342.TXKR1836.tomts16-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <1108349680.5981.8.camel@dhcp-63.ccc.ox.ac.uk>

This can be simplified slightly by 
 unique( t( sapply( permn( c(1,1,0) ), "c" )  ) )


Here is another possibility :

 a <- expand.grid( 0:1, 0:1, 0:1 )
 a[ which( rowSums(a) == 2 ),  ]

which gives

  Var1 Var2 Var3
4    1    1    0
6    1    0    1
7    0    1    1


Regards, Adai



On Sun, 2005-02-13 at 10:33 -0500, John Fox wrote:
> Dear Marcelo,
> 
> This is, I guess, permn() in the combinat package.
> 
> How about unique(matrix(unlist(permn(c(1,1,0))), ncol=3, byrow=TRUE)), which
> leaves the permutations in the rows of a matrix?
> 
> I hope this helps,
>  John
> 
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox 
> -------------------------------- 
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> > mjenny at rumms.uni-mannheim.de
> > Sent: Sunday, February 13, 2005 9:45 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] combinations without repetition
> > 
> > The solution is probably simple but I need someone to point me to it.
> > How can I reduce the following 6 permutations to the 3 unique 
> > combinations
> > (1,1,0) (1,0,1) (0,1,1)?
> > 
> > permn(c(1,1,0))
> > [[1]]
> > [1] 1 1 0
> > 
> > [[2]]
> > [1] 1 0 1
> > 
> > [[3]]
> > [1] 0 1 1
> > 
> > [[4]]
> > [1] 0 1 1
> > 
> > [[5]]
> > [1] 1 0 1
> > 
> > [[6]]
> > [1] 1 1 0
> > 
> > Thanks a lot
> > Marcelo Jenny
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From cuiczhao at yahoo.com  Mon Feb 14 05:53:32 2005
From: cuiczhao at yahoo.com (Cuichang Zhao)
Date: Sun, 13 Feb 2005 20:53:32 -0800 (PST)
Subject: [R] how can i make my program faster
Message-ID: <20050214045332.27313.qmail@web30705.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050213/58510f39/attachment.pl

From Robert.Denham at nrm.qld.gov.au  Mon Feb 14 08:01:06 2005
From: Robert.Denham at nrm.qld.gov.au (Denham Robert)
Date: Mon, 14 Feb 2005 17:01:06 +1000
Subject: [R] Sub with and without perl=TRUE
Message-ID: <66CE7F4ACF0C74439F9F6E0A568293020BA6FA@INDMAIL.lands.resnet.qg>

I have a problem doing substitution using sub and perl=TRUE when the
elements of x have fewer characters than the replacement string.  Let me
show you what I mean:

> sub("m","billy","m")
[1] "billy"

But using perl=TRUE, I can only return a result as long as my x:
> sub("m","billy","m",perl=TRUE)
[1] "b"

> sub("m","billy","ma",perl=TRUE)
[1] "bi"

Etc.

Is this supposed to happen like this? I couldn't see a mention of this
requirement in the help.

I am using R 2.0.1 on windows xp.

Robert




************************************************************************
The information in this e-mail together with any attachments is
intended only for the person or entity to which it is addressed
and may contain confidential and/or privileged material.
Any form of review, disclosure, modification, distribution
and/or publication of this e-mail message is prohibited.  
If you have received this message in error, you are asked to
inform the sender as quickly as possible and delete this message
and any copies of this message from your computer and/or your
computer system network.



From ligges at statistik.uni-dortmund.de  Mon Feb 14 08:40:42 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 14 Feb 2005 08:40:42 +0100
Subject: [R] graphics - current filename
In-Reply-To: <5E06BFED29594F4C9C5EBE230DE320C6068027E5@ewok.vsl.com.au>
References: <5E06BFED29594F4C9C5EBE230DE320C6068027E5@ewok.vsl.com.au>
Message-ID: <421055FA.4080805@statistik.uni-dortmund.de>

Paul Sorenson wrote:

> I would like to query R for the current (or last used) filename for a graphics device.
> 
> Eg after png(filename="plot%02d.png") I would like something like the output of dev.cur() but with the %02d expanded to the current name.

You cannot, it is handled internally and the name is not returned.
So you have to workaround yourself either by specifying filenames 
yourself and looping over the png() calls, or counting yourself ...

Uwe Ligges


> Can anyone point me at where I can find this please?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Mon Feb 14 09:18:14 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 14 Feb 2005 08:18:14 +0000 (GMT)
Subject: [R] Sub with and without perl=TRUE
In-Reply-To: <66CE7F4ACF0C74439F9F6E0A568293020BA6FA@INDMAIL.lands.resnet.qg>
References: <66CE7F4ACF0C74439F9F6E0A568293020BA6FA@INDMAIL.lands.resnet.qg>
Message-ID: <Pine.LNX.4.61.0502140810460.22249@gannet.stats>

This was a bug which is already fixed in R-2.0.1 patched.  From the NEWS 
file

     o	gsub(perl=TRUE) returned a string which printed with trailing
 	garbage if there was a match at the beginning whose
 	replacement was shorter.  (PR#7479)

The crucial comment is that it only happened for a match at the beginning, 
and sub() was using the same code.

Pre-compiled versions of R-patched for Windows XP are available on CRAN.


On Mon, 14 Feb 2005, Denham Robert wrote:

> I have a problem doing substitution using sub and perl=TRUE when the
> elements of x have fewer characters than the replacement string.  Let me
> show you what I mean:
>
>> sub("m","billy","m")
> [1] "billy"
>
> But using perl=TRUE, I can only return a result as long as my x:
>> sub("m","billy","m",perl=TRUE)
> [1] "b"
>
>> sub("m","billy","ma",perl=TRUE)
> [1] "bi"
>
> Etc.
>
> Is this supposed to happen like this? I couldn't see a mention of this
> requirement in the help.
>
> I am using R 2.0.1 on windows xp.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From b.stollenwerk at gmx.de  Mon Feb 14 12:21:46 2005
From: b.stollenwerk at gmx.de (=?windows-1252?Q?Bj=F6rn_Stollenwerk?=)
Date: Mon, 14 Feb 2005 12:21:46 +0100
Subject: [R] gam(mgcv) starting values
Message-ID: <421089CA.3020504@gmx.de>

Hi all!

I?ve got some problems with the function gam (library mgcv). For some 
models I get the error message :

Error: no valid set of coefficients has been found:please supply 
starting values
In addition: Warning message:
NaNs produced in: log(x)

This is a shortened code I used:

gam(y ~ M1 + M3 + M4 + M5 + M6 + sex + M1*M3 + s(age),
family=Gamma(link ="identity"),
weights=days)

If I add for example an additional variable, say M7, the error-message 
occures. If I add M7 in combination with for example M8 it works.

Does somebody know, how to supply starting values or how to handle this 
problem.

I didn?t suxceed by adding
control=gam.control(spIterType="outer"),
or by
sp=137722.1

Thank you very much,

Bj?rn



From ramasamy at cancer.org.uk  Mon Feb 14 12:22:36 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 14 Feb 2005 11:22:36 +0000
Subject: [R] how can i make my program faster
In-Reply-To: <20050214045332.27313.qmail@web30705.mail.mud.yahoo.com>
References: <20050214045332.27313.qmail@web30705.mail.mud.yahoo.com>
Message-ID: <1108380156.5801.20.camel@ndmpc126.orc.ox.ac.uk>

On Sun, 2005-02-13 at 20:53 -0800, Cuichang Zhao wrote:
> Hello,
> right now, i have a program to collect data into a table. right now, my table is 
> table1 <- data.frame(trial = NA, x = NA, y = NA)

One often uses the term 'table' as in tabulate or cross-tabulate
discrete values.

> for each time when i want to add data into my data, i have to copy data of table into an array for each column, and then i add new data into my array, then i copy my array into the table one column by one column. For example
> temptrial <- table1$trial;
> temptrial <- c(temptrial, "1, 2");
> tempx <- table1$x;
> tempx <- c(tempx, "1, 2");
> tempy <- table1$y;
> tempy <- c(tempy, "1, 2");
>
> table1 <- data.frame(trial = temptrial, x = tempx, y = tempy);

I am not exactly sure what you are trying to do, but here is one way to
achieve the same results.

df <- data.frame(trial=NA, x=NA, y=NA)
rbind( df, "1, 2" )
  trial    x    y
1  <NA> <NA> <NA>
2  1, 2 1, 2 1, 2

Note that R will recycle an element such as "1, 2". See if the following
example helps

 set.seed(1)
 output <- matrix(nc=2, nr=5)
 for(i in 1:5){
   x <- sample(0:1, 100, replace=T) # simulate data
   output[i, ] <- c( table(x) )
 }
 output
     [,1] [,2]
[1,]   52   48
[2,]   46   54
[3,]   62   38
[4,]   57   43
[5,]   53   47

You can do the same for a cross-table where you flatten a table into a
vector using c() before inserting it into a the pre-fabricated output
matrix.

> the way i am doing makes my program very slow, because i need to copy the data from one variable to another variable all the time. however, it should not be necessary, so i am just wasting the memory, can any one tell me how can i make my program run faster without copying temparary data all the time.
>  
> Also, i have to write this table in a file using "cat", however, "cat" does not support the table format, so i can't do it. could any one also tell me how can i write my data into an outfile.

What do you mean by cat() does not support table format ? 

Have you tried sink() or write.table() ? Here are some examples

sink(file="R_sink.txt")
table(x, y)
sink("")

write.table( table(x, y), row.names=FALSE, file="R_table.txt" )

> Thank you so much
>  
> C-Ming 
>  
> Feb 13, 2005
> 
> 		
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From phdhwang at gmail.com  Mon Feb 14 13:29:31 2005
From: phdhwang at gmail.com (Kum-Hoe Hwang)
Date: Mon, 14 Feb 2005 21:29:31 +0900
Subject: [R] How to solve error : "cannot allocate vector of size 1208235
	Kb"
In-Reply-To: <Pine.LNX.4.44.0502111249560.4087-100000@reclus.nhh.no>
References: <b040cbb005021021337d7aa621@mail.gmail.com>
	<Pine.LNX.4.44.0502111249560.4087-100000@reclus.nhh.no>
Message-ID: <b040cbb0050214042930a363c1@mail.gmail.com>

I appreciate all who helped all.
My problem was not enough memory definietely.
First I tried smaple function that solved my problem nicely.
Second, I tried a method="sparse" option in lagsarlm model.
Comparing results led me to conclude the first trial that estimates 
better results in the lagsarlm model.

Thanks millions,

Kum

On Fri, 11 Feb 2005 13:02:53 +0100 (CET), Roger Bivand
<Roger.Bivand at nhh.no> wrote:
> On Fri, 11 Feb 2005, Kum-Hoe Hwang wrote:
> 
> > Howdy R gurus !
> >
> > I am newbie to R
> > I use R 2.0.1 in Windows XP. When I run R
> > I got the follwoing memory error.
> > My physical memory size is 3 Gb.
> > My R got the memory problem when it reached to
> > about 2 Gb.
> 
> This is not a question just about memory but also about methods. The
> number of rows in sfr.data is sufficiently large (about 35000?) that the
> NxN matrices being used need a lot of room. That is why the function
> provides the "SparseM" method to cater for your situation (unless you have
> chosen a representation of neighbour weights that is not similar to
> symmetric).
> 
> If you choose a symmetric or similar to symmetric representation, you can
> use the "SparseM" method, which, as its name suggests, uses sparse
> matrices. All of this is described in help(lagsarlm).
> 
> It is also quite possible that the last 95% of your data add almost no new
> information, so you could also consider choosing a subset for analysis, in
> which case the "eigen" method using dense matrices should work.
> 
> >
> > Thanks in advance,
> >
> >
> > > library(spdep)
> > > sfr.lagsarlm <- lagsarlm(sfr.data$Bldgsqft ~ sfr.data$Ncounty + sfr.data$Nugb + sfr.data$Ngroup, data=sfr.data, listw=sfr.listw, method="eigen")
> > Error: cannot allocate vector of size 1208235 Kb
> > > memory.size(max=FALSE)
> > [1] 17862584
> > > memory.limit(size=NA)
> > [1] 3145728000
> > >
> >
> >
> >
> 
> --
> Roger Bivand
> Economic Geography Section, Department of Economics, Norwegian School of
> Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
> Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
> e-mail: Roger.Bivand at nhh.no
> 
> 


-- 
Kum-Hoe Hwang, Ph.D.

Kyonggi Research Institute, Korea (ROK)
(Urban Planning and GIS)
Phone : 82-31-250-3283
Email : phdhwang at gmail.com



From gregor.gorjanc at bfro.uni-lj.si  Mon Feb 14 13:32:04 2005
From: gregor.gorjanc at bfro.uni-lj.si (Gregor GORJANC)
Date: Mon, 14 Feb 2005 13:32:04 +0100
Subject: [R] Re: [Rd] corrupt data frame: columns will be truncated or padded
 with NAs in: format.data.frame(x, digits = digits)
In-Reply-To: <Pine.LNX.4.61.0502141127480.5184@gannet.stats>
References: <7FFEE688B57D7346BC6241C55900E730B6FF14@pollux.bfro.uni-lj.si>
	<Pine.LNX.4.61.0502140725570.22249@gannet.stats>
	<421089C7.8080401@bfro.uni-lj.si>
	<Pine.LNX.4.61.0502141127480.5184@gannet.stats>
Message-ID: <42109A44.4050701@bfro.uni-lj.si>

Hello!

Sending this also to r-help so anyone can read it also there and maybe also 
help me with my puzzle if this trivial and I don't see it.

Prof Brian Ripley wrote:
[... removed some ...]
> You add a column, not replace part of a non-existent column.  Isn't that 
> obvious, given what you wrote?

# OK. If I do
tmp <- data.frame(y1=1:4, f1=factor(c("A", "B", "C", "D")))
tmp[1:2, "y2"] <- 2
tmp
# I am changing nonexistent column y2 in data frame tmp.

# If I do
tmp <- data.frame(y1=1:4, f1=factor(c("A", "B", "C", "D")))
tmp$y2 <- NA
tmp[1:2, "y2"] <- 2
tmp
# I am changing existent column. I understand now the difference. However,
# it is weird for me that this is OK (if column y2 does not yet exist)
tmp["y2"] <- 2
# but this is not
tmp[1:2, "y2"] <- 2

> There is a lot of basic documentation on data manipulation in R/S, and a 
> whole chapter in MASS4.  Somehow most other people don't seem to find 
> this a problem.

I just ordered MASS4 last week and I am eager to get it in my hands. In 
meanwhile I read quite some documentation and what I more or less saw is

tmp <- data.frame(y1=1:4, f1=factor(c("A", "B", "C", "D")))
tmp$y2 <- 1:4
tmp$y3 <- 2*tmp$y1
...
...

i.e. everybody is adding full column to data frame. But I would like to add 
just one part.

-- 
Lep pozdrav / With regards,
     Gregor GORJANC

---------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty       URI: http://www.bfro.uni-lj.si
Zootechnical Department    mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                  tel: +386 (0)1 72 17 861
SI-1230 Domzale            fax: +386 (0)1 72 17 888
Slovenia



From markus.jantti at iki.fi  Mon Feb 14 13:52:14 2005
From: markus.jantti at iki.fi (=?ISO-8859-1?Q?Markus_J=E4ntti?=)
Date: Mon, 14 Feb 2005 14:52:14 +0200
Subject: [R] using poly in a linear regression in the presence of NA fails
 (despite subsetting them out)
Message-ID: <42109EFE.2040208@iki.fi>

I ran into a to me surprising result on running lm with an orthogonal 
polynomial among the predictors.

The lm command resulted in

Error in qr(X) : NA/NaN/Inf in foreign function call (arg 1)
Error during wrapup:

despite my using a "subset" in the call to get rid of NA's.

poly is apparently evaluated before any NA's are subsetted out
of the data.

Example code (attached to this e-mail as as a script):
 > ## generate some data
 > n <- 50
 > x <- runif(n)
 > a0 <- 10
 > a1 <- .5
 > sigma.e <- 1
 > y <- a0 + a1*x + rnorm(n)*sigma.e
 > tmp.d <- data.frame(y, x)
 > rm(list=c("n", "x", "a0", "a1", "sigma.e", "y"))
 >
 > print(lm.1 <- lm(y ~ poly(x, 2), data = tmp.d)
+
+ ## now make a few NA's
+
+ tmp.d$x[1:2] <- rep(NA, 2)
Error: syntax error
Error during wrapup:
 >
 > ## this fails, just as it should
 > print(lm.1 <- lm(y ~ poly(x, 2), data = tmp.d))

Call:
lm(formula = y ~ poly(x, 2), data = tmp.d)

Coefficients:
(Intercept)  poly(x, 2)1  poly(x, 2)2
      10.380       -0.242       -1.441

 >
 > ## these also fail, but should not?
 >
 > print(lm.2 <- lm(y ~ poly(x, 2), data = tmp.d, subset = !is.na(x)))

Call:
lm(formula = y ~ poly(x, 2), data = tmp.d, subset = !is.na(x))

Coefficients:
(Intercept)  poly(x, 2)1  poly(x, 2)2
      10.380       -0.242       -1.441

 > print(lm.3 <- lm(y ~ poly(x, 2), data = tmp.d, na.action = na.omit))

Call:
lm(formula = y ~ poly(x, 2), data = tmp.d, na.action = na.omit)

Coefficients:
(Intercept)  poly(x, 2)1  poly(x, 2)2
      10.380       -0.242       -1.441

 >
 > ## but this works
 >
 > print(lm.3 <- lm(y ~ poly(x, 2), data = subset(tmp.d, subset = 
!is.na(x))))

Call:
lm(formula = y ~ poly(x, 2), data = subset(tmp.d, subset = !is.na(x)))

Coefficients:
(Intercept)  poly(x, 2)1  poly(x, 2)2
      10.380       -0.242       -1.441

--------------------

The documentation of lm is *not* misleading at this point, saying that

subset 	an optional vector specifying a subset of observations to be 
used in the fitting process.

which implies that data are subsetted once lm.fit is called.
All the same, this behavior is a little unexpected to me.
Is it to be considered a feature, that is, does it produce beneficial 
side effects which explain why it works as it does?

Regards,

Markus

I am running R on a Debian testing system with kernel 2.6.10 and

 > version
          _
platform i386-pc-linux-gnu
arch     i386
os       linux-gnu
system   i386, linux-gnu
status
major    2
minor    0.1
year     2004
month    11
day      15
language R
-- 
Markus Jantti
Abo Akademi University
markus.jantti at iki.fi
http://www.iki.fi/~mjantti
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: poly-example.R
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050214/68cdf3f1/poly-example.pl

From simon at stats.gla.ac.uk  Mon Feb 14 14:00:53 2005
From: simon at stats.gla.ac.uk (Simon Wood)
Date: Mon, 14 Feb 2005 13:00:53 +0000 (GMT)
Subject: [R] gam(mgcv) starting values
In-Reply-To: <421089CA.3020504@gmx.de>
References: <421089CA.3020504@gmx.de>
Message-ID: <Pine.LNX.4.58.0502141246280.23431@moon.stats.gla.ac.uk>

My guess is that your model is predicting a negative mean for some of your 
data. Since this is not possible for a Gamma r.v. the deviance calculation 
returns something non finite, which triggers the error message. This is 
possible because you have used an identity link. Is it not possible to use 
a log link?

If you have to use an identity link then I'd first check that 

y ~ M1 + M3 + M4 + M5 + M6 + M7+ sex + M1*M3 + age

works. If it does, then you could try starting with a very large min.sp 
argument when fitting the model with s(age), and slowly reducing it until 
the the estimated smoothing parameter is non-zero --- if this works then 
you've succeeded in finding the best fit model without any E(y) becoming 
negative in the process, but if it doesn't it probably means either that 
the model structure is wrong, or some E(y) really is very close to zero.

I doubt that altering starting values is likely to help here (the starting 
values won't make any E(y)<=0, after all).

best,
Simon

_____________________________________________________________________
> Simon Wood simon at stats.gla.ac.uk        www.stats.gla.ac.uk/~simon/
>>  Department of Statistics, University of Glasgow, Glasgow, G12 8QQ
>>>   Direct telephone: (0)141 330 4530          Fax: (0)141 330 4814


 

> Hi all!
> 
> I've got some problems with the function gam (library mgcv). For some models I
> get the error message :
> 
> Error: no valid set of coefficients has been found:please supply starting
> values
> In addition: Warning message:
> NaNs produced in: log(x)
> 
> This is a shortened code I used:
> 
> gam(y ~ M1 + M3 + M4 + M5 + M6 + sex + M1*M3 + s(age),
> family=Gamma(link ="identity"),
> weights=days)
> 
> If I add for example an additional variable, say M7, the error-message
> occures. If I add M7 in combination with for example M8 it works.
> 
> Does somebody know, how to supply starting values or how to handle this
> problem.
> 
> I didn't suxceed by adding
> control=gam.control(spIterType="outer"),
> or by
> sp=137722.1
> 
> Thank you very much,
> 
> Bj?rn
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From christoph.lehmann at gmx.ch  Mon Feb 14 14:16:07 2005
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Mon, 14 Feb 2005 14:16:07 +0100
Subject: [R] R on solaris 10 on a 64-bit ultra60 sparc
Message-ID: <4210A497.6090805@gmx.ch>

Hi
since I have no experience with solaris and sparc-architecture. We 
installed the latest Solaris 10 on a 64-bit ultra60 sparc machine. Since 
the solaris 10 is said to run native linux-applications: can I just 
download any r-binaries for linux? if yes, for which distribution?

or are there any solaris 10 binaries? or do I need to compile R myself 
on the sparc?

sorry for this beginner-question. but I am grateful for any small hint

cheers
christoph



From ripley at stats.ox.ac.uk  Mon Feb 14 14:27:03 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 14 Feb 2005 13:27:03 +0000 (GMT)
Subject: [R] Re: [Rd] corrupt data frame: columns will be truncated or
	padded with NAs in: format.data.frame(x, digits = digits)
In-Reply-To: <42109A44.4050701@bfro.uni-lj.si>
References: <7FFEE688B57D7346BC6241C55900E730B6FF14@pollux.bfro.uni-lj.si>
	<Pine.LNX.4.61.0502140725570.22249@gannet.stats>
	<421089C7.8080401@bfro.uni-lj.si>
	<Pine.LNX.4.61.0502141127480.5184@gannet.stats>
	<42109A44.4050701@bfro.uni-lj.si>
Message-ID: <Pine.LNX.4.61.0502141324130.19217@gannet.stats>

On Mon, 14 Feb 2005, Gregor GORJANC wrote:

> Sending this also to r-help so anyone can read it also there and maybe also 
> help me with my puzzle if this trivial and I don't see it.

Please don't, and especially do not after having removed the context.
So I have replied only on R-devel.

> Prof Brian Ripley wrote:
> [... removed some ...]
>> You add a column, not replace part of a non-existent column.  Isn't that 
>> obvious, given what you wrote?

Not if you subsequently remove what you wrote and re-post elsewhere, of 
course,

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Feb 14 14:30:50 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 14 Feb 2005 13:30:50 +0000 (GMT)
Subject: [R] R on solaris 10 on a 64-bit ultra60 sparc
In-Reply-To: <4210A497.6090805@gmx.ch>
References: <4210A497.6090805@gmx.ch>
Message-ID: <Pine.LNX.4.61.0502141327390.19217@gannet.stats>

On Mon, 14 Feb 2005, Christoph Lehmann wrote:

> since I have no experience with solaris and sparc-architecture. We installed 
> the latest Solaris 10 on a 64-bit ultra60 sparc machine. Since the solaris 10 
> is said to run native linux-applications: can I just download any r-binaries 
> for linux? if yes, for which distribution?

You would need sparc-linux binaries, and there are none available.

> or are there any solaris 10 binaries? or do I need to compile R myself on the 
> sparc?

You need to compile it yourself.  That is easy if you have a compiler, 
which Solaris does not by default.  Look in the R-admin manual for hints, 
and then ask whoever locally does have such experience to install what you 
need.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Mon Feb 14 14:37:38 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 14 Feb 2005 14:37:38 +0100
Subject: [R] R on solaris 10 on a 64-bit ultra60 sparc
In-Reply-To: <4210A497.6090805@gmx.ch>
References: <4210A497.6090805@gmx.ch>
Message-ID: <4210A9A2.7060303@statistik.uni-dortmund.de>

Christoph Lehmann wrote:

> Hi
> since I have no experience with solaris and sparc-architecture. We 
> installed the latest Solaris 10 on a 64-bit ultra60 sparc machine. Since 
> the solaris 10 is said to run native linux-applications: can I just 

Native for what? 32 bit linux on x86 architecture, 64 bit on x86, 32 bit 
on sparc, 64 bit on sparc??? I guess the latter, and there are no R 
binaries for linux on sparc available, AFAIK.

Conclusion: Compile yourself, so you can also make use of the SUN 
performance libraries (hope I remember name correctly) or ATLAS / Goto's 
BLAS.

Uwe Ligges


> download any r-binaries for linux? if yes, for which distribution?
> 
> or are there any solaris 10 binaries? or do I need to compile R myself 
> on the sparc?
> 
> sorry for this beginner-question. but I am grateful for any small hint
> 
> cheers
> christoph
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From skt at stat.psu.edu  Mon Feb 14 15:07:24 2005
From: skt at stat.psu.edu (Steven K Thompson)
Date: Mon, 14 Feb 2005 09:07:24 -0500
Subject: [R] missing X11 graphics title bar
In-Reply-To: <x2sm404x32.fsf@biostat.ku.dk>
References: <20050213042149.GC12378@linux18.stat.psu.edu>
	<x2sm404x32.fsf@biostat.ku.dk>
Message-ID: <20050214140724.GA30117@linux18.stat.psu.edu>

Thanks, Peter.  I think that helps narrow down the problem.  The
change of window manager seems to be a pretty decent solution.  If the bug gets
fixed or a workaround discovered, though, there might be some
advantage to going with the gnome default.  Thanks for the keystroke
information (on my laptop the resize one is alt fn10).  

Steve

On Sun, Feb 13, 2005 at 10:40:17AM +0100, Peter Dalgaard wrote:
> skt at stat.psu.edu (Steven K Thompson) writes:
> 
> > I have on occasion had the problem of missing title bars on X11
> > graphics windows when using R, and would like to know what others have
> > found in terms of the occurance, source, or solution of this problem.
> > In searching for information on this I found only the brief thread
> > from last November which I have copied below.  Anyone who has
> > experienced this knows it brings a very unwelcome interruption to work
> > as one struggles to move or close the obstructing graphics windows
> > having no frames.  
> > 
> > I first experienced this problem with a Mac OSX system in which I was
> > using R under X11.  My R program called C functions and the problem
> > only occured when I was using graphics with fairly intensive
> > computations going on at the same time.
> > 
> > Recently, I've encountered this problem with a Linux system (Ubuntu)
> > using Gnome 2.8 with its default window manager Metacity.  On the
> > advice from the thread below, I changed window manager (to icewm) and
> > that eliminated the problem, except that the default Metacity seems
> > otherwise to work more efficiently and faster with Gnome.
> > 
> > I should note that I have been using Debian 3.0 (with gnome/icewm) for
> > almost three years with never any problem like this.
> > 
> > Thanks in advance for any information anyone may have on this issue.
> > 
> > Steve
> 
> I did a bit of poking around a month or so ago, and found that the
> example programs from an Xlib tutorial (I believe it was from
> http://tronche.com/gui/x/xlib-tutorial/xlib-tutorial.tar.gz) did the
> same thing. So this has much more to do with Metacity than it does
> with R. The mystery is of course that all the non-raw-Xlib
> applications seem to get by quite nicely, even xterms, so there is
> probably a trick but I was unable to find it.
> 
> One should probably relay this information to the Gnome/Metacity guys
> and get them either to acknowledge the bug or divulge the workaround.
>  
> (BTW, alt-F7 and alt-F8 allow you to move and resize the frame even
> without the title bar, and maximize followed by unmaximize does put
> the decoration right).
> 
> > From: Luke Tierney <luke_at_stat.uiowa.edu>
> > Date: Fri 12 Nov 2004 - 01:16:00 EST
> > 
> > On Thu, 11 Nov 2004, Martyn Plummer wrote:
> > 
> > > On Wed, 2004-11-10 at 09:19 +0100, Martyn Plummer wrote:
> > > > On Tue, 2004-11-09 at 19:47, Jonathan Baron wrote:
> > > > > On 11/09/04 20:37, Jari Oksanen wrote:
> > > > > >
> > > > > >On 9 Nov 2004, at 19:44, Jonathan Baron wrote:
> > > > > >
> > > > > >> The RPM for Fedora Core 2 seems to work just fine on Core 3.
> > > > > >>
> > > > > >> (The graphics window got smaller, but I'm sure there is a
> > > setting
> > > > > >> for that.)
> > > > > >>
> > > > > >That would be good news. I really don't know how the graphics
> > > window
> > > > > >became so big at some stage. (MacOS X is just cute here: tiny,
> > > sharp,
> > > > > >fast graphics window.)
> > > > >
> > > > > I have the opposite problem, a 1680x1050 display.
> > > > >
> > > > > >Has the options()printcmd reappeared, so that dev.print() works
> > > without
> > > > > >changing default options?
> > > > >
> > > > > I can't imagine how this would change. This is the same "old"
> > > > > RPM, not a new one. The option is there, and I don't think it
> > > > > ever disappeared. I can't test it. This is my laptop, which is
> > > > > not set up to print anything.
> > > >
> > > > My mistake. The default print command is determined at configure
> > > time.
> > > > But the RedHat RPMS are built in a sandbox that has only the
> > > minimal
> > > > configuration needed to build R. This doesn't include the lpr
> > > package so
> > > > the default print command is null. I will fix this in the next RPM
> > > > release, but right now I am upgrading to FC3.
> > >
> > > An RPM for Fedora Core 3 should be on a CRAN mirror near you by the
> > > weekend. This fixes the printcmd bug.
> > >
> > > The X11() window is the right size for me, but it doesn't have a
> > > title
> > > bar, which is a nuisance.
> > >
> > > Martyn
> > >
> > 
> > I've seen this with X11 a little on FC2 but much more with rgl (maybe
> > 50% of the time with rgl vs 5-10% for X11()). Ross Ihaka tried
> > replacing the default metacity window manager with sawfish and found
> > that the problem seemed to go away. Ross also found a bug report for
> > metacity,
> > 
> >    https://bugzilla.redhat.com/bugzilla/show_bug.cgi?id=126571
> > 
> > that might be related. It might also be something we are not doing
> > quite right in opening a window that happens to bite metacity more
> > than other wms. If anyone has the time and energy to pursue this
> > please do.
> > 
> > Best,
> > 
> > luke
> 
> 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907

-- 
Steven K. Thompson
Department of Statistics
Pennsylvania State University
address for 2004-5 academic year:
Santa Fe, NM 87501
phone: 505 982 5466



From lee at molgen.mpg.de  Mon Feb 14 15:47:02 2005
From: lee at molgen.mpg.de (Ho-Joon Lee)
Date: Mon, 14 Feb 2005 15:47:02 +0100
Subject: [R] 'combinations' in gtools and stack overflow
Message-ID: <4210B9E6.4000603@molgen.mpg.de>

Dear R-users,

Let me ask about the 'stack overflow' error which I got when I used 
the function 'combinations' in "gtools".

The following is what I did:

---------
library(gtools)
options(expressions=1e5)
combinations(500, 3, 1:500)
# or combinations(400, 2, 1:400)
Error: protect(): stack overflow
---------

How can I overcome this error? 
Is there perhaps any other function to do this more efficiently?

Hope that somone can help me out.
Thanks in advance.

Lee

P.S. R 2.0.0; alphaev68-dec-osf4.0f



From davidr at rhotrading.com  Mon Feb 14 15:59:58 2005
From: davidr at rhotrading.com (davidr@rhotrading.com)
Date: Mon, 14 Feb 2005 08:59:58 -0600
Subject: [R] combinations without repetition
Message-ID: <12AE52872B5C5348BE5CF47C707FF53A502403@rhosvr02.rhotrading.com>

These would more properly be called permutations of a multiset.
You can find code on the net by searching for "generate permutations
multiset" (though not in R AFAICS.)

David L. Reiner

-----Original Message-----
From: mjenny at rumms.uni-mannheim.de [mailto:mjenny at rumms.uni-mannheim.de]

Sent: Sunday, February 13, 2005 8:45 AM
To: r-help at stat.math.ethz.ch
Subject: [R] combinations without repetition

The solution is probably simple but I need someone to point me to it.
How can I reduce the following 6 permutations to the 3 unique
combinations
(1,1,0) (1,0,1) (0,1,1)?

permn(c(1,1,0))
[[1]]
[1] 1 1 0

[[2]]
[1] 1 0 1

[[3]]
[1] 0 1 1

[[4]]
[1] 0 1 1

[[5]]
[1] 1 0 1

[[6]]
[1] 1 1 0

Thanks a lot
Marcelo Jenny

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Mon Feb 14 16:22:58 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 14 Feb 2005 15:22:58 +0000 (GMT)
Subject: [R] 'combinations' in gtools and stack overflow
In-Reply-To: <4210B9E6.4000603@molgen.mpg.de>
References: <4210B9E6.4000603@molgen.mpg.de>
Message-ID: <Pine.LNX.4.61.0502141520400.11692@gannet.stats>

On Mon, 14 Feb 2005, Ho-Joon Lee wrote:

> Dear R-users,
>
> Let me ask about the 'stack overflow' error which I got when I used the 
> function 'combinations' in "gtools".
>
> The following is what I did:
>
> ---------
> library(gtools)
> options(expressions=1e5)
> combinations(500, 3, 1:500)
> # or combinations(400, 2, 1:400)
> Error: protect(): stack overflow
> ---------
>
> How can I overcome this error? Is there perhaps any other function to do this 
> more efficiently?

R --help lists a flag that you can use to increase the size of the protect 
stack.  It is also described in `An Introduction to R'.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tlumley at u.washington.edu  Mon Feb 14 16:23:23 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 14 Feb 2005 07:23:23 -0800 (PST)
Subject: [R] graphics - current filename
In-Reply-To: <421055FA.4080805@statistik.uni-dortmund.de>
References: <5E06BFED29594F4C9C5EBE230DE320C6068027E5@ewok.vsl.com.au>
	<421055FA.4080805@statistik.uni-dortmund.de>
Message-ID: <Pine.A41.4.61b.0502140720280.344934@homer07.u.washington.edu>

On Mon, 14 Feb 2005, Uwe Ligges wrote:

> Paul Sorenson wrote:
>
>> I would like to query R for the current (or last used) filename for a 
>> graphics device.
>> 
>> Eg after png(filename="plot%02d.png") I would like something like the 
>> output of dev.cur() but with the %02d expanded to the current name.
>
> You cannot, it is handled internally and the name is not returned.
> So you have to workaround yourself either by specifying filenames yourself 
> and looping over the png() calls, or counting yourself ...

Or just look on the disk with list.files().

   sort(list.files(pattern="plot[0-9][0-9]\\.png"),
                   decreasing=TRUE)[1]


 	-thomas



From CYRIL.CAILLAULT at FORTISINVESTMENTS.COM  Mon Feb 14 16:25:55 2005
From: CYRIL.CAILLAULT at FORTISINVESTMENTS.COM (CYRIL.CAILLAULT@FORTISINVESTMENTS.COM)
Date: Mon, 14 Feb 2005 16:25:55 +0100
Subject: [R] DCUHRE - Multiple integration
Message-ID: <4100F581E9E1AE46A6D206637C9E7EF90F3649@i04mb144.fr.fimgroup>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050214/f102df08/attachment.pl

From tlumley at u.washington.edu  Mon Feb 14 16:35:46 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 14 Feb 2005 07:35:46 -0800 (PST)
Subject: [R] 64 Bit R Background Question
In-Reply-To: <200502140039.j1E0dDrT002388@uni08mr.unity.ncsu.edu>
References: <200502140039.j1E0dDrT002388@uni08mr.unity.ncsu.edu>
Message-ID: <Pine.A41.4.61b.0502140729500.344934@homer07.u.washington.edu>

On Sun, 13 Feb 2005, Thomas Colson wrote:

> Hi,
>
> I've collected quite a bit of elevation data (LIDAR elevation points) and am
> looking for a suitable platform to do analysis and modeling on it. The data
> is sitting in an Oracle database, one table, 200 million rows of x,y, and z.
> I'm trying to figure out what hardware resources we need to reserve in order
> to run 64 BIT R for this size data.
>
> Here's my question:
> Is the 64 BIT version of R appropriate for this size? Or is attempting to
> read all 200 million rows a pipe dream no matter what platform I'm using?

In principle R can handle this with enough memory. However, 200 million 
rows and three columns is 4.8Gb of storage, and R usually needs a few 
times the size of the data for working space.

You would likely be better off not reading the whole data set at once, but 
loading sections of it from Oracle as needed.


 	-thomas



From Kurt.Hornik at wu-wien.ac.at  Sun Feb 13 15:52:11 2005
From: Kurt.Hornik at wu-wien.ac.at (Kurt Hornik)
Date: Sun, 13 Feb 2005 15:52:11 +0100
Subject: [R] algorithms for matching and Hungarian method 
In-Reply-To: <420BB52F.3010702@ensam.inra.fr>
References: <420BB52F.3010702@ensam.inra.fr>
Message-ID: <16911.27035.2894.139389@mithrandir.hornik.net>

>>>>> Martin Olivier writes:

> Hi all,

> I would like to match two partitions. That is, if I have exactly the
> same objects grouped together for the two partitions, the labels may
> be arbitrarly permuted.  and so, i would like to know the
> correspondances of the groups between the two clusterings.

> In the e1701 pachage, it is possible to use the function
> matchClasses() for this problem. The problem is that for k greater
> than 10 (k number of classes), I have a memory problem. So I would
> like to know if this function explicitly examine all k! possible
> matches, or if it uses the Hungarian method (or an other optimal
> algorithm).  If not, do you know if I can find one.

e1071::matchClasses() does not.

However, clue::solve_LSAP() provides an implementation of the Hungarian
method for solving the LSAP.

Of course, you can also use the Simplex algorithm for solving the LSAP,
and in fact lpSolve::lp.assign() does that for you.

Hth
-k



From jtk at cmp.uea.ac.uk  Mon Feb 14 18:12:23 2005
From: jtk at cmp.uea.ac.uk (Jan T. Kim)
Date: Mon, 14 Feb 2005 17:12:23 +0000
Subject: [R] graphics - current filename
In-Reply-To: <Pine.A41.4.61b.0502140720280.344934@homer07.u.washington.edu>
References: <5E06BFED29594F4C9C5EBE230DE320C6068027E5@ewok.vsl.com.au>
	<421055FA.4080805@statistik.uni-dortmund.de>
	<Pine.A41.4.61b.0502140720280.344934@homer07.u.washington.edu>
Message-ID: <20050214171223.GB1424@jtkpc.cmp.uea.ac.uk>

On Mon, Feb 14, 2005 at 07:23:23AM -0800, Thomas Lumley wrote:
> On Mon, 14 Feb 2005, Uwe Ligges wrote:
> 
> >Paul Sorenson wrote:
> >
> >>I would like to query R for the current (or last used) filename for a 
> >>graphics device.
> >>
> >>Eg after png(filename="plot%02d.png") I would like something like the 
> >>output of dev.cur() but with the %02d expanded to the current name.
> >
> >You cannot, it is handled internally and the name is not returned.
> >So you have to workaround yourself either by specifying filenames yourself 
> >and looping over the png() calls, or counting yourself ...
> 
> Or just look on the disk with list.files().
> 
>   sort(list.files(pattern="plot[0-9][0-9]\\.png"),
>                   decreasing=TRUE)[1]

As a note of caution: Don't use this technique if you intend to run your
program more than once (unless you arrange for your program to remove
any preexisting plot[0-9][0-9].png files before entering into the loop in
question, which may also prove undesired some day down the line...).

Otherwise, if you e.g. repeat the same thing that produces files
plot01.png to plot99.png, the first run will work as intended, but the
second run will deal with plot99.png in each cycle of the loop...

The idea of querying the device for the current file name is basically
correct, because the device would be the authorive source of that
information, but as that isn't available, I'd second the recommendation
to specifying the file names yourself. Then, you are the authoritative
source.

Kind regards, Jan
-- 
 +- Jan T. Kim -------------------------------------------------------+
 |    *NEW*    email: jtk at cmp.uea.ac.uk                               |
 |    *NEW*    WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
 *-----=<  hierarchical systems are for files, not for humans  >=-----*



From gregory.r.warnes at pfizer.com  Mon Feb 14 18:00:31 2005
From: gregory.r.warnes at pfizer.com (Warnes, Gregory R)
Date: Mon, 14 Feb 2005 12:00:31 -0500
Subject: [R] 'combinations' in gtools and stack overflow
Message-ID: <915D2D65A9986440A277AC5C98AA466F978A7B@groamrexm02.amer.pfizer.com>


The documentation for 'combinations' explicitly describes this problem:

  Details:

     Caution: The number of combinations and permutations increases
     rapidly with 'n' and 'r'!.

     To use values of 'n' above about 45, you will need to increase R's
     recursion limit.  See the 'expression' argument to the 'options'
     command for details on how to do this.

and gives an example of the solution:

     # To use large 'n', you need to change the default recusion limit
     options(expressions=1e5)
     cmat <- combinations(300,2)
     dim(cmat) # 44850 by 2 

-Greg

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Prof 
> Brian Ripley
> Sent: Monday, February 14, 2005 10:23 AM
> To: Ho-Joon Lee
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] 'combinations' in gtools and stack overflow
> 
> 
> On Mon, 14 Feb 2005, Ho-Joon Lee wrote:
> 
> > Dear R-users,
> >
> > Let me ask about the 'stack overflow' error which I got 
> when I used the 
> > function 'combinations' in "gtools".
> >
> > The following is what I did:
> >
> > ---------
> > library(gtools)
> > options(expressions=1e5)
> > combinations(500, 3, 1:500)
> > # or combinations(400, 2, 1:400)
> > Error: protect(): stack overflow
> > ---------
> >
> > How can I overcome this error? Is there perhaps any other 
> function to do this 
> > more efficiently?
> 
> R --help lists a flag that you can use to increase the size 
> of the protect 
> stack.  It is also described in `An Introduction to R'.
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html



LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}



From davidr at rhotrading.com  Mon Feb 14 18:12:21 2005
From: davidr at rhotrading.com (davidr@rhotrading.com)
Date: Mon, 14 Feb 2005 11:12:21 -0600
Subject: [R] 'combinations' in gtools and stack overflow
Message-ID: <12AE52872B5C5348BE5CF47C707FF53A50240E@rhosvr02.rhotrading.com>

combn in package combinat does not use recursion, and so may be simpler
to use and may be able to do larger sets.

David L. Reiner

-----Original Message-----
From: Warnes, Gregory R [mailto:gregory.r.warnes at pfizer.com] 
Sent: Monday, February 14, 2005 11:01 AM
To: Ho-Joon Lee
Cc: r-help at stat.math.ethz.ch
Subject: RE: [R] 'combinations' in gtools and stack overflow


The documentation for 'combinations' explicitly describes this problem:

  Details:

     Caution: The number of combinations and permutations increases
     rapidly with 'n' and 'r'!.

     To use values of 'n' above about 45, you will need to increase R's
     recursion limit.  See the 'expression' argument to the 'options'
     command for details on how to do this.

and gives an example of the solution:

     # To use large 'n', you need to change the default recusion limit
     options(expressions=1e5)
     cmat <- combinations(300,2)
     dim(cmat) # 44850 by 2 

-Greg

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Prof 
> Brian Ripley
> Sent: Monday, February 14, 2005 10:23 AM
> To: Ho-Joon Lee
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] 'combinations' in gtools and stack overflow
> 
> 
> On Mon, 14 Feb 2005, Ho-Joon Lee wrote:
> 
> > Dear R-users,
> >
> > Let me ask about the 'stack overflow' error which I got 
> when I used the 
> > function 'combinations' in "gtools".
> >
> > The following is what I did:
> >
> > ---------
> > library(gtools)
> > options(expressions=1e5)
> > combinations(500, 3, 1:500)
> > # or combinations(400, 2, 1:400)
> > Error: protect(): stack overflow
> > ---------
> >
> > How can I overcome this error? Is there perhaps any other 
> function to do this 
> > more efficiently?
> 
> R --help lists a flag that you can use to increase the size 
> of the protect 
> stack.  It is also described in `An Introduction to R'.
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html



LEGAL NOTICE\ Unless expressly stated otherwise, this\ messa...{{dropped}}



From lee at molgen.mpg.de  Mon Feb 14 18:31:03 2005
From: lee at molgen.mpg.de (Ho-Joon Lee)
Date: Mon, 14 Feb 2005 18:31:03 +0100
Subject: [R] 'combinations' in gtools and stack overflow
In-Reply-To: <Pine.LNX.4.61.0502141520400.11692@gannet.stats>
References: <4210B9E6.4000603@molgen.mpg.de>
	<Pine.LNX.4.61.0502141520400.11692@gannet.stats>
Message-ID: <4210E057.5070700@molgen.mpg.de>

Thank you for your replies, David and Brian.

The function 'combn' in the package "combinat" works really fine without 
using any particular options.
Thanks a lot again for referring me to the package, David.

On the other hand, I do not know the meaning of "stack overflow" 
technically and am not interested in that.
It was the first time that I got the error.
But I am now, thanks to Brian, able to handle it by starting R with the 
following command-line option:

--max-ppsize=100000,

which is described in Appendix B in 'An Introduction to R'.
This is the only thing I can find in the material about 'stack'.

Kind regards,

Lee

Prof Brian Ripley wrote:

> On Mon, 14 Feb 2005, Ho-Joon Lee wrote:
>
>> Dear R-users,
>>
>> Let me ask about the 'stack overflow' error which I got when I used 
>> the function 'combinations' in "gtools".
>>
>> The following is what I did:
>>
>> ---------
>> library(gtools)
>> options(expressions=1e5)
>> combinations(500, 3, 1:500)
>> # or combinations(400, 2, 1:400)
>> Error: protect(): stack overflow
>> ---------
>>
>> How can I overcome this error? Is there perhaps any other function to 
>> do this more efficiently?
>
>
> R --help lists a flag that you can use to increase the size of the 
> protect stack.  It is also described in `An Introduction to R'.
>



From ripley at stats.ox.ac.uk  Mon Feb 14 18:38:38 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 14 Feb 2005 17:38:38 +0000 (GMT)
Subject: [R] 'combinations' in gtools and stack overflow
In-Reply-To: <915D2D65A9986440A277AC5C98AA466F978A7B@groamrexm02.amer.pfizer.com>
References: <915D2D65A9986440A277AC5C98AA466F978A7B@groamrexm02.amer.pfizer.com>
Message-ID: <Pine.LNX.4.61.0502141729510.13293@gannet.stats>

On Mon, 14 Feb 2005, Warnes, Gregory R wrote:

> The documentation for 'combinations' explicitly describes this problem:

Not so: this is misinformation send in reply to correct and helpful 
comments.

>  Details:
>
>     Caution: The number of combinations and permutations increases
>     rapidly with 'n' and 'r'!.
>
>     To use values of 'n' above about 45, you will need to increase R's
>     recursion limit.  See the 'expression' argument to the 'options'
>     command for details on how to do this.
>
> and gives an example of the solution:
>
>     # To use large 'n', you need to change the default recusion limit
>     options(expressions=1e5)
>     cmat <- combinations(300,2)
>     dim(cmat) # 44850 by 2

Let me repeat for you what he said he did:

>>> library(gtools)
>>> options(expressions=1e5)
>>> combinations(500, 3, 1:500)
>>> # or combinations(400, 2, 1:400)
>>> Error: protect(): stack overflow

Do you notice the second line there this time around?

The message is not about the number of expressions, but about the 
sizd of the protect stack.

R --max-ppsize=50000

allows combinations(400, 2, 1:400) to work, as I correctly suggested.

>
> -Greg
>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Prof
>> Brian Ripley
>> Sent: Monday, February 14, 2005 10:23 AM
>> To: Ho-Joon Lee
>> Cc: r-help at stat.math.ethz.ch
>> Subject: Re: [R] 'combinations' in gtools and stack overflow
>>
>>
>> On Mon, 14 Feb 2005, Ho-Joon Lee wrote:
>>
>>> Dear R-users,
>>>
>>> Let me ask about the 'stack overflow' error which I got
>> when I used the
>>> function 'combinations' in "gtools".
>>>
>>> The following is what I did:
>>>
>>> ---------
>>> library(gtools)
>>> options(expressions=1e5)
>>> combinations(500, 3, 1:500)
>>> # or combinations(400, 2, 1:400)
>>> Error: protect(): stack overflow
>>> ---------
>>>
>>> How can I overcome this error? Is there perhaps any other
>> function to do this
>>> more efficiently?
>>
>> R --help lists a flag that you can use to increase the size
>> of the protect
>> stack.  It is also described in `An Introduction to R'.
>>
>> --
>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>> University of Oxford,             Tel:  +44 1865 272861 (self)
>> 1 South Parks Road,                     +44 1865 272866 (PA)
>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
>
>
> LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From gunter.berton at gene.com  Mon Feb 14 18:41:25 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 14 Feb 2005 09:41:25 -0800
Subject: Off topic -- large data sets. Was RE: [R] 64 Bit R Background Question
In-Reply-To: <Pine.A41.4.61b.0502140729500.344934@homer07.u.washington.edu>
Message-ID: <200502141741.j1EHfPCk017510@volta.gene.com>


> > read all 200 million rows a pipe dream no matter what 
> platform I'm using?
> 
> In principle R can handle this with enough memory. However, 
> 200 million 
> rows and three columns is 4.8Gb of storage, and R usually needs a few 
> times the size of the data for working space.
> 
> You would likely be better off not reading the whole data set 
> at once, but 
> loading sections of it from Oracle as needed.
> 
> 
>  	-thomas
> 

Thomas's comment raises a question:

Can comeone give me an example (perhaps in a private response, since I'm off
topic here) where one actually needs all cases in a large data set ("large"
being > 1e6, say) to do a STATISTICAL analysis? By "statistical" I exclude,
say searching for some particular characteristic like an adverse event in a
medical or customer repair database, etc. Maybe a definition of
"statistical" is: anything that cannot be routinely done in a single pass
database query.

The reason I ask this is that it seems to me that with millions of cases,
(careful, perhaps stratified or in some other not completely at random way)
sampling should always suffice to reduce a dataset to manageable size
sufficient for the data analysis needs at hand. But my ignorance and naivete
probably show here.

Thanks.

-- Bert



From lee at molgen.mpg.de  Mon Feb 14 18:44:50 2005
From: lee at molgen.mpg.de (Ho-Joon Lee)
Date: Mon, 14 Feb 2005 18:44:50 +0100
Subject: [R] 'combinations' in gtools and stack overflow
In-Reply-To: <12AE52872B5C5348BE5CF47C707FF53A50240E@rhosvr02.rhotrading.com>
References: <12AE52872B5C5348BE5CF47C707FF53A50240E@rhosvr02.rhotrading.com>
Message-ID: <4210E392.3050805@molgen.mpg.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050214/ab72746d/attachment.pl

From tom_colson at ncsu.edu  Mon Feb 14 19:02:31 2005
From: tom_colson at ncsu.edu (Thomas Colson)
Date: Mon, 14 Feb 2005 13:02:31 -0500
Subject: Off topic -- large data sets. Was RE: [R] 64 Bit R Background
	Question
In-Reply-To: <200502141741.j1EHfPCk017510@volta.gene.com>
Message-ID: <200502141802.j1EI2aWL029796@uni03mr.unity.ncsu.edu>

 The purpose of investigating the entire (200 million record) data set is to
investigate several interpolation models for creating gridded elevation
data. Most models and algorithms do just that...take a manageable number of
"points" and do the math. My reasoning behind using the entire dataset
(which is still a sample of the entire population of possible elevation
values) is perhaps we can "tweak" algorithms that were created 10 years ago
using photo derived contour lines and shuttle radar...but using high
resolution (1 meter average posting density) LIDAR elevation data. A review
of interpolating elevation surfaces to digital terrain models isn't
appropriate for this forum, but, needless to say, the more points I can get
into the model, the more confidence I can get in its output. 


Recent studies have shown that hydrologic models using coarser resolution
(elevation points spaced farther apart) are often way off in their
predictions of aspect and slop calculations...which manifest themselves to
all manner of hydrologic processes (Wetness index, time to concentration,
peak flow, etc..) Thus, the relationship between every single point in this
particular data set must be investigated so we can somehow quantify the
exact impact of error in datasets with coarser resolution, or perhaps come
up with a custom routine based on what R tells us to create a new
interpolation method. 

Ambitious? Yes....



Thanks for the replies so far. 




Tom Colson
Center for Earth Observation
North Carolina State University 
Raleigh, NC 27695
(919) 515 3434
(919) 673 8023
tom_colson at ncsu.edu

Online Calendar:
http://www4.ncsu.edu/~tpcolson



-----Original Message-----
From: Berton Gunter [mailto:gunter.berton at gene.com] 
Sent: Monday, February 14, 2005 12:41 PM
To: 'Thomas Lumley'; 'Thomas Colson'
Cc: r-help at stat.math.ethz.ch
Subject: Off topic -- large data sets. Was RE: [R] 64 Bit R Background
Question


> > read all 200 million rows a pipe dream no matter what
> platform I'm using?
> 
> In principle R can handle this with enough memory. However, 200 
> million rows and three columns is 4.8Gb of storage, and R usually 
> needs a few times the size of the data for working space.
> 
> You would likely be better off not reading the whole data set at once, 
> but loading sections of it from Oracle as needed.
> 
> 
>  	-thomas
> 

Thomas's comment raises a question:

Can comeone give me an example (perhaps in a private response, since I'm off
topic here) where one actually needs all cases in a large data set ("large"
being > 1e6, say) to do a STATISTICAL analysis? By "statistical" I exclude,
say searching for some particular characteristic like an adverse event in a
medical or customer repair database, etc. Maybe a definition of
"statistical" is: anything that cannot be routinely done in a single pass
database query.

The reason I ask this is that it seems to me that with millions of cases,
(careful, perhaps stratified or in some other not completely at random way)
sampling should always suffice to reduce a dataset to manageable size
sufficient for the data analysis needs at hand. But my ignorance and naivete
probably show here.

Thanks.

-- Bert




From uofiowa at gmail.com  Mon Feb 14 19:08:17 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Mon, 14 Feb 2005 13:08:17 -0500
Subject: [R] roll id
Message-ID: <3f87cc6d050214100815df23c8@mail.gmail.com>

I am collecting one price for "market" daily. 

date1 price1  
date2 price2 
date3 price3 

I have a roll schedule where if 
date1 is between d1 and d2 then market is roll_id 0
date1 is between d2 and d3 then market is roll_id 1
date1 is between d3 and d4 then market is roll_id 2

and so on for date2 and date3

is there a function in anywhere or in a time series package that I can
use or should I loop through my price data and figure out the roll_id
based on teh roll schedule and price date?



From ripley at stats.ox.ac.uk  Mon Feb 14 19:25:22 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 14 Feb 2005 18:25:22 +0000 (GMT)
Subject: Off topic -- large data sets. Was RE: [R] 64 Bit R Background
	Question
In-Reply-To: <200502141741.j1EHfPCk017510@volta.gene.com>
References: <200502141741.j1EHfPCk017510@volta.gene.com>
Message-ID: <Pine.LNX.4.61.0502141814330.18960@gannet.stats>

On Mon, 14 Feb 2005, Berton Gunter wrote:

>
>>> read all 200 million rows a pipe dream no matter what
>> platform I'm using?
>>
>> In principle R can handle this with enough memory. However,
>> 200 million
>> rows and three columns is 4.8Gb of storage, and R usually needs a few
>> times the size of the data for working space.
>>
>> You would likely be better off not reading the whole data set
>> at once, but
>> loading sections of it from Oracle as needed.
>>
>>
>>  	-thomas
>>
>
> Thomas's comment raises a question:
>
> Can comeone give me an example (perhaps in a private response, since I'm off
> topic here) where one actually needs all cases in a large data set ("large"
> being > 1e6, say) to do a STATISTICAL analysis? By "statistical" I exclude,
> say searching for some particular characteristic like an adverse event in a
> medical or customer repair database, etc. Maybe a definition of
> "statistical" is: anything that cannot be routinely done in a single pass
> database query.
>
> The reason I ask this is that it seems to me that with millions of cases,
> (careful, perhaps stratified or in some other not completely at random way)
> sampling should always suffice to reduce a dataset to manageable size
> sufficient for the data analysis needs at hand. But my ignorance and naivete
> probably show here.

I think they are very rare.  I have seen just one, a Poisson glm for a not 
common event (so ca 70% of the counts were zero) with nine categorical 
predictors.  There were about 0.7m records, and a 10% sample was not 
sufficient to select a model (we got quite different answers from 
different samples) that predicted accurately on fairly small (but still 
10,000 or more) subgroups.  Homogeneity is always suspect in large 
datasets, and in retrospect we could have used several sub-models 
splitting on some of the variables.  But a log-linear model is one of the 
few ways I know to effectively summarize such large categorical datasets.

In the original problem (which was spatial) I suspect that a local (in 
space) model would suffice.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ramasamy at cancer.org.uk  Mon Feb 14 20:09:45 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 14 Feb 2005 19:09:45 +0000
Subject: [R] roll id
In-Reply-To: <3f87cc6d050214100815df23c8@mail.gmail.com>
References: <3f87cc6d050214100815df23c8@mail.gmail.com>
Message-ID: <1108408185.6013.45.camel@ndmpc126.orc.ox.ac.uk>

See help("cut") for general or help("cut.POSIXt") for time related cut.


On Mon, 2005-02-14 at 13:08 -0500, Omar Lakkis wrote:
> I am collecting one price for "market" daily. 
> 
> date1 price1  
> date2 price2 
> date3 price3 
> 
> I have a roll schedule where if 
> date1 is between d1 and d2 then market is roll_id 0
> date1 is between d2 and d3 then market is roll_id 1
> date1 is between d3 and d4 then market is roll_id 2
> 
> and so on for date2 and date3
> 
> is there a function in anywhere or in a time series package that I can
> use or should I loop through my price data and figure out the roll_id
> based on teh roll schedule and price date?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From bill.shipley at usherbrooke.ca  Mon Feb 14 20:18:17 2005
From: bill.shipley at usherbrooke.ca (Bill Shipley)
Date: Mon, 14 Feb 2005 14:18:17 -0500
Subject: [R] testing equality of variances across groups in lme?
Message-ID: <001e01c512c9$f05d73f0$ae1ad284@BIO041>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050214/1f82cbaf/attachment.pl

From KMarkus at aol.com  Mon Feb 14 20:32:09 2005
From: KMarkus at aol.com (KMarkus@aol.com)
Date: Mon, 14 Feb 2005 14:32:09 EST
Subject: [R] Re: Bubble Plot with Pie Charts 
Message-ID: <fa.cca01d7.2f4256b9@aol.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050214/7ad4d6e3/attachment.pl

From p.murrell at auckland.ac.nz  Mon Feb 14 21:09:46 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Tue, 15 Feb 2005 09:09:46 +1300
Subject: [R] R News: Call for Papers
Message-ID: <4211058A.3080503@stat.auckland.ac.nz>

Dear useRs and developeRs,

the next issue of `R News' is scheduled for the beginning of May
and we are now  accepting submissions for this first issue in 2005.
For more information see

       http://cran.r-project.org/doc/Rnews/

If you are the author of a package on CRAN and you would like to promote
it a little bit, or if you simply have an interesting application using
R, we hope you can find some time to write a short article on it. We
suggest that it be approximately 3 pages or less. The idea of the
newsletter is that it be interesting to R users without being too
technical. For example an article describing a package could begin by
briefly outlining the statistical background and go on to demonstrate
the usage on some typical data set. Of course graphics are more than
welcome!

Bill Venables <Bill.Venables at csiro.au> is also encouraging submissions
to the more specialist Programmer's Niche column. In this case the
technical level could be a little higher, of course, but not necessarily:
ingeniousness is the key.

The R Help Desk column is intended to present answers to frequently
asked questions as well as tricks that are useful to the majority of
useRs. Please send submissions to Uwe Ligges <Uwe.Ligges at R-project.org>.

The deadline for submissions is

	April, 10th, 2005

Keep the contributions rolling in!

The Editorial Board,

Doug Bates, Paul Murrell and Torsten Hothorn



From helprhelp at gmail.com  Mon Feb 14 21:30:58 2005
From: helprhelp at gmail.com (WeiWei Shi)
Date: Mon, 14 Feb 2005 14:30:58 -0600
Subject: [R] a syntax question: $V100
Message-ID: <cdf8178305021412304459153a@mail.gmail.com>

Hi, there:

I have a syntax question. I have a dataset x with 100 variables. I did
not set the column name so I used x$V1...x$V100. For my case, I need
to put the number (e.g. 20) into another variable, like index so that
I can refer to x$V20 by using something like x$V(index) but I don't
know how to do that in R.

thanks,

Ed



From andy_liaw at merck.com  Mon Feb 14 21:37:48 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 14 Feb 2005 15:37:48 -0500
Subject: [R] a syntax question: $V100
Message-ID: <3A822319EB35174CA3714066D590DCD50994E6DD@usrymx25.merck.com>

x[,20] or x[[20]] will give you the 20th column of the data frame.  Things
like these are covered in `An Introduction to R'.

If the column names are not necessarily in order, you can always use
something like x[[paste("V", 20, sep="")]].

Andy

> From: WeiWei Shi
> 
> Hi, there:
> 
> I have a syntax question. I have a dataset x with 100 variables. I did
> not set the column name so I used x$V1...x$V100. For my case, I need
> to put the number (e.g. 20) into another variable, like index so that
> I can refer to x$V20 by using something like x$V(index) but I don't
> know how to do that in R.
> 
> thanks,
> 
> Ed
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From rountree at cs.uga.edu  Mon Feb 14 22:56:10 2005
From: rountree at cs.uga.edu (Barry Rountree)
Date: Mon, 14 Feb 2005 16:56:10 -0500
Subject: [Thank-you]  [R] Help: sorting data frames
Message-ID: <200502141656.17367.rountree@cs.uga.edu>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hello,

Back in 2002, you answered a question posted to the r-help mailing list, 
something like:

> Hello everyone:
> I have a data frame with two columns of data. I want to create a new 
> data frame, in which the first column is sorted in accending order, and 
> the elements of the second column are properly re-ordered (to be in the 
> same row with their corresponding first column elements)

R> mydf[order(mydf[,1]),]

Torsten

That was exactly the question I had, the answer was exactly what I needed, and 
I appreciate you taking the trouble to answer it publicly.  

Thanks,

Barry Rountree
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.2.4 (GNU/Linux)

iD8DBQFCER56hlM7FvEcdO8RAtvGAJ0T7urq8lBjIGQ465anqV7KaovnTwCdGpo2
BbdDRnqc43maaBdzoqa80sY=
=ZPHp
-----END PGP SIGNATURE-----



From nair at sdsc.edu  Mon Feb 14 23:48:01 2005
From: nair at sdsc.edu (T. Murlidharan Nair)
Date: Mon, 14 Feb 2005 14:48:01 -0800
Subject: [R] sample
References: <3A822319EB35174CA3714066D590DCD50994E6B7@usrymx25.merck.com>	
	<420CE18B.3070705@sdsc.edu>
	<1108172210.6237.74.camel@dhcp-63.ccc.ox.ac.uk>
Message-ID: <42112AA1.8070402@sdsc.edu>

My R version s 2.0.1 and I am running it under windows. I want to use R 
directly, but
in this case since I could not figure out what went wrong I tried to 
transpose it in perl.
Certainly I understand your concern regarding  handling numerical data 
in Perl.  I read the file using read.csv, it
reads the original file very well.  I am not using "-" as a field 
 seperator.  The data is too huge and
I need to check with my collaborators before I send it to out.
Many thanks for your comments.
Cheers ./Murli



Adaikalavan Ramasamy wrote:

>See comments below.
>
>
>On Fri, 2005-02-11 at 08:47 -0800, T. Murlidharan Nair wrote:
>  
>
>>Thanks to all for trying to help me with problem. After spending a long 
>>time, I eventually solved it
>>by writing a perl script and transposing the matrix 
>>    
>>
>
>If you plan on doing your analysis mostly in R, it would be best to use
>R directly. While perl is great for many things, I recommend caution
>with numerical data (e.g. it will happily add a character and a number)
>
>Besides, you may risk extra computing overhead and debugging nightmares.
>
>
>  
>
>>and re-reading the file. When I did this I got an
>>error saying I had duplicate row names (which in fact was not true or 
>>    
>>
>
>How did you read the file ? e.g. read.delim, read.table, read.csv, ...
>
>  
>
>>may be something to with
>>the naming conventions in R).
>>The column names were  :
>>    
>>
>
>Wouldn't this become row names after you transpose it ? Nevermind, I
>have tested both cases below.
>
>
>  
>
>>Tumor_VA_114-1, Tumor_VA_114-2,.....
>>But when I changed it to Tumor_VA_114_1, Tumor_VA_114_2 it worked fine.
>>I was not aware that - cannot be used to differentiate variables. Is 
>>this the case ?
>>    
>>
>
>Not true. I successfully read in (tab delimited) files containing either
>of the following contents in Redhat Fedora Core 3 and R-1.9.1 :
>
>read.delim(file="file1.txt", row.names=1, header=TRUE)
>read.delim(file="file2.txt", row.names=1, header=TRUE)
>
>----------------- File 1 ---------------------
>Index	Tumor_VA_114-1	Tumor_VA_114-2
>A	10	100
>B	20	200
>----------------------------------------------
>
>----------------- File 2 ---------------------
>Index	Value
>Tumor_VA_114-1	100
>Tumor_VA_114-2	200
>---------------------------------------------
>
>Another possibility is that somehow you are using "-" as the field
>separator. 
>
>Maybe some other software that use in between corrupted the dimnames ?
>
>We can only guess since you have not provided neither a simple
>reproducible example, your operating system or working R version. Please
>read the posting guide first.
>
>
>  
>
>>Sorry, if I wasted any of your time.
>>Cheers ../Murli
>>
>>    
>>



From jcjorgensen at wisc.edu  Tue Feb 15 01:12:20 2005
From: jcjorgensen at wisc.edu (Jeff Jorgensen)
Date: Mon, 14 Feb 2005 18:12:20 -0600
Subject: [R] lattice multiple plots per page
Message-ID: <5.2.1.1.2.20050214174512.01ce0e90@wiscmail.wisc.edu>

Dear R-sters,

I was wondering if anyone has encountered the following issues.  I've 
figured out how to get multiple levelplots [library(lattice)] on a single 
plot.  However, when I add text (adding axis labels for the entire four 
panel plot) the text is missing when I insert the *.eps file I've created 
into my LaTeX document (via MikTeX-WinEdt).  And, I've just upgraded to R 
v2.0.1 from v1.8.1 (Windows), and each individual levelplot is smaller 
compared to the older R release.

Any clues as to 1) how I can recover the lost text, and 2) increase the 
size of each of the levelplots?

Cheers,

Jeff Jorgensen

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Sample code that illustrates what I'm trying to do:

#create a levelplot
x<-seq(pi/4, 5*pi, length = 100)
y<-seq(pi/4, 5*pi, length = 100)
r<-as.vector(sqrt(outer(x^2, y^2, "+")))
grid<-expand.grid(x=x, y=y)
grid$z<-cos(r^2) * exp(-r/(pi^3))
a<-levelplot(z~x*y, grid, cuts = 50, xlab="", ylab=",
		colorkey = FALSE)

#create the multiple panel plot, here using all the same levelplot
trellis.par.set(list(background=list(col="white"))) #white background
#using position to scale the plots up and to the right ~10%
#to make room for the axis labels
print(a,position=c(0.1,0.1,1,1),split=c(1,1,2,2),more=T)
print(a,position=c(0.1,0.1,1,1),split=c(1,2,2,2),more=T)
print(a,position=c(0.1,0.1,1,1),split=c(2,1,2,2),more=T)
print(a,position=c(0.1,0.1,1,1),split=c(2,2,2,2),more=F)
#commands that let you click where you want the labels centered
ltext(grid::grid.locator(),lab="x-axis label, where I click",cex=1.5)
ltext(grid::grid.locator(),lab="y-axis label, where I click",cex=1.5,srt=90)
#save device to an *.eps file, to be called later by a \includegraphics command
dev.copy2eps(file="twobytwoplot.eps")



-------------- next part --------------

---




From andy_liaw at merck.com  Tue Feb 15 04:41:22 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 14 Feb 2005 22:41:22 -0500
Subject: [R] using poly in a linear regression in the presence of NA
	f ails (despite subsetting them out)
Message-ID: <3A822319EB35174CA3714066D590DCD50994E6E6@usrymx25.merck.com>

This smells like a bug to me.  The error is triggered by the line:

   variables <- eval(predvars, data, env)

inside model.frame.default().  At that point, na.action has not been
applied, so poly() ended being called on data that still contains missing
values.  The qr() that issued the error is for generating the orthogonal
basis when evaluating poly(), not for fitting the linear model itself.

Essentially, calling 

  model.frame(y ~ poly(x, 2), data=data.frame(x=c(NA, 1:3), y=rnorm(4)), 
              na.action=na.omit)

would show the same error.  The obvious workaround is to omit cases with NAs
before calling lm().

Andy

> From: Markus J?ntti
> 
> I ran into a to me surprising result on running lm with an orthogonal 
> polynomial among the predictors.
> 
> The lm command resulted in
> 
> Error in qr(X) : NA/NaN/Inf in foreign function call (arg 1)
> Error during wrapup:
> 
> despite my using a "subset" in the call to get rid of NA's.
> 
> poly is apparently evaluated before any NA's are subsetted out
> of the data.
> 
> Example code (attached to this e-mail as as a script):
>  > ## generate some data
>  > n <- 50
>  > x <- runif(n)
>  > a0 <- 10
>  > a1 <- .5
>  > sigma.e <- 1
>  > y <- a0 + a1*x + rnorm(n)*sigma.e
>  > tmp.d <- data.frame(y, x)
>  > rm(list=c("n", "x", "a0", "a1", "sigma.e", "y"))
>  >
>  > print(lm.1 <- lm(y ~ poly(x, 2), data = tmp.d)
> +
> + ## now make a few NA's
> +
> + tmp.d$x[1:2] <- rep(NA, 2)
> Error: syntax error
> Error during wrapup:
>  >
>  > ## this fails, just as it should
>  > print(lm.1 <- lm(y ~ poly(x, 2), data = tmp.d))
> 
> Call:
> lm(formula = y ~ poly(x, 2), data = tmp.d)
> 
> Coefficients:
> (Intercept)  poly(x, 2)1  poly(x, 2)2
>       10.380       -0.242       -1.441
> 
>  >
>  > ## these also fail, but should not?
>  >
>  > print(lm.2 <- lm(y ~ poly(x, 2), data = tmp.d, subset = !is.na(x)))
> 
> Call:
> lm(formula = y ~ poly(x, 2), data = tmp.d, subset = !is.na(x))
> 
> Coefficients:
> (Intercept)  poly(x, 2)1  poly(x, 2)2
>       10.380       -0.242       -1.441
> 
>  > print(lm.3 <- lm(y ~ poly(x, 2), data = tmp.d, na.action = 
> na.omit))
> 
> Call:
> lm(formula = y ~ poly(x, 2), data = tmp.d, na.action = na.omit)
> 
> Coefficients:
> (Intercept)  poly(x, 2)1  poly(x, 2)2
>       10.380       -0.242       -1.441
> 
>  >
>  > ## but this works
>  >
>  > print(lm.3 <- lm(y ~ poly(x, 2), data = subset(tmp.d, subset = 
> !is.na(x))))
> 
> Call:
> lm(formula = y ~ poly(x, 2), data = subset(tmp.d, subset = !is.na(x)))
> 
> Coefficients:
> (Intercept)  poly(x, 2)1  poly(x, 2)2
>       10.380       -0.242       -1.441
> 
> --------------------
> 
> The documentation of lm is *not* misleading at this point, saying that
> 
> subset 	an optional vector specifying a subset of 
> observations to be 
> used in the fitting process.
> 
> which implies that data are subsetted once lm.fit is called.
> All the same, this behavior is a little unexpected to me.
> Is it to be considered a feature, that is, does it produce beneficial 
> side effects which explain why it works as it does?
> 
> Regards,
> 
> Markus
> 
> I am running R on a Debian testing system with kernel 2.6.10 and
> 
>  > version
>           _
> platform i386-pc-linux-gnu
> arch     i386
> os       linux-gnu
> system   i386, linux-gnu
> status
> major    2
> minor    0.1
> year     2004
> month    11
> day      15
> language R
> -- 
> Markus Jantti
> Abo Akademi University
> markus.jantti at iki.fi
> http://www.iki.fi/~mjantti
>



From Lorenz.Gygax at fat.admin.ch  Tue Feb 15 07:22:53 2005
From: Lorenz.Gygax at fat.admin.ch (Lorenz.Gygax@fat.admin.ch)
Date: Tue, 15 Feb 2005 07:22:53 +0100
Subject: [R] testing equality of variances across groups in lme?
Message-ID: <BF74FADD4B44554CA7E53D0B5242CD6A01FC64E5@evd-s7014.evd.admin.ch>

> I am fitting a two-level mixed model which assumes equality of
> variance in the lowest-level residuals across groups. 
> The call is:
> 
> fit3<-lme(CLnNAR~CLnRGR,data=meta.analysis,
> + na.action="na.omit",random=~1+CLnRGR|study.code)

I assume that CLnRGR is a factor and thus the groups which might have
different variance in residuals are given by the different levels of
CLnRGR!?

> ...
> know that one can test a nested model in which the residuals are
> given weights, such as:

You are on the right track here, just use:

fit4<-lme(CLnNAR~CLnRGR,data=meta.analysis, na.action="na.omit",
random=~CLnRGR|study.code, weights= varIdent (form= ~ 1 | CLnRGR))

with anova (fit3, fit4) you can test whether these weights improve the fits
statistically.

Lorenz
- 
Lorenz Gygax, Dr. sc. nat.
Centre for proper housing of ruminants and pigs
Swiss Federal Veterinary Office
agroscope FAT T?nikon, CH-8356 Ettenhausen / Switzerland



From ripley at stats.ox.ac.uk  Tue Feb 15 08:19:18 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 15 Feb 2005 07:19:18 +0000 (GMT)
Subject: [R] using poly in a linear regression in the presence of NA f
	ails (despite subsetting them out)
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E6E6@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E6E6@usrymx25.merck.com>
Message-ID: <Pine.LNX.4.61.0502150708360.31658@gannet.stats>

Andy,

I don't think it is a bug.  The problem is that poly(x, 2) depends on the 
possible set of x values, and so needs to know all of them, unlike 
e.g. log(x) which is observation-by-observation.  Silently omitting 
missing values is not a good idea in such cases, especially if the values 
are missing in other variables (which is what na.action is likely to do).

I would say models with poly, ns, bs etc are inadvisable in the presence 
of missing values in their argument.  We could make poly() give an 
informative message, though.

Brian


On Mon, 14 Feb 2005, Liaw, Andy wrote:

> This smells like a bug to me.  The error is triggered by the line:
>
>   variables <- eval(predvars, data, env)
>
> inside model.frame.default().  At that point, na.action has not been
> applied, so poly() ended being called on data that still contains missing
> values.  The qr() that issued the error is for generating the orthogonal
> basis when evaluating poly(), not for fitting the linear model itself.
>
> Essentially, calling
>
>  model.frame(y ~ poly(x, 2), data=data.frame(x=c(NA, 1:3), y=rnorm(4)),
>              na.action=na.omit)
>
> would show the same error.  The obvious workaround is to omit cases with NAs
> before calling lm().
>
> Andy
>
>> From: Markus J?ntti
>>
>> I ran into a to me surprising result on running lm with an orthogonal
>> polynomial among the predictors.
>>
>> The lm command resulted in
>>
>> Error in qr(X) : NA/NaN/Inf in foreign function call (arg 1)
>> Error during wrapup:
>>
>> despite my using a "subset" in the call to get rid of NA's.
>>
>> poly is apparently evaluated before any NA's are subsetted out
>> of the data.
>>
>> Example code (attached to this e-mail as as a script):
>> > ## generate some data
>> > n <- 50
>> > x <- runif(n)
>> > a0 <- 10
>> > a1 <- .5
>> > sigma.e <- 1
>> > y <- a0 + a1*x + rnorm(n)*sigma.e
>> > tmp.d <- data.frame(y, x)
>> > rm(list=c("n", "x", "a0", "a1", "sigma.e", "y"))
>> >
>> > print(lm.1 <- lm(y ~ poly(x, 2), data = tmp.d)
>> +
>> + ## now make a few NA's
>> +
>> + tmp.d$x[1:2] <- rep(NA, 2)
>> Error: syntax error
>> Error during wrapup:
>> >
>> > ## this fails, just as it should
>> > print(lm.1 <- lm(y ~ poly(x, 2), data = tmp.d))
>>
>> Call:
>> lm(formula = y ~ poly(x, 2), data = tmp.d)
>>
>> Coefficients:
>> (Intercept)  poly(x, 2)1  poly(x, 2)2
>>       10.380       -0.242       -1.441
>>
>> >
>> > ## these also fail, but should not?
>> >
>> > print(lm.2 <- lm(y ~ poly(x, 2), data = tmp.d, subset = !is.na(x)))
>>
>> Call:
>> lm(formula = y ~ poly(x, 2), data = tmp.d, subset = !is.na(x))
>>
>> Coefficients:
>> (Intercept)  poly(x, 2)1  poly(x, 2)2
>>       10.380       -0.242       -1.441
>>
>> > print(lm.3 <- lm(y ~ poly(x, 2), data = tmp.d, na.action =
>> na.omit))
>>
>> Call:
>> lm(formula = y ~ poly(x, 2), data = tmp.d, na.action = na.omit)
>>
>> Coefficients:
>> (Intercept)  poly(x, 2)1  poly(x, 2)2
>>       10.380       -0.242       -1.441
>>
>> >
>> > ## but this works
>> >
>> > print(lm.3 <- lm(y ~ poly(x, 2), data = subset(tmp.d, subset =
>> !is.na(x))))
>>
>> Call:
>> lm(formula = y ~ poly(x, 2), data = subset(tmp.d, subset = !is.na(x)))
>>
>> Coefficients:
>> (Intercept)  poly(x, 2)1  poly(x, 2)2
>>       10.380       -0.242       -1.441
>>
>> --------------------
>>
>> The documentation of lm is *not* misleading at this point, saying that
>>
>> subset 	an optional vector specifying a subset of
>> observations to be
>> used in the fitting process.
>>
>> which implies that data are subsetted once lm.fit is called.
>> All the same, this behavior is a little unexpected to me.
>> Is it to be considered a feature, that is, does it produce beneficial
>> side effects which explain why it works as it does?
>>
>> Regards,
>>
>> Markus
>>
>> I am running R on a Debian testing system with kernel 2.6.10 and
>>
>> > version
>>           _
>> platform i386-pc-linux-gnu
>> arch     i386
>> os       linux-gnu
>> system   i386, linux-gnu
>> status
>> major    2
>> minor    0.1
>> year     2004
>> month    11
>> day      15
>> language R
>> --
>> Markus Jantti
>> Abo Akademi University
>> markus.jantti at iki.fi
>> http://www.iki.fi/~mjantti
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From mjf at ansto.gov.au  Tue Feb 15 08:23:01 2005
From: mjf at ansto.gov.au (FISCHER, Matthew)
Date: Tue, 15 Feb 2005 18:23:01 +1100
Subject: [R] special symobol / character
Message-ID: <283982AD9F3CD211B3AC00A0C983032F0F8F3F5B@paradise.ansto.gov.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050215/b7621749/attachment.pl

From Torsten.Hothorn at rzmail.uni-erlangen.de  Tue Feb 15 08:29:02 2005
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Tue, 15 Feb 2005 08:29:02 +0100 (CET)
Subject: [R] R News: Call for Papers
Message-ID: <Pine.LNX.4.51.0502150828070.28678@artemis.imbe.med.uni-erlangen.de>


Dear useRs and developeRs,

the next issue of `R News' is scheduled for the beginning of May
and we are now  accepting submissions for this first issue in 2005.
For more information see

      http://cran.r-project.org/doc/Rnews/

If you are the author of a package on CRAN and you would like to promote
it a little bit, or if you simply have an interesting application using
R, we hope you can find some time to write a short article on it. We
suggest that it be approximately 3 pages or less. The idea of the
newsletter is that it be interesting to R users without being too
technical. For example an article describing a package could begin by
briefly outlining the statistical background and go on to demonstrate
the usage on some typical data set. Of course graphics are more than
welcome!

Bill Venables <Bill.Venables at csiro.au> is also encouraging submissions
to the more specialist Programmer's Niche column. In this case the
technical level could be a little higher, of course, but not necessarily:
ingeniousness is the key.

The R Help Desk column is intended to present answers to frequently
asked questions as well as tricks that are useful to the majority of
useRs. Please send submissions to Uwe Ligges <Uwe.Ligges at R-project.org>.

The deadline for submissions is

        April, 10th, 2005

Keep the contributions rolling in!

The Editorial Board,

Doug Bates, Paul Murrell and Torsten Hothorn



From gregor.gorjanc at bfro.uni-lj.si  Tue Feb 15 08:45:21 2005
From: gregor.gorjanc at bfro.uni-lj.si (Gregor GORJANC)
Date: Tue, 15 Feb 2005 08:45:21 +0100
Subject: [R] Re: [Rd] corrupt data frame: columns will be truncated or
	padded with NAs in: format.data.frame(x, digits = digits)
In-Reply-To: <OFAB72D683.605397CC-ON85256FA8.0052AA3B-85256FA8.0052CBCB@nd.convergys.com>
References: <OFAB72D683.605397CC-ON85256FA8.0052AA3B-85256FA8.0052CBCB@nd.convergys.com>
Message-ID: <4211A891.5040500@bfro.uni-lj.si>

James, thanks for the response.

I understand now my puzzle.

tmp <- data.frame(y1=1:4, f1=factor(c("A", "B", "C", "D")))
tmp$y2 <- NA
tmp[1:2, "y2"] <- 2

Does the job. I see that I should add a full column. In my case adding full 
column of NAs and then adding values, solves my problems.

Thanks to all.

james.holtman at convergys.com wrote:
> 
> 
> 
> By definition, a data.frame has to have all columns of the same length
> (think of it as a matrix).  You can not add partial data to a column.  What
> is it you are trying to do?  Should you consider the use of a 'list' if you
> want each 'column' (data element) to have a different mode and length?
> __________________________________________________________
> James Holtman        "What is the problem you are trying to solve?"
> Executive Technical Consultant  --  Office of Technology, Convergys
> james.holtman at convergys.com
> +1 (513) 723-2929
> 
> 
>                                                                                                                                            
>                       Gregor GORJANC                                                                                                       
>                       <gregor.gorjanc at bfro.        To:       Prof Brian Ripley <ripley at stats.ox.ac.uk>, r-devel at stat.math.ethz.ch,         
>                       uni-lj.si>                    r-help at stat.math.ethz.ch                                                               
>                       Sent by:                     cc:                                                                                     
>                       r-help-bounces at stat.m        Subject:  [R] Re: [Rd] corrupt data frame: columns will be truncated or padded with NAs 
>                       ath.ethz.ch                   in: format.data.frame(x, digits = digits)                                              
>                                                                                                                                            
>                                                                                                                                            
>                       02/14/2005 07:32                                                                                                     
>                       Please respond to                                                                                                    
>                       gregor.gorjanc                                                                                                       
>                                                                                                                                            
> 
> 
> 
> 
> Hello!
> 
> Sending this also to r-help so anyone can read it also there and maybe also
> 
> help me with my puzzle if this trivial and I don't see it.
> 
> Prof Brian Ripley wrote:
> [... removed some ...]
> 
>>You add a column, not replace part of a non-existent column.  Isn't that
>>obvious, given what you wrote?
> 
> 
> # OK. If I do
> tmp <- data.frame(y1=1:4, f1=factor(c("A", "B", "C", "D")))
> tmp[1:2, "y2"] <- 2
> tmp
> # I am changing nonexistent column y2 in data frame tmp.
> 
> # If I do
> tmp <- data.frame(y1=1:4, f1=factor(c("A", "B", "C", "D")))
> tmp$y2 <- NA
> tmp[1:2, "y2"] <- 2
> tmp
> # I am changing existent column. I understand now the difference. However,
> # it is weird for me that this is OK (if column y2 does not yet exist)
> tmp["y2"] <- 2
> # but this is not
> tmp[1:2, "y2"] <- 2
> 
> 
>>There is a lot of basic documentation on data manipulation in R/S, and a
>>whole chapter in MASS4.  Somehow most other people don't seem to find
>>this a problem.
> 
> 
> I just ordered MASS4 last week and I am eager to get it in my hands. In
> meanwhile I read quite some documentation and what I more or less saw is
> 
> tmp <- data.frame(y1=1:4, f1=factor(c("A", "B", "C", "D")))
> tmp$y2 <- 1:4
> tmp$y3 <- 2*tmp$y1
> ...
> ...
> 
> i.e. everybody is adding full column to data frame. But I would like to add
> 
> just one part.
> 
> --
> Lep pozdrav / With regards,
>      Gregor GORJANC
> 
> ---------------------------------------------------------------
> University of Ljubljana
> Biotechnical Faculty       URI: http://www.bfro.uni-lj.si
> Zootechnical Department    mail: gregor.gorjanc <at> bfro.uni-lj.si
> Groblje 3                  tel: +386 (0)1 72 17 861
> SI-1230 Domzale            fax: +386 (0)1 72 17 888
> Slovenia
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 
> 

-- 
Lep pozdrav / With regards,
     Gregor GORJANC

---------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty       URI: http://www.bfro.uni-lj.si
Zootechnical Department    mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                  tel: +386 (0)1 72 17 861
SI-1230 Domzale            fax: +386 (0)1 72 17 888
Slovenia



From lecoutre at stat.ucl.ac.be  Tue Feb 15 08:53:51 2005
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Tue, 15 Feb 2005 08:53:51 +0100
Subject: [R] special symobol / character
In-Reply-To: <283982AD9F3CD211B3AC00A0C983032F0F8F3F5B@paradise.ansto.go v.au>
References: <283982AD9F3CD211B3AC00A0C983032F0F8F3F5B@paradise.ansto.gov.au>
Message-ID: <6.0.1.1.2.20050215085048.02bfbec0@stat4ux.stat.ucl.ac.be>


Hi Matthew,

Most systems allow to enter any ASCII (or extended ASCII) character 
directly using a key combination.
Accessing ANSI charcaters under Windows is possible with:
ALT+0xxx (press ALT, hold it down, press 0 and the number of the character, 
release ALT)
Thus: ALT+0137 makes: ?
The future seems promising with the Unicode support: kudo R core team!

Eric


At 08:23 15/02/2005, FISCHER, Matthew wrote:

>Hi all,
>
>     Is it possible to add a permil (or per mille) symbol to
>an R plot (I couldn't find this symbol under demo(Hershey) or
>the plotmath information).
>
>In some ascii tables it is symbol no. 137.
>
>cheers,
>
>Matt.
>
>         [[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Eric Lecoutre
UCL /  Institut de Statistique
Voie du Roman Pays, 20
1348 Louvain-la-Neuve
Belgium

tel: (+32)(0)10473050
lecoutre at stat.ucl.ac.be
http://www.stat.ucl.ac.be/ISpersonnel/lecoutre

If the statistics are boring, then you've got the wrong numbers. -Edward 
Tufte



From mjf at ansto.gov.au  Tue Feb 15 09:03:47 2005
From: mjf at ansto.gov.au (FISCHER, Matthew)
Date: Tue, 15 Feb 2005 19:03:47 +1100
Subject: [R] special symobol / character
Message-ID: <283982AD9F3CD211B3AC00A0C983032F0F8F3F5C@paradise.ansto.gov.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050215/e56cea92/attachment.pl

From nassar at noos.fr  Tue Feb 15 09:24:00 2005
From: nassar at noos.fr (Naji)
Date: Tue, 15 Feb 2005 09:24:00 +0100
Subject: Off topic -- large data sets. Was RE: [R] 64 Bit R Background
	Question
In-Reply-To: <200502141741.j1EHfPCk017510@volta.gene.com>
Message-ID: <BE377030.1271%nassar@noos.fr>


Hi,


Also I agree those cases are relatively rare in STATISTICAL analysis, you
can encounter them for simulation topics (natural catalysm a 5 meter in the
topographics can change all the simulations)
Two ideas (in addition to loading several sections) is
1- to search for duplicate cases and estimate your model upon a frequency
weighted shema, perhaps you don't have 200millions different 'cases'
2- take into account your data and model the used algorythm
precision/accuracy, (i.e. No need to take into account 1million case, a
precision close to .001, if the gradient, or any other function used, has a
.01 accuracy) ...

Best regards
Naji

Le 14/02/05 18:41, ? Berton Gunter ? <gunter.berton at gene.com> a ?crit :

> 
>>> read all 200 million rows a pipe dream no matter what
>> platform I'm using?
>> 
>> In principle R can handle this with enough memory. However,
>> 200 million 
>> rows and three columns is 4.8Gb of storage, and R usually needs a few
>> times the size of the data for working space.
>> 
>> You would likely be better off not reading the whole data set
>> at once, but 
>> loading sections of it from Oracle as needed.
>> 
>> 
>> -thomas
>> 
> 
> Thomas's comment raises a question:
> 
> Can comeone give me an example (perhaps in a private response, since I'm off
> topic here) where one actually needs all cases in a large data set ("large"
> being > 1e6, say) to do a STATISTICAL analysis? By "statistical" I exclude,
> say searching for some particular characteristic like an adverse event in a
> medical or customer repair database, etc. Maybe a definition of
> "statistical" is: anything that cannot be routinely done in a single pass
> database query.
> 
> The reason I ask this is that it seems to me that with millions of cases,
> (careful, perhaps stratified or in some other not completely at random way)
> sampling should always suffice to reduce a dataset to manageable size
> sufficient for the data analysis needs at hand. But my ignorance and naivete
> probably show here.
> 
> Thanks.
> 
> -- Bert
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Tue Feb 15 11:06:41 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 15 Feb 2005 10:06:41 +0000 (GMT)
Subject: [R] special symobol / character
In-Reply-To: <283982AD9F3CD211B3AC00A0C983032F0F8F3F5C@paradise.ansto.gov.au>
References: <283982AD9F3CD211B3AC00A0C983032F0F8F3F5C@paradise.ansto.gov.au>
Message-ID: <Pine.LNX.4.61.0502150954100.20064@gannet.stats>

On Tue, 15 Feb 2005, FISCHER, Matthew wrote:

>    Thanks for your speedy reply,  I should have noted
> that I'm using a Linux machine.  However, when I copy
> the symbol from Windows to Linux (using R/emacs) via an x-win 32 window
> it replaces the per mille symbol with a /211.

\211 I hope.  (That is octal for 137.)

> R then produces the character (not a per mille symbol!)
> that can be found in the equivalent place
> using character.table() in the Hmisc package.  I'd use
> windows, except we have huge output datasets generated
> by a climate model, and its not possible to move it to a machine
> running windows.
>
> Any other suggestions are welcome!,

Note that this is not as you claimed an ASCII symbol and it is not even an 
ISO Latin-1 symbol.  The area 128 to 143 is a control area in almost all 
encodings except WinAnsi, which is what you are using on Windows, I 
believe.

Try `man ascii' on your Linux system.

per mille is not AFAICS in the Adobe symbol font that plotmath uses, and 
so it cannot be added there.  In any case, it is text-like, not a symbol 
(you would want it in the same font as %).

It is the Unicode character U+2030, so you will be able to use it in a 
UTF-8 locale in R-2.1.0 via \u2030.


> -----Original Message-----
> From: Eric Lecoutre [mailto:lecoutre at stat.ucl.ac.be]
> Sent: Tuesday, 15 February 2005 18:54
> To: FISCHER, Matthew; 'r-help at stat.math.ethz.ch'
> Subject: Re: [R] special symobol / character
>
>
>
> Hi Matthew,
>
> Most systems allow to enter any ASCII (or extended ASCII) character
> directly using a key combination.
> Accessing ANSI charcaters under Windows is possible with:
> ALT+0xxx (press ALT, hold it down, press 0 and the number of the character,
> release ALT)
> Thus: ALT+0137 makes: ?
> The future seems promising with the Unicode support: kudo R core team!
>
> Eric
>
>
> At 08:23 15/02/2005, FISCHER, Matthew wrote:
>
>> Hi all,
>>
>>     Is it possible to add a permil (or per mille) symbol to
>> an R plot (I couldn't find this symbol under demo(Hershey) or
>> the plotmath information).
>>
>> In some ascii tables it is symbol no. 137.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mgamiz at goliatb.ugr.es  Tue Feb 15 11:18:39 2005
From: mgamiz at goliatb.ugr.es (Mari Luz Gamiz Perez)
Date: Tue, 15 Feb 2005 11:18:39 +0100
Subject: [R] convolution of functions
Message-ID: <3.0.6.32.20050215111839.00c4abd0@pop.ugr.es>

Dear sir,

we would like to know if there exist  any  R package containing the
computational performance of  the n-fold Stieljes' convolution of functions. 


We look forward to hearing from you.

Thank you in advance.





____________________________________

M.Luz G?miz P?rez
Dpt. Estad?stica e Investigaci?n Operativa
Facultad de Ciencias
Universidad de Granada
Telf.: 958-243156
e-mail: mgamiz at goliat.ugr.es



From tuechler at gmx.at  Tue Feb 15 11:54:59 2005
From: tuechler at gmx.at (Heinz Tuechler)
Date: Tue, 15 Feb 2005 11:54:59 +0100
Subject: [R] update.packages - delete downloaded - N
Message-ID: <3.0.6.32.20050215115459.007ad970@pop.gmx.net>

Hello,

Using Update Packages from CRAN in R Version 2.0.1 Patched (2005-01-15)
under Windows 98, I found it tricky to save the downloaded files.
Even if I answer "N" to the question, if the downloaded files should be
deleted, they are deleted after R is quitted. I understood that I have to
copy them to a different location before quitting R. Is this an intended
behavoiur of R or did I miss some instruction?

Heinz T?chler



From ligges at statistik.uni-dortmund.de  Tue Feb 15 12:15:20 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 15 Feb 2005 12:15:20 +0100
Subject: [R] update.packages - delete downloaded - N
In-Reply-To: <3.0.6.32.20050215115459.007ad970@pop.gmx.net>
References: <3.0.6.32.20050215115459.007ad970@pop.gmx.net>
Message-ID: <4211D9C8.4000602@statistik.uni-dortmund.de>

Heinz Tuechler wrote:

> Hello,
> 
> Using Update Packages from CRAN in R Version 2.0.1 Patched (2005-01-15)
> under Windows 98, I found it tricky to save the downloaded files.
> Even if I answer "N" to the question, if the downloaded files should be
> deleted, they are deleted after R is quitted. I understood that I have to
> copy them to a different location before quitting R. Is this an intended
> behavoiur of R or did I miss some instruction?

Yes, intended, yes, you missed "destdir":

It is intended to save in tempdir() by default.
You can specify argument "destdir", if you want something different.

Uwe Ligges


> Heinz T?chler
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jacques.veslot at cirad.fr  Tue Feb 15 12:35:36 2005
From: jacques.veslot at cirad.fr (Jacques VESLOT)
Date: Tue, 15 Feb 2005 15:35:36 +0400
Subject: [R] Tests on contingency tables
Message-ID: <HHEDKBCGCMDOHEDELFBCEEBOCJAA.jacques.veslot@cirad.fr>

Dear all,

I have a dataset with qualitative variables (factors) and I want to test the
null hypothesis of independance between two variables for each pair by using
appropriate tests on contingency tables.

I first applied chisq.test and obtained dependance in almost all cases with
extremely small p-values and warning messages.

> chisq.test(table(data$ins.f, data$ins.st))$p.val
[1] 4.811263e-100
Warning message:
Chi-squared approximation may be incorrect in: chisq.test(table(data$ins.f,
data$ins.st))

I then turned to Fisher's Exact Test for Count Data, but I got only error
messages such as:

Error in fisher.test(table(data$ins.f, data$ins.st)) :
        FEXACT error 501.
The hash table key cannot be computed because the largest key
is larger than the largest representable int.
The algorithm cannot proceed.
Reduce the workspace size or use another algorithm.

maybe cause the dimensions of contingency tables are too large (?).

> dim(table(data$ins.f, data$ins.st))
[1] 10  8

I then tried likelihood-ratio G-statistic on contingency table (g.stats()
from hierfstat package), as follows:

> g.stats(data.frame(as.numeric(data$ins.f),as.numeric(data$ins.s)))$g.stats
[1] 486.1993

and I replaced in Chi2 distribution function to get p-value:

> 1-pchisq(486.199, df=63)
[1] 0


Is there a better way to perform this or a more appropriate function
dedicated to tests on large-dimensioned contingency tables ?

Thanks in advance,

Jacques VESLOT



From murdoch at stats.uwo.ca  Tue Feb 15 13:02:47 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 15 Feb 2005 12:02:47 +0000
Subject: [R] Problem with "R CMD Rd2dvi": Rd.sty not found
In-Reply-To: <loom.20050210T222445-608@post.gmane.org>
References: <9CC1B717EF3BD511AD98000103D63FC53FA754@us-arl-asg.mail.saic.com>
	<loom.20050210T222445-608@post.gmane.org>
Message-ID: <pg1u0191bs54kumk4mqou3hoklo5de51bu@4ax.com>

On Thu, 10 Feb 2005 21:27:03 +0000 (UTC), Gabor Grothendieck
<ggrothendieck at myway.com> wrote :

>Tuszynski, Jaroslaw W. <JAROSLAW.W.TUSZYNSKI <at> saic.com> writes:
>
>: 
>: Hi,
>: 
>: I run into a problem with "R CMD Rd2dvi" command: it gives me "File `Rd.sty'
>: not found" error (See the output message on the bottom). I get the same
>: error when running "RCMD check". My system is: Windows 2000, R version
>: 2.0.1, MiKTeX version 2.4. I do have a Rd.sty file in R_HOME/share/texm
>: directory.
>: 
 ...

>There is some information on this at:
>
>http://www.murdoch-sutherland.com/Rtools/miktex.html

The information there is slightly out of date:  the last version of
Miktex that I've tried has broken that solution.  I asked the author
about it, and he replied as below.

I think this is pretty poor behaviour on the part of Miktex, but in
other respects I prefer it to fptex.  

For what you're doing, you need to set up your
"$R_HOME/bin/helpprint.bat" file so that it makes use of the TEXINPUTS
environment variable.  You want the pdftex command to have something
like

-include-directory=%TEXINPUTS%

in it and then it will look in the right places for the includes.

Duncan Murdoch


>To: "Duncan Murdoch" <murdoch at stats.uwo.ca>
>Subject: Re: Help with e-tex?
>From: "MiKTeX Support" <support at miktex.org>
>Date: Sun, 6 Feb 2005 22:04:15 +0100
>
>Duncan Murdoch wrote:
>> Unfortunately, these instructions do not seem to work any more with
>> pdfTeX.  Sometime between these two releases:
>>
>> MiKTeX-pdfTeX 2.4.1603 (1.20a-rc4) (MiKTeX 2.4)
>>
>> MiKTeX-pdfTeX 2.4.1817 (1.20b) (MiKTeX 2.4)
>>
>> it seems that support for TEXINPUTS was dropped from pdfTeX.
>
>Sorry, I can confirm that TEXINPUTS isn't supported by newer versions of 
>MiKTeX-pdfTeX. No promise that this will be fixed in the 2.4 branch. Sorry!
>
>Kind regards,
>Christian Schenk



From chrysopa at insecta.ufv.br  Tue Feb 15 13:34:27 2005
From: chrysopa at insecta.ufv.br (Ronaldo Reis-Jr.)
Date: Tue, 15 Feb 2005 10:34:27 -0200
Subject: [R] R + MacOSX + Emacs(XEmacs) + ESS
Message-ID: <200502151034.27771.chrysopa@insecta.ufv.br>

Hi,

I try to use Emacs or XEmacs with R in a MacOS X Panter without X11.

Anybody can make this work?

How?

Thanks
Ronaldo
-- 
 Um mapa-mundi que nao inclua a utopia, nao e digno de 
 consulta, pois deixa de fora as terras a que a Humanidade 
 esta sempre apontando.
  -- Oscar Wilde
--
|>   // | \\   [***********************************]
|   ( ?   ? )  [Ronaldo Reis J?nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36571-000 Vi?osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-2532                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From jarioksa at sun3.oulu.fi  Tue Feb 15 13:58:41 2005
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: Tue, 15 Feb 2005 14:58:41 +0200
Subject: [R] R + MacOSX + Emacs(XEmacs) + ESS
In-Reply-To: <200502151034.27771.chrysopa@insecta.ufv.br>
References: <200502151034.27771.chrysopa@insecta.ufv.br>
Message-ID: <1108472322.6393.36.camel@biol102145.oulu.fi>

On Tue, 2005-02-15 at 10:34 -0200, Ronaldo Reis-Jr. wrote:
> Hi,
> 
> I try to use Emacs or XEmacs with R in a MacOS X Panter without X11.
> 
> Anybody can make this work?

Did you try googling for "macos X emacs"? That's the way you get it. I
have found two different versions, both work graphically without X11.
ESS installs quite smoothly. Depending on your configuration, you may
have to use ESC for "Meta" instead of Alt of some other systems. So
start R in ESS using ESC-R. 

(The emacs that comes with MacOS X also is GNU Emacs, but works only
within terminal window.) 

cheers, jari oksanen
-- 
Jari Oksanen <jarioksa at sun3.oulu.fi>



From tuechler at gmx.at  Tue Feb 15 14:01:30 2005
From: tuechler at gmx.at (Heinz Tuechler)
Date: Tue, 15 Feb 2005 14:01:30 +0100
Subject: [R] update.packages - delete downloaded - N
In-Reply-To: <4211D9C8.4000602@statistik.uni-dortmund.de>
References: <3.0.6.32.20050215115459.007ad970@pop.gmx.net>
	<3.0.6.32.20050215115459.007ad970@pop.gmx.net>
Message-ID: <3.0.6.32.20050215140130.007b3660@pop.gmx.net>

Thank you for the hint regarding the destdir parameter of update.packages.

Heinz T?chler

At 12:15 15.02.2005 +0100, Uwe Ligges wrote:
>Heinz Tuechler wrote:
>
>> Hello,
>> 
>> Using Update Packages from CRAN in R Version 2.0.1 Patched (2005-01-15)
>> under Windows 98, I found it tricky to save the downloaded files.
>> Even if I answer "N" to the question, if the downloaded files should be
>> deleted, they are deleted after R is quitted. I understood that I have to
>> copy them to a different location before quitting R. Is this an intended
>> behavoiur of R or did I miss some instruction?
>
>Yes, intended, yes, you missed "destdir":
>
>It is intended to save in tempdir() by default.
>You can specify argument "destdir", if you want something different.
>
>Uwe Ligges
>
>
>> Heinz T?chler
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>
>



From andy_liaw at merck.com  Tue Feb 15 14:30:45 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 15 Feb 2005 08:30:45 -0500
Subject: [R] using poly in a linear regression in the presence of NA
	f ails (despite subsetting them out)
Message-ID: <3A822319EB35174CA3714066D590DCD50994E6E7@usrymx25.merck.com>

My apologies:  It's another case of me not thinking statistically...  It may
also help those of us whose brains run at slow clock speeds to have ?poly,
?bs and ?ns mention how they react to NAs.

Best,
Andy


> From: Prof Brian Ripley 
> 
> Andy,
> 
> I don't think it is a bug.  The problem is that poly(x, 2) 
> depends on the 
> possible set of x values, and so needs to know all of them, unlike 
> e.g. log(x) which is observation-by-observation.  Silently omitting 
> missing values is not a good idea in such cases, especially 
> if the values 
> are missing in other variables (which is what na.action is 
> likely to do).
> 
> I would say models with poly, ns, bs etc are inadvisable in 
> the presence 
> of missing values in their argument.  We could make poly() give an 
> informative message, though.
> 
> Brian
> 
> 
> On Mon, 14 Feb 2005, Liaw, Andy wrote:
> 
> > This smells like a bug to me.  The error is triggered by the line:
> >
> >   variables <- eval(predvars, data, env)
> >
> > inside model.frame.default().  At that point, na.action has not been
> > applied, so poly() ended being called on data that still 
> contains missing
> > values.  The qr() that issued the error is for generating 
> the orthogonal
> > basis when evaluating poly(), not for fitting the linear 
> model itself.
> >
> > Essentially, calling
> >
> >  model.frame(y ~ poly(x, 2), data=data.frame(x=c(NA, 1:3), 
> y=rnorm(4)),
> >              na.action=na.omit)
> >
> > would show the same error.  The obvious workaround is to 
> omit cases with NAs
> > before calling lm().
> >
> > Andy
> >
> >> From: Markus J?ntti
> >>
> >> I ran into a to me surprising result on running lm with an 
> orthogonal
> >> polynomial among the predictors.
> >>
> >> The lm command resulted in
> >>
> >> Error in qr(X) : NA/NaN/Inf in foreign function call (arg 1)
> >> Error during wrapup:
> >>
> >> despite my using a "subset" in the call to get rid of NA's.
> >>
> >> poly is apparently evaluated before any NA's are subsetted out
> >> of the data.
> >>
> >> Example code (attached to this e-mail as as a script):
> >> > ## generate some data
> >> > n <- 50
> >> > x <- runif(n)
> >> > a0 <- 10
> >> > a1 <- .5
> >> > sigma.e <- 1
> >> > y <- a0 + a1*x + rnorm(n)*sigma.e
> >> > tmp.d <- data.frame(y, x)
> >> > rm(list=c("n", "x", "a0", "a1", "sigma.e", "y"))
> >> >
> >> > print(lm.1 <- lm(y ~ poly(x, 2), data = tmp.d)
> >> +
> >> + ## now make a few NA's
> >> +
> >> + tmp.d$x[1:2] <- rep(NA, 2)
> >> Error: syntax error
> >> Error during wrapup:
> >> >
> >> > ## this fails, just as it should
> >> > print(lm.1 <- lm(y ~ poly(x, 2), data = tmp.d))
> >>
> >> Call:
> >> lm(formula = y ~ poly(x, 2), data = tmp.d)
> >>
> >> Coefficients:
> >> (Intercept)  poly(x, 2)1  poly(x, 2)2
> >>       10.380       -0.242       -1.441
> >>
> >> >
> >> > ## these also fail, but should not?
> >> >
> >> > print(lm.2 <- lm(y ~ poly(x, 2), data = tmp.d, subset = 
> !is.na(x)))
> >>
> >> Call:
> >> lm(formula = y ~ poly(x, 2), data = tmp.d, subset = !is.na(x))
> >>
> >> Coefficients:
> >> (Intercept)  poly(x, 2)1  poly(x, 2)2
> >>       10.380       -0.242       -1.441
> >>
> >> > print(lm.3 <- lm(y ~ poly(x, 2), data = tmp.d, na.action =
> >> na.omit))
> >>
> >> Call:
> >> lm(formula = y ~ poly(x, 2), data = tmp.d, na.action = na.omit)
> >>
> >> Coefficients:
> >> (Intercept)  poly(x, 2)1  poly(x, 2)2
> >>       10.380       -0.242       -1.441
> >>
> >> >
> >> > ## but this works
> >> >
> >> > print(lm.3 <- lm(y ~ poly(x, 2), data = subset(tmp.d, subset =
> >> !is.na(x))))
> >>
> >> Call:
> >> lm(formula = y ~ poly(x, 2), data = subset(tmp.d, subset = 
> !is.na(x)))
> >>
> >> Coefficients:
> >> (Intercept)  poly(x, 2)1  poly(x, 2)2
> >>       10.380       -0.242       -1.441
> >>
> >> --------------------
> >>
> >> The documentation of lm is *not* misleading at this point, 
> saying that
> >>
> >> subset 	an optional vector specifying a subset of
> >> observations to be
> >> used in the fitting process.
> >>
> >> which implies that data are subsetted once lm.fit is called.
> >> All the same, this behavior is a little unexpected to me.
> >> Is it to be considered a feature, that is, does it produce 
> beneficial
> >> side effects which explain why it works as it does?
> >>
> >> Regards,
> >>
> >> Markus
> >>
> >> I am running R on a Debian testing system with kernel 2.6.10 and
> >>
> >> > version
> >>           _
> >> platform i386-pc-linux-gnu
> >> arch     i386
> >> os       linux-gnu
> >> system   i386, linux-gnu
> >> status
> >> major    2
> >> minor    0.1
> >> year     2004
> >> month    11
> >> day      15
> >> language R
> >> --
> >> Markus Jantti
> >> Abo Akademi University
> >> markus.jantti at iki.fi
> >> http://www.iki.fi/~mjantti
> >>
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> >
> 
> -- 
> Brian D. 
> Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From ripley at stats.ox.ac.uk  Tue Feb 15 14:40:57 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 15 Feb 2005 13:40:57 +0000 (GMT)
Subject: [R] Tests on contingency tables
In-Reply-To: <HHEDKBCGCMDOHEDELFBCEEBOCJAA.jacques.veslot@cirad.fr>
References: <HHEDKBCGCMDOHEDELFBCEEBOCJAA.jacques.veslot@cirad.fr>
Message-ID: <Pine.LNX.4.61.0502151327001.25989@gannet.stats>

You can test independence via a log-linear model.  More importantly, you 
can model that dependence and learn something useful about the data.

I don't see your point here: the two factors are clearly highly dependent: 
who cares what the exact p value is?   Did you do e.g. a mosaicplot as I 
suspect the dependence is obvious in any reasonable plot?

On Tue, 15 Feb 2005, Jacques VESLOT wrote:

> Dear all,
>
> I have a dataset with qualitative variables (factors) and I want to test the
> null hypothesis of independance between two variables for each pair by using
> appropriate tests on contingency tables.
>
> I first applied chisq.test and obtained dependance in almost all cases with
> extremely small p-values and warning messages.
>
>> chisq.test(table(data$ins.f, data$ins.st))$p.val
> [1] 4.811263e-100
> Warning message:
> Chi-squared approximation may be incorrect in: chisq.test(table(data$ins.f,
> data$ins.st))
>
> I then turned to Fisher's Exact Test for Count Data, but I got only error
> messages such as:
>
> Error in fisher.test(table(data$ins.f, data$ins.st)) :
>        FEXACT error 501.
> The hash table key cannot be computed because the largest key
> is larger than the largest representable int.
> The algorithm cannot proceed.
> Reduce the workspace size or use another algorithm.
>
> maybe cause the dimensions of contingency tables are too large (?).

The help file does says

      Note this fails (with an error message) when the entries of the table
      are too large.

Note, the _entries_, not the dimensions.  The issue is how many tables 
need to be enumerated.

>> dim(table(data$ins.f, data$ins.st))
> [1] 10  8
>
> I then tried likelihood-ratio G-statistic on contingency table (g.stats()
> from hierfstat package), as follows:
>
>> g.stats(data.frame(as.numeric(data$ins.f),as.numeric(data$ins.s)))$g.stats
> [1] 486.1993
>
> and I replaced in Chi2 distribution function to get p-value:
>
>> 1-pchisq(486.199, df=63)
> [1] 0
>
>
> Is there a better way to perform this or a more appropriate function
> dedicated to tests on large-dimensioned contingency tables ?


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From cg.pettersson at evp.slu.se  Tue Feb 15 15:15:33 2005
From: cg.pettersson at evp.slu.se (CG Pettersson)
Date: Tue, 15 Feb 2005 15:15:33 +0100
Subject: [R] Correct effect plots from lme() objects
Message-ID: <42120405.8070906@evp.slu.se>

Hello all!

R2.0.1, W2k

I posted this question to the list last Sunday without getting any 
replies on the list. I got two off the list though, suggesting me to 
plot "manually" as a second step, from estimable() or intervals() 
objects respectively. As this was not really what I wished for, I take 
the risk to upset somebody with a trivial question, and re-post it (just 
a little edited).

So, here it is again:

I use lme() from nlme to make mean estimates from series of field
experiments where not all treatments (like "variety") are present in
all experiments. (An old problem in variety evaluation).

A typical call is:

yield.lme <- lme(Yield ~ Variety, data = data,
                             na.action = na.omit,
                             random = ~ 1 | Experiment/Block)

This works well, even when observations are lacking. I have checked
against the accepted method for doing this in Sweden, which is
PROC MIXED in SAS, and the fitted fixed effects are more or less
identical. I use estimable() from gmodels (gregmisc package) to
extract estimates, standard errors and such. I use matrices with the
variety names as row names, it works smooth.

What I am unable to, as yet, is to make nice plots of the estimates
for a given set of varieties. To use only the fixed call directly on
the dataset works for many plooting functions, but produces the wrong 
graphs, as the structure is not used. The structure (the random call)
has to be used, as there are NA:s in the dataset.

Pinheiro & Bates have a lot of graphics on lme objects, but they try
to illustrate more sophisticated relations than my need. I?ve looked
through gplots and the graphic parts of nlme without any hits.
Probably, my difficulties are just due to my own lack of skill. Some 
standard plotting facility plotting directly from the lme object ought 
to work, but I don?t understand how.

How should I do this?

Cheers
/CG
-- 
CG Pettersson MSci. PhD.Stud.
Swedish University of Agricultural Sciences (SLU)
Dep. of Ecology and Crop production sciences (EVP).
http://www.slu.se/
cg.pettersson at evp.slu.se



From nassar at noos.fr  Tue Feb 15 15:05:31 2005
From: nassar at noos.fr (Naji)
Date: Tue, 15 Feb 2005 15:05:31 +0100
Subject: [R] R,  MacOSX G5 bi-proc
In-Reply-To: <1108472322.6393.36.camel@biol102145.oulu.fi>
Message-ID: <BE37C03B.1289%nassar@noos.fr>

Hi all,


I've downloaded binary files from CRAN to install R and it's working quite
fine..I'm wandering if I can take advantage of two processors from the Mac.
Where can I find lecture/help for this?


Thanks in advance
Naji



From ripley at stats.ox.ac.uk  Tue Feb 15 15:08:55 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 15 Feb 2005 14:08:55 +0000 (GMT)
Subject: [R] Tests on contingency tables
In-Reply-To: <Pine.LNX.4.61.0502151327001.25989@gannet.stats>
References: <HHEDKBCGCMDOHEDELFBCEEBOCJAA.jacques.veslot@cirad.fr>
	<Pine.LNX.4.61.0502151327001.25989@gannet.stats>
Message-ID: <Pine.LNX.4.61.0502151359001.26438@gannet.stats>

I forgot to mention a crucial statistical point:

 	As you are doing this pairwise, remember Simpson's paradox.

or if you don't know about it, Google it (it is not really due to 
Simpson, an instance of Stigler's Law of Eponymy).

On Tue, 15 Feb 2005, Prof Brian Ripley wrote:

> You can test independence via a log-linear model.  More importantly, you can 
> model that dependence and learn something useful about the data.
>
> I don't see your point here: the two factors are clearly highly dependent: 
> who cares what the exact p value is?   Did you do e.g. a mosaicplot as I 
> suspect the dependence is obvious in any reasonable plot?
>
> On Tue, 15 Feb 2005, Jacques VESLOT wrote:
>
>> Dear all,
>> 
>> I have a dataset with qualitative variables (factors) and I want to test 
>> the
>> null hypothesis of independance between two variables for each pair by 
>> using
>> appropriate tests on contingency tables.
>> 
>> I first applied chisq.test and obtained dependance in almost all cases with
>> extremely small p-values and warning messages.
>> 
>>> chisq.test(table(data$ins.f, data$ins.st))$p.val
>> [1] 4.811263e-100
>> Warning message:
>> Chi-squared approximation may be incorrect in: chisq.test(table(data$ins.f,
>> data$ins.st))
>> 
>> I then turned to Fisher's Exact Test for Count Data, but I got only error
>> messages such as:
>> 
>> Error in fisher.test(table(data$ins.f, data$ins.st)) :
>>        FEXACT error 501.
>> The hash table key cannot be computed because the largest key
>> is larger than the largest representable int.
>> The algorithm cannot proceed.
>> Reduce the workspace size or use another algorithm.
>> 
>> maybe cause the dimensions of contingency tables are too large (?).
>
> The help file does says
>
>     Note this fails (with an error message) when the entries of the table
>     are too large.
>
> Note, the _entries_, not the dimensions.  The issue is how many tables need 
> to be enumerated.
>
>>> dim(table(data$ins.f, data$ins.st))
>> [1] 10  8
>> 
>> I then tried likelihood-ratio G-statistic on contingency table (g.stats()
>> from hierfstat package), as follows:
>> 
>>> g.stats(data.frame(as.numeric(data$ins.f),as.numeric(data$ins.s)))$g.stats
>> [1] 486.1993
>> 
>> and I replaced in Chi2 distribution function to get p-value:
>> 
>>> 1-pchisq(486.199, df=63)
>> [1] 0
>> 
>> 
>> Is there a better way to perform this or a more appropriate function
>> dedicated to tests on large-dimensioned contingency tables ?
>
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andy_liaw at merck.com  Tue Feb 15 15:13:10 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 15 Feb 2005 09:13:10 -0500
Subject: [R] R,  MacOSX G5 bi-proc
Message-ID: <3A822319EB35174CA3714066D590DCD50994E6EB@usrymx25.merck.com>

You can use the snow package on CRAN.  I don't know if RPVM/RMPI work on the
Macs, but they are also on CRAN.

Andy

> From: Naji
> 
> Hi all,
> 
> 
> I've downloaded binary files from CRAN to install R and it's 
> working quite
> fine..I'm wandering if I can take advantage of two processors 
> from the Mac.
> Where can I find lecture/help for this?
> 
> 
> Thanks in advance
> Naji
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From Delphine.Gille at eleves.polytech-lille.fr  Tue Feb 15 15:23:08 2005
From: Delphine.Gille at eleves.polytech-lille.fr (Delphine.Gille@eleves.polytech-lille.fr)
Date: Tue, 15 Feb 2005 15:23:08 +0100
Subject: [R] memory problem with package mix
Message-ID: <1108477388.421205cc52325@webmail.polytech-lille.fr>


Hello,

I think we have a memory problem with  em.mix.

We have done:

>library(mix)
>Manq <- read.table("C:/.../file.txt")
>attach(Manq)
>Manq
>    V1 V2 V3 V4 .............V27
> 1  1  1  1  1...........
> 2  1 NA  3  6
> 3  1  2  6  2
> ...
> ...
> 300  2  NA  6  2...........

> Essaimanq <-prelim.mix(as.matrix(Manq),5)
> test <- em.mix(Essaimanq)
         error cannot allocated vector of size 535808 KB
           in addition : warning message reached total allocation of 509MB

Thank you



From christoph.lehmann at gmx.ch  Tue Feb 15 15:45:29 2005
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Tue, 15 Feb 2005 15:45:29 +0100
Subject: [R] matlab norm(h) command in R: sqrt(sum(h^2)) - use in an
	expression
Message-ID: <42120B09.2040109@gmx.ch>

Hi

in matlab I defined a function (double gamma, parameters at the end of 
this mail) as
   h(i)=((t/d1)^a1)*exp(-(t-d1)/b1)-c*((t/d2)^a2)*exp(-(t-d2)/b2);
   h=h/norm(h);

I do know that norm() in matlab is equal to:

   sqrt(sum(x^2))
in R
so in R I do it like:

#function (double gamama)
h <- expression((t/d1)^a1*exp(-(t-d1)/b1)-c*(t/d2)^a2*exp(-(t-d2)/b2))

# plot it
t <- seq(0, 20000, by = 100)
t <- t/1000
plot(eval(h), type = 'l')

# however this yields an error
h <- h/sqrt(sum(h^2))
Error in h^2 : non-numeric argument to binary operator

what shall I do to get the matlab: h = h/norm(h) implemented in R?

thanks for a hint

christoph


----
# parameters
peak1 <- 5.4
fwhm1 <- 5.2
peak2 <- 10.8
fwhm2 <- 7.35
dip <- 0.35

b1 <- 0.9 # dispersion
b2 <- 0.9 #dispersion
a1 <- peak1/b1
a2 <- peak2/b2
d1 <- a1*b1
d2 <- a2*b2
c <- dip



From andy_liaw at merck.com  Tue Feb 15 15:56:08 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 15 Feb 2005 09:56:08 -0500
Subject: [R] matlab norm(h) command in R: sqrt(sum(h^2)) - use in an
	e xpression
Message-ID: <3A822319EB35174CA3714066D590DCD50994E6F0@usrymx25.merck.com>

> From: Christoph Lehmann
> 
> Hi
> 
> in matlab I defined a function (double gamma, parameters at 
> the end of 
> this mail) as
>    h(i)=((t/d1)^a1)*exp(-(t-d1)/b1)-c*((t/d2)^a2)*exp(-(t-d2)/b2);
>    h=h/norm(h);
> 
> I do know that norm() in matlab is equal to:
> 
>    sqrt(sum(x^2))
> in R
> so in R I do it like:
> 
> #function (double gamama)
> h <- expression((t/d1)^a1*exp(-(t-d1)/b1)-c*(t/d2)^a2*exp(-(t-d2)/b2))
> 
> # plot it
> t <- seq(0, 20000, by = 100)
> t <- t/1000
> plot(eval(h), type = 'l')
> 
> # however this yields an error
> h <- h/sqrt(sum(h^2))
       ^          ^

Those are still expressions, not vectors.  You need to wrap those in eval()
as you did in plot(), or do h <- eval(h) beforehand.

Andy




> Error in h^2 : non-numeric argument to binary operator
> 
> what shall I do to get the matlab: h = h/norm(h) implemented in R?
> 
> thanks for a hint
> 
> christoph
> 
> 
> ----
> # parameters
> peak1 <- 5.4
> fwhm1 <- 5.2
> peak2 <- 10.8
> fwhm2 <- 7.35
> dip <- 0.35
> 
> b1 <- 0.9 # dispersion
> b2 <- 0.9 #dispersion
> a1 <- peak1/b1
> a2 <- peak2/b2
> d1 <- a1*b1
> d2 <- a2*b2
> c <- dip
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From jacques.veslot at cirad.fr  Tue Feb 15 15:57:08 2005
From: jacques.veslot at cirad.fr (Jacques VESLOT)
Date: Tue, 15 Feb 2005 18:57:08 +0400
Subject: [R] Tests on contingency tables
In-Reply-To: <Pine.LNX.4.61.0502151327001.25989@gannet.stats>
Message-ID: <HHEDKBCGCMDOHEDELFBCIECBCJAA.jacques.veslot@cirad.fr>

Thanks a lot for your help !

Right ! According to tables, most factors look indeed highly dependent...
but, because of strange p-values and warning messages, as I tried to test it
with Chisquare test, and because Fisher's Exact Test function doesn't work
on my data, I wondered whether there were other functions to perform such
tests.

I will try with test independence via a log-linear model.
Is this code correct ? (I can't catch exactly how to put 'formula' argument)

> z <- table(data$fac1, data$fac2)
> names(dimnames(z)) <- c("fac1", "fac2")
> fm <- loglm(~ins+pl,z)
> fm
Call:
loglm(formula = ~ins + pl, data = z)

Statistics:
                      X^2 df P(> X^2)
Likelihood Ratio 286.1236 49        0
Pearson          450.5332 49        0


Jacques

-----Message d'origine-----
De : Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
Envoye : mardi 15 fevrier 2005 17:41
A : Jacques VESLOT
Cc : R-HELP; jerome.goudet at unil.ch
Objet : Re: [R] Tests on contingency tables


You can test independence via a log-linear model.  More importantly, you
can model that dependence and learn something useful about the data.

I don't see your point here: the two factors are clearly highly dependent:
who cares what the exact p value is?   Did you do e.g. a mosaicplot as I
suspect the dependence is obvious in any reasonable plot?

On Tue, 15 Feb 2005, Jacques VESLOT wrote:

> Dear all,
>
> I have a dataset with qualitative variables (factors) and I want to test
the
> null hypothesis of independance between two variables for each pair by
using
> appropriate tests on contingency tables.
>
> I first applied chisq.test and obtained dependance in almost all cases
with
> extremely small p-values and warning messages.
>
>> chisq.test(table(data$ins.f, data$ins.st))$p.val
> [1] 4.811263e-100
> Warning message:
> Chi-squared approximation may be incorrect in:
chisq.test(table(data$ins.f,
> data$ins.st))
>
> I then turned to Fisher's Exact Test for Count Data, but I got only error
> messages such as:
>
> Error in fisher.test(table(data$ins.f, data$ins.st)) :
>        FEXACT error 501.
> The hash table key cannot be computed because the largest key
> is larger than the largest representable int.
> The algorithm cannot proceed.
> Reduce the workspace size or use another algorithm.
>
> maybe cause the dimensions of contingency tables are too large (?).

The help file does says

      Note this fails (with an error message) when the entries of the table
      are too large.

Note, the _entries_, not the dimensions.  The issue is how many tables
need to be enumerated.

>> dim(table(data$ins.f, data$ins.st))
> [1] 10  8
>
> I then tried likelihood-ratio G-statistic on contingency table (g.stats()
> from hierfstat package), as follows:
>
>>
g.stats(data.frame(as.numeric(data$ins.f),as.numeric(data$ins.s)))$g.stats
> [1] 486.1993
>
> and I replaced in Chi2 distribution function to get p-value:
>
>> 1-pchisq(486.199, df=63)
> [1] 0
>
>
> Is there a better way to perform this or a more appropriate function
> dedicated to tests on large-dimensioned contingency tables ?


--
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dimitris.rizopoulos at med.kuleuven.ac.be  Tue Feb 15 16:04:19 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Tue, 15 Feb 2005 16:04:19 +0100
Subject: [R] matlab norm(h) command in R: sqrt(sum(h^2)) - use in
	anexpression
References: <42120B09.2040109@gmx.ch>
Message-ID: <005801c5136f$9ffaf990$0540210a@www.domain>

you are trying to square an expression not an number! try this:

h.fun <- function(tt) ((tt/d1)^a1) * exp(-(tt-d1)/b1) - 
cc*((tt/d2)^a2) * exp(-(tt-d2)/b2)

h <- h.fun(tt)
h <- h/sqrt(sum(h*h))

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "Christoph Lehmann" <christoph.lehmann at gmx.ch>
To: <R-help at stat.math.ethz.ch>
Sent: Tuesday, February 15, 2005 3:45 PM
Subject: [R] matlab norm(h) command in R: sqrt(sum(h^2)) - use in 
anexpression


> Hi
>
> in matlab I defined a function (double gamma, parameters at the end 
> of this mail) as
>   h(i)=((t/d1)^a1)*exp(-(t-d1)/b1)-c*((t/d2)^a2)*exp(-(t-d2)/b2);
>   h=h/norm(h);
>
> I do know that norm() in matlab is equal to:
>
>   sqrt(sum(x^2))
> in R
> so in R I do it like:
>
> #function (double gamama)
> h <- 
> expression((t/d1)^a1*exp(-(t-d1)/b1)-c*(t/d2)^a2*exp(-(t-d2)/b2))
>
> # plot it
> t <- seq(0, 20000, by = 100)
> t <- t/1000
> plot(eval(h), type = 'l')
>
> # however this yields an error
> h <- h/sqrt(sum(h^2))
> Error in h^2 : non-numeric argument to binary operator
>
> what shall I do to get the matlab: h = h/norm(h) implemented in R?
>
> thanks for a hint
>
> christoph
>
>
> ----
> # parameters
> peak1 <- 5.4
> fwhm1 <- 5.2
> peak2 <- 10.8
> fwhm2 <- 7.35
> dip <- 0.35
>
> b1 <- 0.9 # dispersion
> b2 <- 0.9 #dispersion
> a1 <- peak1/b1
> a2 <- peak2/b2
> d1 <- a1*b1
> d2 <- a2*b2
> c <- dip
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From christoph.lehmann at gmx.ch  Tue Feb 15 16:33:29 2005
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Tue, 15 Feb 2005 16:33:29 +0100
Subject: [R] D(eval(g))  problem,
 since "Function `eval' is not in the derivatives table"
Message-ID: <42121649.7000804@gmx.ch>

thanks Andy and Dimitris for your reply to my expression/eval - problem
starting with the resulting expression g I need g's derivative as 
expression, but I get: "Function `eval' is not in the derivatives table":

#function (double gamama)
h <- expression((t/d1)^a1*exp(-(t-d1)/b1)-c*(t/d2)^a2*exp(-(t-d2)/b2))
# plot it
t <- seq(0, 20000, by = 100)
t <- t/1000
g <- expression(eval(h)/sqrt(sum(eval(h)^2)))
plot(eval(g), type = 'l')
g.deriv <- D(g, "t")

 > Error in D(g, "t") : Function `eval' is not in the derivatives table

is there any way one can solve this problem?

thanks a lot
christoph



## --
## parameters
peak1 <- 5.4
fwhm1 <- 5.2
peak2 <- 10.8
fwhm2 <- 7.35
dip <- 0.35

b1 <- 0.9 # dispersion
b2 <- 0.9 #dispersion
a1 <- peak1/b1
a2 <- peak2/b2
d1 <- a1*b1
d2 <- a2*b2
c <- dip



From reid_huntsinger at merck.com  Tue Feb 15 16:47:45 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Tue, 15 Feb 2005 10:47:45 -0500
Subject: [R] convolution of functions
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A92DC@uswpmx00.merck.com>

If you mean the convolution 

(f*g)(x) = integral f(x-y)g(y) dy 

for integrable functions f and g on R^n, then I think using the fact that
the Fourier transform of the convolution is the product of the Fourier
transforms of f and g is the most efficient approach, unless f or g have
some special structure.

For this you just need fft() in base R. You do have to do a little
bookkeeping to manage the discretizations.

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Mari Luz Gamiz Perez
Sent: Tuesday, February 15, 2005 5:19 AM
To: r-help at stat.math.ethz.ch
Subject: [R] convolution of functions


Dear sir,

we would like to know if there exist  any  R package containing the
computational performance of  the n-fold Stieljes' convolution of functions.



We look forward to hearing from you.

Thank you in advance.





____________________________________

M.Luz G?miz P?rez
Dpt. Estad?stica e Investigaci?n Operativa
Facultad de Ciencias
Universidad de Granada
Telf.: 958-243156
e-mail: mgamiz at goliat.ugr.es

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From deepayan at stat.wisc.edu  Tue Feb 15 16:56:16 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Tue, 15 Feb 2005 09:56:16 -0600
Subject: [R] Correct effect plots from lme() objects
In-Reply-To: <42120405.8070906@evp.slu.se>
References: <42120405.8070906@evp.slu.se>
Message-ID: <200502150956.16678.deepayan@stat.wisc.edu>

On Tuesday 15 February 2005 08:15, CG Pettersson wrote:
> Hello all!
>
> R2.0.1, W2k
>
> I posted this question to the list last Sunday without getting any
> replies on the list. I got two off the list though, suggesting me to
> plot "manually" as a second step, from estimable() or intervals()
> objects respectively. As this was not really what I wished for, I
> take the risk to upset somebody with a trivial question, and re-post
> it (just a little edited).
>
> So, here it is again:
>
> I use lme() from nlme to make mean estimates from series of field
> experiments where not all treatments (like "variety") are present in
> all experiments. (An old problem in variety evaluation).
>
> A typical call is:
>
> yield.lme <- lme(Yield ~ Variety, data = data,
>                              na.action = na.omit,
>                              random = ~ 1 | Experiment/Block)
>
> This works well, even when observations are lacking. I have checked
> against the accepted method for doing this in Sweden, which is
> PROC MIXED in SAS, and the fitted fixed effects are more or less
> identical. I use estimable() from gmodels (gregmisc package) to
> extract estimates, standard errors and such. I use matrices with the
> variety names as row names, it works smooth.
>
> What I am unable to, as yet, is to make nice plots of the estimates
> for a given set of varieties. To use only the fixed call directly on
> the dataset works for many plooting functions, but produces the wrong
> graphs, as the structure is not used. The structure (the random call)
> has to be used, as there are NA:s in the dataset.

It's not clear to me what you are looking for. In particular, exactly 
what 'estimates' do you want to plot? Could you give us a reproducible 
example (preferably using a dataset from nlme) and describe the plot 
you are looking for in terms of that example?

Deepayan

> Pinheiro & Bates have a lot of graphics on lme objects, but they try
> to illustrate more sophisticated relations than my need. I?ve looked
> through gplots and the graphic parts of nlme without any hits.
> Probably, my difficulties are just due to my own lack of skill. Some
> standard plotting facility plotting directly from the lme object
> ought to work, but I don?t understand how.
>
> How should I do this?
>
> Cheers
> /CG



From bill.shipley at usherbrooke.ca  Tue Feb 15 17:17:54 2005
From: bill.shipley at usherbrooke.ca (Bill Shipley)
Date: Tue, 15 Feb 2005 11:17:54 -0500
Subject: [R] shrinkage estimates in lme
Message-ID: <002701c51379$e744e630$ae1ad284@BIO041>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050215/655655ab/attachment.pl

From deepayan at stat.wisc.edu  Tue Feb 15 17:20:07 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Tue, 15 Feb 2005 10:20:07 -0600
Subject: [R] lattice multiple plots per page
In-Reply-To: <5.2.1.1.2.20050214174512.01ce0e90@wiscmail.wisc.edu>
References: <5.2.1.1.2.20050214174512.01ce0e90@wiscmail.wisc.edu>
Message-ID: <200502151020.07183.deepayan@stat.wisc.edu>

On Monday 14 February 2005 18:12, Jeff Jorgensen wrote:
> Dear R-sters,
>
> I was wondering if anyone has encountered the following issues.  I've
> figured out how to get multiple levelplots [library(lattice)] on a
> single plot.  However, when I add text (adding axis labels for the
> entire four panel plot) the text is missing when I insert the *.eps
> file I've created into my LaTeX document (via MikTeX-WinEdt).  And,
> I've just upgraded to R v2.0.1 from v1.8.1 (Windows), and each
> individual levelplot is smaller compared to the older R release.
>
> Any clues as to 1) how I can recover the lost text, and 

Your use is incorrect, in the sense that the return value of 
grid.locator() is not a valid input of ltext. You could do the 
following to fix this, 

library(grid)
ltext(lapply(grid.locator(), convertX, "native", TRUE),
      lab="x-axis label, where I click",cex=1.5)
ltext(lapply(grid.locator(), convertX, "native", TRUE),
      lab = "y-axis label, where I click",
      cex=1.5, srt=90)

but that doesn't seem to help either. So, there seems to be a bug 
somewhere. However, you presumably want something like 


grid.text(x = .05, y = .5, lab="y-axis label", 
          default.units = "npc", gp = gpar(cex=1.5), 
          rot = 90)
grid.text(x = .5, y = .05, lab="x-axis label", 
          default.units = "npc", gp = gpar(cex=1.5))

for the last two steps, which seems to work. 


> 2) increase the size of each of the levelplots?

You need to change the following settings, in particular the entries 
that end with 'padding' (similarly for 'layout.heights').

> str(trellis.par.get("layout.widths"))
List of 13
 $ left.padding     : num 1
 $ key.left         : num 1
 $ key.ylab.padding : num 1
 $ ylab             : num 1
 $ ylab.axis.padding: num 1
 $ axis.left        : num 1
 $ axis.panel       : num 1
 $ panel            : num 1
 $ between          : num 1
 $ axis.right       : num 1
 $ axis.key.padding : num 1
 $ key.right        : num 1
 $ right.padding    : num 1

Deepayan

>
> Cheers,
>
> Jeff Jorgensen
>
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> Sample code that illustrates what I'm trying to do:
>
> #create a levelplot
> x<-seq(pi/4, 5*pi, length = 100)
> y<-seq(pi/4, 5*pi, length = 100)
> r<-as.vector(sqrt(outer(x^2, y^2, "+")))
> grid<-expand.grid(x=x, y=y)
> grid$z<-cos(r^2) * exp(-r/(pi^3))
> a<-levelplot(z~x*y, grid, cuts = 50, xlab="", ylab=",
>   colorkey = FALSE)
>
> #create the multiple panel plot, here using all the same levelplot
> trellis.par.set(list(background=list(col="white"))) #white background
> #using position to scale the plots up and to the right ~10%
> #to make room for the axis labels
> print(a,position=c(0.1,0.1,1,1),split=c(1,1,2,2),more=T)
> print(a,position=c(0.1,0.1,1,1),split=c(1,2,2,2),more=T)
> print(a,position=c(0.1,0.1,1,1),split=c(2,1,2,2),more=T)
> print(a,position=c(0.1,0.1,1,1),split=c(2,2,2,2),more=F)
> #commands that let you click where you want the labels centered
> ltext(grid::grid.locator(),lab="x-axis label, where I click",cex=1.5)
> ltext(grid::grid.locator(),lab="y-axis label, where I
> click",cex=1.5,srt=90) #save device to an *.eps file, to be called
> later by a \includegraphics command
> dev.copy2eps(file="twobytwoplot.eps")



From Ted.Harding at nessie.mcc.ac.uk  Tue Feb 15 17:24:18 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 15 Feb 2005 16:24:18 -0000 (GMT)
Subject: [R] memory problem with package mix
In-Reply-To: <1108477388.421205cc52325@webmail.polytech-lille.fr>
Message-ID: <XFMail.050215162418.Ted.Harding@nessie.mcc.ac.uk>

On 15-Feb-05 Delphine.Gille at eleves.polytech-lille.fr wrote:
> Hello,
> I think we have a memory problem with  em.mix.
> 
> We have done:
> 
>>library(mix)
>>Manq <- read.table("C:/.../file.txt")
>>attach(Manq)
>>Manq
>>    V1 V2 V3 V4 .............V27
>> 1  1  1  1  1...........
>> 2  1 NA  3  6
>> 3  1  2  6  2
>> ...
>> ...
>> 300  2  NA  6  2...........
> 
>> Essaimanq <-prelim.mix(as.matrix(Manq),5)
>> test <- em.mix(Essaimanq)
>     error cannot allocated vector of size 535808 KB
>     in addition : warning message reached total allocation of 509MB

Hmm.

According to the above, it seems you might have 5 categorical
variables V1...V5 with at least 6 levels, so since your call to
em.mix does not specify any model restriction (for which you
need to call ecm.mix insead) you may have at least 6^5 = 46656
"cells" for the different combinations of levels. This will
require 46655 parameters for the probabilities of these cells.

For each cell, you have a separate vector of means for the
multivariate normal distribution to be fitted to the (27-5)=22
continuous variables. This requires 22*46656 = 1026432 parameters.

Sub-total: 1073087

Then, as a bit of sugar on the cake, you have the 22*21/2 = 121
parameters for the covariance matrix.

Sub-total: 1073208

Since em.mix does quite complicated things, it is perhaps
not surpising that it demands more than 509MB (corresponding
to about 500 bytes per parameter or, with 8 bytes per number,
about 60 numbers per parameter). Not to mention the 8100
numbers (about 65000 bytes) required for each working copy
of the representation of the data.

In any case, apparently you only have 300*27 = 8100 data,
quite inadequate for this unrestricted model!

Even if you could have allocated enough memory, you would
then have found that the EM fit would not get anywhere.

Suggested solution: think about restricting the number of
parameters in the model, using the parameter "margins" to
ecnm.mix to restrict the number of independent combinations
of categorical levels, and also "design" to specify a simpler
model for the dependence of the continuous variables
on the categoricals (e.g. the matrix corresponding to the
model "~ V1+V2+V3+V4+V5" only introduces 5*6*22=660 new
parameters, namely a simple additive effect of level of each Vi
on the mean of each of the 22 continuous variables).

Hoping this helps,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 15-Feb-05                                       Time: 16:24:18
------------------------------ XFMail ------------------------------



From uofiowa at gmail.com  Tue Feb 15 17:59:10 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Tue, 15 Feb 2005 11:59:10 -0500
Subject: [R] how many 7th of the month is there between two dates
Message-ID: <3f87cc6d05021508595a860294@mail.gmail.com>

This is a eaeir way to ask my prior question:

I want to caculate how many an exact day of the month there is between
two dates.

For example; How many 7th of the month is there between "1998/12/17"
and "2000/1/7". To make the problem simple, the day of the month (7)
is the day in the 2nd date.



From fhilal at yahoo.com  Tue Feb 15 18:05:06 2005
From: fhilal at yahoo.com (Fairouz Makhlouf)
Date: Tue, 15 Feb 2005 09:05:06 -0800 (PST)
Subject: [R] reading a 40kb csv file in R
Message-ID: <20050215170506.90075.qmail@web14021.mail.yahoo.com>

I am trying to read a matrix of  5000*2000 from a csv
file of real numbers in R using the scan function. I
am getting an error saying that R can not read a
vector > 32KB and it can't allocate a memory of 191 MB
etc. 

What should I do. I am using R1.7.1.

Thanks

=====
Thanks
Fairouz Makhlouf



From dimitris.rizopoulos at med.kuleuven.ac.be  Tue Feb 15 18:07:19 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Tue, 15 Feb 2005 18:07:19 +0100
Subject: [R] shrinkage estimates in lme
References: <002701c51379$e744e630$ae1ad284@BIO041>
Message-ID: <001f01c51380$ce860b40$0540210a@www.domain>

What exactly do you mean by slope estimates? Marginal or 
subject-specific?

Presuming that you mean the fixed-effects (since you refer to OLS) 
then these are unbiased since they are Weighted Least Squares 
estimators based on the marginal model. They are unbiased even in the 
case where you misspecify the correlation structure. Moreover if you 
use a reasonably well chosen covariance structure then they are also 
very efficient.

The subject-specific fitted values are shrunken toward the mean in the 
sense:

\hat{y}_i = \sum_i V_i^{-1}X_i\hat{\bfbeta} + (I-\sum_i V_i^{-1})y_i

and thus it is a weighted average of the population average profile 
X_i\hat{\bfbeta} and the observed data y_i with weights \sum_i 
V_i^{-1} and (I-\sum_i V_i^{-1}), respectively (where \sum_i V_i^{-1} 
is the residual covariance matrix).

I hope this helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Bill Shipley" <bill.shipley at usherbrooke.ca>
To: "R help list" <r-help at stat.math.ethz.ch>
Sent: Tuesday, February 15, 2005 5:17 PM
Subject: [R] shrinkage estimates in lme


Hello.  Slope estimates in lme are shrinkage estimates which pull the
OLS slope estimates towards the population estimates, the degree of
which depends on the group sample size and the distance between the
group-based estimate and the overall population estimate.  Although
these shrinkage estimates as said to be more precise with respect to 
the
true values, they are also biased.  So there is a tradeoff between
precision and bias.

Are there rules of thumb to help determine when it is better to use 
the
OLS slope estimates and when to use the mixed model (lme) shrinkage
estimates?  I have 35 groups but the numbers per group vary from over 
50
to as low as 4.

Thanks for any help.



Bill Shipley

Subject Matter Editor, Ecology

North American Editor, Annals of Botany

D?partement de biologie, Universit? de Sherbrooke,

Sherbrooke (Qu?bec) J1K 2R1 CANADA

Bill.Shipley at USherbrooke.ca

 <http://callisto.si.usherb.ca:8080/bshipley/>
http://callisto.si.usherb.ca:8080/bshipley/




[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html



From maillists at visiv.co.uk  Tue Feb 15 18:06:06 2005
From: maillists at visiv.co.uk (Graham Jones)
Date: Tue, 15 Feb 2005 17:06:06 +0000
Subject: Off topic -- large data sets. Was RE: [R] 64 Bit R Background
	Question 
In-Reply-To: <200502151112.j1FB5fZ5002722@hypatia.math.ethz.ch>
References: <200502151112.j1FB5fZ5002722@hypatia.math.ethz.ch>
Message-ID: <xhdN1XA+viECFwWl@visiv.co.uk>

In message <200502151112.j1FB5fZ5002722 at hypatia.math.ethz.ch>, r-help-
request at stat.math.ethz.ch writes

>Can comeone give me an example (perhaps in a private response, since I'm off
>topic here) where one actually needs all cases in a large data set ("large"
>being > 1e6, say) to do a STATISTICAL analysis? By "statistical" I exclude,
>say searching for some particular characteristic like an adverse event in a
>medical or customer repair database, etc. Maybe a definition of
>"statistical" is: anything that cannot be routinely done in a single pass
>database query.

If the dimensionality of the data is large, you may need a large number
of cases too. An example from my own experience would be using quadratic
discriminant analysis (with regularization) for classifying symbols for
an OCR program. With 200 classes and 100 features, I'd really like many
millions of cases. I've been using about 20,000 per class or 4 million
in total, but if I had 40 million it would probably work better.
Compared to many applications in pattern recognition and data mining, I
think this is a fairly small example. 

-- 
Graham Jones, author of SharpEye Music Reader
http://www.visiv.co.uk
21e Balnakeil, Durness, Lairg, Sutherland, IV27 4PT, Scotland, UK



From ggrothendieck at myway.com  Tue Feb 15 18:16:40 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 15 Feb 2005 17:16:40 +0000 (UTC)
Subject: [R] how many 7th of the month is there between two dates
References: <3f87cc6d05021508595a860294@mail.gmail.com>
Message-ID: <loom.20050215T181524-337@post.gmane.org>

Omar Lakkis <uofiowa <at> gmail.com> writes:


: I want to caculate how many an exact day of the month there is between
: two dates.
: 
: For example; How many 7th of the month is there between "1998/12/17"
: and "2000/1/7". To make the problem simple, the day of the month (7)
: is the day in the 2nd date.

d1 <- as.Date("1998/12/17")
d2 <- as.Date("2000/1/7")

day <- function(x) as.numeric(format(x, "%d"))

sum(day(seq(from = d1, to = d2, by = "day")) == day(d2))



From Manuel.A.Morales at williams.edu  Tue Feb 15 18:24:31 2005
From: Manuel.A.Morales at williams.edu (Manuel Morales)
Date: Tue, 15 Feb 2005 12:24:31 -0500
Subject: [R] nlsList vs nlme with lsoda function
Message-ID: <4212304F.5030507@williams.edu>

Hello list members,

I'm trying to get nlme to work with lsoda from odesolve. Currently, I 
can use nlsList to fit a simple logistic growth model to some simulated 
data, but I get the following error message with nlme: "Error in 
model.frame(formula, rownames, variables, varnames, extras, extranames, 
  : invalid variable type"

The commands I'm trying are below. Any suggestions for how to get this 
to work?

Thanks!

This works:
fit.nlsList<-nlsList(xvals~lsoda(x0,times,Logist,
+c(r=r,K=K,x0=x0))[,2]|group,
+start=list(r=1,K=500,x0=2),data=data.group)

This returns the error message above:
fit.nlme<-nlme(xvals~(lsoda(x0,times,Logist,
+c(r=r,K=K,x0=x0))[,2]),
+start=list(r=1,K=500,x0=2),data=data.group,
+fixed=r+K+x0~1,random=x0~1)

The model function is:
Logist=function(t,x,parms) {
N1<-x[1]
with(as.list(parms),{
dN1=r*N1*(1-(N1/K))
list(c(dN1))
})}

My data set looks like:
Grouped Data: xvals ~ times | group
            xvals times group
1    -0.44543969     0     1
2    38.86411972     3     1
3   310.77106961     6     1
4   486.78653327     9     1
5     0.40613173     0     2
6    34.94635643     3     2
7   307.81884870     6     2
8   486.46098417     9     2
etc...



From ripley at stats.ox.ac.uk  Tue Feb 15 18:51:18 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 15 Feb 2005 17:51:18 +0000 (GMT)
Subject: Off topic -- large data sets. Was RE: [R] 64 Bit R Background
	Question 
In-Reply-To: <xhdN1XA+viECFwWl@visiv.co.uk>
References: <200502151112.j1FB5fZ5002722@hypatia.math.ethz.ch>
	<xhdN1XA+viECFwWl@visiv.co.uk>
Message-ID: <Pine.LNX.4.61.0502151735100.31845@gannet.stats>

On Tue, 15 Feb 2005, Graham Jones wrote:

> In message <200502151112.j1FB5fZ5002722 at hypatia.math.ethz.ch>, r-help-
> request at stat.math.ethz.ch writes

[Actually quoting Bert Gunter, BTW]

>> Can comeone give me an example (perhaps in a private response, since I'm off
>> topic here) where one actually needs all cases in a large data set ("large"
>> being > 1e6, say) to do a STATISTICAL analysis? By "statistical" I exclude,
>> say searching for some particular characteristic like an adverse event in a
>> medical or customer repair database, etc. Maybe a definition of
>> "statistical" is: anything that cannot be routinely done in a single pass
>> database query.
>
> If the dimensionality of the data is large, you may need a large number
> of cases too. An example from my own experience would be using quadratic
> discriminant analysis (with regularization) for classifying symbols for
> an OCR program. With 200 classes and 100 features, I'd really like many
> millions of cases. I've been using about 20,000 per class or 4 million
> in total, but if I had 40 million it would probably work better.
> Compared to many applications in pattern recognition and data mining, I
> think this is a fairly small example.

But Bert's caveats apply: you have 200 problems of size 20,000 since in 
QDA each class's distribution is estimated separately, and a single pass 
will give you the sufficient statistics however large the dataset is.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Feb 15 19:04:52 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 15 Feb 2005 18:04:52 +0000 (GMT)
Subject: [R] how many 7th of the month is there between two dates
In-Reply-To: <3f87cc6d05021508595a860294@mail.gmail.com>
References: <3f87cc6d05021508595a860294@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0502151751390.31845@gannet.stats>

It is really easy using R's date and datetime classes:

st <- as.Date("1998-12-17")
en <- as.Date("2000-1-7")
st0 <- st
# Get the 7th of the month before
st0 <- as.POSIXlt(st); st0$mday <- 7; st0$mon <- st0$mon - 1
st0 <- as.Date(st0)
ll <- seq.Date(st0, en, by="month")
sum(ll > as.Date(st) & ll < en)

and use >= etc depending what you mean by `between'.

On Tue, 15 Feb 2005, Omar Lakkis wrote:

> This is a eaeir way to ask my prior question:
>
> I want to caculate how many an exact day of the month there is between
> two dates.
>
> For example; How many 7th of the month is there between "1998/12/17"
> and "2000/1/7". To make the problem simple, the day of the month (7)
> is the day in the 2nd date.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at myway.com  Tue Feb 15 20:44:51 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 15 Feb 2005 19:44:51 +0000 (UTC)
Subject: [R] Problem with "R CMD Rd2dvi": Rd.sty not found
References: <9CC1B717EF3BD511AD98000103D63FC53FA754@us-arl-asg.mail.saic.com>
	<loom.20050210T222445-608@post.gmane.org>
	<pg1u0191bs54kumk4mqou3hoklo5de51bu@4ax.com>
Message-ID: <loom.20050215T202405-377@post.gmane.org>

Duncan Murdoch <murdoch <at> stats.uwo.ca> writes:

: 
: On Thu, 10 Feb 2005 21:27:03 +0000 (UTC), Gabor Grothendieck
: <ggrothendieck <at> myway.com> wrote :
: 
: >Tuszynski, Jaroslaw W. <JAROSLAW.W.TUSZYNSKI <at> saic.com> writes:
: >
: >: 
: >: Hi,
: >: 
: >: I run into a problem with "R CMD Rd2dvi" command: it gives me "File 
`Rd.sty'
: >: not found" error (See the output message on the bottom). I get the same
: >: error when running "RCMD check". My system is: Windows 2000, R version
: >: 2.0.1, MiKTeX version 2.4. I do have a Rd.sty file in R_HOME/share/texm
: >: directory.
: >: 
:  ...
: 
: >There is some information on this at:
: >
: >http://www.murdoch-sutherland.com/Rtools/miktex.html
: 
: The information there is slightly out of date:  the last version of
: Miktex that I've tried has broken that solution.  I asked the author
: about it, and he replied as below.
: 
: I think this is pretty poor behaviour on the part of Miktex, but in
: other respects I prefer it to fptex.  
: 
: For what you're doing, you need to set up your
: "$R_HOME/bin/helpprint.bat" file so that it makes use of the TEXINPUTS
: environment variable.  You want the pdftex command to have something
: like
: 
: -include-directory=%TEXINPUTS%
: 
: in it and then it will look in the right places for the includes.
: 
: Duncan Murdoch

I tried playing around with MiKTeX and got some advice on the TeX 
list and now am using this config.  This example assumes you have
put your localtexmf file in \localtexmf and are running
R 2.0.1 from \Program Files\R\rw2001; otherwise, make the obvious
changes.

1. Create a new tex subfolder of your \localtexmf\texmf folder:

    md \localtexmf\texmf\tex

2. Copy your R .sty and .fd files into it:

    cd \Program Files\R\rw2001\share\texmf
    copy *.* \localtexmf\texmf\tex

3. Go into
 
    Start | Programs | MiKTeX | MiKTeX Options | General

   and press Update Now and Refresh Now.   (I am not certain that this 
   step needs to be done but it can't hurt.)

#2 and #3 need to be repeated each time one installs a new version 
of R if the *.sty or *.fd files have changed but if they have not
changed then nothing at all needs to be done.  It has the advantage 
that it leaves all MiKTeX options at their defaults.  It represents
a fourth alternative to the three listed at:

   http://www.murdoch-sutherland.com/Rtools/miktex.html



From ggrothendieck at myway.com  Tue Feb 15 20:59:56 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 15 Feb 2005 19:59:56 +0000 (UTC)
Subject: [R] Problem with "R CMD Rd2dvi": Rd.sty not found
References: <9CC1B717EF3BD511AD98000103D63FC53FA754@us-arl-asg.mail.saic.com>
	<loom.20050210T222445-608@post.gmane.org>
	<pg1u0191bs54kumk4mqou3hoklo5de51bu@4ax.com>
	<loom.20050215T202405-377@post.gmane.org>
Message-ID: <loom.20050215T205503-844@post.gmane.org>

Gabor Grothendieck <ggrothendieck <at> myway.com> writes:


> I tried playing around with MiKTeX and got some advice on the TeX 
> list and now am using this config.  This example assumes you have
> put your localtexmf file in \localtexmf and are running
> R 2.0.1 from \Program Files\R\rw2001; otherwise, make the obvious
> changes.

Sorry, there were some errors in my description of configuring
MiKTeX so that it finds the Rd.sty file and associated files.  
It should be:
 
1. Create a new tex subfolder of your \localtexmf folder:

    md \localtexmf\tex

2. Copy your R .sty and .fd files into it:
 
     cd \Program Files\R\rw2001\share\texmf
     copy *.* \localtexmf\tex
 
3. Go into
 
     Start | Programs | MiKTeX | MiKTeX Options | General
 
    and press Update Now and Refresh Now.   (I am not certain that this 
    step needs to be done but it can't hurt.)

4.  You can check whether it is finding Rd.sty in the right place
    with the command:

     findtexmf Rd.sty
 
#2 - #4 need to be repeated each time one installs a new version 
of R if the *.sty or *.fd files have changed but if they have not
changed then nothing at all needs to be done.  It has the advantage 
that it leaves all MiKTeX options at their defaults and does not
need a custom miktex.ini file.  It represents a fourth alternative 
to the three listed at:
 
    http://www.murdoch-sutherland.com/Rtools/miktex.html



From murdoch at stats.uwo.ca  Tue Feb 15 21:24:46 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 15 Feb 2005 20:24:46 +0000
Subject: [R] Problem with "R CMD Rd2dvi": Rd.sty not found
In-Reply-To: <loom.20050215T202405-377@post.gmane.org>
References: <9CC1B717EF3BD511AD98000103D63FC53FA754@us-arl-asg.mail.saic.com>
	<loom.20050210T222445-608@post.gmane.org>
	<pg1u0191bs54kumk4mqou3hoklo5de51bu@4ax.com>
	<loom.20050215T202405-377@post.gmane.org>
Message-ID: <cam4119e553t15tgeh651u1heni275iev1@4ax.com>

On Tue, 15 Feb 2005 19:44:51 +0000 (UTC), Gabor Grothendieck
<ggrothendieck at myway.com> wrote :


>I tried playing around with MiKTeX and got some advice on the TeX 
>list and now am using this config.  This example assumes you have
>put your localtexmf file in \localtexmf and are running
>R 2.0.1 from \Program Files\R\rw2001; otherwise, make the obvious
>changes.

Thanks, I'll put this on the web page (with the corrections from your
next message).

It's probably the best solution for most users, but it doesn't work
for me:  I need to have more than one R version on my machine at the
same time (I currently have R 2.0.1, R-patched, and R-devel; usually I
have a few old ones too, but I'm working on a laptop with limited
space.)  This solution assumes you have only one version installed.

I don't know what people developing LaTeX styles do:  presumably
they'd have multiple versions of the style online, testable in
multiple versions of LaTeX.

Duncan Murdoch



From MBock at arcadis-us.com  Tue Feb 15 21:57:30 2005
From: MBock at arcadis-us.com (Bock, Michael)
Date: Tue, 15 Feb 2005 13:57:30 -0700
Subject: [R] Trellis barchart, column display order issue
Message-ID: <0016F5677B1F1D4281EEBC0349935951025064A3@CORPEXBE1.arcadis-us.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050215/142f9e30/attachment.pl

From kkthird at yahoo.com  Tue Feb 15 22:32:08 2005
From: kkthird at yahoo.com (KKThird@Yahoo.Com)
Date: Tue, 15 Feb 2005 13:32:08 -0800 (PST)
Subject: [R] Making a Package
Message-ID: <20050215213208.12010.qmail@web52502.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050215/1b4f38cb/attachment.pl

From ggrothendieck at myway.com  Tue Feb 15 22:59:12 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 15 Feb 2005 21:59:12 +0000 (UTC)
Subject: [R] Making a Package
References: <20050215213208.12010.qmail@web52502.mail.yahoo.com>
Message-ID: <loom.20050215T225556-805@post.gmane.org>

KKThird <at> Yahoo.Com <kkthird <at> yahoo.com> writes:

: 
: Hello.
: I have what I know to be a simple question, but never having done anything 
like this it is
: pretty tough.
: 
: I'm trying to write an R package. I have a collection of functions that I 
loaded into R and
: then used package.skeleton(). After editing everything in the resulting 
folder, call it
: NewPackage, I tried to follow along with some instructions I found for 
Windows users.
: 
: I installed ActivePearl to the C drive, placed the unzipped Rtools folder, 
tools, there also
: (is placing it there all that is necessary; I couldn't find anything 
to "install" tools), and
: I'm using R 2.00 (also installed on the C drive) on a Win-XP machine.
: 
: My understaning is that (since I have no C, Fortran, etc., code) I can move 
the NewPackage
: folder (with all of the edited material) to the C:\r\rw2000\src\library 
folder and then open
: MS-Dos in C:\r\rw2000\src\gnuwin32 and type: make pkg-NewPackage. After 
that, a folder called
: NewPackage should be placed in the C:\r\rw2000\library and from there loaded 
into R and/or zipped and
: distributed. If everything I have said is correct (which it may not be) then 
I'm
: stuck. When I type 'make pkg-NewPackage' in the C:\r\rw2000\src\gnuwin32 
directory I get
: "'make' is not recognized as an internal or external command, operable 
program or batch file."
: 
: I've tried to use "Writing R Extensions," but I could only follow part of 
what it was saying
: and got confused as to what was done on Windows and what was done on Unix 
machines.
: I know this is probably an easy question, but it has proved difficult for me 
to figure out how
: to make my own package.
: Thanks, Ken


Make sure that:
- the tools and the R bin folders are in your path
- you have installed fptex or MiKTeX, perl and the Microsoft help compiler
  (MiKTeX is a bit harder to install but I otherwise prefer it.  If you
  are writing vignettes you must use MiKTeX.)
- you have read:
   Writing R Extensions Manual
   http://www.murdoch-sutherland.com/Rtools/

I am assuming here that your R installation is in \Program Files\R\rw....

1. Assuming your source package tree is in \Rpkgs\mypackage
   then in the Windows cmd line:

	cd \Rpkgs
	Rcmd install mypackage

   which will install it to your \Program Files\R\rw....\library\mypackage
   or if you want to install it to a separate library:

	cd \Rpkgs
	md library
	Rcmd install -l library mypackage

2. Now in R:

	library(mypackage)
	... test it out ...

or if you installed it to a separate library:

	library(mypackage, lib.loc = "/Rpkgs/library")

3. Once it seems reasonably ok,  see if it passes Rcmd check:

	cd \Rpkgs
	Rcmd check mypackage

   and fix it up until it does.

4. Now create versions for UNIX and Windows that you can distribute:

	cd \Rpkgs
	Rcmd build mypackage
	Rcmd build mypackage --binary



From ripley at stats.ox.ac.uk  Tue Feb 15 23:05:35 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 15 Feb 2005 22:05:35 +0000 (GMT)
Subject: [R] summary(aov(...)) into a string?
In-Reply-To: <20050215220321.4720f818@zygiella.local>
References: <20050215220321.4720f818@zygiella.local>
Message-ID: <Pine.LNX.4.61.0502152203270.3985@gannet.stats>

It doesn't print anything: the summary.aov (or summary.aovlist) 
print method does.

?summary.aov tells you the structure of the objects they return.

On Tue, 15 Feb 2005, RenE J.V. Bertin wrote:

> I'd like to annotate a plot with the output of summary(aov(model)), 
> ideally just with the significant effects. I don't find a means to 
> redirect what that command prints into a string. Is this possible, and 
> if so, how?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From deepayan at stat.wisc.edu  Tue Feb 15 23:02:42 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Tue, 15 Feb 2005 16:02:42 -0600
Subject: [R] Trellis barchart, column display order issue
In-Reply-To: <0016F5677B1F1D4281EEBC0349935951025064A3@CORPEXBE1.arcadis-us.com>
References: <0016F5677B1F1D4281EEBC0349935951025064A3@CORPEXBE1.arcadis-us.com>
Message-ID: <200502151602.42574.deepayan@stat.wisc.edu>

On Tuesday 15 February 2005 14:57, Bock, Michael wrote:
> I have searched the archives and instructions high and low but have
> not found what I need.
>
> I have a dataframe named Data with columns:
> AdjResND0 - the numeric result
> Parameter - the classification (chemical name)
> Sort - the order I want the chemical names to appear, with leading
> zeros so all are the same length
> Client.Name - the sample name

Try looking at 

levels(Data$Parameter)

I'll bet they are in alphabetical order. See ?factor for how to specify 
the levels explicitly. Alternatively, barring unforeseen complications, 
and assuming that you have a sufficiently modern version of R, you 
should be able to do 

barchart(AdjResND0 ~ reorder(Parameter, sort) | Client.Name, 
         Data, ...

and if that doesn't work as intended,

barchart(AdjResND0 ~ reorder(Parameter, 
             as.numeric(as.character(sort))) | Client.Name, 
         Data, ...

Deepayan

> I am trying to generate a series of 1 page barcharts, one for each
> sample. The dataframe has been sorted by Client.name and sort
>  I use the following command:
>
> barchart(AdjResND0 ~ Parameter | Client.Name , Data,
>    box.ratio = 0.8, ylab= ("Concentration (mg/kg)"),
>    layout = c(0,1), scales = list(x="free",rot=90,cex = 0.5,
>    axs = "i",abbreviate = TRUE ) )
>
> The parameters are listed alphabetic order from left to right in the
> bar chart so I constructed a new variable
> Label which is the sort and parameter variables concatenated
> together.
>
> barchart(AdjResND0 ~ Label | Client.Name , Data,
>    box.ratio = 0.8, ylab= ("Concentration (mg/kg)"),
>    layout = c(0,1), scales = list(x="free",rot=90,cex = 0.5,
>    axs = "i",abbreviate = TRUE ) )
>
> This has everything in the correct order but obviously I don't want
> the column labels to have the sort order glued on the front. Any way
> to get the order I want and the labels I want?
>
> I am using :
> R : Copyright 2004, The R Foundation for Statistical Computing
> Version 2.0.1  (2004-11-15), ISBN 3-900051-07-0
>
> Under Windows XP
>
> Also, I am as yet unable to figure out how to get R to spool the
> graphs to a pdf file so I get all 30 graphs and I can save them for
> an appendix. I just want to be able to save all the graphs for future
> use, so I am not set on pdf or anything else. An example would be
> great!
>
>
>
> Thanks,
>
> Mike
>
>
>
> Michael J. Bock, PhD.
> ARCADIS
> 24 Preble St. Suite 100
> Portland, ME 04101
> 207.828.0046
> fax 207.828.0062



From ggrothendieck at myway.com  Tue Feb 15 23:12:04 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 15 Feb 2005 22:12:04 +0000 (UTC)
Subject: [R] summary(aov(...)) into a string?
References: <20050215220321.4720f818@zygiella.local>
Message-ID: <loom.20050215T231121-4@post.gmane.org>

RenE J.V. Bertin <rjvbertin <at> hotmail.com> writes:

: 
: Hello,
: 
: I'd like to annotate a plot with the output of summary(aov(model)), ideally 
just with the significant
: effects. I don't find a means to redirect what that command prints into a 
string.
: Is this possible, and if so, how?
: 

?capture.output



From Alexander.O.BETTINARDI at odot.state.or.us  Tue Feb 15 23:29:21 2005
From: Alexander.O.BETTINARDI at odot.state.or.us (Alexander.O.BETTINARDI@odot.state.or.us)
Date: Tue, 15 Feb 2005 14:29:21 -0800
Subject: [R] Tcl/tk
Message-ID: <76A000A82289D411952F001083F9DD0607ACE3DC@exsalem4-bu.odot.state.or.us>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050215/dc4510b5/attachment.pl

From michael.watson at bbsrc.ac.uk  Tue Feb 15 23:30:16 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Tue, 15 Feb 2005 22:30:16 -0000
Subject: [R] Making a Package
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950121BA4A@iahce2knas1.iah.bbsrc.reserved>

I follow the guide here and it never fails:
 
http://www.biostat.jhsph.edu/~kbroman/Rintro/Rwinpack.html
 
So if I have the MyPackage directory, that was created with package.skeleton and subsequently edited, I will cd to the directory that contains the MyPackage directory and:
 
R CMD INSTALL --build MyPackage
 
MyPackage_1.0.zip will be created.  Then install using the menus.

	-----Original Message----- 
	From: r-help-bounces at stat.math.ethz.ch on behalf of Gabor Grothendieck 
	Sent: Tue 2/15/2005 9:59 PM 
	To: r-help at stat.math.ethz.ch 
	Cc: 
	Subject: Re: [R] Making a Package
	
	

	KKThird <at> Yahoo.Com <kkthird <at> yahoo.com> writes:
	
	:
	: Hello.
	: I have what I know to be a simple question, but never having done anything
	like this it is
	: pretty tough.
	:
	: I'm trying to write an R package. I have a collection of functions that I
	loaded into R and
	: then used package.skeleton(). After editing everything in the resulting
	folder, call it
	: NewPackage, I tried to follow along with some instructions I found for
	Windows users.
	:
	: I installed ActivePearl to the C drive, placed the unzipped Rtools folder,
	tools, there also
	: (is placing it there all that is necessary; I couldn't find anything
	to "install" tools), and
	: I'm using R 2.00 (also installed on the C drive) on a Win-XP machine.
	:
	: My understaning is that (since I have no C, Fortran, etc., code) I can move
	the NewPackage
	: folder (with all of the edited material) to the C:\r\rw2000\src\library
	folder and then open
	: MS-Dos in C:\r\rw2000\src\gnuwin32 and type: make pkg-NewPackage. After
	that, a folder called
	: NewPackage should be placed in the C:\r\rw2000\library and from there loaded
	into R and/or zipped and
	: distributed. If everything I have said is correct (which it may not be) then
	I'm
	: stuck. When I type 'make pkg-NewPackage' in the C:\r\rw2000\src\gnuwin32
	directory I get
	: "'make' is not recognized as an internal or external command, operable
	program or batch file."
	:
	: I've tried to use "Writing R Extensions," but I could only follow part of
	what it was saying
	: and got confused as to what was done on Windows and what was done on Unix
	machines.
	: I know this is probably an easy question, but it has proved difficult for me
	to figure out how
	: to make my own package.
	: Thanks, Ken
	
	
	Make sure that:
	- the tools and the R bin folders are in your path
	- you have installed fptex or MiKTeX, perl and the Microsoft help compiler
	  (MiKTeX is a bit harder to install but I otherwise prefer it.  If you
	  are writing vignettes you must use MiKTeX.)
	- you have read:
	   Writing R Extensions Manual
	   http://www.murdoch-sutherland.com/Rtools/
	
	I am assuming here that your R installation is in \Program Files\R\rw....
	
	1. Assuming your source package tree is in \Rpkgs\mypackage
	   then in the Windows cmd line:
	
	        cd \Rpkgs
	        Rcmd install mypackage
	
	   which will install it to your \Program Files\R\rw....\library\mypackage
	   or if you want to install it to a separate library:
	
	        cd \Rpkgs
	        md library
	        Rcmd install -l library mypackage
	
	2. Now in R:
	
	        library(mypackage)
	        ... test it out ...
	
	or if you installed it to a separate library:
	
	        library(mypackage, lib.loc = "/Rpkgs/library")
	
	3. Once it seems reasonably ok,  see if it passes Rcmd check:
	
	        cd \Rpkgs
	        Rcmd check mypackage
	
	   and fix it up until it does.
	
	4. Now create versions for UNIX and Windows that you can distribute:
	
	        cd \Rpkgs
	        Rcmd build mypackage
	        Rcmd build mypackage --binary
	
	______________________________________________
	R-help at stat.math.ethz.ch mailing list
	https://stat.ethz.ch/mailman/listinfo/r-help
	PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From cuiczhao at yahoo.com  Tue Feb 15 23:36:12 2005
From: cuiczhao at yahoo.com (Cuichang Zhao)
Date: Tue, 15 Feb 2005 14:36:12 -0800 (PST)
Subject: [R] Could anyone answer for the following question
Message-ID: <20050215223612.26756.qmail@web30706.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050215/b01239ff/attachment.pl

From p.murrell at auckland.ac.nz  Tue Feb 15 23:40:28 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Wed, 16 Feb 2005 11:40:28 +1300
Subject: [R] special symobol / character
References: <283982AD9F3CD211B3AC00A0C983032F0F8F3F5C@paradise.ansto.gov.au>
Message-ID: <42127A5C.1060505@stat.auckland.ac.nz>

Hi


FISCHER, Matthew wrote:
> 
> Hi Eric,
> 
>     Thanks for your speedy reply,  I should have noted
> that I'm using a Linux machine.  However, when I copy
> the symbol from Windows to Linux (using R/emacs) via an x-win 32 window
> it replaces the per mille symbol with a /211.
> R then produces the character (not a per mille symbol!)
> that can be found in the equivalent place
> using character.table() in the Hmisc package.  I'd use
> windows, except we have huge output datasets generated
> by a climate model, and its not possible to move it to a machine
> running windows.
> 
> Any other suggestions are welcome!,


If postscript output is sufficient, the following trick should work ...

postscript(encoding="WinAnsi.enc")
plot(1:10, ylab="\211")
dev.off()

Paul


> -----Original Message-----
> From: Eric Lecoutre [mailto:lecoutre at stat.ucl.ac.be]
> Sent: Tuesday, 15 February 2005 18:54
> To: FISCHER, Matthew; 'r-help at stat.math.ethz.ch'
> Subject: Re: [R] special symobol / character
> 
> 
> 
> Hi Matthew,
> 
> Most systems allow to enter any ASCII (or extended ASCII) character 
> directly using a key combination.
> Accessing ANSI charcaters under Windows is possible with:
> ALT+0xxx (press ALT, hold it down, press 0 and the number of the character, 
> release ALT)
> Thus: ALT+0137 makes: ?
> The future seems promising with the Unicode support: kudo R core team!
> 
> Eric
> 
> 
> At 08:23 15/02/2005, FISCHER, Matthew wrote:
> 
> 
>>Hi all,
>>
>>    Is it possible to add a permil (or per mille) symbol to
>>an R plot (I couldn't find this symbol under demo(Hershey) or
>>the plotmath information).
>>
>>In some ascii tables it is symbol no. 137.
>>
>>cheers,
>>
>>Matt.
>>
>>        [[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 
> Eric Lecoutre
> UCL /  Institut de Statistique
> Voie du Roman Pays, 20
> 1348 Louvain-la-Neuve
> Belgium
> 
> tel: (+32)(0)10473050
> lecoutre at stat.ucl.ac.be
> http://www.stat.ucl.ac.be/ISpersonnel/lecoutre
> 
> If the statistics are boring, then you've got the wrong numbers. -Edward 
> Tufte
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From p.dalgaard at biostat.ku.dk  Tue Feb 15 23:42:19 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 15 Feb 2005 23:42:19 +0100
Subject: [R] summary(aov(...)) into a string?
In-Reply-To: <Pine.LNX.4.61.0502152203270.3985@gannet.stats>
References: <20050215220321.4720f818@zygiella.local>
	<Pine.LNX.4.61.0502152203270.3985@gannet.stats>
Message-ID: <x23bvx4f90.fsf@biostat.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> It doesn't print anything: the summary.aov (or summary.aovlist) print
> method does.
> 
> ?summary.aov tells you the structure of the objects they return.

Yes..... but wouldn't capture.output() and par(family="mono") be
closer to the mark?
 
> On Tue, 15 Feb 2005, RenE J.V. Bertin wrote:
> 
> > I'd like to annotate a plot with the output of summary(aov(model)),
> > ideally just with the significant effects. I don't find a means to
> > redirect what that command prints into a string. Is this possible,
> > and if so, how?


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Tue Feb 15 23:51:45 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 15 Feb 2005 23:51:45 +0100
Subject: [R] Tcl/tk
In-Reply-To: <76A000A82289D411952F001083F9DD0607ACE3DC@exsalem4-bu.odot.state.or.us>
References: <76A000A82289D411952F001083F9DD0607ACE3DC@exsalem4-bu.odot.state.or.us>
Message-ID: <x2y8dp308u.fsf@biostat.ku.dk>

Alexander.O.BETTINARDI at odot.state.or.us writes:

> I am new to R so I apologize if this is a simple question
> 
> I have created a tkgrid with a button that, once pushed, will extract the
> information from the grid, but I can not figure out the command to save the
> entries from my grid.
> Any thoughts?

Several, all starting with "What"...

What is tkgrid (not the geometry manager surely)?
What information?
What form is it "extracted" in?
What "entries"?
What do you want to save it to?



-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ggrothendieck at myway.com  Wed Feb 16 00:02:44 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 15 Feb 2005 18:02:44 -0500 (EST)
Subject: [R] Problem with 
Message-ID: <20050215230244.9193339AA@mprdmxin.myway.com>



I am just in the middle of a conversation on the tex list but
just in case you are about to post anything to your web site
here is a slight simplification.  It consists of updating the 
MiKTeX file database using initexmf instead of the GUI so you 
don't have to move back and forth during the instructions.
The discussion there might correct or improve this further.
Regards.

---

1. Create a new tex subfolder of your \localtexmf folder:

    md \localtexmf\tex

2. Copy your R .sty and .fd files into it, update the
   MiKTeX texmf file name database and check that Rd.sty
   is found in the correct place, viz. \localtexmf\tex:

     cd \Program Files\R\rw2001\share\texmf
     copy *.* \localtexmf\tex
     initexmf -u
     findtexmf Rd.sty
  
#2 should be repeated each time one installs a new version 
of R or any time the *.sty or *.fd files are changed.

(Actually nothing needs to be done if the .sty and .fd files 
have not changed from one version of R to another and the
initexmf command does not need to be run unless the filenames
themselves have changed even if their contents have; however,
none of these steps will hurt and its safest to do them all.)



From gunter.berton at gene.com  Wed Feb 16 00:09:52 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 15 Feb 2005 15:09:52 -0800
Subject: [R] Could anyone answer for the following question
In-Reply-To: <20050215223612.26756.qmail@web30706.mail.mud.yahoo.com>
Message-ID: <200502152309.j1FN9pgw022481@meitner.gene.com>

options(graphics.record=TRUE) turns on graphics recording.
This can also be done from the History menu of the graphics window.

All subsequent plots to the graphics device will be saved.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Cuichang Zhao
> Sent: Tuesday, February 15, 2005 2:36 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Could anyone answer for the following question
> 
>  
> Hello , 
> could anyone answer for the following question for me:
>  
>  
> I am using R 2.0.1 under Windows XP. I want to write a function that 
> makes four graphs and stores each of them in graphics 
> history. When the 
> function finishes, in other words, I want its graphical output to be 
> stored in a way that I can look at it using PgUp and PgDn. I 
> think I need 
> commands I can put in a function that
> 
> --clear graphics history
> --either turn on graphics history recording or store the current graph
> 
> Can anyone explain how to do this?
> 
> 
> Thank you so much
>  
> C-Ming
>  
> Feb 15, 2005
> 
> 		
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ggrothendieck at myway.com  Wed Feb 16 00:48:14 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 15 Feb 2005 23:48:14 +0000 (UTC)
Subject: [R] Could anyone answer for the following question
References: <20050215223612.26756.qmail@web30706.mail.mud.yahoo.com>
Message-ID: <loom.20050216T003953-936@post.gmane.org>

Cuichang Zhao <cuiczhao <at> yahoo.com> writes:


: I am using R 2.0.1 under Windows XP. I want to write a function that 
: makes four graphs and stores each of them in graphics history. When the 
: function finishes, in other words, I want its graphical output to be 
: stored in a way that I can look at it using PgUp and PgDn. I think I need 
: commands I can put in a function that
: 
: --clear graphics history
: --either turn on graphics history recording or store the current graph


windows(record = TRUE) will turn on windows graphics history and savePlot will
save a windows plot in a file.   You can also turn it on manually by
clicking on the graphics window and making sure History | Record is 
checked.

(Its also possible to save the low level graphics that R displays
using the displaylist and you can search r-help for more info
on 'displaylist' but I don't think that is what you are looking for.)



From Tom.Mulholland at dpi.wa.gov.au  Wed Feb 16 02:05:13 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Wed, 16 Feb 2005 09:05:13 +0800
Subject: [R] reading a 40kb csv file in R
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3C99E@afhex01.dpi.wa.gov.au>

I think you may need to check the list for posts relating to "memory", "large datasets", etc
 http://cran.r-project.org/search.html

It would help those on the list, if you gave more details about the machine you are using. Given the vintage of R you areusing is considered quite old, there is the possibility that you are using a machine with 256 Mb of RAM in which case it is likely you have hit a barrier that requires you to think about alternative ways of processing your data.

The reason I suggest you serach the posts yourself rather than give advice is that there are many possibilities which depend on information you have that I don't, about other software you have such as sql databases or you may understand better than I do some of the posts about sparse matrices.

Tom

> -----Original Message-----
> From: Fairouz Makhlouf [mailto:fhilal at yahoo.com]
> Sent: Wednesday, 16 February 2005 1:05 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] reading a 40kb csv file in R
> 
> 
> I am trying to read a matrix of  5000*2000 from a csv
> file of real numbers in R using the scan function. I
> am getting an error saying that R can not read a
> vector > 32KB and it can't allocate a memory of 191 MB
> etc. 
> 
> What should I do. I am using R1.7.1.
> 
> Thanks
> 
> =====
> Thanks
> Fairouz Makhlouf
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From p.murrell at auckland.ac.nz  Wed Feb 16 03:19:57 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Wed, 16 Feb 2005 15:19:57 +1300
Subject: [R] lattice multiple plots per page
References: <5.2.1.1.2.20050214174512.01ce0e90@wiscmail.wisc.edu>
	<200502151020.07183.deepayan@stat.wisc.edu>
Message-ID: <4212ADCD.2000508@stat.auckland.ac.nz>

Hi


Deepayan Sarkar wrote:
> On Monday 14 February 2005 18:12, Jeff Jorgensen wrote:
> 
>>Dear R-sters,
>>
>>I was wondering if anyone has encountered the following issues.  I've
>>figured out how to get multiple levelplots [library(lattice)] on a
>>single plot.  However, when I add text (adding axis labels for the
>>entire four panel plot) the text is missing when I insert the *.eps
>>file I've created into my LaTeX document (via MikTeX-WinEdt).  And,
>>I've just upgraded to R v2.0.1 from v1.8.1 (Windows), and each
>>individual levelplot is smaller compared to the older R release.
>>
>>Any clues as to 1) how I can recover the lost text, and 
> 
> 
> Your use is incorrect, in the sense that the return value of 
> grid.locator() is not a valid input of ltext. You could do the 
> following to fix this, 
> 
> library(grid)
> ltext(lapply(grid.locator(), convertX, "native", TRUE),
>       lab="x-axis label, where I click",cex=1.5)
> ltext(lapply(grid.locator(), convertX, "native", TRUE),
>       lab = "y-axis label, where I click",
>       cex=1.5, srt=90)
 > but that doesn't seem to help either. So, there seems to be a bug
 > somewhere.


I think this has the same problem as the original code, which is that it 
attempts to use ltext() outside of a lattice panel function, so the 
"native" units assumed by ltext() could be inappropriate.

Within the top-level grid viewport, "native" units are (at least 
approximately) the native units of the device (i.e., "pixels").  The 
original drawing on screen is occurring in terms of screen pixels, but 
when you do the dev.copy2eps(), "native" means something different and 
you end up drawing in terms of PostScript points (1/72 inches), so the 
locations may be different (especially if your screen is, say, 96 dpi).


 > However, you presumably want something like
 >
> grid.text(x = .05, y = .5, lab="y-axis label", 
>           default.units = "npc", gp = gpar(cex=1.5), 
>           rot = 90)
> grid.text(x = .5, y = .05, lab="x-axis label", 
>           default.units = "npc", gp = gpar(cex=1.5))
> 
> for the last two steps, which seems to work. 


Right.  First of all, use grid.text() not ltext(), and secondly,  use 
"npc" coordinates (in fact, anything other than "native" should do), 
which should transform to the same location on the dev.copy2eps().

If you want to position the labels using a mouse click ...

xy <- grid.locator("npc")
grid.text(xy$x, xy$y, lab="y-axis label",
           gp = gpar(cex=1.5), rot = 90)
xy <- grid.locator("npc")
grid.text(xy$x, xy$y, lab="x-axis label",
           gp = gpar(cex=1.5))

... again using "npc" locations rather than "native".

Paul


>>2) increase the size of each of the levelplots?
> 
> 
> You need to change the following settings, in particular the entries 
> that end with 'padding' (similarly for 'layout.heights').
> 
> 
>>str(trellis.par.get("layout.widths"))
> 
> List of 13
>  $ left.padding     : num 1
>  $ key.left         : num 1
>  $ key.ylab.padding : num 1
>  $ ylab             : num 1
>  $ ylab.axis.padding: num 1
>  $ axis.left        : num 1
>  $ axis.panel       : num 1
>  $ panel            : num 1
>  $ between          : num 1
>  $ axis.right       : num 1
>  $ axis.key.padding : num 1
>  $ key.right        : num 1
>  $ right.padding    : num 1
> 
> Deepayan
> 
> 
>>Cheers,
>>
>>Jeff Jorgensen
>>
>>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>>Sample code that illustrates what I'm trying to do:
>>
>>#create a levelplot
>>x<-seq(pi/4, 5*pi, length = 100)
>>y<-seq(pi/4, 5*pi, length = 100)
>>r<-as.vector(sqrt(outer(x^2, y^2, "+")))
>>grid<-expand.grid(x=x, y=y)
>>grid$z<-cos(r^2) * exp(-r/(pi^3))
>>a<-levelplot(z~x*y, grid, cuts = 50, xlab="", ylab=",
>>  colorkey = FALSE)
>>
>>#create the multiple panel plot, here using all the same levelplot
>>trellis.par.set(list(background=list(col="white"))) #white background
>>#using position to scale the plots up and to the right ~10%
>>#to make room for the axis labels
>>print(a,position=c(0.1,0.1,1,1),split=c(1,1,2,2),more=T)
>>print(a,position=c(0.1,0.1,1,1),split=c(1,2,2,2),more=T)
>>print(a,position=c(0.1,0.1,1,1),split=c(2,1,2,2),more=T)
>>print(a,position=c(0.1,0.1,1,1),split=c(2,2,2,2),more=F)
>>#commands that let you click where you want the labels centered
>>ltext(grid::grid.locator(),lab="x-axis label, where I click",cex=1.5)
>>ltext(grid::grid.locator(),lab="y-axis label, where I
>>click",cex=1.5,srt=90) #save device to an *.eps file, to be called
>>later by a \includegraphics command
>>dev.copy2eps(file="twobytwoplot.eps")
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From ggrothendieck at myway.com  Wed Feb 16 08:09:55 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 16 Feb 2005 07:09:55 +0000 (UTC)
Subject: [R] Making a Package
References: <8975119BCD0AC5419D61A9CF1A923E950121BA4A@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <loom.20050216T074328-689@post.gmane.org>



michael watson (IAH-C <michael.watson <at> bbsrc.ac.uk> writes:

: 
: I follow the guide here and it never fails:
: 
: http://www.biostat.jhsph.edu/~kbroman/Rintro/Rwinpack.html
: 
: So if I have the MyPackage directory, that was created with package.skeleton 
and subsequently edited, I
: will cd to the directory that contains the MyPackage directory and:
: 
: R CMD INSTALL --build MyPackage
: 
: MyPackage_1.0.zip will be created.  Then install using the menus.

Its faster to do:

  Rcmd install mypackage

followed by 

  library(mypackage) # done from R

as it eliminates the install using menus part.  (Also, except
in the initial development, its common to have a production and 
development version in which case one can usefully keep the latter 
in a separate library as explained below.)  The above is done repeatedly 
and then one checks it using 'Rcmd check mypackage' and further 
testing until it passes the check.  This check step should not be 
ignored as its a key step for quality in the package building 
process.  Finally after its all working one can produce the .zip and 
.tar.gz files using 'Rcmd build mypackage' and 'Rcmd build 
--binary mypackage', respectively.

: 
: 	-----Original Message----- 
: 	From: r-help-bounces <at> stat.math.ethz.ch on behalf of Gabor 
Grothendieck 
: 	Sent: Tue 2/15/2005 9:59 PM 
: 	To: r-help <at> stat.math.ethz.ch 
: 	Cc: 
: 	Subject: Re: [R] Making a Package
: 	
: 	
: 
: 	KKThird <at> Yahoo.Com <kkthird <at> yahoo.com> writes:
: 	
: 	:
: 	: Hello.
: 	: I have what I know to be a simple question, but never having done 
anything
: 	like this it is
: 	: pretty tough.
: 	:
: 	: I'm trying to write an R package. I have a collection of functions 
that I
: 	loaded into R and
: 	: then used package.skeleton(). After editing everything in the 
resulting
: 	folder, call it
: 	: NewPackage, I tried to follow along with some instructions I found 
for
: 	Windows users.
: 	:
: 	: I installed ActivePearl to the C drive, placed the unzipped Rtools 
folder,
: 	tools, there also
: 	: (is placing it there all that is necessary; I couldn't find anything
: 	to "install" tools), and
: 	: I'm using R 2.00 (also installed on the C drive) on a Win-XP machine.
: 	:
: 	: My understaning is that (since I have no C, Fortran, etc., code) I 
can move
: 	the NewPackage
: 	: folder (with all of the edited material) to the C:\r\rw2000
\src\library
: 	folder and then open
: 	: MS-Dos in C:\r\rw2000\src\gnuwin32 and type: make pkg-NewPackage. 
After
: 	that, a folder called
: 	: NewPackage should be placed in the C:\r\rw2000\library and from 
there loaded
: 	into R and/or zipped and
: 	: distributed. If everything I have said is correct (which it may not 
be) then
: 	I'm
: 	: stuck. When I type 'make pkg-NewPackage' in the C:\r\rw2000
\src\gnuwin32
: 	directory I get
: 	: "'make' is not recognized as an internal or external command, 
operable
: 	program or batch file."
: 	:
: 	: I've tried to use "Writing R Extensions," but I could only follow 
part of
: 	what it was saying
: 	: and got confused as to what was done on Windows and what was done on 
Unix
: 	machines.
: 	: I know this is probably an easy question, but it has proved 
difficult for me
: 	to figure out how
: 	: to make my own package.
: 	: Thanks, Ken
: 	
: 	
: 	Make sure that:
: 	- the tools and the R bin folders are in your path
: 	- you have installed fptex or MiKTeX, perl and the Microsoft help 
compiler
: 	  (MiKTeX is a bit harder to install but I otherwise prefer it.  If you
: 	  are writing vignettes you must use MiKTeX.)
: 	- you have read:
: 	   Writing R Extensions Manual
: 	   http://www.murdoch-sutherland.com/Rtools/
: 	
: 	I am assuming here that your R installation is in \Program 
Files\R\rw....
: 	
: 	1. Assuming your source package tree is in \Rpkgs\mypackage
: 	   then in the Windows cmd line:
: 	
: 	        cd \Rpkgs
: 	        Rcmd install mypackage
: 	
: 	   which will install it to your \Program 
Files\R\rw....\library\mypackage
: 	   or if you want to install it to a separate library:
: 	
: 	        cd \Rpkgs
: 	        md library
: 	        Rcmd install -l library mypackage
: 	
: 	2. Now in R:
: 	
: 	        library(mypackage)
: 	        ... test it out ...
: 	
: 	or if you installed it to a separate library:
: 	
: 	        library(mypackage, lib.loc = "/Rpkgs/library")
: 	
: 	3. Once it seems reasonably ok,  see if it passes Rcmd check:
: 	
: 	        cd \Rpkgs
: 	        Rcmd check mypackage
: 	
: 	   and fix it up until it does.
: 	
: 	4. Now create versions for UNIX and Windows that you can distribute:
: 	
: 	        cd \Rpkgs
: 	        Rcmd build mypackage
: 	        Rcmd build mypackage --binary



From ligges at statistik.uni-dortmund.de  Wed Feb 16 09:02:37 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 16 Feb 2005 09:02:37 +0100
Subject: [R] Problem with "R CMD Rd2dvi": Rd.sty not found
In-Reply-To: <cam4119e553t15tgeh651u1heni275iev1@4ax.com>
References: <9CC1B717EF3BD511AD98000103D63FC53FA754@us-arl-asg.mail.saic.com>	<loom.20050210T222445-608@post.gmane.org>	<pg1u0191bs54kumk4mqou3hoklo5de51bu@4ax.com>	<loom.20050215T202405-377@post.gmane.org>
	<cam4119e553t15tgeh651u1heni275iev1@4ax.com>
Message-ID: <4212FE1D.6040804@statistik.uni-dortmund.de>

Duncan Murdoch wrote:

> On Tue, 15 Feb 2005 19:44:51 +0000 (UTC), Gabor Grothendieck
> <ggrothendieck at myway.com> wrote :
> 
> 
> 
>>I tried playing around with MiKTeX and got some advice on the TeX 
>>list and now am using this config.  This example assumes you have

Using the menu or using texhash from the command line worked for ages. I 
wonder they have not told you to read the docs.
I don't understand what is that new in using style files with (MiK)TeX. 
The only thing that has changed with MiKTeX using eTeX the env-var/path 
stuff.


>>put your localtexmf file in \localtexmf and are running
>>R 2.0.1 from \Program Files\R\rw2001; otherwise, make the obvious
>>changes.
> 
> 
> Thanks, I'll put this on the web page (with the corrections from your
> next message).
> 
> It's probably the best solution for most users, but it doesn't work
> for me:  I need to have more than one R version on my machine at the
> same time (I currently have R 2.0.1, R-patched, and R-devel; usually I
> have a few old ones too, but I'm working on a laptop with limited
> space.)  This solution assumes you have only one version installed.
> 
> I don't know what people developing LaTeX styles do:  presumably
> they'd have multiple versions of the style online, testable in
> multiple versions of LaTeX.

Put them in separate local directories?

Uwe Ligges



> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From pensterfuzzer at yahoo.de  Wed Feb 16 09:59:14 2005
From: pensterfuzzer at yahoo.de (Werner Wernersen)
Date: Wed, 16 Feb 2005 09:59:14 +0100 (CET)
Subject: [R] Easy cut & paste from Excel to R?
Message-ID: <20050216085914.27179.qmail@web25806.mail.ukl.yahoo.com>

Hi!

Is it possible to easily cut & paste data from an
Excel spreadsheet to 
an R edit( ) grid or to variable?
It seems that R cannot handle the cell delimiters
Excel hands over.

Regards,
  Werner



From ligges at statistik.uni-dortmund.de  Wed Feb 16 10:14:52 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 16 Feb 2005 10:14:52 +0100
Subject: [R] Easy cut & paste from Excel to R?
In-Reply-To: <20050216085914.27179.qmail@web25806.mail.ukl.yahoo.com>
References: <20050216085914.27179.qmail@web25806.mail.ukl.yahoo.com>
Message-ID: <42130F0C.1070905@statistik.uni-dortmund.de>

Werner Wernersen wrote:

> Hi!
> 
> Is it possible to easily cut & paste data from an
> Excel spreadsheet to 
> an R edit( ) grid or to variable?
> It seems that R cannot handle the cell delimiters
> Excel hands over.
> 
> Regards,
>   Werner
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

copy in Excel and say in R:
read.table(file("clipboard"))

Uwe Ligges



From r.hankin at soc.soton.ac.uk  Wed Feb 16 11:00:01 2005
From: r.hankin at soc.soton.ac.uk (Robin Hankin)
Date: Wed, 16 Feb 2005 10:00:01 +0000
Subject: [R] real and complex vectors
Message-ID: <e412debaf2a7310c1a748e81c55514b6@soc.soton.ac.uk>

The following caught me off-guard:


R> z <- 1i + 1:10
R> z <- Re(z)
R> z
  [1]  1  2  3  4  5  6  7  8  9 10

as expected.  But look:

R> z <- 1i + 1:10
R> make.real <- abs(z) < 1000
R> z[make.real] <- Re(z[make.real])
R> z
  [1]  1+0i  2+0i  3+0i  4+0i  5+0i  6+0i  7+0i  8+0i  9+0i 10+0i
R>

didn't make z a real vector, which is what I wanted.  ?"[<-" says

      If one of these expressions appears on the left side of an
      assignment then that part of 'x' is set to the value of the right
      hand side of the assignment.

so the behaviour is as documented: class(z) is unchanged in the second 
session.

Would modifying "[<-" to add a test for all elements of an object being 
replaced (and
if this is the case  to change the class of z appropriately),  be a bad 
idea?


--
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From maillists at visiv.co.uk  Wed Feb 16 11:07:51 2005
From: maillists at visiv.co.uk (Graham Jones)
Date: Wed, 16 Feb 2005 10:07:51 +0000
Subject: Off topic -- large data sets. Was RE: [R] 64 Bit R Background
	Question
In-Reply-To: <Pine.LNX.4.61.0502151735100.31845@gannet.stats>
References: <200502151112.j1FB5fZ5002722@hypatia.math.ethz.ch>
	<xhdN1XA+viECFwWl@visiv.co.uk>
	<Pine.LNX.4.61.0502151735100.31845@gannet.stats>
Message-ID: <1mXEVIA3txECFwmF@visiv.co.uk>

In message <Pine.LNX.4.61.0502151735100.31845 at gannet.stats>, Prof Brian
Ripley <ripley at stats.ox.ac.uk> writes

>But Bert's caveats apply: you have 200 problems of size 20,000 since in 
>QDA each class's distribution is estimated separately, and a single pass 
>will give you the sufficient statistics however large the dataset is.
>

I think we've interpreted Bert's question differently. I am not saying I
need to have vast amounts of data in RAM, or in a single data structure,
or anything like that, and I am not saying I need a 64-bit version of R.
What I am saying is that if I had 40 million cases for a problem like
the one I described, I'd want to use all of them when designing a
classifier.

Patrick Burns, if you're reading: OCR = optical character recognition.

-- 
Graham Jones, author of SharpEye Music Reader
http://www.visiv.co.uk
21e Balnakeil, Durness, Lairg, Sutherland, IV27 4PT, Scotland, UK



From ripley at stats.ox.ac.uk  Wed Feb 16 11:19:37 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 16 Feb 2005 10:19:37 +0000 (GMT)
Subject: [R] real and complex vectors
In-Reply-To: <e412debaf2a7310c1a748e81c55514b6@soc.soton.ac.uk>
References: <e412debaf2a7310c1a748e81c55514b6@soc.soton.ac.uk>
Message-ID: <Pine.LNX.4.61.0502161016500.9842@gannet.stats>

On Wed, 16 Feb 2005, Robin Hankin wrote:

> The following caught me off-guard:
>
>
> R> z <- 1i + 1:10
> R> z <- Re(z)
> R> z
> [1]  1  2  3  4  5  6  7  8  9 10
>
> as expected.  But look:
>
> R> z <- 1i + 1:10
> R> make.real <- abs(z) < 1000
> R> z[make.real] <- Re(z[make.real])
> R> z
> [1]  1+0i  2+0i  3+0i  4+0i  5+0i  6+0i  7+0i  8+0i  9+0i 10+0i
> R>
>
> didn't make z a real vector, which is what I wanted.  ?"[<-" says
>
>     If one of these expressions appears on the left side of an
>     assignment then that part of 'x' is set to the value of the right
>     hand side of the assignment.
>
> so the behaviour is as documented: class(z) is unchanged in the second 
> session.
>
> Would modifying "[<-" to add a test for all elements of an object being 
> replaced (and if this is the case to change the class of z 
> appropriately), be a bad idea?

Yes.  Don't expect your interpreter to mind-read. Changing basic things 
like this is likely to break lots of existing code.

R-help is not really the place for programming design questions (R-devel 
is).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Wed Feb 16 11:18:53 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 16 Feb 2005 11:18:53 +0100
Subject: [R] Easy cut & paste from Excel to R?
In-Reply-To: <42130F0C.1070905@statistik.uni-dortmund.de>
References: <20050216085914.27179.qmail@web25806.mail.ukl.yahoo.com>
	<42130F0C.1070905@statistik.uni-dortmund.de>
Message-ID: <x2d5v0iz8y.fsf@biostat.ku.dk>

Uwe Ligges <ligges at statistik.uni-dortmund.de> writes:

> Werner Wernersen wrote:
> 
> > Hi!
> > Is it possible to easily cut & paste data from an
> > Excel spreadsheet to an R edit( ) grid or to variable?
> > It seems that R cannot handle the cell delimiters
> > Excel hands over.
> > Regards,
> >   Werner
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> copy in Excel and say in R:
> read.table(file("clipboard"))

Er, doesn't that want to be read.delim (or read.delim2 in
"comma-locales")? Plain read.table() could cause some grief if there
are empty cells.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ligges at statistik.uni-dortmund.de  Wed Feb 16 11:33:51 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 16 Feb 2005 11:33:51 +0100
Subject: [R] real and complex vectors
In-Reply-To: <e412debaf2a7310c1a748e81c55514b6@soc.soton.ac.uk>
References: <e412debaf2a7310c1a748e81c55514b6@soc.soton.ac.uk>
Message-ID: <4213218F.5090500@statistik.uni-dortmund.de>

Robin Hankin wrote:

> The following caught me off-guard:
> 
> 
> R> z <- 1i + 1:10
> R> z <- Re(z)
> R> z
>  [1]  1  2  3  4  5  6  7  8  9 10
> 
> as expected.  But look:
> 
> R> z <- 1i + 1:10
> R> make.real <- abs(z) < 1000
> R> z[make.real] <- Re(z[make.real])
> R> z
>  [1]  1+0i  2+0i  3+0i  4+0i  5+0i  6+0i  7+0i  8+0i  9+0i 10+0i
> R>
> 
> didn't make z a real vector, which is what I wanted.  ?"[<-" says
> 
>      If one of these expressions appears on the left side of an
>      assignment then that part of 'x' is set to the value of the right
>      hand side of the assignment.
> 
> so the behaviour is as documented: class(z) is unchanged in the second 
> session.
> 
> Would modifying "[<-" to add a test for all elements of an object being 
> replaced (and
> if this is the case  to change the class of z appropriately),  be a bad 
> idea?

Sorry, but yes, a bad idea.

If you use

  z[someIndex] <- someValues

you expect that z only changes its class if required to represent 
"someValues", but never vice versa.
That's in particular TRUE for the case of explicitly indexing all 
elements as in:

  z <- 1i + 1:10
  z[] <- 1:10

And why should R do different things in the following two cases, comparing
   z[1:5] <- 1:5
and
   z[1:10] <- 1:10
?


Uwe Ligges



> 
> -- 
> Robin Hankin
> Uncertainty Analyst
> Southampton Oceanography Centre
> European Way, Southampton SO14 3ZH, UK
>  tel  023-8059-7743
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From Christoph.Scherber at uni-jena.de  Wed Feb 16 11:35:53 2005
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Wed, 16 Feb 2005 11:35:53 +0100
Subject: [R] some help interpreting ANOVA results, please?
In-Reply-To: <20050215213324.6d080865@zygiella.local>
References: <20041010183749.2f535f68@portia.local>	<Pine.LNX.4.44.0410101753210.29146-100000@gannet.stats>	<20041010195541.5f05817f@portia.local>
	<20050215213324.6d080865@zygiella.local>
Message-ID: <42132209.8070804@uni-jena.de>

Dear RenE,

Can you explain a bit more how you derive your T.SPart? That?s what I 
think is the tricky part of your analysis.

I would suggest you should try to end up with something like this:

model1<-aov(SR~WasSick*Time+Error(Subject/Time)
model2<-aov(SR~SC*Time+Error(Subject/Time)

This way it would be like a repeated measures ANOVA, where WasSick (or 
SC) are the primary covariates, and Time is nested within Subject.

I think the correct specification of "time" is crucial for the whole 
analysis. It?s like in a split-plot ANOVA, where finding the appropriate 
codings for plots of different sizes can sometimes take a very long time.

Regards,
Christoph


0) Subject, the subject identifier
1) physiological recordings, say SR (skin resistance): time series
2) a SessionPart variable (parts R1 and R2, separated in time by a pause)
3) time, T.SPart: normalised per subject and per SessionPart, so twice 0..1
4) a subjective sickness estimate (SC): time series
5) a per-subject classification: WasSick or not (available as a time series, but constant in time of course)



 

RenE J.V. Bertin wrote:

>On Sun, 10 Oct 2004 19:55:41 +0200, "RenE J.V. Bertin" <rjvbertin at hotmail.com> wrote regarding "Re:
>[R] some help interpreting ANOVA results, please?"
>
>I'm would like to come back to a question I posted quite a while ago, concerning the analysis of data of an ongoing experiment. I have, for a given number of subjects:
>0) Subject, the subject identifier
>1) physiological recordings, say SR (skin resistance): time series
>2) a SessionPart variable (parts R1 and R2, separated in time by a pause)
>3) time, T.SPart: normalised per subject and per SessionPart, so twice 0..1
>4) a subjective sickness estimate (SC): time series
>5) a per-subject classification: WasSick or not (available as a time series, but constant in time of course)
>
>I would like to make statements on whether or not sickness (measured by 4 or 5) can be deduced from the physiological recordings, e.g. something like
>  
>
>>aov( SR ~ WasSick * T.SPart )
>>    
>>
>
>expecting a significant effect of time (sickness building up), of WasSick, and a significant interaction showing that the effect is stronger (or only significant) in the WasSick=TRUE subjects. A simple t.test(SR~WasSick) gives a significant difference, as well as t.test( SR~ (T.SPart>=0.5) ) .
>
>The problem I'm having is that WasSick (and SC) are not independent variables properly speaking. So I cannot do
>
>  
>
>>aov( SR ~ WasSick * T.SPart + Error(Subject/WasSick*T.SPart) )
>>    
>>
>
>R would remove WasSick from the Error term, and do the analysis without it, giving a significant T.SPart effect and WasSick:T.SPart interaction (?), both listed under Error: Subject:T.SPart :
>Error: Subject:T.SPart
>                            Df Sum Sq Mean Sq F value   Pr(>F)    
>T.SPart                      5  318.2    63.6   8.336 7.46e-07 ***
>WasSick:T.SPart              5  125.5    25.1   3.289   0.0079 ** 
>Residuals                  129  984.9     7.6                     
>
>There is no trace of a WasSick effect other than in that interaction (of which I'm not sure it is truly one).
>
>
>
>I have 2 questions at this point:
>
>A) I think one could assimilate WasSick to a grouping variable (like in a clinical stdudy), forgetting it is actually an observation on the subjects. In that case, I could do
>  
>
>>aov( SR ~ WasSick * T.SPart )
>>    
>>
>which gives me the expected two significant main effects and the significant interaction (which agrees with visual inspection of the data).
>Is this an acceptable approach/model?
>
>B) Should I contine putting the Subject id in an Error term, e.g.
>  
>
>>aov( SR ~ WasSick + Error(Subject) )
>>    
>>
>WithOUT this error term, that anova gives a significant effect, confirming the t.test mentioned above. If I include the error term, the effect is no longer significant.
>Is that because the model does not make sense, rather because my data are so non-normal that a t.test cannot be used? (?Error has a similar model, and calls it "not particularly sensible statistically".)
>
>
>I would really appreciate some more constructive comments!
>Thanks,
>RenE Bertin
>
>PS: I must add that it has been suggested to try lme. I went over what docs I have (help and MASS 4), but these are far to specialistic for me, so I haven't gotten anywhere in that direction :(
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From ligges at statistik.uni-dortmund.de  Wed Feb 16 11:42:15 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 16 Feb 2005 11:42:15 +0100
Subject: [R] Easy cut & paste from Excel to R?
In-Reply-To: <x2d5v0iz8y.fsf@biostat.ku.dk>
References: <20050216085914.27179.qmail@web25806.mail.ukl.yahoo.com>	<42130F0C.1070905@statistik.uni-dortmund.de>
	<x2d5v0iz8y.fsf@biostat.ku.dk>
Message-ID: <42132387.8060109@statistik.uni-dortmund.de>

Peter Dalgaard wrote:

> Uwe Ligges <ligges at statistik.uni-dortmund.de> writes:
> 
> 
>>Werner Wernersen wrote:
>>
>>
>>>Hi!
>>>Is it possible to easily cut & paste data from an
>>>Excel spreadsheet to an R edit( ) grid or to variable?
>>>It seems that R cannot handle the cell delimiters
>>>Excel hands over.
>>>Regards,
>>>  Werner
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>copy in Excel and say in R:
>>read.table(file("clipboard"))
> 
> 
> Er, doesn't that want to be read.delim (or read.delim2 in
> "comma-locales")? Plain read.table() could cause some grief if there
> are empty cells.
> 


Well, yes, some arguments twisting might be required as for my german 
locales / german version of Excel the following works even for empty 
cells and real valued entries:

   read.table(file("clipboard"), sep="\t", dec=",")

    V1  V2  V3
1 1.2  NA 2.3
2 3.4 4.5 5.6


Uwe Ligges



From p.dalgaard at biostat.ku.dk  Wed Feb 16 11:42:38 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 16 Feb 2005 11:42:38 +0100
Subject: [R] Easy cut & paste from Excel to R?
In-Reply-To: <42132387.8060109@statistik.uni-dortmund.de>
References: <20050216085914.27179.qmail@web25806.mail.ukl.yahoo.com>
	<42130F0C.1070905@statistik.uni-dortmund.de>
	<x2d5v0iz8y.fsf@biostat.ku.dk>
	<42132387.8060109@statistik.uni-dortmund.de>
Message-ID: <x28y5oiy5d.fsf@biostat.ku.dk>

Uwe Ligges <ligges at statistik.uni-dortmund.de> writes:

> Well, yes, some arguments twisting might be required as for my german
> locales / german version of Excel the following works even for empty
> cells and real valued entries:
> 
>    read.table(file("clipboard"), sep="\t", dec=",")
> 
>     V1  V2  V3
> 1 1.2  NA 2.3
> 2 3.4 4.5 5.6

...which is of course the same as 

     read.delim2(file("clipboard"), header=FALSE)

except for possible variations in the fill and quote settings. (What
happens if you have empty cells in the last columns, or cells with
the text "Don't do this"?)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From murdoch at stats.uwo.ca  Wed Feb 16 11:47:29 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 16 Feb 2005 10:47:29 +0000
Subject: [R] real and complex vectors
In-Reply-To: <e412debaf2a7310c1a748e81c55514b6@soc.soton.ac.uk>
References: <e412debaf2a7310c1a748e81c55514b6@soc.soton.ac.uk>
Message-ID: <op86119ck7l7bhklu51qid6dgdditd3obm@4ax.com>

On Wed, 16 Feb 2005 10:00:01 +0000, Robin Hankin
<r.hankin at soc.soton.ac.uk> wrote :

>The following caught me off-guard:
>
>
>R> z <- 1i + 1:10
>R> z <- Re(z)
>R> z
>  [1]  1  2  3  4  5  6  7  8  9 10
>
>as expected.  But look:
>
>R> z <- 1i + 1:10
>R> make.real <- abs(z) < 1000
>R> z[make.real] <- Re(z[make.real])
>R> z
>  [1]  1+0i  2+0i  3+0i  4+0i  5+0i  6+0i  7+0i  8+0i  9+0i 10+0i
>R>
>
>didn't make z a real vector, which is what I wanted.  ?"[<-" says
>
>      If one of these expressions appears on the left side of an
>      assignment then that part of 'x' is set to the value of the right
>      hand side of the assignment.
>
>so the behaviour is as documented: class(z) is unchanged in the second 
>session.
>
>Would modifying "[<-" to add a test for all elements of an object being 
>replaced (and
>if this is the case  to change the class of z appropriately),  be a bad 
>idea?

I think it might be.  Think of a situation where make.real is almost
never all true.  Then z would almost always remain complex after

 z[make.real] <- Re(z[make.real])

but on rare occasions would switch to being real.  If some poor
programmer assumed that z was always complex, it would likely pass
tests, but on rare occasions would give garbage.  (Off the top of my
head I can't think of any cases where R code that expects a complex
vector would fail if passed a real one, but it's certainly easy to
construct cases in external code.)

I think it's safer to make the conversion explicitly if you happen to
know that all(make.real) is true.

Duncan Murdoch



From ligges at statistik.uni-dortmund.de  Wed Feb 16 11:54:55 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 16 Feb 2005 11:54:55 +0100
Subject: [R] Easy cut & paste from Excel to R?
In-Reply-To: <x28y5oiy5d.fsf@biostat.ku.dk>
References: <20050216085914.27179.qmail@web25806.mail.ukl.yahoo.com>	<42130F0C.1070905@statistik.uni-dortmund.de>	<x2d5v0iz8y.fsf@biostat.ku.dk>	<42132387.8060109@statistik.uni-dortmund.de>
	<x28y5oiy5d.fsf@biostat.ku.dk>
Message-ID: <4213267F.2070903@statistik.uni-dortmund.de>

Peter Dalgaard wrote:

> Uwe Ligges <ligges at statistik.uni-dortmund.de> writes:
> 
> 
>>Well, yes, some arguments twisting might be required as for my german
>>locales / german version of Excel the following works even for empty
>>cells and real valued entries:
>>
>>   read.table(file("clipboard"), sep="\t", dec=",")
>>
>>    V1  V2  V3
>>1 1.2  NA 2.3
>>2 3.4 4.5 5.6
> 
> 
> ...which is of course the same as 
> 
>      read.delim2(file("clipboard"), header=FALSE)
> 
> except for possible variations in the fill and quote settings. (What
> happens if you have empty cells in the last columns, or cells with
> the text "Don't do this"?)
> 

Yes, you are right, thanks!

Uwe



From Allan at STATS.uct.ac.za  Wed Feb 16 11:58:33 2005
From: Allan at STATS.uct.ac.za (Clark Allan)
Date: Wed, 16 Feb 2005 12:58:33 +0200
Subject: [R] R: ridge regression
Message-ID: <42132759.37F58802@STATS.uct.ac.za>

hi all

a technical question for those bright statisticians.

my question involves ridge regression.

definition:

n=sample size of a data set

X is the matrix of data with , say p variables

Y is the y matrix i.e the response variable

Z(i,j) =  ( X(i,j)- xbar(j) / [ (n-1)^0.5* std(x(j))]

Y_new(i)=( Y(i)- ybar(j) ) / [ (n-1)^0.5* std(Y(i))]	(note that i have
scaled the Y matrix as well)

k is the ridge constant

the ridge estimate for the betas is = inverse(Z'Z+kI)*Z'Y_new=W*Z'Y_new

the associated variance covariance matrix sigma*W*(Z'Z)*W	where sigma is
the residual variance based on the transformed variables

if we transform the variables back to the original variables the beta
estimates are now: beta(j)= std(y)*betaridge(j)/std(x(j))

but what is the covariance matrix of these estimates???

i know that this might not be the correct forum for this question, but
since i know that many users are statisticians i know that i will get an
informed response.

From slist at oomvanlieshout.net  Wed Feb 16 12:20:37 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Wed, 16 Feb 2005 13:20:37 +0200
Subject: [R] Repeating grey scale in graph?
Message-ID: <42132C85.1080003@oomvanlieshout.net>

Dear R users,

Could somebody tell me why the grey color ramp is repeated in this 
graph, eventhough the ramp values go from 0 to 1? I must be missing 
something obvious, but I can not see it!

z <- 
c(0.064329041,0.117243316,0.161565116,0.19923015,0.231642175,0.259835539,0.284571226,
0.038507288,0.094184749,0.140959431,0.180803984,0.215159105,0.245096084,0.271412845,
0.00775022,0.066198255,0.115433207,0.157494219,0.193836765,0.225569076,0.253518629,
-0.02820814,0.032958752,0.084661362,0.128946221,0.167320522,0.200892494,0.230504392,
-0.07003273,-0.005814512,0.048304039,0.094805358,0.135196637,0.170630435,0.201956395,
-0.117878701,-0.050461393,0.005991829,0.054672666,0.097103088,0.134398711,0.167423957)

x <- c(0,1,2,3,4,5)
y <- c(50, 100, 150, 200, 250, 300, 350)
z <- matrix(z, nrow=length(x), ncol=length(y), byrow=TRUE)

#persp(x, y, z, theta = 30, phi = 30, expand = 0.5,
#  box= TRUE, axes= TRUE, ticktype = "detailed", main="Title of plot")

hgt <- (z - min(z))/ (max(z) - min(z))
z
hgt
cols <- grey(hgt)
persp(x, y, z, col = cols, theta = 30, phi = 30, expand = 0.5,
   box= TRUE, axes= TRUE, ticktype = "detailed", main="Title of plot")


Thanks,

Sander.

 > version
          _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    2
minor    0.1
year     2004
month    11
day      15
language R
 >


-- 
---------------------------------------------------------
Dr. Sander P. Oom
Animal, Plant and Environmental Sciences
University of the Witwatersrand
Private Bag 3
Wits 2050
South Africa

Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64

Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From solares at unsl.edu.ar  Wed Feb 16 12:34:35 2005
From: solares at unsl.edu.ar (solares@unsl.edu.ar)
Date: Wed, 16 Feb 2005 08:34:35 -0300 (ARGSL-ST)
Subject: [R] curve(x,y)?
Message-ID: <52525.64.76.125.131.1108553675.squirrel@inter17.unsl.edu.ar>

HI, i search for a function what plot a curve but my function f(x) have
two variables, exist a funcion curve(x,y) in R. Thanks



From andy_liaw at merck.com  Wed Feb 16 12:47:30 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 16 Feb 2005 06:47:30 -0500
Subject: [R] R: ridge regression
Message-ID: <3A822319EB35174CA3714066D590DCD50994E6FC@usrymx25.merck.com>

If I'm not mistaken, you only need to know that if V is the covariance
matrix of a random vector X, then the covariance of the linear
transformation AX + b is AVA'.  Substitute betahat for X, and figure out
what A is and you're set.  (b is 0 in your case.)

Andy



> From: Clark Allan
> 
> hi all
> 
> a technical question for those bright statisticians.
> 
> my question involves ridge regression.
> 
> definition:
> 
> n=sample size of a data set
> 
> X is the matrix of data with , say p variables
> 
> Y is the y matrix i.e the response variable
> 
> Z(i,j) =  ( X(i,j)- xbar(j) / [ (n-1)^0.5* std(x(j))]
> 
> Y_new(i)=( Y(i)- ybar(j) ) / [ (n-1)^0.5* std(Y(i))]	(note 
> that i have
> scaled the Y matrix as well)
> 
> k is the ridge constant
> 
> the ridge estimate for the betas is = 
> inverse(Z'Z+kI)*Z'Y_new=W*Z'Y_new
> 
> the associated variance covariance matrix sigma*W*(Z'Z)*W	
> where sigma is
> the residual variance based on the transformed variables
> 
> if we transform the variables back to the original variables the beta
> estimates are now: beta(j)= std(y)*betaridge(j)/std(x(j))
> 
> but what is the covariance matrix of these estimates???
> 
> i know that this might not be the correct forum for this question, but
> since i know that many users are statisticians i know that i 
> will get an
> informed response.
>



From nicole.raschun at chello.at  Wed Feb 16 12:54:42 2005
From: nicole.raschun at chello.at (Nicole)
Date: Wed, 16 Feb 2005 12:54:42 +0100
Subject: [R] phi correlation
Message-ID: <20050216115418.HNTZ2566.viefep11-int.chello.at@home1>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050216/52ea6787/attachment.pl

From Terji78 at yahoo.com  Wed Feb 16 13:08:30 2005
From: Terji78 at yahoo.com (Terji Petersen)
Date: Wed, 16 Feb 2005 13:08:30 +0100
Subject: [R] Setting log(0) to 0
Message-ID: <421337BE.4020300@yahoo.com>

Hi,

I'm trying to do  a regression like this:

wage.r = lm( log(WAGE) ~ log(EXPER)

where EXPER is an integer that goes from 0 to about 50. EXPER contains 
some zeros, so you can't take its log, and the above regression 
therefore fails. I would like to make R accept log(0) as 0, is that 
possible? Or do I have first have to turn the 0's into 1's to be able to 
do the above regression?

Regards

T Petersen



From Luisr at frs.fo  Wed Feb 16 13:26:11 2005
From: Luisr at frs.fo (Luis Ridao Cruz)
Date: Wed, 16 Feb 2005 12:26:11 +0000
Subject: [R] Lattice --reference line in panels does not come up
Message-ID: <s2133bef.068@ffdata.setur.fo>

R-help,

I am quite new to lattice.
I am plotting something in which I want some reference lines.
I do the foolowing :

library ( lattice )
reference.line <- trellis.par.get ( "reference.line" ) 
reference.line$lty <- 2    ## not working with any of the
reference.line components
# reference.line$col <- "red" 
trellis.par.set("reference.line", reference.line)
xyplot ( number ~ cm | year , data = lgda., type = "l", col = "black" ,
ylab = "Number", xlab = "Length (cm) ") ## the actual plot

The result is without reference lines.

What am I doing wrong?


Thanks in advance



From ligges at statistik.uni-dortmund.de  Wed Feb 16 13:30:58 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 16 Feb 2005 13:30:58 +0100
Subject: [R] Repeating grey scale in graph?
In-Reply-To: <42132C85.1080003@oomvanlieshout.net>
References: <42132C85.1080003@oomvanlieshout.net>
Message-ID: <42133D02.9000205@statistik.uni-dortmund.de>

Sander Oom wrote:

> Dear R users,
> 
> Could somebody tell me why the grey color ramp is repeated in this 
> graph, eventhough the ramp values go from 0 to 1? I must be missing 
> something obvious, but I can not see it!

You missed to read the ehlp page ?persp:

"col: the color(s) of the surface facets. Transparent colours are 
ignored. This is recycled to the (nx-1)(ny-1) facets."

Attention: (nx-1)x(ny-1) facets, but not nx x ny ....

Uwe Ligges



> 
> z <- 
> c(0.064329041,0.117243316,0.161565116,0.19923015,0.231642175,0.259835539,0.284571226, 
> 
> 0.038507288,0.094184749,0.140959431,0.180803984,0.215159105,0.245096084,0.271412845, 
> 
> 0.00775022,0.066198255,0.115433207,0.157494219,0.193836765,0.225569076,0.253518629, 
> 
> -0.02820814,0.032958752,0.084661362,0.128946221,0.167320522,0.200892494,0.230504392, 
> 
> -0.07003273,-0.005814512,0.048304039,0.094805358,0.135196637,0.170630435,0.201956395, 
> 
> -0.117878701,-0.050461393,0.005991829,0.054672666,0.097103088,0.134398711,0.167423957) 
> 
> 
> x <- c(0,1,2,3,4,5)
> y <- c(50, 100, 150, 200, 250, 300, 350)
> z <- matrix(z, nrow=length(x), ncol=length(y), byrow=TRUE)
> 
> #persp(x, y, z, theta = 30, phi = 30, expand = 0.5,
> #  box= TRUE, axes= TRUE, ticktype = "detailed", main="Title of plot")
> 
> hgt <- (z - min(z))/ (max(z) - min(z))
> z
> hgt
> cols <- grey(hgt)
> persp(x, y, z, col = cols, theta = 30, phi = 30, expand = 0.5,
>   box= TRUE, axes= TRUE, ticktype = "detailed", main="Title of plot")
> 
> 
> Thanks,
> 
> Sander.
> 
>  > version
>          _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    2
> minor    0.1
> year     2004
> month    11
> day      15
> language R
>  >
> 
>



From p.dalgaard at biostat.ku.dk  Wed Feb 16 13:28:09 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 16 Feb 2005 13:28:09 +0100
Subject: [R] Repeating grey scale in graph?
In-Reply-To: <42132C85.1080003@oomvanlieshout.net>
References: <42132C85.1080003@oomvanlieshout.net>
Message-ID: <x24qgcit9i.fsf@biostat.ku.dk>

Sander Oom <slist at oomvanlieshout.net> writes:

> Dear R users,
> 
> Could somebody tell me why the grey color ramp is repeated in this
> graph, eventhough the ramp values go from 0 to 1? I must be missing
> something obvious, but I can not see it!
> 
> z <-
> c(0.064329041,0.117243316,0.161565116,0.19923015,0.231642175,0.259835539,0.284571226,
> 0.038507288,0.094184749,0.140959431,0.180803984,0.215159105,0.245096084,0.271412845,
> 0.00775022,0.066198255,0.115433207,0.157494219,0.193836765,0.225569076,0.253518629,
> -0.02820814,0.032958752,0.084661362,0.128946221,0.167320522,0.200892494,0.230504392,
> -0.07003273,-0.005814512,0.048304039,0.094805358,0.135196637,0.170630435,0.201956395,
> -0.117878701,-0.050461393,0.005991829,0.054672666,0.097103088,0.134398711,0.167423957)
> 
> x <- c(0,1,2,3,4,5)
> y <- c(50, 100, 150, 200, 250, 300, 350)
> z <- matrix(z, nrow=length(x), ncol=length(y), byrow=TRUE)
> 
> #persp(x, y, z, theta = 30, phi = 30, expand = 0.5,
> #  box= TRUE, axes= TRUE, ticktype = "detailed", main="Title of plot")
> 
> hgt <- (z - min(z))/ (max(z) - min(z))
> z
> hgt
> cols <- grey(hgt)
> persp(x, y, z, col = cols, theta = 30, phi = 30, expand = 0.5,
>    box= TRUE, axes= TRUE, ticktype = "detailed", main="Title of plot")
> 

You have 30 facets and 42 colour values. Try it with

cols <- grey(hgt[-1,-1])


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ligges at statistik.uni-dortmund.de  Wed Feb 16 13:37:24 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 16 Feb 2005 13:37:24 +0100
Subject: [R] curve(x,y)?
In-Reply-To: <52525.64.76.125.131.1108553675.squirrel@inter17.unsl.edu.ar>
References: <52525.64.76.125.131.1108553675.squirrel@inter17.unsl.edu.ar>
Message-ID: <42133E84.8000706@statistik.uni-dortmund.de>

solares at unsl.edu.ar wrote:

> HI, i search for a function what plot a curve but my function f(x) have
> two variables, exist a funcion curve(x,y) in R. Thanks
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


Not exactly, but quite easy to do it yourself, example:


   f <- function(x, y) x+y
   x <- 1:10
   y <- 1:10
   z <- outer(x, y, f)
   persp(x, y, z)


Uwe Ligges



From Allan at STATS.uct.ac.za  Wed Feb 16 13:38:44 2005
From: Allan at STATS.uct.ac.za (Clark Allan)
Date: Wed, 16 Feb 2005 14:38:44 +0200
Subject: [R] R: ridge regression
References: <3A822319EB35174CA3714066D590DCD50994E6FC@usrymx25.merck.com>
Message-ID: <42133ED4.736DB974@STATS.uct.ac.za>

hi Andy and other r users

i never gave the full picture. 

beta(j)= std(y)*betaridge(j)/std(x(j))	for j=1,2,...p

but beta(0) = ybar- sum( i= 1 to p, betaridge(i)*xbar(j) )

note that ybar and the xbars are estimated parameters.

we can split the covariance matrix into three sections namely:

1. var(beta(0))
2. covar(beta(0), other betas) and
3. covar(other betas)	, (this is your answer, which was correct)

but now i need var(beta(0)) and covar(beta(0), other betas)

any suggestions!




"Liaw, Andy" wrote:
> 
> If I'm not mistaken, you only need to know that if V is the covariance
> matrix of a random vector X, then the covariance of the linear
> transformation AX + b is AVA'.  Substitute betahat for X, and figure out
> what A is and you're set.  (b is 0 in your case.)
> 
> Andy
> 
> > From: Clark Allan
> >
> > hi all
> >
> > a technical question for those bright statisticians.
> >
> > my question involves ridge regression.
> >
> > definition:
> >
> > n=sample size of a data set
> >
> > X is the matrix of data with , say p variables
> >
> > Y is the y matrix i.e the response variable
> >
> > Z(i,j) =  ( X(i,j)- xbar(j) / [ (n-1)^0.5* std(x(j))]
> >
> > Y_new(i)=( Y(i)- ybar(j) ) / [ (n-1)^0.5* std(Y(i))]  (note
> > that i have
> > scaled the Y matrix as well)
> >
> > k is the ridge constant
> >
> > the ridge estimate for the betas is =
> > inverse(Z'Z+kI)*Z'Y_new=W*Z'Y_new
> >
> > the associated variance covariance matrix sigma*W*(Z'Z)*W
> > where sigma is
> > the residual variance based on the transformed variables
> >
> > if we transform the variables back to the original variables the beta
> > estimates are now: beta(j)= std(y)*betaridge(j)/std(x(j))
> >
> > but what is the covariance matrix of these estimates???
> >
> > i know that this might not be the correct forum for this question, but
> > since i know that many users are statisticians i know that i
> > will get an
> > informed response.
> >
> 
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachment...{{dropped}}


From bernd.weiss at uni-koeln.de  Wed Feb 16 13:48:57 2005
From: bernd.weiss at uni-koeln.de (Bernd Weiss)
Date: Wed, 16 Feb 2005 13:48:57 +0100
Subject: [R] phi correlation
In-Reply-To: <20050216115418.HNTZ2566.viefep11-int.chello.at@home1>
Message-ID: <42134F49.9269.138C9BB@localhost>

On 16 Feb 2005 at 12:54, Nicole wrote:

> Hello my big problem is, i can?t find the phi-correlation instruction
> in the R -  programm. (correlation method= spearman, pearson, kendall,
>  I have found)

See package "vcd", command "assoc.stats".
 
> I also cant find the transform instruction which I can transform
> rational vector into nominal vectors (binary)

What about using "as.factor"? 

# little example...
x<-c(0,1,0,0,0,1,1)
y<-as.factor(x)
y
assoc.stats(table(y,y))

HTH,

Bernd



From ligges at statistik.uni-dortmund.de  Wed Feb 16 13:57:55 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 16 Feb 2005 13:57:55 +0100
Subject: [R] phi correlation
In-Reply-To: <20050216115418.HNTZ2566.viefep11-int.chello.at@home1>
References: <20050216115418.HNTZ2566.viefep11-int.chello.at@home1>
Message-ID: <42134353.8060703@statistik.uni-dortmund.de>

Nicole wrote:

> Hello my big problem is, i can?t find the phi-correlation instruction in the
> R -  programm. (correlation method= spearman, pearson, kendall,  I have
> found)
> 
>  
> 
> I also cant find the transform instruction which I can transform rational
> vector into nominal vectors (binary)
> 
> Transforming into ordinaI I  have found with the "rank" instruction, but I
> have no found into nominal dates. 
> 
>  
> 
>  
> 
> Please help me .
> 
> I need it for an examination !!!

So what? Time to do your homework, isn't it?

phi: it is the same as pearson for a 2x2 table of dichotomous/binary 
data. Should be written in any basic textbook on descriptive statsitics. 
Do your homework!!! I really dislike these triple exclamation marks.

nominal: Well, try factor() or as.character(), or whatever .... depends 
on your aims....

Who are you? "Adolf Pessernig", or "nicole.raschun"???

Uwe Ligges



>  
> 
> Thank you Adolf Pessernig
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From drewbrewit at yahoo.com  Wed Feb 16 14:08:36 2005
From: drewbrewit at yahoo.com (Nick Drew)
Date: Wed, 16 Feb 2005 05:08:36 -0800 (PST)
Subject: [R] Easy cut & paste from Excel to R?
Message-ID: <20050216130836.88537.qmail@web50901.mail.yahoo.com>

I've had good luck with the scan() function when I
want to get a few numbers from Excel into R quickly to
use it as a calculator. CAVEAT: you have to have the
numbers you want to copy in a column not a row in
Excel. For example:

In Excel your data are in a column as follows:
Col A
1
2
3

Then copy the 3 cells (e.g. 1, 2,3) in Excel and open
R and type in:
> data <- scan()

Then Paste using Ctrl-V. Hit the Enter key. You know
have an object called "data" that you can use and
manipulate in R.

I've taken this even further by creating an R function
that will take a column of numbers from Excel and then
scan() them into R, create a matrix, and then perform
a Chi-square test. Let me know if you'd like to know
more. I'm a beginner and if I can do so can you!!

~Nick



From cafanselmo12 at yahoo.com.br  Wed Feb 16 14:11:54 2005
From: cafanselmo12 at yahoo.com.br (=?iso-8859-1?q?C=E9zar=20Freitas?=)
Date: Wed, 16 Feb 2005 10:11:54 -0300 (ART)
Subject: [R] histogram and boxplot in a same postscript
Message-ID: <20050216131154.48241.qmail@web14525.mail.yahoo.com>

Hi, all. I need plot a boxplot under a histogram like
below, but some configs are troubled:

- the boxplot contours the plot, even if I put
bty="n", modifying the histogram plot;
- I changed the configs of axis to do a 3x3 inches
plot, but the result is 2 different axis.

For example, the code below ilustrates this...


  scores<-c(2.0, 0.0, 5.0, 5.0, 5.0, 2.0, 0.0, 5.0,
2.5, 4.0, 5.0, 0.0, 5.0, 0.0, 2.0, 5.0, 5.0, 2.0, 3.0,
3.0)

  postscript("test.ps", width=3, height=3,
horizontal=FALSE, family="Times", paper="special")

  gra<-hist(scores, breaks=((0:11)/2-.2),
xlim=c(-1,6), plot=FALSE, cex.axis=.5, cex.main=.5,
cex.sub=.5, cex.lab=.5, mgp=c(1.5,.5,0))

  yy<-ceiling(max(gra$counts)/10)*10
  
  hist(scores, breaks=((0:11)/2-.2), xlim=c(-1,6),
ylim=c(0,yy), main="scores", ylab="Freq", xlab="math",
cex.axis=.5, cex.main=.5, cex.sub=.5, cex.lab=.5,
mgp=c(1.5,.5,0))
  boxplot(scores, horizontal=1, add=1, at=-yy/50,
boxwex=yy/20, bty="n", cex.axis=.5, cex.main=.5,
cex.sub=.5, cex.lab=.5, ann=FALSE)
  points(q[4], -yy/50, col=1, pch="x", cex=.2)

  dev.off()


Thanks,
C.



From ligges at statistik.uni-dortmund.de  Wed Feb 16 14:21:27 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 16 Feb 2005 14:21:27 +0100
Subject: [R] Lattice --reference line in panels does not come up
In-Reply-To: <s2133bef.068@ffdata.setur.fo>
References: <s2133bef.068@ffdata.setur.fo>
Message-ID: <421348D7.2050506@statistik.uni-dortmund.de>

Luis Ridao Cruz wrote:

> R-help,
> 
> I am quite new to lattice.
> I am plotting something in which I want some reference lines.
> I do the foolowing :
> 
> library ( lattice )
> reference.line <- trellis.par.get ( "reference.line" ) 
> reference.line$lty <- 2    ## not working with any of the
> reference.line components
> # reference.line$col <- "red" 
> trellis.par.set("reference.line", reference.line)
> xyplot ( number ~ cm | year , data = lgda., type = "l", col = "black" ,
> ylab = "Number", xlab = "Length (cm) ") ## the actual plot
> 
> The result is without reference lines.
> 
> What am I doing wrong?


You have forgotten to specify panel.grid() in your call, which is the 
only function accepting reference line settings, AFAIK. The following 
should do the trick:

  xyplot(.....,
    panel = function(x, y, ...){
      panel.xyplot(x, y, ...)
      panel.grid()
    }
  )



Uwe Ligges




> 
> Thanks in advance
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From tom_colson at ncsu.edu  Wed Feb 16 14:19:53 2005
From: tom_colson at ncsu.edu (Thomas Colson)
Date: Wed, 16 Feb 2005 08:19:53 -0500
Subject: [R] RE: Off topic -- large data sets. Experiences using R on
	clusters/Grids
In-Reply-To: <1mXEVIA3txECFwmF@visiv.co.uk>
Message-ID: <200502161320.j1GDK0jc024021@uni09mr.unity.ncsu.edu>

Thanks for all the input. Now to go further off topic..

Does anyone have any comments regarding running 64 BIT R on cluster/grid
systems? Given an (almost) unlimited amount of memory, can R hypotheticaly
handle Very Large Datasets? 

I'm finding that even small sub sets of this data come in at 1 GB (1-5
million rows), which no R 32 BIT workstation (at least in this lab) can
handle. 


This type of stuff is done effortlessly in genomic research, mapping DNA,
etc.... 


Tom Colson
Center for Earth Observation
North Carolina State University 
Raleigh, NC 27695
(919) 515 3434
(919) 673 8023
tom_colson at ncsu.edu

Online Calendar:
http://www4.ncsu.edu/~tpcolson



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Graham Jones
Sent: Wednesday, February 16, 2005 5:08 AM
To: Prof Brian Ripley
Cc: r-help at stat.math.ethz.ch
Subject: Re: Off topic -- large data sets. Was RE: [R] 64 Bit R
BackgroundQuestion

In message <Pine.LNX.4.61.0502151735100.31845 at gannet.stats>, Prof Brian
Ripley <ripley at stats.ox.ac.uk> writes

>But Bert's caveats apply: you have 200 problems of size 20,000 since in 
>QDA each class's distribution is estimated separately, and a single 
>pass will give you the sufficient statistics however large the dataset is.
>

I think we've interpreted Bert's question differently. I am not saying I
need to have vast amounts of data in RAM, or in a single data structure, or
anything like that, and I am not saying I need a 64-bit version of R.
What I am saying is that if I had 40 million cases for a problem like the
one I described, I'd want to use all of them when designing a classifier.

Patrick Burns, if you're reading: OCR = optical character recognition.

--
Graham Jones, author of SharpEye Music Reader http://www.visiv.co.uk 21e
Balnakeil, Durness, Lairg, Sutherland, IV27 4PT, Scotland, UK

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

From Luisr at frs.fo  Wed Feb 16 14:24:18 2005
From: Luisr at frs.fo (Luis Ridao Cruz)
Date: Wed, 16 Feb 2005 13:24:18 +0000
Subject: [R] Lattice --reference line in panels does not come up
Message-ID: <s213498a.098@ffdata.setur.fo>

Thank you Uwe,

It worked !

Luis

>>> Uwe Ligges <ligges at statistik.uni-dortmund.de> 16/02/2005 13:21:27
>>>
Luis Ridao Cruz wrote:

> R-help,
> 
> I am quite new to lattice.
> I am plotting something in which I want some reference lines.
> I do the foolowing :
> 
> library ( lattice )
> reference.line <- trellis.par.get ( "reference.line" ) 
> reference.line$lty <- 2    ## not working with any of the
> reference.line components
> # reference.line$col <- "red" 
> trellis.par.set("reference.line", reference.line)
> xyplot ( number ~ cm | year , data = lgda., type = "l", col = "black"
,
> ylab = "Number", xlab = "Length (cm) ") ## the actual plot
> 
> The result is without reference lines.
> 
> What am I doing wrong?


You have forgotten to specify panel.grid() in your call, which is the 
only function accepting reference line settings, AFAIK. The following 
should do the trick:

  xyplot(.....,
    panel = function(x, y, ...){
      panel.xyplot(x, y, ...)
      panel.grid()
    }
  )



Uwe Ligges




> 
> Thanks in advance
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Ted.Harding at nessie.mcc.ac.uk  Wed Feb 16 14:26:53 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 16 Feb 2005 13:26:53 -0000 (GMT)
Subject: [R] Setting log(0) to 0
In-Reply-To: <421337BE.4020300@yahoo.com>
Message-ID: <XFMail.050216132653.Ted.Harding@nessie.mcc.ac.uk>

On 16-Feb-05 Terji Petersen wrote:
> Hi,
> 
> I'm trying to do  a regression like this:
> 
> wage.r = lm( log(WAGE) ~ log(EXPER)
> 
> where EXPER is an integer that goes from 0 to about 50.
> EXPER contains some zeros, so you can't take its log,
> and the above regression therefore fails. I would like
> to make R accept log(0) as 0, is that  possible?
> Or do I have first have to turn the 0's into 1's to be
> able to do the above regression?

If treating "log(0)" as 0 would do your business, then a
preliminary pass to turn "0" into "1" would be the simplest
method. It only takes 1 line.

This is a bit of a Catch-22. It looks like you're trying
to fit a power law

  WAGE = A*(EXPER^B)

(where I guess "EXPER" means experience) and you've got
some cases with no experience. Whether your work-round
is appropriate depends in part on the unit of "experience".
If it's in years, then a case with 3 months experience
would have log(EXPER) = -1.39, thereby weighing in with
a lesser value than someone with zero experience, on your
proposal.

On the other hand, if it's in days, then

  log(EXPER) = log(91) = 4.51

and even someone with only a week has log(EXPER) = 1.95

But your log(0) = 0 data would be sitting there all the
time, whatever the scale of EXPER, and so would have an
influence on your regression which depended on this scale.
You might have to consider using log(0) --> const
where the "const" is such as to give reasonable results,
given what comes out of the rest of the data (where EXPER>0).

The fundamental problem is that your power law predicts
zero wage for zero experience, which is rarely the case.

You might do better to try a non-linear fit

  WAGE = W0 + A*(EXPER^B)

for which sort of thing there are several resources in R,
perhaps the simplest being 'nls'.

For what you have in your installed packages, try a

  help.search("nonlinear")

Once you open this door, you can try perhaps more realistic
non-linear models, including what can be found amongst the
"SS....." (Self-Starting) models in "nls" --  have a look
at what's listed by

  library(help=nls)

as well as what is allowed according to "?nls".

Such models would allow an initial (zero-experience) wage,
perhaps not changing much for some time, then rising more
rapidly once an "experience threshold" is passed, then
flattening out to a lower slope over a longer time (something
which many of us have experience of). And even ultimately
ending to decrease ...

Hoping this helps,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 16-Feb-05                                       Time: 13:26:53
------------------------------ XFMail ------------------------------



From slist at oomvanlieshout.net  Wed Feb 16 14:44:00 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Wed, 16 Feb 2005 15:44:00 +0200
Subject: [R] Repeating grey scale in graph?
In-Reply-To: <x24qgcit9i.fsf@biostat.ku.dk>
References: <42132C85.1080003@oomvanlieshout.net>
	<x24qgcit9i.fsf@biostat.ku.dk>
Message-ID: <42134E20.9050000@oomvanlieshout.net>

Thanks Peter!

Of course I only have (nx-1)(ny-1) facets in a x*y plot!

The help page line:
...
col 	the color(s) of the surface facets. Transparent colours are 
ignored. This is recycled to the (nx-1)(ny-1) facets.
...
just did not ring a bell.

In fact, it is still not clear to me why it recycles the ramp even 
though it has a surplus of colours (grey levels)! Why not just ignore 
the surplus colours?

Anyway it works,

Sander.


Peter Dalgaard wrote:
> Sander Oom <slist at oomvanlieshout.net> writes:
> 
> 
>>Dear R users,
>>
>>Could somebody tell me why the grey color ramp is repeated in this
>>graph, eventhough the ramp values go from 0 to 1? I must be missing
>>something obvious, but I can not see it!
>>
>>z <-
>>c(0.064329041,0.117243316,0.161565116,0.19923015,0.231642175,0.259835539,0.284571226,
>>0.038507288,0.094184749,0.140959431,0.180803984,0.215159105,0.245096084,0.271412845,
>>0.00775022,0.066198255,0.115433207,0.157494219,0.193836765,0.225569076,0.253518629,
>>-0.02820814,0.032958752,0.084661362,0.128946221,0.167320522,0.200892494,0.230504392,
>>-0.07003273,-0.005814512,0.048304039,0.094805358,0.135196637,0.170630435,0.201956395,
>>-0.117878701,-0.050461393,0.005991829,0.054672666,0.097103088,0.134398711,0.167423957)
>>
>>x <- c(0,1,2,3,4,5)
>>y <- c(50, 100, 150, 200, 250, 300, 350)
>>z <- matrix(z, nrow=length(x), ncol=length(y), byrow=TRUE)
>>
>>#persp(x, y, z, theta = 30, phi = 30, expand = 0.5,
>>#  box= TRUE, axes= TRUE, ticktype = "detailed", main="Title of plot")
>>
>>hgt <- (z - min(z))/ (max(z) - min(z))
>>z
>>hgt
>>cols <- grey(hgt)
>>persp(x, y, z, col = cols, theta = 30, phi = 30, expand = 0.5,
>>   box= TRUE, axes= TRUE, ticktype = "detailed", main="Title of plot")
>>
> 
> 
> You have 30 facets and 42 colour values. Try it with
> 
> cols <- grey(hgt[-1,-1])
> 
> 

-- 
---------------------------------------------------------
Dr. Sander P. Oom
Animal, Plant and Environmental Sciences
University of the Witwatersrand
Private Bag 3
Wits 2050
South Africa

Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64

Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From Yongchao.Ge at mssm.edu  Wed Feb 16 14:46:57 2005
From: Yongchao.Ge at mssm.edu (Yongchao Ge)
Date: Wed, 16 Feb 2005 08:46:57 -0500 (EST)
Subject: [R] Profiling R code and C code (Rprof and gprof)
Message-ID: <Pine.LNX.4.44.0502160804590.11791-100000@ge.biomath.mssm.edu>

Hi,

I have searched R mail list archive and couldn't find my answers. The R 
extension describes how to make use of Rprof to profile R code. 
gprof can be also used for the same purpose for the 
C codes when the C codes are written independently and provided with a 
main() function.

I'm currently writing R codes meshed with C Codes, and use .Call as the 
interface between the two parts. Rprof reports details of each R 
functions, but does not report the details of the C functions.

I'm wondering if there is a way such that Rprof can report the detials of  
each C functions as did in gprof. 

One may suggest I can compile all of the C codes and write a main 
function, and then use the gprof. That's definitely true before I moved from .C to 
.Call. Since now the R codes and C codes are meshed in the same program, 
it does not seem a trivial job to separate the two parts neatly.

We can use "R -d gdb" to debug C codes meshed with R codes. If we can also 
profile the C functions called by the R codes, then it will much be 
productive to write useful C codes.

Any suggestions how to use Rprof and gprof to help me to spot a C 
function which I can work on to speed up my program will be much 
appreciated.

Thanks,

Yongchao



From laura at env.leeds.ac.uk  Wed Feb 16 14:56:42 2005
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Wed, 16 Feb 2005 13:56:42 +0000 (GMT)
Subject: [R] Passing colnames to graphics title
Message-ID: <Pine.LNX.4.44.0502161354480.4164-100000@env-pc-phd13>

Hi,

Just a quick query - if I'm creating a function to produce a number of
histograms per page of output (one per column from a matrix), how can I
pass the column name of the matrix into the title (or indeed to form part
of the x-axis label)?

TIA,
Laura

Laura Quinn
Institute of Atmospheric Science
School of Earth and Environment
University of Leeds
Leeds
LS2 9JT

tel: +44 113 343 1596
fax: +44 113 343 6716
mail: laura at env.leeds.ac.uk



From bates at stat.wisc.edu  Wed Feb 16 15:00:04 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 16 Feb 2005 08:00:04 -0600
Subject: [R] Lattice --reference line in panels does not come up
In-Reply-To: <421348D7.2050506@statistik.uni-dortmund.de>
References: <s2133bef.068@ffdata.setur.fo>
	<421348D7.2050506@statistik.uni-dortmund.de>
Message-ID: <421351E4.8070805@stat.wisc.edu>

Uwe Ligges wrote:
> Luis Ridao Cruz wrote:
> 
>> R-help,
>>
>> I am quite new to lattice.
>> I am plotting something in which I want some reference lines.
>> I do the foolowing :
>>
>> library ( lattice )
>> reference.line <- trellis.par.get ( "reference.line" ) 
>> reference.line$lty <- 2    ## not working with any of the
>> reference.line components
>> # reference.line$col <- "red" trellis.par.set("reference.line", 
>> reference.line)
>> xyplot ( number ~ cm | year , data = lgda., type = "l", col = "black" ,
>> ylab = "Number", xlab = "Length (cm) ") ## the actual plot
>>
>> The result is without reference lines.
>>
>> What am I doing wrong?
> 
> 
> 
> You have forgotten to specify panel.grid() in your call, which is the 
> only function accepting reference line settings, AFAIK. The following 
> should do the trick:
> 
>  xyplot(.....,
>    panel = function(x, y, ...){
>      panel.xyplot(x, y, ...)
>      panel.grid()
>    }
>  )

Or, in recent versions of the lattice package,

  xyplot(..., type = c("g", "p"))



From jgu at codan.dk  Wed Feb 16 15:08:06 2005
From: jgu at codan.dk (Jim Gustafsson)
Date: Wed, 16 Feb 2005 15:08:06 +0100
Subject: [R] (no subject)
Message-ID: <OFC470C642.0219955F-ONC1256FAA.004D6E94@codan.dk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050216/f7125501/attachment.pl

From uofiowa at gmail.com  Wed Feb 16 15:31:30 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Wed, 16 Feb 2005 09:31:30 -0500
Subject: [R] Informix from Mac
Message-ID: <3f87cc6d0502160631494482c1@mail.gmail.com>

I am using R 2.0.1 on Max OS X. The database is Informix. There is no
Informix ODBC client for Mac, as far as I know, so I can't use RODBC.
Other than going with SJava/JDBC, is there a way for me to connect to
the database?



From Achim.Zeileis at wu-wien.ac.at  Wed Feb 16 15:35:19 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed, 16 Feb 2005 15:35:19 +0100
Subject: [R] Repeating grey scale in graph?
In-Reply-To: <42134E20.9050000@oomvanlieshout.net>
References: <42132C85.1080003@oomvanlieshout.net>
	<x24qgcit9i.fsf@biostat.ku.dk>
	<42134E20.9050000@oomvanlieshout.net>
Message-ID: <20050216153519.4827a3cd.Achim.Zeileis@wu-wien.ac.at>

On Wed, 16 Feb 2005 15:44:00 +0200 Sander Oom wrote:

> Thanks Peter!
> 
> Of course I only have (nx-1)(ny-1) facets in a x*y plot!
> 
> The help page line:
> ...
> col 	the color(s) of the surface facets. Transparent colours are 
> ignored. This is recycled to the (nx-1)(ny-1) facets.
> ...
> just did not ring a bell.
> 
> In fact, it is still not clear to me why it recycles the ramp even 
> though it has a surplus of colours (grey levels)! Why not just ignore 
> the surplus colours?

It does! 
cols is a vector of length 42 and only the first 30 are used. Try to
use your persp call below with col = cols and col = cols[1:30].
Z

> Anyway it works,
> 
> Sander.
> 
> 
> Peter Dalgaard wrote:
> > Sander Oom <slist at oomvanlieshout.net> writes:
> > 
> > 
> >>Dear R users,
> >>
> >>Could somebody tell me why the grey color ramp is repeated in this
> >>graph, eventhough the ramp values go from 0 to 1? I must be missing
> >>something obvious, but I can not see it!
> >>
> >>z <-
> >>c(0.064329041,0.117243316,0.161565116,0.19923015,0.231642175,0.2598
> >35539,0.284571226,>0.038507288,0.094184749,0.140959431,0.180803984,0
> >.215159105,0.245096084,0.271412845,>0.00775022,0.066198255,0.1154332
> >07,0.157494219,0.193836765,0.225569076,0.253518629,>-0.02820814,0.03
> >2958752,0.084661362,0.128946221,0.167320522,0.200892494,0.230504392,
> >>-0.07003273,-0.005814512,0.048304039,0.094805358,0.135196637,0.1706
> >30435,0.201956395,>-0.117878701,-0.050461393,0.005991829,0.054672666
> >,0.097103088,0.134398711,0.167423957)>
> >>x <- c(0,1,2,3,4,5)
> >>y <- c(50, 100, 150, 200, 250, 300, 350)
> >>z <- matrix(z, nrow=length(x), ncol=length(y), byrow=TRUE)
> >>
> >>#persp(x, y, z, theta = 30, phi = 30, expand = 0.5,
> >>#  box= TRUE, axes= TRUE, ticktype = "detailed", main="Title of
> >plot")>
> >>hgt <- (z - min(z))/ (max(z) - min(z))
> >>z
> >>hgt
> >>cols <- grey(hgt)
> >>persp(x, y, z, col = cols, theta = 30, phi = 30, expand = 0.5,
> >>   box= TRUE, axes= TRUE, ticktype = "detailed", main="Title of
> >plot")>
> > 
> > 
> > You have 30 facets and 42 colour values. Try it with
> > 
> > cols <- grey(hgt[-1,-1])
> > 
> > 
> 
> -- 
> ---------------------------------------------------------
> Dr. Sander P. Oom
> Animal, Plant and Environmental Sciences
> University of the Witwatersrand
> Private Bag 3
> Wits 2050
> South Africa
> 
> Tel (work)      +27 (0)11 717 64 04
> Tel (home)      +27 (0)18 297 44 51
> Fax             +27 (0)18 299 24 64
> 
> Email   sander at oomvanlieshout.net
> Web     www.oomvanlieshout.net/sander
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From ligges at statistik.uni-dortmund.de  Wed Feb 16 15:41:16 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 16 Feb 2005 15:41:16 +0100
Subject: [R] Repeating grey scale in graph?
In-Reply-To: <42134E20.9050000@oomvanlieshout.net>
References: <42132C85.1080003@oomvanlieshout.net>	<x24qgcit9i.fsf@biostat.ku.dk>
	<42134E20.9050000@oomvanlieshout.net>
Message-ID: <42135B8C.9000808@statistik.uni-dortmund.de>

Sander Oom wrote:

> Thanks Peter!
> 
> Of course I only have (nx-1)(ny-1) facets in a x*y plot!
> 
> The help page line:
> ...
> col     the color(s) of the surface facets. Transparent colours are 
> ignored. This is recycled to the (nx-1)(ny-1) facets.
> ...
> just did not ring a bell.
> 
> In fact, it is still not clear to me why it recycles the ramp even 
> though it has a surplus of colours (grey levels)! Why not just ignore 
> the surplus colours?

Indeed, it ignores them in your case, but since each row of your matrix 
there is one too much this one is moved to the next row, 2 from row two 
to three, 3 from three to four, .... and the last nx+ny-1 are omitted.

Uwe Ligges


> 
> Anyway it works,
> 
> Sander.
> 
> 
> Peter Dalgaard wrote:
> 
>> Sander Oom <slist at oomvanlieshout.net> writes:
>>
>>
>>> Dear R users,
>>>
>>> Could somebody tell me why the grey color ramp is repeated in this
>>> graph, eventhough the ramp values go from 0 to 1? I must be missing
>>> something obvious, but I can not see it!
>>>
>>> z <-
>>> c(0.064329041,0.117243316,0.161565116,0.19923015,0.231642175,0.259835539,0.284571226, 
>>>
>>> 0.038507288,0.094184749,0.140959431,0.180803984,0.215159105,0.245096084,0.271412845, 
>>>
>>> 0.00775022,0.066198255,0.115433207,0.157494219,0.193836765,0.225569076,0.253518629, 
>>>
>>> -0.02820814,0.032958752,0.084661362,0.128946221,0.167320522,0.200892494,0.230504392, 
>>>
>>> -0.07003273,-0.005814512,0.048304039,0.094805358,0.135196637,0.170630435,0.201956395, 
>>>
>>> -0.117878701,-0.050461393,0.005991829,0.054672666,0.097103088,0.134398711,0.167423957) 
>>>
>>>
>>> x <- c(0,1,2,3,4,5)
>>> y <- c(50, 100, 150, 200, 250, 300, 350)
>>> z <- matrix(z, nrow=length(x), ncol=length(y), byrow=TRUE)
>>>
>>> #persp(x, y, z, theta = 30, phi = 30, expand = 0.5,
>>> #  box= TRUE, axes= TRUE, ticktype = "detailed", main="Title of plot")
>>>
>>> hgt <- (z - min(z))/ (max(z) - min(z))
>>> z
>>> hgt
>>> cols <- grey(hgt)
>>> persp(x, y, z, col = cols, theta = 30, phi = 30, expand = 0.5,
>>>   box= TRUE, axes= TRUE, ticktype = "detailed", main="Title of plot")
>>>
>>
>>
>> You have 30 facets and 42 colour values. Try it with
>>
>> cols <- grey(hgt[-1,-1])
>>
>>
>



From p.dalgaard at biostat.ku.dk  Wed Feb 16 15:35:37 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 16 Feb 2005 15:35:37 +0100
Subject: [R] Repeating grey scale in graph?
In-Reply-To: <20050216153519.4827a3cd.Achim.Zeileis@wu-wien.ac.at>
References: <42132C85.1080003@oomvanlieshout.net>
	<x24qgcit9i.fsf@biostat.ku.dk> <42134E20.9050000@oomvanlieshout.net>
	<20050216153519.4827a3cd.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <x2vf8sh8sm.fsf@biostat.ku.dk>

Achim Zeileis <Achim.Zeileis at wu-wien.ac.at> writes:

> > In fact, it is still not clear to me why it recycles the ramp even 
> > though it has a surplus of colours (grey levels)! Why not just ignore 
> > the surplus colours?
> 
> It does! 
> cols is a vector of length 42 and only the first 30 are used. Try to
> use your persp call below with col = cols and col = cols[1:30].
> Z

Just to rub it in, consider

> M
     [,1] [,2] [,3] [,4] [,5]
[1,]    2    3    4    5    6
[2,]    3    4    5    6    7
[3,]    4    5    6    7    8
[4,]    5    6    7    8    9
[5,]    6    7    8    9   10
> M2 <- matrix(M[1:16],4,4)
> M2
     [,1] [,2] [,3] [,4]
[1,]    2    6    6    6
[2,]    3    3    7    7
[3,]    4    4    4    8
[4,]    5    5    5    5


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ligges at statistik.uni-dortmund.de  Wed Feb 16 15:44:40 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 16 Feb 2005 15:44:40 +0100
Subject: [R] Passing colnames to graphics title
In-Reply-To: <Pine.LNX.4.44.0502161354480.4164-100000@env-pc-phd13>
References: <Pine.LNX.4.44.0502161354480.4164-100000@env-pc-phd13>
Message-ID: <42135C58.1010601@statistik.uni-dortmund.de>

Laura Quinn wrote:

> Hi,
> 
> Just a quick query - if I'm creating a function to produce a number of
> histograms per page of output (one per column from a matrix), how can I
> pass the column name of the matrix into the title (or indeed to form part
> of the x-axis label)?


By extracting them using colnames()?

Uwe Ligges



> TIA,
> Laura
> 
> Laura Quinn
> Institute of Atmospheric Science
> School of Earth and Environment
> University of Leeds
> Leeds
> LS2 9JT
> 
> tel: +44 113 343 1596
> fax: +44 113 343 6716
> mail: laura at env.leeds.ac.uk
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From dimitris.rizopoulos at med.kuleuven.ac.be  Wed Feb 16 15:46:18 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Wed, 16 Feb 2005 15:46:18 +0100
Subject: [R] Passing colnames to graphics title
References: <Pine.LNX.4.44.0502161354480.4164-100000@env-pc-phd13>
Message-ID: <007201c51436$45f6f850$0540210a@www.domain>

a simple thing to do is:

mat <- matrix(...) # your matrix
nams <- dimnames(mat)[[2]]
for(j in 1:ncol(mat)) hist(mat[,j], main=nams[j])
# or hist(mat[,j], xlab=paste("...", nams[j], "..."))

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Laura Quinn" <laura at env.leeds.ac.uk>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, February 16, 2005 2:56 PM
Subject: [R] Passing colnames to graphics title


> Hi,
>
> Just a quick query - if I'm creating a function to produce a number 
> of
> histograms per page of output (one per column from a matrix), how 
> can I
> pass the column name of the matrix into the title (or indeed to form 
> part
> of the x-axis label)?
>
> TIA,
> Laura
>
> Laura Quinn
> Institute of Atmospheric Science
> School of Earth and Environment
> University of Leeds
> Leeds
> LS2 9JT
>
> tel: +44 113 343 1596
> fax: +44 113 343 6716
> mail: laura at env.leeds.ac.uk
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From laura at env.leeds.ac.uk  Wed Feb 16 15:46:51 2005
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Wed, 16 Feb 2005 14:46:51 +0000 (GMT)
Subject: [R] Passing colnames to graphics title
In-Reply-To: <42135C58.1010601@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.44.0502161444280.4164-100000@env-pc-phd13>

Obviously I have been trying to use the colnames() function!

However, when I try to subscript ie:

for(i in 1:20){
main=paste("Site:",colnames(i),sep="")
}

this doesn't work! I thought that as.character(colnames(i)) or
substitute(colnames(i)) might work, but to no avail...

Laura Quinn
Institute of Atmospheric Science
School of Earth and Environment
University of Leeds
Leeds
LS2 9JT

tel: +44 113 343 1596
fax: +44 113 343 6716
mail: laura at env.leeds.ac.uk

On Wed, 16 Feb 2005, Uwe Ligges wrote:

> Laura Quinn wrote:
>
> > Hi,
> >
> > Just a quick query - if I'm creating a function to produce a number of
> > histograms per page of output (one per column from a matrix), how can I
> > pass the column name of the matrix into the title (or indeed to form part
> > of the x-axis label)?
>
>
> By extracting them using colnames()?
>
> Uwe Ligges
>
>
>
> > TIA,
> > Laura
> >
> > Laura Quinn
> > Institute of Atmospheric Science
> > School of Earth and Environment
> > University of Leeds
> > Leeds
> > LS2 9JT
> >
> > tel: +44 113 343 1596
> > fax: +44 113 343 6716
> > mail: laura at env.leeds.ac.uk
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From ligges at statistik.uni-dortmund.de  Wed Feb 16 15:50:03 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 16 Feb 2005 15:50:03 +0100
Subject: [R] histogram and boxplot in a same postscript
In-Reply-To: <20050216131154.48241.qmail@web14525.mail.yahoo.com>
References: <20050216131154.48241.qmail@web14525.mail.yahoo.com>
Message-ID: <42135D9B.1050400@statistik.uni-dortmund.de>

C?zar Freitas wrote:

> Hi, all. I need plot a boxplot under a histogram like
> below, but some configs are troubled:
> 
> - the boxplot contours the plot, even if I put
> bty="n", modifying the histogram plot;

You want to set  "axes = FALSE" in boxplot()
BTW: What is q[4]  in your call to points()?


Uwe Ligges


> - I changed the configs of axis to do a 3x3 inches
> plot, but the result is 2 different axis.
> 
> For example, the code below ilustrates this...
> 
> 
>   scores<-c(2.0, 0.0, 5.0, 5.0, 5.0, 2.0, 0.0, 5.0,
> 2.5, 4.0, 5.0, 0.0, 5.0, 0.0, 2.0, 5.0, 5.0, 2.0, 3.0,
> 3.0)
> 
>   postscript("test.ps", width=3, height=3,
> horizontal=FALSE, family="Times", paper="special")
> 
>   gra<-hist(scores, breaks=((0:11)/2-.2),
> xlim=c(-1,6), plot=FALSE, cex.axis=.5, cex.main=.5,
> cex.sub=.5, cex.lab=.5, mgp=c(1.5,.5,0))
> 
>   yy<-ceiling(max(gra$counts)/10)*10
>   
>   hist(scores, breaks=((0:11)/2-.2), xlim=c(-1,6),
> ylim=c(0,yy), main="scores", ylab="Freq", xlab="math",
> cex.axis=.5, cex.main=.5, cex.sub=.5, cex.lab=.5,
> mgp=c(1.5,.5,0))
>   boxplot(scores, horizontal=1, add=1, at=-yy/50,
> boxwex=yy/20, bty="n", cex.axis=.5, cex.main=.5,
> cex.sub=.5, cex.lab=.5, ann=FALSE)
>   points(q[4], -yy/50, col=1, pch="x", cex=.2)
> 
>   dev.off()
> 
> 
> Thanks,
> C.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Wed Feb 16 15:46:36 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 16 Feb 2005 14:46:36 +0000 (UTC)
Subject: [R] Easy cut & paste from Excel to R?
References: <20050216085914.27179.qmail@web25806.mail.ukl.yahoo.com>
	<42130F0C.1070905@statistik.uni-dortmund.de>
	<x2d5v0iz8y.fsf@biostat.ku.dk>
	<42132387.8060109@statistik.uni-dortmund.de>
	<x28y5oiy5d.fsf@biostat.ku.dk>
Message-ID: <loom.20050216T154537-927@post.gmane.org>

Peter Dalgaard <p.dalgaard <at> biostat.ku.dk> writes:

: 
: Uwe Ligges <ligges <at> statistik.uni-dortmund.de> writes:
: 
: > Well, yes, some arguments twisting might be required as for my german
: > locales / german version of Excel the following works even for empty
: > cells and real valued entries:
: > 
: >    read.table(file("clipboard"), sep="\t", dec=",")
: > 
: >     V1  V2  V3
: > 1 1.2  NA 2.3
: > 2 3.4 4.5 5.6
: 
: ...which is of course the same as 
: 
:      read.delim2(file("clipboard"), header=FALSE)

which is the same as

       read.delim2("clipboard", header = FALSE)

: 
: except for possible variations in the fill and quote settings. (What
: happens if you have empty cells in the last columns, or cells with
: the text "Don't do this"?)
:



From laura at env.leeds.ac.uk  Wed Feb 16 15:54:00 2005
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Wed, 16 Feb 2005 14:54:00 +0000 (GMT)
Subject: [R] Passing colnames to graphics title
In-Reply-To: <007201c51436$45f6f850$0540210a@www.domain>
Message-ID: <Pine.LNX.4.44.0502161453520.4164-100000@env-pc-phd13>

Wonderful, thank you very much!

Laura Quinn
Institute of Atmospheric Science
School of Earth and Environment
University of Leeds
Leeds
LS2 9JT

tel: +44 113 343 1596
fax: +44 113 343 6716
mail: laura at env.leeds.ac.uk

On Wed, 16 Feb 2005, Dimitris Rizopoulos wrote:

> a simple thing to do is:
>
> mat <- matrix(...) # your matrix
> nams <- dimnames(mat)[[2]]
> for(j in 1:ncol(mat)) hist(mat[,j], main=nams[j])
> # or hist(mat[,j], xlab=paste("...", nams[j], "..."))
>
> I hope it helps.
>
> Best,
> Dimitris
>
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
>
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/16/336899
> Fax: +32/16/337015
> Web: http://www.med.kuleuven.ac.be/biostat/
>      http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
>
>
> ----- Original Message -----
> From: "Laura Quinn" <laura at env.leeds.ac.uk>
> To: <r-help at stat.math.ethz.ch>
> Sent: Wednesday, February 16, 2005 2:56 PM
> Subject: [R] Passing colnames to graphics title
>
>
> > Hi,
> >
> > Just a quick query - if I'm creating a function to produce a number
> > of
> > histograms per page of output (one per column from a matrix), how
> > can I
> > pass the column name of the matrix into the title (or indeed to form
> > part
> > of the x-axis label)?
> >
> > TIA,
> > Laura
> >
> > Laura Quinn
> > Institute of Atmospheric Science
> > School of Earth and Environment
> > University of Leeds
> > Leeds
> > LS2 9JT
> >
> > tel: +44 113 343 1596
> > fax: +44 113 343 6716
> > mail: laura at env.leeds.ac.uk
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
>
>



From buser at stat.math.ethz.ch  Wed Feb 16 15:56:33 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Wed, 16 Feb 2005 15:56:33 +0100
Subject: [R] (no subject)
In-Reply-To: <OFC470C642.0219955F-ONC1256FAA.004D6E94@codan.dk>
References: <OFC470C642.0219955F-ONC1256FAA.004D6E94@codan.dk>
Message-ID: <16915.24353.376524.252737@stat.math.ethz.ch>

Hi Jim

I'm not sure if I understand your problem correctly. Is this a solution?

(li <- list(a=1,b=200.44))
as.numeric(paste(unlist(li), collapse = ""))

Best regards,

Christoph Buser

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C11
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-1-632-5414		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------

Jim Gustafsson writes:
 > 
 > R-people
 > 
 > I wonder if one could change a list of table with number of the form 
 > 1,200.44 , to 1200.44
 > 
 > Regards
 > JG
 > 
 > 
 > ------------------------------------------------------------------------------
 > This e-mail and any attachment may be confidential and may also be privileged.
 > If you are not the intended recipient, please notify us immediately and then
 > delete this e-mail and any attachment without retaining copies or disclosing
 > the contents thereof to any other person.
 > Thank you.
 > ------------------------------------------------------------------------------
 > 	[[alternative HTML version deleted]]
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From andreas.wolf at uni-jena.de  Wed Feb 16 16:00:45 2005
From: andreas.wolf at uni-jena.de (Andreas Wolf)
Date: Wed, 16 Feb 2005 16:00:45 +0100
Subject: [R] splitting items into groups according to correlations
Message-ID: <AF874507D8BE4945941321229E2AAF3906BF58@cat.app.metpsy.uni-jena.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050216/50b2dcf3/attachment.pl

From murdoch at stats.uwo.ca  Wed Feb 16 16:28:58 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 16 Feb 2005 15:28:58 +0000
Subject: [R] Repeating grey scale in graph?
In-Reply-To: <42134E20.9050000@oomvanlieshout.net>
References: <42132C85.1080003@oomvanlieshout.net>
	<x24qgcit9i.fsf@biostat.ku.dk>
	<42134E20.9050000@oomvanlieshout.net>
Message-ID: <4bp611hp8ac9n5k3tggahg1i6l22pmq3na@4ax.com>

On Wed, 16 Feb 2005 15:44:00 +0200, Sander Oom
<slist at oomvanlieshout.net> wrote :

>Thanks Peter!
>
>Of course I only have (nx-1)(ny-1) facets in a x*y plot!
>
>The help page line:
>...
>col 	the color(s) of the surface facets. Transparent colours are 
>ignored. This is recycled to the (nx-1)(ny-1) facets.
>...
>just did not ring a bell.
>
>In fact, it is still not clear to me why it recycles the ramp even 
>though it has a surplus of colours (grey levels)! Why not just ignore 
>the surplus colours?

Your z array is 6 by 7.  Your cols will be mapped to a 5 by 6 array.
They don't look like an array, because the grey() function stripped
off the dimension attribute.

The problem is that if you pass the entries from a 6 by 7 array to
something that expects the entries from a 5 by 6 array, you get things
in the wrong order.   You see the same effect here:

> rownum <- as.vector(row(matrix(NA, 6, 7)))
> matrix(rownum, 6, 7)
     [,1] [,2] [,3] [,4] [,5] [,6] [,7]
[1,]    1    1    1    1    1    1    1
[2,]    2    2    2    2    2    2    2
[3,]    3    3    3    3    3    3    3
[4,]    4    4    4    4    4    4    4
[5,]    5    5    5    5    5    5    5
[6,]    6    6    6    6    6    6    6
> matrix(rownum, 5, 6)
     [,1] [,2] [,3] [,4] [,5] [,6]
[1,]    1    6    5    4    3    2
[2,]    2    1    6    5    4    3
[3,]    3    2    1    6    5    4
[4,]    4    3    2    1    6    5
[5,]    5    4    3    2    1    6
Warning message:
data length [42] is not a sub-multiple or multiple of the number of
rows [5] in matrix 

except that in this case you get a warning about the wrong length;
persp doesn't give you the warning.  Maybe it should?

Duncan Murdoch



From aukema at entomology.wisc.edu  Wed Feb 16 16:31:31 2005
From: aukema at entomology.wisc.edu (Brian Aukema)
Date: Wed, 16 Feb 2005 09:31:31 -0600
Subject: [R] glmm with negative binomial
Message-ID: <5.2.1.1.2.20050216085655.03054a50@gwia.pen.wisc.edu>

Hello,

At present, can generalized linear mixed models with negative binomial 
distribution and estimating the shape parameter be fit using R?  I am aware 
of glm.nb but am wondering about incorporation of mixed effects.

Thanks in advance,
Brian Aukema



From petr.pikal at precheza.cz  Wed Feb 16 16:32:43 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 16 Feb 2005 16:32:43 +0100
Subject: [R] (no subject)
In-Reply-To: <OFC470C642.0219955F-ONC1256FAA.004D6E94@codan.dk>
Message-ID: <421375AB.9318.1CA757A@localhost>

Hi Jim

Something like

> x<-"1,200.44"
> as.numeric(sub(",", "" , x))
[1] 1200.44


Petr


On 16 Feb 2005 at 15:08, Jim Gustafsson wrote:

> 
> R-people
> 
> I wonder if one could change a list of table with number of the form
> 1,200.44 , to 1200.44
> 
> Regards
> JG
> 
> 
> ----------------------------------------------------------------------
> -------- This e-mail and any attachment may be confidential and may
> also be privileged. If you are not the intended recipient, please
> notify us immediately and then delete this e-mail and any attachment
> without retaining copies or disclosing the contents thereof to any
> other person. Thank you.
> ----------------------------------------------------------------------
> --------
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From p.dalgaard at biostat.ku.dk  Wed Feb 16 16:28:14 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 16 Feb 2005 16:28:14 +0100
Subject: [R] (no subject)
In-Reply-To: <OFC470C642.0219955F-ONC1256FAA.004D6E94@codan.dk>
References: <OFC470C642.0219955F-ONC1256FAA.004D6E94@codan.dk>
Message-ID: <x2r7jgh6cx.fsf@biostat.ku.dk>

Jim Gustafsson <jgu at codan.dk> writes:

> R-people
> 
> I wonder if one could change a list of table with number of the form 
> 1,200.44 , to 1200.44
> 
> Regards
> JG

On input, I assume?

Read as character variable, get rid of the comma(s) using (g)sub, then
use as.numeric.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ligges at statistik.uni-dortmund.de  Wed Feb 16 16:41:11 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 16 Feb 2005 16:41:11 +0100
Subject: [R] Passing colnames to graphics title
In-Reply-To: <Pine.LNX.4.44.0502161444280.4164-100000@env-pc-phd13>
References: <Pine.LNX.4.44.0502161444280.4164-100000@env-pc-phd13>
Message-ID: <42136997.5090501@statistik.uni-dortmund.de>

Laura Quinn wrote:

> Obviously I have been trying to use the colnames() function!
> 
> However, when I try to subscript ie:
> 
> for(i in 1:20){
> main=paste("Site:",colnames(i),sep="")
> }

Example (which you should have provided):

  # Generate an example-matrix:
  X <- matrix(1:9, 3)
  colnames(X) <- letters[1:3]

  # now try to get histograms of columns using a loop:
  par(mfrow = c(3, 1))
  cnames <- colnames(X)
  for(i in 1:ncol(X)){
    hist(X[,i], main = paste("Site:", cnames[i], sep=""))
  }

Uwe Ligges


> this doesn't work! I thought that as.character(colnames(i)) or
> substitute(colnames(i)) might work, but to no avail...
> 
> Laura Quinn
> Institute of Atmospheric Science
> School of Earth and Environment
> University of Leeds
> Leeds
> LS2 9JT
> 
> tel: +44 113 343 1596
> fax: +44 113 343 6716
> mail: laura at env.leeds.ac.uk
> 
> On Wed, 16 Feb 2005, Uwe Ligges wrote:
> 
> 
>>Laura Quinn wrote:
>>
>>
>>>Hi,
>>>
>>>Just a quick query - if I'm creating a function to produce a number of
>>>histograms per page of output (one per column from a matrix), how can I
>>>pass the column name of the matrix into the title (or indeed to form part
>>>of the x-axis label)?
>>
>>
>>By extracting them using colnames()?
>>
>>Uwe Ligges
>>
>>
>>
>>
>>>TIA,
>>>Laura
>>>
>>>Laura Quinn
>>>Institute of Atmospheric Science
>>>School of Earth and Environment
>>>University of Leeds
>>>Leeds
>>>LS2 9JT
>>>
>>>tel: +44 113 343 1596
>>>fax: +44 113 343 6716
>>>mail: laura at env.leeds.ac.uk
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>



From ligges at statistik.uni-dortmund.de  Wed Feb 16 16:43:15 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 16 Feb 2005 16:43:15 +0100
Subject: [R] Passing colnames to graphics title
In-Reply-To: <42136997.5090501@statistik.uni-dortmund.de>
References: <Pine.LNX.4.44.0502161444280.4164-100000@env-pc-phd13>
	<42136997.5090501@statistik.uni-dortmund.de>
Message-ID: <42136A13.6050406@statistik.uni-dortmund.de>

Uwe Ligges wrote:

> Laura Quinn wrote:
> 
>> Obviously I have been trying to use the colnames() function!
>>
>> However, when I try to subscript ie:
>>
>> for(i in 1:20){
>> main=paste("Site:",colnames(i),sep="")
>> }


BTW: colnames(i) is the same as colnames(1) in the first iteration of 
your loop. What do you expect colnames(1) to be?

Uwe Ligges


> 
> Example (which you should have provided):
> 
>  # Generate an example-matrix:
>  X <- matrix(1:9, 3)
>  colnames(X) <- letters[1:3]
> 
>  # now try to get histograms of columns using a loop:
>  par(mfrow = c(3, 1))
>  cnames <- colnames(X)
>  for(i in 1:ncol(X)){
>    hist(X[,i], main = paste("Site:", cnames[i], sep=""))
>  }
> 
> Uwe Ligges
> 
> 
>> this doesn't work! I thought that as.character(colnames(i)) or
>> substitute(colnames(i)) might work, but to no avail...
>>
>> Laura Quinn
>> Institute of Atmospheric Science
>> School of Earth and Environment
>> University of Leeds
>> Leeds
>> LS2 9JT
>>
>> tel: +44 113 343 1596
>> fax: +44 113 343 6716
>> mail: laura at env.leeds.ac.uk
>>
>> On Wed, 16 Feb 2005, Uwe Ligges wrote:
>>
>>
>>> Laura Quinn wrote:
>>>
>>>
>>>> Hi,
>>>>
>>>> Just a quick query - if I'm creating a function to produce a number of
>>>> histograms per page of output (one per column from a matrix), how can I
>>>> pass the column name of the matrix into the title (or indeed to form 
>>>> part
>>>> of the x-axis label)?
>>>
>>>
>>>
>>> By extracting them using colnames()?
>>>
>>> Uwe Ligges
>>>
>>>
>>>
>>>
>>>> TIA,
>>>> Laura
>>>>
>>>> Laura Quinn
>>>> Institute of Atmospheric Science
>>>> School of Earth and Environment
>>>> University of Leeds
>>>> Leeds
>>>> LS2 9JT
>>>>
>>>> tel: +44 113 343 1596
>>>> fax: +44 113 343 6716
>>>> mail: laura at env.leeds.ac.uk
>>>>
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide! 
>>>> http://www.R-project.org/posting-guide.html
>>>
>>>
>>>
> 
>



From balbo at programmer.net  Wed Feb 16 16:45:32 2005
From: balbo at programmer.net (balbo@programmer.net)
Date: Wed, 16 Feb 2005 10:45:32 -0500
Subject: [R] Rmpi - cluster with no name
Message-ID: <20050216154532.A67964BDAB@ws1-1.us4.outblaze.com>

Hello everybody, thanks for help.
I'm having this problem using the package Rmpi.
I run R with mpi and then I load the library
'Rmpi' and 'snow'. But when I try to create the cluster:

mycluster <- makeCluster(2)

I get this error

Error in makeMPIcluster(spec, ...) : a cluster already exists 1

The first time I did this was ok, but I didn't link the created
cluster to any object, to see if it worked I just tiped:

makeCluster("name")

So, now, I think I have this cluster. But since it has no name I
cannot communicate with it. I can't even remove it
or just link it to an object I can handle with.

I checked all the documentations but I couldn't find anything useful.
-- 
___________________________________________________________
Sign-up for Ads Free at Mail.com
http://promo.mail.com/adsfreejump.htm



From sundar.dorai-raj at pdf.com  Wed Feb 16 16:58:32 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 16 Feb 2005 09:58:32 -0600
Subject: [R] Passing colnames to graphics title
In-Reply-To: <Pine.LNX.4.44.0502161444280.4164-100000@env-pc-phd13>
References: <Pine.LNX.4.44.0502161444280.4164-100000@env-pc-phd13>
Message-ID: <42136DA8.80103@pdf.com>

Laura Quinn wrote:
> Obviously I have been trying to use the colnames() function!
> 
> However, when I try to subscript ie:
> 
> for(i in 1:20){
> main=paste("Site:",colnames(i),sep="")
> }
> 
> this doesn't work! I thought that as.character(colnames(i)) or
> substitute(colnames(i)) might work, but to no avail...
> 

Laura,

You should (re)read ?colnames. It takes a matrix as it's argument, not 
an integer as you have supplied.

I think you want:

for(i in 1:20){
   main=paste("Site:",colnames(mat)[i],sep="")
}

--sundar

> Laura Quinn
> Institute of Atmospheric Science
> School of Earth and Environment
> University of Leeds
> Leeds
> LS2 9JT
> 
> tel: +44 113 343 1596
> fax: +44 113 343 6716
> mail: laura at env.leeds.ac.uk
> 
> On Wed, 16 Feb 2005, Uwe Ligges wrote:
> 
> 
>>Laura Quinn wrote:
>>
>>
>>>Hi,
>>>
>>>Just a quick query - if I'm creating a function to produce a number of
>>>histograms per page of output (one per column from a matrix), how can I
>>>pass the column name of the matrix into the title (or indeed to form part
>>>of the x-axis label)?
>>
>>
>>By extracting them using colnames()?
>>
>>Uwe Ligges
>>
>>
>>
>>
>>>TIA,
>>>Laura
>>>
>>>Laura Quinn
>>>Institute of Atmospheric Science
>>>School of Earth and Environment
>>>University of Leeds
>>>Leeds
>>>LS2 9JT
>>>
>>>tel: +44 113 343 1596
>>>fax: +44 113 343 6716
>>>mail: laura at env.leeds.ac.uk
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jtk at cmp.uea.ac.uk  Wed Feb 16 17:57:05 2005
From: jtk at cmp.uea.ac.uk (Jan T. Kim)
Date: Wed, 16 Feb 2005 16:57:05 +0000
Subject: [R] Passing colnames to graphics title
In-Reply-To: <Pine.LNX.4.44.0502161444280.4164-100000@env-pc-phd13>
References: <42135C58.1010601@statistik.uni-dortmund.de>
	<Pine.LNX.4.44.0502161444280.4164-100000@env-pc-phd13>
Message-ID: <20050216165705.GA17341@jtkpc.cmp.uea.ac.uk>

On Wed, Feb 16, 2005 at 02:46:51PM +0000, Laura Quinn wrote:
> Obviously I have been trying to use the colnames() function!
> 
> However, when I try to subscript ie:
> 
> for(i in 1:20){
> main=paste("Site:",colnames(i),sep="")
                     ^^^^^^^^^^^

it looks to me that this should be something like

    colnames(foo)[i]

where foo is the matrix or data.frame you use.

Best regards, Jan
-- 
 +- Jan T. Kim -------------------------------------------------------+
 |    *NEW*    email: jtk at cmp.uea.ac.uk                               |
 |    *NEW*    WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
 *-----=<  hierarchical systems are for files, not for humans  >=-----*



From jerk_alert at hotmail.com  Wed Feb 16 16:22:29 2005
From: jerk_alert at hotmail.com (Ken Termiso)
Date: Wed, 16 Feb 2005 15:22:29 +0000
Subject: [R] Multiple instances of R shell in Mac OS X 10.3? 
Message-ID: <BAY101-F321675840CA2ABEFC6EDFEE86C0@phx.gbl>

Wondering if there's a way to do this. I'm not referring to running Rterm 
from OSX terminal, but actually running multiple instances of the R command 
shell within the OS X GUI...

I have R v2.0.1, OS X 10.3.8.

Thanks in advance,
Ken



From maustin at amgen.com  Wed Feb 16 17:07:25 2005
From: maustin at amgen.com (Austin, Matt)
Date: Wed, 16 Feb 2005 08:07:25 -0800
Subject: [R] Passing colnames to graphics title
Message-ID: <E7D5AB4811D20B489622AABA9C53859104E0DEF7@teal-exch.amgen.com>

If i is 1:20, there are no column names.  Make sure you are indexing the
names from your your dataframe.

> xx <- data.frame(a=c(1:10), b = letters[1:10])
> colnames(xx)
[1] "a" "b"
> for(i in 1:2) print(colnames(xx)[i])
[1] "a"
[1] "b"
> for(i in colnames(xx)) print(i)
[1] "a"
[1] "b"

Matt Austin
Statistician

Amgen 
One Amgen Center Drive
M/S 24-2-C
Thousand Oaks CA 93021
(805) 447 - 7431


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Laura Quinn
> Sent: Wednesday, February 16, 2005 6:47 AM
> To: Uwe Ligges
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Passing colnames to graphics title
> 
> 
> Obviously I have been trying to use the colnames() function!
> 
> However, when I try to subscript ie:
> 
> for(i in 1:20){
> main=paste("Site:",colnames(i),sep="")
> }
> 
> this doesn't work! I thought that as.character(colnames(i)) or
> substitute(colnames(i)) might work, but to no avail...
> 
> Laura Quinn
> Institute of Atmospheric Science
> School of Earth and Environment
> University of Leeds
> Leeds
> LS2 9JT
> 
> tel: +44 113 343 1596
> fax: +44 113 343 6716
> mail: laura at env.leeds.ac.uk
> 
> On Wed, 16 Feb 2005, Uwe Ligges wrote:
> 
> > Laura Quinn wrote:
> >
> > > Hi,
> > >
> > > Just a quick query - if I'm creating a function to 
> produce a number of
> > > histograms per page of output (one per column from a 
> matrix), how can I
> > > pass the column name of the matrix into the title (or 
> indeed to form part
> > > of the x-axis label)?
> >
> >
> > By extracting them using colnames()?
> >
> > Uwe Ligges
> >
> >
> >
> > > TIA,
> > > Laura
> > >
> > > Laura Quinn
> > > Institute of Atmospheric Science
> > > School of Earth and Environment
> > > University of Leeds
> > > Leeds
> > > LS2 9JT
> > >
> > > tel: +44 113 343 1596
> > > fax: +44 113 343 6716
> > > mail: laura at env.leeds.ac.uk
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html
>
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From slist at oomvanlieshout.net  Wed Feb 16 17:33:01 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Wed, 16 Feb 2005 18:33:01 +0200
Subject: [R] Repeating grey scale in graph?
In-Reply-To: <4bp611hp8ac9n5k3tggahg1i6l22pmq3na@4ax.com>
References: <42132C85.1080003@oomvanlieshout.net>
	<x24qgcit9i.fsf@biostat.ku.dk>
	<42134E20.9050000@oomvanlieshout.net>
	<4bp611hp8ac9n5k3tggahg1i6l22pmq3na@4ax.com>
Message-ID: <421375BD.7020707@oomvanlieshout.net>

Aaaah...the inner workings of R! Now I also see why the colours are not 
only repeated, but also 'wrongly' allocated to the facets! Very clear 
example!

Indeed a warning or error would have been more helpful!

Cheers,

Sander.

PS: I hope that after all this, I can still convince the creator of the 
original data that it is a good idea to plot his graphs in R instead of 
excel.  ;-)


Duncan Murdoch wrote:

>On Wed, 16 Feb 2005 15:44:00 +0200, Sander Oom
><slist at oomvanlieshout.net> wrote :
>
>  
>
>>Thanks Peter!
>>
>>Of course I only have (nx-1)(ny-1) facets in a x*y plot!
>>
>>The help page line:
>>...
>>col 	the color(s) of the surface facets. Transparent colours are 
>>ignored. This is recycled to the (nx-1)(ny-1) facets.
>>...
>>just did not ring a bell.
>>
>>In fact, it is still not clear to me why it recycles the ramp even 
>>though it has a surplus of colours (grey levels)! Why not just ignore 
>>the surplus colours?
>>    
>>
>
>Your z array is 6 by 7.  Your cols will be mapped to a 5 by 6 array.
>They don't look like an array, because the grey() function stripped
>off the dimension attribute.
>
>The problem is that if you pass the entries from a 6 by 7 array to
>something that expects the entries from a 5 by 6 array, you get things
>in the wrong order.   You see the same effect here:
>
>  
>
>>rownum <- as.vector(row(matrix(NA, 6, 7)))
>>matrix(rownum, 6, 7)
>>    
>>
>     [,1] [,2] [,3] [,4] [,5] [,6] [,7]
>[1,]    1    1    1    1    1    1    1
>[2,]    2    2    2    2    2    2    2
>[3,]    3    3    3    3    3    3    3
>[4,]    4    4    4    4    4    4    4
>[5,]    5    5    5    5    5    5    5
>[6,]    6    6    6    6    6    6    6
>  
>
>>matrix(rownum, 5, 6)
>>    
>>
>     [,1] [,2] [,3] [,4] [,5] [,6]
>[1,]    1    6    5    4    3    2
>[2,]    2    1    6    5    4    3
>[3,]    3    2    1    6    5    4
>[4,]    4    3    2    1    6    5
>[5,]    5    4    3    2    1    6
>Warning message:
>data length [42] is not a sub-multiple or multiple of the number of
>rows [5] in matrix 
>
>except that in this case you get a warning about the wrong length;
>persp doesn't give you the warning.  Maybe it should?
>
>Duncan Murdoch
>
>
>  
>

-- 
---------------------------------------------------------
Dr. Sander P. Oom
Animal, Plant and Environmental Sciences
University of the Witwatersrand
Private Bag 3
Wits 2050
South Africa

Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64

Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From james.holtman at convergys.com  Wed Feb 16 17:34:46 2005
From: james.holtman at convergys.com (james.holtman@convergys.com)
Date: Wed, 16 Feb 2005 11:34:46 -0500
Subject: [R] (no subject)
In-Reply-To: <OFC470C642.0219955F-ONC1256FAA.004D6E94@codan.dk>
Message-ID: <OF0EDA963F.226419BE-ON85256FAA.005AEAD3-85256FAA.005B11D6@nd.convergys.com>





use 'gsub'

> x <- c('1,200.44', '23,345.66')
> gsub(',','',x)
[1] "1200.44"  "23345.66"
> as.numeric(gsub(',','',x))
[1]  1200.44 23345.66
>
__________________________________________________________
James Holtman        "What is the problem you are trying to solve?"
Executive Technical Consultant  --  Office of Technology, Convergys
james.holtman at convergys.com
+1 (513) 723-2929


                                                                                                                                           
                      Jim Gustafsson                                                                                                       
                      <jgu at codan.dk>               To:       r-help at stat.math.ethz.ch                                                      
                      Sent by:                     cc:                                                                                     
                      r-help-bounces at stat.m        Subject:  [R] (no subject)                                                              
                      ath.ethz.ch                                                                                                          
                                                                                                                                           
                                                                                                                                           
                      02/16/2005 09:08                                                                                                     
                                                                                                                                           





R-people

I wonder if one could change a list of table with number of the form
1,200.44 , to 1200.44

Regards
JG


------------------------------------------------------------------------------

This e-mail and any attachment may be confidential and may also be
privileged.
If you are not the intended recipient, please notify us immediately and
then
delete this e-mail and any attachment without retaining copies or
disclosing
the contents thereof to any other person.
Thank you.
------------------------------------------------------------------------------

             [[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From jfox at mcmaster.ca  Wed Feb 16 17:38:17 2005
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 16 Feb 2005 11:38:17 -0500
Subject: [R] Passing colnames to graphics title
In-Reply-To: <Pine.LNX.4.44.0502161444280.4164-100000@env-pc-phd13>
Message-ID: <20050216163817.VDZL2026.tomts5-srv.bellnexxia.net@JohnDesktop8300>

Dear Laura,

It doesn't make sense to call colnames() with the loop index; you could do
something like (for the matrix or data frame X):

par(mfrow=c(1, ncol(X)))
names <- colnames(X)
for (i in seq(along=names)) hist(X[,i], main="", xlab=paste("Site:",
names[i]))

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Laura Quinn
> Sent: Wednesday, February 16, 2005 9:47 AM
> To: Uwe Ligges
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Passing colnames to graphics title
> 
> Obviously I have been trying to use the colnames() function!
> 
> However, when I try to subscript ie:
> 
> for(i in 1:20){
> main=paste("Site:",colnames(i),sep="")
> }
> 
> this doesn't work! I thought that as.character(colnames(i)) or
> substitute(colnames(i)) might work, but to no avail...
> 
> Laura Quinn
> Institute of Atmospheric Science
> School of Earth and Environment
> University of Leeds
> Leeds
> LS2 9JT
> 
> tel: +44 113 343 1596
> fax: +44 113 343 6716
> mail: laura at env.leeds.ac.uk
> 
> On Wed, 16 Feb 2005, Uwe Ligges wrote:
> 
> > Laura Quinn wrote:
> >
> > > Hi,
> > >
> > > Just a quick query - if I'm creating a function to 
> produce a number 
> > > of histograms per page of output (one per column from a 
> matrix), how 
> > > can I pass the column name of the matrix into the title 
> (or indeed 
> > > to form part of the x-axis label)?
> >
> >
> > By extracting them using colnames()?
> >
> > Uwe Ligges
> >
> >
> >
> > > TIA,
> > > Laura
> > >
> > > Laura Quinn
> > > Institute of Atmospheric Science
> > > School of Earth and Environment
> > > University of Leeds
> > > Leeds
> > > LS2 9JT
> > >
> > > tel: +44 113 343 1596
> > > fax: +44 113 343 6716
> > > mail: laura at env.leeds.ac.uk
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list 
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! 
> > > http://www.R-project.org/posting-guide.html
> >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From bhyde at pobox.com  Wed Feb 16 17:39:52 2005
From: bhyde at pobox.com (Ben Hyde)
Date: Wed, 16 Feb 2005 11:39:52 -0500
Subject: [R] Sampling given a table of percentages?
Message-ID: <8d3d93a27b2f3ee1eac0a15a38bdaec2@pobox.com>

I have a vector V.  sum(V) = 100, i.e. it's percentages.  length(V) is 
large.  I wish to generate samples (with replacement is fine), 
integers, in the range 1:length(V) who's distribution is driven by the 
distribution implied by the percentages in V.    V is unsorted, but 
that could change.  I'd rather not be too specific about the 
distribution of V.    I can certainly solve the problem intersecting my 
thin knowledge of R with my skills in programming, but I suspect this 
is trivial to somebody fluent in R.   Suggestions?  Thanks!  - ben

----
http://enthusiasm.cozy.org/   -- blog
tel:+1-781-240-2221  -- mobile, et.al.



From Stephanie.Tomczak at eleves.polytech-lille.fr  Wed Feb 16 17:54:29 2005
From: Stephanie.Tomczak at eleves.polytech-lille.fr (Stephanie.Tomczak@eleves.polytech-lille.fr)
Date: Wed, 16 Feb 2005 17:54:29 +0100
Subject: [R] problem with da.mix
Message-ID: <1108572869.42137ac5496ee@webmail.polytech-lille.fr>



Hello,

We use the mix package and we have a problem with the DA function. We aren't
sure, but it's maybbe a memory problem.

We have done:
> Ent<--read.table("C:/.../File.txt")
> attach(Ent)
> Ent 
    V1  V2   V3  V4 ... V16  V17
1    1   1   2   6      18   18 
2    1   1   1   NA     14   17
3    1   1   2   1      16   14
....
199  2   1   NA  7      19   18
200  2   1   3   2      14   17

> EntPrelim<-prelim.mix(as.matrix(Ent),9)
> EntEM<-em.mix(EntPrelim,maxits=500)
> rngseed(1234567)
> EntDA<-da.mix(EntPrelim, EntEM, steps=100, showits=TRUE)
 Steps of data Augmentation:
1... Error in da.mix(EntPrelim, EntEM, steps=100; showits=TRUE):
    Improper posterior--empty cells


If you know what is the matter, please help us.



From rpruim at calvin.edu  Wed Feb 16 18:11:35 2005
From: rpruim at calvin.edu (Randall Pruim)
Date: Wed, 16 Feb 2005 12:11:35 -0500
Subject: [R] panel/prepanel for polar plots ala xYplot
Message-ID: <2ad03ae5d4d494de9d0be01ae50b71ea@calvin.edu>


First a bit of background:

After doing a search for a flexible polar plot function and coming up 
empty, I have begun writing one myself.  Since I am new to writing 
mid-level graphics routines, this has required some learning about 
lattice, grid and related things.

I am to the point where I have a workable proof of concept, but still 
need to make some improvements.  My goal is to have something akin to a 
polar version of xYplot.  Although the particular plot I need requires 
some customization of that idea, the use of Cbind() has made that 
fairly easy.

Now some questions:

1) What are the best sources of documentation for lattice/grid?  I have 
found a few things by googling, but mostly I have been reverse 
engineering code and experimenting to figure out how things work.

2) What is the best way to generate "axis" and labels for them?

	Currently my radplot is a wrapper for xyplot that (a) turns off the 
axes and labels, and (b) calls xyplot with panel.radplot, 
prepanel.radplot, and radplot.superpose replacing the obvious things.  
I am generating the "axes" (concentric circles and peripheral labels) 
in panel.radplot, but this means that they are redrawn for each group 
when there is superposition.  Furthermore, there seems to be some 
jittering, so besides inefficiency, the result is not crisp.

	How are axes generated in xyplot?  To do this correctly will I have to 
go deeper and make a new version of trellis.skeleton or make a new call 
to it?  If I put the code into prepanel.radplot will it be executed 
once per panel or once per group in each panel?

3) I'd be happy to receive any other suggestions for how to approach 
the design of a robust, formula-based (including xYplot-like options) 
radial/polar plot.  I'd also be happy to hear of any packages that 
include something heading in this direction, if they exist and I just 
didn't locate them.

Thanks in advance for any assistance.

---rjp

PS.  For the curioius, the plot I am designing has a function call like

	radplot(Cbind(y,ratio)~x|g, groups=h, ...)

where y is numeric and x may be numeric or a factor.  The resulting 
plot has "spokes" for each value of x with length y, the last 1 - 
min(ratio, 1/ratio) fraction of the spoke rendered differently.  This 
is similar to adding error bars to a plot in xYplot -- only in polar 
coordinates.

==============================================
   Randall Pruim
   Dept. of Biostatistics, University of Michigan

   email:  rpruim at umich.edu
   phone:  734.615.9825



From pensterfuzzer at yahoo.de  Wed Feb 16 18:20:43 2005
From: pensterfuzzer at yahoo.de (Werner Wernersen)
Date: Wed, 16 Feb 2005 18:20:43 +0100 (CET)
Subject: [R] Easy cut & paste from Excel to R?
Message-ID: <20050216172043.2153.qmail@web25801.mail.ukl.yahoo.com>

Thank you all very much for the answers!
The read.table / read.delim2 commands are exactly what
I was looking for to get 
a couple of numbers or a little matrix quickly into R
without creating an extra 
text file every time.

And it works the other way around as well:
write.table(x, file("clipboard"), sep="\t")
Fantastic!

Thanks again,
   Werner

Nick Drew wrote:
> I've had good luck with the scan() function when I
> want to get a few numbers from Excel into R quickly
to
> use it as a calculator. CAVEAT: you have to have the
> numbers you want to copy in a column not a row in
> Excel. For example:
> 
> In Excel your data are in a column as follows:
> Col A
> 1
> 2
> 3
> 
> Then copy the 3 cells (e.g. 1, 2,3) in Excel and
open
> R and type in:
> 
>>data <- scan()
> 
> 
> Then Paste using Ctrl-V. Hit the Enter key. You know
> have an object called "data" that you can use and
> manipulate in R.
> 
> I've taken this even further by creating an R
function
> that will take a column of numbers from Excel and
then
> scan() them into R, create a matrix, and then
perform
> a Chi-square test. Let me know if you'd like to know
> more. I'm a beginner and if I can do so can you!!
> 
> ~Nick
> 
> 
> 
> 		
> __________________________________ 



From fxx103 at hotmail.com  Wed Feb 16 17:28:24 2005
From: fxx103 at hotmail.com (Francisca xuan)
Date: Wed, 16 Feb 2005 11:28:24 -0500
Subject: [R] about library(boot)
In-Reply-To: <Pine.LNX.4.61.0501120915030.10520@gannet.stats>
Message-ID: <BAY102-F40D89F230E67DC3F0F84649B6C0@phx.gbl>

Dear Sir/Madam:

I try to use the library boot to bootstrap the median of a data set. Can 
anybody tell me why this doesn't work? Thanks.

library(boot)
x=rnorm(100)
boot(x,median,999)

I know I can write a simple code for bootstrapping myself. but I am so 
curious to know why the above code does not work.

F. Xuan



From ripley at stats.ox.ac.uk  Wed Feb 16 18:48:49 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 16 Feb 2005 17:48:49 +0000 (GMT)
Subject: [R] glmm with negative binomial
In-Reply-To: <5.2.1.1.2.20050216085655.03054a50@gwia.pen.wisc.edu>
References: <5.2.1.1.2.20050216085655.03054a50@gwia.pen.wisc.edu>
Message-ID: <Pine.LNX.4.61.0502161741220.3656@gannet.stats>

On Wed, 16 Feb 2005, Brian Aukema wrote:

> At present, can generalized linear mixed models with negative binomial 
> distribution and estimating the shape parameter be fit using R?  I am aware 
> of glm.nb but am wondering about incorporation of mixed effects.

I am not aware of anyone who knows how to do that reasonably robustly. 
If the model were simple enough you might be able to do numerical 
integration well enough to get a likelihood to maximize, but we usually 
have enough trouble with a Poisson model (and our experiments seem to show 
that other people's software has much more trouble).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From vision1_quest at yahoo.com  Wed Feb 16 18:48:52 2005
From: vision1_quest at yahoo.com (Jeff Knoblett)
Date: Wed, 16 Feb 2005 09:48:52 -0800 (PST)
Subject: [R] (no subject)
Message-ID: <20050216174852.74892.qmail@web20027.mail.yahoo.com>

Please remove me from the mailing list!



From Terji78 at yahoo.com  Wed Feb 16 18:50:28 2005
From: Terji78 at yahoo.com (Terji Petersen)
Date: Wed, 16 Feb 2005 18:50:28 +0100
Subject: [R] Setting log(0) to 0
In-Reply-To: <XFMail.050216132653.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050216132653.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <421387E4.9060406@yahoo.com>

Thank you both of you, Kenneth and Ted:-). Log1p(x) is not what I asked 
for, but it is better:-D

And Ted, thanks for your thoughts on funcional form. I'm just starting 
out with R, and feel like I've barely scratched the surface of the 
program. I have never in my life done a non-linear regression, but that 
will soon change:-)

(Ted Harding) wrote:

>On 16-Feb-05 Terji Petersen wrote:
>  
>
>>Hi,
>>
>>I'm trying to do  a regression like this:
>>
>>wage.r = lm( log(WAGE) ~ log(EXPER)
>>
>>where EXPER is an integer that goes from 0 to about 50.
>>EXPER contains some zeros, so you can't take its log,
>>and the above regression therefore fails. I would like
>>to make R accept log(0) as 0, is that  possible?
>>Or do I have first have to turn the 0's into 1's to be
>>able to do the above regression?
>>    
>>
>
>If treating "log(0)" as 0 would do your business, then a
>preliminary pass to turn "0" into "1" would be the simplest
>method. It only takes 1 line.
>
>This is a bit of a Catch-22. It looks like you're trying
>to fit a power law
>
>  WAGE = A*(EXPER^B)
>
>(where I guess "EXPER" means experience) and you've got
>some cases with no experience. Whether your work-round
>is appropriate depends in part on the unit of "experience".
>If it's in years, then a case with 3 months experience
>would have log(EXPER) = -1.39, thereby weighing in with
>a lesser value than someone with zero experience, on your
>proposal.
>
>On the other hand, if it's in days, then
>
>  log(EXPER) = log(91) = 4.51
>
>and even someone with only a week has log(EXPER) = 1.95
>
>But your log(0) = 0 data would be sitting there all the
>time, whatever the scale of EXPER, and so would have an
>influence on your regression which depended on this scale.
>You might have to consider using log(0) --> const
>where the "const" is such as to give reasonable results,
>given what comes out of the rest of the data (where EXPER>0).
>
>The fundamental problem is that your power law predicts
>zero wage for zero experience, which is rarely the case.
>
>You might do better to try a non-linear fit
>
>  WAGE = W0 + A*(EXPER^B)
>
>for which sort of thing there are several resources in R,
>perhaps the simplest being 'nls'.
>
>For what you have in your installed packages, try a
>
>  help.search("nonlinear")
>
>Once you open this door, you can try perhaps more realistic
>non-linear models, including what can be found amongst the
>"SS....." (Self-Starting) models in "nls" --  have a look
>at what's listed by
>
>  library(help=nls)
>
>as well as what is allowed according to "?nls".
>
>Such models would allow an initial (zero-experience) wage,
>perhaps not changing much for some time, then rising more
>rapidly once an "experience threshold" is passed, then
>flattening out to a lower slope over a longer time (something
>which many of us have experience of). And even ultimately
>ending to decrease ...
>
>Hoping this helps,
>Ted.
>
>
>--------------------------------------------------------------------
>E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
>Fax-to-email: +44 (0)870 094 0861
>Date: 16-Feb-05                                       Time: 13:26:53
>------------------------------ XFMail ------------------------------
>
>  
>



From ramasamy at cancer.org.uk  Wed Feb 16 18:51:06 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Wed, 16 Feb 2005 17:51:06 +0000
Subject: [R] Passing colnames to graphics title
In-Reply-To: <Pine.LNX.4.44.0502161354480.4164-100000@env-pc-phd13>
References: <Pine.LNX.4.44.0502161354480.4164-100000@env-pc-phd13>
Message-ID: <1108576267.30916.7.camel@ramasamy.stats>

Either set the 'main' or 'xlab' in the hist(). See help("par") for more
information on graphical arguments or help("hist").

mat <- matrix( rnorm(1000), nc=5 )
colnames(mat) <- LETTERS[1:ncol(mat)]

for( i in 1:ncol(mat) ){
 hist( mat[ ,i], 
       main=paste( "Histogram of data from column ", 
       colnames(mat)[i] ),  xlab="" )
}




On Wed, 2005-02-16 at 13:56 +0000, Laura Quinn wrote:
> Hi,
> 
> Just a quick query - if I'm creating a function to produce a number of
> histograms per page of output (one per column from a matrix), how can I
> pass the column name of the matrix into the title (or indeed to form part
> of the x-axis label)?
> 
> TIA,
> Laura
> 
> Laura Quinn
> Institute of Atmospheric Science
> School of Earth and Environment
> University of Leeds
> Leeds
> LS2 9JT
> 
> tel: +44 113 343 1596
> fax: +44 113 343 6716
> mail: laura at env.leeds.ac.uk
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From cafanselmo12 at yahoo.com.br  Wed Feb 16 19:26:11 2005
From: cafanselmo12 at yahoo.com.br (=?iso-8859-1?q?C=E9zar=20Freitas?=)
Date: Wed, 16 Feb 2005 15:26:11 -0300 (ART)
Subject: [R] Re: histogram ... postscript - width of lines / plot margins
In-Reply-To: <42135D9B.1050400@statistik.uni-dortmund.de>
Message-ID: <20050216182611.23140.qmail@web14525.mail.yahoo.com>

Thanks. You answer works well. But this time I get 2
problems:

- because the region of plot is shorter then the
default, the width of lines of plots is larger (see
the two attached pictures .ps) and the lwd command
don't deals with it;
- the plot region contains big margins (the picture
has 3x3 inches, but the plot uses only 2x2)

The code:

  par(fin=c(3,3))
  scores<-c(2.0, 0.0, 5.0, 5.0, 5.0, 2.0, 0.0, 5.0,
2.5, 4.0, 5.0, 0.0, 5.0, 0.0, 2.0, 5.0, 5.0, 2.0, 3.0,
3.0)
  q<-summary(scores)
  gra<-hist(scores, breaks=((0:11)/2-.2), plot=FALSE)
  yy<-ceiling(max(gra$counts)/10)*10
  yz<-yy/12

  postscript("test.ps", width=3, height=3,
horizontal=FALSE, family="Times", paper="special")

  hist(scores, breaks=((0:11)/2-.2), xlim=c(-1,6),
ylim=c(-yz,yy), main="scores", ylab="Freq",
xlab="math", cex.axis=.3, cex.main=.3, cex.sub=.3,
cex.lab=.3, mgp=c(1.5,.5,0))
  boxplot(scores, horizontal=1, add=1, at=-2*yz/3,
boxwex=1.5*yz, bty="n", axes=FALSE)
  points(q[4], -2*yz/3, col=1, pch="+", cex=.5)

  dev.off()



 --- Uwe Ligges <ligges at statistik.uni-dortmund.de>
escreveu: 
> C?zar Freitas wrote:
> 
> > Hi, all. I need plot a boxplot under a histogram
> like
> > below, but some configs are troubled:
> > 
> > - the boxplot contours the plot, even if I put
> > bty="n", modifying the histogram plot;
> 
> You want to set  "axes = FALSE" in boxplot()
> BTW: What is q[4]  in your call to points()?
> 
> 
> Uwe Ligges
> 
> 
> > - I changed the configs of axis to do a 3x3 inches
> > plot, but the result is 2 different axis.
> > 
> > For example, the code below ilustrates this...
> > 
> > 
> >   scores<-c(2.0, 0.0, 5.0, 5.0, 5.0, 2.0, 0.0,
> 5.0,
> > 2.5, 4.0, 5.0, 0.0, 5.0, 0.0, 2.0, 5.0, 5.0, 2.0,
> 3.0,
> > 3.0)
> > 
> >   postscript("test.ps", width=3, height=3,
> > horizontal=FALSE, family="Times", paper="special")
> > 
> >   gra<-hist(scores, breaks=((0:11)/2-.2),
> > xlim=c(-1,6), plot=FALSE, cex.axis=.5,
> cex.main=.5,
> > cex.sub=.5, cex.lab=.5, mgp=c(1.5,.5,0))
> > 
> >   yy<-ceiling(max(gra$counts)/10)*10
> >   
> >   hist(scores, breaks=((0:11)/2-.2), xlim=c(-1,6),
> > ylim=c(0,yy), main="scores", ylab="Freq",
> xlab="math",
> > cex.axis=.5, cex.main=.5, cex.sub=.5, cex.lab=.5,
> > mgp=c(1.5,.5,0))
> >   boxplot(scores, horizontal=1, add=1, at=-yy/50,
> > boxwex=yy/20, bty="n", cex.axis=.5, cex.main=.5,
> > cex.sub=.5, cex.lab=.5, ann=FALSE)
> >   points(q[4], -yy/50, col=1, pch="x", cex=.2)
> > 
> >   dev.off()
> > 
> > 
> > Thanks,
> > C.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
>  


	
	
		
_______________________________________________________ 
-------------- next part --------------
A non-text attachment was scrubbed...
Name: via_postscript.ps
Type: application/postscript
Size: 6629 bytes
Desc: via_postscript.ps
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20050216/5b70d984/via_postscript.ps
-------------- next part --------------
A non-text attachment was scrubbed...
Name: via_R.ps
Type: application/postscript
Size: 32296 bytes
Desc: via_R.ps
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20050216/5b70d984/via_R.ps

From andy_liaw at merck.com  Wed Feb 16 20:00:14 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 16 Feb 2005 14:00:14 -0500
Subject: [R] Sampling given a table of percentages?
Message-ID: <3A822319EB35174CA3714066D590DCD50994E6FF@usrymx25.merck.com>

Wouldn't sample(length(V), prob=V) do?

Andy

> From: Ben Hyde
> 
> I have a vector V.  sum(V) = 100, i.e. it's percentages.  
> length(V) is 
> large.  I wish to generate samples (with replacement is fine), 
> integers, in the range 1:length(V) who's distribution is 
> driven by the 
> distribution implied by the percentages in V.    V is unsorted, but 
> that could change.  I'd rather not be too specific about the 
> distribution of V.    I can certainly solve the problem 
> intersecting my 
> thin knowledge of R with my skills in programming, but I suspect this 
> is trivial to somebody fluent in R.   Suggestions?  Thanks!  - ben
> 
> ----
> http://enthusiasm.cozy.org/   -- blog
> tel:+1-781-240-2221  -- mobile, et.al.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From jjarabek at stat.ufl.edu  Wed Feb 16 20:16:07 2005
From: jjarabek at stat.ufl.edu (Jamie Jarabek)
Date: Wed, 16 Feb 2005 14:16:07 -0500
Subject: [R] problem with se.contrast()
Message-ID: <42139BF7.2090809@stat.ufl.edu>

I am having trouble getting standard errors for contrasts using se.contrast() in 
what appears to be a simple case to me. The following test example illustrates 
my problem:

Lab <- factor(rep(c("1","2","3"),each=12))
Material <- factor(rep(c("A","B","C","D"),each=3,times=3))
Measurement <- c(12.20,12.28,12.16,15.51,15.02,15.29,18.14,18.08,18.21,18.54,18.36
,18.45,12.59,12.30,12.67,14.98,15.46,15.22,18.54,18.31,18.60,19.21,18.77
,18.69,12.72,12.78,12.66,15.33,15.19,15.24,18.00,18.15,17.93,18.88,18.12,18.03)

testdata <- data.frame(Lab,Material,Measurement)
rm(list=c("Lab","Material","Measurement"))

test.aov <- with(testdata,aov(Measurement ~ Material + Error(Lab/Material)))

This gives me the desired ANOVA table. I next want to get the standard
errors for certain contrasts and following the help page for
se.contrast() I tried the following but I get an error:

>se.contrast(test.aov,list(Material=="A",Material=="B",Material=="C",Material=="D"),coef=c(1,1,-1,-1),data=testdata)
Error in matrix(0, length(asgn), ncol(effects), dimnames = list(nm[1 + :
         length of dimnames [1] not equal to array extent

I have tested this on R 2.0.1 on Windows XP and Solaris and get the same
error on both systems. I am unsure as to what I am doing wrong here. Thanks for 
any help.

Jamie Jarabek



From mail at bymouth.com  Wed Feb 16 20:48:59 2005
From: mail at bymouth.com (Stephen Choularton)
Date: Thu, 17 Feb 2005 06:48:59 +1100
Subject: [R] running out of memory
Message-ID: <000001c51460$8fff8230$9701a8c0@Tablet>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050217/6a78fdbd/attachment.pl

From deepayan at stat.wisc.edu  Wed Feb 16 21:11:53 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Wed, 16 Feb 2005 14:11:53 -0600
Subject: [R] panel/prepanel for polar plots ala xYplot
In-Reply-To: <2ad03ae5d4d494de9d0be01ae50b71ea@calvin.edu>
References: <2ad03ae5d4d494de9d0be01ae50b71ea@calvin.edu>
Message-ID: <200502161411.53620.deepayan@stat.wisc.edu>

On Wednesday 16 February 2005 11:11, Randall Pruim wrote:
> First a bit of background:
>
> After doing a search for a flexible polar plot function and coming up
> empty, I have begun writing one myself.  Since I am new to writing
> mid-level graphics routines, this has required some learning about
> lattice, grid and related things.
>
> I am to the point where I have a workable proof of concept, but still
> need to make some improvements.  My goal is to have something akin to
> a polar version of xYplot.  Although the particular plot I need
> requires some customization of that idea, the use of Cbind() has made
> that fairly easy.
>
> Now some questions:
>
> 1) What are the best sources of documentation for lattice/grid?  I
> have found a few things by googling, but mostly I have been reverse
> engineering code and experimenting to figure out how things work.

Why is the package documentation insufficient? There are some grid 
articles on Paul's website, and S-PLUS Trellis documentation mostly 
applies to lattice.

> 2) What is the best way to generate "axis" and labels for them?
>
>  Currently my radplot is a wrapper for xyplot that (a) turns off the
> axes and labels, and (b) calls xyplot with panel.radplot,
> prepanel.radplot, and radplot.superpose replacing the obvious things.
> I am generating the "axes" (concentric circles and peripheral labels)
> in panel.radplot, but this means that they are redrawn for each group
> when there is superposition.  Furthermore, there seems to be some
> jittering, so besides inefficiency, the result is not crisp.

If you want something that's to be done once per panel and something 
else that's to be done once per group within panel, I would suggest you 
separate out the per-group part in a function and call that multiple 
times from your panel function. An example of this is panel.dotplot, 
which essentially looks like:

function (x, y, 
          levels.fos = unique(y), 
          groups = NULL,
          ...)
{
    panel.abline(h = levels.fos)
    if (is.null(groups)) panel.xyplot(x = x, y = y, ...)
    else panel.superpose(x = x, y = y, groups = groups, 
                         panel.groups = panel.xyplot, ...)
}

This ensures that the reference lines are drawn only once per panel (and 
not once per group, which would have overwritten the dots from earlier 
groups).

>  How are axes generated in xyplot?  

In xyplot, the axes are generated separately from the panel function, 
because they are outside the panels (and the same axes are associated 
with multiple panels).  It sounds like you want something similar to 
cloud, where the axes are part of each panel. In that case, your panel 
function needs to be responsible for drawing the axes.

>  To do this correctly will I have 
> to go deeper and make a new version of trellis.skeleton or make a new
> call to it?  

I don't see why.

> If I put the code into prepanel.radplot will it be 
> executed once per panel or once per group in each panel?

The prepanel function should absolutely not do any plotting.

> 3) I'd be happy to receive any other suggestions for how to approach
> the design of a robust, formula-based (including xYplot-like options)
> radial/polar plot.  I'd also be happy to hear of any packages that
> include something heading in this direction, if they exist and I just
> didn't locate them.
>
> Thanks in advance for any assistance.
>
> ---rjp
>
> PS.  For the curioius, the plot I am designing has a function call
> like
>
>  radplot(Cbind(y,ratio)~x|g, groups=h, ...)
>
> where y is numeric and x may be numeric or a factor.  The resulting
> plot has "spokes" for each value of x with length y, the last 1 -
> min(ratio, 1/ratio) fraction of the spoke rendered differently.  This
> is similar to adding error bars to a plot in xYplot -- only in polar
> coordinates.
>
> ==============================================
>    Randall Pruim
>    Dept. of Biostatistics, University of Michigan
>
>    email:  rpruim at umich.edu
>    phone:  734.615.9825
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From uofiowa at gmail.com  Wed Feb 16 21:26:47 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Wed, 16 Feb 2005 15:26:47 -0500
Subject: [R] intersection or == of date vectors
Message-ID: <3f87cc6d0502161226364278be@mail.gmail.com>

I have a vector of unique dates v1, and a vector of unique dates v2
(the vectors are of different lengths). How do I find out the count of
elements that matches between the two vectors?



From Ted.Harding at nessie.mcc.ac.uk  Wed Feb 16 21:37:15 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 16 Feb 2005 20:37:15 -0000 (GMT)
Subject: [R] problem with da.mix
In-Reply-To: <1108572869.42137ac5496ee@webmail.polytech-lille.fr>
Message-ID: <XFMail.050216203715.Ted.Harding@nessie.mcc.ac.uk>

On 16-Feb-05 Stephanie.Tomczak at eleves.polytech-lille.fr wrote:
> We use the mix package and we have a problem with the DA
> function. We aren't sure, but it's maybbe a memory problem.
> 
> We have done:
>> Ent<--read.table("C:/.../File.txt")
>> attach(Ent)
>> Ent 
>     V1  V2   V3  V4 ... V16  V17
> 1    1   1   2   6      18   18 
> 2    1   1   1   NA     14   17
> 3    1   1   2   1      16   14
> ....
> 199  2   1   NA  7      19   18
> 200  2   1   3   2      14   17
> 
>> EntPrelim<-prelim.mix(as.matrix(Ent),9)
>> EntEM<-em.mix(EntPrelim,maxits=500)
>> rngseed(1234567)
>> EntDA<-da.mix(EntPrelim, EntEM, steps=100, showits=TRUE)
>  Steps of data Augmentation:
> 1... Error in da.mix(EntPrelim, EntEM, steps=100; showits=TRUE):
>     Improper posterior--empty cells

Dear St?phanie,

This problem is closely related to the problem reported yesterday
by Delphine Gille from your same institution:

>> From: Delphine.Gille at eleves.polytech-lille.fr
>> To: r-help at stat.math.ethz.ch
>> Subject: [R] memory problem with package mix
>> Date: Tue, 15 Feb 2005 15:23:08 +0100
>> 
>> Hello,
>> 
>> I think we have a memory problem with  em.mix.
>> 
>> We have done:
>>
>> >library(mix)
>> >Manq <- read.table("C:/.../file.txt")
>> >attach(Manq)
>> >Manq
>> >    V1 V2 V3 V4 .............V27
>> > 1  1  1  1  1...........
>> > 2  1 NA  3  6
>> > 3  1  2  6  2
>> > ...
>> > ...
>> > 300  2  NA  6  2...........
>> 
>> > Essaimanq <-prelim.mix(as.matrix(Manq),5)
>> > test <- em.mix(Essaimanq)
>>     error cannot allocated vector of size 535808 KB
>>     in addition : warning message
>>                   reached total allocation of 509MB


The reason is almost certainly the same fact that I pointed
out in my reply to Delphine: you have 9 categorical variables,
each necessarily at at least 2 levels (and in your case at
least one has >=3 levels and at least one has >=6 levels)
so you have at least (2^7)*3*6 = 2304 cells (possibly many
more, depending on the numbers of levels in the variables)
in your unrestricted model for the categorical variables
(as implied by your usage of em.mix and da.mix).

With only 200 rows of data, there will (even if it is only
2304 cells) be at least 2104 of them empty (i.e. with no
data falling in them). Therefore, given the improper Dirichlet
prior which da.mix uses by default, you will almost certainly
end up with an improper posterior distribution as a result of
your many empty cells, which is just what your error message
is telling you.

With so few data, you need to severely restrict the level of
interaction allowed for the categorical variables (and use
ecm.mix instead of em.mix, dabipf.mix instead of da.mix).

In the best possible case (7 variables at 2 levels, one at 3,
one at 6) implied by your data excerpt above, you need
7 + 2 + 5 = 14 parameters at a minimum (no-interaction or
complete-independence model). If you admit the first-order
(2-factor) interactions as well, you need 84 parameters
(I hope I have calculated this right!). Going to 2nd-order
(3-factor) will surely take you over your data size of 200
(I haven't worked this one out: maybe there's a snappy R
function for this sort of thing!). But if your variables
have more levels than the minimum I have assumed (based
on your data excerpt) then the situation will rapidly
get much worse.

Another approach might be to consider using an informative
("proper") prior distribution for the Dirichlet probabilities,
but unless you are very careful you risk adopting something
which is not realistic for your problem. You can do this with
both da.mix with em.mix (provided em.mix works with your sparse
data, which it didn't for Delphine) and da.bipf.mix with ecm.mix.

See also the explanations in "?da.mix" and "?dabipf.mix",
section "Details", which refer to just the kind of problem
you are having.

Hoping this helps,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 16-Feb-05                                       Time: 20:37:15
------------------------------ XFMail ------------------------------



From ramasamy at cancer.org.uk  Wed Feb 16 21:51:01 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Wed, 16 Feb 2005 20:51:01 +0000
Subject: [R] Sampling given a table of percentages?
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E6FF@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E6FF@usrymx25.merck.com>
Message-ID: <1108587061.6546.9.camel@dhcp-63.ccc.ox.ac.uk>

yes but add 'replace=TRUE' into that statement to sample with
replacement. 


On Wed, 2005-02-16 at 14:00 -0500, Liaw, Andy wrote:
> Wouldn't sample(length(V), prob=V) do?
> 
> Andy
> 
> > From: Ben Hyde
> > 
> > I have a vector V.  sum(V) = 100, i.e. it's percentages.  
> > length(V) is 
> > large.  I wish to generate samples (with replacement is fine), 
> > integers, in the range 1:length(V) who's distribution is 
> > driven by the 
> > distribution implied by the percentages in V.    V is unsorted, but 
> > that could change.  I'd rather not be too specific about the 
> > distribution of V.    I can certainly solve the problem 
> > intersecting my 
> > thin knowledge of R with my skills in programming, but I suspect this 
> > is trivial to somebody fluent in R.   Suggestions?  Thanks!  - ben
> > 
> > ----
> > http://enthusiasm.cozy.org/   -- blog
> > tel:+1-781-240-2221  -- mobile, et.al.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From p.dalgaard at biostat.ku.dk  Wed Feb 16 21:59:48 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 16 Feb 2005 21:59:48 +0100
Subject: [R] (no subject)
In-Reply-To: <20050216174852.74892.qmail@web20027.mail.yahoo.com>
References: <20050216174852.74892.qmail@web20027.mail.yahoo.com>
Message-ID: <x2acq4mda3.fsf@biostat.ku.dk>

Jeff Knoblett <vision1_quest at yahoo.com> writes:

> Please remove me from the mailing list!

Well, the list manager *might* remove you, *when* he gets back from
his holiday, *if* he notices it and is in a good mood. If you want a
quicker reaction, please use the web interface as indicated in the
footer:

> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From gunter.berton at gene.com  Wed Feb 16 22:36:59 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 16 Feb 2005 13:36:59 -0800
Subject: [R] intersection or == of date vectors
In-Reply-To: <3f87cc6d0502161226364278be@mail.gmail.com>
Message-ID: <200502162137.j1GLaxbN027992@ohm.gene.com>

length(intersect(v1,v2))

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Omar Lakkis
> Sent: Wednesday, February 16, 2005 12:27 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] intersection or == of date vectors
> 
> I have a vector of unique dates v1, and a vector of unique dates v2
> (the vectors are of different lengths). How do I find out the count of
> elements that matches between the two vectors?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From PAlspach at hortresearch.co.nz  Wed Feb 16 23:02:55 2005
From: PAlspach at hortresearch.co.nz (Peter Alspach)
Date: Thu, 17 Feb 2005 11:02:55 +1300
Subject: [R] Positive log-likelihood in lme
Message-ID: <s2147a1e.082@hra2.marc.hort.cri.nz>


Kia ora

I'm a using lme (from nlme package) with data similar to the Orthodont dataset and am getting positive log-likelihoods (>100).  This seems usual and I wondered if someone could offer a possible explanation.

I can supply a sample dataset if requested, but I feel almost certain that this question has been asked and answered recently.  However, I can find no trace of it in the mail archives (although I have spent several hours reading lots of other interesting things :-)).

Thanks .........

Peter Alspach


______________________________________________________

The contents of this e-mail are privileged and/or confidenti...{{dropped}}



From Benjamin.Osborne at uvm.edu  Wed Feb 16 23:04:13 2005
From: Benjamin.Osborne at uvm.edu (Benjamin M. Osborne)
Date: Wed, 16 Feb 2005 17:04:13 -0500
Subject: [R] scaling axes when plotting multiple data sets
Message-ID: <1108591453.4213c35d1c4a2@webmail.uvm.edu>


1) When adding additional data sets to a plot using "plot" followed by "lines",
is there a way to automate the scaling of the axes to allow for all data sets
to fit within the plot area?

2) I attempted to solve this by setting
xlim=c(min(c(data1,data2,data3)),max(c(data1,data2,data3)))
however, there are some NAs and Infs in these data sets, and min(data1) and
max(data1) both return NA, as with data2 and data3.  (These are time series).

Thank you,
Ben Osborne

-- 
Botany Department
University of Vermont
109 Carrigan Drive
Burlington, VT 05405

benjamin.osborne at uvm.edu
phone: 802-656-0297
fax: 802-656-0440



From jfox at mcmaster.ca  Wed Feb 16 23:07:23 2005
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 16 Feb 2005 17:07:23 -0500
Subject: [R] about library(boot)
In-Reply-To: <BAY102-F40D89F230E67DC3F0F84649B6C0@phx.gbl>
Message-ID: <web-82977086@cgpsrv2.cis.mcmaster.ca>

Dear Francisca,

On Wed, 16 Feb 2005 11:28:24 -0500
 "Francisca xuan" <fxx103 at hotmail.com> wrote:
> Dear Sir/Madam:
> 
> I try to use the library boot to bootstrap the median of a data set.
> Can anybody tell me why this doesn't work? Thanks.
> 
> library(boot)
> x=rnorm(100)
> boot(x,median,999)
> 
> I know I can write a simple code for bootstrapping myself. but I am
> so curious to know why the above code does not work.

Take a closer look at ?boot: The function that computes the statistic
that you're bootstrapping (in a simple situation like this) should take
two arguments -- the data and an index vector. Thus,

boot.median <- function(x, i) median(x[i])
result <- boot(x, boot.median, 999)

I hope this helps,
 John



From spencer.graves at pdf.com  Wed Feb 16 23:19:41 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 16 Feb 2005 14:19:41 -0800
Subject: [R] Positive log-likelihood in lme
In-Reply-To: <s2147a1e.082@hra2.marc.hort.cri.nz>
References: <s2147a1e.082@hra2.marc.hort.cri.nz>
Message-ID: <4213C6FD.3080704@pdf.com>

      The "likelihood" is the probability density function, which can be 
greater than 1 for continuous distributions with a fairly narrow 
spread.  For discrete distributions, the density never exceeds 1, in 
which case the log(likelihood) would always be negative(*). 

      hope this helps. 
      spencer graves
(*) If you are using measure-theoretic probability with some 
non-standard measure, it might be possible to get a discrete probability 
density greater than 1.  One might want to use such as a class exercise, 
but I can't think of a real world application for such. 

Peter Alspach wrote:

>Kia ora
>
>I'm a using lme (from nlme package) with data similar to the Orthodont dataset and am getting positive log-likelihoods (>100).  This seems usual and I wondered if someone could offer a possible explanation.
>
>I can supply a sample dataset if requested, but I feel almost certain that this question has been asked and answered recently.  However, I can find no trace of it in the mail archives (although I have spent several hours reading lots of other interesting things :-)).
>
>Thanks .........
>
>Peter Alspach
>
>
>______________________________________________________
>
>The contents of this e-mail are privileged and/or confidenti...{{dropped}}
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From i.visser at uva.nl  Wed Feb 16 23:24:45 2005
From: i.visser at uva.nl (Ingmar Visser)
Date: Wed, 16 Feb 2005 23:24:45 +0100
Subject: [R] Positive log-likelihood in lme
In-Reply-To: <s2147a1e.082@hra2.marc.hort.cri.nz>
Message-ID: <BE3986BD.21D3%i.visser@uva.nl>

Hi Peter,
Why do you think positive log-likelihoods are unusual?
consider:
> dnorm(1,1,0.1)
[1] 3.989423
> log(dnorm(1,1,0.1))
[1] 1.383647
Any log-likelihood would be a sum of such terms.
Hth, ingmar


On 2/16/05 11:02 PM, "Peter Alspach" <PAlspach at hortresearch.co.nz> wrote:

> 
> Kia ora
> 
> I'm a using lme (from nlme package) with data similar to the Orthodont dataset
> and am getting positive log-likelihoods (>100).  This seems usual and I
> wondered if someone could offer a possible explanation.
> 
> I can supply a sample dataset if requested, but I feel almost certain that
> this question has been asked and answered recently.  However, I can find no
> trace of it in the mail archives (although I have spent several hours reading
> lots of other interesting things :-)).
> 
> Thanks .........
> 
> Peter Alspach
> 
> 
> ______________________________________________________
> 
> The contents of this e-mail are privileged and/or confidenti...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Ingmar Visser
Roetersstraat 15
1018 WB Amsterdam
The Netherlands
i.visser at uva.nl
http://users.fmg.uva.nl/ivisser/



From jfox at mcmaster.ca  Wed Feb 16 23:27:40 2005
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 16 Feb 2005 17:27:40 -0500
Subject: [R] scaling axes when plotting multiple data sets
In-Reply-To: <1108591453.4213c35d1c4a2@webmail.uvm.edu>
Message-ID: <web-82979334@cgpsrv2.cis.mcmaster.ca>

Dear Ben,

On Wed, 16 Feb 2005 17:04:13 -0500
 "Benjamin M. Osborne" <Benjamin.Osborne at uvm.edu> wrote:
> 
> 1) When adding additional data sets to a plot using "plot" followed
> by "lines",
> is there a way to automate the scaling of the axes to allow for all
> data sets
> to fit within the plot area?
> 

Not, to my knowledge, after the fact.

> 2) I attempted to solve this by setting
> xlim=c(min(c(data1,data2,data3)),max(c(data1,data2,data3)))
> however, there are some NAs and Infs in these data sets, and
> min(data1) and
> max(data1) both return NA, as with data2 and data3.  (These are time
> series).
> 

Specifying, e.g., min(data1, data2, data3, na.rm=TRUE) will get rid of
the NAs, but it's not obvious that Infs should be removed, since if one
is present, shouldn't max be Inf?

If you want to get rid of the Infs, however, you could change them into
NAs, as in data1[data1 == Inf] <- NA, and proceed as above.

I hope this helps,
 John

> Thank you,
> Ben Osborne
> 
> -- 
> Botany Department
> University of Vermont
> 109 Carrigan Drive
> Burlington, VT 05405
> 
> benjamin.osborne at uvm.edu
> phone: 802-656-0297
> fax: 802-656-0440
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada
http://socserv.mcmaster.ca/jfox/



From spencer.graves at pdf.com  Wed Feb 16 23:28:47 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 16 Feb 2005 14:28:47 -0800
Subject: [R] scaling axes when plotting multiple data sets
In-Reply-To: <1108591453.4213c35d1c4a2@webmail.uvm.edu>
References: <1108591453.4213c35d1c4a2@webmail.uvm.edu>
Message-ID: <4213C91F.6060206@pdf.com>

      I just did "?min" and found an argument na.rm, which when TRUE 
causes "min" to ignore NAs.  Also, "See Also" for "?min" mentions 
"range", which returns a 2-vector consisting of both min and max.  The 
function "range" also accepts the na.rm argument.  AND the documentation 
for "range" includes a simple example that seems to demonstrate exactly 
what you are requesting here. 

      hope this helps. 
      spencer graves

Benjamin M. Osborne wrote:

>1) When adding additional data sets to a plot using "plot" followed by "lines",
>is there a way to automate the scaling of the axes to allow for all data sets
>to fit within the plot area?
>
>2) I attempted to solve this by setting
>xlim=c(min(c(data1,data2,data3)),max(c(data1,data2,data3)))
>however, there are some NAs and Infs in these data sets, and min(data1) and
>max(data1) both return NA, as with data2 and data3.  (These are time series).
>
>Thank you,
>Ben Osborne
>
>  
>



From davidr at rhotrading.com  Wed Feb 16 23:37:14 2005
From: davidr at rhotrading.com (davidr@rhotrading.com)
Date: Wed, 16 Feb 2005 16:37:14 -0600
Subject: [R] scaling axes when plotting multiple data sets
Message-ID: <12AE52872B5C5348BE5CF47C707FF53A502468@rhosvr02.rhotrading.com>

min, max, and range (and many other functions) take the na.rm parameter
to ignore NAs, but the Infs must be removed by hand as far as I know:

dat <- c(data1,data2,data3,data4)
dat <- dat[(dat!=Inf)&(dat!=(-Inf))]
rng <- range(dat, na.rm=TRUE)
then use xlim = rng

-- David Reiner

-----Original Message-----
From: Benjamin M. Osborne [mailto:Benjamin.Osborne at uvm.edu] 
Sent: Wednesday, February 16, 2005 4:04 PM
To: r-help at stat.math.ethz.ch
Subject: [R] scaling axes when plotting multiple data sets


1) When adding additional data sets to a plot using "plot" followed by
"lines",
is there a way to automate the scaling of the axes to allow for all data
sets
to fit within the plot area?

2) I attempted to solve this by setting
xlim=c(min(c(data1,data2,data3)),max(c(data1,data2,data3)))
however, there are some NAs and Infs in these data sets, and min(data1)
and
max(data1) both return NA, as with data2 and data3.  (These are time
series).

Thank you,
Ben Osborne

-- 
Botany Department
University of Vermont
109 Carrigan Drive
Burlington, VT 05405

benjamin.osborne at uvm.edu
phone: 802-656-0297
fax: 802-656-0440

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From rchebolu at yahoo.com  Thu Feb 17 00:03:38 2005
From: rchebolu at yahoo.com (Radha Chebolu)
Date: Wed, 16 Feb 2005 15:03:38 -0800 (PST)
Subject: [R] Unable to create histograms
Message-ID: <20050216230338.77859.qmail@web30505.mail.mud.yahoo.com>

Hi,
could someone pelase help me with this?

My data set's name is db1(say) and one of the
variables is var1. I gave the command:
hist(db1$var1). The values of Var1 are numbers.
I got an error which says: 'x' must be numeric.
Sometimes it works for other datasets and it's not
working for this dataset. Also, does R let us import
data from an excel spreadhsheet?

Thanks,
Radha



From jfox at mcmaster.ca  Thu Feb 17 00:26:58 2005
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 16 Feb 2005 18:26:58 -0500
Subject: [R] Unable to create histograms
In-Reply-To: <20050216230338.77859.qmail@web30505.mail.mud.yahoo.com>
Message-ID: <web-82984320@cgpsrv2.cis.mcmaster.ca>

Dear Radha,

On Wed, 16 Feb 2005 15:03:38 -0800 (PST)
 Radha Chebolu <rchebolu at yahoo.com> wrote:
> Hi,
> could someone pelase help me with this?
> 
> My data set's name is db1(say) and one of the
> variables is var1. I gave the command:
> hist(db1$var1). The values of Var1 are numbers.
> I got an error which says: 'x' must be numeric.
> Sometimes it works for other datasets and it's not
> working for this dataset.

If db1$var1 really were numeric, you wouldn't get this error. It's not
possible to tell what went wrong without more information.

> Also, does R let us import
> data from an excel spreadhsheet?
> 

Perhaps the simplest approach is to copy the cells from the spreadsheet
to the clipboard and read via read.table() or read.csv(). Remarkably,
this was addressed on the r-help list earlier today; see the thread
<https://stat.ethz.ch/pipermail/r-help/2005-February/064641.html> in
the list archive.

Other approaches are to output a csv file or to use the R-(D)COM
interface (see http://cran.r-project.org/other-software.html).

Regards,
 John

> Thanks,
> Radha
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada
http://socserv.mcmaster.ca/jfox/



From spencer.graves at pdf.com  Thu Feb 17 00:29:52 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 16 Feb 2005 15:29:52 -0800
Subject: [R] Unable to create histograms
In-Reply-To: <20050216230338.77859.qmail@web30505.mail.mud.yahoo.com>
References: <20050216230338.77859.qmail@web30505.mail.mud.yahoo.com>
Message-ID: <4213D770.4060009@pdf.com>

      Are you sure it's numeric?  Have you looked at the following: 

      class(db1$var1)

      Do you mean "hist(db1$Var1)"?  R is case sensitive. 

      Importing data from an Excel spreadsheet was discussed earlier 
today on this list.  My favorite, contributed by Gabor Grothendieck, was 
to select what you want in Excel, then "Copy", then use the following in 
R: 

	  read.delim2("clipboard", header = FALSE)

	  Werner Wernersen then reported that it works the other way around as well:  

write.table(x, file("clipboard"), sep="\t")

      hope this helps.  spencer graves    

Radha Chebolu wrote:

>Hi,
>could someone pelase help me with this?
>
>My data set's name is db1(say) and one of the
>variables is var1. I gave the command:
>hist(db1$var1). The values of Var1 are numbers.
>I got an error which says: 'x' must be numeric.
>Sometimes it works for other datasets and it's not
>working for this dataset. Also, does R let us import
>data from an excel spreadhsheet?
>
>Thanks,
>Radha
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From Ted.Harding at nessie.mcc.ac.uk  Wed Feb 16 23:06:58 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 16 Feb 2005 22:06:58 -0000 (GMT)
Subject: [R] (no subject)
In-Reply-To: <x2acq4mda3.fsf@biostat.ku.dk>
Message-ID: <XFMail.050216220658.Ted.Harding@nessie.mcc.ac.uk>

On 16-Feb-05 Peter Dalgaard wrote:
> Well, the list manager *might* remove you, *when* he gets back from
> his holiday, *if* he notices it and is in a good mood. If you want a
> quicker reaction, please use the web interface as indicated in the
> footer:

Error: syntax error (: use *and* :)
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 16-Feb-05                                       Time: 22:06:58
------------------------------ XFMail ------------------------------



From ggrothendieck at myway.com  Thu Feb 17 00:41:28 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 16 Feb 2005 23:41:28 +0000 (UTC)
Subject: [R] scaling axes when plotting multiple data sets
References: <1108591453.4213c35d1c4a2@webmail.uvm.edu>
Message-ID: <loom.20050217T002857-849@post.gmane.org>

Benjamin M. Osborne <Benjamin.Osborne <at> uvm.edu> writes:

: 
: 1) When adding additional data sets to a plot using "plot" followed 
by "lines",
: is there a way to automate the scaling of the axes to allow for all data sets
: to fit within the plot area?
: 
: 2) I attempted to solve this by setting
: xlim=c(min(c(data1,data2,data3)),max(c(data1,data2,data3)))
: however, there are some NAs and Infs in these data sets, and min(data1) and
: max(data1) both return NA, as with data2 and data3.  (These are time series).


Any of the following solutions would avoid having to calculate
maximum and minimum in the first place:

1. You can first plot all the data together using type = "n" (no points 
are actually shown) and then add them one by one 

plot(c(x1,x2), c(y1,y2), type = "n")
lines(x1, y1)
lines(x2, y2)

2. you may be able to use matplot.  See ?matplot .

3. if these are 'ts' class time series you could plot them all at once
using ts.plot even if they have different time bases.  See ?ts.plot

4. the 'zoo' library's plot function has facilities for plotting 
multiple time series all at once even if they have different time bases.
See    library(zoo); ?plot.zoo



From yhtong at u.washington.edu  Thu Feb 17 01:25:13 2005
From: yhtong at u.washington.edu (Yen H., Tong)
Date: Wed, 16 Feb 2005 16:25:13 -0800 (PST)
Subject: [R] Multiple Fstats/breakpoints test using Panel data
Message-ID: <Pine.LNX.4.43.0502161625130.32627@hymn08.u.washington.edu>

Hi,

I have recently use the strucchange package in R with a single time series observation.  I found it extremely useful in the testing of change points.

Now, I am thinking of using the strucchange package with panel data (about 500 firms, with 73 monthly time series observations each).  For each firm, I have to conduct the Fstats and breakpoints tests.  Based on the test of each firm, I have to output a table/histogram with the number of times each SupF test is  significant and the frequency that a breakpoint is observed in each of the 73 months.  In summary, I have to conduct the Fstats/breakpoint test 500 times and summarize the results.  I have the 500 firms with 73 observations (approx 36,500 observations) stacked in one data file.

Is there anyone who can help mw with this?


Warm Regards,

Yen



From Gregor.Gorjanc at bfro.uni-lj.si  Thu Feb 17 01:55:45 2005
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Thu, 17 Feb 2005 01:55:45 +0100
Subject: [R] Adding new column to data frame and filling some rows of it -
	classes
Message-ID: <7FFEE688B57D7346BC6241C55900E730B6FF27@pollux.bfro.uni-lj.si>

Hello!

Few days ago I was asking on a list about adding a column and filling
it in some rows. I was satisfied, but one thing raised my attention. I
will show itthrough an example:

tmp <- data.frame(y1=1:4, f1=factor(c("A", "B", "C", "D")))
f2 <- factor(c("Z", "Y"))

# I would like to add f2 to tmp and I know that values in f2 fit to
# first two values in tmp. I used

tmp$f2 <- NA
tmp[1:2, "f2"] <- f2
tmp
  y1 f1 f2
1  1  A  2
2  2  B  1
3  3  C NA
4  4  D NA

# However tmp$f2 is not factor. How can I make it to be a factor? I tried
# with 
class(tmp$f2) <- "factor"

# but I get this
tmp
  y1 f1   f2
1  1  A NULL
2  2  B <NA>
3  3  C <NA>
4  4  D <NA>
Warning message: 
corrupt data frame: columns will be truncated or padded with NAs in: format.data.frame(x, digits = digits) 

# I tried the other approach
tmp <- data.frame(y1=1:4, f1=factor(c("A", "B", "C", "D")))
f2 <- factor(c("Z", "Y"))
tmp$f2 <- f2
tmp[3:4, "f2"] <- NA
tmp
  y1 f1   f2
1  1  A    Z
2  2  B    Y
3  3  C <NA>
4  4  D <NA>

class(tmp$f2)
[1] "factor"

# And I have now the same class for tmp$f2 as in f2. Is this the
# only way?

--
Lep pozdrav / With regards,
    Gregor GORJANC

---------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty       URI: http://www.bfro.uni-lj.si
Zootechnical Department    email: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                  tel: +386 (0)1 72 17 861
SI-1230 Domzale            fax: +386 (0)1 72 17 888
Slovenia



From ggrothendieck at myway.com  Thu Feb 17 02:53:32 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 17 Feb 2005 01:53:32 +0000 (UTC)
Subject: [R] Adding new column to data frame and filling some rows of it
	=?utf-8?b?LQljbGFzc2Vz?=
References: <7FFEE688B57D7346BC6241C55900E730B6FF27@pollux.bfro.uni-lj.si>
Message-ID: <loom.20050217T024905-674@post.gmane.org>

Gorjanc Gregor <Gregor.Gorjanc <at> bfro.uni-lj.si> writes:


: Few days ago I was asking on a list about adding a column and filling
: it in some rows. I was satisfied, but one thing raised my attention. I
: will show itthrough an example:
: 
: tmp <- data.frame(y1=1:4, f1=factor(c("A", "B", "C", "D")))
: f2 <- factor(c("Z", "Y"))
: 
: # I would like to add f2 to tmp and I know that values in f2 fit to
: # first two values in tmp. I used
: 
: tmp$f2 <- NA
: tmp[1:2, "f2"] <- f2
: tmp
:   y1 f1 f2
: 1  1  A  2
: 2  2  B  1
: 3  3  C NA
: 4  4  D NA
: 
: # However tmp$f2 is not factor. How can I make it to be a factor? I tried
: # with 
: class(tmp$f2) <- "factor"
: 
: # but I get this
: tmp
:   y1 f1   f2
: 1  1  A NULL
: 2  2  B <NA>
: 3  3  C <NA>
: 4  4  D <NA>
: Warning message: 
: corrupt data frame: columns will be truncated or padded with NAs in: 
format.data.frame(x, digits =
: digits) 
: 
: # I tried the other approach
: tmp <- data.frame(y1=1:4, f1=factor(c("A", "B", "C", "D")))
: f2 <- factor(c("Z", "Y"))
: tmp$f2 <- f2
: tmp[3:4, "f2"] <- NA
: tmp
:   y1 f1   f2
: 1  1  A    Z
: 2  2  B    Y
: 3  3  C <NA>
: 4  4  D <NA>
: 
: class(tmp$f2)
: [1] "factor"
: 
: # And I have now the same class for tmp$f2 as in f2. Is this the
: # only way?


Here are a few possibilities:

R> # 1
R> 
R> tmp <- data.frame(y1=1:4, f1=factor(c("A", "B", "C", "D")))
R> f2 <- factor(c("Z", "Y"))
R> 
R> length(f2) <- nrow(tmp)
R> tmp$f2 <- f2
R> tmp
  y1 f1   f2
1  1  A    Z
2  2  B    Y
3  3  C <NA>
4  4  D <NA>
R> 
R> # 2
R> 
R> tmp <- data.frame(y1=1:4, f1=factor(c("A", "B", "C", "D")))
R> f2 <- factor(c("Z", "Y"))
R> 
R> tmp$f2 <- factor(NA, levels = levels(f2))
R> tmp[seq(along = f2),"f2"] <- f2
R> tmp
  y1 f1   f2
1  1  A    Z
2  2  B    Y
3  3  C <NA>
4  4  D <NA>
R> 
R> # 3
R> 
R> tmp <- data.frame(y1=1:4, f1=factor(c("A", "B", "C", "D")))
R> f2 <- factor(c("Z", "Y"))
R> 
R> merge(tmp, list(f2 = f2), by = 0, all.x = TRUE)
  Row.names y1 f1   f2
1         1  1  A    Z
2         2  2  B    Y
3         3  3  C <NA>
4         4  4  D <NA>


---------------

Here is just the input:


# 1

tmp <- data.frame(y1=1:4, f1=factor(c("A", "B", "C", "D")))
f2 <- factor(c("Z", "Y"))

length(f2) <- nrow(tmp)
tmp$f2 <- f2
tmp

# 2

tmp <- data.frame(y1=1:4, f1=factor(c("A", "B", "C", "D")))
f2 <- factor(c("Z", "Y"))   

tmp$f2 <- factor(NA, levels = levels(f2))
tmp[seq(along = f2),"f2"] <- f2
tmp

# 3

tmp <- data.frame(y1=1:4, f1=factor(c("A", "B", "C", "D")))
f2 <- factor(c("Z", "Y"))   

merge(tmp, list(f2 = f2), by = 0, all.x = TRUE)



From rsubscriber at slcmsr.net  Thu Feb 17 04:08:14 2005
From: rsubscriber at slcmsr.net (Christian Lederer)
Date: Thu, 17 Feb 2005 04:08:14 +0100 (CET)
Subject: [R] socket problems (maybe bugs?)
Message-ID: <33346.217.229.15.144.1108609694.squirrel@mail.mytrium.com>

Dear R Gurus,

for some purpose i have to use a socket connection, where i have to read
and write both text and binary data (each binary data package will be
preceeded by a header line).
When experimenting, i encountered some problems (with R-2.0.1 under
different Linuxes (SuSE and Gentoo)).

Since the default mode for socket connections is non-blocking,
i first tried socketSelect() in order to see whether the socket is ready
for reading:

# Server:
s <- socketConnection(port=2222, server=TRUE, open="w+b")
writeLines("test", s)
writeBin(1:10, s, size=4, endian="big")

# Client, variation 1:
s <- socketConnection(port=2222, server=FALSE, open="w+b")
socketSelect(list(s))
readLines(s, n=1)     # works, "test" is read
socketSelect(list(s)) # does never return, although the server wrote 1:10

(This seems to happen only, when i mix text and binary reads.)
However, without socketSelect(), R may crash if i try to read from an
empty socket:

Server:
s <- socketConnection(port=2222, server=TRUE, open="w+b")
writeLines("test", s)
writeBin(1:10, s, size=4, endian="big")

# Client, variation 2:
s <- socketConnection(port=2222, server=FALSE, open="w+b")
readLines(s, n=1)                              # works, "test" is read
readBin(s, "int", size=4, n=10, endian="big")  # works, 1:10 is read
readBin(s, "int", size=4, n=10, endian="big")  # second read leads to
                                               # segmentation fault

If i omit the endian="big" option, the second read does not crash, but
just gets 10 random numbers.

On the first view, this does not seem to be a problem, since the
data will be preeceded by a header, which contains the number of
bytes in the binary block.
However, due to race conditions, i cannot exclude this situation:

time    server             client
t0      sends header
t1                         reads header
t2                         tries to read binary, crashes
t3      sends binary


If i open the client socket in blocking mode, the second variation seems
to work (the second read just blocks as desired).
When using only one socket, i can do without socketSelect(), but
i have the follwoing questions:

1. Can i be sure, the the blocking variation will also work for larger
data sets, when e.g. the server starts writing before the client is
reading?

2. How could i proceed, if i needed several sockets?
Then i cannot use socketSelect due to the problem described in
variation 1.
I also cannot use blocking sockets, since reading from an empty socket
would block the others.
Without blocking and socketSelect(), i might run into the race condition
described above.

In any case, the readBin() crash with endian="big" is a bug in
my eyes. For non-blocking sockets, readBin() should just return numeric(0),
if no data are written on the socket.
I also stronlgy suspect that the socketSelect() behaviour as described in
variation 1 is a bug.


Christian :-(



From YiYao_Jiang at smics.com  Thu Feb 17 04:12:09 2005
From: YiYao_Jiang at smics.com (YiYao_Jiang)
Date: Thu, 17 Feb 2005 11:12:09 +0800
Subject: [R] How to upgrade library from R 1.9.1 to R 2.0.1
Message-ID: <E0B8EBA2EBD83347A588FC4840F8C9E91F586F@ex115.smic-sh.com>

   Dear All:

	I have a library for R 1.9.1, it is very easy to setup a library in R 1.9.1.
    For example:
    I want to setup a library "test" for R1.9.1.
    1. Create a folder "test" in the direct X:\ R\ rw1091\ library \.
    2. Create a file "DESCRIPTION" in the direct X:\ R\ rw1091\ library\ test\.
    3. Modify the "Package:" and "Title:" item to "test" in the file "DESCRIPTION" .
    4.Create a folder "R" in the direct X:\ R\ rw1091\ library\ test\ .
    5.Create a file "test" in the direct X:\ R\ rw1091\ library\ test\ R\ .
    6. Add all the function into the "test" file.
    Then I can call the "test" using command " library (test) ". 
    I setup the library using the same way in R2.0.1, but the library can't work.
    The error message is " Error in library(test) : 'test' is not a valid package -- installed < 2.0.0? "
    I search the help file and document but still can't upgrade the library. 
    Who can tell me how to upgrade library from R 1.9.1 to R 2.0.1 or give me a sample, thanks.




Best Regards
YiYao Jiang  
Product Division/ Product Testing Department
Semiconductor Manufacturing International Corporation
18 ZhangJiang Road, PuDong New Area, Shanghai  ZIP: 201203
Tel:86-21-5080-2000 Ext. 15173



From ggrothendieck at myway.com  Thu Feb 17 05:26:32 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 17 Feb 2005 04:26:32 +0000 (UTC)
Subject: [R] How to upgrade library from R 1.9.1 to R 2.0.1
References: <E0B8EBA2EBD83347A588FC4840F8C9E91F586F@ex115.smic-sh.com>
Message-ID: <loom.20050217T050645-913@post.gmane.org>

YiYao_Jiang <YiYao_Jiang <at> smics.com> writes:

: 
: Dear All:
: 
: 	I have a library for R 1.9.1, it is very easy to setup a library in R 
1.9.1.
:     For example:
:     I want to setup a library "test" for R1.9.1.
:     1. Create a folder "test" in the direct X:\ R\ rw1091\ library \.
:     2. Create a file "DESCRIPTION" in the direct X:\ R\ rw1091\ library\ 
test\.
:     3. Modify the "Package:" and "Title:" item to "test" in the 
file "DESCRIPTION" .
:     4.Create a folder "R" in the direct X:\ R\ rw1091\ library\ test\ .
:     5.Create a file "test" in the direct X:\ R\ rw1091\ library\ test\ R\ .
:     6. Add all the function into the "test" file.
:     Then I can call the "test" using command " library (test) ". 
:     I setup the library using the same way in R2.0.1, but the library can't 
work.
:     The error message is " Error in library(test) : 'test' is not a valid 
package -- installed < 2.0.0? "
:     I search the help file and document but still can't upgrade the library. 
:     Who can tell me how to upgrade library from R 1.9.1 to R 2.0.1 or give 
me a sample, thanks.

I assume you are referring to a package that you wrote and want to
upgrade.

I think there may be confusion here between the source and installed
package.  The source package is what you wrote and it should _not_ go
into \R\rw2001\library\test.    Lets say we put that in \Rpkgs\test .
Then, assuming you just installed R 2.0.1, use the Rcmd.exe program
to install the source package into your R tree. 

cd \Rpkgs
\R\rw2001\bin\Rcmd install test

to install it.  Fix up any errors you get during the install
and repeat until it installs. Now go into R and test it out.  
If it seems to work try running 'Rcmd check' on it:

cd \Rpkgs
\R\rw2001\bin\Rcmd check test

and fix up any errors and repeat until it passes Rcmd check and
reinstall.

See the Writing Extensions Manual if the above is not sufficient.



From mail at bymouth.com  Thu Feb 17 07:18:46 2005
From: mail at bymouth.com (Stephen Choularton)
Date: Thu, 17 Feb 2005 17:18:46 +1100
Subject: [R] Error in eval(expr, envir,
	enclos) : numeric envir arg not of length one
Message-ID: <000001c514b8$916a7be0$9701a8c0@Tablet>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050217/07c621ce/attachment.pl

From ligges at statistik.uni-dortmund.de  Thu Feb 17 08:50:27 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 17 Feb 2005 08:50:27 +0100
Subject: [R] running out of memory
In-Reply-To: <000001c51460$8fff8230$9701a8c0@Tablet>
References: <000001c51460$8fff8230$9701a8c0@Tablet>
Message-ID: <42144CC3.3010702@statistik.uni-dortmund.de>

Stephen Choularton wrote:

> Hi
>  
> I am trying to do a large glm and running into this message.  
>  
> Error: cannot allocate vector of size 3725426 Kb
> In addition: Warning message: 
> Reached total allocation of 494Mb: see help(memory.size)
>  
> Am I simply out of memory (I only  have .5 gig)?
>  
> Is there something I can do?

You have to rethink whether the analyses you are doing is sensible this 
way, or whether you can respecify things. R claims to need almost 4Gb(!) 
for the next memory allocation step, so you will get in trouble even on 
huge machines....

  Uwe Ligges


> Stephen
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Thu Feb 17 08:49:36 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 17 Feb 2005 07:49:36 +0000 (GMT)
Subject: [R] scaling axes when plotting multiple data sets
In-Reply-To: <12AE52872B5C5348BE5CF47C707FF53A502468@rhosvr02.rhotrading.com>
References: <12AE52872B5C5348BE5CF47C707FF53A502468@rhosvr02.rhotrading.com>
Message-ID: <Pine.LNX.4.61.0502170747420.15589@gannet.stats>

On Wed, 16 Feb 2005 davidr at rhotrading.com wrote:

> min, max, and range (and many other functions) take the na.rm parameter
> to ignore NAs, but the Infs must be removed by hand as far as I know:
>
> dat <- c(data1,data2,data3,data4)
> dat <- dat[(dat!=Inf)&(dat!=(-Inf))]
> rng <- range(dat, na.rm=TRUE)
> then use xlim = rng

range(dat[is.finite(dat)]) is clearer, I find.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Feb 17 08:58:40 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 17 Feb 2005 07:58:40 +0000 (GMT)
Subject: [R] How to upgrade library from R 1.9.1 to R 2.0.1
In-Reply-To: <E0B8EBA2EBD83347A588FC4840F8C9E91F586F@ex115.smic-sh.com>
References: <E0B8EBA2EBD83347A588FC4840F8C9E91F586F@ex115.smic-sh.com>
Message-ID: <Pine.LNX.4.61.0502170750430.15589@gannet.stats>

This is both in the NEWS file and in the rw-FAQ (you are on Windows, I 
believe, but have not mentioned it).

You absolutely must install packages properly (with R CMD INSTALL) on R 
2.0.1.   What you did has never to my knowledge been documented, and often 
did not work under 1.9.1, so you were fortunate.

You will find it easier to find the pertinent information if you search 
for the concept `package' (you are talking about a package) under 
`package' not `library'.  See

   3.1 Can I install packages into libraries in this version?

Yes, but you will need a lot of tools to do so, unless the author or the
maintainers of the `bin/windows/contrib' section on CRAN have been kind
enough to provide a pre-compiled version for Windows as a `.zip' file.
...

On Thu, 17 Feb 2005, YiYao_Jiang wrote:

> 	I have a library for R 1.9.1, it is very easy to setup a library in R 1.9.1.
>    For example:
>    I want to setup a library "test" for R1.9.1.
>    1. Create a folder "test" in the direct X:\ R\ rw1091\ library \.
>    2. Create a file "DESCRIPTION" in the direct X:\ R\ rw1091\ library\ test\.
>    3. Modify the "Package:" and "Title:" item to "test" in the file "DESCRIPTION" .
>    4.Create a folder "R" in the direct X:\ R\ rw1091\ library\ test\ .
>    5.Create a file "test" in the direct X:\ R\ rw1091\ library\ test\ R\ .
>    6. Add all the function into the "test" file.
>    Then I can call the "test" using command " library (test) ".
>    I setup the library using the same way in R2.0.1, but the library can't work.
>    The error message is " Error in library(test) : 'test' is not a valid package -- installed < 2.0.0? "
>    I search the help file and document but still can't upgrade the library.
>    Who can tell me how to upgrade library from R 1.9.1 to R 2.0.1 or give me a sample, thanks.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Feb 17 09:09:04 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 17 Feb 2005 08:09:04 +0000 (GMT)
Subject: [R] Error in eval(expr, envir, enclos) : numeric envir arg not
	of length one
In-Reply-To: <000001c514b8$916a7be0$9701a8c0@Tablet>
References: <000001c514b8$916a7be0$9701a8c0@Tablet>
Message-ID: <Pine.LNX.4.61.0502170759190.15589@gannet.stats>

On Thu, 17 Feb 2005, Stephen Choularton wrote:

> I am working with a largish dataset of 25k lines and I am now tying to
> use predict.
>
> pred = predict(cuDataGlmModel, length + meanPitch + minimumPitch +
> maximumPitch + meanF1 + meanF2 + meanF3 +  meanF4 +  meanF5 +
> ratioF1ToF2 + rationF3ToF1  +  jitter + shimmer + percentUnvoicedFrames
> + numberOfVoiceBreaks + percentOfVoiceBreaks + meanIntensity +
> minimumIntensity + maximumIntensity + ratioIntensity + noSyllsIntensity
> + startSpeech + syllables)
>
> I keep on getting this error message
>
> Error in eval(expr, envir, enclos) : numeric envir arg not of length one
>
> To be quite frank I am at a bit of a loss to know where to start looking
> to solve the problem.

`To be quite frank' so are we!  You have not told us the class of model, 
or where you got the idea than its predict method has a sum (not a 
formula) as second argument, nor given us code to reproduce this nor used 
traceback().

Some reading of the help page for the predict() method was suggested by 
the posting guide, and at a guess, try ?predict.glm.

Most predict() methods require a data frame as their second argument.

One might also question the inconsistency of `ratioF1ToF2 + rationF3ToF1'.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tom_hoary at web.de  Thu Feb 17 09:14:30 2005
From: tom_hoary at web.de (Thomas =?iso-8859-15?q?Sch=F6nhoff?=)
Date: Thu, 17 Feb 2005 09:14:30 +0100
Subject: [R] running out of memory
In-Reply-To: <000001c51460$8fff8230$9701a8c0@Tablet>
References: <000001c51460$8fff8230$9701a8c0@Tablet>
Message-ID: <200502170914.30413.tom_hoary@web.de>

Hello,

Am Mittwoch, 16. Februar 2005 20:48 schrieb Stephen Choularton:
> Hi
>
> I am trying to do a large glm and running into this message.
>
> Error: cannot allocate vector of size 3725426 Kb
> In addition: Warning message:
> Reached total allocation of 494Mb: see help(memory.size)
>
> Am I simply out of memory (I only  have .5 gig)?
>
> Is there something I can do?

This question has been answered a hundred times on this list. The best 
idea is trying to search for "memory datasets" in the lists mail 
archive  . Which gives you 246 hits!
Please give some more information on what system are you using (32, 64 
bit) etc.

Regards
Thomas



From Achim.Zeileis at wu-wien.ac.at  Thu Feb 17 10:31:20 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 17 Feb 2005 10:31:20 +0100 (CET)
Subject: [R] Multiple Fstats/breakpoints test using Panel data
In-Reply-To: <Pine.LNX.4.43.0502161625130.32627@hymn08.u.washington.edu>
References: <Pine.LNX.4.43.0502161625130.32627@hymn08.u.washington.edu>
Message-ID: <Pine.LNX.4.58.0502171017240.4995@thorin.ci.tuwien.ac.at>

On Wed, 16 Feb 2005, Yen H., Tong wrote:

> Hi,
>
> I have recently use the strucchange package in R with a single time
> series observation.  I found it extremely useful in the testing of
> change points.
>
> Now, I am thinking of using the strucchange package with panel data
> (about 500 firms, with 73 monthly time series observations each).  For
> each firm, I have to conduct the Fstats and breakpoints tests.  Based on
> the test of each firm, I have to output a table/histogram with the
> number of times each SupF test is  significant and the frequency that a
> breakpoint is observed in each of the 73 months.  In summary, I have to
> conduct the Fstats/breakpoint test 500 times and summarize the results.
> I have the 500 firms with 73 observations (approx 36,500 observations)
> stacked in one data file.
>
> Is there anyone who can help mw with this?

You already described in detail what you want to do and if you know how to
use Fstats() and breakpoints() then it shouldn't be hard to write a for
loop that extracts the data for firm i, applies Fstats() and breakpoints()
and stores the information you need in a matrix data.frame or list. For
example you could store them in 500 x 75 data.frame/matrix with
rows corresponding to firm i and columns corresponding to supF statistic,
p value and break (e.g., in the minimum BIC partition) at observation j (j
= 1, ..., 73).

BTW: I'm not sure if this is the most appropriate way to analyze such
data. I haven't worked with panel data myself so I'm not sure whether
there are structural change techniques devoted to scenarios that you
describe. But if you take the approach you describe above, you should at
least worry about the multiple testing.

Best,
Z

> Warm Regards,
>
> Yen
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From nishan.sugathadasa at iotc.org  Thu Feb 17 10:30:21 2005
From: nishan.sugathadasa at iotc.org (nishan)
Date: Thu, 17 Feb 2005 13:30:21 +0400
Subject: [R] package bulding in windows.
Message-ID: <200502170923.j1H9NsT18388@mail.seychelles.sc>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050217/90f30caa/attachment.pl

From Helene.Dryssens at eleves.polytech-lille.fr  Thu Feb 17 10:50:50 2005
From: Helene.Dryssens at eleves.polytech-lille.fr (Helene.Dryssens@eleves.polytech-lille.fr)
Date: Thu, 17 Feb 2005 10:50:50 +0100
Subject: [R] problem with logistic regression
In-Reply-To: <x2k6pkxn9o.fsf@biostat.ku.dk>
References: <1107793194.4207952a37db5@webmail.polytech-lille.fr>
	<x2k6pkxn9o.fsf@biostat.ku.dk>
Message-ID: <1108633850.421468fad7687@webmail.polytech-lille.fr>

Hi,
we try to do a logistic regression with the function glm() but we have an error:
>Error: cannot allocate vector of size 31273 Kb
>In addition: Warning message:
> Reached total allocation of 446Mb: see help(memory.size)

The data contains 40030 rows and we try to do logistic regression with 7 variables.

We don't understand because we have already done a logistic regression with
39779 rows and 7 variables!!!
The difference between these two files is that in the one, which we have problem
, the variables have more levels (about 6).

Could somebody help us?
Thank you.



From ligges at statistik.uni-dortmund.de  Thu Feb 17 11:03:50 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 17 Feb 2005 11:03:50 +0100
Subject: [R] package bulding in windows.
In-Reply-To: <200502170923.j1H9NsT18388@mail.seychelles.sc>
References: <200502170923.j1H9NsT18388@mail.seychelles.sc>
Message-ID: <42146C06.2050900@statistik.uni-dortmund.de>

nishan wrote:

> Hello everyone.
> 
> I am using R 2.0.1 in windows XP sp2 and I want to create a package. 
> 
> I get the following errors when I run these commands at the dos prompt.
> Please help.
> 
> Thanks.
> NIshan
> 
>  
> 
>  
> 
> F:\Program Files\R\rw2001\bin>Rcmd build f:\testingskeke
> 
> * checking for file 'f:\testingskeke/DESCRIPTION' ... OK
> 
> * preparing 'f:\testingskeke':
> 
> * cleaning src
> 
> * removing junk files
> 
> * building 'testingskeke_1.0.tar.gz'
> 
> 'sh' is not recognized as an internal or external command,

So you don't have the tools in your path. Please read the R for Windows 
FAQ and the docs cited therein how to install packages under Windows.

Uwe Ligges




> operable program or batch file.
> 
> 'sh' is not recognized as an internal or external command,
> 
> operable program or batch file.
> 
> Error: cannot open file 'testingskeke/DESCRIPTION' for reading
> 
>  
> 
> F:\Program Files\R\rw2001\bin>
> 
>  
> 
>  
> 
> F:\Program Files\R\rw2001\bin>Rcmd check f:\testingskeke
> 
> * checking for working latex ...'sh' is not recognized as an internal or
> external command,
> 
> operable program or batch file.
> 
>  NO
> 
> * using log directory 'F:/Program Files/R/rw2001/bin/testingskeke.Rcheck'
> 
> * checking for file 'testingskeke/DESCRIPTION' ... OK
> 
> * checking if this is a source package ... OK
> 
>  
> 
> 'sh' is not recognized as an internal or external command,
> 
> operable program or batch file.
> 
>  ERROR
> 
> Installation failed.
> 
>  
> 
> F:\Program Files\R\rw2001\bin>
> 
>  
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From dimitris.rizopoulos at med.kuleuven.ac.be  Thu Feb 17 11:02:59 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Thu, 17 Feb 2005 11:02:59 +0100
Subject: [R] package bulding in windows.
References: <200502170923.j1H9NsT18388@mail.seychelles.sc>
Message-ID: <013601c514d7$dc33e5c0$0540210a@www.domain>

you have to put Rtools in your path! This has been recently discussed 
on the list (two or three days ago).

Assuming that you have downloaded Rtools from: 
http://www.murdoch-sutherland.com/Rtools and you have extracted them 
at, e.g., C:\Rtools do the following:

control panel -> system -> advanced -> environment variables -> system 
variables -> path -> edit

and add this: C:\Rtools\bin;

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "nishan" <nishan.sugathadasa at iotc.org>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, February 17, 2005 10:30 AM
Subject: [R] package bulding in windows.


> Hello everyone.
>
>
>
> I am using R 2.0.1 in windows XP sp2 and I want to create a package.
>
>
>
> I get the following errors when I run these commands at the dos 
> prompt.
> Please help.
>
>
>
> Thanks.
>
>
>
> NIshan
>
>
>
>
>
> F:\Program Files\R\rw2001\bin>Rcmd build f:\testingskeke
>
> * checking for file 'f:\testingskeke/DESCRIPTION' ... OK
>
> * preparing 'f:\testingskeke':
>
> * cleaning src
>
> * removing junk files
>
> * building 'testingskeke_1.0.tar.gz'
>
> 'sh' is not recognized as an internal or external command,
>
> operable program or batch file.
>
> 'sh' is not recognized as an internal or external command,
>
> operable program or batch file.
>
> Error: cannot open file 'testingskeke/DESCRIPTION' for reading
>
>
>
> F:\Program Files\R\rw2001\bin>
>
>
>
>
>
> F:\Program Files\R\rw2001\bin>Rcmd check f:\testingskeke
>
> * checking for working latex ...'sh' is not recognized as an 
> internal or
> external command,
>
> operable program or batch file.
>
> NO
>
> * using log directory 'F:/Program 
> Files/R/rw2001/bin/testingskeke.Rcheck'
>
> * checking for file 'testingskeke/DESCRIPTION' ... OK
>
> * checking if this is a source package ... OK
>
>
>
> 'sh' is not recognized as an internal or external command,
>
> operable program or batch file.
>
> ERROR
>
> Installation failed.
>
>
>
> F:\Program Files\R\rw2001\bin>
>
>
>
>
>
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From tuechler at gmx.at  Thu Feb 17 11:30:07 2005
From: tuechler at gmx.at (Heinz Tuechler)
Date: Thu, 17 Feb 2005 11:30:07 +0100
Subject: [R] Again: Variable names in functions
Message-ID: <3.0.6.32.20050217113007.007b0860@pop.gmx.net>

Hello,

still I have difficulties with variable names in functions. I know the
famous example form help for deparse/substitute but I will give a simpler
one to explain my problem.
I know from Reid Huntsinger (Tue, 8 Feb 2005 12:39:32 -0500) that:
"Semantically, R is pass-by-value, so you don't really have the names, just
the values. In implementation, though, R *does* pass names, in part at least
in order to do "lazy evaluation". You can get them via "substitute" ; see
the help for that." 

The output of several functions does not make much sense, if these names do
not appear (e.g. parameter estimates in Cox-regression, ...)
Only to give a trivial example I show my problem with the table function.

As you know, if I call table as follows, the output is labelled properly.
> charly<-c(rep(1,3),rep(2,7));delta<-c(rep(1:2,5))
> table(charly, delta)
      delta
charly 1 2
     1 2 1
     2 3 4
If I define a trivial function to call table, the output is less satisfying.
(Of course, I know that this function is useless.)
> mytable1<-function(x,y){table(x,y)}
> mytable1(charly, delta)
   y
x   1 2
  1 2 1
  2 3 4
If I define the function in the following way, it does what I wish, namely
it returns output equivalent to the simple call "table(charly, delta)".
> mytable2<-function(x,y){
+   cat("table(",as.symbol((deparse(substitute(x)))),
+   "," ,  as.symbol(deparse(substitute(y))),")\n",
+   file="temp",sep="",append=F)
+   eval(parse("temp",n=-1))
+                       }
> mytable2(charly, delta)
      delta
charly 1 2
     1 2 1
     2 3 4
> 
I assume that there is a better way to solve this problem and I would be
happy about hints, where to find solutions in the documentation.

Thanks,

Heinz T?chler



From T.A.Wassenaar at rug.nl  Thu Feb 17 11:39:55 2005
From: T.A.Wassenaar at rug.nl (T.A.Wassenaar)
Date: Thu, 17 Feb 2005 11:39:55 +0100
Subject: [R] Factor level coloring in trellis plot
Message-ID: <web-1245939@mail3.rug.nl>


Hi :)

Was just wondering whether someone could help me with 
adjustments to trellis plots (parallel).

I've got two way multivariate data. I want to make 
parallel plots for one of the factors, and want to color 
the lines according to the other factor. The first thing I 
manage, but with the other I'm lost :( Can only change the 
overall color.

This is basically how far I get:

parallel(~data | factor, layout=c(4,1))

Any hints will be greatly appreciated.

Thanks in advance,

Tsjerk



From ghoermann at hydrology.uni-kiel.de  Thu Feb 17 11:44:59 2005
From: ghoermann at hydrology.uni-kiel.de (Georg Hoermann)
Date: Thu, 17 Feb 2005 11:44:59 +0100
Subject: [R] Getting *types* of arguments?
Message-ID: <200502171144.59466.ghoermann@hydrology.uni-kiel.de>

Hello world,

short question: is there a possibility to get a list of
arguments of a function *with* variable/parameter types?

formals() gives me the names of the parameters, but says
nothing about the parameter type it expects (I know I can always use the 
help function). 

I would like somthing like

$x: vector or data.frame...

Thanks in advance...

Georg


-- 
Georg Hoermann, Fachabteilung Wasserwirtschaft / Dep. Hydrology  
Ecosystem Research Center, Kiel University, Germany, Penguin #189476
Tel. 0431-880-1207, 0172/4315715, ICQ: 348340729, MSN: hlschorsch



From ligges at statistik.uni-dortmund.de  Thu Feb 17 11:48:44 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 17 Feb 2005 11:48:44 +0100
Subject: [R] Again: Variable names in functions
In-Reply-To: <3.0.6.32.20050217113007.007b0860@pop.gmx.net>
References: <3.0.6.32.20050217113007.007b0860@pop.gmx.net>
Message-ID: <4214768C.4070708@statistik.uni-dortmund.de>

Heinz Tuechler wrote:

> Hello,
> 
> still I have difficulties with variable names in functions. I know the
> famous example form help for deparse/substitute but I will give a simpler
> one to explain my problem.
> I know from Reid Huntsinger (Tue, 8 Feb 2005 12:39:32 -0500) that:
> "Semantically, R is pass-by-value, so you don't really have the names, just
> the values. In implementation, though, R *does* pass names, in part at least
> in order to do "lazy evaluation". You can get them via "substitute" ; see
> the help for that." 
> 
> The output of several functions does not make much sense, if these names do
> not appear (e.g. parameter estimates in Cox-regression, ...)
> Only to give a trivial example I show my problem with the table function.
> 
> As you know, if I call table as follows, the output is labelled properly.
> 
>>charly<-c(rep(1,3),rep(2,7));delta<-c(rep(1:2,5))
>>table(charly, delta)
> 
>       delta
> charly 1 2
>      1 2 1
>      2 3 4
> If I define a trivial function to call table, the output is less satisfying.
> (Of course, I know that this function is useless.)
> 
>>mytable1<-function(x,y){table(x,y)}
>>mytable1(charly, delta)
> 
>    y
> x   1 2
>   1 2 1
>   2 3 4
> If I define the function in the following way, it does what I wish, namely
> it returns output equivalent to the simple call "table(charly, delta)".
> 
>>mytable2<-function(x,y){
> 
> +   cat("table(",as.symbol((deparse(substitute(x)))),
> +   "," ,  as.symbol(deparse(substitute(y))),")\n",
> +   file="temp",sep="",append=F)
> +   eval(parse("temp",n=-1))
> +                       }
> 
>>mytable2(charly, delta)

See argument "dnn" in ?table:


   mytable2 <- function(x,y){
     table(x, y, dnn = c(deparse(substitute(x)),
                         deparse(substitute(y))))
   }

Uwe Ligges



>       delta
> charly 1 2
>      1 2 1
>      2 3 4
> 
> I assume that there is a better way to solve this problem and I would be
> happy about hints, where to find solutions in the documentation.
> 
> Thanks,
> 
> Heinz T?chler
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Thu Feb 17 11:51:47 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 17 Feb 2005 11:51:47 +0100
Subject: [R] Again: Variable names in functions
In-Reply-To: <3.0.6.32.20050217113007.007b0860@pop.gmx.net>
References: <3.0.6.32.20050217113007.007b0860@pop.gmx.net>
Message-ID: <x24qgbwjb0.fsf@biostat.ku.dk>

Heinz Tuechler <tuechler at gmx.at> writes:

> > mytable1<-function(x,y){table(x,y)}
> > mytable1(charly, delta)
>    y
> x   1 2
>   1 2 1
>   2 3 4
> If I define the function in the following way, it does what I wish, namely
> it returns output equivalent to the simple call "table(charly, delta)".
> > mytable2<-function(x,y){
> +   cat("table(",as.symbol((deparse(substitute(x)))),
> +   "," ,  as.symbol(deparse(substitute(y))),")\n",
> +   file="temp",sep="",append=F)
> +   eval(parse("temp",n=-1))
> +                       }
> > mytable2(charly, delta)
>       delta
> charly 1 2
>      1 2 1
>      2 3 4
> > 
> I assume that there is a better way to solve this problem and I would be
> happy about hints, where to find solutions in the documentation.

What did Thomas L. say recently? "If the answer involves parse(), you
probably asked the wrong question", I think it was.

The canonical way is

mytable <- function(x,y) eval.parent(substitute(table(x,y)))

or, you could of course modify the names(dimnames(...)) and just pass
the names along.


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From cafanselmo12 at yahoo.com.br  Thu Feb 17 12:07:35 2005
From: cafanselmo12 at yahoo.com.br (=?iso-8859-1?q?C=E9zar=20Freitas?=)
Date: Thu, 17 Feb 2005 08:07:35 -0300 (ART)
Subject: [R] short plots: lwd, margin and postscript behavior
Message-ID: <20050217110735.86202.qmail@web14526.mail.yahoo.com>

Hi all.
I'm working with a short plot (3x3 inches), but the
results (via postscript command) are not nice. The lwd
command don't affect the lines (that are very large)
and the margins don't change using oma, mai, mar, ...
Below I put an example. Moreover, save the graphics
via postscript command isn't working well (see the
attached ps).
Thanks by the help,

Cezar Freitas.

#Example:
#data
  scores<-c(2.0, 0.0, 5.0, 5.0, 5.0, 2.0, 0.0, 5.0,
2.5, 4.0, 5.0, 0.0, 5.0, 0.0, 2.0, 5.0, 5.0, 2.0, 3.0,
3.0)
  q<-summary(scores)
  gra<-hist(scores, breaks=((0:11)/2-.2), plot=FALSE)
  yy<-ceiling(max(gra$counts)/10)*10
  yz<-yy/12

#plot via postscript
  postscript("test.ps", width=3, height=3,
horizontal=FALSE, family="Times", paper="special")

  hist(scores, breaks=((0:11)/2-.2), xlim=c(-1,6),
ylim=c(-yz,yy), main="scores", ylab="Freq",
xlab="math", cex.axis=.3, cex.main=.3, cex.sub=.3,
cex.lab=.3, mgp=c(1.5,.5,0))
  boxplot(scores, horizontal=1, add=1, at=-2*yz/3,
boxwex=1.5*yz, bty="n", axes=FALSE)
  points(q[4], -2*yz/3, col=1, pch="+", cex=.5)

  dev.off()




	
	
		
_______________________________________________________ 
-------------- next part --------------
A non-text attachment was scrubbed...
Name: via_postscript.ps
Type: application/postscript
Size: 6629 bytes
Desc: via_postscript.ps
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20050217/abac04f1/via_postscript.ps
-------------- next part --------------
A non-text attachment was scrubbed...
Name: via_R.ps
Type: application/postscript
Size: 32296 bytes
Desc: via_R.ps
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20050217/abac04f1/via_R.ps

From jtk at cmp.uea.ac.uk  Thu Feb 17 13:32:29 2005
From: jtk at cmp.uea.ac.uk (Jan T. Kim)
Date: Thu, 17 Feb 2005 12:32:29 +0000
Subject: [R] Getting *types* of arguments?
In-Reply-To: <200502171144.59466.ghoermann@hydrology.uni-kiel.de>
References: <200502171144.59466.ghoermann@hydrology.uni-kiel.de>
Message-ID: <20050217123229.GB18434@jtkpc.cmp.uea.ac.uk>

On Thu, Feb 17, 2005 at 11:44:59AM +0100, Georg Hoermann wrote:
> short question: is there a possibility to get a list of
> arguments of a function *with* variable/parameter types?
> 
> formals() gives me the names of the parameters, but says
> nothing about the parameter type it expects (I know I can always use the 
> help function). 
> 
> I would like somthing like
> 
> $x: vector or data.frame...

I think you're looking for something that does not exist. There is no
notion of a parameter type in R in the way you have it in C or Java.
Whether or not a type is appropriate for a parameter cannot be decided
before the function call is actually carried out. If all operations
performed on the parameter turn out to be valid during function execution,
the parameter type can be considered valid (at least in a syntactic
view). Multiple types may well be syntactically valid in this sense,
and worse, it may depend on the content of a parameter rather than on
its type whether it is valid or not. For example, for

    foo <- function(x)
    {
      if (x[1] != "bar") x / 2 else x
    }

you'd have to expect something like

    $x: a numeric vector, data.frame etc. or a character with x[1] == "bar"

Clearly, no (finite) amount of syntactic analysis of a function's code can
produce such a result, such information has to be provided in the docs.

If you look for more strict typing, the methods package may be of interest,
methods of S4 classes allow that.

Best regards, Jan
-- 
 +- Jan T. Kim -------------------------------------------------------+
 |    *NEW*    email: jtk at cmp.uea.ac.uk                               |
 |    *NEW*    WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
 *-----=<  hierarchical systems are for files, not for humans  >=-----*



From andy_liaw at merck.com  Thu Feb 17 12:36:06 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 17 Feb 2005 06:36:06 -0500
Subject: [R] Getting *types* of arguments?
Message-ID: <3A822319EB35174CA3714066D590DCD50994E708@usrymx25.merck.com>

Not to my knowledge.  In some cases an argument to a function can take on
different types, especially S3 methods.  The only sure way is to read the
help page (or if necessarily, the code).

This is one thing that stumbles me sometimes:  Some help pages describes
what the argument does, but not what it needs to be.  In some cases it may
be obvious, but not all...

Andy

> From: Georg Hoermann
> 
> Hello world,
> 
> short question: is there a possibility to get a list of
> arguments of a function *with* variable/parameter types?
> 
> formals() gives me the names of the parameters, but says
> nothing about the parameter type it expects (I know I can 
> always use the 
> help function). 
> 
> I would like somthing like
> 
> $x: vector or data.frame...
> 
> Thanks in advance...
> 
> Georg
> 
> 
> -- 
> Georg Hoermann, Fachabteilung Wasserwirtschaft / Dep. Hydrology  
> Ecosystem Research Center, Kiel University, Germany, Penguin #189476
> Tel. 0431-880-1207, 0172/4315715, ICQ: 348340729, MSN: hlschorsch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ripley at stats.ox.ac.uk  Thu Feb 17 13:34:26 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 17 Feb 2005 12:34:26 +0000 (GMT)
Subject: [R] Getting *types* of arguments?
In-Reply-To: <200502171144.59466.ghoermann@hydrology.uni-kiel.de>
References: <200502171144.59466.ghoermann@hydrology.uni-kiel.de>
Message-ID: <Pine.LNX.4.61.0502171059110.20476@gannet.stats>

On Thu, 17 Feb 2005, Georg Hoermann wrote:

> Hello world,
>
> short question: is there a possibility to get a list of
> arguments of a function *with* variable/parameter types?

Do you mean `argument' or `formal argument' or `parameter' or `variable'?

> formals() gives me the names of the parameters, but says
> nothing about the parameter type it expects (I know I can always use the
> help function).
>
> I would like somthing like
>
> $x: vector or data.frame...

No, because R's argument matching is polymorphic and many functions have 
arguments that accept many types, or coerce arguments to the required 
type.

BTW, a data frame is a list which is a vector, so your example isn't very 
realistic and shows that `type' is not a concept you have clear.  R has 
modes (?mode), types (?typeof) and classes (?class).  Neither `vector' nor 
`data frame' is a type.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Feb 17 13:36:28 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 17 Feb 2005 12:36:28 +0000 (GMT)
Subject: [R] Getting *types* of arguments?
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E708@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E708@usrymx25.merck.com>
Message-ID: <Pine.LNX.4.61.0502171234420.21232@gannet.stats>

On Thu, 17 Feb 2005, Liaw, Andy wrote:

> Not to my knowledge.  In some cases an argument to a function can take on
> different types, especially S3 methods.  The only sure way is to read the
> help page (or if necessarily, the code).
>
> This is one thing that stumbles me sometimes:  Some help pages describes
> what the argument does, but not what it needs to be.  In some cases it may
> be obvious, but not all...

Or if they say, they say inaccurately.  Often coercion is not mentioned, 
for example, and I keep on finding ones that are years out of date.

So

> or if necessarily, the code

usually is necessary.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Thu Feb 17 13:52:09 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 17 Feb 2005 13:52:09 +0100
Subject: [R] short plots: lwd, margin and postscript behavior
In-Reply-To: <20050217110735.86202.qmail@web14526.mail.yahoo.com>
References: <20050217110735.86202.qmail@web14526.mail.yahoo.com>
Message-ID: <42149379.90708@statistik.uni-dortmund.de>

C?zar Freitas wrote:

> Hi all.
> I'm working with a short plot (3x3 inches), but the
> results (via postscript command) are not nice. The lwd
> command don't affect the lines (that are very large)
> and the margins don't change using oma, mai, mar, ...
> Below I put an example. Moreover, save the graphics
> via postscript command isn't working well (see the
> attached ps).
> Thanks by the help,


Setting lwd=0.5 works for me, as well as using par(mar=c(....))



> Cezar Freitas.
> 
> #Example:
> #data
>   scores<-c(2.0, 0.0, 5.0, 5.0, 5.0, 2.0, 0.0, 5.0,
> 2.5, 4.0, 5.0, 0.0, 5.0, 0.0, 2.0, 5.0, 5.0, 2.0, 3.0,
> 3.0)
>   q<-summary(scores)
>   gra<-hist(scores, breaks=((0:11)/2-.2), plot=FALSE)
>   yy<-ceiling(max(gra$counts)/10)*10
>   yz<-yy/12
> 
> #plot via postscript
>   postscript("test.ps", width=3, height=3,
> horizontal=FALSE, family="Times", paper="special")

par(mar=rep(1,4)+.1, lwd=0.5)


>   hist(scores, breaks=((0:11)/2-.2), xlim=c(-1,6),
> ylim=c(-yz,yy), main="scores", ylab="Freq",
> xlab="math", cex.axis=.3, cex.main=.3, cex.sub=.3,
> cex.lab=.3, mgp=c(1.5,.5,0))

add lwd=0.5

Uwe Ligges

>   boxplot(scores, horizontal=1, add=1, at=-2*yz/3,
> boxwex=1.5*yz, bty="n", axes=FALSE)
>   points(q[4], -2*yz/3, col=1, pch="+", cex=.5)
> 
>   dev.off()
> 
> 
> 
> 
> 	
> 	
> 		
> _______________________________________________________ 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Thu Feb 17 13:53:22 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 17 Feb 2005 12:53:22 +0000 (GMT)
Subject: [R] problem with se.contrast()
In-Reply-To: <42139BF7.2090809@stat.ufl.edu>
References: <42139BF7.2090809@stat.ufl.edu>
Message-ID: <Pine.LNX.4.61.0502171237010.21232@gannet.stats>

The first problem is that Material is undefined when you called 
se.contrast, as you removed it.  This may work, but it certainly is not 
intentional and what variable gets picked up may not be predicted 
reliably.

The second is that I think you need R-devel which has some fixes.

However, I think this is an inappropriate analysis (which is why the older 
code trips up).  Your analysis is equivalent to using the fixed-effects 
model Lab + Material on the means of each Material block.  If there is 
something different about the replicates of the Material, this was a very 
poor design and so I suspect that is not what was done.  It looks like a 
randomized block design with replication in the blocks, for which

aov(Measurement ~ Lab + Material, data = testdata)

is the appropriate analysis.  In any case, there is no need of a 
multi-stratum analysis.


On Wed, 16 Feb 2005, Jamie Jarabek wrote:

> I am having trouble getting standard errors for contrasts using se.contrast() 
> in what appears to be a simple case to me. The following test example 
> illustrates my problem:
>
> Lab <- factor(rep(c("1","2","3"),each=12))
> Material <- factor(rep(c("A","B","C","D"),each=3,times=3))
> Measurement <- 
> c(12.20,12.28,12.16,15.51,15.02,15.29,18.14,18.08,18.21,18.54,18.36
> ,18.45,12.59,12.30,12.67,14.98,15.46,15.22,18.54,18.31,18.60,19.21,18.77
> ,18.69,12.72,12.78,12.66,15.33,15.19,15.24,18.00,18.15,17.93,18.88,18.12,18.03)
>
> testdata <- data.frame(Lab,Material,Measurement)
> rm(list=c("Lab","Material","Measurement"))
>
> test.aov <- with(testdata,aov(Measurement ~ Material + Error(Lab/Material)))
>
> This gives me the desired ANOVA table. I next want to get the standard
> errors for certain contrasts and following the help page for
> se.contrast() I tried the following but I get an error:
>
>> se.contrast(test.aov,list(Material=="A",Material=="B",Material=="C",Material=="D"),coef=c(1,1,-1,-1),data=testdata)
> Error in matrix(0, length(asgn), ncol(effects), dimnames = list(nm[1 + :
>        length of dimnames [1] not equal to array extent
>
> I have tested this on R 2.0.1 on Windows XP and Solaris and get the same
> error on both systems. I am unsure as to what I am doing wrong here. Thanks 
> for any help.
>
> Jamie Jarabek
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From michael.watson at bbsrc.ac.uk  Thu Feb 17 14:01:48 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Thu, 17 Feb 2005 13:01:48 -0000
Subject: [R] A vector or array of data frames
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950172CED3@iahce2knas1.iah.bbsrc.reserved>

Hi

A simple question again, but I can't find it by google-ing R-help.

Quite simply, I want to read in the contents of a number of files, using
read.table, and assign the results to elements of a
vector/array/list/whatever.

I want it so that, if my vector/array/whatever is "pos", that pos[1]
will give me the first data frame, pos[2] will give me the second etc...

Kind of basic stuff I know... Sorry..

Mick



From cafanselmo12 at yahoo.com.br  Thu Feb 17 14:15:20 2005
From: cafanselmo12 at yahoo.com.br (=?iso-8859-1?q?C=E9zar=20Freitas?=)
Date: Thu, 17 Feb 2005 10:15:20 -0300 (ART)
Subject: [R] short plots: lwd, margin and postscript behavior
In-Reply-To: <42149379.90708@statistik.uni-dortmund.de>
Message-ID: <20050217131520.32792.qmail@web14524.mail.yahoo.com>

I tested in R versions 1.8.1 and 2, but doesn't works.
The attached plots can explain this. And mar isn't a
parameter to postscript command. If I use in par, it
doesn't affect the outpu.

Thanks,
C.

 --- Uwe Ligges <ligges at statistik.uni-dortmund.de>
escreveu: 
> C?zar Freitas wrote:
> 
> > Hi all.
> > I'm working with a short plot (3x3 inches), but
> the
> > results (via postscript command) are not nice. The
> lwd
> > command don't affect the lines (that are very
> large)
> > and the margins don't change using oma, mai, mar,
> ...
> > Below I put an example. Moreover, save the
> graphics
> > via postscript command isn't working well (see the
> > attached ps).
> > Thanks by the help,
> 
> 
> Setting lwd=0.5 works for me, as well as using
> par(mar=c(....))
> 
> 
> 
> > Cezar Freitas.
> > 
> > #Example:
> > #data
> >   scores<-c(2.0, 0.0, 5.0, 5.0, 5.0, 2.0, 0.0,
> 5.0,
> > 2.5, 4.0, 5.0, 0.0, 5.0, 0.0, 2.0, 5.0, 5.0, 2.0,
> 3.0,
> > 3.0)
> >   q<-summary(scores)
> >   gra<-hist(scores, breaks=((0:11)/2-.2),
> plot=FALSE)
> >   yy<-ceiling(max(gra$counts)/10)*10
> >   yz<-yy/12
> > 
> > #plot via postscript
> >   postscript("test.ps", width=3, height=3,
> > horizontal=FALSE, family="Times", paper="special")
> 
> par(mar=rep(1,4)+.1, lwd=0.5)
> 
> 
> >   hist(scores, breaks=((0:11)/2-.2), xlim=c(-1,6),
> > ylim=c(-yz,yy), main="scores", ylab="Freq",
> > xlab="math", cex.axis=.3, cex.main=.3, cex.sub=.3,
> > cex.lab=.3, mgp=c(1.5,.5,0))
> 
> add lwd=0.5
> 
> Uwe Ligges
> 
> >   boxplot(scores, horizontal=1, add=1, at=-2*yz/3,
> > boxwex=1.5*yz, bty="n", axes=FALSE)
> >   points(q[4], -2*yz/3, col=1, pch="+", cex=.5)
> > 
> >   dev.off()
> > 
> > 
> > 
> > 
> > 	
> > 	
> > 		
> >
>
_______________________________________________________
> 
> > 
> > 
> >
>
------------------------------------------------------------------------
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
>



From ligges at statistik.uni-dortmund.de  Thu Feb 17 14:19:41 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 17 Feb 2005 14:19:41 +0100
Subject: [R] short plots: lwd, margin and postscript behavior
In-Reply-To: <20050217131520.32792.qmail@web14524.mail.yahoo.com>
References: <20050217131520.32792.qmail@web14524.mail.yahoo.com>
Message-ID: <421499ED.80209@statistik.uni-dortmund.de>

C?zar Freitas wrote:

> I tested in R versions 1.8.1 and 2, but doesn't works.
> The attached plots can explain this. And mar isn't a
> parameter to postscript command. If I use in par, it
> doesn't affect the outpu.

It does in R-2.0.1!!! Please try out what poeple are suggesting!
You have to set par() for the current device, so after your call to 
postscript, as I have indicated below.

Uwe Ligges




> 
> Thanks,
> C.
> 
>  --- Uwe Ligges <ligges at statistik.uni-dortmund.de>
> escreveu: 
> 
>>C?zar Freitas wrote:
>>
>>
>>>Hi all.
>>>I'm working with a short plot (3x3 inches), but
>>
>>the
>>
>>>results (via postscript command) are not nice. The
>>
>>lwd
>>
>>>command don't affect the lines (that are very
>>
>>large)
>>
>>>and the margins don't change using oma, mai, mar,
>>
>>...
>>
>>>Below I put an example. Moreover, save the
>>
>>graphics
>>
>>>via postscript command isn't working well (see the
>>>attached ps).
>>>Thanks by the help,
>>
>>
>>Setting lwd=0.5 works for me, as well as using
>>par(mar=c(....))
>>
>>
>>
>>
>>>Cezar Freitas.
>>>
>>>#Example:
>>>#data
>>>  scores<-c(2.0, 0.0, 5.0, 5.0, 5.0, 2.0, 0.0,
>>
>>5.0,
>>
>>>2.5, 4.0, 5.0, 0.0, 5.0, 0.0, 2.0, 5.0, 5.0, 2.0,
>>
>>3.0,
>>
>>>3.0)
>>>  q<-summary(scores)
>>>  gra<-hist(scores, breaks=((0:11)/2-.2),
>>
>>plot=FALSE)
>>
>>>  yy<-ceiling(max(gra$counts)/10)*10
>>>  yz<-yy/12
>>>
>>>#plot via postscript
>>>  postscript("test.ps", width=3, height=3,
>>>horizontal=FALSE, family="Times", paper="special")
>>
>>par(mar=rep(1,4)+.1, lwd=0.5)
>>
>>
>>
>>>  hist(scores, breaks=((0:11)/2-.2), xlim=c(-1,6),
>>>ylim=c(-yz,yy), main="scores", ylab="Freq",
>>>xlab="math", cex.axis=.3, cex.main=.3, cex.sub=.3,
>>>cex.lab=.3, mgp=c(1.5,.5,0))
>>
>>add lwd=0.5
>>
>>Uwe Ligges
>>
>>
>>>  boxplot(scores, horizontal=1, add=1, at=-2*yz/3,
>>>boxwex=1.5*yz, bty="n", axes=FALSE)
>>>  points(q[4], -2*yz/3, col=1, pch="+", cex=.5)
>>>
>>>  dev.off()
>>>
>>>
>>>
>>>
>>>	
>>>	
>>>		
>>>
>>
> _______________________________________________________
> 
>>>
>>>
> ------------------------------------------------------------------------
> 
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide!
>>
>>http://www.R-project.org/posting-guide.html
>>
>> 
> 
> 
> 
> 	
> 	
> 		
> _______________________________________________________ 



From ligges at statistik.uni-dortmund.de  Thu Feb 17 14:23:04 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 17 Feb 2005 14:23:04 +0100
Subject: [R] A vector or array of data frames
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950172CED3@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E950172CED3@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <42149AB8.3040802@statistik.uni-dortmund.de>

michael watson (IAH-C) wrote:

> Hi
> 
> A simple question again, but I can't find it by google-ing R-help.
> 
> Quite simply, I want to read in the contents of a number of files, using
> read.table, and assign the results to elements of a
> vector/array/list/whatever.
> 
> I want it so that, if my vector/array/whatever is "pos", that pos[1]
> will give me the first data frame, pos[2] will give me the second etc...

For sure you will find a solution! This has been answered several times.

You can have a list "pos" (where pos is a bad time which suggest to have 
something in common with environments) so that pos[[1]] is your first 
data frame etc. See ?list

Uwe Ligges




> Kind of basic stuff I know... Sorry..
> 
> Mick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From dimitris.rizopoulos at med.kuleuven.ac.be  Thu Feb 17 14:26:40 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Thu, 17 Feb 2005 14:26:40 +0100
Subject: [R] A vector or array of data frames
References: <8975119BCD0AC5419D61A9CF1A923E950172CED3@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <002c01c514f4$504f30b0$0540210a@www.domain>

you could use something like:

file. <- c:/data/dat # the path for the data
lis.dat <- lapply(1:B, function(i) read.table(paste(file., i, 
sep="")) )

where B is the number of files whose names are: dat1, dat2, ..., datB. 
Then lis.dat[[1]] will be the first data.frame.

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "michael watson (IAH-C)" <michael.watson at bbsrc.ac.uk>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, February 17, 2005 2:01 PM
Subject: [R] A vector or array of data frames


> Hi
>
> A simple question again, but I can't find it by google-ing R-help.
>
> Quite simply, I want to read in the contents of a number of files, 
> using
> read.table, and assign the results to elements of a
> vector/array/list/whatever.
>
> I want it so that, if my vector/array/whatever is "pos", that pos[1]
> will give me the first data frame, pos[2] will give me the second 
> etc...
>
> Kind of basic stuff I know... Sorry..
>
> Mick
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From cafanselmo12 at yahoo.com.br  Thu Feb 17 14:36:13 2005
From: cafanselmo12 at yahoo.com.br (=?iso-8859-1?q?C=E9zar=20Freitas?=)
Date: Thu, 17 Feb 2005 10:36:13 -0300 (ART)
Subject: [R] short plots: lwd, margin and postscript behavior
In-Reply-To: <421499ED.80209@statistik.uni-dortmund.de>
Message-ID: <20050217133613.61151.qmail@web14521.mail.yahoo.com>

Thank you so much, but the commands in par doesn't
affect the pictures generated by postscript.
For example, I put lwd=3, and the postscript file is
the same, using par(lwd=3) or par(lwd=.5)...

 --- Uwe Ligges <ligges at statistik.uni-dortmund.de>
escreveu: 
> C?zar Freitas wrote:
> 
> > I tested in R versions 1.8.1 and 2, but doesn't
> works.
> > The attached plots can explain this. And mar isn't
> a
> > parameter to postscript command. If I use in par,
> it
> > doesn't affect the outpu.
> 
> It does in R-2.0.1!!! Please try out what poeple are
> suggesting!
> You have to set par() for the current device, so
> after your call to 
> postscript, as I have indicated below.
> 
> Uwe Ligges
> 
> 
> 
> 
> > 
> > Thanks,
> > C.
> > 
> >  --- Uwe Ligges <ligges at statistik.uni-dortmund.de>
> > escreveu: 
> > 
> >>C?zar Freitas wrote:
> >>
> >>
> >>>Hi all.
> >>>I'm working with a short plot (3x3 inches), but
> >>
> >>the
> >>
> >>>results (via postscript command) are not nice.
> The
> >>
> >>lwd
> >>
> >>>command don't affect the lines (that are very
> >>
> >>large)
> >>
> >>>and the margins don't change using oma, mai, mar,
> >>
> >>...
> >>
> >>>Below I put an example. Moreover, save the
> >>
> >>graphics
> >>
> >>>via postscript command isn't working well (see
> the
> >>>attached ps).
> >>>Thanks by the help,
> >>
> >>
> >>Setting lwd=0.5 works for me, as well as using
> >>par(mar=c(....))
> >>
> >>
> >>
> >>
> >>>Cezar Freitas.
> >>>
> >>>#Example:
> >>>#data
> >>>  scores<-c(2.0, 0.0, 5.0, 5.0, 5.0, 2.0, 0.0,
> >>
> >>5.0,
> >>
> >>>2.5, 4.0, 5.0, 0.0, 5.0, 0.0, 2.0, 5.0, 5.0, 2.0,
> >>
> >>3.0,
> >>
> >>>3.0)
> >>>  q<-summary(scores)
> >>>  gra<-hist(scores, breaks=((0:11)/2-.2),
> >>
> >>plot=FALSE)
> >>
> >>>  yy<-ceiling(max(gra$counts)/10)*10
> >>>  yz<-yy/12
> >>>
> >>>#plot via postscript
> >>>  postscript("test.ps", width=3, height=3,
> >>>horizontal=FALSE, family="Times",
> paper="special")
> >>
> >>par(mar=rep(1,4)+.1, lwd=0.5)
> >>
> >>
> >>
> >>>  hist(scores, breaks=((0:11)/2-.2),
> xlim=c(-1,6),
> >>>ylim=c(-yz,yy), main="scores", ylab="Freq",
> >>>xlab="math", cex.axis=.3, cex.main=.3,
> cex.sub=.3,
> >>>cex.lab=.3, mgp=c(1.5,.5,0))
> >>
> >>add lwd=0.5
> >>
> >>Uwe Ligges
> >>
> >>
> >>>  boxplot(scores, horizontal=1, add=1,
> at=-2*yz/3,
> >>>boxwex=1.5*yz, bty="n", axes=FALSE)
> >>>  points(q[4], -2*yz/3, col=1, pch="+", cex=.5)
> >>>
> >>>  dev.off()
> >>>
> >>>
> >>>
> >>>
> >>>	
> >>>	
> >>>		
> >>>
> >>
> >
>
_______________________________________________________
> > 
> >>>
> >>>
> >
>
------------------------------------------------------------------------
> > 
> >>>______________________________________________
> >>>R-help at stat.math.ethz.ch mailing list
> >>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>PLEASE do read the posting guide!
> >>
> >>http://www.R-project.org/posting-guide.html
> >>
> >> 
> > 
> > 
> > 
> > 	
> > 	
> > 		
> >
>
_______________________________________________________
> 


> r?pida e gr?tis
> 
>



From bxc at steno.dk  Thu Feb 17 14:39:53 2005
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Thu, 17 Feb 2005 14:39:53 +0100
Subject: [R] How to get interction terms first in a model
Message-ID: <0ABD88905D18E347874E0FB71C0B29E9027FEAFF@exdkba022.novo.dk>

Consider the following two specifications of a model:

library( splines )
x <- 1:100
y <- rnorm( 100 )
w <- rep( 1, 100 )
A <- factor( sample( 1:2, 100, replace=T ) )
B <- factor( sample( letters[1:4], 100, replace=T ) )
summary( lm( y ~ ns( x, knots=c(30, 50, 70 ), intercept=T ):A - 1 + B )
)
summary( lm( y ~ ns( x, knots=c(30, 50, 70 ), intercept=T ):A - 1 + B:w
) )

The interaction with the constant variable w is how I got to have the
two spline terms as "proper" intercept terms.

Is there another way to do this, or is it a featur of the model formulae
that lower order terms appear before higher order terms regardless of
the
order they are specified in the model?

Bendix Carstensen
----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc



From mike_saunders at umenfa.maine.edu  Thu Feb 17 14:43:46 2005
From: mike_saunders at umenfa.maine.edu (Mike Saunders)
Date: Thu, 17 Feb 2005 08:43:46 -0500
Subject: [R] Split-split plot ANOVA
Message-ID: <000601c514f6$b3ed7080$9ba76f82@CFRU0104>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050217/25ba0a0f/attachment.pl

From p.dalgaard at biostat.ku.dk  Thu Feb 17 14:53:52 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 17 Feb 2005 14:53:52 +0100
Subject: [R] A vector or array of data frames
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950172CED3@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E950172CED3@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <x2vf8ruwb3.fsf@biostat.ku.dk>

"michael watson (IAH-C)" <michael.watson at bbsrc.ac.uk> writes:

> Hi
> 
> A simple question again, but I can't find it by google-ing R-help.
> 
> Quite simply, I want to read in the contents of a number of files, using
> read.table, and assign the results to elements of a
> vector/array/list/whatever.
> 
> I want it so that, if my vector/array/whatever is "pos", that pos[1]
> will give me the first data frame, pos[2] will give me the second etc...

No can do. pos[[1]] etc, however.... (pos[1] is another list, with one
element, in this case the first data frame)

x <- c("fee.dat","foe.dat","fie.dat","fum.dat")
lapply(x, read.table, header=TRUE) # or whatever is needed
 
> Kind of basic stuff I know... Sorry..


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From mike_saunders at umenfa.maine.edu  Thu Feb 17 15:20:14 2005
From: mike_saunders at umenfa.maine.edu (Mike Saunders)
Date: Thu, 17 Feb 2005 09:20:14 -0500
Subject: [R] short plots: lwd, margin and postscript behavior
References: <20050217133613.61151.qmail@web14521.mail.yahoo.com>
Message-ID: <001c01c514fb$cc6009c0$9ba76f82@CFRU0104>

Don't feel alone C?zar; I have had the same problems getting par commands to 
work when making *.pdf files.  I also run R 2.0.1 and use par() after 
opening the device.  Sometimes, the par() commands work; sometimes it opens 
a new windows device.  Maybe something with MS Windows is causing this.

Mike

Mike Saunders
Research Assistant
Forest Ecosystem Research Program
Department of Forest Ecosystem Sciences
University of Maine
Orono, ME  04469
207-581-2763 (O)
207-581-4257 (F)

----- Original Message ----- 
From: "C?zar Freitas" <cafanselmo12 at yahoo.com.br>
To: "Uwe Ligges" <ligges at statistik.uni-dortmund.de>; 
<R-help at stat.math.ethz.ch>
Sent: Thursday, February 17, 2005 8:36 AM
Subject: Re: [R] short plots: lwd, margin and postscript behavior


Thank you so much, but the commands in par doesn't
affect the pictures generated by postscript.
For example, I put lwd=3, and the postscript file is
the same, using par(lwd=3) or par(lwd=.5)...

 --- Uwe Ligges <ligges at statistik.uni-dortmund.de>
escreveu:
> C?zar Freitas wrote:
>
> > I tested in R versions 1.8.1 and 2, but doesn't
> works.
> > The attached plots can explain this. And mar isn't
> a
> > parameter to postscript command. If I use in par,
> it
> > doesn't affect the outpu.
>
> It does in R-2.0.1!!! Please try out what poeple are
> suggesting!
> You have to set par() for the current device, so
> after your call to
> postscript, as I have indicated below.
>
> Uwe Ligges
>
>
>
>
> >
> > Thanks,
> > C.
> >
> >  --- Uwe Ligges <ligges at statistik.uni-dortmund.de>
> > escreveu:
> >
> >>C?zar Freitas wrote:
> >>
> >>
> >>>Hi all.
> >>>I'm working with a short plot (3x3 inches), but
> >>
> >>the
> >>
> >>>results (via postscript command) are not nice.
> The
> >>
> >>lwd
> >>
> >>>command don't affect the lines (that are very
> >>
> >>large)
> >>
> >>>and the margins don't change using oma, mai, mar,
> >>
> >>...
> >>
> >>>Below I put an example. Moreover, save the
> >>
> >>graphics
> >>
> >>>via postscript command isn't working well (see
> the
> >>>attached ps).
> >>>Thanks by the help,
> >>
> >>
> >>Setting lwd=0.5 works for me, as well as using
> >>par(mar=c(....))
> >>
> >>
> >>
> >>
> >>>Cezar Freitas.
> >>>
> >>>#Example:
> >>>#data
> >>>  scores<-c(2.0, 0.0, 5.0, 5.0, 5.0, 2.0, 0.0,
> >>
> >>5.0,
> >>
> >>>2.5, 4.0, 5.0, 0.0, 5.0, 0.0, 2.0, 5.0, 5.0, 2.0,
> >>
> >>3.0,
> >>
> >>>3.0)
> >>>  q<-summary(scores)
> >>>  gra<-hist(scores, breaks=((0:11)/2-.2),
> >>
> >>plot=FALSE)
> >>
> >>>  yy<-ceiling(max(gra$counts)/10)*10
> >>>  yz<-yy/12
> >>>
> >>>#plot via postscript
> >>>  postscript("test.ps", width=3, height=3,
> >>>horizontal=FALSE, family="Times",
> paper="special")
> >>
> >>par(mar=rep(1,4)+.1, lwd=0.5)
> >>
> >>
> >>
> >>>  hist(scores, breaks=((0:11)/2-.2),
> xlim=c(-1,6),
> >>>ylim=c(-yz,yy), main="scores", ylab="Freq",
> >>>xlab="math", cex.axis=.3, cex.main=.3,
> cex.sub=.3,
> >>>cex.lab=.3, mgp=c(1.5,.5,0))
> >>
> >>add lwd=0.5
> >>
> >>Uwe Ligges
> >>
> >>
> >>>  boxplot(scores, horizontal=1, add=1,
> at=-2*yz/3,
> >>>boxwex=1.5*yz, bty="n", axes=FALSE)
> >>>  points(q[4], -2*yz/3, col=1, pch="+", cex=.5)
> >>>
> >>>  dev.off()
> >>>
> >>>
> >>>
> >>>
> >>>
> >>>
> >>>
> >>>
> >>
> >
>
_______________________________________________________
> >
> >>>
> >>>
> >
>
------------------------------------------------------------------------
> >
> >>>______________________________________________
> >>>R-help at stat.math.ethz.ch mailing list
> >>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>PLEASE do read the posting guide!
> >>
> >>http://www.R-project.org/posting-guide.html
> >>
> >>
> >
> >
> >
> >
> >
> >
> >
>
_______________________________________________________
>


> r?pida e gr?tis
>
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html



From michael.watson at bbsrc.ac.uk  Thu Feb 17 15:36:43 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Thu, 17 Feb 2005 14:36:43 -0000
Subject: [R] Converting a list to a matrix - I still don't think I have it
	right
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950121BA57@iahce2knas1.iah.bbsrc.reserved>

Hi

We have touched on this before, but I don't think I quite got it right.

So I have a list, each element of which is a a vector of 2 numbers:

> l2
$cat000_a01
[1] 0.3429944 4.5138244

$cat000_a02
[1] 0.1929336 4.3064944

$cat000_a03
[1] -0.2607796  4.1551591

What I actually want to convert this into is a matrix with the names
(cat000_a01 etc) as row names, the first element of each of the vectors
forming the first column of the new matrix, and the second element of
each of the vectors forming the second column:

cat000_a01	0.3429944	4.5138244
cat000_a02	0.1929336	4.3064944
cat000_a03	-0.2607796  4.1551591

What was suggested on the list last time was
matrix(unlist(mylist),nrow=length(mylist)).  But if I do this I get:

> matrix(unlist(l2),nrow=length(l2))
          [,1]       [,2]
[1,] 0.3429944  4.3064944
[2,] 4.5138244 -0.2607796
[3,] 0.1929336  4.1551591

Which is not what I want.  Here, the second element of the first vector
in my list has gone into the first column of the new matrix, and that's
not what I want at all.

Any more help would be appreciated.

Thanks
Mick



From knoblauch at lyon.inserm.fr  Thu Feb 17 14:54:46 2005
From: knoblauch at lyon.inserm.fr (Ken Knoblauch)
Date: Thu, 17 Feb 2005 14:54:46 +0100
Subject: [R] Easy cut & paste from Excel to R?
Message-ID: <1108648485.4214a22601b1d@webmail.lyon.inserm.fr>

Hi,

I tried the interesting suggestion below, discussed in several postings 
yesterday on the help-list, on my Mac (0S 10.3.7) but could not get it to
work, as shown in the tests indicated below.

>>
>>   read.table(file("clipboard"), sep="\t", dec=",")
>>

If it is obvious (even, if not), can someone tell me what I am doing wrong?  
Do I need to perform an additional operation to open "clipboard", or is this 
option not available on Mac? I did not find any special discussion of this in
the FAQ nor searching under "clipboard" in the archives.  Thank you, in 
advance, for any enlightenment.

data.frame(matrix(rnorm(12),ncol=3))
          X1          X2          X3
1  0.4276964 -0.49584891  0.02150469
2 -0.8323586 -0.40120649 -1.90733346
3 -0.8954563 -1.33195844 -1.28261484
4  0.4772382 -0.03703087  0.46719156
#At this point, I block-marked the printed output and apple-C'd it into the 
clipboard.
#I then checked the clipboard to verify that the data was indeed copied there.
> read.table("clipboard")
Error in file(file, "r") : unable to open connection
In addition: Warning message: 
cannot open file `clipboard' 
> read.table(file("clipboard"))
Error in open.connection(file, "r") : unable to open connection
In addition: Warning message: 
cannot open file `clipboard' 
> read.table(file("clipboard","r"))
Error in file("clipboard", "r") : unable to open connection
In addition: Warning message: 
cannot open file `clipboard' 
> read.table(file("clipboard","r"),header=TRUE)
Error in file("clipboard", "r") : unable to open connection
In addition: Warning message: 
cannot open file `clipboard' 
>read.delim(file("clipboard","r"),header=TRUE)
Error in file("clipboard", "r") : unable to open connection
In addition: Warning message: 
cannot open file `clipboard' 

> file("clipboard")
description       class        mode        text      opened    can read 
"clipboard"      "file"         "r"      "text"    "closed"       "yes" 
  can write 
      "yes" 

> file("clipboard","r")
Error in file("clipboard", "r") : unable to open connection
In addition: Warning message: 
cannot open file `clipboard' 

platform powerpc-apple-darwin6.8
arch     powerpc                
os       darwin6.8              
system   powerpc, darwin6.8     
status                          
major    2                      
minor    0.1                    
year     2004                   
month    11                     
day      15                     
language R   

____________________
Ken Knoblauch
Inserm U 371
Cerveau et Vision
18 avenue du Doyen Lepine
69675 Bron cedex
France
tel: +33 (0)4 72 91 34 77
fax: +33 (0)4 72 91 34 61
portable: 06 84 10 64 10



From andy_liaw at merck.com  Thu Feb 17 16:00:03 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 17 Feb 2005 10:00:03 -0500
Subject: [R] Converting a list to a matrix - I still don't think I
	hav e it right
Message-ID: <3A822319EB35174CA3714066D590DCD50994E70C@usrymx25.merck.com>

Try:

mat <- do.call("rbind", l2)

Not sure if the names get put into the rownames automatically, but if not,
just do:

rownames(mat) <- names(l2)

Andy

> From: michael watson (IAH-C)
> 
> Hi
> 
> We have touched on this before, but I don't think I quite got 
> it right.
> 
> So I have a list, each element of which is a a vector of 2 numbers:
> 
> > l2
> $cat000_a01
> [1] 0.3429944 4.5138244
> 
> $cat000_a02
> [1] 0.1929336 4.3064944
> 
> $cat000_a03
> [1] -0.2607796  4.1551591
> 
> What I actually want to convert this into is a matrix with the names
> (cat000_a01 etc) as row names, the first element of each of 
> the vectors
> forming the first column of the new matrix, and the second element of
> each of the vectors forming the second column:
> 
> cat000_a01	0.3429944	4.5138244
> cat000_a02	0.1929336	4.3064944
> cat000_a03	-0.2607796  4.1551591
> 
> What was suggested on the list last time was
> matrix(unlist(mylist),nrow=length(mylist)).  But if I do this I get:
> 
> > matrix(unlist(l2),nrow=length(l2))
>           [,1]       [,2]
> [1,] 0.3429944  4.3064944
> [2,] 4.5138244 -0.2607796
> [3,] 0.1929336  4.1551591
> 
> Which is not what I want.  Here, the second element of the 
> first vector
> in my list has gone into the first column of the new matrix, 
> and that's
> not what I want at all.
> 
> Any more help would be appreciated.
> 
> Thanks
> Mick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From deepayan at stat.wisc.edu  Thu Feb 17 15:52:28 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 17 Feb 2005 08:52:28 -0600
Subject: [R] Factor level coloring in trellis plot
In-Reply-To: <web-1245939@mail3.rug.nl>
References: <web-1245939@mail3.rug.nl>
Message-ID: <200502170852.29067.deepayan@stat.wisc.edu>

On Thursday 17 February 2005 04:39, T.A.Wassenaar wrote:
> Hi :)
>
> Was just wondering whether someone could help me with
> adjustments to trellis plots (parallel).
>
> I've got two way multivariate data. I want to make
> parallel plots for one of the factors, and want to color
> the lines according to the other factor. The first thing I
> manage, but with the other I'm lost :( Can only change the
> overall color.
>
> This is basically how far I get:
>
> parallel(~data | factor, layout=c(4,1))
>
> Any hints will be greatly appreciated.

panel.parallel doesn't handle groups, so you'll need to write a panel 
function that does. This is easy enough, the current panel function 
needs only a few additions:


panel.parallel.new <- 
    function(z, subscripts,
             groups = NULL,
             col=superpose.line$col,
             lwd=superpose.line$lwd,
             lty=superpose.line$lty, ...)
{
    superpose.line <- trellis.par.get("superpose.line")
    reference.line <- trellis.par.get("reference.line")

    n.r <- ncol(z)
    n.c <- length(subscripts)
    if (is.null(groups)) {
        col <- rep(col, length=n.c)
        lty <- rep(lty, length=n.c)
        lwd <- rep(lwd, length=n.c)
    }
    else
    {
        gnum <- as.integer(as.factor(groups))
        n.g <- length(unique(gnum))
        col <- rep(col, length=n.g)
        lty <- rep(lty, length=n.g)
        lwd <- rep(lwd, length=n.g)
    }

    llim <- numeric(n.r)
    ulim <- numeric(n.r)
    dif <- numeric(n.r)
    if (n.r > 0)
        for(i in 1:n.r) {
            grid.lines(x = c(0,1), y = c(i,i),
                       default.units = "native",
                       gp = gpar(col = reference.line$col,
                       lwd = reference.line$lwd,
                       lty = reference.line$lty))
            llim[i] <- range(as.numeric(z[,i]))[1]
            ulim[i] <- range(as.numeric(z[,i]))[2]
            dif[i] <- ulim[i] - llim[i]
        }
   
    if (is.null(groups))
        for (i in seq(along=subscripts))
        {
            x <- (as.numeric(z[subscripts[i],,])-llim)/dif
            grid.lines(x = x,
                       y = 1:n.r, 
                       gp = gpar(col=col[i], lty=lty[i], lwd=lwd[i]),
                       default.units="native")
        }
    else 
        for (i in seq(along=subscripts))
        {
            x <- (as.numeric(z[subscripts[i],,])-llim)/dif
            grid.lines(x = x,
                       y = 1:n.r, 
                       gp =
                       gpar(col=col[gnum[subscripts[i]]],
                            lty=lty[gnum[subscripts[i]]],
                            lwd=lwd[gnum[subscripts[i]]]),
                       default.units="native")
        }
    invisible()
}


With this, you should get what you want with 

library(grid)
library(lattice)
parallel(~iris[1:4], iris, 
         panel = panel.parallel.new, 
         groups = Species)

I'll include this in the next release.

Deepayan



From tuechler at gmx.at  Thu Feb 17 15:49:59 2005
From: tuechler at gmx.at (Heinz Tuechler)
Date: Thu, 17 Feb 2005 15:49:59 +0100
Subject: [R] Again: Variable names in functions
In-Reply-To: <4214768C.4070708@statistik.uni-dortmund.de>
References: <3.0.6.32.20050217113007.007b0860@pop.gmx.net>
	<3.0.6.32.20050217113007.007b0860@pop.gmx.net>
Message-ID: <3.0.6.32.20050217154959.007b3180@pop.gmx.net>

At 11:48 17.02.2005 +0100, Uwe Ligges wrote:
>
>See argument "dnn" in ?table:
>
>
>   mytable2 <- function(x,y){
>     table(x, y, dnn = c(deparse(substitute(x)),
>                         deparse(substitute(y))))
>   }
>
>Uwe Ligges
>
>
>

Thank you for your hint. This is of course a good solution for the example.
Still I am looking for a more general solution, but it is not urgent.

Heinz T?chler


>Heinz Tuechler wrote:
>
>> Hello,
>> 
>> still I have difficulties with variable names in functions. I know the
>> famous example form help for deparse/substitute but I will give a simpler
>> one to explain my problem.
>> I know from Reid Huntsinger (Tue, 8 Feb 2005 12:39:32 -0500) that:
>> "Semantically, R is pass-by-value, so you don't really have the names, just
>> the values. In implementation, though, R *does* pass names, in part at
least
>> in order to do "lazy evaluation". You can get them via "substitute" ; see
>> the help for that." 
>> 
>> The output of several functions does not make much sense, if these names do
>> not appear (e.g. parameter estimates in Cox-regression, ...)
>> Only to give a trivial example I show my problem with the table function.
>> 
>> As you know, if I call table as follows, the output is labelled properly.
>> 
>>>charly<-c(rep(1,3),rep(2,7));delta<-c(rep(1:2,5))
>>>table(charly, delta)
>> 
>>       delta
>> charly 1 2
>>      1 2 1
>>      2 3 4
>> If I define a trivial function to call table, the output is less
satisfying.
>> (Of course, I know that this function is useless.)
>> 
>>>mytable1<-function(x,y){table(x,y)}
>>>mytable1(charly, delta)
>> 
>>    y
>> x   1 2
>>   1 2 1
>>   2 3 4
>> If I define the function in the following way, it does what I wish, namely
>> it returns output equivalent to the simple call "table(charly, delta)".
>> 
>>>mytable2<-function(x,y){
>> 
>> +   cat("table(",as.symbol((deparse(substitute(x)))),
>> +   "," ,  as.symbol(deparse(substitute(y))),")\n",
>> +   file="temp",sep="",append=F)
>> +   eval(parse("temp",n=-1))
>> +                       }
>> 
>>>mytable2(charly, delta)
>>       delta
>> charly 1 2
>>      1 2 1
>>      2 3 4
>> 
>> I assume that there is a better way to solve this problem and I would be
>> happy about hints, where to find solutions in the documentation.
>> 
>> Thanks,
>> 
>> Heinz T?chler
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>
>



From tuechler at gmx.at  Thu Feb 17 16:03:49 2005
From: tuechler at gmx.at (Heinz Tuechler)
Date: Thu, 17 Feb 2005 16:03:49 +0100
Subject: [R] Again: Variable names in functions
In-Reply-To: <x24qgbwjb0.fsf@biostat.ku.dk>
References: <3.0.6.32.20050217113007.007b0860@pop.gmx.net>
	<3.0.6.32.20050217113007.007b0860@pop.gmx.net>
Message-ID: <3.0.6.32.20050217160349.007b4370@pop.gmx.net>

At 11:51 17.02.2005 +0100, Peter Dalgaard wrote:
>Heinz Tuechler <tuechler at gmx.at> writes:
>
>> > mytable1<-function(x,y){table(x,y)}
>> > mytable1(charly, delta)
>>    y
>> x   1 2
>>   1 2 1
>>   2 3 4
>> If I define the function in the following way, it does what I wish, namely
>> it returns output equivalent to the simple call "table(charly, delta)".
>> > mytable2<-function(x,y){
>> +   cat("table(",as.symbol((deparse(substitute(x)))),
>> +   "," ,  as.symbol(deparse(substitute(y))),")\n",
>> +   file="temp",sep="",append=F)
>> +   eval(parse("temp",n=-1))
>> +                       }
>> > mytable2(charly, delta)
>>       delta
>> charly 1 2
>>      1 2 1
>>      2 3 4
>> > 
>> I assume that there is a better way to solve this problem and I would be
>> happy about hints, where to find solutions in the documentation.
>
>What did Thomas L. say recently? "If the answer involves parse(), you
>probably asked the wrong question", I think it was.
>
>The canonical way is
>
>mytable <- function(x,y) eval.parent(substitute(table(x,y)))
>
>or, you could of course modify the names(dimnames(...)) and just pass
>the names along.
>
>
>-- 
>   O__  ---- Peter Dalgaard             Blegdamsvej 3  
>  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
> (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
>~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907
>

Thank you, this method works well. One step further I am again using
parse(), but maybe there is a better solution for that situation too.
The example would be a function, where I pass the variable name as string
instead of the name. The motivation for this is that it seems easier to
handle if I want to pass several variables (i.e. a vector of variable
names) to the function (as I learned recently from this help-list). 
In this case I have to use get(). In the case of calling table() the
variable name disappeares.

> alpha<-c(rep(1:5,10))
> name.alpha<-"alpha"
> mytable1<-function(x){print(table(get(x)))}
> mytable1(name.alpha)

 1  2  3  4  5 
10 10 10 10 10 

If I use eval(parse()) instead, it works as expected. I tried several
combinations of eval() and substitute() but I did not find a solution.
Is there a similar "trick"?

> mytable2<-function(x){
+   string<-paste("print(table(",as.symbol(x),"))")
+   eval(parse(text=string))}
> mytable2(name.alpha)
alpha
 1  2  3  4  5 
10 10 10 10 10 


Thanks,
Heinz T?chler



From sdavis2 at mail.nih.gov  Thu Feb 17 16:03:32 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 17 Feb 2005 10:03:32 -0500
Subject: [R] Converting a list to a matrix - I still don't think I have it
	right
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950121BA57@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E950121BA57@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <1720F4E3-80F5-11D9-ADA4-000D933565E8@mail.nih.gov>

Mick,

Does this do it?

mymat <- do.call('rbind',l2)
rownames(mymat) <- names(l2)

Sean

On Feb 17, 2005, at 9:36 AM, michael watson ((IAH-C)) wrote:

> Hi
>
> We have touched on this before, but I don't think I quite got it right.
>
> So I have a list, each element of which is a a vector of 2 numbers:
>
>> l2
> $cat000_a01
> [1] 0.3429944 4.5138244
>
> $cat000_a02
> [1] 0.1929336 4.3064944
>
> $cat000_a03
> [1] -0.2607796  4.1551591
>
> What I actually want to convert this into is a matrix with the names
> (cat000_a01 etc) as row names, the first element of each of the vectors
> forming the first column of the new matrix, and the second element of
> each of the vectors forming the second column:
>
> cat000_a01	0.3429944	4.5138244
> cat000_a02	0.1929336	4.3064944
> cat000_a03	-0.2607796  4.1551591
>
> What was suggested on the list last time was
> matrix(unlist(mylist),nrow=length(mylist)).  But if I do this I get:
>
>> matrix(unlist(l2),nrow=length(l2))
>           [,1]       [,2]
> [1,] 0.3429944  4.3064944
> [2,] 4.5138244 -0.2607796
> [3,] 0.1929336  4.1551591
>
> Which is not what I want.  Here, the second element of the first vector
> in my list has gone into the first column of the new matrix, and that's
> not what I want at all.
>
> Any more help would be appreciated.
>
> Thanks
> Mick
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From rvaradha at jhsph.edu  Thu Feb 17 16:04:52 2005
From: rvaradha at jhsph.edu (Ravi Varadhan)
Date: Thu, 17 Feb 2005 10:04:52 -0500
Subject: [R] Converting a list to a matrix - I still don't think I have
	itright
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950121BA57@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <OWA-1gWRjfXr6nbb3MO00002600@owa-1.sph.ad.jhsph.edu>

This should do it:

matrix(unlist(mylist),nrow=length(mylist), by=T)


--------------------------------------------------------------------------
Ravi Varadhan, Ph.D.
Assistant Professor,  The Center on Aging and Health
Division of Geriatric Medicine and Gerontology
Johns Hopkins University
Ph: (410) 502-2619
Fax: (410) 614-9625
Email:  rvaradhan at jhmi.edu
--------------------------------------------------------------------------
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of michael watson (IAH-C)
> Sent: Thursday, February 17, 2005 9:37 AM
> To: r-help at r-project.org
> Subject: [R] Converting a list to a matrix - I still don't think I have
> itright
> 
> Hi
> 
> We have touched on this before, but I don't think I quite got it right.
> 
> So I have a list, each element of which is a a vector of 2 numbers:
> 
> > l2
> $cat000_a01
> [1] 0.3429944 4.5138244
> 
> $cat000_a02
> [1] 0.1929336 4.3064944
> 
> $cat000_a03
> [1] -0.2607796  4.1551591
> 
> What I actually want to convert this into is a matrix with the names
> (cat000_a01 etc) as row names, the first element of each of the vectors
> forming the first column of the new matrix, and the second element of
> each of the vectors forming the second column:
> 
> cat000_a01	0.3429944	4.5138244
> cat000_a02	0.1929336	4.3064944
> cat000_a03	-0.2607796  4.1551591
> 
> What was suggested on the list last time was
> matrix(unlist(mylist),nrow=length(mylist)).  But if I do this I get:
> 
> > matrix(unlist(l2),nrow=length(l2))
>           [,1]       [,2]
> [1,] 0.3429944  4.3064944
> [2,] 4.5138244 -0.2607796
> [3,] 0.1929336  4.1551591
> 
> Which is not what I want.  Here, the second element of the first vector
> in my list has gone into the first column of the new matrix, and that's
> not what I want at all.
> 
> Any more help would be appreciated.
> 
> Thanks
> Mick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html



From dimitris.rizopoulos at med.kuleuven.ac.be  Thu Feb 17 16:08:54 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Thu, 17 Feb 2005 16:08:54 +0100
Subject: [R] Converting a list to a matrix - I still don't think I have
	itright
References: <8975119BCD0AC5419D61A9CF1A923E950121BA57@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <005201c51502$98813690$0540210a@www.domain>

mat <- matrix(unlist(l2), nrow=length(l2), byrow=TRUE)
rownames(mat) <- names(l2)
mat

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "michael watson (IAH-C)" <michael.watson at bbsrc.ac.uk>
To: <r-help at r-project.org>
Sent: Thursday, February 17, 2005 3:36 PM
Subject: [R] Converting a list to a matrix - I still don't think I 
have itright


> Hi
>
> We have touched on this before, but I don't think I quite got it 
> right.
>
> So I have a list, each element of which is a a vector of 2 numbers:
>
>> l2
> $cat000_a01
> [1] 0.3429944 4.5138244
>
> $cat000_a02
> [1] 0.1929336 4.3064944
>
> $cat000_a03
> [1] -0.2607796  4.1551591
>
> What I actually want to convert this into is a matrix with the names
> (cat000_a01 etc) as row names, the first element of each of the 
> vectors
> forming the first column of the new matrix, and the second element 
> of
> each of the vectors forming the second column:
>
> cat000_a01 0.3429944 4.5138244
> cat000_a02 0.1929336 4.3064944
> cat000_a03 -0.2607796  4.1551591
>
> What was suggested on the list last time was
> matrix(unlist(mylist),nrow=length(mylist)).  But if I do this I get:
>
>> matrix(unlist(l2),nrow=length(l2))
>          [,1]       [,2]
> [1,] 0.3429944  4.3064944
> [2,] 4.5138244 -0.2607796
> [3,] 0.1929336  4.1551591
>
> Which is not what I want.  Here, the second element of the first 
> vector
> in my list has gone into the first column of the new matrix, and 
> that's
> not what I want at all.
>
> Any more help would be appreciated.
>
> Thanks
> Mick
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ligges at statistik.uni-dortmund.de  Thu Feb 17 16:19:40 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 17 Feb 2005 16:19:40 +0100
Subject: [R] Easy cut & paste from Excel to R?
In-Reply-To: <1108648485.4214a22601b1d@webmail.lyon.inserm.fr>
References: <1108648485.4214a22601b1d@webmail.lyon.inserm.fr>
Message-ID: <4214B60C.1020409@statistik.uni-dortmund.de>

Ken Knoblauch wrote:

> Hi,
> 
> I tried the interesting suggestion below, discussed in several postings 
> yesterday on the help-list, on my Mac (0S 10.3.7) but could not get it to
> work, as shown in the tests indicated below.
> 
> 
>>>  read.table(file("clipboard"), sep="\t", dec=",")
>>>

Connections to the clipboard are only available on Windows.

Uwe Ligges


> 
> If it is obvious (even, if not), can someone tell me what I am doing wrong?  
> Do I need to perform an additional operation to open "clipboard", or is this 
> option not available on Mac? I did not find any special discussion of this in
> the FAQ nor searching under "clipboard" in the archives.  Thank you, in 
> advance, for any enlightenment.
> 
> data.frame(matrix(rnorm(12),ncol=3))
>           X1          X2          X3
> 1  0.4276964 -0.49584891  0.02150469
> 2 -0.8323586 -0.40120649 -1.90733346
> 3 -0.8954563 -1.33195844 -1.28261484
> 4  0.4772382 -0.03703087  0.46719156
> #At this point, I block-marked the printed output and apple-C'd it into the 
> clipboard.
> #I then checked the clipboard to verify that the data was indeed copied there.
> 
>>read.table("clipboard")
> 
> Error in file(file, "r") : unable to open connection
> In addition: Warning message: 
> cannot open file `clipboard' 
> 
>>read.table(file("clipboard"))
> 
> Error in open.connection(file, "r") : unable to open connection
> In addition: Warning message: 
> cannot open file `clipboard' 
> 
>>read.table(file("clipboard","r"))
> 
> Error in file("clipboard", "r") : unable to open connection
> In addition: Warning message: 
> cannot open file `clipboard' 
> 
>>read.table(file("clipboard","r"),header=TRUE)
> 
> Error in file("clipboard", "r") : unable to open connection
> In addition: Warning message: 
> cannot open file `clipboard' 
> 
>>read.delim(file("clipboard","r"),header=TRUE)
> 
> Error in file("clipboard", "r") : unable to open connection
> In addition: Warning message: 
> cannot open file `clipboard' 
> 
> 
>>file("clipboard")
> 
> description       class        mode        text      opened    can read 
> "clipboard"      "file"         "r"      "text"    "closed"       "yes" 
>   can write 
>       "yes" 
> 
> 
>>file("clipboard","r")
> 
> Error in file("clipboard", "r") : unable to open connection
> In addition: Warning message: 
> cannot open file `clipboard' 
> 
> platform powerpc-apple-darwin6.8
> arch     powerpc                
> os       darwin6.8              
> system   powerpc, darwin6.8     
> status                          
> major    2                      
> minor    0.1                    
> year     2004                   
> month    11                     
> day      15                     
> language R   
> 
> ____________________
> Ken Knoblauch
> Inserm U 371
> Cerveau et Vision
> 18 avenue du Doyen Lepine
> 69675 Bron cedex
> France
> tel: +33 (0)4 72 91 34 77
> fax: +33 (0)4 72 91 34 61
> portable: 06 84 10 64 10
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From rpeng at jhsph.edu  Thu Feb 17 16:23:41 2005
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Thu, 17 Feb 2005 10:23:41 -0500
Subject: [R] Converting a list to a matrix - I still don't think I have
	it	right
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950121BA57@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E950121BA57@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <4214B6FD.8050609@jhsph.edu>

Does

do.call("rbind", l2)

do what you want?

-roger

michael watson (IAH-C) wrote:
> Hi
> 
> We have touched on this before, but I don't think I quite got it right.
> 
> So I have a list, each element of which is a a vector of 2 numbers:
> 
> 
>>l2
> 
> $cat000_a01
> [1] 0.3429944 4.5138244
> 
> $cat000_a02
> [1] 0.1929336 4.3064944
> 
> $cat000_a03
> [1] -0.2607796  4.1551591
> 
> What I actually want to convert this into is a matrix with the names
> (cat000_a01 etc) as row names, the first element of each of the vectors
> forming the first column of the new matrix, and the second element of
> each of the vectors forming the second column:
> 
> cat000_a01	0.3429944	4.5138244
> cat000_a02	0.1929336	4.3064944
> cat000_a03	-0.2607796  4.1551591
> 
> What was suggested on the list last time was
> matrix(unlist(mylist),nrow=length(mylist)).  But if I do this I get:
> 
> 
>>matrix(unlist(l2),nrow=length(l2))
> 
>           [,1]       [,2]
> [1,] 0.3429944  4.3064944
> [2,] 4.5138244 -0.2607796
> [3,] 0.1929336  4.1551591
> 
> Which is not what I want.  Here, the second element of the first vector
> in my list has gone into the first column of the new matrix, and that's
> not what I want at all.
> 
> Any more help would be appreciated.
> 
> Thanks
> Mick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From jlh599 at psu.edu  Thu Feb 17 16:26:34 2005
From: jlh599 at psu.edu (Jessica Higgs)
Date: Thu, 17 Feb 2005 10:26:34 -0500
Subject: [R] eigen vector question
Message-ID: <5.2.0.9.2.20050217095954.02185730@email.psu.edu>

Sorry to bother everyone, but I've looked in all of the help files and 
manuals I have and I can't find the answer to this question.  I'm doing 
principle component analysis by calculating the eigen vectors of a 
correlation matrix that I have that is composed of 21 parameters.  I have 
the eigen vectors and their values that R produced for me but I'm not sure 
how to tell which eigen vector/value corresponds to which parameter because 
when R produces eigen vectors it does so in decreasing order of 
significance, meaning that the eigen vector that explains the most of the 
variance is listed first, followed by the next eigen vector, etc etc. Any 
help would be appreciated. Feel free to write back if you need more 
information on my problem.  Thanks!



From petr.pikal at precheza.cz  Thu Feb 17 16:28:28 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 17 Feb 2005 16:28:28 +0100
Subject: [R] Converting a list to a matrix - I still don't think I have
	it	right
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950121BA57@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <4214C62C.20262.1DC86A9@localhost>

Hi Michael

On 17 Feb 2005 at 14:36, michael watson (IAH-C) wrote:

> Hi
> 
> We have touched on this before, but I don't think I quite got it
> right.
> 
> So I have a list, each element of which is a a vector of 2 numbers:
> 
> > l2
> $cat000_a01
> [1] 0.3429944 4.5138244
> 
> $cat000_a02
> [1] 0.1929336 4.3064944
> 
> $cat000_a03
> [1] -0.2607796  4.1551591
> 
> What I actually want to convert this into is a matrix with the names
> (cat000_a01 etc) as row names, the first element of each of the
> vectors forming the first column of the new matrix, and the second
> element of each of the vectors forming the second column:
> 
> cat000_a01	0.3429944	4.5138244
> cat000_a02	0.1929336	4.3064944
> cat000_a03	-0.2607796  4.1551591
> 
> What was suggested on the list last time was
> matrix(unlist(mylist),nrow=length(mylist)).  But if I do this I get:
> 
> > matrix(unlist(l2),nrow=length(l2))
>           [,1]       [,2]
> [1,] 0.3429944  4.3064944
> [2,] 4.5138244 -0.2607796
> [3,] 0.1929336  4.1551591

Try byrow =TRUE argument

> x<-list(a=c(1,2), b=c(4,5), d= c(10,12))

> matrix(unlist(x),nrow=length(x), byrow=T)
     [,1] [,2]
[1,]    1    2
[2,]    4    5
[3,]   10   12


Cheers 
Petr




> Which is not what I want.  Here, the second element of the first
> vector in my list has gone into the first column of the new matrix,
> and that's not what I want at all.
> 
> Any more help would be appreciated.
> 
> Thanks
> Mick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From p.dalgaard at biostat.ku.dk  Thu Feb 17 16:36:12 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 17 Feb 2005 16:36:12 +0100
Subject: [R] Again: Variable names in functions
In-Reply-To: <3.0.6.32.20050217160349.007b4370@pop.gmx.net>
References: <3.0.6.32.20050217113007.007b0860@pop.gmx.net>
	<3.0.6.32.20050217113007.007b0860@pop.gmx.net>
	<3.0.6.32.20050217160349.007b4370@pop.gmx.net>
Message-ID: <x2psyzurkj.fsf@biostat.ku.dk>

Heinz Tuechler <tuechler at gmx.at> writes:

> 
> Thank you, this method works well. One step further I am again using
> parse(), but maybe there is a better solution for that situation too.
> The example would be a function, where I pass the variable name as string
> instead of the name. The motivation for this is that it seems easier to
> handle if I want to pass several variables (i.e. a vector of variable
> names) to the function (as I learned recently from this help-list). 
> In this case I have to use get(). In the case of calling table() the
> variable name disappeares.
> 
> > alpha<-c(rep(1:5,10))
> > name.alpha<-"alpha"
> > mytable1<-function(x){print(table(get(x)))}
> > mytable1(name.alpha)
> 
>  1  2  3  4  5 
> 10 10 10 10 10 
> 
> If I use eval(parse()) instead, it works as expected. I tried several
> combinations of eval() and substitute() but I did not find a solution.
> Is there a similar "trick"?
> 
> > mytable2<-function(x){
> +   string<-paste("print(table(",as.symbol(x),"))")
> +   eval(parse(text=string))}
> > mytable2(name.alpha)
> alpha
>  1  2  3  4  5 
> 10 10 10 10 10 

You're almost there: 

eval(substitute(table(e),list(e=as.symbol(x))))

(or eval.parent, which() safeguards somewhat against scoping issues)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From pkhomski at wiwi.uni-bielefeld.de  Thu Feb 17 16:41:48 2005
From: pkhomski at wiwi.uni-bielefeld.de (Pavel Khomski)
Date: Thu, 17 Feb 2005 16:41:48 +0100
Subject: [R] lme4--->GLMM
Message-ID: <4214BB3C.2030005@wiwi.uni-bielefeld.de>

Hello,

I'm very sorry for my repeated question, which i  asked 2 weeks ago, namely:

i'm interested in possibly simple random-part specification in the call 
of GLMM(...)   (from lme4-package)
i have a random blocked structure (i.e. ~var.a1+var.a2+var.a3, 
~var.b1+var.b2,~var.c1+var.c2+var.c3+var.c4),
and each one part of it i would like to model as Identity-structure 
matrix. So i had, in symbols of nlme-package,
and for only one cluster-variable my.Subject:

random=list(my.Subject=pdBlocked(list(pdIdent(~var.a1+var.a2+var.a3,...),pdIdent(~var.b1+var.b2,...),pdIdent(~var.c1+var.c2+var.c3+var.c4),...)))

As the lme4-package doesn't use the pdMat-classes for specification of 
the random-part in GLMM,    i used,  with advice of Douglas Bates,
the explicit specification in the GLMM-call (this call can also been 
simplified, if i first attach the lme4 package and then the nlme-package):

GLMM(....., random=list(my.Subject=~var.a1+var.a2+var.a3, 
my.Subject=~var.b1+var.b2, my.Subject=var.c1+var.c2+var.c3+var.c4),.....)

and really could make estimates in such a way.

The problem is how can i specify  a simple matrix sructure for each  
block, such as pdIdent(...) in symbols of lme4. is it possible? 
Does the lme4 any use of pdMat-specification or something like this? If 
not, it seems, that i actually specify a general Covariance Matrix
for each block, what i in principle don't want.

thanks in advance



From p.dalgaard at biostat.ku.dk  Thu Feb 17 16:39:39 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 17 Feb 2005 16:39:39 +0100
Subject: [R] Converting a list to a matrix - I still don't think I have it
	right
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950121BA57@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E950121BA57@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <x2ll9nures.fsf@biostat.ku.dk>

"michael watson (IAH-C)" <michael.watson at bbsrc.ac.uk> writes:

> Hi
> 
> We have touched on this before, but I don't think I quite got it right.
> 
> So I have a list, each element of which is a a vector of 2 numbers:
> 
> > l2
> $cat000_a01
> [1] 0.3429944 4.5138244
> 
> $cat000_a02
> [1] 0.1929336 4.3064944
> 
> $cat000_a03
> [1] -0.2607796  4.1551591
> 
> What I actually want to convert this into is a matrix with the names
> (cat000_a01 etc) as row names, the first element of each of the vectors
> forming the first column of the new matrix, and the second element of
> each of the vectors forming the second column:
> 
> cat000_a01	0.3429944	4.5138244
> cat000_a02	0.1929336	4.3064944
> cat000_a03	-0.2607796  4.1551591

do.call("rbind", list)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From president at usa-il.com  Thu Feb 17 17:11:46 2005
From: president at usa-il.com (Bjorn Martinoff)
Date: Thu, 17 Feb 2005 11:11:46 -0500 (EST)
Subject: [R] Why good employees leave...and how to keep them.
Message-ID: <1100497445401.1011269667070.31345.0.391046@scheduler>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050217/7e0f6b1f/attachment.pl

From ligges at statistik.uni-dortmund.de  Thu Feb 17 16:57:52 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 17 Feb 2005 16:57:52 +0100
Subject: [R] short plots: lwd, margin and postscript behavior
In-Reply-To: <001c01c514fb$cc6009c0$9ba76f82@CFRU0104>
References: <20050217133613.61151.qmail@web14521.mail.yahoo.com>
	<001c01c514fb$cc6009c0$9ba76f82@CFRU0104>
Message-ID: <4214BF00.9000504@statistik.uni-dortmund.de>

Mike Saunders wrote:

> Don't feel alone C?zar; I have had the same problems getting par 
> commands to work when making *.pdf files.  I also run R 2.0.1 and use 
> par() after opening the device.  Sometimes, the par() commands work; 
> sometimes it opens a new windows device.  Maybe something with MS 
> Windows is causing this.

Which version of Windows (I assume NT, 2k or XP; each with latest 
Service Pack), which version of R (I assume R-2.0.1)?

Please start R with --vanilla and tell us exactly the sequence of calls 
until this happens, if it is reproducible in any way...

Uwe Ligges



> Mike
> 
> Mike Saunders
> Research Assistant
> Forest Ecosystem Research Program
> Department of Forest Ecosystem Sciences
> University of Maine
> Orono, ME  04469
> 207-581-2763 (O)
> 207-581-4257 (F)
> 
> ----- Original Message ----- From: "C?zar Freitas" 
> <cafanselmo12 at yahoo.com.br>
> To: "Uwe Ligges" <ligges at statistik.uni-dortmund.de>; 
> <R-help at stat.math.ethz.ch>
> Sent: Thursday, February 17, 2005 8:36 AM
> Subject: Re: [R] short plots: lwd, margin and postscript behavior
> 
> 
> Thank you so much, but the commands in par doesn't
> affect the pictures generated by postscript.
> For example, I put lwd=3, and the postscript file is
> the same, using par(lwd=3) or par(lwd=.5)...
> 
> --- Uwe Ligges <ligges at statistik.uni-dortmund.de>
> escreveu:
> 
>> C?zar Freitas wrote:
>>
>> > I tested in R versions 1.8.1 and 2, but doesn't
>> works.
>> > The attached plots can explain this. And mar isn't
>> a
>> > parameter to postscript command. If I use in par,
>> it
>> > doesn't affect the outpu.
>>
>> It does in R-2.0.1!!! Please try out what poeple are
>> suggesting!
>> You have to set par() for the current device, so
>> after your call to
>> postscript, as I have indicated below.
>>
>> Uwe Ligges
>>
>>
>>
>>
>> >
>> > Thanks,
>> > C.
>> >
>> >  --- Uwe Ligges <ligges at statistik.uni-dortmund.de>
>> > escreveu:
>> >
>> >>C?zar Freitas wrote:
>> >>
>> >>
>> >>>Hi all.
>> >>>I'm working with a short plot (3x3 inches), but
>> >>
>> >>the
>> >>
>> >>>results (via postscript command) are not nice.
>> The
>> >>
>> >>lwd
>> >>
>> >>>command don't affect the lines (that are very
>> >>
>> >>large)
>> >>
>> >>>and the margins don't change using oma, mai, mar,
>> >>
>> >>...
>> >>
>> >>>Below I put an example. Moreover, save the
>> >>
>> >>graphics
>> >>
>> >>>via postscript command isn't working well (see
>> the
>> >>>attached ps).
>> >>>Thanks by the help,
>> >>
>> >>
>> >>Setting lwd=0.5 works for me, as well as using
>> >>par(mar=c(....))
>> >>
>> >>
>> >>
>> >>
>> >>>Cezar Freitas.
>> >>>
>> >>>#Example:
>> >>>#data
>> >>>  scores<-c(2.0, 0.0, 5.0, 5.0, 5.0, 2.0, 0.0,
>> >>
>> >>5.0,
>> >>
>> >>>2.5, 4.0, 5.0, 0.0, 5.0, 0.0, 2.0, 5.0, 5.0, 2.0,
>> >>
>> >>3.0,
>> >>
>> >>>3.0)
>> >>>  q<-summary(scores)
>> >>>  gra<-hist(scores, breaks=((0:11)/2-.2),
>> >>
>> >>plot=FALSE)
>> >>
>> >>>  yy<-ceiling(max(gra$counts)/10)*10
>> >>>  yz<-yy/12
>> >>>
>> >>>#plot via postscript
>> >>>  postscript("test.ps", width=3, height=3,
>> >>>horizontal=FALSE, family="Times",
>> paper="special")
>> >>
>> >>par(mar=rep(1,4)+.1, lwd=0.5)
>> >>
>> >>
>> >>
>> >>>  hist(scores, breaks=((0:11)/2-.2),
>> xlim=c(-1,6),
>> >>>ylim=c(-yz,yy), main="scores", ylab="Freq",
>> >>>xlab="math", cex.axis=.3, cex.main=.3,
>> cex.sub=.3,
>> >>>cex.lab=.3, mgp=c(1.5,.5,0))
>> >>
>> >>add lwd=0.5
>> >>
>> >>Uwe Ligges
>> >>
>> >>
>> >>>  boxplot(scores, horizontal=1, add=1,
>> at=-2*yz/3,
>> >>>boxwex=1.5*yz, bty="n", axes=FALSE)
>> >>>  points(q[4], -2*yz/3, col=1, pch="+", cex=.5)
>> >>>
>> >>>  dev.off()
>> >>>
>> >>>
>> >>>
>> >>>
>> >>>
>> >>>
>> >>>
>> >>>
>> >>
>> >
>>
> _______________________________________________________
> 
>> >
>> >>>
>> >>>
>> >
>>
> ------------------------------------------------------------------------
> 
>> >
>> >>>______________________________________________
>> >>>R-help at stat.math.ethz.ch mailing list
>> >>>https://stat.ethz.ch/mailman/listinfo/r-help
>> >>>PLEASE do read the posting guide!
>> >>
>> >>http://www.R-project.org/posting-guide.html
>> >>
>> >>
>> >
>> >
>> >
>> >
>> >
>> >
>> >
>>
> _______________________________________________________
> 
>>
> 
> 
>> r?pida e gr?tis
>>
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From vito_ricci at yahoo.com  Thu Feb 17 16:58:18 2005
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Thu, 17 Feb 2005 16:58:18 +0100 (CET)
Subject: [R] Fitting distributions
Message-ID: <20050217155818.70756.qmail@web41210.mail.yahoo.com>

Dear UseRs,

I'm glad to inform that an English version of my
contribute concerning fitting distributions is now
available on CRAN:

http://cran.r-project.org/doc/contrib/Ricci-distributions-en.pdf

Any comments will be appreciated.

Best regards,
Vito

=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box

Top 10 reasons to become a Statistician

     1. Deviation is considered normal
     2. We feel complete and sufficient
     3. We are 'mean' lovers
     4. Statisticians do it discretely and continuously
     5. We are right 95% of the time
     6. We can legally comment on someone's posterior distribution
     7. We may not be normal, but we are transformable
     8. We never have to say we are certain
     9. We are honestly significantly different
    10. No one wants our jobs


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/



From buser at stat.math.ethz.ch  Thu Feb 17 17:31:10 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Thu, 17 Feb 2005 17:31:10 +0100
Subject: [R] problem with se.contrast()
In-Reply-To: <42139BF7.2090809@stat.ufl.edu>
References: <42139BF7.2090809@stat.ufl.edu>
Message-ID: <16916.50894.173426.42673@stat.math.ethz.ch>

Dear Jamie

As Prof. Ripley explained your analysis is equivalent to the fixed effect
models for the means, so you can calculate it by (if this is your design): 

> Lab <- factor(rep(c("1","2","3"),each=12))
> Material <- factor(rep(c("A","B","C","D"),each=3,times=3))
> Measurement <- c(12.20,12.28,12.16,15.51,15.02,15.29,18.14,18.08,18.21,
>                  18.54,18.36,18.45,12.59,12.30,12.67,14.98,15.46,15.22,
>                  18.54,18.31,18.60,19.21,18.77,18.69,12.72,12.78,12.66,
>                  15.33,15.19,15.24,18.00,18.15,17.93,18.88,18.12,18.03)
> testdata <- data.frame(Lab,Material,Measurement)
> rm(list=c("Lab","Material","Measurement"))
 
You can aggregate your data

> dat.mean <- aggregate(testdata$Measurement,
>                       by = list(Material=testdata$Material,Lab=testdata$Lab),
>                       FUN = mean)
> names(dat.mean)[3] <- "Measurement"

> test.red.aov1 <- aov(Measurement ~ Lab + Material, data = dat.mean)
> se.contrast(test.red.aov1,
>             list(Material=="A",Material=="B",Material=="C",Material=="D"),
>             coef=c(0.5,0.5,-0.5,-0.5),dat.mean)
> [1] 0.1220339

By aggregating the data you bypass the problem in se.contrast and you do
not need R-devel.

-----------------------------------------------------------------------------

The second way to get the same is to set your contrast for the factor
"Material" and calculate you model with this contrast and use summary.lm: 

> dat.mean$Material <- C(dat.mean$Material, c(-0.5,-0.5,0.5,0.5), 
>                        how.many = 3) 
> test.red.aov2 <- aov(Measurement ~ Lab + Material, data = dat.mean)
> summary.lm(test.red.aov2)

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 16.02000    0.10568 151.583 5.56e-12 ***
Lab2         0.25833    0.14946   1.728    0.135    
Lab3         0.06583    0.14946   0.440    0.675    
Material1    4.52278    0.12203  37.062 2.58e-08 ***
Material2    1.21056    0.12203   9.920 6.06e-05 ***
Material3    1.55389    0.12203  12.733 1.44e-05 ***
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 

Material1 is now the contrast you are interested in and you get beside the
Std. Error the Estimate, too. Material2 and Material3 are just orthogonal
contrasts to complete. 

-----------------------------------------------------------------------------

The third way (which I prefer) is using lme from the package nlme

> library(nlme)

Here I use the origianl data set (not aggregated) and set the desired
contrast, too.

> testdata$Material <- C(testdata$Material, c(-0.5,-0.5,0.5,0.5), 
>                        how.many = 3) 
> test.lme <- lme(fixed = Measurement ~ Material, data = testdata, 
>                 random = ~1|Lab/Material)

With anova you get the F-Test for the fixed factor "Material"

> anova(test.lme)
>           numDF denDF  F-value p-value
  (Intercept)     1    24 43301.14  <.0001
  Material        3     6   544.71  <.0001

and with the summary you have your contrast with standard error: 

> summary(test.lme)
Fixed effects: Measurement ~ Material 
                Value  Std.Error DF   t-value p-value
(Intercept) 16.128056 0.07750547 24 208.08925   0e+00
Material1    4.522778 0.12203325  6  37.06185   0e+00
Material2    1.210556 0.12203325  6   9.91988   1e-04
Material3    1.553889 0.12203325  6  12.73332   0e+00

-----------------------------------------------------------------------------

Last but not least I tried it with R-devel and the original data frame:
First I reset the contrast on the default value:

testdata$Material <- C(testdata$Material, "contr.treatment", how.many = 3) 

I used your syntax and se.contrast():

> test.aov <- with(testdata,aov(Measurement ~ Material + Error(Lab/Material)))
> se.contrast(test.aov,
>             list(Material=="A",Material=="B",Material=="C",Material=="D"),
>             coef=c(0.5,0.5,-0.5,-0.5),data=testdata)
> [1] 0.1432572

I got a different result and I have admit that I didn't understand why
there is a differnce between the lme model and this one. There are some
comments in the help pages but I'm not sure if this is the answer.

-----------------------------------------------------------------------------

I hope some of the code above can help to analyse your data. Maybe Prof.
Ripley was right and you have another design. Then you just can ignore this 
and your life is much more easier :-)

Best regards,

Christoph Buser


--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C11
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-1-632-5414		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------



Jamie Jarabek writes:
 > I am having trouble getting standard errors for contrasts using se.contrast() in 
 > what appears to be a simple case to me. The following test example illustrates 
 > my problem:
 > 
 > Lab <- factor(rep(c("1","2","3"),each=12))
 > Material <- factor(rep(c("A","B","C","D"),each=3,times=3))
 > Measurement <- c(12.20,12.28,12.16,15.51,15.02,15.29,18.14,18.08,18.21,18.54,18.36
 > ,18.45,12.59,12.30,12.67,14.98,15.46,15.22,18.54,18.31,18.60,19.21,18.77
 > ,18.69,12.72,12.78,12.66,15.33,15.19,15.24,18.00,18.15,17.93,18.88,18.12,18.03)
 > 
 > testdata <- data.frame(Lab,Material,Measurement)
 > rm(list=c("Lab","Material","Measurement"))
 > 
 > test.aov <- with(testdata,aov(Measurement ~ Material + Error(Lab/Material)))
 > 
 > This gives me the desired ANOVA table. I next want to get the standard
 > errors for certain contrasts and following the help page for
 > se.contrast() I tried the following but I get an error:
 > 
 > >se.contrast(test.aov,list(Material=="A",Material=="B",Material=="C",Material=="D"),coef=c(1,1,-1,-1),data=testdata)
 > Error in matrix(0, length(asgn), ncol(effects), dimnames = list(nm[1 + :
 >          length of dimnames [1] not equal to array extent
 > 
 > I have tested this on R 2.0.1 on Windows XP and Solaris and get the same
 > error on both systems. I am unsure as to what I am doing wrong here. Thanks for 
 > any help.
 > 
 > Jamie Jarabek
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From gunter.berton at gene.com  Thu Feb 17 17:32:24 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 17 Feb 2005 08:32:24 -0800
Subject: [R] Again: Variable names in functions
In-Reply-To: <3.0.6.32.20050217160349.007b4370@pop.gmx.net>
Message-ID: <200502171632.j1HGWO2T006698@faraday.gene.com>


I thought Thomas L. was clear, but apparently not...

** Do not pass character string names as arguments to functions. ** Pass the
objects (or expressions) which can consist of lists of vectors, dataframes,
etc. instead. 

If you need the names (e.g. as labels) you can use the deparse(substitute())
construction. I strongly recommend that you study pp. 44-46 and section 3.5
("Computing on the Language") of V&R's S PROGRAMMING. The point is that one
can, of course, do things the way you want, but it makes life unnecessarily
difficult and complex because R is set up to pass arguments by value and can
keep better track of proper evaluation environments when this is done (which
means you don't have to).

-- Bert Gunter

> >
> 
> Thank you, this method works well. One step further I am again using
> parse(), but maybe there is a better solution for that situation too.
> The example would be a function, where I pass the variable 
> name as string
> instead of the name. The motivation for this is that it seems 
> easier to
> handle if I want to pass several variables (i.e. a vector of variable
> names) to the function (as I learned recently from this help-list). 
> In this case I have to use get(). In the case of calling table() the
> variable name disappeares.
> 
> > alpha<-c(rep(1:5,10))
> > name.alpha<-"alpha"
> > mytable1<-function(x){print(table(get(x)))}
> > mytable1(name.alpha)
> 
>  1  2  3  4  5 
> 10 10 10 10 10 
> 
> If I use eval(parse()) instead, it works as expected. I tried several
> combinations of eval() and substitute() but I did not find a solution.
> Is there a similar "trick"?
> 
> > mytable2<-function(x){
> +   string<-paste("print(table(",as.symbol(x),"))")
> +   eval(parse(text=string))}
> > mytable2(name.alpha)
> alpha
>  1  2  3  4  5 
> 10 10 10 10 10 
> 
> 
> Thanks,
> Heinz T?chler
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ramasamy at cancer.org.uk  Thu Feb 17 17:58:04 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Thu, 17 Feb 2005 16:58:04 +0000
Subject: [R] Converting a list to a matrix - I still don't think I have
	itright
In-Reply-To: <005201c51502$98813690$0540210a@www.domain>
References: <8975119BCD0AC5419D61A9CF1A923E950121BA57@iahce2knas1.iah.bbsrc.reserved>
	<005201c51502$98813690$0540210a@www.domain>
Message-ID: <1108659484.5921.31.camel@ndmpc126.orc.ox.ac.uk>

Please try to use something other than "l2" for showing an example
because it is looks awfully similar to the number "12".

Here is another of doing this but this will only work if you have equal
lengths in each of your elements.

# simulate data
mylist <- lapply( 1:5, rnorm, n=2 )
names(mylist) <- LETTERS[1:5]

t( sapply(mylist, "c") )
      [,1]      [,2]
A 1.908977 0.1801167
B 2.384840 3.3355432
C 4.463627 3.5321393
D 2.952132 5.7028436
E 3.891464 3.6214637

If your vectors are of unequal lengths, then you would have to think of
something clever.

Regards, Adai


On Thu, 2005-02-17 at 16:08 +0100, Dimitris Rizopoulos wrote:
> mat <- matrix(unlist(l2), nrow=length(l2), byrow=TRUE)
> rownames(mat) <- names(l2)
> mat
> 
> Best,
> Dimitris
> 
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
> 
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/16/336899
> Fax: +32/16/337015
> Web: http://www.med.kuleuven.ac.be/biostat/
>      http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
> 
> 
> ----- Original Message ----- 
> From: "michael watson (IAH-C)" <michael.watson at bbsrc.ac.uk>
> To: <r-help at r-project.org>
> Sent: Thursday, February 17, 2005 3:36 PM
> Subject: [R] Converting a list to a matrix - I still don't think I 
> have itright
> 
> 
> > Hi
> >
> > We have touched on this before, but I don't think I quite got it 
> > right.
> >
> > So I have a list, each element of which is a a vector of 2 numbers:
> >
> >> l2
> > $cat000_a01
> > [1] 0.3429944 4.5138244
> >
> > $cat000_a02
> > [1] 0.1929336 4.3064944
> >
> > $cat000_a03
> > [1] -0.2607796  4.1551591
> >
> > What I actually want to convert this into is a matrix with the names
> > (cat000_a01 etc) as row names, the first element of each of the 
> > vectors
> > forming the first column of the new matrix, and the second element 
> > of
> > each of the vectors forming the second column:
> >
> > cat000_a01 0.3429944 4.5138244
> > cat000_a02 0.1929336 4.3064944
> > cat000_a03 -0.2607796  4.1551591
> >
> > What was suggested on the list last time was
> > matrix(unlist(mylist),nrow=length(mylist)).  But if I do this I get:
> >
> >> matrix(unlist(l2),nrow=length(l2))
> >          [,1]       [,2]
> > [1,] 0.3429944  4.3064944
> > [2,] 4.5138244 -0.2607796
> > [3,] 0.1929336  4.1551591
> >
> > Which is not what I want.  Here, the second element of the first 
> > vector
> > in my list has gone into the first column of the new matrix, and 
> > that's
> > not what I want at all.
> >
> > Any more help would be appreciated.
> >
> > Thanks
> > Mick
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From TBrough at russell.com  Thu Feb 17 18:14:02 2005
From: TBrough at russell.com (Brough, Tyler (FRS))
Date: Thu, 17 Feb 2005 09:14:02 -0800
Subject: [R] Creating a new factor from other factors and a date range
Message-ID: <F03AB0AFC2B70D46B57F8522656513AF0195F054@tac-nt-exch8.russell.com>

Hello useRs,

I'm using version 2.0.1 on Windows XP.  I have a data.frame with 3 factors
and a date. The data.frame is sorted by the 3 factors and by date.  I would
like to create a new factor designating membership in a group.  Each group
is defined as having the same factor values and dates that are less than
some number (e.g. 10) days apart.  Does anyone have any suggestions as to
how to accomplish this task without looping over my data?  I would like to
be able to possibly use more than three factors and any other number of
days. 

The following replicates my data:

set.seed(5)
my.df <- data.frame(f1 = as.factor(sample(1000:1010,size=100,replace=T)),
	f2 = as.factor(sample(c("A","B"),size=100,replace=T)),
	f3 = as.factor(sample(c(T,F),size=100,replace=T)),
	td = sample(seq(ISOdate(2004,1,1),ISOdate(2004,2,29),by="days"),
		 size = 100, replace = T) )

my.df <- my.df[order(my.df[,1],my.df[,2],my.df[,3],my.df[,4]), ]

So, for example looking at the first 5 rows of my.df gives:

> my.df[1:5,]
     f1 f2    f3                  td
35 1000  A FALSE 2004-01-28 12:00:00
77 1000  A FALSE 2004-02-16 12:00:00
32 1000  A  TRUE 2004-01-02 12:00:00
75 1000  A  TRUE 2004-01-22 12:00:00
86 1000  A  TRUE 2004-01-29 12:00:00


The first row would be assigned to group 1, row 2 to group 2, row 3 to group
3, and the next two rows to group 4 (using a 10 day date range).

Thank you in advance for your suggestions.

-Tyler



From ripley at stats.ox.ac.uk  Thu Feb 17 18:17:52 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 17 Feb 2005 17:17:52 +0000 (GMT)
Subject: [R] How to get interction terms first in a model
In-Reply-To: <0ABD88905D18E347874E0FB71C0B29E9027FEAFF@exdkba022.novo.dk>
References: <0ABD88905D18E347874E0FB71C0B29E9027FEAFF@exdkba022.novo.dk>
Message-ID: <Pine.LNX.4.61.0502171710440.2259@gannet.stats>

It can be done via an explicit call to terms[.formula] with
keep.order = TRUE.

summary( lm(terms(y ~ ns(x, knots=c(30, 50, 70), intercept=T):A - 1 + B,
                   keep.order = TRUE)) )

I've given other examples in the past, and I think this is in the White 
Book.

BTW, it is not quite the same as this drops Ba not Bd (as I think it 
should).


On Thu, 17 Feb 2005, BXC (Bendix Carstensen) wrote:

> Consider the following two specifications of a model:
>
> library( splines )
> x <- 1:100
> y <- rnorm( 100 )
> w <- rep( 1, 100 )
> A <- factor( sample( 1:2, 100, replace=T ) )
> B <- factor( sample( letters[1:4], 100, replace=T ) )
> summary( lm( y ~ ns( x, knots=c(30, 50, 70 ), intercept=T ):A - 1 + B )
> )
> summary( lm( y ~ ns( x, knots=c(30, 50, 70 ), intercept=T ):A - 1 + B:w
> ) )
>
> The interaction with the constant variable w is how I got to have the
> two spline terms as "proper" intercept terms.
>
> Is there another way to do this, or is it a featur of the model formulae
> that lower order terms appear before higher order terms regardless of
> the
> order they are specified in the model?
>
> Bendix Carstensen
> ----------------------
> Bendix Carstensen
> Senior Statistician
> Steno Diabetes Center
> Niels Steensens Vej 2
> DK-2820 Gentofte
> Denmark
> tel: +45 44 43 87 38
> mob: +45 30 75 87 38
> fax: +45 44 43 07 06
> bxc at steno.dk
> www.biostat.ku.dk/~bxc
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mike_saunders at umenfa.maine.edu  Thu Feb 17 18:17:14 2005
From: mike_saunders at umenfa.maine.edu (Mike Saunders)
Date: Thu, 17 Feb 2005 12:17:14 -0500
Subject: [R] short plots: lwd, margin and postscript behavior
References: <20050217133613.61151.qmail@web14521.mail.yahoo.com>
	<001c01c514fb$cc6009c0$9ba76f82@CFRU0104>
	<4214BF00.9000504@statistik.uni-dortmund.de>
Message-ID: <002c01c51514$863d4a20$9ba76f82@CFRU0104>

Uwe:

I will try to isolate what sequence of calls is causing the problem.  I am 
running XP Pro with R 2.0.1.

Mike


Mike Saunders
Research Assistant
Forest Ecosystem Research Program
Department of Forest Ecosystem Sciences
University of Maine
Orono, ME  04469
207-581-2763 (O)
207-581-4257 (F)

----- Original Message ----- 
From: "Uwe Ligges" <ligges at statistik.uni-dortmund.de>
To: "Mike Saunders" <mike_saunders at umenfa.maine.edu>
Cc: "R Help" <r-help at stat.math.ethz.ch>
Sent: Thursday, February 17, 2005 10:57 AM
Subject: Re: [R] short plots: lwd, margin and postscript behavior


Mike Saunders wrote:

> Don't feel alone C?zar; I have had the same problems getting par commands 
> to work when making *.pdf files.  I also run R 2.0.1 and use par() after 
> opening the device.  Sometimes, the par() commands work; sometimes it 
> opens a new windows device.  Maybe something with MS Windows is causing 
> this.

Which version of Windows (I assume NT, 2k or XP; each with latest
Service Pack), which version of R (I assume R-2.0.1)?

Please start R with --vanilla and tell us exactly the sequence of calls
until this happens, if it is reproducible in any way...

Uwe Ligges



> Mike
>
> Mike Saunders
> Research Assistant
> Forest Ecosystem Research Program
> Department of Forest Ecosystem Sciences
> University of Maine
> Orono, ME  04469
> 207-581-2763 (O)
> 207-581-4257 (F)
>
> ----- Original Message ----- From: "C?zar Freitas" 
> <cafanselmo12 at yahoo.com.br>
> To: "Uwe Ligges" <ligges at statistik.uni-dortmund.de>; 
> <R-help at stat.math.ethz.ch>
> Sent: Thursday, February 17, 2005 8:36 AM
> Subject: Re: [R] short plots: lwd, margin and postscript behavior
>
>
> Thank you so much, but the commands in par doesn't
> affect the pictures generated by postscript.
> For example, I put lwd=3, and the postscript file is
> the same, using par(lwd=3) or par(lwd=.5)...
>
> --- Uwe Ligges <ligges at statistik.uni-dortmund.de>
> escreveu:
>
>> C?zar Freitas wrote:
>>
>> > I tested in R versions 1.8.1 and 2, but doesn't
>> works.
>> > The attached plots can explain this. And mar isn't
>> a
>> > parameter to postscript command. If I use in par,
>> it
>> > doesn't affect the outpu.
>>
>> It does in R-2.0.1!!! Please try out what poeple are
>> suggesting!
>> You have to set par() for the current device, so
>> after your call to
>> postscript, as I have indicated below.
>>
>> Uwe Ligges
>>
>>
>>
>>
>> >
>> > Thanks,
>> > C.
>> >
>> >  --- Uwe Ligges <ligges at statistik.uni-dortmund.de>
>> > escreveu:
>> >
>> >>C?zar Freitas wrote:
>> >>
>> >>
>> >>>Hi all.
>> >>>I'm working with a short plot (3x3 inches), but
>> >>
>> >>the
>> >>
>> >>>results (via postscript command) are not nice.
>> The
>> >>
>> >>lwd
>> >>
>> >>>command don't affect the lines (that are very
>> >>
>> >>large)
>> >>
>> >>>and the margins don't change using oma, mai, mar,
>> >>
>> >>...
>> >>
>> >>>Below I put an example. Moreover, save the
>> >>
>> >>graphics
>> >>
>> >>>via postscript command isn't working well (see
>> the
>> >>>attached ps).
>> >>>Thanks by the help,
>> >>
>> >>
>> >>Setting lwd=0.5 works for me, as well as using
>> >>par(mar=c(....))
>> >>
>> >>
>> >>
>> >>
>> >>>Cezar Freitas.
>> >>>
>> >>>#Example:
>> >>>#data
>> >>>  scores<-c(2.0, 0.0, 5.0, 5.0, 5.0, 2.0, 0.0,
>> >>
>> >>5.0,
>> >>
>> >>>2.5, 4.0, 5.0, 0.0, 5.0, 0.0, 2.0, 5.0, 5.0, 2.0,
>> >>
>> >>3.0,
>> >>
>> >>>3.0)
>> >>>  q<-summary(scores)
>> >>>  gra<-hist(scores, breaks=((0:11)/2-.2),
>> >>
>> >>plot=FALSE)
>> >>
>> >>>  yy<-ceiling(max(gra$counts)/10)*10
>> >>>  yz<-yy/12
>> >>>
>> >>>#plot via postscript
>> >>>  postscript("test.ps", width=3, height=3,
>> >>>horizontal=FALSE, family="Times",
>> paper="special")
>> >>
>> >>par(mar=rep(1,4)+.1, lwd=0.5)
>> >>
>> >>
>> >>
>> >>>  hist(scores, breaks=((0:11)/2-.2),
>> xlim=c(-1,6),
>> >>>ylim=c(-yz,yy), main="scores", ylab="Freq",
>> >>>xlab="math", cex.axis=.3, cex.main=.3,
>> cex.sub=.3,
>> >>>cex.lab=.3, mgp=c(1.5,.5,0))
>> >>
>> >>add lwd=0.5
>> >>
>> >>Uwe Ligges
>> >>
>> >>
>> >>>  boxplot(scores, horizontal=1, add=1,
>> at=-2*yz/3,
>> >>>boxwex=1.5*yz, bty="n", axes=FALSE)
>> >>>  points(q[4], -2*yz/3, col=1, pch="+", cex=.5)
>> >>>
>> >>>  dev.off()
>> >>>
>> >>>
>> >>>
>> >>>
>> >>>
>> >>>
>> >>>
>> >>>
>> >>
>> >
>>
> _______________________________________________________
>
>> >
>> >>>
>> >>>
>> >
>>
> ------------------------------------------------------------------------
>
>> >
>> >>>______________________________________________
>> >>>R-help at stat.math.ethz.ch mailing list
>> >>>https://stat.ethz.ch/mailman/listinfo/r-help
>> >>>PLEASE do read the posting guide!
>> >>
>> >>http://www.R-project.org/posting-guide.html
>> >>
>> >>
>> >
>> >
>> >
>> >
>> >
>> >
>> >
>>
> _______________________________________________________
>
>>
>
>
>> r?pida e gr?tis
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Thu Feb 17 18:20:39 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 17 Feb 2005 17:20:39 +0000 (GMT)
Subject: [R] short plots: lwd, margin and postscript behavior
In-Reply-To: <20050217133613.61151.qmail@web14521.mail.yahoo.com>
References: <20050217133613.61151.qmail@web14521.mail.yahoo.com>
Message-ID: <Pine.LNX.4.61.0502171718240.2259@gannet.stats>

Time for a completely reproducible example a la the posting guide.

Plese show us _exactly_ what you did that does not work for you: your
original posting did not have lwd in the example.


On Thu, 17 Feb 2005, C?zar Freitas wrote:

> Thank you so much, but the commands in par doesn't
> affect the pictures generated by postscript.
> For example, I put lwd=3, and the postscript file is
> the same, using par(lwd=3) or par(lwd=.5)...
>
> --- Uwe Ligges <ligges at statistik.uni-dortmund.de>
> escreveu:
>> C?zar Freitas wrote:
>>
>>> I tested in R versions 1.8.1 and 2, but doesn't
>> works.
>>> The attached plots can explain this. And mar isn't
>> a
>>> parameter to postscript command. If I use in par,
>> it
>>> doesn't affect the outpu.
>>
>> It does in R-2.0.1!!! Please try out what poeple are
>> suggesting!
>> You have to set par() for the current device, so
>> after your call to
>> postscript, as I have indicated below.
>>
>> Uwe Ligges
>>
>>
>>
>>
>>>
>>> Thanks,
>>> C.
>>>
>>>  --- Uwe Ligges <ligges at statistik.uni-dortmund.de>
>>> escreveu:
>>>
>>>> C?zar Freitas wrote:
>>>>
>>>>
>>>>> Hi all.
>>>>> I'm working with a short plot (3x3 inches), but
>>>>
>>>> the
>>>>
>>>>> results (via postscript command) are not nice.
>> The
>>>>
>>>> lwd
>>>>
>>>>> command don't affect the lines (that are very
>>>>
>>>> large)
>>>>
>>>>> and the margins don't change using oma, mai, mar,
>>>>
>>>> ...
>>>>
>>>>> Below I put an example. Moreover, save the
>>>>
>>>> graphics
>>>>
>>>>> via postscript command isn't working well (see
>> the
>>>>> attached ps).
>>>>> Thanks by the help,
>>>>
>>>>
>>>> Setting lwd=0.5 works for me, as well as using
>>>> par(mar=c(....))
>>>>
>>>>
>>>>
>>>>
>>>>> Cezar Freitas.
>>>>>
>>>>> #Example:
>>>>> #data
>>>>>  scores<-c(2.0, 0.0, 5.0, 5.0, 5.0, 2.0, 0.0,
>>>>
>>>> 5.0,
>>>>
>>>>> 2.5, 4.0, 5.0, 0.0, 5.0, 0.0, 2.0, 5.0, 5.0, 2.0,
>>>>
>>>> 3.0,
>>>>
>>>>> 3.0)
>>>>>  q<-summary(scores)
>>>>>  gra<-hist(scores, breaks=((0:11)/2-.2),
>>>>
>>>> plot=FALSE)
>>>>
>>>>>  yy<-ceiling(max(gra$counts)/10)*10
>>>>>  yz<-yy/12
>>>>>
>>>>> #plot via postscript
>>>>>  postscript("test.ps", width=3, height=3,
>>>>> horizontal=FALSE, family="Times",
>> paper="special")
>>>>
>>>> par(mar=rep(1,4)+.1, lwd=0.5)
>>>>
>>>>
>>>>
>>>>>  hist(scores, breaks=((0:11)/2-.2),
>> xlim=c(-1,6),
>>>>> ylim=c(-yz,yy), main="scores", ylab="Freq",
>>>>> xlab="math", cex.axis=.3, cex.main=.3,
>> cex.sub=.3,
>>>>> cex.lab=.3, mgp=c(1.5,.5,0))
>>>>
>>>> add lwd=0.5
>>>>
>>>> Uwe Ligges
>>>>
>>>>
>>>>>  boxplot(scores, horizontal=1, add=1,
>> at=-2*yz/3,
>>>>> boxwex=1.5*yz, bty="n", axes=FALSE)
>>>>>  points(q[4], -2*yz/3, col=1, pch="+", cex=.5)
>>>>>
>>>>>  dev.off()

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ripley at stats.ox.ac.uk  Thu Feb 17 18:22:58 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 17 Feb 2005 17:22:58 +0000 (GMT)
Subject: [R] Easy cut & paste from Excel to R?
In-Reply-To: <4214B60C.1020409@statistik.uni-dortmund.de>
References: <1108648485.4214a22601b1d@webmail.lyon.inserm.fr>
	<4214B60C.1020409@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.61.0502171720590.2259@gannet.stats>

On Thu, 17 Feb 2005, Uwe Ligges wrote:

> Ken Knoblauch wrote:
>
>> I tried the interesting suggestion below, discussed in several postings 
>> yesterday on the help-list, on my Mac (0S 10.3.7) but could not get it to
>> work, as shown in the tests indicated below.
>> 
>> 
>>>>  read.table(file("clipboard"), sep="\t", dec=",")
>
> Connections to the clipboard are only available on Windows.

Ken is of course welcome to contribute them for MacOS X (or indeed for X11).
People do take for granted the work the developers do to provide such 
things ....

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From zvalim at gmail.com  Thu Feb 17 18:31:09 2005
From: zvalim at gmail.com (Uri)
Date: Thu, 17 Feb 2005 11:31:09 -0600
Subject: [R] memory garbage management when updating DFs in for loops
Message-ID: <656a77fc050217093117925bb7@mail.gmail.com>

Dear R wizards,

I'm running analyses on entries residing in the database.  A FOR loop
brings in entries in batches and populates a DataFrame with them.

That is, on each run of the for loop, the DF is re-populated with values.
 I soon noticed that as the processes goes on, R's computation speed
decreases.  UNIX's "top" shows a slow increment in memory usage, and
R's gc() shows a constant increase in the Vcells-gc_trigger value.

When I manually rm() the populated dataframe at the end of each loop,
there is no increase in memory usage -- everything is completely
stable.

Is this normal behavior for R?

Cheers,
Uri Hasson.



From jack_wang2000 at yahoo.com  Thu Feb 17 18:34:55 2005
From: jack_wang2000 at yahoo.com (JTW)
Date: Thu, 17 Feb 2005 09:34:55 -0800 (PST)
Subject: [R] Is there a way to specify different significance levels in
	jarque.bera.test()
Message-ID: <20050217173455.7388.qmail@web40421.mail.yahoo.com>

Dear List:

I am trying to understand how to use
jarque.bera.test() function of the "tseries" package. 
A numeric vector or time series seems to be the only
argument required.  What is the default significance
level for rejecting/accepting the null of normality? 
Is there a way to specify different significance
levels?

platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    0.0            
year     2004           
month    10             
day      04             
language R 

JW



From cafanselmo12 at yahoo.com.br  Thu Feb 17 18:42:07 2005
From: cafanselmo12 at yahoo.com.br (=?iso-8859-1?q?C=E9zar=20Freitas?=)
Date: Thu, 17 Feb 2005 14:42:07 -0300 (ART)
Subject: [R] short plots: lwd, margin and postscript behavior
In-Reply-To: <Pine.LNX.4.61.0502171718240.2259@gannet.stats>
Message-ID: <20050217174207.69054.qmail@web14526.mail.yahoo.com>

OK. I'll put the two forms and the results in two
attaches. There are two problems: the main is the
plots saved via postscript command have large lines
(lwd doesn't works). The other one is the big margins.


Thanks,
C.

PS: two attached pictures

#data
  par(fin=c(3,3))
  scores<-c(2.0, 0.0, 5.0, 5.0, 5.0, 2.0, 0.0, 5.0,
2.5, 4.0, 5.0, 0.0, 5.0, 0.0, 2.0, 5.0, 5.0, 2.0, 3.0,
3.0)
  q<-summary(scores)
  gra<-hist(scores, breaks=((0:11)/2-.2), plot=FALSE)
  yy<-ceiling(max(gra$counts)/10)*10
  yz<-yy/12

#picture saved via windows gui
  par(mai=rep(.2,4), lwd=.5, fin=c(3,3))

  hist(scores, breaks=((0:11)/2-.2), xlim=c(-1,6),
ylim=c(-yz,yy), main="scores", ylab="Freq",
xlab="math", cex.axis=.3, cex.main=.3, cex.sub=.3,
cex.lab=.3, mgp=c(1.5,.5,0), lwd=.5)
  boxplot(scores, horizontal=1, add=1, at=-2*yz/3,
boxwex=1.5*yz, bty="n", axes=FALSE)
  points(q[4], -2*yz/3, col=1, pch="+", cex=.5)


#picture saved via postscript command
  par(mai=rep(.2,4), lwd=.5, fin=c(3,3))
  postscript("test.ps", width=3, height=3,
horizontal=FALSE, family="Times", paper="special")

  hist(scores, breaks=((0:11)/2-.2), xlim=c(-1,6),
ylim=c(-yz,yy), main="scores", ylab="Freq",
xlab="math", cex.axis=.3, cex.main=.3, cex.sub=.3,
cex.lab=.3, mgp=c(1.5,.5,0), lwd=.5)
  boxplot(scores, horizontal=1, add=1, at=-2*yz/3,
boxwex=1.5*yz, bty="n", axes=FALSE)
  points(q[4], -2*yz/3, col=1, pch="+", cex=.5)

  dev.off()



 --- Prof Brian Ripley <ripley at stats.ox.ac.uk>
escreveu: 
> Time for a completely reproducible example a la the
> posting guide.
> 
> Plese show us _exactly_ what you did that does not
> work for you: your
> original posting did not have lwd in the example.
> 
> 
> On Thu, 17 Feb 2005, C?zar Freitas wrote:
> 
> > Thank you so much, but the commands in par doesn't
> > affect the pictures generated by postscript.
> > For example, I put lwd=3, and the postscript file
> is
> > the same, using par(lwd=3) or par(lwd=.5)...
> >
> > --- Uwe Ligges <ligges at statistik.uni-dortmund.de>
> > escreveu:
> >> C?zar Freitas wrote:
> >>
> >>> I tested in R versions 1.8.1 and 2, but doesn't
> >> works.
> >>> The attached plots can explain this. And mar
> isn't
> >> a
> >>> parameter to postscript command. If I use in
> par,
> >> it
> >>> doesn't affect the outpu.
> >>
> >> It does in R-2.0.1!!! Please try out what poeple
> are
> >> suggesting!
> >> You have to set par() for the current device, so
> >> after your call to
> >> postscript, as I have indicated below.
> >>
> >> Uwe Ligges
> >>
> >>
> >>
> >>
> >>>
> >>> Thanks,
> >>> C.
> >>>
> >>>  --- Uwe Ligges
> <ligges at statistik.uni-dortmund.de>
> >>> escreveu:
> >>>
> >>>> C?zar Freitas wrote:
> >>>>
> >>>>
> >>>>> Hi all.
> >>>>> I'm working with a short plot (3x3 inches),
> but
> >>>>
> >>>> the
> >>>>
> >>>>> results (via postscript command) are not nice.
> >> The
> >>>>
> >>>> lwd
> >>>>
> >>>>> command don't affect the lines (that are very
> >>>>
> >>>> large)
> >>>>
> >>>>> and the margins don't change using oma, mai,
> mar,
> >>>>
> >>>> ...
> >>>>
> >>>>> Below I put an example. Moreover, save the
> >>>>
> >>>> graphics
> >>>>
> >>>>> via postscript command isn't working well (see
> >> the
> >>>>> attached ps).
> >>>>> Thanks by the help,
> >>>>
> >>>>
> >>>> Setting lwd=0.5 works for me, as well as using
> >>>> par(mar=c(....))
> >>>>
> >>>>
> >>>>
> >>>>
> >>>>> Cezar Freitas.
> >>>>>
> >>>>> #Example:
> >>>>> #data
> >>>>>  scores<-c(2.0, 0.0, 5.0, 5.0, 5.0, 2.0, 0.0,
> >>>>
> >>>> 5.0,
> >>>>
> >>>>> 2.5, 4.0, 5.0, 0.0, 5.0, 0.0, 2.0, 5.0, 5.0,
> 2.0,
> >>>>
> >>>> 3.0,
> >>>>
> >>>>> 3.0)
> >>>>>  q<-summary(scores)
> >>>>>  gra<-hist(scores, breaks=((0:11)/2-.2),
> >>>>
> >>>> plot=FALSE)
> >>>>
> >>>>>  yy<-ceiling(max(gra$counts)/10)*10
> >>>>>  yz<-yy/12
> >>>>>
> >>>>> #plot via postscript
> >>>>>  postscript("test.ps", width=3, height=3,
> >>>>> horizontal=FALSE, family="Times",
> >> paper="special")
> >>>>
> >>>> par(mar=rep(1,4)+.1, lwd=0.5)
> >>>>
> >>>>
> >>>>
> >>>>>  hist(scores, breaks=((0:11)/2-.2),
> >> xlim=c(-1,6),
> >>>>> ylim=c(-yz,yy), main="scores", ylab="Freq",
> >>>>> xlab="math", cex.axis=.3, cex.main=.3,
> >> cex.sub=.3,
> >>>>> cex.lab=.3, mgp=c(1.5,.5,0))
> >>>>
> >>>> add lwd=0.5
> >>>>
> >>>> Uwe Ligges
> >>>>
> >>>>
> >>>>>  boxplot(scores, horizontal=1, add=1,
> >> at=-2*yz/3,
> >>>>> boxwex=1.5*yz, bty="n", axes=FALSE)
> >>>>>  points(q[4], -2*yz/3, col=1, pch="+", cex=.5)
> >>>>>
> >>>>>  dev.off()
> 
> -- 
> Brian D. Ripley,                 
> ripley at stats.ox.ac.uk
> Professor of Applied Statistics, 
> http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865
> 272861 (self)
> 1 South Parks Road,                     +44 1865
> 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865
272595 


	
	
		
_______________________________________________________ 
-------------- next part --------------
A non-text attachment was scrubbed...
Name: via_postscript.ps
Type: application/postscript
Size: 6629 bytes
Desc: via_postscript.ps
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20050217/3b6f5910/via_postscript.ps
-------------- next part --------------
A non-text attachment was scrubbed...
Name: via_R.ps
Type: application/postscript
Size: 32555 bytes
Desc: via_R.ps
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20050217/3b6f5910/via_R.ps

From jjarabek at stat.ufl.edu  Thu Feb 17 18:56:42 2005
From: jjarabek at stat.ufl.edu (Jamie Jarabek)
Date: Thu, 17 Feb 2005 12:56:42 -0500
Subject: [R] problem with se.contrast()
In-Reply-To: <16916.50894.173426.42673@stat.math.ethz.ch>
References: <42139BF7.2090809@stat.ufl.edu>
	<16916.50894.173426.42673@stat.math.ethz.ch>
Message-ID: <4214DADA.9010102@stat.ufl.edu>

Christoph,

Thank you for your advice. My actual design is indeed more complicated than what 
I have indicated here. I was just using this as a toy example illustrate my 
particular problem. As suggested by Prof. Ripley I will download R-devel and see 
if the fixes included within alleviate my problems.

Jamie Jarabek

Christoph Buser wrote:
> Dear Jamie
> 
> As Prof. Ripley explained your analysis is equivalent to the fixed effect
> models for the means, so you can calculate it by (if this is your design): 
> 
> 
>>Lab <- factor(rep(c("1","2","3"),each=12))
>>Material <- factor(rep(c("A","B","C","D"),each=3,times=3))
>>Measurement <- c(12.20,12.28,12.16,15.51,15.02,15.29,18.14,18.08,18.21,
>>                 18.54,18.36,18.45,12.59,12.30,12.67,14.98,15.46,15.22,
>>                 18.54,18.31,18.60,19.21,18.77,18.69,12.72,12.78,12.66,
>>                 15.33,15.19,15.24,18.00,18.15,17.93,18.88,18.12,18.03)
>>testdata <- data.frame(Lab,Material,Measurement)
>>rm(list=c("Lab","Material","Measurement"))
> 
>  
> You can aggregate your data
> 
> 
>>dat.mean <- aggregate(testdata$Measurement,
>>                      by = list(Material=testdata$Material,Lab=testdata$Lab),
>>                      FUN = mean)
>>names(dat.mean)[3] <- "Measurement"
> 
> 
>>test.red.aov1 <- aov(Measurement ~ Lab + Material, data = dat.mean)
>>se.contrast(test.red.aov1,
>>            list(Material=="A",Material=="B",Material=="C",Material=="D"),
>>            coef=c(0.5,0.5,-0.5,-0.5),dat.mean)
>>[1] 0.1220339
> 
> 
> By aggregating the data you bypass the problem in se.contrast and you do
> not need R-devel.
> 
> -----------------------------------------------------------------------------
> 
> The second way to get the same is to set your contrast for the factor
> "Material" and calculate you model with this contrast and use summary.lm: 
> 
> 
>>dat.mean$Material <- C(dat.mean$Material, c(-0.5,-0.5,0.5,0.5), 
>>                       how.many = 3) 
>>test.red.aov2 <- aov(Measurement ~ Lab + Material, data = dat.mean)
>>summary.lm(test.red.aov2)
> 
> 
> Coefficients:
>             Estimate Std. Error t value Pr(>|t|)    
> (Intercept) 16.02000    0.10568 151.583 5.56e-12 ***
> Lab2         0.25833    0.14946   1.728    0.135    
> Lab3         0.06583    0.14946   0.440    0.675    
> Material1    4.52278    0.12203  37.062 2.58e-08 ***
> Material2    1.21056    0.12203   9.920 6.06e-05 ***
> Material3    1.55389    0.12203  12.733 1.44e-05 ***
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 
> 
> Material1 is now the contrast you are interested in and you get beside the
> Std. Error the Estimate, too. Material2 and Material3 are just orthogonal
> contrasts to complete. 
> 
> -----------------------------------------------------------------------------
> 
> The third way (which I prefer) is using lme from the package nlme
> 
> 
>>library(nlme)
> 
> 
> Here I use the origianl data set (not aggregated) and set the desired
> contrast, too.
> 
> 
>>testdata$Material <- C(testdata$Material, c(-0.5,-0.5,0.5,0.5), 
>>                       how.many = 3) 
>>test.lme <- lme(fixed = Measurement ~ Material, data = testdata, 
>>                random = ~1|Lab/Material)
> 
> 
> With anova you get the F-Test for the fixed factor "Material"
> 
> 
>>anova(test.lme)
>>          numDF denDF  F-value p-value
> 
>   (Intercept)     1    24 43301.14  <.0001
>   Material        3     6   544.71  <.0001
> 
> and with the summary you have your contrast with standard error: 
> 
> 
>>summary(test.lme)
> 
> Fixed effects: Measurement ~ Material 
>                 Value  Std.Error DF   t-value p-value
> (Intercept) 16.128056 0.07750547 24 208.08925   0e+00
> Material1    4.522778 0.12203325  6  37.06185   0e+00
> Material2    1.210556 0.12203325  6   9.91988   1e-04
> Material3    1.553889 0.12203325  6  12.73332   0e+00
> 
> -----------------------------------------------------------------------------
> 
> Last but not least I tried it with R-devel and the original data frame:
> First I reset the contrast on the default value:
> 
> testdata$Material <- C(testdata$Material, "contr.treatment", how.many = 3) 
> 
> I used your syntax and se.contrast():
> 
> 
>>test.aov <- with(testdata,aov(Measurement ~ Material + Error(Lab/Material)))
>>se.contrast(test.aov,
>>            list(Material=="A",Material=="B",Material=="C",Material=="D"),
>>            coef=c(0.5,0.5,-0.5,-0.5),data=testdata)
>>[1] 0.1432572
> 
> 
> I got a different result and I have admit that I didn't understand why
> there is a differnce between the lme model and this one. There are some
> comments in the help pages but I'm not sure if this is the answer.
> 
> -----------------------------------------------------------------------------
> 
> I hope some of the code above can help to analyse your data. Maybe Prof.
> Ripley was right and you have another design. Then you just can ignore this 
> and your life is much more easier :-)
> 
> Best regards,
> 
> Christoph Buser
> 
> 
> --------------------------------------------------------------
> Christoph Buser <buser at stat.math.ethz.ch>
> Seminar fuer Statistik, LEO C11
> ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
> phone: x-41-1-632-5414		fax: 632-1228
> http://stat.ethz.ch/~buser/
> --------------------------------------------------------------
> 
> 
> 
> Jamie Jarabek writes:
>  > I am having trouble getting standard errors for contrasts using se.contrast() in 
>  > what appears to be a simple case to me. The following test example illustrates 
>  > my problem:
>  > 
>  > Lab <- factor(rep(c("1","2","3"),each=12))
>  > Material <- factor(rep(c("A","B","C","D"),each=3,times=3))
>  > Measurement <- c(12.20,12.28,12.16,15.51,15.02,15.29,18.14,18.08,18.21,18.54,18.36
>  > ,18.45,12.59,12.30,12.67,14.98,15.46,15.22,18.54,18.31,18.60,19.21,18.77
>  > ,18.69,12.72,12.78,12.66,15.33,15.19,15.24,18.00,18.15,17.93,18.88,18.12,18.03)
>  > 
>  > testdata <- data.frame(Lab,Material,Measurement)
>  > rm(list=c("Lab","Material","Measurement"))
>  > 
>  > test.aov <- with(testdata,aov(Measurement ~ Material + Error(Lab/Material)))
>  > 
>  > This gives me the desired ANOVA table. I next want to get the standard
>  > errors for certain contrasts and following the help page for
>  > se.contrast() I tried the following but I get an error:
>  > 
>  > >se.contrast(test.aov,list(Material=="A",Material=="B",Material=="C",Material=="D"),coef=c(1,1,-1,-1),data=testdata)
>  > Error in matrix(0, length(asgn), ncol(effects), dimnames = list(nm[1 + :
>  >          length of dimnames [1] not equal to array extent
>  > 
>  > I have tested this on R 2.0.1 on Windows XP and Solaris and get the same
>  > error on both systems. I am unsure as to what I am doing wrong here. Thanks for 
>  > any help.
>  > 
>  > Jamie Jarabek
>  > 
>  > ______________________________________________
>  > R-help at stat.math.ethz.ch mailing list
>  > https://stat.ethz.ch/mailman/listinfo/r-help
>  > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Thu Feb 17 19:00:46 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 17 Feb 2005 18:00:46 +0000 (GMT)
Subject: [R] short plots: lwd, margin and postscript behavior
In-Reply-To: <20050217174207.69054.qmail@web14526.mail.yahoo.com>
References: <20050217174207.69054.qmail@web14526.mail.yahoo.com>
Message-ID: <Pine.LNX.4.61.0502171744090.2842@gannet.stats>

As I have already seen several times in this thread:

 	***** par() applies to the current device.  *****

Setting it before postscript() will have no effect on the postscript plot.

I am not at all sure what you are using fin= for, but that is what you are 
seeing the effects of in the margins on-screen.

Your margins are too small and your title text is too small, but that is 
what you asked for ....

The posting guide does give good advice: please spend an hour or so 
reading it and its references.


On Thu, 17 Feb 2005, C?zar Freitas wrote:

> OK. I'll put the two forms and the results in two
> attaches. There are two problems: the main is the
> plots saved via postscript command have large lines
> (lwd doesn't works). The other one is the big margins.
>
>
> Thanks,
> C.
>
> PS: two attached pictures
>
> #data
>  par(fin=c(3,3))
>  scores<-c(2.0, 0.0, 5.0, 5.0, 5.0, 2.0, 0.0, 5.0,
> 2.5, 4.0, 5.0, 0.0, 5.0, 0.0, 2.0, 5.0, 5.0, 2.0, 3.0,
> 3.0)
>  q<-summary(scores)
>  gra<-hist(scores, breaks=((0:11)/2-.2), plot=FALSE)
>  yy<-ceiling(max(gra$counts)/10)*10
>  yz<-yy/12
>
> #picture saved via windows gui
>  par(mai=rep(.2,4), lwd=.5, fin=c(3,3))
>
>  hist(scores, breaks=((0:11)/2-.2), xlim=c(-1,6),
> ylim=c(-yz,yy), main="scores", ylab="Freq",
> xlab="math", cex.axis=.3, cex.main=.3, cex.sub=.3,
> cex.lab=.3, mgp=c(1.5,.5,0), lwd=.5)
>  boxplot(scores, horizontal=1, add=1, at=-2*yz/3,
> boxwex=1.5*yz, bty="n", axes=FALSE)
>  points(q[4], -2*yz/3, col=1, pch="+", cex=.5)
>
>
> #picture saved via postscript command
>  par(mai=rep(.2,4), lwd=.5, fin=c(3,3))
>  postscript("test.ps", width=3, height=3,
> horizontal=FALSE, family="Times", paper="special")
>
>  hist(scores, breaks=((0:11)/2-.2), xlim=c(-1,6),
> ylim=c(-yz,yy), main="scores", ylab="Freq",
> xlab="math", cex.axis=.3, cex.main=.3, cex.sub=.3,
> cex.lab=.3, mgp=c(1.5,.5,0), lwd=.5)
>  boxplot(scores, horizontal=1, add=1, at=-2*yz/3,
> boxwex=1.5*yz, bty="n", axes=FALSE)
>  points(q[4], -2*yz/3, col=1, pch="+", cex=.5)
>
>  dev.off()
>
>
>
> --- Prof Brian Ripley <ripley at stats.ox.ac.uk>
> escreveu:
>> Time for a completely reproducible example a la the
>> posting guide.
>>
>> Plese show us _exactly_ what you did that does not
>> work for you: your
>> original posting did not have lwd in the example.
>>
>>
>> On Thu, 17 Feb 2005, C?zar Freitas wrote:
>>
>>> Thank you so much, but the commands in par doesn't
>>> affect the pictures generated by postscript.
>>> For example, I put lwd=3, and the postscript file
>> is
>>> the same, using par(lwd=3) or par(lwd=.5)...
>>>
>>> --- Uwe Ligges <ligges at statistik.uni-dortmund.de>
>>> escreveu:
>>>> C?zar Freitas wrote:
>>>>
>>>>> I tested in R versions 1.8.1 and 2, but doesn't
>>>> works.
>>>>> The attached plots can explain this. And mar
>> isn't
>>>> a
>>>>> parameter to postscript command. If I use in
>> par,
>>>> it
>>>>> doesn't affect the outpu.
>>>>
>>>> It does in R-2.0.1!!! Please try out what poeple
>> are
>>>> suggesting!
>>>> You have to set par() for the current device, so
>>>> after your call to
>>>> postscript, as I have indicated below.
>>>>
>>>> Uwe Ligges
>>>>
>>>>
>>>>
>>>>
>>>>>
>>>>> Thanks,
>>>>> C.
>>>>>
>>>>>  --- Uwe Ligges
>> <ligges at statistik.uni-dortmund.de>
>>>>> escreveu:
>>>>>
>>>>>> C?zar Freitas wrote:
>>>>>>
>>>>>>
>>>>>>> Hi all.
>>>>>>> I'm working with a short plot (3x3 inches),
>> but
>>>>>>
>>>>>> the
>>>>>>
>>>>>>> results (via postscript command) are not nice.
>>>> The
>>>>>>
>>>>>> lwd
>>>>>>
>>>>>>> command don't affect the lines (that are very
>>>>>>
>>>>>> large)
>>>>>>
>>>>>>> and the margins don't change using oma, mai,
>> mar,
>>>>>>
>>>>>> ...
>>>>>>
>>>>>>> Below I put an example. Moreover, save the
>>>>>>
>>>>>> graphics
>>>>>>
>>>>>>> via postscript command isn't working well (see
>>>> the
>>>>>>> attached ps).
>>>>>>> Thanks by the help,
>>>>>>
>>>>>>
>>>>>> Setting lwd=0.5 works for me, as well as using
>>>>>> par(mar=c(....))
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>> Cezar Freitas.
>>>>>>>
>>>>>>> #Example:
>>>>>>> #data
>>>>>>>  scores<-c(2.0, 0.0, 5.0, 5.0, 5.0, 2.0, 0.0,
>>>>>>
>>>>>> 5.0,
>>>>>>
>>>>>>> 2.5, 4.0, 5.0, 0.0, 5.0, 0.0, 2.0, 5.0, 5.0,
>> 2.0,
>>>>>>
>>>>>> 3.0,
>>>>>>
>>>>>>> 3.0)
>>>>>>>  q<-summary(scores)
>>>>>>>  gra<-hist(scores, breaks=((0:11)/2-.2),
>>>>>>
>>>>>> plot=FALSE)
>>>>>>
>>>>>>>  yy<-ceiling(max(gra$counts)/10)*10
>>>>>>>  yz<-yy/12
>>>>>>>
>>>>>>> #plot via postscript
>>>>>>>  postscript("test.ps", width=3, height=3,
>>>>>>> horizontal=FALSE, family="Times",
>>>> paper="special")
>>>>>>
>>>>>> par(mar=rep(1,4)+.1, lwd=0.5)
>>>>>>
>>>>>>
>>>>>>
>>>>>>>  hist(scores, breaks=((0:11)/2-.2),
>>>> xlim=c(-1,6),
>>>>>>> ylim=c(-yz,yy), main="scores", ylab="Freq",
>>>>>>> xlab="math", cex.axis=.3, cex.main=.3,
>>>> cex.sub=.3,
>>>>>>> cex.lab=.3, mgp=c(1.5,.5,0))
>>>>>>
>>>>>> add lwd=0.5
>>>>>>
>>>>>> Uwe Ligges
>>>>>>
>>>>>>
>>>>>>>  boxplot(scores, horizontal=1, add=1,
>>>> at=-2*yz/3,
>>>>>>> boxwex=1.5*yz, bty="n", axes=FALSE)
>>>>>>>  points(q[4], -2*yz/3, col=1, pch="+", cex=.5)
>>>>>>>
>>>>>>>  dev.off()
>>
>> --
>> Brian D. Ripley,
>> ripley at stats.ox.ac.uk
>> Professor of Applied Statistics,
>> http://www.stats.ox.ac.uk/~ripley/
>> University of Oxford,             Tel:  +44 1865
>> 272861 (self)
>> 1 South Parks Road,                     +44 1865
>> 272866 (PA)
>> Oxford OX1 3TG, UK                Fax:  +44 1865
> 272595
>
>
>
>
>
> _______________________________________________________


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From jack_wang2000 at yahoo.com  Thu Feb 17 19:23:02 2005
From: jack_wang2000 at yahoo.com (JTW)
Date: Thu, 17 Feb 2005 10:23:02 -0800 (PST)
Subject: [R] Is there a way to specify different significance levels in
	jarque.bera.test()?
Message-ID: <20050217182302.78838.qmail@web40426.mail.yahoo.com>

Dear List:

I am trying to understand how to use the
jarque.bera.test() function of the "tseries" package. 
A numeric vector or time series seems to be the only
argument required.  What is the default significance
level for rejecting the null of normality? 
Is there a way to specify different significance
levels?

platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    0.0            
year     2004           
month    10             
day      04             
language R 

JW



From avneetaquarian at yahoo.com  Thu Feb 17 19:25:46 2005
From: avneetaquarian at yahoo.com (avneet singh)
Date: Thu, 17 Feb 2005 10:25:46 -0800 (PST)
Subject: [R] How to convert a particular column of data.frame from numerical
	into factor
Message-ID: <20050217182546.22957.qmail@web14922.mail.yahoo.com>

I found ways to convert the entire data frame into
factors but could you please tell me a way to convert
just few particular columns from numeric to factors.

Thank you


I believe in equality for everyone, except reporters and photographers.
~Mahatma Gandhi



From itchycanuck at yahoo.com  Thu Feb 17 19:26:57 2005
From: itchycanuck at yahoo.com (Peter Rooney)
Date: Thu, 17 Feb 2005 10:26:57 -0800 (PST)
Subject: [R] Newbie: How to produce lm results at sub-level within df
Message-ID: <20050217182657.37147.qmail@web61106.mail.yahoo.com>

Hi,

Given a data frame:

df1:  application id (appid), project id (pid), person
months (pm), function points (fp)

How do I produce linear modelling results at the appid
level.  That is, I would like to find the coefficent
and intercept for the formula "pm ~ fp" for each
application.

Thanks



From ccleland at optonline.net  Thu Feb 17 21:08:31 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Thu, 17 Feb 2005 15:08:31 -0500
Subject: [R] Newbie: How to produce lm results at sub-level within df
In-Reply-To: <20050217182657.37147.qmail@web61106.mail.yahoo.com>
References: <20050217182657.37147.qmail@web61106.mail.yahoo.com>
Message-ID: <4214F9BF.3010709@optonline.net>

?by

by(df1, df1$appid, function(x) summary(lm(pm ~ fp, data=x)))

Peter Rooney wrote:
> Hi,
> 
> Given a data frame:
> 
> df1:  application id (appid), project id (pid), person
> months (pm), function points (fp)
> 
> How do I produce linear modelling results at the appid
> level.  That is, I would like to find the coefficent
> and intercept for the formula "pm ~ fp" for each
> application.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From avneetaquarian at yahoo.com  Thu Feb 17 21:15:19 2005
From: avneetaquarian at yahoo.com (avneet singh)
Date: Thu, 17 Feb 2005 12:15:19 -0800 (PST)
Subject: [R] How to convert a particular column of data.frame from numerical
	into factor
Message-ID: <20050217201519.92724.qmail@web14921.mail.yahoo.com>

Thanks to the two Gentlemen i have my answer. I am
posting it in case some newbie like me would need
advice while starting with R.

I am not sure how often it is conveyed but your help
is surely appreciated.

ANSWER:
df$yourColumn <- factor(df$yourColumn)

Thanks to: James Holtman & Peter Alspach

ps: i cant believe it was that simple!

I believe in equality for everyone, except reporters and photographers.
~Mahatma Gandhi



From sue at xlsolutions-corp.com  Thu Feb 17 21:46:04 2005
From: sue at xlsolutions-corp.com (sue@xlsolutions-corp.com)
Date: Thu, 17 Feb 2005 13:46:04 -0700
Subject: [R] Course***R/S-plus Fundamentals and Programming Techniques @ 3
	locations, March 2005
Message-ID: <20050217204604.1145.qmail@webmail13.mesa1.secureserver.net>

XLSolutions Corporation (www.xlsolutions-corp.com) is proud to
announce  2-day "R/S-plus Fundamentals and Programming
Techniques" in San Diego: www.xlsolutions-corp.com/training.htm
Please email us for our R/Splus Advanced Programming Courses.

****Irvine, CA ------------------------------------ March 14th -15th
****San Francisco, CA ---------------------- March 14th - 15th
****Washington, DC -------------------------- March 24th - 25th

Reserve your seat now at the early bird rates! Payment due AFTER
the class.



Course Description:

This two-day beginner to intermediate R/S-plus course focuses on a
broad spectrum of topics, from reading raw data to a comparison of R
and S. We will learn the essentials of data manipulation, graphical
visualization and R/S-plus programming. We will explore statistical
data analysis tools,including graphics with data sets. How to enhance
your plots. We will perform basic statistics and fit linear regression
models. Participants are encouraged to bring data for interactive
sessions


With the following outline:

- An Overview of R and S
- Data Manipulation and Graphics
- Using Lattice Graphics
- A Comparison of R and S-Plus
- How can R Complement SAS?
- Writing Functions
- Avoiding Loops
- Vectorization
- Statistical Modeling
- Project Management
- Techniques for Effective use of R and S
- Enhancing Plots
- Using High-level Plotting Functions
- Building and Distributing Packages (libraries)


Email us for group discounts.
Email Sue Turner: sue at xlsolutions-corp.com
Phone: 206-686-1578
Visit us: www.xlsolutions-corp.com/training.htm
Please let us know if you and your colleagues are interested in this
classto take advantage of group discount. Register now to secure your
seat! Interested in R/Splus Advanced course? email us.


Cheers,
Elvis Miller, PhD
Manager Training.
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com
elvis at xlsolutions-corp.com



From KeLin at mdanderson.org  Thu Feb 17 21:54:10 2005
From: KeLin at mdanderson.org (KeLin@mdanderson.org)
Date: Thu, 17 Feb 2005 14:54:10 -0600
Subject: [R] help on deleting NAs
Message-ID: <OF80B79DC7.07A98AF5-ON86256FAB.00724618-86256FAB.0072E125@mdacc.tmc.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050217/a546b1b6/attachment.pl

From adrian_d at eskimo.com  Thu Feb 17 21:54:54 2005
From: adrian_d at eskimo.com (Adrian Dragulescu)
Date: Thu, 17 Feb 2005 12:54:54 -0800 (PST)
Subject: [R] dumping the summary of lm to a text file
Message-ID: <Pine.SUN.4.58.0502171223120.5461@eskimo.com>


Hello list,

I have a linear regression

 ctl    <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
 trt    <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
 group  <- gl(2,10,20, labels=c("Ctl","Trt"))
 weight <- c(ctl, trt)
 reg    <- lm(weight ~ group)

 sreg   <- summary(reg)

and I would like to dump exactly what I see on the console with
print(sreg) to a text file.  I've tried using cat but it did not work.
I've read the list questions but I did not see a solution.

Can you help with this?

Thank you,
Adrian Dragulescu



From spencer.graves at pdf.com  Thu Feb 17 22:59:04 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 17 Feb 2005 13:59:04 -0800
Subject: [R]  Echo in sink?  Was:  dumping the summary of lm to a text file
In-Reply-To: <Pine.SUN.4.58.0502171223120.5461@eskimo.com>
References: <Pine.SUN.4.58.0502171223120.5461@eskimo.com>
Message-ID: <421513A8.7060404@pdf.com>

NEW QUESTION:  Is it possible to get the commands echoed in a sink file? 

ANSWER TO ADRIAN: 

      Have you considered "sink"?  This captures print results but not 
the commands that generated the print.  For example, consider the 
following: 

 > sink("tst.txt")
 > a <- 1
 > a
 > sink()

      The resulting file "tst.txt" contains only the one line obtained 
from "a", which is the same as "print(a)": 

[1] 1

      Similarly, it looks to me like the following 3 commands will 
produce what you want: 

sink("tst.txt")
sreg
sink()

      hope this helps.
      spencer graves

Adrian Dragulescu wrote:

>Hello list,
>
>I have a linear regression
>
> ctl    <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
> trt    <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
> group  <- gl(2,10,20, labels=c("Ctl","Trt"))
> weight <- c(ctl, trt)
> reg    <- lm(weight ~ group)
>
> sreg   <- summary(reg)
>
>and I would like to dump exactly what I see on the console with
>print(sreg) to a text file.  I've tried using cat but it did not work.
>I've read the list questions but I did not see a solution.
>
>Can you help with this?
>
>Thank you,
>Adrian Dragulescu
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From roebuck at odin.mdacc.tmc.edu  Thu Feb 17 23:08:00 2005
From: roebuck at odin.mdacc.tmc.edu (Paul Roebuck)
Date: Thu, 17 Feb 2005 16:08:00 -0600 (CST)
Subject: [R] dumping the summary of lm to a text file
In-Reply-To: <Pine.SUN.4.58.0502171223120.5461@eskimo.com>
References: <Pine.SUN.4.58.0502171223120.5461@eskimo.com>
Message-ID: <Pine.OSF.4.58.0502171605090.83514@odin.mdacc.tmc.edu>

On Thu, 17 Feb 2005, Adrian Dragulescu wrote:

> I have a linear regression
>
>  ctl    <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
>  trt    <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
>  group  <- gl(2,10,20, labels=c("Ctl","Trt"))
>  weight <- c(ctl, trt)
>  reg    <- lm(weight ~ group)
>
>  sreg   <- summary(reg)
>
> and I would like to dump exactly what I see on the console with
> print(sreg) to a text file.  I've tried using cat but it did not work.
> I've read the list questions but I did not see a solution.

outfile <- file("regsumm.out", "w")
capture.output(summary(reg), file=outfile)
close(outfile)

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)



From hmaughan at u.arizona.edu  Thu Feb 17 23:12:41 2005
From: hmaughan at u.arizona.edu (Heather Maughan)
Date: Thu, 17 Feb 2005 15:12:41 -0700
Subject: [R] Extracting values from linear models
Message-ID: <BE3A64E9.9DA%hmaughan@u.arizona.edu>

Hello:

I want to use values from the output of linear models done using permuted
data to construct a random distribution.  The problem I am having is the
extraction of a value, say the p-value or the regression coefficient, from
the summary of a linear model. When summarizing a linear model I get this:

Call:
lm(formula = fitness ~ mm)

Residuals:
     Min       1Q   Median       3Q      Max
-0.57369 -0.17551 -0.01602  0.15723  0.68844

Coefficients:
             Estimate Std. Error t value Pr(>|t|)
(Intercept)  1.783440   0.074052  24.084  < 2e-16 ***
mm          -0.004272   0.001456  -2.933  0.00662 **
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1

Residual standard error: 0.3261 on 28 degrees of freedom
Multiple R-Squared: 0.2351,    Adjusted R-squared: 0.2077
F-statistic: 8.604 on 1 and 28 DF,  p-value: 0.006621

How do I pick out the p-value, or the R-squared using R code?

Thanks,
Heather
-- 
Heather Maughan
Department of Ecology and Evolutionary Biology
Biosciences West 310
University of Arizona
Tucson, AZ  85701
Phone: 520-626-5108
Fax: 520-621-9190
hmaughan at u.arizona.edu



From p.connolly at hortresearch.co.nz  Thu Feb 17 23:16:10 2005
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Fri, 18 Feb 2005 11:16:10 +1300
Subject: [R] Lattice tick labels not all appearing
Message-ID: <20050217221610.GS22446@hortresearch.co.nz>

Recently I've encountered a phenomenon that can be illustrated thus:

> bwplot(runif(45)*100)

Depending on the random seed, one or other or both of the outer tick
labels are not labelled, even though there'd be heaps of space to do
so.  The same happens with the postscript device.

I can get them both to appear if I add an excessive xlim parameter
like so:

> bwplot(runif(25)*100, xlim = c(0, 100) + 7*c(-1,1))

Stretching by anything less than 7 will generally not succeed.

I'm sure this behaviour was not the case until recently.  For this
example, it's not a big deal, but I often do multiple page plots that
require different limits.  A more automatic way of making an
adjustment would be a great find.

TIA

platform i686-pc-linux-gnu
arch     i686             
os       linux-gnu        
system   i686, linux-gnu  
status                    
major    2                
minor    0.1              
year     2004             
month    11               
day      15               
language R                
> 
-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From bhyde at pobox.com  Thu Feb 17 23:54:16 2005
From: bhyde at pobox.com (Ben Hyde)
Date: Thu, 17 Feb 2005 17:54:16 -0500
Subject: [R] Sampling given a table of percentages?
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E6FF@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E6FF@usrymx25.merck.com>
Message-ID: <fe1fba169770ed9ca78719dc835ca7b1@pobox.com>

Thank you one and all for the answer to my question, I just knew it 
would be painfully obvious.   sample, duh.



From knoblauch at lyon.inserm.fr  Thu Feb 17 23:09:24 2005
From: knoblauch at lyon.inserm.fr (Ken Knoblauch)
Date: Thu, 17 Feb 2005 23:09:24 +0100
Subject: [R] Easy cut & paste from Excel to R?
Message-ID: <1108678164.4215161465ccc@webmail.lyon.inserm.fr>

Here is something quick & dirty for Mac that may be serviceable in
some cases, while awaiting someone with greater understanding of
programming connections than I have currently.

With the following copied to the clipboard from Excell:
H	T	Q	F
1	2	3.3	a
3	5	10.2	b
5	9	11	A

I tried in R:

read.table(pipe("pbpaste"),header=TRUE)
  H T    Q F
1 1 2  3.3 a
2 3 5 10.2 b
3 5 9 11.0 A
Warning message: 
incomplete final line found by readTableHeader on `pbpaste' 
> str(read.table(pipe("pbpaste"),header=TRUE))
`data.frame':	3 obs. of  4 variables:
 $ H: int  1 3 5
 $ T: int  2 5 9
 $ Q: num  3.3 10.2 11
 $ F: Factor w/ 3 levels "A","a","b": 2 3 1
Warning message: 
incomplete final line found by readTableHeader on `pbpaste'

I haven't been able to track down readTableHeader yet.  The warning
occurs even without headers in the data. 


Quoting Prof Brian Ripley <ripley at stats.ox.ac.uk>:

> On Thu, 17 Feb 2005, Uwe Ligges wrote:
> 
> > Ken Knoblauch wrote:
> >
> >> I tried the interesting suggestion below, discussed in several postings 
> >> yesterday on the help-list, on my Mac (0S 10.3.7) but could not get it
> to
> >> work, as shown in the tests indicated below.
> >> 
> >> 
> >>>>  read.table(file("clipboard"), sep="\t", dec=",")
> >
> > Connections to the clipboard are only available on Windows.
> 
> Ken is of course welcome to contribute them for MacOS X (or indeed for
> X11).
> People do take for granted the work the developers do to provide such 
> things ....
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 



____________________
Ken Knoblauch
Inserm U 371
Cerveau et Vision
18 avenue du Doyen Lepine
69675 Bron cedex
France
tel: +33 (0)4 72 91 34 77
fax: +33 (0)4 72 91 34 61
portable: 06 84 10 64 10



From p.dalgaard at biostat.ku.dk  Fri Feb 18 00:01:05 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 Feb 2005 00:01:05 +0100
Subject: [R] Extracting values from linear models
In-Reply-To: <BE3A64E9.9DA%hmaughan@u.arizona.edu>
References: <BE3A64E9.9DA%hmaughan@u.arizona.edu>
Message-ID: <x2acq2bxla.fsf@biostat.ku.dk>

Heather Maughan <hmaughan at u.arizona.edu> writes:

> Hello:
> 
> I want to use values from the output of linear models done using permuted
> data to construct a random distribution.  The problem I am having is the
> extraction of a value, say the p-value or the regression coefficient, from
> the summary of a linear model. When summarizing a linear model I get this:
> 
> Call:
> lm(formula = fitness ~ mm)
> 
> Residuals:
>      Min       1Q   Median       3Q      Max
> -0.57369 -0.17551 -0.01602  0.15723  0.68844
> 
> Coefficients:
>              Estimate Std. Error t value Pr(>|t|)
> (Intercept)  1.783440   0.074052  24.084  < 2e-16 ***
> mm          -0.004272   0.001456  -2.933  0.00662 **
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
> 
> Residual standard error: 0.3261 on 28 degrees of freedom
> Multiple R-Squared: 0.2351,    Adjusted R-squared: 0.2077
> F-statistic: 8.604 on 1 and 28 DF,  p-value: 0.006621
> 
> How do I pick out the p-value, or the R-squared using R code?

Take a look inside the print method for summary.lm objects
(getAnywhere(print.summary.lm)). Notice that the the x$fstatistic is
there but the p value is being calculated by the print method.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ramasamy at cancer.org.uk  Fri Feb 18 00:23:37 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Thu, 17 Feb 2005 23:23:37 +0000
Subject: [R] Extracting values from linear models
In-Reply-To: <BE3A64E9.9DA%hmaughan@u.arizona.edu>
References: <BE3A64E9.9DA%hmaughan@u.arizona.edu>
Message-ID: <1108682617.5978.9.camel@localhost.localdomain>

Assume that you have stored the lm object as 'fit' and the summary as
fit.summ as such

 x   <- rnorm(100)
 y   <- rnorm(100)
 fit <- lm( y ~ x )
 fit.summ <- summary( fit )

fit.summ$coefficients and fit.summ$adj.r.squared gives you the
coefficients and adjusted R-square.

names( fit.summ ) or str( fit.summ ) will give further clue as how
fit.summ looks like.

Regards, Adai


On Thu, 2005-02-17 at 15:12 -0700, Heather Maughan wrote:
> Hello:
> 
> I want to use values from the output of linear models done using permuted
> data to construct a random distribution.  The problem I am having is the
> extraction of a value, say the p-value or the regression coefficient, from
> the summary of a linear model. When summarizing a linear model I get this:
> 
> Call:
> lm(formula = fitness ~ mm)
> 
> Residuals:
>      Min       1Q   Median       3Q      Max
> -0.57369 -0.17551 -0.01602  0.15723  0.68844
> 
> Coefficients:
>              Estimate Std. Error t value Pr(>|t|)
> (Intercept)  1.783440   0.074052  24.084  < 2e-16 ***
> mm          -0.004272   0.001456  -2.933  0.00662 **
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
> 
> Residual standard error: 0.3261 on 28 degrees of freedom
> Multiple R-Squared: 0.2351,    Adjusted R-squared: 0.2077
> F-statistic: 8.604 on 1 and 28 DF,  p-value: 0.006621
> 
> How do I pick out the p-value, or the R-squared using R code?
> 
> Thanks,
> Heather



From p.dalgaard at biostat.ku.dk  Fri Feb 18 00:59:56 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 Feb 2005 00:59:56 +0100
Subject: [R] Easy cut & paste from Excel to R?
In-Reply-To: <1108678164.4215161465ccc@webmail.lyon.inserm.fr>
References: <1108678164.4215161465ccc@webmail.lyon.inserm.fr>
Message-ID: <x2braispoj.fsf@biostat.ku.dk>

Ken Knoblauch <knoblauch at lyon.inserm.fr> writes:

> Here is something quick & dirty for Mac that may be serviceable in
> some cases, while awaiting someone with greater understanding of
> programming connections than I have currently.
> 
> With the following copied to the clipboard from Excell:
> H	T	Q	F
> 1	2	3.3	a
> 3	5	10.2	b
> 5	9	11	A
> 
> I tried in R:
> 
> read.table(pipe("pbpaste"),header=TRUE)
>   H T    Q F
> 1 1 2  3.3 a
> 2 3 5 10.2 b
> 3 5 9 11.0 A
> Warning message: 
> incomplete final line found by readTableHeader on `pbpaste' 
> > str(read.table(pipe("pbpaste"),header=TRUE))
> `data.frame':	3 obs. of  4 variables:
>  $ H: int  1 3 5
>  $ T: int  2 5 9
>  $ Q: num  3.3 10.2 11
>  $ F: Factor w/ 3 levels "A","a","b": 2 3 1
> Warning message: 
> incomplete final line found by readTableHeader on `pbpaste'
> 
> I haven't been able to track down readTableHeader yet.  The warning
> occurs even without headers in the data. 

I think the "Header" means the first handful of lines (up to 5) used
to determine the file layout.

FWIW, here's what you can do with oocalc and tcltk on Linux:

> library(tcltk)
> read.delim(textConnection(tclvalue(tcl("clipboard","get"))))
  col1 col2
1   12   NA
2   34   56

OK, so it leaves an open connection dangling, but so does your
pipe()... 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.connolly at hortresearch.co.nz  Fri Feb 18 01:56:33 2005
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Fri, 18 Feb 2005 13:56:33 +1300
Subject: [R] help on deleting NAs
In-Reply-To: <OF80B79DC7.07A98AF5-ON86256FAB.00724618-86256FAB.0072E125@mdac
	c.tmc.edu>
References: <OF80B79DC7.07A98AF5-ON86256FAB.00724618-86256FAB.0072E125@mdacc
	.tmc.edu>
Message-ID: <20050218005632.GT22446@hortresearch.co.nz>

On Thu, 17-Feb-2005 at 02:54PM -0600, KeLin at mdanderson.org wrote:

|> Dear R friends
|> 
|> My goal is to eliminate this specific group(1) if the # of NAs in this 
|> group greater than
|> 50%(specifically say greater than 3). Would you please show me how to do 
|> it. 
|> I have a sample data as following:
|> 
|> Thanks a lot.
|> 
|> Kevin Lin
|> 
|>            y group f1 f2 f3
|> 30         NA     1  1  1  1
|> 27         NA     1  1  2  2
|> 48         NA     1  2  1  2
|> 40 -0.6066416     1  2  2  1
|> 24 -0.8323225     1  3  2  2
|> 25  1.3401226     2  1  1  1
|> 13  1.2619082     2  1  2  1
|> 14 -0.4323220     2  3  1  1
|> 36  0.8406529     2  3  2  2
|> 21  0.9604758     3  1  2  1
|> 18  0.9562072     3  2  1  1
|> 45  1.1285016     3  2  1  1
|> 50         NA     4  1  1  1
|> 11         NA     4  1  1  2
|> 41 -1.1017167     4  2  1  1
|> 37  0.9661283     4  3  1  1
|> 39 -0.2540905     4  3  1  2


There's probably a lot of niftier ways but this will give an idea:
If X is your dataframe above,

> aa <- with(X, tapply((y), group, function(x) length(x[is.na(x)])))
> names(aa[aa>2])
[1] "1"

> X[!with(X, group%in%as.numeric(names(aa[aa>2]))),]
            y group f1 f2 f3
6   1.3401226     2  1  1  1
7   1.2619082     2  1  2  1
8  -0.4323220     2  3  1  1
9   0.8406529     2  3  2  2
10  0.9604758     3  1  2  1
11  0.9562072     3  2  1  1
12  1.1285016     3  2  1  1
13         NA     4  1  1  1
14         NA     4  1  1  2
15 -1.1017167     4  2  1  1
16  0.9661283     4  3  1  1
17 -0.2540905     4  3  1  2
> 

The function in the tapply part could be made more general if 3
doesn't always constitute a majority.

HTH

-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From p.murrell at auckland.ac.nz  Fri Feb 18 01:57:59 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Fri, 18 Feb 2005 13:57:59 +1300
Subject: [R] JOBS: Lecturer/Senior Lect/Assoc Prof at Auckland New Zealand
Message-ID: <42153D97.5090401@stat.auckland.ac.nz>


                        (Apologies for cross-posting)

JOBS: Lecturer/Senior Lect/Assoc Prof at Auckland New Zealand
(North American equivalents are Assistant, Associate and full Professor)

The University of Auckland's Department of Statistics is the largest and 
most active in New Zealand and one of the most active in the southern 
hemisphere. It has approximately 30 academic staff of whom 20 have 
positions that include substantial research components. It teaches a 
full range of programmes from Bachelors to PhD.  The Department 
currently has approximately 60 graduate students of whom 19 are PhD 
students. It also has a substantial service role, has over 900 
equivalent full-time students in total, and is expanding.

The Department covers a wide range of areas of statistics but with 
particular strengths in statistical computing (it is the birthplace of 
R), statistical ecology, and biostatistics. It has strong Bayesian and 
applied probability groups.  It is an outward looking Department with 
strong collaborative links with many other Departments and Research 
Institutes. It is a partner in NZIMA, one of the first four Centres of 
Research Excellence established in New Zealand. With Auckland being an 
easy stop-over point en route to Australia and only 3 hours from Sydney, 
the Department attracts many visitors.

Applications are invited from those with research interests in any 
branch of Statistics broadly interpreted but applications from those 
specializing in social statistics or sampling, applied probability, 
statistical computing, bioinformatics, biostatistics, or areas related 
to business are particularly welcome.

Appointments may be made at Postdoctoral Fellow, Lecturer, Senior 
Lecturer or Associate-Professor level depending on the qualifications 
and experience of the applicant. (The range of positions at which 
appointments can be made is equivalent, in North American terms, to 
Postdoctoral Fellow, Assistant Professor, Associate Professor and full 
Professor)

Further details are available at
         Department:  http://www.stat.auckland.ac.nz
         Jobs:        http://www.stat.auckland.ac.nz/jobs/A081-05.pdf

Closing Date:    30 March 2005



From Michael.Grottke at duke.edu  Fri Feb 18 02:32:03 2005
From: Michael.Grottke at duke.edu (Michael Grottke)
Date: Thu, 17 Feb 2005 20:32:03 -0500
Subject: [R] (arbitrary) precision
Message-ID: <42154593.1070709@duke.edu>

Hello,

I am currently using R for fitting a model to various data sets 
(minimizing the negative log-likelihood) and calculating a number of 
metrics based on the parameter estimates. Within these calculations, I 
have steps of the form

log(log(1+x)),

where x can be very small (e.g., around 1e-16). Unfortunately, the 
precision of doubles does not suffice for coping with the difference in 
the orders of magnitude of 1 and x: 1+x is rounded to 1.

One way for solving this problem seems to be to use an arbitrary 
precision library implemented in C and call the respective routines for 
calculating the logarithm(s) from within R.

My questions are as follows:
1. Is there any better/more direct way to solve the problem?
2. Is there any arbitrary precision library you can suggest in particular?

Thanks a lot for your help.

Best regards,
   Michael

-- 
-------------------------------------------------
Dr. Michael Grottke
Department of Electrical and Computer Engineering
Box 90291
Duke University
Durham, NC 27708-0291, USA

Phone:  ++1 (919) 660-5051
E-mail: Michael.Grottke at duke.edu



From p.connolly at hortresearch.co.nz  Fri Feb 18 02:39:16 2005
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Fri, 18 Feb 2005 14:39:16 +1300
Subject: [R] Examples of multiple key grobs
Message-ID: <20050218013916.GU22446@hortresearch.co.nz>

The xyplot help page gives quite a lot of information how to use key
and indicates that legend needs to be used if multiple keys are
needed.  However, it gives only a brief description of what the grob
needs to contain to do multiple keys.

 I've only used the occasional grid function in panel functions, so I
don't have much of a sense of how grobs are constructed.  I've been
unable to find examples of code used to do multiple keys and would
appreciate being pointed in the direction of some.

TIA

-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From tuechler at gmx.at  Fri Feb 18 03:05:38 2005
From: tuechler at gmx.at (Heinz Tuechler)
Date: Fri, 18 Feb 2005 03:05:38 +0100
Subject: [R] Again: Variable names in functions
Message-ID: <3.0.6.32.20050218030538.007b4530@pop.gmx.net>

At 08:32 17.02.2005 -0800, Berton Gunter wrote:
>
>I thought Thomas L. was clear, but apparently not...
>
>** Do not pass character string names as arguments to functions. ** Pass the
>objects (or expressions) which can consist of lists of vectors, dataframes,
>etc. instead. 
>
>If you need the names (e.g. as labels) you can use the deparse(substitute())
>construction. I strongly recommend that you study pp. 44-46 and section 3.5
>("Computing on the Language") of V&R's S PROGRAMMING. The point is that one
>can, of course, do things the way you want, but it makes life unnecessarily
>difficult and complex because R is set up to pass arguments by value and can
>keep better track of proper evaluation environments when this is done (which
>means you don't have to).
>
>-- Bert Gunter
>
Thank you for your advice. I will try to get the book as soon as possible
and meanwhiles study again other publications regarding these questions.
At the moment I am uncertain which way to prefer, since I am making my
first steps in R. Somehow I got the impression that there is a little
ambiguity concerning the use of object (variable) names in functions. My
little experience with R gave me the imression that much output
automatically has a sensible format (title, naming of estimates for
factors, ...) when you pass the object (=variable) name. If you don't do
it, you have more work to arrive at the same result. A title with the
variable's name is usually more informative than one with "get(x)". So for
a beginner like me it's temting to try to program functions in a way that
they work as similar as possible to the simple call from the R prompt.
Maybe objects like columns of data frames could have something like a name
attribute equal to their (column)name which is passed automatically to
functions.
Again, many thanks,
Heinz T?chler

>> >
>> 
>> Thank you, this method works well. One step further I am again using
>> parse(), but maybe there is a better solution for that situation too.
>> The example would be a function, where I pass the variable 
>> name as string
>> instead of the name. The motivation for this is that it seems 
>> easier to
>> handle if I want to pass several variables (i.e. a vector of variable
>> names) to the function (as I learned recently from this help-list). 
>> In this case I have to use get(). In the case of calling table() the
>> variable name disappeares.
>> 
>> > alpha<-c(rep(1:5,10))
>> > name.alpha<-"alpha"
>> > mytable1<-function(x){print(table(get(x)))}
>> > mytable1(name.alpha)
>> 
>>  1  2  3  4  5 
>> 10 10 10 10 10 
>> 
>> If I use eval(parse()) instead, it works as expected. I tried several
>> combinations of eval() and substitute() but I did not find a solution.
>> Is there a similar "trick"?
>> 
>> > mytable2<-function(x){
>> +   string<-paste("print(table(",as.symbol(x),"))")
>> +   eval(parse(text=string))}
>> > mytable2(name.alpha)
>> alpha
>>  1  2  3  4  5 
>> 10 10 10 10 10 
>> 
>> 
>> Thanks,
>> Heinz T?chler
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>> 
>
>
>



From deepayan at stat.wisc.edu  Fri Feb 18 03:02:58 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 17 Feb 2005 20:02:58 -0600
Subject: [R] Lattice tick labels not all appearing
In-Reply-To: <20050217221610.GS22446@hortresearch.co.nz>
References: <20050217221610.GS22446@hortresearch.co.nz>
Message-ID: <200502172002.58611.deepayan@stat.wisc.edu>

On Thursday 17 February 2005 16:16, Patrick Connolly wrote:
> Recently I've encountered a phenomenon that can be illustrated thus:
> > bwplot(runif(45)*100)
>
> Depending on the random seed, one or other or both of the outer tick
> labels are not labelled, even though there'd be heaps of space to do
> so.  The same happens with the postscript device.
>
> I can get them both to appear if I add an excessive xlim parameter
>
> like so:
> > bwplot(runif(25)*100, xlim = c(0, 100) + 7*c(-1,1))
>
> Stretching by anything less than 7 will generally not succeed.
>
> I'm sure this behaviour was not the case until recently.  For this
> example, it's not a big deal, but I often do multiple page plots that
> require different limits.  A more automatic way of making an
> adjustment would be a great find.

This was an experimental feature that didn't turn out that well. You can 
disable it by setting

> lattice.options("skip.boundary.labels" = 0)

2.1.0 should behave better.

Deepayan



From deepayan at stat.wisc.edu  Fri Feb 18 03:38:54 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 17 Feb 2005 20:38:54 -0600
Subject: [R] Examples of multiple key grobs
In-Reply-To: <20050218013916.GU22446@hortresearch.co.nz>
References: <20050218013916.GU22446@hortresearch.co.nz>
Message-ID: <200502172038.54799.deepayan@stat.wisc.edu>

On Thursday 17 February 2005 19:39, Patrick Connolly wrote:
> The xyplot help page gives quite a lot of information how to use key
> and indicates that legend needs to be used if multiple keys are
> needed.  However, it gives only a brief description of what the grob
> needs to contain to do multiple keys.

You may have a slight confusion. Each ``key'' has to be a single grob 
(but a grob can be arbitrarily complicated).

>  I've only used the occasional grid function in panel functions, so I
> don't have much of a sense of how grobs are constructed.  I've been
> unable to find examples of code used to do multiple keys and would
> appreciate being pointed in the direction of some.

You would need to read up on grid documentation to learn about grobs.  
lattice offers two functions that explicitly create grobs, namely 
draw.key and draw.colorkey. The second last example in 'demo(lattice)' 
has an example of shadowed text, which I've modified in the example 
below:


g1 <- textGrob(rep("Fancy text", 2),
               x = unit(.5, "npc") + unit(c(.5, 0), "mm"),
               y = unit(.5, "npc") + unit(c(0, .5), "mm"),
               gp = gpar(col = c("black", "red"), cex = 3))

g2 <- textGrob("This is \na text grob")

xyplot(rnorm(10) ~ rnorm(10),
       legend =
       list(inside =
            list(fun = draw.key,
                 x = .3, y = .7, 
                 args =
                 list(key = list(points = list(col = 1:3),
                                 text = list(letters[1:3])),
                      draw = FALSE)),
            bottom = list(fun = g1),
            top    = list(fun = g2)))


The 'inside' component illustrates how you can create grobs inline using 
a function. This is useful if you need to create your grobs at the time 
of printing (e.g. if it makes uses of trellis.par.get), but otherwise a 
more readable version of this example is:


g3 <-
    draw.key(key = list(points = list(col = 1:3),
             text = list(letters[1:3])),
             draw = FALSE)


xyplot(rnorm(10) ~ rnorm(10),
       legend =
       list(inside = list(fun = g3, x = .3, y = .7),
            bottom = list(fun = g1),
            top    = list(fun = g2)))

Hope that helps somewhat,

Deepayan



From ggrothendieck at myway.com  Fri Feb 18 03:55:15 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 18 Feb 2005 02:55:15 +0000 (UTC)
Subject: [R] (arbitrary) precision
References: <42154593.1070709@duke.edu>
Message-ID: <loom.20050218T034935-782@post.gmane.org>

Michael Grottke <Michael.Grottke <at> duke.edu> writes:


: I am currently using R for fitting a model to various data sets 
: (minimizing the negative log-likelihood) and calculating a number of 
: metrics based on the parameter estimates. Within these calculations, I 
: have steps of the form
: 
: log(log(1+x)),
: 
: where x can be very small (e.g., around 1e-16). Unfortunately, the 
: precision of doubles does not suffice for coping with the difference in 
: the orders of magnitude of 1 and x: 1+x is rounded to 1.
: 
: One way for solving this problem seems to be to use an arbitrary 
: precision library implemented in C and call the respective routines for 
: calculating the logarithm(s) from within R.
: 
: My questions are as follows:
: 1. Is there any better/more direct way to solve the problem?
: 2. Is there any arbitrary precision library you can suggest in particular?
: 

The approximation log(1+x) = x would be accuate to several decimal
places in your case so your expression would reduce to 
log(log(1+x)) = log(x).



From bates at stat.wisc.edu  Fri Feb 18 04:38:44 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 17 Feb 2005 21:38:44 -0600
Subject: [R] (arbitrary) precision
In-Reply-To: <loom.20050218T034935-782@post.gmane.org>
References: <42154593.1070709@duke.edu>
	<loom.20050218T034935-782@post.gmane.org>
Message-ID: <42156344.5060901@stat.wisc.edu>

Gabor Grothendieck wrote:
> Michael Grottke <Michael.Grottke <at> duke.edu> writes:
> 
> 
> : I am currently using R for fitting a model to various data sets 
> : (minimizing the negative log-likelihood) and calculating a number of 
> : metrics based on the parameter estimates. Within these calculations, I 
> : have steps of the form
> : 
> : log(log(1+x)),
> : 
> : where x can be very small (e.g., around 1e-16). Unfortunately, the 
> : precision of doubles does not suffice for coping with the difference in 
> : the orders of magnitude of 1 and x: 1+x is rounded to 1.
> : 
> : One way for solving this problem seems to be to use an arbitrary 
> : precision library implemented in C and call the respective routines for 
> : calculating the logarithm(s) from within R.
> : 
> : My questions are as follows:
> : 1. Is there any better/more direct way to solve the problem?
> : 2. Is there any arbitrary precision library you can suggest in particular?
> : 
> 
> The approximation log(1+x) = x would be accuate to several decimal
> places in your case so your expression would reduce to 
> log(log(1+x)) = log(x).

Another possibility is to use the log1p function to evaluate log1p(x) = 
log(1+x).  The two expressions give similar answers in this case
 > options(digits=12)
 > log(log1p(1e-16))
[1] -36.8413614879
 > log(1e-16)
[1] -36.8413614879



From Innkeyp-r at yahoo.com  Fri Feb 18 06:14:37 2005
From: Innkeyp-r at yahoo.com (T Petersen)
Date: Fri, 18 Feb 2005 06:14:37 +0100
Subject: [R] Barplot - Can't figure it out
Message-ID: <421579BD.9030606@yahoo.com>

Hi,

I have two catagorical vectors like this;

x = c(1, 2, 4, 2, 1)
y = c(2, 4, 2 ,4, 1)

I want to set up a barplot with the catagories 1-4 horizontally  and 
number of occurances vertically for each vector x,y. I've tried

boxplot(table(x,y), beside=T)

and

boxplot(c(x,y), beside=T)

among others, but can't get it to work...Any ideas? I'd apppreciate any help



From Kevin.Wang at maths.anu.edu.au  Fri Feb 18 06:25:22 2005
From: Kevin.Wang at maths.anu.edu.au (Kevin Wang)
Date: Fri, 18 Feb 2005 16:25:22 +1100
Subject: [R] Barplot - Can't figure it out
In-Reply-To: <421579BD.9030606@yahoo.com>
References: <421579BD.9030606@yahoo.com>
Message-ID: <42157C42.3000200@maths.anu.edu.au>

Hi,

T Petersen wrote:
> Hi,
> 
> I have two catagorical vectors like this;
> 
> x = c(1, 2, 4, 2, 1)
> y = c(2, 4, 2 ,4, 1)
> 
> I want to set up a barplot with the catagories 1-4 horizontally  and 
> number of occurances vertically for each vector x,y. I've tried
> 
> boxplot(table(x,y), beside=T)
> 
> and
> 
> boxplot(c(x,y), beside=T)

Have you tried barplot(), instead of boxplot()???

Cheers,

Kev

-- 
Ko-Kang Kevin Wang
PhD Student
Centre for Mathematics and its Applications
Building 27, Room 1004
Mathematical Sciences Institute (MSI)
Australian National University
Canberra, ACT 0200
Australia

Homepage: http://wwwmaths.anu.edu.au/~wangk/
Ph (W): +61-2-6125-2431
Ph (H): +61-2-6125-7407
Ph (M): +61-40-451-8301



From Terji78 at yahoo.com  Fri Feb 18 06:34:55 2005
From: Terji78 at yahoo.com (T Petersen)
Date: Fri, 18 Feb 2005 06:34:55 +0100
Subject: [R] Barplot - Can't figure it out
In-Reply-To: <42157C42.3000200@maths.anu.edu.au>
References: <421579BD.9030606@yahoo.com> <42157C42.3000200@maths.anu.edu.au>
Message-ID: <42157E7F.1010801@yahoo.com>

Ups, it should of course be barplot() in my mail, not boxplot:-)

Kevin Wang wrote:

> Hi,
>
> T Petersen wrote:
>
>> Hi,
>>
>> I have two catagorical vectors like this;
>>
>> x = c(1, 2, 4, 2, 1)
>> y = c(2, 4, 2 ,4, 1)
>>
>> I want to set up a barplot with the catagories 1-4 horizontally  and 
>> number of occurances vertically for each vector x,y. I've tried
>>
>> boxplot(table(x,y), beside=T)
>>
>> and
>>
>> boxplot(c(x,y), beside=T)
>
>
> Have you tried barplot(), instead of boxplot()???
>
> Cheers,
>
> Kev
>



From Tom.Mulholland at dpi.wa.gov.au  Fri Feb 18 07:31:59 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Fri, 18 Feb 2005 14:31:59 +0800
Subject: [R] Barplot - Can't figure it out
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3C9A5@afhex01.dpi.wa.gov.au>

barplot(matrix(c(x,y),ncol = 2),beside=T)

Does this help 
 
?barplot notes

height: either a vector or matrix of values describing the bars which
          make up the plot.  If 'height' is a vector, the plot consists
          of a sequence of rectangular bars with heights given by the
          values in the vector.  If 'height' is a matrix and 'beside'
          is 'FALSE' then each bar of the plot corresponds to a column
          of 'height', with the values in the column giving the heights
          of stacked "sub-bars" making up the bar.  If 'height' is a
          matrix and 'beside' is 'TRUE', then the values in each column
          are juxtaposed rather than stacked.


> -----Original Message-----
> From: T Petersen [mailto:Terji78 at yahoo.com]
> Sent: Friday, 18 February 2005 1:35 PM
> To: Kevin Wang
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Barplot - Can't figure it out
> 
> 
> Ups, it should of course be barplot() in my mail, not boxplot:-)
> 
> Kevin Wang wrote:
> 
> > Hi,
> >
> > T Petersen wrote:
> >
> >> Hi,
> >>
> >> I have two catagorical vectors like this;
> >>
> >> x = c(1, 2, 4, 2, 1)
> >> y = c(2, 4, 2 ,4, 1)
> >>
> >> I want to set up a barplot with the catagories 1-4 
> horizontally  and 
> >> number of occurances vertically for each vector x,y. I've tried
> >>
> >> boxplot(table(x,y), beside=T)
> >>
> >> and
> >>
> >> boxplot(c(x,y), beside=T)
> >
> >
> > Have you tried barplot(), instead of boxplot()???
> >
> > Cheers,
> >
> > Kev
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Innkeyp-r at yahoo.com  Fri Feb 18 07:51:38 2005
From: Innkeyp-r at yahoo.com (T Petersen)
Date: Fri, 18 Feb 2005 07:51:38 +0100
Subject: [R] Barplot - Can't figure it out
In-Reply-To: <33F91FB3FDF42E4180428AC66A5CF30B02D3C9A5@afhex01.dpi.wa.gov.au>
References: <33F91FB3FDF42E4180428AC66A5CF30B02D3C9A5@afhex01.dpi.wa.gov.au>
Message-ID: <4215907A.8000502@yahoo.com>

Almost. Catagories aren't stacked - I would like to see that x has 2 
instances of "1" while y has 1 instance of "1". What's more, there are 
now TWO distinct barplots - the left one shows x, while the right one 
shows y. I could live with that, but what I'd ideally want is to have x 
and y beside each other for EACH catagory - so for catagory "1" you 
could see taht there are more x's than y's (two x's versus one y). But 
thanks for the help

Mulholland, Tom wrote:

>barplot(matrix(c(x,y),ncol = 2),beside=T)
>
>Does this help 
> 
>?barplot notes
>
>height: either a vector or matrix of values describing the bars which
>          make up the plot.  If 'height' is a vector, the plot consists
>          of a sequence of rectangular bars with heights given by the
>          values in the vector.  If 'height' is a matrix and 'beside'
>          is 'FALSE' then each bar of the plot corresponds to a column
>          of 'height', with the values in the column giving the heights
>          of stacked "sub-bars" making up the bar.  If 'height' is a
>          matrix and 'beside' is 'TRUE', then the values in each column
>          are juxtaposed rather than stacked.
>
>
>  
>
>>-----Original Message-----
>>From: T Petersen [mailto:Terji78 at yahoo.com]
>>Sent: Friday, 18 February 2005 1:35 PM
>>To: Kevin Wang
>>Cc: r-help at stat.math.ethz.ch
>>Subject: Re: [R] Barplot - Can't figure it out
>>
>>
>>Ups, it should of course be barplot() in my mail, not boxplot:-)
>>
>>Kevin Wang wrote:
>>
>>    
>>
>>>Hi,
>>>
>>>T Petersen wrote:
>>>
>>>      
>>>
>>>>Hi,
>>>>
>>>>I have two catagorical vectors like this;
>>>>
>>>>x = c(1, 2, 4, 2, 1)
>>>>y = c(2, 4, 2 ,4, 1)
>>>>
>>>>I want to set up a barplot with the catagories 1-4 
>>>>        
>>>>
>>horizontally  and 
>>    
>>
>>>>number of occurances vertically for each vector x,y. I've tried
>>>>
>>>>boxplot(table(x,y), beside=T)
>>>>
>>>>and
>>>>
>>>>boxplot(c(x,y), beside=T)
>>>>        
>>>>
>>>Have you tried barplot(), instead of boxplot()???
>>>
>>>Cheers,
>>>
>>>Kev
>>>
>>>      
>>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>    
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From petr.pikal at precheza.cz  Fri Feb 18 08:41:51 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 18 Feb 2005 08:41:51 +0100
Subject: [R] help on deleting NAs
In-Reply-To: <20050218005632.GT22446@hortresearch.co.nz>
References: <OF80B79DC7.07A98AF5-ON86256FAB.00724618-86256FAB.0072E125@mdac	c.tmc.edu>
Message-ID: <4215AA4F.31575.40D143@localhost>



On 18 Feb 2005 at 13:56, Patrick Connolly wrote:

> On Thu, 17-Feb-2005 at 02:54PM -0600, KeLin at mdanderson.org wrote:
> 
> |> Dear R friends
> |> 
> |> My goal is to eliminate this specific group(1) if the # of NAs in
> this |> group greater than |> 50%(specifically say greater than 3).
> Would you please show me how to do |> it. |> I have a sample data as
> following: |> |> Thanks a lot. |> |> Kevin Lin |> |>            y
> group f1 f2 f3 |> 30         NA     1  1  1  1 |> 27         NA     1 
> 1  2  2 |> 48         NA     1  2  1  2 |> 40 -0.6066416     1  2  2 
> 1 |> 24 -0.8323225     1  3  2  2 |> 25  1.3401226     2  1  1  1 |>
> 13  1.2619082     2  1  2  1 |> 14 -0.4323220     2  3  1  1 |> 36 
> 0.8406529     2  3  2  2 |> 21  0.9604758     3  1  2  1 |> 18 
> 0.9562072     3  2  1  1 |> 45  1.1285016     3  2  1  1 |> 50        
> NA     4  1  1  1 |> 11         NA     4  1  1  2 |> 41 -1.1017167    
> 4  2  1  1 |> 37  0.9661283     4  3  1  1 |> 39 -0.2540905     4  3 
> 1  2
> 
> 
> There's probably a lot of niftier ways but this will give an idea: If
> X is your dataframe above,

Hi

I am not sure if it is niftier but

> x <- read.table("clipboard",header=T)
> x[!x$group %in% which(tapply(is.na(x$y), x$group, sum) > 2), ]
            y group f1 f2 f3
25  1.3401226     2  1  1  1
13  1.2619082     2  1  2  1
14 -0.4323220     2  3  1  1
36  0.8406529     2  3  2  2
21  0.9604758     3  1  2  1
18  0.9562072     3  2  1  1
45  1.1285016     3  2  1  1
50         NA     4  1  1  1
11         NA     4  1  1  2
41 -1.1017167     4  2  1  1
37  0.9661283     4  3  1  1
39 -0.2540905     4  3  1  2

or if you want to use this 50% margin

x[!x$group %in% which (tapply(is.na(x$y),x$group,sum)/ 
tapply(is.na(x$y),x$group,length)>.5),]

gives you what you want.

Cheers
Petr






> 
> > aa <- with(X, tapply((y), group, function(x) length(x[is.na(x)])))
> > names(aa[aa>2])
> [1] "1"
> 
> > X[!with(X, group%in%as.numeric(names(aa[aa>2]))),]
>             y group f1 f2 f3
> 6   1.3401226     2  1  1  1
> 7   1.2619082     2  1  2  1
> 8  -0.4323220     2  3  1  1
> 9   0.8406529     2  3  2  2
> 10  0.9604758     3  1  2  1
> 11  0.9562072     3  2  1  1
> 12  1.1285016     3  2  1  1
> 13         NA     4  1  1  1
> 14         NA     4  1  1  2
> 15 -1.1017167     4  2  1  1
> 16  0.9661283     4  3  1  1
> 17 -0.2540905     4  3  1  2
> > 
> 
> The function in the tapply part could be made more general if 3
> doesn't always constitute a majority.
> 
> HTH
> 
> -- 
> Patrick Connolly
> HortResearch
> Mt Albert
> Auckland
> New Zealand 
> Ph: +64-9 815 4200 x 7188
> ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.
> ~ I have the world`s largest collection of seashells. I keep it on all
> the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright
> ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.
> ~
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From bela_b at gmx.net  Fri Feb 18 09:05:15 2005
From: bela_b at gmx.net (Bela Bauer)
Date: Fri, 18 Feb 2005 09:05:15 +0100
Subject: [R] Two-factorial Huynh-Feldt-Test
Message-ID: <4215A1BB.3020400@gmx.net>

Hi,

I'm currently working on porting some SAS scripts to R, and hence need 
to do the same calculation (and get the same results) as SAS in order to 
make the transition easier for users of the script.
In the script, I'm dealing with a two-factorial repeated-measures anova. 
I'll try to give you a short overview of the setup:

- two between-cell factors: facBetweenROI (numbering regions of 
interest, 6 levels) and facBetweenCond (numbering conditions, 2 levels).
- one within-cell factor: facWithin (which numbers subjects, while 
subjects are considered repetitions of the same measurement). This is 
basically the repeatedness of the data.

For this data, I do anovas for several linear models. SAS also 
calculates the Huynh-Feldt-Test, which is in this case very important to 
the users and cannot be replaced with nlme or something of the kind (as 
recommended in http://maths.newcastle.edu.au/~rking/R/help/03b/0813.html.

The models I use for the anovas are the following:

aov(vecData ~ (facWithin + facBetweenROI + facBetweenCond)^2)
aov(vecData ~ facBetweenROI + facBetweenCond %in% facWithin + 
Error(facBetweenROI %in% facWithin))
aov(vecData ~ facBetweenCond + facBetweenROI %in% facWithin + 
Error(facBetweenCond %in% facWithin))

SAS seems to calculate the Huynh-Feldt test for the first and the second 
model. The SAS output is (for the second aov)

Source                    DF   Type III SS   Mean Square  F Value  Pr > F

 roi                        5   258.7726806    51.7545361    21.28  <.0001
 Error(roi)                95   231.0511739     2.4321176

                                           Adj Pr > F
                  Source                 G - G     H - F

                  roi                   <.0001    <.0001
                  Error(roi)


                   Greenhouse-Geisser Epsilon    0.5367
                   Huynh-Feldt Epsilon           0.6333

and for the first one:

Source                    DF   Type III SS   Mean Square  F Value  Pr > F

 ord                        1     2.2104107     2.2104107     0.24  0.6276
 Error(ord)                19   172.7047994     9.0897263


 Source                    DF   Type III SS   Mean Square  F Value  Pr > F

 roi*ord                    5   10.37034918    2.07406984     2.89  0.0180
 Error(roi*ord)            95   68.25732255    0.71849813

                                           Adj Pr > F
                  Source                 G - G     H - F

                  roi*ord               0.0663    0.0591
                  Error(roi*ord)

Now, to do this in R, I have a short (and not very thorough) description 
of it in "Lehrbuch der Statistik", Bortz and I have the script from 
http://maths.newcastle.edu.au/~rking/R/help/03b/7891.html
Still, I can't figure out how to do this correctly in the two-factorial 
way (and with the different models that SAS seems to use.) I'll attach 
my code at the end, in case there's something that makes sense about it. 
I've tried to do it in several ways, but this is the only one that gives 
a somewhat reasonable result.

Can anyone give me a suggestion of how I could do this, where I could 
find information about it etc? Google doesn't help much except more SAS 
examples...:-(

hf <- function(mtxCov,ncol,nrow) {
  X <- mtxCov*(nrow-1)
  r <- length(X[,1])
  D <- 0
  for (i in 1: r) D<- D+ X[i,i]
  D <- D/r
  SPm  <- mean(X)
  SPm2 <- sum(X^2)
  SSrm <- 0
  for (i in 1: r) SSrm<- SSrm + mean(X[i,])^2
  epsilon <- (ncol^2*(D-SPm)^2) / ((ncol-1) * (SPm2 - 2*ncol*SSrm + 
ncol^2*SPm^2))
  print(epsilon)
}

# 2. do variance-covariance matrices for conditions first
avCov2 <- matrix(rep(0,36),ncol=length(unique( roi )))
for (iCond in 1:length(preparedData[1,3:4])) {
  mtx <- NULL
  for (iROI in 1:length(unique( roi ))) {
    mtx <- c(mtx,
             preparedData[roi==unique(roi)[iROI],iCond + 4])
  }
  mtx <- matrix(mtx, ncol=length(unique( roi )), byrow=F)
  avCov2 <- avCov2 + cov(mtx)
}
avCov2 <- avCov2 / length(preparedData[1,3:4])

hf(avCov2,
   length(unique( roi )),
   length(unique( subj )))



From bbruemmer at web.de  Fri Feb 18 09:24:33 2005
From: bbruemmer at web.de (Bernhard Bruemmer)
Date: Fri, 18 Feb 2005 09:24:33 +0100
Subject: [R] Easy cut & paste from Excel to R?
In-Reply-To: <x2braispoj.fsf@biostat.ku.dk> (Peter Dalgaard's message of "18
	Feb 2005 00:59:56 +0100")
References: <1108678164.4215161465ccc@webmail.lyon.inserm.fr>
	<x2braispoj.fsf@biostat.ku.dk>
Message-ID: <87brai9sxq.fsf@ug-uaao-c018.agrarokoenomie.agrar.uni-goettingen.de>

Peter Dalgaard <p.dalgaard at biostat.ku.dk> writes:

> Ken Knoblauch <knoblauch at lyon.inserm.fr> writes:
>
>> Here is something quick & dirty for Mac that may be serviceable in
>> some cases, while awaiting someone with greater understanding of
>> programming connections than I have currently.
>> 
>> With the following copied to the clipboard from Excell: H T Q F 1 2
>> 3.3 a 3 5 10.2 b 5 9 11 A
>> 
>> I tried in R:
>> 
>> read.table(pipe("pbpaste"),header=TRUE) H T Q F 1 1 2 3.3 a 2 3 5
>> 10.2 b 3 5 9 11.0 A Warning message: incomplete final line found by
>> readTableHeader on `pbpaste' >
>> str(read.table(pipe("pbpaste"),header=TRUE)) `data.frame': 3 obs. of
>> 4 variables: $ H: int 1 3 5 $ T: int 2 5 9 $ Q: num 3.3 10.2 11 $ F:
>> Factor w/ 3 levels "A","a","b": 2 3 1 Warning message: incomplete
>> final line found by readTableHeader on `pbpaste'
>> 
>> I haven't been able to track down readTableHeader yet.  The warning
>> occurs even without headers in the data.
>
> I think the "Header" means the first handful of lines (up to 5) used
> to determine the file layout.
>
> FWIW, here's what you can do with oocalc and tcltk on Linux:
>
>> library(tcltk)
>> read.delim(textConnection(tclvalue(tcl("clipboard","get"))))
>   col1 col2 1 12 NA 2 34 56

An alternative solution for X11 is provided by the utility program xclip
(http://people.debian.org/~kims/xclip). After selecting the desired
data in, say, oocalc, the data can be
read using the appropriate read.table variant:

data <- read.delim(pipe("xclip -o"))

showConnections() does not reveal any problems after this command.

-- 
Dr. Bernhard Br?mmer
Institute of Agricultural Economics
Georg-August-Universit?t G?ttingen
Platz der G?ttinger Sieben 5
37073 G?ttingen
Germany
Tel +49 (0)551 394811 Fax +49 (0)551 399866



From tom_hoary at web.de  Fri Feb 18 10:31:18 2005
From: tom_hoary at web.de (Thomas =?iso-8859-1?q?Sch=F6nhoff?=)
Date: Fri, 18 Feb 2005 10:31:18 +0100
Subject: [R] Unable to create histograms
In-Reply-To: <20050216230338.77859.qmail@web30505.mail.mud.yahoo.com>
References: <20050216230338.77859.qmail@web30505.mail.mud.yahoo.com>
Message-ID: <200502181031.18432.tom_hoary@web.de>

Am Donnerstag, 17. Februar 2005 00:03 schrieb Radha Chebolu:
> Hi,
. Also, does R let us import
> data from an excel spreadhsheet?

Yes, there are very helpful examples in import/export data in your R 
documentation help.start() . AFAIR, there is also an example on how 
to import excel sheets to R

Thomas



From buser at stat.math.ethz.ch  Fri Feb 18 10:29:29 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Fri, 18 Feb 2005 10:29:29 +0100
Subject: [R] problem with se.contrast()
In-Reply-To: <4214DADA.9010102@stat.ufl.edu>
References: <42139BF7.2090809@stat.ufl.edu>
	<16916.50894.173426.42673@stat.math.ethz.ch>
	<4214DADA.9010102@stat.ufl.edu>
Message-ID: <16917.46457.239283.474169@stat.math.ethz.ch>

Dear Jamie

I already thought that your data structure could be more
complicated than in the example. 
I would be careful anywhere. Since there is a difference in the
results of se.contrasts() in R-devel and the results from lme
(and the with lme consistent results of the aggregated data)
in this nice balanced design, I am suspicious, especially if you
real design is more complicated.
Even if you get no error message for your data, I'd calculate
the analysis using for example lme for a second time to control
the results. 
I wish you all the best for your analysis.

Christoph Buser

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C11
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-1-632-5414		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------


Jamie Jarabek writes:
 > Christoph,
 > 
 > Thank you for your advice. My actual design is indeed more complicated than what 
 > I have indicated here. I was just using this as a toy example illustrate my 
 > particular problem. As suggested by Prof. Ripley I will download R-devel and see 
 > if the fixes included within alleviate my problems.
 > 
 > Jamie Jarabek
 >



From p.dalgaard at biostat.ku.dk  Fri Feb 18 10:27:52 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 Feb 2005 10:27:52 +0100
Subject: [R] Two-factorial Huynh-Feldt-Test
In-Reply-To: <4215A1BB.3020400@gmx.net>
References: <4215A1BB.3020400@gmx.net>
Message-ID: <x2r7je5iav.fsf@biostat.ku.dk>

Bela Bauer <bela_b at gmx.net> writes:

> Hi,
> 
> I'm currently working on porting some SAS scripts to R, and hence need
> to do the same calculation (and get the same results) as SAS in order
> to make the transition easier for users of the script.
> In the script, I'm dealing with a two-factorial repeated-measures
> anova. I'll try to give you a short overview of the setup:
> 
> - two between-cell factors: facBetweenROI (numbering regions of
> interest, 6 levels) and facBetweenCond (numbering conditions, 2
> levels).
> - one within-cell factor: facWithin (which numbers subjects, while
> subjects are considered repetitions of the same measurement). This is
> basically the repeatedness of the data.
> 
> For this data, I do anovas for several linear models. SAS also
> calculates the Huynh-Feldt-Test, which is in this case very important
> to the users and cannot be replaced with nlme or something of the kind
> (as recommended in
> http://maths.newcastle.edu.au/~rking/R/help/03b/0813.html.

[There's no "Huynh-Feldt-Test", there's a H-F epsilon, which is a
correction to the F test]
 
> The models I use for the anovas are the following:
> 
> aov(vecData ~ (facWithin + facBetweenROI + facBetweenCond)^2)
> aov(vecData ~ facBetweenROI + facBetweenCond %in% facWithin +
> Error(facBetweenROI %in% facWithin))
> aov(vecData ~ facBetweenCond + facBetweenROI %in% facWithin +
> Error(facBetweenCond %in% facWithin))
> 
> SAS seems to calculate the Huynh-Feldt test for the first and the
> second model. The SAS output is (for the second aov)

Would you happen to know what logic SAS uses to recognize cases  where
the corrections apply?  I thought it only did it in the case of
repeated measurements, as in

proc glm;
        model bmc2-bmc7=bmc1 grp / nouni;
        repeated time ...


 
> Source                    DF   Type III SS   Mean Square  F Value  Pr > F
> 
>  roi                        5   258.7726806    51.7545361    21.28  <.0001
>  Error(roi)                95   231.0511739     2.4321176
> 
>                                            Adj Pr > F
>                   Source                 G - G     H - F
> 
>                   roi                   <.0001    <.0001
>                   Error(roi)
> 
> 
>                    Greenhouse-Geisser Epsilon    0.5367
>                    Huynh-Feldt Epsilon           0.6333
> 
> and for the first one:
> 
> Source                    DF   Type III SS   Mean Square  F Value  Pr > F
> 
>  ord                        1     2.2104107     2.2104107     0.24  0.6276
>  Error(ord)                19   172.7047994     9.0897263
> 
> 
>  Source                    DF   Type III SS   Mean Square  F Value  Pr > F
> 
>  roi*ord                    5   10.37034918    2.07406984     2.89  0.0180
>  Error(roi*ord)            95   68.25732255    0.71849813
> 
>                                            Adj Pr > F
>                   Source                 G - G     H - F
> 
>                   roi*ord               0.0663    0.0591
>                   Error(roi*ord)
> 
> Now, to do this in R, I have a short (and not very thorough)
> description of it in "Lehrbuch der Statistik", Bortz and I have the
> script from http://maths.newcastle.edu.au/~rking/R/help/03b/7891.html
> Still, I can't figure out how to do this correctly in the
> two-factorial way (and with the different models that SAS seems to
> use.) I'll attach my code at the end, in case there's something that
> makes sense about it. I've tried to do it in several ways, but this is
> the only one that gives a somewhat reasonable result.
> 
> Can anyone give me a suggestion of how I could do this, where I could
> find information about it etc? Google doesn't help much except more
> SAS examples...:-(

I went looking recently and all *I* got was SPSS examples... As it
turned out, Jon Baron/Yuelin Li has the H-F and G-G calculations
neatly outlined with R code and everything in

http://www.psych.upenn.edu/~baron/rpsych.pdf

> hf <- function(mtxCov,ncol,nrow) {
>   X <- mtxCov*(nrow-1)
>   r <- length(X[,1])
>   D <- 0
>   for (i in 1: r) D<- D+ X[i,i]
>   D <- D/r
>   SPm  <- mean(X)
>   SPm2 <- sum(X^2)
>   SSrm <- 0
>   for (i in 1: r) SSrm<- SSrm + mean(X[i,])^2
>   epsilon <- (ncol^2*(D-SPm)^2) / ((ncol-1) * (SPm2 - 2*ncol*SSrm +
> ncol^2*SPm^2))
>   print(epsilon)
> }
> 
> # 2. do variance-covariance matrices for conditions first
> avCov2 <- matrix(rep(0,36),ncol=length(unique( roi )))
> for (iCond in 1:length(preparedData[1,3:4])) {
>   mtx <- NULL
>   for (iROI in 1:length(unique( roi ))) {
>     mtx <- c(mtx,
>              preparedData[roi==unique(roi)[iROI],iCond + 4])
>   }
>   mtx <- matrix(mtx, ncol=length(unique( roi )), byrow=F)
>   avCov2 <- avCov2 + cov(mtx)
> }
> avCov2 <- avCov2 / length(preparedData[1,3:4])
> 
> hf(avCov2,
>    length(unique( roi )),
>    length(unique( subj )))
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From xuhy at ucla.edu  Fri Feb 18 11:11:41 2005
From: xuhy at ucla.edu (Haiyong Xu)
Date: Fri, 18 Feb 2005 02:11:41 -0800
Subject: [R] Question about legend
Message-ID: <4215BF5D.10807@ucla.edu>

Hi there,

I made a plot with histogram and the curve of kernel density estimation 
together. The question is to add a legend to it. What I want is to use a 
small box with shade representing the histogram and a line representing 
the kernel estimation. Is there any way to implement this?

Thanks a lot.

Haiyong



From ligges at statistik.uni-dortmund.de  Fri Feb 18 11:20:59 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 18 Feb 2005 11:20:59 +0100
Subject: [R] Is there a way to specify different significance levels in
	jarque.bera.test()?
In-Reply-To: <20050217182302.78838.qmail@web40426.mail.yahoo.com>
References: <20050217182302.78838.qmail@web40426.mail.yahoo.com>
Message-ID: <4215C18B.8030909@statistik.uni-dortmund.de>

JTW wrote:

> Dear List:
> 
> I am trying to understand how to use the
> jarque.bera.test() function of the "tseries" package. 
> A numeric vector or time series seems to be the only
> argument required.  What is the default significance
> level for rejecting the null of normality? 
> Is there a way to specify different significance
> levels?


?????

It gives you the p-value! So you don't need to specify any significance 
level \alpha. You can reject if p < \alpha for arbitrary (fixed) values 
of \alpha.

Uwe Ligges


> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    2              
> minor    0.0            
> year     2004           
> month    10             
> day      04             
> language R 
> 
> JW
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri Feb 18 11:26:05 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 18 Feb 2005 11:26:05 +0100
Subject: [R] eigen vector question
In-Reply-To: <5.2.0.9.2.20050217095954.02185730@email.psu.edu>
References: <5.2.0.9.2.20050217095954.02185730@email.psu.edu>
Message-ID: <4215C2BD.60001@statistik.uni-dortmund.de>

Jessica Higgs wrote:

> Sorry to bother everyone, but I've looked in all of the help files and 
> manuals I have and I can't find the answer to this question.  I'm doing 
> principle component analysis by calculating the eigen vectors of a 
> correlation matrix that I have that is composed of 21 parameters.  I 
> have the eigen vectors and their values that R produced for me but I'm 
> not sure how to tell which eigen vector/value corresponds to which 
> parameter because when R produces eigen vectors it does so in decreasing 
> order of significance, meaning that the eigen vector that explains the 
> most of the variance is listed first, followed by the next eigen vector, 
> etc etc. Any help would be appreciated. Feel free to write back if you 
> need more information on my problem.  Thanks!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html


Have you considered to use princomp()?

Uwe Ligges



From petr.pikal at precheza.cz  Fri Feb 18 11:25:24 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 18 Feb 2005 11:25:24 +0100
Subject: [R] Barplot - Can't figure it out
In-Reply-To: <4215907A.8000502@yahoo.com>
References: <33F91FB3FDF42E4180428AC66A5CF30B02D3C9A5@afhex01.dpi.wa.gov.au>
Message-ID: <4215D0A4.25614.D69692@localhost>

Hi

If I understand correctly

barplot(rbind(table(x), table(y)), beside=T)

does what you want.

Cheers
Petr



On 18 Feb 2005 at 7:51, T Petersen wrote:

> Almost. Catagories aren't stacked - I would like to see that x has 2
> instances of "1" while y has 1 instance of "1". What's more, there are
> now TWO distinct barplots - the left one shows x, while the right one
> shows y. I could live with that, but what I'd ideally want is to have
> x and y beside each other for EACH catagory - so for catagory "1" you
> could see taht there are more x's than y's (two x's versus one y). But
> thanks for the help
> 
> Mulholland, Tom wrote:
> 
> >barplot(matrix(c(x,y),ncol = 2),beside=T)
> >
> >Does this help 
> > 
> >?barplot notes
> >
> >height: either a vector or matrix of values describing the bars which
> >          make up the plot.  If 'height' is a vector, the plot
> >          consists of a sequence of rectangular bars with heights
> >          given by the values in the vector.  If 'height' is a matrix
> >          and 'beside' is 'FALSE' then each bar of the plot
> >          corresponds to a column of 'height', with the values in the
> >          column giving the heights of stacked "sub-bars" making up
> >          the bar.  If 'height' is a matrix and 'beside' is 'TRUE',
> >          then the values in each column are juxtaposed rather than
> >          stacked.
> >
> >
> >  
> >
> >>-----Original Message-----
> >>From: T Petersen [mailto:Terji78 at yahoo.com]
> >>Sent: Friday, 18 February 2005 1:35 PM
> >>To: Kevin Wang
> >>Cc: r-help at stat.math.ethz.ch
> >>Subject: Re: [R] Barplot - Can't figure it out
> >>
> >>
> >>Ups, it should of course be barplot() in my mail, not boxplot:-)
> >>
> >>Kevin Wang wrote:
> >>
> >>    
> >>
> >>>Hi,
> >>>
> >>>T Petersen wrote:
> >>>
> >>>      
> >>>
> >>>>Hi,
> >>>>
> >>>>I have two catagorical vectors like this;
> >>>>
> >>>>x = c(1, 2, 4, 2, 1)
> >>>>y = c(2, 4, 2 ,4, 1)
> >>>>
> >>>>I want to set up a barplot with the catagories 1-4 
> >>>>        
> >>>>
> >>horizontally  and 
> >>    
> >>
> >>>>number of occurances vertically for each vector x,y. I've tried
> >>>>
> >>>>boxplot(table(x,y), beside=T)
> >>>>
> >>>>and
> >>>>
> >>>>boxplot(c(x,y), beside=T)
> >>>>        
> >>>>
> >>>Have you tried barplot(), instead of boxplot()???
> >>>
> >>>Cheers,
> >>>
> >>>Kev
> >>>
> >>>      
> >>>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide! 
> >>http://www.R-project.org/posting-guide.html
> >>
> >>    
> >>
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide!
> >http://www.R-project.org/posting-guide.html
> >
> >  
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From jarioksa at sun3.oulu.fi  Fri Feb 18 12:07:26 2005
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: Fri, 18 Feb 2005 13:07:26 +0200
Subject: [R] eigen vector question
In-Reply-To: <4215C2BD.60001@statistik.uni-dortmund.de>
References: <5.2.0.9.2.20050217095954.02185730@email.psu.edu>
	<4215C2BD.60001@statistik.uni-dortmund.de>
Message-ID: <1108724846.26897.11.camel@biol102145.oulu.fi>

On Fri, 2005-02-18 at 11:26 +0100, Uwe Ligges wrote:
> Jessica Higgs wrote:
> 
> > Sorry to bother everyone, but I've looked in all of the help files and 
> > manuals I have and I can't find the answer to this question.  I'm doing 
> > principle component analysis by calculating the eigen vectors of a 
> > correlation matrix that I have that is composed of 21 parameters.  I 
> > have the eigen vectors and their values that R produced for me but I'm 
> > not sure how to tell which eigen vector/value corresponds to which 
> > parameter because when R produces eigen vectors it does so in decreasing 
> > order of significance, meaning that the eigen vector that explains the 
> > most of the variance is listed first, followed by the next eigen vector, 
> > etc etc. Any help would be appreciated. Feel free to write back if you 
> > need more information on my problem.  Thanks!
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> 
> 
> Have you considered to use princomp()?
> 
It is really weird that people always recommend using princomp, although
it is numerically inferior to prcomp and fails with rank deficit data.
The natural solution would be to define functions:

loadings <- 
function(x) UseMethod("loadings")

loadings.princomp <- 
function (x) x$loadings

loadings.prcomp <- 
function(x) structure(x$rotation, class="loadings")

cheers, jari oksanen
-- 
Jari Oksanen <jarioksa at sun3.oulu.fi>



From ahenningsen at email.uni-kiel.de  Fri Feb 18 12:19:47 2005
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Fri, 18 Feb 2005 12:19:47 +0100
Subject: [R] Question about legend
In-Reply-To: <4215BF5D.10807@ucla.edu>
References: <4215BF5D.10807@ucla.edu>
Message-ID: <200502181219.47923.ahenningsen@email.uni-kiel.de>

type "?legend" (and please read the posting guide)

On Friday 18 February 2005 11:11, Haiyong Xu wrote:
> Hi there,
>
> I made a plot with histogram and the curve of kernel density estimation
> together. The question is to add a legend to it. What I want is to use a
> small box with shade representing the histogram and a line representing
> the kernel estimation. Is there any way to implement this?
>
> Thanks a lot.
>
> Haiyong
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From belabauer at cbs.mpg.de  Fri Feb 18 12:34:56 2005
From: belabauer at cbs.mpg.de (Bela Bauer)
Date: Fri, 18 Feb 2005 12:34:56 +0100
Subject: [R] Two-factorial Huynh-Feldt-Test
In-Reply-To: <x2r7je5iav.fsf@biostat.ku.dk>
References: <4215A1BB.3020400@gmx.net> <x2r7je5iav.fsf@biostat.ku.dk>
Message-ID: <4215D2E0.5040606@cbs.mpg.de>

Peter Dalgaard wrote:

>>The models I use for the anovas are the following:
>>
>>aov(vecData ~ (facWithin + facBetweenROI + facBetweenCond)^2)
>>aov(vecData ~ facBetweenROI + facBetweenCond %in% facWithin +
>>Error(facBetweenROI %in% facWithin))
>>aov(vecData ~ facBetweenCond + facBetweenROI %in% facWithin +
>>Error(facBetweenCond %in% facWithin))
>>
>>SAS seems to calculate the Huynh-Feldt test for the first and the
>>second model. The SAS output is (for the second aov)
>>    
>>
>
>Would you happen to know what logic SAS uses to recognize cases  where
>the corrections apply?  I thought it only did it in the case of
>repeated measurements, as in
>
>proc glm;
>        model bmc2-bmc7=bmc1 grp / nouni;
>        repeated time ...
>  
>

Yes, of course, the entire SAS script is available to me:
PROC GLM;
MODEL col1 col3 col5 col7 col9 col11 col13 col15 col17 col19 col21 col23 
=/nouni;
repeated roi 6, ord 2/nom mean;
TITLE 'ABDERUS lat ACC 300-500';

This script wasn't written by me and unfortunately, I don't know 
anything about SAS scripting, which makes it hard for me to follow the 
script.

>http://www.psych.upenn.edu/~baron/rpsych.pdf
>
Thanks for that link, it seems like a quite useful paper!

Now, I've tried to follow it and apply the steps to my own problem, but 
I came up with a H-F and G-G value that is still slightly different from 
the one that SAS calculates (I'll attach my code at the end of this 
mail). With my old code (see my last email), I get 0.4682799. With the 
new code, I get 0.4494588 as G-G epsilon and 0.6063626 for the H-F 
correction. I think that this corresponds (or should do so, rather) to 
the SAS value of  0.6333.
Now, as you can see, the differences are small, but still there.
The main difference between my two versions of the code are that in the 
old code (based on that book), a covariance matrix is calculcated for 
every condition and then these are averaged for the covariance matrix 
that is used in the H-F formula. The new code, on the other hand, 
averages the data over all conditions and then calculcates the 
covariance matrix for that, which could probably explain the 
differences. There also seems to be a difference between the original 
H-F correction and the algorithm that SAS uses; this is mentioned in the 
PDF, but they don't explain it any further. I suspect that this 
difference could be exactly the difference between my two code versions, 
but I don't know for sure.

Now, with the difference between my value and the one from SAS being so 
small, I suspect that there's only a very slight difference between the 
algorithms. Do you have any hints what these could be, or how I could go 
about investigating it?

Thanks a lot for your help!

Bela Bauer

mtx <- NULL
for (iROI in 1:length(unique( roi )) ) {
  for (iSubj in 1:length(unique (subj )) ) {
    mtx <- c(mtx,
             mean(asa[subj==unique(subj)[iSubj] & roi==unique(roi)[iROI]],
                  aoa[subj==unique(subj)[iSubj] & roi==unique(roi)[iROI]]))
  }
}
mtx <- matrix(mtx,ncol=length(unique( roi )),byrow=F)

S <- var(mtx)

k <- 6
D <- (k^2 * (mean(S) - mean(diag(S)))^2)
N1 <- sum(S^2)
N2 <- 2 * k * sum(apply(S, 1, mean)^2)
N3 <- k^2 * mean(S)^2
epsiGG <- D / ((k - 1) * (N1 - N2 + N3))
epsiHF <- (10 * (k-1) * epsiGG - 2) / ((k-1) * ((10-1) - (k-1)*epsiGG))
print(epsiGG)
print(epsiHF)



From ligges at statistik.uni-dortmund.de  Fri Feb 18 12:49:37 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 18 Feb 2005 12:49:37 +0100
Subject: [R] eigen vector question
In-Reply-To: <1108724846.26897.11.camel@biol102145.oulu.fi>
References: <5.2.0.9.2.20050217095954.02185730@email.psu.edu>	
	<4215C2BD.60001@statistik.uni-dortmund.de>
	<1108724846.26897.11.camel@biol102145.oulu.fi>
Message-ID: <4215D651.9030705@statistik.uni-dortmund.de>

Jari Oksanen wrote:

> On Fri, 2005-02-18 at 11:26 +0100, Uwe Ligges wrote:
> 
>>Jessica Higgs wrote:
>>
>>
>>>Sorry to bother everyone, but I've looked in all of the help files and 
>>>manuals I have and I can't find the answer to this question.  I'm doing 
>>>principle component analysis by calculating the eigen vectors of a 
>>>correlation matrix that I have that is composed of 21 parameters.  I 
>>>have the eigen vectors and their values that R produced for me but I'm 
>>>not sure how to tell which eigen vector/value corresponds to which 
>>>parameter because when R produces eigen vectors it does so in decreasing 
>>>order of significance, meaning that the eigen vector that explains the 
>>>most of the variance is listed first, followed by the next eigen vector, 
>>>etc etc. Any help would be appreciated. Feel free to write back if you 
>>>need more information on my problem.  Thanks!
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! 
>>>http://www.R-project.org/posting-guide.html
>>
>>
>>Have you considered to use princomp()?
>>
> 
> It is really weird that people always recommend using princomp, although

Well, the question was related to *eigenvalues*, hence princomp springs 
to mind...

Uwe Ligges


> it is numerically inferior to prcomp and fails with rank deficit data.
> The natural solution would be to define functions:
> 
> loadings <- 
> function(x) UseMethod("loadings")
> 
> loadings.princomp <- 
> function (x) x$loadings
> 
> loadings.prcomp <- 
> function(x) structure(x$rotation, class="loadings")
> 
> cheers, jari oksanen



From kjetil at acelerate.com  Fri Feb 18 04:20:40 2005
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Thu, 17 Feb 2005 23:20:40 -0400
Subject: [R] (arbitrary) precision
In-Reply-To: <loom.20050218T034935-782@post.gmane.org>
References: <42154593.1070709@duke.edu>
	<loom.20050218T034935-782@post.gmane.org>
Message-ID: <42155F08.7000308@acelerate.com>

Gabor Grothendieck wrote:

>Michael Grottke <Michael.Grottke <at> duke.edu> writes:
>
>
>: I am currently using R for fitting a model to various data sets 
>: (minimizing the negative log-likelihood) and calculating a number of 
>: metrics based on the parameter estimates. Within these calculations, I 
>: have steps of the form
>: 
>: log(log(1+x)),
>: 
>: where x can be very small (e.g., around 1e-16). Unfortunately, the 
>: precision of doubles does not suffice for coping with the difference in 
>: the orders of magnitude of 1 and x: 1+x is rounded to 1.
>: 
>: One way for solving this problem seems to be to use an arbitrary 
>: precision library implemented in C and call the respective routines for 
>: calculating the logarithm(s) from within R.
>: 
>: My questions are as follows:
>: 1. Is there any better/more direct way to solve the problem?
>: 2. Is there any arbitrary precision library you can suggest in particular?
>: 
>
>The approximation log(1+x) = x would be accuate to several decimal
>places in your case so your expression would reduce to 
>log(log(1+x)) = log(x).
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>  
>
see also   ?log1p

Kjetil

-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra





-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From Innkeyp-r at yahoo.com  Fri Feb 18 13:00:40 2005
From: Innkeyp-r at yahoo.com (T Petersen)
Date: Fri, 18 Feb 2005 13:00:40 +0100
Subject: [R] Barplot - Can't figure it out
In-Reply-To: <4215D0A4.25614.D69692@localhost>
References: <33F91FB3FDF42E4180428AC66A5CF30B02D3C9A5@afhex01.dpi.wa.gov.au>
	<4215D0A4.25614.D69692@localhost>
Message-ID: <4215D8E8.1050108@yahoo.com>

Wow, I'm getting confused...The syntax Petr suggested does what I 
wanted, but things are stille wrong...Maybe a bug? Let me explain.

I got two vectors:

x = c(3, 3, 3, 4, 3, 4, 3, 4, 3, 4)

y = c(5, 2, 5, 5, 2, 2, 5, 5, 4, 2)

then I do the barplot you suggest

barplot(rbind(table(x), table(y)), beside=T)

but things are wrong(there is no bar for catagory "3") and I get an error message:
Warning message: 
number of columns of result
        not a multiple of vector length (arg 1) in: rbind(table(Quest1), table(Quest2))

Any ideas?

Petr Pikal wrote:

>Hi
>
>If I understand correctly
>
>barplot(rbind(table(x), table(y)), beside=T)
>
>does what you want.
>
>Cheers
>Petr
>
>
>
>On 18 Feb 2005 at 7:51, T Petersen wrote:
>
>  
>
>>Almost. Catagories aren't stacked - I would like to see that x has 2
>>instances of "1" while y has 1 instance of "1". What's more, there are
>>now TWO distinct barplots - the left one shows x, while the right one
>>shows y. I could live with that, but what I'd ideally want is to have
>>x and y beside each other for EACH catagory - so for catagory "1" you
>>could see taht there are more x's than y's (two x's versus one y). But
>>thanks for the help
>>
>>Mulholland, Tom wrote:
>>
>>    
>>
>>>barplot(matrix(c(x,y),ncol = 2),beside=T)
>>>
>>>Does this help 
>>>
>>>?barplot notes
>>>
>>>height: either a vector or matrix of values describing the bars which
>>>         make up the plot.  If 'height' is a vector, the plot
>>>         consists of a sequence of rectangular bars with heights
>>>         given by the values in the vector.  If 'height' is a matrix
>>>         and 'beside' is 'FALSE' then each bar of the plot
>>>         corresponds to a column of 'height', with the values in the
>>>         column giving the heights of stacked "sub-bars" making up
>>>         the bar.  If 'height' is a matrix and 'beside' is 'TRUE',
>>>         then the values in each column are juxtaposed rather than
>>>         stacked.
>>>
>>>
>>> 
>>>
>>>      
>>>
>>>>-----Original Message-----
>>>>From: T Petersen [mailto:Terji78 at yahoo.com]
>>>>Sent: Friday, 18 February 2005 1:35 PM
>>>>To: Kevin Wang
>>>>Cc: r-help at stat.math.ethz.ch
>>>>Subject: Re: [R] Barplot - Can't figure it out
>>>>
>>>>
>>>>Ups, it should of course be barplot() in my mail, not boxplot:-)
>>>>
>>>>Kevin Wang wrote:
>>>>
>>>>   
>>>>
>>>>        
>>>>
>>>>>Hi,
>>>>>
>>>>>T Petersen wrote:
>>>>>
>>>>>     
>>>>>
>>>>>          
>>>>>
>>>>>>Hi,
>>>>>>
>>>>>>I have two catagorical vectors like this;
>>>>>>
>>>>>>x = c(1, 2, 4, 2, 1)
>>>>>>y = c(2, 4, 2 ,4, 1)
>>>>>>
>>>>>>I want to set up a barplot with the catagories 1-4 
>>>>>>       
>>>>>>
>>>>>>            
>>>>>>
>>>>horizontally  and 
>>>>   
>>>>
>>>>        
>>>>
>>>>>>number of occurances vertically for each vector x,y. I've tried
>>>>>>
>>>>>>boxplot(table(x,y), beside=T)
>>>>>>
>>>>>>and
>>>>>>
>>>>>>boxplot(c(x,y), beside=T)
>>>>>>       
>>>>>>
>>>>>>            
>>>>>>
>>>>>Have you tried barplot(), instead of boxplot()???
>>>>>
>>>>>Cheers,
>>>>>
>>>>>Kev
>>>>>
>>>>>     
>>>>>
>>>>>          
>>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide! 
>>>>http://www.R-project.org/posting-guide.html
>>>>
>>>>   
>>>>
>>>>        
>>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide!
>>>http://www.R-project.org/posting-guide.html
>>>
>>> 
>>>
>>>      
>>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>>http://www.R-project.org/posting-guide.html
>>    
>>
>
>Petr Pikal
>petr.pikal at precheza.cz
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From ligges at statistik.uni-dortmund.de  Fri Feb 18 13:12:27 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 18 Feb 2005 13:12:27 +0100
Subject: [R] Question about legend
In-Reply-To: <200502181219.47923.ahenningsen@email.uni-kiel.de>
References: <4215BF5D.10807@ucla.edu>
	<200502181219.47923.ahenningsen@email.uni-kiel.de>
Message-ID: <4215DBAB.4040402@statistik.uni-dortmund.de>

Arne Henningsen wrote:

> type "?legend" (and please read the posting guide)


I don't think it is that easy, but you have to tweak manually., e.g. by 
two calls to legend.

Uwe Ligges


> On Friday 18 February 2005 11:11, Haiyong Xu wrote:
> 
>>Hi there,
>>
>>I made a plot with histogram and the curve of kernel density estimation
>>together. The question is to add a legend to it. What I want is to use a
>>small box with shade representing the histogram and a line representing
>>the kernel estimation. Is there any way to implement this?
>>
>>Thanks a lot.
>>
>>Haiyong
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>>http://www.R-project.org/posting-guide.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Achim.Zeileis at wu-wien.ac.at  Fri Feb 18 13:36:12 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Fri, 18 Feb 2005 13:36:12 +0100
Subject: [R] Barplot - Can't figure it out
In-Reply-To: <4215D8E8.1050108@yahoo.com>
References: <33F91FB3FDF42E4180428AC66A5CF30B02D3C9A5@afhex01.dpi.wa.gov.au>
	<4215D0A4.25614.D69692@localhost> <4215D8E8.1050108@yahoo.com>
Message-ID: <20050218133612.34d2d7fe.Achim.Zeileis@wu-wien.ac.at>

On Fri, 18 Feb 2005 13:00:40 +0100 T Petersen wrote:

> Wow, I'm getting confused...The syntax Petr suggested does what I 
> wanted, but things are stille wrong...Maybe a bug? Let me explain.
> 
> I got two vectors:
> 
> x = c(3, 3, 3, 4, 3, 4, 3, 4, 3, 4)
> 
> y = c(5, 2, 5, 5, 2, 2, 5, 5, 4, 2)
> 
> then I do the barplot you suggest
> 
> barplot(rbind(table(x), table(y)), beside=T)
> 
> but things are wrong(there is no bar for catagory "3") and I get an
> error message: Warning message: 
> number of columns of result
>         not a multiple of vector length (arg 1) in:
>         rbind(table(Quest1), table(Quest2))
> 
> Any ideas?

If x and y are categorical variables, you should tell R so (i.e.,
convert to a factor) and if both should have the same categories (i.e.,
levels) you can supply this information as well:

R> x <- factor(x, levels = 2:5)
R> y <- factor(y, levels = 2:5)

Then, table() knows which categories to use:

R> rbind(x = table(x), y = table(y))
  2 3 4 5
x 0 6 4 0
y 4 0 1 5

and also the barplot() call given above will do the right thing.
Z

> Petr Pikal wrote:
> 
> >Hi
> >
> >If I understand correctly
> >
> >barplot(rbind(table(x), table(y)), beside=T)
> >
> >does what you want.
> >
> >Cheers
> >Petr
> >
> >
> >
> >On 18 Feb 2005 at 7:51, T Petersen wrote:
> >
> >  
> >
> >>Almost. Catagories aren't stacked - I would like to see that x has 2
> >>instances of "1" while y has 1 instance of "1". What's more, there
> >are>now TWO distinct barplots - the left one shows x, while the right
> >one>shows y. I could live with that, but what I'd ideally want is to
> >have>x and y beside each other for EACH catagory - so for catagory
> >"1" you>could see taht there are more x's than y's (two x's versus
> >one y). But>thanks for the help
> >>
> >>Mulholland, Tom wrote:
> >>
> >>    
> >>
> >>>barplot(matrix(c(x,y),ncol = 2),beside=T)
> >>>
> >>>Does this help 
> >>>
> >>>?barplot notes
> >>>
> >>>height: either a vector or matrix of values describing the bars
> >which>>         make up the plot.  If 'height' is a vector, the plot
> >>>         consists of a sequence of rectangular bars with heights
> >>>         given by the values in the vector.  If 'height' is a
> >matrix>>         and 'beside' is 'FALSE' then each bar of the plot
> >>>         corresponds to a column of 'height', with the values in
> >the>>         column giving the heights of stacked "sub-bars" making
> >up>>         the bar.  If 'height' is a matrix and 'beside' is
> >'TRUE',>>         then the values in each column are juxtaposed
> >rather than>>         stacked.
> >>>
> >>>
> >>> 
> >>>
> >>>      
> >>>
> >>>>-----Original Message-----
> >>>>From: T Petersen [mailto:Terji78 at yahoo.com]
> >>>>Sent: Friday, 18 February 2005 1:35 PM
> >>>>To: Kevin Wang
> >>>>Cc: r-help at stat.math.ethz.ch
> >>>>Subject: Re: [R] Barplot - Can't figure it out
> >>>>
> >>>>
> >>>>Ups, it should of course be barplot() in my mail, not boxplot:-)
> >>>>
> >>>>Kevin Wang wrote:
> >>>>
> >>>>   
> >>>>
> >>>>        
> >>>>
> >>>>>Hi,
> >>>>>
> >>>>>T Petersen wrote:
> >>>>>
> >>>>>     
> >>>>>
> >>>>>          
> >>>>>
> >>>>>>Hi,
> >>>>>>
> >>>>>>I have two catagorical vectors like this;
> >>>>>>
> >>>>>>x = c(1, 2, 4, 2, 1)
> >>>>>>y = c(2, 4, 2 ,4, 1)
> >>>>>>
> >>>>>>I want to set up a barplot with the catagories 1-4 
> >>>>>>       
> >>>>>>
> >>>>>>            
> >>>>>>
> >>>>horizontally  and 
> >>>>   
> >>>>
> >>>>        
> >>>>
> >>>>>>number of occurances vertically for each vector x,y. I've tried
> >>>>>>
> >>>>>>boxplot(table(x,y), beside=T)
> >>>>>>
> >>>>>>and
> >>>>>>
> >>>>>>boxplot(c(x,y), beside=T)
> >>>>>>       
> >>>>>>
> >>>>>>            
> >>>>>>
> >>>>>Have you tried barplot(), instead of boxplot()???
> >>>>>
> >>>>>Cheers,
> >>>>>
> >>>>>Kev
> >>>>>
> >>>>>     
> >>>>>
> >>>>>          
> >>>>>
> >>>>______________________________________________
> >>>>R-help at stat.math.ethz.ch mailing list
> >>>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>PLEASE do read the posting guide! 
> >>>>http://www.R-project.org/posting-guide.html
> >>>>
> >>>>   
> >>>>
> >>>>        
> >>>>
> >>>______________________________________________
> >>>R-help at stat.math.ethz.ch mailing list
> >>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>PLEASE do read the posting guide!
> >>>http://www.R-project.org/posting-guide.html
> >>>
> >>> 
> >>>
> >>>      
> >>>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide!
> >>http://www.R-project.org/posting-guide.html
> >>    
> >>
> >
> >Petr Pikal
> >petr.pikal at precheza.cz
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide!
> >http://www.R-project.org/posting-guide.html
> >
> >  
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From bxc at steno.dk  Fri Feb 18 13:48:00 2005
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Fri, 18 Feb 2005 13:48:00 +0100
Subject: [R] Barplot - Can't figure it out
Message-ID: <0ABD88905D18E347874E0FB71C0B29E9027FEB2D@exdkba022.novo.dk>

What you want is probably:

> cxy <- c(x,y)
> xy <- rep( c("x","y"), c(length(x),length(y)) )
> ( txy <- table(xy, cxy ) )
   cxy
xy  2 3 4 5
  x 0 6 4 0
  y 4 0 1 5
> barplot( txy, beside=T )

Bendix Carstensen
----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc
----------------------



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of T Petersen
> Sent: Friday, February 18, 2005 1:01 PM
> To: Petr Pikal
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Barplot - Can't figure it out
> 
> 
> Wow, I'm getting confused...The syntax Petr suggested does what I 
> wanted, but things are stille wrong...Maybe a bug? Let me explain.
> 
> I got two vectors:
> 
> x = c(3, 3, 3, 4, 3, 4, 3, 4, 3, 4)
> 
> y = c(5, 2, 5, 5, 2, 2, 5, 5, 4, 2)
> 
> then I do the barplot you suggest
> 
> barplot(rbind(table(x), table(y)), beside=T)
> 
> but things are wrong(there is no bar for catagory "3") and I 
> get an error message: Warning message: 
> number of columns of result
>         not a multiple of vector length (arg 1) in: 
> rbind(table(Quest1), table(Quest2))
> 
> Any ideas?



From s-plus at wiwi.uni-bielefeld.de  Fri Feb 18 13:49:43 2005
From: s-plus at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Fri, 18 Feb 2005 13:49:43 +0100
Subject: [R] Barplot - Can't figure it out
References: <33F91FB3FDF42E4180428AC66A5CF30B02D3C9A5@afhex01.dpi.wa.gov.au>
	<4215D0A4.25614.D69692@localhost> <4215D8E8.1050108@yahoo.com>
Message-ID: <4215E467.1070606@wiwi.uni-bielefeld.de>

Try:
x = c(3, 3, 3, 4, 3, 4, 3, 4, 3, 4,1:5)
y = c(5, 2, 5, 5, 2, 2, 5, 5, 4, 2,1:5)
barplot(rbind(table(x)-1, table(y)-1), beside=T)

P. Wolf

T Petersen wrote:

> Wow, I'm getting confused...The syntax Petr suggested does what I 
> wanted, but things are stille wrong...Maybe a bug? Let me explain.
>
> I got two vectors:
>
> x = c(3, 3, 3, 4, 3, 4, 3, 4, 3, 4)
>
> y = c(5, 2, 5, 5, 2, 2, 5, 5, 4, 2)
>
> then I do the barplot you suggest
>
> barplot(rbind(table(x), table(y)), beside=T)
>
> but things are wrong(there is no bar for catagory "3") and I get an 
> error message:
> Warning message: number of columns of result
>        not a multiple of vector length (arg 1) in: 
> rbind(table(Quest1), table(Quest2))
>
> Any ideas?
>
> Petr Pikal wrote:
>
>> Hi
>>
>> If I understand correctly
>>
>> barplot(rbind(table(x), table(y)), beside=T)
>>
>> does what you want.
>>
>> Cheers
>> Petr
>>
>>
>>
>> On 18 Feb 2005 at 7:51, T Petersen wrote:
>>
>>  
>>
>>> Almost. Catagories aren't stacked - I would like to see that x has 2
>>> instances of "1" while y has 1 instance of "1". What's more, there are
>>> now TWO distinct barplots - the left one shows x, while the right one
>>> shows y. I could live with that, but what I'd ideally want is to have
>>> x and y beside each other for EACH catagory - so for catagory "1" you
>>> could see taht there are more x's than y's (two x's versus one y). But
>>> thanks for the help
>>>
>>> Mulholland, Tom wrote:
>>>
>>>   
>>>
>>>> barplot(matrix(c(x,y),ncol = 2),beside=T)
>>>>
>>>> Does this help
>>>> ?barplot notes
>>>>
>>>> height: either a vector or matrix of values describing the bars which
>>>>         make up the plot.  If 'height' is a vector, the plot
>>>>         consists of a sequence of rectangular bars with heights
>>>>         given by the values in the vector.  If 'height' is a matrix
>>>>         and 'beside' is 'FALSE' then each bar of the plot
>>>>         corresponds to a column of 'height', with the values in the
>>>>         column giving the heights of stacked "sub-bars" making up
>>>>         the bar.  If 'height' is a matrix and 'beside' is 'TRUE',
>>>>         then the values in each column are juxtaposed rather than
>>>>         stacked.
>>>>
>>>>
>>>>
>>>>
>>>>     
>>>>
>>>>> -----Original Message-----
>>>>> From: T Petersen [mailto:Terji78 at yahoo.com]
>>>>> Sent: Friday, 18 February 2005 1:35 PM
>>>>> To: Kevin Wang
>>>>> Cc: r-help at stat.math.ethz.ch
>>>>> Subject: Re: [R] Barplot - Can't figure it out
>>>>>
>>>>>
>>>>> Ups, it should of course be barplot() in my mail, not boxplot:-)
>>>>>
>>>>> Kevin Wang wrote:
>>>>>
>>>>>  
>>>>>       
>>>>>
>>>>>> Hi,
>>>>>>
>>>>>> T Petersen wrote:
>>>>>>
>>>>>>    
>>>>>>         
>>>>>>
>>>>>>> Hi,
>>>>>>>
>>>>>>> I have two catagorical vectors like this;
>>>>>>>
>>>>>>> x = c(1, 2, 4, 2, 1)
>>>>>>> y = c(2, 4, 2 ,4, 1)
>>>>>>>
>>>>>>> I want to set up a barplot with the catagories 1-4      
>>>>>>>           
>>>>>>
>>>>> horizontally  and  
>>>>>       
>>>>>
>>>>>>> number of occurances vertically for each vector x,y. I've tried
>>>>>>>
>>>>>>> boxplot(table(x,y), beside=T)
>>>>>>>
>>>>>>> and
>>>>>>>
>>>>>>> boxplot(c(x,y), beside=T)
>>>>>>>      
>>>>>>>           
>>>>>>
>>>>>> Have you tried barplot(), instead of boxplot()???
>>>>>>
>>>>>> Cheers,
>>>>>>
>>>>>> Kev
>>>>>>
>>>>>>    
>>>>>>         
>>>>>
>>>>> ______________________________________________
>>>>> R-help at stat.math.ethz.ch mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide! 
>>>>> http://www.R-project.org/posting-guide.html
>>>>>
>>>>>  
>>>>>       
>>>>
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide!
>>>> http://www.R-project.org/posting-guide.html
>>>>
>>>>
>>>>
>>>>     
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide!
>>> http://www.R-project.org/posting-guide.html
>>>   
>>
>>
>> Petr Pikal
>> petr.pikal at precheza.cz
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>>  
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Fri Feb 18 13:50:41 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 Feb 2005 13:50:41 +0100
Subject: [R] Barplot - Can't figure it out
In-Reply-To: <4215D8E8.1050108@yahoo.com>
References: <33F91FB3FDF42E4180428AC66A5CF30B02D3C9A5@afhex01.dpi.wa.gov.au>
	<4215D0A4.25614.D69692@localhost> <4215D8E8.1050108@yahoo.com>
Message-ID: <x2fyzuc9r2.fsf@biostat.ku.dk>

T Petersen <Innkeyp-r at yahoo.com> writes:

> Wow, I'm getting confused...The syntax Petr suggested does what I
> wanted, but things are stille wrong...Maybe a bug? Let me explain.
> 
> I got two vectors:
> 
> x = c(3, 3, 3, 4, 3, 4, 3, 4, 3, 4)
> 
> y = c(5, 2, 5, 5, 2, 2, 5, 5, 4, 2)
> 
> then I do the barplot you suggest
> 
> barplot(rbind(table(x), table(y)), beside=T)
> 
> but things are wrong(there is no bar for catagory "3") and I get an error message:
> Warning message: number of columns of result
>         not a multiple of vector length (arg 1) in: rbind(table(Quest1), table(Quest2))
> 
> Any ideas?

Yes. Tabulation of numerics only counts values that are present in
data (what else could it do?). Try it with table(factor(x,
levels=1:5)), etc.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From petr.pikal at precheza.cz  Fri Feb 18 13:56:46 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 18 Feb 2005 13:56:46 +0100
Subject: [R] Barplot - Can't figure it out
In-Reply-To: <4215D8E8.1050108@yahoo.com>
References: <4215D0A4.25614.D69692@localhost>
Message-ID: <4215F41E.1717.1612C99@localhost>



On 18 Feb 2005 at 13:00, T Petersen wrote:

> Wow, I'm getting confused...The syntax Petr suggested does what I
> wanted, but things are stille wrong...Maybe a bug? Let me explain.

Bugs are exceptionally rare in R.

> 
> I got two vectors:
> 
> x = c(3, 3, 3, 4, 3, 4, 3, 4, 3, 4)
> 
> y = c(5, 2, 5, 5, 2, 2, 5, 5, 4, 2)
> 
> then I do the barplot you suggest
> 
> barplot(rbind(table(x), table(y)), beside=T)
> 

So you do not have same categories in both vectors.

The only thing I can come up with is to fill some dummy vector, like

xx<-rep(NA,5)

with

xx[1:5 %in% as.numeric(names(table(x)))]<-table(x)

to keep both vectors same length and with NA in place where there is 
no category. Than simply rbinding both vectors and making barplot.


> but things are wrong(there is no bar for catagory "3") and I get an
> error message: Warning message: number of columns of result
>         not a multiple of vector length (arg 1) in:
>         rbind(table(Quest1), table(Quest2))

warning message says exactly what it says

table(Quest1) and table(Quest2) does not result in the same 
categories, so rbinding vectors with different lengths is performed, 
shorter vector is recycled and warning is issued.


Cheers
Petr



> 
> Any ideas?
> 
> Petr Pikal wrote:
> 
> >Hi
> >
> >If I understand correctly
> >
> >barplot(rbind(table(x), table(y)), beside=T)
> >
> >does what you want.
> >
> >Cheers
> >Petr
> >
> >
> >
> >On 18 Feb 2005 at 7:51, T Petersen wrote:
> >
> >  
> >
> >>Almost. Catagories aren't stacked - I would like to see that x has 2
> >>instances of "1" while y has 1 instance of "1". What's more, there
> >>are now TWO distinct barplots - the left one shows x, while the
> >>right one shows y. I could live with that, but what I'd ideally want
> >>is to have x and y beside each other for EACH catagory - so for
> >>catagory "1" you could see taht there are more x's than y's (two x's
> >>versus one y). But thanks for the help
> >>
> >>Mulholland, Tom wrote:
> >>
> >>    
> >>
> >>>barplot(matrix(c(x,y),ncol = 2),beside=T)
> >>>
> >>>Does this help 
> >>>
> >>>?barplot notes
> >>>
> >>>height: either a vector or matrix of values describing the bars
> >>>which
> >>>         make up the plot.  If 'height' is a vector, the plot
> >>>         consists of a sequence of rectangular bars with heights
> >>>         given by the values in the vector.  If 'height' is a
> >>>         matrix and 'beside' is 'FALSE' then each bar of the plot
> >>>         corresponds to a column of 'height', with the values in
> >>>         the column giving the heights of stacked "sub-bars" making
> >>>         up the bar.  If 'height' is a matrix and 'beside' is
> >>>         'TRUE', then the values in each column are juxtaposed
> >>>         rather than stacked.
> >>>
> >>>
> >>> 
> >>>
> >>>      
> >>>
> >>>>-----Original Message-----
> >>>>From: T Petersen [mailto:Terji78 at yahoo.com]
> >>>>Sent: Friday, 18 February 2005 1:35 PM
> >>>>To: Kevin Wang
> >>>>Cc: r-help at stat.math.ethz.ch
> >>>>Subject: Re: [R] Barplot - Can't figure it out
> >>>>
> >>>>
> >>>>Ups, it should of course be barplot() in my mail, not boxplot:-)
> >>>>
> >>>>Kevin Wang wrote:
> >>>>
> >>>>   
> >>>>
> >>>>        
> >>>>
> >>>>>Hi,
> >>>>>
> >>>>>T Petersen wrote:
> >>>>>
> >>>>>     
> >>>>>
> >>>>>          
> >>>>>
> >>>>>>Hi,
> >>>>>>
> >>>>>>I have two catagorical vectors like this;
> >>>>>>
> >>>>>>x = c(1, 2, 4, 2, 1)
> >>>>>>y = c(2, 4, 2 ,4, 1)
> >>>>>>
> >>>>>>I want to set up a barplot with the catagories 1-4 
> >>>>>>       
> >>>>>>
> >>>>>>            
> >>>>>>
> >>>>horizontally  and 
> >>>>   
> >>>>
> >>>>        
> >>>>
> >>>>>>number of occurances vertically for each vector x,y. I've tried
> >>>>>>
> >>>>>>boxplot(table(x,y), beside=T)
> >>>>>>
> >>>>>>and
> >>>>>>
> >>>>>>boxplot(c(x,y), beside=T)
> >>>>>>       
> >>>>>>
> >>>>>>            
> >>>>>>
> >>>>>Have you tried barplot(), instead of boxplot()???
> >>>>>
> >>>>>Cheers,
> >>>>>
> >>>>>Kev
> >>>>>
> >>>>>     
> >>>>>
> >>>>>          
> >>>>>
> >>>>______________________________________________
> >>>>R-help at stat.math.ethz.ch mailing list
> >>>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>PLEASE do read the posting guide! 
> >>>>http://www.R-project.org/posting-guide.html
> >>>>
> >>>>   
> >>>>
> >>>>        
> >>>>
> >>>______________________________________________
> >>>R-help at stat.math.ethz.ch mailing list
> >>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>PLEASE do read the posting guide!
> >>>http://www.R-project.org/posting-guide.html
> >>>
> >>> 
> >>>
> >>>      
> >>>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide!
> >>http://www.R-project.org/posting-guide.html
> >>    
> >>
> >
> >Petr Pikal
> >petr.pikal at precheza.cz
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide!
> >http://www.R-project.org/posting-guide.html
> >
> >  
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From Innkeyp-r at yahoo.com  Fri Feb 18 14:00:51 2005
From: Innkeyp-r at yahoo.com (T Petersen)
Date: Fri, 18 Feb 2005 14:00:51 +0100
Subject: [R] Barplot - Can't figure it out
In-Reply-To: <20050218133612.34d2d7fe.Achim.Zeileis@wu-wien.ac.at>
References: <33F91FB3FDF42E4180428AC66A5CF30B02D3C9A5@afhex01.dpi.wa.gov.au>	<4215D0A4.25614.D69692@localhost>
	<4215D8E8.1050108@yahoo.com>
	<20050218133612.34d2d7fe.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <4215E703.20503@yahoo.com>

Yeah, that's it. I have to catagorize the data AND tell R how many 
catagories there are. It works perfectly now and I've learned some 
more:-D Great.



Achim Zeileis wrote:

>On Fri, 18 Feb 2005 13:00:40 +0100 T Petersen wrote:
>
>  
>
>>Wow, I'm getting confused...The syntax Petr suggested does what I 
>>wanted, but things are stille wrong...Maybe a bug? Let me explain.
>>
>>I got two vectors:
>>
>>x = c(3, 3, 3, 4, 3, 4, 3, 4, 3, 4)
>>
>>y = c(5, 2, 5, 5, 2, 2, 5, 5, 4, 2)
>>
>>then I do the barplot you suggest
>>
>>barplot(rbind(table(x), table(y)), beside=T)
>>
>>but things are wrong(there is no bar for catagory "3") and I get an
>>error message: Warning message: 
>>number of columns of result
>>        not a multiple of vector length (arg 1) in:
>>        rbind(table(Quest1), table(Quest2))
>>
>>Any ideas?
>>    
>>
>
>If x and y are categorical variables, you should tell R so (i.e.,
>convert to a factor) and if both should have the same categories (i.e.,
>levels) you can supply this information as well:
>
>R> x <- factor(x, levels = 2:5)
>R> y <- factor(y, levels = 2:5)
>
>Then, table() knows which categories to use:
>
>R> rbind(x = table(x), y = table(y))
>  2 3 4 5
>x 0 6 4 0
>y 4 0 1 5
>
>and also the barplot() call given above will do the right thing.
>Z
>
>  
>
>>Petr Pikal wrote:
>>
>>    
>>
>>>Hi
>>>
>>>If I understand correctly
>>>
>>>barplot(rbind(table(x), table(y)), beside=T)
>>>
>>>does what you want.
>>>
>>>Cheers
>>>Petr
>>>
>>>
>>>
>>>On 18 Feb 2005 at 7:51, T Petersen wrote:
>>>
>>> 
>>>
>>>      
>>>
>>>>Almost. Catagories aren't stacked - I would like to see that x has 2
>>>>instances of "1" while y has 1 instance of "1". What's more, there
>>>>        
>>>>
>>>are>now TWO distinct barplots - the left one shows x, while the right
>>>one>shows y. I could live with that, but what I'd ideally want is to
>>>have>x and y beside each other for EACH catagory - so for catagory
>>>"1" you>could see taht there are more x's than y's (two x's versus
>>>one y). But>thanks for the help
>>>      
>>>
>>>>Mulholland, Tom wrote:
>>>>
>>>>   
>>>>
>>>>        
>>>>
>>>>>barplot(matrix(c(x,y),ncol = 2),beside=T)
>>>>>
>>>>>Does this help 
>>>>>
>>>>>?barplot notes
>>>>>
>>>>>height: either a vector or matrix of values describing the bars
>>>>>          
>>>>>
>>>which>>         make up the plot.  If 'height' is a vector, the plot
>>>      
>>>
>>>>>        consists of a sequence of rectangular bars with heights
>>>>>        given by the values in the vector.  If 'height' is a
>>>>>          
>>>>>
>>>matrix>>         and 'beside' is 'FALSE' then each bar of the plot
>>>      
>>>
>>>>>        corresponds to a column of 'height', with the values in
>>>>>          
>>>>>
>>>the>>         column giving the heights of stacked "sub-bars" making
>>>up>>         the bar.  If 'height' is a matrix and 'beside' is
>>>'TRUE',>>         then the values in each column are juxtaposed
>>>rather than>>         stacked.
>>>      
>>>
>>>>>
>>>>>
>>>>>     
>>>>>
>>>>>          
>>>>>
>>>>>>-----Original Message-----
>>>>>>From: T Petersen [mailto:Terji78 at yahoo.com]
>>>>>>Sent: Friday, 18 February 2005 1:35 PM
>>>>>>To: Kevin Wang
>>>>>>Cc: r-help at stat.math.ethz.ch
>>>>>>Subject: Re: [R] Barplot - Can't figure it out
>>>>>>
>>>>>>
>>>>>>Ups, it should of course be barplot() in my mail, not boxplot:-)
>>>>>>
>>>>>>Kevin Wang wrote:
>>>>>>
>>>>>>  
>>>>>>
>>>>>>       
>>>>>>
>>>>>>            
>>>>>>
>>>>>>>Hi,
>>>>>>>
>>>>>>>T Petersen wrote:
>>>>>>>
>>>>>>>    
>>>>>>>
>>>>>>>         
>>>>>>>
>>>>>>>              
>>>>>>>
>>>>>>>>Hi,
>>>>>>>>
>>>>>>>>I have two catagorical vectors like this;
>>>>>>>>
>>>>>>>>x = c(1, 2, 4, 2, 1)
>>>>>>>>y = c(2, 4, 2 ,4, 1)
>>>>>>>>
>>>>>>>>I want to set up a barplot with the catagories 1-4 
>>>>>>>>      
>>>>>>>>
>>>>>>>>           
>>>>>>>>
>>>>>>>>                
>>>>>>>>
>>>>>>horizontally  and 
>>>>>>  
>>>>>>
>>>>>>       
>>>>>>
>>>>>>            
>>>>>>
>>>>>>>>number of occurances vertically for each vector x,y. I've tried
>>>>>>>>
>>>>>>>>boxplot(table(x,y), beside=T)
>>>>>>>>
>>>>>>>>and
>>>>>>>>
>>>>>>>>boxplot(c(x,y), beside=T)
>>>>>>>>      
>>>>>>>>
>>>>>>>>           
>>>>>>>>
>>>>>>>>                
>>>>>>>>
>>>>>>>Have you tried barplot(), instead of boxplot()???
>>>>>>>
>>>>>>>Cheers,
>>>>>>>
>>>>>>>Kev
>>>>>>>
>>>>>>>    
>>>>>>>
>>>>>>>         
>>>>>>>
>>>>>>>              
>>>>>>>
>>>>>>______________________________________________
>>>>>>R-help at stat.math.ethz.ch mailing list
>>>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>PLEASE do read the posting guide! 
>>>>>>http://www.R-project.org/posting-guide.html
>>>>>>
>>>>>>  
>>>>>>
>>>>>>       
>>>>>>
>>>>>>            
>>>>>>
>>>>>______________________________________________
>>>>>R-help at stat.math.ethz.ch mailing list
>>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>PLEASE do read the posting guide!
>>>>>http://www.R-project.org/posting-guide.html
>>>>>
>>>>>
>>>>>
>>>>>     
>>>>>
>>>>>          
>>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide!
>>>>http://www.R-project.org/posting-guide.html
>>>>   
>>>>
>>>>        
>>>>
>>>Petr Pikal
>>>petr.pikal at precheza.cz
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide!
>>>http://www.R-project.org/posting-guide.html
>>>
>>> 
>>>
>>>      
>>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>>http://www.R-project.org/posting-guide.html
>>
>>    
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From p.dalgaard at biostat.ku.dk  Fri Feb 18 14:15:31 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 Feb 2005 14:15:31 +0100
Subject: [R] Two-factorial Huynh-Feldt-Test
In-Reply-To: <4215D2E0.5040606@cbs.mpg.de>
References: <4215A1BB.3020400@gmx.net> <x2r7je5iav.fsf@biostat.ku.dk>
	<4215D2E0.5040606@cbs.mpg.de>
Message-ID: <x2braic8lo.fsf@biostat.ku.dk>

Bela Bauer <belabauer at cbs.mpg.de> writes:

> Peter Dalgaard wrote:
> 
> >>The models I use for the anovas are the following:
> >>
> >>aov(vecData ~ (facWithin + facBetweenROI + facBetweenCond)^2)
> >>aov(vecData ~ facBetweenROI + facBetweenCond %in% facWithin +
> >>Error(facBetweenROI %in% facWithin))
> >>aov(vecData ~ facBetweenCond + facBetweenROI %in% facWithin +
> >>Error(facBetweenCond %in% facWithin))
> >>
> >>SAS seems to calculate the Huynh-Feldt test for the first and the
> >>second model. The SAS output is (for the second aov)
> >>
> >
> >Would you happen to know what logic SAS uses to recognize cases  where
> >the corrections apply?  I thought it only did it in the case of
> >repeated measurements, as in
> >
> >proc glm;
> >        model bmc2-bmc7=bmc1 grp / nouni;
> >        repeated time ...
> >
> 
> Yes, of course, the entire SAS script is available to me:
> PROC GLM;
> MODEL col1 col3 col5 col7 col9 col11 col13 col15 col17 col19 col21
> col23 =/nouni;
> repeated roi 6, ord 2/nom mean;
> TITLE 'ABDERUS lat ACC 300-500';
> 
> This script wasn't written by me and unfortunately, I don't know
> anything about SAS scripting, which makes it hard for me to follow the
> script.

OK, so it is basically isomorphic to the structure I had above. You
just have no grouping and no covariate but a 2x6 substructure in the
12 responses.
 
> >http://www.psych.upenn.edu/~baron/rpsych.pdf
> >
> Thanks for that link, it seems like a quite useful paper!
> 
> Now, I've tried to follow it and apply the steps to my own problem,
> but I came up with a H-F and G-G value that is still slightly
> different from the one that SAS calculates (I'll attach my code at the
> end of this mail). With my old code (see my last email), I get
> 0.4682799. With the new code, I get 0.4494588 as G-G epsilon and
> 0.6063626 for the H-F
> correction. I think that this corresponds (or should do so, rather) to
> the SAS value of  0.6333.
> Now, as you can see, the differences are small, but still there.
> The main difference between my two versions of the code are that in
> the old code (based on that book), a covariance matrix is calculcated
> for every condition and then these are averaged for the covariance
> matrix that is used in the H-F formula. The new code, on the other
> hand, averages the data over all conditions and then calculcates the
> covariance matrix for that, which could probably explain the
> differences. There also seems to be a difference between the original
> H-F correction and the algorithm that SAS uses; this is mentioned in
> the PDF, but they don't explain it any further. I suspect that this
> difference could be exactly the difference between my two code
> versions, but I don't know for sure.
> 
> Now, with the difference between my value and the one from SAS being
> so small, I suspect that there's only a very slight difference between
> the algorithms. Do you have any hints what these could be, or how I
> could go about investigating it?
> 
> Thanks a lot for your help!

My suspicion would be that it has something to do with calculating the
correction terms before or after contrast transformations (there must
be a coordinate-free version of the corrections?), but I can't grok
the details that easily. However, if you peek over on the R-devel
list, you will see that I was just about to get serious with
programming some of this stuff as methods for the "mlm" class. I think
you just volunteered to test the code...
 
        -p

> Bela Bauer
> 
> mtx <- NULL
> for (iROI in 1:length(unique( roi )) ) {
>   for (iSubj in 1:length(unique (subj )) ) {
>     mtx <- c(mtx,
>              mean(asa[subj==unique(subj)[iSubj] & roi==unique(roi)[iROI]],
>                   aoa[subj==unique(subj)[iSubj] & roi==unique(roi)[iROI]]))
>   }
> }
> mtx <- matrix(mtx,ncol=length(unique( roi )),byrow=F)
> 
> S <- var(mtx)
> 
> k <- 6
> D <- (k^2 * (mean(S) - mean(diag(S)))^2)
> N1 <- sum(S^2)
> N2 <- 2 * k * sum(apply(S, 1, mean)^2)
> N3 <- k^2 * mean(S)^2
> epsiGG <- D / ((k - 1) * (N1 - N2 + N3))
> epsiHF <- (10 * (k-1) * epsiGG - 2) / ((k-1) * ((10-1) - (k-1)*epsiGG))
> print(epsiGG)
> print(epsiHF)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ahenningsen at email.uni-kiel.de  Fri Feb 18 14:27:23 2005
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Fri, 18 Feb 2005 14:27:23 +0100
Subject: [R] Question about legend
In-Reply-To: <4215DBAB.4040402@statistik.uni-dortmund.de>
References: <4215BF5D.10807@ucla.edu>
	<200502181219.47923.ahenningsen@email.uni-kiel.de>
	<4215DBAB.4040402@statistik.uni-dortmund.de>
Message-ID: <200502181427.23769.ahenningsen@email.uni-kiel.de>

On Friday 18 February 2005 13:12, Uwe Ligges wrote:
> Arne Henningsen wrote:
> > type "?legend" (and please read the posting guide)
>
> I don't think it is that easy, but you have to tweak manually., e.g. by
> two calls to legend.
>
> Uwe Ligges

You are right, you have to call legend twice, e.g.

R> barplot(c(1:10))
R> lines(c(1:11),5*abs(sin(c(1:11))))
R> legend( 1.25, 10, c(" bar"), fill="grey", bty="n" )
R> legend( 1, 9.5, c("line"), lty=1, bty="n" )
R> rect(1,8.7,3,10)

Arne

> > On Friday 18 February 2005 11:11, Haiyong Xu wrote:
> >>Hi there,
> >>
> >>I made a plot with histogram and the curve of kernel density estimation
> >>together. The question is to add a legend to it. What I want is to use a
> >>small box with shade representing the histogram and a line representing
> >>the kernel estimation. Is there any way to implement this?
> >>
> >>Thanks a lot.
> >>
> >>Haiyong
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide!
> >>http://www.R-project.org/posting-guide.html
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From slist at oomvanlieshout.net  Fri Feb 18 14:33:48 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Fri, 18 Feb 2005 15:33:48 +0200
Subject: [R] Hosting a R Graph Gallery?
Message-ID: <4215EEBC.6070304@oomvanlieshout.net>

Dear R users,

Following some of the recent questions and discussions about the R 
plotting abilities, it occurred to me again that it would be very 
valuable to have an R graph gallery.

Eric Lecoutre made a very nice example in:
http://www.stat.ucl.ac.be/ISpersonnel/lecoutre/stats/fichiers/_gallery.pdf

It would be very useful to many beginners, but probably also advanced 
users of R, to have an overview of R graph types with graphical examples 
  and associated R code.

In order to facilitate the evolution of a large gallery, some sort of 
wiki environment might be most suitable, thus providing access to all 
users, but with limited maintenance costs for the provider.

Do others agree this could be a valuable resource? Would anybody have 
the resources to host such an R graph gallery?

Yours,

Sander Oom.

-- 
---------------------------------------------------------
Dr. Sander P. Oom
Animal, Plant and Environmental Sciences
University of the Witwatersrand
Private Bag 3
Wits 2050
South Africa

Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64

Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From jc.meyfredi at edhec.edu  Fri Feb 18 14:34:50 2005
From: jc.meyfredi at edhec.edu (MEYFREDI Jean-Christophe)
Date: Fri, 18 Feb 2005 14:34:50 +0100
Subject: [R] bivariate empirical cdf
Message-ID: <718BA1E837FCD2418DF74A783ECAAFB422198A@exclille2.EDHEC.ASSO.FR>

Dear R users,
 

	I'm trying to write a small function in order to compute empirical cumulative density function.All seems to work but when I try to plot the function, I always get error messages.
	 
	This is the function I use
	 
	mecdf<-function(u,v,z) {
	u=sort(u)
	v=sort(v)
	n=length(u)
	nb=0
	 
	for (i in seq(1,n)) {
	if (u[i]<z & v[i]<z) {
	nb<-nb+1
	}
	}
	nb=nb/n
	return(nb)
	}
	 
	In fact if I try to obtain mecdf(u,v,0.1) or mecdf(u,v,0.2), all is good, but with mecdf(u,v,c(0.1,0.2)), for example, I get errors and wrong results for both mecdf(u,v,0.1) and mecdf(u,v,0.2). I think that it consitutes the key point of my plot difficulty. Can someone help me ?
	 
	Best regards
	 
	JCM



From marginalutility at gmail.com  Fri Feb 18 14:51:46 2005
From: marginalutility at gmail.com (James Chapman)
Date: Fri, 18 Feb 2005 07:51:46 -0600
Subject: [R] R crashes on Repeated ODBC Queries
Message-ID: <4215f2e3.44419ef9.6f82.0567@smtp.gmail.com>

Hi,

I've been up all night trying to get the following to work. 

First here is the setup. I have a P4 1G mem running WinXP SP2, R 2.0.1
patched 2005-01-15, MySQL 4.1.7 and MyODBC 3.51.

I've been pulling data from MySQL using the code at the end of the post,
where start.date and end.date are chron objects and auction.type is a
string. I've been rushed so the code, especially the "CMPC.load" function,
is a little less then elegent.

The problem lies in the fact that R crashes, with an exit value of 5, after
looping through nine months or so of observations. I've tried different
values as the function arguments and it still happens at around nine
monthes, or about 270 loops through the "for (i in dates)" loop or around
500 calls to the ODBC connection. I say around since I can start the
function with the same inputs and it will crash at different months.

My question is this a problem in R or ODBC?

Thanks for your help,

James

"CMPC.load" <-
  function(start.date,end.date,auction.type){
    the.data <- NULL
    dates <- seq(from=start.date,to=end.date);
    for (i in dates){
      if (!is.weekend(i)){
        the.date <- day.list(i)
        tranches <- tranche.find(i,auction.type)
        if (tranches$num != 0){
          for (j in 1:dim(tranches$data)[1]){
            the.tranche <- tranches$data[j,]
            the.auc.date <- day.list(the.tranche$aucdate)
            the.mat.date <- day.list(the.tranche$matdate)
            the.is.date <- day.list(the.tranche$issdate)
            #get the data from a given auction/tranche
            cmnd <- paste("SELECT submitted_by_fi_id AS fiid , bid_yield AS
yld,bid_amt/100000000 AS amt,bid_allotted_amt/100000000 AS aamt,on_target as
target, on_rate as rate FROM official_bid,rates,auction WHERE
auction.auction_id = ",the.tranche$id," AND auction.auction_id =
official_bid.auction_id AND auction.auction_type_code like
'",auction.type,"%' AND rates.day =
'",sprintf('%i-%i-%i',the.auc.date[1],the.auc.date[2],the.auc.date[3]),"'
AND issue_date =
'",sprintf('%i-%i-%i',the.is.date[1],the.is.date[2],the.is.date[3]),"' AND
maturity_date =
'",sprintf('%i-%i-%i',the.mat.date[1],the.mat.date[2],the.mat.date[3]),"';",
sep="")
            tmp.data <- auction.connect(cmnd)
            if (!is.nan(tmp.data[1,1])){
              tmp.bidders <- unique(tmp.data$fiid)
              for (k in tmp.bidders){
                tmp.id.data <- tmp.data[tmp.data$fiid == k,]
                tmp.order <- order(tmp.id.data$yld,decreasing = T)
                tmp.id.data <- tmp.id.data[tmp.order,]
                tmp.id.data$bnum <- seq(from=1,along=tmp.id.data$yld)
                tmp.id.data$id <- the.tranche$id
                tmp.id.data$auc.date <- the.tranche$aucdate
                tmp.id.data$mat.date <- the.tranche$matdate
                tmp.id.data$the.is.date <- the.tranche$issdate
                the.data <- rbind(the.data,tmp.id.data)
              }
            }
          }
        }
      }
    }
    return(the.data)
  }


"tranche.find" <- function(the.date,auction.type){
  #This function returns a list containing the number of tranches in
  #an auciton as well as the issue date, the maturity dates, term codes and
IDs for
  #the auctions.
  the.dates <- day.list(the.date)
  cmnd <- paste("SELECT
auction.auction_id as id,
tranche.term_days as term,
tranche.issue_date as issdate,
tranche.term_type_code as termcode,
tranche.maturity_date as matdate,
auction.auction_date as aucdate
FROM
auction,tranche
WHERE
auction.auction_id = tranche.auction_id
AND
auction_type_code like '",auction.type,"%'
AND
auction.auction_date =
",sprintf("'%i-%i-%i'",the.dates[1],the.dates[2],the.dates[3]),";"
                ,sep="");
  the.output <- auction.connect(cmnd);
  #now see if we got anythin back
  if (length(the.output$id)){
    #if so count how many auctions there were
    num.auctions <- length(the.output$term)
    the.output$issdate <- chron(the.output$issdate,format='y-m-d')
    the.output$matdate <- chron(the.output$matdate,format='y-m-d')
    the.output$aucdate <- chron(the.output$aucdate,format='y-m-d')
  }
  else {
    num.auctions <- 0
  }
  the.return <- list(num=num.auctions,data=the.output)
  return(the.return)
}

#This is a wrapper to take care of the odbc connection
"auction.connect" <- function(query){
  auctions <- odbcConnect("auctions");
  the.output <- sqlQuery(auctions,query);
  odbcClose(auctions);
  return(the.output);
}



From buser at stat.math.ethz.ch  Fri Feb 18 15:16:17 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Fri, 18 Feb 2005 15:16:17 +0100
Subject: [R] bivariate empirical cdf
In-Reply-To: <718BA1E837FCD2418DF74A783ECAAFB422198A@exclille2.EDHEC.ASSO.FR>
References: <718BA1E837FCD2418DF74A783ECAAFB422198A@exclille2.EDHEC.ASSO.FR>
Message-ID: <16917.63665.660987.420185@stat.math.ethz.ch>

Hi

I changed you function a little bit, so there is no more conflict
in the if condition:

mecdf<-function(u,v,z) {
  u=sort(u)
  v=sort(v)
  n=length(u)
  m=length(z)
  nb <- numeric(m)

  for(j in seq(1,m)) {
    nb.temp=0
    for (i in seq(1,n)) {
      if (u[i]<z[j] & v[i]<z[j]) {
        nb.temp<-nb.temp+1
      }
    }
    nb[j]=nb.temp/n
  }
  return(nb)
}

Attention: I didn't look in detail what you did so you sould
check my changes if it is really what you're looking for !!

Best regards, 

Christoph

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C11
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-1-632-5414		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------

MEYFREDI Jean-Christophe writes:
 > Dear R users,
 >  
 > 
 > 	I'm trying to write a small function in order to compute empirical cumulative density function.All seems to work but when I try to plot the function, I always get error messages.
 > 	 
 > 	This is the function I use
 > 	 
 > 	mecdf<-function(u,v,z) {
 > 	u=sort(u)
 > 	v=sort(v)
 > 	n=length(u)
 > 	nb=0
 > 	 
 > 	for (i in seq(1,n)) {
 > 	if (u[i]<z & v[i]<z) {
 > 	nb<-nb+1
 > 	}
 > 	}
 > 	nb=nb/n
 > 	return(nb)
 > 	}
 > 	 
 > 	In fact if I try to obtain mecdf(u,v,0.1) or mecdf(u,v,0.2), all is good, but with mecdf(u,v,c(0.1,0.2)), for example, I get errors and wrong results for both mecdf(u,v,0.1) and mecdf(u,v,0.2). I think that it consitutes the key point of my plot difficulty. Can someone help me ?
 > 	 
 > 	Best regards
 > 	 
 > 	JCM
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From anne.kervahu at adria.tm.fr  Fri Feb 18 15:14:53 2005
From: anne.kervahu at adria.tm.fr (Kervahu Anne)
Date: Fri, 18 Feb 2005 15:14:53 +0100
Subject: [R] nls.regression syntax problem
Message-ID: <7AB23E9F7AF4D311854200C04F0136D001CD156F@SRV_QMP2.adrianet>

hi,
i try to apply a nls regression but i always have this error message
"Error in numericDeriv(form[[3]], names(ind), env) : 
        Missing value or an Infinity produced when evaluating the model"
i don't understand why it doesn't work

this is the programm that i run:

f <- function (x,p) {
u <- p[1]
v <- p[2]
w <- p[3]
(I((x*u)^v))+w    
}

x<-c(1:10)
y<-c(5,6,6.5,8,7,4,3,5,9,15)

r <- nls( y ~ f(x,c(a,b,d)), start=c(a=3, b=2.5, d=3) )

so, if you see a mistake can you contact me:

Anne KERVAHU
annekervahu at yahoo.fr



From dmb at mrc-dunn.cam.ac.uk  Fri Feb 18 15:47:24 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Fri, 18 Feb 2005 14:47:24 +0000 (GMT)
Subject: [R] barplot and ylim - display problems
Message-ID: <Pine.LNX.4.21.0502181437500.31704-100000@mail.mrc-dunn.cam.ac.uk>



The following single line of code shows what I am trying to do, and the
problem I am having...

barplot(c(101,102,103),ylim=c(100,103))

The 'xaxis' is missing, and the grey bars 'fall off' the plot area. This
is generally ugly, and I would like to trim the bars (ideally they would
have a ragged appearance to show that I am 'zooming in').

I can see why what I am trying to do is conceptually a bit tricky, as the
yaxis needs to be closly inspected to see the data in its propper context.
This is simply fixed by showing...

barplot(c(101,102,103))

However, I want to first show the data in its propper context, then 'zoom
in' to highlight the difference between the bars. 

I tried covering up the bottom of the chart with a rect() command, but it
wont draw ouside the area highlighted with the box command, for example 


barplot(c(101,102,103),ylim=c(100,103))
box()
rect(0.7,0,1.9,102.5,col="white")

So I can't work out how to stop bars falling off the end of the plot, so
my labels are being written on the bars.

How can I fix this?



From tom_colson at ncsu.edu  Fri Feb 18 15:58:26 2005
From: tom_colson at ncsu.edu (Thomas Colson)
Date: Fri, 18 Feb 2005 09:58:26 -0500
Subject: [R] Precompiled x86_64 Binaries?
In-Reply-To: <718BA1E837FCD2418DF74A783ECAAFB422198A@exclille2.EDHEC.ASSO.FR>
Message-ID: <200502181458.j1IEwHFR025547@uni05mr.unity.ncsu.edu>

Can't seem to find a x86_64 RPM for R on any of the DL mirrors. Does one
exist? 


 
From: Peter Dalgaard <p.dalgaard_at_biostat.ku.dk> 
Date: Fri 22 Oct 2004 - 23:00:41 EST


Prof Brian Ripley <ripley at stats.ox.ac.uk> writes: 

> R does not run under (beta) 64-bit Windows on x86_64 and we have no plans 
> to port it there, but it does run under 32-bit Windows on that processor. 
> I am sure it will run on several 64-bit operating systems on Apple 
> Macintosh's so-called G5 hardware -- I believe we have seen a report that 
> includes an in-progress version of MacOS 10.4 -- and also several 32-bit 
> ones. 


Re. Windows, the issue is that we're using the mingw32 toolkit. If there is
ever a mingw64, I'm sure we'd attempt a port almost immediately.   

> If you want a prebuilt version you are out of luck except for Debian Linux

> on Alpha or ia64, from a quick glance. 


Actually, I've rolled up an x86_64 RPM for SuSE 9.1 which should be en route
to CRAN. Detlef Steuer is awaiting delivery of a 64bit machine and plan to
maintain the SuSE version in the longer run. It's sitting in
http://www.biostat.ku.dk/~pd/R-suse9.1-x86_64 for now. 

In general, it seems to be quite trivial to take a SPEC or SRPM for the 32
bit version of a distribution and rebuild an RPM for 64 bits. 

However, you might prefer to install from the source tarball anyway if you
want to take advantage of high-performance libraries (some of which can not
be redistributed). 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907

______________________________________________
R-devel at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-devel


Tom Colson
Center for Earth Observation
North Carolina State University 
Raleigh, NC 27695
(919) 515 3434
(919) 673 8023
tom_colson at ncsu.edu

Online Calendar:
http://www4.ncsu.edu/~tpcolson



From ccleland at optonline.net  Fri Feb 18 15:58:19 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Fri, 18 Feb 2005 09:58:19 -0500
Subject: [R] barplot and ylim - display problems
In-Reply-To: <Pine.LNX.4.21.0502181437500.31704-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0502181437500.31704-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <4216028B.5040103@optonline.net>

How about approaching it this way?

 > barplot(c(1,2,3), ylim=c(0,3), yaxt="n")
 > axis(side=2, at=c(0,1,2,3), labels=seq(100,103,1))

Dan Bolser wrote:
> 
> The following single line of code shows what I am trying to do, and the
> problem I am having...
> 
> barplot(c(101,102,103),ylim=c(100,103))
> 
> The 'xaxis' is missing, and the grey bars 'fall off' the plot area. This
> is generally ugly, and I would like to trim the bars (ideally they would
> have a ragged appearance to show that I am 'zooming in').
> 
> I can see why what I am trying to do is conceptually a bit tricky, as the
> yaxis needs to be closly inspected to see the data in its propper context.
> This is simply fixed by showing...
> 
> barplot(c(101,102,103))
> 
> However, I want to first show the data in its propper context, then 'zoom
> in' to highlight the difference between the bars. 
> 
> I tried covering up the bottom of the chart with a rect() command, but it
> wont draw ouside the area highlighted with the box command, for example 
> 
> 
> barplot(c(101,102,103),ylim=c(100,103))
> box()
> rect(0.7,0,1.9,102.5,col="white")
> 
> So I can't work out how to stop bars falling off the end of the plot, so
> my labels are being written on the bars.
> 
> How can I fix this?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From MSchwartz at MedAnalytics.com  Fri Feb 18 16:02:53 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 18 Feb 2005 09:02:53 -0600
Subject: [R] barplot and ylim - display problems
In-Reply-To: <Pine.LNX.4.21.0502181437500.31704-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0502181437500.31704-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <1108738973.25867.15.camel@horizons.localdomain>

On Fri, 2005-02-18 at 14:47 +0000, Dan Bolser wrote:
> 
> The following single line of code shows what I am trying to do, and the
> problem I am having...
> 
> barplot(c(101,102,103),ylim=c(100,103))
> 
> The 'xaxis' is missing, and the grey bars 'fall off' the plot area. This
> is generally ugly, and I would like to trim the bars (ideally they would
> have a ragged appearance to show that I am 'zooming in').
> 
> I can see why what I am trying to do is conceptually a bit tricky, as the
> yaxis needs to be closly inspected to see the data in its propper context.
> This is simply fixed by showing...
> 
> barplot(c(101,102,103))
> 
> However, I want to first show the data in its propper context, then 'zoom
> in' to highlight the difference between the bars. 
> 
> I tried covering up the bottom of the chart with a rect() command, but it
> wont draw ouside the area highlighted with the box command, for example 
> 
> 
> barplot(c(101,102,103),ylim=c(100,103))
> box()
> rect(0.7,0,1.9,102.5,col="white")
> 
> So I can't work out how to stop bars falling off the end of the plot, so
> my labels are being written on the bars.
> 
> How can I fix this?

Dan,

Try this:

barplot(c(101,102,103),ylim=c(100,103), xpd = FALSE)

Note that the setting of par("xpd") clips the bars outside the plot
region.

See ?par for more information.

HTH,

Marc Schwartz



From ligges at statistik.uni-dortmund.de  Fri Feb 18 16:09:28 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 18 Feb 2005 16:09:28 +0100
Subject: [R] barplot and ylim - display problems
In-Reply-To: <Pine.LNX.4.21.0502181437500.31704-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0502181437500.31704-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <42160528.1070304@statistik.uni-dortmund.de>

Dan Bolser wrote:

> 
> The following single line of code shows what I am trying to do, and the
> problem I am having...
> 
> barplot(c(101,102,103),ylim=c(100,103))
> 
> The 'xaxis' is missing, and the grey bars 'fall off' the plot area. This
> is generally ugly, and I would like to trim the bars (ideally they would
> have a ragged appearance to show that I am 'zooming in').
> 
> I can see why what I am trying to do is conceptually a bit tricky, as the
> yaxis needs to be closly inspected to see the data in its propper context.
> This is simply fixed by showing...
> 
> barplot(c(101,102,103))

So you want to turn clipping on using xpd=FALSE, e.g. something like

   bp <- barplot(c(101,102,103), ylim=c(100,103), xpd=FALSE)
   axis(1, at=bp, labels=1:3)


Uwe Ligges




> However, I want to first show the data in its propper context, then 'zoom
> in' to highlight the difference between the bars. 
> 
> I tried covering up the bottom of the chart with a rect() command, but it
> wont draw ouside the area highlighted with the box command, for example 
> 
> 
> barplot(c(101,102,103),ylim=c(100,103))
> box()
> rect(0.7,0,1.9,102.5,col="white")
> 
> So I can't work out how to stop bars falling off the end of the plot, so
> my labels are being written on the bars.
> 
> How can I fix this?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Achim.Zeileis at wu-wien.ac.at  Fri Feb 18 16:13:59 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Fri, 18 Feb 2005 16:13:59 +0100
Subject: [R] barplot and ylim - display problems
In-Reply-To: <Pine.LNX.4.21.0502181437500.31704-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0502181437500.31704-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <20050218161359.0b50e165.Achim.Zeileis@wu-wien.ac.at>

I think a workaround, that will do what you want is:

barplot(c(101,102,103) - 100, offset = 100)

hth,
Z


On Fri, 18 Feb 2005 14:47:24 +0000 (GMT) Dan Bolser wrote:

> 
> 
> The following single line of code shows what I am trying to do, and
> the problem I am having...
> 
> barplot(c(101,102,103),ylim=c(100,103))
> 
> The 'xaxis' is missing, and the grey bars 'fall off' the plot area.
> This is generally ugly, and I would like to trim the bars (ideally
> they would have a ragged appearance to show that I am 'zooming in').
> 
> I can see why what I am trying to do is conceptually a bit tricky, as
> the yaxis needs to be closly inspected to see the data in its propper
> context. This is simply fixed by showing...
> 
> barplot(c(101,102,103))
> 
> However, I want to first show the data in its propper context, then
> 'zoom in' to highlight the difference between the bars. 
> 
> I tried covering up the bottom of the chart with a rect() command, but
> it wont draw ouside the area highlighted with the box command, for
> example 
> 
> 
> barplot(c(101,102,103),ylim=c(100,103))
> box()
> rect(0.7,0,1.9,102.5,col="white")
> 
> So I can't work out how to stop bars falling off the end of the plot,
> so my labels are being written on the bars.
> 
> How can I fix this?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Fri Feb 18 16:14:32 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 18 Feb 2005 15:14:32 +0000 (GMT)
Subject: [R] Precompiled x86_64 Binaries?
In-Reply-To: <200502181458.j1IEwHFR025547@uni05mr.unity.ncsu.edu>
References: <200502181458.j1IEwHFR025547@uni05mr.unity.ncsu.edu>
Message-ID: <Pine.LNX.4.61.0502181510420.17053@gannet.stats>

Yes, under redhat/fc3.  Did you forget to mention SuSE?

On Fri, 18 Feb 2005, Thomas Colson wrote:

> Can't seem to find a x86_64 RPM for R on any of the DL mirrors. Does one
> exist?
>
> From: Peter Dalgaard <p.dalgaard_at_biostat.ku.dk>
> Date: Fri 22 Oct 2004 - 23:00:41 EST
> Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:
>
>> R does not run under (beta) 64-bit Windows on x86_64 and we have no plans
>> to port it there, but it does run under 32-bit Windows on that processor.
>> I am sure it will run on several 64-bit operating systems on Apple
>> Macintosh's so-called G5 hardware -- I believe we have seen a report that
>> includes an in-progress version of MacOS 10.4 -- and also several 32-bit
>> ones.
>
> Re. Windows, the issue is that we're using the mingw32 toolkit. If there is
> ever a mingw64, I'm sure we'd attempt a port almost immediately.
>
>> If you want a prebuilt version you are out of luck except for Debian Linux
>> on Alpha or ia64, from a quick glance.
>
> Actually, I've rolled up an x86_64 RPM for SuSE 9.1 which should be en route
> to CRAN. Detlef Steuer is awaiting delivery of a 64bit machine and plan to
> maintain the SuSE version in the longer run. It's sitting in
> http://www.biostat.ku.dk/~pd/R-suse9.1-x86_64 for now.
>
> In general, it seems to be quite trivial to take a SPEC or SRPM for the 32
> bit version of a distribution and rebuild an RPM for 64 bits.
>
> However, you might prefer to install from the source tarball anyway if you
> want to take advantage of high-performance libraries (some of which can not
> be redistributed).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tom_colson at ncsu.edu  Fri Feb 18 16:24:55 2005
From: tom_colson at ncsu.edu (Tom Colson)
Date: Fri, 18 Feb 2005 10:24:55 -0500
Subject: [R] Precompiled x86_64 Binaries?
In-Reply-To: <Pine.LNX.4.61.0502181510420.17053@gannet.stats>
Message-ID: <200502181524.j1IFOpFS029647@uni05mr.unity.ncsu.edu>

 Yes, I meant to say Suse. 

I had tried Fedora....but I'm one of those who has to have bleeding edge
hardware...for which there are no working Fedora Drivers. Hence the switch
to Suse...so I can get video..so I can get a 64 Bit R running. 


All of this is just a "test". If we can get 64Bit R compiled and running
here...then we'll go to the Blade center with a procedure log on install and
repeat it on the Grid Computer. 

Thanks. 



-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Sent: Friday, February 18, 2005 10:15 AM
To: Thomas Colson
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Precompiled x86_64 Binaries?

Yes, under redhat/fc3.  Did you forget to mention SuSE?

On Fri, 18 Feb 2005, Thomas Colson wrote:

> Can't seem to find a x86_64 RPM for R on any of the DL mirrors. Does 
> one exist?
>
> From: Peter Dalgaard <p.dalgaard_at_biostat.ku.dk>
> Date: Fri 22 Oct 2004 - 23:00:41 EST
> Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:
>
>> R does not run under (beta) 64-bit Windows on x86_64 and we have no 
>> plans to port it there, but it does run under 32-bit Windows on that
processor.
>> I am sure it will run on several 64-bit operating systems on Apple 
>> Macintosh's so-called G5 hardware -- I believe we have seen a report 
>> that includes an in-progress version of MacOS 10.4 -- and also 
>> several 32-bit ones.
>
> Re. Windows, the issue is that we're using the mingw32 toolkit. If 
> there is ever a mingw64, I'm sure we'd attempt a port almost immediately.
>
>> If you want a prebuilt version you are out of luck except for Debian 
>> Linux on Alpha or ia64, from a quick glance.
>
> Actually, I've rolled up an x86_64 RPM for SuSE 9.1 which should be en 
> route to CRAN. Detlef Steuer is awaiting delivery of a 64bit machine 
> and plan to maintain the SuSE version in the longer run. It's sitting 
> in
> http://www.biostat.ku.dk/~pd/R-suse9.1-x86_64 for now.
>
> In general, it seems to be quite trivial to take a SPEC or SRPM for 
> the 32 bit version of a distribution and rebuild an RPM for 64 bits.
>
> However, you might prefer to install from the source tarball anyway if 
> you want to take advantage of high-performance libraries (some of 
> which can not be redistributed).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From cepardot at cable.net.co  Fri Feb 18 16:50:37 2005
From: cepardot at cable.net.co (=?iso-8859-1?Q?Campo_El=EDas_PARDO?=)
Date: Fri, 18 Feb 2005 10:50:37 -0500
Subject: [R] Contingency tables profiles
Message-ID: <004701c515d1$98f6dab0$a74176c8@PARDOBERNAL>

Thank for your help

I obtained profiles and I found mosaicplot as an interesting alternative.

I don't like my solution about legend in profiles graphics: I inserted empty 
extra columns in order to avoid tue superimposed of legend.

#Data
N <- matrix(0,3,6)
N[1,] <- c(7,7,5,0,4,4)
N[2,] <- c(0,0,0,5,5,5)
N[3,] <- c(4,4,0,0,3,0)
rownames(N) <- c("Ple1","Ple2","Ple3")
colnames(N) <- c("A","B","C","D","E","F")
N
# Row profiles
PF <- N/rowSums(N)
# Columns profiles
PC <- t(N)/rowSums(t(N))
# Graphics
par(mfrow=c(2,2))
barplot(cbind(cbind(t(PF),0),0),legend=colnames(F),density=100,cex.names=0.8)
barplot(cbind(cbind(cbind(t(PC),0),0),0),legend=rownames(F),density=100,cex.names=0.8)
mosaicplot(N,color=TRUE)
library(ade4)
table.cont(t(N),csize=4.5,col.labels=rownames(N))



Campo El?as  PARDO
cepardot at cable.net.co
cepardot at unal.edu.co 

From Achim.Zeileis at wu-wien.ac.at  Fri Feb 18 17:27:15 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Fri, 18 Feb 2005 17:27:15 +0100
Subject: [R] Contingency tables profiles
In-Reply-To: <004701c515d1$98f6dab0$a74176c8@PARDOBERNAL>
References: <004701c515d1$98f6dab0$a74176c8@PARDOBERNAL>
Message-ID: <20050218172715.3c5739cc.Achim.Zeileis@wu-wien.ac.at>

On Fri, 18 Feb 2005 10:50:37 -0500 Campo El?as PARDO wrote:

> Thank for your help
> 
> I obtained profiles and I found mosaicplot as an interesting
> alternative.
> 
> I don't like my solution about legend in profiles graphics: I inserted
> empty extra columns in order to avoid tue superimposed of legend.
> 
> #Data
> N <- matrix(0,3,6)
> N[1,] <- c(7,7,5,0,4,4)
> N[2,] <- c(0,0,0,5,5,5)
> N[3,] <- c(4,4,0,0,3,0)
> rownames(N) <- c("Ple1","Ple2","Ple3")
> colnames(N) <- c("A","B","C","D","E","F")
> N
> # Row profiles
> PF <- N/rowSums(N)

This could also be done by
  prop.table(N, 1)

> # Columns profiles
> PC <- t(N)/rowSums(t(N))

and this by
  prop.table(N, 2)

> # Graphics
> par(mfrow=c(2,2))
> barplot(cbind(cbind(t(PF),0),0),legend=colnames(F),density=100,cex.na

Note, that you could also do
  cbind(t(PF), 0, 0)

Instead of adding 0s, you could also set xlim.

> mes=0.8)
> barplot(cbind(cbind(cbind(t(PC),0),0),0),legend=rownames(F),density=1
> 00,cex.names=0.8)
> mosaicplot(N,color=TRUE)

Note that the mosaicplot is very similar to the stacked barplot (in fact
it is a generalization of stacked barplots).

R> mosaicplot(N, color = TRUE)

gives you the distribution of A-F conditional on Ple1-3, but it also
visualizes the marginal distribution of Ple1-3. If you should want to
ignore the latter, you could do

R> mosaicplot(prop.table(N, 1), color = TRUE)

and the latter is essentially equivalent to your barplot

R> barplot(t(PF), legend = TRUE, xlim = c(0.2, 4))

And, vice versa, if you want to visualize the conditional distribution
of Ple1-3 given A-F, you can do.

R> mosaicplot(t(N), color = TRUE)
R> mosaicplot(t(prop.table(N, 2)), color = TRUE)

hth,
Z

> library(ade4)
> table.cont(t(N),csize=4.5,col.labels=rownames(N))
> 
> 
> 
> Campo El?as  PARDO
> cepardot at cable.net.co
> cepardot at unal.edu.co 
>



From matthieu.cornec at gmail.com  Fri Feb 18 17:35:29 2005
From: matthieu.cornec at gmail.com (Matthieu Cornec)
Date: Fri, 18 Feb 2005 17:35:29 +0100
Subject: [R] Using time series and lm
Message-ID: <8a83e50005021808354a6f6687@mail.gmail.com>

Hello,

I apologize for this question that may has been asked a lot of times
but I could not go through it.

I create a multivariate time series containing NA values. 
I want to compute a linear regression and obtain a time serie for both
residuals and fitted values. I have tried the trick ts.intersect,
without success.

Could you help me out of this?
####
Example:

y<-ts(1:10+rnorm(10))
x<-ts(1:10)
datats<-cbind(y,lagx=lag(x))

Notice the datats could come directly from an imported file, that is
why I did not use ts.intersect(y,lagx=lag(x))

fit<-lm(y~lagx,data=datats,na.action=na.omit)

but how do I get a time serie of residuals instead of a vector residuals(fit)?
######

Matthieu Cornec



From p.dalgaard at biostat.ku.dk  Fri Feb 18 17:30:52 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 Feb 2005 17:30:52 +0100
Subject: [R] Precompiled x86_64 Binaries?
In-Reply-To: <200502181524.j1IFOpFS029647@uni05mr.unity.ncsu.edu>
References: <200502181524.j1IFOpFS029647@uni05mr.unity.ncsu.edu>
Message-ID: <x2y8dlbzk3.fsf@biostat.ku.dk>

"Tom Colson" <tom_colson at ncsu.edu> writes:

>  Yes, I meant to say Suse. 
> 
> I had tried Fedora....but I'm one of those who has to have bleeding edge
> hardware...for which there are no working Fedora Drivers. Hence the switch
> to Suse...so I can get video..so I can get a 64 Bit R running. 
> 
> 
> All of this is just a "test". If we can get 64Bit R compiled and running
> here...then we'll go to the Blade center with a procedure log on install and
> repeat it on the Grid Computer. 

It's fairly easy to build from source on SuSE. I've been doing it
regularly for almost a year. I might switch it over to Fedora though,
because it is having trouble with the software for my MP3 player...

One advantage of building from source is that you can incorporate a
fast BLAS.

You can even build your own RPM using Detlef's spec files. Just make
sure that you have all the prerequisites since he hasn't been too good
at putting build-depends in. (But now he seem to have caught the
Post-Thesis Workaholism Syndrome, so he might get around to it soon.
Or provide the 64bit RPMs.) 


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From matthieu.cornec at gmail.com  Fri Feb 18 17:39:55 2005
From: matthieu.cornec at gmail.com (Matthieu Cornec)
Date: Fri, 18 Feb 2005 17:39:55 +0100
Subject: [R] Using sweave
Message-ID: <8a83e500050218083947c91911@mail.gmail.com>

hello,

Suppose in Rnw file, I compute a numeric of name x containing the value 1.
In my tex file, I want to write 

Let x= "the real value of x"
so that I can see in my dvi file : Let x = 1,
with 1, the actual value of x written in a math environnement for example.

Thanks,

Matthieu



From ramasamy at cancer.org.uk  Fri Feb 18 17:41:00 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Fri, 18 Feb 2005 16:41:00 +0000
Subject: [R] Barplot - Can't figure it out
In-Reply-To: <4215E703.20503@yahoo.com>
References: <33F91FB3FDF42E4180428AC66A5CF30B02D3C9A5@afhex01.dpi.wa.gov.au>
	<4215D0A4.25614.D69692@localhost> <4215D8E8.1050108@yahoo.com>
	<20050218133612.34d2d7fe.Achim.Zeileis@wu-wien.ac.at>
	<4215E703.20503@yahoo.com>
Message-ID: <1108744860.5969.55.camel@ndmpc126.orc.ox.ac.uk>

Here is a generalisation of the function that others have suggested to
take take more than 2 vectors.

my.barplot <- function(...){

  my.list <- list(...)
  lev <- sort( unique( unlist(my.list) ) )
  tmp <- t(sapply( my.list, function(v) table(factor(v, levels=lev))) )
  barplot(tmp, beside=T)

}

w <- c(3, 3, 3, 4, 3, 4, 3, 4, 3, 4)
x <- c(5, 2, 5, 5, 2, 2, 5, 5, 4, 2)
y <- c(1, 2, 3, 0, 5, 0, 1, 2, 1, 2)
z <- sample( c(w,x,y,z), 50, replace=TRUE )

my.barplot( w, x, y, z )


Regards, Adai


On Fri, 2005-02-18 at 14:00 +0100, T Petersen wrote:
> Yeah, that's it. I have to catagorize the data AND tell R how many 
> catagories there are. It works perfectly now and I've learned some 
> more:-D Great.
> 
> 
> 
> Achim Zeileis wrote:
> 
> >On Fri, 18 Feb 2005 13:00:40 +0100 T Petersen wrote:
> >
> >  
> >
> >>Wow, I'm getting confused...The syntax Petr suggested does what I 
> >>wanted, but things are stille wrong...Maybe a bug? Let me explain.
> >>
> >>I got two vectors:
> >>
> >>x = c(3, 3, 3, 4, 3, 4, 3, 4, 3, 4)
> >>
> >>y = c(5, 2, 5, 5, 2, 2, 5, 5, 4, 2)
> >>
> >>then I do the barplot you suggest
> >>
> >>barplot(rbind(table(x), table(y)), beside=T)
> >>
> >>but things are wrong(there is no bar for catagory "3") and I get an
> >>error message: Warning message: 
> >>number of columns of result
> >>        not a multiple of vector length (arg 1) in:
> >>        rbind(table(Quest1), table(Quest2))
> >>
> >>Any ideas?
> >>    
> >>
> >
> >If x and y are categorical variables, you should tell R so (i.e.,
> >convert to a factor) and if both should have the same categories (i.e.,
> >levels) you can supply this information as well:
> >
> >R> x <- factor(x, levels = 2:5)
> >R> y <- factor(y, levels = 2:5)
> >
> >Then, table() knows which categories to use:
> >
> >R> rbind(x = table(x), y = table(y))
> >  2 3 4 5
> >x 0 6 4 0
> >y 4 0 1 5
> >
> >and also the barplot() call given above will do the right thing.
> >Z
> >
> >  
> >
> >>Petr Pikal wrote:
> >>
> >>    
> >>
> >>>Hi
> >>>
> >>>If I understand correctly
> >>>
> >>>barplot(rbind(table(x), table(y)), beside=T)
> >>>
> >>>does what you want.
> >>>
> >>>Cheers
> >>>Petr
> >>>
> >>>
> >>>
> >>>On 18 Feb 2005 at 7:51, T Petersen wrote:
> >>>
> >>> 
> >>>
> >>>      
> >>>
> >>>>Almost. Catagories aren't stacked - I would like to see that x has 2
> >>>>instances of "1" while y has 1 instance of "1". What's more, there
> >>>>        
> >>>>
> >>>are>now TWO distinct barplots - the left one shows x, while the right
> >>>one>shows y. I could live with that, but what I'd ideally want is to
> >>>have>x and y beside each other for EACH catagory - so for catagory
> >>>"1" you>could see taht there are more x's than y's (two x's versus
> >>>one y). But>thanks for the help
> >>>      
> >>>
> >>>>Mulholland, Tom wrote:
> >>>>
> >>>>   
> >>>>
> >>>>        
> >>>>
> >>>>>barplot(matrix(c(x,y),ncol = 2),beside=T)
> >>>>>
> >>>>>Does this help 
> >>>>>
> >>>>>?barplot notes
> >>>>>
> >>>>>height: either a vector or matrix of values describing the bars
> >>>>>          
> >>>>>
> >>>which>>         make up the plot.  If 'height' is a vector, the plot
> >>>      
> >>>
> >>>>>        consists of a sequence of rectangular bars with heights
> >>>>>        given by the values in the vector.  If 'height' is a
> >>>>>          
> >>>>>
> >>>matrix>>         and 'beside' is 'FALSE' then each bar of the plot
> >>>      
> >>>
> >>>>>        corresponds to a column of 'height', with the values in
> >>>>>          
> >>>>>
> >>>the>>         column giving the heights of stacked "sub-bars" making
> >>>up>>         the bar.  If 'height' is a matrix and 'beside' is
> >>>'TRUE',>>         then the values in each column are juxtaposed
> >>>rather than>>         stacked.
> >>>      
> >>>
> >>>>>
> >>>>>
> >>>>>     
> >>>>>
> >>>>>          
> >>>>>
> >>>>>>-----Original Message-----
> >>>>>>From: T Petersen [mailto:Terji78 at yahoo.com]
> >>>>>>Sent: Friday, 18 February 2005 1:35 PM
> >>>>>>To: Kevin Wang
> >>>>>>Cc: r-help at stat.math.ethz.ch
> >>>>>>Subject: Re: [R] Barplot - Can't figure it out
> >>>>>>
> >>>>>>
> >>>>>>Ups, it should of course be barplot() in my mail, not boxplot:-)
> >>>>>>
> >>>>>>Kevin Wang wrote:
> >>>>>>
> >>>>>>  
> >>>>>>
> >>>>>>       
> >>>>>>
> >>>>>>            
> >>>>>>
> >>>>>>>Hi,
> >>>>>>>
> >>>>>>>T Petersen wrote:
> >>>>>>>
> >>>>>>>    
> >>>>>>>
> >>>>>>>         
> >>>>>>>
> >>>>>>>              
> >>>>>>>
> >>>>>>>>Hi,
> >>>>>>>>
> >>>>>>>>I have two catagorical vectors like this;
> >>>>>>>>
> >>>>>>>>x = c(1, 2, 4, 2, 1)
> >>>>>>>>y = c(2, 4, 2 ,4, 1)
> >>>>>>>>
> >>>>>>>>I want to set up a barplot with the catagories 1-4 
> >>>>>>>>      
> >>>>>>>>
> >>>>>>>>           
> >>>>>>>>
> >>>>>>>>                
> >>>>>>>>
> >>>>>>horizontally  and 
> >>>>>>  
> >>>>>>
> >>>>>>       
> >>>>>>
> >>>>>>            
> >>>>>>
> >>>>>>>>number of occurances vertically for each vector x,y. I've tried
> >>>>>>>>
> >>>>>>>>boxplot(table(x,y), beside=T)
> >>>>>>>>
> >>>>>>>>and
> >>>>>>>>
> >>>>>>>>boxplot(c(x,y), beside=T)
> >>>>>>>>      
> >>>>>>>>
> >>>>>>>>           
> >>>>>>>>
> >>>>>>>>                
> >>>>>>>>
> >>>>>>>Have you tried barplot(), instead of boxplot()???
> >>>>>>>
> >>>>>>>Cheers,
> >>>>>>>
> >>>>>>>Kev
> >>>>>>>
> >>>>>>>    
> >>>>>>>
> >>>>>>>         
> >>>>>>>
> >>>>>>>              
> >>>>>>>
> >>>>>>______________________________________________
> >>>>>>R-help at stat.math.ethz.ch mailing list
> >>>>>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>>PLEASE do read the posting guide! 
> >>>>>>http://www.R-project.org/posting-guide.html
> >>>>>>
> >>>>>>  
> >>>>>>
> >>>>>>       
> >>>>>>
> >>>>>>            
> >>>>>>
> >>>>>______________________________________________
> >>>>>R-help at stat.math.ethz.ch mailing list
> >>>>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>PLEASE do read the posting guide!
> >>>>>http://www.R-project.org/posting-guide.html
> >>>>>
> >>>>>
> >>>>>
> >>>>>     
> >>>>>
> >>>>>          
> >>>>>
> >>>>______________________________________________
> >>>>R-help at stat.math.ethz.ch mailing list
> >>>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>PLEASE do read the posting guide!
> >>>>http://www.R-project.org/posting-guide.html
> >>>>   
> >>>>
> >>>>        
> >>>>
> >>>Petr Pikal
> >>>petr.pikal at precheza.cz
> >>>
> >>>______________________________________________
> >>>R-help at stat.math.ethz.ch mailing list
> >>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>PLEASE do read the posting guide!
> >>>http://www.R-project.org/posting-guide.html
> >>>
> >>> 
> >>>
> >>>      
> >>>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide!
> >>http://www.R-project.org/posting-guide.html
> >>
> >>    
> >>
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> >  
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From macq at llnl.gov  Fri Feb 18 18:03:47 2005
From: macq at llnl.gov (Don MacQueen)
Date: Fri, 18 Feb 2005 09:03:47 -0800
Subject: [R] Easy cut & paste from Excel to R?
In-Reply-To: <1108678164.4215161465ccc@webmail.lyon.inserm.fr>
References: <1108678164.4215161465ccc@webmail.lyon.inserm.fr>
Message-ID: <p06210200be3bbc39a47e@[128.115.153.6]>

I tried Ken's suggestion
    read.table(pipe("pbpaste"),header=TRUE)
on my Mac OS X system and it worked *without* generating any warning message.

If my experience represents the norm, and Ken's is the exception, it 
is so simple that no further contribution to R is needed, I would 
say. Thank you, Ken.

The method can also be to go the other way, using pbcopy instead of pbpaste.
Emulating an example found in R's help under ?'pipe':

>  test <- data.frame(a=1:3,b=letters[1:3])
>  zz <- pipe('pbcopy','w')
>  write.table(test,file=zz,sep='\t',row.names=FALSE)
>  close(zz)

Then in Excel (or any other Mac-native application) use the Paste command.


In the past I would get the warning message that Ken reports when I 
used read.delim() on tab-delimited files
created using Excel's "Save as tab delimited" option. Excel does not 
put a newline at the
end of the last line, and as a result R would generate that error 
message.  Excel still does not.
However, R now reads such files correctly, without generating the 
warning message.

pbpaste and pbcopy are included in the OS as distributed by Apple

[163]% which pbpaste
/usr/bin/pbpaste

(though perhaps only if one of the optional developer-related 
packages has been installed)
so these methods should be available to all Mac users of R, without 
any extra work on their part
(other than learning about them, that is).

By the way, there doesn't appear to be open connection left behind:

>   bah <- read.table(pipe('pbpaste'),header=TRUE)
>  dim(bah)
[1] 21  5
>  showConnections(all=TRUE)
   description class      mode text   isopen   can read can write
0 "stdin"     "terminal" "r"  "text" "opened" "yes"    "no"    
1 "stdout"    "terminal" "w"  "text" "opened" "no"     "yes"   
2 "stderr"    "terminal" "w"  "text" "opened" "no"     "yes"   


>  version
          _                       
platform powerpc-apple-darwin6.8.5
arch     powerpc                 
os       darwin6.8.5             
system   powerpc, darwin6.8.5    
status                           
major    2                       
minor    0.1                     
year     2004                    
month    11                      
day      15                      
language R                       

Mac OS  10.3.8

Excel 2004, version 11.1 (040909)

-Don

At 11:09 PM +0100 2/17/05, Ken Knoblauch wrote:
>Here is something quick & dirty for Mac that may be serviceable in
>some cases, while awaiting someone with greater understanding of
>programming connections than I have currently.
>
>With the following copied to the clipboard from Excell:
>H	T	Q	F
>1	2	3.3	a
>3	5	10.2	b
>5	9	11	A
>
>I tried in R:
>
>read.table(pipe("pbpaste"),header=TRUE)
>   H T    Q F
>1 1 2  3.3 a
>2 3 5 10.2 b
>3 5 9 11.0 A
>Warning message:
>incomplete final line found by readTableHeader on `pbpaste'
>>  str(read.table(pipe("pbpaste"),header=TRUE))
>`data.frame':	3 obs. of  4 variables:
>  $ H: int  1 3 5
>  $ T: int  2 5 9
>  $ Q: num  3.3 10.2 11
>  $ F: Factor w/ 3 levels "A","a","b": 2 3 1
>Warning message:
>incomplete final line found by readTableHeader on `pbpaste'
>
>I haven't been able to track down readTableHeader yet.  The warning
>occurs even without headers in the data.
>
>
>Quoting Prof Brian Ripley <ripley at stats.ox.ac.uk>:
>
>>  On Thu, 17 Feb 2005, Uwe Ligges wrote:
>>
>>  > Ken Knoblauch wrote:
>>  >
>>  >> I tried the interesting suggestion below, discussed in several postings
>>  >> yesterday on the help-list, on my Mac (0S 10.3.7) but could not get it
>>  to
>>  >> work, as shown in the tests indicated below.
>>  >>
>>  >>
>>  >>>>  read.table(file("clipboard"), sep="\t", dec=",")
>>  >
>>  > Connections to the clipboard are only available on Windows.
>>
>>  Ken is of course welcome to contribute them for MacOS X (or indeed for
>>  X11).
>>  People do take for granted the work the developers do to provide such
>>  things ....
>>
>>  --
>>  Brian D. Ripley,                  ripley at stats.ox.ac.uk
>  > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>>  University of Oxford,             Tel:  +44 1865 272861 (self)
>>  1 South Parks Road,                     +44 1865 272866 (PA)
>>  Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>
>
>
>
>____________________
>Ken Knoblauch
>Inserm U 371
>Cerveau et Vision
>18 avenue du Doyen Lepine
>69675 Bron cedex
>France
>tel: +33 (0)4 72 91 34 77
>fax: +33 (0)4 72 91 34 61
>portable: 06 84 10 64 10
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From paulojus at est.ufpr.br  Fri Feb 18 18:11:50 2005
From: paulojus at est.ufpr.br (Paulo Justiniano Ribeiro Jr)
Date: Fri, 18 Feb 2005 15:11:50 -0200 (BRST)
Subject: [R] Using sweave
In-Reply-To: <8a83e500050218083947c91911@mail.gmail.com>
References: <8a83e500050218083947c91911@mail.gmail.com>
Message-ID: <Pine.LNX.4.58L0.0502181510440.22441@est.ufpr.br>

I believe you want \Sexpr{}

for instance:
let $x = \Sexpr{x}$

or if you need rounding
let $x = \Sexpr{round(x, dig=4)}$



On Fri, 18 Feb 2005, Matthieu Cornec wrote:

> hello,
>
> Suppose in Rnw file, I compute a numeric of name x containing the value 1.
> In my tex file, I want to write
>
> Let x= "the real value of x"
> so that I can see in my dvi file : Let x = 1,
> with 1, the actual value of x written in a math environnement for example.
>
> Thanks,
>
> Matthieu
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

Paulo Justiniano Ribeiro Jr
LEG (Laborat?rio de Estat?stica e Geoinforma??o)
Departamento de Estat?stica
Universidade Federal do Paran?
Caixa Postal 19.081
CEP 81.531-990
Curitiba, PR  -  Brasil
Tel: (+55) 41 361 3573
Fax: (+55) 41 361 3141
e-mail: paulojus at est.ufpr.br
http://www.est.ufpr.br/~paulojus



From cdeclercq at nordnet.fr  Fri Feb 18 18:18:51 2005
From: cdeclercq at nordnet.fr (Christophe Declercq)
Date: Fri, 18 Feb 2005 18:18:51 +0100
Subject: [R] Using sweave
In-Reply-To: <8a83e500050218083947c91911@mail.gmail.com>
Message-ID: <MJELLLFFFCNHMHOOLCMBMEECDAAA.cdeclercq@nordnet.fr>



> De : r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]De la part de
> Matthieu Cornec
> Envoye : vendredi 18 fevrier 2005 17:40
>
> hello,
>
> Suppose in Rnw file, I compute a numeric of name x
> containing the value 1.
> In my tex file, I want to write
>
> Let x= "the real value of x"
> so that I can see in my dvi file : Let x = 1,
> with 1, the actual value of x written in a math
> environnement for example.

Try the \Sexpr{} command in LaTeX (see the Sweave manual). It's OK for
a scalar.

For example, something like this in the body of your LaTeX file should
do what you want:

<<>>=
x<-1
@

Let $x=\Sexpr{x}$


Christophe



From cdsmith at ksu.edu  Fri Feb 18 18:32:41 2005
From: cdsmith at ksu.edu (Christina D Smith)
Date: Fri, 18 Feb 2005 11:32:41 -0600
Subject: [R] export to text file
Message-ID: <1108747961.421626b927bc4@webmail.ksu.edu>

I'm trying to export a large data frame to a text file for permanent
storage.  The only thing I could find was the treeglia Package but that
didn't work.  Any suggestions?
Thanks!

Christina D Smith
PhD Student, GRA
Statistics Department
Kansas State University



From p.dalgaard at biostat.ku.dk  Fri Feb 18 18:31:34 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 Feb 2005 18:31:34 +0100
Subject: [R] Easy cut & paste from Excel to R?
In-Reply-To: <p06210200be3bbc39a47e@[128.115.153.6]>
References: <1108678164.4215161465ccc@webmail.lyon.inserm.fr>
	<p06210200be3bbc39a47e@[128.115.153.6]>
Message-ID: <x2u0o9bwqx.fsf@biostat.ku.dk>

Don MacQueen <macq at llnl.gov> writes:

> I tried Ken's suggestion
>     read.table(pipe("pbpaste"),header=TRUE)
> on my Mac OS X system and it worked *without* generating any warning message.
> 
> If my experience represents the norm, and Ken's is the exception, it
> is so simple that no further contribution to R is needed, I would say.
> Thank you, Ken.

My conjecture is that it only happens when there are fewer than 5 data
lines. 

We still need to sort out X11. Too bad that the xclip program isn't
ubiquitous. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From knoblauch at lyon.inserm.fr  Fri Feb 18 17:44:26 2005
From: knoblauch at lyon.inserm.fr (Ken Knoblauch)
Date: Fri, 18 Feb 2005 17:44:26 +0100
Subject: [R] Easy cut & paste from Excel to R?
Message-ID: <1108745066.42161b6ae24dc@webmail.lyon.inserm.fr>

In fact, I noticed today that when I copied from an Excell spreadsheet,
rather than just putting some text in a file, that it worked as you
say.

I also downloaded xclip and compiled it, which works just fine
on the Mac, and this mechanisms seems to work the same way as
pbpaste, in terms of generating or not warnings under the same
circumstances.  xclip has slightly different options than pbpaste
(e.g., you can access 3 different pasteboards) and it would seem to cover
the X11 situation.  So, it represents a solution that is 
slightly more general, as applying to both Mac and linux.


Quoting Don MacQueen <macq at llnl.gov>:

> I tried Ken's suggestion
>     read.table(pipe("pbpaste"),header=TRUE)
> on my Mac OS X system and it worked *without* generating any warning
> message.
> 
> If my experience represents the norm, and Ken's is the exception, it 
> is so simple that no further contribution to R is needed, I would 
> say. Thank you, Ken.
> 
> The method can also be to go the other way, using pbcopy instead of
> pbpaste.
> Emulating an example found in R's help under ?'pipe':
> 
> >  test <- data.frame(a=1:3,b=letters[1:3])
> >  zz <- pipe('pbcopy','w')
> >  write.table(test,file=zz,sep='\t',row.names=FALSE)
> >  close(zz)
> 
> Then in Excel (or any other Mac-native application) use the Paste command.
> 
> 
> In the past I would get the warning message that Ken reports when I 
> used read.delim() on tab-delimited files
> created using Excel's "Save as tab delimited" option. Excel does not 
> put a newline at the
> end of the last line, and as a result R would generate that error 
> message.  Excel still does not.
> However, R now reads such files correctly, without generating the 
> warning message.
> 
> pbpaste and pbcopy are included in the OS as distributed by Apple
> 
> [163]% which pbpaste
> /usr/bin/pbpaste
> 
> (though perhaps only if one of the optional developer-related 
> packages has been installed)
> so these methods should be available to all Mac users of R, without 
> any extra work on their part
> (other than learning about them, that is).
> 
> By the way, there doesn't appear to be open connection left behind:
> 
> >   bah <- read.table(pipe('pbpaste'),header=TRUE)
> >  dim(bah)
> [1] 21  5
> >  showConnections(all=TRUE)
>    description class      mode text   isopen   can read can write
> 0 "stdin"     "terminal" "r"  "text" "opened" "yes"    "no"    
> 1 "stdout"    "terminal" "w"  "text" "opened" "no"     "yes"   
> 2 "stderr"    "terminal" "w"  "text" "opened" "no"     "yes"   
> 
> 
> >  version
>           _                       
> platform powerpc-apple-darwin6.8.5
> arch     powerpc                 
> os       darwin6.8.5             
> system   powerpc, darwin6.8.5    
> status                           
> major    2                       
> minor    0.1                     
> year     2004                    
> month    11                      
> day      15                      
> language R                       
> 
> Mac OS  10.3.8
> 
> Excel 2004, version 11.1 (040909)
> 
> -Don
> 
> At 11:09 PM +0100 2/17/05, Ken Knoblauch wrote:
> >Here is something quick & dirty for Mac that may be serviceable in
> >some cases, while awaiting someone with greater understanding of
> >programming connections than I have currently.
> >
> >With the following copied to the clipboard from Excell:
> >H	T	Q	F
> >1	2	3.3	a
> >3	5	10.2	b
> >5	9	11	A
> >
> >I tried in R:
> >
> >read.table(pipe("pbpaste"),header=TRUE)
> >   H T    Q F
> >1 1 2  3.3 a
> >2 3 5 10.2 b
> >3 5 9 11.0 A
> >Warning message:
> >incomplete final line found by readTableHeader on `pbpaste'
> >>  str(read.table(pipe("pbpaste"),header=TRUE))
> >`data.frame':	3 obs. of  4 variables:
> >  $ H: int  1 3 5
> >  $ T: int  2 5 9
> >  $ Q: num  3.3 10.2 11
> >  $ F: Factor w/ 3 levels "A","a","b": 2 3 1
> >Warning message:
> >incomplete final line found by readTableHeader on `pbpaste'
> >
> >I haven't been able to track down readTableHeader yet.  The warning
> >occurs even without headers in the data.
> >
> >
> >Quoting Prof Brian Ripley <ripley at stats.ox.ac.uk>:
> >
> >>  On Thu, 17 Feb 2005, Uwe Ligges wrote:
> >>
> >>  > Ken Knoblauch wrote:
> >>  >
> >>  >> I tried the interesting suggestion below, discussed in several
> postings
> >>  >> yesterday on the help-list, on my Mac (0S 10.3.7) but could not get
> it
> >>  to
> >>  >> work, as shown in the tests indicated below.
> >>  >>
> >>  >>
> >>  >>>>  read.table(file("clipboard"), sep="\t", dec=",")
> >>  >
> >>  > Connections to the clipboard are only available on Windows.
> >>
> >>  Ken is of course welcome to contribute them for MacOS X (or indeed for
> >>  X11).
> >>  People do take for granted the work the developers do to provide such
> >>  things ....
> >>
> >>  --
> >>  Brian D. Ripley,                  ripley at stats.ox.ac.uk
> >  > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> >>  University of Oxford,             Tel:  +44 1865 272861 (self)
> >>  1 South Parks Road,                     +44 1865 272866 (PA)
> >>  Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> >>
> >
> >
> >
> >____________________
> >Ken Knoblauch
> >Inserm U 371
> >Cerveau et Vision
> >18 avenue du Doyen Lepine
> >69675 Bron cedex
> >France
> >tel: +33 (0)4 72 91 34 77
> >fax: +33 (0)4 72 91 34 61
> >portable: 06 84 10 64 10
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 
> -- 
> --------------------------------------
> Don MacQueen
> Environmental Protection Department
> Lawrence Livermore National Laboratory
> Livermore, CA, USA
> --------------------------------------
> 



____________________
Ken Knoblauch
Inserm U 371
Cerveau et Vision
18 avenue du Doyen Lepine
69675 Bron cedex
France
tel: +33 (0)4 72 91 34 77
fax: +33 (0)4 72 91 34 61
portable: 06 84 10 64 10



From sdavis2 at mail.nih.gov  Fri Feb 18 18:44:02 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Fri, 18 Feb 2005 12:44:02 -0500
Subject: [R] export to text file
In-Reply-To: <1108747961.421626b927bc4@webmail.ksu.edu>
References: <1108747961.421626b927bc4@webmail.ksu.edu>
Message-ID: <AD1BFEDB-81D4-11D9-8E0D-000D933565E8@mail.nih.gov>

Look at ?write.table.

Also, the data import/export manual on the r-project website is quite 
useful.

Sean

On Feb 18, 2005, at 12:32 PM, Christina D Smith wrote:

> I'm trying to export a large data frame to a text file for permanent
> storage.  The only thing I could find was the treeglia Package but that
> didn't work.  Any suggestions?
> Thanks!
>
> Christina D Smith
> PhD Student, GRA
> Statistics Department
> Kansas State University
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Fri Feb 18 19:01:38 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 18 Feb 2005 10:01:38 -0800
Subject: [R] export to text file
In-Reply-To: <1108747961.421626b927bc4@webmail.ksu.edu>
References: <1108747961.421626b927bc4@webmail.ksu.edu>
Message-ID: <42162D82.5080706@pdf.com>

      Did you try "write.table"?  If yes, what do you see as its 
deficiencies? 

      spencer graves

Christina D Smith wrote:

>I'm trying to export a large data frame to a text file for permanent
>storage.  The only thing I could find was the treeglia Package but that
>didn't work.  Any suggestions?
>Thanks!
>
>Christina D Smith
>PhD Student, GRA
>Statistics Department
>Kansas State University
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From francoisromain at free.fr  Fri Feb 18 19:22:41 2005
From: francoisromain at free.fr (Romain Francois)
Date: Fri, 18 Feb 2005 19:22:41 +0100
Subject: [R] Hosting a R Graph Gallery?
In-Reply-To: <4215EEBC.6070304@oomvanlieshout.net>
References: <4215EEBC.6070304@oomvanlieshout.net>
Message-ID: <42163271.5040906@free.fr>

Hello Sander,

That's a good idea and i am up to it.

Right now i am in an exam period, so it's not really the better time, 
give me a couple of weeks and i will come up with a specific format of R 
files to submit to me that i could post-process to generate html documents.
To my mind, those html files should show :

- the plot itself
+ Submitter(s)
        - web page
        - email (eventually protected, I don't know how to do it)
- Bibliographic references
- Required R packages
+ Commentaries
       - in english
       - and in any other languages

I'm open to any suggestion.

Romain.

Le 18.02.2005 14:33, Sander Oom a ?crit :

> Dear R users,
>
> Following some of the recent questions and discussions about the R 
> plotting abilities, it occurred to me again that it would be very 
> valuable to have an R graph gallery.
>
> Eric Lecoutre made a very nice example in:
> http://www.stat.ucl.ac.be/ISpersonnel/lecoutre/stats/fichiers/_gallery.pdf 
>
>
> It would be very useful to many beginners, but probably also advanced 
> users of R, to have an overview of R graph types with graphical 
> examples  and associated R code.
>
> In order to facilitate the evolution of a large gallery, some sort of 
> wiki environment might be most suitable, thus providing access to all 
> users, but with limited maintenance costs for the provider.
>
> Do others agree this could be a valuable resource? Would anybody have 
> the resources to host such an R graph gallery?
>
> Yours,
>
> Sander Oom.
>
-- 
Romain FRANCOIS : francoisromain at free.fr
page web : http://addictedtor.free.fr/  (en construction)
06 18 39 14 69 / 01 46 80 65 60
_______________________________________________________
Etudiant en 3eme ann?e
Institut de Statistique de l'Universit? de Paris (ISUP)
Fili?re Industrie et Services
http://www.isup.cicrp.jussieu.fr/



From dieter.menne at menne-biomed.de  Fri Feb 18 19:50:12 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Fri, 18 Feb 2005 18:50:12 +0000 (UTC)
Subject: [R] nls.regression syntax problem
References: <7AB23E9F7AF4D311854200C04F0136D001CD156F@SRV_QMP2.adrianet>
Message-ID: <loom.20050218T194403-18@post.gmane.org>

If you add a print(p), you will find that after a few iterations
u is negative which leads to problems with the power. It sometimes help write 
u=exp(p), and compute the log later. From a statistical point of view, I am not 
always happy with that solution, but even Pinheiro/Bates use it in the nlme 
samples. 
 
 f <- function (x,p) {
 print(p)
 u <- exp(p[1])
 v <- p[2]
 w <- p[3]
 (x*u)^v+w    # You don't need the I(( here.
 }


Nonlinear regression model
  model:  y ~ f(x, c(a, b, d)) 
   data:  parent.frame() 
        a         b         d 
-2.125873 12.781581  5.577249 
 residual sum-of-squares:  21.12331 
 
Dieter



From yhtong at u.washington.edu  Fri Feb 18 20:03:13 2005
From: yhtong at u.washington.edu (Yen H., Tong)
Date: Fri, 18 Feb 2005 11:03:13 -0800 (PST)
Subject: [R] Partial structural Change in STRUCCHANGE PACKAGE
Message-ID: <Pine.LNX.4.43.0502181103130.636@hymn01.u.washington.edu>

Hi,

I am using the Strucchange package in R to test for structural change in regression coeffcient.  Given a model y = b0 + b1*X + b2*Z, the Fstats test whether there is a change in both b1 and b2 over a time period.

Is there any way where I can restrict the test to hold b2 constant and test for break in only b1?  That is, instead of a pure structural change, could I test for partial structural change in only one of the coefficient estimate?

Warm Regards,

Yen



From dieter.menne at menne-biomed.de  Fri Feb 18 19:53:39 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Fri, 18 Feb 2005 18:53:39 +0000 (UTC)
Subject: [R] R crashes on Repeated ODBC Queries
References: <4215f2e3.44419ef9.6f82.0567@smtp.gmail.com>
Message-ID: <loom.20050218T195237-895@post.gmane.org>

> values as the function arguments and it still happens at around nine
> monthes, or about 270 loops through the "for (i in dates)" loop or around
> 500 calls to the ODBC connection. I say around since I can start the
> function with the same inputs and it will crash at different months.

Try to boil your example down. If it fails in a 10-liner, chances are better 
you get a response.

Dieter



From francoisromain at free.fr  Fri Feb 18 20:14:23 2005
From: francoisromain at free.fr (Romain Francois)
Date: Fri, 18 Feb 2005 20:14:23 +0100
Subject: [R] Partial structural Change in STRUCCHANGE PACKAGE
In-Reply-To: <Pine.LNX.4.43.0502181103130.636@hymn01.u.washington.edu>
References: <Pine.LNX.4.43.0502181103130.636@hymn01.u.washington.edu>
Message-ID: <42163E8F.8010904@free.fr>

Hello,

one way could be to compute the residuals of the regression y=b0 + b2*Z, 
let's call them U, and then test the structural change on the model 
U=c0+c1*X.
Maybe there's a better way.

Romain.

Le 18.02.2005 20:03, Yen H., Tong a ?crit :

> Hi,
>
> I am using the Strucchange package in R to test for structural change 
> in regression coeffcient.  Given a model y = b0 + b1*X + b2*Z, the 
> Fstats test whether there is a change in both b1 and b2 over a time 
> period.
>
> Is there any way where I can restrict the test to hold b2 constant and 
> test for break in only b1?  That is, instead of a pure structural 
> change, could I test for partial structural change in only one of the 
> coefficient estimate?
>
> Warm Regards,
>
> Yen

-- 
Romain FRANCOIS : francoisromain at free.fr
page web : http://addictedtor.free.fr/  (en construction)
06 18 39 14 69 / 01 46 80 65 60
_______________________________________________________
Etudiant en 3eme ann?e
Institut de Statistique de l'Universit? de Paris (ISUP)
Fili?re Industrie et Services
http://www.isup.cicrp.jussieu.fr/



From Achim.Zeileis at wu-wien.ac.at  Fri Feb 18 20:31:42 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Fri, 18 Feb 2005 20:31:42 +0100
Subject: [R] Partial structural Change in STRUCCHANGE PACKAGE
In-Reply-To: <Pine.LNX.4.43.0502181103130.636@hymn01.u.washington.edu>
References: <Pine.LNX.4.43.0502181103130.636@hymn01.u.washington.edu>
Message-ID: <20050218203142.45074fa4.Achim.Zeileis@wu-wien.ac.at>

On Fri, 18 Feb 2005 11:03:13 -0800 (PST) Yen H., Tong wrote:

> Hi,
> 
> I am using the Strucchange package in R to test for structural change
> in regression coeffcient.  Given a model y = b0 + b1*X + b2*Z, the
> Fstats test whether there is a change in both b1 and b2 over a time
> period.
> 
> Is there any way where I can restrict the test to hold b2 constant and
> test for break in only b1?  That is, instead of a pure structural
> change, could I test for partial structural change in only one of the
> coefficient estimate?

Not in Fstats(). Fstats() computes a sequence of Wald statistics such
that in the case of partial changes I would have to compute
partially segmented models which I haven't implemented because I haven't
seen a real application where a partial change approach is intuitive.
(How do you know that the other coefficients are really stable?)

However, you can compute a sequence of LM statistics using gefp() and
selecting the coefficients you want to assess via the parm argument. The
trick is that you only estimate the model once under the null and thus
avoid the problem of partially segmented models. But I haven't provided
an "efpFunctional" for the supLM test, yet. But if you want to use only
a certain trimming parameter, that is very easy to set up. Contact me
off-list, if you want to do so.
Z

> Warm Regards,
> 
> Yen
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From Achim.Zeileis at wu-wien.ac.at  Fri Feb 18 20:38:07 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Fri, 18 Feb 2005 20:38:07 +0100
Subject: [R] Partial structural Change in STRUCCHANGE PACKAGE
In-Reply-To: <42163E8F.8010904@free.fr>
References: <Pine.LNX.4.43.0502181103130.636@hymn01.u.washington.edu>
	<42163E8F.8010904@free.fr>
Message-ID: <20050218203807.27bffe55.Achim.Zeileis@wu-wien.ac.at>

On Fri, 18 Feb 2005 20:14:23 +0100 Romain Francois wrote:

> Hello,
> 
> one way could be to compute the residuals of the regression y=b0 +
> b2*Z, let's call them U, and then test the structural change on the
> model U=c0+c1*X.

The approach of Andrews (1993, Econometrica) would be preferrable where
you explicitely estimate partially segmented models with a breakpoint
shifted across the sample period. This woul usually lead to a varying
coefficient of Z as well.
But as I explained in my previous mail, this is not currently
implemented in strucchange. 

> Maybe there's a better way.

Personally, if I really believed in a partial change model, I would
compute a statistic (supLM, Cramer-von Mises, maximum, etc.) from the
corresponding cumulative score process(es) computed by gefp().
Z

> Romain.
> 
> Le 18.02.2005 20:03, Yen H., Tong a ?crit :
> 
> > Hi,
> >
> > I am using the Strucchange package in R to test for structural
> > change in regression coeffcient.  Given a model y = b0 + b1*X +
> > b2*Z, the Fstats test whether there is a change in both b1 and b2
> > over a time period.
> >
> > Is there any way where I can restrict the test to hold b2 constant
> > and test for break in only b1?  That is, instead of a pure
> > structural change, could I test for partial structural change in
> > only one of the coefficient estimate?
> >
> > Warm Regards,
> >
> > Yen
> 
> -- 
> Romain FRANCOIS : francoisromain at free.fr
> page web : http://addictedtor.free.fr/  (en construction)
> 06 18 39 14 69 / 01 46 80 65 60
> _______________________________________________________
> Etudiant en 3eme ann?e
> Institut de Statistique de l'Universit? de Paris (ISUP)
> Fili?re Industrie et Services
> http://www.isup.cicrp.jussieu.fr/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From outsourcing at pennieimaging.com  Fri Feb 18 21:07:25 2005
From: outsourcing at pennieimaging.com (Hellman Chica)
Date: Fri, 18 Feb 2005 15:07:25 -0500 (EST)
Subject: [R] Records Manager Contact
Message-ID: <1100502083532.1011165401102.14627.1.71441@scheduler>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050218/22af2cf4/attachment.pl

From francoisromain at free.fr  Fri Feb 18 20:44:57 2005
From: francoisromain at free.fr (Romain Francois)
Date: Fri, 18 Feb 2005 20:44:57 +0100
Subject: [R] Hosting a R Graph Gallery?
In-Reply-To: <12AE52872B5C5348BE5CF47C707FF53A5024A2@rhosvr02.rhotrading.com>
References: <12AE52872B5C5348BE5CF47C707FF53A5024A2@rhosvr02.rhotrading.com>
Message-ID: <421645B9.3030009@free.fr>

Le 18.02.2005 20:18, davidr at rhotrading.com a ?crit :

>Please say you forgot to include putting the R code in also. 
>  
>
Obviously !!
Great suggestion.  I didn't thought of it ;-)
Romain.

>Otherwise we will all be just wistful graphics appreciators, rather than generators. ;-)
>David Reiner
>
>
>-----Original Message-----
>From: Romain Francois [mailto:francoisromain at free.fr] 
>Sent: Friday, February 18, 2005 12:23 PM
>To: sander at oomvanlieshout.net; RHELP
>Subject: Re: [R] Hosting a R Graph Gallery?
>
>Hello Sander,
>
>That's a good idea and i am up to it.
>
>Right now i am in an exam period, so it's not really the better time, 
>give me a couple of weeks and i will come up with a specific format of R 
>files to submit to me that i could post-process to generate html documents.
>To my mind, those html files should show :
>
>- the plot itself
>+ Submitter(s)
>        - web page
>        - email (eventually protected, I don't know how to do it)
>- Bibliographic references
>- Required R packages
>+ Commentaries
>       - in english
>       - and in any other languages
>
>I'm open to any suggestion.
>
>Romain.
>
>Le 18.02.2005 14:33, Sander Oom a ?crit :
>
>  
>
>>Dear R users,
>>
>>Following some of the recent questions and discussions about the R 
>>plotting abilities, it occurred to me again that it would be very 
>>valuable to have an R graph gallery.
>>
>>Eric Lecoutre made a very nice example in:
>>http://www.stat.ucl.ac.be/ISpersonnel/lecoutre/stats/fichiers/_gallery.pdf 
>>
>>
>>It would be very useful to many beginners, but probably also advanced 
>>users of R, to have an overview of R graph types with graphical 
>>examples  and associated R code.
>>
>>In order to facilitate the evolution of a large gallery, some sort of 
>>wiki environment might be most suitable, thus providing access to all 
>>users, but with limited maintenance costs for the provider.
>>
>>Do others agree this could be a valuable resource? Would anybody have 
>>the resources to host such an R graph gallery?
>>
>>Yours,
>>
>>Sander Oom.
>>
>>    
>>

-- 
Romain FRANCOIS : francoisromain at free.fr
page web : http://addictedtor.free.fr/  (en construction)
06 18 39 14 69 / 01 46 80 65 60
_______________________________________________________
Etudiant en 3eme ann?e
Institut de Statistique de l'Universit? de Paris (ISUP)
Fili?re Industrie et Services
http://www.isup.cicrp.jussieu.fr/



From yhtong at u.washington.edu  Fri Feb 18 20:53:25 2005
From: yhtong at u.washington.edu (Yen H., Tong)
Date: Fri, 18 Feb 2005 11:53:25 -0800 (PST)
Subject: [R] Partial structural Change in STRUCCHANGE PACKAGE
In-Reply-To: <20050218203807.27bffe55.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <Pine.LNX.4.43.0502181153250.7641@hymn01.u.washington.edu>

Hi,

Thanks for all the thoughtful replies.  Let me think on this further.

Warm Regards,

Yen




On Fri, 18 Feb 2005, Achim Zeileis wrote:

> On Fri, 18 Feb 2005 20:14:23 +0100 Romain Francois wrote:
>
>> Hello,
>>
>> one way could be to compute the residuals of the regression y=b0 +
>> b2*Z, let's call them U, and then test the structural change on the
>> model U=c0+c1*X.
>
> The approach of Andrews (1993, Econometrica) would be preferrable where
> you explicitely estimate partially segmented models with a breakpoint
> shifted across the sample period. This woul usually lead to a varying
> coefficient of Z as well.
> But as I explained in my previous mail, this is not currently
> implemented in strucchange.
>
>> Maybe there's a better way.
>
> Personally, if I really believed in a partial change model, I would
> compute a statistic (supLM, Cramer-von Mises, maximum, etc.) from the
> corresponding cumulative score process(es) computed by gefp().
> Z
>
>> Romain.
>>
>> Le 18.02.2005 20:03, Yen H., Tong a ??crit :
>>
>>> Hi,
>>>
>>> I am using the Strucchange package in R to test for structural
>>> change in regression coeffcient.  Given a model y = b0 + b1*X +
>>> b2*Z, the Fstats test whether there is a change in both b1 and b2
>>> over a time period.
>>>
>>> Is there any way where I can restrict the test to hold b2 constant
>>> and test for break in only b1?  That is, instead of a pure
>>> structural change, could I test for partial structural change in
>>> only one of the coefficient estimate?
>>>
>>> Warm Regards,
>>>
>>> Yen
>>
>> --
>> Romain FRANCOIS : francoisromain at free.fr
>> page web : http://addictedtor.free.fr/  (en construction)
>> 06 18 39 14 69 / 01 46 80 65 60
>> _______________________________________________________
>> Etudiant en 3eme ann??e
>> Institut de Statistique de l'Universit?? de Paris (ISUP)
>> Fili??re Industrie et Services
>> http://www.isup.cicrp.jussieu.fr/
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>



From munroe.9 at osu.edu  Fri Feb 18 22:27:51 2005
From: munroe.9 at osu.edu (Darla Munroe)
Date: Fri, 18 Feb 2005 16:27:51 -0500
Subject: [R] single equation IV estimation in R using systemfit
Message-ID: <200502182130.j1ILUaYA015654@defang8.net.ohio-state.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050218/fd0a8390/attachment.pl

From knoblauch at lyon.inserm.fr  Fri Feb 18 21:48:31 2005
From: knoblauch at lyon.inserm.fr (Ken Knoblauch)
Date: Fri, 18 Feb 2005 21:48:31 +0100
Subject: [R] Easy cut & paste from Excel to R?
Message-ID: <1108759711.4216549f39d8c@webmail.lyon.inserm.fr>

You are right.  The warning disappears at exactly five lines of data,
not including the header.  

Well, how often do you come across a data.frame with less than 5 rows
and too many covariates to enter by hand?

kk


Quoting Peter Dalgaard <p.dalgaard at biostat.ku.dk>:

> Don MacQueen <macq at llnl.gov> writes:
> 
> > I tried Ken's suggestion
> >     read.table(pipe("pbpaste"),header=TRUE)
> > on my Mac OS X system and it worked *without* generating any warning
> message.
> 
> My conjecture is that it only happens when there are fewer than 5 data
> lines. 
> 
> We still need to sort out X11. Too bad that the xclip program isn't
> ubiquitous. 
> 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907
> 



____________________
Ken Knoblauch
Inserm U 371
Cerveau et Vision
18 avenue du Doyen Lepine
69675 Bron cedex
France
tel: +33 (0)4 72 91 34 77
fax: +33 (0)4 72 91 34 61
portable: 06 84 10 64 10



From KeLin at mdanderson.org  Fri Feb 18 22:41:25 2005
From: KeLin at mdanderson.org (KeLin@mdanderson.org)
Date: Fri, 18 Feb 2005 15:41:25 -0600
Subject: [R] help on deleting NAs
In-Reply-To: <4215AA4F.31575.40D143@localhost>
Message-ID: <OF6BFF407D.265DEFED-ON86256FAC.0075F83F-86256FAC.00772613@mdacc.tmc.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050218/eeaaa9e7/attachment.pl

From ozric at web.de  Fri Feb 18 22:52:44 2005
From: ozric at web.de (Christian Schulz)
Date: Fri, 18 Feb 2005 22:52:44 +0100
Subject: [R] R & bash script
Message-ID: <421663AC.8020001@web.de>

Hi

how is it possible to use more than one command when i'm
didn't want use R CMD BATCH for specific reason?

$ echo "(x<-1:10)" | R --vanilla
works

$ echo "(x<-1:10 ;y<-20:30 ;lm(y ~ x))" | R --vanilla
works not.

Is it further possible using  bash variables like $i  from a loop
in the bash echo call  i.e.   dm$x$i$k <- 
read.data("dmdata$x$i$k.dat",header=T)

many thanks for a starting point!
christian



From mrennie at utm.utoronto.ca  Fri Feb 18 23:05:44 2005
From: mrennie at utm.utoronto.ca (Michael Rennie)
Date: Fri, 18 Feb 2005 17:05:44 -0500
Subject: [R] extracting F, df and r squared using "sapply"?
Message-ID: <6.1.0.6.0.20050218165226.01a760c0@mail.utm.utoronto.ca>


Hi, All

How does one remove relevant information from a regression output besides 
just the coefficients?

I've been able to modify the example given under "help(by)" to give me some 
additional information, but not everything I need.

If you adjust the call statement from what is listed by adding the summary 
statement like so:

tmp <- by(warpbreaks, tension, function(x) summary(lm(breaks ~ wool, data=x)))
      sapply(tmp, coef)

I am able to get the coefficients and the p-value (as well as 5 other rows 
of information I don't need, corresponding to St. error of the 
coefficients, t-values and the p-value of the intercept, but I can delete 
all this later).

If you look at tmp, all the F statistics, r-squared values, etc. are there, 
but is there an easy way to get at them reported in a tabulated form?

Specifically, is there a simple way I can adjust the "sapply" command to 
extract the F statistic, r squared, and degrees of freedom (in addition to 
the coefficients) and have them appear as rows in the output? Ultimately, I 
have 200+ groups that I am running regressions on in the same dataset, so I 
need some way to extract the relevant information in a tabulated form.

and no smart-alec comments about bonferonni corrections :)

Thanks,

Mike


Michael Rennie
Ph.D. Candidate
University of Toronto at Mississauga
3359 Mississauga Rd. N.
Mississauga, ON  L5L 1C6
Ph: 905-828-5452  Fax: 905-828-3792



From francoisromain at free.fr  Fri Feb 18 23:16:26 2005
From: francoisromain at free.fr (Romain Francois)
Date: Fri, 18 Feb 2005 23:16:26 +0100
Subject: [R] R & bash script
In-Reply-To: <421663AC.8020001@web.de>
References: <421663AC.8020001@web.de>
Message-ID: <4216693A.9080809@free.fr>

Le 18.02.2005 22:52, Christian Schulz a ?crit :

> Hi
>
> how is it possible to use more than one command when i'm
> didn't want use R CMD BATCH for specific reason?
>
> $ echo "(x<-1:10)" | R --vanilla
> works
>
> $ echo "(x<-1:10 ;y<-20:30 ;lm(y ~ x))" | R --vanilla
> works not.
>
The following works for me:

echo "x<-1:10 ;y<-21:30 ;lm(y ~ x)" | R --vanilla

Romain

> Is it further possible using  bash variables like $i  from a loop
> in the bash echo call  i.e.   dm$x$i$k <- 
> read.data("dmdata$x$i$k.dat",header=T)
>
> many thanks for a starting point!
> christian
>
-- 
Romain FRANCOIS : francoisromain at free.fr
page web : http://addictedtor.free.fr/  (en construction)
06 18 39 14 69 / 01 46 80 65 60
_______________________________________________________
Etudiant en 3eme ann?e
Institut de Statistique de l'Universit? de Paris (ISUP)
Fili?re Industrie et Services
http://www.isup.cicrp.jussieu.fr/



From p.dalgaard at biostat.ku.dk  Fri Feb 18 23:30:13 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 Feb 2005 23:30:13 +0100
Subject: [R] R & bash script
In-Reply-To: <421663AC.8020001@web.de>
References: <421663AC.8020001@web.de>
Message-ID: <x2sm3tjybu.fsf@biostat.ku.dk>

Christian Schulz <ozric at web.de> writes:

> Hi
> 
> how is it possible to use more than one command when i'm
> didn't want use R CMD BATCH for specific reason?
> 
> $ echo "(x<-1:10)" | R --vanilla
> works
> 
> $ echo "(x<-1:10 ;y<-20:30 ;lm(y ~ x))" | R --vanilla
> works not.

It would probably help to use proper R syntax:

> (x<-1:10 ;y<-20:30 ;lm(y ~ x))
Error: syntax error

What were those outer parentheses supposed to be good for?
 
> Is it further possible using  bash variables like $i  from a loop
> in the bash echo call  i.e.   dm$x$i$k <-
> read.data("dmdata$x$i$k.dat",header=T)

Yes. But it is not an R issue, so look up the details in the bash
manual.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From helprhelp at gmail.com  Fri Feb 18 23:36:29 2005
From: helprhelp at gmail.com (WeiWei Shi)
Date: Fri, 18 Feb 2005 16:36:29 -0600
Subject: [R] gbm
Message-ID: <cdf8178305021814364b406588@mail.gmail.com>

Hi, there:

I am always experiencing the scalability of some R packages. This
time, I am trying gbm to do adaboosting on my project. Initially I
tried to grow trees by using rpart on a dataset with 200 variables and
30,000 observations. Now, I am thinking if I can apply adaboosting on
it.

I am wondering if here is anyone who did a similar thing before and
can provide some sample codes. Also any comments on the scalability
and feasiblity is welcome.  Also, is there any limitation on the
requirement of data, like any categorical variable cannot have more
than 32 levels. Is there anything like that?

Thanks in advance,

Ed



From WWei at mdanderson.org  Sat Feb 19 00:08:00 2005
From: WWei at mdanderson.org (WWei@mdanderson.org)
Date: Fri, 18 Feb 2005 17:08:00 -0600
Subject: [R] Contrast in GEE
Message-ID: <OF0FB24FEB.8FF3A793-ON86256FAC.007EEE1D-86256FAC.007F1358@mdacc.tmc.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050218/6b417ea0/attachment.pl

From pkleiber at honlab.nmfs.hawaii.edu  Sat Feb 19 00:23:25 2005
From: pkleiber at honlab.nmfs.hawaii.edu (Pierre Kleiber)
Date: Fri, 18 Feb 2005 13:23:25 -1000
Subject: [R] R & bash script
In-Reply-To: <421663AC.8020001@web.de>
References: <421663AC.8020001@web.de>
Message-ID: <421678ED.8020109@honlab.nmfs.hawaii.edu>

One way to run multiple R commands within a bash script is with a "here 
document".  See http://www.tldp.org/LDP/abs/html/here-docs.html

Here is an excerpt of a bash script showing its use -- notice that several 
bash variables are referenced within it:

#!/bin/bash
              .
              .
R --slave --vanilla --quiet --no-save  <<EEE
#  ---edit so path to R-graphics/.RData is correct---
attach("/home/mfcl/R-graphics/.RData")
if( $keep ) {
   junk <- paste("$1",".medley.png",sep="")
   print(paste("saving plotfile",junk))
} else {
   junk <-  "$tempfile"
}
png(file=junk,height=$ht,width=$wd)  # open plot file
plotmedley("$1")                # do the plot
dum <- dev.off()                # close file
system(paste("display",junk))   # display it
EEE
              .
              .
 

Cheers, Pierre

Christian Schulz wrote:
> Hi
> 
> how is it possible to use more than one command when i'm
> didn't want use R CMD BATCH for specific reason?
> 
> $ echo "(x<-1:10)" | R --vanilla
> works
> 
> $ echo "(x<-1:10 ;y<-20:30 ;lm(y ~ x))" | R --vanilla
> works not.
> 
> Is it further possible using  bash variables like $i  from a loop
> in the bash echo call  i.e.   dm$x$i$k <- 
> read.data("dmdata$x$i$k.dat",header=T)
> 
> many thanks for a starting point!
> christian
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 

-- 
-----------------------------------------------------------------
Pierre Kleiber, Ph.D       Email: pkleiber at honlab.nmfs.hawaii.edu
Fishery Biologist           Tel: 808 983-5399 / 808 737-7544 (hm)
NOAA FISHERIES - Honolulu Laboratory         Fax: 808 983-2902
2570 Dole St., Honolulu, HI 96822-2396
-----------------------------------------------------------------
  "God could have told Moses about galaxies and mitochondria and
   all.  But behold... It was good enough for government work."



From gregr at rand.org  Sat Feb 19 01:03:09 2005
From: gregr at rand.org (Ridgeway, Greg)
Date: Fri, 18 Feb 2005 16:03:09 -0800
Subject: [R] RE: gbm
Message-ID: <461D139C576A2C49B0C4720EF89EC164BF4532@smmail4.rand.org>


> I am always experiencing the scalability of some R packages. This
> time, I am trying gbm to do adaboosting on my project. Initially I
> tried to grow trees by using rpart on a dataset with 200 variables and
> 30,000 observations. Now, I am thinking if I can apply adaboosting on
> it.

R seems to be particularly slow in general when having a wide dataset (like your 200 variables) and using formula interfaces. I seem to remember that calls to model.frame() are particularly slow. The gbm package also offers gbm.fit() (which gbm() itself uses) that avoids model.frame(). It does not have a formula interface and takes a little more work up front to organize the data. With gbm.fit() and a bit of patience you should be able to fit your model.

> I am wondering if here is anyone who did a similar thing before and
> can provide some sample codes. Also any comments on the scalability
> and feasiblity is welcome.  Also, is there any limitation on the
> requirement of data, like any categorical variable cannot have more
> than 32 levels. Is there anything like that?

gbm() currently allows for categorical predictors with up to 256 levels. There's no particular reason it is set to that and could be increased if needed. Just increase k_cMaxClasses in line 11 of node_search.cpp, recompile, and install. If I get sufficient complaints about the 256 barrier I'll change it.

Greg Ridgeway
RAND Statistics Group

--------------------

This email message is for the sole use of the intended recipient(s) and
may contain privileged information. Any unauthorized review, use,
disclosure or distribution is prohibited. If you are not the intended
recipient, please contact the sender by reply email and destroy all copies
of the original message.



From ozric at web.de  Sat Feb 19 01:03:49 2005
From: ozric at web.de (Christian Schulz)
Date: Sat, 19 Feb 2005 01:03:49 +0100
Subject: [R] R & bash script
In-Reply-To: <421678ED.8020109@honlab.nmfs.hawaii.edu>
References: <421663AC.8020001@web.de> <421678ED.8020109@honlab.nmfs.hawaii.edu>
Message-ID: <42168265.8000202@web.de>

echo "x<-1:10 ;y<-21:30 ;lm(y ~ x)" | R --vanilla

..uups counting is sometimes difficult, or to late :-( , but many
thanks for this link and example.
regards, christian

Pierre Kleiber wrote:

> One way to run multiple R commands within a bash script is with a 
> "here document".  See http://www.tldp.org/LDP/abs/html/here-docs.html
>
> Here is an excerpt of a bash script showing its use -- notice that 
> several bash variables are referenced within it:
>
> #!/bin/bash
>              .
>              .
> R --slave --vanilla --quiet --no-save  <<EEE
> #  ---edit so path to R-graphics/.RData is correct---
> attach("/home/mfcl/R-graphics/.RData")
> if( $keep ) {
>   junk <- paste("$1",".medley.png",sep="")
>   print(paste("saving plotfile",junk))
> } else {
>   junk <-  "$tempfile"
> }
> png(file=junk,height=$ht,width=$wd)  # open plot file
> plotmedley("$1")                # do the plot
> dum <- dev.off()                # close file
> system(paste("display",junk))   # display it
> EEE
>              .
>              .
>
>
> Cheers, Pierre
>
> Christian Schulz wrote:
>
>> Hi
>>
>> how is it possible to use more than one command when i'm
>> didn't want use R CMD BATCH for specific reason?
>>
>> $ echo "(x<-1:10)" | R --vanilla
>> works
>>
>> $ echo "(x<-1:10 ;y<-20:30 ;lm(y ~ x))" | R --vanilla
>> works not.
>>
>> Is it further possible using  bash variables like $i  from a loop
>> in the bash echo call  i.e.   dm$x$i$k <- 
>> read.data("dmdata$x$i$k.dat",header=T)
>>
>> many thanks for a starting point!
>> christian
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>



From parkhurs at ariel.ucs.indiana.edu  Sat Feb 19 01:03:18 2005
From: parkhurs at ariel.ucs.indiana.edu (David Parkhurst)
Date: Fri, 18 Feb 2005 19:03:18 -0500
Subject: [R] scales argument in lattice plots
Message-ID: <42168246.4060905@ariel.ucs.indiana.edu>

I'm using R 2.0.1 under windows XP.  I'd like to produce an xyplot with
lattice, in which the x axis shows normally (with tics and numbers), but
there are neither tics nor numbers for y.  (I'm using layout=c(1,10), 
and ylab="".)

 From the html help for xyplot, it appears I need to use the scales
argument, but I don't fully understand the description there, about
how to treat the x and y axes differently.

I'd much appreciate any help.  Please reply directly, as I don't
subscribe to the list.

Thank you.

David Parkhurst



From deepayan at stat.wisc.edu  Sat Feb 19 01:27:55 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Fri, 18 Feb 2005 18:27:55 -0600
Subject: [R] scales argument in lattice plots
In-Reply-To: <42168246.4060905@ariel.ucs.indiana.edu>
References: <42168246.4060905@ariel.ucs.indiana.edu>
Message-ID: <200502181827.55525.deepayan@stat.wisc.edu>

On Friday 18 February 2005 18:03, David Parkhurst wrote:
> I'm using R 2.0.1 under windows XP.  I'd like to produce an xyplot
> with lattice, in which the x axis shows normally (with tics and
> numbers), but there are neither tics nor numbers for y.  (I'm using
> layout=c(1,10), and ylab="".)
>
>  From the html help for xyplot, it appears I need to use the scales
> argument, but I don't fully understand the description there, about
> how to treat the x and y axes differently.

You need

scales = list(y = list(draw = FALSE))

-Deepayan



From sfalcon at fhcrc.org  Sat Feb 19 01:56:25 2005
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Fri, 18 Feb 2005 16:56:25 -0800
Subject: [R] Comment on loadURL: should default to mode="wb"
Message-ID: <20050219005625.GR29138@gopher5.fhcrc.org>

So the help for loadURL says:

     'loadURL' is a convenience wrapper which downloads a file, loads
     it and deletes the downloaded copy.

Trying to load an rda file on Windows (XP, R-devel) I was surprised
when I received an error about input being corrupted with LF
replaced by CR.

The fix was to specify mode="wb" to loadURL.  This gets passed to
download.file and the right thing happens... 

On unix platforms it doesn't matter whether mode="w" or mode="wb".

Will loadURL ever do anything useful on Windows without passing
mode="wb"?  If not, I think changing the default (in loadURL) to
mode="wb" would be a nice improvement.

Best,

+ seth



From ggrothendieck at myway.com  Sat Feb 19 04:15:36 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 19 Feb 2005 03:15:36 +0000 (UTC)
Subject: [R] Using time series and lm
References: <8a83e50005021808354a6f6687@mail.gmail.com>
Message-ID: <loom.20050219T041145-134@post.gmane.org>

Matthieu Cornec <matthieu.cornec <at> gmail.com> writes:
> I create a multivariate time series containing NA values. 
> I want to compute a linear regression and obtain a time serie for both
> residuals and fitted values. I have tried the trick ts.intersect,
> without success.
> 
> Could you help me out of this?
> ####
> Example:
> 
> y<-ts(1:10+rnorm(10))
> x<-ts(1:10)
> datats<-cbind(y,lagx=lag(x))

Are you sure that you want lag(x) here?  lag(x) moves the time scale
one unit earlier so this will predict a past y based on a future x.
I think you probably meant lag(x, -1).    (I have continued to use
lag(x) below for consitency with your post.)

> 
> Notice the datats could come directly from an imported file, that is
> why I did not use ts.intersect(y,lagx=lag(x))
> 
> fit<-lm(y~lagx,data=datats,na.action=na.omit)
> 
> but how do I get a time serie of residuals instead of a vector residuals
(fit)?

# Define this function:
plain2ts <- function(x, ts.) ts(x, start = start(ts.), freq = frequency(ts.))

# It can be used like this:
datats.na <- na.omit(datats)
datats.fit <- lm(y ~ lagx, datats.na)
datats.resid <- plain2ts(resid(datats.fit), datats.na)

# Also note that using the zoo package one can directly perform
# lm on ts objects and it will automatically align them provided
# you surround the formula with I(...):

library(zoo)
fit2 <- lm(I(y ~ lag(x))) # y and x are the ts objects in your post

however, you would still have to use the previous method to convert
the residuals back to ts class as that is not currently handled.
For more on zoo:

library(zoo)
vignette("zoo")



From ggrothendieck at myway.com  Sat Feb 19 04:28:04 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 19 Feb 2005 03:28:04 +0000 (UTC)
Subject: [R] Hosting a R Graph Gallery?
References: <4215EEBC.6070304@oomvanlieshout.net>
Message-ID: <loom.20050219T041909-785@post.gmane.org>

Sander Oom <slist <at> oomvanlieshout.net> writes:

: 
: Dear R users,
: 
: Following some of the recent questions and discussions about the R 
: plotting abilities, it occurred to me again that it would be very 
: valuable to have an R graph gallery.
: 
: Eric Lecoutre made a very nice example in:
: http://www.stat.ucl.ac.be/ISpersonnel/lecoutre/stats/fichiers/_gallery.pdf

Paul Murrell also has an extensive set of images:

   http://www.stat.auckland.ac.nz/~paul/RGraphics/rgraphics.html

: It would be very useful to many beginners, but probably also advanced 
: users of R, to have an overview of R graph types with graphical examples 
:   and associated R code.
: 
: In order to facilitate the evolution of a large gallery, some sort of 
: wiki environment might be most suitable, thus providing access to all 
: users, but with limited maintenance costs for the provider.
: 
: Do others agree this could be a valuable resource? Would anybody have 
: the resources to host such an R graph gallery?


Note that Detlef Steuer set up an R wiki at:

   http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl?RwikiHome

that could be used as it does support images:

   http://www.usemod.com/cgi-bin/wiki.pl?TextFormattingRules/WikiImages



From phdhwang at gmail.com  Sat Feb 19 06:22:35 2005
From: phdhwang at gmail.com (Kum-Hoe Hwang)
Date: Sat, 19 Feb 2005 14:22:35 +0900
Subject: [R] best analysis method : for time series ans cross sectional data
Message-ID: <b040cbb005021821225f255fca@mail.gmail.com>

Howdy

What I 'd like to analyze with a large data on building permits is to find
time series effect of urban policy on buildings as well as
cross-sectional effects in any. In 1990 the specialZone urban policy
was introduced. I guess that the effects of this specialZone policy
would be different from countys. There are counties that do not
welcome this specialZone forced to design it.

One of the important aims is to find 1) time series effect using Dummy
variable,  2) cross-sectional effects using specialZones variable
below.

The data has items like year(1970-2000), floorSpace, county,
specialZones agianst permitting large buildings. specialZones have
been designed after 1990.
(Dummy = 1 after 1990, Dummy =0 before 1990)

I have tried three methods, such as
 lm(floorSpace ~ county, specialZones, Dummy), 
 glm(floorSpace ~ county, specialZones, Dummy),
 aov(floorSpace ~ county, specialZones, Dummy).

What I am focusing on is best method among lm, glm, aov or others not
siginificant results.

I have wasted  too  much time for this. I welcome your comments.

Thanks a lot,

-- 
Kum-Hoe Hwang, Ph.D.

Kyonggi Research Institute, Korea (ROK)
(Urban Planning and GIS)
Phone : 82-31-250-3283
Email : phdhwang at gmail.com



From ripley at stats.ox.ac.uk  Sat Feb 19 07:42:03 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 19 Feb 2005 06:42:03 +0000 (GMT)
Subject: [R] Comment on loadURL: should default to mode="wb"
In-Reply-To: <20050219005625.GR29138@gopher5.fhcrc.org>
References: <20050219005625.GR29138@gopher5.fhcrc.org>
Message-ID: <Pine.LNX.4.61.0502190640240.1764@gannet.stats>

On Fri, 18 Feb 2005, Seth Falcon wrote:

> So the help for loadURL says:
>
>     'loadURL' is a convenience wrapper which downloads a file, loads
>     it and deletes the downloaded copy.
>
> Trying to load an rda file on Windows (XP, R-devel) I was surprised
> when I received an error about input being corrupted with LF
> replaced by CR.
>
> The fix was to specify mode="wb" to loadURL.  This gets passed to
> download.file and the right thing happens...
>
> On unix platforms it doesn't matter whether mode="w" or mode="wb".
>
> Will loadURL ever do anything useful on Windows without passing
> mode="wb"?  If not, I think changing the default (in loadURL) to
> mode="wb" would be a nice improvement.

Yes of course, as almost all Web files are text and the default is correct 
for them.

Why are you using an out-dated interface anyway?  See the examples for 
loadURL!

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sat Feb 19 07:56:19 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 19 Feb 2005 06:56:19 +0000 (GMT)
Subject: [R] Comment on loadURL: should default to mode="wb"
In-Reply-To: <Pine.LNX.4.61.0502190640240.1764@gannet.stats>
References: <20050219005625.GR29138@gopher5.fhcrc.org>
	<Pine.LNX.4.61.0502190640240.1764@gannet.stats>
Message-ID: <Pine.LNX.4.61.0502190646470.1764@gannet.stats>

A bit more explanation.

On Sat, 19 Feb 2005, Prof Brian Ripley wrote:

> On Fri, 18 Feb 2005, Seth Falcon wrote:
>
>> So the help for loadURL says:
>> 
>>     'loadURL' is a convenience wrapper which downloads a file, loads
>>     it and deletes the downloaded copy.
>> 
>> Trying to load an rda file on Windows (XP, R-devel) I was surprised
>> when I received an error about input being corrupted with LF
>> replaced by CR.

That *is* described on the help page for loadURL.

>> The fix was to specify mode="wb" to loadURL.  This gets passed to
>> download.file and the right thing happens...
>> 
>> On unix platforms it doesn't matter whether mode="w" or mode="wb".
>> 
>> Will loadURL ever do anything useful on Windows without passing
>> mode="wb"?  If not, I think changing the default (in loadURL) to
>> mode="wb" would be a nice improvement.

On Windows.

> Yes of course, as almost all Web files are text and the default is correct 
> for them.

That means ASCII=TRUE saves.  When loadURL was written, they could be 
transferred as text file.

> Why are you using an out-dated interface anyway?  See the examples for 
> loadURL!

The modern interface does work on Windows:

> load(url("http://www.stats.ox.ac.uk/pub/bdr/hills.rda"))

no error (and I have removed the file).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Ted.Harding at nessie.mcc.ac.uk  Sat Feb 19 09:53:58 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 19 Feb 2005 08:53:58 -0000 (GMT)
Subject: [R] R & bash script
In-Reply-To: <421663AC.8020001@web.de>
Message-ID: <XFMail.050219085358.Ted.Harding@nessie.mcc.ac.uk>

On 18-Feb-05 Christian Schulz wrote:
> how is it possible to use more than one command when i'm
> didn't want use R CMD BATCH for specific reason?
> 
> $ echo "(x<-1:10)" | R --vanilla
> works
> 
> $ echo "(x<-1:10 ;y<-20:30 ;lm(y ~ x))" | R --vanilla
> works not.

Two things wrong with the above:

1. Use y<-21:30 with x<-1:10 otherwise "variable lengths differ"!

2. While (x<-1:10) returns the value of the assignment, i.e.
   [1]  1  2  3  4  5  6  7  8  9 10
   (which I suppose you wanted to see, as a check),
   even (x<-1:10 ;y<-20:30) is a syntax error in R, and
   (x<-1:10 ;y<-20:30 ;lm(y ~ x)) is an even bigger one!

You can emulate completely the typing in of commands by using
the "-e" option to echo, which allows you to use escape
sequences in the string, so that you can insert "\n" where
you would usually press Return. So:

echo -e "(x<-1:10) \n (y<-21:30) \n lm(y~x)" | R --vanilla

will work fine; and here the "(...)" cause R to print out
confirmation of the assignments "x<-1:10" and "y<-1:10"
which normally you perhaps wouldn't want, so the "production"
version of the above would be

echo -e "x<-1:10 \n y<-21:30 \n lm(y~x)" | R --vanilla

> Is it further possible using  bash variables like $i  from a loop
> in the bash echo call  i.e.
>   dm$x$i$k <- read.data("dmdata$x$i$k.dat",header=T)

Of course it is, as in

export LIM="10"
echo -e "x<-1:$LIM \n y<-21:30 \n lm(y~x)" | R --vanilla

but just make sure that you escape "$" when you need it
as part of an R syntax, e.g. when accessing named components
of a list. Thus:

echo -e "x<-1:$LIM \n y<-21:30 \n lm(y~x)\$fitted.values" | R --vanilla

Compare with what happens when you don't escape the "$", i.e.

echo -e "x<-1:$LIM \n y<-21:30 \n lm(y~x)$fitted.values" | R --vanilla

You need to take similar precautions for all shell metacharacters
which might occur in an R statement (see "man bash" for these).

Hoping this helps,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 19-Feb-05                                       Time: 08:53:58
------------------------------ XFMail ------------------------------



From engle at unr.nevada.edu  Sat Feb 19 10:17:14 2005
From: engle at unr.nevada.edu (mark engle)
Date: Sat, 19 Feb 2005 01:17:14 -0800
Subject: [R] Problems installing quantreg
Message-ID: <200502190117.15081.engle@unr.nevada.edu>

Hi All,
 I'm trying to install the quantreg library on my MEPIS box (debian-based 
linux).  When I try >install.packages("quantreg") it fails to loads and 
errors with: /usr/bin/ld: cannot find -lblas-3.  I tried installing a ton of 
packages to fix the missing part but the only thing I manage to do it cause 
problems with another shared library and had to re-install a couple of 
packages to even get R to run again (It's pretty late here and I'm just too 
tired to remember all the packages I messed with) .

As a work around I tried downloading an .rpm for the package and converting it 
to a .deb using alien.  I then installed the .deb through kpackage.  It 
installed without any complaints and I can run the library command with 
quantreg.  However when I try to run a quantreg command (in this case, rq) at 
the R prompt, I get: "Error: Object "rq" not found".  This suggests to me 
that the package hasn't really installed itself or isn't working properly.

Does anyone have any ideas?  I'd really like to be able to use this library 
with R.  In fact at the moment R doesn't do me much good without it.
Thanks.

-ME



From dieter.menne at menne-biomed.de  Sat Feb 19 12:33:37 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Sat, 19 Feb 2005 11:33:37 +0000 (UTC)
Subject: [R] extracting F, df and r squared using "sapply"?
References: <6.1.0.6.0.20050218165226.01a760c0@mail.utm.utoronto.ca>
Message-ID: <loom.20050219T123017-564@post.gmane.org>

> 
> If you look at tmp, all the F statistics, r-squared values, etc. are there, 
> but is there an easy way to get at them reported in a tabulated form?
> 

Try:

tb=by(warpbreaks, tension,
  function(x) {
    sumlm=summary(lm(breaks ~ wool, data=x))
    c(sumlm$df,F=sumlm$fstatistic)
  }
  )
do.call("rbind",tb)

and don't forget to give the df columns a name.

> and no smart-alec comments about bonferonni corrections :)

Use package (or library ???... just kidding) multcomp.

No smart-alec comments about packages and libraries, please.


Dieter



From mailpuls at gmx.net  Sat Feb 19 12:48:35 2005
From: mailpuls at gmx.net (mailpuls@gmx.net)
Date: Sat, 19 Feb 2005 12:48:35 +0100
Subject: [R] Warnings by functions mean(), median()
Message-ID: <42172793.4080908@gmx.net>

Hello,

following functions doesnt work correct with my data: median(), geo.mean().

My datafiles contain more than 10.000 lines and six columns from a 
flow-cytometer-measurment. I need the arithmetic and geometric mean and 
median. For the calculation of the geometric mean i wrote following 
function:

     fix(geo.mean)

     function(x)
     {
         n<-length(x)
         gm<-prod(x)^(1/n)
         return(gm)
     }

The function median() error tells me "need numeric data". The data are 
numeric. The function geo.mean() gave out "[1] NaN". What are the 
reasons and what are the solutions?

I'am a newbie and need urgently information.
Thanks.

Here is an short output with the results:

9997   385.42   68.54   9.82  124.09  23.93  138.24
9998   342.89   73.65 133.35 1134.19 345.99 1876.88
9999   316.23   76.35  48.26  421.70 129.80  873.79
10000  291.64  103.66   6.85  107.46  26.42  189.38
10001    0.00    0.00   0.00    0.00   0.00    0.00
 > mean(data)
       FSC       SSC       FL1       FL2      FL32       FL4
375.94880  73.76219  50.73413 434.42837 110.06393 637.34980
 > geo.mean(data)
[1] NaN
 > median(data)
Error in median(data) : need numeric data
 >



From sekemp at glam.ac.uk  Sat Feb 19 13:30:46 2005
From: sekemp at glam.ac.uk (Kemp S E (Comp))
Date: Sat, 19 Feb 2005 12:30:46 -0000
Subject: [R] hessian and standard errors
Message-ID: <0BA7EE4D4646E0409D458D347C508B783C6238@MAILSERV1.uni.glam.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050219/3ef2c463/attachment.pl

From kjetil at acelerate.com  Sat Feb 19 13:49:17 2005
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Sat, 19 Feb 2005 08:49:17 -0400
Subject: [R] Warnings by functions mean(), median()
In-Reply-To: <42172793.4080908@gmx.net>
References: <42172793.4080908@gmx.net>
Message-ID: <421735CD.6050103@acelerate.com>

mailpuls at gmx.net wrote:

> Hello,
>
> following functions doesnt work correct with my data: median(), 
> geo.mean().
>
> My datafiles contain more than 10.000 lines and six columns from a 
> flow-cytometer-measurment. I need the arithmetic and geometric mean 
> and median. For the calculation of the geometric mean i wrote 
> following function:
>
>     fix(geo.mean)
>
>     function(x)
>     {
>         n<-length(x)
>         gm<-prod(x)^(1/n)

This is probably what gives the NaN below.

exp(mean(log(x)))

would be more to the point.

Kjetil

>         return(gm)
>     }
>
> The function median() error tells me "need numeric data". The data are 
> numeric. The function geo.mean() gave out "[1] NaN". What are the 
> reasons and what are the solutions?
>
> I'am a newbie and need urgently information.
> Thanks.
>
> Here is an short output with the results:
>
> 9997   385.42   68.54   9.82  124.09  23.93  138.24
> 9998   342.89   73.65 133.35 1134.19 345.99 1876.88
> 9999   316.23   76.35  48.26  421.70 129.80  873.79
> 10000  291.64  103.66   6.85  107.46  26.42  189.38
> 10001    0.00    0.00   0.00    0.00   0.00    0.00
> > mean(data)
>       FSC       SSC       FL1       FL2      FL32       FL4
> 375.94880  73.76219  50.73413 434.42837 110.06393 637.34980
> > geo.mean(data)
> [1] NaN
> > median(data)
> Error in median(data) : need numeric data
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra




-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From hj_peking at yahoo.com.cn  Sat Feb 19 13:57:01 2005
From: hj_peking at yahoo.com.cn (j h)
Date: Sat, 19 Feb 2005 20:57:01 +0800 (CST)
Subject: [R] bootstrap
Message-ID: <20050219125701.36424.qmail@web15510.mail.cnb.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050219/8574ed29/attachment.pl

From ripley at stats.ox.ac.uk  Sat Feb 19 13:58:27 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 19 Feb 2005 12:58:27 +0000 (GMT)
Subject: [R] Warnings by functions mean(), median()
In-Reply-To: <42172793.4080908@gmx.net>
References: <42172793.4080908@gmx.net>
Message-ID: <Pine.LNX.4.61.0502191243400.12971@gannet.stats>

On Sat, 19 Feb 2005 mailpuls at gmx.net wrote:

> following functions doesnt work correct with my data: median(), geo.mean().
>
> My datafiles contain more than 10.000 lines and six columns from a 
> flow-cytometer-measurment. I need the arithmetic and geometric mean and 
> median. For the calculation of the geometric mean i wrote following function:
>
>    fix(geo.mean)
>
>    function(x)
>    {
>        n<-length(x)
>        gm<-prod(x)^(1/n)
>        return(gm)
>    }
>
> The function median() error tells me "need numeric data". The data are 
> numeric. The function geo.mean() gave out "[1] NaN". What are the reasons and 
> what are the solutions?
>
> I'am a newbie and need urgently information.

0) `data' is a bad choice of name as it masks an R system function.

1) `data' appears to be a data frame, not numeric data, as median says.
Do you want a summary for each column or the whole table?
So you need sapply(data, median) or median(as.matrix(data)).

2) Your function is trying to take a fractional power of 0, and what you 
think that is?  (0)  However, it is liable to under/overflow (10000 
numbers of size 100 have product 10^20000, way more than IEC60559 
arithmetic can represent, so you have (Inf*0)^(1/100001) = NaN).  You want 
something like

geo.mean <- function(x)
{
     if(any(x < 0)) stop("need positive data")
     exp(mean(log(x)))
}

which will even work for a data frame.  But I can tell you the answer is 
0 for the data you show.

For more information, see `An Introduction to R' or a good book on data 
manipulation with S/R, plus Numerical Analysis 101.


> Here is an short output with the results:
>
> 9997   385.42   68.54   9.82  124.09  23.93  138.24
> 9998   342.89   73.65 133.35 1134.19 345.99 1876.88
> 9999   316.23   76.35  48.26  421.70 129.80  873.79
> 10000  291.64  103.66   6.85  107.46  26.42  189.38
> 10001    0.00    0.00   0.00    0.00   0.00    0.00
>> mean(data)
>      FSC       SSC       FL1       FL2      FL32       FL4
> 375.94880  73.76219  50.73413 434.42837 110.06393 637.34980
>> geo.mean(data)
> [1] NaN
>> median(data)
> Error in median(data) : need numeric data

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From marcosjbd_g5a at mail.com  Sat Feb 19 14:19:44 2005
From: marcosjbd_g5a at mail.com (marcosjbd_g5a@mail.com)
Date: Sat, 19 Feb 2005 10:19:44 -0300
Subject: [R] Listas de E-mails para Mala Direta , Marketing Direto
Message-ID: <20050219131909.B5BBB7E08@scorpion5.uol.com.br>

Cadastros para mala direta livre de spam, cadastros de e-mails divididos por 
segmento, e-mails para mala direta por classe social.

Visite agora:
http://www.gueb.de/divulgamail

cadastros de e-mails divididos por profiss?o, programas gr?tis para e-mail marketing, 
Listas de E-mails para Mala Direta , Marketing Direto, cadastros de e-mails divididos 
por segmento:

http://www.gueb.de/divulgamail



From bates at stat.wisc.edu  Sat Feb 19 14:37:40 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 19 Feb 2005 07:37:40 -0600
Subject: [R] Problems installing quantreg
In-Reply-To: <200502190117.15081.engle@unr.nevada.edu>
References: <200502190117.15081.engle@unr.nevada.edu>
Message-ID: <42174124.3080502@stat.wisc.edu>

mark engle wrote:
> Hi All,
>  I'm trying to install the quantreg library on my MEPIS box (debian-based 
> linux).  When I try >install.packages("quantreg") it fails to loads and 
> errors with: /usr/bin/ld: cannot find -lblas-3.  I tried installing a ton of 
> packages to fix the missing part but the only thing I manage to do it cause 
> problems with another shared library and had to re-install a couple of 
> packages to even get R to run again (It's pretty late here and I'm just too 
> tired to remember all the packages I messed with) .
> 
> As a work around I tried downloading an .rpm for the package and converting it 
> to a .deb using alien.  I then installed the .deb through kpackage.  It 
> installed without any complaints and I can run the library command with 
> quantreg.  However when I try to run a quantreg command (in this case, rq) at 
> the R prompt, I get: "Error: Object "rq" not found".  This suggests to me 
> that the package hasn't really installed itself or isn't working properly.
> 
> Does anyone have any ideas?  I'd really like to be able to use this library 
> with R.  In fact at the moment R doesn't do me much good without it.
> Thanks.

How was your R or r-base-core installed?  If it came from one of Dirk's 
Debian packages it should already have a dependency on one of the atlas3 
packages (either atlas3-base or atlas3-sse or atlas3-sse2 or 
atlas3-3dnow) that provides a blas-3 library.

I would suggest installing one of the atlas3 packages (sse2 for recent 
Pentium or 3dnow for a recent Athlon chip).

Although the arrangements with the atlas3 packages are complex they 
provide a great performance boost on some linear algebra operations with 
large matrices.



From jeaneid at chass.utoronto.ca  Sat Feb 19 15:19:17 2005
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Sat, 19 Feb 2005 09:19:17 -0500
Subject: [R] Problems installing quantreg
In-Reply-To: <42174124.3080502@stat.wisc.edu>
Message-ID: <Pine.SGI.4.40.0502190917380.13061061-100000@origin.chass.utoronto.ca>

I have the same problem as well. I installed atalas3-sse2 and still hasd
the same problem. Here's the log of messages
* Installing *source* package 'quantreg' ...
** libs
g77   -fPIC  -g -O2 -c akj.f -o akj.o
g77   -fPIC  -g -O2 -c boot.f -o boot.o
g77   -fPIC  -g -O2 -c boundc.f -o boundc.o
g77   -fPIC  -g -O2 -c bound.f -o bound.o
g77   -fPIC  -g -O2 -c chlfct.f -o chlfct.o
g77   -fPIC  -g -O2 -c cholesky.f -o cholesky.o
g77   -fPIC  -g -O2 -c extract.f -o extract.o
gcc -I/usr/lib/R/include     -fPIC  -g -O2 -c mcmb.c -o mcmb.o
g77   -fPIC  -g -O2 -c penalty.f -o penalty.o
g77   -fPIC  -g -O2 -c rls.f -o rls.o
g77   -fPIC  -g -O2 -c rq1.f -o rq1.o
g77   -fPIC  -g -O2 -c rqbr.f -o rqbr.o
rqbr.f: In subroutine `rqbr':
rqbr.f:418: warning:
         go to 60
               1
rqbr.f:547: (continued):
   60    idxcf = idxcf+1
   2
Reference to label at (1) is outside block containing definition at (2)
rqbr.f:405: warning:
         go to 70
               1
rqbr.f:551: (continued):
   70    if(lup)then
   2
Reference to label at (1) is outside block containing definition at (2)
g77   -fPIC  -g -O2 -c rqfnb.f -o rqfnb.o
g77   -fPIC  -g -O2 -c rqfnc.f -o rqfnc.o
g77   -fPIC  -g -O2 -c rqfn.f -o rqfn.o
g77   -fPIC  -g -O2 -c sparskit2.f -o sparskit2.o
g77   -fPIC  -g -O2 -c srqfnc.f -o srqfnc.o
g77   -fPIC  -g -O2 -c srqfn.f -o srqfn.o
g77   -fPIC  -g -O2 -c srtpai.f -o srtpai.o
g77   -fPIC  -g -O2 -c xlapack.f -o xlapack.o
gcc -shared  -o quantreg.so akj.o boot.o boundc.o bound.o chlfct.o
cholesky.o extract.o mcmb.o penalty.o rls.o rq1.o rqbr.o rqfnb.o rqfnc.o
rqfn.o sparskit2.o srqfnc.o srqfn.o srtpai.o xlapack.o -lblas-3
-L/usr/lib/gcc-lib/i486-linux/3.3.5
-L/usr/lib/gcc-lib/i486-linux/3.3.5/../../.. -lfrtbegin -lg2c -lm -lgcc_s
-L/usr/lib/R/lib -lR
/usr/bin/ld: cannot find -lblas-3
collect2: ld returned 1 exit status
make: *** [quantreg.so] Error 1
ERROR: compilation failed for package 'quantreg'
** Removing '/usr/local/lib/R/site-library/quantreg'
** Restoring previous '/usr/local/lib/R/site-library/quantreg'




On Sat, 19 Feb 2005, Douglas Bates wrote:

> mark engle wrote:
> > Hi All,
> >  I'm trying to install the quantreg library on my MEPIS box (debian-based
> > linux).  When I try >install.packages("quantreg") it fails to loads and
> > errors with: /usr/bin/ld: cannot find -lblas-3.  I tried installing a ton of
> > packages to fix the missing part but the only thing I manage to do it cause
> > problems with another shared library and had to re-install a couple of
> > packages to even get R to run again (It's pretty late here and I'm just too
> > tired to remember all the packages I messed with) .
> >
> > As a work around I tried downloading an .rpm for the package and converting it
> > to a .deb using alien.  I then installed the .deb through kpackage.  It
> > installed without any complaints and I can run the library command with
> > quantreg.  However when I try to run a quantreg command (in this case, rq) at
> > the R prompt, I get: "Error: Object "rq" not found".  This suggests to me
> > that the package hasn't really installed itself or isn't working properly.
> >
> > Does anyone have any ideas?  I'd really like to be able to use this library
> > with R.  In fact at the moment R doesn't do me much good without it.
> > Thanks.
>
> How was your R or r-base-core installed?  If it came from one of Dirk's
> Debian packages it should already have a dependency on one of the atlas3
> packages (either atlas3-base or atlas3-sse or atlas3-sse2 or
> atlas3-3dnow) that provides a blas-3 library.
>
> I would suggest installing one of the atlas3 packages (sse2 for recent
> Pentium or 3dnow for a recent Athlon chip).
>
> Although the arrangements with the atlas3 packages are complex they
> provide a great performance boost on some linear algebra operations with
> large matrices.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Sat Feb 19 15:17:50 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 19 Feb 2005 14:17:50 +0000 (GMT)
Subject: [R] bootstrap
In-Reply-To: <20050219125701.36424.qmail@web15510.mail.cnb.yahoo.com>
References: <20050219125701.36424.qmail@web15510.mail.cnb.yahoo.com>
Message-ID: <Pine.LNX.4.61.0502191409560.13886@gannet.stats>

On Sat, 19 Feb 2005, j h wrote:

> Please, can you help me with a problem concerning bootstrap. I have a 
> 20000 by 20 matrix. I want to define each row as a variable. Then using 
> bootstrap to calculate standard errors, confidence intervals, and 
> significance tests. How can I do from defining the variables to 
> bootstrap analysis..

You want to have 20 variables with 20,000 values each (suspiciously round 
numbers -- is this a real problem or a student exercise?)

mydf <- as.data.frame(t(mymatrix))  # normal to have columns as variables.

library(boot)
?boot

Now, what statistic do you want to bootstrap?  You have not told us!
Nor what parameter you want confidence intervals for
nor what [null] hypothesis you want a significance test for.

I can strongly recommend Davison & Hinkley's book, for which package boot 
is the support software.  It seems your problems are about statistical 
concepts, not R software, but that book covers both.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bates at stat.wisc.edu  Sat Feb 19 16:23:53 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 19 Feb 2005 09:23:53 -0600
Subject: [R] Problems installing quantreg
In-Reply-To: <Pine.SGI.4.40.0502190917380.13061061-100000@origin.chass.utoronto.ca>
References: <Pine.SGI.4.40.0502190917380.13061061-100000@origin.chass.utoronto.ca>
Message-ID: <42175A09.7070309@stat.wisc.edu>

Jean Eid wrote:
> I have the same problem as well. I installed atalas3-sse2 and still hasd
> the same problem. Here's the log of messages
> * Installing *source* package 'quantreg' ...
> ** libs
> g77   -fPIC  -g -O2 -c akj.f -o akj.o
> g77   -fPIC  -g -O2 -c boot.f -o boot.o
> g77   -fPIC  -g -O2 -c boundc.f -o boundc.o
> g77   -fPIC  -g -O2 -c bound.f -o bound.o
> g77   -fPIC  -g -O2 -c chlfct.f -o chlfct.o
> g77   -fPIC  -g -O2 -c cholesky.f -o cholesky.o
> g77   -fPIC  -g -O2 -c extract.f -o extract.o
> gcc -I/usr/lib/R/include     -fPIC  -g -O2 -c mcmb.c -o mcmb.o
> g77   -fPIC  -g -O2 -c penalty.f -o penalty.o
> g77   -fPIC  -g -O2 -c rls.f -o rls.o
> g77   -fPIC  -g -O2 -c rq1.f -o rq1.o
> g77   -fPIC  -g -O2 -c rqbr.f -o rqbr.o
> rqbr.f: In subroutine `rqbr':
> rqbr.f:418: warning:
>          go to 60
>                1
> rqbr.f:547: (continued):
>    60    idxcf = idxcf+1
>    2
> Reference to label at (1) is outside block containing definition at (2)
> rqbr.f:405: warning:
>          go to 70
>                1
> rqbr.f:551: (continued):
>    70    if(lup)then
>    2
> Reference to label at (1) is outside block containing definition at (2)
> g77   -fPIC  -g -O2 -c rqfnb.f -o rqfnb.o
> g77   -fPIC  -g -O2 -c rqfnc.f -o rqfnc.o
> g77   -fPIC  -g -O2 -c rqfn.f -o rqfn.o
> g77   -fPIC  -g -O2 -c sparskit2.f -o sparskit2.o
> g77   -fPIC  -g -O2 -c srqfnc.f -o srqfnc.o
> g77   -fPIC  -g -O2 -c srqfn.f -o srqfn.o
> g77   -fPIC  -g -O2 -c srtpai.f -o srtpai.o
> g77   -fPIC  -g -O2 -c xlapack.f -o xlapack.o
> gcc -shared  -o quantreg.so akj.o boot.o boundc.o bound.o chlfct.o
> cholesky.o extract.o mcmb.o penalty.o rls.o rq1.o rqbr.o rqfnb.o rqfnc.o
> rqfn.o sparskit2.o srqfnc.o srqfn.o srtpai.o xlapack.o -lblas-3
> -L/usr/lib/gcc-lib/i486-linux/3.3.5
> -L/usr/lib/gcc-lib/i486-linux/3.3.5/../../.. -lfrtbegin -lg2c -lm -lgcc_s
> -L/usr/lib/R/lib -lR
> /usr/bin/ld: cannot find -lblas-3
> collect2: ld returned 1 exit status
> make: *** [quantreg.so] Error 1
> ERROR: compilation failed for package 'quantreg'
> ** Removing '/usr/local/lib/R/site-library/quantreg'
> ** Restoring previous '/usr/local/lib/R/site-library/quantreg'

Hmm.  Perhaps we should take this off-list or to the newly created 
R-SIG-Debian list.

Could you do a quick check of

  locate libblas-3

and send me (either privately or on the R-SIG-Debian list) the results. 
  The output should include /etc/alternatives/libblas-3.so and 
/usr/lib/libblas-3.so



From edd at debian.org  Sat Feb 19 16:53:57 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sat, 19 Feb 2005 09:53:57 -0600
Subject: [R] Problems installing quantreg
In-Reply-To: <200502190117.15081.engle@unr.nevada.edu>
References: <200502190117.15081.engle@unr.nevada.edu>
Message-ID: <16919.24853.295295.717531@basebud.nulle.part>


Mark,

I have a .deb packages of quantreg 3.40 and 3.50 here -- they were was
created last July when I was preparing explicit .deb packages for Quantian.
Newer Quantian releases have (almost) all of CRAN and BioConductor straight
in /usr/local which was easier for me to set up.

Quantian may be of interest to you -- a live dvd similar to mepis, but loaded
with scientific software and openMosix ready. See
http://dirk.eddelbuettel.com/quantian 

Email me off-list if you want the deb of quantreg 3.50.

Cheers, Dirk

-- 
Better to have an approximate answer to the right question than a precise 
answer to the wrong question.  --  John Tukey as quoted by John Chambers



From luke at stat.uiowa.edu  Sat Feb 19 17:23:10 2005
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Sat, 19 Feb 2005 10:23:10 -0600 (CST)
Subject: [R] socket problems (maybe bugs?)
In-Reply-To: <33346.217.229.15.144.1108609694.squirrel@mail.mytrium.com>
References: <33346.217.229.15.144.1108609694.squirrel@mail.mytrium.com>
Message-ID: <Pine.LNX.4.61.0502190933201.6368@itasca2.stat.uiowa.edu>

On Thu, 17 Feb 2005, Christian Lederer wrote:

> Dear R Gurus,
>
> for some purpose i have to use a socket connection, where i have to read
> and write both text and binary data (each binary data package will be
> preceeded by a header line).
> When experimenting, i encountered some problems (with R-2.0.1 under
> different Linuxes (SuSE and Gentoo)).
>
> Since the default mode for socket connections is non-blocking,
> i first tried socketSelect() in order to see whether the socket is ready
> for reading:
>
> # Server:
> s <- socketConnection(port=2222, server=TRUE, open="w+b")
> writeLines("test", s)
> writeBin(1:10, s, size=4, endian="big")
>
> # Client, variation 1:
> s <- socketConnection(port=2222, server=FALSE, open="w+b")
> socketSelect(list(s))
> readLines(s, n=1)     # works, "test" is read
> socketSelect(list(s)) # does never return, although the server wrote 1:10
>
> (This seems to happen only, when i mix text and binary reads.)
> However, without socketSelect(), R may crash if i try to read from an
> empty socket:
>
> Server:
> s <- socketConnection(port=2222, server=TRUE, open="w+b")
> writeLines("test", s)
> writeBin(1:10, s, size=4, endian="big")
>
> # Client, variation 2:
> s <- socketConnection(port=2222, server=FALSE, open="w+b")
> readLines(s, n=1)                              # works, "test" is read
> readBin(s, "int", size=4, n=10, endian="big")  # works, 1:10 is read
> readBin(s, "int", size=4, n=10, endian="big")  # second read leads to
>                                               # segmentation fault
>
> If i omit the endian="big" option, the second read does not crash, but
> just gets 10 random numbers.
>
> On the first view, this does not seem to be a problem, since the
> data will be preeceded by a header, which contains the number of
> bytes in the binary block.
> However, due to race conditions, i cannot exclude this situation:
>
> time    server             client
> t0      sends header
> t1                         reads header
> t2                         tries to read binary, crashes
> t3      sends binary
>
>
> If i open the client socket in blocking mode, the second variation seems
> to work (the second read just blocks as desired).
> When using only one socket, i can do without socketSelect(), but
> i have the follwoing questions:
>
> 1. Can i be sure, the the blocking variation will also work for larger
> data sets, when e.g. the server starts writing before the client is
> reading?
>
> 2. How could i proceed, if i needed several sockets?
> Then i cannot use socketSelect due to the problem described in
> variation 1.
> I also cannot use blocking sockets, since reading from an empty socket
> would block the others.
> Without blocking and socketSelect(), i might run into the race condition
> described above.
>
> In any case, the readBin() crash with endian="big" is a bug in
> my eyes. For non-blocking sockets, readBin() should just return numeric(0),
> if no data are written on the socket.
> I also stronlgy suspect that the socketSelect() behaviour as described in
> variation 1 is a bug.

Thanks for the report and the examples.  Both issues are bugs.

The crash is due to the fact that a low level routine
(sock_read_helper) correctly marks the connection as incomplete and
returns -EAGAIN as its result but the next higher routine (sock_read)
treats the result as a character count, unsigns it on return, and bad
tings happen the third level up (do_readbin).  I'm not quite sure
whether the best fix is to change sock_read_helper to return 0 or to
have sock_read to do some checking on the result it gets from
sock_read.

The issue with socketSelect is that socketSelect ought to return
immediately if buffered input is available but it does not.  As a
result, when you execute both writes before the first read then the
read will read all available input and store the part it does not use;
socketSelect then waits for _additional_ input which never comes.
This should be fixed in R-devel soon.

I always use blocking reads and writes with sockets--its a lot easier
than trying to figure out how to deal with incomplete reads or writes.
You need to make sure to use a protocol that guarantees that a reader
will read what a writer writes before the writer needs to move on.  If
you don't then you get deadlock with blocking writes and data large
enough to fill the buffer.  Using non-blocking sockets doesn't cure
the problem, it just changes the symptoms.

I use socketSelect in the cocket version of my snow package for the
load balaned cluster apply to detect the first slave to finish its
work.  In my setup the final write/read pairs in each communication
exchange are binary.  With the current implementation this ensures
that the read completely empties the buffer and so this problem does
not bite.  It sounds like the same stategy should allow you to work
with the current implementation.

Best,

luke

-- 
Luke Tierney
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu



From nawaaz at inktomi.com  Sat Feb 19 18:18:36 2005
From: nawaaz at inktomi.com (Nawaaz Ahmed)
Date: Sat, 19 Feb 2005 09:18:36 -0800
Subject: [R] Memory Fragmentation in R
Message-ID: <421774EC.3020007@inktomi.com>

I have a data set of roughly 700MB which during processing grows up to 
2G ( I'm using a 4G linux box). After the work is done I clean up (rm()) 
and the state is returned to 700MB. Yet I find I cannot run the same 
routine again as it claims to not be able to allocate memory even though 
gcinfo() claims there is 1.1G left.

	At the start of the second time
	===============================
           	 used  (Mb) gc trigger   (Mb)
	Ncells  2261001  60.4    3493455   93.3
	Vcells 98828592 754.1  279952797 2135.9

	Before Failing
	==============
	Garbage collection 459 = 312+51+96 (level 0) ...
	1222596 cons cells free (34%)
	1101.7 Mbytes of heap free (51%)
	Error: cannot allocate vector of size 559481 Kb

This looks like a fragmentation problem. Anyone have a handle on this 
situation? (ie. any work around?) Anyone working on improving R's 
fragmentation problems?

On the other hand, is it possible there is a memory leak? In order to 
make my functions work on this dataset I tried to eliminate copies by 
coding with references (basic new.env() tricks). I presume that my 
cleaning up returned the temporary data (as evidenced by the gc output 
at the start of the second round of processing). Is it possible that it 
was not really cleaned up and is sitting around somewhere even though 
gc() thinks it has been returned?

Thanks - any clues to follow up will be very helpful.
Nawaaz



From sfalcon at fhcrc.org  Sat Feb 19 19:11:20 2005
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Sat, 19 Feb 2005 10:11:20 -0800
Subject: [R] Comment on loadURL: should default to mode="wb"
In-Reply-To: <Pine.LNX.4.61.0502190646470.1764@gannet.stats>
References: <20050219005625.GR29138@gopher5.fhcrc.org>
	<Pine.LNX.4.61.0502190640240.1764@gannet.stats>
	<Pine.LNX.4.61.0502190646470.1764@gannet.stats>
Message-ID: <967a5c11d8ffef10e1fb42d4e68c5770@fhcrc.org>

On Feb 18, 2005, at 10:56 PM, Prof Brian Ripley wrote:
> A bit more explanation.
>
> On Sat, 19 Feb 2005, Prof Brian Ripley wrote:
>
>> Why are you using an out-dated interface anyway?  See the examples 
>> for loadURL!

Yep, the example section told me how to do the right thing and I missed 
it.

The funny thing is that I was thinking to myself, "hey, I bet I can use 
a connection with url() instead of calling download.file" --- I went to 
check the help for load() to make sure connections were ok and got led 
astray by loadURL which in my mind was exactly what I wanted to do.

I maintain that since the default of save() is ascii=FALSE, that 
loadURL should either do the right thing on Windows or perhaps have a 
deprecation note in the help file to help guide users towards the 
modern interface.

> The modern interface does work on Windows:
>
>> load(url("http://www.stats.ox.ac.uk/pub/bdr/hills.rda"))
>
> no error (and I have removed the file).

Excellent.  I will use the modern interface.

Best,

+ seth



From alserkli at inbox.ru  Sat Feb 19 20:03:39 2005
From: alserkli at inbox.ru (Alexander Klimov)
Date: Sat, 19 Feb 2005 21:03:39 +0200 (IST)
Subject: [R] Multiprecision arithmetics?
Message-ID: <TheMailAgent.863e9f0645a4ba@33e5a857e98bc94f18585>

Hi.

Is there a support of multiprecision arithmetics (e.g., interface to
gmp)?

-- 
Regards,
ASK



From ripley at stats.ox.ac.uk  Sat Feb 19 20:27:25 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 19 Feb 2005 19:27:25 +0000 (GMT)
Subject: [R] Multiprecision arithmetics?
In-Reply-To: <TheMailAgent.863e9f0645a4ba@33e5a857e98bc94f18585>
References: <TheMailAgent.863e9f0645a4ba@33e5a857e98bc94f18585>
Message-ID: <Pine.LNX.4.61.0502191927040.21268@gannet.stats>

See the gmp package on CRAN.

On Sat, 19 Feb 2005, Alexander Klimov wrote:

> Is there a support of multiprecision arithmetics (e.g., interface to
> gmp)?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sat Feb 19 20:52:12 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 19 Feb 2005 19:52:12 +0000 (GMT)
Subject: [R] Memory Fragmentation in R
In-Reply-To: <421774EC.3020007@inktomi.com>
References: <421774EC.3020007@inktomi.com>
Message-ID: <Pine.LNX.4.61.0502191815380.15926@gannet.stats>

BTW, I think this is really an R-devel question, and if you want to pursue 
this please use that list.  (See the posting guide as to why I think so.)

This looks like fragmentation of the address space: many of us are using 
64-bit OSes with 2-4Gb of RAM precisely to avoid such fragmentation.

Notice (memory.c line 1829 in the current sources) that large vectors are 
malloc-ed separately, so this is a malloc failure, and there is not a lot 
R can do about how malloc fragments the (presumably in your case as you 
did not say) 32-bit process address space.

The message
   1101.7 Mbytes of heap free (51%)
is a legacy of an earlier gc() and is not really `free': I believe it 
means something like `may be allocated before garbage collection is 
triggered': see memory.c.


On Sat, 19 Feb 2005, Nawaaz Ahmed wrote:

> I have a data set of roughly 700MB which during processing grows up to 2G ( 
> I'm using a 4G linux box). After the work is done I clean up (rm()) and the 
> state is returned to 700MB. Yet I find I cannot run the same routine again as 
> it claims to not be able to allocate memory even though gcinfo() claims there 
> is 1.1G left.
>
> 	At the start of the second time
> 	===============================
>          	 used  (Mb) gc trigger   (Mb)
> 	Ncells  2261001  60.4    3493455   93.3
> 	Vcells 98828592 754.1  279952797 2135.9
>
> 	Before Failing
> 	==============
> 	Garbage collection 459 = 312+51+96 (level 0) ...
> 	1222596 cons cells free (34%)
> 	1101.7 Mbytes of heap free (51%)
> 	Error: cannot allocate vector of size 559481 Kb
>
> This looks like a fragmentation problem. Anyone have a handle on this 
> situation? (ie. any work around?) Anyone working on improving R's 
> fragmentation problems?
>
> On the other hand, is it possible there is a memory leak? In order to make my 
> functions work on this dataset I tried to eliminate copies by coding with 
> references (basic new.env() tricks). I presume that my cleaning up returned 
> the temporary data (as evidenced by the gc output at the start of the second 
> round of processing). Is it possible that it was not really cleaned up and is 
> sitting around somewhere even though gc() thinks it has been returned?
>
> Thanks - any clues to follow up will be very helpful.
> Nawaaz

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tom_colson at ncsu.edu  Sat Feb 19 21:28:04 2005
From: tom_colson at ncsu.edu (Tom Colson)
Date: Sat, 19 Feb 2005 15:28:04 -0500
Subject: [R] R GUI for Gnome?
In-Reply-To: <Pine.LNX.4.61.0502191815380.15926@gannet.stats>
Message-ID: <200502192028.j1JKRxFQ019520@uni00mr.unity.ncsu.edu>

 Just installed R 64 Bit precompiled binaries on Fedora Core 3 (Kernel
2.6.10)....http://cran.stat.ucla.edu/bin/linux/redhat/fc3/x86_64/R-2.0.1-0.f
dr.1.x86_64.rpm

And R seem to start right up. Problem....I a windows slave...and miss the
handy R gui that come with the windows version (for doing repetitive tasks
like loading packages and changing directories).

If I try R -g XLL...it still starts in the terminal mode. R --gui-gnome
returns GNOME GUI not available in this verions. 

A read of the manual reveals that I must compile GNOME Support with
./configure --with-gnome. 

But I'm at a loss as to how to configure/compile R for 64 Bit using the
source. 

Thanks.



From spencer.graves at pdf.com  Sat Feb 19 21:48:48 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 19 Feb 2005 12:48:48 -0800
Subject: [R] Multiprecision arithmetics?
In-Reply-To: <Pine.LNX.4.61.0502191927040.21268@gannet.stats>
References: <TheMailAgent.863e9f0645a4ba@33e5a857e98bc94f18585>
	<Pine.LNX.4.61.0502191927040.21268@gannet.stats>
Message-ID: <4217A630.2060907@pdf.com>

Dear Prof. Ripley: 

      I just got a negative result from 'install.packages("gmp")': 

trying URL `http://cran.r-project.org/bin/windows/contrib/2.0/PACKAGES'
Content type `text/plain; charset=iso-8859-1' length 25432 bytes
opened URL
downloaded 24Kb

Warning message:
No package "gmp" on CRAN. in: download.packages(pkgs, destdir = tmpd, 
available = available, 

      This was from R 2.0.1 under Windows 2000. 

      I see "gmp" listed among contributed packages on CRAN, but the 
above command didn't get it for me. 

      Thanks,
      spencer graves

Prof Brian Ripley wrote:

> See the gmp package on CRAN.
>
> On Sat, 19 Feb 2005, Alexander Klimov wrote:
>
>> Is there a support of multiprecision arithmetics (e.g., interface to
>> gmp)?
>
>



From spencer.graves at pdf.com  Sat Feb 19 21:56:25 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 19 Feb 2005 12:56:25 -0800
Subject: [R] best analysis method : for time series ans cross sectional
	data
In-Reply-To: <b040cbb005021821225f255fca@mail.gmail.com>
References: <b040cbb005021821225f255fca@mail.gmail.com>
Message-ID: <4217A7F9.2070801@pdf.com>

      It looks to me like what you want is "intervention analysis" in 
the time series literature.  Have you considered the arima function, 
especially the example in the documentation using the xreg argument?  
Also, have you looked at ch. 14 in Venables and Ripley (2002) Modern 
Applied Statistics with S, 4th ed. (Springer)? 

      There are other time series packages available, e.g, dse, fSeries, 
its, GeneTS, msm, pastecs, splancs, tseries, urca, uroot, but I haven't 
used them and so can't comment further on them. 

      hope this helps.  spencer graves

Kum-Hoe Hwang wrote:

>Howdy
>
>What I 'd like to analyze with a large data on building permits is to find
>time series effect of urban policy on buildings as well as
>cross-sectional effects in any. In 1990 the specialZone urban policy
>was introduced. I guess that the effects of this specialZone policy
>would be different from countys. There are counties that do not
>welcome this specialZone forced to design it.
>
>One of the important aims is to find 1) time series effect using Dummy
>variable,  2) cross-sectional effects using specialZones variable
>below.
>
>The data has items like year(1970-2000), floorSpace, county,
>specialZones agianst permitting large buildings. specialZones have
>been designed after 1990.
>(Dummy = 1 after 1990, Dummy =0 before 1990)
>
>I have tried three methods, such as
> lm(floorSpace ~ county, specialZones, Dummy), 
> glm(floorSpace ~ county, specialZones, Dummy),
> aov(floorSpace ~ county, specialZones, Dummy).
>
>What I am focusing on is best method among lm, glm, aov or others not
>siginificant results.
>
>I have wasted  too  much time for this. I welcome your comments.
>
>Thanks a lot,
>
>  
>



From ripley at stats.ox.ac.uk  Sat Feb 19 22:02:04 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 19 Feb 2005 21:02:04 +0000 (GMT)
Subject: [R] Multiprecision arithmetics?
In-Reply-To: <4217A630.2060907@pdf.com>
References: <TheMailAgent.863e9f0645a4ba@33e5a857e98bc94f18585>
	<Pine.LNX.4.61.0502191927040.21268@gannet.stats>
	<4217A630.2060907@pdf.com>
Message-ID: <Pine.LNX.4.61.0502192053430.10960@gannet.stats>

On Sat, 19 Feb 2005, Spencer Graves wrote:

> Dear Prof. Ripley: 
>     I just got a negative result from 'install.packages("gmp")': 
> trying URL `http://cran.r-project.org/bin/windows/contrib/2.0/PACKAGES'
> Content type `text/plain; charset=iso-8859-1' length 25432 bytes
> opened URL
> downloaded 24Kb
>
> Warning message:
> No package "gmp" on CRAN. in: download.packages(pkgs, destdir = tmpd, 
> available = available, 
>     This was from R 2.0.1 under Windows 2000. 
>     I see "gmp" listed among contributed packages on CRAN, but the above 
> command didn't get it for me.

That's because you did not look in the ReadMe!  Really, you should know to 
do so by now: about 40 packages do not have Windows versions on CRAN and 
it all cases there is an explanation.

My ISP has lost the r-project.org domain in the last hour, so I can't 
actually check, but I think it is the same as

http://www.sourcekeg.co.uk/cran/bin/windows/contrib/2.0/@ReadMe

which says:

The packages
   fork, gmp, gtkDevice, hdf5, ncdf, nice, RArcInfo, RCurl, RNetCDF,
   rpart.permutation, rpvm, RQuantLib, RScaLAPACK, rsprng, snowFT,
   taskPR, and udunits
require additional libraries / software to build on Windows I do not
have installed.

The packages
   gmp, gsl, hdf5, ncdf, rgdal, RNetCDF, udunits, xgobi and XML
do not build out of the box. Nevertheless these are available at
   http://www.stats.ox.ac.uk/pub/RWin/2.0.0/
kindly provided by Professor Brian D. Ripley.

In R-devel (2.1.0-to-be) they will magically appear on the list if the 
repositories are the default list.



>     Thanks,
>     spencer graves
>
> Prof Brian Ripley wrote:
>
>> See the gmp package on CRAN.
>> 
>> On Sat, 19 Feb 2005, Alexander Klimov wrote:
>> 
>>> Is there a support of multiprecision arithmetics (e.g., interface to
>>> gmp)?
>> 
>> 
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sat Feb 19 22:13:39 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 19 Feb 2005 21:13:39 +0000 (GMT)
Subject: [R] R GUI for Gnome?
In-Reply-To: <200502192028.j1JKRxFQ019520@uni00mr.unity.ncsu.edu>
References: <200502192028.j1JKRxFQ019520@uni00mr.unity.ncsu.edu>
Message-ID: <Pine.LNX.4.61.0502192103130.11072@gannet.stats>

On Sat, 19 Feb 2005, Tom Colson wrote:

> Just installed R 64 Bit precompiled binaries on Fedora Core 3 (Kernel
> 2.6.10)....http://cran.stat.ucla.edu/bin/linux/redhat/fc3/x86_64/R-2.0.1-0.f
> dr.1.x86_64.rpm

Was it not SuSE yesterday?

I am glad it works: it is hard to test RPMs as all our x86_64 machines 
were cloned from the one I built that RPM on.

> And R seem to start right up. Problem....I a windows slave...and miss the
> handy R gui that come with the windows version (for doing repetitive tasks
> like loading packages and changing directories).
>
> If I try R -g XLL...it still starts in the terminal mode. R --gui-gnome
> returns GNOME GUI not available in this verions.
>
> A read of the manual reveals that I must compile GNOME Support with
> ./configure --with-gnome.
>
> But I'm at a loss as to how to configure/compile R for 64 Bit using the
> source.

The same way you do it for 32-bit platforms: there is a manual with full 
details.  On FC3 with enough tools (my machine has them all)

unpack tarball
./configure --with-gnome
make

is all that is needed (except to make sure you run that version and not 
the RPM).


You are making assumptions about that GUI: it does not do anything like as 
much as the Windows one. It might be easier just to learn the command-line 
equivalents.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From spencer.graves at pdf.com  Sat Feb 19 22:33:45 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 19 Feb 2005 13:33:45 -0800
Subject: [R] Time series documentation?  (was:  best analysis method : for
	time series ans cross sectional data)
In-Reply-To: <b040cbb005021821225f255fca@mail.gmail.com>
References: <b040cbb005021821225f255fca@mail.gmail.com>
Message-ID: <4217B0B9.8060707@pdf.com>

	  Hello, All:

	  What documentation do you recommend for someone trying to learn how 
to analyze time series in R beyond ch. 14 in Venables and Ripley (2002) 
Modern Applied Statistics with S, 4th ed. (Springer), and a not-quite 
random walk through the documentation on commands like arima and 
provided with packages like dse?  I've installed dse and its, and I've 
made progress using arima, acf, etc.  I tried working through 
"dse1-guide.pdf", but far I have not successfully used that package.

	  In particular, what's the preferred way to keep track of dates with 
time series?  I tried assigning a "Date" object somehow to a "ts" 
object, so far without success.  Two of my attempts are as follows:

 > tst2 <- ts(1:11, frequency=365,
+        start=c(2005, 11))
# This seemed to work, but the time does not seem to have class "Date"

 > tst3 <- ts(1:11, frequency=365,
+        start=as.Date("21/01/2005", "%d/%m/%Y"))
Error in Math.difftime((end - start) * frequency + 1.01) :
	floor not defined for difftime objects

	  Any suggestions would be greatly appreciated.  I'd gladly rtfm ("read 
the f****** manual"), but I don't know which fm to r.

	  Thanks,
	  Spencer Graves	

############################
      It looks to me like what you want is "intervention analysis" in
the time series literature.  Have you considered the arima function,
especially the example in the documentation using the xreg argument?
Also, have you looked at ch. 14 in Venables and Ripley (2002) Modern
Applied Statistics with S, 4th ed. (Springer)?

      There are other time series packages available, e.g, dse, fSeries,
its, GeneTS, msm, pastecs, splancs, tseries, urca, uroot, but I haven't
used them and so can't comment further on them.

      hope this helps.  spencer graves

Kum-Hoe Hwang wrote:

>Howdy
>
>What I 'd like to analyze with a large data on building permits is to find
>time series effect of urban policy on buildings as well as
>cross-sectional effects in any. In 1990 the specialZone urban policy
>was introduced. I guess that the effects of this specialZone policy
>would be different from countys. There are counties that do not
>welcome this specialZone forced to design it.
>
>One of the important aims is to find 1) time series effect using Dummy
>variable,  2) cross-sectional effects using specialZones variable
>below.
>
>The data has items like year(1970-2000), floorSpace, county,
>specialZones agianst permitting large buildings. specialZones have
>been designed after 1990.
>(Dummy = 1 after 1990, Dummy =0 before 1990)
>
>I have tried three methods, such as
> lm(floorSpace ~ county, specialZones, Dummy), 
> glm(floorSpace ~ county, specialZones, Dummy),
> aov(floorSpace ~ county, specialZones, Dummy).
>
>What I am focusing on is best method among lm, glm, aov or others not
>siginificant results.
>
>I have wasted  too  much time for this. I welcome your comments.
>
>Thanks a lot,
>
>  
>



From luke at stat.uiowa.edu  Sat Feb 19 23:49:07 2005
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Sat, 19 Feb 2005 16:49:07 -0600 (CST)
Subject: [R] socket problems (maybe bugs?)
In-Reply-To: <Pine.LNX.4.61.0502190933201.6368@itasca2.stat.uiowa.edu>
References: <33346.217.229.15.144.1108609694.squirrel@mail.mytrium.com>
	<Pine.LNX.4.61.0502190933201.6368@itasca2.stat.uiowa.edu>
Message-ID: <Pine.LNX.4.61.0502191648010.6368@itasca2.stat.uiowa.edu>

On Sat, 19 Feb 2005, Luke Tierney wrote:

> On Thu, 17 Feb 2005, Christian Lederer wrote:
>
>> Dear R Gurus,
>> 
>> for some purpose i have to use a socket connection, where i have to read
>> and write both text and binary data (each binary data package will be
>> preceeded by a header line).
>> When experimenting, i encountered some problems (with R-2.0.1 under
>> different Linuxes (SuSE and Gentoo)).
>> 
>> Since the default mode for socket connections is non-blocking,
>> i first tried socketSelect() in order to see whether the socket is ready
>> for reading:
>> 
>> # Server:
>> s <- socketConnection(port=2222, server=TRUE, open="w+b")
>> writeLines("test", s)
>> writeBin(1:10, s, size=4, endian="big")
>> 
>> # Client, variation 1:
>> s <- socketConnection(port=2222, server=FALSE, open="w+b")
>> socketSelect(list(s))
>> readLines(s, n=1)     # works, "test" is read
>> socketSelect(list(s)) # does never return, although the server wrote 1:10
>> 
>> (This seems to happen only, when i mix text and binary reads.)
>> However, without socketSelect(), R may crash if i try to read from an
>> empty socket:
>> 
>> Server:
>> s <- socketConnection(port=2222, server=TRUE, open="w+b")
>> writeLines("test", s)
>> writeBin(1:10, s, size=4, endian="big")
>> 
>> # Client, variation 2:
>> s <- socketConnection(port=2222, server=FALSE, open="w+b")
>> readLines(s, n=1)                              # works, "test" is read
>> readBin(s, "int", size=4, n=10, endian="big")  # works, 1:10 is read
>> readBin(s, "int", size=4, n=10, endian="big")  # second read leads to
>>                                               # segmentation fault
>> 
>> If i omit the endian="big" option, the second read does not crash, but
>> just gets 10 random numbers.
>> 
>> On the first view, this does not seem to be a problem, since the
>> data will be preeceded by a header, which contains the number of
>> bytes in the binary block.
>> However, due to race conditions, i cannot exclude this situation:
>> 
>> time    server             client
>> t0      sends header
>> t1                         reads header
>> t2                         tries to read binary, crashes
>> t3      sends binary
>> 
>> 
>> If i open the client socket in blocking mode, the second variation seems
>> to work (the second read just blocks as desired).
>> When using only one socket, i can do without socketSelect(), but
>> i have the follwoing questions:
>> 
>> 1. Can i be sure, the the blocking variation will also work for larger
>> data sets, when e.g. the server starts writing before the client is
>> reading?
>> 
>> 2. How could i proceed, if i needed several sockets?
>> Then i cannot use socketSelect due to the problem described in
>> variation 1.
>> I also cannot use blocking sockets, since reading from an empty socket
>> would block the others.
>> Without blocking and socketSelect(), i might run into the race condition
>> described above.
>> 
>> In any case, the readBin() crash with endian="big" is a bug in
>> my eyes. For non-blocking sockets, readBin() should just return numeric(0),
>> if no data are written on the socket.
>> I also stronlgy suspect that the socketSelect() behaviour as described in
>> variation 1 is a bug.
>
> Thanks for the report and the examples.  Both issues are bugs.
>
> The crash is due to the fact that a low level routine
> (sock_read_helper) correctly marks the connection as incomplete and
> returns -EAGAIN as its result but the next higher routine (sock_read)
> treats the result as a character count, unsigns it on return, and bad
> tings happen the third level up (do_readbin).  I'm not quite sure
> whether the best fix is to change sock_read_helper to return 0 or to
> have sock_read to do some checking on the result it gets from
> sock_read.
>
> The issue with socketSelect is that socketSelect ought to return
> immediately if buffered input is available but it does not.  As a
> result, when you execute both writes before the first read then the
> read will read all available input and store the part it does not use;
> socketSelect then waits for _additional_ input which never comes.
> This should be fixed in R-devel soon.
>
> I always use blocking reads and writes with sockets--its a lot easier
> than trying to figure out how to deal with incomplete reads or writes.
> You need to make sure to use a protocol that guarantees that a reader
> will read what a writer writes before the writer needs to move on.  If
> you don't then you get deadlock with blocking writes and data large
> enough to fill the buffer.  Using non-blocking sockets doesn't cure
> the problem, it just changes the symptoms.
>
> I use socketSelect in the cocket version of my snow package for the
> load balaned cluster apply to detect the first slave to finish its
> work.  In my setup the final write/read pairs in each communication
> exchange are binary.  With the current implementation this ensures
> that the read completely empties the buffer and so this problem does
> not bite.  It sounds like the same stategy should allow you to work
> with the current implementation.
>

Both the socketSelect and the segfault in reading from non-blockign
sockets are now fixed in R-devel.

Best,

luke

-- 
Luke Tierney
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu



From ggrothendieck at myway.com  Sun Feb 20 02:14:13 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 20 Feb 2005 01:14:13 +0000 (UTC)
Subject: [R] Time series documentation? (was: best analysis method :
	=?utf-8?b?Zm9yCXRpbWU=?= series ans cross sectional data)
References: <b040cbb005021821225f255fca@mail.gmail.com>
	<4217B0B9.8060707@pdf.com>
Message-ID: <loom.20050220T015540-671@post.gmane.org>

Spencer Graves <spencer.graves <at> pdf.com> writes:

: 	  In particular, what's the preferred way to keep track of dates with 
: time series?  I tried assigning a "Date" object somehow to a "ts" 
: object, so far without success.  Two of my attempts are as follows:
: 
:  > tst2 <- ts(1:11, frequency=365,
: +        start=c(2005, 11))
: # This seemed to work, but the time does not seem to have class "Date"
: 
:  > tst3 <- ts(1:11, frequency=365,
: +        start=as.Date("21/01/2005", "%d/%m/%Y"))
: Error in Math.difftime((end - start) * frequency + 1.01) :
: 	floor not defined for difftime objects
: 
: 	  Any suggestions would be greatly appreciated.  I'd gladly rtfm 
("read 
: the f****** manual"), but I don't know which fm to r.
: 


A 'ts' class series does not store a sequence of times but rather stores 
a tsp attribute consisting of three numbers (see ?tsp).  The closest
you can get to daily data with 'ts' is to use the internal 
numeric representation of the Date like this:

# daily ts series 1:10 starting at Jan 22, 2000
st <- as.Date("2000-01-22")
my.ts <- ts(1:10, start = unclass(st))

# and then to get back the Date class dates
structure(as.vector(time(my.ts)), class = "Date")

This won't allow give you automatic date handling on plots etc.
since it does not actually know that the numbers represent
Date class dates.  (Note that if you want monthly data then
ts has its own home represenation of dates and will automatically
assume monthly data if you use the frequency = 12 argument
to ts.)

The zoo library can represent time series using the Date class
(or nearly any other date class).  Here is an example of using
zoo with daily Date class data:

my.zoo <- zoo(1:10, as.Date("2000-01-22") + 0:9)
time(my.zoo)

The other time series packages (its, fBasics, tseries) use POSIXct 
underneath.

For more info about zoo:

library(zoo)
vignette("zoo")



From mlau at nd.edu  Sun Feb 20 03:45:30 2005
From: mlau at nd.edu (Michael Y. Lau)
Date: Sat, 19 Feb 2005 21:45:30 -0500
Subject: [R] logistic regression and 3PL model
Message-ID: <006701c516f6$3e674b90$ba0d4a81@MLAUND>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050219/72e055ee/attachment.pl

From spencer.graves at pdf.com  Sun Feb 20 05:45:08 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 19 Feb 2005 20:45:08 -0800
Subject: [R] Time series documentation?
In-Reply-To: <loom.20050220T015540-671@post.gmane.org>
References: <b040cbb005021821225f255fca@mail.gmail.com>	<4217B0B9.8060707@pdf.com>
	<loom.20050220T015540-671@post.gmane.org>
Message-ID: <421815D4.7030802@pdf.com>

Hi, Gabor: 

      Thanks.  Which package(s) do you prefer for which purposes? 

      Best Wishes,
      Spencer Graves

Gabor Grothendieck wrote:

>Spencer Graves <spencer.graves <at> pdf.com> writes:
>
>: 	  In particular, what's the preferred way to keep track of dates with 
>: time series?  I tried assigning a "Date" object somehow to a "ts" 
>: object, so far without success.  Two of my attempts are as follows:
>: 
>:  > tst2 <- ts(1:11, frequency=365,
>: +        start=c(2005, 11))
>: # This seemed to work, but the time does not seem to have class "Date"
>: 
>:  > tst3 <- ts(1:11, frequency=365,
>: +        start=as.Date("21/01/2005", "%d/%m/%Y"))
>: Error in Math.difftime((end - start) * frequency + 1.01) :
>: 	floor not defined for difftime objects
>: 
>: 	  Any suggestions would be greatly appreciated.  I'd gladly rtfm 
>("read 
>: the f****** manual"), but I don't know which fm to r.
>: 
>
>
>A 'ts' class series does not store a sequence of times but rather stores 
>a tsp attribute consisting of three numbers (see ?tsp).  The closest
>you can get to daily data with 'ts' is to use the internal 
>numeric representation of the Date like this:
>
># daily ts series 1:10 starting at Jan 22, 2000
>st <- as.Date("2000-01-22")
>my.ts <- ts(1:10, start = unclass(st))
>
># and then to get back the Date class dates
>structure(as.vector(time(my.ts)), class = "Date")
>
>This won't allow give you automatic date handling on plots etc.
>since it does not actually know that the numbers represent
>Date class dates.  (Note that if you want monthly data then
>ts has its own home represenation of dates and will automatically
>assume monthly data if you use the frequency = 12 argument
>to ts.)
>
>The zoo library can represent time series using the Date class
>(or nearly any other date class).  Here is an example of using
>zoo with daily Date class data:
>
>my.zoo <- zoo(1:10, as.Date("2000-01-22") + 0:9)
>time(my.zoo)
>
>The other time series packages (its, fBasics, tseries) use POSIXct 
>underneath.
>
>For more info about zoo:
>
>library(zoo)
>vignette("zoo")
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From lorin at cs.umd.edu  Sun Feb 20 06:43:18 2005
From: lorin at cs.umd.edu (Lorin Hochstein)
Date: Sun, 20 Feb 2005 00:43:18 -0500
Subject: [R] Treatment-Contrast Interactions
Message-ID: <42182376.40602@cs.umd.edu>

Hello all,

(Apologies in advance if my terminology is incorrect, I'm relatively new 
to R and statistics).

I have data from a factorial design with two treatments (CRF-23), and 
I'm trying to compute treatment-contrast interactions through analysis 
of variance. I can't figure out how to do contrasts properly, despite 
reading the help for "C" and "contrasts" functions.

(I'm actually trying to solve an exercise in a textbook: "Experimental 
Design" by Kirk, Ex. 9.7b).

 Here's what my data looks like:

score <- c(12, 8,10, 6, 8, 4,
       10,12, 8, 6,10,14,
        9, 7, 9, 5,11,12,
        7,13, 9, 9, 5,11,
        8, 7, 3, 8,12,10,
       13,14,19, 9,16,14)
n <- 6
A <- gl(2,3*n,labels=c("a1","a2"))
B <- rep(gl(3,n,labels=c("b1","b2","b3")),2)

I understand how to test for the effects of A, B, and AB: 
summary(aov(score~A*B))

Let's say I want to compute some contrasts on B and see if there is an 
interaction with A. I try to specify a matrix with the columns being the 
different contrasts on B:

contrasts.B <- matrix(c(1,-1,0,1,0,-1,0,1,-1),nrow=3)  

I know the following is wrong:
summary(aov(score~A*B,contrasts=contrasts.B))

I know I'm doing multiple things wrong here, because R can't possibly 
know I want those contrasts to be for the "B" variable, and because 
passing a contrast matrix never seems to change the result no matter 
what I do, so clearly I misunderstand how contrasts work. Can anyone advise?

(I really want the result for each contrast separately, so should I be 
passing one vector as an argument to contrasts?)

Lorin



From ggrothendieck at myway.com  Sun Feb 20 07:04:20 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 20 Feb 2005 06:04:20 +0000 (UTC)
Subject: [R] Time series documentation?
References: <b040cbb005021821225f255fca@mail.gmail.com>	<4217B0B9.8060707@pdf.com>
	<loom.20050220T015540-671@post.gmane.org>
	<421815D4.7030802@pdf.com>
Message-ID: <loom.20050220T063906-894@post.gmane.org>

Spencer Graves <spencer.graves <at> pdf.com> writes:

: 
:       Thanks.  Which package(s) do you prefer for which purposes? 

'ts' can be used for regularly spaced time series and supports 
monthly and quarterly dates.  The other ones listed below supports 
irregular time series.

zoo's design goals are consistency with base R, ability to use many
datetime classes (including Date, chron and POSIXct) and S3.  It has
some specific features of note in the areas of joins, graphics and
regression interfaces and includes a 15 page pdf vignette and reference
card written in Sweave.  Achim Zeileis initiated zoo and I joined 
the development last year.  Achim's strucchange package uses zoo.

'its' is based on POSIXct and S4 and has been proven in real financial
applications.

fBasics is part of the rmetrics.org project and is consistent with
the S-Plus FinMetrics software and would be of interest to someone
who was interested in compatability with that as well as
the broad range of financial functions in rmetrics.  It uses S4
and POSIXct underneath with the modification that it uses the Olsen 
time zone data base rather than the POSIXct scheme for time zones.  
It does require that you set the process or computer to GMT.  It has
extensive accompanying documentation and a web site (google for rmetrics).

Its possible to convert series from one package to another.



From dieter.menne at menne-biomed.de  Sun Feb 20 11:14:03 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Sun, 20 Feb 2005 10:14:03 +0000 (UTC)
Subject: [R] extracting F, df and r squared using
References: <6.1.0.6.0.20050218165226.01a760c0@mail.utm.utoronto.ca>
	<loom.20050219T123017-564@post.gmane.org>
Message-ID: <loom.20050220T110551-319@post.gmane.org>

>

Dear all,

After posting the reply quoted below, I received an email by Prof. Brian Ripley 
stating:

"It would be helpful if this credited the quotes, and I believe it to be 
a breach of copyright not too. It's way out of context for me."

This is the THIRD time I got a message with a similar content from the Prof. 
Brian Ripley, and I am a little bothered, to be polite.

1) It is considered good manners in most lists to shorten the question in such 
a way that anyone with a thread-aware reader knows which question, possible out 
of a multipart message, was answered. This is more work than citing the whole 
message, but it assumes that people can handle threaded lists. I personally 
don't like to scroll through fifty lines just to find a one-liner, even a good 
one, and there are still a few people around who are connected by 56k modems.

2) Could someone else please comment on the "breach of copyright"? Looks like 
it is a little different in GB.

3) The posting guide says that answers normally should be directed to the list. 
I was surprised about the many comments I have received by private email which 
could have been helpful to others. I understand now why I should use private 
mail to avoid breaches of copyright.

Dieter Menne

----- My posting -----

> 
> If you look at tmp, all the F statistics, r-squared values, etc. are there, 
> but is there an easy way to get at them reported in a tabulated form?
> 
 
 Try:
 
 tb=by(warpbreaks, tension,
   function(x) {
     sumlm=summary(lm(breaks ~ wool, data=x))
     c(sumlm$df,F=sumlm$fstatistic)
   }
   )
 do.call("rbind",tb)
 
and don't forget to give the df columns a name.
 
> and no smart-alec comments about bonferonni corrections :)
 
Use package (or library ???... just kidding) multcomp.

No smart-alec comments about packages and libraries, please.
 
 Dieter
 
> ______________________________________________
> R-help <at> stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From ripley at stats.ox.ac.uk  Sun Feb 20 11:18:00 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 20 Feb 2005 10:18:00 +0000 (GMT)
Subject: [R] Treatment-Contrast Interactions
In-Reply-To: <42182376.40602@cs.umd.edu>
References: <42182376.40602@cs.umd.edu>
Message-ID: <Pine.LNX.4.61.0502200953210.27414@gannet.stats>

On Sun, 20 Feb 2005, Lorin Hochstein wrote:

> Hello all,
>
> (Apologies in advance if my terminology is incorrect, I'm relatively new to R 
> and statistics).
>
> I have data from a factorial design with two treatments (CRF-23), and I'm 
> trying to compute treatment-contrast interactions through analysis of 
> variance. I can't figure out how to do contrasts properly, despite reading 
> the help for "C" and "contrasts" functions.
>
> (I'm actually trying to solve an exercise in a textbook: "Experimental 
> Design" by Kirk, Ex. 9.7b).
>
> Here's what my data looks like:
>
> score <- c(12, 8,10, 6, 8, 4,
>      10,12, 8, 6,10,14,
>       9, 7, 9, 5,11,12,
>       7,13, 9, 9, 5,11,
>       8, 7, 3, 8,12,10,
>      13,14,19, 9,16,14)
> n <- 6
> A <- gl(2,3*n,labels=c("a1","a2"))
> B <- rep(gl(3,n,labels=c("b1","b2","b3")),2)
>
> I understand how to test for the effects of A, B, and AB: 
> summary(aov(score~A*B))
>
> Let's say I want to compute some contrasts on B and see if there is an 
> interaction with A. I try to specify a matrix with the columns being the 
> different contrasts on B:
>
> contrasts.B <- matrix(c(1,-1,0,1,0,-1,0,1,-1),nrow=3) 
> I know the following is wrong:
> summary(aov(score~A*B,contrasts=contrasts.B))
>
> I know I'm doing multiple things wrong here, because R can't possibly know I 
> want those contrasts to be for the "B" variable, and because passing a 
> contrast matrix never seems to change the result no matter what I do, so 
> clearly I misunderstand how contrasts work. Can anyone advise?

There are only be two independent contrasts for a three-level factor, so 
you need to choose what you really want.

You can't really have `an interaction with A' with a contrast on B: that 
is just another contrast.

> (I really want the result for each contrast separately, so should I be 
> passing one vector as an argument to contrasts?)

I am not at all sure what you want to do.  Here is one possibility

contrasts(B) <- contrasts.B[, 1:2]
fit <- aov(score~A*B)
summary(fit, split=list(B=1:2), expand.split = T)

Another way to specify this is

fit <- aov(score~A*B, contrasts = list(B=contrasts.B[, 1:2]))

which gives a list (as specified in ?aov) and names B.

If you want to look at the patterns of contrasts, use model.tables(), and 
to assess a single contrast, use se.contrast (but beware of lack of 
independence, even collinearity, if using multiple contrasts). Here is one 
way:

cont <- c(1, -1)[A] * c(1, -1, 0)[B]
sum(cont) # 0
sum(cont*score)
se.contrast(fit, as.matrix(cont))

I hope that gives you some progress.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Innkeyp-r at yahoo.com  Sun Feb 20 12:16:28 2005
From: Innkeyp-r at yahoo.com (T Petersen)
Date: Sun, 20 Feb 2005 12:16:28 +0100
Subject: [R] bug in example(hist)?
Message-ID: <4218718C.5040601@yahoo.com>

I tried example(hist) today, but it doesn't want to return the proper 
graphs ...Anyone else seeing this?

I'm using R 2.0.1



From ligges at statistik.uni-dortmund.de  Sun Feb 20 15:01:44 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 20 Feb 2005 15:01:44 +0100
Subject: [R] bug in example(hist)?
In-Reply-To: <4218718C.5040601@yahoo.com>
References: <4218718C.5040601@yahoo.com>
Message-ID: <42189848.7030703@statistik.uni-dortmund.de>

T Petersen wrote:
> I tried example(hist) today, but it doesn't want to return the proper 
> graphs ...Anyone else seeing this?

T Petersen,

0) please read the posting guide,
1) please define "doesn't want to return the proper graphs", and
2) please tell us what OS and what device are you using.

Uwe Ligges



> I'm using R 2.0.1
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From jfox at mcmaster.ca  Sun Feb 20 16:06:10 2005
From: jfox at mcmaster.ca (John Fox)
Date: Sun, 20 Feb 2005 10:06:10 -0500
Subject: [R] logistic regression and 3PL model
In-Reply-To: <006701c516f6$3e674b90$ba0d4a81@MLAUND>
Message-ID: <20050220150609.HPEI1919.tomts22-srv.bellnexxia.net@JohnDesktop8300>

Dear Mike,

You appear to have misinterpreted Brian Ripley's advice. The logitreg()
function that you've included in your message hasn't been modified to
specify a lower bound (higher than 0) to the probability of success. The
"lower" argument, which gets passed through to nlminb(), specifies the lower
bounds for the parameters, not for the fitted probabilities.

You also apparently have sent the S-PLUS version of the function. To get
this to run in R, you should replace the call to nlminb() with one to
optim(), and you'll have to modify the local function gmin() slightly. 

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Michael Y. Lau
> Sent: Saturday, February 19, 2005 9:46 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] logistic regression and 3PL model
> 
> Hello colleagues,
> 
>  
> 
> This is a follow up to a question I posed in November 
> regarding an analysis I was working on.  Thank you to Dr. 
> Brian Ripley and Dr. John Fox for helping me out during that time.
> 
>  
> 
> I am conducting logistic regression on data set on psi (ESP) 
> ganzfeld trials.  The response variable is binary 
> (correct/incorrect), with a 25% guessing base rate.  Dr. 
> Ripley suggested that I modify a maximum likelihood fitting 
> of a binomial logistic regression presented in MASS$ (p. 445):
> 
>  
> 
> logitreg <- function(x, y, wt=rep(1, length(y)), intercept = 
> T, start=rep(0,p), ...) 
> 
> {
> 
>    fmin <- function (beta, X, y, w){
> 
>        p <- plogis(X %*% beta)
> 
>        -sum(2 * w * ifelse(y, log(p), log(1-p)))
> 
>        
> 
>    }
> 
>    gmin <- function (beta, X, y, w) {
> 
>        eta <- X %*% beta; p <- plogis(eta)
> 
>        -2 * (w *dlogis(eta) * ifelse(y, 1/p, -1/(1-p))) %*% X
> 
>    }
> 
>    if(is.null(dim(x))) dim(x) <- c(length(x), 1)
> 
>        dn <- dimnames(x)[[2]]
> 
>        if(!length(dn)) dn <- paste("Var", 1:ncol(x), sep="")
> 
>           p <- ncol(x) + intercept
> 
>           if(intercept) {x <- cbind(1,x); dn <- c("(Intercept)", dn)}
> 
>               if(is.factor(y)) y <- (unclass(y) !=1)
> 
>                  fit <- nlminb(start, fmin, gmin, X=x, y=y, 
> w=wt, method = "BFGS", ...)
> 
>                  names(fit$par) <- dn
> 
>                  cat("\nCoefficients:\n"); print(fit$par)
> 
>                  # R: use fit$value and fit$convergence
> 
>                  cat("\nResidual Deviance:", 
> format(fit$objective), "\n")
> 
>                  cat("\nConvergence message:", fit$message, "\n")
> 
>                  invisible(fit)
> 
> }
> 
>  
> 
> I have been using "lower=.25" in the call for logitreg:
> 
>  
> 
> options(contrasts = c("contr.treatment", "contr.poly"))
> 
> X <- model.matrix(longhit ~ age*gender, data=data)[,-1]
> 
> logitreg(X, data$longhit, lower =.25)
> 
> However, this is giving me .25 as the value for intercepts 
> and all the regression weights.  Am I correcting for the 
> guessing base rate correctly?  
> 
> Any suggestions or pointers would be greatly appreciated.  
> Thank you in advance.
> 
> Mike Lau
> 
>  
> 
> __________________________________
> Michael Y. Lau, M.A. 
> 118 Haggar Hall
> Department of Psychology
> University of Notre Dame
> Notre Dame, IN 46556
> (574) 631-1904
> mlau at nd.edu 
>   
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From lorin at cs.umd.edu  Sun Feb 20 16:50:39 2005
From: lorin at cs.umd.edu (Lorin Hochstein)
Date: Sun, 20 Feb 2005 10:50:39 -0500
Subject: [R] Treatment-Contrast Interactions
In-Reply-To: <Pine.LNX.4.61.0502200953210.27414@gannet.stats>
References: <42182376.40602@cs.umd.edu>
	<Pine.LNX.4.61.0502200953210.27414@gannet.stats>
Message-ID: <4218B1CF.4080807@cs.umd.edu>

Prof Brian Ripley wrote:

> On Sun, 20 Feb 2005, Lorin Hochstein wrote:
>
>> Hello all,
>>
>> (Apologies in advance if my terminology is incorrect, I'm relatively 
>> new to R and statistics).
>>
>> I have data from a factorial design with two treatments (CRF-23), and 
>> I'm trying to compute treatment-contrast interactions through 
>> analysis of variance.
>
[snip]

>>
>> Let's say I want to compute some contrasts on B and see if there is 
>> an interaction with A. I try to specify a matrix with the columns 
>> being the different contrasts on B:
>>
>> contrasts.B <- matrix(c(1,-1,0,1,0,-1,0,1,-1),nrow=3)
>
>
> There are only be two independent contrasts for a three-level factor, 
> so you need to choose what you really want.

As it turns out, the contrasts I'm trying to compute are not 
independent. (This is just an exercise in a textbook...)

>
>> (I really want the result for each contrast separately, so should I 
>> be passing one vector as an argument to contrasts?)
>
>
> I am not at all sure what you want to do.  Here is one possibility
>
> contrasts(B) <- contrasts.B[, 1:2]
> fit <- aov(score~A*B)
> summary(fit, split=list(B=1:2), expand.split = T)

Ah, this gives me what we want! Unfortunately, since no pair of 
contrasts I have are independent, I have to compute them one at a time. 
This seems to work.

for(i in 1:3) {
  contrasts(B) <- contrasts.B[, i]
  fit <- aov(score~A*B)
  s <- summary(fit, split=list(B=1:2), expand.split = T)
  f <- s[[1]]$"F value"[6]
  print(f)
}


>
> If you want to look at the patterns of contrasts, use model.tables(), 
> and to assess a single contrast, use se.contrast (but beware of lack 
> of independence, even collinearity, if using multiple contrasts). Here 
> is one way:
>
> cont <- c(1, -1)[A] * c(1, -1, 0)[B]
> sum(cont) # 0
> sum(cont*score)
> se.contrast(fit, as.matrix(cont))

I'd like to understand this approach as well, but I can't reproduce my 
results using se.contrast. In particular, I get the same standard error 
even though I tried to use different contrasts:

 > c1 <- c(1,-1)[A]*c(1,-1,0)[B]
 > c2 <- c(1,-1)[A]*c(1,0,-1)[B]
 > c3 <- c(1,-1)[A]*c(0,1,-1)[B]
 > se.contrast(fit, as.matrix(c1))
Contrast 1
  14.24547
 > se.contrast(fit,as.matrix(c2))
Contrast 1
  14.24547
 > se.contrast(fit,as.matrix(c3))
Contrast 1
  14.24547



Lorin



From charlie at stat.umn.edu  Sun Feb 20 18:12:08 2005
From: charlie at stat.umn.edu (Charles Geyer)
Date: Sun, 20 Feb 2005 11:12:08 -0600
Subject: [R] minus I and minus L flags
In-Reply-To: <200502201109.j1KB8jOX030138@hypatia.math.ethz.ch>
References: <200502201109.j1KB8jOX030138@hypatia.math.ethz.ch>
Message-ID: <20050220171208.GA13943@stat.umn.edu>

I have been RTFM/doc/www, but I'm still lost.

How does one tell R CMD INSTALL and R CMD check (both) that libraries
are installed in non-usual places and so -I/APPS/include (or whatever)
is necessary in CPPFLAGS and -L/APPS/lib (similarly) is necessary
in LDFLAGS?

I know I can hardwire them in pkg/src/Makevars, but this requires hand
editing of that file by each installer (yuck!)

rgentlem suggested some tricks with configure, but after a lot of grovelling
in autoconf manuals and books, I still don't get it.

Surely this is a pre-solved problem (I hopefully assert).  Is there a standard
solution, and if so where is the example I follow?
-- 
Charles Geyer
Professor, School of Statistics
University of Minnesota
charlie at stat.umn.edu



From p.dalgaard at biostat.ku.dk  Sun Feb 20 18:43:41 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Feb 2005 18:43:41 +0100
Subject: [R] Treatment-Contrast Interactions
In-Reply-To: <4218B1CF.4080807@cs.umd.edu>
References: <42182376.40602@cs.umd.edu>
	<Pine.LNX.4.61.0502200953210.27414@gannet.stats>
	<4218B1CF.4080807@cs.umd.edu>
Message-ID: <x2y8djxh2q.fsf@biostat.ku.dk>

Lorin Hochstein <lorin at cs.umd.edu> writes:

> I'd like to understand this approach as well, but I can't reproduce my
> results using se.contrast. In particular, I get the same standard
> error even though I tried to use different contrasts:
> 
>  > c1 <- c(1,-1)[A]*c(1,-1,0)[B]
>  > c2 <- c(1,-1)[A]*c(1,0,-1)[B]
>  > c3 <- c(1,-1)[A]*c(0,1,-1)[B]
>  > se.contrast(fit, as.matrix(c1))
> Contrast 1
>   14.24547
>  > se.contrast(fit,as.matrix(c2))
> Contrast 1
>   14.24547
>  > se.contrast(fit,as.matrix(c3))
> Contrast 1
>   14.24547

They could well _be_ the same if the design is balanced...
 
-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From edd at debian.org  Sun Feb 20 18:58:22 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 20 Feb 2005 11:58:22 -0600
Subject: [R] minus I and minus L flags
In-Reply-To: <20050220171208.GA13943@stat.umn.edu>
References: <200502201109.j1KB8jOX030138@hypatia.math.ethz.ch>
	<20050220171208.GA13943@stat.umn.edu>
Message-ID: <16920.53182.198048.677096@basebud.nulle.part>


On 20 February 2005 at 11:12, Charles Geyer wrote:
| I have been RTFM/doc/www, but I'm still lost.
| 
| How does one tell R CMD INSTALL and R CMD check (both) that libraries
| are installed in non-usual places and so -I/APPS/include (or whatever)
| is necessary in CPPFLAGS and -L/APPS/lib (similarly) is necessary
| in LDFLAGS?
| 
| I know I can hardwire them in pkg/src/Makevars, but this requires hand
| editing of that file by each installer (yuck!)
| 
| rgentlem suggested some tricks with configure, but after a lot of grovelling
| in autoconf manuals and books, I still don't get it.
| 
| Surely this is a pre-solved problem (I hopefully assert).  Is there a standard
| solution, and if so where is the example I follow?

FWIW, I have followed other packages when I crafted this for RQuantLib.

i)  src/Makevars.in  is a stanza that gets filled by Autoconf:

    edd at chibud:~/src/debian/QuantLib/RQuantLib-0.1.11> cat src/Makevars.in
    PKG_CXXFLAGS=@pkg_cxxflags@
    PKG_LIBS=@pkg_libs@

ii) The actual work is done by configure at package built time. That is the
single best method.  You can snoop tricks from dozens of CRAN packages
that do the same:

I crafted a reasonably short file:

    edd at chibud:~/src/debian/QuantLib/RQuantLib-0.1.11> wc -l configure.in
    149 configure.in

that tests for both QuantLib and Boost headers and libraries. Personally, I
find going by working example the easiest, with the documentation (for
autoconf) at my side.

That said, I also benefited from tips by Kurt who wears a black belt
in autoconf magic.

    edd at chibud:~/src/debian/QuantLib/RQuantLib-0.1.11> head -8 configure.in
    # RQuantLib configure.in by Dirk Eddelbuettel <edd at debian.org>
    # Borrowed pieces from RPgSQL, GNU Gretl, GNU R and QuantLib
    #
    # Greatly simplified thanks to quantlib-config
    # Another contribution by Kurt Hornik gratefully acknowledged
    #
    # $Id: configure.in,v 1.1 2004/12/22 04:07:57 edd Exp $

The fact that this credits RPgSQL is probably a reflection of the point in
time when I first wrote this several years ago -- CRAN was considerably
smaller at the time.

Hope this helps. Dirk

-- 
Better to have an approximate answer to the right question than a precise 
answer to the wrong question.  --  John Tukey as quoted by John Chambers



From Innkeyp-r at yahoo.com  Sun Feb 20 20:54:05 2005
From: Innkeyp-r at yahoo.com (T Petersen)
Date: Sun, 20 Feb 2005 20:54:05 +0100
Subject: [R] bug in example(hist)?
In-Reply-To: <42189848.7030703@statistik.uni-dortmund.de>
References: <4218718C.5040601@yahoo.com>
	<42189848.7030703@statistik.uni-dortmund.de>
Message-ID: <4218EADD.9070701@yahoo.com>

Alright. I typed

 > example(hist)

and I get (one) botched histogram with the title "WRONG diagram", while 
the other four diagrams won't run. I've tried to type

 > par(mfrow=c(2,3))

to see if I can get displayed the other diagrams, but there's still just 
the "WRONG diagram"-histogram. If I comment out the last two lines of 
the example(hist)-script

#hist> hist(islands, br = c(12, 20, 36, 80, 200, 1000, 17000),
#    freq = TRUE, main = "WRONG histogram")

the other diagrams come out fine.

Anyways, it's a small issue, but it just caught me off guard, as I'm a 
newbie trying to get into R...

My version of R is 2.0.1 and I'm running Windows XP.

Cheers

T Petersen

Uwe Ligges wrote:

> T Petersen wrote:
>
>> I tried example(hist) today, but it doesn't want to return the proper 
>> graphs ...Anyone else seeing this?
>
>
> T Petersen,
>
> 0) please read the posting guide,
> 1) please define "doesn't want to return the proper graphs", and
> 2) please tell us what OS and what device are you using.
>
> Uwe Ligges
>
>
>
>> I'm using R 2.0.1
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From p.dalgaard at biostat.ku.dk  Sun Feb 20 21:23:37 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Feb 2005 21:23:37 +0100
Subject: [R] bug in example(hist)?
In-Reply-To: <4218EADD.9070701@yahoo.com>
References: <4218718C.5040601@yahoo.com>
	<42189848.7030703@statistik.uni-dortmund.de>
	<4218EADD.9070701@yahoo.com>
Message-ID: <x2oeefx9o6.fsf@biostat.ku.dk>

T Petersen <Innkeyp-r at yahoo.com> writes:

> Alright. I typed
> 
>  > example(hist)
> 
> and I get (one) botched histogram with the title "WRONG diagram",
> while the other four diagrams won't run. I've tried to type
> 
>  > par(mfrow=c(2,3))
> 
> to see if I can get displayed the other diagrams, but there's still
> just the "WRONG diagram"-histogram. If I comment out the last two
> lines of the example(hist)-script
> 
> #hist> hist(islands, br = c(12, 20, 36, 80, 200, 1000, 17000),
> #    freq = TRUE, main = "WRONG histogram")
> 
> the other diagrams come out fine.

par(ask=TRUE) solves your problem. Setting par(mfrow...) is normally a
good idea, but it's not working when the example is setting it itself.

It's not a bug, for sure. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Warren.Jin at csiro.au  Sun Feb 20 22:35:31 2005
From: Warren.Jin at csiro.au (Warren.Jin@csiro.au)
Date: Mon, 21 Feb 2005 08:35:31 +1100
Subject: [R] Ask for help on the parallel function output to a pdf file
Message-ID: <9EF4A5826DD960459F3A015742550B3D2B7CFD@exactn2-cbr.nexus.csiro.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050221/638f086a/attachment.pl

From deepayan at stat.wisc.edu  Sun Feb 20 23:01:21 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Sun, 20 Feb 2005 16:01:21 -0600
Subject: [R] Ask for help on the parallel function output to a pdf file
In-Reply-To: <9EF4A5826DD960459F3A015742550B3D2B7CFD@exactn2-cbr.nexus.csiro.au>
References: <9EF4A5826DD960459F3A015742550B3D2B7CFD@exactn2-cbr.nexus.csiro.au>
Message-ID: <200502201601.21754.deepayan@stat.wisc.edu>




http://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-do-lattice_002ftrellis-graphics-not-work_003f


On Sunday 20 February 2005 15:35, Warren.Jin at csiro.au wrote:
> Dear all,
>
>
>
> Could you please give me a hand on parallel function from package
> lattice?
>
>
>
> On the R command console, the following commands can output a
> parallel coordinate plot to the pdf file.
>
> library(lattice); pdf("trial.pdf"); data(iris); parallel(~iris[1:4] |
> Species, iris); dev.off();
>
> However, if these commands are encapsulated into a function, the
> resultant pdf file is empty. For example, we define a simple function
> paraExam below, paraExam() may generate a pdf file which is empty
> from a pdf viewer.
>
> paraExam<-function(file="trial.pdf"){
>
>  library(lattice);
>
>  pdf(file);
>
>  data(iris);
>
>  parallel(~iris[1:4] | Species, iris);
>
>  dev.off();
>
>  }
>
>
>
> Please give me some tips to output parallel coordinate plots to a pdf
> file within a function successfully?



From edd at debian.org  Sun Feb 20 23:09:00 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 20 Feb 2005 16:09:00 -0600
Subject: [R] Ask for help on the parallel function output to a pdf file
In-Reply-To: <9EF4A5826DD960459F3A015742550B3D2B7CFD@exactn2-cbr.nexus.csiro.au>
References: <9EF4A5826DD960459F3A015742550B3D2B7CFD@exactn2-cbr.nexus.csiro.au>
Message-ID: <16921.2684.113117.334671@basebud.nulle.part>


On 21 February 2005 at 08:35, Warren.Jin at csiro.au wrote:
| However, if these commands are encapsulated into a function, the
| resultant pdf file is empty. For example, we define a simple function
| paraExam below, paraExam() may generate a pdf file which is empty from a
| pdf viewer. 
| 
| paraExam<-function(file="trial.pdf"){
|  library(lattice);
|  pdf(file);
|  data(iris);
|  parallel(~iris[1:4] | Species, iris);
|  dev.off();
|  }

It's the (current) mother of all FAQs: You need a print() about your lattice
plot command, i.e.

   print(parallel(~iris[1:4] | Species, iris))

Dirk

PS You can drop the ';'.


-- 
Better to have an approximate answer to the right question than a precise 
answer to the wrong question.  --  John Tukey as quoted by John Chambers



From spencer.graves at pdf.com  Sun Feb 20 23:30:54 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 20 Feb 2005 14:30:54 -0800
Subject: [R] Time series documentation?
In-Reply-To: <loom.20050220T063906-894@post.gmane.org>
References: <b040cbb005021821225f255fca@mail.gmail.com>	<4217B0B9.8060707@pdf.com>	<loom.20050220T015540-671@post.gmane.org>	<421815D4.7030802@pdf.com>
	<loom.20050220T063906-894@post.gmane.org>
Message-ID: <42190F9E.7050109@pdf.com>

Hello, Kum-Hoe: 

      Have you considered the "strucchange" package?  Inspired and 
informed by Gabor's comments, it looks to me at the moment like this 
package help you fit intervention / regression fits with time series / 
ARMAX models with minimum hassle. 

      hope this helps. 
      spencer graves

Gabor Grothendieck wrote:

>Spencer Graves <spencer.graves <at> pdf.com> writes:
>
>: 
>:       Thanks.  Which package(s) do you prefer for which purposes? 
>
>'ts' can be used for regularly spaced time series and supports 
>monthly and quarterly dates.  The other ones listed below supports 
>irregular time series.
>
>zoo's design goals are consistency with base R, ability to use many
>datetime classes (including Date, chron and POSIXct) and S3.  It has
>some specific features of note in the areas of joins, graphics and
>regression interfaces and includes a 15 page pdf vignette and reference
>card written in Sweave.  Achim Zeileis initiated zoo and I joined 
>the development last year.  Achim's strucchange package uses zoo.
>
>'its' is based on POSIXct and S4 and has been proven in real financial
>applications.
>
>fBasics is part of the rmetrics.org project and is consistent with
>the S-Plus FinMetrics software and would be of interest to someone
>who was interested in compatability with that as well as
>the broad range of financial functions in rmetrics.  It uses S4
>and POSIXct underneath with the modification that it uses the Olsen 
>time zone data base rather than the POSIXct scheme for time zones.  
>It does require that you set the process or computer to GMT.  It has
>extensive accompanying documentation and a web site (google for rmetrics).
>
>Its possible to convert series from one package to another.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From ripley at stats.ox.ac.uk  Sun Feb 20 23:53:59 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 20 Feb 2005 22:53:59 +0000 (GMT)
Subject: [R] problem with se.contrast()
In-Reply-To: <16916.50894.173426.42673@stat.math.ethz.ch>
References: <42139BF7.2090809@stat.ufl.edu>
	<16916.50894.173426.42673@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.61.0502202250090.2583@gannet.stats>

On Thu, 17 Feb 2005, Christoph Buser wrote:

> Dear Jamie
>
> As Prof. Ripley explained your analysis is equivalent to the fixed effect
> models for the means, so you can calculate it by (if this is your design):
>
>> Lab <- factor(rep(c("1","2","3"),each=12))
>> Material <- factor(rep(c("A","B","C","D"),each=3,times=3))
>> Measurement <- c(12.20,12.28,12.16,15.51,15.02,15.29,18.14,18.08,18.21,
>>                  18.54,18.36,18.45,12.59,12.30,12.67,14.98,15.46,15.22,
>>                  18.54,18.31,18.60,19.21,18.77,18.69,12.72,12.78,12.66,
>>                  15.33,15.19,15.24,18.00,18.15,17.93,18.88,18.12,18.03)
>> testdata <- data.frame(Lab,Material,Measurement)
>> rm(list=c("Lab","Material","Measurement"))
>
> You can aggregate your data
>
>> dat.mean <- aggregate(testdata$Measurement,
>>                       by = list(Material=testdata$Material,Lab=testdata$Lab),
>>                       FUN = mean)
>> names(dat.mean)[3] <- "Measurement"
>
>> test.red.aov1 <- aov(Measurement ~ Lab + Material, data = dat.mean)
>> se.contrast(test.red.aov1,
>>             list(Material=="A",Material=="B",Material=="C",Material=="D"),
>>             coef=c(0.5,0.5,-0.5,-0.5),dat.mean)
>> [1] 0.1220339
>
> By aggregating the data you bypass the problem in se.contrast and you do
> not need R-devel.

Note that this is not the same contrast, and you need to double the value 
for comparability with the original problem.

> -----------------------------------------------------------------------------
>
> The second way to get the same is to set your contrast for the factor
> "Material" and calculate you model with this contrast and use summary.lm:
>
>> dat.mean$Material <- C(dat.mean$Material, c(-0.5,-0.5,0.5,0.5),
>>                        how.many = 3)
>> test.red.aov2 <- aov(Measurement ~ Lab + Material, data = dat.mean)
>> summary.lm(test.red.aov2)
>
> Coefficients:
>            Estimate Std. Error t value Pr(>|t|)
> (Intercept) 16.02000    0.10568 151.583 5.56e-12 ***
> Lab2         0.25833    0.14946   1.728    0.135
> Lab3         0.06583    0.14946   0.440    0.675
> Material1    4.52278    0.12203  37.062 2.58e-08 ***
> Material2    1.21056    0.12203   9.920 6.06e-05 ***
> Material3    1.55389    0.12203  12.733 1.44e-05 ***
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
>
> Material1 is now the contrast you are interested in and you get beside the
> Std. Error the Estimate, too. Material2 and Material3 are just orthogonal
> contrasts to complete.
>
> -----------------------------------------------------------------------------
>
> The third way (which I prefer) is using lme from the package nlme
>
>> library(nlme)
>
> Here I use the origianl data set (not aggregated) and set the desired
> contrast, too.
>
>> testdata$Material <- C(testdata$Material, c(-0.5,-0.5,0.5,0.5),
>>                        how.many = 3)
>> test.lme <- lme(fixed = Measurement ~ Material, data = testdata,
>>                 random = ~1|Lab/Material)
>
> With anova you get the F-Test for the fixed factor "Material"
>
>> anova(test.lme)
>>           numDF denDF  F-value p-value
>  (Intercept)     1    24 43301.14  <.0001
>  Material        3     6   544.71  <.0001
>
> and with the summary you have your contrast with standard error:
>
>> summary(test.lme)
> Fixed effects: Measurement ~ Material
>                Value  Std.Error DF   t-value p-value
> (Intercept) 16.128056 0.07750547 24 208.08925   0e+00
> Material1    4.522778 0.12203325  6  37.06185   0e+00
> Material2    1.210556 0.12203325  6   9.91988   1e-04
> Material3    1.553889 0.12203325  6  12.73332   0e+00
>
> -----------------------------------------------------------------------------
>
> Last but not least I tried it with R-devel and the original data frame:
> First I reset the contrast on the default value:
>
> testdata$Material <- C(testdata$Material, "contr.treatment", how.many = 3)
>
> I used your syntax and se.contrast():
>
>> test.aov <- with(testdata,aov(Measurement ~ Material + Error(Lab/Material)))
>> se.contrast(test.aov,
>>             list(Material=="A",Material=="B",Material=="C",Material=="D"),
>>             coef=c(0.5,0.5,-0.5,-0.5),data=testdata)
>> [1] 0.1432572
>
> I got a different result and I have admit that I didn't understand why
> there is a differnce between the lme model and this one. There are some
> comments in the help pages but I'm not sure if this is the answer.

It is.  You used the default `constrasts', which are not actually 
contrasts.  With

options(contrasts=c("contr.helmert", "contr.poly"))

it gives the same answer as the other two.  Because you used non-contrasts 
there was an efficiency loss (to the Intercept stratum).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From john.maindonald at anu.edu.au  Mon Feb 21 01:05:00 2005
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Mon, 21 Feb 2005 11:05:00 +1100
Subject: [R] is.matrix(), as.matrix, & as(,"matrix")
Message-ID: <ecf49dfa8f6b279da8cf9cd36e2bd401@anu.edu.au>

Under help(matrix) it is written:

      'is.matrix' tests if its argument is a (strict) matrix. It is
      generic: you can write methods to handle specific classes of
      objects, see InternalMethods.

Further down, under "Details", the meaning of "strict" is explained
more explicitly:
      'is.matrix' returns 'TRUE' if 'x' is a matrix (i.e., it is _not_ a
      'data.frame' and has a 'dim' attribute of length 2) and 'FALSE'

(i.e., "strict" means "matrix" in a broad sense)

# The following is consistent with this:
 > tab <- with(ToothGrowth, table(supp, dose))
 > is.matrix(tab)
[1] TRUE
 > class(as.matrix(tab))
[1] "table"

However the function as() has an argument "strict" that has the
connotation "restricted sense:

   strict: A logical flag.  If 'TRUE', the returned object must be
           strictly from the target class (unless that class is a
           virtual class, in which case the object will be from the
           closest actual class (often the original object, if that
           class extends the virtual class directly).

# The following is consistent with this:
 > class(as(tab,"matrix", strict=TRUE))  # TRUE is the default
[1] "matrix"
 > class(as(tab,"matrix", strict=FALSE))  # TRUE is the default
[1] "table"

# Note also:
 > class(data.matrix(tab))
[1] "table"

At the very least, the word "(strict)" should be removed from
      "'is.matrix' tests if its argument is a (strict) matrix."
and replaced by something like "(in the broad sense defined below)".

It would make for greater consistency and convenience if all of
as.matrix(), is.matrix() and data.matrix()
had arguments "strict", where "strict=FALSE" would preserve
the present behaviour.

Additionally, it would be useful to mention, under the documentation
for matrix() and data.matrix(), that as.matrix(tab) is equivalent to
as(tab, "matrix", strict=FALSE) (is that strictly correct?)

I often want to use xtable() with tables.  There is no method
defined for the class "table".  After a bit of rummaging, I found
that I can use:
xtable(as(tab, "matrix")).

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Bioinformation Science, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.



From p.connolly at hortresearch.co.nz  Mon Feb 21 01:58:57 2005
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Mon, 21 Feb 2005 13:58:57 +1300
Subject: [R] help on deleting NAs
In-Reply-To: <OF6BFF407D.265DEFED-ON86256FAC.0075F83F-86256FAC.00772613@mdac
	c.tmc.edu>
References: <4215AA4F.31575.40D143@localhost> 
	<OF6BFF407D.265DEFED-ON86256FAC.0075F83F-86256FAC.00772613@mdacc.tmc.edu>
Message-ID: <20050221005857.GX22446@hortresearch.co.nz>

On Fri, 18-Feb-2005 at 03:41PM -0600, KeLin at mdanderson.org wrote:

|> Thanks a bunch.
|> 
|> I happened to generate another dataset to try the command you all 
|> provided. my.data$group is factor and so the "!my.data$group" command 
|> doesn't work but the "!with" statement works for both situation. So what i 
|> can modify and make the first one work, just curious. 

Try an as.character() with the factor.

HTH

-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From ggrothendieck at myway.com  Mon Feb 21 02:02:49 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 21 Feb 2005 01:02:49 +0000 (UTC)
Subject: [R] is.matrix(), as.matrix, & as(,"matrix")
References: <ecf49dfa8f6b279da8cf9cd36e2bd401@anu.edu.au>
Message-ID: <loom.20050221T015847-100@post.gmane.org>

John Maindonald <john.maindonald <at> anu.edu.au> writes:

: I often want to use xtable() with tables.  There is no method
: defined for the class "table".  After a bit of rummaging, I found
: that I can use:
: xtable(as(tab, "matrix")).

That's probably ok for most practical purposes but it will
not preserve the table's dnn, if it has one:

R> tab <- table(1:3, dnn = "A")
R> tab  # dnn present
A
1 2 3 
1 1 1 
R> as.matrix(tab)  # dnn gone
  [,1]
1    1
2    1
3    1



From Glen.Jones at team.telstra.com  Mon Feb 21 02:45:06 2005
From: Glen.Jones at team.telstra.com (Jones, Glen R)
Date: Mon, 21 Feb 2005 12:45:06 +1100
Subject: [R] Sorting a matrix on two columns
Message-ID: <5D01E8305096D3119D7D00508B5EBBF4156EA123@ntmsg0133.corpmail.telstra.com.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050221/22744c07/attachment.pl

From p.dalgaard at biostat.ku.dk  Mon Feb 21 03:29:55 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 21 Feb 2005 03:29:55 +0100
Subject: [R] problem with se.contrast()
In-Reply-To: <Pine.LNX.4.61.0502202250090.2583@gannet.stats>
References: <42139BF7.2090809@stat.ufl.edu>
	<16916.50894.173426.42673@stat.math.ethz.ch>
	<Pine.LNX.4.61.0502202250090.2583@gannet.stats>
Message-ID: <x2k6p2y7a4.fsf@biostat.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> >> test.aov <- with(testdata,aov(Measurement ~ Material + Error(Lab/Material)))
> >> se.contrast(test.aov,
> >>             list(Material=="A",Material=="B",Material=="C",Material=="D"),
> >>             coef=c(0.5,0.5,-0.5,-0.5),data=testdata)
> >> [1] 0.1432572
> >
> > I got a different result and I have admit that I didn't understand why
> > there is a differnce between the lme model and this one. There are some
> > comments in the help pages but I'm not sure if this is the answer.
> 
> It is.  You used the default `constrasts', which are not actually
> contrasts.  With
> 
> options(contrasts=c("contr.helmert", "contr.poly"))
> 
> it gives the same answer as the other two.  Because you used
> non-contrasts there was an efficiency loss (to the Intercept stratum).

Brian, 

I'm not sure how useful that contrasts-that-are-not-contrasts line is.
It certainly depends on your definition of "contrasts". Contrast
matrices having zero column sums was not part of the definition I was
taught. I have contrasts as "representations of the group mean
structure that are invariant to changes of the overall level", so
treatment contrasts are perfectly good contrasts in my book. 

The zero-sum condition strikes me as a bit arbitrary: after all there
are perfectly nice orthogonal designs where some levels of a factor
occur more frequently than others. This in turn makes me a bit wary of
what is going on inside se.contrasts, but it's gotten too late for me
to actually study the code tonight.

Could you elaborate on where precisely the condition on the contrast
matrices comes into play?

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From jmronca at wisc.edu  Mon Feb 21 04:34:43 2005
From: jmronca at wisc.edu (Justin Ronca)
Date: Sun, 20 Feb 2005 21:34:43 -0600
Subject: [R] Problem opening and saving script files -- R crashes
Message-ID: <000e01c517c6$48733380$6701a8c0@justindesktop>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050220/9480aa47/attachment.pl

From spencer.graves at pdf.com  Mon Feb 21 06:11:42 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 20 Feb 2005 21:11:42 -0800
Subject: [R] Sorting a matrix on two columns
In-Reply-To: <5D01E8305096D3119D7D00508B5EBBF4156EA123@ntmsg0133.corpmail.telstra.com.au>
References: <5D01E8305096D3119D7D00508B5EBBF4156EA123@ntmsg0133.corpmail.telstra.com.au>
Message-ID: <42196D8E.4070809@pdf.com>

    Have you considered "order"?  The help file includes examples from 
which you should be able to see how to do what you want. 

      hope this helps. 
      spencer graves    

Jones, Glen R wrote:

>Hello,
>
>If a matrix with 5 columns has been defined and the first two columns
>need to be sorted in ascending order, how can this be achieved whilst
>ensuring the 
>other 3 columns data are in relative position to the sorted columns?
>
>
>Glen Jones
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From ripley at stats.ox.ac.uk  Mon Feb 21 08:44:38 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 21 Feb 2005 07:44:38 +0000 (GMT)
Subject: [R] problem with se.contrast()
In-Reply-To: <x2k6p2y7a4.fsf@biostat.ku.dk>
References: <42139BF7.2090809@stat.ufl.edu>
	<16916.50894.173426.42673@stat.math.ethz.ch>
	<Pine.LNX.4.61.0502202250090.2583@gannet.stats>
	<x2k6p2y7a4.fsf@biostat.ku.dk>
Message-ID: <Pine.LNX.4.61.0502210608250.7362@gannet.stats>

On Mon, 21 Feb 2005, Peter Dalgaard wrote:

> Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:
>
>>>> test.aov <- with(testdata,aov(Measurement ~ Material + Error(Lab/Material)))
>>>> se.contrast(test.aov,
>>>>             list(Material=="A",Material=="B",Material=="C",Material=="D"),
>>>>             coef=c(0.5,0.5,-0.5,-0.5),data=testdata)
>>>> [1] 0.1432572
>>>
>>> I got a different result and I have admit that I didn't understand why
>>> there is a differnce between the lme model and this one. There are some
>>> comments in the help pages but I'm not sure if this is the answer.
>>
>> It is.  You used the default `contrasts', which are not actually
>> contrasts.  With
>>
>> options(contrasts=c("contr.helmert", "contr.poly"))
>>
>> it gives the same answer as the other two.  Because you used
>> non-contrasts there was an efficiency loss (to the Intercept stratum).
>
> Brian,
>
> I'm not sure how useful that contrasts-that-are-not-contrasts line is.

I agree, it was not precise enough (too late at night for me).  Try 
`non-orthogonal contrasts'.  The issue was correct though, it is the 
choice of contrasts, and as I would automatically use orthogonal contrasts 
with aov() I had not encountered it and it took me a while to pick up on 
what Christoph had done differently from me (I had run the example and got 
the same result as the randomized-block analysis before my original post).

There's a comment in the code for aov:

         ##  helmert contrasts can be helpful: do we want to force them?
         ##  this version does for the Error model.

and perhaps we should make them the default for the per-stratum fits.

> It certainly depends on your definition of "contrasts". Contrast
> matrices having zero column sums was not part of the definition I was
> taught. I have contrasts as "representations of the group mean
> structure that are invariant to changes of the overall level", so
> treatment contrasts are perfectly good contrasts in my book.

I don't think Yates thought in those terms, and the whole idea of dividing 
into strata (and the generalized Yates algorithm) is based on contrasts 
being orthogonal to the overall mean (and to things in other strata).

It was that, not zero-sum, that I was taught, but in balanced cases such 
as here it is the same thing.

> The zero-sum condition strikes me as a bit arbitrary: after all there
> are perfectly nice orthogonal designs where some levels of a factor
> occur more frequently than others.

Balance and orthogonality are not quite the same, though.

> This in turn makes me a bit wary of what is going on inside 
> se.contrasts, but it's gotten too late for me to actually study the code 
> tonight.
>
> Could you elaborate on where precisely the condition on the contrast
> matrices comes into play?

In finding the projection lengths in eff.aovlist, here

     proj.len <-
 	lapply(aovlist, function(x)
 	   {
 	       asgn <- x$assign[x$qr$pivot[1:x$rank]]
 	       sp <- split(seq(along=asgn), attr(terms(x), "term.labels")[asgn])
 	       sapply(sp, function(x, y) sum(y[x]), y=diag(x$qr$qr)^2)
 	   })

using only the diagonal requires orthogonality of the coding.

It is many years since I looked at this, and it is not immediately clear 
to me how best to calculate efficiencies without this assumption (which 
could be tested inside eff.aovlist, of course).

[People wondering why this was ever useful given lme should be aware that
1) It predates the availability of lme.
2) When I first used this in 1998, lme appeared very slow.
3) People with a classical aov training (not me, but e.g. Bill Venables) 
think in these terms.]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From robut at iinet.net.au  Mon Feb 21 08:46:26 2005
From: robut at iinet.net.au (Robert Cunningham)
Date: Mon, 21 Feb 2005 15:46:26 +0800
Subject: [R] Hosting a R Graph Gallery?
In-Reply-To: <42163271.5040906@free.fr> (Romain Francois's message of "Fri,
	18 Feb 2005 19:22:41 +0100")
References: <4215EEBC.6070304@oomvanlieshout.net> <42163271.5040906@free.fr>
Message-ID: <m11xbawe25.fsf@iinet.net.au>

I too have often though a R-gallery would be useful.

It seems to me that a Wiki-style page with a database backend would be
the best bet.

It also seems to be that the best place to start is a complete image
gallery produced from all the examples in R base, then in packages in
CRAN. In this context the graphicsQC package
(http://www.stat.auckland.ac.nz/~paul/R/graphicsQC_0.4.tar.g) of Paul
Murrell seems useful.

Cheers, 


Robert Cunningham



Romain Francois <francoisromain at free.fr> writes:

> Hello Sander,
>
> That's a good idea and i am up to it.
>
> Right now i am in an exam period, so it's not really the better time,
> give me a couple of weeks and i will come up with a specific format of
> R files to submit to me that i could post-process to generate html
> documents.
> To my mind, those html files should show :
>
> - the plot itself
> + Submitter(s)
>         - web page
>         - email (eventually protected, I don't know how to do it)
> - Bibliographic references
> - Required R packages
> + Commentaries
>        - in english
>        - and in any other languages
>
> I'm open to any suggestion.
>
> Romain.
>
> Le 18.02.2005 14:33, Sander Oom a ?crit :
>
>> Dear R users,
>>
>> Following some of the recent questions and discussions about the R
>> plotting abilities, it occurred to me again that it would be very
>> valuable to have an R graph gallery.
>>
>> Eric Lecoutre made a very nice example in:
>> http://www.stat.ucl.ac.be/ISpersonnel/lecoutre/stats/fichiers/_gallery.pdf
>>
>>
>> It would be very useful to many beginners, but probably also
>> advanced users of R, to have an overview of R graph types with
>> graphical examples  and associated R code.
>>
>> In order to facilitate the evolution of a large gallery, some sort
>> of wiki environment might be most suitable, thus providing access to
>> all users, but with limited maintenance costs for the provider.
>>
>> Do others agree this could be a valuable resource? Would anybody
>> have the resources to host such an R graph gallery?
>>
>> Yours,
>>
>> Sander Oom.
>>
> -- 
> Romain FRANCOIS : francoisromain at free.fr
> page web : http://addictedtor.free.fr/  (en construction)
> 06 18 39 14 69 / 01 46 80 65 60
> _______________________________________________________
> Etudiant en 3eme ann?e
> Institut de Statistique de l'Universit? de Paris (ISUP)
> Fili?re Industrie et Services
> http://www.isup.cicrp.jussieu.fr/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From alserkli at inbox.ru  Mon Feb 21 09:22:17 2005
From: alserkli at inbox.ru (Alexander Klimov)
Date: Mon, 21 Feb 2005 10:22:17 +0200 (IST)
Subject: [R] LD_LIBRARY_PATH is harmfull
Message-ID: <TheMailAgent.1bca527c645a4ba@2188b157a0da1944051f3>

Hi.

Consider the following situation: Solaris 8, /usr/local has gcc 2.95,
my home has gcc 3.4.3, my gcc is first on PATH, LD_LIBRARY_PATH is
unset (everything is perfect since I always use -R). In fact, programs
compiled with my gcc do not work if LD_LIBRARY_PATH is set to
something which has /usr/local/lib before home/lib, because it
overrides stored path (-R) and I got

   libgcc_s.so.1 (GCC_3.3) =>       (version not found)

I found setting of LD_LIBRARY_PATH in bin/R (how /usr/local/lib get
into it at all and especially before PREFIX/lib??? -- there was no
LD_LIBRARY_PATH during configure!) and fixed it (although I spent
quite a while editing lib/R/bin/R and wondering an abscence of
any effect :-)

After all the troubles I manage to load an extension, but, frankly, I
think there were too many problems. It would be very nice if R-project
reject the idea of using LD_LIBRARY_PATH because its setting is
considered harmfull and leads to too many problems:
http://www.linuxmafia.com/faq/Admin/ld-lib-path.html

-- 
Regards,
ASK



From bela_b at gmx.net  Mon Feb 21 09:32:11 2005
From: bela_b at gmx.net (Bela Bauer)
Date: Mon, 21 Feb 2005 09:32:11 +0100
Subject: [R] Two-factorial Huynh-Feldt-Test
In-Reply-To: <x2braic8lo.fsf@biostat.ku.dk>
References: <4215A1BB.3020400@gmx.net>
	<x2r7je5iav.fsf@biostat.ku.dk>	<4215D2E0.5040606@cbs.mpg.de>
	<x2braic8lo.fsf@biostat.ku.dk>
Message-ID: <42199C8B.4080807@gmx.net>

Peter Dalgaard wrote:

 >My suspicion would be that it has something to do with calculating the
 >correction terms before or after contrast transformations (there must
 >be a coordinate-free version of the corrections?), but I can't grok
 >the details that easily.

I must admit that I'm not too familiar with the way R handles these 
calculations internally, so I don't quite see what you mean. Could you 
somehow clarify this, possibly by pointing me to relevant documentation?

 >However, if you peek over on the R-devel
 >list, you will see that I was just about to get serious with
 >programming some of this stuff as methods for the "mlm" class. I think
 >you just volunteered to test the code...


I'd be happy to do that. I've subscribed to R-devel following your last 
email, but I can't seem to be able to find the thread that you're 
referring to. Could you forward the relevent messages or the code to me?

Thanks again

Bela Bauer



From slist at oomvanlieshout.net  Mon Feb 21 09:49:30 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Mon, 21 Feb 2005 10:49:30 +0200
Subject: [R] Hosting a R Graph Gallery?
In-Reply-To: <m11xbawe25.fsf@iinet.net.au>
References: <4215EEBC.6070304@oomvanlieshout.net> <42163271.5040906@free.fr>
	<m11xbawe25.fsf@iinet.net.au>
Message-ID: <4219A09A.7090705@oomvanlieshout.net>

Thanks for the responses and offer!

Romain, good luck with your exams first of all!

Graphics in R base and R contributed is a good start indeed.

I thought of a wiki as well as it will require less maintenance from the 
host. Custom html could be powerful, but will require more input from 
host! With build-in search engines, indexing etc., a wiki environment 
could provide all functionality for an easy to navigate gallery!

To stimulate development and support of SVG functionality, we might want 
to offer people the option of submitting graphics in both png and svg!

The work by Jake looks very promising:
http://www.darkridge.com/~jake/RSvg/
The applet works well with the examples!

Cheers,

Sander.

Robert Cunningham wrote:
> I too have often though a R-gallery would be useful.
> 
> It seems to me that a Wiki-style page with a database backend would be
> the best bet.
> 
> It also seems to be that the best place to start is a complete image
> gallery produced from all the examples in R base, then in packages in
> CRAN. In this context the graphicsQC package
> (http://www.stat.auckland.ac.nz/~paul/R/graphicsQC_0.4.tar.g) of Paul
> Murrell seems useful.
> 
> Cheers, 
> 
> 
> Robert Cunningham
> 
> 
> 
> Romain Francois <francoisromain at free.fr> writes:
> 
> 
>>Hello Sander,
>>
>>That's a good idea and i am up to it.
>>
>>Right now i am in an exam period, so it's not really the better time,
>>give me a couple of weeks and i will come up with a specific format of
>>R files to submit to me that i could post-process to generate html
>>documents.
>>To my mind, those html files should show :
>>
>>- the plot itself
>>+ Submitter(s)
>>        - web page
>>        - email (eventually protected, I don't know how to do it)
>>- Bibliographic references
>>- Required R packages
>>+ Commentaries
>>       - in english
>>       - and in any other languages
>>
>>I'm open to any suggestion.
>>
>>Romain.
>>
>>Le 18.02.2005 14:33, Sander Oom a ?crit :
>>
>>
>>>Dear R users,
>>>
>>>Following some of the recent questions and discussions about the R
>>>plotting abilities, it occurred to me again that it would be very
>>>valuable to have an R graph gallery.
>>>
>>>Eric Lecoutre made a very nice example in:
>>>http://www.stat.ucl.ac.be/ISpersonnel/lecoutre/stats/fichiers/_gallery.pdf
>>>
>>>
>>>It would be very useful to many beginners, but probably also
>>>advanced users of R, to have an overview of R graph types with
>>>graphical examples  and associated R code.
>>>
>>>In order to facilitate the evolution of a large gallery, some sort
>>>of wiki environment might be most suitable, thus providing access to
>>>all users, but with limited maintenance costs for the provider.
>>>
>>>Do others agree this could be a valuable resource? Would anybody
>>>have the resources to host such an R graph gallery?
>>>
>>>Yours,
>>>
>>>Sander Oom.
>>>
>>
>>-- 
>>Romain FRANCOIS : francoisromain at free.fr
>>page web : http://addictedtor.free.fr/  (en construction)
>>06 18 39 14 69 / 01 46 80 65 60
>>_______________________________________________________
>>Etudiant en 3eme ann?e
>>Institut de Statistique de l'Universit? de Paris (ISUP)
>>Fili?re Industrie et Services
>>http://www.isup.cicrp.jussieu.fr/
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
---------------------------------------------------------
Dr. Sander P. Oom
Animal, Plant and Environmental Sciences
University of the Witwatersrand
Private Bag 3
Wits 2050
South Africa

Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64

Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From bbruemmer at web.de  Mon Feb 21 09:57:49 2005
From: bbruemmer at web.de (Bernhard Bruemmer)
Date: Mon, 21 Feb 2005 09:57:49 +0100
Subject: X11 copy & paste (was: Re: [R] Easy cut & paste from Excel to R?)
In-Reply-To: <x2u0o9bwqx.fsf@biostat.ku.dk> (Peter Dalgaard's message of "18
	Feb 2005 18:31:34 +0100")
References: <1108678164.4215161465ccc@webmail.lyon.inserm.fr>
	<p06210200be3bbc39a47e@[128.115.153.6]> <x2u0o9bwqx.fsf@biostat.ku.dk>
Message-ID: <874qg6l27m.fsf_-_@ug-uaao-c018.agrarokoenomie.agrar.uni-goettingen.de>

Peter Dalgaard <p.dalgaard at biostat.ku.dk> writes:

> Don MacQueen <macq at llnl.gov> writes:
>
>> I tried Ken's suggestion read.table(pipe("pbpaste"),header=TRUE) on
>> my Mac OS X system and it worked *without* generating any warning
>> message.
>> 
>> If my experience represents the norm, and Ken's is the exception, it
>> is so simple that no further contribution to R is needed, I would
>> say.  Thank you, Ken.
>
> My conjecture is that it only happens when there are fewer than 5 data
> lines.
>
> We still need to sort out X11. Too bad that the xclip program isn't
> ubiquitous.

Does Perl qualify as ubiquitous? If so, the piped xclip call can be
substituted for by the following:

data <- read.delim(pipe("perl -MTk -e 'print MainWindow->new->SelectionGet'"))

Works fine under Linux.

HTH, Bernhard



From Jussi.Makinen at valtiokonttori.fi  Mon Feb 21 10:06:32 2005
From: Jussi.Makinen at valtiokonttori.fi (=?iso-8859-1?Q?M=E4kinen_Jussi?=)
Date: Mon, 21 Feb 2005 11:06:32 +0200
Subject: [R] its plot with pch-argument
Message-ID: <0BDE2460F08BF0429F933A40431A61E801E82293@vk2kmail01.valtiokonttori.local>

Hi mighty R-gurus and other enthusiastics,

I just encountered this:

library(its)
x <- its(sort(rnorm(10)), as.POSIXct(Sys.time() + 1:10))
plot(x, type = "p", pch = c(rep("A", 5), rep("B", 5)))

Am I missing something if I expect that all the points labeled as 'A' should be below all those labeled as 'B'? 

Thanks,

Jussi M?kinen

platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    0.1            
year     2004           
month    11             
day      15             
language R



From jijunl at amgen.com  Mon Feb 21 10:13:38 2005
From: jijunl at amgen.com (Liu, Jane)
Date: Mon, 21 Feb 2005 01:13:38 -0800
Subject: [R] save plot as jpg/gif
Message-ID: <569E4D734A4F05448D603396CB1C651101CE08B5@USSF-MB1.amgen.com>


I am generating multiple plots and would like to save them as jpg or gif
files. Could someone tell me which function I shall use?

Thanks!



From ripley at stats.ox.ac.uk  Mon Feb 21 10:20:46 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 21 Feb 2005 09:20:46 +0000 (GMT)
Subject: X11 copy & paste (was: Re: [R] Easy cut & paste from Excel to R?)
In-Reply-To: <874qg6l27m.fsf_-_@ug-uaao-c018.agrarokoenomie.agrar.uni-goettingen.de>
References: <1108678164.4215161465ccc@webmail.lyon.inserm.fr>
	<p06210200be3bbc39a47e@[128.115.153.6]> <x2u0o9bwqx.fsf@biostat.ku.dk>
	<874qg6l27m.fsf_-_@ug-uaao-c018.agrarokoenomie.agrar.uni-goettingen.de>
Message-ID: <Pine.LNX.4.61.0502210919490.15235@gannet.stats>

On Mon, 21 Feb 2005, Bernhard Bruemmer wrote:

> Peter Dalgaard <p.dalgaard at biostat.ku.dk> writes:
>
>> Don MacQueen <macq at llnl.gov> writes:
>>
>>> I tried Ken's suggestion read.table(pipe("pbpaste"),header=TRUE) on
>>> my Mac OS X system and it worked *without* generating any warning
>>> message.
>>>
>>> If my experience represents the norm, and Ken's is the exception, it
>>> is so simple that no further contribution to R is needed, I would
>>> say.  Thank you, Ken.
>>
>> My conjecture is that it only happens when there are fewer than 5 data
>> lines.
>>
>> We still need to sort out X11. Too bad that the xclip program isn't
>> ubiquitous.
>
> Does Perl qualify as ubiquitous?

It is specifically not required for R at runtime: see the `Writing R 
Extensions' manual.

> If so, the piped xclip call can be
> substituted for by the following:
>
> data <- read.delim(pipe("perl -MTk -e 'print MainWindow->new->SelectionGet'"))
>
> Works fine under Linux.
>
> HTH, Bernhard
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From lecoutre at stat.ucl.ac.be  Mon Feb 21 10:20:25 2005
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Mon, 21 Feb 2005 10:20:25 +0100
Subject: [R] Hosting a R Graph Gallery?
In-Reply-To: <m11xbawe25.fsf@iinet.net.au>
References: <4215EEBC.6070304@oomvanlieshout.net> <42163271.5040906@free.fr>
	<m11xbawe25.fsf@iinet.net.au>
Message-ID: <6.0.1.1.2.20050221091626.02361e40@stat4ux.stat.ucl.ac.be>


Hi,

About any graph gallery:
Philippe Grojean and me did have made some work. Our goal was to add a clip 
library to the SciViews project that would offer access to a graph gallery. 
I was workiong on the production of the gallery, where as Philippe is still 
working on his GUI API. One of the goal is to have automatic wizards to 
make easier the creation of a graphic.

Here was our approach and some thoughts:

- We should propose a format for a description file. Here are some elements 
that should be gathered for each graphic function:
         - Name of the function (*)
         - Name of the produced graphic (*)
         - Description of the graphic (*)
         - Number of variables (univariate / bivariate / multivariate...)
         - Types of variables
         - Sample code (sample graph) (*)
         - Package (*)
The (*) are some information already available in Rd files (except maybe 
sample graph).

- If someone deos something, I think it would be useful to ensure that all 
is reusable. We should focus on describing graphics. Then, for example, 
SciViews could use the information to create a usable graph gallery.

If someone is interested, I ahve put in the following archive all my 
current code:
http://www.stat.ucl.ac.be/ISpersonnel/lecoutre/R/svGraphGallery.zip

There is no explanation but I would provide comments and help to any 
volonteer (basically, there is a file  .ggs with some descriptions as 
stated before and some R code to that produce HTML files).

The result (the current gallery) is there. It is aimed to be something like 
300 pixels large. At final step, graph would be clickable with a wizard.

http://www.stat.ucl.ac.be/ISpersonnel/lecoutre/R/svGraphGallery/dock/svGallery.html



Eric

At 08:46 21/02/2005, Robert Cunningham wrote:
>I too have often though a R-gallery would be useful.
>
>It seems to me that a Wiki-style page with a database backend would be
>the best bet.
>
>It also seems to be that the best place to start is a complete image
>gallery produced from all the examples in R base, then in packages in
>CRAN. In this context the graphicsQC package
>(http://www.stat.auckland.ac.nz/~paul/R/graphicsQC_0.4.tar.g) of Paul
>Murrell seems useful.
>
>Cheers,
>
>
>Robert Cunningham
>
>
>
>Romain Francois <francoisromain at free.fr> writes:
>
> > Hello Sander,
> >
> > That's a good idea and i am up to it.
> >
> > Right now i am in an exam period, so it's not really the better time,
> > give me a couple of weeks and i will come up with a specific format of
> > R files to submit to me that i could post-process to generate html
> > documents.
> > To my mind, those html files should show :
> >
> > - the plot itself
> > + Submitter(s)
> >         - web page
> >         - email (eventually protected, I don't know how to do it)
> > - Bibliographic references
> > - Required R packages
> > + Commentaries
> >        - in english
> >        - and in any other languages
> >
> > I'm open to any suggestion.
> >
> > Romain.
> >
> > Le 18.02.2005 14:33, Sander Oom a ?crit :
> >
> >> Dear R users,
> >>
> >> Following some of the recent questions and discussions about the R
> >> plotting abilities, it occurred to me again that it would be very
> >> valuable to have an R graph gallery.
> >>
> >> Eric Lecoutre made a very nice example in:
> >> http://www.stat.ucl.ac.be/ISpersonnel/lecoutre/stats/fichiers/_gallery.pdf
> >>
> >>
> >> It would be very useful to many beginners, but probably also
> >> advanced users of R, to have an overview of R graph types with
> >> graphical examples  and associated R code.
> >>
> >> In order to facilitate the evolution of a large gallery, some sort
> >> of wiki environment might be most suitable, thus providing access to
> >> all users, but with limited maintenance costs for the provider.
> >>
> >> Do others agree this could be a valuable resource? Would anybody
> >> have the resources to host such an R graph gallery?
> >>
> >> Yours,
> >>
> >> Sander Oom.
> >>
> > --
> > Romain FRANCOIS : francoisromain at free.fr
> > page web : http://addictedtor.free.fr/ (en construction)
> > 06 18 39 14 69 / 01 46 80 65 60
> > _______________________________________________________
> > Etudiant en 3eme ann?e
> > Institut de Statistique de l'Universit? de Paris (ISUP)
> > Fili?re Industrie et Services
> > http://www.isup.cicrp.jussieu.fr/
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Eric Lecoutre
UCL /  Institut de Statistique
Voie du Roman Pays, 20
1348 Louvain-la-Neuve
Belgium

tel: (+32)(0)10473050
lecoutre at stat.ucl.ac.be
http://www.stat.ucl.ac.be/ISpersonnel/lecoutre

If the statistics are boring, then you've got the wrong numbers. -Edward 
Tufte



From bxc at steno.dk  Mon Feb 21 10:24:38 2005
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Mon, 21 Feb 2005 10:24:38 +0100
Subject: [R] save plot as jpg/gif
Message-ID: <0ABD88905D18E347874E0FB71C0B29E9027FEB69@exdkba022.novo.dk>

?Devices

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Liu, Jane
> Sent: Monday, February 21, 2005 10:14 AM
> To: RHELP
> Subject: [R] save plot as jpg/gif
> 
> 
> 
> I am generating multiple plots and would like to save them as 
> jpg or gif files. Could someone tell me which function I shall use?
> 
> Thanks!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From p.dalgaard at biostat.ku.dk  Mon Feb 21 10:25:52 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 21 Feb 2005 10:25:52 +0100
Subject: [R] Two-factorial Huynh-Feldt-Test
In-Reply-To: <42199C8B.4080807@gmx.net>
References: <4215A1BB.3020400@gmx.net> <x2r7je5iav.fsf@biostat.ku.dk>
	<4215D2E0.5040606@cbs.mpg.de> <x2braic8lo.fsf@biostat.ku.dk>
	<42199C8B.4080807@gmx.net>
Message-ID: <x2psyujmcf.fsf@biostat.ku.dk>

Bela Bauer <bela_b at gmx.net> writes:

> Peter Dalgaard wrote:
> 
>  >My suspicion would be that it has something to do with calculating the
>  >correction terms before or after contrast transformations (there must
>  >be a coordinate-free version of the corrections?), but I can't grok
>  >the details that easily.
> 
> I must admit that I'm not too familiar with the way R handles these
> calculations internally, so I don't quite see what you mean. Could you
> somehow clarify this, possibly by pointing me to relevant
> documentation?

I was thinking of the way SAS does it. This is unusually unclear in
the documentation (you can say many things about SAS, but
underdocumentation is not normally one of them). The point is that the
G-G formula involving on- and off-diagonal terms of the empirical
covariance matrix cannot be expected to work if you're looking at
something other than the simple intercolumn contrasts (and you had a
nested two-way structure in the columns). 

I've been reading up on the original papers by G+G and importantly the
two papers by Box (1954) whose results they are using. As it turns
out, the fundamental approximation is in terms of eigenvalues, and
generalizes neatly to arbitrary sets of contrasts. All the unintuitive
stuff is really about fleshing this out in common-use cases in times
where practitioners would not be expected to carry out an eigenvalue
computation. The interesting bit is now whether an implementation of
the eigenvalue formula gives the same results as SAS gets. I'll get
there... 

 
>  >However, if you peek over on the R-devel
>  >list, you will see that I was just about to get serious with
>  >programming some of this stuff as methods for the "mlm" class. I think
>  >you just volunteered to test the code...
> 
> 
> I'd be happy to do that. I've subscribed to R-devel following your
> last email, but I can't seem to be able to find the thread that you're
> referring to. Could you forward the relevent messages or the code to
> me?
 
https://stat.ethz.ch/pipermail/r-devel/2005-February/032240.html

(No code as yet, it is just an outline)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Mon Feb 21 10:36:19 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 21 Feb 2005 09:36:19 +0000 (GMT)
Subject: [R] save plot as jpg/gif
In-Reply-To: <569E4D734A4F05448D603396CB1C651101CE08B5@USSF-MB1.amgen.com>
References: <569E4D734A4F05448D603396CB1C651101CE08B5@USSF-MB1.amgen.com>
Message-ID: <Pine.LNX.4.61.0502210930550.15731@gannet.stats>

On Mon, 21 Feb 2005, Liu, Jane wrote:

> I am generating multiple plots and would like to save them as jpg or gif
> files. Could someone tell me which function I shall use?

jpeg() or bitmap()

R does not support GIF files because of the patent difficulties until 
recently (and possibly still, although the US, European and Japanese 
patents have expired: http://cloanto.com/users/mcb/19950127giflzw.html) 
and a preference for PNG (see png()) which can easily be converted to gifs 
if needed.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Rau at demogr.mpg.de  Mon Feb 21 11:07:15 2005
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Mon, 21 Feb 2005 11:07:15 +0100
Subject: [R] Sorting a matrix on two columns
Message-ID: <8B08A3A1EA7AAC41BE24C750338754E651FF55@HERMES.demogr.mpg.de>

Hi Glen,

 
-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jones, Glen R

If a matrix with 5 columns has been defined and the first two columns
need to be sorted in ascending order, how can this be achieved whilst
ensuring the 
other 3 columns data are in relative position to the sorted columns?


does the following example-code help you?
mymatrix should resemble the structure of your (original) matrix.
mymatrix2 is the new, sorted matrix.

mymatrix <- matrix(runif(80), ncol=5)
mymatrix[,1] <- sample(c(1,2), size=length(mymatrix[,1]), replace=TRUE)
mymatrix

mymatrix2 <- mymatrix[order(mymatrix[,1],mymatrix[,2]),]
mymatrix2



Best,
Roland


+++++
This mail has been sent through the MPI for Demographic Rese...{{dropped}}



From phgrosjean at sciviews.org  Mon Feb 21 11:33:08 2005
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Mon, 21 Feb 2005 11:33:08 +0100
Subject: [R] Hosting a R Graph Gallery?
In-Reply-To: <6.0.1.1.2.20050221091626.02361e40@stat4ux.stat.ucl.ac.be>
References: <4215EEBC.6070304@oomvanlieshout.net>
	<42163271.5040906@free.fr>	<m11xbawe25.fsf@iinet.net.au>
	<6.0.1.1.2.20050221091626.02361e40@stat4ux.stat.ucl.ac.be>
Message-ID: <4219B8E4.2080903@sciviews.org>

Hello,

Just to add a word to what Eric said. I think a graph gallery for R 
would be a very nice initiative. Of course, the easiest way would be a 
series of HTML of WiKi pages, with downloadable example R code. However, 
if someone volunteers to start and maintain a 'GraphGallery' R package, 
it would be much better. This way, example code could be run simply with:
 > example(agraph)

Definitely, the weakness of the R-help system is that it does not allow 
pictures. Thus you have to actually run the examples to see the results. 
However, one could imagine a very simple function making thumbnails of 
all graphs in the gallery, using par(mfrow=...) and running all examples.

The complex format for graph galleries proposed by Eric, which is 
embedded in the development version of SciViews is probably not suitable 
for a wider use. A Graph Gallery for R should be better independent from 
any complement to R itself, like SciViews.

However, there are several good ideas in the format proposed by Eric 
(well, some debugging is still required), and the idea is to have a 
graph gallery that could be expanded to a GUI graph gallery, like in 
S-PLUS, that is, calling a particular graph could possibly display a 
dialog box where fields for parameters (x, y, line color, title, ...). 
Remember that SciViews is a GUI for R ;-) All this oculd be done with 
the tcltk package.

Anyway, the most important is to collect the material together. Then, 
more ambitous ways of providing code and example to R users could be 
build later...

I think that Paul Murrell, among other certainly has some good ideas, 
because he is preparing a book about R graphics 
(http://www.stat.auckland.ac.nz/~paul/RGraphics/rgraphics.html) and his 
is, after all, the grid gourou!

Best,

Philippe Grosjean

..............................................<?}))><........
  ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
  ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
  ) ) ) ) )   Mons-Hainaut University, Pentagone (3D08)
( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
  ) ) ) ) )   8, av du Champ de Mars, 7000 Mons, Belgium
( ( ( ( (
  ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.30.54
( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
  ) ) ) ) )
( ( ( ( (    web:   http://www.umh.ac.be/~econum
  ) ) ) ) )          http://www.sciviews.org
( ( ( ( (
..............................................................

Eric Lecoutre wrote:
> 
> Hi,
> 
> About any graph gallery:
> Philippe Grojean and me did have made some work. Our goal was to add a 
> clip library to the SciViews project that would offer access to a graph 
> gallery. I was workiong on the production of the gallery, where as 
> Philippe is still working on his GUI API. One of the goal is to have 
> automatic wizards to make easier the creation of a graphic.
> 
> Here was our approach and some thoughts:
> 
> - We should propose a format for a description file. Here are some 
> elements that should be gathered for each graphic function:
>         - Name of the function (*)
>         - Name of the produced graphic (*)
>         - Description of the graphic (*)
>         - Number of variables (univariate / bivariate / multivariate...)
>         - Types of variables
>         - Sample code (sample graph) (*)
>         - Package (*)
> The (*) are some information already available in Rd files (except maybe 
> sample graph).
> 
> - If someone deos something, I think it would be useful to ensure that 
> all is reusable. We should focus on describing graphics. Then, for 
> example, SciViews could use the information to create a usable graph 
> gallery.
> 
> If someone is interested, I ahve put in the following archive all my 
> current code:
> http://www.stat.ucl.ac.be/ISpersonnel/lecoutre/R/svGraphGallery.zip
> 
> There is no explanation but I would provide comments and help to any 
> volonteer (basically, there is a file  .ggs with some descriptions as 
> stated before and some R code to that produce HTML files).
> 
> The result (the current gallery) is there. It is aimed to be something 
> like 300 pixels large. At final step, graph would be clickable with a 
> wizard.
> 
> http://www.stat.ucl.ac.be/ISpersonnel/lecoutre/R/svGraphGallery/dock/svGallery.html 
> 
> 
> 
> 
> Eric
> 
> At 08:46 21/02/2005, Robert Cunningham wrote:
> 
>> I too have often though a R-gallery would be useful.
>>
>> It seems to me that a Wiki-style page with a database backend would be
>> the best bet.
>>
>> It also seems to be that the best place to start is a complete image
>> gallery produced from all the examples in R base, then in packages in
>> CRAN. In this context the graphicsQC package
>> (http://www.stat.auckland.ac.nz/~paul/R/graphicsQC_0.4.tar.g) of Paul
>> Murrell seems useful.
>>
>> Cheers,
>>
>>
>> Robert Cunningham
>>
>>
>>
>> Romain Francois <francoisromain at free.fr> writes:
>>
>> > Hello Sander,
>> >
>> > That's a good idea and i am up to it.
>> >
>> > Right now i am in an exam period, so it's not really the better time,
>> > give me a couple of weeks and i will come up with a specific format of
>> > R files to submit to me that i could post-process to generate html
>> > documents.
>> > To my mind, those html files should show :
>> >
>> > - the plot itself
>> > + Submitter(s)
>> >         - web page
>> >         - email (eventually protected, I don't know how to do it)
>> > - Bibliographic references
>> > - Required R packages
>> > + Commentaries
>> >        - in english
>> >        - and in any other languages
>> >
>> > I'm open to any suggestion.
>> >
>> > Romain.
>> >
>> > Le 18.02.2005 14:33, Sander Oom a ?crit :
>> >
>> >> Dear R users,
>> >>
>> >> Following some of the recent questions and discussions about the R
>> >> plotting abilities, it occurred to me again that it would be very
>> >> valuable to have an R graph gallery.
>> >>
>> >> Eric Lecoutre made a very nice example in:
>> >> 
>> http://www.stat.ucl.ac.be/ISpersonnel/lecoutre/stats/fichiers/_gallery.pdf 
>>
>> >>
>> >>
>> >> It would be very useful to many beginners, but probably also
>> >> advanced users of R, to have an overview of R graph types with
>> >> graphical examples  and associated R code.
>> >>
>> >> In order to facilitate the evolution of a large gallery, some sort
>> >> of wiki environment might be most suitable, thus providing access to
>> >> all users, but with limited maintenance costs for the provider.
>> >>
>> >> Do others agree this could be a valuable resource? Would anybody
>> >> have the resources to host such an R graph gallery?
>> >>
>> >> Yours,
>> >>
>> >> Sander Oom.
>> >>
>> > --
>> > Romain FRANCOIS : francoisromain at free.fr
>> > page web : http://addictedtor.free.fr/ (en construction)
>> > 06 18 39 14 69 / 01 46 80 65 60
>> > _______________________________________________________
>> > Etudiant en 3eme ann?e
>> > Institut de Statistique de l'Universit? de Paris (ISUP)
>> > Fili?re Industrie et Services
>> > http://www.isup.cicrp.jussieu.fr/
>> >
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
> 
> 
> Eric Lecoutre
> UCL /  Institut de Statistique
> Voie du Roman Pays, 20
> 1348 Louvain-la-Neuve
> Belgium
> 
> tel: (+32)(0)10473050
> lecoutre at stat.ucl.ac.be
> http://www.stat.ucl.ac.be/ISpersonnel/lecoutre
> 
> If the statistics are boring, then you've got the wrong numbers. -Edward 
> Tufte
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From mase at is.titech.ac.jp  Mon Feb 21 11:53:23 2005
From: mase at is.titech.ac.jp (Shigeru Mase)
Date: Mon, 21 Feb 2005 19:53:23 +0900
Subject: [R] Hosting a R Graph Gallery?
In-Reply-To: <6.0.1.1.2.20050221091626.02361e40@stat4ux.stat.ucl.ac.be>
References: <4215EEBC.6070304@oomvanlieshout.net>
	<42163271.5040906@free.fr>	<m11xbawe25.fsf@iinet.net.au>
	<6.0.1.1.2.20050221091626.02361e40@stat4ux.stat.ucl.ac.be>
Message-ID: <4219BDA3.8070508@is.titech.ac.jp>

Mere reference. There is already a wiki-based R graphics library at

http://www.okada.jp.org/RWiki/index.php?%A5%B0%A5%E9%A5%D5%A5%A3%A5%C3%A5%AF%A5%B9%BB%B2%B9%CD%BC%C2%CE%E3%BD%B8

Of course, almost all pages are in Japanese and most r-users would feel
difficult to visit (and post an example graphics) a Japanese site :-).
This graphics library pages have been very useful for Japanese R users
to see excellent and diverse graphics capabilities of R. I am looking
forward to the proposed library in near future. From our experience, a
wiki site is an excellent base for such library.

Shigeru Mase


Eric Lecoutre wrote:
> 
> Hi,
> 
> About any graph gallery:
> Philippe Grojean and me did have made some work. Our goal was to add a 
> clip library to the SciViews project that would offer access to a graph 
> gallery. I was workiong on the production of the gallery, where as 
> Philippe is still working on his GUI API. One of the goal is to have 
> automatic wizards to make easier the creation of a graphic.
> 
> Here was our approach and some thoughts:
> 
> - We should propose a format for a description file. Here are some 
> elements that should be gathered for each graphic function:
>         - Name of the function (*)
>         - Name of the produced graphic (*)
>         - Description of the graphic (*)
>         - Number of variables (univariate / bivariate / multivariate...)
>         - Types of variables
>         - Sample code (sample graph) (*)
>         - Package (*)
> The (*) are some information already available in Rd files (except maybe 
> sample graph).
> 
> - If someone deos something, I think it would be useful to ensure that 
> all is reusable. We should focus on describing graphics. Then, for 
> example, SciViews could use the information to create a usable graph 
> gallery.
> 
> If someone is interested, I ahve put in the following archive all my 
> current code:
> http://www.stat.ucl.ac.be/ISpersonnel/lecoutre/R/svGraphGallery.zip
> 
> There is no explanation but I would provide comments and help to any 
> volonteer (basically, there is a file  .ggs with some descriptions as 
> stated before and some R code to that produce HTML files).
> 
> The result (the current gallery) is there. It is aimed to be something 
> like 300 pixels large. At final step, graph would be clickable with a 
> wizard.
> 
> http://www.stat.ucl.ac.be/ISpersonnel/lecoutre/R/svGraphGallery/dock/svGallery.html 
> 
> 
> 
> 
> Eric



From bela_b at gmx.net  Mon Feb 21 12:09:02 2005
From: bela_b at gmx.net (Bela Bauer)
Date: Mon, 21 Feb 2005 12:09:02 +0100
Subject: [R] Two-factorial Huynh-Feldt-Test
In-Reply-To: <x2psyujmcf.fsf@biostat.ku.dk>
References: <4215A1BB.3020400@gmx.net>
	<x2r7je5iav.fsf@biostat.ku.dk>	<4215D2E0.5040606@cbs.mpg.de>
	<x2braic8lo.fsf@biostat.ku.dk>	<42199C8B.4080807@gmx.net>
	<x2psyujmcf.fsf@biostat.ku.dk>
Message-ID: <4219C14E.1080401@gmx.net>

Peter Dalgaard wrote:
> Bela Bauer <bela_b at gmx.net> writes:
>> >My suspicion would be that it has something to do with calculating the
>> >correction terms before or after contrast transformations (there must
>> >be a coordinate-free version of the corrections?), but I can't grok
>> >the details that easily.

It turns out now that the error is caused by a completely different 
problem: I had a small mistake in my code, which caused an error in the 
calculation of the cell means for the covariance matrix. I fixed it and 
now I'm getting the same H-F and G-G values as SAS. Sorry for the 
confusion...

I'm still looking for an efficient way to print the new summary. Is 
there any easy way to tell the summary or print functions about the 
corrected degrees of freedom?

>> >However, if you peek over on the R-devel
>> >list, you will see that I was just about to get serious with
>> >programming some of this stuff as methods for the "mlm" class. I think
>> >you just volunteered to test the code...
>>
>>
>>I'd be happy to do that. I've subscribed to R-devel following your
>>last email, but I can't seem to be able to find the thread that you're
>>referring to. Could you forward the relevent messages or the code to
>>me?
> 
>  
> https://stat.ethz.ch/pipermail/r-devel/2005-February/032240.html
> 
> (No code as yet, it is just an outline)

Well, if you're interested in my (corrected) code, I'll be happy to 
forward it to you. I'll also try to follow the development in R-devel 
and I'll be happy to test the code once it's ready.

Thanks again

Bela



From M.Mamin at intershop.de  Mon Feb 21 12:35:49 2005
From: M.Mamin at intershop.de (Marc Mamin)
Date: Mon, 21 Feb 2005 12:35:49 +0100
Subject: [R] character occurence within a string
Message-ID: <A03188C6623C0D46A703CB5AA59907F201C11CF7@JENMAIL01.ad.intershop.net>

Hello,

I'm looking for a function that counts the occurences of a given character within a string.

f('|','ab|c|d') => 2


More precisely, I need to complete a vector of strings to ensure that all elements have the same count of a "separator":

	a|b|c
	a
	|a|b|c|d

	=>

	a|b|c||
	a||||
	|a|b|c|d

I guess that scan makes use of an internal function that would do the job...


Thanks,

Marc Mamin



From ripley at stats.ox.ac.uk  Mon Feb 21 13:14:28 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 21 Feb 2005 12:14:28 +0000 (GMT)
Subject: [R] character occurence within a string
In-Reply-To: <A03188C6623C0D46A703CB5AA59907F201C11CF7@JENMAIL01.ad.intershop.net>
References: <A03188C6623C0D46A703CB5AA59907F201C11CF7@JENMAIL01.ad.intershop.net>
Message-ID: <Pine.LNX.4.61.0502211209340.30128@gannet.stats>

Here are two ways for one string, both easily vectorized:

1) sum(charToRaw(x) == charToRaw("|"))

2) sum(strsplit(x, "")[[1]] == "|")

In R-devel the first looks for bytes and the second for characters, and in 
UTF-8 locale there is a difference.

On Mon, 21 Feb 2005, Marc Mamin wrote:

> Hello,
>
> I'm looking for a function that counts the occurences of a given 
> character within a string.
>
> f('|','ab|c|d') => 2
>
>
> More precisely, I need to complete a vector of strings to ensure that 
> all elements have the same count of a "separator":
>
> 	a|b|c
> 	a
> 	|a|b|c|d
>
> 	=>
>
> 	a|b|c||
> 	a||||
> 	|a|b|c|d
>
> I guess that scan makes use of an internal function that would do the job...

No, it works on an internal buffer.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dimitris.rizopoulos at med.kuleuven.ac.be  Mon Feb 21 13:23:27 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Mon, 21 Feb 2005 13:23:27 +0100
Subject: [R] character occurence within a string
References: <A03188C6623C0D46A703CB5AA59907F201C11CF7@JENMAIL01.ad.intershop.net>
Message-ID: <02eb01c51810$259f7e00$0540210a@www.domain>

a simple solution is:

vec.strings <- c("ab|c|d", "a|b|c", "a", "|a|b|c|d")
f <- function(pat, vec.strings) sapply(strsplit(vec.strings, pat, 
fixed=TRUE), length)-1
f("|", vec.strings)

but probably there are better proposals.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Marc Mamin" <M.Mamin at intershop.de>
To: <R-help at stat.math.ethz.ch>
Sent: Monday, February 21, 2005 12:35 PM
Subject: [R] character occurence within a string


> Hello,
>
> I'm looking for a function that counts the occurences of a given 
> character within a string.
>
> f('|','ab|c|d') => 2
>
>
> More precisely, I need to complete a vector of strings to ensure 
> that all elements have the same count of a "separator":
>
> a|b|c
> a
> |a|b|c|d
>
> =>
>
> a|b|c||
> a||||
> |a|b|c|d
>
> I guess that scan makes use of an internal function that would do 
> the job...
>
>
> Thanks,
>
> Marc Mamin
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Matthias.Templ at statistik.gv.at  Mon Feb 21 15:48:10 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Mon, 21 Feb 2005 15:48:10 +0100
Subject: [R] Compare rows of two matrices
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BA8BC@xchg1.statistik.local>

Hello,

#I have two matrices, eg.:

y <-  matrix( c(20,  NA,  NA,  45,  50,  19,  32, 101,  10,  22,  NA,  NA,  80,  49,  61, 190), ncol=4 )
x <-  matrix( c(20,  NA,  NA,  NA,  50,  19,  32, 101,  10,  22,  NA,  NA,  80,  49,  61, 190), ncol=4 )

#Whereas x contains all NA?s from y plus some additional NA?s.
#I want to find the index of these additional NA?s. I think, there must be a very easy way to do this.

#Here are the indices of NA?s in x and y:
l1 <- which(is.na(x), arr.ind=TRUE)
l2 <- which(is.na(y), arr.ind=TRUE)

#> l1
#     [,1] [,2]
#[1,]    2    1
#[2,]    3    1
#[3,]    4    1
#[4,]    3    3
#[5,]    4    3

#> l2
#     row col
#[1,]   2   1
#[2,]   3   1
#[3,]   3   3
#[4,]   4   3

#Now I want to find a matrix, which includes the values of l1, without the rows of l2, 
#which has equal entities (the index of the additional NA?S).
#In this example the result should be row 3 of l1 with the values 4 and 1.
#The following code works, but I think there must be a much more elegant way to do this.

l3 <- l1
l3 <- cbind( l1, rep(0, nrow(l1)) )
num <- 1
   
for( i in 1:nrow(l1) ){
  for( j in 1:nrow(l2) ){
    if( l1[i,1] == l2[j,1] & l1[i,2] == l2[j,2]){
      l3[i,3] <- 1
    }
  }
}

l4 <- l3[l3[,3]==0, c(1,2)]

#> l4
#row col 
#  4   1  

I have often such problems like this and I assume, that other people have similar tasks.
My question is: Does anybody know a function in one package, which compares rows of two matrices like this or have anybody an idea to do this in a much more elegant way"? 

Thank you very much,
Matthias



From Xin.Liu at arradx.com  Mon Feb 21 15:57:13 2005
From: Xin.Liu at arradx.com (Liu, Xin)
Date: Mon, 21 Feb 2005 14:57:13 -0000
Subject: [R] rw2001 RMA in GeenSpring "Error in sigToEnv"
Message-ID: <3C4296559D4D4143A537F3F18AFC1C65190FFA@ni-cr-svc-ex1.pharms-services.com>

Dear all,

What I use is the latest version of R: rw2001. And I got the following error message when I run RMA in GeneSpring:

Error in sigToEnv(signature, fdef) : Trying to get slot "signature" from an object of a basic class ("NULL") with no slots
In addition: Warning message: 
Incompatible phenoData object. Created a new one.
 in: read.affybatch(filenames = files)

Which information I should provide furthur or the latest version of R has some bugs?
Thanks a lot!

Xin LIU

This e-mail is from ArraDx Ltd

The e-mail and any files transmitted with it are confidentia...{{dropped}}



From ggrothendieck at myway.com  Mon Feb 21 15:56:42 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 21 Feb 2005 14:56:42 +0000 (UTC)
Subject: [R] its plot with pch-argument
References: <0BDE2460F08BF0429F933A40431A61E801E82293@vk2kmail01.valtiokonttori.local>
Message-ID: <loom.20050221T154108-948@post.gmane.org>

M?kinen Jussi <Jussi.Makinen <at> valtiokonttori.fi> writes:

: 
: Hi mighty R-gurus and other enthusiastics,
: 
: I just encountered this:
: 
: library(its)
: x <- its(sort(rnorm(10)), as.POSIXct(Sys.time() + 1:10))
: plot(x, type = "p", pch = c(rep("A", 5), rep("B", 5)))
: 
: Am I missing something if I expect that all the points labeled as 'A' should 
be below all those labeled as 'B'? 
: 

Try this in place of your last line:

   plot(x, type = "p", pch = letters)

>From that output it seems that it is erroneously plotting the first 
point twice and the last point twice.    This suggests the following
workaround:

   plot(x, type = "p", pch = c(rep("A", 5+1), rep("B", 5+1)))

Another possibility is to use the zoo package to plot it:

   library(zoo)
   plot(as.zoo(x), type = "p", pch = list(c(rep("A", 5), rep("B", 5))))



From R-user at zutt.org  Mon Feb 21 16:14:10 2005
From: R-user at zutt.org (R user)
Date: Mon, 21 Feb 2005 16:14:10 +0100
Subject: [R] Compare rows of two matrices
In-Reply-To: <83536658864BC243BE3C06D7E936ABD5027BA8BC@xchg1.statistik.local>
References: <83536658864BC243BE3C06D7E936ABD5027BA8BC@xchg1.statistik.local>
Message-ID: <1108998850.3263.54.camel@dutiih.st.ewi.tudelft.nl>

> y <-  matrix( c(20,  NA,  NA,  45,  50,  19,  32, 101,  10,  22,  NA,  NA,  80,  49,  61, 190), ncol=4 )
> x <-  matrix( c(20,  NA,  NA,  NA,  50,  19,  32, 101,  10,  22,  NA,  NA,  80,  49,  61, 190), ncol=4 )
> 
> #Whereas x contains all NA?s from y plus some additional NA?s.
> #I want to find the index of these additional NA?s. I think, there must be a very easy way to do this.


How about this:

  is.na(x) & !is.na(y)


Jonne.



From dimitris.rizopoulos at med.kuleuven.ac.be  Mon Feb 21 16:21:05 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Mon, 21 Feb 2005 16:21:05 +0100
Subject: [R] Compare rows of two matrices
References: <83536658864BC243BE3C06D7E936ABD5027BA8BC@xchg1.statistik.local>
Message-ID: <000601c51828$f5eebef0$0540210a@www.domain>

maybe something like this:

which(is.na(y)!=is.na(x), arr.ind=TRUE)

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "TEMPL Matthias" <Matthias.Templ at statistik.gv.at>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, February 21, 2005 3:48 PM
Subject: [R] Compare rows of two matrices


> Hello,
>
> #I have two matrices, eg.:
>
> y <-  matrix( c(20,  NA,  NA,  45,  50,  19,  32, 101,  10,  22, 
> NA,  NA,  80,  49,  61, 190), ncol=4 )
> x <-  matrix( c(20,  NA,  NA,  NA,  50,  19,  32, 101,  10,  22, 
> NA,  NA,  80,  49,  61, 190), ncol=4 )
>
> #Whereas x contains all NA?s from y plus some additional NA?s.
> #I want to find the index of these additional NA?s. I think, there 
> must be a very easy way to do this.
>
> #Here are the indices of NA?s in x and y:
> l1 <- which(is.na(x), arr.ind=TRUE)
> l2 <- which(is.na(y), arr.ind=TRUE)
>
> #> l1
> #     [,1] [,2]
> #[1,]    2    1
> #[2,]    3    1
> #[3,]    4    1
> #[4,]    3    3
> #[5,]    4    3
>
> #> l2
> #     row col
> #[1,]   2   1
> #[2,]   3   1
> #[3,]   3   3
> #[4,]   4   3
>
> #Now I want to find a matrix, which includes the values of l1, 
> without the rows of l2,
> #which has equal entities (the index of the additional NA?S).
> #In this example the result should be row 3 of l1 with the values 4 
> and 1.
> #The following code works, but I think there must be a much more 
> elegant way to do this.
>
> l3 <- l1
> l3 <- cbind( l1, rep(0, nrow(l1)) )
> num <- 1
>
> for( i in 1:nrow(l1) ){
>  for( j in 1:nrow(l2) ){
>    if( l1[i,1] == l2[j,1] & l1[i,2] == l2[j,2]){
>      l3[i,3] <- 1
>    }
>  }
> }
>
> l4 <- l3[l3[,3]==0, c(1,2)]
>
> #> l4
> #row col
> #  4   1
>
> I have often such problems like this and I assume, that other people 
> have similar tasks.
> My question is: Does anybody know a function in one package, which 
> compares rows of two matrices like this or have anybody an idea to 
> do this in a much more elegant way"?
>
> Thank you very much,
> Matthias
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From andy_liaw at merck.com  Mon Feb 21 16:21:06 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 21 Feb 2005 10:21:06 -0500
Subject: [R] Compare rows of two matrices
Message-ID: <3A822319EB35174CA3714066D590DCD50994E745@usrymx25.merck.com>

Here's one way:

> idx <- which(! paste(l1, collapse=":") %in% paste(l2, collpase=":"))
> l1[idx,]
row col 
  2   1 

Andy

> From:  TEMPL Matthias
> 
> Hello,
> 
> #I have two matrices, eg.:
> 
> y <-  matrix( c(20,  NA,  NA,  45,  50,  19,  32, 101,  10,  
> 22,  NA,  NA,  80,  49,  61, 190), ncol=4 )
> x <-  matrix( c(20,  NA,  NA,  NA,  50,  19,  32, 101,  10,  
> 22,  NA,  NA,  80,  49,  61, 190), ncol=4 )
> 
> #Whereas x contains all NA?s from y plus some additional NA?s.
> #I want to find the index of these additional NA?s. I think, 
> there must be a very easy way to do this.
> 
> #Here are the indices of NA?s in x and y:
> l1 <- which(is.na(x), arr.ind=TRUE)
> l2 <- which(is.na(y), arr.ind=TRUE)
> 
> #> l1
> #     [,1] [,2]
> #[1,]    2    1
> #[2,]    3    1
> #[3,]    4    1
> #[4,]    3    3
> #[5,]    4    3
> 
> #> l2
> #     row col
> #[1,]   2   1
> #[2,]   3   1
> #[3,]   3   3
> #[4,]   4   3
> 
> #Now I want to find a matrix, which includes the values of 
> l1, without the rows of l2, 
> #which has equal entities (the index of the additional NA?S).
> #In this example the result should be row 3 of l1 with the 
> values 4 and 1.
> #The following code works, but I think there must be a much 
> more elegant way to do this.
> 
> l3 <- l1
> l3 <- cbind( l1, rep(0, nrow(l1)) )
> num <- 1
>    
> for( i in 1:nrow(l1) ){
>   for( j in 1:nrow(l2) ){
>     if( l1[i,1] == l2[j,1] & l1[i,2] == l2[j,2]){
>       l3[i,3] <- 1
>     }
>   }
> }
> 
> l4 <- l3[l3[,3]==0, c(1,2)]
> 
> #> l4
> #row col 
> #  4   1  
> 
> I have often such problems like this and I assume, that other 
> people have similar tasks.
> My question is: Does anybody know a function in one package, 
> which compares rows of two matrices like this or have anybody 
> an idea to do this in a much more elegant way"? 
> 
> Thank you very much,
> Matthias
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From Matthias.Templ at statistik.gv.at  Mon Feb 21 16:34:26 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Mon, 21 Feb 2005 16:34:26 +0100
Subject: [R] Compare rows of two matrices
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BA8BD@xchg1.statistik.local>

Ohh. Now I begin to see. It?s really simple and elegant!
Thank you very much!!!
Matthias

----------------
> maybe something like this:
> 
> which(is.na(y)!=is.na(x), arr.ind=TRUE)
> 
> I hope it helps.
> 
> Best,
> Dimitris
> 
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
> 
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/16/336899
> Fax: +32/16/337015
> Web: http://www.med.kuleuven.ac.be/biostat/
>      http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
> 
-----------------
> How about this:
> 
>   is.na(x) & !is.na(y)
> 
> 
> Jonne.
> 
-----------------
> 
> ----- Original Message ----- 
> From: "TEMPL Matthias" <Matthias.Templ at statistik.gv.at>
> To: <r-help at stat.math.ethz.ch>
> Sent: Monday, February 21, 2005 3:48 PM
> Subject: [R] Compare rows of two matrices
> 
> 
> > Hello,
> >
> > #I have two matrices, eg.:
> >
> > y <-  matrix( c(20,  NA,  NA,  45,  50,  19,  32, 101,  10,  22,
> > NA,  NA,  80,  49,  61, 190), ncol=4 )
> > x <-  matrix( c(20,  NA,  NA,  NA,  50,  19,  32, 101,  10,  22, 
> > NA,  NA,  80,  49,  61, 190), ncol=4 )
> >
> > #Whereas x contains all NA?s from y plus some additional 
> NA?s. #I want 
> > to find the index of these additional NA?s. I think, there 
> must be a 
> > very easy way to do this.
> >
> > #Here are the indices of NA?s in x and y:
> > l1 <- which(is.na(x), arr.ind=TRUE)
> > l2 <- which(is.na(y), arr.ind=TRUE)
> >
> > #> l1
> > #     [,1] [,2]
> > #[1,]    2    1
> > #[2,]    3    1
> > #[3,]    4    1
> > #[4,]    3    3
> > #[5,]    4    3
> >
> > #> l2
> > #     row col
> > #[1,]   2   1
> > #[2,]   3   1
> > #[3,]   3   3
> > #[4,]   4   3
> >
> > #Now I want to find a matrix, which includes the values of l1,
> > without the rows of l2,
> > #which has equal entities (the index of the additional NA?S).
> > #In this example the result should be row 3 of l1 with the values 4 
> > and 1.
> > #The following code works, but I think there must be a much more 
> > elegant way to do this.
> >
> > l3 <- l1
> > l3 <- cbind( l1, rep(0, nrow(l1)) )
> > num <- 1
> >
> > for( i in 1:nrow(l1) ){
> >  for( j in 1:nrow(l2) ){
> >    if( l1[i,1] == l2[j,1] & l1[i,2] == l2[j,2]){
> >      l3[i,3] <- 1
> >    }
> >  }
> > }
> >
> > l4 <- l3[l3[,3]==0, c(1,2)]
> >
> > #> l4
> > #row col
> > #  4   1
> >
> > I have often such problems like this and I assume, that other people
> > have similar tasks.
> > My question is: Does anybody know a function in one package, which 
> > compares rows of two matrices like this or have anybody an idea to 
> > do this in a much more elegant way"?
> >
> > Thank you very much,
> > Matthias
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > 
>



From bxc at steno.dk  Mon Feb 21 16:38:57 2005
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Mon, 21 Feb 2005 16:38:57 +0100
Subject: [R] What file did R read?
Message-ID: <0ABD88905D18E347874E0FB71C0B29E9027FEB91@exdkba022.novo.dk>

When I run in BATCH mode I use a script (win2000):

c:\stat\r\%R_VERS%\bin\Rcmd BATCH -q --no-restore --no-save %1

Now I want to be able to print the filename of the program, i.e.
the value if the %1 argument, in the .Rout file. 

(Basically I want to write a piece of code in .First() which 
prints date and time and program run, so I need to get to the 
value of %1. Currently I let the operating system print the value 
of %1 to a file, and then read the contents of this file.
Clumsy, it works, but tricky to port to another machine.)

The commandArgs() does NOT capture the name of the input file.

Is it impossible to recover this filename from within R?

Bendix Carstensen
----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc



From ramasamy at cancer.org.uk  Mon Feb 21 16:50:04 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 21 Feb 2005 15:50:04 +0000
Subject: [R] Compare rows of two matrices
In-Reply-To: <83536658864BC243BE3C06D7E936ABD5027BA8BC@xchg1.statistik.local>
References: <83536658864BC243BE3C06D7E936ABD5027BA8BC@xchg1.statistik.local>
Message-ID: <1109001004.29777.26.camel@ndmpc126.orc.ox.ac.uk>

Here is an another way

   count <- is.na(x) + is.na(y)
   which( count == 1, arr.ind=TRUE )

'count' gives you the number of missing values at for each row and
column. Then you can find out how many occurances of both missing, none
missing and one missing. 
 


On Mon, 2005-02-21 at 15:48 +0100, TEMPL Matthias wrote:
> Hello,
> 
> #I have two matrices, eg.:
> 
> y <-  matrix( c(20,  NA,  NA,  45,  50,  19,  32, 101,  10,  22,  NA,  NA,  80,  49,  61, 190), ncol=4 )
> x <-  matrix( c(20,  NA,  NA,  NA,  50,  19,  32, 101,  10,  22,  NA,  NA,  80,  49,  61, 190), ncol=4 )
> 
> #Whereas x contains all NA?s from y plus some additional NA?s.
> #I want to find the index of these additional NA?s. I think, there must be a very easy way to do this.
> 
> #Here are the indices of NA?s in x and y:
> l1 <- which(is.na(x), arr.ind=TRUE)
> l2 <- which(is.na(y), arr.ind=TRUE)
> 
> #> l1
> #     [,1] [,2]
> #[1,]    2    1
> #[2,]    3    1
> #[3,]    4    1
> #[4,]    3    3
> #[5,]    4    3
> 
> #> l2
> #     row col
> #[1,]   2   1
> #[2,]   3   1
> #[3,]   3   3
> #[4,]   4   3
> 
> #Now I want to find a matrix, which includes the values of l1, without the rows of l2, 
> #which has equal entities (the index of the additional NA?S).
> #In this example the result should be row 3 of l1 with the values 4 and 1.
> #The following code works, but I think there must be a much more elegant way to do this.
> 
> l3 <- l1
> l3 <- cbind( l1, rep(0, nrow(l1)) )
> num <- 1
>    
> for( i in 1:nrow(l1) ){
>   for( j in 1:nrow(l2) ){
>     if( l1[i,1] == l2[j,1] & l1[i,2] == l2[j,2]){
>       l3[i,3] <- 1
>     }
>   }
> }
> 
> l4 <- l3[l3[,3]==0, c(1,2)]
> 
> #> l4
> #row col 
> #  4   1  
> 
> I have often such problems like this and I assume, that other people have similar tasks.
> My question is: Does anybody know a function in one package, which compares rows of two matrices like this or have anybody an idea to do this in a much more elegant way"? 
> 
> Thank you very much,
> Matthias
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From murdoch at stats.uwo.ca  Mon Feb 21 17:12:38 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 21 Feb 2005 16:12:38 +0000
Subject: [R] rw2001 RMA in GeenSpring "Error in sigToEnv"
In-Reply-To: <3C4296559D4D4143A537F3F18AFC1C65190FFA@ni-cr-svc-ex1.pharms-services.com>
References: <3C4296559D4D4143A537F3F18AFC1C65190FFA@ni-cr-svc-ex1.pharms-services.com>
Message-ID: <7r1k11putjin47j0kljfhi2uneltnmebhi@4ax.com>

On Mon, 21 Feb 2005 14:57:13 -0000, "Liu, Xin" <Xin.Liu at arradx.com>
wrote :

>Dear all,
>
>What I use is the latest version of R: rw2001. And I got the following error message when I run RMA in GeneSpring:
>
>Error in sigToEnv(signature, fdef) : Trying to get slot "signature" from an object of a basic class ("NULL") with no slots
>In addition: Warning message: 
>Incompatible phenoData object. Created a new one.
> in: read.affybatch(filenames = files)
>
>Which information I should provide furthur or the latest version of R has some bugs?
>Thanks a lot!
>
>Xin LIU

That looks like a bug in GeneSpring, or at least an incompatibility
with the latest version of R.  You want to contact its authors.  

Duncan Murdoch



From Matthias.Templ at statistik.gv.at  Mon Feb 21 17:14:44 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Mon, 21 Feb 2005 17:14:44 +0100
Subject: AW: [R] Compare rows of two matrices
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BA8BE@xchg1.statistik.local>

Excellent. That was very helpful. Now I have full control about my NA?s  :-)
Thank you very much!!!
Matthias

> 
> Here is an another way
> 
>    count <- is.na(x) + is.na(y)
>    which( count == 1, arr.ind=TRUE )
> 
> 'count' gives you the number of missing values at for each 
> row and column. Then you can find out how many occurances of 
> both missing, none missing and one missing. 
>  
> 
> 
> On Mon, 2005-02-21 at 15:48 +0100, TEMPL Matthias wrote:
> > Hello,
> > 
> > #I have two matrices, eg.:
> > 
> > y <-  matrix( c(20,  NA,  NA,  45,  50,  19,  32, 101,  10, 
>  22,  NA,  
> > NA,  80,  49,  61, 190), ncol=4 ) x <-  matrix( c(20,  NA,  
> NA,  NA,  
> > 50,  19,  32, 101,  10,  22,  NA,  NA,  80,  49,  61, 190), ncol=4 )
> > 
> > #Whereas x contains all NA?s from y plus some additional 
> NA?s. #I want 
> > to find the index of these additional NA?s. I think, there 
> must be a 
> > very easy way to do this.
> > 
> > #Here are the indices of NA?s in x and y:
> > l1 <- which(is.na(x), arr.ind=TRUE)
> > l2 <- which(is.na(y), arr.ind=TRUE)
> > 
> > #> l1
> > #     [,1] [,2]
> > #[1,]    2    1
> > #[2,]    3    1
> > #[3,]    4    1
> > #[4,]    3    3
> > #[5,]    4    3
> > 
> > #> l2
> > #     row col
> > #[1,]   2   1
> > #[2,]   3   1
> > #[3,]   3   3
> > #[4,]   4   3
> > 
> > #Now I want to find a matrix, which includes the values of 
> l1, without 
> > the rows of l2,
> > #which has equal entities (the index of the additional NA?S).
> > #In this example the result should be row 3 of l1 with the 
> values 4 and 1..
> > #The following code works, but I think there must be a much 
> more elegant way to do this.
> > 
> > l3 <- l1
> > l3 <- cbind( l1, rep(0, nrow(l1)) )
> > num <- 1
> >    
> > for( i in 1:nrow(l1) ){
> >   for( j in 1:nrow(l2) ){
> >     if( l1[i,1] == l2[j,1] & l1[i,2] == l2[j,2]){
> >       l3[i,3] <- 1
> >     }
> >   }
> > }
> > 
> > l4 <- l3[l3[,3]==0, c(1,2)]
> > 
> > #> l4
> > #row col 
> > #  4   1  
> > 
> > I have often such problems like this and I assume, that 
> other people 
> > have similar tasks. My question is: Does anybody know a function in 
> > one package, which compares rows of two matrices like this or have 
> > anybody an idea to do this in a much more elegant way"?
> > 
> > Thank you very much,
> > Matthias
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
>



From spencer.graves at pdf.com  Mon Feb 21 18:55:16 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 21 Feb 2005 09:55:16 -0800
Subject: [R] export to text file
In-Reply-To: <1108747961.421626b927bc4@webmail.ksu.edu>
References: <1108747961.421626b927bc4@webmail.ksu.edu>
Message-ID: <421A2084.80008@pdf.com>

       1.  Did you work the example at the end of the "write.table" help 
file?  In R 2.0.1, this is:

   write.table(x, file = "foo.csv", sep = ",", col.names = NA)

	  This should create a file "foo.csv" in the working directory.  To 
find the working directory, use "getwd()";  to set it, use "setwd(...)". 
  When you do this, can you find "foo.csv"?

       2.  I tried the command in your email:

	  >write.table(temp.data.data, file = "c:\Current Work")

	  I got a file "Current Work" (with no *.txt or other extension) in 
"C:\".

	  3.  You need to be careful with "\" in text strings, because it is an 
R escape character.  For example, the following modification of your 
example produced an error:

 > write.table(temp.data.data, file = "c:\the Work")
Error in file(file, ifelse(append, "a", "w")) :
	unable to open connection
In addition: Warning message:
cannot open file `c:	he Work'

	  This occurs because "\t" is a tab character.

	  4.  If you wanted a file called, say, "temp.data.data.txt" in 
"c:\Current Work", you need something like the following:

write.table(temp.data.data,
   file = "c:\\Current Work\\temp.data.data.txt")	

	  Since "\" is the escape character in R, to get "\" retained in the 
character string, you need to use "\\";  "/" is accepted as a substitute 
for "\\".

	  hope this helps.
	  spencer graves

Christina D Smith wrote:

 >I'm not sure.  The following code gives me no errors:
 >write.table(temp.data.data, file = "c:\Current Work")
 >Yet no new file is created.
 >Christy
 >
 >Quoting Spencer Graves <spencer.graves at pdf.com>:
 >

###################
      Did you try "write.table"?  If yes, what do you see as its
deficiencies?

      spencer graves

Christina D Smith wrote:

>I'm trying to export a large data frame to a text file for permanent
>storage.  The only thing I could find was the treeglia Package but that
>didn't work.  Any suggestions?
>Thanks!
>
>Christina D Smith
>PhD Student, GRA
>Statistics Department
>Kansas State University
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From ggrothendieck at myway.com  Mon Feb 21 19:13:39 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 21 Feb 2005 13:13:39 -0500 (EST)
Subject: [R] What file did R read?
Message-ID: <20050221181339.576ED3A7D@mprdmxin.myway.com>



A different workaround that avoids generating a file
would be to set an environment variable in your script:

set infile=%1

and then access infile using Sys.getenv within R.


Date:   Mon, 21 Feb 2005 16:38:57 +0100 
From:   BXC (Bendix Carstensen) <bxc at steno.dk>
To:   <r-help at stat.math.ethz.ch> 
Subject:   [R] What file did R read? 

 
When I run in BATCH mode I use a script (win2000):

c:\stat\r\%R_VERS%\bin\Rcmd BATCH -q --no-restore --no-save %1

Now I want to be able to print the filename of the program, i.e.
the value if the %1 argument, in the .Rout file. 

(Basically I want to write a piece of code in .First() which 
prints date and time and program run, so I need to get to the 
value of %1. Currently I let the operating system print the value 
of %1 to a file, and then read the contents of this file.
Clumsy, it works, but tricky to port to another machine.)

The commandArgs() does NOT capture the name of the input file.

Is it impossible to recover this filename from within R?

Bendix Carstensen



From ramasamy at cancer.org.uk  Mon Feb 21 19:27:07 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 21 Feb 2005 18:27:07 +0000
Subject: [R] rw2001 RMA in GeenSpring "Error in sigToEnv"
In-Reply-To: <7r1k11putjin47j0kljfhi2uneltnmebhi@4ax.com>
References: <3C4296559D4D4143A537F3F18AFC1C65190FFA@ni-cr-svc-ex1.pharms-services.com>
	<7r1k11putjin47j0kljfhi2uneltnmebhi@4ax.com>
Message-ID: <1109010427.29777.38.camel@ndmpc126.orc.ox.ac.uk>

The sigToEnv error has been reported several times on the BioConductor
mailing list in this month. See :

https://stat.ethz.ch/pipermail/bioconductor/2005-February/007632.html
https://stat.ethz.ch/pipermail/bioconductor/2005-February/007674.html
https://stat.ethz.ch/pipermail/bioconductor/2005-February/007691.html
https://stat.ethz.ch/pipermail/bioconductor/2005-February/007730.html

There have not been much discussion on this. Either the problem was
solved by something simple such as upgrading to the latest version or
the authors are busy trying to find a fix. In any case, it would be
better to consult the authors or the BioConductor mailing list first.

Regards, Adai



On Mon, 2005-02-21 at 16:12 +0000, Duncan Murdoch wrote:
> On Mon, 21 Feb 2005 14:57:13 -0000, "Liu, Xin" <Xin.Liu at arradx.com>
> wrote :
> 
> >Dear all,
> >
> >What I use is the latest version of R: rw2001. And I got the following error message when I run RMA in GeneSpring:
> >
> >Error in sigToEnv(signature, fdef) : Trying to get slot "signature" from an object of a basic class ("NULL") with no slots
> >In addition: Warning message: 
> >Incompatible phenoData object. Created a new one.
> > in: read.affybatch(filenames = files)
> >
> >Which information I should provide furthur or the latest version of R has some bugs?
> >Thanks a lot!
> >
> >Xin LIU
> 
> That looks like a bug in GeneSpring, or at least an incompatibility
> with the latest version of R.  You want to contact its authors.  
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From fm3a004 at math.uni-hamburg.de  Mon Feb 21 19:36:31 2005
From: fm3a004 at math.uni-hamburg.de (Christian Hennig)
Date: Mon, 21 Feb 2005 19:36:31 +0100 (MET)
Subject: [R] 49 histograms on one page
Message-ID: <Pine.GSO.3.95q.1050221192631.16243N-100000@sun11.math.uni-hamburg.de>

Hi,

I want to do something like this:

par(mfrow=c(7,7))
for (i in 1:49)
  hist(RATDACOM[SUBJNO==i],breaks=0.5+(0:6),
       main="",xlab="",ylab="",xaxt="n",yaxt="n")

(Don't think about what RATDACOM and SUBJNO are.)

I get an error 
Error in plot.new() : Figure margins too large.

36 histograms with mfrow=c(6,6) work.
But the 36 histograms are then so small that I wonder why 49 do not fit.
It seems that though I have main="",xlab="",ylab="",xaxt="n",yaxt="n" the
area reserved for axes and labels is almost as large as it would be if I
would print a single histogram. The histogram itself only gets the remaining
space, which is very small (I could draw 36*4 histograms of this size on a
single page and the margins between them would still be OK).

So the question is:
How do I tell R to print the essential histogram area without main, labs and
axes large enough that it looks well but small enough that 49 fit on one
page? (If I would draw them by hand, no problem...)

Best,
Christian


***********************************************************************
Christian Hennig
Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
>From 1 April 2005: Department of Statistical Science, UCL, London
#######################################################################
ich empfehle www.boag-online.de



From andy_liaw at merck.com  Mon Feb 21 19:45:03 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 21 Feb 2005 13:45:03 -0500
Subject: [R] 49 histograms on one page
Message-ID: <3A822319EB35174CA3714066D590DCD50994E747@usrymx25.merck.com>

You may want to set par(mar, oma) to something small, or start the device
(which one are you using?) with large enough dimension, or both.

Andy

> From: Christian Hennig
> 
> Hi,
> 
> I want to do something like this:
> 
> par(mfrow=c(7,7))
> for (i in 1:49)
>   hist(RATDACOM[SUBJNO==i],breaks=0.5+(0:6),
>        main="",xlab="",ylab="",xaxt="n",yaxt="n")
> 
> (Don't think about what RATDACOM and SUBJNO are.)
> 
> I get an error 
> Error in plot.new() : Figure margins too large.
> 
> 36 histograms with mfrow=c(6,6) work.
> But the 36 histograms are then so small that I wonder why 49 
> do not fit.
> It seems that though I have 
> main="",xlab="",ylab="",xaxt="n",yaxt="n" the
> area reserved for axes and labels is almost as large as it 
> would be if I
> would print a single histogram. The histogram itself only 
> gets the remaining
> space, which is very small (I could draw 36*4 histograms of 
> this size on a
> single page and the margins between them would still be OK).
> 
> So the question is:
> How do I tell R to print the essential histogram area without 
> main, labs and
> axes large enough that it looks well but small enough that 49 
> fit on one
> page? (If I would draw them by hand, no problem...)
> 
> Best,
> Christian
> 
> 
> **************************************************************
> *********
> Christian Hennig
> Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
> hennig at math.uni-hamburg.de, 
> http://www.math.uni-hamburg.de/home/hennig/
> >From 1 April 
> 2005: Department of Statistical Science, UCL, London
> ##############################################################
> #########
> ich empfehle www.boag-online.de
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ligges at statistik.uni-dortmund.de  Mon Feb 21 19:53:23 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 21 Feb 2005 19:53:23 +0100
Subject: [R] 49 histograms on one page
In-Reply-To: <Pine.GSO.3.95q.1050221192631.16243N-100000@sun11.math.uni-hamburg.de>
References: <Pine.GSO.3.95q.1050221192631.16243N-100000@sun11.math.uni-hamburg.de>
Message-ID: <421A2E23.6020600@statistik.uni-dortmund.de>

Christian Hennig wrote:

> Hi,
> 
> I want to do something like this:
> 
> par(mfrow=c(7,7))
> for (i in 1:49)
>   hist(RATDACOM[SUBJNO==i],breaks=0.5+(0:6),
>        main="",xlab="",ylab="",xaxt="n",yaxt="n")
> 
> (Don't think about what RATDACOM and SUBJNO are.)
> 
> I get an error 
> Error in plot.new() : Figure margins too large.
> 
> 36 histograms with mfrow=c(6,6) work.
> But the 36 histograms are then so small that I wonder why 49 do not fit.
> It seems that though I have main="",xlab="",ylab="",xaxt="n",yaxt="n" the
> area reserved for axes and labels is almost as large as it would be if I
> would print a single histogram. The histogram itself only gets the remaining
> space, which is very small (I could draw 36*4 histograms of this size on a
> single page and the margins between them would still be OK).
> 
> So the question is:
> How do I tell R to print the essential histogram area without main, labs and
> axes large enough that it looks well but small enough that 49 fit on one
> page? (If I would draw them by hand, no problem...)

Christian,

set par(mar = ....) very small....

Uwe

> Best,
> Christian
> 
> 
> ***********************************************************************
> Christian Hennig
> Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
> hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
>>From 1 April 2005: Department of Statistical Science, UCL, London
> #######################################################################
> ich empfehle www.boag-online.de
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From fm3a004 at math.uni-hamburg.de  Mon Feb 21 19:53:01 2005
From: fm3a004 at math.uni-hamburg.de (Christian Hennig)
Date: Mon, 21 Feb 2005 19:53:01 +0100 (MET)
Subject: solved: [R] 49 histograms on one page
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E747@usrymx25.merck.com>
Message-ID: <Pine.GSO.3.95q.1050221195208.16243O-100000@sun11.math.uni-hamburg.de>


The first try with mar=c(1,1,1,1) does the job!

Thanks,
Christian

On Mon, 21 Feb 2005, Liaw, Andy wrote:

> You may want to set par(mar, oma) to something small, or start the device
> (which one are you using?) with large enough dimension, or both.
> 
> Andy
> 
> > From: Christian Hennig
> > 
> > Hi,
> > 
> > I want to do something like this:
> > 
> > par(mfrow=c(7,7))
> > for (i in 1:49)
> >   hist(RATDACOM[SUBJNO==i],breaks=0.5+(0:6),
> >        main="",xlab="",ylab="",xaxt="n",yaxt="n")
> > 
> > (Don't think about what RATDACOM and SUBJNO are.)
> > 
> > I get an error 
> > Error in plot.new() : Figure margins too large.
> > 
> > 36 histograms with mfrow=c(6,6) work.
> > But the 36 histograms are then so small that I wonder why 49 
> > do not fit.
> > It seems that though I have 
> > main="",xlab="",ylab="",xaxt="n",yaxt="n" the
> > area reserved for axes and labels is almost as large as it 
> > would be if I
> > would print a single histogram. The histogram itself only 
> > gets the remaining
> > space, which is very small (I could draw 36*4 histograms of 
> > this size on a
> > single page and the margins between them would still be OK).
> > 
> > So the question is:
> > How do I tell R to print the essential histogram area without 
> > main, labs and
> > axes large enough that it looks well but small enough that 49 
> > fit on one
> > page? (If I would draw them by hand, no problem...)
> > 
> > Best,
> > Christian
> > 
> > 
> > **************************************************************
> > *********
> > Christian Hennig
> > Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
> > hennig at math.uni-hamburg.de, 
> > http://www.math.uni-hamburg.de/home/hennig/
> > >From 1 April 
> > 2005: Department of Statistical Science, UCL, London
> > ##############################################################
> > #########
> > ich empfehle www.boag-online.de
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> > 
> 
> 
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachment...{{dropped}}



From sundar.dorai-raj at pdf.com  Mon Feb 21 20:02:01 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Mon, 21 Feb 2005 13:02:01 -0600
Subject: [R] 49 histograms on one page
In-Reply-To: <Pine.GSO.3.95q.1050221192631.16243N-100000@sun11.math.uni-hamburg.de>
References: <Pine.GSO.3.95q.1050221192631.16243N-100000@sun11.math.uni-hamburg.de>
Message-ID: <421A3029.5040109@pdf.com>



Christian Hennig wrote:
> Hi,
> 
> I want to do something like this:
> 
> par(mfrow=c(7,7))
> for (i in 1:49)
>   hist(RATDACOM[SUBJNO==i],breaks=0.5+(0:6),
>        main="",xlab="",ylab="",xaxt="n",yaxt="n")
> 
> (Don't think about what RATDACOM and SUBJNO are.)
> 
> I get an error 
> Error in plot.new() : Figure margins too large.
> 
> 36 histograms with mfrow=c(6,6) work.
> But the 36 histograms are then so small that I wonder why 49 do not fit.
> It seems that though I have main="",xlab="",ylab="",xaxt="n",yaxt="n" the
> area reserved for axes and labels is almost as large as it would be if I
> would print a single histogram. The histogram itself only gets the remaining
> space, which is very small (I could draw 36*4 histograms of this size on a
> single page and the margins between them would still be OK).
> 
> So the question is:
> How do I tell R to print the essential histogram area without main, labs and
> axes large enough that it looks well but small enough that 49 fit on one
> page? (If I would draw them by hand, no problem...)
> 
> Best,
> Christian
> 


You may want to look at ?histogram in package:lattice.

set.seed(1)
m <- 100
n <- 49
RATDACOM <- rnorm(m * n)
SUBJNO <- rep(seq(n), each = m)
histogram(~ RATDACOM | SUBJNO,
           layout = c(7, 7),
           strip = FALSE)

--sundar



From akniss at uwyo.edu  Mon Feb 21 21:01:46 2005
From: akniss at uwyo.edu (Andrew Kniss)
Date: Mon, 21 Feb 2005 13:01:46 -0700
Subject: [R] power.anova.test for interaction effects
Message-ID: <000001c51850$2f6f97e0$6a07070a@andrew>

This question will probably get me in trouble on theoretical grounds, but I
will pose it anyway.

The situation:
I recently ran a field study looking for differences in sugarbeet cultivar
tolerance to a specific herbicide.  The study was set up so that 37
cultivars were treated with 4 different applications of the herbicide (37*4
factorial).  In doing so, we found that the interaction effect was highly
insignificant (ndf=108, ddf=144, F=0.28, p=1.0000).  Now my problem is
this... the study takes up an enormous amount of time, energy, and money (as
you could guess with 37 cultivars in a field study).  We need to determine
weather it is worth the effort to repeat the study this summer (practically,
it is not, but our funding source would like a more concrete demonstration).

I decided to try using power.anova.test just as a starting point to see what
our power was.  My question is: is this valid to do on an interaction term?
If I use power.anova.test with on the interaction term, this is what I get:

~>  power.anova.test(groups=(37*4), n=3, between.var=12.06,
~+                  within.var=21.23, sig.level=0.05)
~
~     Balanced one-way analysis of variance power calculation 
~
~         groups = 148
~              n = 3
~    between.var = 12.06
~     within.var = 21.23
~      sig.level = 0.05
~          power = 1
~
~ NOTE: n is number in each group 


This would imply that given the variability we observed with 3 replications,
we almost certainly would have found differences if they existed.  But given
what I have read on power analysis, a high p-value and wide confidence
intervals nearly always suggest inadequate sample size. (Our 90% confidence
intervals differed from the estimates by as much as 28%, when a 10%
difference would be significant from a practical perspective.) 

So is this a valid approach? Or does the power.anova.test fall apart if
using an interaction effect? 

Thank you in advance for any help or references you are willing to point me
to.
Best regards,
Andrew Kniss
Assistant Research Scientist
University of Wyoming
Department of Plant Sciences
1000 E. University Ave.
Laramie, WY  82071  USA

akniss at uwyo.edu



From paul.boutros at utoronto.ca  Mon Feb 21 22:35:55 2005
From: paul.boutros at utoronto.ca (paul.boutros@utoronto.ca)
Date: Mon, 21 Feb 2005 16:35:55 -0500
Subject: [R] Problems Building R on AIX 5.2.0.0
Message-ID: <1109021755.421a543bacb40@webmail.utoronto.ca>

Hello,

I am trying to build R 2.0.1 on an AIX 5.2.0.0 machine using gcc 3.3.2:
$ oslevel
5.2.0.0
$ gcc -v
Reading specs from /usr/local/lib/gcc-lib/powerpc-ibm-aix5.2.0.0/3.3.2/specs
Configured with: ../gcc-3.3.2/configure  : (reconfigured) ../gcc-3.3.2/configure 
--disable-nls : (reconfigured) ../gcc-3.3.2/configure --disable-nls
Thread model: aix
gcc version 3.3.2

Configure goes okay, but I get an error that I don't quite know how to interpret 
during make.  I've included the summary output from the end of configure as well 
as the error that I get during make below.  Any suggestions/recommendations are 
very much appreciate: I'm stuck on ideas for what could be going wrong.

Paul

$ ./configure --prefix=/db2blaste/R

<snip>

R is now configured for powerpc-ibm-aix5.2.0.0

  Source directory:          .
  Installation directory:    /db2blast/R

  C compiler:                gcc -mno-fp-in-toc -g -O2
  C++ compiler:              g++  -g -O2
  Fortran compiler:          g77  -g -O2

  Interfaces supported:      X11
  External libraries:
  Additional capabilities:   PNG, JPEG
  Options enabled:           R profiling

  Recommended packages:      yes

configure: WARNING: you cannot build DVI versions of the R manuals
configure: WARNING: you cannot build info or html versions of the R manuals
configure: WARNING: you cannot build PDF versions of the R manuals
configure: WARNING: I could not determine a browser
configure: WARNING: I could not determine a PDF viewer

$ make

<snip>

        gcc -Wl,-bM:SRE -Wl,-H512 -Wl,-T512 -Wl,-bnoentry -Wl,-bexpall -Wl,-bI:.
./../../etc/R.exp -L/usr/local/lib -o lapack.so -Wl,-bI:../../../etc/Rlapack.exp 
Lapack.lo rgeev.lo rsyev.lo  -L../../../lib -lRlapack  -L/usr/local/lib -L/usr/
local/lib/gcc-lib/powerpc-ibm-aix5.2.0.0/3.3.2 -L/usr/local/lib/gcc-lib/powerpc-
ibm-aix5.2.0.0/3.3.2/../../.. -lfrtbegin -lg2c -lm -lgcc_s /usr/local/lib/gcc-
lib/powerpc-ibm-aix5.2.0.0/3.3.2/libgcc.a -lg -ldl -ltermcap -lm -lc
ld: 0706-006 Cannot find or open library file: -l Rlapack
        ld:open(): A file or directory in the path name does not exist.
collect2: ld returned 255 exit status
make: 1254-004 The error code from the last command is 1.


Stop.
make: 1254-004 The error code from the last command is 2.


Stop.
make: 1254-004 The error code from the last command is 1.


Stop.
make: 1254-004 The error code from the last command is 1.


Stop.
make: 1254-004 The error code from the last command is 1.


Stop.
$



From rob at fatkat.com  Mon Feb 21 23:03:19 2005
From: rob at fatkat.com (Rob Steele)
Date: Mon, 21 Feb 2005 17:03:19 -0500
Subject: [R] 49 histograms on one page
Message-ID: <421A5AA7.9070102@fatkat.com>

The pdf format works well for large numbers of plots on a single page.  
You can use a viewer like Acrobat Reader to zoom in to the areas of 
interest.  Ideally you can arrange the plots in a meaningful order, 
maybe where both their horizontal and vertical position convey information.

If you want something you can actually print or read without zooming in 
and if you can live with a slightly more abstract view of your data you 
might try putting multiple box plots in a single graph.  Something like:

n = 1000
m = 49

data = data.frame(x = rnorm(n), y = trunc(runif(n, min = 1, max = m + 1)))

splits = split(data$x, data$y)

o = order(sapply(splits, median), decreasing = TRUE)

boxplot(splits[o], horizontal = TRUE)



From lorin at cs.umd.edu  Mon Feb 21 23:40:05 2005
From: lorin at cs.umd.edu (Lorin Hochstein)
Date: Mon, 21 Feb 2005 17:40:05 -0500
Subject: [R] Treatment-Contrast Interactions
In-Reply-To: <x2y8djxh2q.fsf@biostat.ku.dk>
References: <42182376.40602@cs.umd.edu>	<Pine.LNX.4.61.0502200953210.27414@gannet.stats>	<4218B1CF.4080807@cs.umd.edu>
	<x2y8djxh2q.fsf@biostat.ku.dk>
Message-ID: <421A6345.7020102@cs.umd.edu>

Peter Dalgaard wrote:

>Lorin Hochstein <lorin at cs.umd.edu> writes:
>
>  
>
>>I'd like to understand this approach as well, but I can't reproduce my
>>results using se.contrast. In particular, I get the same standard
>>error even though I tried to use different contrasts:
>>
>> > c1 <- c(1,-1)[A]*c(1,-1,0)[B]
>> > c2 <- c(1,-1)[A]*c(1,0,-1)[B]
>> > c3 <- c(1,-1)[A]*c(0,1,-1)[B]
>> > se.contrast(fit, as.matrix(c1))
>>Contrast 1
>>  14.24547
>> > se.contrast(fit,as.matrix(c2))
>>Contrast 1
>>  14.24547
>> > se.contrast(fit,as.matrix(c3))
>>Contrast 1
>>  14.24547
>>    
>>
>
>They could well _be_ the same if the design is balanced...
> 
>
Hmmm... One of my problems is that I don't know how to interpret the 
output of se.contrast.

Here's my example again.
 > score <- c(12, 8,10, 6, 8, 4,
       10,12, 8, 6,10,14,
        9, 7, 9, 5,11,12,
        7,13, 9, 9, 5,11,
        8, 7, 3, 8,12,10,
       13,14,19, 9,16,14)
 > n <- 6
 > A <- gl(2,3*n,labels=c("a1","a2"))
 > B <- rep(gl(3,n,labels=c("b1","b2","b3")),2)
 > contrasts(B) <- c(1,-1,0)
 > fit <- aov(score~A*B)
 > summary(fit, split=list(B=1:2), expand.split = T)
            Df  Sum Sq Mean Sq F value   Pr(>F)  
A            1  18.778  18.778  2.2208 0.146606  
B            2  62.000  31.000  3.6662 0.037629 *
  B: C1      1   1.500   1.500  0.1774 0.676621  
  B: C2      1  60.500  60.500  7.1551 0.011986 *
A:B          2  81.556  40.778  4.8226 0.015274 *
  A:B: C1    1  13.500  13.500  1.5966 0.216119     # <---
  A:B: C2    1  68.056  68.056  8.0486 0.008085 **
Residuals   30 253.667   8.456     

What I'm really looking for is that F value that's labelled A:B: C1, 
1.5966 in this case. (I'm not sure what to call this term, AB interaction?)

 I thought that it might be possible to use se.contrast to compute this 
(or at least, to get the numerator so that I could compute the F value 
once I had the mean square error of the residuals), but I'm not sure how 
to specify the contrast, and I don't know the relationship between the 
"standard error" output by se.contrast and the "mean square error" which 
is the fourth column of the output above.

Lorin



From srini_iyyer_bio at yahoo.com  Tue Feb 22 00:21:58 2005
From: srini_iyyer_bio at yahoo.com (Srinivas Iyyer)
Date: Mon, 21 Feb 2005 15:21:58 -0800 (PST)
Subject: [R] Distribution
In-Reply-To: <421A2084.80008@pdf.com>
Message-ID: <20050221232158.10309.qmail@web53510.mail.yahoo.com>

Dear group, 
apologies for asking a simple question. I have a file
where the data looks like this:
Probe    Intensity
0:0	501.0
1:0	17760.5
2:0	511.0
3:0	18468.3
4:0	199.8
5:0	508.0
6:0	17241.8
7:0	507.5
8:0	17910.0
9:0	482.5
10:0	17480.3
11:0	434.0
12:0	17631.3
13:0	444.8
14:0	17423.0
15:0	505.3
16:0	16693.0
17:0	438.5
18:0	16920.0
19:0	491.3
20:0	16878.0
21:0	486.3
22:0	16582.0
23:0	483.8
24:0	16694.8
25:0	452.3
26:0	16221.5
27:0	438.3
28:0	17119.8
29:0	455.5
30:0	16579.0
31:0	424.5
32:0	16691.3
33:0	472.0


My question is how do I know the distribution of the
intensities. My aim is to find out the number of
intensities or probes that fall in a certain range. 

For example 500 probes has intensities ranging from 50
to 150.

300 probes has intensities ranging from 151-250

I have no clue how to do it for 500,000 probes. Can
any one please help doing it in R.

thanks and apologies again

srini



From bwheeler at echip.com  Tue Feb 22 00:32:30 2005
From: bwheeler at echip.com (Bob Wheeler)
Date: Mon, 21 Feb 2005 18:32:30 -0500
Subject: [R] power.anova.test for interaction effects
In-Reply-To: <000001c51850$2f6f97e0$6a07070a@andrew>
References: <000001c51850$2f6f97e0$6a07070a@andrew>
Message-ID: <421A6F8E.3050603@echip.com>

Your F value is so low as to make me suspect your model. Where did the 
144 denominator degrees of freedom come from?

Andrew Kniss wrote:

> This question will probably get me in trouble on theoretical grounds, but I
> will pose it anyway.
> 
> The situation:
> I recently ran a field study looking for differences in sugarbeet cultivar
> tolerance to a specific herbicide.  The study was set up so that 37
> cultivars were treated with 4 different applications of the herbicide (37*4
> factorial).  In doing so, we found that the interaction effect was highly
> insignificant (ndf=108, ddf=144, F=0.28, p=1.0000).  Now my problem is
> this... the study takes up an enormous amount of time, energy, and money (as
> you could guess with 37 cultivars in a field study).  We need to determine
> weather it is worth the effort to repeat the study this summer (practically,
> it is not, but our funding source would like a more concrete demonstration).
> 
> I decided to try using power.anova.test just as a starting point to see what
> our power was.  My question is: is this valid to do on an interaction term?
> If I use power.anova.test with on the interaction term, this is what I get:
> 
> ~>  power.anova.test(groups=(37*4), n=3, between.var=12.06,
> ~+                  within.var=21.23, sig.level=0.05)
> ~
> ~     Balanced one-way analysis of variance power calculation 
> ~
> ~         groups = 148
> ~              n = 3
> ~    between.var = 12.06
> ~     within.var = 21.23
> ~      sig.level = 0.05
> ~          power = 1
> ~
> ~ NOTE: n is number in each group 
> 
> 
> This would imply that given the variability we observed with 3 replications,
> we almost certainly would have found differences if they existed.  But given
> what I have read on power analysis, a high p-value and wide confidence
> intervals nearly always suggest inadequate sample size. (Our 90% confidence
> intervals differed from the estimates by as much as 28%, when a 10%
> difference would be significant from a practical perspective.) 
> 
> So is this a valid approach? Or does the power.anova.test fall apart if
> using an interaction effect? 
> 
> Thank you in advance for any help or references you are willing to point me
> to.
> Best regards,
> Andrew Kniss
> Assistant Research Scientist
> University of Wyoming
> Department of Plant Sciences
> 1000 E. University Ave.
> Laramie, WY  82071  USA
> 
> akniss at uwyo.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Bob Wheeler --- http://www.bobwheeler.com/
         ECHIP, Inc. ---
Randomness comes in bunches.



From sdavis2 at mail.nih.gov  Tue Feb 22 00:44:02 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Mon, 21 Feb 2005 18:44:02 -0500
Subject: [R] Distribution
References: <20050221232158.10309.qmail@web53510.mail.yahoo.com>
Message-ID: <000801c5186f$38dcfb50$7d75f345@WATSON>

Srini

You should probably look at ?hist.  If you look at the "value" section, you 
will see that you can get the information you want from the values returned 
from hist.  If these are microarray probes and intensities, there may be 
specific methods for visualizing the data available from the bioconductor 
project (www.bioconductor.org).

Hope this helps,
Sean

----- Original Message ----- 
From: "Srinivas Iyyer" <srini_iyyer_bio at yahoo.com>
To: "Rhelp" <r-help at stat.math.ethz.ch>
Sent: Monday, February 21, 2005 6:21 PM
Subject: [R] Distribution


> Dear group,
> apologies for asking a simple question. I have a file
> where the data looks like this:
> Probe    Intensity
> 0:0 501.0
> 1:0 17760.5
> 2:0 511.0
> 3:0 18468.3
> 4:0 199.8
> 5:0 508.0
> 6:0 17241.8
> 7:0 507.5
> 8:0 17910.0
> 9:0 482.5
> 10:0 17480.3
> 11:0 434.0
> 12:0 17631.3
> 13:0 444.8
> 14:0 17423.0
> 15:0 505.3
> 16:0 16693.0
> 17:0 438.5
> 18:0 16920.0
> 19:0 491.3
> 20:0 16878.0
> 21:0 486.3
> 22:0 16582.0
> 23:0 483.8
> 24:0 16694.8
> 25:0 452.3
> 26:0 16221.5
> 27:0 438.3
> 28:0 17119.8
> 29:0 455.5
> 30:0 16579.0
> 31:0 424.5
> 32:0 16691.3
> 33:0 472.0
>
>
> My question is how do I know the distribution of the
> intensities. My aim is to find out the number of
> intensities or probes that fall in a certain range.
>
> For example 500 probes has intensities ranging from 50
> to 150.
>
> 300 probes has intensities ranging from 151-250
>
> I have no clue how to do it for 500,000 probes. Can
> any one please help doing it in R.
>
> thanks and apologies again
>
> srini
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From spencer.graves at pdf.com  Tue Feb 22 00:44:31 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 21 Feb 2005 15:44:31 -0800
Subject: [R] Distribution
In-Reply-To: <20050221232158.10309.qmail@web53510.mail.yahoo.com>
References: <20050221232158.10309.qmail@web53510.mail.yahoo.com>
Message-ID: <421A725F.4080906@pdf.com>

      Have you considered "qqnorm" or "hist"?  If yes, "PLEASE do read 
the posting guide! http://www.R-project.org/posting-guide.html".  It 
might help you phrase your question so you are more likely to get a 
useful response -- and it might help you get the answer for yourself 
without waiting for someone to reply. 

      hope this helps.  spencer graves

Srinivas Iyyer wrote:

>Dear group, 
>apologies for asking a simple question. I have a file
>where the data looks like this:
>Probe    Intensity
>0:0	501.0
>1:0	17760.5
>2:0	511.0
>3:0	18468.3
>4:0	199.8
>5:0	508.0
>6:0	17241.8
>7:0	507.5
>8:0	17910.0
>9:0	482.5
>10:0	17480.3
>11:0	434.0
>12:0	17631.3
>13:0	444.8
>14:0	17423.0
>15:0	505.3
>16:0	16693.0
>17:0	438.5
>18:0	16920.0
>19:0	491.3
>20:0	16878.0
>21:0	486.3
>22:0	16582.0
>23:0	483.8
>24:0	16694.8
>25:0	452.3
>26:0	16221.5
>27:0	438.3
>28:0	17119.8
>29:0	455.5
>30:0	16579.0
>31:0	424.5
>32:0	16691.3
>33:0	472.0
>
>
>My question is how do I know the distribution of the
>intensities. My aim is to find out the number of
>intensities or probes that fall in a certain range. 
>
>For example 500 probes has intensities ranging from 50
>to 150.
>
>300 probes has intensities ranging from 151-250
>
>I have no clue how to do it for 500,000 probes. Can
>any one please help doing it in R.
>
>thanks and apologies again
>
>srini
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From Fritz.Scholz at boeing.com  Tue Feb 22 00:45:06 2005
From: Fritz.Scholz at boeing.com (Scholz, Fritz)
Date: Mon, 21 Feb 2005 15:45:06 -0800
Subject: [R] rnorm??
Message-ID: <0A829231985BC34183758AB7FA42AA0204BE4F57@xch-nw-30.nw.nos.boeing.com>

I am wondering whether there is a bug in rnorm.
When generating rnorm(1000000) and counting 
the cases > 4 and the cases < (-4) I get rather
unexpectedly low counts for the latter. The problem goes away
when using qnorm(runif(1000000)).

Fritz Scholz, PhD
Applied Statistics Group
Boeing Phantom Works
fritz.scholz at pss.boeing.com
425-865-3623
Tu/We 206-542-6545 (most likely)



From andy_liaw at merck.com  Tue Feb 22 00:51:12 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 21 Feb 2005 18:51:12 -0500
Subject: [R] Distribution
Message-ID: <3A822319EB35174CA3714066D590DCD50994E748@usrymx25.merck.com>

You can use table(cut(intensity, breaks)), where `intensity' is the vector
of intensity values, and `breaks' are the boundaries of the bins (e.g., c(0,
150, 250,  ...)).

Andy

> From: Srinivas Iyyer
> 
> Dear group, 
> apologies for asking a simple question. I have a file
> where the data looks like this:
> Probe    Intensity
> 0:0	501.0
> 1:0	17760.5
> 2:0	511.0
> 3:0	18468.3
> 4:0	199.8
> 5:0	508.0
> 6:0	17241.8
> 7:0	507.5
> 8:0	17910.0
> 9:0	482.5
> 10:0	17480.3
> 11:0	434.0
> 12:0	17631.3
> 13:0	444.8
> 14:0	17423.0
> 15:0	505.3
> 16:0	16693.0
> 17:0	438.5
> 18:0	16920.0
> 19:0	491.3
> 20:0	16878.0
> 21:0	486.3
> 22:0	16582.0
> 23:0	483.8
> 24:0	16694.8
> 25:0	452.3
> 26:0	16221.5
> 27:0	438.3
> 28:0	17119.8
> 29:0	455.5
> 30:0	16579.0
> 31:0	424.5
> 32:0	16691.3
> 33:0	472.0
> 
> 
> My question is how do I know the distribution of the
> intensities. My aim is to find out the number of
> intensities or probes that fall in a certain range. 
> 
> For example 500 probes has intensities ranging from 50
> to 150.
> 
> 300 probes has intensities ranging from 151-250
> 
> I have no clue how to do it for 500,000 probes. Can
> any one please help doing it in R.
> 
> thanks and apologies again
> 
> srini
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From akniss at uwyo.edu  Tue Feb 22 00:58:34 2005
From: akniss at uwyo.edu (Andrew Kniss)
Date: Mon, 21 Feb 2005 16:58:34 -0700
Subject: [R] power.anova.test for interaction effects
In-Reply-To: <421A6F8E.3050603@echip.com>
Message-ID: <000501c51871$43961ed0$6a07070a@andrew>

It's a rather complex model.  A 37*4 factorial (37 cultivars[var]; 4
herbicide treatments[trt]) with three replications[rep] was carried out at
two locations[loc], with  different randomizations within each rep at each
location.

Source           DF   Error Term      MS
Loc               1   Trt*rep(loc)    12314
Rep(loc)          4   Trt*rep(loc)    1230.5
Trt               3   Trt*rep(loc)    64.72
Trt*loc           3   Trt*rep(loc)    33.42
Trt*rep(loc)     12   Residual        76.78
Var              36   Var*trt*loc     93.91
Var*trt         108   Var*trt*loc     12.06
Var*trt*loc     144   Residual        43.09
Residual        575   NA              21.23
  

-----Original Message-----
From: Bob Wheeler [mailto:bwheeler at echip.com] 
Sent: Monday, February 21, 2005 4:33 PM
To: akniss at uwyo.edu
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] power.anova.test for interaction effects

Your F value is so low as to make me suspect your model. Where did the 
144 denominator degrees of freedom come from?

Andrew Kniss wrote:

> This question will probably get me in trouble on theoretical grounds, but
I
> will pose it anyway.
> 
> The situation:
> I recently ran a field study looking for differences in sugarbeet cultivar
> tolerance to a specific herbicide.  The study was set up so that 37
> cultivars were treated with 4 different applications of the herbicide
(37*4
> factorial).  In doing so, we found that the interaction effect was highly
> insignificant (ndf=108, ddf=144, F=0.28, p=1.0000).  Now my problem is
> this... the study takes up an enormous amount of time, energy, and money
(as
> you could guess with 37 cultivars in a field study).  We need to determine
> weather it is worth the effort to repeat the study this summer
(practically,
> it is not, but our funding source would like a more concrete
demonstration).
> 
> I decided to try using power.anova.test just as a starting point to see
what
> our power was.  My question is: is this valid to do on an interaction
term?
> If I use power.anova.test with on the interaction term, this is what I
get:
> 
> ~>  power.anova.test(groups=(37*4), n=3, between.var=12.06,
> ~+                  within.var=21.23, sig.level=0.05)
> ~
> ~     Balanced one-way analysis of variance power calculation 
> ~
> ~         groups = 148
> ~              n = 3
> ~    between.var = 12.06
> ~     within.var = 21.23
> ~      sig.level = 0.05
> ~          power = 1
> ~
> ~ NOTE: n is number in each group 
> 
> 
> This would imply that given the variability we observed with 3
replications,
> we almost certainly would have found differences if they existed.  But
given
> what I have read on power analysis, a high p-value and wide confidence
> intervals nearly always suggest inadequate sample size. (Our 90%
confidence
> intervals differed from the estimates by as much as 28%, when a 10%
> difference would be significant from a practical perspective.) 
> 
> So is this a valid approach? Or does the power.anova.test fall apart if
> using an interaction effect? 
> 
> Thank you in advance for any help or references you are willing to point
me
> to.
> Best regards,
> Andrew Kniss
> Assistant Research Scientist
> University of Wyoming
> Department of Plant Sciences
> 1000 E. University Ave.
> Laramie, WY  82071  USA
> 
> akniss at uwyo.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
> 


-- 
Bob Wheeler --- http://www.bobwheeler.com/
         ECHIP, Inc. ---
Randomness comes in bunches.



From andy_liaw at merck.com  Tue Feb 22 01:40:36 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 21 Feb 2005 19:40:36 -0500
Subject: [R] rnorm??
Message-ID: <3A822319EB35174CA3714066D590DCD50994E749@usrymx25.merck.com>

I don't see anything suspicious (R 2.0.1 on Windows XP Pro, Pentium M):

> set.seed(1)
> res <- replicate(50, {x <- rnorm(1e6); c(sum(x < -4), sum(x > 4))})
> apply(res, 1, summary)
        [,1]  [,2]
Min.    20.0 21.00
1st Qu. 27.0 30.00
Median  30.0 33.50
Mean    30.7 33.74
3rd Qu. 34.0 37.00
Max.    42.0 48.00
> pnorm(-4) * 1e6
[1] 31.67124
> res2 <- replicate(50, {x <- qnorm(runif(1e6)); c(sum(x < -4), sum(x >
4))})
> apply(res2, 1, summary)
         [,1]  [,2]
Min.    18.00 19.00
1st Qu. 30.00 27.25
Median  32.00 31.00
Mean    32.60 31.38
3rd Qu. 35.75 35.00
Max.    47.00 45.00
> RNGkind()
[1] "Mersenne-Twister" "Inversion"       

Andy


> From: Scholz, Fritz
> 
> I am wondering whether there is a bug in rnorm.
> When generating rnorm(1000000) and counting 
> the cases > 4 and the cases < (-4) I get rather
> unexpectedly low counts for the latter. The problem goes away
> when using qnorm(runif(1000000)).
> 
> Fritz Scholz, PhD
> Applied Statistics Group
> Boeing Phantom Works
> fritz.scholz at pss.boeing.com
> 425-865-3623
> Tu/We 206-542-6545 (most likely)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From hathaway at sover.net  Tue Feb 22 01:47:38 2005
From: hathaway at sover.net (Allen Hathaway)
Date: Mon, 21 Feb 2005 19:47:38 -0500
Subject: [R] Categories or clusters for univariate data
Message-ID: <000501c51878$39990940$e3aa72d8@ALLEN>

If I have a vector, x, such that

x <- c(1,2,3,4,5,8,9,10,11,12,15,16,17,18,19,22,23,24,33,34,35)

if I plot that vector

plot(x)

it is visibly obvious that the data "groups" or "clusters" into distinct 
groupings.  The data trends along a more-or-less linear path, and then an 
abrupt jump.  For a trivial case, such as I have given, you can pick out the 
groups or categories visually, and manually derive the upper and lower 
bounds for each group.  My question is, is there a function in R that can do 
the same thing for more complex and subtle groupings in univariate data, and 
provide a statistical basis for the result?

Allen



From ramasamy at cancer.org.uk  Tue Feb 22 01:52:48 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Tue, 22 Feb 2005 00:52:48 +0000
Subject: [R] Distribution
In-Reply-To: <000801c5186f$38dcfb50$7d75f345@WATSON>
References: <20050221232158.10309.qmail@web53510.mail.yahoo.com>
	<000801c5186f$38dcfb50$7d75f345@WATSON>
Message-ID: <1109033568.5924.7.camel@dhcp-63.ccc.ox.ac.uk>

You can read in the data using read.delim() or read.table(). For
illustration let us generate some artificial data and suppose that you
are interested in equal sized breaks of 5 (you can define your own break
points instead).

   x   <- rchisq(500000, df=10, ncp=5)
   brk <- seq(0, 5*ceiling(max(x)/5), by=5) # increments of size 5
   h   <- hist(x, breaks=brk, plot=FALSE)

h$breaks, h$counts will give you the count and break points but I always
have trouble matching which interval the counts belong to.


Another easier way is to use cut() followed by table() where the labels
of cut is helpful.

   table( cut( x, breaks=brk ) )

As a bonus, you can simplify specifying the break points by including
Infinite as the endpoint in cut.

   brk2 <- seq(0, max(x), by=5) # increments of size 5
   table( cut( x, breaks=c(brk2, Inf) ) )


Regards, Adai


On Mon, 2005-02-21 at 18:44 -0500, Sean Davis wrote:
> Srini
> 
> You should probably look at ?hist.  If you look at the "value" section, you 
> will see that you can get the information you want from the values returned 
> from hist.  If these are microarray probes and intensities, there may be 
> specific methods for visualizing the data available from the bioconductor 
> project (www.bioconductor.org).
> 
> Hope this helps,
> Sean
> 
> ----- Original Message ----- 
> From: "Srinivas Iyyer" <srini_iyyer_bio at yahoo.com>
> To: "Rhelp" <r-help at stat.math.ethz.ch>
> Sent: Monday, February 21, 2005 6:21 PM
> Subject: [R] Distribution
> 
> 
> > Dear group,
> > apologies for asking a simple question. I have a file
> > where the data looks like this:
> > Probe    Intensity
> > 0:0 501.0
> > 1:0 17760.5
> > 2:0 511.0
> > 3:0 18468.3
> > 4:0 199.8
> > 5:0 508.0
> > 6:0 17241.8
> > 7:0 507.5
> > 8:0 17910.0
> > 9:0 482.5
> > 10:0 17480.3
> > 11:0 434.0
> > 12:0 17631.3
> > 13:0 444.8
> > 14:0 17423.0
> > 15:0 505.3
> > 16:0 16693.0
> > 17:0 438.5
> > 18:0 16920.0
> > 19:0 491.3
> > 20:0 16878.0
> > 21:0 486.3
> > 22:0 16582.0
> > 23:0 483.8
> > 24:0 16694.8
> > 25:0 452.3
> > 26:0 16221.5
> > 27:0 438.3
> > 28:0 17119.8
> > 29:0 455.5
> > 30:0 16579.0
> > 31:0 424.5
> > 32:0 16691.3
> > 33:0 472.0
> >
> >
> > My question is how do I know the distribution of the
> > intensities. My aim is to find out the number of
> > intensities or probes that fall in a certain range.
> >
> > For example 500 probes has intensities ranging from 50
> > to 150.
> >
> > 300 probes has intensities ranging from 151-250
> >
> > I have no clue how to do it for 500,000 probes. Can
> > any one please help doing it in R.
> >
> > thanks and apologies again
> >
> > srini
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From sluque at mun.ca  Tue Feb 22 02:18:24 2005
From: sluque at mun.ca (Sebastian Luque)
Date: Mon, 21 Feb 2005 19:18:24 -0600
Subject: [R] rodbc or unixodbc error
Message-ID: <87wtt1e6jj.fsf@mun.ca>

Hi,

I'm trying to establish a connection to a MySQL database, and am using the
rodbc package for it. This is in a GNU/Debian Linux box, with the
corresponding Debian unstable packages. I can login to my MySQL databases
from any shell and directory, so the problem is probably not there. Here's
an example of what I'm doing:

R> odbcConnect("test", uid="myusername", pwd="mypassword")
[1] -1
Warning messages: 
1: [RODBC] ERROR: state IM002, code 0, message [unixODBC][Driver Manager]Data source name not found, and no default driver specified 
2: ODBC connection failed in: odbcDriverConnect(st, case = case, believeNRows = believeNRows)


The error is apparently from unixodbc, and googling for it I found that
somebody solved it by specifying a default driver in a odbc.ini file. Can
somebody please tell whether this is the right approach, and if so, how to
write that specification? I saw that one might do this in a ~/.odbc.ini
(i.e. the user's config file) file.

Best wishes,
-- 
Sebastian Luque



From corey.bradshaw at cdu.edu.au  Tue Feb 22 02:13:54 2005
From: corey.bradshaw at cdu.edu.au (Corey Bradshaw)
Date: Tue, 22 Feb 2005 10:43:54 +0930
Subject: [R] problems with nonlinear fits using nls
Message-ID: <E62D4EFB4204BF4F947E1BBDA775517B36C188@mail.site.cdu.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050222/f244a2dd/attachment.pl

From Achim.Zeileis at wu-wien.ac.at  Tue Feb 22 02:14:50 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 22 Feb 2005 02:14:50 +0100 (CET)
Subject: [R] Categories or clusters for univariate data
In-Reply-To: <000501c51878$39990940$e3aa72d8@ALLEN>
References: <000501c51878$39990940$e3aa72d8@ALLEN>
Message-ID: <Pine.LNX.4.58.0502220206550.14940@thorin.ci.tuwien.ac.at>

On Mon, 21 Feb 2005, Allen Hathaway wrote:

> If I have a vector, x, such that
>
> x <- c(1,2,3,4,5,8,9,10,11,12,15,16,17,18,19,22,23,24,33,34,35)
>
> if I plot that vector
>
> plot(x)
>
> it is visibly obvious that the data "groups" or "clusters" into distinct
> groupings.  The data trends along a more-or-less linear path, and then an
> abrupt jump.  For a trivial case, such as I have given, you can pick out the
> groups or categories visually, and manually derive the upper and lower
> bounds for each group.  My question is, is there a function in R that can do
> the same thing for more complex and subtle groupings in univariate data, and
> provide a statistical basis for the result?

Maybe breakpoints() in package strucchange can be of help. It looks for
breaks in linear regression relationships over a certain ordering of the
variables.

For the data above:

## setup index variable
idx <- seq(along = x)
## find breaks in linear trend model
library(strucchange)
bp <- breakpoints(x ~ idx)

## visualize fitted model
plot(x)
lines(fitted(bp))

See help(breakpoints) for further information and references for the
underlying theory.

hth,
Z



From ramasamy at cancer.org.uk  Tue Feb 22 02:23:56 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Tue, 22 Feb 2005 01:23:56 +0000
Subject: [R] rnorm??
In-Reply-To: <0A829231985BC34183758AB7FA42AA0204BE4F57@xch-nw-30.nw.nos.boeing.com>
References: <0A829231985BC34183758AB7FA42AA0204BE4F57@xch-nw-30.nw.nos.boeing.com>
Message-ID: <1109035436.5924.21.camel@dhcp-63.ccc.ox.ac.uk>

1) Why do you think there is a bug in rnorm. What is your expected
numbers for cases > 4 or cases < -4 ? If you can show your calculations
then maybe we can see if there is an error in how you calculated it or
with R.


2) Please give a reproducible example. It might be low or high in one
instance on a single run but if you do it several times you may not see
a consistent bias. If you do, then there could be a bug. Otherwise it
could be an element of randomness.

B    <- 10  # increase this if you can 
out1 <- matrix( nr=B, nc=2 )
out2 <- matrix( nr=B, nc=2 )
thr  <- 4

set.seed(1)
for(i in 1:B){
  
  x <- rnorm(1000000)
  out1[ i, 1 ] <- mean( x > thr )    # empirical upper tail prob
  out1[ i, 2 ] <- mean( x < -1*thr ) # empirical lower tail prob

  y <- qnorm(runif(1000000))
  out2[ i, 1 ] <- mean( y > thr )
  out2[ i, 2 ] <- mean( y < -1*thr )

  rm(x, y); print(i)
}

> out1
         [,1]    [,2]
 [1,] 3.6e-05 3.7e-05
 [2,] 3.2e-05 2.6e-05
 [3,] 3.2e-05 2.9e-05
 [4,] 3.7e-05 2.8e-05
 [5,] 3.5e-05 2.5e-05
 [6,] 4.3e-05 3.3e-05
 [7,] 3.9e-05 3.0e-05
 [8,] 3.2e-05 4.8e-05
 [9,] 3.0e-05 3.1e-05
[10,] 3.4e-05 3.7e-05

> out2
         [,1]    [,2]
 [1,] 3.5e-05 2.4e-05
 [2,] 2.4e-05 3.5e-05
 [3,] 4.1e-05 2.6e-05
 [4,] 2.5e-05 3.9e-05
 [5,] 4.1e-05 2.5e-05
 [6,] 4.0e-05 3.1e-05
 [7,] 2.9e-05 2.7e-05
 [8,] 3.1e-05 3.3e-05
 [9,] 3.5e-05 2.5e-05
[10,] 2.7e-05 3.4e-05

The probability of observing any value above 4 or below -4 is pnorm(4,
lower.tail=FALSE) = 3.167124e-05. There does not seem to be anything
suspicious from the run of 10.


3) I think this question might be more appropriate to post this question
to R-devel. Please use an informative subject line. Please read the
posting guide. Thanks.


Regards, Adai



On Mon, 2005-02-21 at 15:45 -0800, Scholz, Fritz wrote:
> I am wondering whether there is a bug in rnorm.
> When generating rnorm(1000000) and counting 
> the cases > 4 and the cases < (-4) I get rather
> unexpectedly low counts for the latter. The problem goes away
> when using qnorm(runif(1000000)).
> 
> Fritz Scholz, PhD
> Applied Statistics Group
> Boeing Phantom Works
> fritz.scholz at pss.boeing.com
> 425-865-3623
> Tu/We 206-542-6545 (most likely)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From edd at debian.org  Tue Feb 22 03:17:54 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 21 Feb 2005 20:17:54 -0600
Subject: [R] rodbc or unixodbc error
In-Reply-To: <87wtt1e6jj.fsf@mun.ca>
References: <87wtt1e6jj.fsf@mun.ca>
Message-ID: <16922.38482.593277.995588@basebud.nulle.part>


Sebastian,

On 21 February 2005 at 19:18, Sebastian Luque wrote:
| I'm trying to establish a connection to a MySQL database, and am using the
| rodbc package for it. This is in a GNU/Debian Linux box, with the
| corresponding Debian unstable packages. I can login to my MySQL databases
| from any shell and directory, so the problem is probably not there. Here's
| an example of what I'm doing:
| 
| R> odbcConnect("test", uid="myusername", pwd="mypassword")
| [1] -1
| Warning messages: 
| 1: [RODBC] ERROR: state IM002, code 0, message [unixODBC][Driver Manager]Data source name not found, and no default driver specified 
| 2: ODBC connection failed in: odbcDriverConnect(st, case = case, believeNRows = believeNRows)
| 
| 
| The error is apparently from unixodbc, and googling for it I found that
| somebody solved it by specifying a default driver in a odbc.ini file. Can
| somebody please tell whether this is the right approach, and if so, how to
| write that specification? I saw that one might do this in a ~/.odbc.ini
| (i.e. the user's config file) file.

Yes, this is under-documented and thus harder than it should be. I owe my
first working setup many, many years ago to a helpful (private) mail from
Fritz.

So here goes, I just tested it again with PostgresSQL (as I don't currently
keep MySQL running) yet it should carry over. If it fails, let's work on this
off-list.

i)  /etc/odbcinst.ini -- I think these entries may even have been created by
    a Debian package but I am not entirely sure.

-----------------------------------------------------------------------------
edd at basebud:~> cat /etc/odbcinst.ini
[PostgreSQL]
Description             = PostgreSQL ODBC driver for Linux and Windows
Driver          = /usr/lib/postgresql/lib/psqlodbc.so
Setup           = /usr/lib/odbc/libodbcpsqlS.so
Debug           = 0
CommLog         = 1

[MySQL]
Description             = MySQL driver
Driver          = /usr/lib/odbc/libmyodbc.so
Setup           = /usr/lib/odbc/libodbcmyS.so
CPTimeout               =
CPReuse         =
FileUsage               = 1
-----------------------------------------------------------------------------

    Make sure you have those files in those places -- and if you only use
    MySQL you can probably do without the first set here.

ii) /etc/odbc.ini -- Here is one such entry:

-----------------------------------------------------------------------------
[beancounter]
Description             = Beancounter DB (Postgresql)
Driver                  = Postgresql
Trace                   = Yes
TraceFile               = /tmp/beancounter_odbc.log
Database                = beancounter
Servername              = localhost
UserName                =
Password                =
Port                    = 5432
Protocol                = 6.4
ReadOnly                = No
RowVersioning           = No
ShowSystemTables        = No
ShowOidColumn           = No
FakeOidIndex            = No
ConnSettings            =
-----------------------------------------------------------------------------

    The only fields that matter may be Driver, Database, Servername and maybe
    Port.  I'm sorry that I don't have a stanza for MySQL in use. An older
    one on another computer is 

-----------------------------------------------------------------------------
[beancounter_mysql]
Driver       = /usr/lib/libmyodbc.so
Database     = beancounter
Servername   = localhost
ReadOnly     = 0
-----------------------------------------------------------------------------

    but I cannot test that one right now.


Hth, Dirk


-- 
Better to have an approximate answer to the right question than a precise 
answer to the wrong question.  --  John Tukey as quoted by John Chambers



From edd at debian.org  Tue Feb 22 03:20:56 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 21 Feb 2005 20:20:56 -0600
Subject: [R] rodbc or unixodbc error
In-Reply-To: <87wtt1e6jj.fsf@mun.ca>
References: <87wtt1e6jj.fsf@mun.ca>
Message-ID: <16922.38664.901829.823340@basebud.nulle.part>


PS  iii)  Make sure you connect with dsn, uid and pwd arguments:

> db <- odbcConnect("beancounter", uid="edd", pwd="********")
> db
RODB Connection 3
Details:
  case=nochange
  DSN=beancounter
  DATABASE=beancounter
  SERVER=basebud
  PORT=5432
  UID=edd
  PWD=
  ReadOnly=No
  Protocol=6.4
  FakeOidIndex=No
  ShowOidColumn=No
  RowVersioning=No
  ShowSystemTables=No
  ConnSettings=
  Fetch=100
  Socket=4096
  UnknownSizes=0
  MaxVarcharSize=254
  MaxLongVarcharSize=8190
  Debug=0
  CommLog=0
  Optimizer=1
  Ksqo=1
  UseDeclareFetch=0
  TextAsLongVarchar=1
  UnknownsAsLongVarchar=0
  BoolsAsChar=1
  Parse=0
  CancelAsFreeStmt=0
  ExtraSysTablePrefixes=dd_
  
  LFConversion=0
  UpdatableCursors=1
  DisallowPremature=0
  TrueIsMinus1=0
  BI=0
  ByteaAsLongVarBinary=0
  UseServerSidePrepare=0

Dirk

-- 
Better to have an approximate answer to the right question than a precise 
answer to the wrong question.  --  John Tukey as quoted by John Chambers



From ggrothendieck at myway.com  Tue Feb 22 04:01:27 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 22 Feb 2005 03:01:27 +0000 (UTC)
Subject: [R] problems with nonlinear fits using nls
References: <E62D4EFB4204BF4F947E1BBDA775517B36C188@mail.site.cdu.edu.au>
Message-ID: <loom.20050222T035951-54@post.gmane.org>

Corey Bradshaw <corey.bradshaw <at> cdu.edu.au> writes:
 
: I am attempting to determine the nonlinear least-squares estimates of
: the nonlinear model parameters using nls. I have come across a common
: problem that R users have reported when I attempt to fit a particular
: 3-parameter nonlinear function to my dataset:
: 
: Error in nls(r ~ tlm(a, N.fix, k, theta), data = tlm.data, start =
: list(a = a.st,  : 

Try it with nlm.  I find I often have better luck that with it.



From ggrothendieck at myway.com  Tue Feb 22 04:17:53 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 22 Feb 2005 03:17:53 +0000 (UTC)
Subject: [R] Categories or clusters for univariate data
References: <000501c51878$39990940$e3aa72d8@ALLEN>
Message-ID: <loom.20050222T041327-984@post.gmane.org>

Allen Hathaway <hathaway <at> sover.net> writes:

: 
: If I have a vector, x, such that
: 
: x <- c(1,2,3,4,5,8,9,10,11,12,15,16,17,18,19,22,23,24,33,34,35)
: 
: if I plot that vector
: 
: plot(x)
: 
: it is visibly obvious that the data "groups" or "clusters" into distinct 
: groupings.  The data trends along a more-or-less linear path, and then an 
: abrupt jump.  For a trivial case, such as I have given, you can pick out the 
: groups or categories visually, and manually derive the upper and lower 
: bounds for each group.  My question is, is there a function in R that can do 
: the same thing for more complex and subtle groupings in univariate data, and 
: provide a statistical basis for the result?

If the actual data is exactly linear and increasing, as with this example, 
then the breakpoints are at points of positive acceleration, thus

   which(diff(x, diff = 2)>0) + 2

gives the indices of the breakpoints.



From Tom.Mulholland at dpi.wa.gov.au  Tue Feb 22 04:35:57 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Tue, 22 Feb 2005 11:35:57 +0800
Subject: [R] Categories or clusters for univariate data
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3C9AC@afhex01.dpi.wa.gov.au>

x <- c(1,2,3,4,5,8,9,10,11,12,15,16,17,18,19,22,23,24,33,34,35)
require(cluster)
pam(x,5)

Medoids:
     [,1]
[1,]    3
[2,]   10
[3,]   17
[4,]   23
[5,]   34
Clustering vector:
 [1] 1 1 1 1 1 2 2 2 2 2 3 3 3 3 3 4 4 4 5 5 5
Objective function:
   build     swap 
1.285714 1.047619 

Available components:
[1] "medoids"    "clustering" "objective"  "isolation"  "clusinfo"   "silinfo"    "diss"       "call"       "data"      

Does this help?


> -----Original Message-----
> From: Allen Hathaway [mailto:hathaway at sover.net]
> Sent: Tuesday, 22 February 2005 8:48 AM
> To: r-help list
> Subject: [R] Categories or clusters for univariate data
> 
> 
> If I have a vector, x, such that
> 
> x <- c(1,2,3,4,5,8,9,10,11,12,15,16,17,18,19,22,23,24,33,34,35)
> 
> if I plot that vector
> 
> plot(x)
> 
> it is visibly obvious that the data "groups" or "clusters" 
> into distinct 
> groupings.  The data trends along a more-or-less linear path, 
> and then an 
> abrupt jump.  For a trivial case, such as I have given, you 
> can pick out the 
> groups or categories visually, and manually derive the upper 
> and lower 
> bounds for each group.  My question is, is there a function 
> in R that can do 
> the same thing for more complex and subtle groupings in 
> univariate data, and 
> provide a statistical basis for the result?
> 
> Allen
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From sluque at mun.ca  Tue Feb 22 06:32:14 2005
From: sluque at mun.ca (Sebastian Luque)
Date: Mon, 21 Feb 2005 23:32:14 -0600
Subject: [R] rodbc or unixodbc error
References: <87wtt1e6jj.fsf@mun.ca>
	<16922.38482.593277.995588@basebud.nulle.part>
Message-ID: <87k6p1dush.fsf@mun.ca>

Hi Dirk,

That worked beautifully, thanks a lot! I was able to connect to a test
database, save to it (sqlSave), and query it (odbcQuery). Some comments
below for future reference.


Dirk Eddelbuettel <edd at debian.org> wrote:

[...]

> -----------------------------------------------------------------------------
> edd at basebud:~> cat /etc/odbcinst.ini
> [PostgreSQL]
> Description = PostgreSQL ODBC driver for Linux and Windows
> Driver = /usr/lib/postgresql/lib/psqlodbc.so
> Setup = /usr/lib/odbc/libodbcpsqlS.so
> Debug = 0
> CommLog = 1
>
> [MySQL]
> Description = MySQL driver
> Driver = /usr/lib/odbc/libmyodbc.so
> Setup = /usr/lib/odbc/libodbcmyS.so
> CPTimeout =
> CPReuse =
> FileUsage = 1
> -----------------------------------------------------------------------------

Except for PostgreSQL, this was the default configuration that MySQL or
unixODBC must have entered here during installation.


> ii) /etc/odbc.ini

This file was empty in my system, so adapting your recommendation:

,-----[ /etc/odbc.ini (lines: 1 - 7) ]
| [test]
| Driver       = /usr/lib/odbc/libmyodbc.so
| Database     = test
| Servername   = localhost
| ReadOnly     = 0
| Port	       = 3306
`-----

and followed the same template to include the rest of my databases further
down.

Can the latter go in ~/.odbc.ini as I said before? I hope so, as this
would allow for much easier maintenance.

Thanks again for the helpful reply.

-- 
Sebastian Luque



From sluque at mun.ca  Tue Feb 22 06:55:40 2005
From: sluque at mun.ca (Sebastian Luque)
Date: Mon, 21 Feb 2005 23:55:40 -0600
Subject: [R] rodbc or unixodbc error
References: <87wtt1e6jj.fsf@mun.ca>
	<16922.38482.593277.995588@basebud.nulle.part>
	<87k6p1dush.fsf@mun.ca>
Message-ID: <87fyzpdtpf.fsf@mun.ca>

Sebastian Luque <sluque at mun.ca> wrote:

[...]

> Can the latter go in ~/.odbc.ini as I said before? I hope so, as this
> would allow for much easier maintenance.

Yes, I just found ODBCConfig, a tool that is supplied with unixodbc for
this sort of manipulations.

Cheers,
-- 
Sebastian Luque



From ripley at stats.ox.ac.uk  Tue Feb 22 08:33:24 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 22 Feb 2005 07:33:24 +0000 (GMT)
Subject: [R] rodbc or unixodbc error
In-Reply-To: <87k6p1dush.fsf@mun.ca>
References: <87wtt1e6jj.fsf@mun.ca>
	<16922.38482.593277.995588@basebud.nulle.part>
	<87k6p1dush.fsf@mun.ca>
Message-ID: <Pine.LNX.4.61.0502220728190.27058@gannet.stats>

For future information, this is almost exactly the contents of the README 
file in the RODBC package, which contains further useful hints.

The unixODBC site (www.unixODBC.org/odbcinst.html) has a useful 
tutorial, too.


On Mon, 21 Feb 2005, Sebastian Luque wrote:

> Hi Dirk,
>
> That worked beautifully, thanks a lot! I was able to connect to a test
> database, save to it (sqlSave), and query it (odbcQuery). Some comments
> below for future reference.
>
>
> Dirk Eddelbuettel <edd at debian.org> wrote:
>
> [...]
>
>> -----------------------------------------------------------------------------
>> edd at basebud:~> cat /etc/odbcinst.ini
>> [PostgreSQL]
>> Description = PostgreSQL ODBC driver for Linux and Windows
>> Driver = /usr/lib/postgresql/lib/psqlodbc.so
>> Setup = /usr/lib/odbc/libodbcpsqlS.so
>> Debug = 0
>> CommLog = 1
>>
>> [MySQL]
>> Description = MySQL driver
>> Driver = /usr/lib/odbc/libmyodbc.so
>> Setup = /usr/lib/odbc/libodbcmyS.so
>> CPTimeout =
>> CPReuse =
>> FileUsage = 1
>> -----------------------------------------------------------------------------
>
> Except for PostgreSQL, this was the default configuration that MySQL or
> unixODBC must have entered here during installation.
>
>
>> ii) /etc/odbc.ini
>
> This file was empty in my system, so adapting your recommendation:
>
> ,-----[ /etc/odbc.ini (lines: 1 - 7) ]
> | [test]
> | Driver       = /usr/lib/odbc/libmyodbc.so
> | Database     = test
> | Servername   = localhost
> | ReadOnly     = 0
> | Port	       = 3306
> `-----
>
> and followed the same template to include the rest of my databases further
> down.
>
> Can the latter go in ~/.odbc.ini as I said before? I hope so, as this
> would allow for much easier maintenance.
>
> Thanks again for the helpful reply.
>
> -- 
> Sebastian Luque
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Feb 22 08:58:43 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 22 Feb 2005 07:58:43 +0000 (GMT)
Subject: rnorm works correctly (was [R] rnorm??)
In-Reply-To: <0A829231985BC34183758AB7FA42AA0204BE4F57@xch-nw-30.nw.nos.boeing.com>
References: <0A829231985BC34183758AB7FA42AA0204BE4F57@xch-nw-30.nw.nos.boeing.com>
Message-ID: <Pine.LNX.4.61.0502220735070.27058@gannet.stats>

On Mon, 21 Feb 2005, Scholz, Fritz wrote:

> I am wondering whether there is a bug in rnorm.

I am wondering if there is a bug in your procedures, or if you used a 
very old version of R.

If you call `bug', do have the courtesy to present your reasons.  See the 
posting guide for other missing information, like the R version, and in 
this case

> RNGkind()
[1] "Mersenne-Twister" "Inversion"

which shows the default.

> When generating rnorm(1000000) and counting
> the cases > 4 and the cases < (-4) I get rather
> unexpectedly low counts for the latter. The problem goes away
> when using qnorm(runif(1000000)).

That is the algorithm used by default internally (since 1.7.0, 
mid-2003, it seems).

> x <- rnorm(1000000); mean(x > 4); mean(x < -4)
[1] 3.5e-05
[1] 2.8e-05
> x <- rnorm(1000000); mean(x > 4); mean(x < -4)
[1] 3.5e-05
[1] 3.5e-05
> x <- rnorm(1000000); mean(x > 4); mean(x < -4)
[1] 2.7e-05
[1] 2.9e-05
> x <- rnorm(1000000); mean(x > 4); mean(x < -4)
[1] 4.3e-05
[1] 3.7e-05

looks about right.  As does

cnt <- 0
for(i in 1:100) {x <- rnorm(1000000); cnt <- cnt + sum(x < -4)}
cnt
[1] 3157

which is (to a good approximation) Poisson(3167)

> ppois(3157, 1e8*pnorm(-4))
[1] 0.4332376

or more exactly

> pbinom(3157, 1e8, pnorm(-4))
[1] 0.4332365


However, this is not a good way to generate such random events, and has 
many times tripped up people: see the cover of my 1987 simulation book for 
the theory behind one famous (and published) instance.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Feb 22 09:09:26 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 22 Feb 2005 08:09:26 +0000 (GMT)
Subject: [R] Treatment-Contrast Interactions
In-Reply-To: <421A6345.7020102@cs.umd.edu>
References: <42182376.40602@cs.umd.edu>
	<Pine.LNX.4.61.0502200953210.27414@gannet.stats>
	<4218B1CF.4080807@cs.umd.edu> <x2y8djxh2q.fsf@biostat.ku.dk>
	<421A6345.7020102@cs.umd.edu>
Message-ID: <Pine.LNX.4.61.0502220802460.27058@gannet.stats>

se.contrast computes standard errors of contrasts.  It does not compute 
the contrast itself, which is fairly simple to do directly.

> c1 <- c(1,-1)[A]*c(1,-1,0)[B]
> sum(c1*score)
[1] -18
> se.contrast(fit, as.matrix(c1))
Contrast 1
   14.24547
> (18/14.24547)^2
[1] 1.596583

so the contrast has value -18, standard error 14.24547, and the squared 
t-ratio is the F statistic 1.5966 you were asking for.  (That's a piece of 
the theory of linear models.)


On Mon, 21 Feb 2005, Lorin Hochstein wrote:

> Peter Dalgaard wrote:
>
>> Lorin Hochstein <lorin at cs.umd.edu> writes:
>> 
>> 
>>> I'd like to understand this approach as well, but I can't reproduce my
>>> results using se.contrast. In particular, I get the same standard
>>> error even though I tried to use different contrasts:
>>> 
>>> > c1 <- c(1,-1)[A]*c(1,-1,0)[B]
>>> > c2 <- c(1,-1)[A]*c(1,0,-1)[B]
>>> > c3 <- c(1,-1)[A]*c(0,1,-1)[B]
>>> > se.contrast(fit, as.matrix(c1))
>>> Contrast 1
>>>  14.24547
>>> > se.contrast(fit,as.matrix(c2))
>>> Contrast 1
>>>  14.24547
>>> > se.contrast(fit,as.matrix(c3))
>>> Contrast 1
>>>  14.24547
>>> 
>> 
>> They could well _be_ the same if the design is balanced...
>> 
>> 
> Hmmm... One of my problems is that I don't know how to interpret the output 
> of se.contrast.
>
> Here's my example again.
>> score <- c(12, 8,10, 6, 8, 4,
>      10,12, 8, 6,10,14,
>       9, 7, 9, 5,11,12,
>       7,13, 9, 9, 5,11,
>       8, 7, 3, 8,12,10,
>      13,14,19, 9,16,14)
>> n <- 6
>> A <- gl(2,3*n,labels=c("a1","a2"))
>> B <- rep(gl(3,n,labels=c("b1","b2","b3")),2)
>> contrasts(B) <- c(1,-1,0)
>> fit <- aov(score~A*B)
>> summary(fit, split=list(B=1:2), expand.split = T)
>           Df  Sum Sq Mean Sq F value   Pr(>F)  A            1  18.778 
> 18.778  2.2208 0.146606  B            2  62.000  31.000  3.6662 0.037629 *
> B: C1      1   1.500   1.500  0.1774 0.676621   B: C2      1  60.500  60.500 
> 7.1551 0.011986 *
> A:B          2  81.556  40.778  4.8226 0.015274 *
> A:B: C1    1  13.500  13.500  1.5966 0.216119     # <---
> A:B: C2    1  68.056  68.056  8.0486 0.008085 **
> Residuals   30 253.667   8.456 
> What I'm really looking for is that F value that's labelled A:B: C1, 1.5966 
> in this case. (I'm not sure what to call this term, AB interaction?)
>
> I thought that it might be possible to use se.contrast to compute this (or at 
> least, to get the numerator so that I could compute the F value once I had 
> the mean square error of the residuals), but I'm not sure how to specify the 
> contrast, and I don't know the relationship between the "standard error" 
> output by se.contrast and the "mean square error" which is the fourth column 
> of the output above.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Tue Feb 22 09:30:45 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 22 Feb 2005 09:30:45 +0100
Subject: [R] problems with nonlinear fits using nls
In-Reply-To: <E62D4EFB4204BF4F947E1BBDA775517B36C188@mail.site.cdu.edu.au>
References: <E62D4EFB4204BF4F947E1BBDA775517B36C188@mail.site.cdu.edu.au>
Message-ID: <x27jl1m1xm.fsf@biostat.ku.dk>

"Corey Bradshaw" <corey.bradshaw at cdu.edu.au> writes:

> Hello colleagues,
> 
>  
> 
> I am attempting to determine the nonlinear least-squares estimates of
> the nonlinear model parameters using nls. I have come across a common
> problem that R users have reported when I attempt to fit a particular
> 3-parameter nonlinear function to my dataset:
> 
>  
> 
> Error in nls(r ~ tlm(a, N.fix, k, theta), data = tlm.data, start =
> list(a = a.st,  : 
> 
>         step factor 0.000488281 reduced below `minFactor' of 0.000976563
> 
>  
....
> My function is:
> 
>  
> 
> tlm <- function(a,N,k,theta) (a*(1-((N/k)^theta)))
> 
>  
> 
> The nls fit I've coded is:
> 
>  
> 
> tlm.fit <- try(nls(r~tlm(a,N.fix,k,theta), data=tlm.data,
> start=list(a=a.st,k=k.st,theta=1),
> 
>             trace=TRUE,
> control=nls.control(maxiter=6000,tol=1e-05,minFactor=1/1024)))

When you wrap the expresssion to fit in the tlm function, you are
effectively keeping nls from using algebraic derivatives and forcing
it to do something like dtlm/da =~ (tlm(a+1e-7,...) -
tlm(a,...))/1e-7.

So you might try sticking in the actual expression 

  r ~ a * (1 - (N.fix/k)^theta)

or modify tlm to return a gradient attribute. Reparametrizing in terms
of log(k) might also help keeping you out fo trouble. 


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ruepalos at jcyl.es  Tue Feb 22 10:54:57 2005
From: ruepalos at jcyl.es (Oscar Rueda Palacio)
Date: Tue, 22 Feb 2005 10:54:57 +0100
Subject: [R] rnorm??
In-Reply-To: <1109035436.5924.21.camel@dhcp-63.ccc.ox.ac.uk>
Message-ID: <001e01c518c4$9155c0a0$ae43100a@jcyl.es>

Is it possible that you've mistyped sum(x<-4) instead of sum(x< -4)?

Oscar

-----Mensaje original-----
De: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]En nombre de Adaikalavan
Ramasamy
Enviado el: martes, 22 de febrero de 2005 2:24
Para: Scholz, Fritz
CC: R-help
Asunto: Re: [R] rnorm??


1) Why do you think there is a bug in rnorm. What is your expected
numbers for cases > 4 or cases < -4 ? If you can show your calculations
then maybe we can see if there is an error in how you calculated it or
with R.


2) Please give a reproducible example. It might be low or high in one
instance on a single run but if you do it several times you may not see
a consistent bias. If you do, then there could be a bug. Otherwise it
could be an element of randomness.

B    <- 10  # increase this if you can 
out1 <- matrix( nr=B, nc=2 )
out2 <- matrix( nr=B, nc=2 )
thr  <- 4

set.seed(1)
for(i in 1:B){
  
  x <- rnorm(1000000)
  out1[ i, 1 ] <- mean( x > thr )    # empirical upper tail prob
  out1[ i, 2 ] <- mean( x < -1*thr ) # empirical lower tail prob

  y <- qnorm(runif(1000000))
  out2[ i, 1 ] <- mean( y > thr )
  out2[ i, 2 ] <- mean( y < -1*thr )

  rm(x, y); print(i)
}

> out1
         [,1]    [,2]
 [1,] 3.6e-05 3.7e-05
 [2,] 3.2e-05 2.6e-05
 [3,] 3.2e-05 2.9e-05
 [4,] 3.7e-05 2.8e-05
 [5,] 3.5e-05 2.5e-05
 [6,] 4.3e-05 3.3e-05
 [7,] 3.9e-05 3.0e-05
 [8,] 3.2e-05 4.8e-05
 [9,] 3.0e-05 3.1e-05
[10,] 3.4e-05 3.7e-05

> out2
         [,1]    [,2]
 [1,] 3.5e-05 2.4e-05
 [2,] 2.4e-05 3.5e-05
 [3,] 4.1e-05 2.6e-05
 [4,] 2.5e-05 3.9e-05
 [5,] 4.1e-05 2.5e-05
 [6,] 4.0e-05 3.1e-05
 [7,] 2.9e-05 2.7e-05
 [8,] 3.1e-05 3.3e-05
 [9,] 3.5e-05 2.5e-05
[10,] 2.7e-05 3.4e-05

The probability of observing any value above 4 or below -4 is pnorm(4,
lower.tail=FALSE) = 3.167124e-05. There does not seem to be anything
suspicious from the run of 10.


3) I think this question might be more appropriate to post this question
to R-devel. Please use an informative subject line. Please read the
posting guide. Thanks.


Regards, Adai



On Mon, 2005-02-21 at 15:45 -0800, Scholz, Fritz wrote:
> I am wondering whether there is a bug in rnorm.
> When generating rnorm(1000000) and counting 
> the cases > 4 and the cases < (-4) I get rather
> unexpectedly low counts for the latter. The problem goes away
> when using qnorm(runif(1000000)).
> 
> Fritz Scholz, PhD
> Applied Statistics Group
> Boeing Phantom Works
> fritz.scholz at pss.boeing.com
> 425-865-3623
> Tu/We 206-542-6545 (most likely)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ckjmaner at carolina.rr.com  Tue Feb 22 13:18:12 2005
From: ckjmaner at carolina.rr.com (Charles and Kimberly Maner)
Date: Tue, 22 Feb 2005 07:18:12 -0500
Subject: [R] rodbc or unixodbc error
Message-ID: <200502221218.j1MCIPke020497@ms-smtp-03-eri0.southeast.rr.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050222/56148a62/attachment.pl

From bela_b at gmx.net  Tue Feb 22 13:44:10 2005
From: bela_b at gmx.net (Bela Bauer)
Date: Tue, 22 Feb 2005 13:44:10 +0100
Subject: [R] Reproducing SAS GLM in R
Message-ID: <421B291A.7070306@gmx.net>

Hi,

I'm still trying to figure out that GLM procedure in SAS.
Let's start with the simple example:

PROC GLM;
MODEL col1 col3 col5 col7 col9 col11 col13 col15 col17 col19 col21 col23 
=/nouni;
repeated roi 6, ord 2/nom mean;
TITLE 'ABDERUS lat ACC 300-500';

That's the same setup that I had in my last email. I have three factors: 
facSubj,facCond and facRoi. I had this pretty much figured out with 
three lengthy calls to lm(), but I have to extend my code to also work 
on models with four, five or even six factors, so that doesn't seem like 
a practical method any more. I've tried with the following code with 
glm(),anova() and drop1() (I use sum contrasts to reproduce those Type 
III SS values); I've also tried many other things, but this is the only 
somewhat reasonable result I get with glm.

 > options(contrasts=c("contr.sum","contr.poly"))
 > test.glm <- glm(vecData ~ (facCond+facSubj+facRoi)^2)
 > anova(test.glm,test="F")
Analysis of Deviance Table

Model: gaussian, link: identity

Response: vecData

Terms added sequentially (first to last)


                  Df Deviance Resid. Df Resid. Dev       F    Pr(>F)
NULL                               239    1429.30
facCond           1     2.21       238    1427.09  3.0764   0.08266 .
facSubj          19   685.94       219     741.16 50.2463 < 2.2e-16 ***
facRoi            5   258.77       214     482.38 72.0316 < 2.2e-16 ***
facCond:facSubj  19   172.70       195     309.68 12.6510 < 2.2e-16 ***
facCond:facRoi    5    10.37       190     299.31  2.8867   0.01803 *
facSubj:facRoi   95   231.05        95      68.26  3.3850 4.266e-09 ***
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
 > drop1(test.glm,scope=.~.,test="F")
Single term deletions

Model:
vecData ~ (facCond + facSubj + facRoi)^2
                 Df Deviance     AIC F value     Pr(F)
<none>                68.26  671.33
facCond          1    70.47  676.97  3.0764   0.08266 .
facSubj         19   754.19 1209.89 50.2463 < 2.2e-16 ***
facRoi           5   327.03 1037.35 72.0316 < 2.2e-16 ***
facCond:facSubj 19   240.96  936.05 12.6510 < 2.2e-16 ***
facCond:facRoi   5    78.63  695.27  2.8867   0.01803 *
facSubj:facRoi  95   299.31  836.09  3.3850 4.266e-09 ***
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1

Now, unfortunately this just isn't the output of SAS (roi corresponds to 
facRoi, ord corresponds to facCond)

> Source                    DF   Type III SS   Mean Square  F Value  Pr > F
> 
>  roi                        5   258.7726806    51.7545361    21.28  <.0001
>  Error(roi)                95   231.0511739     2.4321176
> 
>                                            Adj Pr > F
>                   Source                 G - G     H - F
> 
>                   roi                   <.0001    <.0001
>                   Error(roi)
> 
> 
>                    Greenhouse-Geisser Epsilon    0.5367
>                    Huynh-Feldt Epsilon           0.6333
> 
> 
>  Source                    DF   Type III SS   Mean Square  F Value  Pr > F
> 
>  ord                        1     2.2104107     2.2104107     0.24  0.6276
>  Error(ord)                19   172.7047994     9.0897263
> 
> 
>  Source                    DF   Type III SS   Mean Square  F Value  Pr > F
> 
>  roi*ord                    5   10.37034918    2.07406984     2.89  0.0180
>  Error(roi*ord)            95   68.25732255    0.71849813
> 
>                                            Adj Pr > F
>                   Source                 G - G     H - F
> 
>                   roi*ord               0.0663    0.0591
>                   Error(roi*ord)
> 
> 
>                    Greenhouse-Geisser Epsilon    0.4116
>                    Huynh-Feldt Epsilon           0.4623

As you can see, I get a correct p and F values for the facCond:facRoi 
interaction, but everything else doesn't come out right. The drop1() 
call gives me the correct degrees of freedom, but residual degrees of 
freedom seem to be wrong.

Could you give me any hints how I could get to the SAS results? For the 
lm() calls that work (in special cases), refer to my posting from last 
Friday.
I also have a 4-factorial example, and I've been told that people around 
here do 5- or 6-factorial multivariant ANOVAs, too, so I need a general 
solution.

Thanks a lot!

Bela



From bela_b at gmx.net  Tue Feb 22 13:48:44 2005
From: bela_b at gmx.net (Bela Bauer)
Date: Tue, 22 Feb 2005 13:48:44 +0100
Subject: [R] Reproducing SAS GLM in R
In-Reply-To: <421B291A.7070306@gmx.net>
References: <421B291A.7070306@gmx.net>
Message-ID: <421B2A2C.9000507@gmx.net>

Bela Bauer wrote:
<snip>

A quick addition:
I've read https://stat.ethz.ch/pipermail/r-help/2000-November/007457.html
but I really can't get around using those mechanisms because it will 
already be quite a transition for users to go from SAS to R...and I 
can't give them different results, too!
I've also seen http://www.stats.ox.ac.uk/pub/MASS3/Exegeses.pdf, which 
originally made me try drop1() and all that, but it doesn't seem to help 
me either...

Bela



From gregor.gorjanc at bfro.uni-lj.si  Tue Feb 22 14:29:00 2005
From: gregor.gorjanc at bfro.uni-lj.si (Gregor GORJANC)
Date: Tue, 22 Feb 2005 14:29:00 +0100
Subject: [R] Run Sweave and LaTeX directly from command line
Message-ID: <421B339C.1060204@bfro.uni-lj.si>

Hello!

Those of you, who use Sweave a lot, will probably find my shell script 
usable. You can get it at:

http://www.bfro.uni-lj.si/MR/ggorjan/programs/shell/Sweave.sh

No warranty, however don't hesitate to contact me if you find an error or 
have a patch!

-- 
Lep pozdrav / With regards,
     Gregor GORJANC

---------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty       URI: http://www.bfro.uni-lj.si
Zootechnical Department    mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                  tel: +386 (0)1 72 17 861
SI-1230 Domzale            fax: +386 (0)1 72 17 888
Slovenia



From p.dalgaard at biostat.ku.dk  Tue Feb 22 14:25:17 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 22 Feb 2005 14:25:17 +0100
Subject: [R] Reproducing SAS GLM in R
In-Reply-To: <421B291A.7070306@gmx.net>
References: <421B291A.7070306@gmx.net>
Message-ID: <x2sm3oybeq.fsf@biostat.ku.dk>

Bela Bauer <bela_b at gmx.net> writes:

> Hi,
> 
> I'm still trying to figure out that GLM procedure in SAS.
> Let's start with the simple example:
> 
> PROC GLM;
> MODEL col1 col3 col5 col7 col9 col11 col13 col15 col17 col19 col21
> col23 =/nouni;
> repeated roi 6, ord 2/nom mean;
> TITLE 'ABDERUS lat ACC 300-500';
> 
> That's the same setup that I had in my last email. I have three
> factors: facSubj,facCond and facRoi. I had this pretty much figured
> out with three lengthy calls to lm(), but I have to extend my code to
> also work on models with four, five or even six factors, so that
> doesn't seem like a practical method any more. I've tried with the
> following code with glm(),anova() and drop1() (I use sum contrasts to
> reproduce those Type III SS values); I've also tried many other
> things, but this is the only somewhat reasonable result I get with glm.
> 
>  > options(contrasts=c("contr.sum","contr.poly"))
>  > test.glm <- glm(vecData ~ (facCond+facSubj+facRoi)^2)
>  > anova(test.glm,test="F")

Why glm() and not lm()??

However, the real crucial thing here is that SAS is de facto fitting a
mixed-effects model, with random effects being everything with Subject
inside. So, except for the GG/HF business, you should get something
similar if you try

summary(aov(vecData ~ facCond*facRoi + Error(facSubj/(facCond*facRoi)) ))

(If you want a closer parallel to the SAS approach, you have to wait
for the mlm extensions that I have planned. I'll get to the GG/HF
issue Real Soon Now.)

> Analysis of Deviance Table
> 
> Model: gaussian, link: identity
> 
> Response: vecData
> 
> Terms added sequentially (first to last)
> 
> 
>                   Df Deviance Resid. Df Resid. Dev       F    Pr(>F)
> NULL                               239    1429.30
> facCond           1     2.21       238    1427.09  3.0764   0.08266 .
> facSubj          19   685.94       219     741.16 50.2463 < 2.2e-16 ***
> facRoi            5   258.77       214     482.38 72.0316 < 2.2e-16 ***
> facCond:facSubj  19   172.70       195     309.68 12.6510 < 2.2e-16 ***
> facCond:facRoi    5    10.37       190     299.31  2.8867   0.01803 *
> facSubj:facRoi   95   231.05        95      68.26  3.3850 4.266e-09 ***
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
>  > drop1(test.glm,scope=.~.,test="F")
> Single term deletions
> 
> Model:
> vecData ~ (facCond + facSubj + facRoi)^2
>                  Df Deviance     AIC F value     Pr(F)
> <none>                68.26  671.33
> facCond          1    70.47  676.97  3.0764   0.08266 .
> facSubj         19   754.19 1209.89 50.2463 < 2.2e-16 ***
> facRoi           5   327.03 1037.35 72.0316 < 2.2e-16 ***
> facCond:facSubj 19   240.96  936.05 12.6510 < 2.2e-16 ***
> facCond:facRoi   5    78.63  695.27  2.8867   0.01803 *
> facSubj:facRoi  95   299.31  836.09  3.3850 4.266e-09 ***
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
> 
> Now, unfortunately this just isn't the output of SAS (roi corresponds
> to facRoi, ord corresponds to facCond)
> 
> > Source                    DF   Type III SS   Mean Square  F Value  Pr > F
> >  roi                        5   258.7726806    51.7545361    21.28
> > <.0001
> >  Error(roi)                95   231.0511739     2.4321176
> >                                            Adj Pr > F
> >                   Source                 G - G     H - F
> >                   roi                   <.0001    <.0001
> >                   Error(roi)
> >                    Greenhouse-Geisser Epsilon    0.5367
> >                    Huynh-Feldt Epsilon           0.6333
> >  Source                    DF   Type III SS   Mean Square  F Value
> > Pr > F
> >  ord                        1     2.2104107     2.2104107     0.24
> > 0.6276
> >  Error(ord)                19   172.7047994     9.0897263
> >  Source                    DF   Type III SS   Mean Square  F Value
> > Pr > F
> >  roi*ord                    5   10.37034918    2.07406984     2.89
> > 0.0180
> >  Error(roi*ord)            95   68.25732255    0.71849813
> >                                            Adj Pr > F
> >                   Source                 G - G     H - F
> >                   roi*ord               0.0663    0.0591
> >                   Error(roi*ord)
> >                    Greenhouse-Geisser Epsilon    0.4116
> >                    Huynh-Feldt Epsilon           0.4623
> 
> As you can see, I get a correct p and F values for the facCond:facRoi
> interaction, but everything else doesn't come out right. The drop1()
> call gives me the correct degrees of freedom, but residual degrees of
> freedom seem to be wrong.
> 
> Could you give me any hints how I could get to the SAS results? For
> the lm() calls that work (in special cases), refer to my posting from
> last Friday.
> I also have a 4-factorial example, and I've been told that people
> around here do 5- or 6-factorial multivariant ANOVAs, too, so I need a
> general solution.
> 
> Thanks a lot!
> 
> Bela
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From pallier at lscp.ehess.fr  Tue Feb 22 14:55:09 2005
From: pallier at lscp.ehess.fr (Christophe Pallier)
Date: Tue, 22 Feb 2005 14:55:09 +0100
Subject: [R] Reproducing SAS GLM in R
In-Reply-To: <421B291A.7070306@gmx.net>
References: <421B291A.7070306@gmx.net>
Message-ID: <421B39BD.2070104@lscp.ehess.fr>

It seems that you want to do is in a Anova with the within factors 
"factCond" and "factRoi", right?

If this is correct, then you should try:

summary(aov(vectdata~factCond*factRoi+Error(factCond*factRoi)))

Christophe Pallier
www.pallier.org

Bela Bauer wrote:

> Hi,
>
> I'm still trying to figure out that GLM procedure in SAS.
> Let's start with the simple example:
>
> PROC GLM;
> MODEL col1 col3 col5 col7 col9 col11 col13 col15 col17 col19 col21 
> col23 =/nouni;
> repeated roi 6, ord 2/nom mean;
> TITLE 'ABDERUS lat ACC 300-500';
>
> That's the same setup that I had in my last email. I have three 
> factors: facSubj,facCond and facRoi. I had this pretty much figured 
> out with three lengthy calls to lm(), but I have to extend my code to 
> also work on models with four, five or even six factors, so that 
> doesn't seem like a practical method any more. I've tried with the 
> following code with glm(),anova() and drop1() (I use sum contrasts to 
> reproduce those Type III SS values); I've also tried many other 
> things, but this is the only somewhat reasonable result I get with glm.
>
> > options(contrasts=c("contr.sum","contr.poly"))
> > test.glm <- glm(vecData ~ (facCond+facSubj+facRoi)^2)
> > anova(test.glm,test="F")
> Analysis of Deviance Table
>
> Model: gaussian, link: identity
>
> Response: vecData
>
> Terms added sequentially (first to last)
>
>
>                  Df Deviance Resid. Df Resid. Dev       F    Pr(>F)
> NULL                               239    1429.30
> facCond           1     2.21       238    1427.09  3.0764   0.08266 .
> facSubj          19   685.94       219     741.16 50.2463 < 2.2e-16 ***
> facRoi            5   258.77       214     482.38 72.0316 < 2.2e-16 ***
> facCond:facSubj  19   172.70       195     309.68 12.6510 < 2.2e-16 ***
> facCond:facRoi    5    10.37       190     299.31  2.8867   0.01803 *
> facSubj:facRoi   95   231.05        95      68.26  3.3850 4.266e-09 ***
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
> > drop1(test.glm,scope=.~.,test="F")
> Single term deletions
>
> Model:
> vecData ~ (facCond + facSubj + facRoi)^2
>                 Df Deviance     AIC F value     Pr(F)
> <none>                68.26  671.33
> facCond          1    70.47  676.97  3.0764   0.08266 .
> facSubj         19   754.19 1209.89 50.2463 < 2.2e-16 ***
> facRoi           5   327.03 1037.35 72.0316 < 2.2e-16 ***
> facCond:facSubj 19   240.96  936.05 12.6510 < 2.2e-16 ***
> facCond:facRoi   5    78.63  695.27  2.8867   0.01803 *
> facSubj:facRoi  95   299.31  836.09  3.3850 4.266e-09 ***
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
>
> Now, unfortunately this just isn't the output of SAS (roi corresponds 
> to facRoi, ord corresponds to facCond)
>
>> Source                    DF   Type III SS   Mean Square  F Value  Pr 
>> > F
>>
>>  roi                        5   258.7726806    51.7545361    21.28  
>> <.0001
>>  Error(roi)                95   231.0511739     2.4321176
>>
>>                                            Adj Pr > F
>>                   Source                 G - G     H - F
>>
>>                   roi                   <.0001    <.0001
>>                   Error(roi)
>>
>>
>>                    Greenhouse-Geisser Epsilon    0.5367
>>                    Huynh-Feldt Epsilon           0.6333
>>
>>
>>  Source                    DF   Type III SS   Mean Square  F Value  
>> Pr > F
>>
>>  ord                        1     2.2104107     2.2104107     0.24  
>> 0.6276
>>  Error(ord)                19   172.7047994     9.0897263
>>
>>
>>  Source                    DF   Type III SS   Mean Square  F Value  
>> Pr > F
>>
>>  roi*ord                    5   10.37034918    2.07406984     2.89  
>> 0.0180
>>  Error(roi*ord)            95   68.25732255    0.71849813
>>
>>                                            Adj Pr > F
>>                   Source                 G - G     H - F
>>
>>                   roi*ord               0.0663    0.0591
>>                   Error(roi*ord)
>>
>>
>>                    Greenhouse-Geisser Epsilon    0.4116
>>                    Huynh-Feldt Epsilon           0.4623
>
>
> As you can see, I get a correct p and F values for the facCond:facRoi 
> interaction, but everything else doesn't come out right. The drop1() 
> call gives me the correct degrees of freedom, but residual degrees of 
> freedom seem to be wrong.
>
> Could you give me any hints how I could get to the SAS results? For 
> the lm() calls that work (in special cases), refer to my posting from 
> last Friday.
> I also have a 4-factorial example, and I've been told that people 
> around here do 5- or 6-factorial multivariant ANOVAs, too, so I need a 
> general solution.
>
> Thanks a lot!
>
> Bela
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From Friedrich.Leisch at tuwien.ac.at  Tue Feb 22 14:58:32 2005
From: Friedrich.Leisch at tuwien.ac.at (Friedrich.Leisch@tuwien.ac.at)
Date: Tue, 22 Feb 2005 14:58:32 +0100
Subject: [R] Re: Run Sweave and LaTeX directly from command line
In-Reply-To: <421B339C.1060204@bfro.uni-lj.si>
References: <421B339C.1060204@bfro.uni-lj.si>
Message-ID: <16923.14984.335575.186928@galadriel.ci.tuwien.ac.at>

>>>>> On Tue, 22 Feb 2005 14:29:00 +0100,
>>>>> Gregor GORJANC (GG) wrote:

  > Hello!
  > Those of you, who use Sweave a lot, will probably find my shell script 
  > usable. You can get it at:

  > http://www.bfro.uni-lj.si/MR/ggorjan/programs/shell/Sweave.sh

  > No warranty, however don't hesitate to contact me if you find an error or 
  > have a patch!

Very nice!

Side note 1: R ships a version of texi2dvi, hence you might use that
one in case rubber is not found.

side note 2: For make afficionados the follwing 2 rules in combination
with the Sweave script from the FAQ do almost the same (that's what I
use :-)

%.tex: %.Rnw
        Sweave $<

%.pdf : %.tex
        texi2dvi --clean --pdf $<


Best,
Fritz Leisch

-- 
-------------------------------------------------------------------
                        Friedrich Leisch 
Institut f?r Statistik                     Tel: (+43 1) 58801 10715
Technische Universit?t Wien                Fax: (+43 1) 58801 10798
Wiedner Hauptstra?e 8-10/1071
A-1040 Wien, Austria             http://www.ci.tuwien.ac.at/~leisch



From talitaperciano at hotmail.com  Tue Feb 22 15:00:41 2005
From: talitaperciano at hotmail.com (Talita Leite)
Date: Tue, 22 Feb 2005 14:00:41 +0000
Subject: [R] Loading C functions into R
Message-ID: <BAY103-F197D8E0C7065CC938E95B6C7620@phx.gbl>

Hi everybody,

I have the source of a C program that includes some archives .c and some 
libraries .h. I'm developing a program using R and I want it to load the C 
program I told before. How can I do that? I was looking for some function in 
R to do that and I found the .C() but I can't understand how it works. 
Somebody could help me?

Thanx,

Talita Perciano Costa Leite
Graduanda em Ci?ncia da Computa??o
Universidade Federal de Alagoas - UFAL
Departamento de Tecnologia da Informa??o - TCI
Constru??o de Conhecimento por Agrupamento de Dados - CoCADa



From Jan.Verbesselt at agr.kuleuven.ac.be  Tue Feb 22 15:19:21 2005
From: Jan.Verbesselt at agr.kuleuven.ac.be (Jan Verbesselt)
Date: Tue, 22 Feb 2005 15:19:21 +0100
Subject: [R] ERROR NaNs produced;
	when comparing two logistic regression models with the ANOVA
	CHI test
Message-ID: <001301c518e9$809c9840$1145210a@agr.ad10.intern.kuleuven.ac.be>

Dear R-list, 

*When comparing two logistic regression models with the anova CHi test, I
obtain the following error: (there are no NA's in the time series). How can
this be solved such that I can compare two models on the same dataset were
different explanatory variables are used? 


    l.KBDI <- glm(zna.arson2 ~ zna.KBDI,family = binomial)
    l.NDWI <- glm(zna.arson2 ~ zna.NDWI,family = binomial)

anova(l.KBDI,l.NDWI, test = "Chi")

 
Analysis of Deviance Table

Model 1: zna.arson2 ~ zna.KBDI
Model 2: zna.arson2 ~ zna.NDWI
  Resid. Df Resid. Dev  Df Deviance P(>|Chi|)
1       110    123.227                       
2       110    113.593   0    9.635          
Warning message: 
NaNs produced in: pchisq(q, df, lower.tail, log.p) 


*Is this caused by the non-linearity of the data? 
*How can I obtain a p-value?


Thanks,
Jan

_______________________________________________________________________
ir. Jan Verbesselt 
Research Associate 
Lab of Geomatics and Forest Engineering K.U. Leuven
Vital Decosterstraat 102. B-3000 Leuven Belgium 
Tel:+32-16-329750   Fax: +32-16-329760
http://gloveg.kuleuven.ac.be/



From Luisr at frs.fo  Tue Feb 22 15:28:30 2005
From: Luisr at frs.fo (Luis Ridao Cruz)
Date: Tue, 22 Feb 2005 14:28:30 +0000
Subject: [R] R-help
Message-ID: <s21b4193.026@ffdata.setur.fo>

R-help,

I am tring to create a plot with two  y-axis.
I found an example which is fine but the problem is that the range of
the second y-axes appears in the first y-axes causing confusion.
The example I refer to is :

##################################
dev.off()   ## start with a new graphics device
# X11() or postscript()
plot(x<-rnorm(100),y<-rnorm(100))
z<-rnorm(100)*250
par(new=T)   ## Tell R not to reinitialize graphic device
             ## for subsequent plots
plot(x,z,col='blue',axes=F)
axis(side=4,col.axis='blue')

par(new=F)
##################################

which can be found at :

http://www.demog.berkeley.edu/faq/node21.html 

I also have a home made-example with exactly the same problem (pretty
much the same as above)

Any solution?

Thanks in advance.

> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    0.1            
year     2004           
month    11             
day      15             
language R



From bela_b at gmx.net  Tue Feb 22 15:53:27 2005
From: bela_b at gmx.net (Bela Bauer)
Date: Tue, 22 Feb 2005 15:53:27 +0100
Subject: [R] Reproducing SAS GLM in R
In-Reply-To: <x2sm3oybeq.fsf@biostat.ku.dk>
References: <421B291A.7070306@gmx.net> <x2sm3oybeq.fsf@biostat.ku.dk>
Message-ID: <421B4767.1090209@gmx.net>

Peter Dalgaard wrote:
> However, the real crucial thing here is that SAS is de facto fitting a
> mixed-effects model, with random effects being everything with Subject
> inside. So, except for the GG/HF business, you should get something
> similar if you try
> 
> summary(aov(vecData ~ facCond*facRoi + Error(facSubj/(facCond*facRoi)) ))

Hm...that worked. Didn't expect it to be that simple!
I had tried models that were almost the same, but I didn't try that one...

> (If you want a closer parallel to the SAS approach, you have to wait
> for the mlm extensions that I have planned. I'll get to the GG/HF
> issue Real Soon Now.)

Well, I'll probably be the first one to be all over testing it!

Thanks a lot!

Bela



From ripley at stats.ox.ac.uk  Tue Feb 22 15:58:03 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 22 Feb 2005 14:58:03 +0000 (GMT)
Subject: [R] ERROR NaNs produced; when comparing two logistic regression
	models with the ANOVA CHI test
In-Reply-To: <001301c518e9$809c9840$1145210a@agr.ad10.intern.kuleuven.ac.be>
References: <001301c518e9$809c9840$1145210a@agr.ad10.intern.kuleuven.ac.be>
Message-ID: <Pine.LNX.4.61.0502221453420.10511@gannet.stats>

You need the models to be NESTED for this test to be valid.  Look up 
`analysis of deviance' in your textbooks.

You get the error because you have two unnested models with the same 
number of parameters, so the F numerator is divided by 0. (But this is 
merely a symptom of incorrect usage.)

On Tue, 22 Feb 2005, Jan Verbesselt wrote:

> Dear R-list,
>
> *When comparing two logistic regression models with the anova CHi test, I
> obtain the following error: (there are no NA's in the time series). How can
> this be solved such that I can compare two models on the same dataset were
> different explanatory variables are used?
>
>
>    l.KBDI <- glm(zna.arson2 ~ zna.KBDI,family = binomial)
>    l.NDWI <- glm(zna.arson2 ~ zna.NDWI,family = binomial)
>
> anova(l.KBDI,l.NDWI, test = "Chi")
>
>
> Analysis of Deviance Table
>
> Model 1: zna.arson2 ~ zna.KBDI
> Model 2: zna.arson2 ~ zna.NDWI
>  Resid. Df Resid. Dev  Df Deviance P(>|Chi|)
> 1       110    123.227
> 2       110    113.593   0    9.635
> Warning message:
> NaNs produced in: pchisq(q, df, lower.tail, log.p)
>
>
> *Is this caused by the non-linearity of the data?
> *How can I obtain a p-value?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dimitris.rizopoulos at med.kuleuven.ac.be  Tue Feb 22 16:02:30 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Tue, 22 Feb 2005 16:02:30 +0100
Subject: [R] ERROR NaNs produced;
	when comparing two logistic regression models with the ANOVACHI test
References: <001301c518e9$809c9840$1145210a@agr.ad10.intern.kuleuven.ac.be>
Message-ID: <007f01c518ef$881205a0$0540210a@www.domain>

these models are not nested and thus LRT should not be used. You get 
no p-value because Df=0 in your case. As an alternative you could look 
at the deviances.

I hope it helps.

Best,
Dimitris

p.s., you referred to `non-linearity of the data' if you mean that the 
probability of positive responses is a nonlinear function of your 
explanatory variables then you could use splines to model it (see 
e.g., at function `lrm()' from the Design package).

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Jan Verbesselt" <Jan.Verbesselt at agr.kuleuven.ac.be>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, February 22, 2005 3:19 PM
Subject: [R] ERROR NaNs produced;when comparing two logistic 
regression models with the ANOVACHI test


> Dear R-list,
>
> *When comparing two logistic regression models with the anova CHi 
> test, I
> obtain the following error: (there are no NA's in the time series). 
> How can
> this be solved such that I can compare two models on the same 
> dataset were
> different explanatory variables are used?
>
>
>    l.KBDI <- glm(zna.arson2 ~ zna.KBDI,family = binomial)
>    l.NDWI <- glm(zna.arson2 ~ zna.NDWI,family = binomial)
>
> anova(l.KBDI,l.NDWI, test = "Chi")
>
>
> Analysis of Deviance Table
>
> Model 1: zna.arson2 ~ zna.KBDI
> Model 2: zna.arson2 ~ zna.NDWI
>  Resid. Df Resid. Dev  Df Deviance P(>|Chi|)
> 1       110    123.227
> 2       110    113.593   0    9.635
> Warning message:
> NaNs produced in: pchisq(q, df, lower.tail, log.p)
>
>
> *Is this caused by the non-linearity of the data?
> *How can I obtain a p-value?
>
>
> Thanks,
> Jan
>
> _______________________________________________________________________
> ir. Jan Verbesselt
> Research Associate
> Lab of Geomatics and Forest Engineering K.U. Leuven
> Vital Decosterstraat 102. B-3000 Leuven Belgium
> Tel:+32-16-329750   Fax: +32-16-329760
> http://gloveg.kuleuven.ac.be/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From rob at fatkat.com  Tue Feb 22 16:08:01 2005
From: rob at fatkat.com (Rob Steele)
Date: Tue, 22 Feb 2005 10:08:01 -0500
Subject: [R] R-help
Message-ID: <421B4AD1.8030204@fatkat.com>

> I am tring to create a plot with two y-axis.
> I found an example which is fine but the problem is that the range of
> the second y-axes appears in the first y-axes causing confusion. [...]

The key is the "ann = FALSE" in the second plot():

n = 100
x = rnorm(n)
y = rnorm(n)
z = rnorm(n) * 250

par(mar = c(5, 4, 4, 5) + 0.1)
plot(x, y, ann = FALSE)

par(new = TRUE)
plot(x, z, col = 'blue', axes = FALSE, ann = FALSE)
axis(side = 4, col.axis = 'blue')

title(xlab = 'x', ylab = 'y')
mtext('z', side = 4, line = 3)



From abunn at whrc.org  Tue Feb 22 16:15:26 2005
From: abunn at whrc.org (abunn)
Date: Tue, 22 Feb 2005 10:15:26 -0500
Subject: [R] R-help
In-Reply-To: <s21b4193.026@ffdata.setur.fo>
Message-ID: <NEBBIPHDAMMOKDKPOFFIKEBFDBAA.abunn@whrc.org>

This is what you are trying to get at I think:

dev.off()   ## start with a new graphics device
# X11() or postscript()
#par(mar = c(3,3,1,3), oma = c(0,0,0,0), mgp = c(2, 1, 0), bg = "white")
par(mar = rep(5,4))
plot(x<-rnorm(100),y<-rnorm(100), ylab = "This is y", xlab = "This is x")
z<-rnorm(100)*250
par(new=T)   ## Tell R not to reinitialize graphic device
             ## for subsequent plots
plot(x,z,col='blue',axes=F, xlab="", ylab="")
axis(side=4,col.axis='blue', labels = T)
mtext("This is z", 4, 2, col = "blue")
par(new=F)


HTH, Andy




> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Luis Ridao Cruz
> Sent: Tuesday, February 22, 2005 9:29 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] R-help
> 
> 
> R-help,
> 
> I am tring to create a plot with two  y-axis.
> I found an example which is fine but the problem is that the range of
> the second y-axes appears in the first y-axes causing confusion.
> The example I refer to is :
> 
> ##################################
> dev.off()   ## start with a new graphics device
> # X11() or postscript()
> plot(x<-rnorm(100),y<-rnorm(100))
> z<-rnorm(100)*250
> par(new=T)   ## Tell R not to reinitialize graphic device
>              ## for subsequent plots
> plot(x,z,col='blue',axes=F)
> axis(side=4,col.axis='blue')
> 
> par(new=F)
> ##################################
> 
> which can be found at :
> 
> http://www.demog.berkeley.edu/faq/node21.html 
> 
> I also have a home made-example with exactly the same problem (pretty
> much the same as above)
> 
> Any solution?
> 
> Thanks in advance.
> 
> > version
>          _              
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    2              
> minor    0.1            
> year     2004           
> month    11             
> day      15             
> language R
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html



From rbb at nebiometrics.com  Tue Feb 22 16:25:01 2005
From: rbb at nebiometrics.com (Robert Burrows)
Date: Tue, 22 Feb 2005 10:25:01 -0500 (EST)
Subject: [R] using 'nice' with R
Message-ID: <Pine.LNX.4.62.0502221021420.1696@neblt2>


I would like to use the 'nice' command when running CPU-intensive R 
functions on a server. Will starting R with 'nice R' do the trick?

-- 
Robert Burrows, PhD
New England Biometrics
rbb at nebiometrics.com



From rpeng at jhsph.edu  Tue Feb 22 16:54:28 2005
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Tue, 22 Feb 2005 10:54:28 -0500
Subject: [R] using 'nice' with R
In-Reply-To: <Pine.LNX.4.62.0502221021420.1696@neblt2>
References: <Pine.LNX.4.62.0502221021420.1696@neblt2>
Message-ID: <421B55B4.6020600@jhsph.edu>

On a Unix like system you can do `nice +19 R' or perhaps `nice +19 R 
CMD BATCH commands.R'.

-roger

Robert Burrows wrote:
> 
> I would like to use the 'nice' command when running CPU-intensive R 
> functions on a server. Will starting R with 'nice R' do the trick?
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From Fritz.Scholz at boeing.com  Tue Feb 22 17:06:33 2005
From: Fritz.Scholz at boeing.com (Scholz, Fritz)
Date: Tue, 22 Feb 2005 08:06:33 -0800
Subject: [R] re: rnorm??
Message-ID: <0A829231985BC34183758AB7FA42AA0204BE4F5D@xch-nw-30.nw.nos.boeing.com>

I was not able to reproduce the behavior reported 
in my message below. I was trying it on the same laptop
in the same workspace using again R 2.0.0, but the
low counts of 4 to 7 (witnesses by some of my colleagues)
did not show up again. Please do not spend any more 
effort on this. 

>I am wondering whether there is a bug in rnorm.
>When generating rnorm(1000000) and counting 
>the cases > 4 and the cases < (-4) I get rather
>unexpectedly low counts for the latter. The problem goes away
>when using qnorm(runif(1000000)).


Fritz Scholz, PhD
Applied Statistics Group
Boeing Phantom Works
fritz.scholz at pss.boeing.com
425-865-3623
Tu/We 206-542-6545 (most likely)



From buser at stat.math.ethz.ch  Tue Feb 22 17:19:55 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Tue, 22 Feb 2005 17:19:55 +0100
Subject: [R] problem with se.contrast()
In-Reply-To: <Pine.LNX.4.61.0502210608250.7362@gannet.stats>
References: <42139BF7.2090809@stat.ufl.edu>
	<16916.50894.173426.42673@stat.math.ethz.ch>
	<Pine.LNX.4.61.0502202250090.2583@gannet.stats>
	<x2k6p2y7a4.fsf@biostat.ku.dk>
	<Pine.LNX.4.61.0502210608250.7362@gannet.stats>
Message-ID: <16923.23467.603519.651397@stat.math.ethz.ch>

Dear Prof Ripley, Dear Prof Dalgaard 

Thank you both for your help. I tried it with helmert contrasts
and got a result that is consistent with lme. I didn't realize
that the parameterization of the model has an influence on the
contrasts that I tried to test. 
It seems that I should read a little bit more about this issue
for my better understanding.

I have one last point to propose:

You could include (as interim solution) a warning (that there might
be an in efficiency loss) in se.contrast() if one uses
non-orthogonal contrasts and a multi-stratum model.

Best regards,

Christoph Buser

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C11
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-1-632-5414		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------


Prof Brian Ripley writes:
 > On Mon, 21 Feb 2005, Peter Dalgaard wrote:
 > 
 > > Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:
 > >
 > >>>> test.aov <- with(testdata,aov(Measurement ~ Material + Error(Lab/Material)))
 > >>>> se.contrast(test.aov,
 > >>>>             list(Material=="A",Material=="B",Material=="C",Material=="D"),
 > >>>>             coef=c(0.5,0.5,-0.5,-0.5),data=testdata)
 > >>>> [1] 0.1432572
 > >>>
 > >>> I got a different result and I have admit that I didn't understand why
 > >>> there is a differnce between the lme model and this one. There are some
 > >>> comments in the help pages but I'm not sure if this is the answer.
 > >>
 > >> It is.  You used the default `contrasts', which are not actually
 > >> contrasts.  With
 > >>
 > >> options(contrasts=c("contr.helmert", "contr.poly"))
 > >>
 > >> it gives the same answer as the other two.  Because you used
 > >> non-contrasts there was an efficiency loss (to the Intercept stratum).
 > >
 > > Brian,
 > >
 > > I'm not sure how useful that contrasts-that-are-not-contrasts line is.
 > 
 > I agree, it was not precise enough (too late at night for me).  Try 
 > `non-orthogonal contrasts'.  The issue was correct though, it is the 
 > choice of contrasts, and as I would automatically use orthogonal contrasts 
 > with aov() I had not encountered it and it took me a while to pick up on 
 > what Christoph had done differently from me (I had run the example and got 
 > the same result as the randomized-block analysis before my original post).
 > 
 > There's a comment in the code for aov:
 > 
 >          ##  helmert contrasts can be helpful: do we want to force them?
 >          ##  this version does for the Error model.
 > 
 > and perhaps we should make them the default for the per-stratum fits.
 > 
 > > It certainly depends on your definition of "contrasts". Contrast
 > > matrices having zero column sums was not part of the definition I was
 > > taught. I have contrasts as "representations of the group mean
 > > structure that are invariant to changes of the overall level", so
 > > treatment contrasts are perfectly good contrasts in my book.
 > 
 > I don't think Yates thought in those terms, and the whole idea of dividing 
 > into strata (and the generalized Yates algorithm) is based on contrasts 
 > being orthogonal to the overall mean (and to things in other strata).
 > 
 > It was that, not zero-sum, that I was taught, but in balanced cases such 
 > as here it is the same thing.
 > 
 > > The zero-sum condition strikes me as a bit arbitrary: after all there
 > > are perfectly nice orthogonal designs where some levels of a factor
 > > occur more frequently than others.
 > 
 > Balance and orthogonality are not quite the same, though.
 > 
 > > This in turn makes me a bit wary of what is going on inside 
 > > se.contrasts, but it's gotten too late for me to actually study the code 
 > > tonight.
 > >
 > > Could you elaborate on where precisely the condition on the contrast
 > > matrices comes into play?
 > 
 > In finding the projection lengths in eff.aovlist, here
 > 
 >      proj.len <-
 >  	lapply(aovlist, function(x)
 >  	   {
 >  	       asgn <- x$assign[x$qr$pivot[1:x$rank]]
 >  	       sp <- split(seq(along=asgn), attr(terms(x), "term.labels")[asgn])
 >  	       sapply(sp, function(x, y) sum(y[x]), y=diag(x$qr$qr)^2)
 >  	   })
 > 
 > using only the diagonal requires orthogonality of the coding.
 > 
 > It is many years since I looked at this, and it is not immediately clear 
 > to me how best to calculate efficiencies without this assumption (which 
 > could be tested inside eff.aovlist, of course).
 > 
 > [People wondering why this was ever useful given lme should be aware that
 > 1) It predates the availability of lme.
 > 2) When I first used this in 1998, lme appeared very slow.
 > 3) People with a classical aov training (not me, but e.g. Bill Venables) 
 > think in these terms.]
 > 
 > -- 
 > Brian D. Ripley,                  ripley at stats.ox.ac.uk
 > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
 > University of Oxford,             Tel:  +44 1865 272861 (self)
 > 1 South Parks Road,                     +44 1865 272866 (PA)
 > Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From klaus.ladner at uni-graz.at  Tue Feb 22 17:23:53 2005
From: klaus.ladner at uni-graz.at (Klaus Ladner)
Date: Tue, 22 Feb 2005 17:23:53 +0100
Subject: [R] colorbar for image
Message-ID: <421B5C99.50005@uni-graz.at>

Dear colleges,

Does anyone know, how to insert a color bar as used with 
"filled.contour" when using "image"?



From ripley at stats.ox.ac.uk  Tue Feb 22 17:26:06 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 22 Feb 2005 16:26:06 +0000 (GMT)
Subject: [R] problem with se.contrast()
In-Reply-To: <16923.23467.603519.651397@stat.math.ethz.ch>
References: <42139BF7.2090809@stat.ufl.edu>
	<16916.50894.173426.42673@stat.math.ethz.ch>
	<Pine.LNX.4.61.0502202250090.2583@gannet.stats>
	<x2k6p2y7a4.fsf@biostat.ku.dk>
	<Pine.LNX.4.61.0502210608250.7362@gannet.stats>
	<16923.23467.603519.651397@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.61.0502221623210.11051@gannet.stats>

On Tue, 22 Feb 2005, Christoph Buser wrote:

> Dear Prof Ripley, Dear Prof Dalgaard
>
> Thank you both for your help. I tried it with helmert contrasts
> and got a result that is consistent with lme. I didn't realize
> that the parameterization of the model has an influence on the
> contrasts that I tried to test.
> It seems that I should read a little bit more about this issue
> for my better understanding.
>
> I have one last point to propose:
>
> You could include (as interim solution) a warning (that there might
> be an in efficiency loss) in se.contrast() if one uses
> non-orthogonal contrasts and a multi-stratum model.

If you meant on the help page, I have already done that.
We are still thinking about how to detect the problem well enough, and 
possibly even to see if we can avoid it.

It's trickier than I remembered, although I think I did probably once 
know.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From matthias.tips at student.kuleuven.ac.be  Tue Feb 22 17:37:52 2005
From: matthias.tips at student.kuleuven.ac.be (Matthias Tips)
Date: Tue, 22 Feb 2005 17:37:52 +0100
Subject: [R] Question about NA's 
Message-ID: <1109090272.421b5fe0ea438@webmail1.kuleuven.be>

Hi,

i got a little question about the NA's in vectors and matrices. When I 
want to do some operations on a matrix with some NA's it is possible 
to stripp them before the computation proceed (by na.rm=TRUE).

But how can I stripp NA's when I want to proceed a 'if', 'for' 
or 'while' string? Because when I don't stripp them I always reveive 
the message :"missing value where TRUE/FALSE needed". I don't know if 
I can use the method with na.rm=TRUE and if so, how I can do it.

Thanks a lot in advance!

Matthias Tips



From TBrough at russell.com  Tue Feb 22 17:40:32 2005
From: TBrough at russell.com (Brough, Tyler (FRS))
Date: Tue, 22 Feb 2005 08:40:32 -0800
Subject: [R] Error when using do.call
Message-ID: <4107412F78EEFF43A0F6F389061E8B32091B14@MAIL002.prod.ds.russell.com>

useRs,

I'm using version 2.0.1 on Windows XP.  I am a bit of a newbie and I am
trying to learn the concept of computing on the language.  I have an example
that I think ought to work, but will not and I am not sure what I am doing
wrong.  

I would like to sort a data frame by a list of columns.  Eventually I would
like to wrap this in a function so that I could sort data frames by a list
determined from context.  Any suggestions?  Thanks for your patience as I
strive to learn some of the finer points.

Here is a reproduction of the problem:

#***************************************************************************
*************************
A <- data.frame(X = sample(c("A","B"),size=10,replace=T), Y =
sample(c(T,F),size = 10, replace = T),
	Z = sample(1:10,size=10,replace=T));

arglist <- list();
ind <- 3:1
for(i in 1:3) {
	arglist[[i]] <- as.name(paste("A[,",ind[i],"]",sep=""));
}

do.call(what="order",args=arglist)
#***************************************************************************
*************************

I get the following error message:

> do.call(what="order",args=arglist)
Error in order('A[,3]','A[,2]','A[,1]') : 
	  Object "A[,1]" not found
>

Thanks, 

-Tyler



From gunter.berton at gene.com  Tue Feb 22 17:41:07 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 22 Feb 2005 08:41:07 -0800
Subject: [R] Categories or clusters for univariate data
In-Reply-To: <33F91FB3FDF42E4180428AC66A5CF30B02D3C9AC@afhex01.dpi.wa.gov.au>
Message-ID: <200502221641.j1MGf7NJ018845@compton.gene.com>

> > bounds for each group.  My question is, is there a function 
> > in R that can do 
> > the same thing for more complex and subtle groupings in 
> > univariate data, and 

>>    ** provide a statistical basis for the result? **

No. Others have suggested useful ways to **generate** reasonable hypotheses
about "subtle groupings" in the data; however, by the nature and logic of
hypothesis testing, one cannot then evaluate the statistical "significance"
of any groupings that one purports to have found.

One **possible** way of overcoming this dilemma is to randomly bifurcate the
data into training and test sets, do ALL model development on the training
set, and then evaluate statistical "significance" (once and only once) on
the test set. However, one may argue that even this blows up type I error,
as the random split likely preserves the same structures in both and thus
doesn't eliminate the large bias of testing models fit to the random
anomalies of the data set at hand.

As A.S.C Ehrenberg argued many years ago -- and recent events on the U.S.
Cox II regulatory stage have dramatized -- single sets of data cannot be
used as the basis for scientific knowledge; multiple sets of data generated
under different conditions and with different sources of exogenous variation
are required.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box



From Cedric.Ginestet at tvu.ac.uk  Tue Feb 22 17:50:51 2005
From: Cedric.Ginestet at tvu.ac.uk (Cedric.Ginestet@tvu.ac.uk)
Date: Tue, 22 Feb 2005 16:50:51 -0000
Subject: [R] Graphics
Message-ID: <4E9E9056B353854C8AB90B2B6647183009A09813@exchange1.exchange.tvu.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050222/b1922d29/attachment.pl

From friendly at yorku.ca  Tue Feb 22 17:52:01 2005
From: friendly at yorku.ca (Michael Friendly)
Date: Tue, 22 Feb 2005 11:52:01 -0500
Subject: [R] Two-factorial Huynh-Feldt-Test
In-Reply-To: <4219C14E.1080401@gmx.net>
References: <4215A1BB.3020400@gmx.net>	<x2r7je5iav.fsf@biostat.ku.dk>	<4215D2E0.5040606@cbs.mpg.de>	<x2braic8lo.fsf@biostat.ku.dk>	<42199C8B.4080807@gmx.net>	<x2psyujmcf.fsf@biostat.ku.dk>
	<4219C14E.1080401@gmx.net>
Message-ID: <421B6331.8040104@yorku.ca>



Bela Bauer wrote:

> I'm still looking for an efficient way to print the new summary. Is 
> there any easy way to tell the summary or print functions about the 
> corrected degrees of freedom?
> 

The standard way is not to print the adjusted df, but rather just the
adjusted p-values (along with the \epsilon s)

-Michael



From W.E.Wolski at newcastle.ac.uk  Tue Feb 22 17:56:27 2005
From: W.E.Wolski at newcastle.ac.uk (W.E. Wolski)
Date: Tue, 22 Feb 2005 16:56:27 +0000 (GMT)
Subject: [R] The system command and the LD_LIBRARY_PATH
Message-ID: <Pine.SOL.4.21.0502221648540.11886-100000@finan.ncl.ac.uk>

Hi R-gurus,

In my envirovment I have specified the LD_LIBRARY_PATH to
/data/opt/libmy/lib

After starting R the LD_LIBRARY_PATH variable is changed.
> Sys.getenv("LD_LIBRARY_PATH")

"/data/opt/R-devel//lib/R/lib:/usr/local/lib:/usr/X11R6/lib:/data/opt/libmy/lib" 


The problem is that in /usr/local/lib is an acient version of mylib hence
a call to _system_ with fails badly.



Suggestions?

Eryk



From friendly at yorku.ca  Tue Feb 22 17:52:01 2005
From: friendly at yorku.ca (Michael Friendly)
Date: Tue, 22 Feb 2005 11:52:01 -0500
Subject: [R] Two-factorial Huynh-Feldt-Test
In-Reply-To: <4219C14E.1080401@gmx.net>
References: <4215A1BB.3020400@gmx.net>	<x2r7je5iav.fsf@biostat.ku.dk>	<4215D2E0.5040606@cbs.mpg.de>	<x2braic8lo.fsf@biostat.ku.dk>	<42199C8B.4080807@gmx.net>	<x2psyujmcf.fsf@biostat.ku.dk>
	<4219C14E.1080401@gmx.net>
Message-ID: <421B6331.8040104@yorku.ca>



Bela Bauer wrote:

> I'm still looking for an efficient way to print the new summary. Is 
> there any easy way to tell the summary or print functions about the 
> corrected degrees of freedom?
> 

The standard way is not to print the adjusted df, but rather just the
adjusted p-values (along with the \epsilon s)

-Michael



From ggrothendieck at myway.com  Tue Feb 22 18:15:15 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 22 Feb 2005 12:15:15 -0500 (EST)
Subject: [R] Run Sweave and LaTeX directly from command line
Message-ID: <20050222171515.BBD4C3AB4@mprdmxin.myway.com>



It might be possible to translate 90% of this into R leaving
just a short shell portion in which case it would be easy
to translate just that part into Windows batch making it
available on that OS too.  As someone already mentioned,
using texi2dvi could be helpful too.

Regards.


Date:   Tue, 22 Feb 2005 14:29:00 +0100 
From:   Gregor GORJANC <gregor.gorjanc at bfro.uni-lj.si>
To:   <Friedrich.Leisch at ci.tuwien.ac.at>, <r-help at stat.math.ethz.ch> 
Subject:   [R] Run Sweave and LaTeX directly from command line 

 
Hello!

Those of you, who use Sweave a lot, will probably find my shell script 
usable. You can get it at:

http://www.bfro.uni-lj.si/MR/ggorjan/programs/shell/Sweave.sh

No warranty, however don't hesitate to contact me if you find an error or 
have a patch!

-- 
Lep pozdrav / With regards,
Gregor GORJANC

---------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty URI: http://www.bfro.uni-lj.si
Zootechnical Department mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3 tel: +386 (0)1 72 17 861
SI-1230 Domzale fax: +386 (0)1 72 17 888
Slovenia



From murdoch at stats.uwo.ca  Tue Feb 22 18:22:40 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 22 Feb 2005 17:22:40 +0000
Subject: [R] Graphics
In-Reply-To: <4E9E9056B353854C8AB90B2B6647183009A09813@exchange1.exchange.tvu.ac.uk>
References: <4E9E9056B353854C8AB90B2B6647183009A09813@exchange1.exchange.tvu.ac.uk>
Message-ID: <2hqm11pkmc6f041sf8fejahsla1dik40r5@4ax.com>

On Tue, 22 Feb 2005 16:50:51 -0000, <Cedric.Ginestet at tvu.ac.uk> wrote
:

>Hi, 
>
>The R platform that I installed on my Windows XP crashes everytime that
>I try to run some sophisticated graphics (e.g. Demo Graphics). Is that
>to do with the configuration? Shall I reinstall it? 

You should say what version you installed, and give the command that
crashes it, but I would say there is definitely something wrong with
your system or your install, because it certainly doesn't crash for
most people.

Duncan Murdoch



From ripley at stats.ox.ac.uk  Tue Feb 22 18:31:38 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 22 Feb 2005 17:31:38 +0000 (GMT)
Subject: [R] Graphics
In-Reply-To: <4E9E9056B353854C8AB90B2B6647183009A09813@exchange1.exchange.tvu.ac.uk>
References: <4E9E9056B353854C8AB90B2B6647183009A09813@exchange1.exchange.tvu.ac.uk>
Message-ID: <Pine.LNX.4.61.0502221728460.2197@gannet.stats>


On Tue, 22 Feb 2005 Cedric.Ginestet at tvu.ac.uk wrote:

> The R platform that I installed on my Windows XP crashes everytime that
> I try to run some sophisticated graphics (e.g. Demo Graphics). Is that
> to do with the configuration? Shall I reinstall it?

Please consult the rw-FAQ.

It is likely to be a problem with your Windows installation, as R runs on 
literally thousands (maybe tens of thousands) of Windows XP machines.

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

which points you at the rw-FAQ.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rolf at math.unb.ca  Tue Feb 22 18:32:27 2005
From: rolf at math.unb.ca (Rolf Turner)
Date: Tue, 22 Feb 2005 13:32:27 -0400 (AST)
Subject: [R] colorbar for image
Message-ID: <200502221732.j1MHWRf9002989@erdos.math.unb.ca>


Klaus Ladner wrote:

> Does anyone know, how to insert a color bar as used with 
> "filled.contour" when using "image"?

See plot.im() (and im()) in the package spatstat.

				cheers,

					Rolf Turner
					rolf at math.unb.ca



From ripley at stats.ox.ac.uk  Tue Feb 22 18:35:01 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 22 Feb 2005 17:35:01 +0000 (GMT)
Subject: [R] The system command and the LD_LIBRARY_PATH
In-Reply-To: <Pine.SOL.4.21.0502221648540.11886-100000@finan.ncl.ac.uk>
References: <Pine.SOL.4.21.0502221648540.11886-100000@finan.ncl.ac.uk>
Message-ID: <Pine.LNX.4.61.0502221732010.2197@gannet.stats>

On Tue, 22 Feb 2005, W.E. Wolski wrote:

> Hi R-gurus,
>
> In my envirovment I have specified the LD_LIBRARY_PATH to
> /data/opt/libmy/lib
>
> After starting R the LD_LIBRARY_PATH variable is changed.
>> Sys.getenv("LD_LIBRARY_PATH")
>
> "/data/opt/R-devel//lib/R/lib:/usr/local/lib:/usr/X11R6/lib:/data/opt/libmy/lib"
>
>
> The problem is that in /usr/local/lib is an acient version of mylib hence
> a call to _system_ with fails badly.
>
>
>
> Suggestions?

Read the R-admin manual (as the INSTALL file asks) and set the flags 
correctly for your setup when you install R.  In particular, read 
carefully what it says about LDFLAGS: this should not be a surprise to 
you.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Achim.Zeileis at wu-wien.ac.at  Tue Feb 22 18:37:27 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 22 Feb 2005 18:37:27 +0100
Subject: [R] Categories or clusters for univariate data
In-Reply-To: <200502221641.j1MGf7NJ018845@compton.gene.com>
References: <33F91FB3FDF42E4180428AC66A5CF30B02D3C9AC@afhex01.dpi.wa.gov.au>
	<200502221641.j1MGf7NJ018845@compton.gene.com>
Message-ID: <20050222183727.67d21ef7.Achim.Zeileis@wu-wien.ac.at>

On Tue, 22 Feb 2005 08:41:07 -0800 Berton Gunter wrote:

> > > bounds for each group.  My question is, is there a function 
> > > in R that can do 
> > > the same thing for more complex and subtle groupings in 
> > > univariate data, and 
> 
> >>    ** provide a statistical basis for the result? **
> 
> No. Others have suggested useful ways to **generate** reasonable
> hypotheses about "subtle groupings" in the data; however, by the
> nature and logic of hypothesis testing, one cannot then evaluate the
> statistical "significance" of any groupings that one purports to have
> found.

Just one more remark on this:
The above is, of course, true if standard inference would be applied
on the same data that was used for finding the groupings. But it is also
possible to test for the existence of such groupings using non-standard
inference.

For example, in a structural change context the supF test of Andrews
(1993, Econometrica) is very popular in econometrics. It is essentially
the LR statistic of the model without a breakpoint vs. the optimally
segmented model with one breakpoint. But the distribution is then no
longer Chi-squared as it has to be accounted for the selection of the
breakpoint (i.e., the groupings).

Hence, the standard approach in econometrics for this would be:
  1. test for the existence of breaks
  2. estimate breakpoints (if not already  implicitely done in 1.)
 
Of course, using a (cross-)validation approach is not a bad idea,
either!
Z



From choid at ohsu.edu  Tue Feb 22 19:10:58 2005
From: choid at ohsu.edu (Dongseok Choi)
Date: Tue, 22 Feb 2005 10:10:58 -0800
Subject: [R] round() - strange results
Message-ID: <s21b0547.039@ohsu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050222/3f8a012f/attachment.pl

From rolf at math.unb.ca  Tue Feb 22 19:30:12 2005
From: rolf at math.unb.ca (Rolf Turner)
Date: Tue, 22 Feb 2005 14:30:12 -0400 (AST)
Subject: [R] round() - strange results
Message-ID: <200502221830.j1MIUCN5005258@erdos.math.unb.ca>


This in NOT a bug.  It is a convention (the IEEE standard).
Read the help on round().  I.e. RTFM.

			cheers,

				Rolf Turner
				rolf at math.unb.ca



From ligges at statistik.uni-dortmund.de  Tue Feb 22 19:36:02 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 22 Feb 2005 19:36:02 +0100
Subject: [R] round() - strange results
In-Reply-To: <s21b0547.039@ohsu.edu>
References: <s21b0547.039@ohsu.edu>
Message-ID: <421B7B92.2030504@statistik.uni-dortmund.de>

Dongseok Choi wrote:

> Hello,
>  
>   I found that round() does not behave as I expected.
>   Have you had similar experience as following?

You have found the DOCUMENTED "round to even" rule, hence not a bug...

Uwe Ligges

> 
>>x<-seq(0.5,10.5,by=1)
>>x
> 
>  [1]  0.5  1.5  2.5  3.5  4.5  5.5  6.5  7.5  8.5  9.5 10.5
> 
>>round(x)
> 
>  [1]  0  2  2  4  4  6  6  8  8 10 10
> 
>>cbind(x,round(x))
> 
>          x   
>  [1,]  0.5  0
>  [2,]  1.5  2
>  [3,]  2.5  2
>  [4,]  3.5  4
>  [5,]  4.5  4
>  [6,]  5.5  6
>  [7,]  6.5  6
>  [8,]  7.5  8
>  [9,]  8.5  8
> [10,]  9.5 10
> [11,] 10.5 10
> 
>  
>   Is this a well-known bug?
>  
> Thanks in advance,
>  
>  
> Dongseok Choi, Ph.D.
> Assistant Professor
> Division of Biostatistics
> Department of Public Health & Preventive Medicine
> Oregon Health & Science University
> 3181 SW Sam Jackson Park Road, CB-669
> Portland, OR 97239-3098
> TEL) 503-494-5336
> FAX) 503-494-4981
> choid at ohsu.edu
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ccleland at optonline.net  Tue Feb 22 19:34:11 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Tue, 22 Feb 2005 13:34:11 -0500
Subject: [R] round() - strange results
In-Reply-To: <s21b0547.039@ohsu.edu>
References: <s21b0547.039@ohsu.edu>
Message-ID: <421B7B23.2030406@optonline.net>

?round

'round' rounds the values in its first argument to the specified
number of decimal places (default 0). Note that for rounding off a
5, the IEEE standard is used, "_go to the even digit_". Therefore
'round(0.5)' is '0' and 'round(-1.5)' is '-2'.

Dongseok Choi wrote:
> Hello,
>  
>   I found that round() does not behave as I expected.
>   Have you had similar experience as following?
>  
> 
>>x<-seq(0.5,10.5,by=1)
>>x
> 
>  [1]  0.5  1.5  2.5  3.5  4.5  5.5  6.5  7.5  8.5  9.5 10.5
> 
>>round(x)
> 
>  [1]  0  2  2  4  4  6  6  8  8 10 10
> 
>>cbind(x,round(x))
> 
>          x   
>  [1,]  0.5  0
>  [2,]  1.5  2
>  [3,]  2.5  2
>  [4,]  3.5  4
>  [5,]  4.5  4
>  [6,]  5.5  6
>  [7,]  6.5  6
>  [8,]  7.5  8
>  [9,]  8.5  8
> [10,]  9.5 10
> [11,] 10.5 10
> 
>  
>   Is this a well-known bug?
>  
> Thanks in advance,
>  
>  
> Dongseok Choi, Ph.D.
> Assistant Professor
> Division of Biostatistics
> Department of Public Health & Preventive Medicine
> Oregon Health & Science University
> 3181 SW Sam Jackson Park Road, CB-669
> Portland, OR 97239-3098
> TEL) 503-494-5336
> FAX) 503-494-4981
> choid at ohsu.edu
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From andy_liaw at merck.com  Tue Feb 22 19:41:34 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 22 Feb 2005 13:41:34 -0500
Subject: [R] Error when using do.call
Message-ID: <3A822319EB35174CA3714066D590DCD50994E74F@usrymx25.merck.com>

Why not just do:

do.call("order", A[, ind])

??  A data frame is already a list.

Andy

> From: Brough, Tyler (FRS)
> 
> useRs,
> 
> I'm using version 2.0.1 on Windows XP.  I am a bit of a 
> newbie and I am
> trying to learn the concept of computing on the language.  I 
> have an example
> that I think ought to work, but will not and I am not sure 
> what I am doing
> wrong.  
> 
> I would like to sort a data frame by a list of columns.  
> Eventually I would
> like to wrap this in a function so that I could sort data 
> frames by a list
> determined from context.  Any suggestions?  Thanks for your 
> patience as I
> strive to learn some of the finer points.
> 
> Here is a reproduction of the problem:
> 
> #*************************************************************
> **************
> *************************
> A <- data.frame(X = sample(c("A","B"),size=10,replace=T), Y =
> sample(c(T,F),size = 10, replace = T),
> 	Z = sample(1:10,size=10,replace=T));
> 
> arglist <- list();
> ind <- 3:1
> for(i in 1:3) {
> 	arglist[[i]] <- as.name(paste("A[,",ind[i],"]",sep=""));
> }
> 
> do.call(what="order",args=arglist)
> #*************************************************************
> **************
> *************************
> 
> I get the following error message:
> 
> > do.call(what="order",args=arglist)
> Error in order('A[,3]','A[,2]','A[,1]') : 
> 	  Object "A[,1]" not found
> >
> 
> Thanks, 
> 
> -Tyler
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From p.murrell at auckland.ac.nz  Tue Feb 22 19:42:51 2005
From: p.murrell at auckland.ac.nz (p.murrell@auckland.ac.nz)
Date: Wed, 23 Feb 2005 07:42:51 +1300 (NZDT)
Subject: [R] Hosting a R Graph Gallery?
In-Reply-To: <6.0.1.1.2.20050221091626.02361e40@stat4ux.stat.ucl.ac.be>
References: <4215EEBC.6070304@oomvanlieshout.net>
	<42163271.5040906@free.fr><m11xbawe25.fsf@iinet.net.au>
	<6.0.1.1.2.20050221091626.02361e40@stat4ux.stat.ucl.ac.be>
Message-ID: <1165.130.216.146.11.1109097771.squirrel@stat11.stat.auckland.ac.nz>

Hi


> Hi,
>
> About any graph gallery:
> Philippe Grojean and me did have made some work. Our goal was to add a
> clip
> library to the SciViews project that would offer access to a graph
> gallery.
> I was workiong on the production of the gallery, where as Philippe is
> still
> working on his GUI API. One of the goal is to have automatic wizards to
> make easier the creation of a graphic.
>
> Here was our approach and some thoughts:
>
> - We should propose a format for a description file. Here are some
> elements
> that should be gathered for each graphic function:
>          - Name of the function (*)
>          - Name of the produced graphic (*)
>          - Description of the graphic (*)
>          - Number of variables (univariate / bivariate / multivariate...)
>          - Types of variables
>          - Sample code (sample graph) (*)
>          - Package (*)
> The (*) are some information already available in Rd files (except maybe
> sample graph).
>
> - If someone deos something, I think it would be useful to ensure that all
> is reusable. We should focus on describing graphics. Then, for example,
> SciViews could use the information to create a usable graph gallery.


Along similar lines, it would be useful if gallery entries could be
submitted as a plain text or maybe an XML file.  Something like:

<plot title="blah">
  <desc>yadda yadda</desc>
  <code>plot(1)</code>
</plot>
<plot title="blah blah">
  <desc>yadda yadda yadda</desc>
  <code>plot(2)</code>
</plot>

This would allow people with existing sets of plots to generate an entire
set of gallery submissions automagically from a script.  It would also
make it feasible to automagically generate gallery entries from the
examples in packages such as graphics and lattice.

If the central gallery repository actually stored the gallery entries in
this sort of format (or possibly even in a database) then the gallery
itself could be automagically "published" in a variety of different
formats via scripts (e.g., web pages for display, web pages for editing
entries, an enormous PDF document, an R package, ...).

Paul


> If someone is interested, I ahve put in the following archive all my
> current code:
> http://www.stat.ucl.ac.be/ISpersonnel/lecoutre/R/svGraphGallery.zip
>
> There is no explanation but I would provide comments and help to any
> volonteer (basically, there is a file  .ggs with some descriptions as
> stated before and some R code to that produce HTML files).
>
> The result (the current gallery) is there. It is aimed to be something
> like
> 300 pixels large. At final step, graph would be clickable with a wizard.
>
> http://www.stat.ucl.ac.be/ISpersonnel/lecoutre/R/svGraphGallery/dock/svGallery.html
>
>
>
> Eric
>
> At 08:46 21/02/2005, Robert Cunningham wrote:
>>I too have often though a R-gallery would be useful.
>>
>>It seems to me that a Wiki-style page with a database backend would be
>>the best bet.
>>
>>It also seems to be that the best place to start is a complete image
>>gallery produced from all the examples in R base, then in packages in
>>CRAN. In this context the graphicsQC package
>>(http://www.stat.auckland.ac.nz/~paul/R/graphicsQC_0.4.tar.g) of Paul
>>Murrell seems useful.
>>
>>Cheers,
>>
>>
>>Robert Cunningham
>>
>>
>>
>>Romain Francois <francoisromain at free.fr> writes:
>>
>> > Hello Sander,
>> >
>> > That's a good idea and i am up to it.
>> >
>> > Right now i am in an exam period, so it's not really the better time,
>> > give me a couple of weeks and i will come up with a specific format of
>> > R files to submit to me that i could post-process to generate html
>> > documents.
>> > To my mind, those html files should show :
>> >
>> > - the plot itself
>> > + Submitter(s)
>> >         - web page
>> >         - email (eventually protected, I don't know how to do it)
>> > - Bibliographic references
>> > - Required R packages
>> > + Commentaries
>> >        - in english
>> >        - and in any other languages
>> >
>> > I'm open to any suggestion.
>> >
>> > Romain.
>> >
>> > Le 18.02.2005 14:33, Sander Oom a ?crit :
>> >
>> >> Dear R users,
>> >>
>> >> Following some of the recent questions and discussions about the R
>> >> plotting abilities, it occurred to me again that it would be very
>> >> valuable to have an R graph gallery.
>> >>
>> >> Eric Lecoutre made a very nice example in:
>> >> http://www.stat.ucl.ac.be/ISpersonnel/lecoutre/stats/fichiers/_gallery.pdf
>> >>
>> >>
>> >> It would be very useful to many beginners, but probably also
>> >> advanced users of R, to have an overview of R graph types with
>> >> graphical examples  and associated R code.
>> >>
>> >> In order to facilitate the evolution of a large gallery, some sort
>> >> of wiki environment might be most suitable, thus providing access to
>> >> all users, but with limited maintenance costs for the provider.
>> >>
>> >> Do others agree this could be a valuable resource? Would anybody
>> >> have the resources to host such an R graph gallery?
>> >>
>> >> Yours,
>> >>
>> >> Sander Oom.
>> >>
>> > --
>> > Romain FRANCOIS : francoisromain at free.fr
>> > page web : http://addictedtor.free.fr/ (en construction)
>> > 06 18 39 14 69 / 01 46 80 65 60
>> > _______________________________________________________
>> > Etudiant en 3eme ann?e
>> > Institut de Statistique de l'Universit? de Paris (ISUP)
>> > Fili?re Industrie et Services
>> > http://www.isup.cicrp.jussieu.fr/
>> >
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>
> Eric Lecoutre
> UCL /  Institut de Statistique
> Voie du Roman Pays, 20
> 1348 Louvain-la-Neuve
> Belgium
>
> tel: (+32)(0)10473050
> lecoutre at stat.ucl.ac.be
> http://www.stat.ucl.ac.be/ISpersonnel/lecoutre
>
> If the statistics are boring, then you've got the wrong numbers. -Edward
> Tufte
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From zhliur at yahoo.com  Tue Feb 22 19:48:10 2005
From: zhliur at yahoo.com (yyan liu)
Date: Tue, 22 Feb 2005 10:48:10 -0800 (PST)
Subject: [R] include C functions from nmath in my own C functions
Message-ID: <20050222184810.85073.qmail@web53103.mail.yahoo.com>

Hi:
  I am writing a C program which need a gamma random
number generator. I download the source file of R and
compile, make it myself. There is a "rgamma.c"
function in the installing directory of
R("/home/zhliu/Backup/R-2.0.1/src/nmath/rgamma.c"). My
question is how to call this function in my own
program which is in another directory. I can not copy
this "rgamma.c" to my working directory and use
#inclucde"rgamma.c" because in the file "rgamma.c", it
includes other header files. Or I can use makefile,
but I do not know how to edit my makefile to do this
job. 
  A related question is whether are similar .c files
contains the matrix functions(product, invert) in the
nmath library which I can use for my own C program?
  Thanks a lot!

liu



From phgrosjean at sciviews.org  Tue Feb 22 20:40:42 2005
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Tue, 22 Feb 2005 20:40:42 +0100
Subject: [R] Hosting a R Graph Gallery?
In-Reply-To: <1165.130.216.146.11.1109097771.squirrel@stat11.stat.auckland.ac.nz>
References: <4215EEBC.6070304@oomvanlieshout.net>	<42163271.5040906@free.fr><m11xbawe25.fsf@iinet.net.au>	<6.0.1.1.2.20050221091626.02361e40@stat4ux.stat.ucl.ac.be>
	<1165.130.216.146.11.1109097771.squirrel@stat11.stat.auckland.ac.nz>
Message-ID: <421B8ABA.8050804@sciviews.org>

Paul Murrell wrote:
 > Along similar lines, it would be useful if gallery entries could be
 > submitted as a plain text or maybe an XML file.  Something like:
 >
 > <plot title="blah">
 >   <desc>yadda yadda</desc>
 >   <code>plot(1)</code>
 > </plot>
 > <plot title="blah blah">
 >   <desc>yadda yadda yadda</desc>
 >   <code>plot(2)</code>
 > </plot>
 >
 > This would allow people with existing sets of plots to generate an entire
 > set of gallery submissions automagically from a script.  It would also
 > make it feasible to automagically generate gallery entries from the
 > examples in packages such as graphics and lattice.
 >
 > If the central gallery repository actually stored the gallery entries in
 > this sort of format (or possibly even in a database) then the gallery
 > itself could be automagically "published" in a variety of different
 > formats via scripts (e.g., web pages for display, web pages for editing
 > entries, an enormous PDF document, an R package, ...).
 >
 > Paul

Huuumm, Paul,... all this remember me something: it is the definition of 
R help .Rd files! After all these are plain ASCII files, you can put 
various sections in them, you have one section to hold executable R code 
(example) and you already have tools to compile them into web pages, 
enormous PDF files, R packages, etc...

The only thing that is missing is the possibility to include pictures in 
them... which seem to me quite important for a graph gallery! However, I 
guess this could be quite easily circumvented. The key thing would be to 
replace the textual index by a thumbnail of icons for the various 
graphs. Then, the way R packages are structured is just fine. You can 
even embed datasets you want to use as examples to illustrate your 
graphs, or custom functions for producing exotic graphs.

If someone volunteers to maintain such a 'GraphGallery' package, them 
people could simply send .Rd pages to him, and the graph gallery would 
grow little by little that way. What? Why do you all look at me? No, I 
am not volunteer for that... at least not for the moment! I was just 
dreaming that it would be a nice tool for someone's "R Graphics" book 
that will be published shortly... :-)
Best,

Philippe

..............................................<?}))><........
  ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
  ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
  ) ) ) ) )   Mons-Hainaut University, Pentagone (3D08)
( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
  ) ) ) ) )   8, av du Champ de Mars, 7000 Mons, Belgium
( ( ( ( (
  ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.30.54
( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
  ) ) ) ) )
( ( ( ( (    web:   http://www.umh.ac.be/~econum
  ) ) ) ) )          http://www.sciviews.org
( ( ( ( (
..............................................................

p.murrell at auckland.ac.nz wrote:
> Hi
> 
> 
> 
>>Hi,
>>
>>About any graph gallery:
>>Philippe Grojean and me did have made some work. Our goal was to add a
>>clip
>>library to the SciViews project that would offer access to a graph
>>gallery.
>>I was workiong on the production of the gallery, where as Philippe is
>>still
>>working on his GUI API. One of the goal is to have automatic wizards to
>>make easier the creation of a graphic.
>>
>>Here was our approach and some thoughts:
>>
>>- We should propose a format for a description file. Here are some
>>elements
>>that should be gathered for each graphic function:
>>         - Name of the function (*)
>>         - Name of the produced graphic (*)
>>         - Description of the graphic (*)
>>         - Number of variables (univariate / bivariate / multivariate...)
>>         - Types of variables
>>         - Sample code (sample graph) (*)
>>         - Package (*)
>>The (*) are some information already available in Rd files (except maybe
>>sample graph).
>>
>>- If someone deos something, I think it would be useful to ensure that all
>>is reusable. We should focus on describing graphics. Then, for example,
>>SciViews could use the information to create a usable graph gallery.
> 
> 
> 
> Along similar lines, it would be useful if gallery entries could be
> submitted as a plain text or maybe an XML file.  Something like:
> 
> <plot title="blah">
>   <desc>yadda yadda</desc>
>   <code>plot(1)</code>
> </plot>
> <plot title="blah blah">
>   <desc>yadda yadda yadda</desc>
>   <code>plot(2)</code>
> </plot>
> 
> This would allow people with existing sets of plots to generate an entire
> set of gallery submissions automagically from a script.  It would also
> make it feasible to automagically generate gallery entries from the
> examples in packages such as graphics and lattice.
> 
> If the central gallery repository actually stored the gallery entries in
> this sort of format (or possibly even in a database) then the gallery
> itself could be automagically "published" in a variety of different
> formats via scripts (e.g., web pages for display, web pages for editing
> entries, an enormous PDF document, an R package, ...).
> 
> Paul
> 
> 
> 
>>If someone is interested, I ahve put in the following archive all my
>>current code:
>>http://www.stat.ucl.ac.be/ISpersonnel/lecoutre/R/svGraphGallery.zip
>>
>>There is no explanation but I would provide comments and help to any
>>volonteer (basically, there is a file  .ggs with some descriptions as
>>stated before and some R code to that produce HTML files).
>>
>>The result (the current gallery) is there. It is aimed to be something
>>like
>>300 pixels large. At final step, graph would be clickable with a wizard.
>>
>>http://www.stat.ucl.ac.be/ISpersonnel/lecoutre/R/svGraphGallery/dock/svGallery.html
>>
>>
>>
>>Eric
>>
>>At 08:46 21/02/2005, Robert Cunningham wrote:
>>
>>>I too have often though a R-gallery would be useful.
>>>
>>>It seems to me that a Wiki-style page with a database backend would be
>>>the best bet.
>>>
>>>It also seems to be that the best place to start is a complete image
>>>gallery produced from all the examples in R base, then in packages in
>>>CRAN. In this context the graphicsQC package
>>>(http://www.stat.auckland.ac.nz/~paul/R/graphicsQC_0.4.tar.g) of Paul
>>>Murrell seems useful.
>>>
>>>Cheers,
>>>
>>>
>>>Robert Cunningham
>>>
>>>
>>>
>>>Romain Francois <francoisromain at free.fr> writes:
>>>
>>>
>>>>Hello Sander,
>>>>
>>>>That's a good idea and i am up to it.
>>>>
>>>>Right now i am in an exam period, so it's not really the better time,
>>>>give me a couple of weeks and i will come up with a specific format of
>>>>R files to submit to me that i could post-process to generate html
>>>>documents.
>>>>To my mind, those html files should show :
>>>>
>>>>- the plot itself
>>>>+ Submitter(s)
>>>>        - web page
>>>>        - email (eventually protected, I don't know how to do it)
>>>>- Bibliographic references
>>>>- Required R packages
>>>>+ Commentaries
>>>>       - in english
>>>>       - and in any other languages
>>>>
>>>>I'm open to any suggestion.
>>>>
>>>>Romain.
>>>>
>>>>Le 18.02.2005 14:33, Sander Oom a ?crit :
>>>>
>>>>
>>>>>Dear R users,
>>>>>
>>>>>Following some of the recent questions and discussions about the R
>>>>>plotting abilities, it occurred to me again that it would be very
>>>>>valuable to have an R graph gallery.
>>>>>
>>>>>Eric Lecoutre made a very nice example in:
>>>>>http://www.stat.ucl.ac.be/ISpersonnel/lecoutre/stats/fichiers/_gallery.pdf
>>>>>
>>>>>
>>>>>It would be very useful to many beginners, but probably also
>>>>>advanced users of R, to have an overview of R graph types with
>>>>>graphical examples  and associated R code.
>>>>>
>>>>>In order to facilitate the evolution of a large gallery, some sort
>>>>>of wiki environment might be most suitable, thus providing access to
>>>>>all users, but with limited maintenance costs for the provider.
>>>>>
>>>>>Do others agree this could be a valuable resource? Would anybody
>>>>>have the resources to host such an R graph gallery?
>>>>>
>>>>>Yours,
>>>>>
>>>>>Sander Oom.
>>>>>
>>>>
>>>>--
>>>>Romain FRANCOIS : francoisromain at free.fr
>>>>page web : http://addictedtor.free.fr/ (en construction)
>>>>06 18 39 14 69 / 01 46 80 65 60
>>>>_______________________________________________________
>>>>Etudiant en 3eme ann?e
>>>>Institut de Statistique de l'Universit? de Paris (ISUP)
>>>>Fili?re Industrie et Services
>>>>http://www.isup.cicrp.jussieu.fr/
>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide!
>>>
>>>http://www.R-project.org/posting-guide.html
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide!
>>>http://www.R-project.org/posting-guide.html
>>
>>Eric Lecoutre
>>UCL /  Institut de Statistique
>>Voie du Roman Pays, 20
>>1348 Louvain-la-Neuve
>>Belgium
>>
>>tel: (+32)(0)10473050
>>lecoutre at stat.ucl.ac.be
>>http://www.stat.ucl.ac.be/ISpersonnel/lecoutre
>>
>>If the statistics are boring, then you've got the wrong numbers. -Edward
>>Tufte
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>>http://www.R-project.org/posting-guide.html
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From Gregor.Gorjanc at bfro.uni-lj.si  Tue Feb 22 20:46:01 2005
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Tue, 22 Feb 2005 20:46:01 +0100
Subject: [R] Run Sweave and LaTeX directly from command line
Message-ID: <7FFEE688B57D7346BC6241C55900E730B6FF3E@pollux.bfro.uni-lj.si>

Gabor,

I definitely agree about use of R, however I am not so "much in" R 
as I am in Bash. It took me less than two hours to write this script.

At home I use R under windows and I solve problem of portability 
with Cygwin. I know that this is not optimal solution for everyone 
but ...

Can you provide me some insights/thoughts how this script might be 
written in R? I am open for discussion and cooperation on this script.

What do you think Friedrich? I must look at this texi2dvi. I didn't 
hav any experience with it jet. I will look at it.

--
Lep pozdrav / With regards,
    Gregor GORJANC

---------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty       URI: http://www.bfro.uni-lj.si
Zootechnical Department    email: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                  tel: +386 (0)1 72 17 861
SI-1230 Domzale            fax: +386 (0)1 72 17 888
Slovenia
---------------------------------------------------------------



-----Original Message-----
From: Gabor Grothendieck [mailto:ggrothendieck at myway.com]
Sent: tor 2005-02-22 18:15
To: Gorjanc Gregor; Friedrich.Leisch at ci.tuwien.ac.at; r-help at stat.math.ethz.ch
Subject: RE: [R] Run Sweave and LaTeX directly from command line
 



It might be possible to translate 90% of this into R leaving

just a short shell portion in which case it would be easy

to translate just that part into Windows batch making it

available on that OS too.  As someone already mentioned,

using texi2dvi could be helpful too.



Regards.





Date:   Tue, 22 Feb 2005 14:29:00 +0100 

From:   Gregor GORJANC <gregor.gorjanc at bfro.uni-lj.si>

To:   <Friedrich.Leisch at ci.tuwien.ac.at>, <r-help at stat.math.ethz.ch> 

Subject:   [R] Run Sweave and LaTeX directly from command line 



 

Hello!



Those of you, who use Sweave a lot, will probably find my shell script 

usable. You can get it at:



http://www.bfro.uni-lj.si/MR/ggorjan/programs/shell/Sweave.sh



No warranty, however don't hesitate to contact me if you find an error or 

have a patch!



-- 

Lep pozdrav / With regards,

Gregor GORJANC



---------------------------------------------------------------

University of Ljubljana

Biotechnical Faculty URI: http://www.bfro.uni-lj.si

Zootechnical Department mail: gregor.gorjanc <at> bfro.uni-lj.si

Groblje 3 tel: +386 (0)1 72 17 861

SI-1230 Domzale fax: +386 (0)1 72 17 888

Slovenia



From davidr at rhotrading.com  Tue Feb 22 20:47:53 2005
From: davidr at rhotrading.com (davidr@rhotrading.com)
Date: Tue, 22 Feb 2005 13:47:53 -0600
Subject: [R] Sorting a matrix on two columns
Message-ID: <12AE52872B5C5348BE5CF47C707FF53A5024EB@rhosvr02.rhotrading.com>

Perhaps I read more into the question than others, but
I thought he was asking about a stable sort. If that is
the case, ?order says that the sort is stable except for method =
"quick".

David L. Reiner

-----Original Message-----
From: Rau, Roland [mailto:Rau at demogr.mpg.de] 
Sent: Monday, February 21, 2005 4:07 AM
To: Jones, Glen R; r-help at stat.math.ethz.ch
Subject: RE: [R] Sorting a matrix on two columns

Hi Glen,

 
-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jones, Glen R

If a matrix with 5 columns has been defined and the first two columns
need to be sorted in ascending order, how can this be achieved whilst
ensuring the 
other 3 columns data are in relative position to the sorted columns?


does the following example-code help you?
mymatrix should resemble the structure of your (original) matrix.
mymatrix2 is the new, sorted matrix.

mymatrix <- matrix(runif(80), ncol=5)
mymatrix[,1] <- sample(c(1,2), size=length(mymatrix[,1]), replace=TRUE)
mymatrix

mymatrix2 <- mymatrix[order(mymatrix[,1],mymatrix[,2]),]
mymatrix2



Best,
Roland


+++++
This mail has been sent through the MPI for Demographic
Rese...{{dropped}}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Tue Feb 22 21:03:28 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 22 Feb 2005 20:03:28 +0000 (GMT)
Subject: [R] include C functions from nmath in my own C functions
In-Reply-To: <20050222184810.85073.qmail@web53103.mail.yahoo.com>
References: <20050222184810.85073.qmail@web53103.mail.yahoo.com>
Message-ID: <Pine.LNX.4.61.0502221959560.6867@gannet.stats>

On Tue, 22 Feb 2005, yyan liu wrote:

> Hi:
>  I am writing a C program which need a gamma random
> number generator. I download the source file of R and
> compile, make it myself. There is a "rgamma.c"
> function in the installing directory of
> R("/home/zhliu/Backup/R-2.0.1/src/nmath/rgamma.c"). My
> question is how to call this function in my own
> program which is in another directory. I can not copy
> this "rgamma.c" to my working directory and use
> #inclucde"rgamma.c" because in the file "rgamma.c", it
> includes other header files. Or I can use makefile,
> but I do not know how to edit my makefile to do this
> job.

See src/nmath/standalone/README.

>  A related question is whether are similar .c files
> contains the matrix functions(product, invert) in the
> nmath library which I can use for my own C program?

No.  In general R uses LAPACK (or perhaps LINPACK) for such operations, 
and so can you.

The API for nmath is described in the `Writing R Extensions' manual.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Avneet.Singh at graftech.com  Tue Feb 22 21:28:14 2005
From: Avneet.Singh at graftech.com (Singh, Avneet)
Date: Tue, 22 Feb 2005 15:28:14 -0500
Subject: [R] innovative fun ways for teaching quality control
Message-ID: <A8722C0C0FB4D3118A13009027C3C82E042A817F@U742EXC1>

Hello:

I was looking for innovative a fun ways of teaching quality control.
Could you suggest me a resource where i could find material that might help
me develop something like this.

Thank you
avneet

(something like the paper helicopter or catapult etc.)

"I have no data yet. It is a capital mistake to theorize before one has
data. Insensibly one begins to twist facts to suit theories instead of
theories to suit facts."
~ Sir Arthur Conan Doyle (1859-1930), Sherlock Holmes



From murdoch at stats.uwo.ca  Tue Feb 22 21:51:37 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 22 Feb 2005 20:51:37 +0000
Subject: [R] round() - strange results
In-Reply-To: <s21b0547.039@ohsu.edu>
References: <s21b0547.039@ohsu.edu>
Message-ID: <fe6n115pd873rc8ff0bb0v2u8lmu0p6o1u@4ax.com>

On Tue, 22 Feb 2005 10:10:58 -0800, "Dongseok Choi" <choid at ohsu.edu>
wrote :

>Hello,
> 
>  I found that round() does not behave as I expected.
>  Have you had similar experience as following?
> 
>> x<-seq(0.5,10.5,by=1)
>> x
> [1]  0.5  1.5  2.5  3.5  4.5  5.5  6.5  7.5  8.5  9.5 10.5
>> round(x)
> [1]  0  2  2  4  4  6  6  8  8 10 10
>> cbind(x,round(x))
>         x   
> [1,]  0.5  0
> [2,]  1.5  2
> [3,]  2.5  2
> [4,]  3.5  4
> [5,]  4.5  4
> [6,]  5.5  6
> [7,]  6.5  6
> [8,]  7.5  8
> [9,]  8.5  8
>[10,]  9.5 10
>[11,] 10.5 10
>
> 
>  Is this a well-known bug?

As others have said, it's not a bug at all.  If you would like to
round up instead, what you should do is use a function like this:

> roundup <- function(x) trunc(x+0.5)

> x <- -5:5 + 0.5
> x
 [1] -4.5 -3.5 -2.5 -1.5 -0.5  0.5  1.5  2.5  3.5  4.5  5.5
> round(x)
 [1] -4 -4 -2 -2  0  0  2  2  4  4  6
> roundup(x)
 [1] -4 -3 -2 -1  0  1  2  3  4  5  6


If this doesn't handle negatives the way you want, play around a bit
with abs().

Duncan Murdoch



From roger.bos at gmail.com  Tue Feb 22 22:54:06 2005
From: roger.bos at gmail.com (roger bos)
Date: Tue, 22 Feb 2005 16:54:06 -0500
Subject: [R] Rdbi and ODBC
Message-ID: <1db7268005022213541ac2f069@mail.gmail.com>

I have been using RODBC for a while with no complaints (R 2.0.1
patched under WinXP), then I saw a link
(http://grass.itc.it/statsgrass/r_and_dbms.html) showing that Rdbi
claims to run a query in 5 seconds that takes RODBC 4.3 minutes.  That
was hard enough for me to believe that I wanted to try some tests
myself, but I cannot get Rdbi to connect to my ODBC database.

Here is what works for RODBC for my setup:
conn <- odbcConnect("xf", "xfl2", "xfl2")

Now for Rdbi the documentation is lacking.  "?Rdbi" doesn't work and
"?dbConnect" gives little detail and no examples.

I tried all of the following without success:
conn <- dbConnect("ODBC()", dbname="xf", user="xfl2", password="xfl2")
conn <- dbConnect("ODBC", dbname="xf", user="xfl2", password="xfl2")
conn <- dbConnect(ODBC, dbname="xf", user="xfl2", password="xfl2")
conn <- dbConnect(ODBC(), dbname="xf", user="xfl2", password="xfl2")

Also, if this package is so fast, how come its not available on CRAN?

If I may ask a second question for my enlightenment.  It seems that
PostgreSQL and mySQL are pretty popular, but my database is in SQL
Server and I do not have the ability to change it.  How come every
package has native drivers to PostreSQL and Oracle but none of them
seem to have a native drive to SQL Server and I have to use ODBC?

Thanks in advance for any advice.



From Gregor.Gorjanc at bfro.uni-lj.si  Tue Feb 22 23:03:03 2005
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Tue, 22 Feb 2005 23:03:03 +0100
Subject: [R] Run Sweave and LaTeX directly from command line
Message-ID: <7FFEE688B57D7346BC6241C55900E730B6FF41@pollux.bfro.uni-lj.si>

Friedrich,

This texi2dvi is also very nice. Dou you know of any pros and cons
in comparison to rubber?

Is tex2dvi also shipped with R for windows? I am not able to find  
a binnary in my windows installation, however I found help page. 
Did I missed anything? I have 2.0.1

>>>>> On Tue, 22 Feb 2005 14:29:00 +0100,
>>>>> Gregor GORJANC (GG) wrote:

  > Hello!
  > Those of you, who use Sweave a lot, will probably find my shell script 
  > usable. You can get it at:

  > http://www.bfro.uni-lj.si/MR/ggorjan/programs/shell/Sweave.sh

  > No warranty, however don't hesitate to contact me if you find an error or 
  > have a patch!

Very nice!

Side note 1: R ships a version of texi2dvi, hence you might use that
one in case rubber is not found.

side note 2: For make afficionados the follwing 2 rules in combination
with the Sweave script from the FAQ do almost the same (that's what I
use :-)

%.tex: %.Rnw
        Sweave $<

%.pdf : %.tex
        texi2dvi --clean --pdf $<


Best,
Fritz Leisch

-- 
-------------------------------------------------------------------
                        Friedrich Leisch 
Institut f?r Statistik                     Tel: (+43 1) 58801 10715
Technische Universit?t Wien                Fax: (+43 1) 58801 10798
Wiedner Hauptstra?e 8-10/1071
A-1040 Wien, Austria             http://www.ci.tuwien.ac.at/~leisch



From ggrothendieck at myway.com  Tue Feb 22 23:30:18 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 22 Feb 2005 17:30:18 -0500 (EST)
Subject: [R] Run Sweave and LaTeX directly from command line
Message-ID: <20050222223018.8EA7F3967@mprdmxin.myway.com>


There are two main TeX distributions for Windows, MiKTeX and
fptex.   texi2dvi.exe ships with MiKTeX but not fptex.
I think fptex has a texi2dvi UNIX shell script that in principle
could be used with the Cygwin shell on Windows though I have
never tried it.  Because of that MiKTeX is really the only
one of the two distributions that fully supports R although
many people who use R on Windows apparently use fptex and it
does not give problems apparently provided you don't get into
currently lesser used features such as vignettes (althoutgh that
may change).

Date:   Tue, 22 Feb 2005 23:03:03 +0100 
From:   Gorjanc Gregor <Gregor.Gorjanc at bfro.uni-lj.si>
To:   <r-help at stat.math.ethz.ch> 
Cc:   <Friedrich.Leisch at tuwien.ac.at> 
Subject:   RE: [R] Run Sweave and LaTeX directly from command line 

 
Friedrich,

This texi2dvi is also very nice. Dou you know of any pros and cons
in comparison to rubber?

Is tex2dvi also shipped with R for windows? I am not able to find 
a binnary in my windows installation, however I found help page. 
Did I missed anything? I have 2.0.1

>>>>> On Tue, 22 Feb 2005 14:29:00 +0100,
>>>>> Gregor GORJANC (GG) wrote:

> Hello!
> Those of you, who use Sweave a lot, will probably find my shell script 
> usable. You can get it at:

> http://www.bfro.uni-lj.si/MR/ggorjan/programs/shell/Sweave.sh

> No warranty, however don't hesitate to contact me if you find an error or 
> have a patch!

Very nice!

Side note 1: R ships a version of texi2dvi, hence you might use that
one in case rubber is not found.

side note 2: For make afficionados the follwing 2 rules in combination
with the Sweave script from the FAQ do almost the same (that's what I
use :-)

%.tex: %.Rnw
Sweave $<

%.pdf : %.tex
texi2dvi --clean --pdf $<


Best,
Fritz Leisch

-- 
-------------------------------------------------------------------
Friedrich Leisch 
Institut fr Statistik Tel: (+43 1) 58801 10715
Technische Universitt Wien Fax: (+43 1) 58801 10798
Wiedner Hauptstrae 8-10/1071
A-1040 Wien, Austria http://www.ci.tuwien.ac.at/~leisch



From john.maindonald at anu.edu.au  Tue Feb 22 23:37:04 2005
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Wed, 23 Feb 2005 09:37:04 +1100
Subject: [R] Re: R-help Digest, Vol 24, Issue 22
In-Reply-To: <200502221106.j1MB5qjK030105@hypatia.math.ethz.ch>
References: <200502221106.j1MB5qjK030105@hypatia.math.ethz.ch>
Message-ID: <b98ebca92feea77f131d1a265077c35d@anu.edu.au>

You need to give the model formula that gave your output.
There are two sources of variation (at least), within and
between locations; though it looks as though your analysis
may have tried to account for this (but if so, the terms are
not laid out in a way that makes for ready interpretation.
The design is such (two locations) that you do not have
much of a check that effects are consistent over locations.

You need to check whether results really are similar
for all cultivars and for all herbicides, so that it is
legitimate to pool as happens in the overall analysis.
If a herbicide:cultivar combination has little effect the
variability may be large, while if it has a dramatic effect
(kills everything!), there may be no variability to speak of.
John Maindonald.

On 22 Feb 2005, at 10:06 PM, r-help-request at stat.math.ethz.ch wrote:

> To: "'Bob Wheeler'" <bwheeler at echip.com>
> Cc: r-help at stat.math.ethz.ch
> Subject: RE: [R] power.anova.test for interaction effects
> Reply-To: akniss at uwyo.edu
>
>
> It's a rather complex model.  A 37*4 factorial (37 cultivars[var]; 4
> herbicide treatments[trt]) with three replications[rep] was carried 
> out at
> two locations[loc], with  different randomizations within each rep at 
> each
> location.
>
> Source           DF   Error Term      MS
> Loc               1   Trt*rep(loc)    12314
> Rep(loc)          4   Trt*rep(loc)    1230.5
> Trt               3   Trt*rep(loc)    64.72
> Trt*loc           3   Trt*rep(loc)    33.42
> Trt*rep(loc)     12   Residual        76.78
> Var              36   Var*trt*loc     93.91
> Var*trt         108   Var*trt*loc     12.06
> Var*trt*loc     144   Residual        43.09
> Residual        575   NA              21.23
>
>
> -----Original Message-----
> From: Bob Wheeler [mailto:bwheeler at echip.com]
> Sent: Monday, February 21, 2005 4:33 PM
> To: akniss at uwyo.edu
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] power.anova.test for interaction effects
>
> Your F value is so low as to make me suspect your model. Where did the
> 144 denominator degrees of freedom come from?
>
John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Bioinformation Science, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.



From jijunl at amgen.com  Tue Feb 22 23:43:17 2005
From: jijunl at amgen.com (Liu, Jane)
Date: Tue, 22 Feb 2005 14:43:17 -0800
Subject: [R] ROracle installation
Message-ID: <569E4D734A4F05448D603396CB1C651101CE08BC@USSF-MB1.amgen.com>

Can someone tell me how to install ROracle package to PC? I couldn't find it
CRAN package list. And when I tried to install from zip file, it failed.

Thanks,
Jane



From lgilbert at berkeley.edu  Tue Feb 22 23:57:06 2005
From: lgilbert at berkeley.edu (Betty Gilbert)
Date: Tue, 22 Feb 2005 14:57:06 -0800
Subject: [R] Memory error in Mac OS X Aqua GUI v1.01 with cluster package
 functions
Message-ID: <p06110404be4165d8df65@[128.32.8.36]>

I'm sorry if the answer to my problem is buried in the archives. I 
have limited experience with R  and I couldn't find a solution to my 
particular problem. I am running  Mac OS X Aqua GUI v1.01 on a new G5 
running os 10.3.8 with a 1.8Ghz processor and 1GB of sdram. I just 
downloaded bioconducter a week ago and I'm trying to cluster a matrix 
I created with a simulation with dimensions
dim(nca35)
[1] 10481    12

with size
>  object.size(nca352)
[1] 1426204

I checked my ulimits variable on the unix terminal and it says it's 
unlimited as does
>  mem.limits()
nsize vsize
    NA    NA
But I'm still getting errors like the following with funtions in the 
cluster package
>  daisy(nca352, metric= "euclidean", stand=FALSE)->dnca35
Error: cannot allocate vector of size 858213 Kb
*** malloc: vm_allocate(size=878813184) failed (error code=3)
*** malloc[599]: error: Can't allocate region
if it helps i also checked
>  gc()
          used (Mb) gc trigger   (Mb)
Ncells 448662 12.0     741108   19.8
Vcells 847630  6.5  135357901 1032.7

I tried the suggested unix command in the memory help doc but that 
doesn't work in the Aqua GUI. Can someone tell me how to change the 
Vcells? Although to the best of my understanding (which is limited) I 
shouldn't have to do that. Any suggestions would be greatly 
appreciated.
thanks,
betty
-- 
Betty Gilbert
lgilbert at berkeley.edu
Taylor Lab
Plant and Microbial Biology
321 Koshland Hall
U.C. Berkeley
Berkeley, Ca 94720



From DAVID.BICKEL at PIONEER.COM  Wed Feb 23 00:01:04 2005
From: DAVID.BICKEL at PIONEER.COM (Bickel, David)
Date: Tue, 22 Feb 2005 17:01:04 -0600
Subject: [R] estimate the parameter of exponential distribution, etc.
Message-ID: <5F883C17941B9F4E80E5FA8C9F1C5E0E010FBDDA@jhms08.phibred.com>

Given a numeric vector of observations, does R have any generic way to estimate the parameters of commonly used distributions (exponential, gamma, etc.) without numerically optimizing the likelihood function?

Thanks,
David
_______________________________________
David R. Bickel  http://davidbickel.com
Research Scientist
Pioneer Hi-Bred International
Bioinformatics & Exploratory Research
7250 NW 62nd Ave., PO Box 552
Johnston, Iowa 50131-0552
515-334-4739 Tel
515-334-6634 Fax
david.bickel at pioneer.com, bickel at prueba.info



This communication is for use by the intended recipient and ...{{dropped}}



From nongluck.klibbua at studentmail.newcastle.edu.au  Tue Feb 22 23:48:15 2005
From: nongluck.klibbua at studentmail.newcastle.edu.au (NONGLUCK KLIBBUA)
Date: Wed, 23 Feb 2005 05:48:15 +0700
Subject: [R] Does R has the function for garch-t, gjr-garch,
	qgarch and egarch
Message-ID: <808f0865d5.865d5808f0@studentmail.newcastle.edu.au>

Dear all,
I would like to know that R has the function for garch-t,gjr-
garch,qgarch and egarch.
Best Regards,
Luck



From yhu_2003 at yahoo.com  Wed Feb 23 01:04:52 2005
From: yhu_2003 at yahoo.com (Eric Hu)
Date: Tue, 22 Feb 2005 16:04:52 -0800 (PST)
Subject: [R] nonlinear least square fit of an unknown function
Message-ID: <20050223000453.2052.qmail@web52406.mail.yahoo.com>

Hi, I have a set of twelve points and wonder how I can
get a function that can then be used to calculate the
area under the curve (most important). Thanks.

Eric



From rsubscriber at slcmsr.net  Wed Feb 23 01:33:14 2005
From: rsubscriber at slcmsr.net (Christian Lederer)
Date: Wed, 23 Feb 2005 01:33:14 +0100 (CET)
Subject: [R] Package pixmap breaks try() under circumstances
Message-ID: <33570.217.229.7.242.1109118794.squirrel@mail.mytrium.com>


Dear R users,

in some circumstances, try() shows a strange behaviour,
when the pixmap package is loaded.

The following piece of code works as expected, if it is
either sourced in an interactive session or invoked via
R CMD BATCH (the try-error is printed).

However, if i invoke R using ``R --vanilla < source.R'',
the execution halts (without printing the try-error).

# source.R
library(pixmap)
x <- numeric()
y <- numeric()
result <- try(plot(lm(x~y)))
print(result)

If i don't use library(pixmap), than also R --vanilla < source.R
works as expected.

This happens with R-2.0.1 and pixmap-0.4.2 under SuSE 9.2.

Christian :-(

P.S.
The reason for preferring R --vanilla script.R over R CMD BATCH is,
that i have to produce png images for a server and want to avoid
the bitmap device for performance reasons.
With R --vanilla < script.R, this is possible, using the xvnc
virtual X server.



From Innkeyp-r at yahoo.com  Wed Feb 23 01:43:10 2005
From: Innkeyp-r at yahoo.com (T Petersen)
Date: Wed, 23 Feb 2005 01:43:10 +0100
Subject: [R] Solving systems of non-linear equations in R
Message-ID: <421BD19E.1090309@yahoo.com>

I'm about to write my thesis in economics and will need to setup and 
solve a system of non-linear equations. At our university we usually use 
GAMS for this, and though GAMS is a fine program, it bugs me a that I 
won't be able to run my code after I finish my thesis without buying a 
license for the program(about $3.500 :-(( )

So I've looked around for NL-stuff for R, but I can't find anything. The 
closest thing appears to be optim(), but it doesn't seem to allow 
constraints(as in fn = constant) or equations systems. So, anyone knows 
if there is a method in R that you can use for this purpose?

regards



From talitaperciano at hotmail.com  Wed Feb 23 01:51:25 2005
From: talitaperciano at hotmail.com (Talita Leite)
Date: Wed, 23 Feb 2005 00:51:25 +0000
Subject: [R] Loading C functions into R
In-Reply-To: <D9A95B4B7B20354992E165EEADA31999056A92F5@uswpmx00.merck.com>
Message-ID: <BAY103-F410D9D121A9A2BED28FBCAC7630@phx.gbl>

Thanx very much! I got to load the program into R with no problem!



Talita Perciano Costa Leite
Graduanda em Ci?ncia da Computa??o
Universidade Federal de Alagoas - UFAL
Departamento de Tecnologia da Informa??o - TCI
Constru??o de Conhecimento por Agrupamento de Dados - CoCADa




>From: "Huntsinger, Reid" <reid_huntsinger at merck.com>
>To: "'Talita Leite'" <talitaperciano at hotmail.com>
>Subject: RE: [R] Loading C functions into R
>Date: Tue, 22 Feb 2005 16:43:21 -0500
>
>In case you didn't get an answer:
>
>.C() works by calling a function in a shared library you've already loaded
>into R. So you need to 1) compile the code into a shared library (DLL) and
>2) load it into R via dyn.load. The function needs to be declared as 
>"void";
>it passes results back via pointers passed in. (So you may need to write a
>simple wrapper to use your code.)
>
>.C() passes pointers to the data of the listed R objects to the function
>being called. (So you have to pass things like array dimensions as 
>arguments
>as well.) One of those (or more) could be for output, ie, the function
>writes data into the area pointed to. The return value of .C() is a list
>with (copies of) the R objects passed in, possibly modified.
>
>See the "Writing R Extensions" manual for details of .C() and examples.
>Also, "help(.C)" from an R session, and the R source code itself--for
>example have a look at the functions in packages included with R. The file
>"filter.c" in "ts" (I think) is a good example.
>
>Reid Huntsinger
>
>
>-----Original Message-----
>From: r-help-bounces at stat.math.ethz.ch
>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Talita Leite
>Sent: Tuesday, February 22, 2005 9:01 AM
>To: r-help at stat.math.ethz.ch
>Subject: [R] Loading C functions into R
>
>
>Hi everybody,
>
>I have the source of a C program that includes some archives .c and some
>libraries .h. I'm developing a program using R and I want it to load the C
>program I told before. How can I do that? I was looking for some function 
>in
>
>R to do that and I found the .C() but I can't understand how it works.
>Somebody could help me?
>
>Thanx,
>
>Talita Perciano Costa Leite
>Graduanda em Ci?ncia da Computa??o
>Universidade Federal de Alagoas - UFAL
>Departamento de Tecnologia da Informa??o - TCI
>Constru??o de Conhecimento por Agrupamento de Dados - CoCADa
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
>http://www.R-project.org/posting-guide.html
>
>
>
>------------------------------------------------------------------------------
>Notice:  This e-mail message, together with any attachments, contains 
>information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New 
>Jersey, USA 08889), and/or its affiliates (which may be known outside the 
>United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as 
>Banyu) that may be confidential, proprietary copyrighted and/or legally 
>privileged. It is intended solely for the use of the individual or entity 
>named on this message.  If you are not the intended recipient, and have 
>received this message in error, please notify us immediately by reply 
>e-mail and then delete it from your system.
>------------------------------------------------------------------------------



From saverio.vicario at yale.edu  Wed Feb 23 03:05:57 2005
From: saverio.vicario at yale.edu (saverio vicario)
Date: Tue, 22 Feb 2005 21:05:57 -0500
Subject: [R] how to calculate density of multinormal distribution at a given
 point?
Message-ID: <p06010214be41930de41c@[130.132.249.6]>

Dear all,
I am trying to estimate the point density of a posterior distribution 
(this to estimate a bayesian factor in aphylogenetic analysis ).  The 
problem that I would like to look at several variable at the same 
time. From a quick look several of the paramaters that interest me 
are correlated  and I need to estimate the density altogether.
  I would be very interest to know a kernel density (like the function 
"density") for multinomial distribution, but I would be equally happy 
if somebody could tell how to estimate point density with a 
multinormal distribution with known mean and covariance matrix
thanks
Saverio Vicario
Ecology and Evolutionary Biology
Yale University



From nhe101 at psu.edu  Wed Feb 23 03:32:11 2005
From: nhe101 at psu.edu (Nic Ellis)
Date: Tue, 22 Feb 2005 21:32:11 -0500
Subject: [R] data.frame error message
Message-ID: <5.2.0.9.2.20050222204546.00b09a60@email.psu.edu>

Dear R users,

I am using v2.0.1 on Windows 2000.  I have read a ".dat" file with several 
vectors, including 2 factors (2 levels x 3 levels), and a vector of 
responses, into R. There are no unique row names.  When I try plot(x,y) or 
lm(y~x) the following error is returned:

Error in model.frame(formula, rownames,...extranames: variable lengths differ

What am I doing wrong?  I am a new R user, and I didn't figure it out from 
the sections on data.frames in Dalgaard, Venable/Ripley, or simpleR, or the 
help threads.

Thanks-

Nic Ellis
Penn State



From edd at debian.org  Wed Feb 23 03:21:46 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 22 Feb 2005 20:21:46 -0600
Subject: [R] Re: Run Sweave and LaTeX directly from command line
In-Reply-To: <16923.14984.335575.186928@galadriel.ci.tuwien.ac.at>
References: <421B339C.1060204@bfro.uni-lj.si>
	<16923.14984.335575.186928@galadriel.ci.tuwien.ac.at>
Message-ID: <16923.59578.86884.820643@basebud.nulle.part>


On 22 February 2005 at 14:58, Friedrich.Leisch at tuwien.ac.at wrote:
| side note 2: For make afficionados the follwing 2 rules in combination
| with the Sweave script from the FAQ do almost the same (that's what I
| use :-)

Sure, but it requires a (arguably small) Makefile in every sweave project
directory.  Would you consider integrating Gregor's script (or a suitable
modification) instead?

Dirk

-- 
Better to have an approximate answer to the right question than a precise 
answer to the wrong question.  --  John Tukey as quoted by John Chambers



From andy_liaw at merck.com  Wed Feb 23 03:47:10 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 22 Feb 2005 21:47:10 -0500
Subject: [R] data.frame error message
Message-ID: <3A822319EB35174CA3714066D590DCD50994E753@usrymx25.merck.com>

Following the suggestions in the Posting Guide would help us to help you
much better.

What command(s) did you use to get the data into R?  What does the `.dat'
file look like?  What are `x' and `y'?

Data frames in R, by definition, has to have (unique) rownames.  If they are
not present in the data source (e.g., a file), they will be created for the
data frame, and defaults to 1 through n where n is the number of rows.

Andy

> From: Nic Ellis
> 
> Dear R users,
> 
> I am using v2.0.1 on Windows 2000.  I have read a ".dat" file 
> with several 
> vectors, including 2 factors (2 levels x 3 levels), and a vector of 
> responses, into R. There are no unique row names.  When I try 
> plot(x,y) or 
> lm(y~x) the following error is returned:
> 
> Error in model.frame(formula, rownames,...extranames: 
> variable lengths differ
> 
> What am I doing wrong?  I am a new R user, and I didn't 
> figure it out from 
> the sections on data.frames in Dalgaard, Venable/Ripley, or 
> simpleR, or the 
> help threads.
> 
> Thanks-
> 
> Nic Ellis
> Penn State
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From hamaoka at fbc.keio.ac.jp  Wed Feb 23 03:47:52 2005
From: hamaoka at fbc.keio.ac.jp (yutaka hamaoka)
Date: Wed, 23 Feb 2005 11:47:52 +0900
Subject: [R] Solving systems of non-linear equations in R
In-Reply-To: <421BD19E.1090309@yahoo.com>
References: <421BD19E.1090309@yahoo.com>
Message-ID: <421BEED8.8060509@fbc.keio.ac.jp>


I believe
library(systemfit)
has nlsytemfit function.

Yh


T Petersen wrote:
> I'm about to write my thesis in economics and will need to setup and 
> solve a system of non-linear equations. At our university we usually use 
> GAMS for this, and though GAMS is a fine program, it bugs me a that I 
> won't be able to run my code after I finish my thesis without buying a 
> license for the program(about $3.500 :-(( )
> 
> So I've looked around for NL-stuff for R, but I can't find anything. The 
> closest thing appears to be optim(), but it doesn't seem to allow 
> constraints(as in fn = constant) or equations systems. So, anyone knows 
> if there is a method in R that you can use for this purpose?
> 
> regards
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From vincent.goulet at act.ulaval.ca  Wed Feb 23 04:49:17 2005
From: vincent.goulet at act.ulaval.ca (Vincent Goulet)
Date: Tue, 22 Feb 2005 22:49:17 -0500
Subject: [R] estimate the parameter of exponential distribution, etc.
In-Reply-To: <5F883C17941B9F4E80E5FA8C9F1C5E0E010FBDDA@jhms08.phibred.com>
References: <5F883C17941B9F4E80E5FA8C9F1C5E0E010FBDDA@jhms08.phibred.com>
Message-ID: <200502222249.18167.vincent.goulet@act.ulaval.ca>

Le 22 F?vrier 2005 18:01, Bickel, David a ?crit :
> Given a numeric vector of observations, does R have any generic way to
> estimate the parameters of commonly used distributions (exponential, gamma,
> etc.) without numerically optimizing the likelihood function?

Assuming you want to use maximum likelihood estimation, look into 'fitdistr' 
in package MASS.

Hope this helps!

-- 
  Vincent Goulet, Professeur agr?g?
  ?cole d'actuariat
  Universit? Laval, Qu?bec 
  Vincent.Goulet at act.ulaval.ca   http://vgoulet.act.ulaval.ca



From kkonst at yahoo.com  Wed Feb 23 04:57:02 2005
From: kkonst at yahoo.com (Konstantinos Kleisouris)
Date: Tue, 22 Feb 2005 19:57:02 -0800 (PST)
Subject: [R] Memory problems
Message-ID: <20050223035702.53546.qmail@web60606.mail.yahoo.com>

Hi,

   I use R to do some ARIMA forecasting and R runs out
of memory. The problem is that I have 20160
samples(which are quite alot) and when I try to fit
the model it runs out of memory. I tried with
memory.size() to change the limit, but it wouldn't
work. Is there anything you can suggest? Is it
possible R can use virtual memory?

Thank you,
Kosta



From Innkeyp-r at yahoo.com  Wed Feb 23 05:08:05 2005
From: Innkeyp-r at yahoo.com (T Petersen)
Date: Wed, 23 Feb 2005 05:08:05 +0100
Subject: [R] Solving systems of non-linear equations in R
In-Reply-To: <421BEED8.8060509@fbc.keio.ac.jp>
References: <421BD19E.1090309@yahoo.com> <421BEED8.8060509@fbc.keio.ac.jp>
Message-ID: <421C01A5.4030306@yahoo.com>

No, this doesn't seem right. What I look for is something that could 
solve nonlinear systems with n unknowns and n equations. So there will 
be zero degrees of freedom, and statistical methods can't be the right 
way forward.

Specifically  I can see that the litterature mentions's "Scarf's 
algoritm" (Scarf 1967) and Merril's refinement of Scarf's algoritm in 
1972, but there might be other algoritms too...

Regards...TP

yutaka hamaoka wrote:

>
> I believe
> library(systemfit)
> has nlsytemfit function.
>
> Yh
>
>
> T Petersen wrote:
>
>> I'm about to write my thesis in economics and will need to setup and 
>> solve a system of non-linear equations. At our university we usually 
>> use GAMS for this, and though GAMS is a fine program, it bugs me a 
>> that I won't be able to run my code after I finish my thesis without 
>> buying a license for the program(about $3.500 :-(( )
>>
>> So I've looked around for NL-stuff for R, but I can't find anything. 
>> The closest thing appears to be optim(), but it doesn't seem to allow 
>> constraints(as in fn = constant) or equations systems. So, anyone 
>> knows if there is a method in R that you can use for this purpose?
>>
>> regards
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From lists at norvelle.org  Tue Feb 22 21:58:53 2005
From: lists at norvelle.org (List account)
Date: Tue, 22 Feb 2005 21:58:53 +0100
Subject: [R] Trying to get a side-by-side barplot of data with levels
	(Modified by List account)
Message-ID: <c9ab814b6c2890b16e81e4455703b368@norvelle.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050222/81dabe27/attachment.pl

From spencer.graves at pdf.com  Wed Feb 23 06:08:33 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 22 Feb 2005 21:08:33 -0800
Subject: [R] Solving systems of non-linear equations in R
In-Reply-To: <421C01A5.4030306@yahoo.com>
References: <421BD19E.1090309@yahoo.com> <421BEED8.8060509@fbc.keio.ac.jp>
	<421C01A5.4030306@yahoo.com>
Message-ID: <421C0FD1.8090008@pdf.com>

      A system of n equations in n unknowns has a unique solution if the 
n equations are linear and linearly independent.  If the system is 
nonlinear, then one must characterize the nonlinearity before saying 
anything about whether a solution exists and if so how many solutions 
are there? 

      Example 1:  Solve sin(x)=0 for x.  Answer:  x = 2*n*pi, for n = 
any integer. 

      Example 2:  Solve sin(x) = 2 for x.  Answer:  If x must be a real 
number, then this equation has no solutions. 

      Are your functions monotonic?  Continuous?  Differentiable? 

      Without getting into pathologies like the Cantor function (e.g., 
http://www.cut-the-knot.org/do_you_know/cantor.shtml), my experience 
with a variety of practical problem like this suggests that it is best 
to recast the problem as one of minimizing, e.g., the sum of squared 
deviations from target.  Moreover, I've had good luck transforming the 
parameter space to eliminate constraints -- or incorporating the 
constraints into the objective function and then solving the 
superficially unconstrained problem.  If my functions have singularities 
where I might get 0/0 or Inf-Inf, for example, I use asymptotic 
expansions to "approximate" the function(s) near the singularities more 
accurately than can be achieved with any finite-precision arithmetic. 

      With all of this, I'm confident that there are better algorithms 
than the different methods in "optim", but I don't have not had the need 
to hunt for them.  The methods in "optim" provide a reasonable range of 
options for the problems I've encountered. 

      The R project has another advantage over a commercial software:  
You can see the source code.  You can trace it step by step and find out 
where it does not work well for the specific problems you consider.  If 
you're clever, you might be able to find a way to improve that algorithm 
and make it part of your thesis -- and get a publication on it in some 
statistical software journal.  Where else can you so easily climb up and 
stand on the shoulders of giants?  If you find a platform for innovation 
better than R, please let me know. 

      hope this helps. 
      spencer graves    

T Petersen wrote:

> No, this doesn't seem right. What I look for is something that could 
> solve nonlinear systems with n unknowns and n equations. So there will 
> be zero degrees of freedom, and statistical methods can't be the right 
> way forward.
>
> Specifically  I can see that the litterature mentions's "Scarf's 
> algoritm" (Scarf 1967) and Merril's refinement of Scarf's algoritm in 
> 1972, but there might be other algoritms too...
>
> Regards...TP
>
> yutaka hamaoka wrote:
>
>>
>> I believe
>> library(systemfit)
>> has nlsytemfit function.
>>
>> Yh
>>
>>
>> T Petersen wrote:
>>
>>> I'm about to write my thesis in economics and will need to setup and 
>>> solve a system of non-linear equations. At our university we usually 
>>> use GAMS for this, and though GAMS is a fine program, it bugs me a 
>>> that I won't be able to run my code after I finish my thesis without 
>>> buying a license for the program(about $3.500 :-(( )
>>>
>>> So I've looked around for NL-stuff for R, but I can't find anything. 
>>> The closest thing appears to be optim(), but it doesn't seem to 
>>> allow constraints(as in fn = constant) or equations systems. So, 
>>> anyone knows if there is a method in R that you can use for this 
>>> purpose?
>>>
>>> regards
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Wed Feb 23 06:16:28 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 23 Feb 2005 00:16:28 -0500 (EST)
Subject: [R] Run Sweave and LaTeX directly from command line
Message-ID: <20050223051628.5A9CB39D7@mprdmxin.myway.com>



This does not really answer your question and frankly
I have not really spent the time to figure out what your
script does but assuming ...\R\rw????\bin is in your PATH
then the following one line batch file will run Sweave
from the command line in the Windows console (assuming
its placed in a batch file called Sweave.bat):

rem Usage: Sweave filename
echo Sweave("%1") | R --no-save --no-restore

After running this the user would run 
texi2dvi --pdf myfile.tex


Date:   Tue, 22 Feb 2005 20:46:01 +0100 
From:   Gorjanc Gregor <Gregor.Gorjanc at bfro.uni-lj.si>
To:   <ggrothendieck at myway.com>, <r-help at stat.math.ethz.ch>, <Friedrich.Leisch at ci.tuwien.ac.at> 
Subject:   RE: [R] Run Sweave and LaTeX directly from command line 
 
Gabor,

I definitely agree about use of R, however I am not so "much in" R 
as I am in Bash. It took me less than two hours to write this script.

At home I use R under windows and I solve problem of portability 
with Cygwin. I know that this is not optimal solution for everyone 
but ...

Can you provide me some insights/thoughts how this script might be 
written in R? I am open for discussion and cooperation on this script.

What do you think Friedrich? I must look at this texi2dvi. I didn't 
hav any experience with it jet. I will look at it.

--
Lep pozdrav / With regards,
Gregor GORJANC



From josh8912 at yahoo.com  Wed Feb 23 07:05:21 2005
From: josh8912 at yahoo.com (JJ)
Date: Tue, 22 Feb 2005 22:05:21 -0800 (PST)
Subject: [R] How to conctruct an inner grouping for nlme random statement?
Message-ID: <20050223060521.21821.qmail@web51710.mail.yahoo.com>

Hello.  Im hoping someone can help with a grouping
question related to the "random=" statement within the
nlme function.  How do you specify that some grouping
levels are inner to others?  I tried several things,
given below.

Lets say I have a data frame with five variables,
resp, cov1, ran1, ran2, group1, and group 2.  The
formula is resp~cov1 + ran1 + ran2, where the ran are
random variables.  The data is of length 80, and there
are 4 unique factors in group1 and 20 unique factors
in group2.  These are factors related to ran1 and
ran2, respectively.  

The difficult part is that I want to estimate only 4
random variables for ran1|group1 and the full 20 for
ran2|group2.  I have tried many ways, and I cannot
find a way to do this.  Is there a way?  Can someone
suggest a code snippet?  

First I tried making the data frame a groupedData
object, so that group2 is inner to group1, as it
should be.  Then I used the statement: random =
as.formula(ran1+ran2~1).  But this produced 20
estimates for both ran1 and ran2.

I have also tried it without the data frame as a
groupedData object, using the following:
random = list(group1= c(ran1~1, group2=ran2~1)).  But
this gave only 4 estimates for ran2.  I also tried: 
random = list(c(group1= ran1~1, group2=ran2~1)), but
this just gave a parse error message.

Any suggestions would be greatly appreciated.  Is it
even possible to do what I want to do?  John



From tom_hoary at web.de  Wed Feb 23 08:25:44 2005
From: tom_hoary at web.de (Thomas =?iso-8859-1?q?Sch=F6nhoff?=)
Date: Wed, 23 Feb 2005 08:25:44 +0100
Subject: [R] Memory problems
In-Reply-To: <20050223035702.53546.qmail@web60606.mail.yahoo.com>
References: <20050223035702.53546.qmail@web60606.mail.yahoo.com>
Message-ID: <200502230825.44842.tom_hoary@web.de>



Looking at the posting guide will increase the chance to get a helpful 
response from this list.

No one knows what kind of operating system are you running: Is it 
Windows,MacOS or Linux (32 or 64 bit)?
Memory related problems are reported daily so it could be much 
beneficial to browse the help-archive ( has an efficient search 
facility ) Some reccent suggestions on memory limitations are found 
there....

Thomas




> I use R to do some ARIMA forecasting and R runs out
> of memory. The problem is that I have 20160
> samples(which are quite alot) and when I try to fit
> the model it runs out of memory. I tried with
> memory.size() to change the limit, but it wouldn't
> work. Is there anything you can suggest? Is it
> possible R can use virtual memory?



From ligges at statistik.uni-dortmund.de  Wed Feb 23 08:36:31 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 23 Feb 2005 08:36:31 +0100
Subject: [R] ROracle installation
In-Reply-To: <569E4D734A4F05448D603396CB1C651101CE08BC@USSF-MB1.amgen.com>
References: <569E4D734A4F05448D603396CB1C651101CE08BC@USSF-MB1.amgen.com>
Message-ID: <421C327F.4080504@statistik.uni-dortmund.de>

Liu, Jane wrote:
> Can someone tell me how to install ROracle package to PC? I couldn't find it
> CRAN package list. And when I tried to install from zip file, it failed.
> 
> Thanks,
> Jane
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


Please read the ReadMe at the corresponding CRAN repository
CRAN/bin/windows/contrib/2.0/ReadMe
that tells you:

"Although the packages
   RMySQL, ROracle, and snow
pass make check, it seems to be dangerous to distribute them:
I do not have the software available these packages depend on."

Uwe Ligges



From ligges at statistik.uni-dortmund.de  Wed Feb 23 08:39:22 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 23 Feb 2005 08:39:22 +0100
Subject: [R] Rdbi and ODBC
In-Reply-To: <1db7268005022213541ac2f069@mail.gmail.com>
References: <1db7268005022213541ac2f069@mail.gmail.com>
Message-ID: <421C332A.6010504@statistik.uni-dortmund.de>

roger bos wrote:

> I have been using RODBC for a while with no complaints (R 2.0.1
> patched under WinXP), then I saw a link
> (http://grass.itc.it/statsgrass/r_and_dbms.html) showing that Rdbi
> claims to run a query in 5 seconds that takes RODBC 4.3 minutes.  That
> was hard enough for me to believe that I wanted to try some tests
> myself, but I cannot get Rdbi to connect to my ODBC database.
> 
> Here is what works for RODBC for my setup:
> conn <- odbcConnect("xf", "xfl2", "xfl2")
> 
> Now for Rdbi the documentation is lacking.  "?Rdbi" doesn't work and
> "?dbConnect" gives little detail and no examples.
> 
> I tried all of the following without success:
> conn <- dbConnect("ODBC()", dbname="xf", user="xfl2", password="xfl2")
> conn <- dbConnect("ODBC", dbname="xf", user="xfl2", password="xfl2")
> conn <- dbConnect(ODBC, dbname="xf", user="xfl2", password="xfl2")
> conn <- dbConnect(ODBC(), dbname="xf", user="xfl2", password="xfl2")
> 
> Also, if this package is so fast, how come its not available on CRAN?


Rdbi is a Bioconductor packge.

Uwe Ligges


> If I may ask a second question for my enlightenment.  It seems that
> PostgreSQL and mySQL are pretty popular, but my database is in SQL
> Server and I do not have the ability to change it.  How come every
> package has native drivers to PostreSQL and Oracle but none of them
> seem to have a native drive to SQL Server and I have to use ODBC?
> 
> Thanks in advance for any advice.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From vito_ricci at yahoo.com  Wed Feb 23 09:03:37 2005
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Wed, 23 Feb 2005 09:03:37 +0100 (CET)
Subject: [R] Re: estimate the parameter of exponential distribution, etc.
Message-ID: <20050223080337.55297.qmail@web41210.mail.yahoo.com>

Hi David,

You can estimate parameters using fitdistr() in MASS
package. 
I suggest also to read my contribute "Fitting
distributions with R" available on CRAN:
http://cran.r-project.org/doc/contrib/Ricci-distributions-en.pdf
Best regards,
Vito



you wrote:

Given a numeric vector of observations, does R have
any generic way to estimate the parameters of commonly
used distributions (exponential, gamma, etc.) without
numerically optimizing the likelihood function?

Thanks,
David
_______________________________________
David R. Bickel  http://davidbickel.com
Research Scientist
Pioneer Hi-Bred International
Bioinformatics & Exploratory Research
7250 NW 62nd Ave., PO Box 552
Johnston, Iowa 50131-0552
515-334-4739 Tel
515-334-6634 Fax
david.bickel at pioneer.com, bickel at prueba.info

=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box

Top 10 reasons to become a Statistician

     1. Deviation is considered normal
     2. We feel complete and sufficient
     3. We are 'mean' lovers
     4. Statisticians do it discretely and continuously
     5. We are right 95% of the time
     6. We can legally comment on someone's posterior distribution
     7. We may not be normal, but we are transformable
     8. We never have to say we are certain
     9. We are honestly significantly different
    10. No one wants our jobs


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/



From vito_ricci at yahoo.com  Wed Feb 23 09:12:26 2005
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Wed, 23 Feb 2005 09:12:26 +0100 (CET)
Subject: [R] Re: nonlinear least square fit of an unknown function
Message-ID: <20050223081227.22513.qmail@web41212.mail.yahoo.com>

Hi Eric,
if I understand you question,  are you trying to fit
an unknown distribution? Are looking for computing
theorical frequencies (area under the curve)?
In this case you could see:
http://cran.r-project.org/doc/contrib/Ricci-distributions-en.pdf
Use hist() and density() to identify your distribution
and then fit parameters.
Another way could be using splines (?spline).
Hoping I helped you.
Cordially
Vito


you wrote:
Hi, I have a set of twelve points and wonder how I can
get a function that can then be used to calculate the
area under the curve (most important). Thanks.

Eric

=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box

Top 10 reasons to become a Statistician

     1. Deviation is considered normal
     2. We feel complete and sufficient
     3. We are 'mean' lovers
     4. Statisticians do it discretely and continuously
     5. We are right 95% of the time
     6. We can legally comment on someone's posterior distribution
     7. We may not be normal, but we are transformable
     8. We never have to say we are certain
     9. We are honestly significantly different
    10. No one wants our jobs


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/



From ripley at stats.ox.ac.uk  Wed Feb 23 09:18:20 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 23 Feb 2005 08:18:20 +0000 (GMT)
Subject: [R] Memory error in Mac OS X Aqua GUI v1.01 with cluster package
	functions
In-Reply-To: <p06110404be4165d8df65@[128.32.8.36]>
References: <p06110404be4165d8df65@[128.32.8.36]>
Message-ID: <Pine.LNX.4.61.0502230805050.6989@gannet.stats>

The problem is probably that your 32-bit application has become fragmented 
in its address space.  (I have been told that 64-bit builds of R on MacOS 
X will need 10.4, and the binary on CRAN is definitely 32-bit.)

The distance matrix for your object alone takes 800Mb, so you will 
struggle on any 32-bit OS, let alone on a machine with `only' 1Gb RAM.
However, using R's dist() rather than cluster's daisy() will need less 
memory for this task.

This is R-help not the Bioconductor list, and outside bioinformatics the 
prevailing opinion is that clustering 10,000 cases is not sensible.  But 
there are methods of clustering that do not use the distance matrix, e.g. 
clara() in the (unmentioned) package cluster than provides daisy().


On Tue, 22 Feb 2005, Betty Gilbert wrote:

> I'm sorry if the answer to my problem is buried in the archives. I have 
> limited experience with R  and I couldn't find a solution to my particular 
> problem. I am running  Mac OS X Aqua GUI v1.01 on a new G5 running os 10.3.8 
> with a 1.8Ghz processor and 1GB of sdram. I just downloaded bioconducter a 
> week ago and I'm trying to cluster a matrix I created with a simulation with 
> dimensions
> dim(nca35)
> [1] 10481    12
>
> with size
>>  object.size(nca352)
> [1] 1426204
>
> I checked my ulimits variable on the unix terminal and it says it's unlimited 
> as does

But your machine and OS impose limits that ulimits is not telling you.

>>  mem.limits()
> nsize vsize
>   NA    NA
> But I'm still getting errors like the following with funtions in the cluster 
> package
>>  daisy(nca352, metric= "euclidean", stand=FALSE)->dnca35
> Error: cannot allocate vector of size 858213 Kb
> *** malloc: vm_allocate(size=878813184) failed (error code=3)
> *** malloc[599]: error: Can't allocate region
> if it helps i also checked
>>  gc()
>         used (Mb) gc trigger   (Mb)
> Ncells 448662 12.0     741108   19.8
> Vcells 847630  6.5  135357901 1032.7
>
> I tried the suggested unix command in the memory help doc but that doesn't 
> work in the Aqua GUI. Can someone tell me how to change the Vcells? Although 
> to the best of my understanding (which is limited) I shouldn't have to do 
> that. Any suggestions would be greatly appreciated.
> thanks,
> betty
> -- 
> Betty Gilbert
> lgilbert at berkeley.edu
> Taylor Lab
> Plant and Microbial Biology
> 321 Koshland Hall
> U.C. Berkeley
> Berkeley, Ca 94720

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Feb 23 09:24:57 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 23 Feb 2005 08:24:57 +0000 (GMT)
Subject: [R] Memory problems
In-Reply-To: <20050223035702.53546.qmail@web60606.mail.yahoo.com>
References: <20050223035702.53546.qmail@web60606.mail.yahoo.com>
Message-ID: <Pine.LNX.4.61.0502230821480.6989@gannet.stats>

R does use virtual memory, and memory.size() (Windows only) is documented 
to report the usage, not change the limit.  Please read that help page 
more carefully.

If you are not already doing so, try arima0 rather than arima.

And do see the posting guide!

On Tue, 22 Feb 2005, Konstantinos Kleisouris wrote:

>   I use R to do some ARIMA forecasting and R runs out
> of memory. The problem is that I have 20160
> samples(which are quite alot) and when I try to fit
> the model it runs out of memory. I tried with
> memory.size() to change the limit, but it wouldn't
> work. Is there anything you can suggest? Is it
> possible R can use virtual memory?

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From vito_ricci at yahoo.com  Wed Feb 23 09:36:02 2005
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Wed, 23 Feb 2005 09:36:02 +0100 (CET)
Subject: [R] Re: Does R has the function for garch-t, gjr-garch,
	qgarch and egarch
Message-ID: <20050223083603.45449.qmail@web41203.mail.yahoo.com>

Hi,

see: 

- HestonNandiGarchFit(fOptions)                       
Heston-Nandi Garch(1,1) Modelling
- HestonNandiOptions(fOptions)
Option Price for the Heston-Nandi Garch Option        
               Model
- GarchModelling(fSeries)
Univariate GARCH Time Series Modelling
- GarchDistributionFits(fSeries)
Parameter Fit of a Distribution
- garch(tseries)          
Fit GARCH Models to Time Series

Best regards
Vito




You wrote:

Dear all,
I would like to know that R has the function for
garch-t,gjr-
garch,qgarch and egarch.
Best Regards,
Luck

=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box

Top 10 reasons to become a Statistician

     1. Deviation is considered normal
     2. We feel complete and sufficient
     3. We are 'mean' lovers
     4. Statisticians do it discretely and continuously
     5. We are right 95% of the time
     6. We can legally comment on someone's posterior distribution
     7. We may not be normal, but we are transformable
     8. We never have to say we are certain
     9. We are honestly significantly different
    10. No one wants our jobs


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/



From Achim.Zeileis at wu-wien.ac.at  Wed Feb 23 09:37:31 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed, 23 Feb 2005 09:37:31 +0100
Subject: [R] Re: Run Sweave and LaTeX directly from command line
In-Reply-To: <16923.59578.86884.820643@basebud.nulle.part>
References: <421B339C.1060204@bfro.uni-lj.si>
	<16923.14984.335575.186928@galadriel.ci.tuwien.ac.at>
	<16923.59578.86884.820643@basebud.nulle.part>
Message-ID: <20050223093731.251b5d75.Achim.Zeileis@wu-wien.ac.at>

On Tue, 22 Feb 2005 20:21:46 -0600 Dirk Eddelbuettel wrote:

> 
> On 22 February 2005 at 14:58, Friedrich.Leisch at tuwien.ac.at wrote:
> | side note 2: For make afficionados the follwing 2 rules in
> | combination with the Sweave script from the FAQ do almost the same
> | (that's what I use :-)
> 
> Sure, but it requires a (arguably small) Makefile in every sweave
> project directory.  Would you consider integrating Gregor's script (or
> a suitable modification) instead?

As discussed in various bits and pieces in this thread: the main
building blocks are all there in R so that you could do something like

Rnw2dvi <- function(x, ...) {
  Sweave(paste(x, ".Rnw", sep = ""))
  texi2dvi(paste(x, ".tex", sep = ""), ...)
}

But as texi2dvi() requires some attention under Windows, people might
want to use something like Gabor suggested. Others will prefer a
Makefile like Fritz, others a shell script like Gregor or...
Personally, I've got a Rnw2pdf script with
---
Sweave $1
texi2dvi --pdf --clean $(basename $1 .Rnw).tex
rm -f $(basename $1 .Rnw)-*.pdf
rm -f Rplots.ps
---
in my bin/ directory.

So I think it is probably easiest if every user sets up something he/she
is comfortable with on his/her platform.
Z

> Dirk
> 
> -- 
> Better to have an approximate answer to the right question than a
> precise answer to the wrong question.  --  John Tukey as quoted by
> John Chambers
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Wed Feb 23 09:55:52 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 23 Feb 2005 08:55:52 +0000 (GMT)
Subject: [R] Easy cut & paste from Excel to R?
In-Reply-To: <x2u0o9bwqx.fsf@biostat.ku.dk>
References: <1108678164.4215161465ccc@webmail.lyon.inserm.fr>
	<p06210200be3bbc39a47e@[128.115.153.6]> <x2u0o9bwqx.fsf@biostat.ku.dk>
Message-ID: <Pine.LNX.4.61.0502230845230.6989@gannet.stats>

On Fri, 18 Feb 2005, Peter Dalgaard wrote:

> Don MacQueen <macq at llnl.gov> writes:
>
>> I tried Ken's suggestion
>>     read.table(pipe("pbpaste"),header=TRUE)
>> on my Mac OS X system and it worked *without* generating any warning message.
>>
>> If my experience represents the norm, and Ken's is the exception, it
>> is so simple that no further contribution to R is needed, I would say.
>> Thank you, Ken.
>
> My conjecture is that it only happens when there are fewer than 5 data
> lines.
>
> We still need to sort out X11. Too bad that the xclip program isn't
> ubiquitous.

The read side in X11 is not too hard, and R-devel now has read from the 
primary selection via file("clipboard").  (I may change that name and 
allow reading from other selections later: I just made small changes to 
the Windows code.)

Xlib doesn't it seems really have a clipboard, and so it is much harder to 
act as the provider of the primary selection (you need to respond to X11 
events) -- xclip forks to do so.

BTW, xclip is at http://people.debian.org/~kims/xclip/ and seems no longer 
under active development (last change 18 months ago).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bhx2 at mevik.net  Wed Feb 23 10:17:42 2005
From: bhx2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Wed, 23 Feb 2005 10:17:42 +0100
Subject: [R] using 'nice' with R
In-Reply-To: <421B55B4.6020600@jhsph.edu> (Roger D. Peng's message of "Tue,
	22 Feb 2005 10:54:28 -0500")
References: <Pine.LNX.4.62.0502221021420.1696@neblt2>
	<421B55B4.6020600@jhsph.edu>
Message-ID: <m0y8dfr5xl.fsf@bar.nemo-project.org>

Roger D. Peng writes:

> On a Unix like system you can do `nice +19 R' or perhaps `nice +19 R
> CMD BATCH commands.R'.

At least on Suse (9.1) and Debian (3.0) Linux, the syntax is
`nice -19 R' (i.e. with `-', not `+'.)

-- 
Bj?rn-Helge Mevik



From robut at iinet.net.au  Wed Feb 23 10:18:59 2005
From: robut at iinet.net.au (Robert Cunningham)
Date: Wed, 23 Feb 2005 17:18:59 +0800
Subject: [R] Hosting a R Graph Gallery?
In-Reply-To: <421B8ABA.8050804@sciviews.org> (Philippe Grosjean's message of
	"Tue, 22 Feb 2005 20:40:42 +0100")
References: <4215EEBC.6070304@oomvanlieshout.net> <42163271.5040906@free.fr>
	<m11xbawe25.fsf@iinet.net.au>
	<6.0.1.1.2.20050221091626.02361e40@stat4ux.stat.ucl.ac.be>
	<1165.130.216.146.11.1109097771.squirrel@stat11.stat.auckland.ac.nz>
	<421B8ABA.8050804@sciviews.org>
Message-ID: <m1psyrab24.fsf@iinet.net.au>


Earlier I suggested as a *start* for the R-gallery to process the
examples in R base and in the packages on CRAN. However, this was only
as a start It seems to me that there are three types of images that
would be useful in such a gallery;

1: The processed examples of base and CRAN
2: Figures based upon code AND data sent in by users
3: Figures with code but WITHOUT data sent in by users (we could
have figures without code or data [pure decorations] I would not support that myself)

I think a package to automate production of gallery items would
certainly be useful. I would suggest the basic function in the package
would be one that produces the a Gallery Item with whatever details
and in whatever format is suitable. Other functions may allow
automatic submission of the Gallery Item and another would allow bulk
processing of packages etc by looping over the function etc (Paul
Murrell's graphicsQC package already processes all the example images
in a package).

I agree with Paul Murrell that storage in a database would be the best
option though probably more difficult to set up at the start.

I agree with Sander Om that encouraging SVG would be nice.

I looked at the Japanese R-Wiki mentioned by Shigeru Mase and the
RGallery Eric Lecoutre mentioned and certainly these are the *sort* of
things we are interested in though perhaps with different formats and
language ;-) I also agree with Gabor Grothendieck that it would be
useful to make use of the existing R Wiki, though this depends on how
much Gallery Item processing we would expect the host to do and whether
this could be arranged on the Wiki.

I guess how much the server would be expected to do is a critical
question, from there we could proceed to the details of a Gallery Item
format and then functions and a package.


Cheers, 


Robert Cunningham



From Roger.Bivand at nhh.no  Wed Feb 23 10:43:09 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 23 Feb 2005 10:43:09 +0100 (CET)
Subject: [R] Package pixmap breaks try() under circumstances
In-Reply-To: <33570.217.229.7.242.1109118794.squirrel@mail.mytrium.com>
Message-ID: <Pine.LNX.4.44.0502231028550.4767-100000@reclus.nhh.no>

On Wed, 23 Feb 2005, Christian Lederer wrote:

> 
> Dear R users,
> 
> in some circumstances, try() shows a strange behaviour,
> when the pixmap package is loaded.
> 
> The following piece of code works as expected, if it is
> either sourced in an interactive session or invoked via
> R CMD BATCH (the try-error is printed).
> 
> However, if i invoke R using ``R --vanilla < source.R'',
> the execution halts (without printing the try-error).
> 
> # source.R
> library(pixmap)
> x <- numeric()
> y <- numeric()
> result <- try(plot(lm(x~y)))
> print(result)
> 
> If i don't use library(pixmap), than also R --vanilla < source.R
> works as expected.
> 
> This happens with R-2.0.1 and pixmap-0.4.2 under SuSE 9.2.

But this does work as expected in your setting:

library(pixmap)
x <- numeric()
y <- numeric()
result <- try(lm(x~y))
#result <- try(plot(lm(x~y)))
print(result)
res1 <- try(plot(result))
print(res1)

The failed lm() in your version seems to be muddling plot(), and the try() 
is on the plot(), not the lm(). Without pixmap present:

> getAnywhere("plot")
A single object matching 'plot' was found
It was found in the following places
  package:graphics
  namespace:graphics
with value
 ...

With pixmap:

> getAnywhere("plot")
2 differing objects matching 'plot' were found
in the following places
  package:pixmap
  package:graphics
  namespace:graphics
Use [] to view one of them

This also works:

library(pixmap)
x <- numeric()
y <- numeric()
result <- try(plot(try(lm(x~y))))
print(result)

Hope this helps.

> 
> Christian :-(
> 
> P.S.
> The reason for preferring R --vanilla script.R over R CMD BATCH is,
> that i have to produce png images for a server and want to avoid
> the bitmap device for performance reasons.
> With R --vanilla < script.R, this is possible, using the xvnc
> virtual X server.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From ripley at stats.ox.ac.uk  Wed Feb 23 10:43:43 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 23 Feb 2005 09:43:43 +0000 (GMT)
Subject: [R] Run Sweave and LaTeX directly from command line
In-Reply-To: <7FFEE688B57D7346BC6241C55900E730B6FF41@pollux.bfro.uni-lj.si>
References: <7FFEE688B57D7346BC6241C55900E730B6FF41@pollux.bfro.uni-lj.si>
Message-ID: <Pine.LNX.4.61.0502230931001.8649@gannet.stats>

On Tue, 22 Feb 2005, Gorjanc Gregor wrote:

> Friedrich,
>
> This texi2dvi is also very nice. Dou you know of any pros and cons
> in comparison to rubber?

texi2dvi is pretty standard, part of the texinfo distribution.  rubber is 
not in the standard Linux distros, and uses python, also not very 
standard, especially on Windows.

> Is tex2dvi also shipped with R for windows? I am not able to find
> a binnary in my windows installation, however I found help page.
> Did I missed anything? I have 2.0.1

I believe Fritz was referring to the script src/script/texi2dvi.  That is 
far too slow on Windows even if you have the tools installed as it runs ca 
2000 processes per invocation.

R has a function texi2dvi in package tools that avoids this on Windows. 
Let me repost (from a thread on R-devel about automating Sweave scripts on 
Windows).

% cat Rnw.bat
rterm --no-save --args "%1" < /tmp/MakeSweave.R > "%1.log"

% cat /tmp/MakeSweave.R
library(tools)
args <- commandArgs()
inp <- args[length(args)]
Sweave(inp)
base <- sub("\.(Rnw|Rtex)$", "", inp)
texi2dvi(paste("base", ".tex", sep=""), pdf=TRUE)
shell.exec(paste("base", ".pdf", sep="")) # display PDF file on Windows.

which shows how easy this is to script in R in a cross-platform way

>
>>>>>> On Tue, 22 Feb 2005 14:29:00 +0100,
>>>>>> Gregor GORJANC (GG) wrote:
>
>  > Hello!
>  > Those of you, who use Sweave a lot, will probably find my shell script
>  > usable. You can get it at:
>
>  > http://www.bfro.uni-lj.si/MR/ggorjan/programs/shell/Sweave.sh
>
>  > No warranty, however don't hesitate to contact me if you find an error or
>  > have a patch!
>
> Very nice!
>
> Side note 1: R ships a version of texi2dvi, hence you might use that
> one in case rubber is not found.
>
> side note 2: For make afficionados the follwing 2 rules in combination
> with the Sweave script from the FAQ do almost the same (that's what I
> use :-)
>
> %.tex: %.Rnw
>        Sweave $<
>
> %.pdf : %.tex
>        texi2dvi --clean --pdf $<
>
>
> Best,
> Fritz Leisch
>
> -- 
> -------------------------------------------------------------------
>                        Friedrich Leisch
> Institut f?r Statistik                     Tel: (+43 1) 58801 10715
> Technische Universit?t Wien                Fax: (+43 1) 58801 10798
> Wiedner Hauptstra?e 8-10/1071
> A-1040 Wien, Austria             http://www.ci.tuwien.ac.at/~leisch
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ripley at stats.ox.ac.uk  Wed Feb 23 10:52:21 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 23 Feb 2005 09:52:21 +0000 (GMT)
Subject: [R] using 'nice' with R
In-Reply-To: <m0y8dfr5xl.fsf@bar.nemo-project.org>
References: <Pine.LNX.4.62.0502221021420.1696@neblt2>
	<421B55B4.6020600@jhsph.edu> <m0y8dfr5xl.fsf@bar.nemo-project.org>
Message-ID: <Pine.LNX.4.61.0502230945470.8649@gannet.stats>

On Wed, 23 Feb 2005, Bj?rn-Helge Mevik wrote:

> Roger D. Peng writes:
>
>> On a Unix like system you can do `nice +19 R' or perhaps `nice +19 R
>> CMD BATCH commands.R'.
>
> At least on Suse (9.1) and Debian (3.0) Linux, the syntax is
> `nice -19 R' (i.e. with `-', not `+'.)

The syntax depends on the shell you use: tcsh for example works as Roger 
says, and the `nice' executable as you say.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From Friedrich.Leisch at tuwien.ac.at  Wed Feb 23 09:30:54 2005
From: Friedrich.Leisch at tuwien.ac.at (Friedrich.Leisch@tuwien.ac.at)
Date: Wed, 23 Feb 2005 09:30:54 +0100
Subject: [R] Re: Run Sweave and LaTeX directly from command line
In-Reply-To: <16923.59578.86884.820643@basebud.nulle.part>
References: <421B339C.1060204@bfro.uni-lj.si>
	<16923.14984.335575.186928@galadriel.ci.tuwien.ac.at>
	<16923.59578.86884.820643@basebud.nulle.part>
Message-ID: <16924.16190.800239.123486@celebrian.ci.tuwien.ac.at>

>>>>> On Tue, 22 Feb 2005 20:21:46 -0600,
>>>>> Dirk Eddelbuettel (DE) wrote:

  > On 22 February 2005 at 14:58, Friedrich.Leisch at tuwien.ac.at wrote:
  > | side note 2: For make afficionados the follwing 2 rules in combination
  > | with the Sweave script from the FAQ do almost the same (that's what I
  > | use :-)

  > Sure, but it requires a (arguably small) Makefile in every sweave project
  > directory.

Not really, the way I do it is that I have a $HOME/etc/Makeconf with a
lot of personal rules like those for .Snw files, and in my .bashrc I
define

fmake ()
{
    make -f ~/etc/Makeconf $*
}

such that calling fmake instead of make gives me my private rules in
every place I need them :-)



  > Would you consider integrating Gregor's script (or a suitable
  > modification) instead?

Yes, sure, that's why I made him aware of texi2dvi: At least on Unix
systems we make sure that that is always available, while rubber might
not be. On the other hand, the R 2-liner I mentioned in an earlier
mail might be the better way to go. I always use the make rules I
described, hence had no need for either one of them -> none is in the
R sources (yet).

Best,
Fritz



From Friedrich.Leisch at tuwien.ac.at  Wed Feb 23 09:20:51 2005
From: Friedrich.Leisch at tuwien.ac.at (Friedrich.Leisch@tuwien.ac.at)
Date: Wed, 23 Feb 2005 09:20:51 +0100
Subject: [R] Run Sweave and LaTeX directly from command line
In-Reply-To: <7FFEE688B57D7346BC6241C55900E730B6FF3E@pollux.bfro.uni-lj.si>
References: <7FFEE688B57D7346BC6241C55900E730B6FF3E@pollux.bfro.uni-lj.si>
Message-ID: <16924.15587.668697.799418@celebrian.ci.tuwien.ac.at>

>>>>> On Tue, 22 Feb 2005 20:46:01 +0100,
>>>>> Gorjanc Gregor (GG) wrote:

  > Gabor,
  > I definitely agree about use of R, however I am not so "much in" R 
  > as I am in Bash. It took me less than two hours to write this script.

  > At home I use R under windows and I solve problem of portability 
  > with Cygwin. I know that this is not optimal solution for everyone 
  > but ...

  > Can you provide me some insights/thoughts how this script might be 
  > written in R? I am open for discussion and cooperation on this script.



  > What do you think Friedrich? I must look at this texi2dvi. I didn't 
  > hav any experience with it jet. I will look at it.

Well, I didn't know about rubber before I read your script ... seems
like the 2 do the same job. Some windows installations have a tool
called "texify" which again does the same: run latex, bibtex, etc. as
many times as needed.

Ad shell vs. R: In R we already have almost all that is needed:

example(Sweave)
library(tools)
texi2dvi("Sweave-test-1.tex")

should do what you want ... and a Snw2dvi function is the
corresponding 2-liner. If there is need for it I'm happy to include it
in package utils.

HTH,
Fritz



From nhe101 at psu.edu  Wed Feb 23 11:42:27 2005
From: nhe101 at psu.edu (Nic Ellis)
Date: Wed, 23 Feb 2005 05:42:27 -0500
Subject: [R] data.frame error message
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E753@usrymx25.merck.co
 m>
Message-ID: <5.2.0.9.2.20050223052744.00b1aa78@email.psu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050223/763234b3/attachment.pl

From jrgonzalez at ico.scs.es  Wed Feb 23 11:38:16 2005
From: jrgonzalez at ico.scs.es (Gonzalez Ruiz, Juan Ramon)
Date: Wed, 23 Feb 2005 11:38:16 +0100
Subject: [R] new package
Message-ID: <5FF3F11444E3A9439191AA1EDCB69A170BC1A1@icosrvmail01.ICO.SCS.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050223/2ccae371/attachment.pl

From Katharina.Steinmann at stud.unibas.ch  Wed Feb 23 11:42:05 2005
From: Katharina.Steinmann at stud.unibas.ch (K. Steinmann)
Date: Wed, 23 Feb 2005 11:42:05 +0100
Subject: [R] bias of a boot statistic
Message-ID: <1109155325.421c5dfd0d9a9@webmail.unibas.ch>

Question:
How can I get access to the bias value of a boot statistic?

Details:
Boot function:
 boot(data, statistic, R, sim="ordinary", stype="i",
           strata=rep(1,n), L=NULL, m=0, weights=NULL,
           ran.gen=function(d, p) d, mle=NULL, ...)

When I create an object, containing the bootstrap statistic (object <- boot
(....))I can call it and will get an output with t, bias and standarderror as
follows:
Bootstrap Statistics :
    original  bias    std. error
t1*     5.65    0.01   0.9134185

My question is now, where is the value of the bias stored? How can I get access
to this value to do further caluculations?

Thanks,
K. St.



From christian.ritter at shell.com  Wed Feb 23 11:52:26 2005
From: christian.ritter at shell.com (Ritter, Christian C GSMCIL-GSTMS/2)
Date: Wed, 23 Feb 2005 11:52:26 +0100
Subject: [R] Slightly off topic but concerning R#DSC-2005
Message-ID: <156CDC8CCFD1894295D2907F16337A48B2AE66@bru-s-006.europe.shell.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050223/51b969b1/attachment.pl

From bela_b at gmx.net  Wed Feb 23 12:08:53 2005
From: bela_b at gmx.net (Bela Bauer)
Date: Wed, 23 Feb 2005 12:08:53 +0100
Subject: [R] H-F corr.: covariance matrix for interaction effect
Message-ID: <421C6445.4080305@gmx.net>

Hi,

I'm still not quite there with my H-F (G-G) correction code. I have it 
working for the main effects, but I just can't figure out how to do it 
for the effect interactions. The thing I really don't know (and can't 
find anything about) is how to calculate the covariance matrix for the 
interaction between the two (or even n) main factors.
I've looked through some books here and I've tried everything that came 
to my mind, but I can't seem to be able to figure out an algorithm that 
does it for me.

Could anyone give me a hint about how I could do this?
(I'll append my code at the end, in case that helps in any way...)

Thanks

Bela

# parameters for this function are:
# S - variance matrix (created by var() )
# k - number of factor levels (i.e. dim of S)
# n - number of measurements (i.e. number of rows in original matrix)
epsi.GG.HF <- function (S,k,n) {
   D <- (k^2 * (mean(S) - mean(diag(S)))^2)
   N1 <- sum(S^2)
   N2 <- 2 * k * sum(apply(S, 1, mean)^2)
   N3 <- k^2 * mean(S)^2
   epsiGG <- D / ((k - 1) * (N1 - N2 + N3))
   epsiHF <- (n * (k-1) * epsiGG - 2) / ((k-1) * ((n-1) - (k-1)*epsiGG))
   c(epsiGG,epsiHF)
}


# three factors, facROI,facCond,facSubj
# facROI,facCond are main effects, facSubj is "repeatedness"
# G-G and H-F corrections for a main effect
# we do the gghf stuff for the ROI, which means ROIs in columns,
# subjects in rows
mtx <- NULL
for (iROI in 1:length(unique( facROI ))) {
   for (iSubj in 1:length(unique( facSubj ))) {
     mtx <- c(mtx,
              mean(vecData[facROI==unique(facROI)[iROI] & 
facSubj==unique(facSubj)[iSubj]])
              )
   }
}
mtx <- matrix(mtx,ncol=length(unique( facROI )),byrow=F)
GgHfROI <- epsi.GG.HF(var(mtx),length(mtx[1,]),length(mtx[,1]))
print(GgHfROI)

# now for the facROI:facCond interaction...how to go about this?



From ripley at stats.ox.ac.uk  Wed Feb 23 12:17:53 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 23 Feb 2005 11:17:53 +0000 (GMT)
Subject: [R] bias of a boot statistic
In-Reply-To: <1109155325.421c5dfd0d9a9@webmail.unibas.ch>
References: <1109155325.421c5dfd0d9a9@webmail.unibas.ch>
Message-ID: <Pine.LNX.4.61.0502231116430.27882@gannet.stats>

On Wed, 23 Feb 2005, K. Steinmann wrote:

> Question:
> How can I get access to the bias value of a boot statistic?
>
> Details:
> Boot function:
> boot(data, statistic, R, sim="ordinary", stype="i",
>           strata=rep(1,n), L=NULL, m=0, weights=NULL,
>           ran.gen=function(d, p) d, mle=NULL, ...)
>
> When I create an object, containing the bootstrap statistic (object <- boot
> (....))I can call it and will get an output with t, bias and standarderror as
> follows:
> Bootstrap Statistics :
>    original  bias    std. error
> t1*     5.65    0.01   0.9134185
>
> My question is now, where is the value of the bias stored? How can I get access
> to this value to do further caluculations?

apply(object, 2, mean, na.rm = TRUE) - boot.out$t0[1]

in your case.  I read that from

getS3method("print", "boot")


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andy_liaw at merck.com  Wed Feb 23 12:23:18 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 23 Feb 2005 06:23:18 -0500
Subject: [R] data.frame error message
Message-ID: <3A822319EB35174CA3714066D590DCD50994E755@usrymx25.merck.com>

> From: Nic Ellis
> 
> At 09:47 PM 2/22/2005 -0500, Liaw, Andy wrote:
> >Following the suggestions in the Posting Guide would help us 
> to help you
> >much better.
> >
> >What command(s) did you use to get the data into 
> >R?   read.table("C:/.../persist.dat",col.names=c("a","b",etc."))
> 
> 
> >What does the `.dat' file look like?  Itn was created in 
> Word, as a plain 
> >text file.
> 
> 6000 24 female 0.0014 1.4 1 3.47 0.25 2478.57
> 6000 168 female 0.0014 1.4 1 0.73 0.05 521.43
> 6000 96 female 0.0014 1.4 1 0.96 0.07 685.71
> 6000 168 female 0.0014 1.4 1 1.36 0.10 971.43
> 6000 24 female 0.0014 1.4 1 2.69 0.19 1921.43
> 6000 96 female 0.0014 1.4 1 0.76 0.05 542.86...
> 
> >What are `x' and `y'?  'x' is "trt" and 'y' is "ppmtrans".  
> I did subset 
> >commands to find all the males with "ppmsamp"
> above a certain value.
> >
> 
> persist[1:5,]
>     dose     trt    sex      massg   massmg vol  ppm  perc ppmsamp
> 1  6000  24     female 0.0014    1.4         1   3.47  0.25 2478.57
> 2  6000  168  female 0.0014    1.4          1   0.73  0.05  521.43
> 3  6000  96    female 0.0014    1.4          1   0.96  0.07  685.71
> 4  6000  168  female 0.0014    1.4          1   1.36  0.10  971.43
> 5  6000  24    female 0.0014    1.4          1    2.69  0.19 1921.43
> ...subsetting commands...
> ppmtrans<-sqrt(sqrt(ppm.male.mark$ppmsamp))
>  > persist.male<-cbind(ppm.male.mark,ppmtrans)
>  > persist.male[1:5,]
>     dose trt  sex  massg massmg vol  ppm perc ppmsamp ppmtrans
> 27 6000  96 male 0.0012    1.2   1 1.82 0.15 1516.67 6.240549
> 55 6000 168 male 0.0012    1.2   1 0.90 0.08  750.00 5.233176
> 70 6000 168 male 0.0012    1.2   1 1.97 0.16 1641.67 6.365338
> 76 6000  96 male 0.0012    1.2   1 4.02 0.34 3350.00 7.607837
> 83 6000 168 male 0.0012    1.2   1 1.26 0.11 1050.00 5.692425
>  > plot(trt,ppmtrans)
> Error in model.frame(formula, rownames, variables, varnames, extras, 
> extranames,  :
>          variable lengths differ
> 
> After my initial post, I found that if I specified the data 
> set in the plot 
> arguments, I could generate a boxplot.  I then performed
> ANOVA on the response after creating factor levels for "trt", 
> and generated 
> a stripchart showing the data in each of the three trts.  But 
> for future 
> reference, if I need to do linear regression with a similar 
> data set, I'd 
> like to know what is happening.

Seems like you still haven't told all that you did.  What is ppm.male.mark?
Is it an exact copy of `persist', or some subset?  Did you do any subsetting
after you create persiste.male?

If you get that error with plot(trt, ppmtrans), but not with plot(trt,
ppmtrans, data=whatever), then the first version is probably getting `trt'
and `ppmtrans' that you have lying around the global environment, which are
probably results from different subsetting operations (and thus having
different lengths).  Without more info, we can only guess.

One thing to keep in mind:  When using the formula interface, it will save
you some hair if all the variables used in the formula are in the data frame
supplied (and try to always supply the data frame).

Andy
 
> Thank you--
> 
> NH Ellis
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From Achim.Zeileis at wu-wien.ac.at  Wed Feb 23 12:24:57 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed, 23 Feb 2005 12:24:57 +0100
Subject: [R] bias of a boot statistic
In-Reply-To: <1109155325.421c5dfd0d9a9@webmail.unibas.ch>
References: <1109155325.421c5dfd0d9a9@webmail.unibas.ch>
Message-ID: <20050223122457.683c1779.Achim.Zeileis@wu-wien.ac.at>

On Wed, 23 Feb 2005 11:42:05 +0100 K. Steinmann wrote:

> Question:
> How can I get access to the bias value of a boot statistic?
> 
> Details:
> Boot function:
>  boot(data, statistic, R, sim="ordinary", stype="i",
>            strata=rep(1,n), L=NULL, m=0, weights=NULL,
>            ran.gen=function(d, p) d, mle=NULL, ...)
> 
> When I create an object, containing the bootstrap statistic (object <-
> boot(....))I can call it and will get an output with t, bias and
> standarderror as follows:
> Bootstrap Statistics :
>     original  bias    std. error
> t1*     5.65    0.01   0.9134185
> 
> My question is now, where is the value of the bias stored? How can I
> get access to this value to do further caluculations?

In the univariate case, it's simply
  mean(object$t - object$t0)
hth,
Z


> Thanks,
> K. St.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From bela_b at gmx.net  Wed Feb 23 12:35:36 2005
From: bela_b at gmx.net (Bela Bauer)
Date: Wed, 23 Feb 2005 12:35:36 +0100
Subject: [R] H-F corr.: covariance matrix for interaction effect
Message-ID: <421C6A88.3030907@gmx.net>

Hi,

I'm still not quite there with my H-F (G-G) correction code. I have it
working for the main effects, but I just can't figure out how to do it
for the effect interactions. The thing I really don't know (and can't
find anything about) is how to calculate the covariance matrix for the
interaction between the two (or even n) main factors.
I've looked through some books here and I've tried everything that came
to my mind, but I can't seem to be able to figure out an algorithm that
does it for me.

Could anyone give me a hint about how I could do this?
(I'll append my code at the end, in case that helps in any way...)

Thanks

Bela

# parameters for this function are:
# S - variance matrix (created by var() )
# k - number of factor levels (i.e. dim of S)
# n - number of measurements (i.e. number of rows in original matrix)
epsi.GG.HF <- function (S,k,n) {
   D <- (k^2 * (mean(S) - mean(diag(S)))^2)
   N1 <- sum(S^2)
   N2 <- 2 * k * sum(apply(S, 1, mean)^2)
   N3 <- k^2 * mean(S)^2
   epsiGG <- D / ((k - 1) * (N1 - N2 + N3))
   epsiHF <- (n * (k-1) * epsiGG - 2) / ((k-1) * ((n-1) - (k-1)*epsiGG))
   c(epsiGG,epsiHF)
}


# three factors, facROI,facCond,facSubj
# facROI,facCond are main effects, facSubj is "repeatedness"
# G-G and H-F corrections for a main effect
# we do the gghf stuff for the ROI, which means ROIs in columns,
# subjects in rows
mtx <- NULL
for (iROI in 1:length(unique( facROI ))) {
   for (iSubj in 1:length(unique( facSubj ))) {
     mtx <- c(mtx,
              mean(vecData[facROI==unique(facROI)[iROI] &
facSubj==unique(facSubj)[iSubj]])
              )
   }
}
mtx <- matrix(mtx,ncol=length(unique( facROI )),byrow=F)
GgHfROI <- epsi.GG.HF(var(mtx),length(mtx[1,]),length(mtx[,1]))
print(GgHfROI)

# now for the facROI:facCond interaction...how to go about this?



From murdoch at stats.uwo.ca  Wed Feb 23 12:46:44 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 23 Feb 2005 11:46:44 +0000
Subject: [R] Slightly off topic but concerning R#DSC-2005
In-Reply-To: <156CDC8CCFD1894295D2907F16337A48B2AE66@bru-s-006.europe.shell.com>
References: <156CDC8CCFD1894295D2907F16337A48B2AE66@bru-s-006.europe.shell.com>
Message-ID: <c7ro11hfamq54e5a0gmo6kftaqc99h3ola@4ax.com>

On Wed, 23 Feb 2005 11:52:26 +0100, "Ritter, Christian C
GSMCIL-GSTMS/2" <christian.ritter at shell.com> wrote :

>Has anyone seen an official announcement to DSC-2005. I saw and email exchange on the R-news list in mid January in which a date was announced unofficially. (to DSC-organizers: we need and "official" announcement to request funding and travel permission and flights from Europe to Seattle are starting to fill up, so this is urgent).

For the benefit of others, here is the posting giving the time and
place.  I don't think the official call for papers has come out yet.

Duncan Murdoch

>From: Thomas Lumley <tlumley_at_u.washington.edu>
>Date: Thu 13 Jan 2005 - 02:23:50 EST
>
>On Wed, 12 Jan 2005, Marc Schwartz wrote to r-help:
>> I have not seen anything posted yet for DSC 2005, unless I missed it
>> someplace.
>
>DSC 2005 will be held in Seattle, at the University of Washington, August 13-15.
>
>This date is immediately after the Joint Statistical Meetings, and was chosen for the convenience of our European colleagues who might also be attending JSM.
>
>A call for papers and more information will be posted Real Soon Now.
>
>         -thomas



From gregor.gorjanc at bfro.uni-lj.si  Wed Feb 23 13:18:53 2005
From: gregor.gorjanc at bfro.uni-lj.si (Gregor GORJANC)
Date: Wed, 23 Feb 2005 13:18:53 +0100
Subject: [R] Run Sweave and LaTeX directly from command line
In-Reply-To: <16924.15587.668697.799418@celebrian.ci.tuwien.ac.at>
References: <7FFEE688B57D7346BC6241C55900E730B6FF3E@pollux.bfro.uni-lj.si>
	<16924.15587.668697.799418@celebrian.ci.tuwien.ac.at>
Message-ID: <421C74AD.2050307@bfro.uni-lj.si>

Hi!

I really started a nice debate. I have now removed rubber from my script. I 
found tex2dvi equally good and as Brian pointed it is more frequent than 
rubber. I also added some parts from Achim (rm ...).

http://www.bfro.uni-lj.si/MR/ggorjan/programs/shell/Sweave.sh

Shell vs R: It is nice to have such a script/function in R so it is easier 
to port it to Windows. However I find it crucial that user has ability to 
run this script from command line directly not only within R, which should 
not be the problem from what I have read from this mails.

Maybe someone can put all this ideas in one pot. Friedrich?

-- 
Lep pozdrav / With regards,
     Gregor GORJANC

-----------------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty        URI: http://www.bfro.uni-lj.si/MR/ggorjan
Zootechnical Department     mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                   tel: +386 (0)1 72 17 861
SI-1230 Domzale             fax: +386 (0)1 72 17 888
Slovenia, Europe



From petr.pikal at precheza.cz  Wed Feb 23 14:23:54 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 23 Feb 2005 14:23:54 +0100
Subject: [R] data.frame error message
In-Reply-To: <5.2.0.9.2.20050223052744.00b1aa78@email.psu.edu>
References: <3A822319EB35174CA3714066D590DCD50994E753@usrymx25.merck.co m>
Message-ID: <421C91FA.9792.1388F23@localhost>

Hi Nic

Be careful with variables with same names in your environment as 
variables in data.frame. If you have a variable with the same name 
e.g. ppmtrans in data frame and in environment, your commands use 
variable from environment. So probably trt is from data frame and 
ppmtrans from environment.

See what ls() will show you.

In models you can specify data=your.data.frame argument and all 
variables in formula are preferably used from this data frame, if I 
am not mistaken.

Cheers
Petr



On 23 Feb 2005 at 5:42, Nic Ellis wrote:

> At 09:47 PM 2/22/2005 -0500, Liaw, Andy wrote:
> >Following the suggestions in the Posting Guide would help us to help
> >you much better.
> >
> >What command(s) did you use to get the data into 
> >R?   read.table("C:/.../persist.dat",col.names=c("a","b",etc."))
> 
> 
> >What does the `.dat' file look like?  Itn was created in Word, as a
> >plain text file.
> 
> 6000 24 female 0.0014 1.4 1 3.47 0.25 2478.57
> 6000 168 female 0.0014 1.4 1 0.73 0.05 521.43
> 6000 96 female 0.0014 1.4 1 0.96 0.07 685.71
> 6000 168 female 0.0014 1.4 1 1.36 0.10 971.43
> 6000 24 female 0.0014 1.4 1 2.69 0.19 1921.43
> 6000 96 female 0.0014 1.4 1 0.76 0.05 542.86...
> 
> >What are `x' and `y'?  'x' is "trt" and 'y' is "ppmtrans".  I did
> >subset commands to find all the males with "ppmsamp"
> above a certain value.
> >
> 
> persist[1:5,]
>     dose     trt    sex      massg   massmg vol  ppm  perc ppmsamp 1 
> 6000  24     female 0.0014    1.4         1   3.47  0.25 2478.57 2 
> 6000  168  female 0.0014    1.4          1   0.73  0.05  521.43 3 
> 6000  96    female 0.0014    1.4          1   0.96  0.07  685.71 4 
> 6000  168  female 0.0014    1.4          1   1.36  0.10  971.43 5 
> 6000  24    female 0.0014    1.4          1    2.69  0.19 1921.43
> ...subsetting commands... ppmtrans<-sqrt(sqrt(ppm.male.mark$ppmsamp))
>  > persist.male<-cbind(ppm.male.mark,ppmtrans)
>  > persist.male[1:5,]
>     dose trt  sex  massg massmg vol  ppm perc ppmsamp ppmtrans
> 27 6000  96 male 0.0012    1.2   1 1.82 0.15 1516.67 6.240549
> 55 6000 168 male 0.0012    1.2   1 0.90 0.08  750.00 5.233176
> 70 6000 168 male 0.0012    1.2   1 1.97 0.16 1641.67 6.365338
> 76 6000  96 male 0.0012    1.2   1 4.02 0.34 3350.00 7.607837
> 83 6000 168 male 0.0012    1.2   1 1.26 0.11 1050.00 5.692425
>  > plot(trt,ppmtrans)
> Error in model.frame(formula, rownames, variables, varnames, extras,
> extranames,  :
>          variable lengths differ
> 
> After my initial post, I found that if I specified the data set in the
> plot arguments, I could generate a boxplot.  I then performed ANOVA on
> the response after creating factor levels for "trt", and generated a
> stripchart showing the data in each of the three trts.  But for future
> reference, if I need to do linear regression with a similar data set,
> I'd like to know what is happening.
> 
> Thank you--
> 
> NH Ellis
> 
> 
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From jeaneid at chass.utoronto.ca  Wed Feb 23 14:43:36 2005
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Wed, 23 Feb 2005 08:43:36 -0500
Subject: [R] large data set, and RDBMS
Message-ID: <Pine.SGI.4.40.0502230836110.13767595-100000@origin.chass.utoronto.ca>


I have this somewhat large data set that is given to me in a fixed width
format. The file itself is already 100MB (Maybe R can actually handle
this but I am trying to gain some experience in postgres and RODBC).  I
am using postgres to preprocess the file and connect to the database through the RODBC package.

My question is much before the processing of the database in R (Iknow this
is somewhat off topic). I have more than 6000 variables in the data and
postgres is outputing and  error that it cannot handle more than
1600 columns in one table. Is there any Linux database managers that
actually do do this.; Of course I can break it into 4 or 5 tables but I am
wondering if there is a better solution.

System is  a Linux 2.6.8
psql (PostgreSQL) 7.4.7


Thank you,

Jean



From p.dalgaard at biostat.ku.dk  Wed Feb 23 14:59:07 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 23 Feb 2005 14:59:07 +0100
Subject: [R] H-F corr.: covariance matrix for interaction effect
In-Reply-To: <421C6A88.3030907@gmx.net>
References: <421C6A88.3030907@gmx.net>
Message-ID: <x2psyrcr84.fsf@biostat.ku.dk>

Bela Bauer <bela_b at gmx.net> writes:

> Hi,
> 
> I'm still not quite there with my H-F (G-G) correction code. I have it
> working for the main effects, but I just can't figure out how to do it
> for the effect interactions. The thing I really don't know (and can't
> find anything about) is how to calculate the covariance matrix for the
> interaction between the two (or even n) main factors.
> I've looked through some books here and I've tried everything that came
> to my mind, but I can't seem to be able to figure out an algorithm that
> does it for me.
> 
> Could anyone give me a hint about how I could do this?
> (I'll append my code at the end, in case that helps in any way...)

I have given it to you before: My plan is to drop the explicit formula
involving on/off diagonal elements of S and go directly at Box (1954),
theorems 3.1 and 6.1, involving eigenvalues of TST', where T is the
relevant residual operator. In the case where one of the factors have
only two levels, I believe you just take differences and use the usual
formula, but more than two levels is tricky.
 


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Allan at STATS.uct.ac.za  Wed Feb 23 15:11:46 2005
From: Allan at STATS.uct.ac.za (Clark Allan)
Date: Wed, 23 Feb 2005 16:11:46 +0200
Subject: [R] r: ridge regression
Message-ID: <421C8F22.C0830C64@STATS.uct.ac.za>

hello all

some help required once again!

does anyone recall the equations for the following ridge constants?
1. hoerl and kennard (1970)
2. hoerl, kennard and baldwin (1975)
3. lawless and wang

could you also specify whether or not one has to transform the X and Y
variables. if so , how and in which cases. 

a worked example with a data set would be most helpful.

thanking you in advance
***
Allan

From Luisr at frs.fo  Wed Feb 23 15:14:51 2005
From: Luisr at frs.fo (Luis Ridao Cruz)
Date: Wed, 23 Feb 2005 14:14:51 +0000
Subject: [R] filling columns in frame according to another column frame
Message-ID: <s21c8fe1.099@ffdata.setur.fo>

R-help,

I have a frame which I want to fill up conditioning to another data
frame column.

The one I want to fill up is as follows (basically an empty one):

> test2

     cm 0   1   2   3   4   5   6   7   8   9  10  11 12 13 14 15
     1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1
     2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2
     3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3
     4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4
     5   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5
     6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6
     7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7
     8   8   8   8   8   8   8   8   8   8   8   8   8   8   8   8   8
     9   9   9   9   9   9   9   9   9   9   9   9   9   9   9   9   9

The other looks like :

> test1

   cm 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
   38 0 0 1 0 0 0 6 0 0 0  0  0  0  0  0  0
   39 0 0 1 0 0 0 0 0 0 0  0  0  6  0  0  0
   40 0 0 1 0 0 0 0 0 0 0  0  0  0  0  0  0
   41 0 0 2 0 0 0 0 0 0 0  6  0  0  0  0  0
   43 0 0 1 0 0 0 4 0 0 0  0  0  0  0  0  0
   44 0 0 4 0 0 0 5 0 0 0  0  0  0  0  0  0
   45 0 0 2 0 0 0 0 0 0 0  6  0  0  0  0  0
   47 0 0 3 0 0 0 0 0 0 0  0  0  0  0  0  0
   48 0 0 2 0 0 0 0 0 0 6  0  0  0  0  0  0
   49 0 0 2 0 0 0 0 0 0 6  0  0  0  0  0  0
   50 0 0 3 0 0 0 0 0 0 3  0  0  0  0  0  0
   51 0 0 2 0 0 0 0 0 0 3  0  0  0  0  0  0

Length of both frames are different ( test2 = 150 and test1 = 70 )
The key column is 'cm'

I have tried someting (fill column '3' in test2):

test2 [, '3' ]<-
ifelse ( test2$'cm'  %in% test1$'cm' , test1$'3' , 0)

but the result is wrong.

Any suggestions?

Thanks in advance


> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    0.1            
year     2004           
month    11             
day      15             
language R



From sim76 at tiscali.it  Wed Feb 23 15:39:57 2005
From: sim76 at tiscali.it (sim76@tiscali.it)
Date: Wed, 23 Feb 2005 15:39:57 +0100
Subject: [R] to print dataframe
Message-ID: <4212FDE60001C128@mail-5.tiscali.it>

Dear all, 

Is it possible to print a dataframe without the row numbers? 
For example if I have a dataframe like that: 
>df <- data.frame(name1=sample(LETTERS,10),name2=sample(c(0,1),10,replace=TRUE))

after printing 
   name1 name2 
1      O     1 
2      H     0 
3      R     0 
4      T     0 
5      V     1 
6      E     0 
7      W    0 
8      P     1 
9      G     0 
10     J     1 
> 


But I would like the dataframe printed like that 

   name1 name2 
     O      1 
     H      0 
      R     0 
      T     0 
     V      1 
     E      0 
     W     0 
     P      1 
     G     0 
    J       1 

I look at ?print.dataframe ?data.frame but I can't find anything. 
Somebody could help me? Thanks in advance.

__________________________________________________________________
Tiscali Adsl 3 Mega Flat, 3 MESI GRATIS! 
Con Tiscali Adsl 3 Mega Flat navighi in Rete alla supervelocita'
a soli 29.95 euro al mese senza limiti di tempo. Attivati entro
il 28 Febbraio 2005, 3 MESI sono GRATIS
Scopri come http://abbonati.tiscali.it/adsl/sa/2flat_tc/



From d.firth at warwick.ac.uk  Wed Feb 23 15:55:13 2005
From: d.firth at warwick.ac.uk (David Firth)
Date: Wed, 23 Feb 2005 14:55:13 +0000
Subject: [R] model.matrix for a factor effect with no intercept
Message-ID: <feded8c0d3f579a03db1c2543580147d@warwick.ac.uk>

I was surprised by this (in R 2.0.1):

 > a <- ordered(-1:1)
 > a
[1] -1 0  1
Levels: -1 < 0 < 1

 > model.matrix(~ a)
   (Intercept)           a.L        a.Q
1           1 -7.071068e-01  0.4082483
2           1 -9.073800e-17 -0.8164966
3           1  7.071068e-01  0.4082483
attr(,"assign")
[1] 0 1 1
attr(,"contrasts")
attr(,"contrasts")$a
[1] "contr.poly"

 > model.matrix(~ -1 + a)
   a-1 a0 a1
1   1  0  0
2   0  1  0
3   0  0  1
attr(,"assign")
[1] 1 1 1
attr(,"contrasts")
attr(,"contrasts")$a
[1] "contr.poly"

Without the intercept, treatment contrasts seem to have been used (this 
despite the "contr.poly" in the "contrasts" attribute).

It's not restricted to ordered factors.  For example, if Helmert 
contrasts are used for nominal factors, the same sort of thing happens.

I suppose it is a deliberate feature (perhaps to protect the user from 
accidentally fitting models that make no sense?  or maybe some better 
reason?) -- is it explained somewhere?

David



From ramasamy at cancer.org.uk  Wed Feb 23 16:07:55 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Wed, 23 Feb 2005 15:07:55 +0000
Subject: [R] filling columns in frame according to another column frame
In-Reply-To: <s21c8fe1.099@ffdata.setur.fo>
References: <s21c8fe1.099@ffdata.setur.fo>
Message-ID: <1109171275.5865.44.camel@ndmpc126.orc.ox.ac.uk>

I am confused. Are you saying that your two data frames are of different
dimensions ? 

In any case what I think what you are looking for is which.

# generate the conditioning matrix
a <- matrix( sample(0:1, 9, replace=TRUE), nc=3 )
a
     [,1] [,2] [,3]
[1,]    1    1    1
[2,]    1    0    0
[3,]    0    0    1

# find the index where zero is present
( w <- which( a == 0, arr.ind=T ) )
     row col
[1,]   3   1
[2,]   2   2
[3,]   3   2
[4,]   2   3

# generate the matrix of interest
( b <- matrix(1:9, nc=3, byrow=T) )
     [,1] [,2] [,3]
[1,]    1    2    3
[2,]    4    5    6
[3,]    7    8    9

# values that will be used to impute the zero's in a
b[w]
[1] 7 5 8 6

# impute the values of a with b where a is zero
a[w] <- b[w]

# the result
a
     [,1] [,2] [,3]
[1,]    1    1    1
[2,]    1    5    6
[3,]    7    8    1


Regards, Adai


On Wed,	 2005-02-23 at 14:14 +0000, Luis Ridao Cruz wrote:
> R-help,
> 
> I have a frame which I want to fill up conditioning to another data
> frame column.
> 
> The one I want to fill up is as follows (basically an empty one):
> 
> > test2
> 
>      cm 0   1   2   3   4   5   6   7   8   9  10  11 12 13 14 15
>      1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1
>      2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2
>      3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3
>      4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4
>      5   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5
>      6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6
>      7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7
>      8   8   8   8   8   8   8   8   8   8   8   8   8   8   8   8   8
>      9   9   9   9   9   9   9   9   9   9   9   9   9   9   9   9   9
> 
> The other looks like :
> 
> > test1
> 
>    cm 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
>    38 0 0 1 0 0 0 6 0 0 0  0  0  0  0  0  0
>    39 0 0 1 0 0 0 0 0 0 0  0  0  6  0  0  0
>    40 0 0 1 0 0 0 0 0 0 0  0  0  0  0  0  0
>    41 0 0 2 0 0 0 0 0 0 0  6  0  0  0  0  0
>    43 0 0 1 0 0 0 4 0 0 0  0  0  0  0  0  0
>    44 0 0 4 0 0 0 5 0 0 0  0  0  0  0  0  0
>    45 0 0 2 0 0 0 0 0 0 0  6  0  0  0  0  0
>    47 0 0 3 0 0 0 0 0 0 0  0  0  0  0  0  0
>    48 0 0 2 0 0 0 0 0 0 6  0  0  0  0  0  0
>    49 0 0 2 0 0 0 0 0 0 6  0  0  0  0  0  0
>    50 0 0 3 0 0 0 0 0 0 3  0  0  0  0  0  0
>    51 0 0 2 0 0 0 0 0 0 3  0  0  0  0  0  0
> 
> Length of both frames are different ( test2 = 150 and test1 = 70 )
> The key column is 'cm'
> 
> I have tried someting (fill column '3' in test2):
> 
> test2 [, '3' ]<-
> ifelse ( test2$'cm'  %in% test1$'cm' , test1$'3' , 0)
> 
> but the result is wrong.
> 
> Any suggestions?
> 
> Thanks in advance
> 
> 
> > version
>          _              
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    2              
> minor    0.1            
> year     2004           
> month    11             
> day      15             
> language R
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From reid_huntsinger at merck.com  Wed Feb 23 16:22:55 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Wed, 23 Feb 2005 10:22:55 -0500
Subject: [R] Memory error in Mac OS X Aqua GUI v1.01 with cluster
	pack age functions
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A92F6@uswpmx00.merck.com>

It's trying to allocate about 850 MB. And that's just the "object that broke
the camel's back". You probably really are out of memory. You could increase
swap space and cross your fingers, but probably daisy creates the 10481 x
10481 distance matrix, which would be about 800 MB since each entry is 8
bytes. It may even create multiple copies.

You might try increasing RAM to 4 GB or a clustering routine that doesn't
need a distance matrix, like k-means (which is based on Euclinean distance
as well).

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Betty Gilbert
Sent: Tuesday, February 22, 2005 5:57 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Memory error in Mac OS X Aqua GUI v1.01 with cluster package
functions


I'm sorry if the answer to my problem is buried in the archives. I 
have limited experience with R  and I couldn't find a solution to my 
particular problem. I am running  Mac OS X Aqua GUI v1.01 on a new G5 
running os 10.3.8 with a 1.8Ghz processor and 1GB of sdram. I just 
downloaded bioconducter a week ago and I'm trying to cluster a matrix 
I created with a simulation with dimensions
dim(nca35)
[1] 10481    12

with size
>  object.size(nca352)
[1] 1426204

I checked my ulimits variable on the unix terminal and it says it's 
unlimited as does
>  mem.limits()
nsize vsize
    NA    NA
But I'm still getting errors like the following with funtions in the 
cluster package
>  daisy(nca352, metric= "euclidean", stand=FALSE)->dnca35
Error: cannot allocate vector of size 858213 Kb
*** malloc: vm_allocate(size=878813184) failed (error code=3)
*** malloc[599]: error: Can't allocate region
if it helps i also checked
>  gc()
          used (Mb) gc trigger   (Mb)
Ncells 448662 12.0     741108   19.8
Vcells 847630  6.5  135357901 1032.7

I tried the suggested unix command in the memory help doc but that 
doesn't work in the Aqua GUI. Can someone tell me how to change the 
Vcells? Although to the best of my understanding (which is limited) I 
shouldn't have to do that. Any suggestions would be greatly 
appreciated.
thanks,
betty
-- 
Betty Gilbert
lgilbert at berkeley.edu
Taylor Lab
Plant and Microbial Biology
321 Koshland Hall
U.C. Berkeley
Berkeley, Ca 94720

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From lraja at i-com.com  Wed Feb 23 16:28:07 2005
From: lraja at i-com.com (Latha Raja)
Date: Wed, 23 Feb 2005 10:28:07 -0500
Subject: [R] Need your help in calculating the p-value
Message-ID: <E6A6A0A8A6FD0943B588100ECDD9DABA0E5C43@mail-01.i-com.com>

Hi,

I am using R to perform wilcox.test and wondering if you know how the p-value in wilcox.test is calculated?

Thank you for your help:-)

Regards,

Latha



From ripley at stats.ox.ac.uk  Wed Feb 23 16:31:15 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 23 Feb 2005 15:31:15 +0000 (GMT)
Subject: [R] Run Sweave and LaTeX directly from command line
In-Reply-To: <421C74AD.2050307@bfro.uni-lj.si>
References: <7FFEE688B57D7346BC6241C55900E730B6FF3E@pollux.bfro.uni-lj.si>
	<16924.15587.668697.799418@celebrian.ci.tuwien.ac.at>
	<421C74AD.2050307@bfro.uni-lj.si>
Message-ID: <Pine.LNX.4.61.0502231526590.8395@gannet.stats>

On Wed, 23 Feb 2005, Gregor GORJANC wrote:

> Hi!
>
> I really started a nice debate. I have now removed rubber from my script. I 
> found tex2dvi equally good and as Brian pointed it is more frequent than 
> rubber. I also added some parts from Achim (rm ...).
>
> http://www.bfro.uni-lj.si/MR/ggorjan/programs/shell/Sweave.sh
>
> Shell vs R: It is nice to have such a script/function in R so it is easier to 
> port it to Windows. However I find it crucial that user has ability to run 
> this script from command line directly not only within R, which should not be 
> the problem from what I have read from this mails.

You have to run _some_ script interpreter from the command line, and that 
may equally well be R as python, perl or sh.  I gave a complete example of 
doing (as people seems not to pick up on this).

Now we have a `lean-and-mean' base R (the purpose of the package 
reorganization around 1.9.0) it works quite well as a script engine 
(startup time below 100ms), and it is used extensively to install R and 
build packages.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Luisr at frs.fo  Wed Feb 23 16:38:52 2005
From: Luisr at frs.fo (Luis Ridao Cruz)
Date: Wed, 23 Feb 2005 15:38:52 +0000
Subject: [R] filling columns in frame according to another column
	frame
Message-ID: <s21ca393.050@ffdata.setur.fo>

I think I did not explain very well what my problem is,

Both frames have the same number of columns but different number of
rows.
The point is to compare 'cm' column in test1 with 'cm' in test2 as
follows

first element in test2$'cm'  versus first element in test1$'cm' 
first element in test2$'cm'  versus second element in test1$'cm' 
first element in test2$'cm'  versus element element in test1$'cm'
.....


if any of the above matches returns the value in column , lets say 10,
of  element in test1$'cm' , if not then 0

Luis


>>> Adaikalavan Ramasamy <ramasamy at cancer.org.uk> 23/02/2005 15:07:55
>>>
I am confused. Are you saying that your two data frames are of
different
dimensions ? 

In any case what I think what you are looking for is which.

# generate the conditioning matrix
a <- matrix( sample(0:1, 9, replace=TRUE), nc=3 )
a
     [,1] [,2] [,3]
[1,]    1    1    1
[2,]    1    0    0
[3,]    0    0    1

# find the index where zero is present
( w <- which( a == 0, arr.ind=T ) )
     row col
[1,]   3   1
[2,]   2   2
[3,]   3   2
[4,]   2   3

# generate the matrix of interest
( b <- matrix(1:9, nc=3, byrow=T) )
     [,1] [,2] [,3]
[1,]    1    2    3
[2,]    4    5    6
[3,]    7    8    9

# values that will be used to impute the zero's in a
b[w]
[1] 7 5 8 6

# impute the values of a with b where a is zero
a[w] <- b[w]

# the result
a
     [,1] [,2] [,3]
[1,]    1    1    1
[2,]    1    5    6
[3,]    7    8    1


Regards, Adai


On Wed,	 2005-02-23 at 14:14 +0000, Luis Ridao Cruz wrote:
> R-help,
> 
> I have a frame which I want to fill up conditioning to another data
> frame column.
> 
> The one I want to fill up is as follows (basically an empty one):
> 
> > test2
> 
>      cm 0   1   2   3   4   5   6   7   8   9  10  11 12 13 14 15
>      1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1  
1
>      2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2  
2
>      3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3  
3
>      4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4  
4
>      5   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5  
5
>      6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6  
6
>      7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7  
7
>      8   8   8   8   8   8   8   8   8   8   8   8   8   8   8   8  
8
>      9   9   9   9   9   9   9   9   9   9   9   9   9   9   9   9  
9
> 
> The other looks like :
> 
> > test1
> 
>    cm 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
>    38 0 0 1 0 0 0 6 0 0 0  0  0  0  0  0  0
>    39 0 0 1 0 0 0 0 0 0 0  0  0  6  0  0  0
>    40 0 0 1 0 0 0 0 0 0 0  0  0  0  0  0  0
>    41 0 0 2 0 0 0 0 0 0 0  6  0  0  0  0  0
>    43 0 0 1 0 0 0 4 0 0 0  0  0  0  0  0  0
>    44 0 0 4 0 0 0 5 0 0 0  0  0  0  0  0  0
>    45 0 0 2 0 0 0 0 0 0 0  6  0  0  0  0  0
>    47 0 0 3 0 0 0 0 0 0 0  0  0  0  0  0  0
>    48 0 0 2 0 0 0 0 0 0 6  0  0  0  0  0  0
>    49 0 0 2 0 0 0 0 0 0 6  0  0  0  0  0  0
>    50 0 0 3 0 0 0 0 0 0 3  0  0  0  0  0  0
>    51 0 0 2 0 0 0 0 0 0 3  0  0  0  0  0  0
> 
> Length of both frames are different ( test2 = 150 and test1 = 70 )
> The key column is 'cm'
> 
> I have tried someting (fill column '3' in test2):
> 
> test2 [, '3' ]<-
> ifelse ( test2$'cm'  %in% test1$'cm' , test1$'3' , 0)
> 
> but the result is wrong.
> 
> Any suggestions?
> 
> Thanks in advance
> 
> 
> > version
>          _              
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    2              
> minor    0.1            
> year     2004           
> month    11             
> day      15             
> language R
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html 
>



From akniss at uwyo.edu  Wed Feb 23 16:41:35 2005
From: akniss at uwyo.edu (Andrew Kniss)
Date: Wed, 23 Feb 2005 08:41:35 -0700
Subject: [R] RE: R-help Digest, Vol 24, Issue 22
In-Reply-To: <b98ebca92feea77f131d1a265077c35d@anu.edu.au>
Message-ID: <000a01c519be$2b92d190$6a07070a@andrew>

I used SAS to analyze the data initially, since the data set was made up of
several files when I received it, and I'm still not very good at
manipulating data in R.  

I have posted the data set from one location at the following address:
http://uwstudentweb.uwyo.edu/A/AKNISS/sxherb.txt
var=cultivar
trt=herbicide treatment
yield=response variable of interest
All plot# from 101 to 104 are rep 1, 201-204 rep 2, and 301 to 304 rep 3.

It was the only file that was in an easy format for R to read at the moment,
and was probably the most reliable trial of the two locations. I would like
to use power.anova.test() with this data set to plan next years study (to
get a sample size for each herb*var combination), but I'm not quite sure how
that is done for an interaction effect.  Do I just use the MS for herb*var
as the between group variance and the MSE as the within group variance?  Or
do I need to somehow include other variance parameters in the model?
   
The model for this location (split-block design):
 yield = rep + herb + var + herb*var ## all are fixed effects
        rep*herb = error term for herb
        rep*var = error term for cultivar
        residual = error term for herb*var

I hope this attempt at my question was a little more clear.  I appreciate
any help that is offered.

Andrew Kniss
Assistant Research Scientist
University of Wyoming
Department of Plant Sciences
1000 E. Univesity Ave
Laramie, WY  82071  USA
akniss at uwyo.edu



-----Original Message-----
From: John Maindonald [mailto:john.maindonald at anu.edu.au] 
Sent: Tuesday, February 22, 2005 3:37 PM
To: r-help at stat.math.ethz.ch
Cc: akniss at uwyo.edu
Subject: Re: R-help Digest, Vol 24, Issue 22

You need to give the model formula that gave your output.
There are two sources of variation (at least), within and
between locations; though it looks as though your analysis
may have tried to account for this (but if so, the terms are
not laid out in a way that makes for ready interpretation.
The design is such (two locations) that you do not have
much of a check that effects are consistent over locations.

You need to check whether results really are similar
for all cultivars and for all herbicides, so that it is
legitimate to pool as happens in the overall analysis.
If a herbicide:cultivar combination has little effect the
variability may be large, while if it has a dramatic effect
(kills everything!), there may be no variability to speak of.
John Maindonald.

On 22 Feb 2005, at 10:06 PM, r-help-request at stat.math.ethz.ch wrote:

> To: "'Bob Wheeler'" <bwheeler at echip.com>
> Cc: r-help at stat.math.ethz.ch
> Subject: RE: [R] power.anova.test for interaction effects
> Reply-To: akniss at uwyo.edu
>
>
> It's a rather complex model.  A 37*4 factorial (37 cultivars[var]; 4
> herbicide treatments[trt]) with three replications[rep] was carried 
> out at
> two locations[loc], with  different randomizations within each rep at 
> each
> location.
>
> Source           DF   Error Term      MS
> Loc               1   Trt*rep(loc)    12314
> Rep(loc)          4   Trt*rep(loc)    1230.5
> Trt               3   Trt*rep(loc)    64.72
> Trt*loc           3   Trt*rep(loc)    33.42
> Trt*rep(loc)     12   Residual        76.78
> Var              36   Var*trt*loc     93.91
> Var*trt         108   Var*trt*loc     12.06
> Var*trt*loc     144   Residual        43.09
> Residual        575   NA              21.23
>
>
> -----Original Message-----
> From: Bob Wheeler [mailto:bwheeler at echip.com]
> Sent: Monday, February 21, 2005 4:33 PM
> To: akniss at uwyo.edu
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] power.anova.test for interaction effects
>
> Your F value is so low as to make me suspect your model. Where did the
> 144 denominator degrees of freedom come from?
>
John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Bioinformation Science, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.



From Paul.Boutros at utoronto.ca  Wed Feb 23 16:43:52 2005
From: Paul.Boutros at utoronto.ca (Paul Boutros)
Date: Wed, 23 Feb 2005 10:43:52 -0500
Subject: [R] Problems Building R on AIX 5.2.0.0 (Re-post)
In-Reply-To: <1109021755.421a543bacb40@webmail.utoronto.ca>
Message-ID: <CPEAKHBKLBNIKJDIELLCEENPCPAA.Paul.Boutros@utoronto.ca>

Hello,

I am trying to build R 2.0.1 on an AIX 5.2.0.0 machine using gcc 3.3.2:
$ oslevel
5.2.0.0
$ gcc -v
Reading specs from /usr/local/lib/gcc-lib/powerpc-ibm-aix5.2.0.0/3.3.2/specs
Configured with: ../gcc-3.3.2/configure  : (reconfigured)
../gcc-3.3.2/configure
--disable-nls : (reconfigured) ../gcc-3.3.2/configure --disable-nls
Thread model: aix
gcc version 3.3.2

Configure goes okay, but I get an error that I don't quite know how to
interpret
during make.  I've included the summary output from the end of configure as
well
as the error that I get during make below.  Any suggestions/recommendations
are
very much appreciate: I'm stuck on ideas for what could be going wrong.

Paul

$ ./configure --prefix=/db2blaste/R

<snip>

R is now configured for powerpc-ibm-aix5.2.0.0

  Source directory:          .
  Installation directory:    /db2blast/R

  C compiler:                gcc -mno-fp-in-toc -g -O2
  C++ compiler:              g++  -g -O2
  Fortran compiler:          g77  -g -O2

  Interfaces supported:      X11
  External libraries:
  Additional capabilities:   PNG, JPEG
  Options enabled:           R profiling

  Recommended packages:      yes

configure: WARNING: you cannot build DVI versions of the R manuals
configure: WARNING: you cannot build info or html versions of the R manuals
configure: WARNING: you cannot build PDF versions of the R manuals
configure: WARNING: I could not determine a browser
configure: WARNING: I could not determine a PDF viewer

$ make

<snip>

        gcc -Wl,-bM:SRE -Wl,-H512 -Wl,-T512 -Wl,-bnoentry -Wl,-bexpall -Wl,-
bI:.
./../../etc/R.exp -L/usr/local/lib -o
lapack.so -Wl,-bI:../../../etc/Rlapack.exp
Lapack.lo rgeev.lo
rsyev.lo  -L../../../lib -lRlapack  -L/usr/local/lib -L/usr/
local/lib/gcc-lib/powerpc-ibm-aix5.2.0.0/3.3.2 -L/usr/local/lib/gcc-lib/powe
rpc-
ibm-aix5.2.0.0/3.3.2/../../.. -lfrtbegin -lg2c -lm -lgcc_s
/usr/local/lib/gcc-
lib/powerpc-ibm-aix5.2.0.0/3.3.2/libgcc.a -lg -ldl -ltermcap -lm -lc
ld: 0706-006 Cannot find or open library file: -l Rlapack
        ld:open(): A file or directory in the path name does not exist.
collect2: ld returned 255 exit status
make: 1254-004 The error code from the last command is 1.


Stop.
make: 1254-004 The error code from the last command is 2.


Stop.
make: 1254-004 The error code from the last command is 1.


Stop.
make: 1254-004 The error code from the last command is 1.


Stop.
make: 1254-004 The error code from the last command is 1.


Stop.



From ripley at stats.ox.ac.uk  Wed Feb 23 16:45:55 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 23 Feb 2005 15:45:55 +0000 (GMT)
Subject: [R] model.matrix for a factor effect with no intercept
In-Reply-To: <feded8c0d3f579a03db1c2543580147d@warwick.ac.uk>
References: <feded8c0d3f579a03db1c2543580147d@warwick.ac.uk>
Message-ID: <Pine.LNX.4.61.0502231537240.8552@gannet.stats>

MASS4 p.150
White Book p.38

Those are the only two reasonably comprehensive accounts that I am aware 
of (and they have only partial overlap).

The underlying motivation is to span the _additional_ vector space covered 
by the term, the complement to what has gone before.  Put another way, as 
each term is added, only enough columns are added to the model matrix to 
span the same space as if dummy coding had been used for that term and its 
predecessors.  So think of this as a way to produce a parsimonious 
(usually full-rank) basis for the model space.

On Wed, 23 Feb 2005, David Firth wrote:

> I was surprised by this (in R 2.0.1):
>
>> a <- ordered(-1:1)
>> a
> [1] -1 0  1
> Levels: -1 < 0 < 1
>
>> model.matrix(~ a)
>  (Intercept)           a.L        a.Q
> 1           1 -7.071068e-01  0.4082483
> 2           1 -9.073800e-17 -0.8164966
> 3           1  7.071068e-01  0.4082483
> attr(,"assign")
> [1] 0 1 1
> attr(,"contrasts")
> attr(,"contrasts")$a
> [1] "contr.poly"
>
>> model.matrix(~ -1 + a)
>  a-1 a0 a1
> 1   1  0  0
> 2   0  1  0
> 3   0  0  1
> attr(,"assign")
> [1] 1 1 1
> attr(,"contrasts")
> attr(,"contrasts")$a
> [1] "contr.poly"
>
> Without the intercept, treatment contrasts seem to have been used (this 
> despite the "contr.poly" in the "contrasts" attribute).
>
> It's not restricted to ordered factors.  For example, if Helmert contrasts 
> are used for nominal factors, the same sort of thing happens.
>
> I suppose it is a deliberate feature (perhaps to protect the user from 
> accidentally fitting models that make no sense?  or maybe some better 
> reason?) -- is it explained somewhere?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Gregor.Gorjanc at bfro.uni-lj.si  Wed Feb 23 16:50:02 2005
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Wed, 23 Feb 2005 16:50:02 +0100
Subject: [R] Sweave and \input or \include LaTeX commands
Message-ID: <7FFEE688B57D7346BC6241C55900E730B6FF44@pollux.bfro.uni-lj.si>

Hello!

I was just wondering if Sweave can work with \input or \include 
LaTeX commands. So, is it aware of such a possible hierarchy in
documents. I would test that, but I don't have such a report 
available at the moment.

I thought of that when I was writting shell script for Sweave
from command line and I have solved that part there.

--
Lep pozdrav / With regards,
    Gregor GORJANC

---------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty       URI: http://www.bfro.uni-lj.si
Zootechnical Department    email: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                  tel: +386 (0)1 72 17 861
SI-1230 Domzale            fax: +386 (0)1 72 17 888
Slovenia



From pallier at lscp.ehess.fr  Wed Feb 23 17:09:51 2005
From: pallier at lscp.ehess.fr (Christophe Pallier)
Date: Wed, 23 Feb 2005 17:09:51 +0100
Subject: [R] Need your help in calculating the p-value
In-Reply-To: <E6A6A0A8A6FD0943B588100ECDD9DABA0E5C43@mail-01.i-com.com>
References: <E6A6A0A8A6FD0943B588100ECDD9DABA0E5C43@mail-01.i-com.com>
Message-ID: <421CAACF.9090203@lscp.ehess.fr>



Latha Raja wrote:

>Hi,
>
>I am using R to perform wilcox.test and wondering if you know how the p-value in wilcox.test is calculated?
>  
>
You can get the source code of the wilcox.test function:

 > methods(wilcox.test)
[1] wilcox.test.default* wilcox.test.formula*

    Non-visible functions are asterisked
 > getAnywhere(wilcox.test.default)

...

You will see that it uses 'psignrank' (for the one sample case).


Christophe Pallier
www.pallier.org



From ramasamy at cancer.org.uk  Wed Feb 23 17:10:38 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Wed, 23 Feb 2005 16:10:38 +0000
Subject: [R] filling columns in frame according to another column frame
In-Reply-To: <s21ca393.049@ffdata.setur.fo>
References: <s21ca393.049@ffdata.setur.fo>
Message-ID: <1109175038.5865.58.camel@ndmpc126.orc.ox.ac.uk>

So you want to grep for each pattern as indicated by columns of test2 in
the columns of test1. Something like your initial approach.

 p <- c(1,3,5)      # pattern like test2$'cm'
 x <- sample(1:10)  # data    like test1$'cm'

 x
 [1]  3  4  9  8  7  5 10  2  1  6

 x %in% p
 [1]  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE

 (w <- which( x %in% p ))
[1] 1 6 9

If no matches are found, you get numeric(0) for 'w' and I usually test
this using length(w)==0. 


On second thought, I tried your original solution and it appears to work
but not tested thoroughly. Can you provide a _simple_ example where this
does not work. Thank you.

 z <- 1:length(x)             # like test1$'3'
 z
 [1]  1  2  3  4  5  6  7  8  9 10

 ifelse( x %in% p, z, 0 )
 [1] 1 0 0 0 0 6 0 0 9 0


BTW, why are you quoting your names as in 'cm' and '3' and are test1,
test2 really data.frames ? What does class(test1) and class(test1$'cm')
say ?

Regards, Adai


On Wed, 2005-02-23 at 15:38 +0000, Luis Ridao Cruz wrote:
> I think I did not explain very well what my problem is,
> 
> Both frames have the same number of columns but different number of
> rows.
> The point is to compare 'cm' column in test1 with 'cm' in test2 as
> follows
> 
> first element in test2$'cm'  versus first element in test1$'cm' 
> first element in test2$'cm'  versus second element in test1$'cm' 
> first element in test2$'cm'  versus element element in test1$'cm'
> .....
> 
> 
> if any of the above matches returns the value in column , lets say 10,
> of  element in test1$'cm' , if not then 0
> 
> Luis
> 
> 
> >>> Adaikalavan Ramasamy <ramasamy at cancer.org.uk> 23/02/2005 15:07:55
> >>>
> I am confused. Are you saying that your two data frames are of
> different
> dimensions ? 
> 
> In any case what I think what you are looking for is which.
> 
> # generate the conditioning matrix
> a <- matrix( sample(0:1, 9, replace=TRUE), nc=3 )
> a
>      [,1] [,2] [,3]
> [1,]    1    1    1
> [2,]    1    0    0
> [3,]    0    0    1
> 
> # find the index where zero is present
> ( w <- which( a == 0, arr.ind=T ) )
>      row col
> [1,]   3   1
> [2,]   2   2
> [3,]   3   2
> [4,]   2   3
> 
> # generate the matrix of interest
> ( b <- matrix(1:9, nc=3, byrow=T) )
>      [,1] [,2] [,3]
> [1,]    1    2    3
> [2,]    4    5    6
> [3,]    7    8    9
> 
> # values that will be used to impute the zero's in a
> b[w]
> [1] 7 5 8 6
> 
> # impute the values of a with b where a is zero
> a[w] <- b[w]
> 
> # the result
> a
>      [,1] [,2] [,3]
> [1,]    1    1    1
> [2,]    1    5    6
> [3,]    7    8    1
> 
> 
> Regards, Adai
> 
> 
> On Wed,	 2005-02-23 at 14:14 +0000, Luis Ridao Cruz wrote:
> > R-help,
> > 
> > I have a frame which I want to fill up conditioning to another data
> > frame column.
> > 
> > The one I want to fill up is as follows (basically an empty one):
> > 
> > > test2
> > 
> >      cm 0   1   2   3   4   5   6   7   8   9  10  11 12 13 14 15
> >      1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1  
> 1
> >      2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2  
> 2
> >      3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3  
> 3
> >      4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4  
> 4
> >      5   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5  
> 5
> >      6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6  
> 6
> >      7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7  
> 7
> >      8   8   8   8   8   8   8   8   8   8   8   8   8   8   8   8  
> 8
> >      9   9   9   9   9   9   9   9   9   9   9   9   9   9   9   9  
> 9
> > 
> > The other looks like :
> > 
> > > test1
> > 
> >    cm 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
> >    38 0 0 1 0 0 0 6 0 0 0  0  0  0  0  0  0
> >    39 0 0 1 0 0 0 0 0 0 0  0  0  6  0  0  0
> >    40 0 0 1 0 0 0 0 0 0 0  0  0  0  0  0  0
> >    41 0 0 2 0 0 0 0 0 0 0  6  0  0  0  0  0
> >    43 0 0 1 0 0 0 4 0 0 0  0  0  0  0  0  0
> >    44 0 0 4 0 0 0 5 0 0 0  0  0  0  0  0  0
> >    45 0 0 2 0 0 0 0 0 0 0  6  0  0  0  0  0
> >    47 0 0 3 0 0 0 0 0 0 0  0  0  0  0  0  0
> >    48 0 0 2 0 0 0 0 0 0 6  0  0  0  0  0  0
> >    49 0 0 2 0 0 0 0 0 0 6  0  0  0  0  0  0
> >    50 0 0 3 0 0 0 0 0 0 3  0  0  0  0  0  0
> >    51 0 0 2 0 0 0 0 0 0 3  0  0  0  0  0  0
> > 
> > Length of both frames are different ( test2 = 150 and test1 = 70 )
> > The key column is 'cm'
> > 
> > I have tried someting (fill column '3' in test2):
> > 
> > test2 [, '3' ]<-
> > ifelse ( test2$'cm'  %in% test1$'cm' , test1$'3' , 0)
> > 
> > but the result is wrong.
> > 
> > Any suggestions?
> > 
> > Thanks in advance
> > 
> > 
> > > version
> >          _              
> > platform i386-pc-mingw32
> > arch     i386           
> > os       mingw32        
> > system   i386, mingw32  
> > status                  
> > major    2              
> > minor    0.1            
> > year     2004           
> > month    11             
> > day      15             
> > language R
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help 
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html 
> > 
> 
>



From ramasamy at cancer.org.uk  Wed Feb 23 17:15:46 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Wed, 23 Feb 2005 16:15:46 +0000
Subject: [R] Need your help in calculating the p-value
In-Reply-To: <E6A6A0A8A6FD0943B588100ECDD9DABA0E5C43@mail-01.i-com.com>
References: <E6A6A0A8A6FD0943B588100ECDD9DABA0E5C43@mail-01.i-com.com>
Message-ID: <1109175346.5865.63.camel@ndmpc126.orc.ox.ac.uk>

See the details and references section of help("wilcox.test"). A small
section from the details might be relevant :

     By default (if 'exact' is not specified), an exact p-value is
     computed if the samples contain less than 50 finite values and
     there are no ties.  Otherwise, a normal approximation is used.

Also type in stats:::wilcox.test.default or getAnywhere
("wilcox.test.default") to see the actual codes.

Regards, Adai


On Wed, 2005-02-23 at 10:28 -0500, Latha Raja wrote:
> Hi,
> 
> I am using R to perform wilcox.test and wondering if you know how the p-value in wilcox.test is calculated?
> 
> Thank you for your help:-)
> 
> Regards,
> 
> Latha
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From uofiowa at gmail.com  Wed Feb 23 17:30:53 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Wed, 23 Feb 2005 11:30:53 -0500
Subject: [R] RODBC type conversion bug
Message-ID: <3f87cc6d050223083027b87f61@mail.gmail.com>

I run R 2.0.1 on Debian and connect to Informix database via RODBC. In
the table below the column "month" is of type char(1). RODBC seems to
be converting this column to boolean if the value is F or T.

This is the data in my table:

       id month year
      25         F 2005
      26         Z 2005

When I select * for id 25 I get
25 FALSE 2005

When I select * for id 16 I get
26    Z 2005

Is there a fix for this issue?

I invoke odbc connect with: db <- odbcConnect("mydb", believeNRows=FALSE)



From maillists at visiv.co.uk  Wed Feb 23 17:48:12 2005
From: maillists at visiv.co.uk (Graham Jones)
Date: Wed, 23 Feb 2005 16:48:12 +0000
Subject: [R] Graphics (crashes under Windows)
In-Reply-To: <200502231143.j1NBTtYT030301@hypatia.math.ethz.ch>
References: <200502231143.j1NBTtYT030301@hypatia.math.ethz.ch>
Message-ID: <QsKtHXAMPLHCFwa9@visiv.co.uk>

In message <200502231143.j1NBTtYT030301 at hypatia.math.ethz.ch>, r-help-
request at stat.math.ethz.ch writes

>The R platform that I installed on my Windows XP crashes everytime that
>I try to run some sophisticated graphics (e.g. Demo Graphics). Is that
>to do with the configuration? Shall I reinstall it? 

You may have a buggy video driver. If you go to Control Panel, Display,
Settings, Advanced, Troubleshoot, and reduce the hardware acceleration,
it may fix the problem. (Maybe it is worth adding this trick to the R
for Windows FAQ?)

-- 
Graham Jones, author of SharpEye Music Reader
http://www.visiv.co.uk
21e Balnakeil, Durness, Lairg, Sutherland, IV27 4PT, Scotland, UK



From brian_cade at usgs.gov  Wed Feb 23 18:08:45 2005
From: brian_cade at usgs.gov (Brian S Cade)
Date: Wed, 23 Feb 2005 10:08:45 -0700
Subject: [R] stopping a function
Message-ID: <OF215F321E.666B9719-ON87256FB1.005DB9CC-87256FB1.005E4583@usgs.gov>

I've looked for this information in all the R help sources I could find and
found nothing.  Is it possible to use some function key to stop the
execution of some R command without ending the R session (Windows, R 1.91)?
I've several times started functions that for various reasons are not
executing properly and it would be nice to stop them without killing the R
session.  I've been using taskmgr to end the R session but I then lose all
objects created during the session.

Brian

Brian S. Cade

U. S. Geological Survey
Fort Collins Science Center
2150 Centre Ave., Bldg. C
Fort Collins, CO  80526-8818

email:  brian_cade at usgs.gov
tel:  970 226-9326



From bates at stat.wisc.edu  Wed Feb 23 18:29:15 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 23 Feb 2005 11:29:15 -0600
Subject: [R] problems with nonlinear fits using nls
In-Reply-To: <E62D4EFB4204BF4F947E1BBDA775517B36C188@mail.site.cdu.edu.au>
References: <E62D4EFB4204BF4F947E1BBDA775517B36C188@mail.site.cdu.edu.au>
Message-ID: <421CBD6B.4080405@stat.wisc.edu>

Corey Bradshaw wrote:
> Hello colleagues,
> 
>  
> 
> I am attempting to determine the nonlinear least-squares estimates of
> the nonlinear model parameters using nls. I have come across a common
> problem that R users have reported when I attempt to fit a particular
> 3-parameter nonlinear function to my dataset:
> 
>  
> 
> Error in nls(r ~ tlm(a, N.fix, k, theta), data = tlm.data, start =
> list(a = a.st,  : 
> 
>         step factor 0.000488281 reduced below `minFactor' of 0.000976563
> 
>  
> 
> Despite modifying minFactor using nls.control, I am unable to counter
> the apparent singularity in the model fit. I have also tried changing
> the tolerance and start parameter values to no avail. If anyone can
> provide a relatively simple solution (perhaps adjusting the gradient,
> but I'm not sure how to do this), I would be most appreciative. My
> dataset is:
> 
>  
> 
> 
>>tlm.data
> 
> 
>              r N.fix
> 
> 1  -0.52407085    76
> 
> 2   0.10536052    45
> 
> 3  -0.17435339    50
> 
> 4   0.19415601    42
> 
> 5   0.48701498    51
> 
> 6  -0.50681760    83
> 
> 7  -0.17435339    50
> 
> 8   0.55278982    42
> 
> 9   0.15219182    73
> 
> 10  0.49899117    85
> 
> 11  0.10821358   140
> 
> 12 -0.83034830   156
> 
> 13 -0.30748470    68
> 
> 14 -0.22314355    50
> 
> 15  0.04879016    40
> 
> 16 -0.04879016    42
> 
> 17  0.75377180    40
> 
> 18 -0.12516314    85
> 
> 19 -0.36624439    75
> 
>  
> 
> My function is:
> 
>  
> 
> tlm <- function(a,N,k,theta) (a*(1-((N/k)^theta)))
> 
>  
> 
> The nls fit I've coded is:
> 
>  
> 
> tlm.fit <- try(nls(r~tlm(a,N.fix,k,theta), data=tlm.data,
> start=list(a=a.st,k=k.st,theta=1),
> 
>             trace=TRUE,
> control=nls.control(maxiter=6000,tol=1e-05,minFactor=1/1024)))
> 
>  
> 
> I'm using start values parsed in from another (previous, but not shown)
> model fit. In this case, 
> 
>  
> 
> 
>>a.st
> 
> 
> [1] 0.3812922
> 
> 
>>k.st
> 
> 
> [1] 64.66529
> 
>  
> 
> I happen to know the true values for the optimised parameters (from
> another application), but I can't get nls to reproduce them. They are:
> 
>  
> 
> a = 2.0466
> 
> k = 60.8275
> 
> theta = 0.2277
> 
>  
> 
> Any ideas?
> 
>  
> 
> Regards,
> 
> Corey Bradshaw
> 

It is possible to fit this model to these data using nls as shown in the 
enclosed transcript.  You have one conditionally linear parameter ('a') 
in the model so I used the plinear algorithm and I also generated 
analytic derivatives for the tls function using the deriv function.

There are several things to note:

  - Your data are very noisy.  It is not surprising that it is difficult 
to fit a 3-parameter nonlinear model to such data.

  - The fitted model has negative values for both 'a' and 'theta'.

  - The estimates are highly imprecise.

 > summary(fm1)

Formula: r ~ tlm(N.fix, k, theta)

Parameters:
       Estimate Std. Error t value Pr(>|t|)
k      49.0724     6.9153   7.096 2.53e-06
theta  -4.6676     5.1805  -0.901    0.381
.lin   -0.2333     0.1685  -1.385    0.185

Residual standard error: 0.3662 on 16 degrees of freedom

Correlation of Parameter Estimates:
             k   theta
theta  0.8353
.lin  -0.3458 -0.7104
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: nls.Rout
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050223/a9055f2d/nls.pl

From D.R.J.Pleydell at pgr.salford.ac.uk  Wed Feb 23 18:53:01 2005
From: D.R.J.Pleydell at pgr.salford.ac.uk (David Pleydell)
Date: Wed, 23 Feb 2005 17:53:01 +0000
Subject: [R] corCompSymm in nlme package
Message-ID: <1109181181.421cc2fd02921@webmail.salford.ac.uk>

We are trying to use the corCompSymm function in nlme

The example from the help pages for the corAR1 function gives the following

> corAR1(0.2, form = ~ 1 | Mare)
Correlation structure of class corAR1 representing
Phi
0.2

We are expecting a somewhat similar correlation specification with the help page
example for corCompSymm, but just get an error instead

> corCompSymm(0.5, form = ~ 1 | Subject)
Error in "names<-.default"(`*tmp*`, value = "Rho") :
        names attribute [1] must be the same length as the vector [0]

this error is concistent in R1.8 and R2.0 on windows XP and also R2.0.1 on
Redhat

Thanks in advance for any help
Dave and Jerome

----------------------------------------------------------------
Concerns about content should be sent to abuse at salford.ac.uk



From ripley at stats.ox.ac.uk  Wed Feb 23 18:55:52 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 23 Feb 2005 17:55:52 +0000 (GMT)
Subject: RODBC works as documented (was [R] RODBC type conversion bug)
In-Reply-To: <3f87cc6d050223083027b87f61@mail.gmail.com>
References: <3f87cc6d050223083027b87f61@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0502231743350.9922@gannet.stats>

On Wed, 23 Feb 2005, Omar Lakkis wrote:

> I run R 2.0.1 on Debian and connect to Informix database via RODBC. In
> the table below the column "month" is of type char(1). RODBC seems to
> be converting this column to boolean if the value is F or T.

Sounds reasonable.  So would read.table, and that equally is not a bug but 
as documented.

> This is the data in my table:
>
>       id month year
>      25         F 2005
>      26         Z 2005
>
> When I select * for id 25 I get
> 25 FALSE 2005
>
> When I select * for id 16 I get
> 26    Z 2005
>
> Is there a fix for this issue?

Yes, for you to read the help page (as the posting guide asks you to do 
before posting).

See ?sqlGetResults and its description of 'as.is'.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From efg at stowers-institute.org  Wed Feb 23 19:01:45 2005
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Wed, 23 Feb 2005 12:01:45 -0600
Subject: [R] Problem saving logic regression result equation to disk file
Message-ID: <cvig76$3jq$1@sea.gmane.org>

I want to get some "simple" logic regression examples to work before
exploring a hard problem.

I can get results, but I'm having some problems using "cat" to save the
logic regression equation to a disk file.

Consider this:

# Simple Logic Regression Example
# efg, 23 Feb 2005

library(LogicReg)

# Create simulated data with known logic equation:

# "noise" logic matrix
X <- matrix(as.numeric(runif(160) < 0.5), 20,8)
colnames(X) <- paste("X", 1:ncol(X), sep="")
rownames(X) <- paste("case", 1:nrow(X), sep="")

# Define expected result:  Y = (NOT X2) AND X6
Y <- as.numeric(!X[,2] & X[,6])

# set seed for reproducible test
set.seed(19937)

# 100 interations too few:  some results in single node with |Parameter| < 1
Annealing <- logreg.anneal.control(start = -1, end = -4, iter = 500, update
= 50)

logicfit <- logreg(resp=Y, bin=X,
                   type = REGRESSION.TYPE<-2,
                   select = FIT.SINGLE.MODEL<-1,
                   ntrees=1,
                   nleaves=2,   # force shape of final tree
                   anneal.control=Annealing)

# I don't always want to see the plot
plot(logicfit)

# I'd like to write my regression equation to a file and
# then run many times to test my parameter selection
# with a known case before exploring unknown cases

logicfit

# In this case I want either of these equivalent answers
# (equivalent via DeMorgan's Theorem), and no others,
# such as single node results.

I want to run this say 100s (later 1000s) of times and look at the variation
in the results.  I want to figure out what parameters I should use so I only
see these results:

score 0
 +1 * (X6 and (not X2))


 -1 * ((not X6) or X2)


# I can't use cat to write this model to a file:
> cat(logicfit)
Error in cat(list(...), file, sep, fill, labels, append) :
        argument 1 not yet handled by cat


> summary(logicfit)
               Length Class       Mode
nsample          1    -none-      numeric
nbinary          1    -none-      numeric
nseparate        1    -none-      numeric
type             1    -none-      character
select           1    -none-      character
anneal.control   5    -none-      list
tree.control     4    -none-      list
seed             1    -none-      numeric
choice           1    -none-      numeric
nleaves          1    -none-      numeric
ntrees           1    -none-      numeric
penalty          1    -none-      numeric
response        20    -none-      numeric
binary         160    -none-      numeric
separate         1    -none-      numeric
censor          20    -none-      numeric
weight          20    -none-      numeric
model            5    logregmodel list
call             8    -none-      call

# Just the logicfit$model would be good enough but I can't "cat" that
either:

> logicfit$model
 +1 * (X6 and (not X2))
> cat(logicfit$model)
Error in cat(list(...), file, sep, fill, labels, append) :
        argument 1 not yet handled by cat

Using sink to get this result seems to be a huge kludge:
> sink("saveresults.txt")
> logicfit$model
> sink()
> results <- readLines("saveresults.txt")
> results
[1] " +1 * (X6 and (not X2))"

# FINALLY something I could write this result to a file:.
> cat(results, "\n")
 +1 * (X6 and (not X2))


What is a simple way to get my logic regression equation as a string that I
can "cat"
without dealing with the internal data structures that are present here?

Thanks for any help with this.

efg
--
Earl F. Glynn
Scientific Programmer
Bioinformatics Department
Stowers Institute for Medical Research



From ripley at stats.ox.ac.uk  Wed Feb 23 19:05:28 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 23 Feb 2005 18:05:28 +0000 (GMT)
Subject: [R] stopping a function
In-Reply-To: <OF215F321E.666B9719-ON87256FB1.005DB9CC-87256FB1.005E4583@usgs.gov>
References: <OF215F321E.666B9719-ON87256FB1.005DB9CC-87256FB1.005E4583@usgs.gov>
Message-ID: <Pine.LNX.4.61.0502231759490.9922@gannet.stats>

On Wed, 23 Feb 2005, Brian S Cade wrote:

> I've looked for this information in all the R help sources I could find and
> found nothing.  Is it possible to use some function key to stop the
> execution of some R command without ending the R session (Windows, R 1.91)?

R 1.91 is not expected until 2050, if ever.

> I've several times started functions that for various reasons are not
> executing properly and it would be nice to stop them without killing the R
> session.  I've been using taskmgr to end the R session but I then lose all
> objects created during the session.

It's in the README, and for RGui, on the menus!

Esc in RGui, Ctrl-c in Rterm.

Not all computations are interruptible (most compiled code is not), but if 
they are, these will do it.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From elvis at xlsolutions-corp.com  Wed Feb 23 19:15:05 2005
From: elvis at xlsolutions-corp.com (elvis@xlsolutions-corp.com)
Date: Wed, 23 Feb 2005 11:15:05 -0700
Subject: [R] Course***R/S System: Advanced Programming, Washington, DC***
Message-ID: <20050223181505.25416.qmail@webmail11.prod.mesa1.secureserver.net>

XSolutions Corp (www.xlsolutions-corp.com) is proud to announce
our "Advanced R/Splus programming" course taught by R Development
Core Team Guru!

www.xlsolutions-corp.com/Radv.htm

*********Washington, DC -------- April 14th-15th, 2005

*********Boston, MA    ----------  TBD

Early-bird discount ends March 15th!
Ask for group discount and reserve your seat Now  (payment due after
the
class)

Email Sue Turner: sue at xlsolutions-corp.com
Phone: 206-686-1578


Course Outline:

- Overview of R/S fundamentals: Syntax and Semantics
- Class and Inheritance in R/S-Plus
- Concepts, Construction and good use of language objects
- Coercion and efficiency
- Object-oriented programming in R and S-Plus
- Advanced manipulation tools: Parse, Deparse, Substitute, etc.
- How to fully take advantage of Vectorization
- Generic and Method Functions; S4 (S-Plus 6)
- Search path, databases and frames Visibility
- Working with large objects
- Handling Properly Recursion and iterative calculations
- Managing loops; For (S-Plus) and for() loops
- Consequences of Lazy Evaluation
- Efficient Code practices for large computations
- Memory management and Resource monitoring
- Writing R/S-Plus functions to call compiled code
- Writing and debugging compiled code for R/S-Plus system
- Connecting R/S-Plus to External Data Sources
- Understanding the structure of model fitting functions in R/S-Plus
- Designing and Packaging efficiently a new model function

It'll also deal with lots of S-Plus efficiency issues and any special
topics
from participants is welcome.

Please let us know if you and your colleagues are interested in this
class
to take advantage of group discount. Over half of the seats in this
class
are currently reserved.  Register now to secure your seat in this
course!

Cheers,

Elvis Miller, PhD
Manager Training.
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com



From andy_liaw at merck.com  Wed Feb 23 19:25:43 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 23 Feb 2005 13:25:43 -0500
Subject: [R] stopping a function
Message-ID: <3A822319EB35174CA3714066D590DCD50994E773@usrymx25.merck.com>

In Rgui, use the menu `Help' -> `Console' and look at the third line from
the bottom.

Cheers,
Andy

> From: Brian S Cade
> 
> I've looked for this information in all the R help sources I 
> could find and
> found nothing.  Is it possible to use some function key to stop the
> execution of some R command without ending the R session 
> (Windows, R 1.91)?
> I've several times started functions that for various reasons are not
> executing properly and it would be nice to stop them without 
> killing the R
> session.  I've been using taskmgr to end the R session but I 
> then lose all
> objects created during the session.
> 
> Brian
> 
> Brian S. Cade
> 
> U. S. Geological Survey
> Fort Collins Science Center
> 2150 Centre Ave., Bldg. C
> Fort Collins, CO  80526-8818
> 
> email:  brian_cade at usgs.gov
> tel:  970 226-9326
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From bates at stat.wisc.edu  Wed Feb 23 19:28:12 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 23 Feb 2005 12:28:12 -0600
Subject: [R] How to conctruct an inner grouping for nlme random statement?
In-Reply-To: <20050223060521.21821.qmail@web51710.mail.yahoo.com>
References: <20050223060521.21821.qmail@web51710.mail.yahoo.com>
Message-ID: <421CCB3C.4050404@stat.wisc.edu>

JJ wrote:
> Hello.  Im hoping someone can help with a grouping
> question related to the "random=" statement within the
> nlme function.  How do you specify that some grouping
> levels are inner to others?  I tried several things,
> given below.
> 
> Lets say I have a data frame with five variables,
> resp, cov1, ran1, ran2, group1, and group 2.  The
> formula is resp~cov1 + ran1 + ran2, where the ran are
> random variables.  The data is of length 80, and there
> are 4 unique factors in group1 and 20 unique factors
> in group2.  These are factors related to ran1 and
> ran2, respectively.  
> 
> The difficult part is that I want to estimate only 4
> random variables for ran1|group1 and the full 20 for
> ran2|group2.  I have tried many ways, and I cannot
> find a way to do this.  Is there a way?  Can someone
> suggest a code snippet?  
> 
> First I tried making the data frame a groupedData
> object, so that group2 is inner to group1, as it
> should be.  Then I used the statement: random =
> as.formula(ran1+ran2~1).  But this produced 20
> estimates for both ran1 and ran2.
> 
> I have also tried it without the data frame as a
> groupedData object, using the following:
> random = list(group1= c(ran1~1, group2=ran2~1)).  But
> this gave only 4 estimates for ran2.  I also tried: 
> random = list(c(group1= ran1~1, group2=ran2~1)), but
> this just gave a parse error message.
> 
> Any suggestions would be greatly appreciated.  Is it
> even possible to do what I want to do?  John

I think we will need a bit more information before we are able to help. 
  I'm not sure what you mean by ran1 and ran2 being random variables and 
group1 and group2 being factors related to ran1 and ran2.

The lmer function in the lme4 package allows for specification of a 
mixed-effects model using grouping factors and model matrices.  The 
model matrix determines the form of the random effects vector 
corresponding to each of the groups.  The grouping factor determines the 
groups.

In this specification I'm not sure what your ran1 and ran2 variables 
would be.  It sounds as if you want a model that would be specified as

  fm1 <- lmer(resp ~ cov1 + (1|group1) + (1|group2))

but I'm not sure.

Feel free to correspond directly with me if you wish.



From tom_colson at ncsu.edu  Wed Feb 23 19:42:26 2005
From: tom_colson at ncsu.edu (Tom Colson)
Date: Wed, 23 Feb 2005 13:42:26 -0500
Subject: [R] BLAS or ATLAS?
In-Reply-To: <20050218005632.GT22446@hortresearch.co.nz>
References: <OF80B79DC7.07A98AF5-ON86256FAB.00724618-86256FAB.0072E125@mdacc
	.tmc.edu> <20050218005632.GT22446@hortresearch.co.nz>
Message-ID: <1109184146.7051.7.camel@dd>

I'm trying to tweak multi-threading on a multiprocessor box(Xeon).
Reading appendix a.2.2 in the install and admin menu, I see that I can
use the internal BLAS lib and/or a multi-thread version of ATLAS (Where
to get?)

Question: Intel claims their libraries are faster than ATLAS. Has anyone
compiled R against the Intel BLAS? If so...how did you do it? 

http://developer.intel.com/software/products/mkl/features/lin_alg.htm

And I'm in the market for a pre-built Atlas dual processor lib for Xeon.
Know where I can find one? 
Thanks. 


-- 
Tom Colson
Center for Earth Observation
North Carolina State University
Raleigh, NC 27695
(919) 515 3434
(919) 673 8023
tom_colson at ncsu.edu

Online Calendar:
http://www4.ncsu.edu/~tpcolson



From d.firth at warwick.ac.uk  Wed Feb 23 20:18:48 2005
From: d.firth at warwick.ac.uk (David Firth)
Date: Wed, 23 Feb 2005 19:18:48 +0000
Subject: [R] model.matrix for a factor effect with no intercept
In-Reply-To: <Pine.LNX.4.61.0502231537240.8552@gannet.stats>
References: <feded8c0d3f579a03db1c2543580147d@warwick.ac.uk>
	<Pine.LNX.4.61.0502231537240.8552@gannet.stats>
Message-ID: <86a8ba32a3e27f864b6996cfb9784780@warwick.ac.uk>

Brian, many thanks for these helpful pointers:

On 23 Feb 2005, at 15:45, Prof Brian Ripley wrote:

> MASS4 p.150
> White Book p.38
>
> Those are the only two reasonably comprehensive accounts that I am 
> aware of (and they have only partial overlap).
>
> The underlying motivation is to span the _additional_ vector space 
> covered by the term, the complement to what has gone before.  Put 
> another way, as each term is added, only enough columns are added to 
> the model matrix to span the same space as if dummy coding had been 
> used for that term and its predecessors.  So think of this as a way to 
> produce a parsimonious (usually full-rank) basis for the model space.

Yes, indeed.  My surprise was that *this* particular basis (dummy 
coding) was the one used here.  I should have got a clue from what 
contrasts() does, and is documented to do,

 > options("contrasts")
$contrasts
[1] "contr.treatment" "contr.poly"
 > contrasts(a)
                 .L         .Q
[1,] -7.071068e-01  0.4082483
[2,] -9.073800e-17 -0.8164966
[3,]  7.071068e-01  0.4082483

but when there's no intercept in the model the contrasts used appear to 
be

 > contrasts(a, contrasts = FALSE)
    -1 0 1
-1  1 0 0
0   0 1 0
1   0 0 1

which are not the same as

 > contr.poly(a, contrasts = FALSE)
      ^0            ^1         ^2
[1,]  1 -7.071068e-01  0.4082483
[2,]  1 -9.073800e-17 -0.8164966
[3,]  1  7.071068e-01  0.4082483

-- which is what I had naively expected to get in my model matrix. 

This is of course all a matter of convention.  The present convention 
does seem a touch confusing though: the basis for the space spanned by 
a factor is determined by options("contrasts") or by the contrasts 
attribute of the factor or by the contrasts argument in the call, 
*except* when there's no intercept or other factor earlier in the model 
in which case all such settings are ignored (well, not *quite* ignored: 
they do get put in the contrasts attribute of the resultant model 
matrix).

On the other hand, one good reason to use dummy coding for the 
first-encountered factor when there's no intercept is that the 
associated parameters are then often interpretable as group-specific 
intercepts.

Would it be an improvement, though, if the "contrasts" attribute of the 
resultant model matrix contained "contr.treatment" in such cases 
instead of the name of a contrast function that was not actually used?

Best wishes,
David


>
> On Wed, 23 Feb 2005, David Firth wrote:
>
>> I was surprised by this (in R 2.0.1):
>>
>>> a <- ordered(-1:1)
>>> a
>> [1] -1 0  1
>> Levels: -1 < 0 < 1
>>
>>> model.matrix(~ a)
>>  (Intercept)           a.L        a.Q
>> 1           1 -7.071068e-01  0.4082483
>> 2           1 -9.073800e-17 -0.8164966
>> 3           1  7.071068e-01  0.4082483
>> attr(,"assign")
>> [1] 0 1 1
>> attr(,"contrasts")
>> attr(,"contrasts")$a
>> [1] "contr.poly"
>>
>>> model.matrix(~ -1 + a)
>>  a-1 a0 a1
>> 1   1  0  0
>> 2   0  1  0
>> 3   0  0  1
>> attr(,"assign")
>> [1] 1 1 1
>> attr(,"contrasts")
>> attr(,"contrasts")$a
>> [1] "contr.poly"
>>
>> Without the intercept, treatment contrasts seem to have been used 
>> (this despite the "contr.poly" in the "contrasts" attribute).
>>
>> It's not restricted to ordered factors.  For example, if Helmert 
>> contrasts are used for nominal factors, the same sort of thing 
>> happens.
>>
>> I suppose it is a deliberate feature (perhaps to protect the user 
>> from accidentally fitting models that make no sense?  or maybe some 
>> better reason?) -- is it explained somewhere?
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Feb 23 20:25:00 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 23 Feb 2005 19:25:00 +0000 (GMT)
Subject: [R] BLAS or ATLAS?
In-Reply-To: <1109184146.7051.7.camel@dd>
References: <OF80B79DC7.07A98AF5-ON86256FAB.00724618-86256FAB.0072E125@mdacc
	.tmc.edu> <20050218005632.GT22446@hortresearch.co.nz>
	<1109184146.7051.7.camel@dd>
Message-ID: <Pine.LNX.4.61.0502231918120.10820@gannet.stats>

What OS is this for?

Several OSes (Windows, Linux, Solaris, FreeBSD ...) run on Xeons (and 
there are several sorts of Xeons, some even EM64T AMD-clones).

On Wed, 23 Feb 2005, Tom Colson wrote:

> I'm trying to tweak multi-threading on a multiprocessor box(Xeon).
> Reading appendix a.2.2 in the install and admin menu, I see that I can
> use the internal BLAS lib and/or a multi-thread version of ATLAS (Where
> to get?)

The last _is_ described in Appendix A.2.2, together with several other 
possibilities.

> Question: Intel claims their libraries are faster than ATLAS. Has anyone
> compiled R against the Intel BLAS? If so...how did you do it?
>
> http://developer.intel.com/software/products/mkl/features/lin_alg.htm

That _is_ described in Appendix A.2.2.

> And I'm in the market for a pre-built Atlas dual processor lib for Xeon.
> Know where I can find one?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tom_colson at ncsu.edu  Wed Feb 23 20:28:21 2005
From: tom_colson at ncsu.edu (Tom Colson)
Date: Wed, 23 Feb 2005 14:28:21 -0500
Subject: [R] BLAS or ATLAS?
In-Reply-To: <Pine.LNX.4.61.0502231918120.10820@gannet.stats>
References: <OF80B79DC7.07A98AF5-ON86256FAB.00724618-86256FAB.0072E125@mdacc
	.tmc.edu> <20050218005632.GT22446@hortresearch.co.nz>
	<1109184146.7051.7.camel@dd>
	<Pine.LNX.4.61.0502231918120.10820@gannet.stats>
Message-ID: <1109186901.7051.10.camel@dd>

Fedora Core 3

I installed the Intel MLk, 

and tried : --with-blas="-lmkl -lguide -lpthread" 

and got :  External libraries:        readline, BLAS(generic)

thus I'm assuming I'm missing something when telling config where to
look for the recently installed Intel BLAS?

thanks for all replies thus far. 


On Wed, 2005-02-23 at 19:25 +0000, Prof Brian Ripley wrote:
> What OS is this for?
> 
> Several OSes (Windows, Linux, Solaris, FreeBSD ...) run on Xeons (and 
> there are several sorts of Xeons, some even EM64T AMD-clones).
> 
> On Wed, 23 Feb 2005, Tom Colson wrote:
> 
> > I'm trying to tweak multi-threading on a multiprocessor box(Xeon).
> > Reading appendix a.2.2 in the install and admin menu, I see that I can
> > use the internal BLAS lib and/or a multi-thread version of ATLAS (Where
> > to get?)
> 
> The last _is_ described in Appendix A.2.2, together with several other 
> possibilities.
> 
> > Question: Intel claims their libraries are faster than ATLAS. Has anyone
> > compiled R against the Intel BLAS? If so...how did you do it?
> >
> > http://developer.intel.com/software/products/mkl/features/lin_alg.htm
> 
> That _is_ described in Appendix A.2.2.
> 
> > And I'm in the market for a pre-built Atlas dual processor lib for Xeon.
> > Know where I can find one?
>



From andy_liaw at merck.com  Wed Feb 23 20:45:25 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 23 Feb 2005 14:45:25 -0500
Subject: [R] BLAS or ATLAS?
Message-ID: <3A822319EB35174CA3714066D590DCD50994E77A@usrymx25.merck.com>

I believe that actually means configure was able to use the BLAS you
specified (MKL in this case).  You can scan through config.log to make sure.

Andy

> From: 
> 
> Fedora Core 3
> 
> I installed the Intel MLk, 
> 
> and tried : --with-blas="-lmkl -lguide -lpthread" 
> 
> and got :  External libraries:        readline, BLAS(generic)
> 
> thus I'm assuming I'm missing something when telling config where to
> look for the recently installed Intel BLAS?
> 
> thanks for all replies thus far. 
> 
> 
> On Wed, 2005-02-23 at 19:25 +0000, Prof Brian Ripley wrote:
> > What OS is this for?
> > 
> > Several OSes (Windows, Linux, Solaris, FreeBSD ...) run on 
> Xeons (and 
> > there are several sorts of Xeons, some even EM64T AMD-clones).
> > 
> > On Wed, 23 Feb 2005, Tom Colson wrote:
> > 
> > > I'm trying to tweak multi-threading on a multiprocessor box(Xeon).
> > > Reading appendix a.2.2 in the install and admin menu, I 
> see that I can
> > > use the internal BLAS lib and/or a multi-thread version 
> of ATLAS (Where
> > > to get?)
> > 
> > The last _is_ described in Appendix A.2.2, together with 
> several other 
> > possibilities.
> > 
> > > Question: Intel claims their libraries are faster than 
> ATLAS. Has anyone
> > > compiled R against the Intel BLAS? If so...how did you do it?
> > >
> > > 
> http://developer.intel.com/software/products/m> kl/features/lin_alg.htm
> > 
> > That _is_ described in Appendix A.2.2.
> > 
> > > And I'm in the market for a pre-built Atlas dual 
> processor lib for Xeon.
> > > Know where I can find one?
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From tom_colson at ncsu.edu  Wed Feb 23 21:04:20 2005
From: tom_colson at ncsu.edu (Tom Colson)
Date: Wed, 23 Feb 2005 15:04:20 -0500
Subject: [R] BLAS or ATLAS?
In-Reply-To: <421CDD6F.7070401@stat.wisc.edu>
References: <OF80B79DC7.07A98AF5-ON86256FAB.00724618-86256FAB.0072E125@mdacc	.tmc.edu>
	<20050218005632.GT22446@hortresearch.co.nz>	<1109184146.7051.7.camel@dd>
	<Pine.LNX.4.61.0502231918120.10820@gannet.stats>
	<1109186901.7051.10.camel@dd>  <421CDD6F.7070401@stat.wisc.edu>
Message-ID: <1109189060.21220.2.camel@dd>

In makeconf I get
BLAS_LIBS = -lblas

I believe that actually means configure was able to use the BLAS you
specified (MKL in this case).  You can scan through config.log to make
sure.
in config.log....can't seem to find any reference to the Intel BLAS.

Thanks for all the replies. 


On Wed, 2005-02-23 at 13:45 -0600, Douglas Bates wrote:
> Tom Colson wrote:
> > Fedora Core 3
> > 
> > I installed the Intel MLk, 
> > 
> > and tried : --with-blas="-lmkl -lguide -lpthread" 
> > 
> > and got :  External libraries:        readline, BLAS(generic)
> > 
> > thus I'm assuming I'm missing something when telling config where to
> > look for the recently installed Intel BLAS?
> 
> That's what you would expect to see in the summary section from 
> configure.  Look back in the configure output to see exactly what it 
> reported for the blas.  You may also check the Makeconf file that was 
> generated to see what it defines for BLAS_LIBS
>



From canty at math.mcmaster.ca  Wed Feb 23 21:10:54 2005
From: canty at math.mcmaster.ca (Angelo Canty)
Date: Wed, 23 Feb 2005 15:10:54 -0500 (EST)
Subject: [R] bias of a boot statistic
In-Reply-To: <1109155325.421c5dfd0d9a9@webmail.unibas.ch>
Message-ID: <Pine.LNX.4.44.0502231506330.29988-100000@mathserv2.math.mcmaster.ca>

The bias and standard error are calculated in the print.boot function and
are not saved anywhere.  It is very easy to calculate, however, as the
mean of the replicate statistics minus the original statistic.

apply(boot.out$t,2,mean)-boot.out$t0

HTH,
Angelo

On Wed, 23 Feb 2005, K. Steinmann wrote:

> Question:
> How can I get access to the bias value of a boot statistic?
> 
> Details:
> Boot function:
>  boot(data, statistic, R, sim="ordinary", stype="i",
>            strata=rep(1,n), L=NULL, m=0, weights=NULL,
>            ran.gen=function(d, p) d, mle=NULL, ...)
> 
> When I create an object, containing the bootstrap statistic (object <- boot
> (....))I can call it and will get an output with t, bias and standarderror as
> follows:
> Bootstrap Statistics :
>     original  bias    std. error
> t1*     5.65    0.01   0.9134185
> 
> My question is now, where is the value of the bias stored? How can I get access
> to this value to do further caluculations?
> 
> Thanks,
> K. St.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
------------------------------------------------------------------
|   Angelo J. Canty                Email: cantya at mcmaster.ca     |
|   Mathematics and Statistics     Phone: (905) 525-9140 x 27079 |
|   McMaster University            Fax  : (905) 522-0935         |
|   1280 Main St. W.                                             |
|   Hamilton ON L8S 4K1                                          |



From Terji78 at yahoo.com  Wed Feb 23 21:16:49 2005
From: Terji78 at yahoo.com (T Petersen)
Date: Wed, 23 Feb 2005 21:16:49 +0100
Subject: [R] Solving systems of non-linear equations in R
In-Reply-To: <421C0FD1.8090008@pdf.com>
References: <421BD19E.1090309@yahoo.com> <421BEED8.8060509@fbc.keio.ac.jp>
	<421C01A5.4030306@yahoo.com> <421C0FD1.8090008@pdf.com>
Message-ID: <421CE4B1.6090702@yahoo.com>

Thank you, that was very helpful. My functions are in general monotonic, 
continous and differentiable(one exception sometimes encountered being y 
= min(a*x1,b*x2)) and do have a unique solution, if you specify the 
problem correctly.
I have never worked with non-liner solving algoritms in my math courses, 
so maybe starting to setup algoritms in R may take the focus away from 
my thesis subject. On the other hand it could be fun :-D
Maybe I should check out the optim()-function again. You can solve 
systems with more tha one equations with optim()?

regards

Spencer Graves wrote:

>      A system of n equations in n unknowns has a unique solution if 
> the n equations are linear and linearly independent.  If the system is 
> nonlinear, then one must characterize the nonlinearity before saying 
> anything about whether a solution exists and if so how many solutions 
> are there?
>      Example 1:  Solve sin(x)=0 for x.  Answer:  x = 2*n*pi, for n = 
> any integer.
>      Example 2:  Solve sin(x) = 2 for x.  Answer:  If x must be a real 
> number, then this equation has no solutions.
>      Are your functions monotonic?  Continuous?  Differentiable?
>      Without getting into pathologies like the Cantor function (e.g., 
> http://www.cut-the-knot.org/do_you_know/cantor.shtml), my experience 
> with a variety of practical problem like this suggests that it is best 
> to recast the problem as one of minimizing, e.g., the sum of squared 
> deviations from target.  Moreover, I've had good luck transforming the 
> parameter space to eliminate constraints -- or incorporating the 
> constraints into the objective function and then solving the 
> superficially unconstrained problem.  If my functions have 
> singularities where I might get 0/0 or Inf-Inf, for example, I use 
> asymptotic expansions to "approximate" the function(s) near the 
> singularities more accurately than can be achieved with any 
> finite-precision arithmetic.
>      With all of this, I'm confident that there are better algorithms 
> than the different methods in "optim", but I don't have not had the 
> need to hunt for them.  The methods in "optim" provide a reasonable 
> range of options for the problems I've encountered.
>      The R project has another advantage over a commercial software:  
> You can see the source code.  You can trace it step by step and find 
> out where it does not work well for the specific problems you 
> consider.  If you're clever, you might be able to find a way to 
> improve that algorithm and make it part of your thesis -- and get a 
> publication on it in some statistical software journal.  Where else 
> can you so easily climb up and stand on the shoulders of giants?  If 
> you find a platform for innovation better than R, please let me know.
>      hope this helps.      spencer graves   
> T Petersen wrote:
>
>> No, this doesn't seem right. What I look for is something that could 
>> solve nonlinear systems with n unknowns and n equations. So there 
>> will be zero degrees of freedom, and statistical methods can't be the 
>> right way forward.
>>
>> Specifically  I can see that the litterature mentions's "Scarf's 
>> algoritm" (Scarf 1967) and Merril's refinement of Scarf's algoritm in 
>> 1972, but there might be other algoritms too...
>>
>> Regards...TP
>>
>> yutaka hamaoka wrote:
>>
>>>
>>> I believe
>>> library(systemfit)
>>> has nlsytemfit function.
>>>
>>> Yh
>>>
>>>
>>> T Petersen wrote:
>>>
>>>> I'm about to write my thesis in economics and will need to setup 
>>>> and solve a system of non-linear equations. At our university we 
>>>> usually use GAMS for this, and though GAMS is a fine program, it 
>>>> bugs me a that I won't be able to run my code after I finish my 
>>>> thesis without buying a license for the program(about $3.500 :-(( )
>>>>
>>>> So I've looked around for NL-stuff for R, but I can't find 
>>>> anything. The closest thing appears to be optim(), but it doesn't 
>>>> seem to allow constraints(as in fn = constant) or equations 
>>>> systems. So, anyone knows if there is a method in R that you can 
>>>> use for this purpose?
>>>>
>>>> regards
>>>>
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide! 
>>>> http://www.R-project.org/posting-guide.html
>>>>
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
>
>
>



From andy_liaw at merck.com  Wed Feb 23 21:17:03 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 23 Feb 2005 15:17:03 -0500
Subject: [R] BLAS or ATLAS?
Message-ID: <3A822319EB35174CA3714066D590DCD50994E77D@usrymx25.merck.com>

Look for `sgemm' in config.log.  I believe that's how configure test for
usable BLAS.

It looks like configure found another system installed BLAS.

Andy

> From: Tom Colson
> 
> In makeconf I get
> BLAS_LIBS = -lblas
> 
> I believe that actually means configure was able to use the BLAS you
> specified (MKL in this case).  You can scan through config.log to make
> sure.
> in config.log....can't seem to find any reference to the Intel BLAS.
> 
> Thanks for all the replies. 
> 
> 
> On Wed, 2005-02-23 at 13:45 -0600, Douglas Bates wrote:
> > Tom Colson wrote:
> > > Fedora Core 3
> > > 
> > > I installed the Intel MLk, 
> > > 
> > > and tried : --with-blas="-lmkl -lguide -lpthread" 
> > > 
> > > and got :  External libraries:        readline, BLAS(generic)
> > > 
> > > thus I'm assuming I'm missing something when telling 
> config where to
> > > look for the recently installed Intel BLAS?
> > 
> > That's what you would expect to see in the summary section from 
> > configure.  Look back in the configure output to see 
> exactly what it 
> > reported for the blas.  You may also check the Makeconf 
> file that was 
> > generated to see what it defines for BLAS_LIBS
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From grover2160 at metrocast.net  Wed Feb 23 22:38:19 2005
From: grover2160 at metrocast.net (Chris)
Date: Wed, 23 Feb 2005 16:38:19 -0500
Subject: [R] how do I get means by factor?
Message-ID: <200502232141.j1NLfb34010326@hypatia.math.ethz.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050223/fe7fc750/attachment.pl

From bxc at steno.dk  Wed Feb 23 23:16:17 2005
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Wed, 23 Feb 2005 23:16:17 +0100
Subject: [R] how do I get means by factor?
Message-ID: <0ABD88905D18E347874E0FB71C0B29E9027FEBEA@exdkba022.novo.dk>

tapply( x, f, mean )

maybe with

tapply( x, f, mean, na.rm=T )

----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc
----------------------



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Chris
> Sent: Wednesday, February 23, 2005 10:38 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] how do I get means by factor?
> 
> 
> I want the mean of a response variable for each level of a 
> factor in a data.frame.  How is this done?
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Wed Feb 23 23:25:09 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 23 Feb 2005 22:25:09 +0000 (GMT)
Subject: [R] model.matrix for a factor effect with no intercept
In-Reply-To: <86a8ba32a3e27f864b6996cfb9784780@warwick.ac.uk>
References: <feded8c0d3f579a03db1c2543580147d@warwick.ac.uk>
	<Pine.LNX.4.61.0502231537240.8552@gannet.stats>
	<86a8ba32a3e27f864b6996cfb9784780@warwick.ac.uk>
Message-ID: <Pine.LNX.4.61.0502232110290.13374@gannet.stats>

David,

I am sorry, I did not read your question as `why dummy coding not poly 
coding', but I still seemed to have answered it.  BTW,

 	contr.poly(a, contrasts = FALSE)

should be an indicator matrix according to its help page, so I would not 
have expected what you expected, and if it had been, you might not have 
been surprised. [More on that below.]

I would say the algorithm was `code by the specified contrasts only if 
dummy coding is not parsimonious'.

Your final point isn't the whole story. If I have 0+a+b+a:b, R uses the 
contrasts for a in the interaction but not the main term.  So the 
attribute "contrasts" for 'a' represents

   `the contrast used for those terms (possibly none) in which it needed
    coding'.

Now, if there were none it might be better to say nothing, but that is not 
actually calculated explicitly.  I've just checked, and we do not document 
on the help page what it means.  I have just drafted

   If there are any factors in terms in the model, there is an attribute
   \code{"contrasts"}, a named list with an entry for each factor. This
   specifies the contrasts that would be used in terms in which the
   factor is coded by contrasts (in some terms dummy coding may be used),
   either as a character vector naming a function or as a numeric matrix.

As far as I recall the contrasts attribute is returned to be tacked on a 
fit and used later to reconstruct the model matrix, or perhaps a model 
matrix for another formula and the same data.  So I think it does need to 
be the contrasts as specified at the time of the model.matrix call.

It is probably helpful to note that it is not specifically treatment 
contrasts that are used but dummy coding: contr.sum and contr.helmert give 
the same matrix.

It is a bug that contr.poly(x, contrasts=FALSE) does not behave as 
documented (a bug shared with S, it seems).  I do dimly recall this having 
been raised in the past (probably by me), but cannot find any record as to 
why the discrepancy remains.  We should change the code or the 
documentation, probably the latter.

Brian


On Wed, 23 Feb 2005, David Firth wrote:

> Brian, many thanks for these helpful pointers:
>
> On 23 Feb 2005, at 15:45, Prof Brian Ripley wrote:
>
>> MASS4 p.150
>> White Book p.38
>> 
>> Those are the only two reasonably comprehensive accounts that I am aware of 
>> (and they have only partial overlap).
>> 
>> The underlying motivation is to span the _additional_ vector space covered 
>> by the term, the complement to what has gone before.  Put another way, as 
>> each term is added, only enough columns are added to the model matrix to 
>> span the same space as if dummy coding had been used for that term and its 
>> predecessors.  So think of this as a way to produce a parsimonious (usually 
>> full-rank) basis for the model space.
>
> Yes, indeed.  My surprise was that *this* particular basis (dummy coding) was 
> the one used here.  I should have got a clue from what contrasts() does, and 
> is documented to do,
>
>> options("contrasts")
> $contrasts
> [1] "contr.treatment" "contr.poly"
>> contrasts(a)
>                .L         .Q
> [1,] -7.071068e-01  0.4082483
> [2,] -9.073800e-17 -0.8164966
> [3,]  7.071068e-01  0.4082483
>
> but when there's no intercept in the model the contrasts used appear to be
>
>> contrasts(a, contrasts = FALSE)
>   -1 0 1
> -1  1 0 0
> 0   0 1 0
> 1   0 0 1
>
> which are not the same as
>
>> contr.poly(a, contrasts = FALSE)
>     ^0            ^1         ^2
> [1,]  1 -7.071068e-01  0.4082483
> [2,]  1 -9.073800e-17 -0.8164966
> [3,]  1  7.071068e-01  0.4082483
>
> -- which is what I had naively expected to get in my model matrix. 
> This is of course all a matter of convention.  The present convention does 
> seem a touch confusing though: the basis for the space spanned by a factor is 
> determined by options("contrasts") or by the contrasts attribute of the 
> factor or by the contrasts argument in the call, *except* when there's no 
> intercept or other factor earlier in the model in which case all such 
> settings are ignored (well, not *quite* ignored: they do get put in the 
> contrasts attribute of the resultant model matrix).
>
> On the other hand, one good reason to use dummy coding for the 
> first-encountered factor when there's no intercept is that the associated 
> parameters are then often interpretable as group-specific intercepts.
>
> Would it be an improvement, though, if the "contrasts" attribute of the 
> resultant model matrix contained "contr.treatment" in such cases instead of 
> the name of a contrast function that was not actually used?
>
> Best wishes,
> David
>
>
>> 
>> On Wed, 23 Feb 2005, David Firth wrote:
>> 
>>> I was surprised by this (in R 2.0.1):
>>> 
>>>> a <- ordered(-1:1)
>>>> a
>>> [1] -1 0  1
>>> Levels: -1 < 0 < 1
>>> 
>>>> model.matrix(~ a)
>>>  (Intercept)           a.L        a.Q
>>> 1           1 -7.071068e-01  0.4082483
>>> 2           1 -9.073800e-17 -0.8164966
>>> 3           1  7.071068e-01  0.4082483
>>> attr(,"assign")
>>> [1] 0 1 1
>>> attr(,"contrasts")
>>> attr(,"contrasts")$a
>>> [1] "contr.poly"
>>> 
>>>> model.matrix(~ -1 + a)
>>>  a-1 a0 a1
>>> 1   1  0  0
>>> 2   0  1  0
>>> 3   0  0  1
>>> attr(,"assign")
>>> [1] 1 1 1
>>> attr(,"contrasts")
>>> attr(,"contrasts")$a
>>> [1] "contr.poly"
>>> 
>>> Without the intercept, treatment contrasts seem to have been used (this 
>>> despite the "contr.poly" in the "contrasts" attribute).
>>> 
>>> It's not restricted to ordered factors.  For example, if Helmert contrasts 
>>> are used for nominal factors, the same sort of thing happens.
>>> 
>>> I suppose it is a deliberate feature (perhaps to protect the user from 
>>> accidentally fitting models that make no sense?  or maybe some better 
>>> reason?) -- is it explained somewhere?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From h.n.lim at tbm.tudelft.nl  Wed Feb 23 23:26:11 2005
From: h.n.lim at tbm.tudelft.nl (Lim,  Jasper H. N.)
Date: Wed, 23 Feb 2005 23:26:11 +0100
Subject: [R] Pls unsubscribe me from this list.
Message-ID: <8E093D9CB10CA848933C342946CC91DF6A4BD2@TUDELFT3V5.tud.tudelft.nl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050223/6955d65e/attachment.pl

From rif at MIT.EDU  Wed Feb 23 23:42:45 2005
From: rif at MIT.EDU (rif)
Date: Wed, 23 Feb 2005 17:42:45 -0500
Subject: [R] basic question about changing limits on generated plots
Message-ID: <200502232242.j1NMgjAq027683@five-percent-nation.mit.edu>


Is it possible to change the limits on plots that are already on the
screen?  In particular, is there any R equivalent to the sequence of
matlab commands

plot(1:10,1:10)
hold on
plot(2:12,5:15)

I know I can use points and lines to add points and lines to plots,
but the limits of the plot do not change when I do this.

Looking at various examples, it seems that the answer is "no", but I
wanted to make sure.  This seems to make exploratory data analysis
somewhat more challenging.

It also seems like it would be plausible to write a package on top of
the standard graphics functions that keeps track of what you've done
and automatically replots.  Which makes me think, has someone already
done this?

Cheers,

rif



From p.dalgaard at biostat.ku.dk  Thu Feb 24 00:06:04 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 24 Feb 2005 00:06:04 +0100
Subject: [R] Pls unsubscribe me from this list.
In-Reply-To: <8E093D9CB10CA848933C342946CC91DF6A4BD2@TUDELFT3V5.tud.tudelft.nl>
References: <8E093D9CB10CA848933C342946CC91DF6A4BD2@TUDELFT3V5.tud.tudelft.nl>
Message-ID: <x2mztuubab.fsf@biostat.ku.dk>

"Lim,  Jasper H. N." <h.n.lim at tbm.tudelft.nl> writes:

> Hi,
>  
> I have difficulty unsubscribing from this list. I have already
> unsubscribed from the website mailing list. But the messages keep
> flooding my mailbox. Pls help!
>  
> 
> 	[[alternative HTML version deleted]]

The mailing list manager is out of town for another couple of days.
The other two thousand recipients of your message cannot help you. If
you really cannot unsubscribe using the web interface, the list
manager's address is found on the same page. 

Please notice that unsubscribing does not dequeue messaged that have
already been sent to you so you may receive messages for some time
afterwards, even though you are no longer on the list.

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From spencer.graves at pdf.com  Thu Feb 24 00:14:58 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 23 Feb 2005 15:14:58 -0800
Subject: [R] Solving systems of non-linear equations in R
In-Reply-To: <421CE4B1.6090702@yahoo.com>
References: <421BD19E.1090309@yahoo.com>
	<421BEED8.8060509@fbc.keio.ac.jp>	<421C01A5.4030306@yahoo.com>
	<421C0FD1.8090008@pdf.com> <421CE4B1.6090702@yahoo.com>
Message-ID: <421D0E72.3060603@pdf.com>

      Write a driver function to compute the sum of squares of 
deviations from target.  If the nonlinear equations are not pathological 
and a unique solution exists, "optim" will find it.  If no solution 
exists, "optim" will find something close -- in terms of the sum of 
squared deviations.  If the functions have discontinuities, multiple 
solutions, or possibly other pathologies, I'd try all 4 methods (after 
transforming parameters to eliminate constraints).  You could also try 
to generate random starting points over a plausible region, evaluate 
your objective function at all the random starting points and feed the 
best one (or the best 10) to "optim". 

      How big is "n"?  If it's bigger than, say, half a dozen, I suspect 
you have a variance component situation, in which case, you would be 
wise to use nlme;  see Pinheiro and Bates (2000) Mixed-Effects Models in 
S and S-Plus (Springer).  Or try Scilab (http://scilabsoft.inria.fr). 

      hope this helps. 
      spencer graves

T Petersen wrote:

> Thank you, that was very helpful. My functions are in general 
> monotonic, continous and differentiable(one exception sometimes 
> encountered being y = min(a*x1,b*x2)) and do have a unique solution, 
> if you specify the problem correctly.
> I have never worked with non-liner solving algoritms in my math 
> courses, so maybe starting to setup algoritms in R may take the focus 
> away from my thesis subject. On the other hand it could be fun :-D
> Maybe I should check out the optim()-function again. You can solve 
> systems with more tha one equations with optim()?
>
> regards
>
> Spencer Graves wrote:
>
>>      A system of n equations in n unknowns has a unique solution if 
>> the n equations are linear and linearly independent.  If the system 
>> is nonlinear, then one must characterize the nonlinearity before 
>> saying anything about whether a solution exists and if so how many 
>> solutions are there?
>>      Example 1:  Solve sin(x)=0 for x.  Answer:  x = 2*n*pi, for n = 
>> any integer.
>>      Example 2:  Solve sin(x) = 2 for x.  Answer:  If x must be a 
>> real number, then this equation has no solutions.
>>      Are your functions monotonic?  Continuous?  Differentiable?
>>      Without getting into pathologies like the Cantor function (e.g., 
>> http://www.cut-the-knot.org/do_you_know/cantor.shtml), my experience 
>> with a variety of practical problem like this suggests that it is 
>> best to recast the problem as one of minimizing, e.g., the sum of 
>> squared deviations from target.  Moreover, I've had good luck 
>> transforming the parameter space to eliminate constraints -- or 
>> incorporating the constraints into the objective function and then 
>> solving the superficially unconstrained problem.  If my functions 
>> have singularities where I might get 0/0 or Inf-Inf, for example, I 
>> use asymptotic expansions to "approximate" the function(s) near the 
>> singularities more accurately than can be achieved with any 
>> finite-precision arithmetic.
>>      With all of this, I'm confident that there are better algorithms 
>> than the different methods in "optim", but I don't have not had the 
>> need to hunt for them.  The methods in "optim" provide a reasonable 
>> range of options for the problems I've encountered.
>>      The R project has another advantage over a commercial software:  
>> You can see the source code.  You can trace it step by step and find 
>> out where it does not work well for the specific problems you 
>> consider.  If you're clever, you might be able to find a way to 
>> improve that algorithm and make it part of your thesis -- and get a 
>> publication on it in some statistical software journal.  Where else 
>> can you so easily climb up and stand on the shoulders of giants?  If 
>> you find a platform for innovation better than R, please let me know.
>>      hope this helps.      spencer graves   T Petersen wrote:
>>
>>> No, this doesn't seem right. What I look for is something that could 
>>> solve nonlinear systems with n unknowns and n equations. So there 
>>> will be zero degrees of freedom, and statistical methods can't be 
>>> the right way forward.
>>>
>>> Specifically  I can see that the litterature mentions's "Scarf's 
>>> algoritm" (Scarf 1967) and Merril's refinement of Scarf's algoritm 
>>> in 1972, but there might be other algoritms too...
>>>
>>> Regards...TP
>>>
>>> yutaka hamaoka wrote:
>>>
>>>>
>>>> I believe
>>>> library(systemfit)
>>>> has nlsytemfit function.
>>>>
>>>> Yh
>>>>
>>>>
>>>> T Petersen wrote:
>>>>
>>>>> I'm about to write my thesis in economics and will need to setup 
>>>>> and solve a system of non-linear equations. At our university we 
>>>>> usually use GAMS for this, and though GAMS is a fine program, it 
>>>>> bugs me a that I won't be able to run my code after I finish my 
>>>>> thesis without buying a license for the program(about $3.500 :-(( )
>>>>>
>>>>> So I've looked around for NL-stuff for R, but I can't find 
>>>>> anything. The closest thing appears to be optim(), but it doesn't 
>>>>> seem to allow constraints(as in fn = constant) or equations 
>>>>> systems. So, anyone knows if there is a method in R that you can 
>>>>> use for this purpose?
>>>>>
>>>>> regards
>>>>>
>>>>> ______________________________________________
>>>>> R-help at stat.math.ethz.ch mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide! 
>>>>> http://www.R-project.org/posting-guide.html
>>>>>
>>>>
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide! 
>>>> http://www.R-project.org/posting-guide.html
>>>>
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>
>>
>>
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From lauraholt_983 at hotmail.com  Thu Feb 24 00:16:51 2005
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Wed, 23 Feb 2005 17:16:51 -0600
Subject: [R] and [ESS] Starting ESS
Message-ID: <BAY10-F53C55C4C4F154B31A1829DD6630@phx.gbl>

Dear R People:

I have finally seen the error of my ways and have decided to use ESS for R 
and S + stuff.

However, I have a question right from the beginning.  I'm somewhat confused 
by the installation instructions.

Do I install XEMACS or ESS first, please?

Windows XP
R Version 2.0.1
(S + 6.2)

Thanks so much!

Sincerely,
Laura Holt
mailto: lauraholt_983 at hotmail.com



From fls at fls.org.au  Thu Feb 24 00:08:28 2005
From: fls at fls.org.au (fls)
Date: Thu, 24 Feb 2005 10:08:28 +1100
Subject: [R] Getting tick positions
Message-ID: <200502241008.28553.fls@fls.org.au>

While writing a function that includes placing grid lines at the same position 
as the axis ticks, I found that the axis* functions don't return anything. 
Thus I have had to copy the appropriate function, removing the call to axis() 
and adding a line to return the tick positions. Is there a more elegant way 
to determine the tick positions on an axis? Thanks.

Jim

(normally bitwrit at ozemail.com.au - my modem is toast)



From ramasamy at cancer.org.uk  Thu Feb 24 01:01:35 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Thu, 24 Feb 2005 00:01:35 +0000
Subject: [R] Pls unsubscribe me from this list.
In-Reply-To: <x2mztuubab.fsf@biostat.ku.dk>
References: <8E093D9CB10CA848933C342946CC91DF6A4BD2@TUDELFT3V5.tud.tudelft.nl>
	<x2mztuubab.fsf@biostat.ku.dk>
Message-ID: <1109203295.5983.12.camel@dhcp-63.ccc.ox.ac.uk>

Two other possibilities come to my mind :

1) If memory serves, when you un-subscribe using the web interface, you
will be sent an email asking for confirmation. Till you click on the
link in that email, you will not be un-subscribed. Did you you receive
any such mail ? If not, you might want to check your junk mail folder or
spam settings.

2) You can disable the "Mail delivery" option under R-help subscription
options in web interface.

Regards, Adai



On Thu, 2005-02-24 at 00:06 +0100, Peter Dalgaard wrote:
> "Lim,  Jasper H. N." <h.n.lim at tbm.tudelft.nl> writes:
> 
> > Hi,
> >  
> > I have difficulty unsubscribing from this list. I have already
> > unsubscribed from the website mailing list. But the messages keep
> > flooding my mailbox. Pls help!
> >  
> > 
> > 	[[alternative HTML version deleted]]
> 
> The mailing list manager is out of town for another couple of days.
> The other two thousand recipients of your message cannot help you. If
> you really cannot unsubscribe using the web interface, the list
> manager's address is found on the same page. 
> 
> Please notice that unsubscribing does not dequeue messaged that have
> already been sent to you so you may receive messages for some time
> afterwards, even though you are no longer on the list.
> 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> > 
>



From ramasamy at cancer.org.uk  Thu Feb 24 01:04:32 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Thu, 24 Feb 2005 00:04:32 +0000
Subject: [R] and [ESS] Starting ESS
In-Reply-To: <BAY10-F53C55C4C4F154B31A1829DD6630@phx.gbl>
References: <BAY10-F53C55C4C4F154B31A1829DD6630@phx.gbl>
Message-ID: <1109203472.5983.16.camel@dhcp-63.ccc.ox.ac.uk>

AFAIK, it does not matter as long as you got the correct paths set in
your init.el file.

Please see the excellent installation instructions from John Fox's 
	http://socserv.mcmaster.ca/jfox/Books/Companion/ESS/

Regards, Adai



On Wed, 2005-02-23 at 17:16 -0600, Laura Holt wrote:
> Dear R People:
> 
> I have finally seen the error of my ways and have decided to use ESS for R 
> and S + stuff.
> 
> However, I have a question right from the beginning.  I'm somewhat confused 
> by the installation instructions.
> 
> Do I install XEMACS or ESS first, please?
> 
> Windows XP
> R Version 2.0.1
> (S + 6.2)
> 
> Thanks so much!
> 
> Sincerely,
> Laura Holt
> mailto: lauraholt_983 at hotmail.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From soros at mac.com  Thu Feb 24 01:20:41 2005
From: soros at mac.com (Peter Soros)
Date: Wed, 23 Feb 2005 19:20:41 -0500
Subject: [R] baseline shift / detrend
Message-ID: <EAFBFEB6-85F9-11D9-8F30-000D93C44EC8@mac.com>

Hello,

I am analyzing data of biological motion (mouth movements) over time (~ 
10 mins). There is a considerable baseline shift which appears to be a 
linear trend. Is it possible to get rid of this baseline shift using R?
Any help is appreciated!
Peter

Dr. Peter Soros
Sunnybrook and Women's College Health Sciences Centre
2075 Bayview Avenue Room S646
Toronto,Z Ontario,Z M4N 3M5
Canada

psoros at sten.sunnybrook.utoronto.ca



From bitwrit at ozemail.com.au  Thu Feb 24 01:24:49 2005
From: bitwrit at ozemail.com.au (bitwrit@ozemail.com.au)
Date: Thu, 24 Feb 2005 11:24:49 +1100
Subject: [R] colorbar for image
Message-ID: <20050224002449.VFPZ29550.swebmail01.mail.ozemail.net@localhost>

Klaus Ladner wrote:
> 
> Dear colleges,
> 
> Does anyone know, how to insert a color bar as used with 
> "filled.contour" when using "image"?
> 
In the package "plotrix", there is a function gradient.rect() that might do what you want.

Jim

This message was sent through MyMail http://www.mymail.com.au



From ramasamy at cancer.org.uk  Thu Feb 24 01:30:59 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Thu, 24 Feb 2005 00:30:59 +0000
Subject: [R] Getting tick positions
In-Reply-To: <200502241008.28553.fls@fls.org.au>
References: <200502241008.28553.fls@fls.org.au>
Message-ID: <1109205060.5983.32.camel@dhcp-63.ccc.ox.ac.uk>

Which plotting function are you using ?

I think most of plotting can accept xaxt="n" which is the command to
supress the x-axis. If this works, at least you do not have to redefine
the function. Examples

 plot(1:10, xaxt="n")
 hist( rnorm(100), xaxt="n" )
 boxplot( rnorm(10), rnorm(10), rnorm(10), xaxt="n" )


Reading help("par") will also shows you that xaxp might be useful but I
have not managed to get this working. Maybe someone on the list can
explain why the following does not work :

plot(1:100, xaxp=c(x1=0,x2=100,n=20) )
par( xaxp=c(x1=0,x2=100,n=20) ); plot(1:100)


Regards, Adai


On Thu, 2005-02-24 at 10:08 +1100, fls wrote:
> While writing a function that includes placing grid lines at the same position 
> as the axis ticks, I found that the axis* functions don't return anything. 
> Thus I have had to copy the appropriate function, removing the call to axis() 
> and adding a line to return the tick positions. Is there a more elegant way 
> to determine the tick positions on an axis? Thanks.
> 
> Jim
> 
> (normally bitwrit at ozemail.com.au - my modem is toast)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Tom.Mulholland at dpi.wa.gov.au  Thu Feb 24 01:32:15 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Thu, 24 Feb 2005 08:32:15 +0800
Subject: [R] Getting tick positions
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3C9B1@afhex01.dpi.wa.gov.au>

?axis

where you will find

See Also:

     'axTicks' returns the axis tick locations corresponding to
     'at=NULL'; 'pretty' is more flexible for computing pretty tick
     coordinates and does _not_ depend on (nor adapt to) the coordinate
     system in use.

Tom

> -----Original Message-----
> From: fls [mailto:fls at fls.org.au]
> Sent: Thursday, 24 February 2005 7:08 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Getting tick positions
> 
> 
> While writing a function that includes placing grid lines at 
> the same position 
> as the axis ticks, I found that the axis* functions don't 
> return anything. 
> Thus I have had to copy the appropriate function, removing 
> the call to axis() 
> and adding a line to return the tick positions. Is there a 
> more elegant way 
> to determine the tick positions on an axis? Thanks.
> 
> Jim
> 
> (normally bitwrit at ozemail.com.au - my modem is toast)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From lgilbert at berkeley.edu  Thu Feb 24 01:43:05 2005
From: lgilbert at berkeley.edu (Betty Gilbert)
Date: Wed, 23 Feb 2005 16:43:05 -0800
Subject: FYI from apple employee RE: [R] Memory error in Mac OS X Aqua GUI
	v1.01 with cluster
Message-ID: <p06110404be42d0e1e0d4@[128.32.8.36]>

Please ignore this is you're not interested in what my friend who 
works for apple had to say regarding my issue which is explained in 
detail at the bottom of this exchange...

me:
  The data from this simulation has to be processed at several stages 
of data processing so at some point I do have to have to make a 
dissimilarity matrix from this large matrix, as well as ones that I 
have filtered.

appleguy:
>>>Your matrix is too big; adding more memory, or killing off other 
>>>processes won't help.  R is clearly not very smart when it comes 
>>>to dealing with large chunks of data, and you're running into a 
>>>fundamental upper limit on the amount of data that it can deal 
>>>with.
>>>
>>>The only hope you have of dealing with this is to find a 64-bit 
>>>system (many Unix boxes fit this description) and use the text or 
>>>X-windows based R there.  If you have a login on a Unix machine at 
>>>work that has R installed, try your program there.  In a few 
>>>months, when Tiger is out and about, the G5 Macs will also be able 
>>>to run 64-bit R.


me:
>>Yeah I was unsuccesful trying to download the unix version of r 
>>last night(which is also the windows version). Christopher said it 
>>needed the fortran compiler(?). According to takao the linux we use 
>>doesn't have any gui capability so I'm not sure I can make the 
>>dendograms I'm gonna need if I installed R on it. It Linux 2.6.7. 
>>and is some version of red hat(?). Oh well...

appleguy:

>Being Linnex, it is probably running on a PC, so it won't be a 
>64-bit system anyway.   What sort of timeframe do you have on this? 
>I might be able to track down a 64-bit system, but it may take a 
>little while.
>>
If time is tight, then you should ask around to see a) whether anyone 
has a 64-bit unix machine; i.e. a Sun, SGI or Alpha box, and b) 
whether they know how to generate the graphs without the GUI (I 
suspect the GUI is not required to generate them, only view them).
Until then, you're going to need to reduce the size of your dataset 
I'm afraid. 8(


OG message to rhelp:

I'm sorry if the answer to my problem is buried in the archives. I 
have limited experience with R  and I couldn't find a solution to my 
particular problem. I am running  Mac OS X Aqua GUI v1.01 on a new G5 
running os 10.3.8 with a 1.8Ghz processor and 1GB of sdram. I just 
downloaded bioconducter a week ago and I'm trying to cluster a matrix 
I created with a simulation with dimensions
dim(nca35)
[1] 10481    12

with size
>  object.size(nca352)
[1] 1426204

I checked my ulimits variable on the unix terminal and it says it's 
unlimited as does
>  mem.limits()
nsize vsize
    NA    NA
But I'm still getting errors like the following with funtions in the 
cluster package
>  daisy(nca352, metric= "euclidean", stand=FALSE)->dnca35
Error: cannot allocate vector of size 858213 Kb
*** malloc: vm_allocate(size=878813184) failed (error code=3)
*** malloc[599]: error: Can't allocate region
if it helps i also checked
>  gc()
          used (Mb) gc trigger   (Mb)
Ncells 448662 12.0     741108   19.8
Vcells 847630  6.5  135357901 1032.7

I tried the suggested unix command in the memory help doc but that 
doesn't work in the Aqua GUI. Can someone tell me how to change the 
Vcells? Although to the best of my understanding (which is limited) I 
shouldn't have to do that. Any suggestions would be greatly 
appreciated.
thanks,
betty

-- 
Betty Gilbert
lgilbert at berkeley.edu
Taylor Lab
Plant and Microbial Biology
321 Koshland Hall
U.C. Berkeley
Berkeley, Ca 94720



From MSchwartz at MedAnalytics.com  Thu Feb 24 02:19:37 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Wed, 23 Feb 2005 19:19:37 -0600
Subject: [R] basic question about changing limits on generated plots
In-Reply-To: <200502232242.j1NMgjAq027683@five-percent-nation.mit.edu>
References: <200502232242.j1NMgjAq027683@five-percent-nation.mit.edu>
Message-ID: <1109207977.25893.17.camel@horizons.localdomain>

On Wed, 2005-02-23 at 17:42 -0500, rif wrote:
> Is it possible to change the limits on plots that are already on the
> screen?  In particular, is there any R equivalent to the sequence of
> matlab commands
> 
> plot(1:10,1:10)
> hold on
> plot(2:12,5:15)
> 
> I know I can use points and lines to add points and lines to plots,
> but the limits of the plot do not change when I do this.
> 
> Looking at various examples, it seems that the answer is "no", but I
> wanted to make sure.  This seems to make exploratory data analysis
> somewhat more challenging.
> 
> It also seems like it would be plausible to write a package on top of
> the standard graphics functions that keeps track of what you've done
> and automatically replots.  Which makes me think, has someone already
> done this?
> 
> Cheers,
> 
> rif

I have not used Matlab, but I suspect that you want to use:

par(new = TRUE)

For example:

> plot(1:10,1:10)

# Check plot region limits
> par("usr")
[1]  0.64 10.36  0.64 10.36

# Set par("new") to not overwrite existing plot
> par(new = TRUE)

> plot(2:12,5:15)

# Re-check plot region limits
> par("usr")
[1]  1.6 12.4  4.6 15.4

See ?par for more information.

Note also that par("usr") is not read-only, so you can explicitly change
it when required:

> plot(1:10,1:10)

> par("usr")
[1]  0.64 10.36  0.64 10.36

# Now reset the plot region limits
> par(usr = c(0, 20, 0, 20))

# Check it
> par("usr")
[1]  0 20  0 20


You do not indicate what OS you are using, but Under Windows, there is
an ability to record plots. See R Windows FAQ 4.2. Otherwise, save the R
code that you use to generate the plot and either C&P it or source() it
or use ESS.

HTH,

Marc Schwartz



From roger.dungan at canterbury.ac.nz  Thu Feb 24 03:14:07 2005
From: roger.dungan at canterbury.ac.nz (Roger Dungan)
Date: Thu, 24 Feb 2005 15:14:07 +1300
Subject: [R] survreg with gamma distribution: re-post
Message-ID: <7C773F4D89C83147A023704E9E781037038EAFB9@cantwe.giga.canterbury.ac.nz>

Dear r-help subscribers,

A couple of weeks ago I sent the following message to the r-help mail
list. It hasn't generated any response, and I could really use some help
on this. Anyone able to help?

Thanks again,

Roger Dungan

>>
I am working on some survival analysis of some interval censored failure
time data in R. I have done similar analysis before using PROC LIFEREG
in SAS. In that instance, a gamma survival function was the optimum
parametric model for describing the survival and hazard functions. I
would like to be able to use a gamma function in R, but apparently the
survival package does not support this distribution. I have been
googling around for some help, and have found some threads to a similar
question posted to the R-Help list in October last year. Because I am a
bit of a survival analysis and R newbie, I didn't really understand the
discussion thread. 

I've been working with a Weibull distribution, thus:

>leafsurv.weibull<-survreg(Surv(minage, maxage, censorcode, type =
>"interval")~1, dist = "weib")

And I guess I'd like to be able to do something that's the equivalent of

>leafsurv.gamma<-survreg(Surv(minage, maxage, censorcode, type = 
>"interval")~1, dist = "gamma")

At least one of the R-help listserver comments mentioned using
survreg.distributions to customise a gamma distribution, but I can't
figure out how to make this work with the resources (intellectual and
bibliographical!) that I have available.

With thanks in advance for your help,

Dr Roger Dungan
School of Biological Sciences
University of Cantebury
Christchurch, New Zealand
ph +64 3 366 7001 ext. 4848
fax +64 3 354 2590



From rif at MIT.EDU  Thu Feb 24 03:14:50 2005
From: rif at MIT.EDU (rif)
Date: Wed, 23 Feb 2005 21:14:50 -0500
Subject: [R] basic question about changing limits on generated plots
In-Reply-To: Your message of "Wed, 23 Feb 2005 19:19:37 CST."
	<1109207977.25893.17.camel@horizons.localdomain> 
Message-ID: <200502240214.j1O2Eoj1030458@five-percent-nation.mit.edu>


> On Wed, 2005-02-23 at 17:42 -0500, rif wrote:
> > Is it possible to change the limits on plots that are already on the
> > screen?  In particular, is there any R equivalent to the sequence of
> > matlab commands
> > 
> > plot(1:10,1:10)
> > hold on
> > plot(2:12,5:15)
> > 
> > rif
> 
> I have not used Matlab, but I suspect that you want to use:
> 
> par(new = TRUE)
> 
> For example:
> 
> > plot(1:10,1:10)
> 
> # Check plot region limits
> > par("usr")
> [1]  0.64 10.36  0.64 10.36
> 
> # Set par("new") to not overwrite existing plot
> > par(new = TRUE)
> 
> > plot(2:12,5:15)
>
> Marc Schwartz

Marc,

This does not do what the matlab code I posted does (the matlab code
also works in the free program octave, if you want to try).  The
matlab code moves already plotted data within the window (replots it).
When I first type plot(1:10,1:10), I see a graph with axis limits [1
10 1 10].  When I type hold on (to keep my original data), and execute
plot(2:12,5:15), the plot I see is equivalent to the plot I'd have
gotten if I'd originally specified axis limits [1 12 5 15].  By
contrast, in the R code you sent, it's as if I'm superimposing two
unrelated plots.

Essentially, the underlying "task" is that I want to compare multiple
functions, but I do not know good limits for the complete set of
functions when I start.  Being able to adjust the graph to show all
the data I've plotted so far would be extremely useful for exploratory
analysis.  This is the mode I and colleagues generally use matlab and
octave in.

Does this question get asked all the time?  It seems to be something
that would come up a lot for people who switch from Matlab/Octave to
R, but I searched the archives and didn't really see anything.

Cheers,

rif



From jsorkin at grecc.umaryland.edu  Thu Feb 24 03:35:22 2005
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Wed, 23 Feb 2005 21:35:22 -0500
Subject: [R] Problems with html help system: help.start()
Message-ID: <s21cf732.001@grecc.umaryland.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050223/727b090d/attachment.pl

From lorin at cs.umd.edu  Thu Feb 24 04:18:43 2005
From: lorin at cs.umd.edu (Lorin Hochstein)
Date: Wed, 23 Feb 2005 22:18:43 -0500
Subject: [R] Adding up intervals by activity
Message-ID: <421D4793.6090600@cs.umd.edu>

Hello all,

I have a data frame with the following columns: timestamp, activity, 
interval.

Each row contains information about an activity that occured over a time 
interval. "Timestamp" is when the interval ended, "Activity" is a factor 
that represents the type of activity, and "Interval" is a double which 
represents an interval in time. The data frame is sorted by timestamp. 
For example:

Timestamp            Activity  Interval
2004-02-12 08:59:08  A         5
2004-02-12 09:23:03  A         7
2004-02-12 09:56:04  A         4
2004-02-12 10:39:00  B         5
2004-02-12 11:23:06  B         3
2004-02-12 12:56:01  A         4
2004-02-12 12:59:01  A         2
...

I would like to know how much time was spent on an activity before the 
activity changed (to compute time, I just care about the lengths of the 
intervals, not the values of the timestamps). For example, given the 
table above, I would like to get an output that looks like:


Activity      TotalTime
A             16
B             8
A             6

Is this possible to do in R without resorting to a for loop or recursion 
(i.e. what's the R-ish way to do this?) I know I can use the "diff" 
function to identify which rows represent a change in activity, but is 
there anything I can do with that?

Lorin

----------
Lorin Hochstein
Graduate Research Assistant
Experimental Software Engineering Group
Computer Science Dept., University of Maryland
email:lorin at cs.umd.edu  tel:301-405-2721



From PAlspach at hortresearch.co.nz  Thu Feb 24 04:46:08 2005
From: PAlspach at hortresearch.co.nz (Peter Alspach)
Date: Thu, 24 Feb 2005 16:46:08 +1300
Subject: [R] Adding up intervals by activity
Message-ID: <s21e051b.011@hra2.marc.hort.cri.nz>


Lorin

You could use rle():  Say your Activity and Interval data is in lorin, then:

tmp.rle <- rle(as.vector(lorin[,1]))[[1]]
tapply(lorin[,2], rep(1:length(tmp.rle), tmp.rle), sum)
 1  2  3 
16  8  6 

HTH

Peter Alspach

>>> Lorin Hochstein <lorin at cs.umd.edu> 24/02/05 16:18:43 >>>
Hello all,

I have a data frame with the following columns: timestamp, activity, 
interval.

Each row contains information about an activity that occured over a time 
interval. "Timestamp" is when the interval ended, "Activity" is a factor 
that represents the type of activity, and "Interval" is a double which 
represents an interval in time. The data frame is sorted by timestamp. 
For example:

Timestamp            Activity  Interval
2004-02-12 08:59:08  A         5
2004-02-12 09:23:03  A         7
2004-02-12 09:56:04  A         4
2004-02-12 10:39:00  B         5
2004-02-12 11:23:06  B         3
2004-02-12 12:56:01  A         4
2004-02-12 12:59:01  A         2
...

I would like to know how much time was spent on an activity before the 
activity changed (to compute time, I just care about the lengths of the 
intervals, not the values of the timestamps). For example, given the 
table above, I would like to get an output that looks like:


Activity      TotalTime
A             16
B             8
A             6

Is this possible to do in R without resorting to a for loop or recursion 
(i.e. what's the R-ish way to do this?) I know I can use the "diff" 
function to identify which rows represent a change in activity, but is 
there anything I can do with that?

Lorin

----------
Lorin Hochstein
Graduate Research Assistant
Experimental Software Engineering Group
Computer Science Dept., University of Maryland
email:lorin at cs.umd.edu  tel:301-405-2721

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

______________________________________________________

The contents of this e-mail are privileged and/or confidenti...{{dropped}}



From Jin.Li at csiro.au  Thu Feb 24 04:45:53 2005
From: Jin.Li at csiro.au (Jin.Li@csiro.au)
Date: Thu, 24 Feb 2005 13:45:53 +1000
Subject: [R] a simple question
Message-ID: <2BEE99D7F6F1484EBDD1D22167385E75442FDE@exqld1-ath.nexus.csiro.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050224/796dffe5/attachment.pl

From weigand.stephen at charter.net  Thu Feb 24 05:00:55 2005
From: weigand.stephen at charter.net (Stephen D. Weigand)
Date: Wed, 23 Feb 2005 22:00:55 -0600
Subject: [R] Row median of Date class variables in a data frame
Message-ID: <7891a0cf2149bfb62822453ba813faa6@charter.net>

I am trying to calculate the median of each row of a
data frame where the data frame consist of
columns of class Date.

Below are my test data and best attempt at using apply.
I didn't see a solution via Google or the Baron search
site.

I'd be grateful for any suggestions or solutions.
I'm using R 2.0.0 on Mac OS X.

Thank you,

Stephen Weigand


### Test data

date1 <- c(1000, 2000, 3000,4000)
date2 <- date1 + 100
date3 <- date2 + 100

class(date1) <- class(date2) <- class(date3) <- "Date"

test <- data.frame(date1, date2, date3)

print(test)

### create a function for apply()
medDate <- function(x){
   obj <- unclass(unlist(x))
   med <- median(obj, na.rm = TRUE)
   med
   class(med) <- "Date"
   med
}

medDate(test$date1) # works
medDate(test[1,])   # works

apply(test, 1, medDate) # gives error: 'need numeric data'
apply(test, 2, medDate) # gives error: 'need numeric data'



From ggrothendieck at myway.com  Thu Feb 24 04:58:56 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 24 Feb 2005 03:58:56 +0000 (UTC)
Subject: [R] survreg with gamma distribution: re-post
References: <7C773F4D89C83147A023704E9E781037038EAFB9@cantwe.giga.canterbury.ac.nz>
Message-ID: <loom.20050224T045744-530@post.gmane.org>


Google for the gfcure package on statlib.  I have not used it myself
and am not entirely sure whether it works on R or just S.

Roger Dungan <roger.dungan <at> canterbury.ac.nz> writes:

: 
: Dear r-help subscribers,
: 
: A couple of weeks ago I sent the following message to the r-help mail
: list. It hasn't generated any response, and I could really use some help
: on this. Anyone able to help?
: 
: Thanks again,
: 
: Roger Dungan
: 
: >>
: I am working on some survival analysis of some interval censored failure
: time data in R. I have done similar analysis before using PROC LIFEREG
: in SAS. In that instance, a gamma survival function was the optimum
: parametric model for describing the survival and hazard functions. I
: would like to be able to use a gamma function in R, but apparently the
: survival package does not support this distribution. I have been
: googling around for some help, and have found some threads to a similar
: question posted to the R-Help list in October last year. Because I am a
: bit of a survival analysis and R newbie, I didn't really understand the
: discussion thread. 
: 
: I've been working with a Weibull distribution, thus:
: 
: >leafsurv.weibull<-survreg(Surv(minage, maxage, censorcode, type =
: >"interval")~1, dist = "weib")
: 
: And I guess I'd like to be able to do something that's the equivalent of
: 
: >leafsurv.gamma<-survreg(Surv(minage, maxage, censorcode, type = 
: >"interval")~1, dist = "gamma")
: 
: At least one of the R-help listserver comments mentioned using
: survreg.distributions to customise a gamma distribution, but I can't
: figure out how to make this work with the resources (intellectual and
: bibliographical!) that I have available.
: 
: With thanks in advance for your help,
: 
: Dr Roger Dungan
: School of Biological Sciences
: University of Cantebury
: Christchurch, New Zealand
: ph +64 3 366 7001 ext. 4848
: fax +64 3 354 2590
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://stat.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
: 
:



From Tom.Mulholland at dpi.wa.gov.au  Thu Feb 24 05:08:42 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Thu, 24 Feb 2005 12:08:42 +0800
Subject: [R] a simple question
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3C9B2@afhex01.dpi.wa.gov.au>

Does strsplit fit what you want?

Tom

> -----Original Message-----
> From: Jin.Li at csiro.au [mailto:Jin.Li at csiro.au]
> Sent: Thursday, 24 February 2005 11:46 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] a simple question
> 
> 
> Dear All,
> 
> I need to separate one column which is in a format like, "can_region",
> into two columns like, "can", and "region".  Do we have a 
> function in R
> to do this? Thanks.
> 
> Regards,
> 
> Jin
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ggrothendieck at myway.com  Thu Feb 24 05:05:17 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 24 Feb 2005 04:05:17 +0000 (UTC)
Subject: [R] how do I get means by factor?
References: <200502232141.j1NLfb34010326@hypatia.math.ethz.ch>
Message-ID: <loom.20050224T050312-121@post.gmane.org>

Chris <grover2160 <at> metrocast.net> writes:

: 
: I want the mean of a response variable for each level of a factor in a
: data.frame.  How is this done?

In addition to the tapply suggestion already given you can use
summary in package Hmisc:

summary(y ~ f)



From ggrothendieck at myway.com  Thu Feb 24 05:01:33 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 24 Feb 2005 04:01:33 +0000 (UTC)
Subject: [R] baseline shift / detrend
References: <EAFBFEB6-85F9-11D9-8F30-000D93C44EC8@mac.com>
Message-ID: <loom.20050224T045931-138@post.gmane.org>


Peter Soros <soros <at> mac.com> writes:

> 
> Hello,
> 
> I am analyzing data of biological motion (mouth movements) over time (~ 
> 10 mins). There is a considerable baseline shift which appears to be a 
> linear trend. Is it possible to get rid of this baseline shift using R?
> Any help is appreciated!


The following will detrend y:

resid(lm(y ~ seq(y)))



From ggrothendieck at myway.com  Thu Feb 24 05:14:48 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 24 Feb 2005 04:14:48 +0000 (UTC)
Subject: [R] Row median of Date class variables in a data frame
References: <7891a0cf2149bfb62822453ba813faa6@charter.net>
Message-ID: <loom.20050224T051226-976@post.gmane.org>

Stephen D. Weigand <weigand.stephen <at> charter.net> writes:

: 
: I am trying to calculate the median of each row of a
: data frame where the data frame consist of
: columns of class Date.
: 
: Below are my test data and best attempt at using apply.
: I didn't see a solution via Google or the Baron search
: site.
: 
: I'd be grateful for any suggestions or solutions.
: I'm using R 2.0.0 on Mac OS X.
: 
: Thank you,
: 
: Stephen Weigand
: 
: ### Test data
: 
: date1 <- c(1000, 2000, 3000,4000)
: date2 <- date1 + 100
: date3 <- date2 + 100
: 
: class(date1) <- class(date2) <- class(date3) <- "Date"
: 
: test <- data.frame(date1, date2, date3)
: 
: print(test)
: 
: ### create a function for apply()
: medDate <- function(x){
:    obj <- unclass(unlist(x))
:    med <- median(obj, na.rm = TRUE)
:    med
:    class(med) <- "Date"
:    med
: }
: 
: medDate(test$date1) # works
: medDate(test[1,])   # works
: 
: apply(test, 1, medDate) # gives error: 'need numeric data'
: apply(test, 2, medDate) # gives error: 'need numeric data'
: 

Try this:

library(zoo)
as.Date(apply(data.matrix(test), 1, median)) # ignore warning

The only reason you need zoo is that it supplies as.Date methods
which convert numerics and integers to Date.



From MSchwartz at MedAnalytics.com  Thu Feb 24 06:50:20 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Wed, 23 Feb 2005 23:50:20 -0600
Subject: [R] basic question about changing limits on generated plots
In-Reply-To: <200502240214.j1O2Eoj1030458@five-percent-nation.mit.edu>
References: <200502240214.j1O2Eoj1030458@five-percent-nation.mit.edu>
Message-ID: <1109224221.23587.40.camel@horizons.localdomain>

On Wed, 2005-02-23 at 21:14 -0500, rif wrote:

<snip>

> Marc,
> 
> This does not do what the matlab code I posted does (the matlab code
> also works in the free program octave, if you want to try).  The
> matlab code moves already plotted data within the window (replots it).
> When I first type plot(1:10,1:10), I see a graph with axis limits [1
> 10 1 10].  When I type hold on (to keep my original data), and execute
> plot(2:12,5:15), the plot I see is equivalent to the plot I'd have
> gotten if I'd originally specified axis limits [1 12 5 15].  By
> contrast, in the R code you sent, it's as if I'm superimposing two
> unrelated plots.
> 
> Essentially, the underlying "task" is that I want to compare multiple
> functions, but I do not know good limits for the complete set of
> functions when I start.  Being able to adjust the graph to show all
> the data I've plotted so far would be extremely useful for exploratory
> analysis.  This is the mode I and colleagues generally use matlab and
> octave in.
> 
> Does this question get asked all the time?  It seems to be something
> that would come up a lot for people who switch from Matlab/Octave to
> R, but I searched the archives and didn't really see anything.
> 
> Cheers,
> 
> rif

A general statement: There are members of the R Community engaged in
Octave, so there is some overlap, at least in terms of expertise with
both tools. Perhaps they can offer some insight here.

The good news is that I have Octave installed on my FC3 system, so I was
able to get a feel for what you are referring to.

A search of the R archives would suggest that there is not a direct
parallel in terms of adding a new set of data to an existing plot, while
having the entire coordinate system adjusted to the new data in a single
step.

There are references to what I suggested, the use of the 'add' argument
in some plot functions and of course the use of points() and lines().

I would defer to others with more low-level knowledge of the standard R
screen based plotting devices, but from my past review of code (both R
and C) and reasonable knowledge, I am not sure that this can be done
without some form of two step approach involving re-plotting the
original data using an updated coordinate system based upon the new data
and then overlaying the new data. Presumably at a low level, this is
what Octave and Matlab do, since I noted that the plot device seems to
be completely re-drawn upon the second plot call. Since Octave is open
source, one can of course review the code to see what is truly
happening.

The key it seems would be to "transparently" save the original data as
an object, re-plot it with the adjusted coordinate system and then add
the new data. 

I would guess that with some thought, one could create some wrapper
plotting functions or methods that would save the data object(s) in a
plot "environment" so that each successive plot call "layers" the older
data sets in turn and then add the newest data set. All this done in a
coordinate system that is re-configured each time, based upon the
maximum x and y axis ranges required for the multiple datasets that have
been plotted to that point.

Almost sounds like a plot "stack", to use an assembly language metaphor.

You might also want to look at the xgobi package on CRAN, which provides
an interface to the XGobi system:

http://www.research.att.com/areas/stat/xgobi/

or the ggobi system:

http://www.ggobi.org/

which is accessible from R. These are both dynamic data visualization
tools.

HTH,

Marc



From charles_loboz at yahoo.com  Thu Feb 24 07:40:50 2005
From: charles_loboz at yahoo.com (charles loboz)
Date: Wed, 23 Feb 2005 22:40:50 -0800 (PST)
Subject: [R] KalmanXXXX and deJong-Penzer statistic?
Message-ID: <20050224064050.21988.qmail@web60804.mail.yahoo.com>

A question about: Kalman in R, time series and
deJong-Penzer statistic - how to compute it using
available artefacts of KalmanXXXXX?

Background. in the paper
http://www.lse.ac.uk/collections/statistics/documents/researchreport34.pdf

'Diagnosing Shocks in TIme Series', de Jong and Penzer
construct a statistic (tau) which can be used to
locate potential shocks. [p15, Theorem 6.1 and below].
They also state that all the components of that
statistic (v_i, F_i, r_i, N_i) 'are computed with
Kalman Filter Smoother applied to the null model'.

Also, as I understand, that part has been implemented
in one of the S packages , SsfPack, as the book on
that states on p 531 'the standardized smoothed
disturbances may be interpreted as t-statistics for
impulse intervention variable in the transition and
measurement equations.' and equations for the
statistic are:
           eta_t / sqrt(var(eta_t), 
where eta_t and var have hats over them. The second
equation is identical, with 'eta' replaced by
'epsilon'. On page 524 we also have:
"the smoothed disturbance estimates are the estimates
of the measurement equation innovations epsilon and
transition equation innovations eta based on all
available information Y.  ... the computation of
hat(eta) and hat(epsilon) from the Kalman smoother
algorithm is described in Durbin and Koopman chapter
7, 'Time series analysis by state space methods', OUP
(2001) "

Local libraries do not have this book and it will take
several weeks to get it. 

Assuming I will get the book: does the KalmanXXX set
of functions produce all the necessary artefacts to
compute this statistic either as per deJong-Penzer or
as per SsfPack? 

Reading carefully through the manual I see that we
have artefacts of states and normalized residuals
(presumably of states - but how can I unscale them if
I need them)? What about other stats? How to compute
smoothed disturbance estimates? 

I am rather confused - that's my first approach to
Kalman filter and state models.



From ripley at stats.ox.ac.uk  Thu Feb 24 08:17:18 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 24 Feb 2005 07:17:18 +0000 (GMT)
Subject: [R] Row median of Date class variables in a data frame
In-Reply-To: <7891a0cf2149bfb62822453ba813faa6@charter.net>
References: <7891a0cf2149bfb62822453ba813faa6@charter.net>
Message-ID: <Pine.LNX.4.61.0502240712540.30109@gannet.stats>

Note that the argument to apply() is a matrix, not a data frame, so your
object has been coerced to a matrix.  That is spelled out on the help page 
for apply(), as it is frequent misuse.

You would be better off using a matrix of class Date in the first place if 
you want row operations.


On Wed, 23 Feb 2005, Stephen D. Weigand wrote:

> I am trying to calculate the median of each row of a
> data frame where the data frame consist of
> columns of class Date.
>
> Below are my test data and best attempt at using apply.
> I didn't see a solution via Google or the Baron search
> site.
>
> I'd be grateful for any suggestions or solutions.
> I'm using R 2.0.0 on Mac OS X.
>
> Thank you,
>
> Stephen Weigand
>
>
> ### Test data
>
> date1 <- c(1000, 2000, 3000,4000)
> date2 <- date1 + 100
> date3 <- date2 + 100
>
> class(date1) <- class(date2) <- class(date3) <- "Date"
>
> test <- data.frame(date1, date2, date3)
>
> print(test)
>
> ### create a function for apply()
> medDate <- function(x){
>  obj <- unclass(unlist(x))
>  med <- median(obj, na.rm = TRUE)
>  med
>  class(med) <- "Date"
>  med
> }
>
> medDate(test$date1) # works
> medDate(test[1,])   # works
>
> apply(test, 1, medDate) # gives error: 'need numeric data'
> apply(test, 2, medDate) # gives error: 'need numeric data'

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From fengchen at hkusua.hku.hk  Thu Feb 24 09:02:07 2005
From: fengchen at hkusua.hku.hk (Feng Chen)
Date: Thu, 24 Feb 2005 16:02:07 +0800
Subject: [R] a question about function eval()
Message-ID: <000601c51a47$2640fa80$77d40893@S119>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050224/dec40a16/attachment.pl

From Allan at STATS.uct.ac.za  Thu Feb 24 10:16:33 2005
From: Allan at STATS.uct.ac.za (Clark Allan)
Date: Thu, 24 Feb 2005 11:16:33 +0200
Subject: [R] r: functions
Message-ID: <421D9B71.D576E4BC@STATS.uct.ac.za>

hi all


i have a function that uses two inputs, say xdata and ydata. An example
is the following,

simple1<-function(xdata,ydata)
{
	ofit<-lm(ydata~xdata)
	list(ofit)
}

say i use arbitray number for xdata and ydata such that

D = 
x1	x2	y
1	1	10
2	6	6
3	10	7


x<-D[,1:2]

and 

y<-D[,3]

if one uses these inputs and rund the program we get the following:

>simple(xdata=x,ydata=y)
Error in model.frame(formula, rownames, variables, varnames, extras,
extranames,  : 
        invalid variable type

why does this happen!

i can get results if i change the program as follows:

simple2<-function(xdata,ydata)
{
	ofit<-lm(as.matrix(ydata)~as.matrix(xdata))
	list(ofit)
}

but then the variable names, if they exist, are not preserved. how can i
preserve these names.

the results are now:

> simple2(xdata=x,ydata=y)
[[1]]

Call:
lm(formula = as.matrix(ydata) ~ as.matrix(xdata))

Coefficients:
       (Intercept)  as.matrix(xdata)x1  as.matrix(xdata)x2  
                -6                  21                  -5  

i've tried converting xdata and ydata to data frames but i still get
errors.

simple3<-function(xdata,ydata)
{
	xdata<-as.data.frame(xdata)
	ydata<-as.data.frame(ydata)
	ofit<-lm(ydata~xdata)
	list(ofit)
}

i.e.

> simple3(xdata=x,ydata=y)
Error in model.frame(formula, rownames, variables, varnames, extras,
extranames,  : 
        invalid variable type

please help!

thanking you in advance

***
allan

From plummer at iarc.fr  Thu Feb 24 10:20:35 2005
From: plummer at iarc.fr (Martyn Plummer)
Date: Thu, 24 Feb 2005 10:20:35 +0100
Subject: [R] Problems with html help system: help.start()
In-Reply-To: <s21cf732.001@grecc.umaryland.edu>
References: <s21cf732.001@grecc.umaryland.edu>
Message-ID: <1109236835.3484.1.camel@seurat>

There was a packaging error. A new RPM version is on its way to CRAN.
Thanks.

Martyn

On Wed, 2005-02-23 at 21:35 -0500, John Sorkin wrote:
> I am having problems installing R under Fedora Core 3. I installed R
> using YUM
> yum -install R. 
> The basic installation appears to have worked, I can start and run R.
> When I try to start the HTML help system, I get the errors listed
> below.
> Any help solving my problem would be appreciated.
> Thanks
> John Sorkin
>  
> R intalled using YUM under Red Hat Fedora Core 3.
> Computer: Dell Latitude C400.
> 
> 
>  
> help.start()Making links in per-session dir
> .../usr/lib/R/share/sh/help-links.sh: line 9:
> /usr/lib/R/bin/mkinstalldirs: No such file or directory ln: creating
> symbolic link `/tmp/RtmpxI5110/.R/doc/manual' to
> `/usr/lib/R/doc/manual': No such file or directoryln: creating symbolic
> link `/tmp/RtmpxI5110/.R/doc/html' to `/usr/lib/R/doc/html/R.css': No
> such file or directoryln: creating symbolic link
> `/tmp/RtmpxI5110/.R/doc/html' to `/usr/lib/R/doc/html/about.html': No
> such file or directoryln: creating symbolic link
> `/tmp/RtmpxI5110/.R/doc/html' to `/usr/lib/R/doc/html/faq.html': No such
> file or directoryln: creating symbolic link
> `/tmp/RtmpxI5110/.R/doc/html' to `/usr/lib/R/doc/html/index.html': No
> such file or directoryln: creating symbolic link
> `/tmp/RtmpxI5110/.R/doc/html' to `/usr/lib/R/doc/html/left.jpg': No such
> file or directoryln: creating symbolic link
> `/tmp/RtmpxI5110/.R/doc/html' to `/usr/lib/R/doc/html/logo.jpg': No such
> file or directoryln: creating symbolic link
> `/tmp/RtmpxI5110/.R/doc/html' to `/usr/lib/R/doc/html/logosm.jpg': No
> such file or directoryln: creating symbolic link
> `/tmp/RtmpxI5110/.R/doc/html' to
> `/usr/lib/R/doc/html/packages-head.html': No such file or directoryln:
> creating symbolic link `/tmp/RtmpxI5110/.R/doc/html' to
> `/usr/lib/R/doc/html/packages.html': No such file or directoryln:
> creating symbolic link `/tmp/RtmpxI5110/.R/doc/html' to
> `/usr/lib/R/doc/html/resources.html': No such file or directoryln:
> creating symbolic link `/tmp/RtmpxI5110/.R/doc/html' to
> `/usr/lib/R/doc/html/right.jpg': No such file or directoryln: creating
> symbolic link `/tmp/RtmpxI5110/.R/doc/html' to
> `/usr/lib/R/doc/html/template.html': No such file or directoryln:
> creating symbolic link `/tmp/RtmpxI5110/.R/doc/html' to
> `/usr/lib/R/doc/html/thanks.html': No such file or directoryln: creating
> symbolic link `/tmp/RtmpxI5110/.R/doc/html' to
> `/usr/lib/R/doc/html/up.jpg': No such file or directorycp: cannot create
> regular file `/tmp/RtmpxI5110/.R/doc/html': No such file or directorycp:
> cannot create regular file `/tmp/RtmpxI5110/.R/doc/html/search': No such
> file or directoryln: creating symbolic link
> `/tmp/RtmpxI5110/.R/doc/html/search' to
> `/usr/lib/R/doc/html/search/SearchEngine-foot.html': No such file or
> directoryln: creating symbolic link `/tmp/RtmpxI5110/.R/doc/html/search'
> to `/usr/lib/R/doc/html/search/SearchEngine-head.html': No such file or
> directoryln: creating symbolic link `/tmp/RtmpxI5110/.R/doc/html/search'
> to `/usr/lib/R/doc/html/search/SearchEngine.html': No such file or
> directoryln: creating symbolic link `/tmp/RtmpxI5110/.R/doc/html/search'
> to `/usr/lib/R/doc/html/search/SearchObject.html': No such file or
> directoryln: creating symbolic link `/tmp/RtmpxI5110/.R/library' to
> `/usr/lib/R/doc/html/R.css': No such file or directoryIf
> /usr/bin/firefox is already running, it is *not* restarted, and you   
> must switch to its window.Otherwise, be patient ...Warning
> message:cannot create HTML package index in: make.packages.html()>
> Error: No running window foundJohn Sorkin M.D., Ph.D.
> Chief, Biostatistics and Informatics
> Baltimore VA Medical Center GRECC and
> University of Maryland School of Medicine Claude Pepper OAICUniversity
> of Maryland School of Medicine
> Division of Gerontology
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524410-605-7119 
> -- NOTE NEW EMAIL ADDRESS:
> jsorkin at grecc.umaryland.edu
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ernst.hoffer at chello.at  Thu Feb 24 10:52:26 2005
From: ernst.hoffer at chello.at (ernst hoffer)
Date: Thu, 24 Feb 2005 10:52:26 +0100
Subject: [R] zeichnung eines wahrscheinlichkeitsnetztes
Message-ID: <000801c51a56$8c6de890$f2356c50@namexrzcok2x9n>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050224/5f2a749e/attachment.pl

From Matthias.Templ at statistik.gv.at  Thu Feb 24 10:51:22 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Thu, 24 Feb 2005 10:51:22 +0100
Subject: [R] r: functions
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BA8D7@xchg1.statistik.local>

Hello,

I have no errors with simple1:

d <- matrix(c(1,2,3,1,6,10,10,6,7),ncol=3)
colnames(d) <- c("one", "two", "three")
x<- d[,c(1,2)]
y<-d[,3]

> y
[1] 10  6  7

> simple1(x,y)
[[1]]

Call:
lm(formula = ydata ~ xdata)

Coefficients:
(Intercept)     xdataone     xdatatwo  
         -6           21           -5  


With:
y <-d[,3, drop=FALSE]

> y
     [,1]
[1,]   10
[2,]    6
[3,]    7

> simple1(x,y)
[[1]]

Call:
lm(formula = ydata ~ xdata)

Coefficients:
(Intercept)     xdataone     xdatatwo  
         -6           21           -5   

You have an error with function simple, which you have not shown! (you have only shown function simple1, simple2 and simple3)

Look at simple3 now:
simple3<-function(xdata,ydata)
{
	datas<-data.frame(xdata=xdata,ydata=ydata)
	ofit<-lm(ydata~xdata, data=datas)
	list(ofit)
}

> simple3(x,y)
[[1]]

Call:
lm(formula = ydata ~ xdata, data = datas)

Coefficients:
(Intercept)     xdataone     xdatatwo  
         -6           21           -5  

Best,
Matthias


> -----Urspr?ngliche Nachricht-----
> Von: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] Im Auftrag von Clark Allan
> Gesendet: Donnerstag, 24. Februar 2005 10:17
> An: r-help at stat.math.ethz.ch
> Betreff: [R] r: functions
> 
> 
> hi all
> 
> 
> i have a function that uses two inputs, say xdata and ydata. 
> An example is the following,
> 
> simple1<-function(xdata,ydata)
> {
> 	ofit<-lm(ydata~xdata)
> 	list(ofit)
> }
> 
> say i use arbitray number for xdata and ydata such that
> 
> D = 
> x1	x2	y
> 1	1	10
> 2	6	6
> 3	10	7
> 
> 
> x<-D[,1:2]
> 
> and 
> 
> y<-D[,3]
> 
> if one uses these inputs and rund the program we get the following:
> 
> >simple(xdata=x,ydata=y)
> Error in model.frame(formula, rownames, variables, varnames, 
> extras, extranames,  : 
>         invalid variable type
> 
> why does this happen!
> 
> i can get results if i change the program as follows:
> 
> simple2<-function(xdata,ydata)
> {
> 	ofit<-lm(as.matrix(ydata)~as.matrix(xdata))
> 	list(ofit)
> }
> 
> but then the variable names, if they exist, are not 
> preserved. how can i preserve these names.
> 
> the results are now:
> 
> > simple2(xdata=x,ydata=y)
> [[1]]
> 
> Call:
> lm(formula = as.matrix(ydata) ~ as.matrix(xdata))
> 
> Coefficients:
>        (Intercept)  as.matrix(xdata)x1  as.matrix(xdata)x2  
>                 -6                  21                  -5  
> 
> i've tried converting xdata and ydata to data frames but i 
> still get errors.
> 
> simple3<-function(xdata,ydata)
> {
> 	xdata<-as.data.frame(xdata)
> 	ydata<-as.data.frame(ydata)
> 	ofit<-lm(ydata~xdata)
> 	list(ofit)
> }
> 
> i.e.
> 
> > simple3(xdata=x,ydata=y)
> Error in model.frame(formula, rownames, variables, varnames, 
> extras, extranames,  : 
>         invalid variable type
> 
> please help!
> 
> thanking you in advance
> 
> ***
> allan
>



From dimitris.rizopoulos at med.kuleuven.ac.be  Thu Feb 24 11:07:06 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Thu, 24 Feb 2005 11:07:06 +0100
Subject: [R] r: functions
References: <421D9B71.D576E4BC@STATS.uct.ac.za>
Message-ID: <00db01c51a58$989da680$0540210a@www.domain>

There was a similar question about one week ago regarding the use of 
"table(x,y)". One approach could be to use the following:

simple1 <- function(xdata, ydata){
    ofit <- substitute(lm(ydata~xdata))
    list(eval.parent(ofit))
}
#############
simple1(xdata=x, ydata=y)
lm(y~x)

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Clark Allan" <Allan at stats.uct.ac.za>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, February 24, 2005 10:16 AM
Subject: [R] r: functions


> hi all
>
>
> i have a function that uses two inputs, say xdata and ydata. An 
> example
> is the following,
>
> simple1<-function(xdata,ydata)
> {
> ofit<-lm(ydata~xdata)
> list(ofit)
> }
>
> say i use arbitray number for xdata and ydata such that
>
> D =
> x1 x2 y
> 1 1 10
> 2 6 6
> 3 10 7
>
>
> x<-D[,1:2]
>
> and
>
> y<-D[,3]
>
> if one uses these inputs and rund the program we get the following:
>
>>simple(xdata=x,ydata=y)
> Error in model.frame(formula, rownames, variables, varnames, extras,
> extranames,  :
>        invalid variable type
>
> why does this happen!
>
> i can get results if i change the program as follows:
>
> simple2<-function(xdata,ydata)
> {
> ofit<-lm(as.matrix(ydata)~as.matrix(xdata))
> list(ofit)
> }
>
> but then the variable names, if they exist, are not preserved. how 
> can i
> preserve these names.
>
> the results are now:
>
>> simple2(xdata=x,ydata=y)
> [[1]]
>
> Call:
> lm(formula = as.matrix(ydata) ~ as.matrix(xdata))
>
> Coefficients:
>       (Intercept)  as.matrix(xdata)x1  as.matrix(xdata)x2
>                -6                  21                  -5
>
> i've tried converting xdata and ydata to data frames but i still get
> errors.
>
> simple3<-function(xdata,ydata)
> {
> xdata<-as.data.frame(xdata)
> ydata<-as.data.frame(ydata)
> ofit<-lm(ydata~xdata)
> list(ofit)
> }
>
> i.e.
>
>> simple3(xdata=x,ydata=y)
> Error in model.frame(formula, rownames, variables, varnames, extras,
> extranames,  :
>        invalid variable type
>
> please help!
>
> thanking you in advance
>
> ***
> allan


--------------------------------------------------------------------------------


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From petr.pikal at precheza.cz  Thu Feb 24 11:13:47 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 24 Feb 2005 11:13:47 +0100
Subject: [R] to print dataframe
In-Reply-To: <4212FDE60001C128@mail-5.tiscali.it>
Message-ID: <421DB6EB.27776.C24873@localhost>

Hi

Perhaps write.table(...., row.names=FALSE)

Cheers
Petr


On 23 Feb 2005 at 15:39, sim76 at tiscali.it wrote:

> Dear all, 
> 
> Is it possible to print a dataframe without the row numbers? 
> For example if I have a dataframe like that: 
> >df <-
> >data.frame(name1=sample(LETTERS,10),name2=sample(c(0,1),10,replace=TR
> >UE))
> 
> after printing 
>    name1 name2 
> 1      O     1 
> 2      H     0 
> 3      R     0 
> 4      T     0 
> 5      V     1 
> 6      E     0 
> 7      W    0 
> 8      P     1 
> 9      G     0 
> 10     J     1 
> > 
> 
> 
> But I would like the dataframe printed like that 
> 
>    name1 name2 
>      O      1 
>      H      0 
>       R     0 
>       T     0 
>      V      1 
>      E      0 
>      W     0 
>      P      1 
>      G     0 
>     J       1 
> 
> I look at ?print.dataframe ?data.frame but I can't find anything.
> Somebody could help me? Thanks in advance.
> 
> __________________________________________________________________
> Tiscali Adsl 3 Mega Flat, 3 MESI GRATIS! Con Tiscali Adsl 3 Mega Flat
> navighi in Rete alla supervelocita' a soli 29.95 euro al mese senza
> limiti di tempo. Attivati entro il 28 Febbraio 2005, 3 MESI sono
> GRATIS Scopri come http://abbonati.tiscali.it/adsl/sa/2flat_tc/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From slist at oomvanlieshout.net  Thu Feb 24 11:28:29 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Thu, 24 Feb 2005 12:28:29 +0200
Subject: [R] zeichnung eines wahrscheinlichkeitsnetztes
In-Reply-To: <000801c51a56$8c6de890$f2356c50@namexrzcok2x9n>
References: <000801c51a56$8c6de890$f2356c50@namexrzcok2x9n>
Message-ID: <421DAC4D.5000008@oomvanlieshout.net>

Hi Ernst,

Pretty soon some one is going to: 1) tell you that the mailing list is 
in English, 2) that you should provide more details of your data and 2) 
that your message should show signs that you have read the posting guide 
and have searched the help documentation in a first attempt to find the 
answer to your question.

Maybe you can reply to you own question, considering the above. Then 
people will surely be very helpful!

Good luck,

Sander.

ernst hoffer wrote:
> ich h?tte ein problem: im rahmen meiner projektarbeit stehe ich vor der aufgabe, einen beliebigen datensatzt einzulesen, die empirische verteilungsfunktion des datensatztes zu zeichnen, aber das ganze so, dass die y-achse nach der quantilsfunktion der exponetialverteilung mit parameter=1 transformiert werden soll!
> leider steh ich da jetzt wirklich an!
> w?rde mich serh freuen, wenn mir jemand weiterhelfen k?nnte,
> mit herzlichem dank,
> reka
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
---------------------------------------------------------
Dr. Sander P. Oom
Animal, Plant and Environmental Sciences
University of the Witwatersrand
Private Bag 3
Wits 2050
South Africa

Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64

Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From robin.smit at tno.nl  Thu Feb 24 11:35:16 2005
From: robin.smit at tno.nl (Smit, Robin)
Date: Thu, 24 Feb 2005 11:35:16 +0100
Subject: [R] Forward Stepwise regression based on partial F test
Message-ID: <2395774549BBDA40AC83BC9E6223FBFF22F8A4@MS-DT01VS01.tsn.tno.nl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050224/dd138a22/attachment.pl

From ligges at statistik.uni-dortmund.de  Thu Feb 24 11:38:47 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 24 Feb 2005 11:38:47 +0100
Subject: [R] zeichnung eines wahrscheinlichkeitsnetztes
In-Reply-To: <421DAC4D.5000008@oomvanlieshout.net>
References: <000801c51a56$8c6de890$f2356c50@namexrzcok2x9n>
	<421DAC4D.5000008@oomvanlieshout.net>
Message-ID: <421DAEB7.2050206@statistik.uni-dortmund.de>

Sander Oom wrote:

> Hi Ernst,
> 
> Pretty soon some one is going to: 1) tell you that the mailing list is 
> in English, 2) that you should provide more details of your data and 2) 
> that your message should show signs that you have read the posting guide 
> and have searched the help documentation in a first attempt to find the 
> answer to your question.

... and in particular make clear why others should solve Ernst's homeworks?!

Uwe Ligges


> 
> Maybe you can reply to you own question, considering the above. Then 
> people will surely be very helpful!
> 
> Good luck,
> 
> Sander.
> 
> ernst hoffer wrote:
> 
>> ich h?tte ein problem: im rahmen meiner projektarbeit stehe ich vor 
>> der aufgabe, einen beliebigen datensatzt einzulesen, die empirische 
>> verteilungsfunktion des datensatztes zu zeichnen, aber das ganze so, 
>> dass die y-achse nach der quantilsfunktion der exponetialverteilung 
>> mit parameter=1 transformiert werden soll!
>> leider steh ich da jetzt wirklich an!
>> w?rde mich serh freuen, wenn mir jemand weiterhelfen k?nnte,
>> mit herzlichem dank,
>> reka
>>
>>     [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>>
>



From jtk at cmp.uea.ac.uk  Thu Feb 24 12:35:49 2005
From: jtk at cmp.uea.ac.uk (Jan T. Kim)
Date: Thu, 24 Feb 2005 11:35:49 +0000
Subject: [R] basic question about changing limits on generated plots
In-Reply-To: <200502240214.j1O2Eoj1030458@five-percent-nation.mit.edu>
References: <1109207977.25893.17.camel@horizons.localdomain>
	<200502240214.j1O2Eoj1030458@five-percent-nation.mit.edu>
Message-ID: <20050224113549.GA9254@jtkpc.cmp.uea.ac.uk>

On Wed, Feb 23, 2005 at 09:14:50PM -0500, rif wrote:

> This does not do what the matlab code I posted does (the matlab code
> also works in the free program octave, if you want to try).  The
> matlab code moves already plotted data within the window (replots it).
> When I first type plot(1:10,1:10), I see a graph with axis limits [1
> 10 1 10].  When I type hold on (to keep my original data), and execute
> plot(2:12,5:15), the plot I see is equivalent to the plot I'd have
> gotten if I'd originally specified axis limits [1 12 5 15].  By
> contrast, in the R code you sent, it's as if I'm superimposing two
> unrelated plots.
> 
> Essentially, the underlying "task" is that I want to compare multiple
> functions, but I do not know good limits for the complete set of
> functions when I start.  Being able to adjust the graph to show all
> the data I've plotted so far would be extremely useful for exploratory
> analysis.  This is the mode I and colleagues generally use matlab and
> octave in.
> 
> Does this question get asked all the time?  It seems to be something
> that would come up a lot for people who switch from Matlab/Octave to
> R, but I searched the archives and didn't really see anything.

FWIW, I use constructs such as

    plotfuncs <- function(x, func, ...)
    {
      y <- as.list(1:length(func));
      for (i in 1:length(func))
      {
	y[[i]] <- sapply(x, func[[i]]);
      }
      xlim <- c(min(x), max(x));
      ylim <- c(min(sapply(y, min)), max(sapply(y, max)));
      plot.new();
      plot.window(xlim, ylim, ...);
      for (i in 1:length(func))
      {
	lines(x, y[[i]]);
      }
      axis(1, ...);
      axis(2, ...);
      box(...);
    }

    plotfuncs(1:100 / 100, list(sqrt, log, exp))

which allows you to add further functions incrementally, as in

    plotfuncs(1:100 / 100, list(sqrt, log, exp, function(x) {3 / (x + 1)}))

Perhaps, that's what you have in mind, and probably, that's what (some)
others do...

Best regards, Jan
-- 
 +- Jan T. Kim -------------------------------------------------------+
 |    *NEW*    email: jtk at cmp.uea.ac.uk                               |
 |    *NEW*    WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
 *-----=<  hierarchical systems are for files, not for humans  >=-----*



From murdoch at stats.uwo.ca  Thu Feb 24 11:51:14 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 24 Feb 2005 10:51:14 +0000
Subject: [R] Slightly off topic but concerning R#DSC-2005
In-Reply-To: <156CDC8CCFD1894295D2907F16337A48B2AE66@bru-s-006.europe.shell.com>
References: <156CDC8CCFD1894295D2907F16337A48B2AE66@bru-s-006.europe.shell.com>
Message-ID: <jacr111mtv31skiouuqetmmqrm09u92lf6@4ax.com>

On Wed, 23 Feb 2005 11:52:26 +0100, "Ritter, Christian C
GSMCIL-GSTMS/2" <christian.ritter at shell.com> wrote :

>Has anyone seen an official announcement to DSC-2005. 

It's just been posted to R-devel; you can see it online at

http://tolstoy.newcastle.edu.au/R/devel/05/02/2326.html

Duncan Murdoch



From thomas at wana.at  Thu Feb 24 12:23:15 2005
From: thomas at wana.at (Thomas Wana)
Date: Thu, 24 Feb 2005 12:23:15 +0100
Subject: [R] Modified allan variance
Message-ID: <421DB923.5040201@wana.at>

Hi,

I was wondering if someone of you has functions ready
for calculating the modified allan variance / modified
allan variation from a time series? I can't find such a
function coming with the stock (Debian) R package.

Thanks,
    Tom



From l.scalone at abdn.ac.uk  Thu Feb 24 12:31:40 2005
From: l.scalone at abdn.ac.uk (Scalone, Luciana)
Date: Thu, 24 Feb 2005 11:31:40 -0000
Subject: [R] How to code a bootstrap version of the Wilcoxon-Mann-Whitney 
	test?
Message-ID: <3674BFE56314494EA344C3512E4B83D501690F@gaffer.medicine.abdn.ac.uk>

Dear Stefan
I need to apply a Mann Whitney test to my data, by applying a bootstrap, in
R.
Loonkig for an answer in internet I found your question to the R mailing
list but I have not found the answer to your question.
Please, could you help me?

thank you very much in advance.
Best regards
Luciana



From andreas.wolf at uni-jena.de  Thu Feb 24 12:37:09 2005
From: andreas.wolf at uni-jena.de (Andreas Wolf)
Date: Thu, 24 Feb 2005 12:37:09 +0100
Subject: [R] problem (bug?) with prelim.norm (package norm)
Message-ID: <AF874507D8BE4945941321229E2AAF3906BF62@cat.app.metpsy.uni-jena.de>

dear list members,
there seems to be a problem with the prelim.norm function (package norm)
as number of items in the dataset increases.

the output of prelim.norm() is a list with different summary statistics,
one of them is the missingness indicator matrix "r". it lists all
patterns of missing data and a count of how often each pattern occured
in the dataset. as the number of items and number of patterns increases,
it seems to malfunction, as it stops after less than 200 patterns and
the count for the last row/pattern equals the number of subjects minus
the number of patterns listed before.

let's give an example: i generate multivariate normal data for 40
variables and 500 observations. i randomly delete 10 percent of the
values for each person (i.e. set them to NA). as the number of possible
patterns of missings (combinations without repetition: 4 over 40) is
91390, you'd expect to have (almost) as many different patterns of
missings as subjects in the dataset (~ 500). however, running
prelim.norm, the "r" matrix indicates some 170 patterns (it varies in
multiple runs !!), the last pattern to be some 320 times in the dataset
(which is, of course, not true if you check).

any ideas? 


INPUT:
x <- matrix(rnorm(20000),500,40)   # generate 50 variables with 500
observations

for (tmp in 1:500) {
  draw <- sample(1:40, 4, replace=F)
  x[tmp, draw] <- NA
}   # set (random) 10 percent of values per observation to NA

library(norm)
s <- prelim.norm(x)   # run prelim.norm from package norm
s$r   # missingness indicator matrix (0-missing, 1-observed)
dimnames(s$r)[[1]][length(s$r[,1])]   # count for (supposedly) last
pattern

tmp <- which(s$r[length(s$r[,1]),] == 0)   # vector of items
(supposedly) missing in last pattern
which(is.na(x[,tmp[1]]) & is.na(x[,tmp[2]]) & is.na(x[,tmp[3]]) &
is.na(x[,tmp[4]]))   # list cases with last pattern




p.s. it works fine up to 30 items ... hence, it's not due to the
absolute number of patterns, as there're almost as many patterns as
subjects with 3 out of 30 items missing (possible patterns: 3 over 30 =
4060)

p.p.s. i first thought of the recursion limit in R, but it doesn't help
( options(expressions = 100000) )



From ligges at statistik.uni-dortmund.de  Thu Feb 24 12:59:45 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 24 Feb 2005 12:59:45 +0100
Subject: [R] a question about function eval()
In-Reply-To: <000601c51a47$2640fa80$77d40893@S119>
References: <000601c51a47$2640fa80$77d40893@S119>
Message-ID: <421DC1B1.60500@statistik.uni-dortmund.de>

Feng Chen wrote:

> Hi,
> 
> I have a question about the usage of eval(). Wonder if any experienced user can help me out of it.
> 
> I use eval() in the following function:
> semireg.pwl <- function(coef.s=rnorm(1),coef.a=rnorm(1),knots.pos=knots.x,knots.ini.val=knots.val){
>   knotn <- length(knots.pos)
>   def.par.env <- sys.frame(1)
>   print(def.par.env)
>   print(environment(coef.s))
>   tg <- eval( (parse(text=
>                      paste(
>                            "function(coef.sex=coef.s,coef.age=coef.a,",
>                            paste("knot.val.",1:knotn,"=knots.ini.val[",1:knotn,"]",sep="",collapse=","),
>                            ")",
>                            "{\n print(sys.frame());print(coef.sex)\n  y <- c(",
>                            paste("knot.val.",1:knotn,sep="",collapse=","),
>                            ")\n",
>                            "  -loglikelihood(coef.sex,coef.age,knots.pos,y)\n}",
>                            sep="",collapse=""
>                            )
>                      )
>                )
>              ,envir=def.par.env)

Why do you want to use eval(parse(.....))? That's not sensible in this 
case. You can either alculate the stuff doirectly or define a function 
instead....

Uwe Ligges



>   print(tg())
>   print(coef.s)
>   print(sys.frame(1))
>   mle(tg)
> }
> 
> But when I ran semireg.pwl(), I got correct value for tg(), but an message 
> "Error in eval(expr, envir, enclos) : Object "coef.s" not found" 
> for mle(tg). I just don't know how to make the environment variable visible for mle().
> 
> Thanks a lot!
> 
> Feng
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From f.harrell at vanderbilt.edu  Thu Feb 24 13:15:03 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 24 Feb 2005 07:15:03 -0500
Subject: [R] Forward Stepwise regression based on partial F test
In-Reply-To: <2395774549BBDA40AC83BC9E6223FBFF22F8A4@MS-DT01VS01.tsn.tno.nl>
References: <2395774549BBDA40AC83BC9E6223FBFF22F8A4@MS-DT01VS01.tsn.tno.nl>
Message-ID: <421DC547.2080402@vanderbilt.edu>

Smit, Robin wrote:
> I am hoping to get some advise on the following:
>  
> I am looking for an automatic variable selection procedure to reduce the
> number of potential predictor variables (~ 50) in a multiple regression
> model.
>  
> I would be interested to use the forward stepwise regression using the
> partial F test. 
> I have looked into possible R-functions but could not find this
> particular approach. 
>  
> There is a function (stepAIC) that uses the Akaike criterion or Mallow's
> Cp criterion. 
> In addition, the drop1 and add1 functions came closest to what I want
> but with them I cannot perform the required procedure. 
> Do you have any ideas? 
>  
> Kind regards,
> Robin Smit
> --------------------------------------------
> Business Unit TNO Automotive
> Environmental Studies & Testing
> PO Box 6033, 2600 JA Delft
> THE NETHERLANDS

Robin,

If you are looking for a method that does not offer the best predictive 
accuracy and that violates every aspect of statistical inference, you 
are on the right track.  See 
http://www.stata.com/support/faqs/stat/stepwise.html for details.

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From Markus.Gesmann at lloyds.com  Thu Feb 24 13:12:49 2005
From: Markus.Gesmann at lloyds.com (Gesmann, Markus)
Date: Thu, 24 Feb 2005 12:12:49 +0000
Subject: [R] Place more than one key with xyplot
Message-ID: <321C3EEBDB00C24185705B8BF733DADD0503F6CC@LNVCNTEXCH01.corp.lloydsnet>

Dear R-users

I have some trouble to generate more than one key with xyplot using the
legend argument.
I would like one key with rectangles:

library(lattice)
library(grid)
key1 <- list(rectangles = list(col= c(rev(heat.colors(5))[1:5],
                                 rev(heat.colors(5))[4:1])),
                    title="Percentiles",
                    space="right",
                    text = list(lab = rev(c(" 5-15", "15-25", "25-35",
"35-45",
                                  "45-55","55-65", "65-75", "75-85",
"85-95"))),
                    columns = 1)

and another one with lines:

key2 <- list(lines=list(col=2), text=list(lab="Mean"))

Each of them works fine: 

xyplot(1~1, key = key1)
xyplot(1~1, key = key2)

Unfortunately I don't really understand which "fun" (for legend) I have
to use.
The following line does not work.

xyplot(1~1, legend = list(right=list(key1, fun="grob"),top=list(key2,
fun="grob")))


Kind Regards

Markus

************LNSCNTMCS01***************************************************
The information in this E-Mail and in any attachments is CONFIDENTIAL and may be privileged.  If you are NOT the intended recipient, please destroy this message and notify the sender immediately.  You should NOT retain, copy or use this E-mail for any purpose, nor disclose all or any part of its contents to any other person or persons.

Any views expressed in this message are those of the individual sender, EXCEPT where the sender specifically states them to be the views of Lloyd's.

Lloyd's may monitor the content of E-mails sent and received via its
network for viruses or unauthorised use and for other lawful
business purposes."

Lloyd's is authorised under the Financial Services and Markets Act 2000



From adi at roda.ro  Thu Feb 24 13:13:31 2005
From: adi at roda.ro (Adrian Dusa)
Date: Thu, 24 Feb 2005 12:13:31 +0000 (UTC)
Subject: [R] Graphics
References: <4E9E9056B353854C8AB90B2B6647183009A09813@exchange1.exchange.tvu.ac.uk>
	<Pine.LNX.4.61.0502221728460.2197@gannet.stats>
Message-ID: <loom.20050224T130825-335@post.gmane.org>

Prof Brian Ripley <ripley <at> stats.ox.ac.uk> writes:

> 
> On Tue, 22 Feb 2005 Cedric.Ginestet <at> tvu.ac.uk wrote:
> 
> > The R platform that I installed on my Windows XP crashes everytime that
> > I try to run some sophisticated graphics (e.g. Demo Graphics). Is that
> > to do with the configuration? Shall I reinstall it?
> 
> Please consult the rw-FAQ.
> 
> It is likely to be a problem with your Windows installation, as R runs on 
> literally thousands (maybe tens of thousands) of Windows XP machines.
> 
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> which points you at the rw-FAQ.
> 

I have a similar problem; I am sure there's something I should do on my machine
but I just can not figure out what.
On:
> demo(graphics)

after two enters, I get:

> title(main = "January Pie Sales", cex.main = 1.8,
    font.main = 1)
Error in title(main = "January Pie Sales", cex.main = 1.8, font.main = 1) :
        X11 font at size 22 could not be loaded

I read the "R Installation and Administration" manual, I recompiled R using all
the options (e.g. --with-x), I have all the requred packages...

My system: SuSE 9.2 Professional
> version
         _
platform i686-pc-linux-gnu
arch     i686
os       linux-gnu
system   i686, linux-gnu
status
major    2
minor    0.1
year     2004
month    11
day      15
language R

Any hint would be highly appreciated,
Adrian



From Allan at STATS.uct.ac.za  Thu Feb 24 13:46:10 2005
From: Allan at STATS.uct.ac.za (Clark Allan)
Date: Thu, 24 Feb 2005 14:46:10 +0200
Subject: [R] r: functions
References: <421D9B71.D576E4BC@STATS.uct.ac.za>
	<00db01c51a58$989da680$0540210a@www.domain>
Message-ID: <421DCC92.65B07C0E@STATS.uct.ac.za>

hi

i still get the same error. i must be doing something wrong. i have
typed in all of the steps i used.

D
  x1 x2  y
1  1  1 10
2  2  6  6
3  3 10  7

x<-D[,1:2]
y<-D[,3]

> x
  x1 x2
1  1  1
2  2  6
3  3 10

> y
[1] 10  6  7

> simple1 <- function(xdata, ydata){
+     ofit <- substitute(lm(ydata~xdata))
+     list(eval.parent(ofit))
+ }

simple1(xdata=x, ydata=y)
Error in model.frame(formula, rownames, variables, varnames, extras,
extranames,  : 
        invalid variable type

I STILL GET THE ERROR. WHAT AM I DOING WRONG?


Dimitris Rizopoulos wrote:
> 
> There was a similar question about one week ago regarding the use of
> "table(x,y)". One approach could be to use the following:
> 
> simple1 <- function(xdata, ydata){
>     ofit <- substitute(lm(ydata~xdata))
>     list(eval.parent(ofit))
> }
> #############
> simple1(xdata=x, ydata=y)
> lm(y~x)
> 
> I hope it helps.
> 
> Best,
> Dimitris
> 
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
> 
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/16/336899
> Fax: +32/16/337015
> Web: http://www.med.kuleuven.ac.be/biostat/
>      http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
> 
> ----- Original Message -----
> From: "Clark Allan" <Allan at stats.uct.ac.za>
> To: <r-help at stat.math.ethz.ch>
> Sent: Thursday, February 24, 2005 10:16 AM
> Subject: [R] r: functions
> 
> > hi all
> >
> >
> > i have a function that uses two inputs, say xdata and ydata. An
> > example
> > is the following,
> >
> > simple1<-function(xdata,ydata)
> > {
> > ofit<-lm(ydata~xdata)
> > list(ofit)
> > }
> >
> > say i use arbitray number for xdata and ydata such that
> >
> > D =
> > x1 x2 y
> > 1 1 10
> > 2 6 6
> > 3 10 7
> >
> >
> > x<-D[,1:2]
> >
> > and
> >
> > y<-D[,3]
> >
> > if one uses these inputs and rund the program we get the following:
> >
> >>simple(xdata=x,ydata=y)
> > Error in model.frame(formula, rownames, variables, varnames, extras,
> > extranames,  :
> >        invalid variable type
> >
> > why does this happen!
> >
> > i can get results if i change the program as follows:
> >
> > simple2<-function(xdata,ydata)
> > {
> > ofit<-lm(as.matrix(ydata)~as.matrix(xdata))
> > list(ofit)
> > }
> >
> > but then the variable names, if they exist, are not preserved. how
> > can i
> > preserve these names.
> >
> > the results are now:
> >
> >> simple2(xdata=x,ydata=y)
> > [[1]]
> >
> > Call:
> > lm(formula = as.matrix(ydata) ~ as.matrix(xdata))
> >
> > Coefficients:
> >       (Intercept)  as.matrix(xdata)x1  as.matrix(xdata)x2
> >                -6                  21                  -5
> >
> > i've tried converting xdata and ydata to data frames but i still get
> > errors.
> >
> > simple3<-function(xdata,ydata)
> > {
> > xdata<-as.data.frame(xdata)
> > ydata<-as.data.frame(ydata)
> > ofit<-lm(ydata~xdata)
> > list(ofit)
> > }
> >
> > i.e.
> >
> >> simple3(xdata=x,ydata=y)
> > Error in model.frame(formula, rownames, variables, varnames, extras,
> > extranames,  :
> >        invalid variable type
> >
> > please help!
> >
> > thanking you in advance
> >
> > ***
> > allan
> 
> --------------------------------------------------------------------------------
> 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html

From dimitris.rizopoulos at med.kuleuven.ac.be  Thu Feb 24 13:57:00 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Thu, 24 Feb 2005 13:57:00 +0100
Subject: [R] r: functions
References: <421D9B71.D576E4BC@STATS.uct.ac.za>
	<00db01c51a58$989da680$0540210a@www.domain>
	<421DCC92.65B07C0E@STATS.uct.ac.za>
Message-ID: <016e01c51a70$54501180$0540210a@www.domain>

probably D (it would be better not to use D since this is reserved for 
an R function) is a data.frame and thus also

x <- D[,1:2]

is a data.frame!

try this:

D <- data.frame(x1=1:3, x2=c(1,6,10), y=c(10,6,7)) # your data.frame
D. <- data.matrix(D)
x <- D.[,1:2]
y <- D.[,3]
simple1 <- function(xdata, ydata){
     ofit <- substitute(lm(ydata~xdata))
     list(eval.parent(ofit))
}
##################
simple1(xdata=x, ydata=y)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Clark Allan" <Allan at STATS.uct.ac.za>
To: "Dimitris Rizopoulos" <dimitris.rizopoulos at med.kuleuven.ac.be>
Cc: <r-help at stat.math.ethz.ch>
Sent: Thursday, February 24, 2005 1:46 PM
Subject: Re: [R] r: functions


> hi
>
> i still get the same error. i must be doing something wrong. i have
> typed in all of the steps i used.
>
> D
>  x1 x2  y
> 1  1  1 10
> 2  2  6  6
> 3  3 10  7
>
> x<-D[,1:2]
> y<-D[,3]
>
>> x
>  x1 x2
> 1  1  1
> 2  2  6
> 3  3 10
>
>> y
> [1] 10  6  7
>
>> simple1 <- function(xdata, ydata){
> +     ofit <- substitute(lm(ydata~xdata))
> +     list(eval.parent(ofit))
> + }
>
> simple1(xdata=x, ydata=y)
> Error in model.frame(formula, rownames, variables, varnames, extras,
> extranames,  :
>        invalid variable type
>
> I STILL GET THE ERROR. WHAT AM I DOING WRONG?
>
>
> Dimitris Rizopoulos wrote:
>>
>> There was a similar question about one week ago regarding the use 
>> of
>> "table(x,y)". One approach could be to use the following:
>>
>> simple1 <- function(xdata, ydata){
>>     ofit <- substitute(lm(ydata~xdata))
>>     list(eval.parent(ofit))
>> }
>> #############
>> simple1(xdata=x, ydata=y)
>> lm(y~x)
>>
>> I hope it helps.
>>
>> Best,
>> Dimitris
>>
>> ----
>> Dimitris Rizopoulos
>> Ph.D. Student
>> Biostatistical Centre
>> School of Public Health
>> Catholic University of Leuven
>>
>> Address: Kapucijnenvoer 35, Leuven, Belgium
>> Tel: +32/16/336899
>> Fax: +32/16/337015
>> Web: http://www.med.kuleuven.ac.be/biostat/
>>      http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
>>
>> ----- Original Message -----
>> From: "Clark Allan" <Allan at stats.uct.ac.za>
>> To: <r-help at stat.math.ethz.ch>
>> Sent: Thursday, February 24, 2005 10:16 AM
>> Subject: [R] r: functions
>>
>> > hi all
>> >
>> >
>> > i have a function that uses two inputs, say xdata and ydata. An
>> > example
>> > is the following,
>> >
>> > simple1<-function(xdata,ydata)
>> > {
>> > ofit<-lm(ydata~xdata)
>> > list(ofit)
>> > }
>> >
>> > say i use arbitray number for xdata and ydata such that
>> >
>> > D =
>> > x1 x2 y
>> > 1 1 10
>> > 2 6 6
>> > 3 10 7
>> >
>> >
>> > x<-D[,1:2]
>> >
>> > and
>> >
>> > y<-D[,3]
>> >
>> > if one uses these inputs and rund the program we get the 
>> > following:
>> >
>> >>simple(xdata=x,ydata=y)
>> > Error in model.frame(formula, rownames, variables, varnames, 
>> > extras,
>> > extranames,  :
>> >        invalid variable type
>> >
>> > why does this happen!
>> >
>> > i can get results if i change the program as follows:
>> >
>> > simple2<-function(xdata,ydata)
>> > {
>> > ofit<-lm(as.matrix(ydata)~as.matrix(xdata))
>> > list(ofit)
>> > }
>> >
>> > but then the variable names, if they exist, are not preserved. 
>> > how
>> > can i
>> > preserve these names.
>> >
>> > the results are now:
>> >
>> >> simple2(xdata=x,ydata=y)
>> > [[1]]
>> >
>> > Call:
>> > lm(formula = as.matrix(ydata) ~ as.matrix(xdata))
>> >
>> > Coefficients:
>> >       (Intercept)  as.matrix(xdata)x1  as.matrix(xdata)x2
>> >                -6                  21                  -5
>> >
>> > i've tried converting xdata and ydata to data frames but i still 
>> > get
>> > errors.
>> >
>> > simple3<-function(xdata,ydata)
>> > {
>> > xdata<-as.data.frame(xdata)
>> > ydata<-as.data.frame(ydata)
>> > ofit<-lm(ydata~xdata)
>> > list(ofit)
>> > }
>> >
>> > i.e.
>> >
>> >> simple3(xdata=x,ydata=y)
>> > Error in model.frame(formula, rownames, variables, varnames, 
>> > extras,
>> > extranames,  :
>> >        invalid variable type
>> >
>> > please help!
>> >
>> > thanking you in advance
>> >
>> > ***
>> > allan
>>
>> --------------------------------------------------------------------------------
>>
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide!
>> > http://www.R-project.org/posting-guide.html



From Allan at STATS.uct.ac.za  Thu Feb 24 14:01:56 2005
From: Allan at STATS.uct.ac.za (Clark Allan)
Date: Thu, 24 Feb 2005 15:01:56 +0200
Subject: [R] Re: r: functions
References: <421D9B71.D576E4BC@STATS.uct.ac.za>
Message-ID: <421DD044.411C96E5@STATS.uct.ac.za>

hi all

i worked it out. the following code seems to work. thanx for those who
replied to previous questions.


simple1<-function(xdata,ydata)
{
	DATA<-data.frame(ydata,xdata)
        ofit<-summary(lm(ydata~.,data=DATA))
        list(ofit)
}
simple1(x,y)


> simple1(x,y)
[[1]]

Call:
lm(formula = ydata ~ ., data = DATA)

Coefficients:
(Intercept)           x1           x2  
         -6           21           -5



***
allan


Clark Allan wrote:
> 
> hi all
> 
> i have a function that uses two inputs, say xdata and ydata. An example
> is the following,
> 
> simple1<-function(xdata,ydata)
> {
>         ofit<-lm(ydata~xdata)
>         list(ofit)
> }
> 
> say i use arbitray number for xdata and ydata such that
> 
> D =
> x1      x2      y
> 1       1       10
> 2       6       6
> 3       10      7
> 
> x<-D[,1:2]
> 
> and
> 
> y<-D[,3]
> 
> if one uses these inputs and rund the program we get the following:
> 
> >simple(xdata=x,ydata=y)
> Error in model.frame(formula, rownames, variables, varnames, extras,
> extranames,  :
>         invalid variable type
> 
> why does this happen!
> 
> i can get results if i change the program as follows:
> 
> simple2<-function(xdata,ydata)
> {
>         ofit<-lm(as.matrix(ydata)~as.matrix(xdata))
>         list(ofit)
> }
> 
> but then the variable names, if they exist, are not preserved. how can i
> preserve these names.
> 
> the results are now:
> 
> > simple2(xdata=x,ydata=y)
> [[1]]
> 
> Call:
> lm(formula = as.matrix(ydata) ~ as.matrix(xdata))
> 
> Coefficients:
>        (Intercept)  as.matrix(xdata)x1  as.matrix(xdata)x2
>                 -6                  21                  -5
> 
> i've tried converting xdata and ydata to data frames but i still get
> errors.
> 
> simple3<-function(xdata,ydata)
> {
>         xdata<-as.data.frame(xdata)
>         ydata<-as.data.frame(ydata)
>         ofit<-lm(ydata~xdata)
>         list(ofit)
> }
> 
> i.e.
> 
> > simple3(xdata=x,ydata=y)
> Error in model.frame(formula, rownames, variables, varnames, extras,
> extranames,  :
>         invalid variable type
> 
> please help!
> 
> thanking you in advance
> 
> ***
> allan

From fciclone at bol.com.br  Thu Feb 24 14:43:39 2005
From: fciclone at bol.com.br (Alex)
Date: Thu, 24 Feb 2005 10:43:39 -0300
Subject: [R] Forward Stepwise regression based on partial F test
Message-ID: <ICF4SR$15C5467A53837D96329317E2C8A022D4@bol.com.br>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050224/ae398769/attachment.pl

From ripley at stats.ox.ac.uk  Thu Feb 24 14:56:48 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 24 Feb 2005 13:56:48 +0000 (GMT)
Subject: [R] Graphics
In-Reply-To: <loom.20050224T130825-335@post.gmane.org>
References: <4E9E9056B353854C8AB90B2B6647183009A09813@exchange1.exchange.tvu.ac.uk>
	<Pine.LNX.4.61.0502221728460.2197@gannet.stats>
	<loom.20050224T130825-335@post.gmane.org>
Message-ID: <Pine.LNX.4.61.0502241353380.11628@gannet.stats>

On Thu, 24 Feb 2005, Adrian Dusa wrote:

> Prof Brian Ripley <ripley <at> stats.ox.ac.uk> writes:
>
>>
>> On Tue, 22 Feb 2005 Cedric.Ginestet <at> tvu.ac.uk wrote:
>>
>>> The R platform that I installed on my Windows XP crashes everytime that
>>> I try to run some sophisticated graphics (e.g. Demo Graphics). Is that
>>> to do with the configuration? Shall I reinstall it?
>>
>> Please consult the rw-FAQ.
>>
>> It is likely to be a problem with your Windows installation, as R runs on
>> literally thousands (maybe tens of thousands) of Windows XP machines.
>>
>>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>> which points you at the rw-FAQ.
>>
>
> I have a similar problem; I am sure there's something I should do on my machine
> but I just can not figure out what.

Excuse me: you are not on Windows XP and your R is *not* crashing (see the 
posting guide).  So in what way is the problem similar?  My advice you 
quote is not pertinent to your problem.

> On:
>> demo(graphics)
>
> after two enters, I get:
>
>> title(main = "January Pie Sales", cex.main = 1.8,
>    font.main = 1)
> Error in title(main = "January Pie Sales", cex.main = 1.8, font.main = 1) :
>        X11 font at size 22 could not be loaded
>
> I read the "R Installation and Administration" manual, I recompiled R using all
> the options (e.g. --with-x), I have all the requred packages...

This is a problem with your X configuration, not with R.

> My system: SuSE 9.2 Professional
>> version
>         _
> platform i686-pc-linux-gnu
> arch     i686
> os       linux-gnu
> system   i686, linux-gnu
> status
> major    2
> minor    0.1
> year     2004
> month    11
> day      15
> language R
>
> Any hint would be highly appreciated,


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From petr.pikal at precheza.cz  Thu Feb 24 15:03:27 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 24 Feb 2005 15:03:27 +0100
Subject: [R] other than default labels in lattice plot
Message-ID: <421DECBF.30483.1948EB6@localhost>

Dear all

I solved a problem of customised labels on strips and boxes in bwplot 
by this construction.

> bbb <- bwplot(zavoj ~ typmleti | pu)
> bbb$condlevels$pu <- c("Povrchov? ?prava", "Bez PU")
> bbb$x.limits <- c("Mleto", "Mleto a s?tov?no", "Nemleto")
> bbb

but I wonder if some other easy option exist. Let say something like

bwplot(zavoj~typmleti | pu, 
some advanced stuff like 
box.labels=c("Mleto", "Mleto a s?tov?no", "Nemleto"), 
strip.labels =  c("Povrchov? ?prava", "Bez PU")
)

Thank you

Petr Pikal
petr.pikal at precheza.cz



From fengchen at hkusua.hku.hk  Thu Feb 24 15:19:44 2005
From: fengchen at hkusua.hku.hk (Feng Chen)
Date: Thu, 24 Feb 2005 22:19:44 +0800
Subject: [R] a question about function eval()
Message-ID: <001c01c51a7b$e641d1e0$77d40893@S119>


----- Original Message ----- 
From: "Feng Chen" <fengchen at hkusua.hku.hk>
To: "Uwe Ligges" <ligges at statistik.uni-dortmund.de>
Sent: Thursday, February 24, 2005 9:52 PM
Subject: Re: [R] a question about function eval()


> Dear Uwe,
>
> Thanks for your advice. As you may have noticed, I am a newbie to R and 
> not fully aware of the power of such mechanisms like eval(), call(), 
> substitute(), ...
>
> In fact, the likelihood function I want to maximize has a nonparametric 
> component. I am trying to approximate it using special functions of known 
> form but with a finite number, say n, of unknown parameters. I want the 
> number n to be variable, so I use vector formals to pass into the 
> (log)likelihood function the parameters of the approximating function. I 
> tried to maximize the loglikelihood, but without success. Then I guess the 
> function passed into mle() should not have vectors or lists as its 
> arguments. Therefore, I try to generate functions with a variable number 
> of arguments based on the loglikelihood function and use mle() to the new 
> functions. That is why I have got all these troubles. Maybe there are more 
> convenient alternatives I simply don't know.
>
> By the way, If I use objects globally visible (such as constants) as the 
> default values for coef.sex,...,knot.val.n. Then it does work. But now I 
> am just curious whether it is possible to use objects existing in the 
> parent function as the default values for the arguments of the child 
> function (in this eval() case).
>
> Any advice is appreciated,
> Feng
>
> ----- Original Message ----- 
> From: "Uwe Ligges" <ligges at statistik.uni-dortmund.de>
> To: "Feng Chen" <fengchen at hkusua.hku.hk>
> Cc: <r-help at stat.math.ethz.ch>
> Sent: Thursday, February 24, 2005 7:59 PM
> Subject: Re: [R] a question about function eval()
>
>
>> Feng Chen wrote:
>>
>>> Hi,
>>>
>>> I have a question about the usage of eval(). Wonder if any experienced 
>>> user can help me out of it.
>>>
>>> I use eval() in the following function:
>>> semireg.pwl <- 
>>> function(coef.s=rnorm(1),coef.a=rnorm(1),knots.pos=knots.x,knots.ini.val=knots.val){
>>>   knotn <- length(knots.pos)
>>>   def.par.env <- sys.frame(1)
>>>   print(def.par.env)
>>>   print(environment(coef.s))
>>>   tg <- eval( (parse(text=
>>>                      paste(
>>>                            "function(coef.sex=coef.s,coef.age=coef.a,",
>>>
>>> paste("knot.val.",1:knotn,"=knots.ini.val[",1:knotn,"]",sep="",collapse=","),
>>>                            ")",
>>>                            "{\n print(sys.frame());print(coef.sex)\n  y 
>>> <- c(",
>>>
>>> paste("knot.val.",1:knotn,sep="",collapse=","),
>>>                            ")\n",
>>>
>>> 
>>>                          "  -loglikelihood(coef.sex,coef.age,knots.pos,y)\n}",
>>>                            sep="",collapse=""
>>>                            )
>>>                      )
>>>                )
>>>              ,envir=def.par.env)
>>
>> Why do you want to use eval(parse(.....))? That's not sensible in this 
>> case. You can either alculate the stuff doirectly or define a function 
>> instead....
>>
>> Uwe Ligges
>>
>>
>>
>>>   print(tg())
>>>   print(coef.s)
>>>   print(sys.frame(1))
>>>   mle(tg)
>>> }
>>>
>>> But when I ran semireg.pwl(), I got correct value for tg(), but an 
>>> message "Error in eval(expr, envir, enclos) : Object "coef.s" not found" 
>>> for mle(tg). I just don't know how to make the environment variable 
>>> visible for mle().
>>>
>>> Thanks a lot!
>>>
>>> Feng
>>> [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>



From zh107 at york.ac.uk  Thu Feb 24 16:47:14 2005
From: zh107 at york.ac.uk (Zhesi He)
Date: Thu, 24 Feb 2005 15:47:14 +0000
Subject: [R] Why R GUI application so slow
In-Reply-To: <421D9B71.D576E4BC@STATS.uct.ac.za>
References: <421D9B71.D576E4BC@STATS.uct.ac.za>
Message-ID: <5AE2371C-867B-11D9-A0C6-000A95AA8E42@york.ac.uk>

Hi, All,

I use RGtk and Rtcltk to build some graphical interface for 
bioinformatics data analysis applications.
I'm just wondering why the graphic display is so slow. For example, I 
tried to build a 2000 rows & 5 levels tree with both Rgtk and Rtcltk. 
RGtk cost about 15 seconds and Rtcltk about 10 seconds to load up...

Is there a fatser way to build such application? I have difficulty 
install RJava in my machine, is RJava faster in building the tree 
representation?

BTW, I'm using ibook G4 MacOS 10.3, R2.0

Many thanks.
___________________________________________________

Zhesi He
Computational Biology Laboratory, University of York
York YO10 5YW, U.K.
Phone:  +44-(0)1904-328279
Email:  zh107 at york.ac.uk



From tlumley at u.washington.edu  Thu Feb 24 17:42:53 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 24 Feb 2005 08:42:53 -0800 (PST)
Subject: [R] survreg with gamma distribution: re-post
In-Reply-To: <7C773F4D89C83147A023704E9E781037038EAFB9@cantwe.giga.canterbury.ac.nz>
References: <7C773F4D89C83147A023704E9E781037038EAFB9@cantwe.giga.canterbury.ac.nz>
Message-ID: <Pine.A41.4.61b.0502240838570.149212@homer05.u.washington.edu>

On Thu, 24 Feb 2005, Roger Dungan wrote:

> Dear r-help subscribers,
>
> A couple of weeks ago I sent the following message to the r-help mail
> list. It hasn't generated any response, and I could really use some help
> on this. Anyone able to help?
>

You can't easily fit a gamma distribution with survreg() unless it has 
known shape parameter.  survreg() fits location-scale models, optionally 
with log transformation, and the gamma family is not one of these.

 	-thomas




> Thanks again,
>
> Roger Dungan
>
>>>
> I am working on some survival analysis of some interval censored failure
> time data in R. I have done similar analysis before using PROC LIFEREG
> in SAS. In that instance, a gamma survival function was the optimum
> parametric model for describing the survival and hazard functions. I
> would like to be able to use a gamma function in R, but apparently the
> survival package does not support this distribution. I have been
> googling around for some help, and have found some threads to a similar
> question posted to the R-Help list in October last year. Because I am a
> bit of a survival analysis and R newbie, I didn't really understand the
> discussion thread.
>
> I've been working with a Weibull distribution, thus:
>
>> leafsurv.weibull<-survreg(Surv(minage, maxage, censorcode, type =
>> "interval")~1, dist = "weib")
>
> And I guess I'd like to be able to do something that's the equivalent of
>
>> leafsurv.gamma<-survreg(Surv(minage, maxage, censorcode, type =
>> "interval")~1, dist = "gamma")
>
> At least one of the R-help listserver comments mentioned using
> survreg.distributions to customise a gamma distribution, but I can't
> figure out how to make this work with the resources (intellectual and
> bibliographical!) that I have available.
>
> With thanks in advance for your help,
>
> Dr Roger Dungan
> School of Biological Sciences
> University of Cantebury
> Christchurch, New Zealand
> ph +64 3 366 7001 ext. 4848
> fax +64 3 354 2590
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From HDoran at air.org  Thu Feb 24 17:53:38 2005
From: HDoran at air.org (Doran, Harold)
Date: Thu, 24 Feb 2005 11:53:38 -0500
Subject: [R] model matrix for random effects (lme)
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7407E41A90@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050224/7e3ca49c/attachment.pl

From deepayan at stat.wisc.edu  Thu Feb 24 17:48:24 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 24 Feb 2005 10:48:24 -0600
Subject: [R] Place more than one key with xyplot
In-Reply-To: <321C3EEBDB00C24185705B8BF733DADD0503F6CC@LNVCNTEXCH01.corp.lloydsnet>
References: <321C3EEBDB00C24185705B8BF733DADD0503F6CC@LNVCNTEXCH01.corp.lloydsnet>
Message-ID: <200502241048.24926.deepayan@stat.wisc.edu>

On Thursday 24 February 2005 06:12, Gesmann, Markus wrote:
> Dear R-users
>
> I have some trouble to generate more than one key with xyplot using
> the legend argument.
> I would like one key with rectangles:
>
> library(lattice)
> library(grid)
> key1 <- list(rectangles = list(col= c(rev(heat.colors(5))[1:5],
>                                  rev(heat.colors(5))[4:1])),
>                     title="Percentiles",
>                     space="right",
>                     text = list(lab = rev(c(" 5-15", "15-25",
> "25-35", "35-45",
>                                   "45-55","55-65", "65-75", "75-85",
> "85-95"))),
>                     columns = 1)
>
> and another one with lines:
>
> key2 <- list(lines=list(col=2), text=list(lab="Mean"))
>
> Each of them works fine:
>
> xyplot(1~1, key = key1)
> xyplot(1~1, key = key2)
>
> Unfortunately I don't really understand which "fun" (for legend) I
> have to use.
> The following line does not work.
>
> xyplot(1~1, legend = list(right=list(key1, fun="grob"),top=list(key2,
> fun="grob")))

You want

xyplot(1~1, 
       legend = list(right = list(fun="draw.key", 
                         args = list(key = key1, draw = FALSE)),
                     top = list(fun="draw.key", 
                         args = list(key = key2, draw = FALSE))))

?draw.key should explain why. 

Deepayan



From spencer.graves at pdf.com  Thu Feb 24 18:29:22 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 24 Feb 2005 09:29:22 -0800
Subject: [R] Bayesian stepwise (was:  Forward Stepwise regression based on
	partial F test)
In-Reply-To: <421DC547.2080402@vanderbilt.edu>
References: <2395774549BBDA40AC83BC9E6223FBFF22F8A4@MS-DT01VS01.tsn.tno.nl>
	<421DC547.2080402@vanderbilt.edu>
Message-ID: <421E0EF2.6040401@pdf.com>

      Does anyone know of research fully Bayesian stepwise procedures 
assuming that models not considered by the stepwise would essentially 
have zero posterior probability? 

      I need to analyze the results of ad hoc experiments run in 
manufacturing with crazy confounding and possible supersaturation (i.e., 
more potentially explanatory variables than runs), when each run is very 
expensive in both time and money.   There have to be ways to summarize 
concisely and intelligently what the data can tell us and what remains 
uncertain, including the level of partial confounding between 
alternative explanations.  I think I've gotten reasonable results with 
my own modification of Venables & Ripley's stepAIC to compute an 
approximate posterior over tested models using the AICc criterion 
described, e.g., by Burnham and Anderson (2002) Model Selection and 
Multi-Model Inference (Springer).  Preliminary simulations showed that 
when I used the naive prior (that all models are equally likely, 
including the null model), the null model is usually rejected when 
true.  What a surprise!  I think I can fix that using a more intelligent 
prior.  I also think I can evaluate the partial confounding between 
alternative models by studying the correlation matrix between the 
predictions of alternative models. 

      Comments?
      Thanks,
      Spencer Graves

Frank E Harrell Jr wrote:

> Smit, Robin wrote:
>
>> I am hoping to get some advise on the following:
>>  
>> I am looking for an automatic variable selection procedure to reduce the
>> number of potential predictor variables (~ 50) in a multiple regression
>> model.
>>  
>> I would be interested to use the forward stepwise regression using the
>> partial F test. I have looked into possible R-functions but could not 
>> find this
>> particular approach.  
>> There is a function (stepAIC) that uses the Akaike criterion or Mallow's
>> Cp criterion. In addition, the drop1 and add1 functions came closest 
>> to what I want
>> but with them I cannot perform the required procedure. Do you have 
>> any ideas?  
>> Kind regards,
>> Robin Smit
>> --------------------------------------------
>> Business Unit TNO Automotive
>> Environmental Studies & Testing
>> PO Box 6033, 2600 JA Delft
>> THE NETHERLANDS
>
>
> Robin,
>
> If you are looking for a method that does not offer the best 
> predictive accuracy and that violates every aspect of statistical 
> inference, you are on the right track.  See 
> http://www.stata.com/support/faqs/stat/stepwise.html for details.
>



From ccaruso at montana.edu  Thu Feb 24 18:36:21 2005
From: ccaruso at montana.edu (Christian J. Caruso)
Date: Thu, 24 Feb 2005 10:36:21 -0700
Subject: [R] circ.summary question
Message-ID: <3a93eebc91591c67af4a194ae65137b7@montana.edu>


When using circ.summary is it is necessary to convert radian data? I 
have run circ.summary and it reports an n of 1 when I have 2620 points.
Thanks,
Chris



From murdoch at stats.uwo.ca  Thu Feb 24 18:48:21 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 24 Feb 2005 17:48:21 +0000
Subject: [R] circ.summary question
In-Reply-To: <3a93eebc91591c67af4a194ae65137b7@montana.edu>
References: <3a93eebc91591c67af4a194ae65137b7@montana.edu>
Message-ID: <fp4s119ntgf2c5ai8m0e1ld30nq44ko1ie@4ax.com>

On Thu, 24 Feb 2005 10:36:21 -0700, "Christian J. Caruso"
<ccaruso at montana.edu> wrote :

>
>When using circ.summary is it is necessary to convert radian data? I 
>have run circ.summary and it reports an n of 1 when I have 2620 points.

You need to give more details.  What package is circ.summary in? What
version are you using?  What are you doing.  And of course: 

>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Duncan Murdoch



From deepayan at stat.wisc.edu  Thu Feb 24 18:19:23 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 24 Feb 2005 11:19:23 -0600
Subject: [R] other than default labels in lattice plot
In-Reply-To: <421DECBF.30483.1948EB6@localhost>
References: <421DECBF.30483.1948EB6@localhost>
Message-ID: <200502241119.23622.deepayan@stat.wisc.edu>

On Thursday 24 February 2005 08:03, Petr Pikal wrote:
> Dear all
>
> I solved a problem of customised labels on strips and boxes in bwplot
> by this construction.
>
> > bbb <- bwplot(zavoj ~ typmleti | pu)
> > bbb$condlevels$pu <- c("Povrchov? ?prava", "Bez PU")
> > bbb$x.limits <- c("Mleto", "Mleto a s?tov?no", "Nemleto")
> > bbb
>
> but I wonder if some other easy option exist. Let say something like
>
> bwplot(zavoj~typmleti | pu,
> some advanced stuff like
> box.labels=c("Mleto", "Mleto a s?tov?no", "Nemleto"),
> strip.labels =  c("Povrchov? ?prava", "Bez PU")
> )

I think the most natural way to do this would be to change the labels of 
the factor levels directly. If you don't want to do this to your 
original data, you can do it on the fly as follows:


relabel <- function(x, labels)
{
    stopifnot(is.factor(x))
    levels(x) <- labels
    x
}

y <- rnorm(100)
x <- gl(3, 1, 100)
g <- gl(2, 1, 100)

bwplot(y ~ relabel(x, c("Mleto", "Mleto a s?tov?no", "Nemleto")) |
       relabel(g, c("Povrchov? ?prava", "Bez PU")))


Of course, you can also do it your way:

bwplot(y ~ x | g,
       xlim = c("Mleto", "Mleto a s?tov?no", "Nemleto"),
       strip = strip.custom(factor.levels =
       c("Povrchov? ?prava", "Bez PU")))

This use of 'xlim' is a short-hand, the traditional way of changing the 
x-axis labels is 

       scales = list(x = list(labels = c("Mleto", "Mleto a s?tov?no",
                                         "Nemleto")))

Changing strip labels is more complicated if you have more than one 
conditioning variable.

Deepayan



From akniss at uwyo.edu  Thu Feb 24 19:34:54 2005
From: akniss at uwyo.edu (Andrew Kniss)
Date: Thu, 24 Feb 2005 11:34:54 -0700
Subject: [R] power.anova.test for interaction effects
In-Reply-To: <421C06E2.8090905@arcriswell.com>
Message-ID: <000501c51a9f$921921d0$6a07070a@andrew>

Using the data set posted at 
http://uwstudentweb.uwyo.edu/A/AKNISS/sbxherb.txt
I obtained this output from R:

> data(sbxherb)
> attach(sbxherb)
> sbxherb$rep<-factor(c("I","II","III"))
> sbxherb$var<-factor(sbxherb$var)
> sbxherb$trt<-factor(sbxherb$trt, labels=c("Check","Early","Late","Micro"))
> detach(sbxherb)
> attach(sbxherb)
> aov.sbx<-aov(yield~rep + trt * var + Error(rep/(trt+var)))
> summary(aov.sbx)

Error: rep
    Df  Sum Sq Mean Sq
rep  2 122.158  61.079

Error: rep:trt
          Df  Sum Sq Mean Sq F value Pr(>F)
trt        3 201.131  67.044  1.3096  0.355
Residuals  6 307.160  51.193               

Error: rep:var
          Df Sum Sq Mean Sq F value    Pr(>F)    
var       36 4613.9   128.2  8.4392 1.425e-14 ***
Residuals 72 1093.5    15.2                      
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 

Error: Within
           Df  Sum Sq Mean Sq F value Pr(>F)
trt:var   108  956.62    8.86  0.9731 0.5574
Residuals 216 1966.04    9.10


I would like to use this output to help design next years study, which will
be carried out with the same design.  However, I would like to know what the
sample size should be in order to have an 80% chance of finding a 10%
difference in the interaction term trt:var at alpha=0.05.  Is there a way to
coerce power.anova.test() to make this calculation for me?  I have tried
using this function to give me the sample size needed to find differences
given the within and between groups variance from the previous study, but
with no success.  If I have it calculate the power from this data set, it
tells me I have a power of 1, which I doubt is true given the high p-value
and wide confidence intervals around the estimates.

> power.anova.test(groups=148,
+                  sig.level=0.05,
+                  power=0.8,
+                  within.var=9.10,
+                  between.var=8.86)
Error in uniroot(function(n) eval(p.body) - power, c(2, 1e+05)) : 
        f() values at end points not of opposite sign
> power.anova.test(groups=148,
+                  sig.level=0.05,
+                  n=3,
+                  within.var=9.10,
+                  between.var=8.86)

     Balanced one-way analysis of variance power calculation 

         groups = 148
              n = 3
    between.var = 8.86
     within.var = 9.1
      sig.level = 0.05
          power = 1

 NOTE: n is number in each group


Any help is greatly appreciated.  I apologize if I seem to be nagging on
this issue, but I thought this post was much better at describing what I am
after.  Thanks in advance.

Andrew Kniss
University of Wyoming
akniss at uwyo.edu




-----Original Message-----
From: Andrew Criswell [mailto:andrew at arcriswell.com] 
Sent: Tuesday, February 22, 2005 9:30 PM
To: akniss at uwyo.edu
Subject: Re: [R] power.anova.test for interaction effects

Dear Andrew,

If you could supply us with your data, more help would possibly be 
forthcoming.

Best wishes,
Andrew

Andrew Criswell, PhD
Graduate School, Bangkok University



From nawaaz at inktomi.com  Thu Feb 24 19:36:27 2005
From: nawaaz at inktomi.com (Nawaaz Ahmed)
Date: Thu, 24 Feb 2005 10:36:27 -0800
Subject: [R] Do environments make copies?
Message-ID: <421E1EAB.90506@inktomi.com>

I am using environments to avoid making copies (by keeping references). 
But it seems like there is a hidden copy going on somewhere - for
example in the code fragment below, I am creating a reference to "y"
(of size 500MB) and storing the reference in object "data". But when I 
save "data" and then restore it in another R session, gc() claims it is 
using twice the amount of memory. Where/How is this happening?

Thanks for any help in working around this - my datasets are just not 
fitting into my 4GB, 32 bit linux machine (even though my actual data 
size is around 800MB)

Nawaaz

 > new.ref <- function(value = NULL) {
+     ref <- list(env = new.env())
+     class(ref) <- "refObject"
+     assign("value", value, env = ref$env)
+     ref
+ }
 > object.size(y)
[1] 587941404
 > y.ref = new.ref(y)
 > object.size(y.ref)
[1] 328
 > data = list()
 > data$y.ref = y.ref
 > object.size(data)
[1] 492
 > save(data, "data.RData")

...

run R again
===========

 > load("data.RData")
 > gc()
             used   (Mb) gc trigger   (Mb)
Ncells    141051    3.8     350000    9.4
Vcells 147037925 1121.9  147390241 1124.5



From dsonneborn at ucdavis.edu  Thu Feb 24 20:16:42 2005
From: dsonneborn at ucdavis.edu (Dean Sonneborn)
Date: Thu, 24 Feb 2005 11:16:42 -0800
Subject: [R] 3 boxplots in one 
Message-ID: <6.1.0.6.2.20050224111152.032d2a30@yellow.ucdavis.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050224/aef59845/attachment.pl

From andy_liaw at merck.com  Thu Feb 24 20:22:56 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 24 Feb 2005 14:22:56 -0500
Subject: [R] 3 boxplots in one
Message-ID: <3A822319EB35174CA3714066D590DCD50994E791@usrymx25.merck.com>

Do you want the boxplots to be in the same panel, or different panels, but
the same page/window?  If the former, it's probably easiest to construct a
data frame that will let you use bwplot to put them in the same panel.  If
the latter, see the help page for print.trellis on how you can place several
lattice graphics into the same page.

Andy

> From: Dean Sonneborn
> 
> 
> I currently have 3 separate boxplots but would like to put 
> them all in the 
> graphic so they would have the same scale. Below are the 
> three statements 
> and as you can see the Y axis is weight:
> bwplot(AWGT~  male2 ....
> bwplot(AWGT ~ bin_pcb2 .....
> bwplot(AWGT ~ bin_pcb2 | male2 .....
> 
> Does anyone have some sample code where they have done 
> something like this?
> Thanks,
> 
> Dean Sonneborn M.S.
> Public Health Sciences *
> University of California, Davis
> 916 734-6656
> 
> * formerly Epidemiology & Preventive Medicine
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From hjaffee at jhmi.edu  Thu Feb 24 21:20:24 2005
From: hjaffee at jhmi.edu (Harris A. Jaffee)
Date: Thu, 24 Feb 2005 15:20:24 -0500 (EST)
Subject: [R] string concatenation operator
Message-ID: <Pine.GSO.4.44.0502241441230.16271-100000@isis.jhmi.edu>

Why doesn't R have one, like "." in Perl or juxtaposition in awk?

It does not seem impossible to introduce one, if that would be
reasonable.  It would seem to involve adding a table entry to
main/names.c for the binary operator and a corresponding internal
function, say do_dot().  This cannot be simply do_paste(), since
the implied separator is "".  So, do_dot() might be similar, but
if it finds a non-string operand, a syntax error would be issued,
in place of the message from do_paste() noting a bad *argument*.
Precedence must be decided, but it may not matter.  For example,

	"a" . "b" + 1

If . precedes +, then a syntax error results, after "a" . "b" is
computed.  If + precedes ., then an error happens, just earlier.

What else, or is this a dumb idea?



From efg at stowers-institute.org  Thu Feb 24 21:28:05 2005
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Thu, 24 Feb 2005 14:28:05 -0600
Subject: [R] Logic regression equation in character form?
References: <cvig76$3jq$1@sea.gmane.org>
Message-ID: <cvld5c$68k$1@sea.gmane.org>

Perhaps I can ask a more focused question:

"Earl F. Glynn" <efg at stowers-institute.org> wrote in message
news:cvig76$3jq$1 at sea.gmane.org...
> I want to get some "simple" logic regression examples to work before
> exploring a hard problem.
..
> logicfit <- logreg(resp=Y, bin=X,
>                    type = REGRESSION.TYPE<-2,
>                    select = FIT.SINGLE.MODEL<-1,
>                    ntrees=1,
>                    nleaves=2,   # force shape of final tree
>                    anneal.control=Annealing)

OK, I have a logic regression equation and just want it in a character form.

For example, I can see this equation from the R command prompt:

> logicfit
score 0.968
 +2.14 * (((not X4) or ((not X13) and X19)) and (not X3)) -1.25 * ((((not
X1) or (not X3)) and ((not X2) or X20)) and (((not X17) and X16) or ((not
X20) and (not X1))))

But I cannot figure out how to get this equation in character form:

Why does this NOT work?
> typeof(logicfit)
[1] "list"
> class(logicfit)
[1] "logreg"
> x <- paste(print(logicfit))
score 0.968
 +2.14 * (((not X4) or ((not X13) and X19)) and (not X3)) -1.25 * ((((not
X1) or (not X3)) and ((not X2) or X20)) and (((not X17) and X16) or ((not
X20) and (not X1))))
> x
character(0)
> cat(x, "\n")
 [nothing]
>

The "print" makes the following error go away but does not give me the
string displayed on the R console:
> cat(logicfit)
Error in cat(list(...), file, sep, fill, labels, append) :
        argument 1 not yet handled by cat


This simpler analogy DOES work:
> x <- paste( print("string") )
[1] "string"
> x
[1] "string"
> cat(x, "\n")
string
>

Any clues?  Thanks for any help with this.

efg



From ggrothendieck at myway.com  Thu Feb 24 21:27:07 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 24 Feb 2005 20:27:07 +0000 (UTC)
Subject: [R] string concatenation operator
References: <Pine.GSO.4.44.0502241441230.16271-100000@isis.jhmi.edu>
Message-ID: <loom.20050224T212548-70@post.gmane.org>

Harris A. Jaffee <hjaffee <at> jhmi.edu> writes:
 
: Why doesn't R have one, like "." in Perl or juxtaposition in awk?

You could define one like this:

R> "%+%" <- function(x,y) paste(x,y,sep="")
R> "abc" %+% "def"
[1] "abcdef"



From tlumley at u.washington.edu  Thu Feb 24 21:35:35 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 24 Feb 2005 12:35:35 -0800 (PST)
Subject: [R] Logic regression equation in character form?
In-Reply-To: <cvld5c$68k$1@sea.gmane.org>
References: <cvig76$3jq$1@sea.gmane.org> <cvld5c$68k$1@sea.gmane.org>
Message-ID: <Pine.A41.4.61b.0502241233300.149212@homer05.u.washington.edu>

On Thu, 24 Feb 2005, Earl F. Glynn wrote:

> Why does this NOT work?
>> typeof(logicfit)
> [1] "list"
>> class(logicfit)
> [1] "logreg"
>> x <- paste(print(logicfit))
> score 0.968
> +2.14 * (((not X4) or ((not X13) and X19)) and (not X3)) -1.25 * ((((not
> X1) or (not X3)) and ((not X2) or X20)) and (((not X17) and X16) or ((not
> X20) and (not X1))))
>> x
> character(0)


It doesn't work because print() doesn't return the same thing that it 
prints. You need the output, not the return value.
You want capture.output().

 	-thomas



From janbulla at yahoo.com  Thu Feb 24 21:36:59 2005
From: janbulla at yahoo.com (Jan Bulla)
Date: Thu, 24 Feb 2005 21:36:59 +0100 (CET)
Subject: [R] Density of the Multivariate T Distribution
Message-ID: <20050224203659.95596.qmail@web14026.mail.yahoo.com>

Hi,

I am looking for an efficient way to compute the
values of the density function of a multivariate T
distribution - something like "dmvnorm", but for T
distr. Does this exist somewhere?

Many thanks,

Jan Bulla

Goettingen University



From vincent.goulet at act.ulaval.ca  Thu Feb 24 21:42:58 2005
From: vincent.goulet at act.ulaval.ca (Vincent Goulet)
Date: Thu, 24 Feb 2005 15:42:58 -0500
Subject: [R] and [ESS] Starting ESS
In-Reply-To: <BAY10-F53C55C4C4F154B31A1829DD6630@phx.gbl>
References: <BAY10-F53C55C4C4F154B31A1829DD6630@phx.gbl>
Message-ID: <200502241542.58190.vincent.goulet@act.ulaval.ca>

To answer your question: I don't think it matters.

If you're willing to use FSG Emacs instead of XEmacs, I repackaged Emacs 21.3 
for Windows together with recent versions of ESS and AuCTeX for my students 
(I personally run Linux). The package and installation instructions are at:

 http://vgoulet.act.ulaval.ca/download/software/emacs/

(Look for files emacs-21.3-ready.zip, emacs_install_en.txt and perhaps 
ess_s-plus_win_en.txt.)

Hope this helps!

Le 23 F?vrier 2005 18:16, Laura Holt a ?crit :
> Dear R People:
>
> I have finally seen the error of my ways and have decided to use ESS for R
> and S + stuff.
>
> However, I have a question right from the beginning.  I'm somewhat confused
> by the installation instructions.
>
> Do I install XEMACS or ESS first, please?
>
> Windows XP
> R Version 2.0.1
> (S + 6.2)
>
> Thanks so much!
>
> Sincerely,
> Laura Holt
> mailto: lauraholt_983 at hotmail.com
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
  Vincent Goulet, Associate Professor
  ?cole d'actuariat
  Universit? Laval, Qu?bec 
  Vincent.Goulet at act.ulaval.ca   http://vgoulet.act.ulaval.ca



From EFG at Stowers-Institute.org  Thu Feb 24 21:43:20 2005
From: EFG at Stowers-Institute.org (Glynn, Earl)
Date: Thu, 24 Feb 2005 14:43:20 -0600
Subject: [R] Logic regression equation in character form?
Message-ID: <200502242045.j1OKjDLn021170@hypatia.math.ethz.ch>

> -----Original Message-----
> From: Thomas Lumley [mailto:tlumley at u.washington.edu] 
> You want capture.output().

Thank you!  This is exactly what I needed.

efg



From spencer.graves at pdf.com  Thu Feb 24 21:45:54 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 24 Feb 2005 12:45:54 -0800
Subject: [R] string concatenation operator
In-Reply-To: <loom.20050224T212548-70@post.gmane.org>
References: <Pine.GSO.4.44.0502241441230.16271-100000@isis.jhmi.edu>
	<loom.20050224T212548-70@post.gmane.org>
Message-ID: <421E3D02.8060404@pdf.com>

      Can we do something to make the following work: 

 > "+.character" <- function(x,y) paste(x,y,sep="")
 > "abc"+"def"
Error in "abc" + "def" : non-numeric argument to binary operator

      Thanks,
      spencer graves

Gabor Grothendieck wrote:

>Harris A. Jaffee <hjaffee <at> jhmi.edu> writes:
> 
>: Why doesn't R have one, like "." in Perl or juxtaposition in awk?
>
>You could define one like this:
>
>R> "%+%" <- function(x,y) paste(x,y,sep="")
>R> "abc" %+% "def"
>[1] "abcdef"
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From ligges at statistik.uni-dortmund.de  Thu Feb 24 21:49:44 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 24 Feb 2005 21:49:44 +0100
Subject: [R] problem (bug?) with prelim.norm (package norm)
In-Reply-To: <AF874507D8BE4945941321229E2AAF3906BF62@cat.app.metpsy.uni-jena.de>
References: <AF874507D8BE4945941321229E2AAF3906BF62@cat.app.metpsy.uni-jena.de>
Message-ID: <421E3DE8.9010509@statistik.uni-dortmund.de>

Please report bugs in contributed R packages to the packages' 
maintainers, in this case Alvaro A. Novo (in CC).

Uwe Ligges


Andreas Wolf wrote:

> dear list members,
> there seems to be a problem with the prelim.norm function (package norm)
> as number of items in the dataset increases.
> 
> the output of prelim.norm() is a list with different summary statistics,
> one of them is the missingness indicator matrix "r". it lists all
> patterns of missing data and a count of how often each pattern occured
> in the dataset. as the number of items and number of patterns increases,
> it seems to malfunction, as it stops after less than 200 patterns and
> the count for the last row/pattern equals the number of subjects minus
> the number of patterns listed before.
> 
> let's give an example: i generate multivariate normal data for 40
> variables and 500 observations. i randomly delete 10 percent of the
> values for each person (i.e. set them to NA). as the number of possible
> patterns of missings (combinations without repetition: 4 over 40) is
> 91390, you'd expect to have (almost) as many different patterns of
> missings as subjects in the dataset (~ 500). however, running
> prelim.norm, the "r" matrix indicates some 170 patterns (it varies in
> multiple runs !!), the last pattern to be some 320 times in the dataset
> (which is, of course, not true if you check).
> 
> any ideas? 
> 
> 
> INPUT:
> x <- matrix(rnorm(20000),500,40)   # generate 50 variables with 500
> observations
> 
> for (tmp in 1:500) {
>   draw <- sample(1:40, 4, replace=F)
>   x[tmp, draw] <- NA
> }   # set (random) 10 percent of values per observation to NA
> 
> library(norm)
> s <- prelim.norm(x)   # run prelim.norm from package norm
> s$r   # missingness indicator matrix (0-missing, 1-observed)
> dimnames(s$r)[[1]][length(s$r[,1])]   # count for (supposedly) last
> pattern
> 
> tmp <- which(s$r[length(s$r[,1]),] == 0)   # vector of items
> (supposedly) missing in last pattern
> which(is.na(x[,tmp[1]]) & is.na(x[,tmp[2]]) & is.na(x[,tmp[3]]) &
> is.na(x[,tmp[4]]))   # list cases with last pattern
> 
> 
> 
> 
> p.s. it works fine up to 30 items ... hence, it's not due to the
> absolute number of patterns, as there're almost as many patterns as
> subjects with 3 out of 30 items missing (possible patterns: 3 over 30 =
> 4060)
> 
> p.p.s. i first thought of the recursion limit in R, but it doesn't help
> ( options(expressions = 100000) )
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Robert.McGehee at geodecapital.com  Thu Feb 24 22:17:14 2005
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Thu, 24 Feb 2005 16:17:14 -0500
Subject: [R] string concatenation operator
Message-ID: <67DCA285A2D7754280D3B8E88EB5480206741E7F@MSGBOSCLB2WIN.DMN1.FMR.COM>

Not without extending character to another class. If you try to define
this "+" method using S4, you get a proper error indicating that
addition over characters is not allowed. 

> setMethod("+", signature("character", "character"), 
		function(e1,e2) paste(e1,e2, sep = ""))

Error in setMethod("+", signature("vector", "vector"), function(e1, e2)
paste(e1,  : 
	The method for function "+" and signature e1="vector",
e2="vector" is sealed and cannot be re-defined

FYI, your S3 method wouldn't work anyway as the arguments for the +
operator are e1, e2 not x, y.

-----Original Message-----
From: Spencer Graves [mailto:spencer.graves at pdf.com] 
Sent: Thursday, February 24, 2005 3:46 PM
To: Gabor Grothendieck
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] string concatenation operator


      Can we do something to make the following work: 

 > "+.character" <- function(x,y) paste(x,y,sep="")
 > "abc"+"def"
Error in "abc" + "def" : non-numeric argument to binary operator

      Thanks,
      spencer graves

Gabor Grothendieck wrote:

>Harris A. Jaffee <hjaffee <at> jhmi.edu> writes:
> 
>: Why doesn't R have one, like "." in Perl or juxtaposition in awk?
>
>You could define one like this:
>
>R> "%+%" <- function(x,y) paste(x,y,sep="")
>R> "abc" %+% "def"
>[1] "abcdef"
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>  
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Thu Feb 24 22:18:06 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 24 Feb 2005 16:18:06 -0500
Subject: [R] string concatenation operator
Message-ID: <3A822319EB35174CA3714066D590DCD50994E793@usrymx25.merck.com>

I believe not, because:

> get("+")
.Primitive("+")

i.e., it's not an S3 generic.

Andy

> From: Spencer Graves
> 
>       Can we do something to make the following work: 
> 
>  > "+.character" <- function(x,y) paste(x,y,sep="")
>  > "abc"+"def"
> Error in "abc" + "def" : non-numeric argument to binary operator
> 
>       Thanks,
>       spencer graves
> 
> Gabor Grothendieck wrote:
> 
> >Harris A. Jaffee <hjaffee <at> jhmi.edu> writes:
> > 
> >: Why doesn't R have one, like "." in Perl or juxtaposition in awk?
> >
> >You could define one like this:
> >
> >R> "%+%" <- function(x,y) paste(x,y,sep="")
> >R> "abc" %+% "def"
> >[1] "abcdef"
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> >  
> >
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From gunter.berton at gene.com  Thu Feb 24 22:24:47 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 24 Feb 2005 13:24:47 -0800
Subject: [R] Do environments make copies?
In-Reply-To: <421E1EAB.90506@inktomi.com>
Message-ID: <200502242124.j1OLOlcr017655@volta.gene.com>

I was hoping that one of the R gurus would reply to this, but as they have't
(thus far) I'll try. Caveat emptor!

First of all, R passes function arguments by values, so as soon as you call
foo(val) you are already making (at least) one other copy of val for the
call.

Second,you seem to implicitly make the assumption that assign(..., env=)
uses a pointer to point to the values in the environment. I do not know how
R handles environments and assignments like this internally, but your data
seems to indicate that it copies the value and does not merely point to it
(this is where R Core folks can shed more authoritative light). 

Finally, it makes perfect sense to me that, as a data structure, the
environment itself may be small even if it effectively points to (one of
several copies of) large objects, so that object.size(an.environment) could
be small although the environment may "contain" huge arguments. Again, the
details depend on the precise implementation and need clarification by
someone who actually knows what's going on here, which ain't me.

I think the important message is that you shouldn't treat R as C, and you
shouldn't try to circumvent R's internal data structures and conventions. R
is a language designed to implements Chambers's S model of "Programming with
Data." Instead of trying to fool R to handle large data sets, maybe you
should consider whether you really **need** all the data in R at one time
and if sensible partitioning or sampling to analyze only a portion or
portions of the data might not be a more effective strategy.


-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Nawaaz Ahmed
> Sent: Thursday, February 24, 2005 10:36 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Do environments make copies?
> 
> I am using environments to avoid making copies (by keeping 
> references). 
> But it seems like there is a hidden copy going on somewhere - for
> example in the code fragment below, I am creating a reference to "y"
> (of size 500MB) and storing the reference in object "data". 
> But when I 
> save "data" and then restore it in another R session, gc() 
> claims it is 
> using twice the amount of memory. Where/How is this happening?
> 
> Thanks for any help in working around this - my datasets are just not 
> fitting into my 4GB, 32 bit linux machine (even though my actual data 
> size is around 800MB)
> 
> Nawaaz
> 
>  > new.ref <- function(value = NULL) {
> +     ref <- list(env = new.env())
> +     class(ref) <- "refObject"
> +     assign("value", value, env = ref$env)
> +     ref
> + }
>  > object.size(y)
> [1] 587941404
>  > y.ref = new.ref(y)
>  > object.size(y.ref)
> [1] 328
>  > data = list()
>  > data$y.ref = y.ref
>  > object.size(data)
> [1] 492
>  > save(data, "data.RData")
> 
> ...
> 
> run R again
> ===========
> 
>  > load("data.RData")
>  > gc()
>              used   (Mb) gc trigger   (Mb)
> Ncells    141051    3.8     350000    9.4
> Vcells 147037925 1121.9  147390241 1124.5
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From spencer.graves at pdf.com  Thu Feb 24 22:50:08 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 24 Feb 2005 13:50:08 -0800
Subject: [R] string concatenation operator
In-Reply-To: <67DCA285A2D7754280D3B8E88EB5480206741E7F@MSGBOSCLB2WIN.DMN1.FMR.COM>
References: <67DCA285A2D7754280D3B8E88EB5480206741E7F@MSGBOSCLB2WIN.DMN1.FMR.COM>
Message-ID: <421E4C10.9040104@pdf.com>

      Thanks to both Robert McGehee and Andy Liaw for their informative 
(and quick) replies. 

      Best Wishes,
      spencer graves

McGehee, Robert wrote:

>Not without extending character to another class. If you try to define
>this "+" method using S4, you get a proper error indicating that
>addition over characters is not allowed. 
>
>  
>
>>setMethod("+", signature("character", "character"), 
>>    
>>
>		function(e1,e2) paste(e1,e2, sep = ""))
>
>Error in setMethod("+", signature("vector", "vector"), function(e1, e2)
>paste(e1,  : 
>	The method for function "+" and signature e1="vector",
>e2="vector" is sealed and cannot be re-defined
>
>FYI, your S3 method wouldn't work anyway as the arguments for the +
>operator are e1, e2 not x, y.
>
>-----Original Message-----
>From: Spencer Graves [mailto:spencer.graves at pdf.com] 
>Sent: Thursday, February 24, 2005 3:46 PM
>To: Gabor Grothendieck
>Cc: r-help at stat.math.ethz.ch
>Subject: Re: [R] string concatenation operator
>
>
>      Can we do something to make the following work: 
>
> > "+.character" <- function(x,y) paste(x,y,sep="")
> > "abc"+"def"
>Error in "abc" + "def" : non-numeric argument to binary operator
>
>      Thanks,
>      spencer graves
>
>Gabor Grothendieck wrote:
>
>  
>
>>Harris A. Jaffee <hjaffee <at> jhmi.edu> writes:
>>
>>: Why doesn't R have one, like "." in Perl or juxtaposition in awk?
>>
>>You could define one like this:
>>
>>R> "%+%" <- function(x,y) paste(x,y,sep="")
>>R> "abc" %+% "def"
>>[1] "abcdef"
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>>    
>>
>http://www.R-project.org/posting-guide.html
>  
>
>> 
>>
>>    
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
>http://www.R-project.org/posting-guide.html
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From rolf at math.unb.ca  Thu Feb 24 23:02:59 2005
From: rolf at math.unb.ca (Rolf Turner)
Date: Thu, 24 Feb 2005 18:02:59 -0400 (AST)
Subject: [R] Density of the Multivariate T Distribution
Message-ID: <200502242202.j1OM2xb0016403@erdos.math.unb.ca>


Jan Bulla wrote:

> I am looking for an efficient way to compute the
> values of the density function of a multivariate T
> distribution - something like "dmvnorm", but for T
> distr. Does this exist somewhere?

Searching CRAN I found the ``sn'' package which includes the function
dmst() which calculates the density for ***skewed*** multivariate t
distributions.  I conjecture that setting the skewness parameters
``alpha'' equal to 0 would give you the ``ordinary'' multivariate t
distribution.  I haven't tried this out.

It puzzles me why the mvtnorm package includes functions pmvt(),
qmvt(), and rmvt() but ***not*** dmvt().  Why on earth not?

				cheers,

					Rolf Turner
					rolf at math.unb.ca



From mingan at math.unm.edu  Thu Feb 24 23:11:56 2005
From: mingan at math.unm.edu (mingan)
Date: Thu, 24 Feb 2005 15:11:56 -0700
Subject: [R] help: how to get coefficients from the result
Message-ID: <421E512C.3060505@math.unm.edu>


 I am using timereg and Survreg packages



 below is the output, but how can I get the coefficient for zt1 zt2 
directly,
that is how can I get 0.000946 and 0.963000 directly ?

  I tried  out$coeff[1,1] or sth. similar, but does not work


since I will run a simulation of several thousand, so I should get the 
coefficients directly

 thanks

 



 >  out=prop.odds(Surv(t,d)~zt1+zt2)
Proportional odds model
 > summary(out)
Proportional Odds model

Test for baseline
Test for non-siginificant effects
         sup| hat B(t)/SD(t) | p-value H_0: B(t)=0
Baseline                  11.9                   0
Test for time invariant effects
         sup| B(t) - (t/tau)B(tau)| p-value H_0: B(t)=b t
Baseline                        358                 0.342

Covariate effects
       Coef. Std. Error Robust SE D2log(L)^-1
zt1 0.000946     0.0890    0.0885      0.0891
zt2 0.963000     0.0574    0.0584      0.0541
  
Score Tests for Goodness-of-fit
    sup| hat U(t) | p-value H_0
zt1            6.73        0.848
zt2           12.20        0.798

  
  Call:
prop.odds(Surv(t, d) ~ zt1 + zt2)



















n=50; iter=1000
beta1=0;beta2=1

zt1=NULL
zt2=NULL
t=NULL
for( i in 1:n)
{
    z1=rbinom(1,1,0.5)
    z2=rexp(1,1)
    zt1=c(zt1,z1)
    zt2=c(zt2,z2)
    u=runif(1,0,1)
    temp=exp(-z1*beta1-z2*beta2)*u/(1-u)
    t=c(t,temp)
}

   d=rep(1,n)

 mt=cbind(t,d,zt1,zt2)

 out=prop.odds(Surv(t,d)~zt1+zt2)
summary(out)



From luke at stat.uiowa.edu  Thu Feb 24 23:46:40 2005
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Thu, 24 Feb 2005 16:46:40 -0600 (CST)
Subject: [R] Do environments make copies?
In-Reply-To: <200502242124.j1OLOlcr017655@volta.gene.com>
References: <200502242124.j1OLOlcr017655@volta.gene.com>
Message-ID: <Pine.LNX.4.61.0502241606570.12878@nokomis.stat.uiowa.edu>

On Thu, 24 Feb 2005, Berton Gunter wrote:

> I was hoping that one of the R gurus would reply to this, but as they have't
> (thus far) I'll try. Caveat emptor!
>
> First of all, R passes function arguments by values, so as soon as you call
> foo(val) you are already making (at least) one other copy of val for the
> call.

Conceptually you have a copy, but internally R trieas to use a
copy-on-modify strategy to avaoid copying unless necessary.  THere are
conservative approximations involved, so there is more copying than
one might like but definitely not as much as this.


> Second,you seem to implicitly make the assumption that assign(..., env=)
> uses a pointer to point to the values in the environment. I do not know how
> R handles environments and assignments like this internally, but your data
> seems to indicate that it copies the value and does not merely point to it
> (this is where R Core folks can shed more authoritative light).

This assignment does just store the pointer.

> Finally, it makes perfect sense to me that, as a data structure, the
> environment itself may be small even if it effectively points to (one of
> several copies of) large objects, so that object.size(an.environment) could
> be small although the environment may "contain" huge arguments. Again, the
> details depend on the precise implementation and need clarification by
> someone who actually knows what's going on here, which ain't me.
>
> I think the important message is that you shouldn't treat R as C, and you
> shouldn't try to circumvent R's internal data structures and conventions. R
> is a language designed to implements Chambers's S model of "Programming with
> Data." Instead of trying to fool R to handle large data sets, maybe you
> should consider whether you really **need** all the data in R at one time
> and if sensible partitioning or sampling to analyze only a portion or
> portions of the data might not be a more effective strategy.

R can do quite a reasonable job with large data sets on a resonable
platform.  A 32 bit platform is not a reasonable one on which to use R
with 800 MB chunks of data. Automatic memory management combined with
the immutable vector semantics require more elbow room than that.  If
you really must use data of this size on a 32-bit platform you will
probably be muchhappier using a limited amoutn of C code and external
pointers.

As to what is happening in this example: look at the default parent
used by new.env and combine that with the fact that the serialization
code does not preserve sharing of atomic objects.  The two references
to the large object are shared in the original session but lead to two
large objects in the saved image and the load.  Using

     ref <- list(env = new.env(parent = .GlobalEnv))

in new.ref avoids the second copy both in the saved image and after
loading.

luke

>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Nawaaz Ahmed
>> Sent: Thursday, February 24, 2005 10:36 AM
>> To: r-help at stat.math.ethz.ch
>> Subject: [R] Do environments make copies?
>>
>> I am using environments to avoid making copies (by keeping
>> references).
>> But it seems like there is a hidden copy going on somewhere - for
>> example in the code fragment below, I am creating a reference to "y"
>> (of size 500MB) and storing the reference in object "data".
>> But when I
>> save "data" and then restore it in another R session, gc()
>> claims it is
>> using twice the amount of memory. Where/How is this happening?
>>
>> Thanks for any help in working around this - my datasets are just not
>> fitting into my 4GB, 32 bit linux machine (even though my actual data
>> size is around 800MB)
>>
>> Nawaaz
>>
>> > new.ref <- function(value = NULL) {
>> +     ref <- list(env = new.env())
>> +     class(ref) <- "refObject"
>> +     assign("value", value, env = ref$env)
>> +     ref
>> + }
>> > object.size(y)
>> [1] 587941404
>> > y.ref = new.ref(y)
>> > object.size(y.ref)
>> [1] 328
>> > data = list()
>> > data$y.ref = y.ref
>> > object.size(data)
>> [1] 492
>> > save(data, "data.RData")
>>
>> ...
>>
>> run R again
>> ===========
>>
>> > load("data.RData")
>> > gc()
>>              used   (Mb) gc trigger   (Mb)
>> Ncells    141051    3.8     350000    9.4
>> Vcells 147037925 1121.9  147390241 1124.5
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Luke Tierney
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu



From ahenningsen at email.uni-kiel.de  Mon Feb 21 12:56:45 2005
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Mon, 21 Feb 2005 12:56:45 +0100
Subject: [R] [R-pkgs] New package for microeconomics: micEcon
Message-ID: <200502211256.45195.ahenningsen@email.uni-kiel.de>

Dear all,

I have uploaded a new package called micEcon (version 0.1-3) to CRAN 
(an early version of this package has been already presented at useR! 2004).
It contains tools for microeconomic analysis and microeconomic modeling.
These are for instance:

- tools for demand analysis with the 'Almost Ideal Demand System' (AIDS): 
e.g. econometric estimation, calculation of price and income/expenditure 
elasticities, checking for theoretical consistency

- tools for production/firm analysis with the 'Symmetric Normalized 
Quadratic' (SNQ) profit function: e.g. econometric estimation, calculation of 
price elasticities and shadow prices, imposing convexity

- tools for quadratic and translog functions, e.g. econometric estimation, 
calculation of Hessians and derivatives

- functions to write input files for and read output files of Tim Coelli's 
(1996) program "Frontier 4.1" (http://www.uq.edu.au/economics/cepa/ 
software.htm). In this way R can use "Frontier 4.1" to perform stochastic 
frontier analysis.

- a function to perform a two-step Heckman ("heckit") estimation to correct 
for non-random sample selection

- functions to calculate "Laspeyres", "Paasche" or "Fisher" price and quantity 
indices


I have checked most of the functions by comparing their results with values 
published in the literature or obtained by other software. However, I cannot 
guarantee that they return correct results in all possible cases. So please 
check these functions (results, code) and send me bug reports or patches.
You are also invited to provide further functions, e.g. tools for other 
functional forms.

Best regards,
Arne


-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From maechler at stat.math.ethz.ch  Fri Feb 25 00:31:17 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 25 Feb 2005 00:31:17 +0100
Subject: [R] accurate log(1+x) {was "(arbitrary) precision"}
In-Reply-To: <loom.20050218T034935-782@post.gmane.org>
References: <42154593.1070709@duke.edu>
	<loom.20050218T034935-782@post.gmane.org>
Message-ID: <16926.25541.843191.587745@stat.math.ethz.ch>

>>>>> "Gabor" == Gabor Grothendieck <ggrothendieck at myway.com>
>>>>>     on Fri, 18 Feb 2005 02:55:15 +0000 (UTC) writes:

    Gabor> Michael Grottke <Michael.Grottke <at> duke.edu>
    Gabor> writes: : I am currently using R for fitting a model
    Gabor> to various data sets : (minimizing the negative
    Gabor> log-likelihood) and calculating a number of : metrics
    Gabor> based on the parameter estimates. Within these
    Gabor> calculations, I : have steps of the form : :
    Gabor> log(log(1+x)), : : where x can be very small (e.g.,
    Gabor> around 1e-16). Unfortunately, the : precision of
    Gabor> doubles does not suffice for coping with the
    Gabor> difference in : the orders of magnitude of 1 and x:
    Gabor> 1+x is rounded to 1.  : : One way for solving this
    Gabor> problem seems to be to use an arbitrary : precision
    Gabor> library implemented in C and call the respective
    Gabor> routines for : calculating the logarithm(s) from
    Gabor> within R.  : : My questions are as follows: : 1. Is
    Gabor> there any better/more direct way to solve the
    Gabor> problem?  : 2. Is there any arbitrary precision
    Gabor> library you can suggest in particular?  :

    Gabor> The approximation log(1+x) = x would be accuate to
    Gabor> several decimal places in your case so your
    Gabor> expression would reduce to log(log(1+x)) = log(x).

but I think the most efficient solution for this case is to use

 log(log1p(x))

The point of the  log1p(.) function is to return  log(1+x)
accurately when ``for any x'', i.e., most importantly for x << 1.

Martin Maechler, ETH Zurich.



From Robert.McGehee at geodecapital.com  Fri Feb 25 01:15:35 2005
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Thu, 24 Feb 2005 19:15:35 -0500
Subject: [R] help: how to get coefficients from the result
Message-ID: <67DCA285A2D7754280D3B8E88EB548020C1ED604@MSGBOSCLB2WIN.DMN1.FMR.COM>

?coef should do the trick.

-----Original Message-----
From: mingan [mailto:mingan at math.unm.edu] 
Sent: Thursday, February 24, 2005 5:12 PM
To: r-help at stat.math.ethz.ch
Subject: [R] help: how to get coefficients from the result



 I am using timereg and Survreg packages



 below is the output, but how can I get the coefficient for zt1 zt2 
directly,
that is how can I get 0.000946 and 0.963000 directly ?

  I tried  out$coeff[1,1] or sth. similar, but does not work


since I will run a simulation of several thousand, so I should get the 
coefficients directly

 thanks

 



 >  out=prop.odds(Surv(t,d)~zt1+zt2)
Proportional odds model
 > summary(out)
Proportional Odds model

Test for baseline
Test for non-siginificant effects
         sup| hat B(t)/SD(t) | p-value H_0: B(t)=0
Baseline                  11.9                   0
Test for time invariant effects
         sup| B(t) - (t/tau)B(tau)| p-value H_0: B(t)=b t
Baseline                        358                 0.342

Covariate effects
       Coef. Std. Error Robust SE D2log(L)^-1
zt1 0.000946     0.0890    0.0885      0.0891
zt2 0.963000     0.0574    0.0584      0.0541
  
Score Tests for Goodness-of-fit
    sup| hat U(t) | p-value H_0
zt1            6.73        0.848
zt2           12.20        0.798

  
  Call:
prop.odds(Surv(t, d) ~ zt1 + zt2)



















n=50; iter=1000
beta1=0;beta2=1

zt1=NULL
zt2=NULL
t=NULL
for( i in 1:n)
{
    z1=rbinom(1,1,0.5)
    z2=rexp(1,1)
    zt1=c(zt1,z1)
    zt2=c(zt2,z2)
    u=runif(1,0,1)
    temp=exp(-z1*beta1-z2*beta2)*u/(1-u)
    t=c(t,temp)
}

   d=rep(1,n)

 mt=cbind(t,d,zt1,zt2)

 out=prop.odds(Surv(t,d)~zt1+zt2)
summary(out)

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From arrayprofile at yahoo.com  Fri Feb 25 01:33:30 2005
From: arrayprofile at yahoo.com (array chip)
Date: Thu, 24 Feb 2005 16:33:30 -0800 (PST)
Subject: [R] main effect & interaction in 2-way ANOVA
Message-ID: <20050225003331.70473.qmail@web40821.mail.yahoo.com>

Hi,

I am just a little confused of mian effect in the
analysis of variance (ANOVA) when you include or do
not include an interaction term. Let's assume a simple
case of 2-way ANOVA with 2 factors A and B, each with
2 levels. If it shows that main effect for A is
significant when the interaction between A and B is
NOT included, and the main effect for A is NOT
significant when the interaction is included, what
simply does this difference mean? I understand that
main effect for A generally means averaging over
levels of B, is this explanation for the situation
when interaction is included or is not included or is
irrelavant?

And if my interest is in the main effect of A, in the
above senario, should I include the interaction (thus
lose the significance) or not include the interaction
(thus keep my significance)?

Thanks!



From hb at maths.lth.se  Fri Feb 25 01:47:43 2005
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Fri, 25 Feb 2005 01:47:43 +0100
Subject: [R] string concatenation operator 
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E793@usrymx25.merck.com>
Message-ID: <002001c51ad3$9f544990$ae0040d5@hblaptop>

.Primitive() does not necessarily apply non-S3 generic function, cf.
as.character().

An options is to do

"+" <- function(...) UseMethod("+")
"+.default" <- .Primitive("+")
"+.character" <- function(...) paste(...,sep="")
"abc" + "def" + "ghi"

# [1] "abcdefghi"

The question is if you want to do this. The "+" is probably non-generic for
a purpose. One may be that having a generic function for such a basic method
will most likely slow down many methods substantially.

You might wanna try other binary operators. See "R Language Definition"
under "3.1.4 Operators". For example

"~" <- function(...) UseMethod("~")
"~.default" <- .Primitive("~")
"~.character" <- function(...) paste(...,sep="")
"abc" ~ "def" ~ "ghi"

or 

"&" <- function(...) UseMethod("&")
"&.default" <- .Primitive("&")
"&.character" <- function(...) paste(...,sep="")
"abc" & "def" & "ghi"

or

":" <- function(...) UseMethod(":")
":.default" <- .Primitive(":")
":.character" <- function(...) paste(...,sep="")
"abc" : "def" : "ghi"

or

"||" <- function(...) UseMethod("||")
"||.default" <- .Primitive("||")
"||.character" <- function(...) paste(...,sep="")
"abc" || "def" || "ghi"

The "&" or "|" operators are nice but probably not to recommend for similar
reason as "+", but "~" and "||" should be pretty safe considering it is not
common to use multiple times within methods and should not slow things down
too much.

To R experts, except for not being standard procedure, are there reasons why
redefining, say, "~" as above, should be avoided? 

Best

Henrik


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Liaw, Andy
> Sent: Thursday, February 24, 2005 10:18 PM
> To: 'Spencer Graves'
> Cc: r-help at stat.math.ethz.ch
> Subject: RE: [R] string concatenation operator
> 
> 
> I believe not, because:
> 
> > get("+")
> .Primitive("+")
> 
> i.e., it's not an S3 generic.
> 
> Andy
> 
> > From: Spencer Graves
> > 
> >       Can we do something to make the following work:
> > 
> >  > "+.character" <- function(x,y) paste(x,y,sep="")
> >  > "abc"+"def"
> > Error in "abc" + "def" : non-numeric argument to binary operator
> > 
> >       Thanks,
> >       spencer graves
> > 
> > Gabor Grothendieck wrote:
> > 
> > >Harris A. Jaffee <hjaffee <at> jhmi.edu> writes:
> > > 
> > >: Why doesn't R have one, like "." in Perl or juxtaposition in awk?
> > >
> > >You could define one like this:
> > >
> > >R> "%+%" <- function(x,y) paste(x,y,sep="")
> > >R> "abc" %+% "def"
> > >[1] "abcdef"
> > >
> > >______________________________________________
> > >R-help at stat.math.ethz.ch mailing list 
> > >https://stat.ethz.ch/mailman/listinfo/r-help
> > >PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > >  
> > >
> > 
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From carsten.steinhoff at gmx.de  Fri Feb 25 01:51:30 2005
From: carsten.steinhoff at gmx.de (Carsten Steinhoff)
Date: Fri, 25 Feb 2005 01:51:30 +0100
Subject: [R] Simulation Progress
In-Reply-To: <200502241106.j1OB3NrP017560@hypatia.math.ethz.ch>
Message-ID: <200502250051.j1P0pbVb010092@hypatia.math.ethz.ch>

Hi,

I've made a function that executes a monte-carlo simulation.
It always needs a lot of time until e.g. 1Mio simulation steps are done.
So I would like to know, how many percent of the work is already done.

In an Excel/VBA Solution I could easily implement a status bar or status
window.

How could an R-Solution look like?

Carsten



From p.dalgaard at biostat.ku.dk  Fri Feb 25 01:57:35 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 25 Feb 2005 01:57:35 +0100
Subject: [R] main effect & interaction in 2-way ANOVA
In-Reply-To: <20050225003331.70473.qmail@web40821.mail.yahoo.com>
References: <20050225003331.70473.qmail@web40821.mail.yahoo.com>
Message-ID: <x2650hpibk.fsf@biostat.ku.dk>

array chip <arrayprofile at yahoo.com> writes:

> Hi,
> 
> I am just a little confused of mian effect in the
> analysis of variance (ANOVA) when you include or do
> not include an interaction term. Let's assume a simple
> case of 2-way ANOVA with 2 factors A and B, each with
> 2 levels. If it shows that main effect for A is
> significant when the interaction between A and B is
> NOT included, and the main effect for A is NOT
> significant when the interaction is included, what
> simply does this difference mean? I understand that
> main effect for A generally means averaging over
> levels of B, is this explanation for the situation
> when interaction is included or is not included or is
> irrelavant?
> 
> And if my interest is in the main effect of A, in the
> above senario, should I include the interaction (thus
> lose the significance) or not include the interaction
> (thus keep my significance)?
> 
> Thanks!

(Uh, oh, here we go again...)

As a number of people will probably soon point out, you shouldn't even
worry about that. 

Anyways, the results depend competely on parametrization. In the 2x2
layout with contr.treatment, you generally end up with "main effects"
of A testing differences within only one level of B.


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From jwd at surewest.net  Fri Feb 25 02:31:57 2005
From: jwd at surewest.net (John Dougherty)
Date: Thu, 24 Feb 2005 17:31:57 -0800
Subject: [R] Graphics
In-Reply-To: <loom.20050224T130825-335@post.gmane.org>
References: <4E9E9056B353854C8AB90B2B6647183009A09813@exchange1.exchange.tvu.ac.uk>
	<Pine.LNX.4.61.0502221728460.2197@gannet.stats>
	<loom.20050224T130825-335@post.gmane.org>
Message-ID: <200502241731.57657.jwd@surewest.net>

On Thursday 24 February 2005 04:13, Adrian Dusa wrote:
> ...

You need to check your font installation.  Be sure the X-11 fonts are 
installed.    

XFree86-fonts-75dpi-4.3.99.902-30
XFree86-fonts-100dpi-4.3.99.902-30

Should both be on your system.  If they aren't bring up the YaST control 
center and select Install and Remove Software.  You can use the search option 
to filter for packages that have "fonts" in their descritpion.  Install any 
that aren't.  SuSE seems to be a little funny about the X-11 fonts.

Peter Dalgaard just let me know about that a short time ago.

JWDougherty



From ggrothendieck at myway.com  Fri Feb 25 02:31:03 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 25 Feb 2005 01:31:03 +0000 (UTC)
Subject: [R] main effect & interaction in 2-way ANOVA
References: <20050225003331.70473.qmail@web40821.mail.yahoo.com>
Message-ID: <loom.20050225T020203-715@post.gmane.org>

array chip <arrayprofile <at> yahoo.com> writes:

: 
: Hi,
: 
: I am just a little confused of mian effect in the
: analysis of variance (ANOVA) when you include or do
: not include an interaction term. Let's assume a simple
: case of 2-way ANOVA with 2 factors A and B, each with
: 2 levels. If it shows that main effect for A is
: significant when the interaction between A and B is
: NOT included, and the main effect for A is NOT
: significant when the interaction is included, what
: simply does this difference mean? I understand that
: main effect for A generally means averaging over
: levels of B, is this explanation for the situation
: when interaction is included or is not included or is
: irrelavant?
: 
: And if my interest is in the main effect of A, in the
: above senario, should I include the interaction (thus
: lose the significance) or not include the interaction
: (thus keep my significance)?
: 


There are simplified interpretations if you restrict yourself
to hierarchical or graphical (graphical are a subset of hierarchical)
models.  These require that if an effect is set to zero that all
higher order effects that include it must be set to zero too.
Thus if A is set to zero you would have to set AB to zero too
if you wish to follow that philosophy (and also if you want to use
any of the R packages that support it).



From kjetil at acelerate.com  Fri Feb 25 03:43:44 2005
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Thu, 24 Feb 2005 22:43:44 -0400
Subject: [R] Bayesian stepwise
In-Reply-To: <421E0EF2.6040401@pdf.com>
References: <2395774549BBDA40AC83BC9E6223FBFF22F8A4@MS-DT01VS01.tsn.tno.nl>	<421DC547.2080402@vanderbilt.edu>
	<421E0EF2.6040401@pdf.com>
Message-ID: <421E90E0.2010108@acelerate.com>

Spencer Graves wrote:

>      Does anyone know of research fully Bayesian stepwise procedures 
> assuming that models not considered by the stepwise would essentially 
> have zero posterior probability?
>      I need to analyze the results of ad hoc experiments run in 
> manufacturing with crazy confounding and possible supersaturation 
> (i.e., more potentially explanatory variables than runs), when each 
> run is very expensive in both time and money.   There have to be ways 
> to summarize concisely and intelligently what the data can tell us and 
> what remains uncertain,

What about chapter 8 section 5 page 363 in Wu/Hamada "Experiments --- 
Planning , analysis, and Parameter
 design Optimization", titled:
A Bayesian variable selection Strategy for Designs with complex Aliasing

Kjetil


> including the level of partial confounding between alternative 
> explanations.  I think I've gotten reasonable results with my own 
> modification of Venables & Ripley's stepAIC to compute an approximate 
> posterior over tested models using the AICc criterion described, e.g., 
> by Burnham and Anderson (2002) Model Selection and Multi-Model 
> Inference (Springer).  Preliminary simulations showed that when I used 
> the naive prior (that all models are equally likely, including the 
> null model), the null model is usually rejected when true.  What a 
> surprise!  I think I can fix that using a more intelligent prior.  I 
> also think I can evaluate the partial confounding between alternative 
> models by studying the correlation matrix between the predictions of 
> alternative models.
>      Comments?
>      Thanks,
>      Spencer Graves
>
> Frank E Harrell Jr wrote:
>
>> Smit, Robin wrote:
>>
>>> I am hoping to get some advise on the following:
>>>  
>>> I am looking for an automatic variable selection procedure to reduce 
>>> the
>>> number of potential predictor variables (~ 50) in a multiple regression
>>> model.
>>>  
>>> I would be interested to use the forward stepwise regression using the
>>> partial F test. I have looked into possible R-functions but could 
>>> not find this
>>> particular approach.  There is a function (stepAIC) that uses the 
>>> Akaike criterion or Mallow's
>>> Cp criterion. In addition, the drop1 and add1 functions came closest 
>>> to what I want
>>> but with them I cannot perform the required procedure. Do you have 
>>> any ideas?  Kind regards,
>>> Robin Smit
>>> --------------------------------------------
>>> Business Unit TNO Automotive
>>> Environmental Studies & Testing
>>> PO Box 6033, 2600 JA Delft
>>> THE NETHERLANDS
>>
>>
>>
>> Robin,
>>
>> If you are looking for a method that does not offer the best 
>> predictive accuracy and that violates every aspect of statistical 
>> inference, you are on the right track.  See 
>> http://www.stata.com/support/faqs/stat/stepwise.html for details.
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra




-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From WeiQiang.Li at seagate.com  Fri Feb 25 04:26:31 2005
From: WeiQiang.Li at seagate.com (WeiQiang.Li@seagate.com)
Date: Fri, 25 Feb 2005 11:26:31 +0800
Subject: [R] How to set up number of prin comp.
Message-ID: <OFB029381A.81A4C899-ON48256FB3.0012AC03-48256FB3.0012FB9D@seagate.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050225/12bc7312/attachment.pl

From dren_scott at yahoo.com  Fri Feb 25 05:09:14 2005
From: dren_scott at yahoo.com (Dren Scott)
Date: Thu, 24 Feb 2005 20:09:14 -0800 (PST)
Subject: [R] calculatingmean value for duplicates in affy array
Message-ID: <20050225040914.13433.qmail@web61302.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050224/ead4d8e2/attachment.pl

From dr.mike at ntlworld.com  Fri Feb 25 06:10:22 2005
From: dr.mike at ntlworld.com (dr mike)
Date: Fri, 25 Feb 2005 05:10:22 -0000
Subject: [R] Bayesian stepwise (was: Forward Stepwise regression based
	onpartial F test)
Message-ID: <20050225051033.UWZS1289.aamta02-winn.mailhost.ntl.com@c400>

oops,

Forgot to cc to the list.

Regards,

Mike

-----Original Message-----
From: dr mike [mailto:dr.mike at ntlworld.com] 
Sent: 24 February 2005 19:21
To: 'Spencer Graves'
Subject: RE: [R] Bayesian stepwise (was: Forward Stepwise regression based
onpartial F test)

Spencer,

Obviously the problem is one of supersaturation. In view of that, are you
aware of the following?

A Two-Stage Bayesian Model Selection Strategy for Supersaturated Designs
Authors: Beattie S. D; Fong D. K. H; Lin D. K. J
Source: Technometrics, 1 February 2002, vol. 44, no. 1, pp. 55-63 

And:

Analysis Methods for Supersaturated Design: Some Comparisons
Authors: Li R; Lin D. K. J
Source: Journal of Data Sciences, 1, 2003, pp. 249-260

The latter is available for download in full (pdf) by googling for the
title.

HTH

Mike

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Spencer Graves
Sent: 24 February 2005 17:29
To: Frank E Harrell Jr
Cc: r-help at stat.math.ethz.ch
Subject: [R] Bayesian stepwise (was: Forward Stepwise regression based
onpartial F test)

      Does anyone know of research fully Bayesian stepwise procedures
assuming that models not considered by the stepwise would essentially have
zero posterior probability? 

      I need to analyze the results of ad hoc experiments run in
manufacturing with crazy confounding and possible supersaturation (i.e.,
more potentially explanatory variables than runs), when each run is very 
expensive in both time and money.   There have to be ways to summarize 
concisely and intelligently what the data can tell us and what remains
uncertain, including the level of partial confounding between alternative
explanations.  I think I've gotten reasonable results with my own
modification of Venables & Ripley's stepAIC to compute an approximate
posterior over tested models using the AICc criterion described, e.g., by
Burnham and Anderson (2002) Model Selection and Multi-Model Inference
(Springer).  Preliminary simulations showed that when I used the naive prior
(that all models are equally likely, including the null model), the null
model is usually rejected when true.  What a surprise!  I think I can fix
that using a more intelligent prior.  I also think I can evaluate the
partial confounding between alternative models by studying the correlation
matrix between the predictions of alternative models. 

      Comments?
      Thanks,
      Spencer Graves

Frank E Harrell Jr wrote:

> Smit, Robin wrote:
>
>> I am hoping to get some advise on the following:
>>  
>> I am looking for an automatic variable selection procedure to reduce 
>> the number of potential predictor variables (~ 50) in a multiple 
>> regression model.
>>  
>> I would be interested to use the forward stepwise regression using 
>> the partial F test. I have looked into possible R-functions but could 
>> not find this particular approach.
>> There is a function (stepAIC) that uses the Akaike criterion or 
>> Mallow's Cp criterion. In addition, the drop1 and add1 functions came 
>> closest to what I want but with them I cannot perform the required 
>> procedure. Do you have any ideas?
>> Kind regards,
>> Robin Smit
>> --------------------------------------------
>> Business Unit TNO Automotive
>> Environmental Studies & Testing
>> PO Box 6033, 2600 JA Delft
>> THE NETHERLANDS
>
>
> Robin,
>
> If you are looking for a method that does not offer the best 
> predictive accuracy and that violates every aspect of statistical 
> inference, you are on the right track.  See 
> http://www.stata.com/support/faqs/stat/stepwise.html for details.
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From phgrosjean at sciviews.org  Fri Feb 25 08:21:54 2005
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Fri, 25 Feb 2005 08:21:54 +0100
Subject: [R] Simulation Progress
In-Reply-To: <200502250051.j1P0pbVb010092@hypatia.math.ethz.ch>
References: <200502250051.j1P0pbVb010092@hypatia.math.ethz.ch>
Message-ID: <421ED212.5090109@sciviews.org>

Carsten Steinhoff wrote:
 > Hi,
 >
 > I've made a function that executes a monte-carlo simulation.
 > It always needs a lot of time until e.g. 1Mio simulation steps are done.
 > So I would like to know, how many percent of the work is already done.
 >
 > In an Excel/VBA Solution I could easily implement a status bar or status
 > window.
 >
 > How could an R-Solution look like?
 >
 > Carsten


I think that progress() in svMisc package (bundle SciViews on CRAN) will 
make what you are looking for: it displays progression right in the console.
Best,

Philippe Grosjean



From ripley at stats.ox.ac.uk  Fri Feb 25 08:48:29 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 25 Feb 2005 07:48:29 +0000 (GMT)
Subject: [R] string concatenation operator 
In-Reply-To: <002001c51ad3$9f544990$ae0040d5@hblaptop>
References: <002001c51ad3$9f544990$ae0040d5@hblaptop>
Message-ID: <Pine.LNX.4.61.0502250730230.24047@gannet.stats>

On Fri, 25 Feb 2005, Henrik Bengtsson wrote:

> .Primitive() does not necessarily apply non-S3 generic function, cf.
> as.character().

You mean `imply'?  In fact "+" *is* S3 generic and has methods defined:

> methods("+")
[1] +.Date   +.POSIXt

It is also group-generic, part of the Ops group with many more methods 
via that route.  See ?Ops.

> An options is to do
>
> "+" <- function(...) UseMethod("+")
> "+.default" <- .Primitive("+")
> "+.character" <- function(...) paste(...,sep="")

> "abc" + "def" + "ghi"
>
> # [1] "abcdefghi"

[Re-defining R basics is seriously not recommended.]

> The question is if you want to do this. The "+" is probably non-generic for
> a purpose. One may be that having a generic function for such a basic method
> will most likely slow down many methods substantially.

Well, it is generic.  The issue is that only coercion between numeric 
(broad sense, including complex) types is supported for the arithmetical 
operators, presumably to avoid the ambiguity of things like

x <- 123.45
y <- as.character(1)
x + y

Should that be 124.45 or "123.451"?  One of the difficulties of any 
dispatch on two arguments is how to do the best matching on two classes, 
especially with symmetric operators like "+".  Internally R favours simple 
fast rules.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Feb 25 08:58:59 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 25 Feb 2005 07:58:59 +0000 (GMT)
Subject: [R] main effect & interaction in 2-way ANOVA
In-Reply-To: <20050225003331.70473.qmail@web40821.mail.yahoo.com>
References: <20050225003331.70473.qmail@web40821.mail.yahoo.com>
Message-ID: <Pine.LNX.4.61.0502250752190.24047@gannet.stats>

On Thu, 24 Feb 2005, array chip wrote:

> I am just a little confused of mian effect in the
> analysis of variance (ANOVA) when you include or do
> not include an interaction term. Let's assume a simple
> case of 2-way ANOVA with 2 factors A and B, each with
> 2 levels. If it shows that main effect for A is
> significant when the interaction between A and B is
> NOT included, and the main effect for A is NOT
> significant when the interaction is included, what
> simply does this difference mean? I understand that
> main effect for A generally means averaging over
> levels of B,

Not in the presence of an interaction, with R's default coding.

  is this explanation for the situation
> when interaction is included or is not included or is
> irrelavant?
>
> And if my interest is in the main effect of A, in the
> above senario, should I include the interaction (thus
> lose the significance) or not include the interaction
> (thus keep my significance)?

In R's default coding you are being told:

1) That the effect of A in the base level of B is not significant
2) There is a significant difference between the effect of A at the
two levels of B.

So, probably, A has no effect at the base level of B and an effect at the 
other level.  You may or may not be interested in the effect of A averaged 
over the two levels of B.


Note that R's default coding is unusual, and using Helmert contrasts will
give results which are easier to interpret from conventional accounts.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From petr.pikal at precheza.cz  Fri Feb 25 10:30:04 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 25 Feb 2005 10:30:04 +0100
Subject: [R] other than default labels in lattice plot
In-Reply-To: <200502241119.23622.deepayan@stat.wisc.edu>
References: <421DECBF.30483.1948EB6@localhost>
Message-ID: <421EFE2C.21881.9C9A83@localhost>

Thank you for quick answer Deepayan. I should have think of 
relabeling myself but this on the fly action is nice.

Thank you again.
Petr


On 24 Feb 2005 at 11:19, Deepayan Sarkar wrote:

> On Thursday 24 February 2005 08:03, Petr Pikal wrote:
> > Dear all
> >
> > I solved a problem of customised labels on strips and boxes in
> > bwplot by this construction.
> >
> > > bbb <- bwplot(zavoj ~ typmleti | pu)
> > > bbb$condlevels$pu <- c("Povrchov? ?prava", "Bez PU")
> > > bbb$x.limits <- c("Mleto", "Mleto a s?tov?no", "Nemleto")
> > > bbb
> >
> > but I wonder if some other easy option exist. Let say something like
> >
> > bwplot(zavoj~typmleti | pu,
> > some advanced stuff like
> > box.labels=c("Mleto", "Mleto a s?tov?no", "Nemleto"),
> > strip.labels =  c("Povrchov? ?prava", "Bez PU")
> > )
> 
> I think the most natural way to do this would be to change the labels
> of the factor levels directly. If you don't want to do this to your
> original data, you can do it on the fly as follows:
> 
> 
> relabel <- function(x, labels)
> {
>     stopifnot(is.factor(x))
>     levels(x) <- labels
>     x
> }
> 
> y <- rnorm(100)
> x <- gl(3, 1, 100)
> g <- gl(2, 1, 100)
> 
> bwplot(y ~ relabel(x, c("Mleto", "Mleto a s?tov?no", "Nemleto")) |
>        relabel(g, c("Povrchov? ?prava", "Bez PU")))
> 
> 
> Of course, you can also do it your way:
> 
> bwplot(y ~ x | g,
>        xlim = c("Mleto", "Mleto a s?tov?no", "Nemleto"),
>        strip = strip.custom(factor.levels =
>        c("Povrchov? ?prava", "Bez PU")))
> 
> This use of 'xlim' is a short-hand, the traditional way of changing
> the x-axis labels is 
> 
>        scales = list(x = list(labels = c("Mleto", "Mleto a s?tov?no",
>                                    "Nemleto")))
> 
> Changing strip labels is more complicated if you have more than one
> conditioning variable.
> 
> Deepayan

Petr Pikal
petr.pikal at precheza.cz



From bhx2 at mevik.net  Fri Feb 25 10:38:08 2005
From: bhx2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Fri, 25 Feb 2005 10:38:08 +0100
Subject: [R] How to set up number of prin comp.
In-Reply-To: <OFB029381A.81A4C899-ON48256FB3.0012AC03-48256FB3.0012FB9D@seagate.com>
	(WeiQiang Li's message of "Fri, 25 Feb 2005 11:26:31 +0800")
References: <OFB029381A.81A4C899-ON48256FB3.0012AC03-48256FB3.0012FB9D@seagate.com>
Message-ID: <m0mzttughr.fsf@bar.nemo-project.org>

>         I am trying to use PrinComp to do principle component analysis. I 
> would like to know how to set the number of principle components.

I assume you mean the function princomp (case _does_ matter in R) in
package stats (which is loaded by default).  This function has no way
of specifying how many components to calculate; it always gives you
all components.  You have to select the components you want
afterwards.  See help(princomp) for details.  E.g.

X <- some matrix
pc <- princomp(X)
pc$scores[,1:4]    # The four first score vectors
pc$loadings[,1:4]  # The four first loadings

(The loadings can also be extracted with loadings(pc)[,1:4] .)

-- 
Bj?rn-Helge Mevik



From michael.beer at unifr.ch  Fri Feb 25 11:03:30 2005
From: michael.beer at unifr.ch (BEER Michael)
Date: Fri, 25 Feb 2005 11:03:30 +0100
Subject: [R] Problem using stepAIC/addterm (MASS package)
Message-ID: <24D0F1947691984E89F3151A7DC314DDAC274D@EXCHANGE2.unifr.ch>

Hello,

I'm currently dealing with a rather strange problem when using the
function "stepAIC" ("MASS" package).  The setting is the following: From
model learning data sets ("learndata"), I want to be able to build
prediction functions (in order to save them in a file for further use).
This is done by the function "pred.function" (see below). Therein, I'd
like to use "stepAIC" for model selection.

However, if I try to evaluate "pred.function" for a specific data set, R
sometimes stops with the message

---
Error in inherits(x, "data.frame") : Object "learndata" not found
---

Debugging "stepAIC" showed me that the problem occurs in the step where
"addterm" is called. "addterm" somehow doesn't seem to see the object
"learndata" which is present (?) in the function's environment.

Here is my code. The dataset "BostonHousing" is just an example for
which this problem can be observed. "my.MC" is inspired by the "Lexical
scoping" section in the R FAQ
(http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html#Lexical-scoping).

---
data(BostonHousing, package = "mlbench")
my.MC <- function (f, env) f  # make closure

pred.function <- function (learndata) {

    my.model <- lm(medv ~ ., learndata)

    if (!require("MASS"))
        stop("Package MASS not loadable.")

    my.model <- stepAIC(my.model, 
		scope = list(lower = medv ~ 1, 
			upper = medv ~ .))

    my.MC(function(newdata) {
        	predict(my.model, newdata)
    	    },
	    list(my.model = my.model)
    )
}

pf <- pred.function(BostonHousing)
---

How can I tell "addterm" (or "stepAIC") where to find "learndata"?

Thanks for your help.

Regards
Michael

-- 
Michael Beer, dipl. math., University of Freiburg/Fribourg Switzerland
Seminar of Statistics, Av. de Beauregard 13, CH-1700 Freiburg
Phone +41 26 300 8278, Fax +41 26 300 9781
E-mail: michael.beer at unifr.ch, Web: http://www.unifr.ch/stat/



From firas at cs.technion.ac.il  Fri Feb 25 12:06:22 2005
From: firas at cs.technion.ac.il (Firas Swidan)
Date: Fri, 25 Feb 2005 13:06:22 +0200 (IST)
Subject: [R] Loops and dataframes
Message-ID: <Pine.GSO.4.33_heb2.09.0502251256280.16872-100000@csd.cs.technion.ac.il>

Hi,
I am experiencing a long delay when using dataframes inside loops and was
wordering if this is a bug or not.
Example code:

> st <- rep(1,100000)
> ed <- rep(2,100000)
> for(i in 1:length(st)) st[i] <- ed[i] # works fine
> df <- data.frame(start=st,end=ed)
> for(i in 1:dim(df)[1]) df[i,1] <- df[i,2] #takes for ever

R: R 2.0.0 (2004-10-04)
OS: Linux, Fedora Core 2
kernel: 2.6.10-1.14_FC2
cpu: AMD Athlon XP 1600.
mem: 500MB.

The example above is only to illustrate the problem. I need loops to apply
some functions on pairs (not necessarily successive) of rows in a
dataframe.

Thankful for any advices,
Firas.



From oarabile at stams.strath.ac.uk  Fri Feb 25 12:21:41 2005
From: oarabile at stams.strath.ac.uk (Oarabile Ruth Molaodi)
Date: Fri, 25 Feb 2005 11:21:41 +0000
Subject: [R] how to produce disease maps
Message-ID: <421F0A45.1000001@stams.strath.ac.uk>


-------------- next part --------------
An embedded message was scrubbed...
From: Martin Maechler <maechler at stat.math.ethz.ch>
Subject: Re: producing maps
Date: Fri, 25 Feb 2005 00:18:59 +0100
Size: 2511
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050225/75bb1c86/producingmaps.mht

From andy_liaw at merck.com  Fri Feb 25 12:28:44 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 25 Feb 2005 06:28:44 -0500
Subject: [R] Loops and dataframes
Message-ID: <3A822319EB35174CA3714066D590DCD50994E795@usrymx25.merck.com>

You are discovering part of the overhead of using a data frame.  The way you
specify the subset of data frame to replace matters somewhat:

> st <- rep(1,1e4)
> ed <- rep(2,1e4)
> df <- data.frame(start=st, end=ed)
> system.time(for (i in 1:dim(df)[1]) df[i,1] <- df[i,2], gcFirst=TRUE)
[1] 35.96  0.10 36.37    NA    NA
> df <- data.frame(start=st, end=ed)
> system.time(for (i in 1:dim(df)[1]) df[[1]][i] <- df[[2]][i],
gcFirst=TRUE)
[1] 22.63  0.17 22.88    NA    NA
> df <- data.frame(start=st, end=ed)
> system.time(for (i in 1:dim(df)[1]) df$start[i] <- df$end[i],
gcFirst=TRUE)
[1] 19.29  0.13 19.46    NA    NA


If you have all numeric data, you might as well use a matrix instead of data
frame:

> m <- cbind(start=st, end=ed)
> str(m)
 num [1:10000, 1:2] 2 2 2 2 2 2 2 2 2 2 ...
 - attr(*, "dimnames")=List of 2
  ..$ : NULL
  ..$ : chr [1:2] "start" "end"
> system.time(for (i in 1:nrow(df)) m[i,1] <- m[i,2], gcFirst=TRUE)
[1] 0.06 0.00 0.08   NA   NA


Andy


> From: Firas Swidan
> 
> Hi,
> I am experiencing a long delay when using dataframes inside 
> loops and was
> wordering if this is a bug or not.
> Example code:
> 
> > st <- rep(1,100000)
> > ed <- rep(2,100000)
> > for(i in 1:length(st)) st[i] <- ed[i] # works fine
> > df <- data.frame(start=st,end=ed)
> > for(i in 1:dim(df)[1]) df[i,1] <- df[i,2] #takes for ever
> 
> R: R 2.0.0 (2004-10-04)
> OS: Linux, Fedora Core 2
> kernel: 2.6.10-1.14_FC2
> cpu: AMD Athlon XP 1600.
> mem: 500MB.
> 
> The example above is only to illustrate the problem. I need 
> loops to apply
> some functions on pairs (not necessarily successive) of rows in a
> dataframe.
> 
> Thankful for any advices,
> Firas.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From andy_liaw at merck.com  Fri Feb 25 12:33:51 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 25 Feb 2005 06:33:51 -0500
Subject: [R] Loops and dataframes
Message-ID: <3A822319EB35174CA3714066D590DCD50994E796@usrymx25.merck.com>

An addendum:  If you must use a data frame (e.g., you have mixed data
types), the following might help:

> df <- list(start=st, end=ed)
> system.time({for (i in 1:length(df[[1]])) df$start[i] <- df$end[i];
+              df <- as.data.frame(df)}, gcFirst=TRUE)
[1] 0.14 0.01 0.15   NA   NA

I.e., keep it as a list until all manipulations are done, then coerce to
data frame.


Andy 


> From: Liaw, Andy
> 
> You are discovering part of the overhead of using a data 
> frame.  The way you
> specify the subset of data frame to replace matters somewhat:
> 
> > st <- rep(1,1e4)
> > ed <- rep(2,1e4)
> > df <- data.frame(start=st, end=ed)
> > system.time(for (i in 1:dim(df)[1]) df[i,1] <- df[i,2], 
> gcFirst=TRUE)
> [1] 35.96  0.10 36.37    NA    NA
> > df <- data.frame(start=st, end=ed)
> > system.time(for (i in 1:dim(df)[1]) df[[1]][i] <- df[[2]][i],
> gcFirst=TRUE)
> [1] 22.63  0.17 22.88    NA    NA
> > df <- data.frame(start=st, end=ed)
> > system.time(for (i in 1:dim(df)[1]) df$start[i] <- df$end[i],
> gcFirst=TRUE)
> [1] 19.29  0.13 19.46    NA    NA
> 
> 
> If you have all numeric data, you might as well use a matrix 
> instead of data
> frame:
> 
> > m <- cbind(start=st, end=ed)
> > str(m)
>  num [1:10000, 1:2] 2 2 2 2 2 2 2 2 2 2 ...
>  - attr(*, "dimnames")=List of 2
>   ..$ : NULL
>   ..$ : chr [1:2] "start" "end"
> > system.time(for (i in 1:nrow(df)) m[i,1] <- m[i,2], gcFirst=TRUE)
> [1] 0.06 0.00 0.08   NA   NA
> 
> 
> Andy
> 
> 
> > From: Firas Swidan
> > 
> > Hi,
> > I am experiencing a long delay when using dataframes inside 
> > loops and was
> > wordering if this is a bug or not.
> > Example code:
> > 
> > > st <- rep(1,100000)
> > > ed <- rep(2,100000)
> > > for(i in 1:length(st)) st[i] <- ed[i] # works fine
> > > df <- data.frame(start=st,end=ed)
> > > for(i in 1:dim(df)[1]) df[i,1] <- df[i,2] #takes for ever
> > 
> > R: R 2.0.0 (2004-10-04)
> > OS: Linux, Fedora Core 2
> > kernel: 2.6.10-1.14_FC2
> > cpu: AMD Athlon XP 1600.
> > mem: 500MB.
> > 
> > The example above is only to illustrate the problem. I need 
> > loops to apply
> > some functions on pairs (not necessarily successive) of rows in a
> > dataframe.
> > 
> > Thankful for any advices,
> > Firas.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
> --------------------------------------------------------------
> ----------------
> Notice:  This e-mail message, together with any attachments, 
> contains information of Merck & Co., Inc. (One Merck Drive, 
> Whitehouse Station, New Jersey, USA 08889), and/or its 
> affiliates (which may be known outside the United States as 
> Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as 
> Banyu) that may be confidential, proprietary copyrighted 
> and/or legally privileged. It is intended solely for the use 
> of the individual or entity named on this message.  If you 
> are not the intended recipient, and have received this 
> message in error, please notify us immediately by reply 
> e-mail and then delete it from your system.
> --------------------------------------------------------------
> ----------------
>



From sdavis2 at mail.nih.gov  Fri Feb 25 12:35:08 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Fri, 25 Feb 2005 06:35:08 -0500
Subject: [R] Loops and dataframes
In-Reply-To: <Pine.GSO.4.33_heb2.09.0502251256280.16872-100000@csd.cs.technion.ac.il>
References: <Pine.GSO.4.33_heb2.09.0502251256280.16872-100000@csd.cs.technion.ac.il>
Message-ID: <7d125bbeed1bdf8a0ff12226176a5421@mail.nih.gov>


On Feb 25, 2005, at 6:06 AM, Firas Swidan wrote:

> Hi,
> I am experiencing a long delay when using dataframes inside loops and 
> was
> wordering if this is a bug or not.
> Example code:
>
>> st <- rep(1,100000)
>> ed <- rep(2,100000)
>> for(i in 1:length(st)) st[i] <- ed[i] # works fine
>> df <- data.frame(start=st,end=ed)
>> for(i in 1:dim(df)[1]) df[i,1] <- df[i,2] #takes for ever
>
> R: R 2.0.0 (2004-10-04)
> OS: Linux, Fedora Core 2
> kernel: 2.6.10-1.14_FC2
> cpu: AMD Athlon XP 1600.
> mem: 500MB.
>
> The example above is only to illustrate the problem. I need loops to 
> apply
> some functions on pairs (not necessarily successive) of rows in a
> dataframe.

I'm not an expert, but working with dataframes is typically slower than 
the eqivalent matrix.  If it is possible (the data is of the same type, 
as it is above), working with the equivalent matrix is prabably faster. 
  So, I think the general answer to the implied question is that 
dataframe processing is slower than vector processing or the equivalent 
matrix processing.

If you post more details about your specific problem, folks may be able 
to find creative ways of speeding things up, if speed remains a 
concern.

Sean



From jemwa at sun.ac.za  Fri Feb 25 12:46:36 2005
From: jemwa at sun.ac.za (Gorden Jemwa)
Date: Fri, 25 Feb 2005 13:46:36 +0200
Subject: [R] Oriented PCA in R?
Message-ID: <421F101C.3010808@sun.ac.za>

Is there an R implementation of Oriented PCA?

Thanks for your help.

Gorden



From georg.hoermann at gmx.de  Fri Feb 25 12:49:36 2005
From: georg.hoermann at gmx.de (Georg Hoermann)
Date: Fri, 25 Feb 2005 12:49:36 +0100
Subject: [R] Strange Colnames
Message-ID: <421F10D0.5060303@gmx.de>

Hello world,

I am trying to create a matrix of lagged time series with the
following code fragment (I know that I can use acf-function...).
I want to set the variable/column name to something like
"lag 1" etc. If I try to do it I get text like
"lagtest.lagtest.lagtest.lag 1" where does this come from?

After a conversion to a data.frame it works as expected.
What did I miss?

Thanks and greetings,

Georg
==================


 > lagtest <- as.ts(seq(1:100)) ;
 > for (i in 1:4) {lagtest <- cbind(lagtest,lag(ts_test,i)); 
colnames(lagtest)[i+1]<- paste("lag",i)}
 > colnames(lagtest)[1]<-"H_GW";
 > colnames(lagtest)
[1] "H_GW"                          "lagtest.lagtest.lagtest.lag 1" 
"lagtest.lagtest.lag 2"
[4] "lagtest.lag 3"                 "lag 4"


# this works...

 >  t5 <- as.data.frame(lagtest)
 > for (i in 1:4) {names(t5)[i+1] <- paste("var",i) }
 > names(t5)
[1] "H_GW"  "var 1" "var 2" "var 3" "var 4"
 >



From B.Rowlingson at lancaster.ac.uk  Fri Feb 25 13:04:24 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 25 Feb 2005 12:04:24 +0000
Subject: [R] Simulation Progress
In-Reply-To: <421ED212.5090109@sciviews.org>
References: <200502250051.j1P0pbVb010092@hypatia.math.ethz.ch>
	<421ED212.5090109@sciviews.org>
Message-ID: <421F1448.1080509@lancaster.ac.uk>


>  > I've made a function that executes a monte-carlo simulation.
>  > It always needs a lot of time until e.g. 1Mio simulation steps are done.
>  > So I would like to know, how many percent of the work is already done.


  This reminds me of my 'iterator' class I was working on - but never 
really finished.

  Instead of doing:

  for(i in 1:10000000){
   dostuff(i)
}

  which creates a vector of c(1,2,...,10000000), you create an iterator 
object, and do a while loop:

myLoop = loop(N=10000000)
while(iterate(myLoop)){
  dostuff(iteration(myLoop))
}

  now all the information about the loop is encapsulated in the iterator 
object 'myLoop', and there are methods for working out when the loop 
might finish:

predictEnd(myLoop)

  Predicted finish at 12-Dec-02 12:12:34

I also started work on a superclass of this for MCMC runs, where you 
could specify a burn-in period and a sampling thinning parameter, and 
then there were methods for telling if you were in the burn-in period or 
if this was an interation that you were sampling in your output.

  Maybe I'll have a go at cleaning this all up over the easter break and 
making a proper package of it.

Baz



From dimitris.rizopoulos at med.kuleuven.ac.be  Fri Feb 25 13:10:46 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Fri, 25 Feb 2005 13:10:46 +0100
Subject: [R] Loops and dataframes
References: <3A822319EB35174CA3714066D590DCD50994E796@usrymx25.merck.com>
Message-ID: <002801c51b33$09418ca0$0540210a@www.domain>

or something like:

df <- data.frame(start=st, end=ed)
system.time(df[] <- lapply(df, function(x) x[1] <- x[2]), 
gcFirst=TRUE)
# or in general if you have a vectorized function `f()' and you wish 
to apply it
# in the ith and jth column of the data frame, e.g.,
# df[] <- lapply(df, function(x, f) f(x[i], x[j]), f=function(x, y) 
x*y)


Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Liaw, Andy" <andy_liaw at merck.com>
To: "'Firas Swidan'" <firas at cs.technion.ac.il>; 
<r-help at stat.math.ethz.ch>
Sent: Friday, February 25, 2005 12:33 PM
Subject: RE: [R] Loops and dataframes


> An addendum:  If you must use a data frame (e.g., you have mixed 
> data
> types), the following might help:
>
>> df <- list(start=st, end=ed)
>> system.time({for (i in 1:length(df[[1]])) df$start[i] <- df$end[i];
> +              df <- as.data.frame(df)}, gcFirst=TRUE)
> [1] 0.14 0.01 0.15   NA   NA
>
> I.e., keep it as a list until all manipulations are done, then 
> coerce to
> data frame.
>
>
> Andy
>
>
>> From: Liaw, Andy
>>
>> You are discovering part of the overhead of using a data
>> frame.  The way you
>> specify the subset of data frame to replace matters somewhat:
>>
>> > st <- rep(1,1e4)
>> > ed <- rep(2,1e4)
>> > df <- data.frame(start=st, end=ed)
>> > system.time(for (i in 1:dim(df)[1]) df[i,1] <- df[i,2],
>> gcFirst=TRUE)
>> [1] 35.96  0.10 36.37    NA    NA
>> > df <- data.frame(start=st, end=ed)
>> > system.time(for (i in 1:dim(df)[1]) df[[1]][i] <- df[[2]][i],
>> gcFirst=TRUE)
>> [1] 22.63  0.17 22.88    NA    NA
>> > df <- data.frame(start=st, end=ed)
>> > system.time(for (i in 1:dim(df)[1]) df$start[i] <- df$end[i],
>> gcFirst=TRUE)
>> [1] 19.29  0.13 19.46    NA    NA
>>
>>
>> If you have all numeric data, you might as well use a matrix
>> instead of data
>> frame:
>>
>> > m <- cbind(start=st, end=ed)
>> > str(m)
>>  num [1:10000, 1:2] 2 2 2 2 2 2 2 2 2 2 ...
>>  - attr(*, "dimnames")=List of 2
>>   ..$ : NULL
>>   ..$ : chr [1:2] "start" "end"
>> > system.time(for (i in 1:nrow(df)) m[i,1] <- m[i,2], gcFirst=TRUE)
>> [1] 0.06 0.00 0.08   NA   NA
>>
>>
>> Andy
>>
>>
>> > From: Firas Swidan
>> >
>> > Hi,
>> > I am experiencing a long delay when using dataframes inside
>> > loops and was
>> > wordering if this is a bug or not.
>> > Example code:
>> >
>> > > st <- rep(1,100000)
>> > > ed <- rep(2,100000)
>> > > for(i in 1:length(st)) st[i] <- ed[i] # works fine
>> > > df <- data.frame(start=st,end=ed)
>> > > for(i in 1:dim(df)[1]) df[i,1] <- df[i,2] #takes for ever
>> >
>> > R: R 2.0.0 (2004-10-04)
>> > OS: Linux, Fedora Core 2
>> > kernel: 2.6.10-1.14_FC2
>> > cpu: AMD Athlon XP 1600.
>> > mem: 500MB.
>> >
>> > The example above is only to illustrate the problem. I need
>> > loops to apply
>> > some functions on pairs (not necessarily successive) of rows in a
>> > dataframe.
>> >
>> > Thankful for any advices,
>> > Firas.
>> >
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide!
>> > http://www.R-project.org/posting-guide.html
>> >
>> >
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>>
>> --------------------------------------------------------------
>> ----------------
>> Notice:  This e-mail message, together with any attachments,
>> contains information of Merck & Co., Inc. (One Merck Drive,
>> Whitehouse Station, New Jersey, USA 08889), and/or its
>> affiliates (which may be known outside the United States as
>> Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as
>> Banyu) that may be confidential, proprietary copyrighted
>> and/or legally privileged. It is intended solely for the use
>> of the individual or entity named on this message.  If you
>> are not the intended recipient, and have received this
>> message in error, please notify us immediately by reply
>> e-mail and then delete it from your system.
>> --------------------------------------------------------------
>> ----------------
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From maarranz at tol-project.org  Fri Feb 25 14:18:18 2005
From: maarranz at tol-project.org (Miguel A. Arranz)
Date: Fri, 25 Feb 2005 14:18:18 +0100
Subject: [R] Strange Colnames
In-Reply-To: <421F10D0.5060303@gmx.de>
References: <421F10D0.5060303@gmx.de>
Message-ID: <200502251418.18251.maarranz@tol-project.org>

You probably want to have a look at help(embed). It might be all you need.


On Friday 25 February 2005 12:49, Georg Hoermann wrote:
> Hello world,
>
> I am trying to create a matrix of lagged time series with the
> following code fragment (I know that I can use acf-function...).
> I want to set the variable/column name to something like
> "lag 1" etc. If I try to do it I get text like
> "lagtest.lagtest.lagtest.lag 1" where does this come from?
>
> After a conversion to a data.frame it works as expected.
> What did I miss?
>
> Thanks and greetings,
>
> Georg
> ==================
>
>  > lagtest <- as.ts(seq(1:100)) ;
>  > for (i in 1:4) {lagtest <- cbind(lagtest,lag(ts_test,i));
>
> colnames(lagtest)[i+1]<- paste("lag",i)}
>
>  > colnames(lagtest)[1]<-"H_GW";
>  > colnames(lagtest)
>
> [1] "H_GW"                          "lagtest.lagtest.lagtest.lag 1"
> "lagtest.lagtest.lag 2"
> [4] "lagtest.lag 3"                 "lag 4"
>
>
> # this works...
>
>  >  t5 <- as.data.frame(lagtest)
>  > for (i in 1:4) {names(t5)[i+1] <- paste("var",i) }
>  > names(t5)
>
> [1] "H_GW"  "var 1" "var 2" "var 3" "var 4"
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From ramasamy at cancer.org.uk  Fri Feb 25 13:14:00 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Fri, 25 Feb 2005 12:14:00 +0000
Subject: [R] display full form in args
Message-ID: <1109333640.7333.13.camel@localhost.localdomain>

Forgive me for I do not fully comprehend the idea of classes and methods
but I was wondering if someone could help explain why the function args
() behaves the way it does.

Why does args(cut) show the simplified version instead of the more
complete one as in help("cut"). This is true for few other functions
(e.g. plot, rep).

 > args(cut)
 function (x, ...) 


Sometime I can get around this by using the "default" method as in

 > args(cut.default) 
 function (x, breaks, labels = NULL, include.lowest = FALSE, right =
TRUE, dig.lab = 3, ...) 


But sometimes I cannot use this workaround. e.g. args(merge.default)
does not give the full form as in the help file.


I find it almost always useful to see the full form. Is there a more
reliable workaround or do I have to look up the help to be certain.

Thank you.

Regards, Adai



From valdar at well.ox.ac.uk  Fri Feb 25 13:20:33 2005
From: valdar at well.ox.ac.uk (William Valdar)
Date: Fri, 25 Feb 2005 12:20:33 +0000 (GMT)
Subject: [R] anova grouping of factors in lme4 / lmer
Message-ID: <Pine.LNX.4.62.0502251212520.15720@octopus.well.ox.ac.uk>

Hi. I'm using lmer() from the lme4 package (version 0.8-3) and I can't get
anova() to group variables properly. I'm fitting the mixed model

  Response ~ Weight + Experimenter + (1|SUBJECT.NAME) + (1|Date.StudyDay)

where Weight is numeric and Experimenter is a factor, ie,

> str(data.df)
`data.frame':	4266 obs. of  5 variables:
  $ SUBJECT.NAME : Factor w/ 2133 levels "A0480","A0..",..: 1 1 2 2 ...
  $ Response     : num  -0.490  0.145  1.992 -0.391  0.917 ...
  $ Date.StudyDay: Factor w/ 161 levels "16","22","24",..: 24 24  ...
  $ Experimenter : Factor w/ 7 levels "amyt","carmena",..: 5 5 1 1 3 3 ...
  $ Weight       : num  25.3 25.3 16.7 16.7 28.2 28.2 27.1 27.1 ...

The model fits fine but when I call anova() to see the effect of the
Experimenter factor, I get a sequential anova table where factor
levels are fit one at a time, ie,

> anova(lmer(Response ~ Weight + Experimenter
+        + (1 | SUBJECT.NAME) + (1 | Date.StudyDay),
+        data=data.df))
Analysis of Variance Table
                      Df  Sum Sq Mean Sq  Denom F value    Pr(>F)
Weight                1    10.8    10.8 4242.0 14.4457 0.0001463 ***
Experimentercarmena   1     0.5     0.5 4242.0  0.6708 0.4128136
Experimentercnunez    1     4.8     4.8 4242.0  6.5042 0.0107969 *
Experimenterlsolberg  1     0.4     0.4 4242.0  0.5910 0.4420783
Experimenterpeterb    1 0.01430 0.01430 4242.0  0.0192 0.8898029
Experimenterpolinka   1     0.2     0.2 4242.0  0.2290 0.6322775
Experimenterstuart    1     0.9     0.9 4242.0  1.2505 0.2635116

... rather than what I want, which is a sequential anova table where the
levels are grouped, eg [using lme()],

> anova(lme(Response ~ Weight + Experimenter, random = ~ 1 | SUBJECT.NAME,
+       data=data.df))
Analysis of Variance Table
              Df Sum Sq Mean Sq  Denom F value    Pr(>F)
Weight        1    9.5     9.5 4242.0 12.7213 0.0003655 ***
Experimenter  6    8.5     1.4 4242.0  1.9067 0.0759496 .

Does anyone know how to make anova() group properly with lmer objects?

Many thanks,

William

=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
Dr William Valdar               ++44 (0)1865 287 717
Wellcome Trust Centre           valdar at well.ox.ac.uk
for Human Genetics, Oxford      www.well.ox.ac.uk/~valdar



From cdeclercq at nordnet.fr  Fri Feb 25 13:23:42 2005
From: cdeclercq at nordnet.fr (Christophe Declercq)
Date: Fri, 25 Feb 2005 13:23:42 +0100
Subject: [R] how to produce disease maps
In-Reply-To: <421F0A45.1000001@stams.strath.ac.uk>
Message-ID: <MJELLLFFFCNHMHOOLCMBCEHKDAAA.cdeclercq@nordnet.fr>


>>>>> "Oarabile" == Oarabile Ruth Molaodi
<oarabile at stams.strath.ac.uk>
>>>>>     on Thu, 24 Feb 2005 17:06:40 +0000 writes:

    Oarabile> how to do disease maps in R?  I want to produce
    Oarabile> disease maps (A map which shows distribution of a
    Oarabile> disease in areas of a city , country e.t.c) for
    Oarabile> the spatial geographical data I have and I'm
    Oarabile> wondering if this can be done in R. I realise that
    Oarabile> there is maps package ,mapdata and e.tc. but these
    Oarabile> do not show how.

Please study the two vignettes in the 'spdep' package.

They should put you on the right track.

Christophe
--
Christophe Declercq, MD
Observatoire regional de la sante Nord-Pas-de-Calais
13, rue Faidherbe
F-59046 LILLE Cedex
Phone 33 3 20 15 49 24
Fax 33 3 20 55 92 30
E-mail c.declercq at orsnpdc.org



From bela_b at gmx.net  Fri Feb 25 13:27:26 2005
From: bela_b at gmx.net (Bela Bauer)
Date: Fri, 25 Feb 2005 13:27:26 +0100
Subject: [R] Repeated measures MANOVA
Message-ID: <421F19AE.4030009@gmx.net>

Hi,

sorry to bother you again, but I can't figure it out myself and I also 
can't find any in-depth documentation about it...

Consider the following SAS code (A1II2... contain the measurements for 
40 subjects):

proc glm;
model
A1II2
A1IN2
A1NI2
A1NN2
= /nouni;
repeated CONTEXT 2, TARGET_SATZ 2;
title "A1 500-900 ms";

This produces not only the univariate ANOVAs, but also a number of 
multivariate MANOVAs, including some test statistics (Wilks' Lambda, 
Pillai's Trace, etc.) on various hypothesis (no CONTEXT effect, no 
TARGET_SATZ effect, no CONTEXT*TARGET_SATZ effect).

Unfortunately, I can't seem to be able to figure out how to reproduce 
this in R. There's obviously little sense in treating A1II2..A1NN2 as 
separate responses, so I came up with the following (for testing a 
single effect):

resp <- cbind(
		c(A1II2,A1IN2),
		c(A1NI2,A1NN2))
cond <- gl(2,40,80)
subj <- gl(40,1,80)

summary(manova( resp ~ cond + Error(subj/cond) ))

Error: subj
           Df Pillai approx F num Df den Df Pr(>F)
Residuals 39

Error: subj:cond
           Df  Pillai approx F num Df den Df Pr(>F)
cond       1 0.07911  1.63232      2     38 0.2089
Residuals 39
Warning message:
Error model is singular in: aov(resp ~ cond + Error(subj/cond))

Now, this looks halfway reasonable, but it's not quite there. My 
questions are:
- why is the Error model singular?
- am I even on the right path, or is this completely wrong?
- what do I do for interaction effects?

I'd be very happy if you could give me any hints of where I should be 
going with this...

Thanks again

Bela



From WeiQiang.Li at seagate.com  Fri Feb 25 13:29:45 2005
From: WeiQiang.Li at seagate.com (WeiQiang.Li@seagate.com)
Date: Fri, 25 Feb 2005 20:29:45 +0800
Subject: [R] How to set up number of prin comp.
In-Reply-To: <m0mzttughr.fsf@bar.nemo-project.org>
Message-ID: <OF96DABFBE.C060E741-ON48256FB3.0043C861-48256FB3.0044B7BA@seagate.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050225/e27e01cd/attachment.pl

From ggrothendieck at myway.com  Fri Feb 25 13:39:57 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 25 Feb 2005 12:39:57 +0000 (UTC)
Subject: [R] Strange Colnames
References: <421F10D0.5060303@gmx.de>
Message-ID: <loom.20050225T133823-567@post.gmane.org>

Georg Hoermann <georg.hoermann <at> gmx.de> writes:

: 
: Hello world,
: 
: I am trying to create a matrix of lagged time series with the
: following code fragment (I know that I can use acf-function...).
: I want to set the variable/column name to something like
: "lag 1" etc. If I try to do it I get text like
: "lagtest.lagtest.lagtest.lag 1" where does this come from?
: 
: After a conversion to a data.frame it works as expected.
: What did I miss?
: 
: Thanks and greetings,
: 
: Georg
: ==================
: 
:  > lagtest <- as.ts(seq(1:100)) ;
:  > for (i in 1:4) {lagtest <- cbind(lagtest,lag(ts_test,i)); 
: colnames(lagtest)[i+1]<- paste("lag",i)}
:  > colnames(lagtest)[1]<-"H_GW";
:  > colnames(lagtest)
: [1] "H_GW"                          "lagtest.lagtest.lagtest.lag 1" 
: "lagtest.lagtest.lag 2"
: [4] "lagtest.lag 3"                 "lag 4"
: 
: # this works...
: 
:  >  t5 <- as.data.frame(lagtest)
:  > for (i in 1:4) {names(t5)[i+1] <- paste("var",i) }
:  > names(t5)
: [1] "H_GW"  "var 1" "var 2" "var 3" "var 4"


Here is something you could try:

# define lags and their names
lags <- 0:4
names(lags) <- c("G_HW", paste("lag", 1:4))

# build mts
do.call("cbind", lapply(lags, lag, x = lagtest))



From valdar at well.ox.ac.uk  Fri Feb 25 13:45:51 2005
From: valdar at well.ox.ac.uk (William Valdar)
Date: Fri, 25 Feb 2005 12:45:51 +0000 (GMT)
Subject: [R] Re: anova grouping of factors in lme4 / lmer
In-Reply-To: <Pine.LNX.4.62.0502251212520.15720@octopus.well.ox.ac.uk>
References: <Pine.LNX.4.62.0502251212520.15720@octopus.well.ox.ac.uk>
Message-ID: <Pine.LNX.4.62.0502251240540.15720@octopus.well.ox.ac.uk>

Hi. Yesterday a revised version of lme4 came out that makes my question 
unnecessary. I was using the latest version available from my mirror 
(which was 6 days old rather than 1 day old).

Please disregard my earlier posting. Thanks.

William

On Fri, 25 Feb 2005, William Valdar wrote:

> Hi. I'm using lmer() from the lme4 package (version 0.8-3) and I can't get
> anova() to group variables properly. I'm fitting the mixed model
>
> Response ~ Weight + Experimenter + (1|SUBJECT.NAME) + (1|Date.StudyDay)
>
> where Weight is numeric and Experimenter is a factor, ie,
>
>> str(data.df)
> `data.frame':	4266 obs. of  5 variables:
> $ SUBJECT.NAME : Factor w/ 2133 levels "A0480","A0..",..: 1 1 2 2 ...
> $ Response     : num  -0.490  0.145  1.992 -0.391  0.917 ...
> $ Date.StudyDay: Factor w/ 161 levels "16","22","24",..: 24 24  ...
> $ Experimenter : Factor w/ 7 levels "amyt","carmena",..: 5 5 1 1 3 3 ...
> $ Weight       : num  25.3 25.3 16.7 16.7 28.2 28.2 27.1 27.1 ...
>
> The model fits fine but when I call anova() to see the effect of the
> Experimenter factor, I get a sequential anova table where factor
> levels are fit one at a time, ie,
>
>> anova(lmer(Response ~ Weight + Experimenter
> +        + (1 | SUBJECT.NAME) + (1 | Date.StudyDay),
> +        data=data.df))
> Analysis of Variance Table
>                     Df  Sum Sq Mean Sq  Denom F value    Pr(>F)
> Weight                1    10.8    10.8 4242.0 14.4457 0.0001463 ***
> Experimentercarmena   1     0.5     0.5 4242.0  0.6708 0.4128136
> Experimentercnunez    1     4.8     4.8 4242.0  6.5042 0.0107969 *
> Experimenterlsolberg  1     0.4     0.4 4242.0  0.5910 0.4420783
> Experimenterpeterb    1 0.01430 0.01430 4242.0  0.0192 0.8898029
> Experimenterpolinka   1     0.2     0.2 4242.0  0.2290 0.6322775
> Experimenterstuart    1     0.9     0.9 4242.0  1.2505 0.2635116
>
> ... rather than what I want, which is a sequential anova table where the
> levels are grouped, eg [using lme()],
>
>> anova(lme(Response ~ Weight + Experimenter, random = ~ 1 | SUBJECT.NAME,
> +       data=data.df))
> Analysis of Variance Table
>             Df Sum Sq Mean Sq  Denom F value    Pr(>F)
> Weight        1    9.5     9.5 4242.0 12.7213 0.0003655 ***
> Experimenter  6    8.5     1.4 4242.0  1.9067 0.0759496 .
>
> Does anyone know how to make anova() group properly with lmer objects?
>
> Many thanks,
>
> William
>
> =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
> Dr William Valdar               ++44 (0)1865 287 717
> Wellcome Trust Centre           valdar at well.ox.ac.uk
> for Human Genetics, Oxford      www.well.ox.ac.uk/~valdar
>
>

=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
Dr William Valdar               ++44 (0)1865 287 717
Wellcome Trust Centre           valdar at well.ox.ac.uk
for Human Genetics, Oxford      www.well.ox.ac.uk/~valdar



From ripley at stats.ox.ac.uk  Fri Feb 25 13:52:41 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 25 Feb 2005 12:52:41 +0000 (GMT)
Subject: [R] display full form in args
In-Reply-To: <1109333640.7333.13.camel@localhost.localdomain>
References: <1109333640.7333.13.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.61.0502251246380.32674@gannet.stats>

On Fri, 25 Feb 2005, Adaikalavan Ramasamy wrote:

> Forgive me for I do not fully comprehend the idea of classes and methods
> but I was wondering if someone could help explain why the function args
> () behaves the way it does.
>
> Why does args(cut) show the simplified version instead of the more
> complete one as in help("cut"). This is true for few other functions
> (e.g. plot, rep).
>
> > args(cut)
> function (x, ...)


Adai,

That _is_ the full form: those are formal arguments shared by all methods.
For help(cut) I get

Usage:

      cut(x, ...)

which is the same.

> Sometime I can get around this by using the "default" method as in
>
> > args(cut.default)
> function (x, breaks, labels = NULL, include.lowest = FALSE, right =
> TRUE, dig.lab = 3, ...)

but other methods have other args, e.g.

> args(cut.Date)
function (x, breaks, labels = NULL, start.on.monday = TRUE, right = FALSE,
     ...)

> But sometimes I cannot use this workaround. e.g. args(merge.default)
> does not give the full form as in the help file.

It is the *data frame* method that you are looking at in the help file.

Usage:

      merge(x, y, ...)

      ## Default S3 method:
      merge(x, y, ...)

      ## S3 method for class 'data.frame':
      merge(x, y, by = intersect(names(x), names(y)),
            by.x = by, by.y = by, all = FALSE, all.x = all, all.y = all,
            sort = TRUE, suffixes = c(".x",".y"), ...)


> I find it almost always useful to see the full form. Is there a more
> reliable workaround or do I have to look up the help to be certain.

You can look at the method you want to use.  Try not to overlook those 
comments in the help files' usage sections.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From georg.hoermann at gmx.de  Fri Feb 25 13:53:11 2005
From: georg.hoermann at gmx.de (Georg Hoermann)
Date: Fri, 25 Feb 2005 13:53:11 +0100
Subject: [R] Strange Colnames
In-Reply-To: <200502251418.18251.maarranz@tol-project.org>
References: <421F10D0.5060303@gmx.de>
	<200502251418.18251.maarranz@tol-project.org>
Message-ID: <421F1FB7.7000906@gmx.de>

Miguel A. Arranz wrote:

> You probably want to have a look at help(embed). It might be all you need.
> 
embed wraps around (no NAs at begin and end of the dataset), this makes 
no sense in hydrology.

Gruss Georg

-- 
Georg Hoermann, Dep. of Hydrology, Ecology, Kiel University, Germany
Tel. 0431-880-1207, Home: 0451/477032, 0172/4315715, Penguin #189476



From dimitris.rizopoulos at med.kuleuven.ac.be  Fri Feb 25 14:32:41 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Fri, 25 Feb 2005 14:32:41 +0100
Subject: [R] display full form in args
References: <1109333640.7333.13.camel@localhost.localdomain>
Message-ID: <00c001c51b3e$7b47c2f0$0540210a@www.domain>

you are looking for the arguments of `merge.data.frame' and not 
`merge.default'. Maybe this function could be helpful:

show.args <- function(fun, method=c("default", "all")){
    old <- options(warn=(-1))
    on.exit(options(old))
    method <- match.arg(method)
    mfun <- methods(fun)
    if( method=="default" && length(ind <- grep(".default", mfun))>0 ) 
args(mfun[ind])
    else{ res <- lapply(mfun, args); names(res) <- as.character(mfun); 
res}
}
############
show.args(rep)
show.args(plot)
show.args(seq, "a")
show.args(merge, "a")


Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Adaikalavan Ramasamy" <ramasamy at cancer.org.uk>
To: "R-help" <r-help at stat.math.ethz.ch>
Sent: Friday, February 25, 2005 1:14 PM
Subject: [R] display full form in args


> Forgive me for I do not fully comprehend the idea of classes and 
> methods
> but I was wondering if someone could help explain why the function 
> args
> () behaves the way it does.
>
> Why does args(cut) show the simplified version instead of the more
> complete one as in help("cut"). This is true for few other functions
> (e.g. plot, rep).
>
> > args(cut)
> function (x, ...)
>
>
> Sometime I can get around this by using the "default" method as in
>
> > args(cut.default)
> function (x, breaks, labels = NULL, include.lowest = FALSE, right =
> TRUE, dig.lab = 3, ...)
>
>
> But sometimes I cannot use this workaround. e.g. args(merge.default)
> does not give the full form as in the help file.
>
>
> I find it almost always useful to see the full form. Is there a more
> reliable workaround or do I have to look up the help to be certain.
>
> Thank you.
>
> Regards, Adai
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From jarioksa at sun3.oulu.fi  Fri Feb 25 14:37:25 2005
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: Fri, 25 Feb 2005 15:37:25 +0200
Subject: [R] How to set up number of prin comp.
In-Reply-To: <OF96DABFBE.C060E741-ON48256FB3.0043C861-48256FB3.0044B7BA@seagate.com>
References: <OF96DABFBE.C060E741-ON48256FB3.0043C861-48256FB3.0044B7BA@seagate.com>
Message-ID: <1109338645.12923.4.camel@biol102145.oulu.fi>

On Fri, 2005-02-25 at 20:29 +0800, WeiQiang.Li at seagate.com wrote:
> Hi Bjrn-Helge,
> 
>         Thanks for your help.
> 
>         In my case, there are more variables in the matrix than the units, 
> so I have to use Prcomp with covariance to do PCA. The problem I am facing 
> is how to get fisrt 8 coefficients and scores and how to write the result 
> into text file. Thanks again.
> 
>         When I change princomp to prcomp below, I will NULL for pc$scores 
> & pc$loadings.
> 
>         X <- some matrix
>         pc <- prcomp(X)
>         pc$scores[,1:4]    # The four first score vectors
>         pc$loadings[,1:4]  # The four first loadings
> 
Three most useful commands are help(), str() and names().

The first tells you how to use prcomp() and how it names its results.
Try help(prcomp).
The second peeks into the result so you see what is in there. Try (with
your result) srt(pc).
The third tells you what names are available in your result.
The first (help) is the most useful of these commands, since it tells
you what these names and items are. If you read it, you should say:
pc$x[, 1:4] # The four first score vectors
pc$rotation[, 1:4] # The four first loadings

Also, loadings(pc) should work with prcomp.

I think I'll write functions as.prcomp.princomp and as.princomp.prcomp
someday. 

cheers, jari oksanen
-- 
Jari Oksanen <jarioksa at sun3.oulu.fi>



From ramasamy at cancer.org.uk  Fri Feb 25 14:38:27 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Fri, 25 Feb 2005 13:38:27 +0000
Subject: [R] display full form in args
In-Reply-To: <Pine.LNX.4.61.0502251246380.32674@gannet.stats>
References: <1109333640.7333.13.camel@localhost.localdomain>
	<Pine.LNX.4.61.0502251246380.32674@gannet.stats>
Message-ID: <1109338707.7333.31.camel@localhost.localdomain>

Prof. Ripley,

As usual thank you for helpful comments. 

In future, I will check methods("cut") or methods("merge") which shows
the available methods before using args() on the appropriate function.

Thanks again.

Regards, Adai



On Fri, 2005-02-25 at 12:52 +0000, Prof Brian Ripley wrote:
> On Fri, 25 Feb 2005, Adaikalavan Ramasamy wrote:
> 
> > Forgive me for I do not fully comprehend the idea of classes and methods
> > but I was wondering if someone could help explain why the function args
> > () behaves the way it does.
> >
> > Why does args(cut) show the simplified version instead of the more
> > complete one as in help("cut"). This is true for few other functions
> > (e.g. plot, rep).
> >
> > > args(cut)
> > function (x, ...)
> 
> 
> Adai,
> 
> That _is_ the full form: those are formal arguments shared by all methods.
> For help(cut) I get
> 
> Usage:
> 
>       cut(x, ...)
> 
> which is the same.
> 
> > Sometime I can get around this by using the "default" method as in
> >
> > > args(cut.default)
> > function (x, breaks, labels = NULL, include.lowest = FALSE, right =
> > TRUE, dig.lab = 3, ...)
> 
> but other methods have other args, e.g.
> 
> > args(cut.Date)
> function (x, breaks, labels = NULL, start.on.monday = TRUE, right = FALSE,
>      ...)
> 
> > But sometimes I cannot use this workaround. e.g. args(merge.default)
> > does not give the full form as in the help file.
> 
> It is the *data frame* method that you are looking at in the help file.
> 
> Usage:
> 
>       merge(x, y, ...)
> 
>       ## Default S3 method:
>       merge(x, y, ...)
> 
>       ## S3 method for class 'data.frame':
>       merge(x, y, by = intersect(names(x), names(y)),
>             by.x = by, by.y = by, all = FALSE, all.x = all, all.y = all,
>             sort = TRUE, suffixes = c(".x",".y"), ...)
> 
> 
> > I find it almost always useful to see the full form. Is there a more
> > reliable workaround or do I have to look up the help to be certain.
> 
> You can look at the method you want to use.  Try not to overlook those 
> comments in the help files' usage sections.
> 
>



From ramasamy at cancer.org.uk  Fri Feb 25 14:51:05 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Fri, 25 Feb 2005 13:51:05 +0000
Subject: [R] display full form in args
In-Reply-To: <00c001c51b3e$7b47c2f0$0540210a@www.domain>
References: <1109333640.7333.13.camel@localhost.localdomain>
	<00c001c51b3e$7b47c2f0$0540210a@www.domain>
Message-ID: <1109339465.7333.36.camel@localhost.localdomain>

Dimitris, thank you. This is a useful function. 


On Fri, 2005-02-25 at 14:32 +0100, Dimitris Rizopoulos wrote:
> you are looking for the arguments of `merge.data.frame' and not 
> `merge.default'. Maybe this function could be helpful:
> 
> show.args <- function(fun, method=c("default", "all")){
>     old <- options(warn=(-1))
>     on.exit(options(old))
>     method <- match.arg(method)
>     mfun <- methods(fun)
>     if( method=="default" && length(ind <- grep(".default", mfun))>0 ) 
> args(mfun[ind])
>     else{ res <- lapply(mfun, args); names(res) <- as.character(mfun); 
> res}
> }
> ############
> show.args(rep)
> show.args(plot)
> show.args(seq, "a")
> show.args(merge, "a")
> 
> 
> Best,
> Dimitris
> 
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
> 
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/16/336899
> Fax: +32/16/337015
> Web: http://www.med.kuleuven.ac.be/biostat/
>      http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
> 
> 
> ----- Original Message ----- 
> From: "Adaikalavan Ramasamy" <ramasamy at cancer.org.uk>
> To: "R-help" <r-help at stat.math.ethz.ch>
> Sent: Friday, February 25, 2005 1:14 PM
> Subject: [R] display full form in args
> 
> 
> > Forgive me for I do not fully comprehend the idea of classes and 
> > methods
> > but I was wondering if someone could help explain why the function 
> > args
> > () behaves the way it does.
> >
> > Why does args(cut) show the simplified version instead of the more
> > complete one as in help("cut"). This is true for few other functions
> > (e.g. plot, rep).
> >
> > > args(cut)
> > function (x, ...)
> >
> >
> > Sometime I can get around this by using the "default" method as in
> >
> > > args(cut.default)
> > function (x, breaks, labels = NULL, include.lowest = FALSE, right =
> > TRUE, dig.lab = 3, ...)
> >
> >
> > But sometimes I cannot use this workaround. e.g. args(merge.default)
> > does not give the full form as in the help file.
> >
> >
> > I find it almost always useful to see the full form. Is there a more
> > reliable workaround or do I have to look up the help to be certain.
> >
> > Thank you.
> >
> > Regards, Adai
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> 
>



From r.hankin at soc.soton.ac.uk  Fri Feb 25 14:52:44 2005
From: r.hankin at soc.soton.ac.uk (Robin Hankin)
Date: Fri, 25 Feb 2005 13:52:44 +0000
Subject: [R] restoring vector v from v[-i]
Message-ID: <5b098593cf15b936a1612a0122b107b4@soc.soton.ac.uk>

Hi

I have a little function that takes a vector v and an integer i.  I 
want to manipulate
v[-i] (to give v.new, say) and
then put (unmanipulated)  v[i] back into the appropriate place.  For 
example,


f <- function(v,i){
v.new <- v[-i]+10
return(c(v.new[1:(i-1)],v[i],v.new[i:length(v.new)]))
}

(my example is adding 10 to v[-i], but the real version is the solution 
of a complicated function involving
a  call to Solve(A,b))


Function f() works most of the time:
 > f(1:10,4)
  [1] 11 12 13  4 15 16 17 18 19 20
 > f(1:10,8)
  [1] 11 12 13 14 15 16 17  8 19 20


but fails at the "edges":

  f(1:10,1)
  [1] 12  1 12 13 14 15 16 17 18 19 20
 > f(1:10,10)
  [1] 11 12 13 14 15 16 17 18 19 10 NA 19
 >


[ I would want f(1:10,1) to start  1,12,13,...   and f(1:10,10) to end  
  . . . 18 19,10]


How best to get intended behaviour for i=1 and i=length(v)?




--
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From Gregor.Gorjanc at bfro.uni-lj.si  Fri Feb 25 15:06:36 2005
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Fri, 25 Feb 2005 15:06:36 +0100
Subject: [R] Simulation Progress
Message-ID: <7FFEE688B57D7346BC6241C55900E730B6FF4D@pollux.bfro.uni-lj.si>

Hi!

What about use of %%, so you print every nth iteration on screen, you can
afcoures calculate how many % of iterations has already passed etc.

Some dummy example:

i <- 0
x <- 1:1000000
for (i in x) {
    if (i %% 100 == 0) {
        print(i)
    }
}

--
Lep pozdrav / With regards,
    Gregor GORJANC

------------------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty       URI: http://www.bfro.uni-lj.si/MR/ggorjan
Zootechnical Department    email: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                  tel: +386 (0)1 72 17 861
SI-1230 Domzale            fax: +386 (0)1 72 17 888
Slovenia
------------------------------------------------------------------------


Date: Fri, 25 Feb 2005 01:51:30 +0100
From: "Carsten Steinhoff" <carsten.steinhoff at gmx.de>
Subject: [R] Simulation Progress
To: <r-help at stat.math.ethz.ch>
Message-ID: <200502250051.j1P0pbVb010092 at hypatia.math.ethz.ch>
Content-Type: text/plain;       charset="us-ascii"

Hi,

I've made a function that executes a monte-carlo simulation.
It always needs a lot of time until e.g. 1Mio simulation steps are done.
So I would like to know, how many percent of the work is already done.

In an Excel/VBA Solution I could easily implement a status bar or status
window.

How could an R-Solution look like?

Carsten



From B.Rowlingson at lancaster.ac.uk  Fri Feb 25 15:14:45 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 25 Feb 2005 14:14:45 +0000
Subject: [R] restoring vector v from v[-i]
In-Reply-To: <5b098593cf15b936a1612a0122b107b4@soc.soton.ac.uk>
References: <5b098593cf15b936a1612a0122b107b4@soc.soton.ac.uk>
Message-ID: <421F32D5.1030209@lancaster.ac.uk>

Robin Hankin wrote:
> Hi
> 
> I have a little function that takes a vector v and an integer i.  I want 
> to manipulate
> v[-i] (to give v.new, say) and
> then put (unmanipulated)  v[i] back into the appropriate place.  For 
> example,
> 
> 
> f <- function(v,i){
> v.new <- v[-i]+10
> return(c(v.new[1:(i-1)],v[i],v.new[i:length(v.new)]))
> }

  Take the complement of 'i', and change the values of v in place?

f <- function(v,i){
   v[-i] <- v[-i]+10
   v
}

  Now i can be an integer or a vector of integers or true/falses.

Baz



From dimitris.rizopoulos at med.kuleuven.ac.be  Fri Feb 25 15:20:49 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Fri, 25 Feb 2005 15:20:49 +0100
Subject: [R] restoring vector v from v[-i]
References: <5b098593cf15b936a1612a0122b107b4@soc.soton.ac.uk>
Message-ID: <00ff01c51b45$343be100$0540210a@www.domain>

why not use this:

f <- function(v,i){
    v.new <- v + 10
    v.new[i] <- v[i]
    v.new
}
###########
f(1:10,4)
f(1:10,8)
f(1:10,1)
f(1:10,10)

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Robin Hankin" <r.hankin at soc.soton.ac.uk>
To: <R-help at stat.math.ethz.ch>
Sent: Friday, February 25, 2005 2:52 PM
Subject: [R] restoring vector v from v[-i]


> Hi
>
> I have a little function that takes a vector v and an integer i.  I 
> want to manipulate
> v[-i] (to give v.new, say) and
> then put (unmanipulated)  v[i] back into the appropriate place.  For 
> example,
>
>
> f <- function(v,i){
> v.new <- v[-i]+10
> return(c(v.new[1:(i-1)],v[i],v.new[i:length(v.new)]))
> }
>
> (my example is adding 10 to v[-i], but the real version is the 
> solution of a complicated function involving
> a  call to Solve(A,b))
>
>
> Function f() works most of the time:
> > f(1:10,4)
>  [1] 11 12 13  4 15 16 17 18 19 20
> > f(1:10,8)
>  [1] 11 12 13 14 15 16 17  8 19 20
>
>
> but fails at the "edges":
>
>  f(1:10,1)
>  [1] 12  1 12 13 14 15 16 17 18 19 20
> > f(1:10,10)
>  [1] 11 12 13 14 15 16 17 18 19 10 NA 19
> >
>
>
> [ I would want f(1:10,1) to start  1,12,13,...   and f(1:10,10) to 
> end  . . . 18 19,10]
>
>
> How best to get intended behaviour for i=1 and i=length(v)?
>
>
>
>
> --
> Robin Hankin
> Uncertainty Analyst
> Southampton Oceanography Centre
> European Way, Southampton SO14 3ZH, UK
>  tel  023-8059-7743
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From wilks at dial.pipex.com  Fri Feb 25 15:35:24 2005
From: wilks at dial.pipex.com (John Wilkinson (pipex))
Date: Fri, 25 Feb 2005 14:35:24 -0000
Subject: [R] two line plot title with expression problem
Message-ID: <JCEIJNOHMNBPLMGFDHNDMEBNCAAA.wilks@dial.pipex.com>

Dear R-users,

I am having a problem formatting a two line plot title with the first line
text and the second line an expression. I can?t get the second line
expression to line up with the first by starting at the LHS of the line.

A simple example illustrates the situation.

x<-seq(-5,5,length=100)
f<-function(x) x^2-x-1


For a one line title the following works fine?

plot(x,f(x),type="l")
title(expression(paste("Golden Section  ",fn:(phi^2-phi-1)),sep=""))

but when attempting a two line title as follows--

plot(x,f(x),type="l")
title(expression(paste("Golden Section\n  ",fn:(phi^2-phi-1)),sep=""))

the second line containing the expression for phi is indented,
to the right, starting immediately after the position of the end
letter of the first line.

If the second line were to be text, this would not be a problem.

How can I format the title so that the expression starts immediately under
the first line?

Thanks for help.


John.

[John Wilkinson : wilks at dial.pipex.com]



From georg.hoermann at gmx.de  Fri Feb 25 15:42:11 2005
From: georg.hoermann at gmx.de (Georg Hoermann)
Date: Fri, 25 Feb 2005 15:42:11 +0100
Subject: [R] Strange Colnames
In-Reply-To: <loom.20050225T133823-567@post.gmane.org>
References: <421F10D0.5060303@gmx.de> <loom.20050225T133823-567@post.gmane.org>
Message-ID: <421F3943.9040106@gmx.de>

Gabor Grothendieck wrote:
> 
> Here is something you could try:
> 
> # define lags and their names
> lags <- 0:4
> names(lags) <- c("G_HW", paste("lag", 1:4))
> 
> # build mts
> do.call("cbind", lapply(lags, lag, x = lagtest))
> 

thank you for the solution,
I will try to understand it during the weekend 8-)
Now I tried to change the lag from the default value of 1 to -1,
but I apparently missed something:

 > do.call("cbind", lapply(lags, function(x) lag(x,-1), x = lagtest))
Error in FUN(X[[1]], ...) : unused argument(s) ( ...)

where is the unused argument?

Thanks,
Georg


-- 
Georg Hoermann, Dep. of Hydrology, Ecology, Kiel University, Germany
Tel. 0431-880-1207, Home: 0451/477032, 0172/4315715, Penguin #189476



From osklyar at ebi.ac.uk  Fri Feb 25 15:57:57 2005
From: osklyar at ebi.ac.uk (Oleg Sklyar)
Date: Fri, 25 Feb 2005 14:57:57 +0000
Subject: [R] passing command line arguments to 'R CMD BATCH myScript.R'
Message-ID: <421F3CF5.2050606@ebi.ac.uk>

Hi Community,

I have a question about how to pass command line parameters to R script 
running in the batch mode. The problem is: there is a banch of data 
files which are to be processed by R script called from a web-server, 
i.e. in the batch mode. The web server generates data files and passes 
their names calling 'R CMD BATCH' one by one for every file. Now the 
question is how to call 'R CMD BATCH myScript.R' with parameters, like 
file name to process? I know how to read the parameters passed to an R 
script, but I don't know how to pass them.

There is an option --args that should cut the rest of the line off the 
command line. The problem is however that BATCH syntax is: 'R CMD BATCH 
[options] inFile [outFile]', i.e. if I write
'R CMD BATCH --args myDataFile myScript.R' or 'R CMD BATCH --args 
myDataFile < myScript.R' (similar was posted on some R help under 
Windows) it is not going to work because then BATCH doesn't know about 
myScript.R - it is considered as a parameter, so only 'R CMD BATCH' is 
executed. If however I use 'R CMD BATCH myScript --args myDataFile' then 
R understands --args as an output file and generates a file with that name.

Does anyone has a solution for the problem?

Best regards
Oleg

-- 
Dr Oleg Sklyar
European Bioinformatics Institute
Wellcome Trust Genome Campus
Hinxton, Cambridge, CB10 1SD
England
phone/fax  +44(0)1223 49 4478/4468
e-mail osklyar at ebi.ac.uk



From adi at roda.ro  Fri Feb 25 15:49:56 2005
From: adi at roda.ro (Adrian Dusa)
Date: Fri, 25 Feb 2005 14:49:56 +0000 (UTC)
Subject: [R] Graphics
References: <4E9E9056B353854C8AB90B2B6647183009A09813@exchange1.exchange.tvu.ac.uk>
	<Pine.LNX.4.61.0502221728460.2197@gannet.stats>
	<loom.20050224T130825-335@post.gmane.org>
	<200502241731.57657.jwd@surewest.net>
Message-ID: <loom.20050225T152029-257@post.gmane.org>

John Dougherty <jwd <at> surewest.net> writes:

> 
> On Thursday 24 February 2005 04:13, Adrian Dusa wrote:
> > ...
> 
> You need to check your font installation.  Be sure the X-11 fonts are 
> installed.    
> 
> XFree86-fonts-75dpi-4.3.99.902-30
> XFree86-fonts-100dpi-4.3.99.902-30
> 
> Should both be on your system.  If they aren't bring up the YaST control 
> center and select Install and Remove Software.  You can use the search option 
> to filter for packages that have "fonts" in their descritpion.  Install any 
> that aren't.  SuSE seems to be a little funny about the X-11 fonts.
> 
> Peter Dalgaard just let me know about that a short time ago.

Thank you for the info; I found that thread as well, but I seem to have both 75
and 100 dpi packages installed:

xorg-x11-fonts-100dpi version 6.8.1-15

xorg-x11-fonts-75dpi version 6.8.1-15

Googling around, I wasn't able to find any XFree86 .rpm fonts package.

I am also playing with the UTF-8 locales, it may have something to do with this.

> JWDougherty
> 
> ______________________________________________
> R-help <at> stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Regards,
Adrian



From osklyar at ebi.ac.uk  Fri Feb 25 16:05:17 2005
From: osklyar at ebi.ac.uk (Oleg Sklyar)
Date: Fri, 25 Feb 2005 15:05:17 +0000
Subject: [R][Rdev] any way to generate "bitmap" (tif,jpeg,png etc) files in
	R CMD BATCH
Message-ID: <421F3EAD.3040607@ebi.ac.uk>

Hi Community,

here is the problem, Linux problem (reported to work on Windows). I need 
to generate graphical output in any of bitmap format under the 'R CMD 
BATCH'. Whereas the script generating png-s works perfectly in the R 
session, such things as X11, png and jpeg are not usable in BATCH (they 
cannot be switched on by --gui-X11 etc) and X11 is prompted to be 
required for png. At the same time, such things as postscript and pdf, 
which are generally X-independent work fine. The problem is that as a 
result I need something previewable in the web browser: png, jpeg. Any 
suggestions how to proceed? Generally I could use command line linux 
tools to convert from almost any bitmpa format to the required png or 
jpeg, but it would be nicer to have them as direct R output.

Kind regards
Oleg

-- 
Dr Oleg Sklyar
European Bioinformatics Institute
Wellcome Trust Genome Campus
Hinxton, Cambridge, CB10 1SD
England
phone/fax  +44(0)1223 49 4478/4468
e-mail osklyar at ebi.ac.uk



From maillists at visiv.co.uk  Fri Feb 25 16:05:07 2005
From: maillists at visiv.co.uk (Graham Jones)
Date: Fri, 25 Feb 2005 15:05:07 +0000
Subject: FYI from apple employee RE: [R] Memory error in Mac OS X Aqua GUI
	v1.01 with cluster
In-Reply-To: <200502241106.j1OB3NrK017560@hypatia.math.ethz.ch>
References: <200502241106.j1OB3NrK017560@hypatia.math.ethz.ch>
Message-ID: <lkRg1VAj6zHCFw8u@visiv.co.uk>

Betty Gilbert wrote:

[...]
>I'm trying to cluster a matrix 
>I created with a simulation with dimensions
>dim(nca35)
>[1] 10481    12
[...]
>But I'm still getting errors like the following with funtions in the 
>cluster package
[...]

I think that the xcluster function in the ctc package from Bioconductor
is specifically designed to deal with data sets of this sort of size
without using much memory. 

-- 
Graham Jones, author of SharpEye Music Reader
http://www.visiv.co.uk
21e Balnakeil, Durness, Lairg, Sutherland, IV27 4PT, Scotland, UK



From ggrothendieck at myway.com  Fri Feb 25 16:17:26 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 25 Feb 2005 10:17:26 -0500 (EST)
Subject: [R] Strange Colnames
Message-ID: <20050225151726.B691A3A4F@mprdmxin.myway.com>


From:   Georg Hoermann <georg.hoermann at gmx.de>
> Gabor Grothendieck wrote:
> > 
> > Here is something you could try:
> > 
> > # define lags and their names
> > lags <- 0:4
> > names(lags) <- c("G_HW", paste("lag", 1:4))
> > 
> > # build mts
> > do.call("cbind", lapply(lags, lag, x = lagtest))
> 
> thank you for the solution,
> I will try to understand it during the weekend 8-)
> Now I tried to change the lag from the default value of 1 to -1,
> but I apparently missed something:
> 
> > do.call("cbind", lapply(lags, function(x) lag(x,-1), x = lagtest))
> Error in FUN(X[[1]], ...) : unused argument(s) ( ...)
> 
> where is the unused argument?
> 


The unused argument is:

        x = lagtest 

I think what you meant is:

     do.call("cbind", lapply(lags, function(k) lag(lagtest,-k)))

or equivalently:

     do.call("cbind", lapply(-lags, function(k) lag(lagtest,k)))

or more succintly:

   do.call("cbind", lapply(-lags, lag, x = lagtest))


The way it works is that lapply calls lags(x,k) repeatedly
always using the value of x = lagtest for the first argument.
Normally lapply repeatedly sets the first argument but since
we have already done that by specifying x=lagtest, lapply
uses the next argument k thereby setting k successively to each 
value of the lag.



From mvida at mac.com  Fri Feb 25 16:17:35 2005
From: mvida at mac.com (Melanie Vida)
Date: Fri, 25 Feb 2005 10:17:35 -0500
Subject: [R] outlier threshold
Message-ID: <15930253.1109344656053.JavaMail.mvida@mac.com>

For the analysis of financial data wih a large variance, what is the best way to select an outlier threshold? 

Listed below, is there a best method to select an outlier threshold and how does R calculate it?

In R, how do you find the outlier threshold through an interquartile range?
In R, how do you find the outlier threshold using the hist command?
In R, how do you find the outlier threshold with Chebyshev Inequality?
In R, how do you find the outlier threshold with Kmeans? 

Also, is there a better way to select an outlier threshold not listed above?



From ggrothendieck at myway.com  Fri Feb 25 16:27:38 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 25 Feb 2005 10:27:38 -0500 (EST)
Subject: [R] restoring vector v from v[-i]
Message-ID: <20050225152738.B1B2A3A63@mprdmxin.myway.com>


From:   Robin Hankin <r.hankin at soc.soton.ac.uk>
> 
> I have a little function that takes a vector v and an integer i. I 
> want to manipulate
> v[-i] (to give v.new, say) and
> then put (unmanipulated) v[i] back into the appropriate place. For 
> example,
> 
> 
> f <- function(v,i){
> v.new <- v[-i]+10
> return(c(v.new[1:(i-1)],v[i],v.new[i:length(v.new)]))
> }
> 

f <- function(v, i) replace(v+10, i, v[i])



From tlumley at u.washington.edu  Fri Feb 25 16:30:05 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 25 Feb 2005 07:30:05 -0800 (PST)
Subject: [R][Rdev] any way to generate "bitmap" (tif,jpeg,png etc) files
	in R CMD BATCH
In-Reply-To: <421F3EAD.3040607@ebi.ac.uk>
References: <421F3EAD.3040607@ebi.ac.uk>
Message-ID: <Pine.A41.4.61b.0502250729280.134190@homer08.u.washington.edu>

On Fri, 25 Feb 2005, Oleg Sklyar wrote:

> Hi Community,
>
> here is the problem, Linux problem (reported to work on Windows). I need to 
> generate graphical output in any of bitmap format under the 'R CMD BATCH'. 
> Whereas the script generating png-s works perfectly in the R session, such 
> things as X11, png and jpeg are not usable in BATCH (they cannot be switched 
> on by --gui-X11 etc) and X11 is prompted to be required for png.

FAQ 7.19 How do I produce PNG graphics in batch mode?

 	-thomas



From Roger.Bivand at nhh.no  Fri Feb 25 16:31:44 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 25 Feb 2005 16:31:44 +0100 (CET)
Subject: [R][Rdev] any way to generate "bitmap" (tif,jpeg,png etc) files
	inR CMD BATCH
In-Reply-To: <421F3EAD.3040607@ebi.ac.uk>
Message-ID: <Pine.LNX.4.44.0502251629590.6765-100000@reclus.nhh.no>

On Fri, 25 Feb 2005, Oleg Sklyar wrote:

> Hi Community,
> 
Use a virtual framebuffer - an example of an earlier question about this 
on the list is:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/15988.html

(the searchable archives have lots of good tips)

> here is the problem, Linux problem (reported to work on Windows). I need 
> to generate graphical output in any of bitmap format under the 'R CMD 
> BATCH'. Whereas the script generating png-s works perfectly in the R 
> session, such things as X11, png and jpeg are not usable in BATCH (they 
> cannot be switched on by --gui-X11 etc) and X11 is prompted to be 
> required for png. At the same time, such things as postscript and pdf, 
> which are generally X-independent work fine. The problem is that as a 
> result I need something previewable in the web browser: png, jpeg. Any 
> suggestions how to proceed? Generally I could use command line linux 
> tools to convert from almost any bitmpa format to the required png or 
> jpeg, but it would be nicer to have them as direct R output.
> 
> Kind regards
> Oleg
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From ripley at stats.ox.ac.uk  Fri Feb 25 16:40:22 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 25 Feb 2005 15:40:22 +0000 (GMT)
Subject: [Rd] [R][Rdev] any way to generate "bitmap" (tif, jpeg, png etc)
	files in R CMD BATCH
In-Reply-To: <421F3EAD.3040607@ebi.ac.uk>
References: <421F3EAD.3040607@ebi.ac.uk>
Message-ID: <Pine.LNX.4.61.0502251533410.2497@gannet.stats>

Please do consult the posting guide and not post to multiple lists.

On Fri, 25 Feb 2005, Oleg Sklyar wrote:

> here is the problem, Linux problem (reported to work on Windows). I need to 
> generate graphical output in any of bitmap format under the 'R CMD BATCH'. 
> Whereas the script generating png-s works perfectly in the R session, such 
> things as X11, png and jpeg are not usable in BATCH (they cannot be switched 
> on by --gui-X11 etc) and X11 is prompted to be required for png. At the same

BATCH is just a script, effectively to do

R --save--restore --save --no-readline < infile > outfile 2>&1

and in earlier versions includes --gui=none.  So if you are running BATCH 
with access to an X server (unusual), you can just use the above.

> time, such things as postscript and pdf, which are generally X-independent 
> work fine. The problem is that as a result I need something previewable in 
> the web browser: png, jpeg. Any suggestions how to proceed?

See also ?bitmap, linked from the help page for png().

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Fri Feb 25 16:59:09 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 25 Feb 2005 16:59:09 +0100
Subject: [R] passing command line arguments to 'R CMD BATCH myScript.R'
In-Reply-To: <421F3CF5.2050606@ebi.ac.uk>
References: <421F3CF5.2050606@ebi.ac.uk>
Message-ID: <421F4B4D.7050605@statistik.uni-dortmund.de>

Oleg Sklyar wrote:

> Hi Community,
> 
> I have a question about how to pass command line parameters to R script 
> running in the batch mode. The problem is: there is a banch of data 
> files which are to be processed by R script called from a web-server, 
> i.e. in the batch mode. The web server generates data files and passes 
> their names calling 'R CMD BATCH' one by one for every file. Now the 
> question is how to call 'R CMD BATCH myScript.R' with parameters, like 
> file name to process? I know how to read the parameters passed to an R 
> script, but I don't know how to pass them.
> 
> There is an option --args that should cut the rest of the line off the 
> command line. The problem is however that BATCH syntax is: 'R CMD BATCH 
> [options] inFile [outFile]', i.e. if I write
> 'R CMD BATCH --args myDataFile myScript.R' or 'R CMD BATCH --args 
> myDataFile < myScript.R' (similar was posted on some R help under 
> Windows) it is not going to work because then BATCH doesn't know about 
> myScript.R - it is considered as a parameter, so only 'R CMD BATCH' is 
> executed. If however I use 'R CMD BATCH myScript --args myDataFile' then 
> R understands --args as an output file and generates a file with that name.
> 
> Does anyone has a solution for the problem?
> 
> Best regards
> Oleg
> 

See ?commandArgs

Uwe Ligges



From rosterma at abo.fi  Fri Feb 25 16:51:25 2005
From: rosterma at abo.fi (rosterma@abo.fi)
Date: Fri, 25 Feb 2005 17:51:25 +0200
Subject: [R] Fwd: want to call R from aplatform written i strict ANSI C
Message-ID: <1109346685.421f497daae7a@webmail.abo.fi>

Hi,
I sent the below message to r-devel but received no answer. I would like
to contribute my system but I need som advice concerning the
connections. Is my proposal of any interest to you?
regards
Ralf ?stermark

----- Forwarded message from Ralf ?stermark <rosterma at abo.fi> -----
    Date: Mon, 21 Feb 2005 11:35:21 +0200
    From: Ralf ?stermark <rosterma at abo.fi>
Reply-To: Ralf ?stermark <rosterma at abo.fi>
 Subject: want to call R from aplatform written i strict ANSI C
      To: r-devel at lists.R-project.org

Hi,
I would like to connect my GHA-system to R and to deliver the
library to users of R.

could you give me a comprehensive example containing the step by step
procedure to:
1. create a callable library with R binaries (not having a standalone
version of R with its own main()). If possible, I would like to run the
system on unix,alpha,linux computers as well as on IBM's parallel
supercomputers.
2. code the interface between a main-program written in C and a
user-defined script written in R. Consider the following case:
The algorithm - written in strict ANSI C (explained below) - needs to
call a user-written
R-program occasionally in order to boost computations, say, in an
ill-conditioned computational problem where classical optimization tools
can be used only locally. For this, the C-program needs somehow to
transfer data, an initialized vector of parameters and the name of the
user-defined R program to some interface function. The R program in turn
can use all the resources available in the R environment. The results of
the computations (e.g., the updated parameter vector, matrices of
forecasts and forecast errors) need to be returned to the C-program from
R. The corresponding allocations need to be done before R is invoked and
freed afterwards. 

Using R would increase the flexibility of the C-platform considerably
and hopefully bring new features to R.

I would be very happy if I could do this both sequentially on
alpha/unix/linux platforms and in parallel on supercomputers.

I have developed an algorithmic platform (GHA = Genetic Hybrid
Algorithm) over the past years (referee articles are listed under
www.abo.fi/~rosterma). The system is compiled and runs without warnings
under maximum warning level in sequential mode on alpha, unix and linux
mainframes and in parallel mode on IBMs supercomputers and Cray T3E (the
latter is a more or less outdated supercomputer today).

The system was built in strict ANSI C and it can be connected to
computational algorithms through an accelerator function. We can solve
problems using pure classical optimization tools on the one extreme
(global, constrained (linear,nonlinear, milp,minlp etc)) and pure
genetic search on the other. We can also use hybrid approaches where
various classical/artificial intelligence approaches are combined.

GHA may be used as a subsystem to other programs
or as the main system using other systems as subsystems (libraries). As
I am not an expert in R, in the first stage I would like to
invoke some simple user-written R routines from within GHA, using R as a
support library for GHA. If successful, I would be in a position to
write instructions for other users and to submit my package to the R
community. For the moment I have no external users of my algorithm. I
have studied web-material concerning R extensions. The file R-exts.pdf
(chapter 5), for example, describes various entry points to R from C.
Also chapter 7 discusses this issue.

However, I have not found a comprehensive example that would give me a
step by step procedure. Consider the following example where I invoke my
GHA-system as a standalone program to solve a minlp problem:

#include <stdio.h>  
#include <stdlib.h> 
#include <string.h> 
#include <time.h>   
#include <ctype.h>  
#include <math.h>   
#include <float.h>
#include "supergha.h"
#include "rpa_proj.h"
MINLP_ptr *SHAREX_PROB;
extern void ghasystem(int argc,char *argv[]);
int main(int argc,char **argv) {
 int LOAD_SYSTEM = TRUE;
 INITIALIZE_RAN1
 SHAREX_PROB  = get_data(argc,argv,LOAD_SYSTEM);
 ghasystem(argc,argv);
 exit(0);
}   /* end of main() */

The function ghasystem() controls all GHA-processes. The key user-level
functions are defined in project.c. In the below example I have used
gha() to solve a minlp-problem using classical optimization tools only.

/* project.c contains the following problem specific functions:
   (1) evaluator()
   (2) accelerator()
   (3) gradient()
   (4) hessian()
   (5) pre_processor()
   (6) post_processor()
*/
/* Global Definitions */
#include "project.h"
#include "rpa_proj.h"
#include "rpa_proj.c"
extern RPA_ptr   *SHAREX_ROOT;
extern MINLP_ptr *SHAREX_PROB;
IVECTOR SUPER_FLAG        = &(INT_STATUS[16]);
IVECTOR ALLOCATION_SWITCH = &(INT_PROTOCOL[30]);
extern void compare_NVAR(int n1,int n2,int n3,char *stage);
/* end of global definitions */
void evaluator(DVECTOR w,double Penalty,double *gf,double *F,double *Dev) {
 int i,n,n_i,n_x,n_d,m_f,ACCELERATE;
 ACCELERATE=INT_STATUS[0];
 REAL_STATUS[4] += 1.0;    /* number of evaluator() calls */
 n   = min(SHAREX_PROB->n,*(TASK.NVAR));
 n_i = min(SHAREX_PROB->n_i,*(TASK.NVAR));
 n_x = min(SHAREX_PROB->n_x,*(TASK.NVAR));
 n_d = min(SHAREX_PROB->n_d,*(TASK.NVAR));
 m_f = SHAREX_PROB->m_f;
 /* NOTE: w[0] is F */
 *F   = 0.0;
 *Dev = 0.0;
 if(m_f EQ 0) {for(i=0;i<n;i++) *F   += w[i]*SHAREX_PROB->c[i];}
 else {
  for(i=0;i<m_f;i++) *F +=
f_function(SHAREX_PROB,SHAREX_PROB->x,SHAREX_PROB->du
al_x,i);
 }
 *Dev = SHAREX_PROB->Dev;
 *gf  = *F+Penalty*(*Dev);
}  /* end of evaluator() */

void accelerator(int FIX_Flip,int ROW,double Penalty,DVECTOR LOWER,
              DVECTOR UPPER,DVECTOR w_IN,DVECTOR w_OUT) {
 double MILP_CALLS;
 if(FIX_Flip > 0) return;
 SHAREX_PROB->INITIALIZED = FALSE;
 SHAREX_PROB->TREE_COUNTER = 0.0;
 SHAREX_PROB->ACTIVE_TREE  = 0.0;
 REAL_STATUS[5] += 1.0;    /* number of accelerator() calls */

 MILP_CALLS = milp_caller(SHAREX_PROB);

  /*
   NOTE: if(ni_g>0 OR nx_g>0) then w must be restructured.
         if(n_change>0 and we want to change TASK.NVAR in full GHA-search, 
         then POP,HISTORY,w_in,START,IMPROVED,BEST must be restructured.
         GHA_SYS.SUMDIM and GHA_SYS.MAXDIM must be adjusted as well
         as TASK.DERIVABLES etc.
  */
 compare_NVAR(SHAREX_PROB->n,*(TASK.NVAR),*(TASK.NVAR),"accelerator() 1");
 memcpy(w_OUT,SHAREX_PROB->x,*(TASK.NVAR)*sizeof(double));
}      /* end of accelerator() */

void gradient(DVECTOR w,int n_w,double Penalty,DVECTOR d,int n_d,IVECTOR
pos) {
 /* NOTE: FIXED_h=1,VARIABLE_h=2 */
 int h_TYPE=2;
 num_gradient(h_TYPE,w,n_w,Penalty,d,n_d,pos);
}   /* end of gradient */

void hessian(DVECTOR w,int n_w,double Penalty,DMATRIX G,int n_G,IVECTOR
pos) {
 /* NOTE: FIXED_h=1,VARIABLE_h=2 */
 int h_TYPE=2;
 num_hessian(h_TYPE,w,n_w,Penalty,G,n_G,pos);
}        /* end of hessian() */

void pre_processor (struct SOLUTION *START) {
 int PHASE = INT_STATUS[17];
 if((PHASE EQ 1) AND (*(TASK.ACTIVE_SYSTEM) EQ 0) AND
(*ALLOCATION_SWITCH EQ FALSE)) {
  *ALLOCATION_SWITCH = TRUE;
  initlp_();
 }   /* end of if() */
}    /* end of pre_processor() */

void post_processor(struct SOLUTION *BEST) {
 double Penalty;
 int PHASE = INT_STATUS[17];
 Penalty = REAL_STATUS[0];
 if((*SUPER_FLAG EQ TRUE) AND (PHASE EQ GHA_SYS.SYSTEM_CALLS)){
  deallocate_MINLP(SHAREX_PROB);
 }   /* end of if() */
}    /* end of post_processor() */

The key connection between R and GHA - from the viewpoint of GHA - is
the accelerator()-function. This function is activated by GHA in
critical stages of the main iterations (a flag indicates where precisely
the function is invoked). If I could call a user-written R-script from
the accelerator() at a critical stage, then I could - for example -
split the solution process into the following sequence:

1. given the current local environment of the problem as defined by
gha(), call the R script
2. the R script returns a local solution to the problem.
3. the system returns to step 1 until convergence.

Since gha works both sequentially and in parallel, I can boost the
search process in difficult problems using the parallel programming
facilities of gha (tested on the parallel supercomputers of the Centre
of Scientific Comuting in Helsinki).

I am grateful for any help on this matter.
regards
Ralf ?stermark
Professor of Accounting and Optimization Systems



----- End forwarded message -----


--



From ligges at statistik.uni-dortmund.de  Fri Feb 25 17:01:27 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 25 Feb 2005 17:01:27 +0100
Subject: [R] outlier threshold
In-Reply-To: <15930253.1109344656053.JavaMail.mvida@mac.com>
References: <15930253.1109344656053.JavaMail.mvida@mac.com>
Message-ID: <421F4BD7.2090106@statistik.uni-dortmund.de>

Melanie Vida wrote:

> For the analysis of financial data wih a large variance, what is the best way to select an outlier threshold? 
> 
> Listed below, is there a best method to select an outlier threshold and how does R calculate it?
> 
> In R, how do you find the outlier threshold through an interquartile range?
> In R, how do you find the outlier threshold using the hist command?
> In R, how do you find the outlier threshold with Chebyshev Inequality?
> In R, how do you find the outlier threshold with Kmeans? 
> 
> Also, is there a better way to select an outlier threshold not listed above?


This depends on your definition of an outlier and the model for your 
data - there is no "best" method in general. The "robust" folks have 
quite a lot of theory and methods ...

Uwe Ligges


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From tlumley at u.washington.edu  Fri Feb 25 16:40:18 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 25 Feb 2005 07:40:18 -0800 (PST)
Subject: [R] [R-pkgs] new version of survey package
Message-ID: <Pine.A41.4.61b.0502250734490.134190@homer08.u.washington.edu>


Version 2.9 of survey is on CRAN.  In addition to various minor 
improvements and bug fixes there are two major changes

- full multistage finite-population sampling is supported (as in SUDAAN)
- the same analysis commands can be used for all design types (eg svymean
   instead of svrepmean for replicate weight designs)

 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From maechler at stat.math.ethz.ch  Fri Feb 25 17:03:13 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 25 Feb 2005 17:03:13 +0100
Subject: [R] display full form in args
In-Reply-To: <1109338707.7333.31.camel@localhost.localdomain>
References: <1109333640.7333.13.camel@localhost.localdomain>
	<Pine.LNX.4.61.0502251246380.32674@gannet.stats>
	<1109338707.7333.31.camel@localhost.localdomain>
Message-ID: <16927.19521.430125.779297@stat.math.ethz.ch>

>>>>> "Adaikalavan" == Adaikalavan Ramasamy <ramasamy at cancer.org.uk>
>>>>>     on Fri, 25 Feb 2005 13:38:27 +0000 writes:

    Adaikalavan> Prof. Ripley, As usual thank you for helpful
    Adaikalavan> comments.

    Adaikalavan> In future, I will check methods("cut") or
    Adaikalavan> methods("merge") which shows the available
    Adaikalavan> methods before using args() on the appropriate
    Adaikalavan> function.

Just a side note:

Using  str() instead of  args() does not print the (somewhat
ugly) "NULL" body 
 {but it also does not *return* anything useful. Consequently the
  show.args() function of Dimitry would have to be rewritten
  slightly when using str() instead of args()}.

Martin



From fm3a004 at math.uni-hamburg.de  Fri Feb 25 17:05:58 2005
From: fm3a004 at math.uni-hamburg.de (Christian Hennig)
Date: Fri, 25 Feb 2005 17:05:58 +0100 (MET)
Subject: [R] outlier threshold
In-Reply-To: <15930253.1109344656053.JavaMail.mvida@mac.com>
Message-ID: <Pine.GSO.3.95q.1050225165355.18110A-100000@sun11.math.uni-hamburg.de>

You may want to read the discussion on detection of outliers in the mailing
list archive.

The boxplot definition has an in-built outlier threshold for univariate
data, namely 1.5 times IQR distance from the upper and lower quartile.

Simple and good is also the threshold c*mad from the median, where c is
sometimes recommended to be 5.2 or 6. 

If you have more complicated data (multivariate/time series), it becomes
more complicated. Even for univariate data, it depends on the distributional
shape and the aim of outlier identification.

What do you mean by "threshold with kmeans"? If you mean the detection of
outliers in presence of clustering, then kmeans does not help you further,
but EMclustN (package mclust) and NNclean (package prabclus) may be
interesting.

Best,
Christian 

On Fri, 25 Feb 2005, Melanie Vida wrote:

> For the analysis of financial data wih a large variance, what is the best way to select an outlier threshold? 
> 
> Listed below, is there a best method to select an outlier threshold and how does R calculate it?
> 
> In R, how do you find the outlier threshold through an interquartile range?
> In R, how do you find the outlier threshold using the hist command?
> In R, how do you find the outlier threshold with Chebyshev Inequality?
> In R, how do you find the outlier threshold with Kmeans? 
> 
> Also, is there a better way to select an outlier threshold not listed above?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

***********************************************************************
Christian Hennig
Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
>From 1 April 2005: Department of Statistical Science, UCL, London
#######################################################################
ich empfehle www.boag-online.de



From daretoyin at yahoo.com  Fri Feb 25 17:32:20 2005
From: daretoyin at yahoo.com (toyin dare)
Date: Fri, 25 Feb 2005 08:32:20 -0800 (PST)
Subject: [R] help me!
Message-ID: <20050225163220.36439.qmail@web20325.mail.yahoo.com>

 i have 3 questions
1 the english linking r
2 the relivance of sociology to nigeria
3with the aid of any communication model explain the
process of communication



From Benjamin.Osborne at uvm.edu  Fri Feb 25 17:33:36 2005
From: Benjamin.Osborne at uvm.edu (Benjamin M. Osborne)
Date: Fri, 25 Feb 2005 11:33:36 -0500
Subject: [R] subsetting by NA
Message-ID: <1109349216.421f5360d423b@webmail.uvm.edu>

I want to know where all the NAs are in a matrix.  The data frame looks like
this:

> vmc[1:5,]
       date year month day snow.new prcp      tmin snow.dep       tmax
1 01NOV1954 1954    11   1       NA   NA -14.44444       NA 12.2222222
2 02NOV1954 1954    11   2       NA   NA -13.88889       NA  2.2222222
3 03NOV1954 1954    11   3       NA   NA -16.66667       NA -1.1111111
4 04NOV1954 1954    11   4       NA   NA        NA       NA -0.5555556
5 05NOV1954 1954    11   5       NA   NA -17.22222       NA -2.7777778
       tmean yearmo
1  -1.111111 195411
2  -5.833333 195411
3  -8.888889 195411
4         NA 195411
5 -10.000000 195411
>

This does not work:
> subset(vmc, snow.new==NA)
 [1] date     year     month    day      snow.new prcp     tmin     snow.dep
 [9] tmax     tmean    yearmo
<0 rows> (or 0-length row.names)


Because:
> NA==NA
[1] NA

How can I return all rows of this data frame that contain NA in a particular
field?

Thanks,
Ben Osborne

-- 
Botany Department
University of Vermont
109 Carrigan Drive
Burlington, VT 05405



From urbanek at research.att.com  Fri Feb 25 17:56:00 2005
From: urbanek at research.att.com (Simon Urbanek)
Date: Fri, 25 Feb 2005 11:56:00 -0500
Subject: [Rd] [R][Rdev] any way to generate "bitmap" (tif, jpeg,
	png etc) files in R CMD BATCH
In-Reply-To: <421F3EAD.3040607@ebi.ac.uk>
References: <421F3EAD.3040607@ebi.ac.uk>
Message-ID: <209DEA18-874E-11D9-AE75-000D93AE1C66@research.att.com>

On Feb 25, 2005, at 10:05 AM, Oleg Sklyar wrote:

> here is the problem, Linux problem (reported to work on Windows). I 
> need to generate graphical output in any of bitmap format under the 'R 
> CMD BATCH'. Whereas the script generating png-s works perfectly in the 
> R session, such things as X11, png and jpeg are not usable in BATCH

There are other ways, but if you have (or can install) libgd and 
freetype, you may consider using the GDD device - it can produce jpeg, 
png and gif on unix w/o the need of X11. The nice thing about it is 
speed (it doesn't use any external process) and that it uses 
anti-aliasing of both text and lines. It was designed specifically for 
graphics-on-demand on web servers.

Currenly GDD is not on CRAN (yet), but you can get it from
http://www.rosuda.org/R/nightly/
Just make sure that you have libgd and freetype (if you any problems or 
suggestions, please drop me a line).

There are other methods, too, like the bitmap device, which uses gs 
(AFAICS), or you can run virtual X11 to satisfy the devices that 
require X11.

Cheers,
Simon



From sundar.dorai-raj at pdf.com  Fri Feb 25 18:03:35 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 25 Feb 2005 11:03:35 -0600
Subject: [R] subsetting by NA
In-Reply-To: <1109349216.421f5360d423b@webmail.uvm.edu>
References: <1109349216.421f5360d423b@webmail.uvm.edu>
Message-ID: <421F5A67.8010300@pdf.com>



Benjamin M. Osborne allegedly said on 2/25/2005 10:33 AM:
> I want to know where all the NAs are in a matrix.  The data frame looks like
> this:
> 
> 
>>vmc[1:5,]
> 
>        date year month day snow.new prcp      tmin snow.dep       tmax
> 1 01NOV1954 1954    11   1       NA   NA -14.44444       NA 12.2222222
> 2 02NOV1954 1954    11   2       NA   NA -13.88889       NA  2.2222222
> 3 03NOV1954 1954    11   3       NA   NA -16.66667       NA -1.1111111
> 4 04NOV1954 1954    11   4       NA   NA        NA       NA -0.5555556
> 5 05NOV1954 1954    11   5       NA   NA -17.22222       NA -2.7777778
>        tmean yearmo
> 1  -1.111111 195411
> 2  -5.833333 195411
> 3  -8.888889 195411
> 4         NA 195411
> 5 -10.000000 195411
> 
> 
> This does not work:
> 
>>subset(vmc, snow.new==NA)
> 
>  [1] date     year     month    day      snow.new prcp     tmin     snow.dep
>  [9] tmax     tmean    yearmo
> <0 rows> (or 0-length row.names)
> 
> 
> Because:
> 
>>NA==NA
> 
> [1] NA
> 
> How can I return all rows of this data frame that contain NA in a particular
> field?
> 
> Thanks,
> Ben Osborne
> 


Ben,

This is a perfect job for ?complete.cases. You should also read 
Introduction to R, pg. 9-10 in the PDF document, regarding how missing 
values are handled in R.

--sundar



From ligges at statistik.uni-dortmund.de  Fri Feb 25 18:08:17 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 25 Feb 2005 18:08:17 +0100
Subject: [R] subsetting by NA
In-Reply-To: <1109349216.421f5360d423b@webmail.uvm.edu>
References: <1109349216.421f5360d423b@webmail.uvm.edu>
Message-ID: <421F5B81.1040207@statistik.uni-dortmund.de>

See ?is.na

Uwe Ligges

Benjamin M. Osborne wrote:

> I want to know where all the NAs are in a matrix.  The data frame looks like
> this:
> 
> 
>>vmc[1:5,]
> 
>        date year month day snow.new prcp      tmin snow.dep       tmax
> 1 01NOV1954 1954    11   1       NA   NA -14.44444       NA 12.2222222
> 2 02NOV1954 1954    11   2       NA   NA -13.88889       NA  2.2222222
> 3 03NOV1954 1954    11   3       NA   NA -16.66667       NA -1.1111111
> 4 04NOV1954 1954    11   4       NA   NA        NA       NA -0.5555556
> 5 05NOV1954 1954    11   5       NA   NA -17.22222       NA -2.7777778
>        tmean yearmo
> 1  -1.111111 195411
> 2  -5.833333 195411
> 3  -8.888889 195411
> 4         NA 195411
> 5 -10.000000 195411
> 
> 
> This does not work:
> 
>>subset(vmc, snow.new==NA)
> 
>  [1] date     year     month    day      snow.new prcp     tmin     snow.dep
>  [9] tmax     tmean    yearmo
> <0 rows> (or 0-length row.names)
> 
> 
> Because:
> 
>>NA==NA
> 
> [1] NA
> 
> How can I return all rows of this data frame that contain NA in a particular
> field?
> 
> Thanks,
> Ben Osborne
>



From David.Brahm at geodecapital.com  Fri Feb 25 18:09:29 2005
From: David.Brahm at geodecapital.com (Brahm, David)
Date: Fri, 25 Feb 2005 12:09:29 -0500
Subject: [R] passing command line arguments to 'R CMD BATCH myScript.R'
Message-ID: <4DD6F8B8782D584FABF50BF3A32B03D801A2BB62@MSGBOSCLF2WIN.DMN1.FMR.COM>

I have never understood the difference between 
> R CMD BATCH --vanilla --slave myScript.R outFile.txt
and
> R --vanilla --slave < myScript.R > outFile.txt

I use the latter method, and then the --args construction works great:
> R --vanilla --slave --args myArg1 myArg2 < myScript.R > outFile.txt

In fact, I define "R --vanilla --slave --args" to be an environment
variable $R, and then it's just
> $R myArg1 myArg2 < myScript.R > outFile.txt

-- David Brahm (brahm at alum.mit.edu)



From spencer.graves at pdf.com  Fri Feb 25 18:12:33 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 25 Feb 2005 09:12:33 -0800
Subject: [R] subsetting by NA
In-Reply-To: <1109349216.421f5360d423b@webmail.uvm.edu>
References: <1109349216.421f5360d423b@webmail.uvm.edu>
Message-ID: <421F5C81.4060509@pdf.com>

      Have you considered "is.na"? 

      My favorite tool for answering questions like this is 
www.r-project.org -> search -> "R site search".  When I asked there for 
"subsetting by NA" just now, I got 101 hits, the third of which 
mentioned "is.na".  Also, the posting guide 
"R-project.org/posting-guide.html" gives other useful tips for how to 
find information about R. 

      hope this helps.  spencer graves

Benjamin M. Osborne wrote:

>I want to know where all the NAs are in a matrix.  The data frame looks like
>this:
>
>  
>
>>vmc[1:5,]
>>    
>>
>       date year month day snow.new prcp      tmin snow.dep       tmax
>1 01NOV1954 1954    11   1       NA   NA -14.44444       NA 12.2222222
>2 02NOV1954 1954    11   2       NA   NA -13.88889       NA  2.2222222
>3 03NOV1954 1954    11   3       NA   NA -16.66667       NA -1.1111111
>4 04NOV1954 1954    11   4       NA   NA        NA       NA -0.5555556
>5 05NOV1954 1954    11   5       NA   NA -17.22222       NA -2.7777778
>       tmean yearmo
>1  -1.111111 195411
>2  -5.833333 195411
>3  -8.888889 195411
>4         NA 195411
>5 -10.000000 195411
>  
>
>
>This does not work:
>  
>
>>subset(vmc, snow.new==NA)
>>    
>>
> [1] date     year     month    day      snow.new prcp     tmin     snow.dep
> [9] tmax     tmean    yearmo
><0 rows> (or 0-length row.names)
>
>
>Because:
>  
>
>>NA==NA
>>    
>>
>[1] NA
>
>How can I return all rows of this data frame that contain NA in a particular
>field?
>
>Thanks,
>Ben Osborne
>
>  
>



From ligges at statistik.uni-dortmund.de  Fri Feb 25 18:34:33 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 25 Feb 2005 18:34:33 +0100
Subject: [R] two line plot title with expression problem
In-Reply-To: <JCEIJNOHMNBPLMGFDHNDMEBNCAAA.wilks@dial.pipex.com>
References: <JCEIJNOHMNBPLMGFDHNDMEBNCAAA.wilks@dial.pipex.com>
Message-ID: <421F61A9.6090703@statistik.uni-dortmund.de>

John Wilkinson (pipex) wrote:

> Dear R-users,
> 
> I am having a problem formatting a two line plot title with the first line
> text and the second line an expression. I can?t get the second line
> expression to line up with the first by starting at the LHS of the line.
> 
> A simple example illustrates the situation.
> 
> x<-seq(-5,5,length=100)
> f<-function(x) x^2-x-1
> 
> 
> For a one line title the following works fine?
> 
> plot(x,f(x),type="l")
> title(expression(paste("Golden Section  ",fn:(phi^2-phi-1)),sep=""))
> 
> but when attempting a two line title as follows--
> 
> plot(x,f(x),type="l")
> title(expression(paste("Golden Section\n  ",fn:(phi^2-phi-1)),sep=""))
> 
> the second line containing the expression for phi is indented,
> to the right, starting immediately after the position of the end
> letter of the first line.
> 
> If the second line were to be text, this would not be a problem.
> 
> How can I format the title so that the expression starts immediately under
> the first line?

Multi-line expressions are not supported. You can add them by separate 
calls to title(), though:

  plot(1:10)
  title("Golden Section", line=3)
  title(expression(fn:(phi^2-phi-1)), line=2)

or for left-adjustment, try the same with calls to mtext() and 
specifying arguments "adj" and "at".

Uwe Ligges



> Thanks for help.
> 
> 
> John.
> 
> [John Wilkinson : wilks at dial.pipex.com]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri Feb 25 18:40:40 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 25 Feb 2005 18:40:40 +0100
Subject: [R] calculatingmean value for duplicates in affy array
In-Reply-To: <20050225040914.13433.qmail@web61302.mail.yahoo.com>
References: <20050225040914.13433.qmail@web61302.mail.yahoo.com>
Message-ID: <421F6318.7040601@statistik.uni-dortmund.de>

Dren Scott wrote:

> Hi ,
>  
> I was trying read in an affy array/matrix and then to calculate the mean value for all the duplicates. Is there a function in the R package that would do this?
>  
> I tried the help function and searched for 'duplicate'. Although it provides a list of functions that would eliminate the duplicate probe ids, I couldn't find one that would calculate the mean for the duplicates.
>  
> Any help would be appreciated.
>


See ?tapply
(for sure you are aware of the Bioconductor project).

Uwe Ligges


> thanks.
> 
> __________________________________________________
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Fri Feb 25 18:51:05 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 25 Feb 2005 17:51:05 +0000 (GMT)
Subject: [R] passing command line arguments to 'R CMD BATCH myScript.R'
In-Reply-To: <4DD6F8B8782D584FABF50BF3A32B03D801A2BB62@MSGBOSCLF2WIN.DMN1.FMR.COM>
References: <4DD6F8B8782D584FABF50BF3A32B03D801A2BB62@MSGBOSCLF2WIN.DMN1.FMR.COM>
Message-ID: <Pine.LNX.4.61.0502251742590.4255@gannet.stats>

On Fri, 25 Feb 2005, Brahm, David wrote:

> I have never understood the difference between
>> R CMD BATCH --vanilla --slave myScript.R outFile.txt
> and
>> R --vanilla --slave < myScript.R > outFile.txt

[On Unix-alikes]

Mainly cosmetics, e.g. the first adds timings and options(echo=TRUE) and 
generates a suitably named outfile.  [Also sets --restore --save which 
you then override.]

It used to set --gui=none to avoid the overhead of loading the X11 module.

More importantly, the first redirects stderr and yours does not, but see 
my reply to the OP's other thread for a closer equivalent.

[...]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From helprhelp at gmail.com  Fri Feb 25 19:00:45 2005
From: helprhelp at gmail.com (WeiWei Shi)
Date: Fri, 25 Feb 2005 12:00:45 -0600
Subject: [R] passing command line arguments to 'R CMD BATCH myScript.R'
In-Reply-To: <421F3CF5.2050606@ebi.ac.uk>
References: <421F3CF5.2050606@ebi.ac.uk>
Message-ID: <cdf817830502251000436c5d4e@mail.gmail.com>

Hi,
I recently solved the problem: I ran a program from Linux. The basic
idea is using enviroment variables and ?Sys.getenv

This is a general approach to calling R from a script file.

Here is part of my codes and explanation:

# assign some values to the arguments
dvar=5
categorical=3
....

# export them as env. var.
export dvar mod_min mod_max tree_no cp categorical

#run your r program in a batch mode
R CMD BATCH < $HOME/r/batch/mk_trees.r

The following is part of my mk_trees.r:
#load arguments
cargs<-Sys.getenv(c('dvar','mod_min','mod_max','tree_no','cp','categorical'))
dvar<-as.numeric(cargs[1])
mod_min<-as.numeric(cargs[2])
mod_max<-as.numeric(cargs[3])
tree_no<-as.numeric(cargs[4])
cp<-as.numeric(cargs[5])
cc<-strsplit(cargs[6], "\n")
categorical<-as.numeric(cc[[1]])


HTH,

Ed

On Fri, 25 Feb 2005 14:57:57 +0000, Oleg Sklyar <osklyar at ebi.ac.uk> wrote:
> Hi Community,
> 
> I have a question about how to pass command line parameters to R script
> running in the batch mode. The problem is: there is a banch of data
> files which are to be processed by R script called from a web-server,
> i.e. in the batch mode. The web server generates data files and passes
> their names calling 'R CMD BATCH' one by one for every file. Now the
> question is how to call 'R CMD BATCH myScript.R' with parameters, like
> file name to process? I know how to read the parameters passed to an R
> script, but I don't know how to pass them.
> 
> There is an option --args that should cut the rest of the line off the
> command line. The problem is however that BATCH syntax is: 'R CMD BATCH
> [options] inFile [outFile]', i.e. if I write
> 'R CMD BATCH --args myDataFile myScript.R' or 'R CMD BATCH --args
> myDataFile < myScript.R' (similar was posted on some R help under
> Windows) it is not going to work because then BATCH doesn't know about
> myScript.R - it is considered as a parameter, so only 'R CMD BATCH' is
> executed. If however I use 'R CMD BATCH myScript --args myDataFile' then
> R understands --args as an output file and generates a file with that name.
> 
> Does anyone has a solution for the problem?
> 
> Best regards
> Oleg
> 
> --
> Dr Oleg Sklyar
> European Bioinformatics Institute
> Wellcome Trust Genome Campus
> Hinxton, Cambridge, CB10 1SD
> England
> phone/fax  +44(0)1223 49 4478/4468
> e-mail osklyar at ebi.ac.uk
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From p.dalgaard at biostat.ku.dk  Fri Feb 25 19:18:54 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 25 Feb 2005 19:18:54 +0100
Subject: [R] two line plot title with expression problem
In-Reply-To: <421F61A9.6090703@statistik.uni-dortmund.de>
References: <JCEIJNOHMNBPLMGFDHNDMEBNCAAA.wilks@dial.pipex.com>
	<421F61A9.6090703@statistik.uni-dortmund.de>
Message-ID: <x2y8dch59t.fsf@biostat.ku.dk>

Uwe Ligges <ligges at statistik.uni-dortmund.de> writes:

> > If the second line were to be text, this would not be a problem.
> > How can I format the title so that the expression starts immediately
> > under
> > the first line?
> 
> Multi-line expressions are not supported. You can add them by separate
> calls to title(), though:
> 
>   plot(1:10)
>   title("Golden Section", line=3)
>   title(expression(fn:(phi^2-phi-1)), line=2)
> 
> or for left-adjustment, try the same with calls to mtext() and
> specifying arguments "adj" and "at".

Well, plotmath does have atop(). It won't get you the same control
over alignment though.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From mvida at mitre.org  Fri Feb 25 19:29:38 2005
From: mvida at mitre.org (Melanie Vida)
Date: Fri, 25 Feb 2005 13:29:38 -0500
Subject: [R] Temporal Analysis of variable x;
 How to select the outlier threshold in R?
Message-ID: <421F6E92.7050409@mitre.org>

For a financial data set with large variance, I'm trying to find the 
outlier threshold of one variable "x" over a two year period. I 
qqplot(x2001, x2002) and found a normal distribution. The latter part of 
the normal distribution did not look linear though. Is there a suitable 
method in R to find the outlier threshold of this variable from 2001 and 
2002  in R?



From helprhelp at gmail.com  Fri Feb 25 19:38:21 2005
From: helprhelp at gmail.com (WeiWei Shi)
Date: Fri, 25 Feb 2005 12:38:21 -0600
Subject: [R] cda
Message-ID: <cdf81783050225103850689476@mail.gmail.com>

Hi, there:

I am wondering if I can get some general help or source about
canonical discriminant analysis in R.

My idea is trying to linearly "combine" 300 variables supervisely
(according to the class lables to the observations". I think it is
kinda PCA to do some decreasing dimentionality work, but w/
considering the class and I used SAS to do CDA proc before.

But I read the introduction from sas on this proc and found the
following statement:

"The process of extracting canonical variables can be repeated until
the number of canonical variables equals the number of original
variables or the number of classes minus one, whichever is smaller.
" 
does it mean I can only have two new variables if  I only have 2 classes?

I am not a stat guy and sorry for the question if it should not be
addressed here.

Ed



From Achim.Zeileis at wu-wien.ac.at  Fri Feb 25 19:49:33 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Fri, 25 Feb 2005 19:49:33 +0100
Subject: [R] Temporal Analysis of variable x; How to select the outlier
	threshold in R?
In-Reply-To: <421F6E92.7050409@mitre.org>
References: <421F6E92.7050409@mitre.org>
Message-ID: <20050225194933.52222d4d.Achim.Zeileis@wu-wien.ac.at>

Please stop posting (almost) identical questions!

You already posted a very similar question to R-help (and received
two answers) and you posted the same question on R-SIG-Finance!

As both Uwe and Christian indicated in their answers, your question is
very vague. If you want to receive better answers, it would help to ask
better questions. Please also read the posting guide at
  http://www.R-project.org/posting-guide.html

On Fri, 25 Feb 2005 13:29:38 -0500 Melanie Vida wrote:

> For a financial data set with large variance, I'm trying to find the 
> outlier threshold of one variable "x" over a two year period. I 

To reiterate Uwe: "This depends on your definition of an outlier and the
model for your data".

> qqplot(x2001, x2002) and found a normal distribution. The latter part

I'm not sure how you could do that from that plot...

> of the normal distribution did not look linear though. Is there a
> suitable method in R to find the outlier threshold of this variable
> from 2001 and 2002  in R?

If you think it appropriate you could fit a normal model and cut at a
quantile of your choice.
Z



From owen at stanford.edu  Fri Feb 25 19:52:27 2005
From: owen at stanford.edu (Art B. Owen)
Date: Fri, 25 Feb 2005 10:52:27 -0800
Subject: [R] Error in x[good, ] * w : non-conformable arrays 
Message-ID: <421F73EB.1020103@stanford.edu>


Dear Jacques,

Did you ever figure that error out?  I have recently
hit it too.  In my case it comes from glm() when
I replace  family = binomial  with family = cauchit
for some home brewed cauchy binary regression.

Things are ok with binomial but not with Cauchy.
The data set has some missing values.  I tried replacing
the x by multiple x's and by a matrix.  I suspect that
the intrinsic binomial regression is doing something
smarter with the missing data.

The code worked with an older R (1.7.1 or 1.8.1 I think)
on other data sets.

-Art


here's my cauchy regression hack:

cauchit = quasibinomial()
cauchit$family = "binomial"
cauchit$link   = "cauchit"
cauchit$linkfun = function(mu){  qcauchy(mu) }
cauchit$linkinv = function(eta){ pcauchy(eta) }
cauchit$mu.eta  = function(eta){ dcauchy(eta) }

then I use glm( ....  family = cauchit )



From Ted.Harding at nessie.mcc.ac.uk  Fri Feb 25 19:54:19 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 25 Feb 2005 18:54:19 -0000 (GMT)
Subject: [R] Temporal Analysis of variable x; How to select the outli
In-Reply-To: <421F6E92.7050409@mitre.org>
Message-ID: <XFMail.050225185419.Ted.Harding@nessie.mcc.ac.uk>

On 25-Feb-05 Melanie Vida wrote:
> For a financial data set with large variance, I'm trying to
> find the outlier threshold of one variable "x" over a two
> year period.
> I qqplot(x2001, x2002) and found a normal distribution.
> The latter part of the normal distribution did not look linear
> though.
> Is there a suitable method in R to find the outlier threshold
> of this variable from 2001 and 2002 in R?

I don't see how you can infer a normal distribution from qqplot(),
which simply compares the distribution of x2002 with the
distribution of x2001.

See ?qqplot for what's available and what they show.

You can check normality with a qqplot() with, e.g.,

  qqnorm(x2001)
  qqnorm(x2002)

or

  qqnorm(c(x2001,x2002))

if you want to look at their combined distirbution.

Have another look at your data, with appropriate method!

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 25-Feb-05                                       Time: 18:54:19
------------------------------ XFMail ------------------------------



From jwd at surewest.net  Fri Feb 25 20:33:52 2005
From: jwd at surewest.net (John Dougherty)
Date: Fri, 25 Feb 2005 11:33:52 -0800
Subject: [R] Graphics
In-Reply-To: <loom.20050225T152029-257@post.gmane.org>
References: <4E9E9056B353854C8AB90B2B6647183009A09813@exchange1.exchange.tvu.ac.uk>
	<200502241731.57657.jwd@surewest.net>
	<loom.20050225T152029-257@post.gmane.org>
Message-ID: <200502251133.52784.jwd@surewest.net>

Adrian, 

That was my fault.  I have the "xorg" fonts.  Peter pointed out the changes 
and I found that the 100dpi fonts were not installed.  Doing so fixed my 
problem.

John



From sdavis2 at mail.nih.gov  Fri Feb 25 21:11:48 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Fri, 25 Feb 2005 15:11:48 -0500
Subject: [R] read.table
Message-ID: <cd35ec8ddc185c41705ab851a982510a@mail.nih.gov>

I have a commonly recurring problem and wondered if folks would share 
tips.  I routinely get tab-delimited text files that I need to read in. 
  In very many cases, I get:

 > a <- read.table('junk.txt.txt',header=T,skip=10,sep="\t")
Error in scan(file = file, what = what, sep = sep, quote = quote, dec = 
dec,  :
	line 67 did not have 88 elements

I am typically able to go through the file and find a single quote or 
something like that causing the problem, but with a recent set of 
files, I haven't been able to find such an issue.  What can I do to get 
around this problem?  I can use perl, also....

Thanks,
Sean



From gunter.berton at gene.com  Fri Feb 25 21:23:34 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 25 Feb 2005 12:23:34 -0800
Subject: [R] read.table
In-Reply-To: <cd35ec8ddc185c41705ab851a982510a@mail.nih.gov>
Message-ID: <200502252023.j1PKNX5P019560@meitner.gene.com>

?readLines

I'm sure Perl will do nicely, but you can also use readLines and grep() or
regexpr() the result in R as you would in Perl to find where the problem
lies. ?nchar can also help to find a non-printing character that may be
messing you up. It's no fun, I know. Excel files can be a particular pain,
especially in their handling of missings.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Sean Davis
> Sent: Friday, February 25, 2005 12:12 PM
> To: r-help
> Subject: [R] read.table
> 
> I have a commonly recurring problem and wondered if folks would share 
> tips.  I routinely get tab-delimited text files that I need 
> to read in. 
>   In very many cases, I get:
> 
>  > a <- read.table('junk.txt.txt',header=T,skip=10,sep="\t")
> Error in scan(file = file, what = what, sep = sep, quote = 
> quote, dec = 
> dec,  :
> 	line 67 did not have 88 elements
> 
> I am typically able to go through the file and find a single quote or 
> something like that causing the problem, but with a recent set of 
> files, I haven't been able to find such an issue.  What can I 
> do to get 
> around this problem?  I can use perl, also....
> 
> Thanks,
> Sean
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From prasad.chalasani at gs.com  Fri Feb 25 21:29:33 2005
From: prasad.chalasani at gs.com (Chalasani, Prasad)
Date: Fri, 25 Feb 2005 15:29:33 -0500
Subject: [R] summary method in URCA package doesn't work
Message-ID: <AF003EF88447964B88823C3F50A6AB75025EF7F7@gsnbp25es.ny.fw.gs.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050225/437461f0/attachment.pl

From jhallman at frb.gov  Fri Feb 25 21:34:20 2005
From: jhallman at frb.gov (jhallman@frb.gov)
Date: Fri, 25 Feb 2005 15:34:20 -0500
Subject: [R] return from nested function?
Message-ID: <20050225203420.F1681AEA30@mail.rsma.frb.gov>

Is is possible from within a function to cause its caller to return()?

I have a function that lets user make edits to certain objects, and then
checks that the edited objects still make sense.  If they don't, the function
puts up a notifier that the edits are being discarded and then returns,
something like:

   if(badEdits){
       notifyDialog("bad edits will be ignored")
       return()
   }
   else {
	  ## some stuff that assigns the edited objects in the calling frame
      return()
   }

This works, but I'd really like to put the return() in the notifyDialog()
function.  Is there an easy way to do this?

Jeff



From tlumley at u.washington.edu  Fri Feb 25 21:51:00 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 25 Feb 2005 12:51:00 -0800 (PST)
Subject: [R] return from nested function?
In-Reply-To: <20050225203420.F1681AEA30@mail.rsma.frb.gov>
References: <20050225203420.F1681AEA30@mail.rsma.frb.gov>
Message-ID: <Pine.A41.4.61b.0502251250040.336378@homer06.u.washington.edu>

On Fri, 25 Feb 2005 jhallman at frb.gov wrote:

> Is is possible from within a function to cause its caller to return()?

Not as such. You probably want to signal and catch a condition.  Look at 
?tryCatch.

 	-thomas


> I have a function that lets user make edits to certain objects, and then
> checks that the edited objects still make sense.  If they don't, the function
> puts up a notifier that the edits are being discarded and then returns,
> something like:
>
>   if(badEdits){
>       notifyDialog("bad edits will be ignored")
>       return()
>   }
>   else {
> 	  ## some stuff that assigns the edited objects in the calling frame
>      return()
>   }
>
> This works, but I'd really like to put the return() in the notifyDialog()
> function.  Is there an easy way to do this?
>
> Jeff
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From Ted.Harding at nessie.mcc.ac.uk  Fri Feb 25 21:54:43 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 25 Feb 2005 20:54:43 -0000 (GMT)
Subject: [R] read.table
In-Reply-To: <cd35ec8ddc185c41705ab851a982510a@mail.nih.gov>
Message-ID: <XFMail.050225205443.Ted.Harding@nessie.mcc.ac.uk>

On 25-Feb-05 Sean Davis wrote:
> I have a commonly recurring problem and wondered if folks
> would share tips.  I routinely get tab-delimited text files
> that I need to read in.
>   In very many cases, I get:
> 
>  > a <- read.table('junk.txt.txt',header=T,skip=10,sep="\t")
> Error in scan(file = file, what = what, sep = sep, quote = quote,
> dec = dec,  :
>       line 67 did not have 88 elements
> 
> I am typically able to go through the file and find a single
> quote or something like that causing the problem, but with a
> recent set of files, I haven't been able to find such an issue.
> What can I do to get around this problem?  I can use perl, also....

Hi Sean,

This is only a shot in the dark, but your description has reminded
me of similar messes in files which have been exported from Excel.

What I have often done in such cases, to check (e.g.) the numbers
of fields in records (using 'awk' on Linux) is on the following
lines:

  cat filename | awk 'BEGIN{FS="\t"} {print NF}' | unique

In that case, if there are varying numbers of fields then
two or more different numbers will be printed instead of
the single value which it should be.

If you know how many fields to expect (e.g. 88), then you can
find the line numbers of offending records by something like

  cat filename | awk 'BEGIN{FS="\t"} {if(NF!=88){print NR}}'

In data files with a lot of records per line, doing it in
this kind of way is vastly superior to trying to spot the
problem by eye -- it's extemely difficult to count 88
tab-separated fields on screen!

Hoping this helps! If not, supply further details and we'll
see what we can think up.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 25-Feb-05                                       Time: 20:54:43
------------------------------ XFMail ------------------------------



From Ted.Harding at nessie.mcc.ac.uk  Fri Feb 25 22:14:55 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 25 Feb 2005 21:14:55 -0000 (GMT)
Subject: [R] read.table
In-Reply-To: <XFMail.050225205443.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <XFMail.050225211455.Ted.Harding@nessie.mcc.ac.uk>

On 25-Feb-05 Ted Harding wrote:
> On 25-Feb-05 Sean Davis wrote:
>> I have a commonly recurring problem and wondered if folks
>> would share tips.  I routinely get tab-delimited text files
>> that I need to read in.
>>   In very many cases, I get:
>> 
>>  > a <- read.table('junk.txt.txt',header=T,skip=10,sep="\t")
>> Error in scan(file = file, what = what, sep = sep, quote = quote,
>> dec = dec,  :
>>       line 67 did not have 88 elements
>> 
>> I am typically able to go through the file and find a single
>> quote or something like that causing the problem, but with a
>> recent set of files, I haven't been able to find such an issue.
>> What can I do to get around this problem?  I can use perl, also....
> 
> Hi Sean,
> 
> This is only a shot in the dark, but your description has reminded
> me of similar messes in files which have been exported from Excel.
> 
> What I have often done in such cases, to check (e.g.) the numbers
> of fields in records (using 'awk' on Linux) is on the following
> lines:
> 
>   cat filename | awk 'BEGIN{FS="\t"} {print NF}' | unique

OOPS!!!

  cat filename | awk 'BEGIN{FS="\t"} {print NF}' | uniq

Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 25-Feb-05                                       Time: 21:14:55
------------------------------ XFMail ------------------------------



From p.dalgaard at biostat.ku.dk  Fri Feb 25 22:18:29 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 25 Feb 2005 22:18:29 +0100
Subject: [R] read.table
In-Reply-To: <200502252023.j1PKNX5P019560@meitner.gene.com>
References: <200502252023.j1PKNX5P019560@meitner.gene.com>
Message-ID: <x2u0o0jq3e.fsf@biostat.ku.dk>

Berton Gunter <gunter.berton at gene.com> writes:

> ?readLines
> 
> I'm sure Perl will do nicely, but you can also use readLines and grep() or
> regexpr() the result in R as you would in Perl to find where the problem
> lies. ?nchar can also help to find a non-printing character that may be
> messing you up. It's no fun, I know. Excel files can be a particular pain,
> especially in their handling of missings.

You might also try read.delim() which has options set specifically to
be able to read Excel-generated CSV files.

Also check out count.fields().

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From drcarbon at gmail.com  Fri Feb 25 22:37:21 2005
From: drcarbon at gmail.com (Dr Carbon)
Date: Fri, 25 Feb 2005 16:37:21 -0500
Subject: [R] Teaching R in 40 minutes. What should be included?
Message-ID: <e89bb7ac05022513374ffacadb@mail.gmail.com>

If _you_ were asked to give a 40 minute dog and pony show about R for
a group of scientists ranging from physicists to geographers what
would you put in? These people want to know what R can do.

I'm thinking about something like:

A. Overview
B. data structures
C. arithmetic and manipulation
D. reading data
E. linear models using glm
F. graphics
G. programming
H. other tricks like rpart or time series analysis?

Thoughts? Other people must do similar things all the time. Is there a
repository of intro to R slide shows anywhere?



From Achim.Zeileis at wu-wien.ac.at  Fri Feb 25 22:40:30 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Fri, 25 Feb 2005 22:40:30 +0100
Subject: [R] summary method in URCA package doesn't work
In-Reply-To: <AF003EF88447964B88823C3F50A6AB75025EF7F7@gsnbp25es.ny.fw.gs.com>
References: <AF003EF88447964B88823C3F50A6AB75025EF7F7@gsnbp25es.ny.fw.gs.com>
Message-ID: <20050225224030.7e83da0f.Achim.Zeileis@wu-wien.ac.at>

Please contact (or at least Cc) the maintainer of contributed packages
when reporting problems with contributed packages. (I've added Bernhard
to Cc: now.)

> I can't figure out how to get the "summary" method in the URCA package
> to work. E.g. when I use the following code fragment in the help for
> the "ca.jo" function, it always tries to use the "summary" method from
> the "base" package, not the "urca" package. 
>
> How do I force it use the "summary" method of the "urca" package?
> I'm sure this is in some documentation somewhere, but
> after (admittedly quickly) scanning several docs, I've not 
> found any help on this.
> 
> Thank you.
> 
>      data(finland)
>      sjf <- finland
>      sjf.vecm <- ca.jo(sjf, constant=FALSE, type="eigen", K=2,
>      spec="longrun", season=4, ctable="A2")
>      summary(sjf.vecm)

If I understand you correctly, the last command does not return the
desired output because the wrong summary method is called?

The above works smoothly for me (using R 2.0.1 and urca 0.7-5). What
version of R and urca are you using?
Z



From p.dalgaard at biostat.ku.dk  Fri Feb 25 22:34:51 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 25 Feb 2005 22:34:51 +0100
Subject: [R] read.table
In-Reply-To: <x2u0o0jq3e.fsf@biostat.ku.dk>
References: <200502252023.j1PKNX5P019560@meitner.gene.com>
	<x2u0o0jq3e.fsf@biostat.ku.dk>
Message-ID: <x2psyojpc4.fsf@biostat.ku.dk>

Peter Dalgaard <p.dalgaard at biostat.ku.dk> writes:

> You might also try read.delim() which has options set specifically to
> be able to read Excel-generated CSV files.

Blah.

*TAB-delimited* files of course. read.csv() for the other ones.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From paul.boutros at utoronto.ca  Fri Feb 25 22:43:14 2005
From: paul.boutros at utoronto.ca (paul.boutros@utoronto.ca)
Date: Fri, 25 Feb 2005 16:43:14 -0500
Subject: [R] Problems Building R on AIX 5.2.0.0 (Update)
Message-ID: <1109367794.421f9bf2669da@webmail.utoronto.ca>

Hi,

My previous message is appended: I'm still struggling with building on AIX.  I 
updated my config.site to follow the suggestions from R-admin:
MAIN_LDFLAGS=-Wl,brtl
SHLIB_LDFLAGS=-Wl,-G

This led to an error during configure:
checking whether mixed C/Fortran code can be run... configure: WARNING: cannot 
run mixed C/Fortan code
configure: error: Maybe check LDFLAGS for paths to Fortran libraries?

This confused me a bit, because before adding the MAIN_LDFLAGS and SHLIB_LDFLAGS 
to config.site this step of configure did not show an error. When I googled this 
I found a previous message from last year:
http://tolstoy.newcastle.edu.au/R/help/04/04/1622.html

At the end of this message Professor Ripley says:
"You need wherever libg2c.so is installed in your LD_LIBRARY_PATH."

So... I went looking for this file and could not find it!  In /usr/local/lib I 
have:
$ ls -al libg2c*
-rw-r--r--   1 freeware staff       7751224 Jan 09 2004  libg2c.a
-rwxr-xr-x   1 freeware staff           714 Jan 09 2004  libg2c.la

But no libg2c.so appears to be on my system.  Does this indicate a bad install 
of gcc, or could anybody offer any suggestions on where to go from here?

Paul

---------------------------------------------------
From: Paul Boutros <Paul.Boutros_at_utoronto.ca> 
Date: Thu 24 Feb 2005 - 02:43:52 EST

Hello, 

I am trying to build R 2.0.1 on an AIX 5.2.0.0 machine using gcc 3.3.2: 
$ oslevel 

5.2.0.0 
$ gcc -v 

Reading specs from /usr/local/lib/gcc-lib/powerpc-ibm-aix5.2.0.0/3.3.2/specs 
Configured with: ../gcc-3.3.2/configure : (reconfigured) ../gcc-3.3.2/configure 
--disable-nls : (reconfigured) ../gcc-3.3.2/configure --disable-nls Thread 
model: aix 
gcc version 3.3.2 

Configure goes okay, but I get an error that I don't quite know how to interpret 
during make. I've included the summary output from the end of configure as well 
as the error that I get during make below. Any suggestions/recommendations are 
very much appreciate: I'm stuck on ideas for what could be going wrong. 

Paul 

$ ./configure --prefix=/db2blaste/R 


<snip> 

R is now configured for powerpc-ibm-aix5.2.0.0 

  Source directory: . 
  Installation directory: /db2blast/R 

  C compiler:                gcc -mno-fp-in-toc -g -O2
  C++ compiler:              g++  -g -O2
  Fortran compiler:          g77  -g -O2

  Interfaces supported:      X11

  External libraries: 
  Additional capabilities: PNG, JPEG 
  Options enabled: R profiling 

  Recommended packages: yes 

configure: WARNING: you cannot build DVI versions of the R manuals
configure: WARNING: you cannot build info or html versions of the R manuals
configure: WARNING: you cannot build PDF versions of the R manuals
configure: WARNING: I could not determine a browser
configure: WARNING: I could not determine a PDF viewer


$ make 


<snip> 

        gcc -Wl,-bM:SRE -Wl,-H512 -Wl,-T512 -Wl,-bnoentry -Wl,-bexpall -Wl,- bI:
. 

./../../etc/R.exp -L/usr/local/lib -o
lapack.so -Wl,-bI:../../../etc/Rlapack.exp
Lapack.lo rgeev.lo

rsyev.lo -L../../../lib -lRlapack -L/usr/local/lib -L/usr/ local/lib/gcc-lib/
powerpc-ibm-aix5.2.0.0/3.3.2 -L/usr/local/lib/gcc-lib/powe rpc- 
ibm-aix5.2.0.0/3.3.2/../../.. -lfrtbegin -lg2c -lm -lgcc_s /usr/local/lib/gcclib 
/powerpc-ibm-aix5.2.0.0/3.3.2/libgcc.a -lg -ldl -ltermcap -lm -lc ld: 0706-006 
Cannot find or open library file: -l Rlapack 

        ld:open(): A file or directory in the path name does not exist. 
collect2: ld returned 255 exit status 
make: 1254-004 The error code from the last command is 1. 

Stop. 
make: 1254-004 The error code from the last command is 2. 

Stop. 
make: 1254-004 The error code from the last command is 1. 

Stop. 
make: 1254-004 The error code from the last command is 1. 

Stop. 
make: 1254-004 The error code from the last command is 1. 

Stop.



From edd at debian.org  Sat Feb 26 03:30:48 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 25 Feb 2005 20:30:48 -0600
Subject: [R][Rdev] any way to generate "bitmap" (tif, jpeg,
	png etc) files in R CMD BATCH
In-Reply-To: <421F3EAD.3040607@ebi.ac.uk>
References: <421F3EAD.3040607@ebi.ac.uk>
Message-ID: <16927.57176.86034.109868@basebud.nulle.part>


Oleg, 

Please do not cross-post between r-devel and r-help. It is considered bad
taste. 

On 25 February 2005 at 15:05, Oleg Sklyar wrote:
| Hi Community,
| 
| here is the problem, Linux problem (reported to work on Windows). I need 
| to generate graphical output in any of bitmap format under the 'R CMD 
| BATCH'. Whereas the script generating png-s works perfectly in the R 
| session, such things as X11, png and jpeg are not usable in BATCH (they 
| cannot be switched on by --gui-X11 etc) and X11 is prompted to be 
| required for png. At the same time, such things as postscript and pdf, 
| which are generally X-independent work fine. The problem is that as a 
| result I need something previewable in the web browser: png, jpeg. Any 
| suggestions how to proceed? Generally I could use command line linux 
| tools to convert from almost any bitmpa format to the required png or 
| jpeg, but it would be nicer to have them as direct R output.

This is a FAQ, see

http://cran.r-project.org/doc/FAQ/R-FAQ.html#How-do-I-produce-PNG-graphics-in-batch-mode_003f

and, say, "?bitmap" in R.

Hth, Dirk

-- 
Better to have an approximate answer to the right question than a precise 
answer to the wrong question.  --  John Tukey as quoted by John Chambers



From tiago17 at socrates.Berkeley.EDU  Sat Feb 26 07:06:38 2005
From: tiago17 at socrates.Berkeley.EDU (Tiago R Magalhaes)
Date: Fri, 25 Feb 2005 22:06:38 -0800
Subject: [R] reshape without timevar argument?
Message-ID: <p06100500be45bd08c568@[192.168.1.113]>

Hi

I have a data.frame with 2 columns. The first column is an ID column. 
The other columns are description of the ids. There is more than one 
description for each Id.
Want I want to get as a value is a data.frame where each row 
corresponds to one ID and has as many columns as different 
descriptions.

I have used a very convoluted step, but I'm very convinced there is 
an easier way to do this.

Basically, I used the reshape function, but even in this convulated 
way there is a step that I can't solve. I used a "fake" timevar using 
the table function.

df <- data.frame(id=c(rep('IDa',3), rep('IDb', 5), rep('IDc', 2),
                    rep('IDd',5)), let=letters[1:5])
#add Freq to each id
xFreqdf <- merge(df, table(df['id']), by.x='id', by.y='Var1')
xFreq <- xFreqdf[,'Freq']
#general way of transforming xFreq into:
xFreq2 <- c(3:1, 5:1, 2:1, 5:1)

#substitute Freq by xFreq2
xFreqdf['Freq'] <- xFreq2

#get final df
xReshape <- reshape(xFreqdf, idvar='id', v.names='let',
                     timevar='Freq', direction='wide')

The final data.frame doesn't order the columns in the way I would 
think the more obvious: let.1 is actually the 4th column instead of 
the second one.

I again thank the kindness of people on this list



From ghoermann at hydrology.uni-kiel.de  Sat Feb 26 08:45:48 2005
From: ghoermann at hydrology.uni-kiel.de (Georg Hoermann)
Date: Sat, 26 Feb 2005 08:45:48 +0100
Subject: [R] Teaching R in 40 minutes. What should be included?
In-Reply-To: <e89bb7ac05022513374ffacadb@mail.gmail.com>
References: <e89bb7ac05022513374ffacadb@mail.gmail.com>
Message-ID: <200502260845.49058.ghoermann@hydrology.uni-kiel.de>

On Friday 25 February 2005 22:37, Dr Carbon wrote:
> If _you_ were asked to give a 40 minute dog and pony show about R for
> a group of scientists ranging from physicists to geographers what
> would you put in? These people want to know what R can do.
>
> I'm thinking about something like:
>
> A. Overview
> B. data structures
> C. arithmetic and manipulation
> D. reading data
> E. linear models using glm
> F. graphics
> G. programming
> H. other tricks like rpart or time series analysis?
>
> Thoughts? Other people must do similar things all the time. Is there
> a repository of intro to R slide shows anywhere?
>
Just my .02$

- examples of pictures Excel is unable to produce
  (Boxplots, lattice graphics, see the recent discussion here
  for examples)
- links to databases and Excel
- an example of the (l)apply function for programmers
- links to GIS (GRASS, ArcInfo) for Geographers
- examples of spatial analysis package
- compare the price tags (students and scientists can use
  it (legally!) everywhere without going to jail 8-),
  no dongles, licence servers and limitiations of the size
  of data sets.

I would focus on the workflow of data in science and
demonstrate how this is handled in R. Perhaps
the outline should have less science and more marketing 8-)

Greetings,

Georg 

-- 
Georg Hoermann, Fachabteilung Wasserwirtschaft / Dep. Hydrology  
Ecosystem Research Center, Kiel University, Germany, Penguin #189476
Tel. 0431-880-1207, 0172/4315715, ICQ: 348340729, MSN: hlschorsch



From ggrothendieck at myway.com  Sat Feb 26 08:49:53 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 26 Feb 2005 07:49:53 +0000 (UTC)
Subject: [R] reshape without timevar argument?
References: <p06100500be45bd08c568@[192.168.1.113]>
Message-ID: <loom.20050226T084648-291@post.gmane.org>

Tiago R Magalhaes <tiago17 <at> socrates.Berkeley.EDU> writes:

: I have a data.frame with 2 columns. The first column is an ID column. 
: The other columns are description of the ids. There is more than one 
: description for each Id.
: Want I want to get as a value is a data.frame where each row 
: corresponds to one ID and has as many columns as different 
: descriptions.
: 
: I have used a very convoluted step, but I'm very convinced there is 
: an easier way to do this.
: 
: Basically, I used the reshape function, but even in this convulated 
: way there is a step that I can't solve. I used a "fake" timevar using 
: the table function.
: 
: df <- data.frame(id=c(rep('IDa',3), rep('IDb', 5), rep('IDc', 2),
:                     rep('IDd',5)), let=letters[1:5])

table(df) gives a table of zeros and ones corresponding to the 
incidence matrix of IDs vs. descriptions.  Is that sufficient?



From olilili at yahoo.com  Sat Feb 26 09:58:45 2005
From: olilili at yahoo.com (Oli)
Date: Sat, 26 Feb 2005 00:58:45 -0800 (PST)
Subject: [R] Pearson Residuals in lm
Message-ID: <20050226085845.58103.qmail@web81406.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050226/fccef2a4/attachment.pl

From tom_hoary at web.de  Sat Feb 26 10:15:37 2005
From: tom_hoary at web.de (Thomas =?iso-8859-1?q?Sch=F6nhoff?=)
Date: Sat, 26 Feb 2005 10:15:37 +0100
Subject: [R] Teaching R in 40 minutes. What should be included?
In-Reply-To: <e89bb7ac05022513374ffacadb@mail.gmail.com>
References: <e89bb7ac05022513374ffacadb@mail.gmail.com>
Message-ID: <200502261015.37930.tom_hoary@web.de>

Hello,

Am Freitag, 25. Februar 2005 22:37 schrieb Dr Carbon:
> If _you_ were asked to give a 40 minute dog and pony show about R
> for a group of scientists ranging from physicists to geographers
> what would you put in? These people want to know what R can do.
>
> I'm thinking about something like:
>
> A. Overview
> B. data structures
> C. arithmetic and manipulation
> D. reading data
> E. linear models using glm
> F. graphics
> G. programming
> H. other tricks like rpart or time series analysis?

If your audience is well known I would be inclined to target some 
(simple) examples derived from physics and geography to demonstrate 
basic ideas of working with R, similar like the ones listed above.

Well, 40 minutes are not too long, so I recommend to simplify your 
presentation as much as you can. You want teach them R in 40 minutes 
but rather tend to confuse them if you don't shorten your plan a bit.
I.E. teaching programming in R in a few minutes for scientists who are 
not at all acustomed to programming  is much overhead, I think.
Well, it's up to your estimation on what is expected to follow your 
presentation. If you are sure that most of them know enough 
programming to unterstand the basic concepts in R-programming, 
everything will be fine!
If not, I'd recommend  to concentrate on basic operations (data 
structures, arithmetic and manipulation, import/export data and some 
often used default statistical procedures demonstrating common tasks 
(is time series analysis important in physics or geography, I don't 
know??), including some remarks on diffenrences to widespread 
statistical packages like SPSS or SAS, maybe LispStat.
Finally there shouuld be some extended view of available ressources 
(manuals, FAQ, community) as a starter to learn, use and program R by 
themselves.
I think this would do for a 40 minutes presentation without taking the 
risk to deter people due to overcomplexity.

regards
Thomas



From ripley at stats.ox.ac.uk  Sat Feb 26 11:22:43 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 26 Feb 2005 10:22:43 +0000 (GMT)
Subject: [R] Problems Building R on AIX 5.2.0.0 (Update)
In-Reply-To: <1109367794.421f9bf2669da@webmail.utoronto.ca>
References: <1109367794.421f9bf2669da@webmail.utoronto.ca>
Message-ID: <Pine.LNX.4.61.0502261013001.25512@gannet.stats>

Quotes from messages about Solaris 9 are not necessarily applicable to 
AIX, and in omitting the context you have misrepresented me.

Please do bear in mind the `moral rights' on quoting given at

http://www.jiscmail.ac.uk/help/policy/copyright.htm

(Perhaps such a reference is needed in the posting guide?)


On Fri, 25 Feb 2005 paul.boutros at utoronto.ca wrote:

> Hi,
>
> My previous message is appended: I'm still struggling with building on AIX.  I
> updated my config.site to follow the suggestions from R-admin:
> MAIN_LDFLAGS=-Wl,brtl
> SHLIB_LDFLAGS=-Wl,-G
>
> This led to an error during configure:
> checking whether mixed C/Fortran code can be run... configure: WARNING: cannot
> run mixed C/Fortan code
> configure: error: Maybe check LDFLAGS for paths to Fortran libraries?
>
> This confused me a bit, because before adding the MAIN_LDFLAGS and SHLIB_LDFLAGS
> to config.site this step of configure did not show an error. When I googled this
> I found a previous message from last year:
> http://tolstoy.newcastle.edu.au/R/help/04/04/1622.html
>
> At the end of this message Professor Ripley says:
> "You need wherever libg2c.so is installed in your LD_LIBRARY_PATH."
>
> So... I went looking for this file and could not find it!  In /usr/local/lib I
> have:
> $ ls -al libg2c*
> -rw-r--r--   1 freeware staff       7751224 Jan 09 2004  libg2c.a
> -rwxr-xr-x   1 freeware staff           714 Jan 09 2004  libg2c.la
>
> But no libg2c.so appears to be on my system.  Does this indicate a bad install
> of gcc, or could anybody offer any suggestions on where to go from here?
>
> Paul
>
> ---------------------------------------------------
> From: Paul Boutros <Paul.Boutros_at_utoronto.ca>
> Date: Thu 24 Feb 2005 - 02:43:52 EST
>
> Hello,
>
> I am trying to build R 2.0.1 on an AIX 5.2.0.0 machine using gcc 3.3.2:
> $ oslevel
>
> 5.2.0.0
> $ gcc -v
>
> Reading specs from /usr/local/lib/gcc-lib/powerpc-ibm-aix5.2.0.0/3.3.2/specs
> Configured with: ../gcc-3.3.2/configure : (reconfigured) ../gcc-3.3.2/configure
> --disable-nls : (reconfigured) ../gcc-3.3.2/configure --disable-nls Thread
> model: aix
> gcc version 3.3.2
>
> Configure goes okay, but I get an error that I don't quite know how to interpret
> during make. I've included the summary output from the end of configure as well
> as the error that I get during make below. Any suggestions/recommendations are
> very much appreciate: I'm stuck on ideas for what could be going wrong.
>
> Paul
>
> $ ./configure --prefix=/db2blaste/R
>
>
> <snip>
>
> R is now configured for powerpc-ibm-aix5.2.0.0
>
>  Source directory: .
>  Installation directory: /db2blast/R
>
>  C compiler:                gcc -mno-fp-in-toc -g -O2
>  C++ compiler:              g++  -g -O2
>  Fortran compiler:          g77  -g -O2
>
>  Interfaces supported:      X11
>
>  External libraries:
>  Additional capabilities: PNG, JPEG
>  Options enabled: R profiling
>
>  Recommended packages: yes
>
> configure: WARNING: you cannot build DVI versions of the R manuals
> configure: WARNING: you cannot build info or html versions of the R manuals
> configure: WARNING: you cannot build PDF versions of the R manuals
> configure: WARNING: I could not determine a browser
> configure: WARNING: I could not determine a PDF viewer
>
>
> $ make
>
>
> <snip>
>
>        gcc -Wl,-bM:SRE -Wl,-H512 -Wl,-T512 -Wl,-bnoentry -Wl,-bexpall -Wl,- bI:
> .
>
> ./../../etc/R.exp -L/usr/local/lib -o
> lapack.so -Wl,-bI:../../../etc/Rlapack.exp
> Lapack.lo rgeev.lo
>
> rsyev.lo -L../../../lib -lRlapack -L/usr/local/lib -L/usr/ local/lib/gcc-lib/
> powerpc-ibm-aix5.2.0.0/3.3.2 -L/usr/local/lib/gcc-lib/powe rpc-
> ibm-aix5.2.0.0/3.3.2/../../.. -lfrtbegin -lg2c -lm -lgcc_s /usr/local/lib/gcclib
> /powerpc-ibm-aix5.2.0.0/3.3.2/libgcc.a -lg -ldl -ltermcap -lm -lc ld: 0706-006
> Cannot find or open library file: -l Rlapack
>
>        ld:open(): A file or directory in the path name does not exist.
> collect2: ld returned 255 exit status
> make: 1254-004 The error code from the last command is 1.
>
> Stop.
> make: 1254-004 The error code from the last command is 2.
>
> Stop.
> make: 1254-004 The error code from the last command is 1.
>
> Stop.
> make: 1254-004 The error code from the last command is 1.
>
> Stop.
> make: 1254-004 The error code from the last command is 1.
>
> Stop.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sat Feb 26 11:26:56 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 26 Feb 2005 10:26:56 +0000 (GMT)
Subject: [R] Pearson Residuals in lm
In-Reply-To: <20050226085845.58103.qmail@web81406.mail.yahoo.com>
References: <20050226085845.58103.qmail@web81406.mail.yahoo.com>
Message-ID: <Pine.LNX.4.61.0502261023210.25512@gannet.stats>

On Sat, 26 Feb 2005, Oli wrote:

> i am trying to figure out the pearson residuals in a lm model:
>
>
> vl.lm <- lm(VL ~ GSS)
>
> resid (vl.lm, ??pearson??)
>
>
> but when i tried other types of residuals such as "working", "deviance", 
> etc...they give me the exact numbers...is there anything wrong with what 
> i am trying to do?

Do see the help page.  Differences emerge only for weighted fits.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From Ted.Harding at nessie.mcc.ac.uk  Sat Feb 26 11:31:05 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 26 Feb 2005 10:31:05 -0000 (GMT)
Subject: [R] R-help Time Series
Message-ID: <XFMail.050226103105.Ted.Harding@nessie.mcc.ac.uk>

Hi Folks,

While I was browsing in the R-help archives yesterday,
I got curious about the time series of the sizes of
the monthly archives in MB.

This turned out to have an unexpected feature or two,
which I leave to readers to explore for themselves.

I'm now wondering at what point in time we might expect
to be receiving 1000MB/month (30+MB/day). It's not that
far away, it seems, but there are a couple of interesting
modelling questions behind it.

In particular, I wonder by what mechanism the numbers
grow, according to the law which the data seem to indicate.

Over to you.

(just my 0.001 MB worth ... excluding headers)

Ted

To save you the trouble, the following sets up the series:

MB<-c(55,19,19,18,19,17,35,27,47,
55,32,50,55,41,49,50,28,53,42,81,54,
99,60,84,80,76,75,78,61,83,97,141,122,
96,144,173,153,226,202,131,165,183,175,168,187,
240,272,262,195,236,244,285,249,326,345,392,268,
455,320,418,453,468,422,447,400,323,516,478,327,
450,487,535,658,573,606,659,543,655,722,677,567,
519,703,886,793,719,816,812,730,698,831,969,736,
855)

April 1997 -- January 2005

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 26-Feb-05                                       Time: 10:31:05
------------------------------ XFMail ------------------------------



From ripley at stats.ox.ac.uk  Sat Feb 26 12:07:15 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 26 Feb 2005 11:07:15 +0000 (GMT)
Subject: [R] R-help Time Series
In-Reply-To: <XFMail.050226103105.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050226103105.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.LNX.4.61.0502261059290.25951@gannet.stats>

I think

1) You have the units wrong: these appear to be the figures quoted for KB 
of compressed files, and the compression is nothing like 1024:1.

2) This is not `a series' unless you add a time base, e.g. via a call 
to ts().

Surely subscribers are aware that they do not get many MB/day and that 
extrapolation to that level is just speculation.

On Sat, 26 Feb 2005 Ted.Harding at nessie.mcc.ac.uk wrote:

> Hi Folks,
>
> While I was browsing in the R-help archives yesterday,
> I got curious about the time series of the sizes of
> the monthly archives in MB.
>
> This turned out to have an unexpected feature or two,
> which I leave to readers to explore for themselves.
>
> I'm now wondering at what point in time we might expect
> to be receiving 1000MB/month (30+MB/day). It's not that
> far away, it seems, but there are a couple of interesting
> modelling questions behind it.
>
> In particular, I wonder by what mechanism the numbers
> grow, according to the law which the data seem to indicate.
>
> Over to you.
>
> (just my 0.001 MB worth ... excluding headers)
>
> Ted
>
> To save you the trouble, the following sets up the series:
>
> MB<-c(55,19,19,18,19,17,35,27,47,
> 55,32,50,55,41,49,50,28,53,42,81,54,
> 99,60,84,80,76,75,78,61,83,97,141,122,
> 96,144,173,153,226,202,131,165,183,175,168,187,
> 240,272,262,195,236,244,285,249,326,345,392,268,
> 455,320,418,453,468,422,447,400,323,516,478,327,
> 450,487,535,658,573,606,659,543,655,722,677,567,
> 519,703,886,793,719,816,812,730,698,831,969,736,
> 855)
>
> April 1997 -- January 2005
>
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 26-Feb-05                                       Time: 10:31:05
> ------------------------------ XFMail ------------------------------
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tilo.blenk at charite.de  Sat Feb 26 12:41:56 2005
From: tilo.blenk at charite.de (Tilo Blenk)
Date: Sat, 26 Feb 2005 12:41:56 +0100
Subject: [R] read.table
Message-ID: <ffb2605507d3e998f9d12596e337c267@charite.de>

Maybe argument 'fill' of read.table is the solution.

The default value is FALSE in read.table and, therefore, any line not 
having the same number of fields as the first line (not skipped) will 
make problems. If set to TRUE, as in read.delim and read.csv, lines 
with less number of fields get blank fields added at the end.

If exporting tab delimited text files from Excel lines with empty 
fields at the end in the Excel file often have less fields than the 
header line in the text file. Reading them with read.delim fixes that.

If the problem is more complicated you probably need to find the lines 
with count.fields and correct them manually.

You can find them (actually the line number) with something like

which(count.fields('data.txt') != count.fields('data.txt')[1])

assuming that the first line has the correct number of fields.

Tilo



From Ted.Harding at nessie.mcc.ac.uk  Sat Feb 26 13:06:29 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 26 Feb 2005 12:06:29 -0000 (GMT)
Subject: [R] R-help Time Series
In-Reply-To: <Pine.LNX.4.61.0502261059290.25951@gannet.stats>
Message-ID: <XFMail.050226120629.Ted.Harding@nessie.mcc.ac.uk>

On 26-Feb-05 Prof Brian Ripley wrote:
> I think
> 
> 1) You have the units wrong: these appear to be the figures
> quoted for KB of compressed files, and the compression is
> nothing like 1024:1.

Sorry, yes, you are correct: it is KB and not MB (a slip of the
eye on my part).

> 2) This is not `a series' unless you add a time base, e.g. via
> a call to ts().

Well, in R terms that is strictly correct; but a sequence of
data corresponding to successive regular time points is usually
described as a "time series"!

> Surely subscribers are aware that they do not get many MB/day
> and that extrapolation to that level is just speculation.

Granted (see above). But anyway, this sort of thing is not
the real point, which is that (regardless of units) this
'sequence' of data has interesting features (which prompted
me to submit my somewhat tongue-in-cheek posting).

The original (quoted below) now suitably amended.

> On Sat, 26 Feb 2005 Ted.Harding at nessie.mcc.ac.uk wrote:
> 
>> Hi Folks,
>>
>> While I was browsing in the R-help archives yesterday,
>> I got curious about the time series of the sizes of
>> the monthly archives in KB [was MB].
>>
>> This turned out to have an unexpected feature or two,
>> which I leave to readers to explore for themselves.
>>
>> I'm now wondering at what point in time we might expect
>> to be receiving 1000KB/month (30+KB/day) [was MB]. It's
>> not that far away, it seems, but there are a couple of
>> interesting modelling questions behind it.
>>
>> In particular, I wonder by what mechanism the numbers
>> grow, according to the law which the data seem to indicate.
>>
>> Over to you.
>>
>> (just my 0.001 MB worth ... excluding headers)
>>
>> Ted
>>
>> To save you the trouble, the following sets up the
>> sequence [was series; and MB]:
>>
 KB<-c(55,19,19,18,19,17,35,27,47,
 55,32,50,55,41,49,50,28,53,42,81,54,
 99,60,84,80,76,75,78,61,83,97,141,122,
 96,144,173,153,226,202,131,165,183,175,168,187,
 240,272,262,195,236,244,285,249,326,345,392,268,
 455,320,418,453,468,422,447,400,323,516,478,327,
 450,487,535,658,573,606,659,543,655,722,677,567,
 519,703,886,793,719,816,812,730,698,831,969,736,
 855)
>>
>> April 1997 -- January 2005
>>
>> --------------------------------------------------------------------
>> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
>> Fax-to-email: +44 (0)870 094 0861
>> Date: 26-Feb-05                                       Time: 10:31:05
>> ------------------------------ XFMail ------------------------------
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 26-Feb-05                                       Time: 12:06:29
------------------------------ XFMail ------------------------------



From ggrothendieck at myway.com  Sat Feb 26 14:16:21 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 26 Feb 2005 13:16:21 +0000 (UTC)
Subject: [R] R-help Time Series
References: <XFMail.050226103105.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <loom.20050226T141451-929@post.gmane.org>

 <Ted.Harding <at> nessie.mcc.ac.uk> writes:

: 
: Hi Folks,
: 
: While I was browsing in the R-help archives yesterday,
: I got curious about the time series of the sizes of
: the monthly archives in MB.
: 
: This turned out to have an unexpected feature or two,
: which I leave to readers to explore for themselves.
: 
: I'm now wondering at what point in time we might expect
: to be receiving 1000MB/month (30+MB/day). It's not that
: far away, it seems, but there are a couple of interesting
: modelling questions behind it.
: 
: In particular, I wonder by what mechanism the numbers
: grow, according to the law which the data seem to indicate.
: 
: Over to you.
: 
: (just my 0.001 MB worth ... excluding headers)
: 
: Ted
: 
: To save you the trouble, the following sets up the series:
: 
: MB<-c(55,19,19,18,19,17,35,27,47,
: 55,32,50,55,41,49,50,28,53,42,81,54,
: 99,60,84,80,76,75,78,61,83,97,141,122,
: 96,144,173,153,226,202,131,165,183,175,168,187,
: 240,272,262,195,236,244,285,249,326,345,392,268,
: 455,320,418,453,468,422,447,400,323,516,478,327,
: 450,487,535,658,573,606,659,543,655,722,677,567,
: 519,703,886,793,719,816,812,730,698,831,969,736,
: 855)
: 
: April 1997 -- January 2005


There were some discussions on this about a year ago:

http://tolstoy.newcastle.edu.au/R/help/04/04/1071.html

http://tolstoy.newcastle.edu.au/R/help/04/04/1095.html

http://tolstoy.newcastle.edu.au/R/help/04/04/1109.html



From ligges at statistik.uni-dortmund.de  Sat Feb 26 14:24:34 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 26 Feb 2005 14:24:34 +0100
Subject: [R] read.table
In-Reply-To: <ffb2605507d3e998f9d12596e337c267@charite.de>
References: <ffb2605507d3e998f9d12596e337c267@charite.de>
Message-ID: <42207892.3030505@statistik.uni-dortmund.de>

Tilo Blenk wrote:
> Maybe argument 'fill' of read.table is the solution.
> 
> The default value is FALSE in read.table and, therefore, any line not 
> having the same number of fields as the first line (not skipped) will 
> make problems. If set to TRUE, as in read.delim and read.csv, lines with 
> less number of fields get blank fields added at the end.
> 
> If exporting tab delimited text files from Excel lines with empty fields 
> at the end in the Excel file often have less fields than the header line 
> in the text file. Reading them with read.delim fixes that.
> 
> If the problem is more complicated you probably need to find the lines 
> with count.fields and correct them manually.
> 
> You can find them (actually the line number) with something like
> 
> which(count.fields('data.txt') != count.fields('data.txt')[1])
> 
> assuming that the first line has the correct number of fields.
> 
> Tilo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



Tilo,

is this an answer to a R-help question?
If so, I'd like to suggest to cite the original post and reply also to 
the original poster who might not be a subscribed member of this list...

Uwe Ligges



From osman.al.radi at utoronto.ca  Sat Feb 26 14:33:01 2005
From: osman.al.radi at utoronto.ca (Osman)
Date: Sat, 26 Feb 2005 08:33:01 -0500
Subject: [R] plot.summary.Design()
Message-ID: <001c01c51c07$b1497fa0$0802a8c0@Toshi>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050226/1c0d49d6/attachment.pl

From ligges at statistik.uni-dortmund.de  Sat Feb 26 14:41:31 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 26 Feb 2005 14:41:31 +0100
Subject: [R] R-help Time Series
In-Reply-To: <Pine.LNX.4.61.0502261059290.25951@gannet.stats>
References: <XFMail.050226103105.Ted.Harding@nessie.mcc.ac.uk>
	<Pine.LNX.4.61.0502261059290.25951@gannet.stats>
Message-ID: <42207C8B.8020701@statistik.uni-dortmund.de>

Prof Brian Ripley wrote:

> I think
> 
> 1) You have the units wrong: these appear to be the figures quoted for 
> KB of compressed files, and the compression is nothing like 1024:1.
> 
> 2) This is not `a series' unless you add a time base, e.g. via a call to 
> ts().
> 
> Surely subscribers are aware that they do not get many MB/day and that 
> extrapolation to that level is just speculation.


So let's be immensely unfair and do some speculation ...

Assuming 1000MB/month means a compressed archive file of (very) 
*roughly* 250MB.

Looking at the data with linear models,
    lm(sqrt(MB) ~ monthindex)
seems not to be the worst model (removing the first observation, perhaps).

So a very *rough* extrapolation shows us that we will get
1000MB/month around 2142.

I hope I'll get a better machine in 137 years to handle all the expected 
traffic. ;-)

Best,
Uwe


> On Sat, 26 Feb 2005 Ted.Harding at nessie.mcc.ac.uk wrote:
> 
>> Hi Folks,
>>
>> While I was browsing in the R-help archives yesterday,
>> I got curious about the time series of the sizes of
>> the monthly archives in MB.
>>
>> This turned out to have an unexpected feature or two,
>> which I leave to readers to explore for themselves.
>>
>> I'm now wondering at what point in time we might expect
>> to be receiving 1000MB/month (30+MB/day). It's not that
>> far away, it seems, but there are a couple of interesting
>> modelling questions behind it.
>>
>> In particular, I wonder by what mechanism the numbers
>> grow, according to the law which the data seem to indicate.
>>
>> Over to you.
>>
>> (just my 0.001 MB worth ... excluding headers)
>>
>> Ted
>>
>> To save you the trouble, the following sets up the series:
>>
>> MB<-c(55,19,19,18,19,17,35,27,47,
>> 55,32,50,55,41,49,50,28,53,42,81,54,
>> 99,60,84,80,76,75,78,61,83,97,141,122,
>> 96,144,173,153,226,202,131,165,183,175,168,187,
>> 240,272,262,195,236,244,285,249,326,345,392,268,
>> 455,320,418,453,468,422,447,400,323,516,478,327,
>> 450,487,535,658,573,606,659,543,655,722,677,567,
>> 519,703,886,793,719,816,812,730,698,831,969,736,
>> 855)
>>
>> April 1997 -- January 2005
>>
>> --------------------------------------------------------------------
>> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
>> Fax-to-email: +44 (0)870 094 0861
>> Date: 26-Feb-05                                       Time: 10:31:05
>> ------------------------------ XFMail ------------------------------
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>



From Ted.Harding at nessie.mcc.ac.uk  Sat Feb 26 15:00:46 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 26 Feb 2005 14:00:46 -0000 (GMT)
Subject: [R] R-help Time Series
In-Reply-To: <42207C8B.8020701@statistik.uni-dortmund.de>
Message-ID: <XFMail.050226140046.Ted.Harding@nessie.mcc.ac.uk>

On 26-Feb-05 Uwe Ligges wrote:
> So let's be immensely unfair and do some speculation ...
> 
> Assuming 1000MB/month means a compressed archive file of (very) 
> *roughly* 250MB.
> 
> Looking at the data with linear models,
>     lm(sqrt(MB) ~ monthindex)
> seems not to be the worst model (removing the first observation,
> perhaps).

Well, Uwe, disregarding the confusion over units, your model
(and caveat) coincides with mine, leading to

  KB = (2.448731+0.282701*T + noise)^2

where T is in months and the first observation is at T=1.

(Actually, I think the initial "burn-in" might be a bit longer,
say over 3-4 months, and there is a slight suggestion that the
growth has been slightly flattening out recently).

And what intrigued me is the question: what mechanism might
lead to a quadratic growth law?

One possible interpretation is the following:

Suppose that the number of postings is proportional to the
number of R users. A quadratic has first difference linear in T.
So the average number of additional postings per month can be
seen as a sum of two components:

a) a constant "kernel"
b) a component proportional to the increment in postings
   in the previous month.

Interpreting this as number of users, it could suggest that
recruitment to R could be due to two causes: recruitment by
a "core" of fixed size, and recruitment by recent recruits!

Of course this is far from the only possibility. Another might
be that postings to R-help reflect the number of issues that
users are concerned to get help or information on.

This might reflect:

a) a "core" of die-hard FAQs asked by more and more people;
b) a growing corpus of packages which more and more people
   need guidance with.

And so on. An essential missing piece of data (where I'm
concerned) is the sequence of numbers of subscribers to
R-help.

Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 26-Feb-05                                       Time: 14:00:46
------------------------------ XFMail ------------------------------



From Paul.Boutros at utoronto.ca  Sat Feb 26 17:17:32 2005
From: Paul.Boutros at utoronto.ca (Paul Boutros)
Date: Sat, 26 Feb 2005 11:17:32 -0500
Subject: [R] Problems Building R on AIX 5.2.0.0 (Update)
In-Reply-To: <Pine.LNX.4.61.0502261013001.25512@gannet.stats>
Message-ID: <CPEAKHBKLBNIKJDIELLCOEPGCPAA.Paul.Boutros@utoronto.ca>

My apologies -- I had believed that by linking the source message I
had made the detailed context available.  I will be more careful in
the future to correctly give full context.

Paul

> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> Sent: Saturday, February 26, 2005 5:23 AM
> To: paul.boutros at utoronto.ca
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Problems Building R on AIX 5.2.0.0 (Update)
>
>
> Quotes from messages about Solaris 9 are not necessarily applicable to
> AIX, and in omitting the context you have misrepresented me.
>
> Please do bear in mind the `moral rights' on quoting given at
>
> http://www.jiscmail.ac.uk/help/policy/copyright.htm
>
> (Perhaps such a reference is needed in the posting guide?)
>
>
> On Fri, 25 Feb 2005 paul.boutros at utoronto.ca wrote:
>
> > Hi,
> >
> > My previous message is appended: I'm still struggling with
> building on AIX.  I
> > updated my config.site to follow the suggestions from R-admin:
> > MAIN_LDFLAGS=-Wl,brtl
> > SHLIB_LDFLAGS=-Wl,-G
> >
> > This led to an error during configure:
> > checking whether mixed C/Fortran code can be run... configure:
> WARNING: cannot
> > run mixed C/Fortan code
> > configure: error: Maybe check LDFLAGS for paths to Fortran libraries?
> >
> > This confused me a bit, because before adding the MAIN_LDFLAGS
> and SHLIB_LDFLAGS
> > to config.site this step of configure did not show an error.
> When I googled this
> > I found a previous message from last year:
> > http://tolstoy.newcastle.edu.au/R/help/04/04/1622.html
> >
> > At the end of this message Professor Ripley says:
> > "You need wherever libg2c.so is installed in your LD_LIBRARY_PATH."
> >
> > So... I went looking for this file and could not find it!  In
> /usr/local/lib I
> > have:
> > $ ls -al libg2c*
> > -rw-r--r--   1 freeware staff       7751224 Jan 09 2004  libg2c.a
> > -rwxr-xr-x   1 freeware staff           714 Jan 09 2004  libg2c.la
> >
> > But no libg2c.so appears to be on my system.  Does this
> indicate a bad install
> > of gcc, or could anybody offer any suggestions on where to go from here?
> >
> > Paul
> >
> > ---------------------------------------------------
> > From: Paul Boutros <Paul.Boutros_at_utoronto.ca>
> > Date: Thu 24 Feb 2005 - 02:43:52 EST
> >
> > Hello,
> >
> > I am trying to build R 2.0.1 on an AIX 5.2.0.0 machine using gcc 3.3.2:
> > $ oslevel
> >
> > 5.2.0.0
> > $ gcc -v
> >
> > Reading specs from
> /usr/local/lib/gcc-lib/powerpc-ibm-aix5.2.0.0/3.3.2/specs
> > Configured with: ../gcc-3.3.2/configure : (reconfigured)
> ../gcc-3.3.2/configure
> > --disable-nls : (reconfigured) ../gcc-3.3.2/configure
> --disable-nls Thread
> > model: aix
> > gcc version 3.3.2
> >
> > Configure goes okay, but I get an error that I don't quite know
> how to interpret
> > during make. I've included the summary output from the end of
> configure as well
> > as the error that I get during make below. Any
> suggestions/recommendations are
> > very much appreciate: I'm stuck on ideas for what could be going wrong.
> >
> > Paul
> >
> > $ ./configure --prefix=/db2blaste/R
> >
> >
> > <snip>
> >
> > R is now configured for powerpc-ibm-aix5.2.0.0
> >
> >  Source directory: .
> >  Installation directory: /db2blast/R
> >
> >  C compiler:                gcc -mno-fp-in-toc -g -O2
> >  C++ compiler:              g++  -g -O2
> >  Fortran compiler:          g77  -g -O2
> >
> >  Interfaces supported:      X11
> >
> >  External libraries:
> >  Additional capabilities: PNG, JPEG
> >  Options enabled: R profiling
> >
> >  Recommended packages: yes
> >
> > configure: WARNING: you cannot build DVI versions of the R manuals
> > configure: WARNING: you cannot build info or html versions of
> the R manuals
> > configure: WARNING: you cannot build PDF versions of the R manuals
> > configure: WARNING: I could not determine a browser
> > configure: WARNING: I could not determine a PDF viewer
> >
> >
> > $ make
> >
> >
> > <snip>
> >
> >        gcc -Wl,-bM:SRE -Wl,-H512 -Wl,-T512 -Wl,-bnoentry
> -Wl,-bexpall -Wl,- bI:
> > .
> >
> > ./../../etc/R.exp -L/usr/local/lib -o
> > lapack.so -Wl,-bI:../../../etc/Rlapack.exp
> > Lapack.lo rgeev.lo
> >
> > rsyev.lo -L../../../lib -lRlapack -L/usr/local/lib -L/usr/
> local/lib/gcc-lib/
> > powerpc-ibm-aix5.2.0.0/3.3.2 -L/usr/local/lib/gcc-lib/powe rpc-
> > ibm-aix5.2.0.0/3.3.2/../../.. -lfrtbegin -lg2c -lm -lgcc_s
> /usr/local/lib/gcclib
> > /powerpc-ibm-aix5.2.0.0/3.3.2/libgcc.a -lg -ldl -ltermcap -lm
> -lc ld: 0706-006
> > Cannot find or open library file: -l Rlapack
> >
> >        ld:open(): A file or directory in the path name does not exist.
> > collect2: ld returned 255 exit status
> > make: 1254-004 The error code from the last command is 1.
> >
> > Stop.
> > make: 1254-004 The error code from the last command is 2.
> >
> > Stop.
> > make: 1254-004 The error code from the last command is 1.
> >
> > Stop.
> > make: 1254-004 The error code from the last command is 1.
> >
> > Stop.
> > make: 1254-004 The error code from the last command is 1.
> >
> > Stop.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>

--
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From matthewpharr at gmail.com  Sat Feb 26 17:46:39 2005
From: matthewpharr at gmail.com (Matthew Pharr)
Date: Sat, 26 Feb 2005 10:46:39 -0600
Subject: [R] Reshaping a data frame
Message-ID: <a26d47d405022608469202e9e@mail.gmail.com>

I am new to R. Having come from SPSS and SAS to this program, I know
that my problem is
reshaping from long to wide. How would I go about reshaping data frame
(A) so that
information for each record is contained on one row. I am aware of the
R reshape
function, but I am uncertain of how to instruct R to reshape the data
set because I
need to move var1 (var1 is a survey question) out wide while placing
the values of var2 (var2 is the answer to the survey question) under
the correct survey question. Thank you for any help provided.

*Data Frame (A)

id        var1               var2

12345  gender_m      1
12345  age_22          1
12345  car_benz       1
12345  reader           28
23456  gender_f        1
23456  age_35          1
23456  workwk         40
23456  reader           30
23456  kid_0_3         1
34567  gender_m      1
34567  age_45          1

*Data Frame (B)
  id     gender_m  age_22  car_benz  reader 
12345       1             1           1           28



From thills at mail.utexas.edu  Sat Feb 26 18:17:13 2005
From: thills at mail.utexas.edu (thomas hills)
Date: Sat, 26 Feb 2005 11:17:13 -0600
Subject: [R] averaging within columns
In-Reply-To: <200502261104.j1QB3aDv025598@hypatia.math.ethz.ch>
Message-ID: <41AE5EFC-881A-11D9-8B5A-000393DC4A86@mail.utexas.edu>

I have a dataframe with names in the first column and wait times 
between decisions in the second column.  Since individuals make 
multiple decisions, I want the average for each individual.  For 
example, the data might look like this

name	wtime
jo		1
jo		2
jo		1
jo 		3
tim		3	
tim		2
tim		2
ro		1
ro		2
etc.

I'm hoping there is something like

mean(dataname$wtime[name])

which will just create a column with length equal to the number of 
different names (levels) and an average wtime for each.  So far though, 
I haven't had much luck figuring that one out.

Thanks.
Thomas



From bxc at steno.dk  Sat Feb 26 18:19:47 2005
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Sat, 26 Feb 2005 18:19:47 +0100
Subject: [R] averaging within columns
Message-ID: <0ABD88905D18E347874E0FB71C0B29E9027FEC49@exdkba022.novo.dk>

?tapply

Bendix Carstensen

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of thomas hills
> Sent: Saturday, February 26, 2005 6:17 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] averaging within columns
> 
> 
> I have a dataframe with names in the first column and wait times 
> between decisions in the second column.  Since individuals make 
> multiple decisions, I want the average for each individual.  For 
> example, the data might look like this
> 
> name	wtime
> jo		1
> jo		2
> jo		1
> jo 		3
> tim		3	
> tim		2
> tim		2
> ro		1
> ro		2
> etc.
> 
> I'm hoping there is something like
> 
> mean(dataname$wtime[name])
> 
> which will just create a column with length equal to the number of 
> different names (levels) and an average wtime for each.  So 
> far though, 
> I haven't had much luck figuring that one out.
> 
> Thanks.
> Thomas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From tlumley at u.washington.edu  Sat Feb 26 18:20:49 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Sat, 26 Feb 2005 09:20:49 -0800 (PST)
Subject: [R] Reshaping a data frame
In-Reply-To: <a26d47d405022608469202e9e@mail.gmail.com>
References: <a26d47d405022608469202e9e@mail.gmail.com>
Message-ID: <Pine.A41.4.61b.0502260919420.52040@homer12.u.washington.edu>

On Sat, 26 Feb 2005, Matthew Pharr wrote:
>         but I am uncertain of how to instruct R to reshape the data
> set because I
> need to move var1 (var1 is a survey question) out wide while placing
> the values of var2 (var2 is the answer to the survey question) under
> the correct survey question. Thank you for any help provided.

You want var1 to be the "timevar":

> reshape(a,direction="wide",timevar="var1",idvar="id")
       id var2.gender_m var2.age_22 var2.car_benz var2.reader var2.gender_f
1  12345             1           1             1          28            NA
5  23456            NA          NA            NA          30             1
10 34567             1          NA            NA          NA            NA
    var2.age_35 var2.workwk var2.kid_0_3 var2.age_45
1           NA          NA           NA          NA
5            1          40            1          NA
10          NA          NA           NA           1


 	-thomas



From m.mamin at intershop.de  Sat Feb 26 18:27:05 2005
From: m.mamin at intershop.de (Marc Mamin)
Date: Sat, 26 Feb 2005 18:27:05 +0100
Subject: [R] averaging within columns
In-Reply-To: <41AE5EFC-881A-11D9-8B5A-000393DC4A86@mail.utexas.edu>
References: <41AE5EFC-881A-11D9-8B5A-000393DC4A86@mail.utexas.edu>
Message-ID: <opsmterfh819ufgc@mail.intershop.de>

Hello Thomas,
The function aggregate will do the job.

Marc Mamin



From dmb at mrc-dunn.cam.ac.uk  Sat Feb 26 18:55:48 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Sat, 26 Feb 2005 17:55:48 +0000 (GMT)
Subject: [R] Hosting a R Graph Gallery?
In-Reply-To: <42163271.5040906@free.fr>
Message-ID: <Pine.LNX.4.21.0502261755400.4670-100000@mail.mrc-dunn.cam.ac.uk>

On Fri, 18 Feb 2005, Romain Francois wrote:

>Hello Sander,
>
>That's a good idea and i am up to it.
>
>Right now i am in an exam period, so it's not really the better time, 
>give me a couple of weeks and i will come up with a specific format of R 
>files to submit to me that i could post-process to generate html documents.
>To my mind, those html files should show :
>
>- the plot itself

The code!


>+ Submitter(s)
>        - web page
>        - email (eventually protected, I don't know how to do it)
>- Bibliographic references
>- Required R packages
>+ Commentaries
>       - in english
>       - and in any other languages
>
>I'm open to any suggestion.
>
>Romain.
>
>Le 18.02.2005 14:33, Sander Oom a crit :
>
>> Dear R users,
>>
>> Following some of the recent questions and discussions about the R 
>> plotting abilities, it occurred to me again that it would be very 
>> valuable to have an R graph gallery.
>>
>> Eric Lecoutre made a very nice example in:
>> http://www.stat.ucl.ac.be/ISpersonnel/lecoutre/stats/fichiers/_gallery.pdf 
>>
>>
>> It would be very useful to many beginners, but probably also advanced 
>> users of R, to have an overview of R graph types with graphical 
>> examples  and associated R code.
>>
>> In order to facilitate the evolution of a large gallery, some sort of 
>> wiki environment might be most suitable, thus providing access to all 
>> users, but with limited maintenance costs for the provider.
>>
>> Do others agree this could be a valuable resource? Would anybody have 
>> the resources to host such an R graph gallery?
>>
>> Yours,
>>
>> Sander Oom.
>>
>



From ggrothendieck at myway.com  Sat Feb 26 19:11:23 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 26 Feb 2005 18:11:23 +0000 (UTC)
Subject: [R] Reshaping a data frame
References: <a26d47d405022608469202e9e@mail.gmail.com>
Message-ID: <loom.20050226T190641-14@post.gmane.org>

Matthew Pharr <matthewpharr <at> gmail.com> writes:

: 
: I am new to R. Having come from SPSS and SAS to this program, I know
: that my problem is
: reshaping from long to wide. How would I go about reshaping data frame
: (A) so that
: information for each record is contained on one row. I am aware of the
: R reshape
: function, but I am uncertain of how to instruct R to reshape the data
: set because I
: need to move var1 (var1 is a survey question) out wide while placing
: the values of var2 (var2 is the answer to the survey question) under
: the correct survey question. Thank you for any help provided.
: 
: *Data Frame (A)
: 
: id        var1               var2
: 
: 12345  gender_m      1
: 12345  age_22          1
: 12345  car_benz       1
: 12345  reader           28
: 23456  gender_f        1
: 23456  age_35          1
: 23456  workwk         40
: 23456  reader           30
: 23456  kid_0_3         1
: 34567  gender_m      1
: 34567  age_45          1
: 
: *Data Frame (B)
:   id     gender_m  age_22  car_benz  reader 
: 12345       1             1           1           28

Another possibility is to use xtabs. The formula specifies
that var2 is to be the table entry while the rows and columns
are to be id and var1:

R> xtabs(var2 ~ id + var1, A)
       var1
id      age_22 age_35 age_45 car_benz gender_f gender_m kid_0_3 reader workwk
  12345      1      0      0        1        0        1       0     28      0
  23456      0      1      0        0        1        0       1     30     40
  34567      0      0      1        0        0        1       0      0      0



From spencer.graves at pdf.com  Sat Feb 26 20:09:32 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 26 Feb 2005 11:09:32 -0800
Subject: [R] Teaching R in 40 minutes. What should be included?
In-Reply-To: <200502261015.37930.tom_hoary@web.de>
References: <e89bb7ac05022513374ffacadb@mail.gmail.com>
	<200502261015.37930.tom_hoary@web.de>
Message-ID: <4220C96C.8010203@pdf.com>

      I agree with Thomas and Georg:  A 40 minute intro should be mostly 
Marketing and very little "how to". 

      I think you'll have a more effective sales job if you target, say, 
4 examples averaging 5 slides each plus some general overview, max 25-30 
slides.  If I had sufficient prep time and a few collaborators among 
physicists, geographers, etc., I might get their help in preparing 
examples, showing how they would do something in Matlab or Scilab or 
something else vs. R.  And I'd end with a discussion of technical 
support via an R site search and r-help and showing a list of available 
contributed packages.  I'd do a couple of searches for physics and 
geographical questions.  ODESOLVE, maps, etc.  Maybe pick examples that 
are part of the help files.  Then show, here is how I find X, here is 
the vignette, help or whatever. 

      Is it fair to say that R is rapidly becoming (if it is not 
already) the primary platform of choice for new statistical algorithm 
development?  I think they might be interested in a brief overview of 
the contributed software.  If this is an academic audience, they might 
like to know how easy it is to contribute software, plus journal on 
statistical computing and graphics, etc. 

      hope this helps.  Good Luck!
      spencer graves    

Thomas Sch?nhoff wrote:

>Hello,
>
>Am Freitag, 25. Februar 2005 22:37 schrieb Dr Carbon:
>  
>
>>If _you_ were asked to give a 40 minute dog and pony show about R
>>for a group of scientists ranging from physicists to geographers
>>what would you put in? These people want to know what R can do.
>>
>>I'm thinking about something like:
>>
>>A. Overview
>>B. data structures
>>C. arithmetic and manipulation
>>D. reading data
>>E. linear models using glm
>>F. graphics
>>G. programming
>>H. other tricks like rpart or time series analysis?
>>    
>>
>
>If your audience is well known I would be inclined to target some 
>(simple) examples derived from physics and geography to demonstrate 
>basic ideas of working with R, similar like the ones listed above.
>
>Well, 40 minutes are not too long, so I recommend to simplify your 
>presentation as much as you can. You want teach them R in 40 minutes 
>but rather tend to confuse them if you don't shorten your plan a bit.
>I.E. teaching programming in R in a few minutes for scientists who are 
>not at all acustomed to programming  is much overhead, I think.
>Well, it's up to your estimation on what is expected to follow your 
>presentation. If you are sure that most of them know enough 
>programming to unterstand the basic concepts in R-programming, 
>everything will be fine!
>If not, I'd recommend  to concentrate on basic operations (data 
>structures, arithmetic and manipulation, import/export data and some 
>often used default statistical procedures demonstrating common tasks 
>(is time series analysis important in physics or geography, I don't 
>know??), including some remarks on diffenrences to widespread 
>statistical packages like SPSS or SAS, maybe LispStat.
>Finally there shouuld be some extended view of available ressources 
>(manuals, FAQ, community) as a starter to learn, use and program R by 
>themselves.
>I think this would do for a 40 minutes presentation without taking the 
>risk to deter people due to overcomplexity.
>
>regards
>Thomas
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From f.harrell at vanderbilt.edu  Sat Feb 26 20:32:05 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sat, 26 Feb 2005 14:32:05 -0500
Subject: [R] Teaching R in 40 minutes. What should be included?
In-Reply-To: <4220C96C.8010203@pdf.com>
References: <e89bb7ac05022513374ffacadb@mail.gmail.com>	<200502261015.37930.tom_hoary@web.de>
	<4220C96C.8010203@pdf.com>
Message-ID: <4220CEB5.2030301@vanderbilt.edu>

Spencer Graves wrote:
>      I agree with Thomas and Georg:  A 40 minute intro should be mostly 
> Marketing and very little "how to".
>      I think you'll have a more effective sales job if you target, say, 
> 4 examples averaging 5 slides each plus some general overview, max 25-30 
> slides.  If I had sufficient prep time and a few collaborators among 
> physicists, geographers, etc., I might get their help in preparing 
> examples, showing how they would do something in Matlab or Scilab or 
> something else vs. R.  And I'd end with a discussion of technical 
> support via an R site search and r-help and showing a list of available 
> contributed packages.  I'd do a couple of searches for physics and 
> geographical questions.  ODESOLVE, maps, etc.  Maybe pick examples that 
> are part of the help files.  Then show, here is how I find X, here is 
> the vignette, help or whatever.
>      Is it fair to say that R is rapidly becoming (if it is not already) 
> the primary platform of choice for new statistical algorithm 
> development?  I think they might be interested in a brief overview of 
> the contributed software.  If this is an academic audience, they might 
> like to know how easy it is to contribute software, plus journal on 
> statistical computing and graphics, etc.
>      hope this helps.  Good Luck!
>      spencer graves   

I often give talks like that.  The thing that has impressed audiences 
the most is a multi-panel lattice graphic with 2 classification 
variables and in each panel a scatterplot and a lowess trend line.  A 
single page with 24 small high-resolution histograms also seems to 
impress people.  The nomogram function in the Design package seems to 
also connect with non-statisticians, as does 
latex(describe(mydataframe)) using Hmisc.  People like seeing in the 
latex previewer some output that mixes tabular summaries and graphics.

Frank Harrell

> Thomas Sch?nhoff wrote:
> 
>> Hello,
>>
>> Am Freitag, 25. Februar 2005 22:37 schrieb Dr Carbon:
>>  
>>
>>> If _you_ were asked to give a 40 minute dog and pony show about R
>>> for a group of scientists ranging from physicists to geographers
>>> what would you put in? These people want to know what R can do.
>>>
>>> I'm thinking about something like:
>>>
>>> A. Overview
>>> B. data structures
>>> C. arithmetic and manipulation
>>> D. reading data
>>> E. linear models using glm
>>> F. graphics
>>> G. programming
>>> H. other tricks like rpart or time series analysis?
>>>   
>>
>>
>> If your audience is well known I would be inclined to target some 
>> (simple) examples derived from physics and geography to demonstrate 
>> basic ideas of working with R, similar like the ones listed above.
>>
>> Well, 40 minutes are not too long, so I recommend to simplify your 
>> presentation as much as you can. You want teach them R in 40 minutes 
>> but rather tend to confuse them if you don't shorten your plan a bit.
>> I.E. teaching programming in R in a few minutes for scientists who are 
>> not at all acustomed to programming  is much overhead, I think.
>> Well, it's up to your estimation on what is expected to follow your 
>> presentation. If you are sure that most of them know enough 
>> programming to unterstand the basic concepts in R-programming, 
>> everything will be fine!
>> If not, I'd recommend  to concentrate on basic operations (data 
>> structures, arithmetic and manipulation, import/export data and some 
>> often used default statistical procedures demonstrating common tasks 
>> (is time series analysis important in physics or geography, I don't 
>> know??), including some remarks on diffenrences to widespread 
>> statistical packages like SPSS or SAS, maybe LispStat.
>> Finally there shouuld be some extended view of available ressources 
>> (manuals, FAQ, community) as a starter to learn, use and program R by 
>> themselves.
>> I think this would do for a 40 minutes presentation without taking the 
>> risk to deter people due to overcomplexity.
>>
>> regards
>> Thomas

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From john.maindonald at anu.edu.au  Sat Feb 26 21:45:14 2005
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Sun, 27 Feb 2005 07:45:14 +1100
Subject: [R] read.table
In-Reply-To: <200502261103.j1QB3Bgs025456@hypatia.math.ethz.ch>
References: <200502261103.j1QB3Bgs025456@hypatia.math.ethz.ch>
Message-ID: <578630e891a395c84f7c8190eef37502@anu.edu.au>

In addition to other suggestions made, note also count.fields().

 > cat("10 9 17  # First of 7 lines", "11 13 1 6", "9 14 16",
+     "12 15 14", "8 15 15", "9 13 12", "7 14 18",
+     file="oneBadRow.txt", sep="\n")
 > nfields <- count.fields("oneBadRow.txt")
 > nfields
[1] 3 4 3 3 3 3 3
 > table(nfields)     ## Use with many records
nfields
3 4
6 1
 > tab <- table(nfields)
 > (1:length(nfields))[nfields == 4]
[1] 2
 > readLines("oneBadRow.txt", n=-1)[2]
[1] "11 13 1 6"

Note the various option settings for count.fields()

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Bioinformation Science, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.


On 26 Feb 2005, at 10:03 PM, r-help-request at stat.math.ethz.ch wrote:

> From: Sean Davis <sdavis2 at mail.nih.gov>
> Date: 26 February 2005 7:11:48 AM
> To: r-help <r-help at stat.math.ethz.ch>
> Subject: [R] read.table
>
>
> I have a commonly recurring problem and wondered if folks would share 
> tips.  I routinely get tab-delimited text files that I need to read 
> in.  In very many cases, I get:
>
> > a <- read.table('junk.txt.txt',header=T,skip=10,sep="\t")
> Error in scan(file = file, what = what, sep = sep, quote = quote, dec 
> = dec,  :
> 	line 67 did not have 88 elements
>
> I am typically able to go through the file and find a single quote or 
> something like that causing the problem, but with a recent set of 
> files, I haven't been able to find such an issue.  What can I do to 
> get around this problem?  I can use perl, also....
>
> Thanks,
> Sean



From m.abdolell at utoronto.ca  Sat Feb 26 21:48:36 2005
From: m.abdolell at utoronto.ca (Mohamed Abdolell)
Date: Sat, 26 Feb 2005 15:48:36 -0500
Subject: [R] Sweave and \input or \include LaTeX commands
In-Reply-To: <7FFEE688B57D7346BC6241C55900E730B6FF44@pollux.bfro.uni-lj.si>
Message-ID: <000101c51c44$8ae25610$6401a8c0@euclid>

I have used \input in my .Snw file and it works fine.

- Mohamed


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gorjanc Gregor
Sent: February 23, 2005 10:50 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Sweave and \input or \include LaTeX commands

Hello!

I was just wondering if Sweave can work with \input or \include 
LaTeX commands. So, is it aware of such a possible hierarchy in
documents. I would test that, but I don't have such a report 
available at the moment.

I thought of that when I was writting shell script for Sweave
from command line and I have solved that part there.

--
Lep pozdrav / With regards,
    Gregor GORJANC

---------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty       URI: http://www.bfro.uni-lj.si
Zootechnical Department    email: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                  tel: +386 (0)1 72 17 861
SI-1230 Domzale            fax: +386 (0)1 72 17 888
Slovenia

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Gregor.Gorjanc at bfro.uni-lj.si  Sun Feb 27 00:33:36 2005
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Sun, 27 Feb 2005 00:33:36 +0100
Subject: [R] Sweave and \input or \include LaTeX commands
Message-ID: <7FFEE688B57D7346BC6241C55900E730B6FF57@pollux.bfro.uni-lj.si>

Imagine this situation:

% --- a.Rnw start ---
\documentclass{book}
\usepackage{Sweave}
\begin{document}
% some toy example
<<print=TRUE>>=
x <- 1:10
@
% now we input additional file
\input{a1}
% and lets look again at x
<<print=TRUE>>=
x
@
\end{document}
% --- a.Rnw end ---

% --- a1.Rnw start ---
%\usepackage{Sweave}
% add 1 to x
<<print=TRUE>>=
x <- x + 1
@
% --- a1.Rnw end ---

> Sweave("a.Rnw")
Writing to file a.tex
Processing code chunks ...
 1 : echo print term verbatim
 2 : echo print term verbatim

You can now run LaTeX on a.tex 

When you run Sweave on a.Rnw it founds only two chunks of R code. So it
does not go into a1.Rnw i.e. it does not follow \input command. This 
would be very usefull if one has such hierarchies in documents. Afcourse
one can use Sweave on a.Rnw and a1.Rnw and then run LaTeX, but it would be
nice if just one Sweave would do Sweave job for all files i.e. the same
as LaTeX does. 

--
Lep pozdrav / With regards,
    Gregor GORJANC

------------------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty       URI: http://www.bfro.uni-lj.si/MR/ggorjan
Zootechnical Department    email: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                  tel: +386 (0)1 72 17 861
SI-1230 Domzale            fax: +386 (0)1 72 17 888
Slovenia
------------------------------------------------------------------------



-----Original Message-----
From: Mohamed Abdolell [mailto:m.abdolell at utoronto.ca]
Sent: sob 2005-02-26 21:48
To: Gorjanc Gregor; r-help at stat.math.ethz.ch
Subject: RE: [R] Sweave and \input or \include LaTeX commands
 
I have used \input in my .Snw file and it works fine.

- Mohamed


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gorjanc Gregor
Sent: February 23, 2005 10:50 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Sweave and \input or \include LaTeX commands

Hello!

I was just wondering if Sweave can work with \input or \include 
LaTeX commands. So, is it aware of such a possible hierarchy in
documents. I would test that, but I don't have such a report 
available at the moment.

I thought of that when I was writting shell script for Sweave
from command line and I have solved that part there.

--
Lep pozdrav / With regards,
    Gregor GORJANC

---------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty       URI: http://www.bfro.uni-lj.si
Zootechnical Department    email: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                  tel: +386 (0)1 72 17 861
SI-1230 Domzale            fax: +386 (0)1 72 17 888
Slovenia

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Sun Feb 27 01:57:34 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 27 Feb 2005 00:57:34 +0000 (UTC)
Subject: [R] Sweave and \input or \include LaTeX commands
References: <7FFEE688B57D7346BC6241C55900E730B6FF57@pollux.bfro.uni-lj.si>
Message-ID: <loom.20050227T015258-636@post.gmane.org>


If this were supported it should have a different command than the 
latex one or else one would not be able to do it at the latex level.
One might want to do it at the latex level rather than the Sweave
level in the case that one wants to have a vignette pass R CMD CHECK
but the vignette depends on software that is not generally available
or perhaps the vignette involves a large computation which would
otherwise impact every run of R CMD CHECK.  In these cases the portion
of the vignette that depends on the unavailable software or large
computation could be preprocessed using Sweave and then included
at the latex level to get by R CMD CHECK.

Gorjanc Gregor <Gregor.Gorjanc <at> bfro.uni-lj.si> writes:

: 
: Imagine this situation:
: 
: % --- a.Rnw start ---
: \documentclass{book}
: \usepackage{Sweave}
: \begin{document}
: % some toy example
: <<print=TRUE>>=
: x <- 1:10
:  <at> 
: % now we input additional file
: \input{a1}
: % and lets look again at x
: <<print=TRUE>>=
: x
:  <at> 
: \end{document}
: % --- a.Rnw end ---
: 
: % --- a1.Rnw start ---
: %\usepackage{Sweave}
: % add 1 to x
: <<print=TRUE>>=
: x <- x + 1
:  <at> 
: % --- a1.Rnw end ---
: 
: > Sweave("a.Rnw")
: Writing to file a.tex
: Processing code chunks ...
:  1 : echo print term verbatim
:  2 : echo print term verbatim
: 
: You can now run LaTeX on a.tex 
: 
: When you run Sweave on a.Rnw it founds only two chunks of R code. So it
: does not go into a1.Rnw i.e. it does not follow \input command. This 
: would be very usefull if one has such hierarchies in documents. Afcourse
: one can use Sweave on a.Rnw and a1.Rnw and then run LaTeX, but it would be
: nice if just one Sweave would do Sweave job for all files i.e. the same
: as LaTeX does. 
: 
: --
: Lep pozdrav / With regards,
:     Gregor GORJANC
: 
: ------------------------------------------------------------------------
: University of Ljubljana
: Biotechnical Faculty       URI: http://www.bfro.uni-lj.si/MR/ggorjan
: Zootechnical Department    email: gregor.gorjanc <at> bfro.uni-lj.si
: Groblje 3                  tel: +386 (0)1 72 17 861
: SI-1230 Domzale            fax: +386 (0)1 72 17 888
: Slovenia
: ------------------------------------------------------------------------
: 
: -----Original Message-----
: From: Mohamed Abdolell [mailto:m.abdolell <at> utoronto.ca]
: Sent: sob 2005-02-26 21:48
: To: Gorjanc Gregor; r-help <at> stat.math.ethz.ch
: Subject: RE: [R] Sweave and \input or \include LaTeX commands
: 
: I have used \input in my .Snw file and it works fine.
: 
: - Mohamed
: 
: -----Original Message-----
: From: r-help-bounces <at> stat.math.ethz.ch
: [mailto:r-help-bounces <at> stat.math.ethz.ch] On Behalf Of Gorjanc Gregor
: Sent: February 23, 2005 10:50 AM
: To: r-help <at> stat.math.ethz.ch
: Subject: [R] Sweave and \input or \include LaTeX commands
: 
: Hello!
: 
: I was just wondering if Sweave can work with \input or \include 
: LaTeX commands. So, is it aware of such a possible hierarchy in
: documents. I would test that, but I don't have such a report 
: available at the moment.
: 
: I thought of that when I was writting shell script for Sweave
: from command line and I have solved that part there.
: 
: --
: Lep pozdrav / With regards,
:     Gregor GORJANC
: 
: ---------------------------------------------------------------
: University of Ljubljana
: Biotechnical Faculty       URI: http://www.bfro.uni-lj.si
: Zootechnical Department    email: gregor.gorjanc <at> bfro.uni-lj.si
: Groblje 3                  tel: +386 (0)1 72 17 861
: SI-1230 Domzale            fax: +386 (0)1 72 17 888
: Slovenia
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://stat.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide!
: http://www.R-project.org/posting-guide.html
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://stat.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
: 
:



From spencer.graves at pdf.com  Sun Feb 27 05:28:08 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 26 Feb 2005 20:28:08 -0800
Subject: [R] POSIXct bug? 
Message-ID: <42214C58.3010407@pdf.com>

      In R 2.0.1 under Windows 2000, at least in some cases, as.POSIXct 
adds one to the date: 

 > March1.1959.POSIXct <- as.POSIXct("1959-03-01")
 > March1.1959.POSIXlt <- as.POSIXlt("1959-03-01")
 >
 > (Mar2.59 <- as.Date(March1.1959.POSIXct))
[1] "1959-03-02"
 > as.Date(March1.1959.POSIXlt)
[1] "1959-03-01"
 >
 > as.Date(as.POSIXct(Mar2.59))
[1] "1959-03-02"
 > as.Date(as.POSIXct(as.character(Mar2.59)))
[1] "1959-03-03"
 > print(POSIX.i <- as.POSIXct("1959-03-01"))
[1] "1959-03-01 Pacific Standard Time"
 > for(i in 1:11){
+   print(date.i <- as.Date(POSIX.i))
+   print(POSIX.i <- as.POSIXct(as.character(date.i)))
+ }
[1] "1959-03-02"
[1] "1959-03-02 Pacific Standard Time"
[1] "1959-03-03"
[1] "1959-03-03 Pacific Standard Time"
[1] "1959-03-04"
[1] "1959-03-04 Pacific Standard Time"
[1] "1959-03-05"
[1] "1959-03-05 Pacific Standard Time"
[1] "1959-03-06"
[1] "1959-03-06 Pacific Standard Time"
[1] "1959-03-07"
[1] "1959-03-07 Pacific Standard Time"
[1] "1959-03-08"
[1] "1959-03-08 Pacific Standard Time"
[1] "1959-03-09"
[1] "1959-03-09 Pacific Standard Time"
[1] "1959-03-10"
[1] "1959-03-10 Pacific Standard Time"
[1] "1959-03-11"
[1] "1959-03-11 Pacific Standard Time"
[1] "1959-03-12"
[1] "1959-03-12 Pacific Standard Time"
 >      
      Comments? 

      Thanks for your help. 
      spencer graves



From ggrothendieck at myway.com  Sun Feb 27 05:36:04 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 27 Feb 2005 04:36:04 +0000 (UTC)
Subject: [R] POSIXct bug?
References: <42214C58.3010407@pdf.com>
Message-ID: <loom.20050227T053210-95@post.gmane.org>

Spencer Graves <spencer.graves <at> pdf.com> writes:

: 
: In R 2.0.1 under Windows 2000, at least in some cases, as.POSIXct 
: adds one to the date: 
: 
:  > March1.1959.POSIXct <- as.POSIXct("1959-03-01")
:  > March1.1959.POSIXlt <- as.POSIXlt("1959-03-01")
:  >
:  > (Mar2.59 <- as.Date(March1.1959.POSIXct))
: [1] "1959-03-02"
:  > as.Date(March1.1959.POSIXlt)
: [1] "1959-03-01"
:  >
:  > as.Date(as.POSIXct(Mar2.59))
: [1] "1959-03-02"
:  > as.Date(as.POSIXct(as.character(Mar2.59)))
: [1] "1959-03-03"
:  > print(POSIX.i <- as.POSIXct("1959-03-01"))
: [1] "1959-03-01 Pacific Standard Time"
:  > for(i in 1:11){
: +   print(date.i <- as.Date(POSIX.i))
: +   print(POSIX.i <- as.POSIXct(as.character(date.i)))
: + }
: [1] "1959-03-02"
: [1] "1959-03-02 Pacific Standard Time"
: [1] "1959-03-03"
: [1] "1959-03-03 Pacific Standard Time"
: [1] "1959-03-04"
: [1] "1959-03-04 Pacific Standard Time"
: [1] "1959-03-05"
: [1] "1959-03-05 Pacific Standard Time"
: [1] "1959-03-06"
: [1] "1959-03-06 Pacific Standard Time"
: [1] "1959-03-07"
: [1] "1959-03-07 Pacific Standard Time"
: [1] "1959-03-08"
: [1] "1959-03-08 Pacific Standard Time"
: [1] "1959-03-09"
: [1] "1959-03-09 Pacific Standard Time"
: [1] "1959-03-10"
: [1] "1959-03-10 Pacific Standard Time"
: [1] "1959-03-11"
: [1] "1959-03-11 Pacific Standard Time"
: [1] "1959-03-12"
: [1] "1959-03-12 Pacific Standard Time"
:  >      
:       Comments?

I am not sure that the code really speaks for itself.  What is the bug?
Note that as.Date converts dates relative to GMT, not the current time
zone.  If you want to convert a POSIXct date to a Date date relative
to the current timezone you can convert it to character first.  RNews 4/1
has a table that provides a number of such idioms.



From gabriele.accetta at virgilio.it  Sun Feb 27 11:18:27 2005
From: gabriele.accetta at virgilio.it (gabriele.accetta@virgilio.it)
Date: Sun, 27 Feb 2005 11:18:27 +0100
Subject: [R] prediction, gam, mgcv
Message-ID: <4200064B00062259@ims1c.cp.tin.it>

I fitted a GAM model with Poisson distribution
using the function gam() in the mgcv package.

My model is of the form:
mod<-gam(y~s(x0)+s(x1)+s(x2),family=poisson).


To extract estimates at a specified set of covariate
values I used the gam `predict' method. 

But I want to get
estimate and standard error of the difference of two fitted values.

Can someone explain what should I do?

Thank you
gabriele



From Gregor.Gorjanc at bfro.uni-lj.si  Sun Feb 27 12:24:31 2005
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Sun, 27 Feb 2005 12:24:31 +0100
Subject: [R] Sweave and \input or \include LaTeX commands
Message-ID: <7FFEE688B57D7346BC6241C55900E730B6FF5D@pollux.bfro.uni-lj.si>

Gabor,

can you say this in some other way. I appologize, but I just don't 
understand anything. I really don't see any connection with R CMD CHECK
and vignettes.

I would just like to see that Sweave would follow \input of \include
commands of LaTeX. You can try with the following example bellow. I
don't know how Sweave is parsing Rnw files, but my idea was that if 
Sweave finds \input or \include in a "top" file (a.Rnw) it would also
parse "included" file (a1.Rnw) and then contninue with the top file
(a.Rnw). As I said just an idea. All this would be done at Sweave
level and one should run LaTeX only after Sweave handles all the files.

Thanks, Gregor

-----------------------------------------------------------------------
If this were supported it should have a different command than the
latex one or else one would not be able to do it at the latex level.
One might want to do it at the latex level rather than the Sweave
level in the case that one wants to have a vignette pass R CMD CHECK
but the vignette depends on software that is not generally available
or perhaps the vignette involves a large computation which would
otherwise impact every run of R CMD CHECK.  In these cases the portion
of the vignette that depends on the unavailable software or large
computation could be preprocessed using Sweave and then included
at the latex level to get by R CMD CHECK.

Gorjanc Gregor <Gregor.Gorjanc <at> bfro.uni-lj.si> writes:

:
: Imagine this situation:
:
: % --- a.Rnw start ---
: \documentclass{book}
: \usepackage{Sweave}
: \begin{document}
: % some toy example
: <<print=TRUE>>=
: x <- 1:10
:  <at>
: % now we input additional file
: \input{a1}
: % and lets look again at x
: <<print=TRUE>>=
: x
:  <at>
: \end{document}
: % --- a.Rnw end ---
:
: % --- a1.Rnw start ---
: %\usepackage{Sweave}
: % add 1 to x
: <<print=TRUE>>=
: x <- x + 1
:  <at>
: % --- a1.Rnw end ---
:
: > Sweave("a.Rnw")
: Writing to file a.tex
: Processing code chunks ...
:  1 : echo print term verbatim
:  2 : echo print term verbatim
:
: You can now run LaTeX on a.tex
:
: When you run Sweave on a.Rnw it founds only two chunks of R code. So it
: does not go into a1.Rnw i.e. it does not follow \input command. This
: would be very usefull if one has such hierarchies in documents. Afcourse
: one can use Sweave on a.Rnw and a1.Rnw and then run LaTeX, but it would be
: nice if just one Sweave would do Sweave job for all files i.e. the same
: as LaTeX does.

--
Lep pozdrav / With regards,
    Gregor GORJANC

------------------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty       URI: http://www.bfro.uni-lj.si/MR/ggorjan
Zootechnical Department    email: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                  tel: +386 (0)1 72 17 861
SI-1230 Domzale            fax: +386 (0)1 72 17 888
Slovenia



From ggrothendieck at myway.com  Sun Feb 27 13:32:44 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 27 Feb 2005 12:32:44 +0000 (UTC)
Subject: [R] Sweave and \input or \include LaTeX commands
References: <7FFEE688B57D7346BC6241C55900E730B6FF5D@pollux.bfro.uni-lj.si>
Message-ID: <loom.20050227T131157-739@post.gmane.org>

Gorjanc Gregor <Gregor.Gorjanc <at> bfro.uni-lj.si> writes:

: 
: Gabor,
: 
: can you say this in some other way. I appologize, but I just don't 
: understand anything. I really don't see any connection with R CMD CHECK
: and vignettes.

Vignettes are pdf documentation files written in Sweave which accompany some
packages.   The 'zoo' package is an example of one which has a vignette.
Try 
   vignette() 
to see a list of vignettes on your system or 
   library(zoo)
   vignette("zoo")
to see the zoo vignette.

A package must pass R CMD CHECK to be accepted on CRAN.  Also, even if
a package is not destined for CRAN its a good idea to make sure one's 
packages pass R CMD CHECK.   The availability of this tool is very helpful 
in improving the quality of one's software.  One of the
things R CMD CHECK does is to test the vignette build to ensure that there 
are no errors.  By doing the include at the latex, rather than the Sweave, 
level one has the possibility of preprocessing portions of the Sweave 
file before R CMD CHECK is run and that could be desirable in the 
two situations I mentioned.   A third reason is to provide upward
compatibility with the way it works now.

: 
: I would just like to see that Sweave would follow \input of \include
: commands of LaTeX. You can try with the following example bellow. I
: don't know how Sweave is parsing Rnw files, but my idea was that if 
: Sweave finds \input or \include in a "top" file (a.Rnw) it would also
: parse "included" file (a1.Rnw) and then contninue with the top file
: (a.Rnw). As I said just an idea. All this would be done at Sweave
: level and one should run LaTeX only after Sweave handles all the files.

Since this might not be desirable in all instances,
if Sweave were to have an include facility then it should
not be implemented in such a way that the latex include facility
can no longer be used.  The point was just that it should be possible 
to do the include at the Sweave or at the latex level.  

: 
: Thanks, Gregor
: 
: -----------------------------------------------------------------------
: If this were supported it should have a different command than the
: latex one or else one would not be able to do it at the latex level.
: One might want to do it at the latex level rather than the Sweave
: level in the case that one wants to have a vignette pass R CMD CHECK
: but the vignette depends on software that is not generally available
: or perhaps the vignette involves a large computation which would
: otherwise impact every run of R CMD CHECK.  In these cases the portion
: of the vignette that depends on the unavailable software or large
: computation could be preprocessed using Sweave and then included
: at the latex level to get by R CMD CHECK.
: 
: Gorjanc Gregor <Gregor.Gorjanc <at> bfro.uni-lj.si> writes:
: 
: :
: : Imagine this situation:
: :
: : % --- a.Rnw start ---
: : \documentclass{book}
: : \usepackage{Sweave}
: : \begin{document}
: : % some toy example
: : <<print=TRUE>>=
: : x <- 1:10
: :  <at>
: : % now we input additional file
: : \input{a1}
: : % and lets look again at x
: : <<print=TRUE>>=
: : x
: :  <at>
: : \end{document}
: : % --- a.Rnw end ---
: :
: : % --- a1.Rnw start ---
: : %\usepackage{Sweave}
: : % add 1 to x
: : <<print=TRUE>>=
: : x <- x + 1
: :  <at>
: : % --- a1.Rnw end ---
: :
: : > Sweave("a.Rnw")
: : Writing to file a.tex
: : Processing code chunks ...
: :  1 : echo print term verbatim
: :  2 : echo print term verbatim
: :
: : You can now run LaTeX on a.tex
: :
: : When you run Sweave on a.Rnw it founds only two chunks of R code. So it
: : does not go into a1.Rnw i.e. it does not follow \input command. This
: : would be very usefull if one has such hierarchies in documents. Afcourse
: : one can use Sweave on a.Rnw and a1.Rnw and then run LaTeX, but it would be
: : nice if just one Sweave would do Sweave job for all files i.e. the same
: : as LaTeX does.
: 
: --
: Lep pozdrav / With regards,
:     Gregor GORJANC
: 
: ------------------------------------------------------------------------
: University of Ljubljana
: Biotechnical Faculty       URI: http://www.bfro.uni-lj.si/MR/ggorjan
: Zootechnical Department    email: gregor.gorjanc <at> bfro.uni-lj.si
: Groblje 3                  tel: +386 (0)1 72 17 861
: SI-1230 Domzale            fax: +386 (0)1 72 17 888
: Slovenia
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://stat.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
: 
:



From charshaw at presby.edu  Sun Feb 27 15:08:36 2005
From: charshaw at presby.edu (Clint Harshaw)
Date: Sun, 27 Feb 2005 09:08:36 -0500
Subject: [R] subsetting data set dimenion problem
Message-ID: <4221D464.1020702@presby.edu>

(See DAAG book, p. 173, ex. 3)

I'm a new user of R, and I'm following the DAAG text. I want to create a 
subset of the races2000 data frame, but get errors because of a mismatch 
of values in some columns:

 > library(DAAG)
 > attach(races2000)
 > hills2000 <- races2000[races2000$type == 'hill']
Error in as.matrix.data.frame(x) : dim<- : dims [product 770] do not 
match the length of object [771]

However, if I follow the solution given, and remove redundant columns 1 
through 6 and column 11 (which I won't need, since I know they are going 
to have the same value), I don't get the error:

 > hills2000 <- races2000[races2000$type == 'hill', -c(1:6,11)]
 > hills2000
                        dist climb      time      timef
Tiso Carnethy          6.00  2500 0.7822222  0.9191667
[...]
Cornalees              5.50   800 0.6183333         NA
[...]

What is causing the error with my original subsetting? I speculated it 
was related to the NA values, but there is an NA in the resulting 
hills2000, corresponding to the Cornalees hill race.

Thanks,
Clint
-- 
Clint Harshaw, PhD		
Department of Mathematics
Presbyterian College
Clinton SC  29325

EMail: charshaw at presby.edu	
Phone: 864.833.8995
Fax: 864.938.3769
Office: Harrington-Peachtree Rm 412



From ccleland at optonline.net  Sun Feb 27 15:35:10 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Sun, 27 Feb 2005 09:35:10 -0500
Subject: [R] subsetting data set dimenion problem
In-Reply-To: <4221D464.1020702@presby.edu>
References: <4221D464.1020702@presby.edu>
Message-ID: <4221DA9E.50207@optonline.net>

Clint Harshaw wrote:
> (See DAAG book, p. 173, ex. 3)
> 
> I'm a new user of R, and I'm following the DAAG text. I want to create a 
> subset of the races2000 data frame, but get errors because of a mismatch 
> of values in some columns:
> 
>  > library(DAAG)
>  > attach(races2000)
>  > hills2000 <- races2000[races2000$type == 'hill']
> Error in as.matrix.data.frame(x) : dim<- : dims [product 770] do not 
> match the length of object [771]
> 
> However, if I follow the solution given, and remove redundant columns 1 
> through 6 and column 11 (which I won't need, since I know they are going 
> to have the same value), I don't get the error:
> 
>  > hills2000 <- races2000[races2000$type == 'hill', -c(1:6,11)]
>  > hills2000
>                        dist climb      time      timef
> Tiso Carnethy          6.00  2500 0.7822222  0.9191667
> [...]
> Cornalees              5.50   800 0.6183333         NA
> [...]
> 
> What is causing the error with my original subsetting? I speculated it 
> was related to the NA values, but there is an NA in the resulting 
> hills2000, corresponding to the Cornalees hill race.

   The error in the first subsetting is caused by a missing comma.  Try 
this:

races2000[races2000$type == 'hill',]

> Thanks,
> Clint

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From Friedrich.Leisch at tuwien.ac.at  Sun Feb 27 15:29:41 2005
From: Friedrich.Leisch at tuwien.ac.at (Friedrich.Leisch@tuwien.ac.at)
Date: Sun, 27 Feb 2005 15:29:41 +0100
Subject: [R] Sweave and \input or \include LaTeX commands
In-Reply-To: <loom.20050227T131157-739@post.gmane.org>
References: <7FFEE688B57D7346BC6241C55900E730B6FF5D@pollux.bfro.uni-lj.si>
	<loom.20050227T131157-739@post.gmane.org>
Message-ID: <16929.55637.291895.439328@celebrian.ci.tuwien.ac.at>

>>>>> On Sun, 27 Feb 2005 12:32:44 +0000 (UTC),
>>>>> Gabor Grothendieck (GG) wrote:

[...]

  > Since this might not be desirable in all instances,
  > if Sweave were to have an include facility then it should
  > not be implemented in such a way that the latex include facility
  > can no longer be used.  The point was just that it should be possible 
  > to do the include at the Sweave or at the latex level.  

I agree that it should not be the same command. I have put an
\SweaveInput{} on my 2do list, should be doable for R 2.1.0.

Best,
Fritz



From ligges at statistik.uni-dortmund.de  Sun Feb 27 16:07:59 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 27 Feb 2005 16:07:59 +0100
Subject: [R] Getting tick positions
In-Reply-To: <20050226195338.VXYR24950.smta03.mail.ozemail.net@there>
References: <20050226195338.VXYR24950.smta03.mail.ozemail.net@there>
Message-ID: <4221E24F.5080301@statistik.uni-dortmund.de>

Jim Lemon wrote:

> Thanks for the answers - I should have been more specific as I had already 
> tried axTicks and pretty.
> 
> The function in question is gantt.chart() in the latest plotrix package 
> (Thanks to Scott Waichler for the original code). I settled on axis.POSIXct 
> as it seemed the most appropriate for this function, but couldn't find a way 
> to get the positions of the ticks so that I could then draw grid lines along 
> the months|days|hours. While the trick of copying the original function 
> works, I wondered why the axis* functions don't return the tick positions as 
> barplot returns the bar positions. Is there any reason not to return "z", or 
> is it just historical?

I think historical.

Replacing
     return R_NilValue;
by
     return at;
in do_axis (plot.c) should do the trick (well, I'm a bit afraid 
concerning the UNPROTECT, you might need to do that later, after the 
recording stuff?).

Uwe Ligges


> Jim
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From schween at snafu.de  Sun Feb 27 16:07:39 2005
From: schween at snafu.de (Sven C. Koehler)
Date: Sun, 27 Feb 2005 16:07:39 +0100
Subject: [R] Finding Equal Elements in Vectors
Message-ID: <20050227150739.GB18940@boing.buug.de>

Hello!

I have two vectors and want to know how many of their elements are equal - 
what's the best way to do this in R?  

Best regards,

Sven



From spencer.graves at pdf.com  Sun Feb 27 16:15:44 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 27 Feb 2005 07:15:44 -0800
Subject: [R] POSIXct bug?
In-Reply-To: <loom.20050227T053210-95@post.gmane.org>
References: <42214C58.3010407@pdf.com> <loom.20050227T053210-95@post.gmane.org>
Message-ID: <4221E420.2030407@pdf.com>

Hi, Gabor: 

      Of course:  time zone vs. GMT. 

       Next question:  Might a simple example that illustrates this be 
added to the help file for "as.POSIXct", and if yes, what should be done 
to make that happen? 

      Thanks. 
      spencer graves

Gabor Grothendieck wrote:

>Spencer Graves <spencer.graves <at> pdf.com> writes:
>
>: 
>: In R 2.0.1 under Windows 2000, at least in some cases, as.POSIXct 
>: adds one to the date: 
>: 
>:  > March1.1959.POSIXct <- as.POSIXct("1959-03-01")
>:  > March1.1959.POSIXlt <- as.POSIXlt("1959-03-01")
>:  >
>:  > (Mar2.59 <- as.Date(March1.1959.POSIXct))
>: [1] "1959-03-02"
>:  > as.Date(March1.1959.POSIXlt)
>: [1] "1959-03-01"
>:  >
>:  > as.Date(as.POSIXct(Mar2.59))
>: [1] "1959-03-02"
>:  > as.Date(as.POSIXct(as.character(Mar2.59)))
>: [1] "1959-03-03"
>:  > print(POSIX.i <- as.POSIXct("1959-03-01"))
>: [1] "1959-03-01 Pacific Standard Time"
>:  > for(i in 1:11){
>: +   print(date.i <- as.Date(POSIX.i))
>: +   print(POSIX.i <- as.POSIXct(as.character(date.i)))
>: + }
>: [1] "1959-03-02"
>: [1] "1959-03-02 Pacific Standard Time"
>: [1] "1959-03-03"
>: [1] "1959-03-03 Pacific Standard Time"
>: [1] "1959-03-04"
>: [1] "1959-03-04 Pacific Standard Time"
>: [1] "1959-03-05"
>: [1] "1959-03-05 Pacific Standard Time"
>: [1] "1959-03-06"
>: [1] "1959-03-06 Pacific Standard Time"
>: [1] "1959-03-07"
>: [1] "1959-03-07 Pacific Standard Time"
>: [1] "1959-03-08"
>: [1] "1959-03-08 Pacific Standard Time"
>: [1] "1959-03-09"
>: [1] "1959-03-09 Pacific Standard Time"
>: [1] "1959-03-10"
>: [1] "1959-03-10 Pacific Standard Time"
>: [1] "1959-03-11"
>: [1] "1959-03-11 Pacific Standard Time"
>: [1] "1959-03-12"
>: [1] "1959-03-12 Pacific Standard Time"
>:  >      
>:       Comments?
>
>I am not sure that the code really speaks for itself.  What is the bug?
>Note that as.Date converts dates relative to GMT, not the current time
>zone.  If you want to convert a POSIXct date to a Date date relative
>to the current timezone you can convert it to character first.  RNews 4/1
>has a table that provides a number of such idioms.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From spencer.graves at pdf.com  Sun Feb 27 16:23:09 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 27 Feb 2005 07:23:09 -0800
Subject: [R] Finding Equal Elements in Vectors
In-Reply-To: <20050227150739.GB18940@boing.buug.de>
References: <20050227150739.GB18940@boing.buug.de>
Message-ID: <4221E5DD.5030406@pdf.com>

      It depends on what you mean by that.  Consider the following two 
examples: 

 > sum((1:5)==(5:1))
[1] 1
 >
 > sum((2:1) %in%(1:6))
[1] 2
 >

      Does one of these solve your problem? 
      spencer graves

Sven C. Koehler wrote:

>Hello!
>
>I have two vectors and want to know how many of their elements are equal - 
>what's the best way to do this in R?  
>
>Best regards,
>
>Sven
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From MSchwartz at MedAnalytics.com  Sun Feb 27 16:24:54 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Sun, 27 Feb 2005 09:24:54 -0600
Subject: [R] Finding Equal Elements in Vectors
In-Reply-To: <20050227150739.GB18940@boing.buug.de>
References: <20050227150739.GB18940@boing.buug.de>
Message-ID: <1109517894.9330.30.camel@horizons.localdomain>

On Sun, 2005-02-27 at 16:07 +0100, Sven C. Koehler wrote:
> Hello!
> 
> I have two vectors and want to know how many of their elements are
> equal - 
> what's the best way to do this in R?  
> 
> Best regards,
> 
> Sven

> a <- 1:10
> b <- 8:20

> a
 [1]  1  2  3  4  5  6  7  8  9 10
> b
 [1]  8  9 10 11 12 13 14 15 16 17 18 19 20


If you just want to know how many elements in 'a' are in 'b':

> sum(a %in% b)
[1] 3

If you want to know which elements in 'a' are in 'b':

> which(a %in% b)
[1]  8  9 10

See ?"%in%" and ?which for more information.

HTH,

Marc Schwartz



From MSchwartz at MedAnalytics.com  Sun Feb 27 16:50:07 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Sun, 27 Feb 2005 09:50:07 -0600
Subject: [R] Finding Equal Elements in Vectors
In-Reply-To: <1109517894.9330.30.camel@horizons.localdomain>
References: <20050227150739.GB18940@boing.buug.de>
	<1109517894.9330.30.camel@horizons.localdomain>
Message-ID: <1109519408.9330.42.camel@horizons.localdomain>

After I hit send, I realized an error in:

> which(a %in% b)
[1]  8  9 10

The above should be:

> a[which(a %in% b)]
[1]  8  9 10

They happen to be the same by coincidence only, given the values that I
used.

The first syntax returns the _indices_ of the matches, not the actual
matched values themselves. Important difference.

For example and clarity:

> a <- 5:15
> b <- 12:20

> a
 [1]  5  6  7  8  9 10 11 12 13 14 15
> b
[1] 12 13 14 15 16 17 18 19 20

> which(a %in% b)
[1]  8  9 10 11

> a[which(a %in% b)]
[1] 12 13 14 15

Sorry for the confusion.

Marc



From msvika at mscc.huji.ac.il  Sun Feb 27 17:21:59 2005
From: msvika at mscc.huji.ac.il (Vicky Landsman)
Date: Sun, 27 Feb 2005 18:21:59 +0200
Subject: [R] Help with  constrained optimization 
Message-ID: <000601c51ce8$76838b90$a200a8c0@HOME2>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050227/df9509ca/attachment.pl

From br44114 at yahoo.com  Sun Feb 27 17:28:18 2005
From: br44114 at yahoo.com (bogdan romocea)
Date: Sun, 27 Feb 2005 08:28:18 -0800 (PST)
Subject: [R] download files through secure http (HTTPS)
Message-ID: <20050227162818.34200.qmail@web50110.mail.yahoo.com>

Dear useRs,

I'm trying to download some data through the HTTPS protocol. However,
download.file() does not support HTTPS (R 2.0.1 on WinXP):
Error in download.file(https.url, destfile = "test.txt") : 
        unsupported URL scheme

1. Is there any other function/package in R that can work with HTTPS?
2. If not, what would need to happen to make download.file() support
HTTPS? 

Thank you,
b.



From MSchwartz at MedAnalytics.com  Sun Feb 27 17:38:32 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Sun, 27 Feb 2005 10:38:32 -0600
Subject: [R] Finding Equal Elements in Vectors
In-Reply-To: <1109519408.9330.42.camel@horizons.localdomain>
References: <20050227150739.GB18940@boing.buug.de>
	<1109517894.9330.30.camel@horizons.localdomain>
	<1109519408.9330.42.camel@horizons.localdomain>
Message-ID: <1109522312.9330.56.camel@horizons.localdomain>

On Sun, 2005-02-27 at 09:50 -0600, Marc Schwartz wrote:

[snip]

> > a[which(a %in% b)]
> [1] 12 13 14 15

One more time:

The above can be simplified to just:

> a[a %in% b]
[1] 12 13 14 15


Also, if you happen to be comparing floating point numbers in the two
vectors, consider that floating point comparisons are inexact due to the
way in which floats are represented in binary on computers.

For example:

> a <- seq(0, 1.5, 0.1)
> b <- seq(1.0, 1.5, 0.1)

> a
 [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 1.5
> b
[1] 1.0 1.1 1.2 1.3 1.4 1.5

> a[a %in% b]
[1] 1.0 1.1 1.3 1.5

Note that in the above, 1.2 and 1.4 are not returned as a result of
attempting an exact comparison of these two floating point numbers.

Using something like:

> a[round(a, 1) %in% round(b, 1)]
[1] 1.0 1.1 1.2 1.3 1.4 1.5

will work, since we are rounding the floats to one decimal place prior
to the comparison.


Marc
<Going for another cup of coffee...>



From spencer.graves at pdf.com  Sun Feb 27 17:48:31 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 27 Feb 2005 08:48:31 -0800
Subject: [R] Help with  constrained optimization
In-Reply-To: <000601c51ce8$76838b90$a200a8c0@HOME2>
References: <000601c51ce8$76838b90$a200a8c0@HOME2>
Message-ID: <4221F9DF.6040309@pdf.com>

      You say that the total number of parameters is approximately 30, 
but you identify only 4, alpha1, alpha2, beta1 and beta2.  I gather 
these are vectors and g is a scalar? 

       Have you considered making the constraint part of the objective 
function, so you minimize lambda*(g(alpha1)-g(alpha2))^2-f1-a*f2.  I'd 
then start with the values for alpha1, alpha2, beta1 and beta2 you got 
from the individual optimizations and with a small or modest value of 
the "pseudo-Lagrange multiplier", lambda.  After you find a solution, 
increase lambda until the constraint is sufficiently well satisfied. 

      You are using "optim"?  If I ran into trouble with this, I'd 
upgrade to R 2.0.1 and try again.  I don't know if "optim" has changed 
since R 1.9.1, but it's worth upgrading anyway.  Also, "optim" has 4 
unconstrained methods, and I've had good luck rotating between the 4 
methods, using the answer from one as starting values for the other.  
I've had the worse results with SANN, simulated annealing, I think 
because my problems were sufficiently well behaved that the other three 
methods tended to give reasonable results and because SANN spent lots of 
time hopping all over the place without finding a better local minimum. 

      Also, have you tried "www.r-project.org" -> Search -> "R site 
search" for "constrained optimization".  When I did this just now, I got 
87 hits, the third of which was me giving essentially this same comment 
last November. 

      hope this helps. 
      spencer graves
p.s.  You might also find useful the posting guide! 
"http://www.R-project.org/posting-guide.html", in case you haven't 
checked it. 

Vicky Landsman wrote:

>Dear all, 
>I need an advice in the following problem. 
>I have to maximize two functions of the form f1(x)=f(y1,x,alpha1,beta1) and f2(x)=(y2,x,alpha2,beta2), the maximization is with respect to alpha1, alpha2, beta1, beta2.  I can maximize each function separately using nlm. 
>The problem is that I have to add the constraint of the form g(alpha1)=g(alpha2). 
>The total number of parameters is approximately 30.  
>What is the best way to do it? 
>I am using R-1.9.1 on Windows XP. 
>Much thanks in advance for any suggestions, 
>Best regards, 
>Vicky Landsman.   
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From ripley at stats.ox.ac.uk  Sun Feb 27 18:13:19 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 27 Feb 2005 17:13:19 +0000 (GMT)
Subject: [R] download files through secure http (HTTPS)
In-Reply-To: <20050227162818.34200.qmail@web50110.mail.yahoo.com>
References: <20050227162818.34200.qmail@web50110.mail.yahoo.com>
Message-ID: <Pine.LNX.4.61.0502271706200.27592@gannet.stats>

On Sun, 27 Feb 2005, bogdan romocea wrote:

> I'm trying to download some data through the HTTPS protocol. However,
> download.file() does not support HTTPS (R 2.0.1 on WinXP):
> Error in download.file(https.url, destfile = "test.txt") :
>        unsupported URL scheme
>
> 1. Is there any other function/package in R that can work with HTTPS?
> 2. If not, what would need to happen to make download.file() support
> HTTPS?

It already does, via its other methods e.g. wget.  Please DO read the help 
page (which DOES tells you that the internal method is file:/ftp:/http: 
only).

Package Rcurl (on Omegahat only these days) can I believe support https:.

Patches to get the internal method to support https: will be considered, 
especially if they support both direct and IE-based code under Windows.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From schween at snafu.de  Sun Feb 27 18:16:04 2005
From: schween at snafu.de (Sven C. Koehler)
Date: Sun, 27 Feb 2005 18:16:04 +0100
Subject: [R] Finding Equal Elements in Vectors
In-Reply-To: <1109517894.9330.30.camel@horizons.localdomain>
References: <20050227150739.GB18940@boing.buug.de>
	<1109517894.9330.30.camel@horizons.localdomain>
Message-ID: <20050227171604.GD18940@boing.buug.de>

On Sun, Feb 27, 2005 at 09:24:54AM -0600, Marc Schwartz wrote:
> > which(a %in% b)
> [1]  8  9 10
> 
> See ?"%in%" and ?which for more information.

Thanks for all your replies.  This was what I actually wanted to use!

-S.



From spencer.graves at pdf.com  Sun Feb 27 18:58:30 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 27 Feb 2005 09:58:30 -0800
Subject: [R] Help with  constrained optimization
In-Reply-To: <000601c51ce8$76838b90$a200a8c0@HOME2>
References: <000601c51ce8$76838b90$a200a8c0@HOME2>
Message-ID: <42220A46.5060806@pdf.com>

	  1.  I have not used nlm, but I would think for many problems, it 
would be roughly equivalent to optim.

	  2.  The parameter "a" in the objective function I wrote was the 
relative weight between optimization of f1 and f2.  I would expect to 
get different answer with a = 0.5, 1, 2, though I could be mistaken. 
This simplest way is to fix "a" in your objective function, though you 
could also have it as an additional argument and pass it as part of the 
"..." argument in optim.  Venables and Ripley (2002) Modern Applied 
Statistics with S, 4th ed. (Springer) discuss optim further, and "..." 
is discussed in both Venables and Ripley (2002) and V&R (2000) S 
Programming (Springer).  Nonlinear optimization is discussed in many 
places.  The book I know the best on this is Bates and Watts (1988) 
Nonlinear Regression Analysis & Its Applications (Wiley).  This book 
includes an important discussion of intrinsic vs. parameter effects 
curvature, noting parameter effects are much bigger in virtually all the 
examples they examined.  That's good news, because it opens the 
possibilities for getting better convergence and better asymptotically 
normal approximations by changing transformations -- or by using 
asymptotic chi-square instead of asymptotic normality to determine 
confidence intervals.  This latter discussion may not apply to your 
immediate problem but is useful in many applications.

	  3.  If you continue to have problems eith nlm, I suggest you give 
optim another try, working through the examples in the help file until 
you think you understand how it works.
	
	  hope this helps.
	  spencer graves

###############################

Dear Spencer Graves,
Thank you very much for your prompt reply. I read your comment from 
November and tried it but smth goes bad in my programs. I thought that I 
did not understand correct your idea.
I am using nlm not optim, and usually I am satisfied with it.
Just, one question re your comment: what is "a" in the objective 
function you have written below?
Thanks again, Vicky.
PS: I have some problems with R-2.0.1 (under XP) - it crashes after I am 
trying to open a source file. I remember that there was a discussion on 
this problem in the list but I did not find time to solve this. Then, I 
have re-installed R-1.9.1 back.

###############################

      You say that the total number of parameters is approximately 30,
but you identify only 4, alpha1, alpha2, beta1 and beta2.  I gather
these are vectors and g is a scalar?

       Have you considered making the constraint part of the objective
function, so you minimize lambda*(g(alpha1)-g(alpha2))^2-f1-a*f2.  I'd
then start with the values for alpha1, alpha2, beta1 and beta2 you got
from the individual optimizations and with a small or modest value of
the "pseudo-Lagrange multiplier", lambda.  After you find a solution,
increase lambda until the constraint is sufficiently well satisfied.

      You are using "optim"?  If I ran into trouble with this, I'd
upgrade to R 2.0.1 and try again.  I don't know if "optim" has changed
since R 1.9.1, but it's worth upgrading anyway.  Also, "optim" has 4
unconstrained methods, and I've had good luck rotating between the 4
methods, using the answer from one as starting values for the other.
I've had the worse results with SANN, simulated annealing, I think
because my problems were sufficiently well behaved that the other three
methods tended to give reasonable results and because SANN spent lots of
time hopping all over the place without finding a better local minimum.

      Also, have you tried "www.r-project.org" -> Search -> "R site
search" for "constrained optimization".  When I did this just now, I got
87 hits, the third of which was me giving essentially this same comment
last November.

      hope this helps.
      spencer graves
p.s.  You might also find useful the posting guide!
"http://www.R-project.org/posting-guide.html", in case you haven't
checked it.

Vicky Landsman wrote:

>Dear all, 
>I need an advice in the following problem. 
>I have to maximize two functions of the form f1(x)=f(y1,x,alpha1,beta1) and f2(x)=(y2,x,alpha2,beta2), the maximization is with respect to alpha1, alpha2, beta1, beta2.  I can maximize each function separately using nlm. 
>The problem is that I have to add the constraint of the form g(alpha1)=g(alpha2). 
>The total number of parameters is approximately 30.  
>What is the best way to do it? 
>I am using R-1.9.1 on Windows XP. 
>Much thanks in advance for any suggestions, 
>Best regards, 
>Vicky Landsman.   
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From srini_iyyer_bio at yahoo.com  Mon Feb 28 00:02:12 2005
From: srini_iyyer_bio at yahoo.com (Srinivas Iyyer)
Date: Sun, 27 Feb 2005 15:02:12 -0800 (PST)
Subject: [R] Writing as matrix
In-Reply-To: <20050226195338.VXYR24950.smta03.mail.ozemail.net@there>
Message-ID: <20050227230213.32641.qmail@web53509.mail.yahoo.com>

Dear Group, I am calculating correlation coeff.
between two populations. After calculating the
cor.coefs I want to represent them as a matrix file. 

   a   b   c   d
A  1   0   1   1
B  1   1   1   1 
C  1   0   1   0
D  0   1   1   1 


could any one please help me how can i acheive this:

> dim(e1)
[1] 22283    60
> dim(e2)
[1] 22283   158
> sink("old_new.txt")
>round(cor(cbind(e1,e2),use="complete.obs"))
>sink()

Currently this is the way my result looksL
            a
A           1
B           1
C           1
D           0
            b
A           0
B           1
C           0
D           1




I want to see my result as:
   a   b   c   d
A  1   0   1   1
B  1   1   1   1 
C  1   0   1   0
D  0   1   1   1 


thankyou 
srini



From jlh599 at psu.edu  Mon Feb 28 00:42:46 2005
From: jlh599 at psu.edu (Jessica Higgs)
Date: Sun, 27 Feb 2005 18:42:46 -0500
Subject: [R] searching through a matrix
Message-ID: <5.2.0.9.2.20050227183007.02073b50@email.psu.edu>

I am trying to search through a matrix I have (x) to see if there are any 
missing values. This is what I have been using:

for (i in 1:3455)
    for (j in 1:24)
        if (is.na(x[i,j])) break;

i
j

When I run the program, it either freezes or stops and returns the last 
entry in the matrix (x[3455,24]). I need to figure out what values in my 
data matrix are missing because I am using prcomp and it keeps giving me an 
error starting that there are missing or infinite values in  x. My data 
matrix does have some values that are equal to zero but zero doesn't mean 
that the data is missing. it just indicates that that value is equivalent 
to zero. Any suggestions for how to fix this problem?



From spencer.graves at pdf.com  Mon Feb 28 00:53:47 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 27 Feb 2005 15:53:47 -0800
Subject: [R] searching through a matrix
In-Reply-To: <5.2.0.9.2.20050227183007.02073b50@email.psu.edu>
References: <5.2.0.9.2.20050227183007.02073b50@email.psu.edu>
Message-ID: <42225D8B.10708@pdf.com>

      Does the following solve your problem: 

 is.na(array(c(NA, 1:5), dim=c(2,3)))
      [,1]  [,2]  [,3]
[1,]  TRUE FALSE FALSE
[2,] FALSE FALSE FALSE

      spencer graves

Jessica Higgs wrote:

> I am trying to search through a matrix I have (x) to see if there are 
> any missing values. This is what I have been using:
>
> for (i in 1:3455)
>    for (j in 1:24)
>        if (is.na(x[i,j])) break;
>
> i
> j
>
> When I run the program, it either freezes or stops and returns the 
> last entry in the matrix (x[3455,24]). I need to figure out what 
> values in my data matrix are missing because I am using prcomp and it 
> keeps giving me an error starting that there are missing or infinite 
> values in  x. My data matrix does have some values that are equal to 
> zero but zero doesn't mean that the data is missing. it just indicates 
> that that value is equivalent to zero. Any suggestions for how to fix 
> this problem?
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Mon Feb 28 01:20:40 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 27 Feb 2005 16:20:40 -0800
Subject: [R] Writing as matrix
In-Reply-To: <20050227230213.32641.qmail@web53509.mail.yahoo.com>
References: <20050227230213.32641.qmail@web53509.mail.yahoo.com>
Message-ID: <422263D8.50603@pdf.com>

      What version of R under which operating system? 

      I just tried something similar and got something sensible, as 
follows: 

set.seed(1)
e1 <- array(1:6, dim=c(3,2),
     dimnames=list(NULL, LETTERS[1:2]))
e2 <- array(rnorm(6), dim=c(3,2),
     dimnames=list(NULL, letters[1:2]))
e12 <- cor(e1, e2)
e12. <- round(e12)

sink("e12.txt")
e12.
sink()

#####################
      File "e12.txt" contained the following: 

  a  b
A 0 -1
B 0 -1

      An alternative is the following: 

write.table(e12., "e12.csv", quote=FALSE)

      File "e12.csv" contained the same as *.txt. 

      Are your names actually "A", "B", etc., or do they include long 
character strings?  If the latter, that would explain what you got.  If 
the former, what do you get from "options()$width"?  I suspect it may be 
some small number. 

      hope this helps. 
      spencer graves
p.s.  Did you read the posting guide, 
"http://www.R-project.org/posting-guide.html"?  If you had followed that 
more carefully, especially regarding supplying a real, simple example 
with a few lines that would illustrate your problem, you would have 
gotten a quicker answer. 
p.p.s.  I found "options()$width" from "www.r-project.org" -> search -> 
"R site search" -> "output line width".  It was in the sixth hit. 

Srinivas Iyyer wrote:

>Dear Group, I am calculating correlation coeff.
>between two populations. After calculating the
>cor.coefs I want to represent them as a matrix file. 
>
>   a   b   c   d
>A  1   0   1   1
>B  1   1   1   1 
>C  1   0   1   0
>D  0   1   1   1 
>
>
>could any one please help me how can i acheive this:
>
>  
>
>>dim(e1)
>>    
>>
>[1] 22283    60
>  
>
>>dim(e2)
>>    
>>
>[1] 22283   158
>  
>
>>sink("old_new.txt")
>>round(cor(cbind(e1,e2),use="complete.obs"))
>>sink()
>>    
>>
>
>Currently this is the way my result looksL
>            a
>A           1
>B           1
>C           1
>D           0
>            b
>A           0
>B           1
>C           0
>D           1
>
>
>
>
>I want to see my result as:
>   a   b   c   d
>A  1   0   1   1
>B  1   1   1   1 
>C  1   0   1   0
>D  0   1   1   1 
>
>
>thankyou 
>srini
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From saurav at sas.upenn.edu  Mon Feb 28 05:32:44 2005
From: saurav at sas.upenn.edu (Saurav Pathak)
Date: Sun, 27 Feb 2005 23:32:44 -0500
Subject: [R] 3d scatterplots of more than 1 data set
Message-ID: <20050228043244.GA18629@mail1.sas.upenn.edu>

hi,

i am need to plot two or more sets of data in a 3d scatterplot,
each set with different color.

i tried Rcmdr, and the 3d scatterplot facility, based on rgl.  that
is what i need.  but i cannot seem to code different sets of data
differently.  any help will be very helpful.

i tried scatterplot3d, but it is difficult to get the right angle in
it.  i need to be able to rotate the axes, and export an eps file
from the right angle.

thanks in advance.
-- 
saurav



From maechler at stat.math.ethz.ch  Mon Feb 28 09:08:42 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 28 Feb 2005 09:08:42 +0100
Subject: [R] POSIXct bug?
In-Reply-To: <4221E420.2030407@pdf.com>
References: <42214C58.3010407@pdf.com> <loom.20050227T053210-95@post.gmane.org>
	<4221E420.2030407@pdf.com>
Message-ID: <16930.53642.355813.402856@stat.math.ethz.ch>

>>>>> "Spencer" == Spencer Graves <spencer.graves at pdf.com>
>>>>>     on Sun, 27 Feb 2005 07:15:44 -0800 writes:

    Spencer> Hi, Gabor: Of course: time zone vs. GMT.

    Spencer>        Next question: Might a simple example that
    Spencer> illustrates this be added to the help file for
    Spencer> "as.POSIXct", and if yes, what should be done to
    Spencer> make that happen?

send such a simple example {few lines of R code + comments ("# ...")}
to R-devel (or to an R-core member you know who would take it up).

Martin

    Spencer> Gabor Grothendieck wrote:

    >> Spencer Graves <spencer.graves <at> pdf.com> writes:
    >> 
    >> : 
    >> : In R 2.0.1 under Windows 2000, at least in some cases, as.POSIXct 
    >> : adds one to the date: 
    >> : 
    >> :  > March1.1959.POSIXct <- as.POSIXct("1959-03-01")
    >> :  > March1.1959.POSIXlt <- as.POSIXlt("1959-03-01")
    >> :  >
    >> :  > (Mar2.59 <- as.Date(March1.1959.POSIXct))
    >> : [1] "1959-03-02"
    >> :  > as.Date(March1.1959.POSIXlt)
    >> : [1] "1959-03-01"
    >> :  >
    >> :  > as.Date(as.POSIXct(Mar2.59))
    >> : [1] "1959-03-02"
    >> :  > as.Date(as.POSIXct(as.character(Mar2.59)))
    >> : [1] "1959-03-03"
    >> :  > print(POSIX.i <- as.POSIXct("1959-03-01"))
    >> : [1] "1959-03-01 Pacific Standard Time"
    >> :  > for(i in 1:11){
    >> : +   print(date.i <- as.Date(POSIX.i))
    >> : +   print(POSIX.i <- as.POSIXct(as.character(date.i)))
    >> : + }
    >> : [1] "1959-03-02"
    >> : [1] "1959-03-02 Pacific Standard Time"
    >> : [1] "1959-03-03"
    >> : [1] "1959-03-03 Pacific Standard Time"
    >> : [1] "1959-03-04"
    >> : [1] "1959-03-04 Pacific Standard Time"
    >> : [1] "1959-03-05"
    >> : [1] "1959-03-05 Pacific Standard Time"
    >> : [1] "1959-03-06"
    >> : [1] "1959-03-06 Pacific Standard Time"
    >> : [1] "1959-03-07"
    >> : [1] "1959-03-07 Pacific Standard Time"
    >> : [1] "1959-03-08"
    >> : [1] "1959-03-08 Pacific Standard Time"
    >> : [1] "1959-03-09"
    >> : [1] "1959-03-09 Pacific Standard Time"
    >> : [1] "1959-03-10"
    >> : [1] "1959-03-10 Pacific Standard Time"
    >> : [1] "1959-03-11"
    >> : [1] "1959-03-11 Pacific Standard Time"
    >> : [1] "1959-03-12"
    >> : [1] "1959-03-12 Pacific Standard Time"
    >> :  >      
    >> :       Comments?
    >> 
    >> I am not sure that the code really speaks for itself.  What is the bug?
    >> Note that as.Date converts dates relative to GMT, not the current time
    >> zone.  If you want to convert a POSIXct date to a Date date relative
    >> to the current timezone you can convert it to character first.  RNews 4/1
    >> has a table that provides a number of such idioms.



From WeiQiang.Li at seagate.com  Mon Feb 28 10:35:54 2005
From: WeiQiang.Li at seagate.com (WeiQiang.Li@seagate.com)
Date: Mon, 28 Feb 2005 17:35:54 +0800
Subject: [R] How to plot 
Message-ID: <OF9A098EDD.55D60231-ON48256FB6.00335591-48256FB6.0034CE5A@seagate.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050228/797ddd95/attachment.pl

From dimitris.rizopoulos at med.kuleuven.ac.be  Mon Feb 28 10:48:47 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Mon, 28 Feb 2005 10:48:47 +0100
Subject: [R] How to plot 
References: <OF9A098EDD.55D60231-ON48256FB6.00335591-48256FB6.0034CE5A@seagate.com>
Message-ID: <019401c51d7a$b32aa820$0540210a@www.domain>

you could use directly the plot function on an `lm' object, e.g.,

x <- runif(100, -3, 3)
y <- 1 + 2*x + rnorm(100)
m <- lm(y~x)
par(mfrow=c(2,2))
plot(m)

This doesn't produce the "I Chart" you wanted but it gives a very good 
picture about any strange behaviour of the residuals.

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: <WeiQiang.Li at seagate.com>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, February 28, 2005 10:35 AM
Subject: [R] How to plot


> Hello,
>
>        I would like to know if R provides the similar function to 
> produce
> the 4-in-1 graph ("Normal Plot of Residuals", "I Chart of 
> Residuals",
> "Histogram of Residuals" & "Residuals vs Fits") as MiniTab. If do 
> not
> have, could anyone please tell me how to produce above-mentioned
> individual chart?
>
>        Your help is greatly appreciated.
>
> Best Regards,
> WeiQiang Li
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Rau at demogr.mpg.de  Mon Feb 28 10:51:49 2005
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Mon, 28 Feb 2005 10:51:49 +0100
Subject: [R] averaging within columns
Message-ID: <8B08A3A1EA7AAC41BE24C750338754E651FFA2@HERMES.demogr.mpg.de>

Hi, 

> I'm hoping there is something like
> 
> mean(dataname$wtime[name])
> 
> which will just create a column with length equal to the number of 
> different names (levels) and an average wtime for each.  So 
> far though, 
> I haven't had much luck figuring that one out.
> 

Does the following code do what you want?

#### given you have the data like this:
name <- c(rep("jo", 4), rep("tim",3), rep("ro", 2))
wtime <- c(1,2,1,3,3,2,2,1,2)
mydf <- data.frame(name=name, wtime=wtime)

#### this should give you the desired result
tapply(X=mydf$wtime, INDEX=list(mydf$name), FUN=mean)


Hope this turns out to be useful.

Best,
Roland


+++++
This mail has been sent through the MPI for Demographic Rese...{{dropped}}



From petr.pikal at precheza.cz  Mon Feb 28 10:55:01 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 28 Feb 2005 10:55:01 +0100
Subject: [R] How to plot 
In-Reply-To: <OF9A098EDD.55D60231-ON48256FB6.00335591-48256FB6.0034CE5A@seagate.com>
Message-ID: <4222F885.19496.C358CA@localhost>



On 28 Feb 2005 at 17:35, WeiQiang.Li at seagate.com wrote:

> Hello,
> 
>         I would like to know if R provides the similar function to
>         produce 
> the 4-in-1 graph ("Normal Plot of Residuals", "I Chart of Residuals",
> "Histogram of Residuals" & "Residuals vs Fits") as MiniTab. If do not
> have, could anyone please tell me how to produce above-mentioned
> individual chart?

Hi

> x<-rnorm(10)
> y<-10*x+5+rnorm(10)
> plot(x,y)
> fit<-lm(y~x)
> par(mfrow=c(2,2))
> qqnorm(resid(fit))
> hist(resid(fit))
> plot(fitted(fit),resid(fit))
>

for three plots. You can try to construct I Chart yourself. Go 
through help page of lm and you can find some other options how to 
plot your lm result.

Cheers
Petr








> 
>         Your help is greatly appreciated.
> 
> Best Regards,
> WeiQiang Li
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From agmartinca at gruposantander.com  Mon Feb 28 10:57:09 2005
From: agmartinca at gruposantander.com (MARTIN CALMARZA AGUSTIN)
Date: Mon, 28 Feb 2005 10:57:09 +0100
Subject: [R] Unable to install packages
Message-ID: <17BE0E8C19D93E448FED29112825876A915779@VEXCHA0005.san.corp>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050228/6a16cbe4/attachment.pl

From mbh at norconserv.no  Mon Feb 28 11:16:43 2005
From: mbh at norconserv.no (Maria Befring Hovda)
Date: Mon, 28 Feb 2005 11:16:43 +0100
Subject: [R] Problems with downloading
Message-ID: <C95A8210236023489922B78053FC23EC10261A@exchange.ncas.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050228/86bac5fe/attachment.pl

From p.dalgaard at biostat.ku.dk  Mon Feb 28 11:24:38 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 28 Feb 2005 11:24:38 +0100
Subject: [R] Unable to install packages
In-Reply-To: <17BE0E8C19D93E448FED29112825876A915779@VEXCHA0005.san.corp>
References: <17BE0E8C19D93E448FED29112825876A915779@VEXCHA0005.san.corp>
Message-ID: <x21xb1hti1.fsf@biostat.ku.dk>

"MARTIN CALMARZA AGUSTIN" <agmartinca at gruposantander.com> writes:

> > Dear Sirs ,
> > Today I've downloaded the last release of the R-Program (I an a novel user). I am interested on performing time series analysys (regression,ARIMA,ARCH) to some data-sets, therefore I would like to use some of the packages of R-Contributors (like the tseries one).When I try to install or update packages from CRAN the following text always appear:
> > 
> >  local({a <- CRAN.packages()
> > + install.packages(select.list(a[,1],,TRUE), .libPaths()[1], available=a, dependencies=TRUE)})
> > trying URL `http://cran.stat.ucla.edu/bin/windows/contrib/2.0/PACKAGES'
> > Error in download.file(url = paste(contriburl, "PACKAGES", sep = "/"),  : 
> >         cannot open URL `http://cran.stat.ucla.edu/bin/windows/contrib/2.0/PACKAGES'
> > In addition: Warning message: 
> > unable to resolve 'cran.stat.ucla.edu'. 
> > 
> > > update.packages()
> > trying URL `http://cran.stat.ucla.edu/bin/windows/contrib/2.0/PACKAGES'
> > Error in download.file(url = paste(contriburl, "PACKAGES", sep = "/"),  : 
> >         cannot open URL `http://cran.stat.ucla.edu/bin/windows/contrib/2.0/PACKAGES'
> > In addition: Warning message: 
> > unable to resolve 'cran.stat.ucla.edu'. 
> > 
> > I have hanged the mirror several times but I am unable to download the packages.What should I do to install them?
> > Thanks very much.Best regards,
> > Agust?n Mart?n
> 

On Windows, I presume?

Did you read the Windows FAQ, Q.2.17?

In all cases, you could try to download the ZIP file using your
browser and then "install from local ZIP file". If you cannot download
that way either, well, the problem is not in R, then.

Notice that you need the Windows binary (*.zip) not the *.tar.gz or
*.tgz files, which contain source distributions and Macintosh files
respectively (unless I guessed your system incorrectly, of course...)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ligges at statistik.uni-dortmund.de  Mon Feb 28 11:56:20 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 28 Feb 2005 11:56:20 +0100
Subject: [R] Problems with downloading
In-Reply-To: <C95A8210236023489922B78053FC23EC10261A@exchange.ncas.local>
References: <C95A8210236023489922B78053FC23EC10261A@exchange.ncas.local>
Message-ID: <4222F8D4.2030705@statistik.uni-dortmund.de>

Maria Befring Hovda wrote:

> Hello
> I'm using your program R in a course I'm taking at the University of
> Oslo, and thereby I need to download it to my PC. Unfortunately I do
> have some problems, I do not know whish files to download and how I do
> it. Can you please send me a list of what to download and a like where
> to find it?

Either ask your supervisor or look at CRAN. You can compile from sources 
yourself, so get the sources from CRAN, or try to get a binary 
corresponding to your operating system. We do not know anything about 
your operating system ...
If my guess that you are working under Windows is correct, also Google 
(looking for "CRAN R Windows binary") points to the same diretory that 
one finds when navigating through CRAN.

Uwe Ligges


> Thanks a lot, best regards
>  
> Maria Befring Hovda
> PhD-student
>  
> Norconserv AS, Seafood Processing Research
> Niels Juelsgt. 50, Postbox 327, N-4002 Stavanger, Norway
>  
> Phone: (+47) 51844600, Fax: (+47) 51844651, mbh at norconserv.no
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ramasamy at cancer.org.uk  Mon Feb 28 12:09:56 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 28 Feb 2005 11:09:56 +0000
Subject: [R] Problems with downloading
In-Reply-To: <C95A8210236023489922B78053FC23EC10261A@exchange.ncas.local>
References: <C95A8210236023489922B78053FC23EC10261A@exchange.ncas.local>
Message-ID: <1109588996.6212.5.camel@ndmpc126.orc.ox.ac.uk>

I am assuming you are talking about a Windows machine. If so, reading
http://cran.r-project.org/bin/windows/base/rw-FAQ.html#Installation-and-
Usage

will point you the following windows installer

http://cran.r-project.org/bin/windows/base/rw2001.exe


However, you may need to ask your IT person to install it for you if you
do not have administrative privileges.

Regards, Adai


On Mon, 2005-02-28 at 11:16 +0100, Maria Befring Hovda wrote:
> Hello
> I'm using your program R in a course I'm taking at the University of
> Oslo, and thereby I need to download it to my PC. Unfortunately I do
> have some problems, I do not know whish files to download and how I do
> it. Can you please send me a list of what to download and a like where
> to find it?
>  
> Thanks a lot, best regards
>  
> Maria Befring Hovda
> PhD-student
>  
> Norconserv AS, Seafood Processing Research
> Niels Juelsgt. 50, Postbox 327, N-4002 Stavanger, Norway
>  
> Phone: (+47) 51844600, Fax: (+47) 51844651, mbh at norconserv.no
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From stp02fm at sheffield.ac.uk  Mon Feb 28 12:11:45 2005
From: stp02fm at sheffield.ac.uk (F Monadjemi)
Date: Mon, 28 Feb 2005 11:11:45 +0000
Subject: [R] Four parameter Lognormal Dis.
Message-ID: <1109589105.4222fc7182179@webmail.shef.ac.uk>

Dear Sir,

I want to generate a random observations from four parameter lognormal
distribution. Would you please tell me how I can do that?

Thanks

Farinaz



From ligges at statistik.uni-dortmund.de  Mon Feb 28 13:03:32 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 28 Feb 2005 13:03:32 +0100
Subject: [R] Problems with downloading
In-Reply-To: <1109588996.6212.5.camel@ndmpc126.orc.ox.ac.uk>
References: <C95A8210236023489922B78053FC23EC10261A@exchange.ncas.local>
	<1109588996.6212.5.camel@ndmpc126.orc.ox.ac.uk>
Message-ID: <42230894.1020404@statistik.uni-dortmund.de>

Adaikalavan Ramasamy wrote:

> I am assuming you are talking about a Windows machine. If so, reading
> http://cran.r-project.org/bin/windows/base/rw-FAQ.html#Installation-and-
> Usage
> 
> will point you the following windows installer
> 
> http://cran.r-project.org/bin/windows/base/rw2001.exe
> 
> 
> However, you may need to ask your IT person to install it for you if you
> do not have administrative privileges.

No, administrative privileges are NOT required.

Uwe Ligges


> 
> Regards, Adai
> 
> 
> On Mon, 2005-02-28 at 11:16 +0100, Maria Befring Hovda wrote:
> 
>>Hello
>>I'm using your program R in a course I'm taking at the University of
>>Oslo, and thereby I need to download it to my PC. Unfortunately I do
>>have some problems, I do not know whish files to download and how I do
>>it. Can you please send me a list of what to download and a like where
>>to find it?
>> 
>>Thanks a lot, best regards
>> 
>>Maria Befring Hovda
>>PhD-student
>> 
>>Norconserv AS, Seafood Processing Research
>>Niels Juelsgt. 50, Postbox 327, N-4002 Stavanger, Norway
>> 
>>Phone: (+47) 51844600, Fax: (+47) 51844651, mbh at norconserv.no
>> 
>>
>>	[[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Tristan.Lefebure at univ-lyon1.fr  Mon Feb 28 14:07:10 2005
From: Tristan.Lefebure at univ-lyon1.fr (Lefebure Tristan)
Date: Mon, 28 Feb 2005 14:07:10 +0100
Subject: [R] persistance of factor levels in a data frame
Message-ID: <200502281407.10996.lefebure@univ-lyon1.fr>

Hi,
Just something I don't understand:

data <- data.frame(V1=c(1:12),F1=c(rep("a",4),rep("b",4),rep("c",4)))
data_ac <- data[which(data$F1 !="b"), ]  
levels(data_ac$F1)    

Why the level "b" is always present ?

thanks

Tristan, R 2.0.1 for Linux Fedora 3

-- 
------------------------------------------------------------
Tristan LEFEBURE
Laboratoire d'?cologie des hydrosyst?mes fluviaux (UMR 5023)
Universit? Lyon I - Campus de la Doua
Bat. Darwin C 69622 Villeurbanne - France

Phone: (33) (0)4 26 23 44 02
Fax: (33) (0)4 72 43 15 23



From dimitris.rizopoulos at med.kuleuven.ac.be  Mon Feb 28 14:27:10 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Mon, 28 Feb 2005 14:27:10 +0100
Subject: [R] persistance of factor levels in a data frame
References: <200502281407.10996.lefebure@univ-lyon1.fr>
Message-ID: <022b01c51d99$34adbbd0$0540210a@www.domain>

look at ?"[.data.frame" and also check this:

dat <- data.frame(V1=c(1:12), F1=rep(letters[1:3], each=4))
dat.ac <- dat[dat$F1 !="b", ]
###############
dat.ac$F1
dat.ac$F1[, drop=TRUE]
###############
dat.ac$F1 <- dat.ac$F1[, drop=TRUE]
levels(dat.ac$F1)

I hope it helps.

best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Lefebure Tristan" <Tristan.Lefebure at univ-lyon1.fr>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, February 28, 2005 2:07 PM
Subject: [R] persistance of factor levels in a data frame


> Hi,
> Just something I don't understand:
>
> data <- 
> data.frame(V1=c(1:12),F1=c(rep("a",4),rep("b",4),rep("c",4)))
> data_ac <- data[which(data$F1 !="b"), ]
> levels(data_ac$F1)
>
> Why the level "b" is always present ?
>
> thanks
>
> Tristan, R 2.0.1 for Linux Fedora 3
>
> -- 
> ------------------------------------------------------------
> Tristan LEFEBURE
> Laboratoire d'?cologie des hydrosyst?mes fluviaux (UMR 5023)
> Universit? Lyon I - Campus de la Doua
> Bat. Darwin C 69622 Villeurbanne - France
>
> Phone: (33) (0)4 26 23 44 02
> Fax: (33) (0)4 72 43 15 23
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From p.dalgaard at biostat.ku.dk  Mon Feb 28 14:21:07 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 28 Feb 2005 14:21:07 +0100
Subject: [R] persistance of factor levels in a data frame
In-Reply-To: <200502281407.10996.lefebure@univ-lyon1.fr>
References: <200502281407.10996.lefebure@univ-lyon1.fr>
Message-ID: <x2sm3ghlbw.fsf@biostat.ku.dk>

Lefebure Tristan <Tristan.Lefebure at univ-lyon1.fr> writes:

> Hi,
> Just something I don't understand:
> 
> data <- data.frame(V1=c(1:12),F1=c(rep("a",4),rep("b",4),rep("c",4)))
> data_ac <- data[which(data$F1 !="b"), ]  
> levels(data_ac$F1)    
> 
> Why the level "b" is always present ?

Because it is a property of the definition, not of the data. E.g. if
you tabulate it, you generally want to get a zero entry if there are
no "b"s in the data. If, for some reason, you want to reduce the
factor to only those levels that are present, factor() gets you there
soon enough:

>  levels(factor(data_ac$F1))
[1] "a" "c"


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From bates at stat.wisc.edu  Mon Feb 28 14:31:39 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 28 Feb 2005 07:31:39 -0600
Subject: [R] persistance of factor levels in a data frame
In-Reply-To: <200502281407.10996.lefebure@univ-lyon1.fr>
References: <200502281407.10996.lefebure@univ-lyon1.fr>
Message-ID: <42231D3B.7010601@stat.wisc.edu>

Lefebure Tristan wrote:
> Hi,
> Just something I don't understand:
> 
> data <- data.frame(V1=c(1:12),F1=c(rep("a",4),rep("b",4),rep("c",4)))
> data_ac <- data[which(data$F1 !="b"), ]  
> levels(data_ac$F1)    
> 
> Why the level "b" is always present ?
> 
> thanks
> 
> Tristan, R 2.0.1 for Linux Fedora 3
> 

You must explicitly drop unused levels of a factor created by subsetting.

 > levels(data_ac$F1[drop = TRUE])
[1] "a" "c"



From jfox at mcmaster.ca  Mon Feb 28 14:35:04 2005
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 28 Feb 2005 08:35:04 -0500
Subject: [R] 3d scatterplots of more than 1 data set
In-Reply-To: <20050228043244.GA18629@mail1.sas.upenn.edu>
Message-ID: <20050228133505.XWVS1919.tomts22-srv.bellnexxia.net@JohnDesktop8300>

Dear Saurav,

The scatter3d() function in the Rcmdr package will plot by groups, using
different colours for the groups. The function can be used directly or
called via the Rcmdr menus. scatter3d() appears to do what you want, but in
any event is a straightforward function, and you should be able to alter it,
or simply add to the plot using, e.g., rgl.spheres() from the rgl package.

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Saurav Pathak
> Sent: Sunday, February 27, 2005 11:33 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] 3d scatterplots of more than 1 data set
> 
> hi,
> 
> i am need to plot two or more sets of data in a 3d 
> scatterplot, each set with different color.
> 
> i tried Rcmdr, and the 3d scatterplot facility, based on rgl. 
>  that is what i need.  but i cannot seem to code different 
> sets of data differently.  any help will be very helpful.
> 
> i tried scatterplot3d, but it is difficult to get the right 
> angle in it.  i need to be able to rotate the axes, and 
> export an eps file from the right angle.
> 
> thanks in advance.
> --
> saurav
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From petr.pikal at precheza.cz  Mon Feb 28 14:35:28 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 28 Feb 2005 14:35:28 +0100
Subject: [R] persistance of factor levels in a data frame
In-Reply-To: <200502281407.10996.lefebure@univ-lyon1.fr>
Message-ID: <42232C30.6874.18D2DBC@localhost>



On 28 Feb 2005 at 14:07, Lefebure Tristan wrote:

> Hi,
> Just something I don't understand:
> 
> data <- data.frame(V1=c(1:12),F1=c(rep("a",4),rep("b",4),rep("c",4)))
> data_ac <- data[which(data$F1 !="b"), ]  levels(data_ac$F1)    
> 
> Why the level "b" is always present ?

H Tristan

from ?"[.factor"

Extract or Replace Parts of a Factor

Description:

     Extract or replace subsets of factors.

Usage:

     x[i, drop = FALSE]

     x[i] <- value

Arguments:

       x: a factor

       i: a specification of indices - see 'Extract'.

    drop: logical.  If true, unused levels are dropped.
***************************************
default is FALSE so unused levels are retained.

factor(data_ac$F1)

gives you the same factor with only existing levels.

Cheers
Petr


> 
> thanks
> 
> Tristan, R 2.0.1 for Linux Fedora 3
> 
> -- 
> ------------------------------------------------------------
> Tristan LEFEBURE
> Laboratoire d'?cologie des hydrosyst?mes fluviaux (UMR 5023)
> Universit? Lyon I - Campus de la Doua
> Bat. Darwin C 69622 Villeurbanne - France
> 
> Phone: (33) (0)4 26 23 44 02
> Fax: (33) (0)4 72 43 15 23
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From MSchwartz at MedAnalytics.com  Mon Feb 28 14:40:34 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Mon, 28 Feb 2005 07:40:34 -0600
Subject: [R] persistance of factor levels in a data frame
In-Reply-To: <200502281407.10996.lefebure@univ-lyon1.fr>
References: <200502281407.10996.lefebure@univ-lyon1.fr>
Message-ID: <1109598034.9330.77.camel@horizons.localdomain>

On Mon, 2005-02-28 at 14:07 +0100, Lefebure Tristan wrote:
> Hi,
> Just something I don't understand:
> 
> data <- data.frame(V1=c(1:12),F1=c(rep("a",4),rep("b",4),rep("c",4)))
> data_ac <- data[which(data$F1 !="b"), ]  
> levels(data_ac$F1)    
> 
> Why the level "b" is always present ?
> 
> thanks
> 
> Tristan, R 2.0.1 for Linux Fedora 3

See ?"[.factor" for details. You will note that the argument 'drop' is
FALSE by default, which means that unused levels of a factor are not
dropped when subsetting.

This can be important if you might want to join or compare factors from
more than one source, where you want to ensure that the factor levels
are the same. If you were to drop the unused levels in one factor, but
it is present in the other, the comparison would be problematic, since
the levels for the same values in the two factors would be different.

If you want to force the unused levels to be dropped before using a
factor, just use:

> data_ac$F1 <- factor(data_ac$F1)

> data_ac$F1
[1] a a a a c c c c
Levels: a c

See ?factor for more information.

HTH,

Marc Schwartz



From tobias.sing at mpi-sb.mpg.de  Mon Feb 28 15:03:50 2005
From: tobias.sing at mpi-sb.mpg.de (Tobias Sing)
Date: Mon, 28 Feb 2005 15:03:50 +0100
Subject: [R] [R-pkgs] New package: ROCR (Visualizing classifier performance)
Message-ID: <200502281503.50641.tobias.sing@mpi-sb.mpg.de>

Dear R users,

we are glad to announce the release of our new R package ROCR, for visualizing 
the performance of scoring classifiers (available on CRAN). We hope that the 
package might be useful for those of you working on classification problems. 
For details, see the package description below, or the ROCR website: 
http://rocr.bioinf.mpi-sb.mpg.de. You can get a short overview by typing 
'demo(ROCR)'. Any kind of feedback (questions, comments, suggestions, bug 
reports) is very welcome.

Best regards,
  the ROCRs (Tobias Sing, Oliver Sander, Niko Beerenwinkel, Thomas Lengauer)

Package description:
-------------------------
ROC graphs, sensitivity/specificity curves, lift charts, and precision/recall 
plots are popular examples of trade-off visualizations for specific pairs of 
performance measures. ROCR is a flexible tool for creating 
cutoff-parametrized 2D performance curves by freely combining two from over 
25 performance measures (new performance measures can be added using a 
standard interface). Curves from different cross-validation or bootstrapping 
runs can be averaged by different methods, and standard deviations, standard 
errors or box plots can be used to visualize the variability across the runs. 
The parametrization can be visualized by printing cutoff values at the 
corresponding curve positions, or by coloring the curve according to cutoff. 
All components of a performance plot can be quickly adjusted using a flexible 
parameter dispatching mechanism. Despite its flexibility, ROCR is easy to 
use, with only three commands and reasonable default values for all optional 
parameters.


__________________________________________________________________________
Tobias Sing                         phone: +49 681 9325 315   
Max-Planck-Institut f?r Informatik  fax  : +49 681 9325 399   
Stuhlsatzenhausweg 85               email: tobias.sing at mpi-sb.mpg.de  
66123 Saarbr?cken, Germany          web  : http://www.tobiassing.net

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From krcabrer at unalmed.edu.co  Mon Feb 28 15:18:34 2005
From: krcabrer at unalmed.edu.co (Kenneth Cabrera)
Date: Mon, 28 Feb 2005 09:18:34 -0500
Subject: [R] Legal issues
In-Reply-To: <x2sm3ghlbw.fsf@biostat.ku.dk>
References: <200502281407.10996.lefebure@univ-lyon1.fr>
	<x2sm3ghlbw.fsf@biostat.ku.dk>
Message-ID: <opsmwvc8euxlluvi@kenneth>

Hi R Users:

I have some legal questions about R development.
R is under GNU Licence, version 2.

How the core team deal with their employer (most of the cases
it Universities, I don't know if public or private) about
their contribution to R development?

Do you have to make all the work outside the institution?

Is it part of the work at the institution, the R development?

Do the institution give resources to the team, for example,
the computer server where R is allocated?

Thank you very much for your help!

-- 
Kenneth Roy Cabrera Torres
Universidad Nacional de Colombia
Sede Medell?n
Cel 315 504 9339



From sdavis2 at mail.nih.gov  Mon Feb 28 15:10:23 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Mon, 28 Feb 2005 09:10:23 -0500
Subject: [R] tkRplot under macos x
Message-ID: <00c601c51d9f$fdf318e0$1f6df345@WATSON>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050228/1cbe716d/attachment.pl

From simon at stats.gla.ac.uk  Mon Feb 28 15:45:40 2005
From: simon at stats.gla.ac.uk (Simon Wood)
Date: Mon, 28 Feb 2005 14:45:40 +0000 (GMT)
Subject: [R] prediction, gam, mgcv
In-Reply-To: <4200064B00062259@ims1c.cp.tin.it>
References: <4200064B00062259@ims1c.cp.tin.it>
Message-ID: <Pine.LNX.4.58.0502281442530.13403@moon.stats.gla.ac.uk>

> My model is of the form:
> mod<-gam(y~s(x0)+s(x1)+s(x2),family=poisson).

> But I want to get
> estimate and standard error of the difference of two fitted values.
> 

The following code shows you how to do this (a) for differences on the 
scale of the linear predictor, and (b) for differences on the scale of the 
response. (Could be made more efficient at the cost of being a bit less 
readable)

best,
Simon


## simulating data
n <- 300
x <- runif(n)
f <- 0.2*x^11*(10*(1-x))^6+10*(10*x)^3*(1-x)^10
f <- exp(f/5)
y <- rpois(f,f)

## fit gam
mod <- gam(y~s(x),family=poisson)

## creat prediction data frame
pd <- data.frame(x=c(.5,.6))

## create matrix, Xp, mapping parameters to linear predictor
Xp <- predict(mod,pd,type="lpmatrix")

### First working on scale of linear predictor, repeatedly using
### fact that if X=CY where C is a matrix of coefficients and
### X and Y are random vectors, then cov(X) = C cov(Y) C'

## extract model coefficients and their covariance matrix
b <- coef(mod)
Vb <- mod$Vp

## obtain predictions and their covariance matrix
pred <- Xp%*%b
Vpred <- Xp%*%Vb%*%t(Xp)

## find difference and associated standard deviation
diff <- c(1,-1)
diff.pred <- t(diff)%*%pred
diff.sd <- sqrt(t(diff)%*%Vpred%*%diff)

diff.pred;diff.sd

### Now working on response scale, by simulation

library(MASS)
## simulate 1000 rep. param. vectors from posterior distn...
br <- mvrnorm(n=1000,b,Vb)
diff.pred <- rep(0,1000) 
for (i in 1:1000) {     ## for each rep
  pred <- Xp%*%br[i,]   ## get l.p. predictions
  ## and hence calculate the required response scale difference
  diff.pred[i] <- exp(pred[1])-exp(pred[2])
}
## diff.pred now contains 1000 rep. differences, so can easily estimated 
## their standard deviation....
sd(diff.pred) 
## and here is the point estimate ... 
pred <- Xp%*%b
exp(pred[1])-exp(pred[2])


_____________________________________________________________________
> Simon Wood simon at stats.gla.ac.uk        www.stats.gla.ac.uk/~simon/
>>  Department of Statistics, University of Glasgow, Glasgow, G12 8QQ
>>>   Direct telephone: (0)141 330 4530          Fax: (0)141 330 4814



From fengchen at hkusua.hku.hk  Mon Feb 28 16:03:28 2005
From: fengchen at hkusua.hku.hk (Feng Chen)
Date: Mon, 28 Feb 2005 23:03:28 +0800
Subject: [R] A problem about outer()
Message-ID: <002c01c51da6$abd33750$77d40893@S119>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050228/fdbf7169/attachment.pl

From vincent.goulet at act.ulaval.ca  Mon Feb 28 16:04:14 2005
From: vincent.goulet at act.ulaval.ca (Vincent Goulet)
Date: Mon, 28 Feb 2005 10:04:14 -0500
Subject: [R] and [ESS] Starting ESS
In-Reply-To: <422003D2.8050308@columbia.edu>
References: <BAY10-F53C55C4C4F154B31A1829DD6630@phx.gbl>
	<200502241542.58190.vincent.goulet@act.ulaval.ca>
	<422003D2.8050308@columbia.edu>
Message-ID: <200502281004.14909.vincent.goulet@act.ulaval.ca>

Suresh,

Did you define a HOME environment variable for Emacs to use (as instructed in 
the installation instructions)? I observed that when the HOME variable is not 
defined (or points to a non-existent directory), Emacs will freeze as soon as 
one toggles from the Emacs window to another application and back to Emacs.

And of course, I should have written FSF Emacs in my original post, not FSG.

Hope this helps!

Le 26 F?vrier 2005 00:06, Suresh Krishna a ?crit :
> Prof. Goulet,
>
> I apologize in advance if I shouldnt have been writing to you directly;
> but I thought that my problem was simple enough that you could very
> quickly say if you knew of a quick fix for this.
>
> Because I am familiar with the Emacs editor, I was happily and
> gratefully using your packaged version of FSG Emacs after you suggested
> it here on the list (R-help); but I have run into a problem that occurs
>
> when i run:
>  >x=1;
>  >y=1;
>  >par(ask=T)
>  >plot(x,y)
>
> emacs/ess/r now freezes because it is waiting for me to hit enter to see
> the next plot, however, when i hit enter, i am not sure R is able to
> realize that i hit enter. also, the "hit enter to see next plot" message
> is not displayed in the window with the r process.
>
> If this is a problem that is somehow specific to what I am doing, then
> please let me know. I have switched back to JGR and the mouse, until then.
>
> Thanks !!
>
> Suresh
>
> Vincent Goulet wrote:
> > To answer your question: I don't think it matters.
> >
> > If you're willing to use FSG Emacs instead of XEmacs, I repackaged Emacs
> > 21.3 for Windows together with recent versions of ESS and AuCTeX for my
> > students (I personally run Linux). The package and installation
> > instructions are at:
> >
> >  http://vgoulet.act.ulaval.ca/download/software/emacs/
> >
> > (Look for files emacs-21.3-ready.zip, emacs_install_en.txt and perhaps
> > ess_s-plus_win_en.txt.)
> >
> > Hope this helps!
> >
> > Le 23 F?vrier 2005 18:16, Laura Holt a ?crit :
> >>Dear R People:
> >>
> >>I have finally seen the error of my ways and have decided to use ESS for
> >> R and S + stuff.
> >>
> >>However, I have a question right from the beginning.  I'm somewhat
> >> confused by the installation instructions.
> >>
> >>Do I install XEMACS or ESS first, please?
> >>
> >>Windows XP
> >>R Version 2.0.1
> >>(S + 6.2)
> >>
> >>Thanks so much!
> >>
> >>Sincerely,
> >>Laura Holt
> >>mailto: lauraholt_983 at hotmail.com
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide!
> >>http://www.R-project.org/posting-guide.html

-- 
  Vincent Goulet, Associate Professor
  ?cole d'actuariat
  Universit? Laval, Qu?bec 
  Vincent.Goulet at act.ulaval.ca   http://vgoulet.act.ulaval.ca



From tlumley at u.washington.edu  Mon Feb 28 17:00:48 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 28 Feb 2005 08:00:48 -0800 (PST)
Subject: [R] A problem about outer()
In-Reply-To: <002c01c51da6$abd33750$77d40893@S119>
References: <002c01c51da6$abd33750$77d40893@S119>
Message-ID: <Pine.A41.4.61b.0502280800170.35300@homer04.u.washington.edu>

On Mon, 28 Feb 2005, Feng Chen wrote:

> Dear all,
>
> I have something about function outer() that I can't understand. Just 
> see the following example. The two NaNs are due to 0/0, but I can't 
> figure out the cause of the last two errors. I wonder if some one can 
> explain this for me.

Look at FAQ 7.18.

 	-thomas

> ___________________________________________________________________
>> sx=rbinom(10,1,0.5);ot=rbinom(10,1,0.5);ag <- rbinom(10,100,0.3);ho <- rbinom(10,100,0.5)
>> dp <- function(s,a,h)sum((sx==s)&(ag==a)&(ho==h)&(ot==1))/sum((sx==s)&(ag==a)&(ho==h))
>> (function(x,y)dp(1,x,y))(2,3)
> [1] NaN
>> (function(x,y)dp(0,x,y))(27,52)
> [1] NaN
>> dpm <- outer(ag,ho,function(x,y)dp(1,x,y))
> Error in outer(ag, ho, function(x, y) dp(1, x, y)) :
> dim<- : dims [product 100] do not match the length of object [1]
>> dpf <- outer(ag,ho,function(x,y)dp(0,x,y))
> Error in outer(ag, ho, function(x, y) dp(0, x, y)) :
> dim<- : dims [product 100] do not match the length of object [1]
>>
> -------------------------------------------------------------------------------------------------------------------
>
> Thanks very much,
> Feng
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From sczhang at mail.ustc.edu.cn  Mon Feb 28 17:17:49 2005
From: sczhang at mail.ustc.edu.cn (sczhang@mail.ustc.edu.cn)
Date: Tue, 1 Mar 2005 00:17:49 +0800 (CST)
Subject: [R] Ask for your help
Message-ID: <16411.61.190.45.80.1109607469.squirrel@webmail.ustc.edu.cn>


Dear Sir(or madam),
I got following error information when I run the haplo.glm program.Could
you tell me which wrong was happened in my program?Many thanks. Shanchun

fit.gaus <- haplo.glm(y ~ SEX+geno, family = gaussian,
+ data=my.data, locus.label=label, control =
+ haplo.glm.control(haplo.freq.min = 0.02))

Error in haplo.model.frame(m, locus.label = locus.label, allele.lev =
allele.lev, :
Missing allele.lev = list of vectors for labels of alleles
Check par list for haplo.glm

--



From ramasamy at cancer.org.uk  Mon Feb 28 17:25:19 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 28 Feb 2005 16:25:19 +0000
Subject: [R] A problem about outer()
In-Reply-To: <002c01c51da6$abd33750$77d40893@S119>
References: <002c01c51da6$abd33750$77d40893@S119>
Message-ID: <1109607919.7070.49.camel@ndmpc126.orc.ox.ac.uk>

You might want to read (or re-read) the posting guide about giving a
simple example. See comments below.


On Mon, 2005-02-28 at 23:03 +0800, Feng Chen wrote:
> Dear all,
> 
> I have something about function outer() that I can't understand. Just see the following example. The two NaNs are due to 0/0, but I can't figure out the cause of the last two errors. I wonder if some one can explain this for me. 
> ___________________________________________________________________
> > sx=rbinom(10,1,0.5);ot=rbinom(10,1,0.5);ag <- rbinom(10,100,0.3);ho <- rbinom(10,100,0.5)


Cute but unfortunately not very legible. Why are you mixing "=" and
"<-" ? Is there a problem with space bars and return key on your
keyboard ? Please learn to wrap the emails at about 72 characters per
line (see http://expita.com/nomime.html).


> > dp <- function(s,a,h)sum((sx==s)&(ag==a)&(ho==h)&(ot==1))/sum((sx==s)&(ag==a)&(ho==h))

> > (function(x,y)dp(1,x,y))(2,3)
> [1] NaN
> > (function(x,y)dp(0,x,y))(27,52)
> [1] NaN

Again this is confusing. Why not define another function (you will need
to anyway - see below). 

Alternatively, you can set 1 as the default value for s in dp().  

> > dpm <- outer(ag,ho,function(x,y)dp(1,x,y))
> Error in outer(ag, ho, function(x, y) dp(1, x, y)) : 
>  dim<- : dims [product 100] do not match the length of object [1]

>From help("outer") :

'FUN' must be a function (or the name of it) which expects at
least two arguments and which operates elementwise on arrays.


And following the suggestion of Prof. Daalgard in the thread
http://tolstoy.newcastle.edu.au/R/help/00a/1445.html

dp.vect <- function(s, x, y){
  sapply( 1:length(x), function(i) dp( s=s, a=x[i], h=y[i]) )
}
outer(ag, ho, FUN=dp.vect, s=1 )
# works but I leave the verification to you



Your problem could be generalised as the following example

one <- rnorm(3); two <- rnorm(4)                 # data
outer( one, two, function(x, y) x + y )          # works fine
outer( one, two, function(x, y) sum(c(x, y)) )   # does not work

sum.vect <- function(x, y){
  sapply( 1:length(x), function(i) sum( c( x[i], y[i] ) ) )
}
outer( one, two, sum.vect )



> > dpf <- outer(ag,ho,function(x,y)dp(0,x,y))
> Error in outer(ag, ho, function(x, y) dp(0, x, y)) : 
>  dim<- : dims [product 100] do not match the length of object [1]
> > 
> -------------------------------------------------------------------------------------------------------------------
> 
> Thanks very much,
> Feng
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From dieter.menne at menne-biomed.de  Mon Feb 28 17:22:58 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Mon, 28 Feb 2005 17:22:58 +0100
Subject: [R] [R-pkgs] phpSerialize 0.8
Message-ID: <INEGIMHGODBGKFPOJBBMCEFMCDAA.dieter.menne@menne-biomed.de>

New on CRAN: phpSerialize Version 0.8
Dieter Menne, dieter.menne at menne-biomed.de

Serializes R objects for PHP import into an associative array. 
Main use is for building web pages with R-support. 

Has mainly been tested with lm,lme, nlme and their summaries.

A web example is provide, showing

-- How to start R from php/Apache
-- How to pass variables from php to R via Environment
-- How to create serialized php output from R
-- How to read the serialize output from php/Apache via pipe
-- How to display individual results in a table
-- How to display the structure of the associative array.

For example, the following R structure...

  Delta=as.numeric(Sys.getenv("DELTA")) # Get Info from php
  wc = wilcox.test(rnorm(10),rnorm(10)+Delta)

... is is piped to standard output
  cat(phpSerialize(wc),"\n")

... and after deserialization

  $pp = popen("$Rterm --no-save  --slave  2>&1 < $RFile DELTA=$Delta","r");
  // Read serialized R output via pipe.
  $sWilcox=fgets($pp);
  pclose($pp);
  $Wilcox=unserialize($sWilcox);
  print_r($Wilcox);

... prints as follows:

Array
(
  [statistic] => Array
  (
    [W] => 29
  )

  [parameter] => 
  [p.value] => 0.1230055
  [null.value] => Array
  (
    [mu] => 0
  )
  [alternative] => two.sided
  [method] => Wilcoxon rank sum test
  [data.name] => rnorm(10) and rnorm(10) + Delta
)

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From shigesong at gmail.com  Mon Feb 28 18:17:42 2005
From: shigesong at gmail.com (Shige Song)
Date: Tue, 1 Mar 2005 01:17:42 +0800
Subject: [R] Using mutiply imputed data in NLME
Message-ID: <5abc11d80502280917118306fa@mail.gmail.com>

Dear All,

I am doing a growth modeling using NLME. I have three levels in my
data: observation, individual, household. About half of my total
sample have missing values in my household-level covariates. Under
this situation, the best way to go is probably to multiply impute the
data (for, say, 5 times), estimate the same model separately on each
model using LME function, and merge the results. My question is: given
the multiply imputed data sets have already been generated, is there a
simple way to automate the process of estimating the mixed model and
merging the results? HLM has similar features...

Thanks!

Best,
Shige



From luke at stat.uiowa.edu  Mon Feb 28 18:24:37 2005
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Mon, 28 Feb 2005 11:24:37 -0600 (CST)
Subject: [R] tkRplot under macos x
In-Reply-To: <00c601c51d9f$fdf318e0$1f6df345@WATSON>
References: <00c601c51d9f$fdf318e0$1f6df345@WATSON>
Message-ID: <Pine.LNX.4.61.0502281124160.10902@nokomis.stat.uiowa.edu>

It only works under X11; if that is what you want this posting should
tell you what needs to be done:

         https://stat.ethz.ch/pipermail/r-sig-mac/2004-December/001465.html

Best,

luke

On Mon, 28 Feb 2005, Sean Davis wrote:

> I have not successfully gotten tkRplot to install on macos 10.3.8, R 2.0.0.  Other tcltk packages work just fine.  Any suggestions?  I can post all the output from the compile,etc., but thought I would check the "short" version first.
>
> Thanks,
> Sean
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Luke Tierney
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu



From ssk2031 at columbia.edu  Mon Feb 28 18:30:44 2005
From: ssk2031 at columbia.edu (B Suresh Krishna)
Date: Mon, 28 Feb 2005 12:30:44 -0500 (EST)
Subject: [R] and [ESS] Starting ESS
In-Reply-To: <200502281004.14909.vincent.goulet@act.ulaval.ca>
References: <BAY10-F53C55C4C4F154B31A1829DD6630@phx.gbl>
	<200502241542.58190.vincent.goulet@act.ulaval.ca>
	<422003D2.8050308@columbia.edu>
	<200502281004.14909.vincent.goulet@act.ulaval.ca>
Message-ID: <Pine.GSO.4.62.0502281224180.26188@mango.cc.columbia.edu>


Hi,

Yes, I defined the HOME environment variable as instructed in the 
installation instructions. Also, I can happily switch buffers in Emacs 
(C-x C-b or using C-x C-o) to the R-window and back to the .r file I am 
editing without any problem...

If it matters, I am using Windows XP on a Dell Inspiron 9100 laptop, and 
the latest version of R.

Thanks, Suresh

On Mon, 28 Feb 2005, Vincent Goulet wrote:

> Suresh,
>
> Did you define a HOME environment variable for Emacs to use (as instructed in
> the installation instructions)? I observed that when the HOME variable is not
> defined (or points to a non-existent directory), Emacs will freeze as soon as
> one toggles from the Emacs window to another application and back to Emacs.
>
> And of course, I should have written FSF Emacs in my original post, not FSG.
>
> Hope this helps!
>
> Le 26 F?vrier 2005 00:06, Suresh Krishna a ?crit?:
>> Prof. Goulet,
>>
>> I apologize in advance if I shouldnt have been writing to you directly;
>> but I thought that my problem was simple enough that you could very
>> quickly say if you knew of a quick fix for this.
>>
>> Because I am familiar with the Emacs editor, I was happily and
>> gratefully using your packaged version of FSG Emacs after you suggested
>> it here on the list (R-help); but I have run into a problem that occurs
>>
>> when i run:
>> >x=1;
>> >y=1;
>> >par(ask=T)
>> >plot(x,y)
>>
>> emacs/ess/r now freezes because it is waiting for me to hit enter to see
>> the next plot, however, when i hit enter, i am not sure R is able to
>> realize that i hit enter. also, the "hit enter to see next plot" message
>> is not displayed in the window with the r process.
>>
>> If this is a problem that is somehow specific to what I am doing, then
>> please let me know. I have switched back to JGR and the mouse, until then.
>>
>> Thanks !!
>>
>> Suresh
>>
>> Vincent Goulet wrote:
>>> To answer your question: I don't think it matters.
>>>
>>> If you're willing to use FSG Emacs instead of XEmacs, I repackaged Emacs
>>> 21.3 for Windows together with recent versions of ESS and AuCTeX for my
>>> students (I personally run Linux). The package and installation
>>> instructions are at:
>>>
>>>  http://vgoulet.act.ulaval.ca/download/software/emacs/
>>>
>>> (Look for files emacs-21.3-ready.zip, emacs_install_en.txt and perhaps
>>> ess_s-plus_win_en.txt.)
>>>
>>> Hope this helps!
>>>
>>> Le 23 F?vrier 2005 18:16, Laura Holt a ?crit :
>>>> Dear R People:
>>>>
>>>> I have finally seen the error of my ways and have decided to use ESS for
>>>> R and S + stuff.
>>>>
>>>> However, I have a question right from the beginning.  I'm somewhat
>>>> confused by the installation instructions.
>>>>
>>>> Do I install XEMACS or ESS first, please?
>>>>
>>>> Windows XP
>>>> R Version 2.0.1
>>>> (S + 6.2)
>>>>
>>>> Thanks so much!
>>>>
>>>> Sincerely,
>>>> Laura Holt
>>>> mailto: lauraholt_983 at hotmail.com
>>>>
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide!
>>>> http://www.R-project.org/posting-guide.html
>
> -- 
>  Vincent Goulet, Associate Professor
>  ?cole d'actuariat
>  Universit? Laval, Qu?bec
>  Vincent.Goulet at act.ulaval.ca   http://vgoulet.act.ulaval.ca
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

From Edouard.Henrion at broca.inserm.fr  Mon Feb 28 18:46:53 2005
From: Edouard.Henrion at broca.inserm.fr (Edouard Henrion)
Date: Mon, 28 Feb 2005 18:46:53 +0100
Subject: [R] memory problem with mac os X
Message-ID: <a234f936b0504592cda0cf97b10c95b9@broca.inserm.fr>

Dear list,

I am using R.2.0.1 on a G5 biprocessor 2.5GHz with 2Go RAM (Mac OS X 
10.3.8).

I'm trying to calculate an object of type "dist". I am getting the 
following memory error :

*** malloc: vm_allocate(size=1295929344) failed (error code=3)
*** malloc[25960]: error: Can't allocate region
Error: cannot allocate vector of size 1265554 Kb

When I do a top on the terminal, I can see that this size has already 
been allocated... It seems that R tries to allocate the memory twice.
Does anybody have an advice about this ?

Thanks,

Edouard Henrion



From br44114 at yahoo.com  Mon Feb 28 18:54:30 2005
From: br44114 at yahoo.com (bogdan romocea)
Date: Mon, 28 Feb 2005 09:54:30 -0800 (PST)
Subject: [R] draw random samples from empirical distribution
Message-ID: <20050228175431.13215.qmail@web50110.mail.yahoo.com>

Dear useRs,

I have an empirical distribution (not normal etc) and I want to draw
random samples from it. One solution I can think of is to compute let's
say 100 quantiles, then use runif() to draw a random number Q between 1
and 100, and finally run runif() again to pull a random value from the
quantile Q. Is there perhaps a better/more elegant way of doing this?

Thank you,
b.



From levin001 at 123mail.cl  Mon Feb 28 19:05:46 2005
From: levin001 at 123mail.cl (Peyuco Porras Porras .)
Date: Mon, 28 Feb 2005 14:05:46 -0400
Subject: [R] formatting output
Message-ID: <7f8fd6d7f8cfa8.7f8cfa87f8fd6d@123mail.cl>


   Dear R-users

   A  basic  question  that I wasn't able to solve: Is it possible to get
   the  results  of the function 'quantile' expressed as data.frame? What
   I'm  doing  is  to  apply the following code to get the quantiles in a
   particular dataset:

   tmp<-tapply(data$DEN,list(Age=data$AGE,Sex=data$SEX),quantile)

   and  then I save this output to HTML using the library R2HTML. However
   in  order  to  format  the  tables  in  HTML I have to use the command
   HTML.data.frame(...)  which  allows  me  to  define,  for example, the
   number of digits in the html table.

   But  the object 'tmp' is not a dataframe and I can't coarce it to this
   format.  Is  it  possible  to  get  the  results of this function as a
   dataframe? I  know  that  I'm probably missing some important concepts
   here but Im not very good in programming.

   Any hint will be appreciated

   Regards

   Peyuco


From gunter.berton at gene.com  Mon Feb 28 19:17:45 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 28 Feb 2005 10:17:45 -0800
Subject: [R] R Reference Card (especially useful for Newbies)
Message-ID: <200502281817.j1SIHj9U004708@compton.gene.com>


Newbies (and others!) may find the R Reference Card made available by  Tom
Short and Rpad at http://www.rpad.org/Rpad/Rpad-refcard.pdf  or on the
"Contributed" link on CRAN (where some other reference cards are also
linked) useful. It categorizes and organizes a bunch of R's (S's) basic,
most used functions so that they can be easily found. For example, paste()
is under the "Strings" heading and expand.grid() is under "Data Creation."
For newbies struggling to find the right R function as well as veterans who
can't quite remember the function name, it's very handy.
 
-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box



From dbogod at gmail.com  Mon Feb 28 20:29:17 2005
From: dbogod at gmail.com (Daniel Bogod)
Date: Mon, 28 Feb 2005 14:29:17 -0500
Subject: [R] fitting gamma using glm
Message-ID: <a16e975c05022811295b33c929@mail.gmail.com>

The data in Table 1, which is Table T.1 in Cox & Snell's Applied
Statistics, gives
the intervals in service-hours between failures of the
air-conditioning eequipment in 10 Boeing 720 jet aircraft. The
following possible models are under consideration:
(a) separate gamma distributions fitted to all aircraft, with 20 parameters;
(b) separate gamma distributions with a common shape, with 11 parameters;
(c) espearate exponential distributions to all aircraft (shape= 1,
separate ?), with 10
parameters;
(d) common exponential distribution to all aircraft (shape = 1), with
1 parameter.

http://www.utstat.toronto.edu/reid/sta410/hw2.pdf


I am having trouble in fitting gamma to the 10 different airplanes
using only one shape parameter.

Hopefully, somebody can suggest  a way to specify the model to be used in R.
I tried to join all 10 datasets into 1, but the estimate I get in
gamma.shape() is not reasonable.

Thank you



From lauraholt_983 at hotmail.com  Mon Feb 28 20:42:57 2005
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Mon, 28 Feb 2005 13:42:57 -0600
Subject: [R] number formatting
Message-ID: <BAY10-F29B22CFE265572381A7A1D6580@phx.gbl>

Dear R People:

I have used the command round(x,3) to produce values with 3 places to the 
right of the decimal.

Is there any command to remove the leading zero before the decimal point, 
please:  that is, if I have 0.375, how do I produce just .375, please?

Thanks in advance
R 2.0.1 for Windows

Sincerely,
Laura Holt
mailto: lauraholt_983 at hotmail.com



From Robert.McGehee at geodecapital.com  Mon Feb 28 20:50:56 2005
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Mon, 28 Feb 2005 14:50:56 -0500
Subject: [R] number formatting
Message-ID: <67DCA285A2D7754280D3B8E88EB548020C1ED619@MSGBOSCLB2WIN.DMN1.FMR.COM>

> x <- .001
> x
[1] 0.001
> sub("+0", "", x)
[1] ".001"

Of course, that means you'll be storing this number as a character, but
if you're looking to format how the number is printed, that's probably
what you want.

-----Original Message-----
From: Laura Holt [mailto:lauraholt_983 at hotmail.com] 
Sent: Monday, February 28, 2005 2:43 PM
To: r-help at stat.math.ethz.ch
Subject: [R] number formatting


Dear R People:

I have used the command round(x,3) to produce values with 3 places to
the 
right of the decimal.

Is there any command to remove the leading zero before the decimal
point, 
please:  that is, if I have 0.375, how do I produce just .375, please?

Thanks in advance
R 2.0.1 for Windows

Sincerely,
Laura Holt
mailto: lauraholt_983 at hotmail.com

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From renaud.lancelot at cirad.fr  Mon Feb 28 21:21:16 2005
From: renaud.lancelot at cirad.fr (Renaud Lancelot)
Date: Mon, 28 Feb 2005 23:21:16 +0300
Subject: [R] problems with Rd format
Message-ID: <42237D3C.9050004@cirad.fr>

Dear R-helpers,

I have an Rd file with a section:

\details{
   The model is:\cr
   \eqn{y | \lambda ~ Binomial(n, \lambda)},\cr
   [snip]
   }

I would like the equation line to be indented, with a \tab character for 
example. How can I do that ?

Moreover, the panel of greek letters available in HTML files generated 
from Rd files looks very limited (e.g., \eqn{\phi} produces phi, not the 
corresponding greek letter). Is there a way to overcome this in HTML files ?

Thanks and best regards,

Renaud


System details:

Win XP

R version:
          _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    2
minor    0.1
year     2004
month    11
day      15
language R

-- 
Dr Renaud Lancelot, v?t?rinaire
C/0 Ambassade de France - SCAC
BP 834 Antananarivo 101 - Madagascar

e-mail: renaud.lancelot at cirad.fr
tel.:   +261 32 40 165 53 (cell)
         +261 20 22 665 36 ext. 225 (work)
         +261 20 22 494 37 (home)



From Jhilbe at aol.com  Mon Feb 28 21:51:44 2005
From: Jhilbe at aol.com (Jhilbe@aol.com)
Date: Mon, 28 Feb 2005 15:51:44 EST
Subject: [R] negative multinomial regression models
Message-ID: <ce.634cb433.2f54de60@aol.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050228/a09d714c/attachment.pl

From MSchwartz at MedAnalytics.com  Mon Feb 28 22:10:04 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Mon, 28 Feb 2005 15:10:04 -0600
Subject: [R] formatting output
In-Reply-To: <7f8fd6d7f8cfa8.7f8cfa87f8fd6d@123mail.cl>
References: <7f8fd6d7f8cfa8.7f8cfa87f8fd6d@123mail.cl>
Message-ID: <1109625004.19511.31.camel@horizons.localdomain>

On Mon, 2005-02-28 at 14:05 -0400, Peyuco Porras Porras . wrote:
>    Dear R-users
> 
>    A  basic  question  that I wasn't able to solve: Is it possible to get
>    the  results  of the function 'quantile' expressed as data.frame? What
>    I'm  doing  is  to  apply the following code to get the quantiles in a
>    particular dataset:
> 
>    tmp<-tapply(data$DEN,list(Age=data$AGE,Sex=data$SEX),quantile)
> 
>    and  then I save this output to HTML using the library R2HTML. However
>    in  order  to  format  the  tables  in  HTML I have to use the command
>    HTML.data.frame(...)  which  allows  me  to  define,  for example, the
>    number of digits in the html table.
> 
>    But  the object 'tmp' is not a dataframe and I can't coarce it to this
>    format.  Is  it  possible  to  get  the  results of this function as a
>    dataframe? I  know  that  I'm probably missing some important concepts
>    here but Im not very good in programming.
> 
>    Any hint will be appreciated


Here is one approach, using an expansion of one of the examples in
?tapply:

# Get quantiles of 'breaks' for each combination of 'wool' and 'tension'
> my.tmp <- tapply(warpbreaks$breaks, 
                   list(warpbreaks$wool, warpbreaks$tension), quantile)

# Note that my.tmp is a 2 x 6 matrix of list elements
# with each list element being the results of quantile
# on each combination of 'wool' and 'tension'

> my.tmp
  L         M         H
A Numeric,5 Numeric,5 Numeric,5
B Numeric,5 Numeric,5 Numeric,5


For example:

> my.tmp[1, 1]
[[1]]
  0%  25%  50%  75% 100%
  25   26   51   54   70


Now get this into a manageable structure by taking each list element in
my.tmp and converting it into a row in a new matrix, use:

> my.mat <- do.call("rbind", my.tmp)

> my.mat
     0% 25% 50% 75% 100%
[1,] 25  26  51  54   70
[2,] 14  20  29  31   44
[3,] 12  18  21  30   36
[4,] 16  21  28  39   42
[5,] 10  18  24  28   43
[6,] 13  15  17  21   28


>From there, you can set the rownames for the matrix, as you require,
based upon the combinations of the vectors in your data. For example,
using expand.grid():

> my.names <- expand.grid(dimnames(my.tmp))

> my.names
  Var1 Var2
1    A    L
2    B    L
3    A    M
4    B    M
5    A    H
6    B    H

> rownames(my.mat) <- paste(my.names$Var1, my.names$Var2, sep = ":")

> my.mat
    0% 25% 50% 75% 100%
A:L 25  26  51  54   70
B:L 14  20  29  31   44
A:M 12  18  21  30   36
B:M 16  21  28  39   42
A:H 10  18  24  28   43
B:H 13  15  17  21   28


There might be an easier way, but that's my quick thought.

HTH,

Marc Schwartz



From john.maindonald at anu.edu.au  Mon Feb 28 22:30:03 2005
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Tue, 1 Mar 2005 08:30:03 +1100
Subject: [R] Re: R-help Digest, Vol 24, Issue 28
In-Reply-To: <200502281107.j1SB72Jp030768@hypatia.math.ethz.ch>
References: <200502281107.j1SB72Jp030768@hypatia.math.ethz.ch>
Message-ID: <462ebc1c68d8ab93604d64f6bf48b799@anu.edu.au>

You've omitted a comma. races2000 is a data frame,
which for purposes of extracting rows behaves like
a 2-dimenional object.  The following works fine:

   hills2000 <- races2000[races2000$type == 'hill', ]

Additionally, you might like to ponder
   > type <- races2000[names(races2000)=="type"]
   > type[1:4]
   Error in "[.data.frame"(type, 1:4) : undefined columns selected
   > length(type)                  # type is a data frame with 1 column
   [1] 1
   > vtype <- unlist(type)    # Extract the vector that is the one
                                              # data frame (list) element
   > vtype[1:4]                      # Try also length(vtype)
   type.type1 type.type2 type.type3 type.type4
     "uphill"    "other"    "other"    "relay"

Your syntax (without the comma) does give a result,
providing that the dimensions match (the condition must
have the same number of elements as races2000 has
columns), but it is probably not the result that you want!
See further pp.320-321 of the DAAG book.

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Bioinformation Science, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.

On 28 Feb 2005, at 10:07 PM, r-help-request at stat.math.ethz.ch wrote:

> From: Clint Harshaw <charshaw at presby.edu>
> Date: 28 February 2005 1:08:36 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] subsetting data set dimenion problem
>
>
> (See DAAG book, p. 173, ex. 3)
>
> I'm a new user of R, and I'm following the DAAG text. I want to create 
> a subset of the races2000 data frame, but get errors because of a 
> mismatch of values in some columns:
>
> > library(DAAG)
> > attach(races2000)
> > hills2000 <- races2000[races2000$type == 'hill']
> Error in as.matrix.data.frame(x) : dim<- : dims [product 770] do not 
> match the length of object [771]
>
> However, if I follow the solution given, and remove redundant columns 
> 1 through 6 and column 11 (which I won't need, since I know they are 
> going to have the same value), I don't get the error:
>
> > hills2000 <- races2000[races2000$type == 'hill', -c(1:6,11)]
> > hills2000
>                        dist climb      time      timef
> Tiso Carnethy          6.00  2500 0.7822222  0.9191667
> [...]
> Cornalees              5.50   800 0.6183333         NA
> [...]
>
> What is causing the error with my original subsetting? I speculated it 
> was related to the NA values, but there is an NA in the resulting 
> hills2000, corresponding to the Cornalees hill race.
>
> Thanks,
> Clint
> -- 
> Clint Harshaw, PhD		
> Department of Mathematics
> Presbyterian College
> Clinton SC  29325
>
> EMail: charshaw at presby.edu	
> Phone: 864.833.8995
> Fax: 864.938.3769
> Office: Harrington-Peachtree Rm 412
>
John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Bioinformation Science, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.



From ripley at stats.ox.ac.uk  Mon Feb 28 22:50:30 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 28 Feb 2005 21:50:30 +0000 (GMT)
Subject: [R] problems with Rd format
In-Reply-To: <42237D3C.9050004@cirad.fr>
References: <42237D3C.9050004@cirad.fr>
Message-ID: <Pine.LNX.4.61.0502282138550.5609@gannet.stats>

On Mon, 28 Feb 2005, Renaud Lancelot wrote:

> Dear R-helpers,
>
> I have an Rd file with a section:
>
> \details{
>  The model is:\cr
>  \eqn{y | \lambda ~ Binomial(n, \lambda)},\cr
>  [snip]
>  }
>
> I would like the equation line to be indented, with a \tab character for 
> example. How can I do that ?

Well, use markup that allows \tab: if you want a table, use \tabular.
But if you find yourself doing this, you are not in the spirit of layout 
languages, and Rd is a layout language.

> Moreover, the panel of greek letters available in HTML files generated from 
> Rd files looks very limited (e.g., \eqn{\phi} produces phi, not the 
> corresponding greek letter). Is there a way to overcome this in HTML files ?

What HTML 4.0 _guarantees_ a browser can show is very limited, and does 
not include phi (at least according to my HTML 4.0 book).  So, not in 
strict HTML.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



