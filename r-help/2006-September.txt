From A.Robinson at ms.unimelb.edu.au  Fri Sep  1 00:02:23 2006
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Fri, 1 Sep 2006 08:02:23 +1000
Subject: [R] differnce between lme and proc mixed
In-Reply-To: <20060831212406.99584.qmail@web32110.mail.mud.yahoo.com>
References: <20060831212406.99584.qmail@web32110.mail.mud.yahoo.com>
Message-ID: <20060831220223.GQ49132@ms.unimelb.edu.au>

Hi Liz,

I am sure that soem will know what the differences may be between proc
mixed and lme.  But unless you provide much more information, such as
the actual code that you used, the output, and *best of all* a
reproducible example, we can't be much help.  I hope that these
thoughts are useful.

Andrew


On Thu, Aug 31, 2006 at 02:24:06PM -0700, Elizabeth Lawson wrote:
> Hey,
>    
>   I was using lme and proc mixed in SAS to run a empirical bayesian model.
>   I used the same method for both lme and proc mixed (the default REML).
>    
>   I got very similar, but not identical results.  I am just wondering if anyone knows what the differneces may be between proc mixed and lme.
>    
>   Any thoughts would be appreciated!
>    
>   Thanks,
>    
>   Liz
> 
>  		
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
Email: a.robinson at ms.unimelb.edu.au         http://www.ms.unimelb.edu.au


From edd at debian.org  Fri Sep  1 00:03:57 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 31 Aug 2006 17:03:57 -0500
Subject: [R] cumulative growth rates indexed to a common starting point
 over n series of observations
In-Reply-To: <OF75466F68.0199D2DA-ON862571DB.006CFE77-862571DB.006FEC8F@americancentury.com>
References: <OF75466F68.0199D2DA-ON862571DB.006CFE77-862571DB.006FEC8F@americancentury.com>
Message-ID: <17655.23757.25543.839815@basebud.nulle.part>


On 31 August 2006 at 15:22, toby_marks at americancentury.com wrote:
| What is the R way of computing cumulative growth rates given a series of 
| discrete values indexed .
| 
| For instance, given a matrix of 20 observations for each of 5 series (zz), 
| what is the most straight forward technique in R for computing cumulative 
| growth (zzcum) ?
| It seems for the solution I'm after might be imbedding the following cum 
| growth rate calc as a function into a function call to apply, mapply, ...? 
| 
| 
| 
| zz = rnorm(100)
| dim(zz) = c(20,5)
| zz
| zzcum=matrix(nrow=20,ncol=5)
| zzcum
| zzcum[1,]=100*(1+zz[1,]/100)
| zzcum
| for(i in 2:20){zzcum[i,] = zzcum[i-1,]*(1+zz[i,]/100)}
| zzcum

How about cumprod() inside apply() ?

 zzcum <- matrix(rnorm(100), ncol=5)
 apply(zzcum/100 + 1, 2, cumprod)

Hth, Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From thefishfinger at ihug.com.au  Fri Sep  1 01:49:41 2006
From: thefishfinger at ihug.com.au (Sam Ferguson)
Date: Fri, 1 Sep 2006 09:49:41 +1000
Subject: [R] Tables with Graphical Representations
Message-ID: <3941216D-6BCA-4156-8D7E-1656288D92E3@ihug.com.au>

Hi useRs -

I was wondering if anyone out there can tell me where to find R-code  
to do mixes of tables and graphics. I am thinking of something  
similar to this:
http://yost.com/information-design/powerpoint-corrupts/
or like the excel routines people are demonstrating:
http://infosthetics.com/archives/2006/08/excel_in_cell_graphing.html

My aim is to provide small graphics to illustrate numbers directly  
beside or behind their position in the table. Maybe there is a way to  
do it with lattice?

Thanks for any help you may be able to provide.
Sam Ferguson


From lord.tyranus.96 at gmail.com  Fri Sep  1 02:05:20 2006
From: lord.tyranus.96 at gmail.com (Lord Tyranus)
Date: Thu, 31 Aug 2006 18:05:20 -0600
Subject: [R] How to get argument number
Message-ID: <4f31b0bd0608311705y6acf296ci5d51dd18f4988bf@mail.gmail.com>

I create a function with R, I want to know how many argument number.
Thanks in advance.

-- 
Web Page
http://geocities.com/lord_tyranus_96/


From arrayprofile at yahoo.com  Fri Sep  1 02:21:59 2006
From: arrayprofile at yahoo.com (array chip)
Date: Thu, 31 Aug 2006 17:21:59 -0700 (PDT)
Subject: [R] write.table to Excel file
In-Reply-To: <451E7762.8000307@ese.u-psud.fr>
Message-ID: <20060901002159.75122.qmail@web56313.mail.re3.yahoo.com>

Hi, I found that when writing a matrix with row names
and column names to an Excel file, the Excel file when
opened has column names shifted towards left resulting
disalignment. Here is an exmaple

x<-matrix(1:20,nrow=4,dimnames=list(paste('r',1:4,sep=''),paste('c',1:5,sep='')))
write.table(x,"xx.xls",sep='\t')

If you open the xx.xls file, you will understand what
I meant. Is there anyway to solve the disalignment?

Thanks


From ggrothendieck at gmail.com  Fri Sep  1 02:31:29 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 31 Aug 2006 20:31:29 -0400
Subject: [R] grep question
In-Reply-To: <644e1f320608311459l47f5e276gd013f1bc3e3a038a@mail.gmail.com>
References: <mailman.15.1156845603.16465.r-help@stat.math.ethz.ch>
	<5.1.0.14.0.20060901070041.00c5e630@pop3.brisnet.org.au>
	<644e1f320608311459l47f5e276gd013f1bc3e3a038a@mail.gmail.com>
Message-ID: <971536df0608311731r3e0229b8r2a2bf336558bcee8@mail.gmail.com>

Or using the same x:

> setdiff(x, grep("Farrah|Common", x, value = TRUE))
[1] "more" "last"

On 8/31/06, jim holtman <jholtman at gmail.com> wrote:
> This finds the matching indices of Farrah and Common and then create a
> set that does not include them:
>
> > x <- c('Farrah', 'more', 'Common', 'last')
> > got.F <- grep('Farrah',x)
> > got.C <- grep('Common', x)
> > not.ForC <- setdiff(seq(along=x), c(got.F, got.C))
> > x[not.ForC]
> [1] "more" "last"
> >
>
>
> On 8/31/06, Bob Green <bgreen at dyson.brisnet.org.au> wrote:
> >
> > I am hoping for some advice as to how to modify the following syntax, so
> > that instead of saving all records which refer to Farrah, I select all
> > instances that do not include Farrah, or the word Coolum.
> >
> >
> > test <- read.csv("c:\\newdat.csv", as.is=TRUE, header=T)
> > sure <- test[grep('Farrah', paste(test$V3.HD, test$V3.LP, test$V3.TD)),]
> > write.csv(sure,"c:/farrah4.csv")
> >
> >
> > Any assistance is appreciated,
> >
> > regards
> >
> > Bob  Green
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
> --
> Jim Holtman
> Cincinnati, OH
> +1 513 646 9390
>
> What is the problem you are trying to solve?
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Fri Sep  1 03:00:58 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 31 Aug 2006 21:00:58 -0400
Subject: [R] problems with plot.data.frame
In-Reply-To: <BAY108-F1F7FAE5A5EC316283ABB8C33F0@phx.gbl>
References: <Pine.LNX.4.64.0608311759030.6673@gannet.stats.ox.ac.uk>
	<BAY108-F1F7FAE5A5EC316283ABB8C33F0@phx.gbl>
Message-ID: <971536df0608311800h49763f01sbc763b2f75d3e0d0@mail.gmail.com>

On 8/31/06, Monica Pisica <pisicandru at hotmail.com> wrote:
> Hi again,
>
> OK i came up with this after i got few good sugegstions.
>
> First my data.frame actually looks like that (Thanks for clarifications to
> Prof. Brian Ripley)
>
>   V1 V2 V3 V4
> 1  jan  3  1   7
> 2  mar 2  4  2
> 3  may 1  3  2
> 4  jul   3  7  4
> 5  sep  5  2  3
> 6  nov  3  1  5
>
> What i want: 1. On x axis i want the ticks with labels column V1 in that
> order and not alpha order.
> 2. i want points to represent the data, not horizoltal bars as a box-plot
> with only one value as it will plot if i use plot.data.frame
>
> Note. In my table i already have the order i want, but if i wouldn't have it
> prof. Ripley's sugegstion is very welcome.
>
> What i've done in the end:
>
> >plot (mydata$V3, xlab="month")
> >axis (side=1, at=c(1:6), labels=c(1:6), ticks=TRUE, col.axis="white")
> >month.label <- as.character(mydata[[1]])
> >axis(side=1, at=c(1:6), labels = month.label, ticks=TRUE)
>
> This is not an elegant solution but i get the graph i wanted. If anybody has
> a better solution certainly i would like to see it.
>
> thanks you again for all your help,
>

Try plotting without the axis using xaxt = "n" and then call axis:

plot(mydata$V3, xlab = "month", xaxt = "n")
axis(1, 1:nrow(mydata), format(mydata$V1))

or convert V1 to "Date" class and then plot:

dd <- as.Date(paste("2006", mydata$V1, 1), "%Y %b %d")
plot(V3 ~ dd, mydata)


From jholtman at gmail.com  Fri Sep  1 03:18:05 2006
From: jholtman at gmail.com (jim holtman)
Date: Thu, 31 Aug 2006 21:18:05 -0400
Subject: [R] grep question
In-Reply-To: <971536df0608311731r3e0229b8r2a2bf336558bcee8@mail.gmail.com>
References: <mailman.15.1156845603.16465.r-help@stat.math.ethz.ch>
	<5.1.0.14.0.20060901070041.00c5e630@pop3.brisnet.org.au>
	<644e1f320608311459l47f5e276gd013f1bc3e3a038a@mail.gmail.com>
	<971536df0608311731r3e0229b8r2a2bf336558bcee8@mail.gmail.com>
Message-ID: <644e1f320608311818s7539ce88oda0c78ccffece9c8@mail.gmail.com>

You have to be careful if the strings are embedded:

> x <- c('xxxFarrahxxx' ,'more than last time', 'some Common numbers', 'last one')
> setdiff(x, grep('Farrah|Common', x))  # not correct
[1] "xxxFarrahxxx"        "more than last time" "some Common numbers"
"last one"
> ForC <- grep('Farrah|Common', x)
> x[setdiff(seq(along=x), ForC)]
[1] "more than last time" "last one"
>


On 8/31/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Or using the same x:
>
> > setdiff(x, grep("Farrah|Common", x, value = TRUE))
> [1] "more" "last"
>
> On 8/31/06, jim holtman <jholtman at gmail.com> wrote:
> > This finds the matching indices of Farrah and Common and then create a
> > set that does not include them:
> >
> > > x <- c('Farrah', 'more', 'Common', 'last')
> > > got.F <- grep('Farrah',x)
> > > got.C <- grep('Common', x)
> > > not.ForC <- setdiff(seq(along=x), c(got.F, got.C))
> > > x[not.ForC]
> > [1] "more" "last"
> > >
> >
> >
> > On 8/31/06, Bob Green <bgreen at dyson.brisnet.org.au> wrote:
> > >
> > > I am hoping for some advice as to how to modify the following syntax, so
> > > that instead of saving all records which refer to Farrah, I select all
> > > instances that do not include Farrah, or the word Coolum.
> > >
> > >
> > > test <- read.csv("c:\\newdat.csv", as.is=TRUE, header=T)
> > > sure <- test[grep('Farrah', paste(test$V3.HD, test$V3.LP, test$V3.TD)),]
> > > write.csv(sure,"c:/farrah4.csv")
> > >
> > >
> > > Any assistance is appreciated,
> > >
> > > regards
> > >
> > > Bob  Green
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> >
> > --
> > Jim Holtman
> > Cincinnati, OH
> > +1 513 646 9390
> >
> > What is the problem you are trying to solve?
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From jdbricker at gmail.com  Fri Sep  1 03:19:32 2006
From: jdbricker at gmail.com (Jeff Bricker)
Date: Thu, 31 Aug 2006 21:19:32 -0400
Subject: [R] write.table to Excel file
In-Reply-To: <20060901002159.75122.qmail@web56313.mail.re3.yahoo.com>
References: <451E7762.8000307@ese.u-psud.fr>
	<20060901002159.75122.qmail@web56313.mail.re3.yahoo.com>
Message-ID: <3a3063250608311819k63b21294jdf9019e26048b31a@mail.gmail.com>

from ?write.table:

     By default there is no column name for a column of row names.  If
     'col.names = NA' and 'row.names = TRUE' a blank column name is
     added, which is the convention for CSV files to be read by
     spreadsheets.


On 8/31/06, array chip <arrayprofile at yahoo.com> wrote:
> Hi, I found that when writing a matrix with row names
> and column names to an Excel file, the Excel file when
> opened has column names shifted towards left resulting
> disalignment. Here is an exmaple
>
> x<-matrix(1:20,nrow=4,dimnames=list(paste('r',1:4,sep=''),paste('c',1:5,sep='')))
> write.table(x,"xx.xls",sep='\t')
>
> If you open the xx.xls file, you will understand what
> I meant. Is there anyway to solve the disalignment?
>
> Thanks
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jholtman at gmail.com  Fri Sep  1 03:19:39 2006
From: jholtman at gmail.com (jim holtman)
Date: Thu, 31 Aug 2006 21:19:39 -0400
Subject: [R] grep question
In-Reply-To: <644e1f320608311818s7539ce88oda0c78ccffece9c8@mail.gmail.com>
References: <mailman.15.1156845603.16465.r-help@stat.math.ethz.ch>
	<5.1.0.14.0.20060901070041.00c5e630@pop3.brisnet.org.au>
	<644e1f320608311459l47f5e276gd013f1bc3e3a038a@mail.gmail.com>
	<971536df0608311731r3e0229b8r2a2bf336558bcee8@mail.gmail.com>
	<644e1f320608311818s7539ce88oda0c78ccffece9c8@mail.gmail.com>
Message-ID: <644e1f320608311819q646cc21pa1c7fa9a5b39bd99@mail.gmail.com>

Forget the last reply.  I left the 'value=TRUE' off the grep.

> x <- c('xxxFarrahxxx' ,'more than last time', 'some Common numbers', 'last one')
> setdiff(x, grep('Farrah|Common', x, value=TRUE))
[1] "more than last time" "last one"
> ForC <- grep('Farrah|Common', x)
> x[setdiff(seq(along=x), ForC)]
[1] "more than last time" "last one"
>


On 8/31/06, jim holtman <jholtman at gmail.com> wrote:
> You have to be careful if the strings are embedded:
>
> > x <- c('xxxFarrahxxx' ,'more than last time', 'some Common numbers', 'last one')
> > setdiff(x, grep('Farrah|Common', x))  # not correct
> [1] "xxxFarrahxxx"        "more than last time" "some Common numbers"
> "last one"
> > ForC <- grep('Farrah|Common', x)
> > x[setdiff(seq(along=x), ForC)]
> [1] "more than last time" "last one"
> >
>
>
> On 8/31/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > Or using the same x:
> >
> > > setdiff(x, grep("Farrah|Common", x, value = TRUE))
> > [1] "more" "last"
> >
> > On 8/31/06, jim holtman <jholtman at gmail.com> wrote:
> > > This finds the matching indices of Farrah and Common and then create a
> > > set that does not include them:
> > >
> > > > x <- c('Farrah', 'more', 'Common', 'last')
> > > > got.F <- grep('Farrah',x)
> > > > got.C <- grep('Common', x)
> > > > not.ForC <- setdiff(seq(along=x), c(got.F, got.C))
> > > > x[not.ForC]
> > > [1] "more" "last"
> > > >
> > >
> > >
> > > On 8/31/06, Bob Green <bgreen at dyson.brisnet.org.au> wrote:
> > > >
> > > > I am hoping for some advice as to how to modify the following syntax, so
> > > > that instead of saving all records which refer to Farrah, I select all
> > > > instances that do not include Farrah, or the word Coolum.
> > > >
> > > >
> > > > test <- read.csv("c:\\newdat.csv", as.is=TRUE, header=T)
> > > > sure <- test[grep('Farrah', paste(test$V3.HD, test$V3.LP, test$V3.TD)),]
> > > > write.csv(sure,"c:/farrah4.csv")
> > > >
> > > >
> > > > Any assistance is appreciated,
> > > >
> > > > regards
> > > >
> > > > Bob  Green
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible code.
> > > >
> > >
> > >
> > > --
> > > Jim Holtman
> > > Cincinnati, OH
> > > +1 513 646 9390
> > >
> > > What is the problem you are trying to solve?
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
> --
> Jim Holtman
> Cincinnati, OH
> +1 513 646 9390
>
> What is the problem you are trying to solve?
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From ggrothendieck at gmail.com  Fri Sep  1 03:26:18 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 31 Aug 2006 21:26:18 -0400
Subject: [R] How to get argument number
In-Reply-To: <4f31b0bd0608311705y6acf296ci5d51dd18f4988bf@mail.gmail.com>
References: <4f31b0bd0608311705y6acf296ci5d51dd18f4988bf@mail.gmail.com>
Message-ID: <971536df0608311826y3719ebx4e742345ebe49040@mail.gmail.com>

Look at ?nargs

On 8/31/06, Lord Tyranus <lord.tyranus.96 at gmail.com> wrote:
> I create a function with R, I want to know how many argument number.
> Thanks in advance.
>
> --
> Web Page
> http://geocities.com/lord_tyranus_96/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From shimpo at fbc.keio.ac.jp  Fri Sep  1 03:57:48 2006
From: shimpo at fbc.keio.ac.jp (Kazushige Shimpo)
Date: Fri, 01 Sep 2006 10:57:48 +0900
Subject: [R] write.table to Excel file
In-Reply-To: <3a3063250608311819k63b21294jdf9019e26048b31a@mail.gmail.com>
References: <451E7762.8000307@ese.u-psud.fr>	<20060901002159.75122.qmail@web56313.mail.re3.yahoo.com>
	<3a3063250608311819k63b21294jdf9019e26048b31a@mail.gmail.com>
Message-ID: <44F7939C.2000307@fbc.keio.ac.jp>

How about using write.csv?

Jeff Bricker wrote:
> from ?write.table:
> 
>      By default there is no column name for a column of row names.  If
>      'col.names = NA' and 'row.names = TRUE' a blank column name is
>      added, which is the convention for CSV files to be read by
>      spreadsheets.
> 
> 
> On 8/31/06, array chip <arrayprofile at yahoo.com> wrote:
> 
>>Hi, I found that when writing a matrix with row names
>>and column names to an Excel file, the Excel file when
>>opened has column names shifted towards left resulting
>>disalignment. Here is an exmaple
>>
>>x<-matrix(1:20,nrow=4,dimnames=list(paste('r',1:4,sep=''),paste('c',1:5,sep='')))
>>write.table(x,"xx.xls",sep='\t')
>>
>>If you open the xx.xls file, you will understand what
>>I meant. Is there anyway to solve the disalignment?
>>
>>Thanks
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
Kazushige Shimpo<shimpo at fbc.keio.ac.jp>
Department of Business & Commerce
Keio University


From epistat at gmail.com  Fri Sep  1 04:19:27 2006
From: epistat at gmail.com (zhijie zhang)
Date: Fri, 1 Sep 2006 10:19:27 +0800
Subject: [R] what's wrong with my simulation programs on logistic
	regression
In-Reply-To: <Pine.LNX.4.64.0608311548520.5222@gannet.stats.ox.ac.uk>
References: <2fc17e30608310712t6be1024bvcbbad215c504e098@mail.gmail.com>
	<Pine.LNX.4.64.0608311548520.5222@gannet.stats.ox.ac.uk>
Message-ID: <2fc17e30608311919n64cc1c3ck3681fd64d8a27f64@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060901/d03b8ce8/attachment.pl 

From mkimpel at iupui.edu  Fri Sep  1 04:40:35 2006
From: mkimpel at iupui.edu (Kimpel, Mark William)
Date: Thu, 31 Aug 2006 22:40:35 -0400
Subject: [R] problem with postscript output of R-devel on Windows
In-Reply-To: <44F74613.3040200@stats.uwo.ca>
Message-ID: <836F00680EECD340A96AD34ECFF3B53461B1F6@iu-mssg-mbx106.ads.iu.edu>

I installed GSView and the file opens correctly and the output is as is
should be. As you suggest, this must be a bug in Adobe CS2 Illustrator
and Photoshop.

Thanks for your help.

Mark

Mark W. Kimpel MD 

 

(317) 490-5129 Work, & Mobile

 

(317) 663-0513 Home (no voice mail please)

1-(317)-536-2730 FAX


-----Original Message-----
From: Duncan Murdoch [mailto:murdoch at stats.uwo.ca] 
Sent: Thursday, August 31, 2006 4:27 PM
To: Kimpel, Mark William
Cc: ripley at stats.ox.ac.uk; r-help at stat.math.ethz.ch
Subject: Re: [R] problem with postscript output of R-devel on Windows

On 8/31/2006 3:51 PM, Kimpel, Mark William wrote:
> I apologize for my previous confusing example. Below is some sample
> code, taken directly from the "image" help file, that reproduces a
> postscript problem. This now happens with both R 2.3.1 and R 2.4
> 
> What I get appears to be output of only certain postscript "objects",
to
> use an Adobe term. When I use the R GUI menu to "save as", jpeg and
pdf
> files save correctly, but the postscript file does not. I am not
getting
> any axis labels or topo labels. This is true whether I import the PS
> file into either Photoshop or Illustrator.

I don't see a problem using GSView.  Maybe this is an Adobe bug?

Duncan

> 
> Thanks, Mark
> 
> x <- 10*(1:nrow(volcano))
>      y <- 10*(1:ncol(volcano))
>      image(x, y, volcano, col = terrain.colors(100), axes = FALSE)
>      contour(x, y, volcano, levels = seq(90, 200, by = 5),
>              add = TRUE, col = "peru")
>      axis(1, at = seq(100, 800, by = 100))
>      axis(2, at = seq(100, 600, by = 100))
>      box()
>      title(main = "Maunga Whau Volcano", font.main = 4)
> 
>> sessionInfo()
> Version 2.3.1 (2006-06-01) 
> i386-pc-mingw32 
> 
> attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"
> "datasets" 
> [7] "base"  
> 
> Mark W. Kimpel MD 
> 
>  
> 
> (317) 490-5129 Work, & Mobile
> 
>  
> 
> (317) 663-0513 Home (no voice mail please)
> 
> 1-(317)-536-2730 FAX
> 
> 
> -----Original Message-----
> From: Duncan Murdoch [mailto:murdoch at stats.uwo.ca] 
> Sent: Thursday, August 31, 2006 12:52 PM
> To: Kimpel, Mark William
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] problem with postscript output of R-devel on Windows
> 
> On 8/31/2006 11:27 AM, Kimpel, Mark William wrote:
>> I have developed a problem with the postscript output of plot on
> Windows. My code still works properly with R 2.3 but, with R 2.4, the
> white text on red background does not show up. It does, however, show
up
> when output is sent to the screen. Below is my code and sessionInfo.
>> 
>> R version 2.4.0 Under development (unstable) (2006-08-29 r39012) 
>> i386-pc-mingw32 
>> 
>> locale:
>> LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
> States.1252;LC_MONETARY=English_United
> States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252
>> 
>> attached base packages:
>> [1] "splines"   "tools"     "methods"   "stats"     "graphics"
> "grDevices" "utils"     "datasets" 
>> [9] "base"     
>> 
>> other attached packages:
>>   Rgraphviz geneplotter         XML     GOstats    Category
> hgu95av2        KEGG    multtest      xtable 
>>    "1.11.9"    "1.11.8"    "0.99-8"     "1.6.0"     "1.4.1"
> "1.12.0"     "1.8.1"    "1.11.2"     "1.3-2" 
>>        RBGL    annotate          GO       graph       Ruuid
> limma  genefilter    survival     rat2302 
>>     "1.8.1"    "1.11.5"     "1.6.5"   "1.11.13"    "1.11.2"
> "2.7.9"    "1.11.8"      "2.28"    "1.12.0" 
>>        affy      affyio     Biobase 
>>    "1.11.6"     "1.1.8"   "1.11.29"
>> 
>> 
>> fileName<-paste(experiment, contrast, "FDR", FDR, "Graph", "ps",
> sep=".")
>>     postscript(file=fileName, paper="special",width=width,
> height=height) #set up graphics device
>>     plot(result.gN, layout.param, nodeAttrs = nAttrs, edgeAttrs =
> eAttrs,
>>         main=paste(paste("Experiment:", experiment, ";  Contrast:",
> contrast,";  FDR:", FDR, sep=""), paste("Min. connections ==",
> min.edges, "Min. citations per connection ==", min.cites, "Additional
> search criteria:",
>>             termAdditional, sep=" "), sep="    "))
> 
> 
> Could you put together a reproducible example to illustrate the
problem?
> 
>   We don't have all the variables used in that example.  I think you 
> should be able to do it with just base packages attached; if not, it's

> likely a problem with one of the contributed packages, rather than
with
> R.
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jeff.hamann at forestinformatics.com  Fri Sep  1 04:40:36 2006
From: jeff.hamann at forestinformatics.com (Jeff D. Hamann)
Date: Thu, 31 Aug 2006 19:40:36 -0700 (PDT)
Subject: [R] core dump with point.in.polygon
Message-ID: <1346.128.193.139.146.1157078436.squirrel@www.forestinformatics.com>

I've been trying to get some code to run on a 64-bit FreeBSD machine running

R : Copyright 2006, The R Foundation for Statistical Computing
Version 2.3.0 (2006-04-24)
ISBN 3-900051-07-0

and keep getting a core dump with the following results:

 *** caught segfault ***
address 0x0, cause 'unknown'

Traceback:
 1: .Call("R_point_in_polygon_sp", as.numeric(point.x),
as.numeric(point.y),     as.numeric(pol.x), as.numeric(pol.y), PACKAGE =
"sp")
 2: sp::point.in.polygon(point.x = stems$easting, point.y =
stems$northing,     pol.x = get.pts(gpc.plot)[[1]]$x, pol.y =
get.pts(gpc.plot)[[1]]$y)
 3: as.logical(sp::point.in.polygon(point.x = stems$easting, point.y =
stems$northing,     pol.x = get.pts(gpc.plot)[[1]]$x, pol.y =
get.pts(gpc.plot)[[1]]$y))
 4: poi.fp(gpc.perim, plot.grid[i, ], stems, plot.designs[1, ], plot.it =
FALSE)
 5: eval.with.vis(expr, envir, enclos)
 6: eval.with.vis(ei, envir)
 7: source("generate_samples.r")

Not sure what to do now. I've been running this on 2.3 Winxp box just fine
and will continue to do so.


-- 
Forest Informatics, Inc.
PO Box 1421
Corvallis, Oregon 97339-1421


From David.Ziger at synopsys.com  Fri Sep  1 06:08:04 2006
From: David.Ziger at synopsys.com (David Ziger)
Date: Thu, 31 Aug 2006 21:08:04 -0700
Subject: [R] problem with png
Message-ID: <8858BFBCB86F8D4BA996BEA6FF05F3DA0187680C@US01WEMBX2.internal.synopsys.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060831/d139a2f0/attachment.pl 

From rajpathakhs at yahoo.co.in  Fri Sep  1 07:39:27 2006
From: rajpathakhs at yahoo.co.in (Hrishikesh Rajpathak)
Date: Fri, 1 Sep 2006 06:39:27 +0100 (BST)
Subject: [R] makeSOCKcluster
Message-ID: <20060901053930.36990.qmail@web8505.mail.in.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060901/5ee8f2ff/attachment.ksh 

From ripley at stats.ox.ac.uk  Fri Sep  1 07:39:50 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 1 Sep 2006 06:39:50 +0100 (BST)
Subject: [R] core dump with point.in.polygon
In-Reply-To: <1346.128.193.139.146.1157078436.squirrel@www.forestinformatics.com>
References: <1346.128.193.139.146.1157078436.squirrel@www.forestinformatics.com>
Message-ID: <Pine.LNX.4.64.0609010602440.19689@gannet.stats.ox.ac.uk>

On Thu, 31 Aug 2006, Jeff D. Hamann wrote:

> I've been trying to get some code to run on a 64-bit FreeBSD machine running
> 
> R : Copyright 2006, The R Foundation for Statistical Computing
> Version 2.3.0 (2006-04-24)
> ISBN 3-900051-07-0
> 
> and keep getting a core dump with the following results:
> 
>  *** caught segfault ***
> address 0x0, cause 'unknown'
> 
> Traceback:
>  1: .Call("R_point_in_polygon_sp", as.numeric(point.x),
> as.numeric(point.y),     as.numeric(pol.x), as.numeric(pol.y), PACKAGE =
> "sp")
>  2: sp::point.in.polygon(point.x = stems$easting, point.y =
> stems$northing,     pol.x = get.pts(gpc.plot)[[1]]$x, pol.y =
> get.pts(gpc.plot)[[1]]$y)
>  3: as.logical(sp::point.in.polygon(point.x = stems$easting, point.y =
> stems$northing,     pol.x = get.pts(gpc.plot)[[1]]$x, pol.y =
> get.pts(gpc.plot)[[1]]$y))
>  4: poi.fp(gpc.perim, plot.grid[i, ], stems, plot.designs[1, ], plot.it =
> FALSE)
>  5: eval.with.vis(expr, envir, enclos)
>  6: eval.with.vis(ei, envir)
>  7: source("generate_samples.r")
> 
> Not sure what to do now. I've been running this on 2.3 Winxp box just fine
> and will continue to do so.

There are contributed packages involved here: please identify which and 
report this to the maintainer with a reproducible example.

There are 10 or so known packages which seqfault on 64-bit platforms: the 
authors have been informed (often long ago).  This looks as if it might be 
'sp', which is not one.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Fri Sep  1 07:43:25 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 1 Sep 2006 06:43:25 +0100 (BST)
Subject: [R] How to get argument number
In-Reply-To: <4f31b0bd0608311705y6acf296ci5d51dd18f4988bf@mail.gmail.com>
References: <4f31b0bd0608311705y6acf296ci5d51dd18f4988bf@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0609010642060.1772@gannet.stats.ox.ac.uk>

?formals
?nargs

depending if you mean the number of formal arguments or the number of 
actual arguments.

On Thu, 31 Aug 2006, Lord Tyranus wrote:

> I create a function with R, I want to know how many argument number.
> Thanks in advance.
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Fri Sep  1 07:46:58 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 1 Sep 2006 06:46:58 +0100 (BST)
Subject: [R] problem with png
In-Reply-To: <8858BFBCB86F8D4BA996BEA6FF05F3DA0187680C@US01WEMBX2.internal.synopsys.com>
References: <8858BFBCB86F8D4BA996BEA6FF05F3DA0187680C@US01WEMBX2.internal.synopsys.com>
Message-ID: <Pine.LNX.4.64.0609010644190.1772@gannet.stats.ox.ac.uk>

On Thu, 31 Aug 2006, David Ziger wrote:

> I cannot generate png files running R2.2.1 on a  SunOS 5.9 machine. See
> below.  I did a search on this problem and people who have virtually the
> same error have solved the problem by installing or updating  libpng .
> We downloaded libpng from Sun but still the problem remains. Does a path
> within R need to be set to this file or does the problem indicate that
> libpng was not installed properly?   I have no problems running on
> various Linux OS's or a SunOS 5.8 machine.  I appreciate any help on
> this issue.  We are using R2.2.1

You have to re-configure, re-make and re-install R.  While you are at it, 
why not use a current version of R (as in fact the posting guide asked you 
to before posting)?

[...]

> In addition: Warning message:
> no png support in this version of R 

This is covered in the 'R Installation and Administration' manual.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Fri Sep  1 07:59:55 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 1 Sep 2006 06:59:55 +0100 (BST)
Subject: [R] makeSOCKcluster
In-Reply-To: <20060901053930.36990.qmail@web8505.mail.in.yahoo.com>
References: <20060901053930.36990.qmail@web8505.mail.in.yahoo.com>
Message-ID: <Pine.LNX.4.64.0609010648130.1772@gannet.stats.ox.ac.uk>

On Fri, 1 Sep 2006, Hrishikesh Rajpathak wrote:

> Hi,
>   
>   I am a newbie to R and trying to implement parallelism in R. I am 
>   currently using R-2.3.1, and Cygwin to run R on Windows xp.

Did you build R under Cygwin (which is not a supported platform), or are 
you running a native Windows build of R?  If the latter, you may be being 
optimistic to expect R to run Unix commands in the same way as under Unix.

I presume you mean this line in package snow:

    system(paste(rshcmd, "-l", user, machine, "env", env, script))
 
which appears to be attempting to run a shell script: no shell is used by 
system() on Windows R.  As the return value is not checked, this will not 
fail.

R under Windows differs from R under Unix in a number of ways, and 
system() is a major one.


>   ssh and all are working fine,
>   
>   When I try to create a socket connection as
>   
>    makeSOCKcluster(c("localhost","localhost")), 
>   
>   it just waits for the other prcess on localhost to get created and respond. But this other process is not created.
>   
>   To debug, I put print statements in the "snow " file in library\snow\r  of R after every statement that comes under Socket Implementation. I  realized that it does the execution till
>   
>   system(a<-paste(rshcmd, "-l", user, machine, "env", env, script))
>   
>   part and then it goes in wait state. It cannot run the next command which is
>   
>   con <- socketConnection(port = port, server=TRUE, blocking=TRUE,
>                                open="a+b")
>   
>   can someone please tell me what exactly could be the problem?
>   
>   Thank you,
>   Rishi

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Fri Sep  1 08:22:33 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 1 Sep 2006 07:22:33 +0100 (BST)
Subject: [R] Substring and strsplit
In-Reply-To: <67A99605-58BF-4263-BB44-48D6BA94C54D@eva.mpg.de>
References: <200608292225.k7TMPQrJ029520@gator.dt.uh.edu>
	<00a901c6cc01$ddfc2240$0540210a@www.domain>
	<67A99605-58BF-4263-BB44-48D6BA94C54D@eva.mpg.de>
Message-ID: <Pine.LNX.4.64.0608301054110.27066@gannet.stats.ox.ac.uk>

On Wed, 30 Aug 2006, Hans-Joerg Bibiko wrote:

> If you are using 'only' English then
> 
> str <- "dog"
> strsplit(str,NULL)[[1]]
> 
> works perfectly and it is fast.

It does also work 'perfectly' and fast in 'Unicode' in all major European 
and CJK languages (and many others): extending the iconv example

> xx
[1] "fa?ile"
> strsplit(xx, NULL)
[[1]]
[1] "f" "a" "?" "i" "l" "e"
> charToRaw(strsplit(xx, NULL)[[1]][3])
[1] c3 a7

on a UTF-8 system.

> But if you also dealing with Unicode character have a look at

http://wiki.r-project.org/rwiki/doku.php?id=tips:data-strings:decomposestring

That is a misleading reference (to your own opinion, and it is usual in 
science to make clear what your source is when citing, especially if it is 
yourself).  Unicode itself has combining diacritical marks as separate 
entries in the 'character code tables' at e.g. 
http://www.unicode.org/charts/, so your understanding of 'character' seems 
to differ from Unicode's.

You write about 'combined Unicode diacritics (accents)', which is 
misleading, as these are not accents (and it is 'combining' not 
'combined', a crucial difference).  To quote Alan Wood 
(http://www.alanwood.net/unicode/combining_diacritical_marks.html)

  The _characters_ in this range are designed to be used in combination 
  with alphanumeric _characters_, to produce a character+diacritic that
  is not present in any of the Unicode ranges. For example, a&#777; 
  to produce a lower case "a" with a hook above.

So they are used for very rare glyphs made up from two Unicode characters, 
and R correctly views them as two characters.  (Actually R relies on the 
OS services to correctly identify characters, but that appears to have 
happened on the example on the RWiki page.)

You could have just thanked the R developers for ensuring that strsplit() 
does work as documented even in Unicode locales.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From Roger.Bivand at nhh.no  Fri Sep  1 08:56:48 2006
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 1 Sep 2006 08:56:48 +0200 (CEST)
Subject: [R] core dump with point.in.polygon
In-Reply-To: <Pine.LNX.4.64.0609010602440.19689@gannet.stats.ox.ac.uk>
Message-ID: <Pine.LNX.4.44.0609010848450.5844-100000@reclus.nhh.no>

On Fri, 1 Sep 2006, Prof Brian Ripley wrote:

> On Thu, 31 Aug 2006, Jeff D. Hamann wrote:
> 
> > I've been trying to get some code to run on a 64-bit FreeBSD machine running
> > 
> > R : Copyright 2006, The R Foundation for Statistical Computing
> > Version 2.3.0 (2006-04-24)
> > ISBN 3-900051-07-0
> > 
> > and keep getting a core dump with the following results:
> > 
> >  *** caught segfault ***
> > address 0x0, cause 'unknown'
> > 
> > Traceback:
> >  1: .Call("R_point_in_polygon_sp", as.numeric(point.x),
> > as.numeric(point.y),     as.numeric(pol.x), as.numeric(pol.y), PACKAGE =
> > "sp")
> >  2: sp::point.in.polygon(point.x = stems$easting, point.y =
> > stems$northing,     pol.x = get.pts(gpc.plot)[[1]]$x, pol.y =
> > get.pts(gpc.plot)[[1]]$y)
> >  3: as.logical(sp::point.in.polygon(point.x = stems$easting, point.y =
> > stems$northing,     pol.x = get.pts(gpc.plot)[[1]]$x, pol.y =
> > get.pts(gpc.plot)[[1]]$y))
> >  4: poi.fp(gpc.perim, plot.grid[i, ], stems, plot.designs[1, ], plot.it =
> > FALSE)
> >  5: eval.with.vis(expr, envir, enclos)
> >  6: eval.with.vis(ei, envir)
> >  7: source("generate_samples.r")
> > 
> > Not sure what to do now. I've been running this on 2.3 Winxp box just fine
> > and will continue to do so.
> 
> There are contributed packages involved here: please identify which and 
> report this to the maintainer with a reproducible example.

Jeff:

Could you please provide a case with the code of your function and the 
data that is going to R_point_in_polygon_sp? Please also consider the 
R-sig-geo list (or the Sourceforge R-spat-dev list on r-spatial) in 
addition to writing to Edzer Pebesma and me as maintainers?

If it helps, we can use the sourceforge CVS repository to try to nail this 
down. Running R -d gdb would be helpful.



> 
> There are 10 or so known packages which seqfault on 64-bit platforms: the 
> authors have been informed (often long ago).  This looks as if it might be 
> 'sp', which is not one.
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From g.russell at eos-finance.com  Fri Sep  1 10:00:19 2006
From: g.russell at eos-finance.com (g.russell at eos-finance.com)
Date: Fri, 1 Sep 2006 10:00:19 +0200
Subject: [R] Antwort: Re:  Antwort: Buying more computer for GLM
In-Reply-To: <x2mz9lf7tv.fsf@viggo.kubism.ku.dk>
Message-ID: <OF0312D345.629C89D5-ONC12571DC.002B5F43-C12571DC.002BF9BE@DFD-Hamburg>

Peter Dalgaard wrote
> Is this floating point bound? (When you say 30 factors does that mean
> 30 parameters or factors representing a much larger number of groups).
> If it is integer bound, I don't think you can do much better than
> increase CPU speed and - note - memory bandwidth (look for large-cache
> systems and fast front-side bus). To increase floating point
> performance, you might consider the option of using optimized BLAS
> (see the Windows FAQ 8.2 and/or the "R Installation and
> Administration" manual) like ATLAS; this in turn may be multithreaded
> and make use of multiple CPUs or multi-core CPUs.

By "factors" I mean "parameters".   I apologise for the confusion.

This is floating point bound, so ATLAS might be a good idea. 


Before I put a lot of work into investigating multiple processors, I need 
to know,
is the bottleneck with GLM going to be BLAS?

Thank you very much for your help!

George Russell


From jacques.veslot at good.ibl.fr  Fri Sep  1 10:35:47 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Fri, 01 Sep 2006 10:35:47 +0200
Subject: [R] newbie question about index
In-Reply-To: <20060831144127.10844.qmail@web15610.mail.cnb.yahoo.com>
References: <20060831144127.10844.qmail@web15610.mail.cnb.yahoo.com>
Message-ID: <44F7F0E3.3010704@good.ibl.fr>

(a==1)*1
or ifelse(a == 1, 1, 0)
-------------------------------------------------------------------
Jacques VESLOT

CNRS UMR 8090
I.B.L (2?me ?tage)
1 rue du Professeur Calmette
B.P. 245
59019 Lille Cedex

Tel : 33 (0)3.20.87.10.44
Fax : 33 (0)3.20.87.10.31

http://www-good.ibl.fr
-------------------------------------------------------------------


z s a ?crit :
> Hi,
> 
>   I am trying to convert a variable a = sample(1:3,100,rep = T) represents choices into a 3X100 dummy varible b with corresponding element set to 1 otherwise 0.
> eg.
> 
> a: 1 3 2 1 2 3 1 1....
> 
> b: 1 0 0 1 0 0 1 1..
>     0 0 1 0 1 0 0 0...
>     0 1 0 0 0 1 0 0...
> 
>  Is there something like b[a] =1 existing? I could not figure this out myself.
> 
>  		
> ---------------------------------
>  Mp3??????-??????????????   
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ggrothendieck at gmail.com  Fri Sep  1 10:42:11 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 1 Sep 2006 04:42:11 -0400
Subject: [R] newbie question about index
In-Reply-To: <20060831144127.10844.qmail@web15610.mail.cnb.yahoo.com>
References: <mailman.13.1156932004.4788.r-help@stat.math.ethz.ch>
	<20060831144127.10844.qmail@web15610.mail.cnb.yahoo.com>
Message-ID: <971536df0609010142w16803a98jee05ddbf82b0511a@mail.gmail.com>

Try using outer:

outer(1:3, a, "==")+0




On 8/31/06, z s <flyhyena at yahoo.com.cn> wrote:
> Hi,
>
>  I am trying to convert a variable a = sample(1:3,100,rep = T) represents choices into a 3X100 dummy varible b with corresponding element set to 1 otherwise 0.
> eg.
>
> a: 1 3 2 1 2 3 1 1....
>
> b: 1 0 0 1 0 0 1 1..
>    0 0 1 0 1 0 0 0...
>    0 1 0 0 0 1 0 0...
>
>  Is there something like b[a] =1 existing? I could not figure this out myself.
>
>
> ---------------------------------
>  Mp3??????-??????????????
>        [[alternative HTML version deleted]]
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


From ggrothendieck at gmail.com  Fri Sep  1 10:45:41 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 1 Sep 2006 04:45:41 -0400
Subject: [R] newbie question about index
In-Reply-To: <971536df0609010142w16803a98jee05ddbf82b0511a@mail.gmail.com>
References: <mailman.13.1156932004.4788.r-help@stat.math.ethz.ch>
	<20060831144127.10844.qmail@web15610.mail.cnb.yahoo.com>
	<971536df0609010142w16803a98jee05ddbf82b0511a@mail.gmail.com>
Message-ID: <971536df0609010145j5e9bed95ga87a8d0bce52585d@mail.gmail.com>

Here is an additional way:

model.matrix(~ factor(a) - 1)


On 9/1/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Try using outer:
>
> outer(1:3, a, "==")+0
>
>
>
>
> On 8/31/06, z s <flyhyena at yahoo.com.cn> wrote:
> > Hi,
> >
> >  I am trying to convert a variable a = sample(1:3,100,rep = T) represents choices into a 3X100 dummy varible b with corresponding element set to 1 otherwise 0.
> > eg.
> >
> > a: 1 3 2 1 2 3 1 1....
> >
> > b: 1 0 0 1 0 0 1 1..
> >    0 0 1 0 1 0 0 0...
> >    0 1 0 0 0 1 0 0...
> >
> >  Is there something like b[a] =1 existing? I could not figure this out myself.
> >
> >
> > ---------------------------------
> >  Mp3??????-??????????????
> >        [[alternative HTML version deleted]]
> >
> >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >
>


From joris.dewolf at cropdesign.com  Fri Sep  1 10:45:04 2006
From: joris.dewolf at cropdesign.com (Joris De Wolf)
Date: Fri, 01 Sep 2006 10:45:04 +0200
Subject: [R] newbie question about index
In-Reply-To: <20060831144127.10844.qmail@web15610.mail.cnb.yahoo.com>
References: <20060831144127.10844.qmail@web15610.mail.cnb.yahoo.com>
Message-ID: <44F7F310.8030501@cropdesign.com>

what about:
b <- rbind(1*(a==1),1*(a==2),1*(a==3))


z s wrote:
> Hi,
> 
>   I am trying to convert a variable a = sample(1:3,100,rep = T) represents choices into a 3X100 dummy varible b with corresponding element set to 1 otherwise 0.
> eg.
> 
> a: 1 3 2 1 2 3 1 1....
> 
> b: 1 0 0 1 0 0 1 1..
>     0 0 1 0 1 0 0 0...
>     0 1 0 0 0 1 0 0...
> 
>  Is there something like b[a] =1 existing? I could not figure this out myself.
> 
>  		
> ---------------------------------
>  Mp3??????-??????????????   
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


confidentiality notice:
The information contained in this e-mail is confidential and...{{dropped}}


From petr.pikal at precheza.cz  Fri Sep  1 11:11:15 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 01 Sep 2006 11:11:15 +0200
Subject: [R] newbie question about index
In-Reply-To: <20060831144127.10844.qmail@web15610.mail.cnb.yahoo.com>
References: <mailman.13.1156932004.4788.r-help@stat.math.ethz.ch>
Message-ID: <44F81553.31541.2CD7BC@localhost>

Hallo

probably there are other options but

outer(1:3,a, "==")*1

can do what you want.

HTH
Petr



On 31 Aug 2006 at 22:41, z s wrote:

Date sent:      	Thu, 31 Aug 2006 22:41:27 +0800 (CST)
From:           	z s <flyhyena at yahoo.com.cn>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] newbie question about index

> Hi,
> 
>   I am trying to convert a variable a = sample(1:3,100,rep = T)
>   represents choices into a 3X100 dummy varible b with corresponding
>   element set to 1 otherwise 0.
> eg.
> 
> a: 1 3 2 1 2 3 1 1....
> 
> b: 1 0 0 1 0 0 1 1..
>     0 0 1 0 1 0 0 0...
>     0 1 0 0 0 1 0 0...
> 
>  Is there something like b[a] =1 existing? I could not figure this out
>  myself.
> 
> 
> ---------------------------------
>  Mp3??????-??????????????   
>  [[alternative HTML version deleted]]
> 
> 

Petr Pikal
petr.pikal at precheza.cz


From ripley at stats.ox.ac.uk  Fri Sep  1 11:12:48 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 1 Sep 2006 10:12:48 +0100 (BST)
Subject: [R] Antwort: Re:  Antwort: Buying more computer for GLM
In-Reply-To: <OF0312D345.629C89D5-ONC12571DC.002B5F43-C12571DC.002BF9BE@DFD-Hamburg>
References: <OF0312D345.629C89D5-ONC12571DC.002B5F43-C12571DC.002BF9BE@DFD-Hamburg>
Message-ID: <Pine.LNX.4.64.0609010919040.26402@gannet.stats.ox.ac.uk>

On Fri, 1 Sep 2006, g.russell at eos-finance.com wrote:

> Peter Dalgaard wrote
> > Is this floating point bound? (When you say 30 factors does that mean
> > 30 parameters or factors representing a much larger number of groups).
> > If it is integer bound, I don't think you can do much better than
> > increase CPU speed and - note - memory bandwidth (look for large-cache
> > systems and fast front-side bus). To increase floating point
> > performance, you might consider the option of using optimized BLAS
> > (see the Windows FAQ 8.2 and/or the "R Installation and
> > Administration" manual) like ATLAS; this in turn may be multithreaded
> > and make use of multiple CPUs or multi-core CPUs.
> 
> By "factors" I mean "parameters".   I apologise for the confusion.
> 
> This is floating point bound, so ATLAS might be a good idea. 
> 
> Before I put a lot of work into investigating multiple processors, I 
> need to know, is the bottleneck with GLM going to be BLAS?

Probably not, but you have the ability to profile in R and find out.


Some more comments;

1) The Fortran code that underlies glm is that of lm.fit that only makes 
   use of level-1 BLAS and so is not going to be helped greatly by an 
   optimized BLAS.

2) No one has as far as I know succeeded in making a multithreaded 
   Rblas.dll for Windows.  And under systems using pthreads, the success 
   with multithreaded BLAS is very mixed, with it resulting in a dramatic 
   slowdown in some problems.

3) As I recall, you were doing model selection via AIC on 20,000 
   observations.  You might want to think hard about that, since AIC is 
   designed for good prediction.  I would do model exploration on a much 
   smaller representative subset, and if I had 20,000 observations and 30 
   parameters and was interested in prediction, not do subset selection at 
   all.

4) glm() alllows you to specify starting parameters, which you could find 
   from a subsample.  Very likely only 1 or 2 iterations would be needed.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Soren.Hojsgaard at agrsci.dk  Fri Sep  1 11:19:00 2006
From: Soren.Hojsgaard at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Fri, 1 Sep 2006 11:19:00 +0200
Subject: [R] Checking a package: "Foreign function calls without 'PACKAGE'
	argument:" - what must I do??
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC047E4999@DJFPOST01.djf.agrsci.dk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060901/2de0c709/attachment.pl 

From p.dalgaard at biostat.ku.dk  Fri Sep  1 11:30:44 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 01 Sep 2006 11:30:44 +0200
Subject: [R] Checking a package: "Foreign function calls without
	'PACKAGE' argument:" - what must I do??
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC047E4999@DJFPOST01.djf.agrsci.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC047E4999@DJFPOST01.djf.agrsci.dk>
Message-ID: <x2ac5kylvf.fsf@viggo.kubism.ku.dk>

S?ren H?jsgaard <Soren.Hojsgaard at agrsci.dk> writes:

> In the process of checking a package, I get the warning
>  
> Foreign function calls without 'PACKAGE' argument:
> .Call("tr", ...)
> .Call("trProd", ...)
> See section 'System and foreign language interfaces' of the 'Writing R..
>  
> These functions are called using the wrappers
> 
> trX <- function(x, package="gRcox") {  .Call("tr", x, package="gRcox") }
> trXY <- function(x, y, package="gRcox") {  .Call("trProd", x, y, package="gRcox") }
>  
> I can't figure out what I am supposed to - and reading the specific section of "writing R extensions.." does not help me. 
>  
> Can anyone help?

Get rid of old DOS habit, and realize that R is case-sensitive....

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From luke at stat.uiowa.edu  Fri Sep  1 13:33:16 2006
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Fri, 1 Sep 2006 06:33:16 -0500 (CDT)
Subject: [R] makeSOCKcluster
In-Reply-To: <Pine.LNX.4.64.0609010648130.1772@gannet.stats.ox.ac.uk>
References: <20060901053930.36990.qmail@web8505.mail.in.yahoo.com>
	<Pine.LNX.4.64.0609010648130.1772@gannet.stats.ox.ac.uk>
Message-ID: <Pine.LNX.4.64.0609010609510.2458@itasca2.wildberry.org>

On Fri, 1 Sep 2006, Prof Brian Ripley wrote:

> On Fri, 1 Sep 2006, Hrishikesh Rajpathak wrote:
>
>> Hi,
>>
>>   I am a newbie to R and trying to implement parallelism in R. I am
>>   currently using R-2.3.1, and Cygwin to run R on Windows xp.
>
> Did you build R under Cygwin (which is not a supported platform), or are
> you running a native Windows build of R?  If the latter, you may be being
> optimistic to expect R to run Unix commands in the same way as under Unix.
>
> I presume you mean this line in package snow:
>
>    system(paste(rshcmd, "-l", user, machine, "env", env, script))
>
> which appears to be attempting to run a shell script: no shell is used by
> system() on Windows R.  As the return value is not checked, this will not
> fail.
>
> R under Windows differs from R under Unix in a number of ways, and
> system() is a major one.

This runs a command of the form

 	ssh -l user machine env X=x Y=z script

so if your ssh is a .exe this should be OK.  I have successfully run a
snow master on Windows with slave nodes on Linux with I believe the
ssh coming from cygwin, but that was a while ago.

My suggestion is that you look at the command that is generated by the
paste and try running it by hand and see if you can work out why it
hangs.

Best,

luke


>
>
>>   ssh and all are working fine,
>>
>>   When I try to create a socket connection as
>>
>>    makeSOCKcluster(c("localhost","localhost")),
>>
>>   it just waits for the other prcess on localhost to get created and respond. But this other process is not created.
>>
>>   To debug, I put print statements in the "snow " file in library\snow\r  of R after every statement that comes under Socket Implementation. I  realized that it does the execution till
>>
>>   system(a<-paste(rshcmd, "-l", user, machine, "env", env, script))
>>
>>   part and then it goes in wait state. It cannot run the next command which is
>>
>>   con <- socketConnection(port = port, server=TRUE, blocking=TRUE,
>>                                open="a+b")
>>
>>   can someone please tell me what exactly could be the problem?
>>
>>   Thank you,
>>   Rishi
>
>

-- 
Luke Tierney
Chair, Statistics and Actuarial Science
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu


From bonfigli at inmi.it  Fri Sep  1 13:56:11 2006
From: bonfigli at inmi.it (Bonfigli Sandro)
Date: Fri, 01 Sep 2006 13:56:11 +0200
Subject: [R] Consulta sobre bases en R
In-Reply-To: <0J4V003IODYJGP00@frontend1.telmexchile.cl>
References: <0J4V003IODYJGP00@frontend1.telmexchile.cl>
Message-ID: <WorldClient-F200609011356.AA56110029@inmi.it>

Yo no soy experto en estos asuntos pero creo que no 
haya una respuesta a su pregunta.
No es que R no tenga tama?o m?ximo de datos que pueda 
llegar a procesar, pero ese tama?o depende de el hardware
(las caracteristica de su computadora) y el software
(el sistema operativo no es fijo, como R trabaja bajo 
win. linux, solaris, os X. ...).

Mas que una vez en esta mailing list se ha discutido sobre 
este asunto y usted tendria que buscar esas letras para 
aprender mas.
Esta claro que, como el idioma oficial de esta mailing list
es el ingl?s, todas sus letras estan escritas en esta lengua
y ademas esta claro que si usted escribe en espa?ol
el exito mas probable es que nadie responda

  Sandro Bonfigli

> Estimados,
> 
> Quisiera saber cuanto es el tama?o m?ximo de datos contenidos en mi
> base de
> datos, que el software R puede llegar a procesar y soporta.
> 
> De antemano agradezco y envi? un cordial saludo.
> 
>  
> 
> Marcela Corrales
> 
> CPData Optimum


From gchappi at gmail.com  Fri Sep  1 13:56:25 2006
From: gchappi at gmail.com (Hans-Peter)
Date: Fri, 1 Sep 2006 13:56:25 +0200
Subject: [R] embed image (png) in postscript (device)
Message-ID: <47fce0650609010456l5739f11yc63892e53b94cd61@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060901/e83feac7/attachment.pl 

From murdoch at stats.uwo.ca  Fri Sep  1 14:02:58 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 01 Sep 2006 08:02:58 -0400
Subject: [R] problem with postscript output of R-devel on Windows
In-Reply-To: <836F00680EECD340A96AD34ECFF3B53461B1F6@iu-mssg-mbx106.ads.iu.edu>
References: <836F00680EECD340A96AD34ECFF3B53461B1F6@iu-mssg-mbx106.ads.iu.edu>
Message-ID: <44F82172.8060409@stats.uwo.ca>

On 8/31/2006 10:40 PM, Kimpel, Mark William wrote:
> I installed GSView and the file opens correctly and the output is as is
> should be. As you suggest, this must be a bug in Adobe CS2 Illustrator
> and Photoshop.

It could conceivably still be our bug, but I'd like to hear what Adobe 
tech support says about it.

Duncan Murdoch

> 
> Thanks for your help.
> 
> Mark
> 
> Mark W. Kimpel MD 
> 
>  
> 
> (317) 490-5129 Work, & Mobile
> 
>  
> 
> (317) 663-0513 Home (no voice mail please)
> 
> 1-(317)-536-2730 FAX
> 
> 
> -----Original Message-----
> From: Duncan Murdoch [mailto:murdoch at stats.uwo.ca] 
> Sent: Thursday, August 31, 2006 4:27 PM
> To: Kimpel, Mark William
> Cc: ripley at stats.ox.ac.uk; r-help at stat.math.ethz.ch
> Subject: Re: [R] problem with postscript output of R-devel on Windows
> 
> On 8/31/2006 3:51 PM, Kimpel, Mark William wrote:
>> I apologize for my previous confusing example. Below is some sample
>> code, taken directly from the "image" help file, that reproduces a
>> postscript problem. This now happens with both R 2.3.1 and R 2.4
>>
>> What I get appears to be output of only certain postscript "objects",
> to
>> use an Adobe term. When I use the R GUI menu to "save as", jpeg and
> pdf
>> files save correctly, but the postscript file does not. I am not
> getting
>> any axis labels or topo labels. This is true whether I import the PS
>> file into either Photoshop or Illustrator.
> 
> I don't see a problem using GSView.  Maybe this is an Adobe bug?
> 
> Duncan
> 
>> Thanks, Mark
>>
>> x <- 10*(1:nrow(volcano))
>>      y <- 10*(1:ncol(volcano))
>>      image(x, y, volcano, col = terrain.colors(100), axes = FALSE)
>>      contour(x, y, volcano, levels = seq(90, 200, by = 5),
>>              add = TRUE, col = "peru")
>>      axis(1, at = seq(100, 800, by = 100))
>>      axis(2, at = seq(100, 600, by = 100))
>>      box()
>>      title(main = "Maunga Whau Volcano", font.main = 4)
>>
>>> sessionInfo()
>> Version 2.3.1 (2006-06-01) 
>> i386-pc-mingw32 
>>
>> attached base packages:
>> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"
>> "datasets" 
>> [7] "base"  
>>
>> Mark W. Kimpel MD 
>>
>>  
>>
>> (317) 490-5129 Work, & Mobile
>>
>>  
>>
>> (317) 663-0513 Home (no voice mail please)
>>
>> 1-(317)-536-2730 FAX
>>
>>
>> -----Original Message-----
>> From: Duncan Murdoch [mailto:murdoch at stats.uwo.ca] 
>> Sent: Thursday, August 31, 2006 12:52 PM
>> To: Kimpel, Mark William
>> Cc: r-help at stat.math.ethz.ch
>> Subject: Re: [R] problem with postscript output of R-devel on Windows
>>
>> On 8/31/2006 11:27 AM, Kimpel, Mark William wrote:
>>> I have developed a problem with the postscript output of plot on
>> Windows. My code still works properly with R 2.3 but, with R 2.4, the
>> white text on red background does not show up. It does, however, show
> up
>> when output is sent to the screen. Below is my code and sessionInfo.
>>> R version 2.4.0 Under development (unstable) (2006-08-29 r39012) 
>>> i386-pc-mingw32 
>>>
>>> locale:
>>> LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
>> States.1252;LC_MONETARY=English_United
>> States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252
>>> attached base packages:
>>> [1] "splines"   "tools"     "methods"   "stats"     "graphics"
>> "grDevices" "utils"     "datasets" 
>>> [9] "base"     
>>>
>>> other attached packages:
>>>   Rgraphviz geneplotter         XML     GOstats    Category
>> hgu95av2        KEGG    multtest      xtable 
>>>    "1.11.9"    "1.11.8"    "0.99-8"     "1.6.0"     "1.4.1"
>> "1.12.0"     "1.8.1"    "1.11.2"     "1.3-2" 
>>>        RBGL    annotate          GO       graph       Ruuid
>> limma  genefilter    survival     rat2302 
>>>     "1.8.1"    "1.11.5"     "1.6.5"   "1.11.13"    "1.11.2"
>> "2.7.9"    "1.11.8"      "2.28"    "1.12.0" 
>>>        affy      affyio     Biobase 
>>>    "1.11.6"     "1.1.8"   "1.11.29"
>>>
>>>
>>> fileName<-paste(experiment, contrast, "FDR", FDR, "Graph", "ps",
>> sep=".")
>>>     postscript(file=fileName, paper="special",width=width,
>> height=height) #set up graphics device
>>>     plot(result.gN, layout.param, nodeAttrs = nAttrs, edgeAttrs =
>> eAttrs,
>>>         main=paste(paste("Experiment:", experiment, ";  Contrast:",
>> contrast,";  FDR:", FDR, sep=""), paste("Min. connections ==",
>> min.edges, "Min. citations per connection ==", min.cites, "Additional
>> search criteria:",
>>>             termAdditional, sep=" "), sep="    "))
>>
>> Could you put together a reproducible example to illustrate the
> problem?
>>   We don't have all the variables used in that example.  I think you 
>> should be able to do it with just base packages attached; if not, it's
> 
>> likely a problem with one of the contributed packages, rather than
> with
>> R.
>>
>> Duncan Murdoch
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bibiko at eva.mpg.de  Fri Sep  1 14:19:27 2006
From: bibiko at eva.mpg.de (Hans-Joerg Bibiko)
Date: Fri, 1 Sep 2006 14:19:27 +0200
Subject: [R] Substring and strsplit
In-Reply-To: <Pine.LNX.4.64.0608301054110.27066@gannet.stats.ox.ac.uk>
References: <200608292225.k7TMPQrJ029520@gator.dt.uh.edu>
	<00a901c6cc01$ddfc2240$0540210a@www.domain>
	<67A99605-58BF-4263-BB44-48D6BA94C54D@eva.mpg.de>
	<Pine.LNX.4.64.0608301054110.27066@gannet.stats.ox.ac.uk>
Message-ID: <D03E8F09-E181-4D79-B3A6-9CC2A12B1C12@eva.mpg.de>


On 1 Sep 2006, at 08:22, Prof Brian Ripley wrote:

> On Wed, 30 Aug 2006, Hans-Joerg Bibiko wrote:
>
>> If you are using 'only' English then
>>
>> str <- "dog"
>> strsplit(str,NULL)[[1]]
>>
>> works perfectly and it is fast.
>
> It does also work 'perfectly' and fast in 'Unicode' in all major  
> European
> and CJK languages (and many others): extending the iconv example
>

YES, of course, you are right. R supports Unicode and other encodings  
very well. This is one of the reasons why I've chosen R for my purposes.

If you look at my first example at this Rwiki-site, it contains  
Russian, German, and two Chinese characters to illustrate that the R  
function strsplit can handle this perfectly.


If I wrote about 'English' and 'Unicode' my only intention was to put  
it simply.
My experience is if I'm writing about 'combining diacritics' or  
'combining vowels' etc. some people don't understand these topics.
If I'm writing about 'Unicode' some have a vage association what I'm  
writing about.
Of course, in a scientific context this is absolutely wrong and  
misleading!

> http://www.unicode.org/charts/, so your understanding of  
> 'character' seems
> to differ from Unicode's.
>

Well, the term 'character' is highly ambiguous. So a better term  
would be glyph to emphasise that I mean a representation of a grapheme.
But still, even the terms 'gylph', 'grapheme', 'phoneme', etc. are  
also ambiguous.
Of course, my fault was that I didn't clarify my terminology in  
beforehand.

> You write about 'combined Unicode diacritics (accents)', which is
> misleading, as these are not accents (and it is 'combining' not
> 'combined', a crucial difference).

This was my grammatical fault. Sorry. I corrected this.

> To quote Alan Wood
> (http://www.alanwood.net/unicode/combining_diacritical_marks.html)

>   The _characters_ in this range are designed to be used in  
> combination
>   with alphanumeric _characters_, to produce a character+diacritic  
> that
>   is not present in any of the Unicode ranges. For example, a&#777;
>   to produce a lower case "a" with a hook above.
>

Yes! This is right, but ...

To illustrate MY problem I use your French example with 'fa?ile'.


>> xx
> [1] "fa?ile"
>> strsplit(xx, NULL)
> [[1]]
> [1] "f" "a" "?" "i" "l" "e"
>> charToRaw(strsplit(xx, NULL)[[1]][3])
> [1] c3 a7
>
> on a UTF-8 system.
>


There are two possibilities by using Unicode to write 'fa?ile':
1) "f" "a" "?" "i" "l" "e"
2) "f" "a" "c" "combining cedilla (\u0327)" "i" "l" "e"

Now I use the R function strsplit and I will get two different results.

 > a <- "fa?ile"
 > strsplit(a,NULL)
[[1]]
[1] "f" "a" "?" "i" "l" "e"

 > b <- "fac?ile"
 > strsplit(b,NULL)
[[1]]
[1] "f" "a" "c" "?"   "i" "l" "e"


On the computer screen you don't see any difference in 1) and 2) {if  
your system supports this rendering}.

Always, the questions are: 'What do I want to split?' 'What is a  
character/glyph in my context?'

An other nice example I added to the wiki-site
http://wiki.r-project.org/rwiki/doku.php?id=tips:data- 
strings:decomposestring


> So they are used for very rare glyphs made up from two Unicode  
> characters,
> and R correctly views them as two characters.

R views them correctly if a character is defined as a single code point.
On the other hand, in my research I'm using hundreds of languages  
using these 'rare' glyphs!

To summarise:
- My intention was only to put it simply and short.
- It was NOT my intention to state that the R function strsplit  
doesn't support Unicode.
   The R developers did and still doing a great job! Thank you so much!
- Last but not least, SORRY for my incompleteness!

With regards,

Hans


From info at aghmed.fsnet.co.uk  Fri Sep  1 14:28:16 2006
From: info at aghmed.fsnet.co.uk (Michael Dewey)
Date: Fri, 01 Sep 2006 13:28:16 +0100
Subject: [R] Consulta sobre bases en R
In-Reply-To: <0J4V003IODYJGP00@frontend1.telmexchile.cl>
References: <0J4V003IODYJGP00@frontend1.telmexchile.cl>
Message-ID: <7.0.0.16.0.20060901132556.019d6780@aghmed.fsnet.co.uk>

At 17:04 31/08/2006, Marcela Corrales wrote:
>Estimados,
>
>Quisiera saber cuanto es el tama?o m?ximo de datos contenidos en mi base de
>datos, que el software R puede llegar a procesar y soporta.

Marcela, you will get more help if you
(a) post in English
(b) give us more information about your problem and your system.

At the moment the answer to your question is 'How long is a piece of string?'

Try using the site search facility for "large databases"


>De antemano agradezco y envi? un cordial saludo.

De nada

>
>
>Marcela Corrales
>
>CPData Optimum

And please do not send HTML.


>         [[alternative HTML version deleted]]

Michael Dewey
http://www.aghmed.fsnet.co.uk


From ripley at stats.ox.ac.uk  Fri Sep  1 14:29:22 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 1 Sep 2006 13:29:22 +0100 (BST)
Subject: [R] problem with postscript output of R-devel on Windows
In-Reply-To: <44F82172.8060409@stats.uwo.ca>
References: <836F00680EECD340A96AD34ECFF3B53461B1F6@iu-mssg-mbx106.ads.iu.edu>
	<44F82172.8060409@stats.uwo.ca>
Message-ID: <Pine.LNX.4.64.0609011310230.11719@gannet.stats.ox.ac.uk>

On Fri, 1 Sep 2006, Duncan Murdoch wrote:

> On 8/31/2006 10:40 PM, Kimpel, Mark William wrote:
> > I installed GSView and the file opens correctly and the output is as is
> > should be. As you suggest, this must be a bug in Adobe CS2 Illustrator
> > and Photoshop.
> 
> It could conceivably still be our bug, but I'd like to hear what Adobe tech
> support says about it.

I tried this in gs 8.53 (which is what GSView calls), Adobe Distiller 
7.0.7 and Photoshop CS2 (9.0.2).  The first two were fine.  CS2 said it 
did not have the fonts used (which might actually be true, if surprising) 
and changed the font of the labels, but converted everything else fine.

So is it possible that you have CS2 set to suppress unknown fonts (I have 
not looked even to see if it is possible)?

I think importing .ps into Illustrator is known to be iffy, and I have 
seen the recommendation to first distill to PDF and then import (so in R 
you could save directly to PDF).


> > > x <- 10*(1:nrow(volcano))
> > >      y <- 10*(1:ncol(volcano))
> > >      image(x, y, volcano, col = terrain.colors(100), axes = FALSE)
> > >      contour(x, y, volcano, levels = seq(90, 200, by = 5),
> > >              add = TRUE, col = "peru")
> > >      axis(1, at = seq(100, 800, by = 100))
> > >      axis(2, at = seq(100, 600, by = 100))
> > >      box()
> > >      title(main = "Maunga Whau Volcano", font.main = 4)

[Run on Windows, use 'Save as' to postscript.]


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From g.russell at eos-finance.com  Fri Sep  1 14:34:07 2006
From: g.russell at eos-finance.com (g.russell at eos-finance.com)
Date: Fri, 1 Sep 2006 14:34:07 +0200
Subject: [R] Antwort: Buying more computer for GLM
In-Reply-To: <Pine.LNX.4.64.0609010919040.26402@gannet.stats.ox.ac.uk>
Message-ID: <OFD38E8F9D.78C60C19-ONC12571DC.0043A9F1-C12571DC.00450AAD@DFD-Hamburg>

Prof Brian Ripley wrote
> Probably not, but you have the ability to profile in R and find out.
Thanks.   This is certainly something I could check, and I shall do so.

> 
> 
> Some more comments;
> 
> 1) The Fortran code that underlies glm is that of lm.fit that only makes 

>    use of level-1 BLAS and so is not going to be helped greatly by an 
>    optimized BLAS.

I was afraid it might be something like that.
> 
> 2) No one has as far as I know succeeded in making a multithreaded 
>    Rblas.dll for Windows.  And under systems using pthreads, the success 

>    with multithreaded BLAS is very mixed, with it resulting in a 
dramatic 
>    slowdown in some problems.

I was afraid of that too.   Oh well.
> 
> 3) As I recall, you were doing model selection via AIC on 20,000 
>    observations.  You might want to think hard about that, since AIC is 
>    designed for good prediction.  I would do model exploration on a much 

>    smaller representative subset, and if I had 20,000 observations and 
30 
>    parameters and was interested in prediction, not do subset selection 
at 
>    all.

One problem is that some of the parameters in the learning set can be very 
highly 
correlated (I have no control over the observations), and I'm worried that 
if I 
don't prune away parameters which don't improve the log likelihood, my 
predictions will be 
busted by inputs which do not exhibit the same linear relationships as 
those of most of the 
learning set.   Of course in such a case you'd have to worry about the 
accuracy of the 
predictions anyway, but in my job we just have to get make the best 
predictions we can, 
even if they aren't perfect.

> 
> 4) glm() alllows you to specify starting parameters, which you could 
find 
>    from a subsample.  Very likely only 1 or 2 iterations would be 
needed.

This sounds like a good idea, but what in fact I do now is build a model 
using simple linear
regression (lm), which is very fast, in the hope that that will pick out 
the important parameters,
which I can then feed to glm.

Many thanks again!

George Russell


From ripley at stats.ox.ac.uk  Fri Sep  1 15:07:14 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 1 Sep 2006 14:07:14 +0100 (BST)
Subject: [R] Antwort: Buying more computer for GLM
In-Reply-To: <OFD38E8F9D.78C60C19-ONC12571DC.0043A9F1-C12571DC.00450AAD@DFD-Hamburg>
References: <OFD38E8F9D.78C60C19-ONC12571DC.0043A9F1-C12571DC.00450AAD@DFD-Hamburg>
Message-ID: <Pine.LNX.4.64.0609011358000.11719@gannet.stats.ox.ac.uk>

On Fri, 1 Sep 2006, g.russell at eos-finance.com wrote:

> Prof Brian Ripley wrote

> > 3) As I recall, you were doing model selection via AIC on 20,000 
> >    observations.  You might want to think hard about that, since AIC is 
> >    designed for good prediction.  I would do model exploration on a much 
> 
> >    smaller representative subset, and if I had 20,000 observations and 
> 30 
> >    parameters and was interested in prediction, not do subset selection 
> at 
> >    all.
> 
> One problem is that some of the parameters in the learning set can be 
> very highly correlated (I have no control over the observations), and 
> I'm worried that if I don't prune away parameters which don't improve 
> the log likelihood, my predictions will be busted by inputs which do not 
> exhibit the same linear relationships as those of most of the learning 
> set.  Of course in such a case you'd have to worry about the accuracy of 
> the predictions anyway, but in my job we just have to get make the best 
> predictions we can, even if they aren't perfect.

In that case I would probably not use AIC as my criterion.  Suppose this 
were logistic regression.  Then if I was doing very well in my 
predictions, the AIC would be around 5,000, and I can only reduce it by 60 
by dropping parameters. So variables will be dropped only if they are 
almost completely useless.  I don't think it is a statistical decision as 
to which of two very similar predictors to keep, and in your size of 
problem AIC is quite likely to keep both.

> > 4) glm() alllows you to specify starting parameters, which you could 
> find 
> >    from a subsample.  Very likely only 1 or 2 iterations would be 
> needed.
> 
> This sounds like a good idea, but what in fact I do now is build a model 
> using simple linear
> regression (lm), which is very fast, in the hope that that will pick out 
> the important parameters,
> which I can then feed to glm.

I would not have expected glm to be more than say 5x slower than lm if CPU 
cycles and not memory were the limiting factor.  In that case more RAM 
might be all you need.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From rolf at erdos.math.unb.ca  Fri Sep  1 15:09:32 2006
From: rolf at erdos.math.unb.ca (Rolf Turner)
Date: Fri, 1 Sep 2006 10:09:32 -0300 (ADT)
Subject: [R] Local library under Windoze.
Message-ID: <200609011309.k81D9WaM015507@erdos.math.unb.ca>


Continuing to try to customize my environment for using R under
Windoze, I experimented with installing a package from CRAN in a
local library ``Lib''.

I created the directory ``Lib'' in the folder in which R starts,
and then executed

	> install.packages("abind",lib="Lib")

Everything went according to form (I got prompted to choose a mirror,
etc.) until the end of the show when I got the warning message

Warning: unable to move temporary installation 'C:\Documents and
Settings\rolf\My Documents\Rstuff\Lib\file5f906952\Lib\file5f906952\abind to
'C:\Documents and Settings\rolf\My Documents\Rstuff\Lib\file5f906952\Lib\abind'

[I have folded the foregoing warning --- which came out as a single line ---
to make it fit in an 80 character wide screen.]

[I was also told that the ``downloaded packages are in ...'', and when
I looked in the indicated folder the zip file was indeed there ---
but a fat lot of good that does me.]

The warning was more like an *error*. When this had finished, the folder
Lib was empty; no sign of the file5f906952 stuff, or anything else.

Can anyone explain to me what's going on/wrong?  There is no problem
apparently if I do

	> install.packages("abind")

which installs into the ``system'' library.  In current circumstances
this is good enough --- since I have write permission on the ``system''
library, I can just use that.  So this is, for the moment, an
academic question.  Still the facility seems to be *there* for
installing to a local user-owned library, and it seems not to be
working for me, and I'd like to figure out why.

I thought for a moment that I'd found the problem a little while
back, when I noticed that ``Lib'' was ``Readonly''.  But then
when I tried to change that --- ``unclicking'' the Readonly box
in the ``Properties'' of Lib --- I found that I couldn't.  When
I looked at the Properties again, I found it was back to being
Readonly again.  (With no warning or error message of any kind.)
Some further investigation seemed to indicate that *all* folders
are Readonly.  (Is this really as it should be?  And if so, what's
the point of having this property for folders?)

Can anyone enlighten me?

				cheers,

					Rolf Turner
					rolf at math.unb.ca


From g.russell at eos-finance.com  Fri Sep  1 15:33:58 2006
From: g.russell at eos-finance.com (g.russell at eos-finance.com)
Date: Fri, 1 Sep 2006 15:33:58 +0200
Subject: [R] Antwort: Re:  Antwort: Buying more computer for GLM
In-Reply-To: <Pine.LNX.4.64.0609011358000.11719@gannet.stats.ox.ac.uk>
Message-ID: <OF4DD68685.5B701747-ONC12571DC.00489FC9-C12571DC.004A8590@DFD-Hamburg>

Prof Brian Ripley wrote:
> I would not have expected glm to be more than say 5x slower than lm if 
CPU 
> cycles and not memory were the limiting factor.  In that case more RAM 
> might be all you need.

The ratio between glm and lm might well be about 5x, but that's still a 
big difference for us.   I am pretty sure that RAM is not the main 
problem; according to the Windows Task Manager the computer is at close to 
100% CPU usage, and swapping is not going on.   Of course L1/L2 caches may 
still be
something one can work on, but I'm not sure whether glm has enough 
repeated access to the same data for that to help.   (I don't know how glm 
works,
but I guess it does a lot of scans through the whole data set, and that 
the amount of working memory it needs during these scans is basically a 
function of the number of parameters, not the number of observations, is 
that right?)

Many thanks for your observations about subset selection by the way, they 
are a lot of help.   Would a good approach be, say, to use some stricter 
criteria like BIC for choosing a model, and then use non-statistical 
methods to improve the plausibility of the chosen parameters?

best wishes,

George Russell


From ripley at stats.ox.ac.uk  Fri Sep  1 15:50:52 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 1 Sep 2006 14:50:52 +0100 (BST)
Subject: [R] Antwort: Re:  Antwort: Buying more computer for GLM
In-Reply-To: <OF4DD68685.5B701747-ONC12571DC.00489FC9-C12571DC.004A8590@DFD-Hamburg>
References: <OF4DD68685.5B701747-ONC12571DC.00489FC9-C12571DC.004A8590@DFD-Hamburg>
Message-ID: <Pine.LNX.4.64.0609011438200.11719@gannet.stats.ox.ac.uk>

On Fri, 1 Sep 2006, g.russell at eos-finance.com wrote:

> Prof Brian Ripley wrote:
> > I would not have expected glm to be more than say 5x slower than lm if 
> CPU 
> > cycles and not memory were the limiting factor.  In that case more RAM 
> > might be all you need.
> 
> The ratio between glm and lm might well be about 5x, but that's still a 
> big difference for us.   

You said lm was 'very fast', so I did not expect 5x 'very fast' to be 'too 
slow'.

> I am pretty sure that RAM is not the main 
> problem; according to the Windows Task Manager the computer is at close to 
> 100% CPU usage, and swapping is not going on.   Of course L1/L2 caches may 
> still be
> something one can work on, but I'm not sure whether glm has enough 
> repeated access to the same data for that to help.   (I don't know how glm 
> works,
> but I guess it does a lot of scans through the whole data set, and that 
> the amount of working memory it needs during these scans is basically a 
> function of the number of parameters, not the number of observations, is 
> that right?)

Not so.  Because glm does weighted fits, it needs to access the whole data 
matrix at each iteration (to re-weight).

> Many thanks for your observations about subset selection by the way, they 
> are a lot of help.   Would a good approach be, say, to use some stricter 
> criteria like BIC for choosing a model, and then use non-statistical 
> methods to improve the plausibility of the chosen parameters?

The latter entirely I would say.  All statistics can say is that a 
variable improves the fit measurably more than one that is unrelated to 
the response: whether it improves it enough to be worthwhile in your 
application is non-statistical. The point here is that all but the most 
uselss variables will measurably improve the fit in large problems with 
few variables.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From bansouvik at gmail.com  Fri Sep  1 15:53:03 2006
From: bansouvik at gmail.com (souvik banerjee)
Date: Fri, 1 Sep 2006 19:23:03 +0530
Subject: [R] defining error structure in bivariate mixed models
Message-ID: <7193991f0609010653p1bc29d28qd8a26de1f2420f22@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060901/0c936bd9/attachment.pl 

From toby_marks at americancentury.com  Fri Sep  1 15:55:26 2006
From: toby_marks at americancentury.com (toby_marks at americancentury.com)
Date: Fri, 1 Sep 2006 08:55:26 -0500
Subject: [R] cumulative growth rates indexed to a common starting point
 over n series of observations
In-Reply-To: <17655.23757.25543.839815@basebud.nulle.part>
Message-ID: <OF89A475AB.A525040E-ON862571DC.004C46AC-862571DC.004C7CEE@americancentury.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060901/3e139322/attachment.pl 

From f.harrell at vanderbilt.edu  Fri Sep  1 16:00:36 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 01 Sep 2006 09:00:36 -0500
Subject: [R] Tables with Graphical Representations
In-Reply-To: <3941216D-6BCA-4156-8D7E-1656288D92E3@ihug.com.au>
References: <3941216D-6BCA-4156-8D7E-1656288D92E3@ihug.com.au>
Message-ID: <44F83D04.2090405@vanderbilt.edu>

Sam Ferguson wrote:
> Hi useRs -
> 
> I was wondering if anyone out there can tell me where to find R-code  
> to do mixes of tables and graphics. I am thinking of something  
> similar to this:
> http://yost.com/information-design/powerpoint-corrupts/
> or like the excel routines people are demonstrating:
> http://infosthetics.com/archives/2006/08/excel_in_cell_graphing.html
> 
> My aim is to provide small graphics to illustrate numbers directly  
> beside or behind their position in the table. Maybe there is a way to  
> do it with lattice?
> 
> Thanks for any help you may be able to provide.
> Sam Ferguson

The mixtures of tables and graphics we've produced are a bit different 
from the examples you gave but demonstrate the value of combining R and 
LaTeX.  R can produce LaTeX code containing LaTeX picture environments, 
for example.  That's how we put tiny high-resolution histograms inside 
tabular output showing descriptive statistics in the describe function 
and its latex method latex.describe in the Hmisc package.  Charles 
Thomas Dupont is working on a more impressive graphic inside a table by 
adding tiny dot charts showing proportions and confidence limits for 
differences in probabilities to the output produced by the latex method 
for Hmisc's summary.formula function.

An example of the first type may be found in 
http://biostat.mc.vanderbilt.edu/StatGraphCourse under "Mixing Text and 
Graphics" and we'll add an example of the second type soon.

LaTeX offers another approach: tables (matrices) of graphics in the 
tabular environment.

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From ripley at stats.ox.ac.uk  Fri Sep  1 16:02:40 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 1 Sep 2006 15:02:40 +0100 (BST)
Subject: [R] Local library under Windoze.
In-Reply-To: <200609011309.k81D9WaM015507@erdos.math.unb.ca>
References: <200609011309.k81D9WaM015507@erdos.math.unb.ca>
Message-ID: <Pine.LNX.4.64.0609011454070.11719@gannet.stats.ox.ac.uk>

This warning is indeed really an error, but do you want all your downloads 
to fail just because one does?  install.packages() behaves the same way on 
Unices.

I was not aware that this works with relative paths for any version of R.
Try using a full path, which always works for me.

If indeed your filesystem is readonly, you will have problems in spades.
But is that box just dark and not ticked?  (That's a lovely Windows 
gotcha that has caught people here many times.)


On Fri, 1 Sep 2006, Rolf Turner wrote:

> 
> Continuing to try to customize my environment for using R under
> Windoze, I experimented with installing a package from CRAN in a
> local library ``Lib''.
> 
> I created the directory ``Lib'' in the folder in which R starts,
> and then executed
> 
> 	> install.packages("abind",lib="Lib")
> 
> Everything went according to form (I got prompted to choose a mirror,
> etc.) until the end of the show when I got the warning message
> 
> Warning: unable to move temporary installation 'C:\Documents and
> Settings\rolf\My Documents\Rstuff\Lib\file5f906952\Lib\file5f906952\abind to
> 'C:\Documents and Settings\rolf\My Documents\Rstuff\Lib\file5f906952\Lib\abind'
> 
> [I have folded the foregoing warning --- which came out as a single line ---
> to make it fit in an 80 character wide screen.]
> 
> [I was also told that the ``downloaded packages are in ...'', and when
> I looked in the indicated folder the zip file was indeed there ---
> but a fat lot of good that does me.]
> 
> The warning was more like an *error*. When this had finished, the folder
> Lib was empty; no sign of the file5f906952 stuff, or anything else.
> 
> Can anyone explain to me what's going on/wrong?  There is no problem
> apparently if I do
> 
> 	> install.packages("abind")
> 
> which installs into the ``system'' library.  In current circumstances
> this is good enough --- since I have write permission on the ``system''
> library, I can just use that.  So this is, for the moment, an
> academic question.  Still the facility seems to be *there* for
> installing to a local user-owned library, and it seems not to be
> working for me, and I'd like to figure out why.
> 
> I thought for a moment that I'd found the problem a little while
> back, when I noticed that ``Lib'' was ``Readonly''.  But then
> when I tried to change that --- ``unclicking'' the Readonly box
> in the ``Properties'' of Lib --- I found that I couldn't.  When
> I looked at the Properties again, I found it was back to being
> Readonly again.  (With no warning or error message of any kind.)
> Some further investigation seemed to indicate that *all* folders
> are Readonly.  (Is this really as it should be?  And if so, what's
> the point of having this property for folders?)
> 
> Can anyone enlighten me?
> 
> 				cheers,
> 
> 					Rolf Turner
> 					rolf at math.unb.ca
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From jmacdon at med.umich.edu  Fri Sep  1 16:10:49 2006
From: jmacdon at med.umich.edu (James W. MacDonald)
Date: Fri, 01 Sep 2006 10:10:49 -0400
Subject: [R] Local library under Windoze.
In-Reply-To: <200609011309.k81D9WaM015507@erdos.math.unb.ca>
References: <200609011309.k81D9WaM015507@erdos.math.unb.ca>
Message-ID: <44F83F69.2060304@med.umich.edu>

Hi Rolf,

Rolf Turner wrote:
> Continuing to try to customize my environment for using R under
> Windoze, I experimented with installing a package from CRAN in a
> local library ``Lib''.
> 
> I created the directory ``Lib'' in the folder in which R starts,
> and then executed
> 
> 	> install.packages("abind",lib="Lib")

I don't think this is how you should do things, since R won't know about 
this library path. Instead, you should use .libPaths() to set your 
library path and then install using that. Note that you will likely need 
to put a call to .libPaths() into a .Rprofile file in order to have this 
set on startup.

 > dir.create("C:/Documents and Settings/Jim/newlib")
 > .libPaths("C:/Documents and Settings/Jim/newlib")
 > install.packages("zoo")
Warning in install.packages("zoo") : argument 'lib' is missing: using 
C:/Documents and Settings/Jim/newlib
trying URL 
'http://www.biometrics.mtu.edu/CRAN/bin/windows/contrib/2.3/zoo_1.2-0.zip'
Content type 'application/zip' length 724426 bytes
opened URL
downloaded 707Kb

package 'zoo' successfully unpacked and MD5 sums checked

The downloaded packages are in
         C:\WINDOWS\Temp\RtmpVcjtqb\downloaded_packages
updating HTML package descriptions
 > dir("C:/Documents and Settings/Jim/newlib")
[1] "zoo"

Then you can use install.packages("packagename", lib = .libPaths()[2]) 
if you want to use the 'stock' library directory, or just 
install.packages("packagename") to use your private one.

HTH,

Jim


> 
> Everything went according to form (I got prompted to choose a mirror,
> etc.) until the end of the show when I got the warning message
> 
> Warning: unable to move temporary installation 'C:\Documents and
> Settings\rolf\My Documents\Rstuff\Lib\file5f906952\Lib\file5f906952\abind to
> 'C:\Documents and Settings\rolf\My Documents\Rstuff\Lib\file5f906952\Lib\abind'
> 
> [I have folded the foregoing warning --- which came out as a single line ---
> to make it fit in an 80 character wide screen.]
> 
> [I was also told that the ``downloaded packages are in ...'', and when
> I looked in the indicated folder the zip file was indeed there ---
> but a fat lot of good that does me.]
> 
> The warning was more like an *error*. When this had finished, the folder
> Lib was empty; no sign of the file5f906952 stuff, or anything else.
> 
> Can anyone explain to me what's going on/wrong?  There is no problem
> apparently if I do
> 
> 	> install.packages("abind")
> 
> which installs into the ``system'' library.  In current circumstances
> this is good enough --- since I have write permission on the ``system''
> library, I can just use that.  So this is, for the moment, an
> academic question.  Still the facility seems to be *there* for
> installing to a local user-owned library, and it seems not to be
> working for me, and I'd like to figure out why.
> 
> I thought for a moment that I'd found the problem a little while
> back, when I noticed that ``Lib'' was ``Readonly''.  But then
> when I tried to change that --- ``unclicking'' the Readonly box
> in the ``Properties'' of Lib --- I found that I couldn't.  When
> I looked at the Properties again, I found it was back to being
> Readonly again.  (With no warning or error message of any kind.)
> Some further investigation seemed to indicate that *all* folders
> are Readonly.  (Is this really as it should be?  And if so, what's
> the point of having this property for folders?)
> 
> Can anyone enlighten me?
> 
> 				cheers,
> 
> 					Rolf Turner
> 					rolf at math.unb.ca
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


-- 
James W. MacDonald, M.S.
Biostatistician
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623


**********************************************************
Electronic Mail is not secure, may not be read every day, and should not be used for urgent or sensitive issues.


From ripley at stats.ox.ac.uk  Fri Sep  1 16:24:57 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 1 Sep 2006 15:24:57 +0100 (BST)
Subject: [R] embed image (png) in postscript (device)
In-Reply-To: <47fce0650609010456l5739f11yc63892e53b94cd61@mail.gmail.com>
References: <47fce0650609010456l5739f11yc63892e53b94cd61@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0609011330120.11719@gannet.stats.ox.ac.uk>

On Fri, 1 Sep 2006, Hans-Peter wrote:

> Hi,
> 
> I output multiple "grid-package-based" plots to the postscript device.
> Because the graphics are complicated and consists of a lot of datapoints
> (~200'000) the files become really big. To avoid this big files and to
> shorten the creation, I currently print the plots to the png device and
> manually combine them into one multipaged pdf document.
> 
> My question is, it is possible to do this in R directly?
> Probably I would have to create a png plot first, then reimport it into R
> and "put" it on the ps device. Is this a realistic way (in principle) ?
> 
> If there is no package to read png files (I didn't find anything), I
> probably could solve this. 

There are various ways to do so: one is to convert to a format pixmap 
knows, another to convert to TIFF and use rtiff.

> But I still don't know if it would be possible to
> "put" this memory representation to a ps device.

Only crudely.  There is nothing in the R graphics model related to 
bitmaps.  What you could do is draw each pixel as a rectangle (as image() 
does), but then your file size will be even bigger and the chances of it 
being rendered accurately are not high.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From vanderg at mail.fpg.unc.edu  Fri Sep  1 16:27:00 2006
From: vanderg at mail.fpg.unc.edu (Nathan Vandergrift)
Date: Fri, 01 Sep 2006 10:27:00 -0400
Subject: [R] xyplot background color
Message-ID: <44F84334.2030902@mail.fpg.unc.edu>

I've been unable to solve this problem using all sorts of variants of 
trellis.par.set, or par.settings within xyplot.

I need this plot not to be transparent, so when I drop the .ps into 
LaTeX it shows up on my black slides.

thanks.
nathan


traj.female<-read.delim("d:\TRAfemales.dat", header = TRUE, sep = "\t")

#This is the key to getting the right legend
trellis.par.set(superpose.line=list(lty=c(1,2,1,2,1,2),col=c(1,1,2,2,3,3)))
trellis.par.set(background=list("gray"))


# This works
female.traj<-xyplot(TRA~Grade, data=traj.female, groups=Group,
            type="l", lwd=2,
            lty=c(1,2,1,2,1,2),
            col=c(1,1,2,2,3,3),
            auto.key = list(
                lines=TRUE,
                points= FALSE,
                x = .25, y = .7, corner = c(0, 0), border=TRUE),
            xlab="Grade", ylab="TRA",
            ylim=c(-.5,6.5),xlim=c(0.5,6.5),
            panel=panel.superpose,
            #par.settings=list(background="white",transparent=FALSE)
        )
female.traj


From goedman at mac.com  Fri Sep  1 16:33:25 2006
From: goedman at mac.com (Rob J Goedman)
Date: Fri, 1 Sep 2006 07:33:25 -0700
Subject: [R] Problems with OS X R
In-Reply-To: <DC9588C8-7F2B-427A-8BCF-E3482F3D610C@umd.edu>
References: <DC9588C8-7F2B-427A-8BCF-E3482F3D610C@umd.edu>
Message-ID: <E94A1E9A-46F0-446B-8A7F-DE839D44F455@mac.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060901/e8578f29/attachment.pl 

From Ted.Harding at nessie.mcc.ac.uk  Fri Sep  1 16:56:54 2006
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 01 Sep 2006 15:56:54 +0100 (BST)
Subject: [R] Tables with Graphical Representations
In-Reply-To: <3941216D-6BCA-4156-8D7E-1656288D92E3@ihug.com.au>
Message-ID: <XFMail.060901155654.Ted.Harding@nessie.mcc.ac.uk>

On 31-Aug-06 Sam Ferguson wrote:
> Hi useRs -
> 
> I was wondering if anyone out there can tell me where to find
> R-code to do mixes of tables and graphics. I am thinking of
> something similar to this:
> http://yost.com/information-design/powerpoint-corrupts/
> or like the excel routines people are demonstrating:
> http://infosthetics.com/archives/2006/08/excel_in_cell_graphing.html
> 
> My aim is to provide small graphics to illustrate numbers directly  
> beside or behind their position in the table. Maybe there is a way
> to do it with lattice?
> 
> Thanks for any help you may be able to provide.
> Sam Ferguson

I dare say there may be a way to do that kind of thing directy within R,
and if so then the graphics experts will no doubt tell us how!

But your examples are just one kind of combined tabular/graphic layout
(and somewhat similar to each other). In a more general context of
combining tables of numerical results with graphic displays, it is
perhaps better to think in terms of using R to produce the numerical
results in the first instance, and then handing these over to software
designed for general-purpose graphical/textual layout. You then have
complete control, and full flexivility of design.

Indeed, in your second (Excel) example, the method of production is
just a nasty kludge -- and it was a happy coincidence that the "REPT"
function was available in Excel at all!

As Frank Harrell has just posted (just as I was completing this one!),
you can do this sort of thing in LaTex (his example shows little
histograms of the data, above each different tabular section). LaTex
is an example of software which allows you to create precisely formatted
graphics within precisely formatted text.

However, I'm no expert on LaTex, preferring what I've been used to for
too many years, namely Unix 'troff' and its more recent GNU implementation
'groff'.

As a preliminary, you will need to get R to output a suitable data
file, or a suitably composed data file with 'groff' formatting tags
interspersed. The latter should not be difficult, though my own approach
would be to simply take a data file of the form (for your first example
as taken from your URL):

"% survival / standard error" "5 year" "10 year" "15 year" "20 year"
"Prostate" 98.8 0.4 95.2 0.9 87.1 1.7 81.3 3.0
"Thyroid" 96.0 0.8 95.8 1.2 94.0 1.6 95.4 2.1
"Testis" 94.7 1.1 94.0 1.3 91.1 1.8 88.2 2.3
[...]

(which would be very straightforward in R) and then use say 'awk'
to compute 'groff' data with embedded tags (see below).

The file which I would then submit to 'groff' would look like



.ds RED "\X'ps: exec 1 0 0 setrgbcolor'
.ds GREY "\X'ps: exec 0.5 0.5 0.5 setrgbcolor'
.ds BLACK "\X'ps: exec 0 0 0 setrgbcolor'
.ds bx \x'-0.2m'\x'-0.2m'\v'0.2m'\Z'\
\*[RED]\D'P \\$1p 0 0 -1m -\\$1p 0 0 1m'\
'\
\Z'\
\h'\\$1p'\
\*[GREY]\D'P 0.5i-\\$1p 0 0 -1m \\$1p-0.5i 0 0 1m'\
'\h'0.5i'\
\v'-0.2m'\*[BLACK]
.LP
.TS
box tab(#);
c3 s1 s1w(0.5i) s s1 s1w(0.5i) s s1 s1w(0.5i) s s1 s1w(0.5i) s.

\f[BMB]\s[15]Estimated survival rates by cancer site\s0\fP

.T&
l c s s s s s s s s s s s.
#\fB\s[12]% survival / standard error\s0\fP
#\_
.T&
l c s s c s s c s s c s s.
#5 year#10 year#15 year#20 year
#\_#\_#\_#\_
.T&
l  n l n n c n n c n n c n.
Prostate#98.8#\*[bx 35.6]#0.4#95.2#\*[bx 34.3]#0.9#87.1#\
\*[bx 31.4]#1.7#81.3#\*[bx 29.3]#3.0
Thyroid#96.0#\*[bx 34.6]#0.8#95.8#\*[bx 34.5]#1.2#94.0#\
\*[bx 33.8]#1.6#95.4#\*[bx 34.3]#2.1
Testis#94.7#\*[bx 34.1]#1.1#94.0#\*[bx 33.8]#1.3#91.1#\
\*[bx 32.8]#1.8#88.2#\*[bx 31.8]#2.3
[...]
Pancreas#4.0#\*[bx 1.4]#0.5#3.0#\*[bx 1.1]#1.5#2.7#\
\*[bx 1.0]#0.6#2.7#\*[bx 1.0]#0.8

.TE



The key here is to define a "parametrised string" which will
be invoked as "\*[bx <number>]". The is the main "embedded tag".

Each box is 0.5 inch wide (36 points), and consists of a lefthand
section in Red which width is 36*percent/100 points, with a
rigthand section in Grey whose width is 36*(1 - percent/100) points.
The height of the box is 1 em (which, in points, is the point-size
of the current font), and the box has been shifted downwards slightly
(0.2 2m) to align it nicely with the text. The parameter "<number>"
in "\*[bx <number>]" is the value of 36*percent/100. So this can, for
instance, be easily computed in an 'awk' run.

The block of "code"

.ds bx \x'-0.2m'\x'-0.2m'\v'0.2m'\Z'\
\*[RED]\D'P \\$1p 0 0 -1m -\\$1p 0 0 1m'\
'\
\Z'\
\h'\\$1p'\
\*[GREY]\D'P 0.5i-\\$1p 0 0 -1m \\$1p-0.5i 0 0 1m'\
'\h'0.5i'\
\v'-0.2m'\*[BLACK]

defines the tag "\*[bx ...]", which is responsible for drawing the
graphical item ion the table wherever it is invoked. Initailly it
is padded above an below with a bit of extra space ("\x...") and
moved down slightly ("\v'0.2m'"), then colour changes to Red and
a filled Red polygon is drawn; then the drawing point is shifted
and a filled Grey polygon is drawn. Finally the colour is changed
back to Black for the text part of the Table. The value of "<number>"
is substituted for "\\$1" wherever this occurs in the definition
of "bx".

The line ".TS" leads in to a Table definition, which ends with ".TE".
The next few lines specifiy table layout (types, spacings and
widths of columns, cell separator "#", etc.); and then come the
data for each line of the table, in which the box tag "\*[bx ...]"
occurs where needed. As indicated above, the full table data could
probably be easily computed in R and can certainly be easily done
in 'awk' or 'perl'.

After all that, the result is quite pleasing -- and, when I compare
it with the graph shown on Sam's URL, it seems to me to represent
the numbers much more accurately, as well as being visually slightly
more expressive.

It would also be quite feasible to "complicate" the graphics with
indications of SE etc., by adding more to the definition of \*[bx ...].

I have looked at the "LaTeX file produced by lstex.describe" for
Frank Harrell's example. Granting that it has no doubt been automatically
produced, it is enormous and, for practical purposes, uneditable if
you want to tweak features of the display. It would be interesting
to see what had to be down further back up the line to produce it;
this might be, of course, much easier to tweak. On the other hand,
my 'groff source' file above is compact and easily changed.

If anyone would like to look at the output I have produced by the
above method (PDF file), and the full groff source file, drop me a
line (I'll send them privately to Sam anyway).

Best wishes to all,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 01-Sep-06                                       Time: 15:56:46
------------------------------ XFMail ------------------------------


From leog at anicca-vijja.de  Fri Sep  1 17:02:54 2006
From: leog at anicca-vijja.de (=?ISO-8859-15?Q?Leo_G=FCrtler?=)
Date: Fri, 01 Sep 2006 17:02:54 +0200
Subject: [R] integration problem with gamma function
Message-ID: <44F84B9E.7020306@anicca-vijja.de>

Dear R-list members,

I have a problem with translating a mathematica script into R. The whole 
script is at the end of the email (with initial values for easy 
reproduction) and can be pasted directly into R. The problematic part 
(which is included below of course) is

<--- Original Mathematica --->
(* p_svbar *)
UiA  = Ni (Dsi - 2Di A + A^2)/2;
UiiA = Nii (Dsii - 2Dii A + A^2)/2;
psvbar = NIntegrate[1/(UiA^(Ni/2)) 1/(UiiA^(Nii/2))
    Gamma[Ni/2,UiA/(sH^2),UiA/(sL^2)]
    Gamma[Nii/2,UiiA/(sH^2),UiiA/(sL^2)],{A,L,H},
    MinRecursion->3];
PSVbar = psvbar/(4 Log[sH/sL]);
Print["p(s?v|D_1D_2I)  = const. ",N[PSVbar,6]];
</--->

<--- translation to R --->
integpsvbar <- function(A)
{
# Mathematica: gamma[a,z0,z1] = gamma[a,z1] - gamma[a,z0]
 UiA <- Ni*(Dsi-2*Di*A+A^2)/2
 UiiA <- Nii*(Dsii-2*Dii*A+A^2)/2
 1/UiA^(Ni/2) *
 1/UiiA^(Nii/2) *
 ( pgamma(UiA/(sL^2), Ni/2) - pgamma(UiA/(sH^2), Ni/2) ) *
 ( pgamma(UiiA/(sL^2), Nii/2) - pgamma(UiiA/(sH^2), Nii/2) )
}

psvbar <- integrate(integpsvbar, lower=L, upper=H)
psvbar$value
PSVbar <- psvbar$value / (4*log(sH/sL))
PSVbar <- PSVbar * sqrt(pi) * sqrt(pi)      #note: two times pgamma in 
integration function above... correct it with two times multiplied by 
'sqrt(pi)' due to differences in defining incomplete gamma function 
between R and mathematica
print(paste("p(s?v|D_1D_2I)  = const. ",PSVbar,sep=""))
</--->

According to Mathematica

psvbar ~ 1.72026 * 10^20
PSVbar ~ 5.24827 * 10^19

The R part produces

psvbar ~ 1.824023 * 10^13
PSVbar ~ 17482463305549.6



The script produces proper results untill the problematic part, so it 
seems quite reasonable that no other error before is responsible for the 
difference of the results.
I assume that I did something wrong with the Gamma function.

The "generalized incomplete gamma function" in Mathematica (Wolfram, 
p.363f.) is defined as

Gamma[a, z0, z1] := Gamma[a, z1] - Gamma[a, z0]

with

Gamma[a, z0, z1] := integral_z0^z1 t^(a-1) exp(-t) dt

Please note: I asked a quite similar question (thread from 07-August/ 
08-August 2006) for which Martin M?chler (08-08-2006 13:53) pointed out, 
that the result has to be multiplied by "sqrt(pi)" because of (-> 
manpage pgamma) differences in formulating the incomplete gamma function 
between Mathematica and R. However, I tried this in various ways and - 
at least for me - it does not help. Thus, I assume it is another issue.

Thank you very much,

best wishes
leo g?rtler

now the R script to reproduce (can be pasted directly into R):

####################
# R-Portierung aus Mathematica (Urban Studer, 90er)
# Ursprung: G.L. Bretthorst "On the difference of means"
# zuerst: 12-06-05
# zuletzt: 21-06-06

# --------------------------------------------------
# Success rates and integration bounds in the case
# of the (conservative) Bayes-Laplace prior
# --------------------------------------------------





SucRatesIntBounds <- function(Ni, Si, Nii, Sii, smin)
{
# BEGIN BLOCK
# necessary variables:
# {Nmax, Nmin}

### defintion of constants
Di <- (Si + 1) / (Ni + 2)
si <- sqrt(Di * (1 - Di) / (Ni + 3))
Dii <- (Sii + 1) / (Nii + 2)
sii <- sqrt(Dii * (1 - Dii) / (Nii + 3))

Nmax <- max(Ni, Nii)
Nmin <- min(Ni, Nii)
sL <- floor(1000 * sqrt((Nmax+1) / (Nmax+3)) / (Nmax + 2)) / 1000
sH <- ceiling(1000 / (2 * sqrt(Nmin + 3))) / 1000
L <- floor(100 * (smin + 1) / (Nmax + 2)) / 100
H <- 1 - L

return(c(Di, si, Dii, sii, sL, sH, L, H))
}
#END BLOCK
# --------------------------------------------------


Si <- 11
Ni <- 15
Sii <- 10
Nii <- 16
smin <- 0
res1 <- SucRatesIntBounds(Ni, Si, Nii, Sii, smin)
res1
#return(c(Di, si, Dii, sii, sL, sH, L, H))

names(res1) <- c("Di","si","Dii","sii","sL","sH","L","H")
res1

Di <- res1[1]
si <- res1[2]
Dii <- res1[3]
sii <- res1[4]
sL <- res1[5]
sH <- res1[6]
L <- res1[7]
H <- res1[8]


# --------------------------------------------------
# Begin: ON THE DIFFERENCE IN MEANS
# --------------------------------------------------

#DiffinMeans <- function(Ni, Di, si, Nii, Dii, sii, L, H, sL, sH)
## BEGIN BLOCK
#{

# necessary variables in the function
#{ NN, DD, Dsi, Dsii, DsD, ss,
#  dd, lownum, upnum, low, up, psv, PSV,
#  zz, lowinum, upinum, lowiinum, upiinum, psbarv, PSbarV,
#  psvbar, PSVbar, psbarvbar, PSbarVbar, cc,
#  sv, sbarv, svbar, sbarvbar,
#  samemeans, diffmeans, samevars, diffvars, diffsets },



### defintion of constants
NN <- Ni+Nii
DD <- (Ni * Di + Nii * Dii) / NN
Dsi <- (Ni-1) / Ni * si^2 + Di^2
Dsii <- (Nii-1) / Nii * sii^2 + Dii^2
DsD <- (Ni * Dsi + Nii * Dsii) / NN
ss <- sqrt(NN * (DsD - DD^2) / (NN-1))


### descriptive statistics
print(" \n------------- Data ---------------------------------------\n")
print(paste("N_1 = ",Ni ," :     Mean_1 ? s_1    = ", Di," ? ",si, sep=""))
print(paste("N_2 = ",Nii," :     Mean_2 ? s_2    = ", Dii," ? ",sii, 
sep=""))
print(paste("N   = ", NN ," :  Mean_comb ? s_comb = ", DD," ? ",ss, sep=""))
print(" ")
print(paste("s_L = ",sL,", s_H = ",sH,";  L = ",L, ", H = ",H,"  (smin = 
",smin,")",sep=""))
if(L < DD)
  {
   print(paste("L - Mean_comb < 0 : ", (L<DD), "\n", "             (-> 
'+'-sign between Gamma-fcts o.k.)", sep=""))
  } else print(paste("L - Mean_comb < 0 : ", (L<DD), "\n", "             
(-> '+'-sign between Gamma-fcts false!)", sep=""))
print(paste(" \n ", sep=""))


#(* p_sv *)
dd <- NN * (DsD - DD^2)
lownum <- NN * (L-DD)^2
upnum  <- NN * (H-DD)^2

#low = lownum / (2*s^2)
#up  = upnum / (2*s^2)
integpsv <- function(s)
{ 1 / (s^NN) * exp(-dd / (2 * s^2)) *
  ( pgamma(upnum/(2*s^2), 1/2) + pgamma(lownum/(2*s^2), 1/2) )
}
psv <- integrate(integpsv, lower=sL, upper=sH)
#???    (* + if (L-DD) < 0 *)
PSV <- psv$value / sqrt(2*NN) * sqrt(pi)    # normalizing factor due to R
# Gamma(1/2)= sqrt(pi)
# psv ~ 1.39848 * 10^20
# ~ 3.44715 * 10 ^20
# ~ 5.24827 * 10 ^20
# ~ 1.52788 * 10 ^20
print("------------- Results ------------------------------------\n")
print(paste("p(sv|D_1D_2I)   = const. ",PSV, sep=""))


### p_sbarv
zz <- Ni * (Dsi - Di^2) + Nii * (Dsii - Dii^2)
lowinum <- Ni * (L - Di)^2
upinum <- Ni * (H - Di)^2
lowiinum <- Nii * (L - Dii)^2
upiinum <- Nii * (H - Dii)^2

#lowi <- lowinum / (2*s^2)
#upi  <- upinum / (2*s^2)
#lowii <- lowiinum / (2*s^2)
#upii <- upiinum / (2*s^2)
integpsbarv <- function(s)
{
 1/(s^(NN-1)) * exp(-zz/(2*s^2)) *
 ( pgamma(upinum/(2*s^2), 1/2) + pgamma(lowinum/(2*s^2), 1/2) ) *
 ( pgamma(upiinum/(2*s^2), 1/2) + pgamma(lowiinum/(2*s^2), 1/2) )
}
psbarv <- integrate(integpsbarv, lower=sL, upper=sH)
#??? (* + if (L-DD) < 0 *)
PSbarV <- psbarv$value / (2*(H-L) * sqrt(Ni*Nii)) * sqrt(pi) * sqrt(pi)
# 2mal mit sqrt(pi) mal nehmen = "* pi", weil zei pgamma-Ausdr?cke 
enthalten sind in der Integration
print(paste("p(?sv|D_1D_2I)  = const. ",PSbarV, sep=""))

###############ALL ABOVE PRODUCES PROPER RESULTS COMPARED TO
###############MATHEMATICA SCRIPT

###############PROBLEMATIC PART STARTS HERE#################  
### p_svbar
###
# Mathematica
#(* p_svbar *)
#UiA  = Ni (Dsi - 2Di A + A^2)/2;
#UiiA = Nii (Dsii - 2Dii A + A^2)/2;
#psvbar = NIntegrate[1/(UiA^(Ni/2)) 1/(UiiA^(Nii/2))
#   Gamma[Ni/2,UiA/(sH^2),UiA/(sL^2)]
#   Gamma[Nii/2,UiiA/(sH^2),UiiA/(sL^2)],{A,L,H},
#    MinRecursion->3];
#PSVbar = psvbar/(4 Log[sH/sL]);
#Print["p(s?v|D_1D_2I)  = const. ",N[PSVbar,6]];

###R trial
integpsvbar <- function(A)
{
# Mathematica: gamma[a,z0,z1] = gamma[a,z1] - gamma[a,z0]
 UiA <- Ni*(Dsi-2*Di*A+A^2)/2
 UiiA <- Nii*(Dsii-2*Dii*A+A^2)/2
 1/UiA^(Ni/2) *
 1/UiiA^(Nii/2) *
 ( pgamma(UiA/(sL^2), Ni/2) - pgamma(UiA/(sH^2), Ni/2) ) *
 ( pgamma(UiiA/(sL^2), Nii/2) - pgamma(UiiA/(sH^2), Nii/2) )
}

psvbar <- integrate(integpsvbar, lower=L, upper=H)
psvbar$value
PSVbar <- psvbar$value / (4*log(sH/sL)) * sqrt(pi) * sqrt(pi) # two 
times sqrt(pi) to correct for differences between R and mathematica
print(paste("p(s?v|D_1D_2I)  = const. ",PSVbar,sep=""))


From f.harrell at vanderbilt.edu  Fri Sep  1 17:06:29 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 01 Sep 2006 10:06:29 -0500
Subject: [R] Tables with Graphical Representations
In-Reply-To: <XFMail.060901155654.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.060901155654.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <44F84C75.1090100@vanderbilt.edu>

(Ted Harding) wrote:
> On 31-Aug-06 Sam Ferguson wrote:
>> Hi useRs -
>>
>> I was wondering if anyone out there can tell me where to find
>> R-code to do mixes of tables and graphics. I am thinking of
>> something similar to this:
>> http://yost.com/information-design/powerpoint-corrupts/
>> or like the excel routines people are demonstrating:
>> http://infosthetics.com/archives/2006/08/excel_in_cell_graphing.html
>>
>> My aim is to provide small graphics to illustrate numbers directly  
>> beside or behind their position in the table. Maybe there is a way
>> to do it with lattice?
>>
>> Thanks for any help you may be able to provide.
>> Sam Ferguson
> 
> I dare say there may be a way to do that kind of thing directy within R,
> and if so then the graphics experts will no doubt tell us how!
> 
> But your examples are just one kind of combined tabular/graphic layout
> (and somewhat similar to each other). In a more general context of
> combining tables of numerical results with graphic displays, it is
> perhaps better to think in terms of using R to produce the numerical
> results in the first instance, and then handing these over to software
> designed for general-purpose graphical/textual layout. You then have
> complete control, and full flexivility of design.
> 
> Indeed, in your second (Excel) example, the method of production is
> just a nasty kludge -- and it was a happy coincidence that the "REPT"
> function was available in Excel at all!
> 
> As Frank Harrell has just posted (just as I was completing this one!),
> you can do this sort of thing in LaTex (his example shows little
> histograms of the data, above each different tabular section). LaTex
> is an example of software which allows you to create precisely formatted
> graphics within precisely formatted text.
> 
> However, I'm no expert on LaTex, preferring what I've been used to for
> too many years, namely Unix 'troff' and its more recent GNU implementation
> 'groff'.
> 
> As a preliminary, you will need to get R to output a suitable data
> file, or a suitably composed data file with 'groff' formatting tags
> interspersed. The latter should not be difficult, though my own approach
> would be to simply take a data file of the form (for your first example
> as taken from your URL):
> 
> "% survival / standard error" "5 year" "10 year" "15 year" "20 year"
> "Prostate" 98.8 0.4 95.2 0.9 87.1 1.7 81.3 3.0
> "Thyroid" 96.0 0.8 95.8 1.2 94.0 1.6 95.4 2.1
> "Testis" 94.7 1.1 94.0 1.3 91.1 1.8 88.2 2.3
> [...]
> 
> (which would be very straightforward in R) and then use say 'awk'
> to compute 'groff' data with embedded tags (see below).
> 
> The file which I would then submit to 'groff' would look like
> 
> 
> 
> .ds RED "\X'ps: exec 1 0 0 setrgbcolor'
> .ds GREY "\X'ps: exec 0.5 0.5 0.5 setrgbcolor'
> .ds BLACK "\X'ps: exec 0 0 0 setrgbcolor'
> .ds bx \x'-0.2m'\x'-0.2m'\v'0.2m'\Z'\
> \*[RED]\D'P \\$1p 0 0 -1m -\\$1p 0 0 1m'\
> '\
> \Z'\
> \h'\\$1p'\
> \*[GREY]\D'P 0.5i-\\$1p 0 0 -1m \\$1p-0.5i 0 0 1m'\
> '\h'0.5i'\
> \v'-0.2m'\*[BLACK]
> .LP
> .TS
> box tab(#);
> c3 s1 s1w(0.5i) s s1 s1w(0.5i) s s1 s1w(0.5i) s s1 s1w(0.5i) s.
> 
> \f[BMB]\s[15]Estimated survival rates by cancer site\s0\fP
> 
> .T&
> l c s s s s s s s s s s s.
> #\fB\s[12]% survival / standard error\s0\fP
> #\_
> .T&
> l c s s c s s c s s c s s.
> #5 year#10 year#15 year#20 year
> #\_#\_#\_#\_
> .T&
> l  n l n n c n n c n n c n.
> Prostate#98.8#\*[bx 35.6]#0.4#95.2#\*[bx 34.3]#0.9#87.1#\
> \*[bx 31.4]#1.7#81.3#\*[bx 29.3]#3.0
> Thyroid#96.0#\*[bx 34.6]#0.8#95.8#\*[bx 34.5]#1.2#94.0#\
> \*[bx 33.8]#1.6#95.4#\*[bx 34.3]#2.1
> Testis#94.7#\*[bx 34.1]#1.1#94.0#\*[bx 33.8]#1.3#91.1#\
> \*[bx 32.8]#1.8#88.2#\*[bx 31.8]#2.3
> [...]
> Pancreas#4.0#\*[bx 1.4]#0.5#3.0#\*[bx 1.1]#1.5#2.7#\
> \*[bx 1.0]#0.6#2.7#\*[bx 1.0]#0.8
> 
> .TE
> 
> 
> 
> The key here is to define a "parametrised string" which will
> be invoked as "\*[bx <number>]". The is the main "embedded tag".
> 
> Each box is 0.5 inch wide (36 points), and consists of a lefthand
> section in Red which width is 36*percent/100 points, with a
> rigthand section in Grey whose width is 36*(1 - percent/100) points.
> The height of the box is 1 em (which, in points, is the point-size
> of the current font), and the box has been shifted downwards slightly
> (0.2 2m) to align it nicely with the text. The parameter "<number>"
> in "\*[bx <number>]" is the value of 36*percent/100. So this can, for
> instance, be easily computed in an 'awk' run.
> 
> The block of "code"
> 
> .ds bx \x'-0.2m'\x'-0.2m'\v'0.2m'\Z'\
> \*[RED]\D'P \\$1p 0 0 -1m -\\$1p 0 0 1m'\
> '\
> \Z'\
> \h'\\$1p'\
> \*[GREY]\D'P 0.5i-\\$1p 0 0 -1m \\$1p-0.5i 0 0 1m'\
> '\h'0.5i'\
> \v'-0.2m'\*[BLACK]
> 
> defines the tag "\*[bx ...]", which is responsible for drawing the
> graphical item ion the table wherever it is invoked. Initailly it
> is padded above an below with a bit of extra space ("\x...") and
> moved down slightly ("\v'0.2m'"), then colour changes to Red and
> a filled Red polygon is drawn; then the drawing point is shifted
> and a filled Grey polygon is drawn. Finally the colour is changed
> back to Black for the text part of the Table. The value of "<number>"
> is substituted for "\\$1" wherever this occurs in the definition
> of "bx".
> 
> The line ".TS" leads in to a Table definition, which ends with ".TE".
> The next few lines specifiy table layout (types, spacings and
> widths of columns, cell separator "#", etc.); and then come the
> data for each line of the table, in which the box tag "\*[bx ...]"
> occurs where needed. As indicated above, the full table data could
> probably be easily computed in R and can certainly be easily done
> in 'awk' or 'perl'.
> 
> After all that, the result is quite pleasing -- and, when I compare
> it with the graph shown on Sam's URL, it seems to me to represent
> the numbers much more accurately, as well as being visually slightly
> more expressive.
> 
> It would also be quite feasible to "complicate" the graphics with
> indications of SE etc., by adding more to the definition of \*[bx ...].
> 
> I have looked at the "LaTeX file produced by lstex.describe" for
> Frank Harrell's example. Granting that it has no doubt been automatically
> produced, it is enormous and, for practical purposes, uneditable if
> you want to tweak features of the display. It would be interesting
> to see what had to be down further back up the line to produce it;
> this might be, of course, much easier to tweak. On the other hand,
> my 'groff source' file above is compact and easily changed.
> 
> If anyone would like to look at the output I have produced by the
> above method (PDF file), and the full groff source file, drop me a
> line (I'll send them privately to Sam anyway).
> 
> Best wishes to all,
> Ted.
>
Ted - neat stuff - my aim is to not have to edit the LaTeX at all, i.e., 
to keep tuning the R code that produces LaTeX.  Your ideas also make me 
think of Xfig and something involving the R xfig driver.

Frank


From sue at xlsolutions-corp.com  Fri Sep  1 17:37:32 2006
From: sue at xlsolutions-corp.com (Sue Turner)
Date: Fri, 01 Sep 2006 08:37:32 -0700
Subject: [R] (1) R/Splus Advanced Programming, NYC,
	Sept 11-12   (2) Regression Modeling Strategies in R/Splus,
	Sept 28-29
Message-ID: <20060901083731.9f08cc34deb45d78e54b3b5664e21546.8e3e678707.wbe@email.secureserver.net>

XLSolutions Corporation (www.xlsolutions-corp.com) R/Splus Advanced
programming course taught the R Development Core Team is almost full!

(1) R/Splus Advanced Programming  --- by the R Development Core Team
Guru! 
  
                   http://www.xlsolutions-corp.com/Radv.htm 
  
                     *** New York City / September 11-12, 2006 *** 
  
(2) Regression Modeling Strategies in R/Splus --- by Prof Frank Harrell 
  
                    http://www.xlsolutions-corp.com/Rstats2.htm 
                           
                        *** Washington DC, September 28-29, 2006 *** 
  
(3) R/Splus Fundamentals and Programming Techniques  
           
                  http://www.xlsolutions-corp.com/Rfund.htm 
  
                    *** Raleigh / September 12-13, 2006 *** 
  
Ask for group discount and reserve your seat Now - Earlybird Rates 
Payment due after the class! Email Sue Turner:  sue at xlsolutions-corp.com

  
Email us for group discounts 
Phone:  206 686 1578 
  
Visit us: www.xlsolutions-corp.com/training.htm 
  
Please let us know if you and your colleagues are interested in this
class to take advantage of group discount. Register now to secure your
seat! 
  
Cheers, 
  
Elvis Miller, PhD 
Manager Training 
XLSolutions Corporation 
206 686 1578 
www.xlsolutions-corp.com/training.htm 
elvis at xlsolutions-corp.com


From maechler at stat.math.ethz.ch  Fri Sep  1 17:51:33 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 1 Sep 2006 17:51:33 +0200
Subject: [R] S4 Method Dispatch for Sealed Classes
In-Reply-To: <44F71029.404@epigenomics.com>
References: <44F71029.404@epigenomics.com>
Message-ID: <17656.22277.828949.269544@stat.math.ethz.ch>

>>>>> "Thomas" == Thomas Koenig <thomas.koenig at epigenomics.com>
>>>>>     on 31 Aug 2006 18:36:57 +0200 writes:

    Thomas> I encounter a problem with method dispatch with S4
    Thomas> classes, using the 'sealed' parameter in setClass.

  [...................]
  [...................]

    Thomas> Tried on R 2.3.1 and R 2.4.0-devel (2006-08-29 r39012): same result.

Sorry for not being really helpful, 
but there have been substantial S4 related changes to R-devel
since r39012 -- particularly about method dispatch.

Could you also try to use last night's r39045 (or something newer)?

There are several R developers who are really eager testing the
new S4 code inside R-devel.
For those R-help readers not subscribed to R-devel and interested
it this: Please look at
  https://stat.ethz.ch/pipermail/r-devel/2006-August/039088.html
and consider reading the "accompanying" paper by John Chambers
"How S4 Methods Work" .

Followups on this topic do rather belong to R-devel than R-help
I think.

Martin


From gunter.berton at gene.com  Fri Sep  1 18:14:43 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 1 Sep 2006 09:14:43 -0700
Subject: [R] New quote ?
In-Reply-To: <Pine.LNX.4.64.0609011438200.11719@gannet.stats.ox.ac.uk>
Message-ID: <001a01c6cde1$bc927b90$711f210a@gne.windows.gene.com>


Is this a candidate for R's package of wise quotes (whose name I've
forgotten and can't find at the moment)?

"The point here is that all but the most 
uselss variables will measurably improve the fit in large problems with 
few variables."

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From heberto.ghezzo at mcgill.ca  Fri Sep  1 18:20:02 2006
From: heberto.ghezzo at mcgill.ca (R Heberto Ghezzo, Dr)
Date: Fri, 1 Sep 2006 12:20:02 -0400
Subject: [R] Compiling a package
Message-ID: <05BE78B0CF1BBC4BBA4AA255568D8611029A9964@EXCHANGE2VS1.campus.mcgill.ca>

Hello,
I am in Win-XP R:2.3.0
latest rtools and Perl - of today
I got Rcmdr.HH source code and tried to compile it myself
copy all directory to R/R-2.3.0/src/library/Rcmdr.HH
from R/R-2.3.0/src/library
I typed:
..\..\bin\R CMD build --force --binary --auto-zip Rcmdr.HH
* checking for file 'Rcmdr.HH/DESCRIPTION' ... OK
* preparing 'Rcmdr.HH':
* checking DESCRIPTION meta-information ... OK
* removing junk files
Error: cannot open file 'Rcmdr.HH/DESCRIPTION' for reading

file Rcmdr.HH/DESCRIPTION is there,

Package: Rcmdr.HH
Type: Package
Title: Rcmdr support for the introductory course at Temple University.
Version: 1.2
Date: 2006-08-04
Author: Richard M. Heiberger, with contributions from Burt Holland.
Maintainer: Richard M. Heiberger <rmh at temple.edu>
Depends: R (>= 2.1.0), Rcmdr, car, multcomp, leaps, lattice, grid
Description: Our introductory course spends time on several topics
        that are not yet in the R Commander.  Therefore we wrote the menu
        items and make them available.
License: GPL version 2 or newer
Packaged: Fri Aug  4 03:57:20 2006; rmh

 is read/write and after all the authors also have a Rcmdr.HH.zip which means that they compiled without errors.

Can someone suggest why I can not compile the file?
I have another version with some modifications and additions which is the one I really want to compile, but if I can not compile the original. . . .
Thanks for any help
H.Ghezzo
McGill University
Canada


From spencer.graves at pdf.com  Fri Sep  1 18:22:36 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 01 Sep 2006 09:22:36 -0700
Subject: [R] New quote ?
In-Reply-To: <001a01c6cde1$bc927b90$711f210a@gne.windows.gene.com>
References: <001a01c6cde1$bc927b90$711f210a@gne.windows.gene.com>
Message-ID: <44F85E4C.6020103@pdf.com>

"fortunes" maintained by Achim Zeileis <Achim.Zeileis at R-project.org>

Berton Gunter wrote:
> Is this a candidate for R's package of wise quotes (whose name I've
> forgotten and can't find at the moment)?
>
> "The point here is that all but the most 
> uselss variables will measurably improve the fit in large problems with 
> few variables."
>
>


From Greg.Snow at intermountainmail.org  Fri Sep  1 18:25:55 2006
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Fri, 1 Sep 2006 10:25:55 -0600
Subject: [R] Tables with Graphical Representations
Message-ID: <07E228A5BE53C24CAD490193A7381BBB591182@LP-EXCHVS07.CO.IHC.COM>

The LaTeX or other solutions suggested are probably best, but here is a
way to do it using only R base graphics (the below code is to get you
started, some graphical parameters need to be set to get the spacing to
look better):

 tmp <- structure(list(Cancer = structure(as.integer(c(19, 23, 22, 13, 
2, 7, 5, 24, 3, 9, 20, 8, 4, 15, 16, 17, 10, 1, 14, 21, 12, 6, 
11, 18)), .Label = c("Brain, nervous system", "Breast", "Cervis, uteri",

"Colon", "Corpus uteri, uterus", "Esophagus", "Hodgkin's Disease", 
"Kidney, renal pelvis", "Larynx", "Leukemia", "Liver, bile duct", 
"Lung and bronchus", "Melanomas", "Multiple myeloma", "Non-Hodgkin's", 
"Oral cavity, pharynx", "Ovary", "Pancreas", "Prostate", "Rectum", 
"Stomach", "Testis", "Thyroid", "Urinary, bladder"), class = "factor"), 
    p5 = c(98.8, 96, 94.7, 89, 86.4, 85.1, 84.3, 82.1, 70.5, 
    68.8, 62.6, 61.8, 61.7, 57.8, 56.7, 55, 42.5, 32, 29.5, 23.8, 
    15, 14.2, 7.5, 4), s5 = c(0.4, 0.8, 1.1, 0.8, 0.4, 1.7, 1, 
    1, 1.6, 2.1, 1.2, 1.3, 0.8, 1, 1.3, 1.3, 1.2, 1.4, 1.6, 1.3, 
    0.4, 1.4, 1.1, 0.5), p10 = c(95.2, 95.8, 94, 86.7, 78.3, 
    79.8, 83.2, 76.2, 64.1, 56.7, 55.2, 54.4, 55.4, 46.3, 44.2, 
    49.3, 32.4, 29.2, 12.7, 19.4, 10.6, 7.9, 5.8, 3), s10 = c(0.9, 
    1.2, 1.3, 1.1, 0.6, 2, 1.3, 1.4, 1.8, 2.5, 1.4, 1.6, 1, 1.2, 
    1.4, 1.6, 1.3, 1.5, 1.5, 1.4, 0.4, 1.3, 1.2, 1.5), p15 = c(87.1, 
    94, 91.1, 83.5, 71.3, 73.8, 80.8, 70.3, 62.8, 45.8, 51.8, 
    49.8, 53.9, 38.3, 37.5, 49.9, 29.7, 27.6, 7, 19, 8.1, 7.7, 
    6.3, 2.7), s15 = c(1.7, 1.6, 1.8, 1.5, 0.7, 2.4, 1.7, 1.9, 
    2.1, 2.8, 1.8, 2, 1.2, 1.4, 1.6, 1.9, 1.5, 1.6, 1.3, 1.7, 
    0.4, 1.6, 1.5, 0.6), p20 = c(81.3, 95.4, 88.2, 82.8, 65, 
    67.1, 79.2, 67.9, 60, 37.8, 49.2, 47.3, 52.3, 34.3, 33, 49.6, 
    26.2, 26.1, 4.8, 14.9, 6.5, 5.4, 7.6, 2.7), s20 = c(3, 2.1, 
    2.3, 1.9, 0.7, 2.8, 2, 2.4, 2.4, 3.1, 2.3, 2.6, 1.6, 1.7, 
    1.8, 2.4, 1.7, 1.9, 1.5, 1.9, 0.4, 2, 2, 0.8)), .Names = c("Cancer",

"p5", "s5", "p10", "s10", "p15", "s15", "p20", "s20"), row.names =
c("1", 
"2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", 
"14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24"
), class = "data.frame")

layout( matrix( c(1,1,2,3,4,5), nrow=1) )
barplot( rep(0, length(tmp$Cancer)), horiz=T, xaxt='n', space=.5,
	name=as.character(tmp$Cancer),las=1,cex.names=1)

ypos <- barplot( rbind(tmp$p5, 100-tmp$p5), horiz=T, xaxt='n', 
	space=.5, names=tmp$p5, cex.names=1, las=1)
title('5 year Survival',cex=.9)
axis(4, at=ypos, labels=tmp$s5, las=1, cex=.7,tick=F)

ypos <- barplot( rbind(tmp$p10, 100-tmp$p10), horiz=T, xaxt='n', 
	space=.5, names=tmp$p10, cex.names=1, las=1)
title('10 year Survival',cex=.9)
axis(4, at=ypos, labels=tmp$s10, las=1, cex=.7,tick=F)

ypos <- barplot( rbind(tmp$p15, 100-tmp$p15), horiz=T, xaxt='n', 
	space=.5, names=tmp$p15, cex.names=1, las=1)
title('15 year Survival',cex=.9)
axis(4, at=ypos, labels=tmp$s15, las=1, cex=.7,tick=F)

ypos <- barplot( rbind(tmp$p20, 100-tmp$p20), horiz=T, xaxt='n', 
	space=.5, names=tmp$p20, cex.names=1, las=1)
title('20 year Survival',cex=.9)
axis(4, at=ypos, labels=tmp$s20, las=1, cex=.7,tick=F)




-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Sam Ferguson
Sent: Thursday, August 31, 2006 5:50 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Tables with Graphical Representations

Hi useRs -

I was wondering if anyone out there can tell me where to find R-code to
do mixes of tables and graphics. I am thinking of something similar to
this:
http://yost.com/information-design/powerpoint-corrupts/
or like the excel routines people are demonstrating:
http://infosthetics.com/archives/2006/08/excel_in_cell_graphing.html

My aim is to provide small graphics to illustrate numbers directly
beside or behind their position in the table. Maybe there is a way to do
it with lattice?

Thanks for any help you may be able to provide.
Sam Ferguson

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ffenics2002 at yahoo.co.uk  Fri Sep  1 18:42:44 2006
From: ffenics2002 at yahoo.co.uk (Ffenics)
Date: Fri, 1 Sep 2006 17:42:44 +0100 (BST)
Subject: [R] repeating the same procedure with a number of files within a
	directory
Message-ID: <20060901164244.10797.qmail@web25507.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060901/2fa4d7a4/attachment.pl 

From henrik.parn at bio.ntnu.no  Fri Sep  1 18:52:27 2006
From: henrik.parn at bio.ntnu.no (Henrik Parn)
Date: Fri, 01 Sep 2006 18:52:27 +0200
Subject: [R] lmer applied to a wellknown (?) example: summary
Message-ID: <44F8654B.2080103@bio.ntnu.no>

Dear all,

I wish to thank Christoph Buser and John Wilkinson for their input, and 
especially John his examples and for for pointing me to the thread 
'Doubt about nested aov output' where the rat-example was hiding...:

http://search.gmane.org/?query=Doubt+about+nested+aov+output&email=&group=gmane.comp.lang.r.general&sort=revdate&DEFAULTOP=and&xP=doubt.nested.aov.output.&xFILTERS=Gcomp.lang.r.general---A

In this great thread you find a description not only on using aov on 
this nested rat-data, but also how to handle it with lmer, e.g. model 
specification and coding of grouping levels.

Best regards,

Henrik


-- 
************************
Henrik P?rn
Department of Biology
NTNU
7491 Trondheim
Norway

+47 735 96282 (office)
+47 909 89 255 (mobile)
+47 735 96100 (fax)


From d.scott at auckland.ac.nz  Fri Sep  1 18:43:48 2006
From: d.scott at auckland.ac.nz (David Scott)
Date: Sat, 2 Sep 2006 04:43:48 +1200 (NZST)
Subject: [R] Lattice plot with fitted curves
Message-ID: <Pine.LNX.4.61.0609020410100.14144@stat12.stat.auckland.ac.nz>


I have some data which consists of time series for a number of sites. It 
appears that there is not much autocorrelation in the data and I have 
fitted a cubic for each site using lm. I would like to obtain a lattice 
plot with one panel for each site and showing the original data, and the 
fitted cubic.

The closest I have got to doing what I want is:

fit <- fitted(paraslm1)
temp <- cbind(paras, fit)
xyplot(Density ~ Year | LocCode, data = temp,
        panel = function(x, y, ...){
          panel.xyplot(x,y)
          panel.xyplot(x[LocCode],fit[LocCode],type="l")
        })

This doesn't give an error (most of my other attempts did), and draws the 
panels correctly with the original data, but doesn't draw the fitted 
lines.

paraslm1 is my fitted linear model with cubics for each location.

David Scott

_________________________________________________________________
David Scott	Visiting (July 06 to January 07)
 		Department of Probability and Statistics
 		The University of Sheffield
 		The Hicks Building
 		Hounsfield Road
 		Sheffield S3 7RH
 		United Kingdom
Phone:	+44 114 222 3908
Email:	d.scott at auckland.ac.nz


From rolf at erdos.math.unb.ca  Fri Sep  1 19:11:11 2006
From: rolf at erdos.math.unb.ca (Rolf Turner)
Date: Fri, 1 Sep 2006 14:11:11 -0300 (ADT)
Subject: [R] Summary --- Local library under Windoze.
Message-ID: <200609011711.k81HBBrH000115@erdos.math.unb.ca>

Prof. Brian Ripley solved the problem.  He wrote:

> I was not aware that this works with relative paths for any version
> of R.  Try using a full path, which always works for me.

	I tried it using a full path, and bingo!  It worked
	like a charm.

	Under Unix the relative path also works, but.

Prof. Ripley also remarked:

> If indeed your filesystem is readonly, you will have problems in
> spades.  But is that box just dark and not ticked?  (That's a lovely
> Windows gotcha that has caught people here many times.)

	That box was indeed just dark and not ticked.  I was got!

James W. MacDonald suggested:

> I don't think this is how you should do things, since R won't know
> about this library path. Instead, you should use .libPaths() to set
> your library path and then install using that. Note that you will
> likely need to put a call to .libPaths() into a .Rprofile file in
> order to have this set on startup.
> 
>  > dir.create("C:/Documents and Settings/Jim/newlib")
>  > .libPaths("C:/Documents and Settings/Jim/newlib")
>  > install.packages("zoo")

   <snip>

> Then you can use install.packages("packagename", lib = .libPaths()[2]) 
> if you want to use the 'stock' library directory, or just 
> install.packages("packagename") to use your private one.

	This is a red herring.  The install.packages() function
	does not care if the specified folder is in the library
	path.  I experimented to verify this.  The crucial thing,
	as Prof. Ripley suggested, is --- under Windoze --- to
	specify the full pathname of the library into which you wish
	to install the package.

	[Perhaps this might be mentioned in the documentation for
	install.packages() --- to save future grief for dweebs
	like myself.]

	It is indeed correct that R won't know about this library
	if it is not in the library path, but you can *tell* it
	about this library:

		library("foo",lib.loc="lnilp")

	(where ``lnilp'' means ``library not in library path'').

	BTW, lib.loc can be specified by the *relative* path name,
	with no problem.

Thanks to all.

				cheers,

					Rolf Turner
					rolf at math.unb.ca


From jholtman at gmail.com  Fri Sep  1 19:02:22 2006
From: jholtman at gmail.com (jim holtman)
Date: Fri, 1 Sep 2006 13:02:22 -0400
Subject: [R] repeating the same procedure with a number of files within
	a directory
In-Reply-To: <20060901164244.10797.qmail@web25507.mail.ukl.yahoo.com>
References: <20060901164244.10797.qmail@web25507.mail.ukl.yahoo.com>
Message-ID: <644e1f320609011002g4327a422md8375a642e04896e@mail.gmail.com>

Just have to use a 'for' loop;

for (i in list.files()){
     x <- read.table(i)  # read in the next file (this is in 'i')
     ....compute & write your distance matrix.....
}

On 9/1/06, Ffenics <ffenics2002 at yahoo.co.uk> wrote:
> Hi there
> I am very new to R so dont know much about the programming side of thing yet. I've worked out how to input a data matrix, create distance matrices and print them to an external file but only for one data matrix at a time.
>
> I actually have a batch of data matrices for which I want to create distance matrices for each in turn and then print these distance matrices to their respective external files. (I'm wanting them printed as CSV files for processing in excel).
>
> How easy is it to get R to read in files one at a time from a directory and do the same thing with all of those files in turn? And could someone please give me some advice as to how I may go about achieving this if its possible? It would be much appreciated
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From ripley at stats.ox.ac.uk  Fri Sep  1 19:24:27 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 1 Sep 2006 18:24:27 +0100 (BST)
Subject: [R] Summary --- Local library under Windoze.
In-Reply-To: <200609011711.k81HBBrH000115@erdos.math.unb.ca>
References: <200609011711.k81HBBrH000115@erdos.math.unb.ca>
Message-ID: <Pine.LNX.4.64.0609011820450.17049@gannet.stats.ox.ac.uk>

install.packages() in R 2.4.0 will make a full path out of a relative 
path, to avoid any confusion.  (normalizePath is a good way to do that, 
BTW).


On Fri, 1 Sep 2006, Rolf Turner wrote:

> Prof. Brian Ripley solved the problem.  He wrote:
> 
> > I was not aware that this works with relative paths for any version
> > of R.  Try using a full path, which always works for me.
> 
> 	I tried it using a full path, and bingo!  It worked
> 	like a charm.
> 
> 	Under Unix the relative path also works, but.
> 
> Prof. Ripley also remarked:
> 
> > If indeed your filesystem is readonly, you will have problems in
> > spades.  But is that box just dark and not ticked?  (That's a lovely
> > Windows gotcha that has caught people here many times.)
> 
> 	That box was indeed just dark and not ticked.  I was got!
> 
> James W. MacDonald suggested:
> 
> > I don't think this is how you should do things, since R won't know
> > about this library path. Instead, you should use .libPaths() to set
> > your library path and then install using that. Note that you will
> > likely need to put a call to .libPaths() into a .Rprofile file in
> > order to have this set on startup.
> > 
> >  > dir.create("C:/Documents and Settings/Jim/newlib")
> >  > .libPaths("C:/Documents and Settings/Jim/newlib")
> >  > install.packages("zoo")
> 
>    <snip>
> 
> > Then you can use install.packages("packagename", lib = .libPaths()[2]) 
> > if you want to use the 'stock' library directory, or just 
> > install.packages("packagename") to use your private one.
> 
> 	This is a red herring.  The install.packages() function
> 	does not care if the specified folder is in the library
> 	path.  I experimented to verify this.  The crucial thing,
> 	as Prof. Ripley suggested, is --- under Windoze --- to
> 	specify the full pathname of the library into which you wish
> 	to install the package.
> 
> 	[Perhaps this might be mentioned in the documentation for
> 	install.packages() --- to save future grief for dweebs
> 	like myself.]
> 
> 	It is indeed correct that R won't know about this library
> 	if it is not in the library path, but you can *tell* it
> 	about this library:
> 
> 		library("foo",lib.loc="lnilp")
> 
> 	(where ``lnilp'' means ``library not in library path'').
> 
> 	BTW, lib.loc can be specified by the *relative* path name,
> 	with no problem.
> 
> Thanks to all.
> 
> 				cheers,
> 
> 					Rolf Turner
> 					rolf at math.unb.ca
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Fri Sep  1 19:30:34 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 1 Sep 2006 18:30:34 +0100 (BST)
Subject: [R] Compiling a package
In-Reply-To: <05BE78B0CF1BBC4BBA4AA255568D8611029A9964@EXCHANGE2VS1.campus.mcgill.ca>
References: <05BE78B0CF1BBC4BBA4AA255568D8611029A9964@EXCHANGE2VS1.campus.mcgill.ca>
Message-ID: <Pine.LNX.4.64.0609011828140.17049@gannet.stats.ox.ac.uk>

Please use Rcmd INSTALL --build and see if that works for you.  It is how 
Uwe Ligges and I build the public repositories.

On just Rcmd INSTALL if you don't want to distribute it.

(R CMD and Rcmd are the same thing, and I prefer the older form which is 
more efficient.)

On Fri, 1 Sep 2006, R Heberto Ghezzo, Dr wrote:

> Hello,
> I am in Win-XP R:2.3.0
> latest rtools and Perl - of today
> I got Rcmdr.HH source code and tried to compile it myself
> copy all directory to R/R-2.3.0/src/library/Rcmdr.HH
> from R/R-2.3.0/src/library
> I typed:
> ..\..\bin\R CMD build --force --binary --auto-zip Rcmdr.HH
> * checking for file 'Rcmdr.HH/DESCRIPTION' ... OK
> * preparing 'Rcmdr.HH':
> * checking DESCRIPTION meta-information ... OK
> * removing junk files
> Error: cannot open file 'Rcmdr.HH/DESCRIPTION' for reading
> 
> file Rcmdr.HH/DESCRIPTION is there,
> 
> Package: Rcmdr.HH
> Type: Package
> Title: Rcmdr support for the introductory course at Temple University.
> Version: 1.2
> Date: 2006-08-04
> Author: Richard M. Heiberger, with contributions from Burt Holland.
> Maintainer: Richard M. Heiberger <rmh at temple.edu>
> Depends: R (>= 2.1.0), Rcmdr, car, multcomp, leaps, lattice, grid
> Description: Our introductory course spends time on several topics
>         that are not yet in the R Commander.  Therefore we wrote the menu
>         items and make them available.
> License: GPL version 2 or newer
> Packaged: Fri Aug  4 03:57:20 2006; rmh
> 
>  is read/write and after all the authors also have a Rcmdr.HH.zip which means that they compiled without errors.
> 
> Can someone suggest why I can not compile the file?
> I have another version with some modifications and additions which is the one I really want to compile, but if I can not compile the original. . . .
> Thanks for any help
> H.Ghezzo
> McGill University
> Canada
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From rmh at temple.edu  Fri Sep  1 20:02:20 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Fri,  1 Sep 2006 14:02:20 -0400 (EDT)
Subject: [R] Compiling a package
Message-ID: <20060901140220.BHC68725@po-d.temple.edu>

These are the commands I used

Rcmd check Rcmdr.HH             ## detailed checks

Rcmd build Rcmdr.HH             ## tar.gz

R CMD INSTALL --build Rcmdr.HH  ## installs and builds .zip


From deepayan.sarkar at gmail.com  Fri Sep  1 20:10:39 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Fri, 1 Sep 2006 11:10:39 -0700
Subject: [R] Lattice plot with fitted curves
In-Reply-To: <Pine.LNX.4.61.0609020410100.14144@stat12.stat.auckland.ac.nz>
References: <Pine.LNX.4.61.0609020410100.14144@stat12.stat.auckland.ac.nz>
Message-ID: <eb555e660609011110w742843b4y243cca9a4659dc14@mail.gmail.com>

On 9/1/06, David Scott <d.scott at auckland.ac.nz> wrote:
>
> I have some data which consists of time series for a number of sites. It
> appears that there is not much autocorrelation in the data and I have
> fitted a cubic for each site using lm. I would like to obtain a lattice
> plot with one panel for each site and showing the original data, and the
> fitted cubic.
>
> The closest I have got to doing what I want is:
>
> fit <- fitted(paraslm1)
> temp <- cbind(paras, fit)
> xyplot(Density ~ Year | LocCode, data = temp,
>         panel = function(x, y, ...){
>           panel.xyplot(x,y)
>           panel.xyplot(x[LocCode],fit[LocCode],type="l")
>         })
>
> This doesn't give an error (most of my other attempts did), and draws the
> panels correctly with the original data, but doesn't draw the fitted
> lines.
>
> paraslm1 is my fitted linear model with cubics for each location.

I'm guessing since I don't have actual data to work with, but the
following is probably what you are trying to do:

xyplot(Density + fit ~ Year | LocCode, data = temp,
       panel = panel.superpose.2,
       type = c('p', 'l'))

-Deepayan


From mnair at iusb.edu  Fri Sep  1 20:49:12 2006
From: mnair at iusb.edu (Nair, Murlidharan T)
Date: Fri, 1 Sep 2006 14:49:12 -0400
Subject: [R] histograms
Message-ID: <A32055BDEA88C34BB3DBBCD2293807786311CA@iu-mssg-mbx109.ads.iu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060901/e220058e/attachment.pl 

From ripley at stats.ox.ac.uk  Fri Sep  1 21:07:10 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 1 Sep 2006 20:07:10 +0100 (BST)
Subject: [R] integration problem with gamma function
In-Reply-To: <44F84B9E.7020306@anicca-vijja.de>
References: <44F84B9E.7020306@anicca-vijja.de>
Message-ID: <Pine.LNX.4.64.0609011814550.17049@gannet.stats.ox.ac.uk>

It is the same issue.  Martin's reply was for a specific value of 'a': the 
factor is gamma(a).

Please do study the help page for pgamma (as the posting guide did ask you 
to), as it has all the details.

Note that the Mathematica version can easily lead to representation 
difficulties for even modest 'a'.

On Fri, 1 Sep 2006, Leo G?rtler wrote:

> Dear R-list members,
> 
> I have a problem with translating a mathematica script into R. The whole 
> script is at the end of the email (with initial values for easy 
> reproduction) and can be pasted directly into R. The problematic part 
> (which is included below of course) is
> 
> <--- Original Mathematica --->
> (* p_svbar *)
> UiA  = Ni (Dsi - 2Di A + A^2)/2;
> UiiA = Nii (Dsii - 2Dii A + A^2)/2;
> psvbar = NIntegrate[1/(UiA^(Ni/2)) 1/(UiiA^(Nii/2))
>     Gamma[Ni/2,UiA/(sH^2),UiA/(sL^2)]
>     Gamma[Nii/2,UiiA/(sH^2),UiiA/(sL^2)],{A,L,H},
>     MinRecursion->3];
> PSVbar = psvbar/(4 Log[sH/sL]);
> Print["p(s?v|D_1D_2I)  = const. ",N[PSVbar,6]];
> </--->
> 
> <--- translation to R --->
> integpsvbar <- function(A)
> {
> # Mathematica: gamma[a,z0,z1] = gamma[a,z1] - gamma[a,z0]
>  UiA <- Ni*(Dsi-2*Di*A+A^2)/2
>  UiiA <- Nii*(Dsii-2*Dii*A+A^2)/2
>  1/UiA^(Ni/2) *
>  1/UiiA^(Nii/2) *
>  ( pgamma(UiA/(sL^2), Ni/2) - pgamma(UiA/(sH^2), Ni/2) ) *
>  ( pgamma(UiiA/(sL^2), Nii/2) - pgamma(UiiA/(sH^2), Nii/2) )
> }
> 
> psvbar <- integrate(integpsvbar, lower=L, upper=H)
> psvbar$value
> PSVbar <- psvbar$value / (4*log(sH/sL))
> PSVbar <- PSVbar * sqrt(pi) * sqrt(pi)      #note: two times pgamma in 
> integration function above... correct it with two times multiplied by 
> 'sqrt(pi)' due to differences in defining incomplete gamma function 
> between R and mathematica
> print(paste("p(s?v|D_1D_2I)  = const. ",PSVbar,sep=""))
> </--->
> 
> According to Mathematica
> 
> psvbar ~ 1.72026 * 10^20
> PSVbar ~ 5.24827 * 10^19
> 
> The R part produces
> 
> psvbar ~ 1.824023 * 10^13
> PSVbar ~ 17482463305549.6
> 
> 
> 
> The script produces proper results untill the problematic part, so it 
> seems quite reasonable that no other error before is responsible for the 
> difference of the results.
> I assume that I did something wrong with the Gamma function.
> 
> The "generalized incomplete gamma function" in Mathematica (Wolfram, 
> p.363f.) is defined as
> 
> Gamma[a, z0, z1] := Gamma[a, z1] - Gamma[a, z0]
> 
> with
> 
> Gamma[a, z0, z1] := integral_z0^z1 t^(a-1) exp(-t) dt
> 
> Please note: I asked a quite similar question (thread from 07-August/ 
> 08-August 2006) for which Martin M?chler (08-08-2006 13:53) pointed out, 
> that the result has to be multiplied by "sqrt(pi)" because of (-> 
> manpage pgamma) differences in formulating the incomplete gamma function 
> between Mathematica and R. However, I tried this in various ways and - 
> at least for me - it does not help. Thus, I assume it is another issue.
> 
> Thank you very much,
> 
> best wishes
> leo g?rtler
> 
> now the R script to reproduce (can be pasted directly into R):
> 
> ####################
> # R-Portierung aus Mathematica (Urban Studer, 90er)
> # Ursprung: G.L. Bretthorst "On the difference of means"
> # zuerst: 12-06-05
> # zuletzt: 21-06-06
> 
> # --------------------------------------------------
> # Success rates and integration bounds in the case
> # of the (conservative) Bayes-Laplace prior
> # --------------------------------------------------
> 
> 
> 
> 
> 
> SucRatesIntBounds <- function(Ni, Si, Nii, Sii, smin)
> {
> # BEGIN BLOCK
> # necessary variables:
> # {Nmax, Nmin}
> 
> ### defintion of constants
> Di <- (Si + 1) / (Ni + 2)
> si <- sqrt(Di * (1 - Di) / (Ni + 3))
> Dii <- (Sii + 1) / (Nii + 2)
> sii <- sqrt(Dii * (1 - Dii) / (Nii + 3))
> 
> Nmax <- max(Ni, Nii)
> Nmin <- min(Ni, Nii)
> sL <- floor(1000 * sqrt((Nmax+1) / (Nmax+3)) / (Nmax + 2)) / 1000
> sH <- ceiling(1000 / (2 * sqrt(Nmin + 3))) / 1000
> L <- floor(100 * (smin + 1) / (Nmax + 2)) / 100
> H <- 1 - L
> 
> return(c(Di, si, Dii, sii, sL, sH, L, H))
> }
> #END BLOCK
> # --------------------------------------------------
> 
> 
> Si <- 11
> Ni <- 15
> Sii <- 10
> Nii <- 16
> smin <- 0
> res1 <- SucRatesIntBounds(Ni, Si, Nii, Sii, smin)
> res1
> #return(c(Di, si, Dii, sii, sL, sH, L, H))
> 
> names(res1) <- c("Di","si","Dii","sii","sL","sH","L","H")
> res1
> 
> Di <- res1[1]
> si <- res1[2]
> Dii <- res1[3]
> sii <- res1[4]
> sL <- res1[5]
> sH <- res1[6]
> L <- res1[7]
> H <- res1[8]
> 
> 
> # --------------------------------------------------
> # Begin: ON THE DIFFERENCE IN MEANS
> # --------------------------------------------------
> 
> #DiffinMeans <- function(Ni, Di, si, Nii, Dii, sii, L, H, sL, sH)
> ## BEGIN BLOCK
> #{
> 
> # necessary variables in the function
> #{ NN, DD, Dsi, Dsii, DsD, ss,
> #  dd, lownum, upnum, low, up, psv, PSV,
> #  zz, lowinum, upinum, lowiinum, upiinum, psbarv, PSbarV,
> #  psvbar, PSVbar, psbarvbar, PSbarVbar, cc,
> #  sv, sbarv, svbar, sbarvbar,
> #  samemeans, diffmeans, samevars, diffvars, diffsets },
> 
> 
> 
> ### defintion of constants
> NN <- Ni+Nii
> DD <- (Ni * Di + Nii * Dii) / NN
> Dsi <- (Ni-1) / Ni * si^2 + Di^2
> Dsii <- (Nii-1) / Nii * sii^2 + Dii^2
> DsD <- (Ni * Dsi + Nii * Dsii) / NN
> ss <- sqrt(NN * (DsD - DD^2) / (NN-1))
> 
> 
> ### descriptive statistics
> print(" \n------------- Data ---------------------------------------\n")
> print(paste("N_1 = ",Ni ," :     Mean_1 ? s_1    = ", Di," ? ",si, sep=""))
> print(paste("N_2 = ",Nii," :     Mean_2 ? s_2    = ", Dii," ? ",sii, 
> sep=""))
> print(paste("N   = ", NN ," :  Mean_comb ? s_comb = ", DD," ? ",ss, sep=""))
> print(" ")
> print(paste("s_L = ",sL,", s_H = ",sH,";  L = ",L, ", H = ",H,"  (smin = 
> ",smin,")",sep=""))
> if(L < DD)
>   {
>    print(paste("L - Mean_comb < 0 : ", (L<DD), "\n", "             (-> 
> '+'-sign between Gamma-fcts o.k.)", sep=""))
>   } else print(paste("L - Mean_comb < 0 : ", (L<DD), "\n", "             
> (-> '+'-sign between Gamma-fcts false!)", sep=""))
> print(paste(" \n ", sep=""))
> 
> 
> #(* p_sv *)
> dd <- NN * (DsD - DD^2)
> lownum <- NN * (L-DD)^2
> upnum  <- NN * (H-DD)^2
> 
> #low = lownum / (2*s^2)
> #up  = upnum / (2*s^2)
> integpsv <- function(s)
> { 1 / (s^NN) * exp(-dd / (2 * s^2)) *
>   ( pgamma(upnum/(2*s^2), 1/2) + pgamma(lownum/(2*s^2), 1/2) )
> }
> psv <- integrate(integpsv, lower=sL, upper=sH)
> #???    (* + if (L-DD) < 0 *)
> PSV <- psv$value / sqrt(2*NN) * sqrt(pi)    # normalizing factor due to R
> # Gamma(1/2)= sqrt(pi)
> # psv ~ 1.39848 * 10^20
> # ~ 3.44715 * 10 ^20
> # ~ 5.24827 * 10 ^20
> # ~ 1.52788 * 10 ^20
> print("------------- Results ------------------------------------\n")
> print(paste("p(sv|D_1D_2I)   = const. ",PSV, sep=""))
> 
> 
> ### p_sbarv
> zz <- Ni * (Dsi - Di^2) + Nii * (Dsii - Dii^2)
> lowinum <- Ni * (L - Di)^2
> upinum <- Ni * (H - Di)^2
> lowiinum <- Nii * (L - Dii)^2
> upiinum <- Nii * (H - Dii)^2
> 
> #lowi <- lowinum / (2*s^2)
> #upi  <- upinum / (2*s^2)
> #lowii <- lowiinum / (2*s^2)
> #upii <- upiinum / (2*s^2)
> integpsbarv <- function(s)
> {
>  1/(s^(NN-1)) * exp(-zz/(2*s^2)) *
>  ( pgamma(upinum/(2*s^2), 1/2) + pgamma(lowinum/(2*s^2), 1/2) ) *
>  ( pgamma(upiinum/(2*s^2), 1/2) + pgamma(lowiinum/(2*s^2), 1/2) )
> }
> psbarv <- integrate(integpsbarv, lower=sL, upper=sH)
> #??? (* + if (L-DD) < 0 *)
> PSbarV <- psbarv$value / (2*(H-L) * sqrt(Ni*Nii)) * sqrt(pi) * sqrt(pi)
> # 2mal mit sqrt(pi) mal nehmen = "* pi", weil zei pgamma-Ausdr?cke 
> enthalten sind in der Integration
> print(paste("p(?sv|D_1D_2I)  = const. ",PSbarV, sep=""))
> 
> ###############ALL ABOVE PRODUCES PROPER RESULTS COMPARED TO
> ###############MATHEMATICA SCRIPT
> 
> ###############PROBLEMATIC PART STARTS HERE#################  
> ### p_svbar
> ###
> # Mathematica
> #(* p_svbar *)
> #UiA  = Ni (Dsi - 2Di A + A^2)/2;
> #UiiA = Nii (Dsii - 2Dii A + A^2)/2;
> #psvbar = NIntegrate[1/(UiA^(Ni/2)) 1/(UiiA^(Nii/2))
> #   Gamma[Ni/2,UiA/(sH^2),UiA/(sL^2)]
> #   Gamma[Nii/2,UiiA/(sH^2),UiiA/(sL^2)],{A,L,H},
> #    MinRecursion->3];
> #PSVbar = psvbar/(4 Log[sH/sL]);
> #Print["p(s?v|D_1D_2I)  = const. ",N[PSVbar,6]];
> 
> ###R trial
> integpsvbar <- function(A)
> {
> # Mathematica: gamma[a,z0,z1] = gamma[a,z1] - gamma[a,z0]
>  UiA <- Ni*(Dsi-2*Di*A+A^2)/2
>  UiiA <- Nii*(Dsii-2*Dii*A+A^2)/2
>  1/UiA^(Ni/2) *
>  1/UiiA^(Nii/2) *
>  ( pgamma(UiA/(sL^2), Ni/2) - pgamma(UiA/(sH^2), Ni/2) ) *
>  ( pgamma(UiiA/(sL^2), Nii/2) - pgamma(UiiA/(sH^2), Nii/2) )
> }
> 
> psvbar <- integrate(integpsvbar, lower=L, upper=H)
> psvbar$value
> PSVbar <- psvbar$value / (4*log(sH/sL)) * sqrt(pi) * sqrt(pi) # two 
> times sqrt(pi) to correct for differences between R and mathematica
> print(paste("p(s?v|D_1D_2I)  = const. ",PSVbar,sep=""))
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ggrothendieck at gmail.com  Fri Sep  1 21:11:00 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 1 Sep 2006 15:11:00 -0400
Subject: [R] histograms
In-Reply-To: <A32055BDEA88C34BB3DBBCD2293807786311CA@iu-mssg-mbx109.ads.iu.edu>
References: <A32055BDEA88C34BB3DBBCD2293807786311CA@iu-mssg-mbx109.ads.iu.edu>
Message-ID: <971536df0609011211r4924e765i2574dac6bbae961d@mail.gmail.com>

Your data seems to have come through messed up but lets
assume you have a data frame with one column per tumor.
The convert your data to stacked form and call histogram:

DF <- data.frame(T1 = 1:10, T2 = 6:15)

library(lattice)
histogram(~ values | ind, stack(DF))


On 9/1/06, Nair, Murlidharan T <mnair at iusb.edu> wrote:
> I am interested in plotting histograms for the following data
>
>
>
> Isoform
>
> Tumor_65_198
>
> Tumor_50_192
>
> Tumor_80_167
>
> Tumor_80_204
>
> Tumor_95_197
>
> Tumor_70_189
>
> Tumor_90_202
>
> Tumor_40_177
>
> Tumor_60_21
>
> Tumor_70_174
>
> Tumor_70_147
>
> Tumor_50_5
>
> ABCC4-2007
>
> 1
>
> 1
>
> 1
>
> 6
>
> 1
>
> 9
>
> 10
>
> 1
>
> 2
>
> 0
>
> 10
>
> 1
>
> ABCC4-2008
>
> 5
>
> 8
>
> 7
>
> 5
>
> 3
>
> 10
>
> 5
>
> 5
>
> 7
>
> 3
>
> 10
>
> 3
>
> ABCC4-2009
>
> 0
>
> 0
>
> 0
>
> 0
>
> 0
>
> 0
>
> 0
>
> 0
>
> 0
>
> 0
>
> 0
>
> 0
>
> ABCC4-2010
>
> 0
>
> 0
>
> 0
>
> 0
>
> 0
>
> 0
>
> 0
>
> 0
>
> 0
>
> 0
>
> 0
>
> 0
>
> ABCC4-2011
>
> 0
>
> 10
>
> 4
>
> 3
>
> 2
>
> 0
>
> 2
>
> 4
>
> 1
>
> 4
>
> 10
>
> 0
>
> ABCC4-2012
>
> 0
>
> 0
>
> 10
>
> 0
>
> 0
>
> 0
>
> 0
>
> 0
>
> 0
>
> 0
>
> 0
>
> 0
>
> ABCC4-2013
>
> 7
>
> 10
>
> 10
>
> 10
>
> 8
>
> 10
>
> 10
>
> 4
>
> 10
>
> 0
>
> 10
>
> 0
>
> ABCG1-0489
>
> 1
>
> 0
>
> 0
>
> 1
>
> 9
>
> 1
>
> 6
>
> 1
>
> 1
>
> 0
>
> 0
>
> 0
>
> ABCG1-0490
>
> 0
>
> 0
>
> 0
>
> 0
>
> 0
>
> 0
>
> 0
>
> 0
>
> 0
>
> 0
>
> 0
>
> 0
>
> ABCG1-0491
>
> 2
>
> 0
>
> 2
>
> 3
>
> 1
>
> 1
>
> 2
>
> 2
>
> 5
>
> 2
>
> 1
>
> 7
>
>
>
> Basically 12 different histograms one for each Tumor sample. What is an
> easy way to do this?
>
> I was trying to modify the following example code, but I guess I need
> some help as I am not very comfortable with R
>
> require(lattice)
>
> sundar.theme <- function() {
>        par <- col.whitebg()
>        par$strip.background$col <- rep("#000099", 7)
>        par$add.text$col <- "#eeeeaa"
>        par$add.text$font <- 2
>        par$background$col <- "#ffffff"
>        par$superpose.line$lty <- rep(1, 7)
>        par$superpose.line$col[1:2] <- c("#880000", "#008800")
>        par$superpose.symbol$col[1:2] <- c("#880000", "#008800")
>        par
>      }
>
> trellis.par.set(sundar.theme())
>
> print(  # necessary if the file is source()'d
>  histogram( ~ height | voice.part, data = singer,
>          xlab = "Height (inches)", type = "density",
>          panel = function(x, ...) {
>              panel.histogram(x, ...)
>              panel.mathdensity(dmath = dnorm, col = "black",
>                                args = list(mean=mean(x),sd=sd(x)))
>          } )
>  )
>
>
>
>
>
>
>
> Thanks ../Murli
>
>
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From toby_marks at americancentury.com  Fri Sep  1 21:30:59 2006
From: toby_marks at americancentury.com (toby_marks at americancentury.com)
Date: Fri, 1 Sep 2006 14:30:59 -0500
Subject: [R] cumulative growth rates indexed to a common starting point
 over n series of observations
In-Reply-To: <17655.23757.25543.839815@basebud.nulle.part>
Message-ID: <OF89A475AB.A525040E-ON862571DC.004C46AC-862571DC.006B352E@americancentury.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060901/6fdd304d/attachment.pl 

From edd at debian.org  Fri Sep  1 21:59:50 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 1 Sep 2006 14:59:50 -0500
Subject: [R] cumulative growth rates indexed to a common starting point
 over n series of observations
In-Reply-To: <OF89A475AB.A525040E-ON862571DC.004C46AC-862571DC.006B352E@americancentury.com>
References: <17655.23757.25543.839815@basebud.nulle.part>
	<OF89A475AB.A525040E-ON862571DC.004C46AC-862571DC.006B352E@americancentury.com>
Message-ID: <17656.37174.657814.437306@basebud.nulle.part>


On 1 September 2006 at 14:30, toby_marks at americancentury.com wrote:
| The apply with the cumprod was exactly what I was after.  The apply just 
| wasn't clicking with me, and I had overlooked the cumprod.  Thanks to all 
| for pushing me down the right path!
| 
| Actually, what I am ultimately after is a way to link this series, without 
| having to use a for loop ( the only way I can think of ... ).  But, would 
| like to see if it can be linked  using mapply or apply against the rows 
| and to compute the linked results. 
| 
| zz = matrix(rnorm(20), ncol=2)
| zzcum = apply(zz/100 + 1, 2, cumprod)
| zzlinkcum = 100*zzcum
| for(i in 2:length(zz[,1])){ zzlinkcum[i,]=zzlinkcum[i-1,]*zzcum[i,]}  ### 
| Is there a better way here ?

Sure, why not call apply again?

> set.seed(42); zz <- matrix(rnorm(20), ncol=2)
> zzcum <- apply(1+zz/100, 2, cumprod)
> apply(rbind(c(1,1), zzcum), 2, cumprod)*100
          [,1]     [,2]
 [1,] 100.0000 100.0000
 [2,] 101.3710 101.3049
 [3,] 102.1804 104.9735
 [4,] 103.3704 107.2642
 [5,] 105.2360 109.2994
 [6,] 107.5684 111.2246
 [7,] 109.8358 113.9036
 [8,] 113.8461 116.3156
 [9,] 117.8912 115.6233
[10,] 124.5442 112.1301
[11,] 131.4900 110.1781
> 

Hth, Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From zwang at scharp.org  Fri Sep  1 22:29:01 2006
From: zwang at scharp.org (Zhu Wang)
Date: Fri, 01 Sep 2006 13:29:01 -0700
Subject: [R] write a table to file with unequal length of lists
Message-ID: <44F8980D.9050004@scharp.org>

Dear R helpers,

To illustrate my problem, here is a simplified example. I want to write 
a table to a file similar to:
x                      a
1                      4,5
2                      8,9,10

Note the length of elements of "a" is 2 and 3 respectively.  This can be 
created by, for example,
x <- c(1,2)
a <- NULL
a[1] <- list(c(4,5))
a[2] <- list(c(8,9,10)

Any suggestions to write such a table to file would be appreciated.

Thanks,

Zhu Wang


From ssj1364 at gmail.com  Fri Sep  1 22:30:18 2006
From: ssj1364 at gmail.com (Spencer Jones)
Date: Fri, 1 Sep 2006 14:30:18 -0600
Subject: [R]  difference between ns and bs in predict.glm
Message-ID: <1c6126db0609011330s229e09a3u27c0f6fe2e9218db@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060901/fa2e8b1d/attachment.pl 

From toby_marks at americancentury.com  Fri Sep  1 22:42:24 2006
From: toby_marks at americancentury.com (toby_marks at americancentury.com)
Date: Fri, 1 Sep 2006 15:42:24 -0500
Subject: [R] cumulative growth rates indexed to a common starting point
 over n series of observations
In-Reply-To: <17656.37174.657814.437306@basebud.nulle.part>
Message-ID: <OFD0B15448.CDD953D7-ON862571DC.0071968C-862571DC.0071BEE9@americancentury.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060901/9418dda8/attachment.pl 

From rolf at erdos.math.unb.ca  Fri Sep  1 22:44:36 2006
From: rolf at erdos.math.unb.ca (Rolf Turner)
Date: Fri, 1 Sep 2006 17:44:36 -0300 (ADT)
Subject: [R] write a table to file with unequal length of lists
Message-ID: <200609012044.k81Kiab9009555@erdos.math.unb.ca>

What about

	> x <- 1:2
	> a <- list(c(4,5),c(8,9,10)) # Which is the way you *should*
                                      # have constructed a!
	> ddd <- data.frame(x=x,a=I(unlist(lapply(a,paste,collapse=","))))
	> write.table(ddd,file="ddd.out",quote=FALSE,row.names=FALSE)

			cheers,

				Rolf Turner
				rolf at math.unb.ca

Original message:

> Dear R helpers,
> 
> To illustrate my problem, here is a simplified example. I want to write 
> a table to a file similar to:
> x                      a
> 1                      4,5
> 2                      8,9,10
> 
> Note the length of elements of "a" is 2 and 3 respectively.  This can be 
> created by, for example,
> x <- c(1,2)
> a <- NULL
> a[1] <- list(c(4,5))
> a[2] <- list(c(8,9,10)
> 
> Any suggestions to write such a table to file would be appreciated.
> 
> Thanks,
> 
> Zhu Wang


From lord.tyranus.96 at gmail.com  Fri Sep  1 22:51:04 2006
From: lord.tyranus.96 at gmail.com (Lord Tyranus)
Date: Fri, 1 Sep 2006 14:51:04 -0600
Subject: [R] Help with singular value decomposition
Message-ID: <4f31b0bd0609011351g395019c0o637d7b91e9a7c785@mail.gmail.com>

Hi wizards, I have seen the function svd of R for singular value
decomposition, but I need to computes the ``economy size'' or ``thin''
singular value decomposition of a matrix in R. Somebody knows how to
do that?. Thanks in advance.

-- 
Web Page
http://geocities.com/lord_tyranus_96/


From srini_iyyer_bio at yahoo.com  Fri Sep  1 22:55:49 2006
From: srini_iyyer_bio at yahoo.com (Srinivas Iyyer)
Date: Fri, 1 Sep 2006 13:55:49 -0700 (PDT)
Subject: [R] Reading many files at once
Message-ID: <20060901205549.71305.qmail@web38101.mail.mud.yahoo.com>

dear group, 
 
i have 100 files starting with 'hsa-*'.

ex. file:

fruit   p-value 
------------
apple  0.0003
orange 0.004
kiwi   0.0003
peach  0.0004



I want to read all these files and create a single
matrix. here each file may have different fruit names.
 
in the matrix i want to have a union of all fruits and
those should be the rows in the matrix and file names
should be columns. 

ex;
        hsa-1   hsa-2  hsa-3  hsa-4  hsa-5
apple   0.003   0.01   0.002   0.002  0.002
orange  0.003   0.01   0.002   0.002  0.002
kiwi    0.003   0.01   0.002   0.002  0.002
peach   0.003   0.01   0.002   0.002  0.002
banana  0.003   0.01   0.008   0.002  0.001
plum    0.003   0.01   0.009   0.002  0.005
mango   0.003   0.001  0.002   0.002  0.008

 
could any one help me please.  

thank you.


From zwang at scharp.org  Fri Sep  1 22:56:38 2006
From: zwang at scharp.org (Zhu Wang)
Date: Fri, 01 Sep 2006 13:56:38 -0700
Subject: [R] write a table to file with unequal length of lists
In-Reply-To: <200609012044.k81Kiab9009555@erdos.math.unb.ca>
References: <200609012044.k81Kiab9009555@erdos.math.unb.ca>
Message-ID: <44F89E86.9010802@scharp.org>

Yes, the code does the job. Thanks, Zhu Wang

Rolf Turner wrote:
> What about
>
> 	> x <- 1:2
> 	> a <- list(c(4,5),c(8,9,10)) # Which is the way you *should*
>                                       # have constructed a!
> 	> ddd <- data.frame(x=x,a=I(unlist(lapply(a,paste,collapse=","))))
> 	> write.table(ddd,file="ddd.out",quote=FALSE,row.names=FALSE)
>
> 			cheers,
>
> 				Rolf Turner
> 				rolf at math.unb.ca
>
> Original message:
>
>   
>> Dear R helpers,
>>
>> To illustrate my problem, here is a simplified example. I want to write 
>> a table to a file similar to:
>> x                      a
>> 1                      4,5
>> 2                      8,9,10
>>
>> Note the length of elements of "a" is 2 and 3 respectively.  This can be 
>> created by, for example,
>> x <- c(1,2)
>> a <- NULL
>> a[1] <- list(c(4,5))
>> a[2] <- list(c(8,9,10)
>>
>> Any suggestions to write such a table to file would be appreciated.
>>
>> Thanks,
>>
>> Zhu Wang


From h.wickham at gmail.com  Fri Sep  1 23:03:40 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Fri, 1 Sep 2006 16:03:40 -0500
Subject: [R] Lattice plot with fitted curves
In-Reply-To: <Pine.LNX.4.61.0609020410100.14144@stat12.stat.auckland.ac.nz>
References: <Pine.LNX.4.61.0609020410100.14144@stat12.stat.auckland.ac.nz>
Message-ID: <f8e6ff050609011403o1c7b20d0h19e22a86640c4737@mail.gmail.com>

> I have some data which consists of time series for a number of sites. It
> appears that there is not much autocorrelation in the data and I have
> fitted a cubic for each site using lm. I would like to obtain a lattice
> plot with one panel for each site and showing the original data, and the
> fitted cubic.

This is very easy to do with ggplot:

install.packages("ggplot")
library(ggplot)

p <- ggplot(temp, . ~ LocCode, aes=list(y=Density, x=Year))
(p <- ggpoint(p))
ggsmooth(p, method=lm, formula=y~poly(x,3))

You can also do it with fitted values in another column.

temp$fitted <- fitted(paraslm1)

p <- ggplot(temp, . ~ LocCode, aes=list(y=Density, x=Year))
(p <- ggpoint(p))
ggline(p, aes=list(y=fitted))

Hadley


From h.wickham at gmail.com  Fri Sep  1 23:07:50 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Fri, 1 Sep 2006 16:07:50 -0500
Subject: [R] histograms
In-Reply-To: <971536df0609011211r4924e765i2574dac6bbae961d@mail.gmail.com>
References: <A32055BDEA88C34BB3DBBCD2293807786311CA@iu-mssg-mbx109.ads.iu.edu>
	<971536df0609011211r4924e765i2574dac6bbae961d@mail.gmail.com>
Message-ID: <f8e6ff050609011407y23c0a7cewfb1eddea5b128b63@mail.gmail.com>

> Your data seems to have come through messed up but lets
> assume you have a data frame with one column per tumor.
> The convert your data to stacked form and call histogram:
>
> DF <- data.frame(T1 = 1:10, T2 = 6:15)
>
> library(lattice)
> histogram(~ values | ind, stack(DF))

Or with ggplot and reshape:

install.packages(ggplot)
library(ggplot)
qplot(x=value, facets=. ~ variable, data=melt(DF,m=1:2), type="histogram")

Hadley


From edd at debian.org  Fri Sep  1 23:07:35 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 1 Sep 2006 16:07:35 -0500
Subject: [R] cumulative growth rates indexed to a common starting point
 over n series of observations
In-Reply-To: <17656.37174.657814.437306@basebud.nulle.part>
References: <17655.23757.25543.839815@basebud.nulle.part>
	<OF89A475AB.A525040E-ON862571DC.004C46AC-862571DC.006B352E@americancentury.com>
	<17656.37174.657814.437306@basebud.nulle.part>
Message-ID: <17656.41239.831930.502240@basebud.nulle.part>


On 1 September 2006 at 14:59, Dirk Eddelbuettel wrote:
| 
| On 1 September 2006 at 14:30, toby_marks at americancentury.com wrote:
| | The apply with the cumprod was exactly what I was after.  The apply just 
| | wasn't clicking with me, and I had overlooked the cumprod.  Thanks to all 
| | for pushing me down the right path!
| | 
| | Actually, what I am ultimately after is a way to link this series, without 
| | having to use a for loop ( the only way I can think of ... ).  But, would 
| | like to see if it can be linked  using mapply or apply against the rows 
| | and to compute the linked results. 
| | 
| | zz = matrix(rnorm(20), ncol=2)
| | zzcum = apply(zz/100 + 1, 2, cumprod)
| | zzlinkcum = 100*zzcum
| | for(i in 2:length(zz[,1])){ zzlinkcum[i,]=zzlinkcum[i-1,]*zzcum[i,]}  ### 
| | Is there a better way here ?
| 
| Sure, why not call apply again?
| 
| > set.seed(42); zz <- matrix(rnorm(20), ncol=2)
| > zzcum <- apply(1+zz/100, 2, cumprod)
| > apply(rbind(c(1,1), zzcum), 2, cumprod)*100

Small mistake, that compounds twice. You probably want

> set.seed(42)
> zz <- matrix(rnorm(20), ncol=2)
> apply( rbind(c(1,1), zz/100+1), 2, cumprod)*100
          [,1]      [,2]
 [1,] 100.0000 100.00000
 [2,] 101.3710 101.30487
 [3,] 100.7985 103.62135
 [4,] 101.1645 102.18220
 [5,] 101.8048 101.89732
 [6,] 102.2163 101.76147
 [7,] 102.1079 102.40863
 [8,] 103.6512 102.11753
 [9,] 103.5531  99.40482
[10,] 105.6433  96.97888
[11,] 105.5770  98.25911
> 

Hth, Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From mnair at iusb.edu  Fri Sep  1 23:15:41 2006
From: mnair at iusb.edu (Nair, Murlidharan T)
Date: Fri, 1 Sep 2006 17:15:41 -0400
Subject: [R] histograms
In-Reply-To: <f8e6ff050609011407y23c0a7cewfb1eddea5b128b63@mail.gmail.com>
Message-ID: <A32055BDEA88C34BB3DBBCD2293807786311FC@iu-mssg-mbx109.ads.iu.edu>

Thanks ../Murli


-----Original Message-----
From: hadley wickham [mailto:h.wickham at gmail.com] 
Sent: Friday, September 01, 2006 5:08 PM
To: Gabor Grothendieck
Cc: Nair, Murlidharan T; r-help at stat.math.ethz.ch
Subject: Re: Re: [R] histograms

> Your data seems to have come through messed up but lets
> assume you have a data frame with one column per tumor.
> The convert your data to stacked form and call histogram:
>
> DF <- data.frame(T1 = 1:10, T2 = 6:15)
>
> library(lattice)
> histogram(~ values | ind, stack(DF))

Or with ggplot and reshape:

install.packages(ggplot)
library(ggplot)
qplot(x=value, facets=. ~ variable, data=melt(DF,m=1:2),
type="histogram")

Hadley


From toby_marks at americancentury.com  Fri Sep  1 23:54:47 2006
From: toby_marks at americancentury.com (toby_marks at americancentury.com)
Date: Fri, 1 Sep 2006 16:54:47 -0500
Subject: [R] cumulative growth rates indexed to a common starting point
 over n series of observations
In-Reply-To: <17656.37174.657814.437306@basebud.nulle.part>
Message-ID: <OFD83C1694.62CEC19E-ON862571DC.00779DAC-862571DC.00785F65@americancentury.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060901/1f8676ef/attachment.pl 

From ggrothendieck at gmail.com  Sat Sep  2 00:06:29 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 1 Sep 2006 18:06:29 -0400
Subject: [R] write a table to file with unequal length of lists
In-Reply-To: <44F8980D.9050004@scharp.org>
References: <44F8980D.9050004@scharp.org>
Message-ID: <971536df0609011506l7c0a50afpa3b60beab4256e6c@mail.gmail.com>

A simple for loop would do it:

x <- 1:2; a <- list(4:5, 8:10) # test data

cat("x a\n", file = "") # only if you want a header
for(i in seq(along = x)) cat(x[i], a[[i]], "\n", file = "")


On 9/1/06, Zhu Wang <zwang at scharp.org> wrote:
> Dear R helpers,
>
> To illustrate my problem, here is a simplified example. I want to write
> a table to a file similar to:
> x                      a
> 1                      4,5
> 2                      8,9,10
>
> Note the length of elements of "a" is 2 and 3 respectively.  This can be
> created by, for example,
> x <- c(1,2)
> a <- NULL
> a[1] <- list(c(4,5))
> a[2] <- list(c(8,9,10)
>
> Any suggestions to write such a table to file would be appreciated.
>
> Thanks,
>
> Zhu Wang
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ripley at stats.ox.ac.uk  Sat Sep  2 00:11:34 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 1 Sep 2006 23:11:34 +0100 (BST)
Subject: [R] Help with singular value decomposition
In-Reply-To: <4f31b0bd0609011351g395019c0o637d7b91e9a7c785@mail.gmail.com>
References: <4f31b0bd0609011351g395019c0o637d7b91e9a7c785@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0609012306240.23924@gannet.stats.ox.ac.uk>

R's svd does this by default (at least according to Wikipedia's 
definition).  Take a closer look at the help page, and in particular 'nu' 
and 'nv'.

On Fri, 1 Sep 2006, Lord Tyranus wrote:

> Hi wizards, I have seen the function svd of R for singular value
> decomposition, but I need to computes the ``economy size'' or ``thin''
> singular value decomposition of a matrix in R. Somebody knows how to
> do that?. Thanks in advance.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From zwang at scharp.org  Sat Sep  2 00:22:56 2006
From: zwang at scharp.org (Zhu Wang)
Date: Fri, 01 Sep 2006 15:22:56 -0700
Subject: [R] write a table to file with unequal length of lists
In-Reply-To: <971536df0609011506l7c0a50afpa3b60beab4256e6c@mail.gmail.com>
References: <44F8980D.9050004@scharp.org>
	<971536df0609011506l7c0a50afpa3b60beab4256e6c@mail.gmail.com>
Message-ID: <44F8B2C0.5050700@scharp.org>

Apparently I missed cat. Thanks. Zhu Wang

Gabor Grothendieck wrote:
> A simple for loop would do it:
>
> x <- 1:2; a <- list(4:5, 8:10) # test data
>
> cat("x a\n", file = "") # only if you want a header
> for(i in seq(along = x)) cat(x[i], a[[i]], "\n", file = "")
>
>
> On 9/1/06, Zhu Wang <zwang at scharp.org> wrote:
>> Dear R helpers,
>>
>> To illustrate my problem, here is a simplified example. I want to write
>> a table to a file similar to:
>> x                      a
>> 1                      4,5
>> 2                      8,9,10
>>
>> Note the length of elements of "a" is 2 and 3 respectively.  This can be
>> created by, for example,
>> x <- c(1,2)
>> a <- NULL
>> a[1] <- list(c(4,5))
>> a[2] <- list(c(8,9,10)
>>
>> Any suggestions to write such a table to file would be appreciated.
>>
>> Thanks,
>>
>> Zhu Wang
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>


From ripley at stats.ox.ac.uk  Sat Sep  2 00:32:06 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 1 Sep 2006 23:32:06 +0100 (BST)
Subject: [R] difference between ns and bs in predict.glm
In-Reply-To: <1c6126db0609011330s229e09a3u27c0f6fe2e9218db@mail.gmail.com>
References: <1c6126db0609011330s229e09a3u27c0f6fe2e9218db@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0609012319020.23924@gannet.stats.ox.ac.uk>

Your example is not actually a regression, and not reproducible.  Here is 
one that is both:

> fm1 <- lm(weight ~ ns(height, df = 5), data = women)
> predict(fm1, newdata=women[1,], se=TRUE)
Error: variable 'ns(height, df = 5)' was fitted with class "nmatrix.5" but 
class "nmatrix.1" was supplied
In addition: Warning message:
'newdata' had 1 rows but variable(s) found have 5 rows

It is only a problem if you try to predict from a single case.
Now take a look at

> attr(terms(fm1), "predvars")[[3]]
ns(height, knots = c(60.8, 63.6, 66.4, 69.2), Boundary.knots = c(58, 72), 
   intercept = FALSE)

If you apply that, you will get strange results: it is a bug in ns()  when 
applied to a length-one variable: a drop=TRUE is missing in

    basis <- as.matrix((t(qr.qty(qr.const, t(basis))))[,  - (1:2)])

On Fri, 1 Sep 2006, Spencer Jones wrote:

> I am fittling a spline to a variable in a regression model, I am then using
> the predict.glm funtion to make some predictions. When I use bs to fit the
> spline I don't have any problems using the predict.glm function however when
> I use ns I get the following error:
> 
> 
> Error in model.frame(formula, rownames, variables, varnames, extras,
> extranames,  :
>         variable lengths differ (found for 'ns(DY, df = 6)')
> In addition: Warning message:
> 'newdata' had 1 rows but variable(s) found have 6 rows
> 
> so for whatever reason this code works
> 
> 
> model. <- glm.nb(CNT ~ WKDY + bs(DY,df=6) + H_FLAG + NH_FLAG + Trend)
> predict(model,newdata=data[i,1:10],type="response",se=TRUE)
> 
> but this code does not work
> 
> model. <- glm.nb(CNT ~ WKDY + ns(DY,df=6) + H_FLAG + NH_FLAG + Trend)
> predict(model,newdata=data[i,1:10],type="response",se=TRUE)
> 
> the two are identical aside from bs vs ns. I looked at the R help and from
> what I could tell, both functions are based on splines.des and they output a
> matrix of the same dimension.
> Any feedback would be appreciated.
> 
> thanks,
> 
> Spencer
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From d.scott at auckland.ac.nz  Sat Sep  2 00:34:03 2006
From: d.scott at auckland.ac.nz (David Scott)
Date: Sat, 2 Sep 2006 10:34:03 +1200 (NZST)
Subject: [R] Lattice plot with fitted curves
In-Reply-To: <eb555e660609011110w742843b4y243cca9a4659dc14@mail.gmail.com>
References: <Pine.LNX.4.61.0609020410100.14144@stat12.stat.auckland.ac.nz>
	<eb555e660609011110w742843b4y243cca9a4659dc14@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0609021031010.15513@stat12.stat.auckland.ac.nz>

On Fri, 1 Sep 2006, Deepayan Sarkar wrote:

> On 9/1/06, David Scott <d.scott at auckland.ac.nz> wrote:
>> 
>> I have some data which consists of time series for a number of sites. It
>> appears that there is not much autocorrelation in the data and I have
>> fitted a cubic for each site using lm. I would like to obtain a lattice
>> plot with one panel for each site and showing the original data, and the
>> fitted cubic.
>> 
>> The closest I have got to doing what I want is:
>> 
>> fit <- fitted(paraslm1)
>> temp <- cbind(paras, fit)
>> xyplot(Density ~ Year | LocCode, data = temp,
>>         panel = function(x, y, ...){
>>           panel.xyplot(x,y)
>>           panel.xyplot(x[LocCode],fit[LocCode],type="l")
>>         })
>> 
>> This doesn't give an error (most of my other attempts did), and draws the
>> panels correctly with the original data, but doesn't draw the fitted
>> lines.
>> 
>> paraslm1 is my fitted linear model with cubics for each location.
>
> I'm guessing since I don't have actual data to work with, but the
> following is probably what you are trying to do:
>
> xyplot(Density + fit ~ Year | LocCode, data = temp,
>      panel = panel.superpose.2,
>      type = c('p', 'l'))
>
> -Deepayan
>

Your guess was spot on Deepayan. That gave exactly what I wanted.
I was not even close to this solution.

Many thanks

David

_________________________________________________________________
David Scott	Visiting (July 06 to January 07)
 		Department of Probability and Statistics
 		The University of Sheffield
 		The Hicks Building
 		Hounsfield Road
 		Sheffield S3 7RH
 		United Kingdom
Phone:	+44 114 222 3908
Email:	d.scott at auckland.ac.nz


From d.scott at auckland.ac.nz  Sat Sep  2 00:38:49 2006
From: d.scott at auckland.ac.nz (David Scott)
Date: Sat, 2 Sep 2006 10:38:49 +1200 (NZST)
Subject: [R] Lattice plot with fitted curves
In-Reply-To: <f8e6ff050609011403o1c7b20d0h19e22a86640c4737@mail.gmail.com>
References: <Pine.LNX.4.61.0609020410100.14144@stat12.stat.auckland.ac.nz>
	<f8e6ff050609011403o1c7b20d0h19e22a86640c4737@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0609021035000.15513@stat12.stat.auckland.ac.nz>

On Fri, 1 Sep 2006, hadley wickham wrote:

>> I have some data which consists of time series for a number of sites. It
>> appears that there is not much autocorrelation in the data and I have
>> fitted a cubic for each site using lm. I would like to obtain a lattice
>> plot with one panel for each site and showing the original data, and the
>> fitted cubic.

I haven't looked at ggplot yet so it might take a while to explore this 
solution, especially since Deepayan's worked for me.

Seems like ggplot is interesting though.

Thanks

David

>
> This is very easy to do with ggplot:
>
> install.packages("ggplot")
> library(ggplot)
>
> p <- ggplot(temp, . ~ LocCode, aes=list(y=Density, x=Year))
> (p <- ggpoint(p))
> ggsmooth(p, method=lm, formula=y~poly(x,3))
>
> You can also do it with fitted values in another column.
>
> temp$fitted <- fitted(paraslm1)
>
> p <- ggplot(temp, . ~ LocCode, aes=list(y=Density, x=Year))
> (p <- ggpoint(p))
> ggline(p, aes=list(y=fitted))
>
> Hadley
>

_________________________________________________________________
David Scott	Visiting (July 06 to January 07)
 		Department of Probability and Statistics
 		The University of Sheffield
 		The Hicks Building
 		Hounsfield Road
 		Sheffield S3 7RH
 		United Kingdom
Phone:	+44 114 222 3908
Email:	d.scott at auckland.ac.nz


From AnupTyagi at yahoo.com  Sat Sep  2 06:26:24 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Sat, 2 Sep 2006 04:26:24 +0000 (UTC)
Subject: [R] Tables with Graphical Representations
References: <07E228A5BE53C24CAD490193A7381BBB591182@LP-EXCHVS07.CO.IHC.COM>
Message-ID: <loom.20060902T061352-202@post.gmane.org>

First Graphic in the initial posting: I think this graphic need to be scaled in
a manner so it can be interpreted correctly while going across rows, columns, and
non-contguous cells, or the correct interpretation and way to read this
provided. For example, in the last row one has to read the numbers to
get the correct information out. I it will be good to have documentation that
explains how to read/interpret this graph, otherwise fixed length boxes are
visually confusing. Anupam.


From spencer.graves at pdf.com  Sat Sep  2 09:44:26 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 02 Sep 2006 00:44:26 -0700
Subject: [R] a generic Adaptive Gauss Quadrature function in R?
In-Reply-To: <7.0.1.0.0.20060822154357.0194fa68@virginia.edu>
References: <7.0.1.0.0.20060822154357.0194fa68@virginia.edu>
Message-ID: <44F9365A.4050802@pdf.com>

      I'm not aware of any generic Adaptive Gaussian Quadrature (AGQ) 
function currently available in R, similar to what you describe.  
Certainly, 'nlme' can NOT do this.  The lmer{lme4 / Matrix} and 
glmmML{glmmML} packages both have specialized versions of this embedded 
in their code but not extracted as a separate function, as far as I 
know.  I agree it would be nice to have such.  However, I see two 
problems with it. 

      First, we want to center the approximating Gaussian at some place 
close to the maximum of the integrand over its variables of integration 
given its parameters.  One problem is that the quadrature points is a 
function of this maximum, and the maximum is a function of the 
parameters.  Thus, we throw away potentially relatively expensive 
function evaluations each time we change the center and scaling of the 
approximating Gaussian.  With a genuinely multidimensional integral, 
this can become computationally quite expensive. 

      Second and often more important, AGQ will only perform well if the 
ratio of the integrand to a normal density is adequately approximated by 
a polynomial.  If that's not the case, we do NOT get the accuracy 
apparently promised by the Gaussian quadrature theorems.  I've tried to 
use AGQ for this type of problem and ultimately abandoned it because I 
was not getting the accuracy I needed. 

      Besides this, any kind of adaptive integration is easier with 
nested than crossed factors. 

      I've wanted to explore the possibilities of spline integration to 
overcome both these problems, but I haven't had time to work on that. 

      This is not the answer you wanted, but I hope it helps. 
      Spencer Graves

Lei Liu wrote:
> Hi there,
>
> I am using SAS Proc NLMIXED to maximize a likelihood with 
> multivariate normal random effects. An example is the two part random 
> effects model for repeated measures semi-continous data with a 
> cluster at 0. I use the "model y ~ general(loglike)" statement in 
> Proc NLMIXED, so I can specify a general log likelihood function 
> constructed by SAS programming statements. Then the likelihood can be 
> maximized by AGQ. Is there a similar generic AGQ function in R to let 
> me write explicitly the log likelihood and then maximize it 
> accordingly? Can nlme do the work? Thanks!
>
> Lei Liu
> Assistant Professor
> Division of Biostatistics and Epidemiology
> Department of Public Health Sciences
> School of Medicine
> University of Virginia
>
> 3181 Hospital West Complex
> Charlottesville, VA 22908-0717
>
> 1-434-982-3364 (o)
> 1-434-806-8086 (c)
> 1-434-243-5787 (f)
>
> liulei at virginia.edu
> ll9f at virginia.edu
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From spencer.graves at pdf.com  Sat Sep  2 10:12:34 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 02 Sep 2006 01:12:34 -0700
Subject: [R] nonlinear least squares trust region fitting ?
In-Reply-To: <1093419308.370791156344904948.JavaMail.nobody@mail09.abv.bg>
References: <1093419308.370791156344904948.JavaMail.nobody@mail09.abv.bg>
Message-ID: <44F93CF2.7080600@pdf.com>

      1.  "Port" is NOT and R package but something more generally 
available.  I just got 189 hits from Google for "nl2sol Port package", 
some of which should answer your questions about that. 

      2.  Have you considered 'nlminb' and 'optim'?  There is also a 
"sequential quadratic programming" algorithm embedded in the code for 
'garchFit{fSeries}'. 

      3.  If you'd like more help from this listserve, please post 
another question.  When you do so, please provide commented, minimal, 
self-contained, reproducible code, as suggested in the posting guide 
"www.R-project.org/posting-guide.html".  Please also help us understand 
why you don't want Gauss-Newton -- in prose as simple and clear as 
possible.  Doing so will increase your chances of a prompt reply that 
will likely be closer to what you want. 

      Hope this helps. 
      Spencer Graves

Martin Ivanov wrote:
> Hello!
>
> I am running R-2.3.1-i386-1 on Slackware Linux 10.2. I am a former matlab user, moving to R. In matlab, via the cftool, I performed nonlinear curve fitting using the method "nonlinear least squares" with the "Trust-Region" algorithm and not using robust fitting. Is it possible to perform the same analysis in R? I read quite a lot of R documentation, but I could not find an alternative solution. If there is such, please forgive my ignorance (I am a newbie in R) and tell me which function from which package is capable of performing the same analysis. If the same analysis is not possible to carry out in R, I would be grateful if you suggest to me some alternative procedure. I found that the "nls" function performs nonlinear least squares. The problem is that I do not want to implement the Gauss-Newton algorithm. In the worst case I would be contented with the "Levenberg-Marquardt" algorithm, if it is implemented in R. R nls's documentation mentions the "port" package and the ?nl
>  2sol? algorithm, but I could not find that package in the CRAN repository, so that I could read and judge whether that algorithm would be appropriate.
>
> Thank you very much in advance. I am looking forward to your answer.
> Regards,
> Martin
>
> -----------------------------------------------------------------
> http://ide.li/ - ?????? ?? ????????? ?? ?????. ??????, ??????, ??????, ??????, ??????????.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From tramni at abv.bg  Sat Sep  2 10:58:09 2006
From: tramni at abv.bg (Martin Ivanov)
Date: Sat, 2 Sep 2006 11:58:09 +0300 (EEST)
Subject: [R] nonlinear least squares fitting Trust-Region"
Message-ID: <1412650035.23521157187489892.JavaMail.nobody@mail04.abv.bg>

Dear Mr Graves,
Thank you very much for your response. Nobody else from this mailing list ventured to reply to me for the two weeks since I posted my question.
"nlminb" and "optim" are just optimization procedures. What I need is not just optimization, but a nonlinear CURVE FITTING procedure. If there is some way to perform nonlinear curve fitting with the "Trust-Region" algorithm using any of these functions, I would me much obliged to you if you suggest to me how to achieve that. You asked me why I do not want Gauss-Newton. Since I am not an expert in the field of optimization, I am just conforming to what matlab documentation suggests, namely:
"Algorithm used for the fitting procedure: Trust-Region -- This is the default algorithm and must be used if you specify coefficient constraints. Levenberg-Marquardt -- If the trust-region algorithm does not produce a reasonable fit, and you do not have coefficient constraints, you should try the Levenberg-Marquardt algorithm. Gauss-Newton --THIS ALGORITHM IS POTENTIALLY FASTER THAN THE OTHER ALGORITHMS, BUT IT ASSUMES THAT THE RESIDUALS ARE CLOSE TO ZERO. IT IS INCLUDED FOR PEDAGOGICAL REASONS AND SHOULD BE THE LAST CHOICE FOR MOST MODELS AND DATA SETS. 
I browsed some literature about the garchfit function, but I did not see the "Trust-Region" algorithm there either: algorithm = c("sqp", "nlminb", "lbfgsb", "nlminb+nm", "lbfgsb+nm"), control = list(), title = NULL, description = NULL, ...)

Thank you for your attention. I am looking forward to your reply.
Regards,
Martin

-----------------------------------------------------------------
vbox7.com - ??????? ????? ???????!


From Ted.Harding at nessie.mcc.ac.uk  Sat Sep  2 11:29:24 2006
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 02 Sep 2006 10:29:24 +0100 (BST)
Subject: [R] Tables with Graphical Representations
In-Reply-To: <loom.20060902T061352-202@post.gmane.org>
Message-ID: <XFMail.060902102924.Ted.Harding@nessie.mcc.ac.uk>

On 02-Sep-06 Anupam Tyagi wrote:
> First Graphic in the initial posting: I think this graphic need
> to be scaled in a manner so it can be interpreted correctly while
> going across rows, columns, and non-contguous cells, or the correct
> interpretation and way to read this provided. For example, in the
> last row one has to read the numbers to get the correct information
> out. It will be good to have documentation that explains how to
> read/interpret this graph, otherwise fixed length boxes are
> visually confusing. Anupam.

You are perhaps asking too much from this kind of graphic.

All graphical displays have both merits and limitations. The design
of the display (if it has been thought out) will be chosen so as to
exhibit what the writer wants the reader to see "immediately", along
with "deeper" detail which can be perceived by taking a longer and
closer look but without demanding too dispersed an attention which
can confuse and overload the reader.

In this particular case, one can very quickely see that some 4 cancers
(Prostate-Melanomas) have quite good survival rates over all 4 5-year
periods. For the next four (Breast-Urinary), though survival is good
for the first 5-year period, it can be seen that it is more variable
for subsequent periods. The next 8 (Cervix-Ovary) have a broadly
similar initial survival rate (50%-75%) with subsequent survival
very variable between different cancers. Then there is a somwhat
suddent jump to the final group of eight (Leukemia-Pancreas) where
initial survival (and therefore longer-term survival) is low.

I think the above summary is all that can be directly derived from
the graphical information, and it may be what the designer wanted
to convey, Or, at least, I hope so -- for, if the deigner wanted
to convey something different then the design has failed.

For instance, one important question is what are the chances of
survival over period 5-10 years, given that one has survived the
first 5 years. One can only get a very approximate and qualitative
idea of this from the graphic (see for instance the above comparison
between cancers 1-4 and cancers 5-8). So this design is bad for
conveying information about this question. A design appropriate for
this would show similar Red/Grey boxes, but now the proportion of
Red would be the probability of survival through the current 5-year
period, conditional on having survived to the beginning of it.

But then it would be difficult to interpret the graphic relative
to the question "what is the survival rate to 5 years, to 10 years,
... ?"

Of course one could combine the two kinds of graphic in the one
display -- a top row of boxes for each cancer as now, and a second
row giving conditional survival rates. But then the eye has trouble
comparing different cancers for one of these two, since there is
visual distraction from the other (a case of requiring dispersed
attention). This could be alleviated by off-setting the second row
to the right of the first row, so that as well as running horizontally
along each row the eye can also run down vertically along the column
for the particular type of survival (unconditional or conditional).
But then the table would become much wider, so there would be problems
about how best to fit it on the page (maybe in landscape).

And so it goes on ...

As to your point about not being able to perceive the numerical
variations in (say) the last row, you have to think about the
technology here. When I view that web page on my screen, I see
boxes about 1cm wide (a little less in fact). A computer screen
has about 5 pixels/mm, so 50 pixels/cm. But the percentages in
the last row: 4.0%, 3.0%, 2.7%, 2.7%, vary over a range 1.3%.
Now a percentage difference of less than 2% simply cannot be
perceived when 1 pixel is at least 2% of the width of the box.
You might argue that this would be helped by having wider boxes,
but they would have to be much wider (by a factor of say 10)
before you could make detailed sense of the last row.

Which goes to show that the main message which can be perceived
in this graphic is in the rather coarse comparisons which can
be made between cancers (and periods) where the rates differ by
fairly substantial amounts -- say at least 10%.

For anything else, you have to look at the numbers anyway. The
merit of this particular design is that you can look at the
boxes without being seriously distracted by the numbers, or look
at the numbers without being seriously distracted by the boxes,
yet both are present at the same time.

In summary: design of a graphic display is literally an art.
What one display can reveal, another will conceal.

Best wishes,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 02-Sep-06                                       Time: 10:29:20
------------------------------ XFMail ------------------------------


From ripley at stats.ox.ac.uk  Sat Sep  2 11:51:03 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 2 Sep 2006 10:51:03 +0100 (BST)
Subject: [R] nonlinear least squares fitting Trust-Region"
In-Reply-To: <1412650035.23521157187489892.JavaMail.nobody@mail04.abv.bg>
References: <1412650035.23521157187489892.JavaMail.nobody@mail04.abv.bg>
Message-ID: <Pine.LNX.4.64.0609021030080.14662@gannet.stats.ox.ac.uk>

I believe people (including me) did not reply because you appeared not to 
have done your homework.  The help page for ?nls _does_ have a reference 
to the 'port' documentation, and RSiteSearch("trust region") is 
informative and leads to an R package that does trust-region optimization.  
(So would looking in the R FAQ.)

You say:

> Since I am not an expert in the field of optimization, I am just 
> conforming to what matlab documentation

Please note that some of the R developers are really expert in that area, 
and their advice (in the R documentation) should be taken as seriously as 
that in some commercial package that is merely commenting about the very 
sparse choice it offers.  Or if R is not in your personal trust region, 
just use 'matlab'.

Please

1) do not shout at your helpers: using all caps is regarded as shouting.

2) study and follow the posting guide.  People are much more likely to 
help you if you demonstrate you have made efforts to help yourself.

3) read the literature.  The R FAQ leads to books that cover fitting 
non-linear models in S/R in considerable detail.


On Sat, 2 Sep 2006, Martin Ivanov wrote:

> Dear Mr Graves,

> Thank you very much for your response. Nobody else from this mailing 
> list ventured to reply to me for the two weeks since I posted my 
> question. "nlminb" and "optim" are just optimization procedures. What I 
> need is not just optimization, but a nonlinear CURVE FITTING procedure.

Which is just optimization: usually by least squares (although you have 
not actually specified that and there are better modern statistical 
ideas).
 
> If there is some way to perform nonlinear curve fitting with the 
> "Trust-Region" algorithm using any of these functions, I would me much 
> obliged to you if you suggest to me how to achieve that. You asked me 
> why I do not want Gauss-Newton. Since I am not an expert in the field of 
> optimization, I am just conforming to what matlab documentation 
> suggests, namely: "Algorithm used for the fitting procedure: 
> Trust-Region -- This is the default algorithm and must be used if you 
> specify coefficient constraints. Levenberg-Marquardt -- If the 
> trust-region algorithm does not produce a reasonable fit, and you do not 
> have coefficient constraints, you should try the Levenberg-Marquardt 
> algorithm. Gauss-Newton --THIS ALGORITHM IS POTENTIALLY FASTER THAN THE 
> OTHER ALGORITHMS, BUT IT ASSUMES THAT THE RESIDUALS ARE CLOSE TO ZERO. 
> IT IS INCLUDED FOR PEDAGOGICAL REASONS AND SHOULD BE THE LAST CHOICE FOR 
> MOST MODELS AND DATA SETS. I browsed some literature about the garchfit 
> function, but I did not see the "Trust-Region" algorithm there either: 
> algorithm = c("sqp", "nlminb", "lbfgsb", "nlminb+nm", "lbfgsb+nm"), 
> control = list(), title = NULL, description = NULL, ...)
> 
> Thank you for your attention. I am looking forward to your reply.
> Regards,
> Martin
> 
> -----------------------------------------------------------------
> vbox7.com - ??????? ????? ???????!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From arinbasu at cashette.com  Sat Sep  2 15:52:48 2006
From: arinbasu at cashette.com (arin basu)
Date: Sat, 2 Sep 2006 06:52:48 -0700 (PDT)
Subject: [R] Cuzick's test for trend
Message-ID: <19439010.1157205168500.JavaMail.Administrator@appsrv>

Hi All:

I was looking for, but could not locate in the packages, or in the R archive searches if there exists an R implementation of Cuzick's test of trend. The test is described as follows:

An extension of the Wilcoxon rank-sum test is developed to handle the situation in which a variable is measured for individuals in three or more (ordered) groups and a non-parametric test for trend across these groups is desired.

Reference:

Cuzick J. A Wilcoxon-type test for trend. Stat Med. 1985 Jan-Mar;4(1):87-90

Would greatly appreciate your insights. The R version I use is R-2.3.1 

Best,
Arin Basu


From rvaradhan at jhmi.edu  Sat Sep  2 16:42:10 2006
From: rvaradhan at jhmi.edu (RAVI VARADHAN)
Date: Sat, 02 Sep 2006 10:42:10 -0400
Subject: [R] nonlinear least squares fitting Trust-Region"
In-Reply-To: <Pine.LNX.4.64.0609021030080.14662@gannet.stats.ox.ac.uk>
References: <1412650035.23521157187489892.JavaMail.nobody@mail04.abv.bg>
	<Pine.LNX.4.64.0609021030080.14662@gannet.stats.ox.ac.uk>
Message-ID: <fcd4fdbd13d.44f96002@johnshopkins.edu>

As suggested by Prof. Ripley, you should read a good book in the optimization area.  One that I would highly recommend is the book by Dennis and Schnabel (1983) - Numerical methods for unconstrained optimization, which does a great job of explaining both "line-search" and "trust-region" approaches for achieving globally-convergent versions of a fast numerical scheme such as Gauss-Newton.

Best,
Ravi.

----- Original Message -----
From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
Date: Saturday, September 2, 2006 5:51 am
Subject: Re: [R] nonlinear least squares fitting Trust-Region"
To: Martin Ivanov <tramni at abv.bg>
Cc: r-help at stat.math.ethz.ch

> I believe people (including me) did not reply because you appeared 
> not to 
> have done your homework.  The help page for ?nls _does_ have a 
> reference 
> to the 'port' documentation, and RSiteSearch("trust region") is 
> informative and leads to an R package that does trust-region 
> optimization.  
> (So would looking in the R FAQ.)
> 
> You say:
> 
> > Since I am not an expert in the field of optimization, I am just 
> > conforming to what matlab documentation
> 
> Please note that some of the R developers are really expert in 
> that area, 
> and their advice (in the R documentation) should be taken as 
> seriously as 
> that in some commercial package that is merely commenting about 
> the very 
> sparse choice it offers.  Or if R is not in your personal trust 
> region, 
> just use 'matlab'.
> 
> Please
> 
> 1) do not shout at your helpers: using all caps is regarded as 
> shouting.
> 2) study and follow the posting guide.  People are much more 
> likely to 
> help you if you demonstrate you have made efforts to help yourself.
> 
> 3) read the literature.  The R FAQ leads to books that cover 
> fitting 
> non-linear models in S/R in considerable detail.
> 
> 
> On Sat, 2 Sep 2006, Martin Ivanov wrote:
> 
> > Dear Mr Graves,
> 
> > Thank you very much for your response. Nobody else from this 
> mailing 
> > list ventured to reply to me for the two weeks since I posted my 
> > question. "nlminb" and "optim" are just optimization procedures. 
> What I 
> > need is not just optimization, but a nonlinear CURVE FITTING 
> procedure.
> Which is just optimization: usually by least squares (although you 
> have 
> not actually specified that and there are better modern 
> statistical 
> ideas).
> 
> > If there is some way to perform nonlinear curve fitting with the 
> > "Trust-Region" algorithm using any of these functions, I would 
> me much 
> > obliged to you if you suggest to me how to achieve that. You 
> asked me 
> > why I do not want Gauss-Newton. Since I am not an expert in the 
> field of 
> > optimization, I am just conforming to what matlab documentation 
> > suggests, namely: "Algorithm used for the fitting procedure: 
> > Trust-Region -- This is the default algorithm and must be used 
> if you 
> > specify coefficient constraints. Levenberg-Marquardt -- If the 
> > trust-region algorithm does not produce a reasonable fit, and 
> you do not 
> > have coefficient constraints, you should try the Levenberg-
> Marquardt 
> > algorithm. Gauss-Newton --THIS ALGORITHM IS POTENTIALLY FASTER 
> THAN THE 
> > OTHER ALGORITHMS, BUT IT ASSUMES THAT THE RESIDUALS ARE CLOSE TO 
> ZERO. 
> > IT IS INCLUDED FOR PEDAGOGICAL REASONS AND SHOULD BE THE LAST 
> CHOICE FOR 
> > MOST MODELS AND DATA SETS. I browsed some literature about the 
> garchfit 
> > function, but I did not see the "Trust-Region" algorithm there 
> either: 
> > algorithm = c("sqp", "nlminb", "lbfgsb", "nlminb+nm", 
> "lbfgsb+nm"), 
> > control = list(), title = NULL, description = NULL, ...)
> > 
> > Thank you for your attention. I am looking forward to your reply.
> > Regards,
> > Martin
> > 
> > -----------------------------------------------------------------
> > vbox7.com - ??????? ????? ???????!
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-
> project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> > 
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.htmland provide commented, minimal, self-contained, 
> reproducible code.
>


From maja.schroeter at gmx.de  Sat Sep  2 16:53:30 2006
From: maja.schroeter at gmx.de (=?iso-8859-1?Q?=22Maja_Schr=F6ter=22?=)
Date: Sat, 02 Sep 2006 16:53:30 +0200
Subject: [R] Dividing objects in classes using function sample()
Message-ID: <20060902145330.150270@gmx.net>

Hello everyone,

I've a problem and dont know how to solve. This is my first posting and it would be fantastic if you could help me.

I want to divide n objects in k classes and need an output with all (n+1)(n+2)/2 possibilities.

For example n=4, k=3:

That would be:
   
   4 0 0
   3 1 0
   3 0 1
   2 2 0
   2 1 1
   2 0 2
   1 3 0 
   1 2 1
   1 1 2
   1 0 3 
   0 3 1 
   0 2 2
   0 1 3
   0 0 4


I tried to you use permn() or sample() but I don't know how to solve.

Please help me!

Best regards,

Maja


-- 


"Feel free" ? 10 GB Mailbox, 100 FreeSMS/Monat ...


From mnair at iusb.edu  Sat Sep  2 18:17:22 2006
From: mnair at iusb.edu (Nair, Murlidharan T)
Date: Sat, 2 Sep 2006 12:17:22 -0400
Subject: [R] venn diagrams
Message-ID: <A32055BDEA88C34BB3DBBCD229380778631209@iu-mssg-mbx109.ads.iu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060902/d027e046/attachment.pl 

From ggrothendieck at gmail.com  Sat Sep  2 18:38:26 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 2 Sep 2006 12:38:26 -0400
Subject: [R] Dividing objects in classes using function sample()
In-Reply-To: <20060902145330.150270@gmx.net>
References: <20060902145330.150270@gmx.net>
Message-ID: <971536df0609020938j56105e24p5f8e9cff0c90e833@mail.gmail.com>

If n and k are small try brute force.

g <- expand.grid(0:4, 0:4, 0:4)
g[rowSums(g) == 4,]

or more generally:

n <- 4; k <- 3
g <- do.call(expand.grid, rep(list(0:n), k))
g[rowSums(g) == n,]

On 9/2/06, "Maja Schr?ter" <maja.schroeter at gmx.de> wrote:
> Hello everyone,
>
> I've a problem and dont know how to solve. This is my first posting and it would be fantastic if you could help me.
>
> I want to divide n objects in k classes and need an output with all (n+1)(n+2)/2 possibilities.
>
> For example n=4, k=3:
>
> That would be:
>
>   4 0 0
>   3 1 0
>   3 0 1
>   2 2 0
>   2 1 1
>   2 0 2
>   1 3 0
>   1 2 1
>   1 1 2
>   1 0 3
>   0 3 1
>   0 2 2
>   0 1 3
>   0 0 4
>
>
> I tried to you use permn() or sample() but I don't know how to solve.
>
> Please help me!
>
> Best regards,
>
> Maja
>
>
> --
>
>
> "Feel free" ? 10 GB Mailbox, 100 FreeSMS/Monat ...
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From spencer.graves at pdf.com  Sat Sep  2 20:04:53 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 02 Sep 2006 11:04:53 -0700
Subject: [R] nonlinear least squares fitting Trust-Region"
In-Reply-To: <fcd4fdbd13d.44f96002@johnshopkins.edu>
References: <1412650035.23521157187489892.JavaMail.nobody@mail04.abv.bg>	<Pine.LNX.4.64.0609021030080.14662@gannet.stats.ox.ac.uk>
	<fcd4fdbd13d.44f96002@johnshopkins.edu>
Message-ID: <44F9C7C5.8020602@pdf.com>

      May I also suggest Bates and Watts (1988) Nonlinear Regression 
Analysis and Its Applications (Wiley).  This book carefully explains the 
difference between "parameter effects" and "intrinsic" curvature in 
non-linear fitting. I don't know if this idea was original with Bates or 
Watts, but I believe that Bates' PhD dissertation made important, 
original contributions to our understanding of it -- and it helped get 
him the faculty position in Statistics at the University of Wisconsin, 
where he still is.  Bates is also a leading contributor to R. 

      hope this helps. 
      spencer graves

RAVI VARADHAN wrote:
> As suggested by Prof. Ripley, you should read a good book in the optimization area.  One that I would highly recommend is the book by Dennis and Schnabel (1983) - Numerical methods for unconstrained optimization, which does a great job of explaining both "line-search" and "trust-region" approaches for achieving globally-convergent versions of a fast numerical scheme such as Gauss-Newton.
>
> Best,
> Ravi.
>
> ----- Original Message -----
> From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
> Date: Saturday, September 2, 2006 5:51 am
> Subject: Re: [R] nonlinear least squares fitting Trust-Region"
> To: Martin Ivanov <tramni at abv.bg>
> Cc: r-help at stat.math.ethz.ch
>
>   
>> I believe people (including me) did not reply because you appeared 
>> not to 
>> have done your homework.  The help page for ?nls _does_ have a 
>> reference 
>> to the 'port' documentation, and RSiteSearch("trust region") is 
>> informative and leads to an R package that does trust-region 
>> optimization.  
>> (So would looking in the R FAQ.)
>>
>> You say:
>>
>>     
>>> Since I am not an expert in the field of optimization, I am just 
>>> conforming to what matlab documentation
>>>       
>> Please note that some of the R developers are really expert in 
>> that area, 
>> and their advice (in the R documentation) should be taken as 
>> seriously as 
>> that in some commercial package that is merely commenting about 
>> the very 
>> sparse choice it offers.  Or if R is not in your personal trust 
>> region, 
>> just use 'matlab'.
>>
>> Please
>>
>> 1) do not shout at your helpers: using all caps is regarded as 
>> shouting.
>> 2) study and follow the posting guide.  People are much more 
>> likely to 
>> help you if you demonstrate you have made efforts to help yourself.
>>
>> 3) read the literature.  The R FAQ leads to books that cover 
>> fitting 
>> non-linear models in S/R in considerable detail.
>>
>>
>> On Sat, 2 Sep 2006, Martin Ivanov wrote:
>>
>>     
>>> Dear Mr Graves,
>>>       
>>> Thank you very much for your response. Nobody else from this 
>>>       
>> mailing 
>>     
>>> list ventured to reply to me for the two weeks since I posted my 
>>> question. "nlminb" and "optim" are just optimization procedures. 
>>>       
>> What I 
>>     
>>> need is not just optimization, but a nonlinear CURVE FITTING 
>>>       
>> procedure.
>> Which is just optimization: usually by least squares (although you 
>> have 
>> not actually specified that and there are better modern 
>> statistical 
>> ideas).
>>
>>     
>>> If there is some way to perform nonlinear curve fitting with the 
>>> "Trust-Region" algorithm using any of these functions, I would 
>>>       
>> me much 
>>     
>>> obliged to you if you suggest to me how to achieve that. You 
>>>       
>> asked me 
>>     
>>> why I do not want Gauss-Newton. Since I am not an expert in the 
>>>       
>> field of 
>>     
>>> optimization, I am just conforming to what matlab documentation 
>>> suggests, namely: "Algorithm used for the fitting procedure: 
>>> Trust-Region -- This is the default algorithm and must be used 
>>>       
>> if you 
>>     
>>> specify coefficient constraints. Levenberg-Marquardt -- If the 
>>> trust-region algorithm does not produce a reasonable fit, and 
>>>       
>> you do not 
>>     
>>> have coefficient constraints, you should try the Levenberg-
>>>       
>> Marquardt 
>>     
>>> algorithm. Gauss-Newton --THIS ALGORITHM IS POTENTIALLY FASTER 
>>>       
>> THAN THE 
>>     
>>> OTHER ALGORITHMS, BUT IT ASSUMES THAT THE RESIDUALS ARE CLOSE TO 
>>>       
>> ZERO. 
>>     
>>> IT IS INCLUDED FOR PEDAGOGICAL REASONS AND SHOULD BE THE LAST 
>>>       
>> CHOICE FOR 
>>     
>>> MOST MODELS AND DATA SETS. I browsed some literature about the 
>>>       
>> garchfit 
>>     
>>> function, but I did not see the "Trust-Region" algorithm there 
>>>       
>> either: 
>>     
>>> algorithm = c("sqp", "nlminb", "lbfgsb", "nlminb+nm", 
>>>       
>> "lbfgsb+nm"), 
>>     
>>> control = list(), title = NULL, description = NULL, ...)
>>>
>>> Thank you for your attention. I am looking forward to your reply.
>>> Regards,
>>> Martin
>>>
>>> -----------------------------------------------------------------
>>> vbox7.com - ??????? ????? ???????!
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-
>>>       
>> project.org/posting-guide.html
>>     
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>       
>> -- 
>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>> University of Oxford,             Tel:  +44 1865 272861 (self)
>> 1 South Parks Road,                     +44 1865 272866 (PA)
>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-
>> guide.htmland provide commented, minimal, self-contained, 
>> reproducible code.
>>
>>     
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From alexnerdy at hotmail.com  Sat Sep  2 21:01:43 2006
From: alexnerdy at hotmail.com (Alexander Nervedi)
Date: Sat, 02 Sep 2006 19:01:43 +0000
Subject: [R] simple matrix division.
In-Reply-To: <19439010.1157205168500.JavaMail.Administrator@appsrv>
Message-ID: <BAY106-F16B1B2A8E3A882193836B8BB3D0@phx.gbl>

Hi

I have

>x<-matrix(c(1,2,3,4), ncol = 2)
>x
     [,1] [,2]
[1,]    1    3
[2,]    2    4

I'd like these two be divided by their column totals. so 1/3 and 2/3 and 3/7 
and 4/7. The obvious
>x/colSums(x)
          [,1]      [,2]
[1,] 0.3333333 1.0000000
[2,] 0.2857143 0.5714286

gives me the wrong results for off diagnal ones,  since it divides the first 
row by 3 and second by 7. The inelegant
>t(t(x)/colSums(x))
          [,1]      [,2]
[1,] 0.3333333 0.4285714
[2,] 0.6666667 0.5714286

gives me the right thing. I was wondering if there is any better way of 
getting what I want.

thanks

Alex

_________________________________________________________________
Get real-time traffic reports with Windows Live Local Search


From ggrothendieck at gmail.com  Sat Sep  2 21:16:56 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 2 Sep 2006 15:16:56 -0400
Subject: [R] simple matrix division.
In-Reply-To: <BAY106-F16B1B2A8E3A882193836B8BB3D0@phx.gbl>
References: <19439010.1157205168500.JavaMail.Administrator@appsrv>
	<BAY106-F16B1B2A8E3A882193836B8BB3D0@phx.gbl>
Message-ID: <971536df0609021216h1d5df0ak99ee1a698730c698@mail.gmail.com>

Try:

prop.table(x, 2)


On 9/2/06, Alexander Nervedi <alexnerdy at hotmail.com> wrote:
> Hi
>
> I have
>
> >x<-matrix(c(1,2,3,4), ncol = 2)
> >x
>     [,1] [,2]
> [1,]    1    3
> [2,]    2    4
>
> I'd like these two be divided by their column totals. so 1/3 and 2/3 and 3/7
> and 4/7. The obvious
> >x/colSums(x)
>          [,1]      [,2]
> [1,] 0.3333333 1.0000000
> [2,] 0.2857143 0.5714286
>
> gives me the wrong results for off diagnal ones,  since it divides the first
> row by 3 and second by 7. The inelegant
> >t(t(x)/colSums(x))
>          [,1]      [,2]
> [1,] 0.3333333 0.4285714
> [2,] 0.6666667 0.5714286
>
> gives me the right thing. I was wondering if there is any better way of
> getting what I want.
>
> thanks
>
> Alex
>
> _________________________________________________________________
> Get real-time traffic reports with Windows Live Local Search
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ripley at stats.ox.ac.uk  Sat Sep  2 21:36:55 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 2 Sep 2006 20:36:55 +0100 (BST)
Subject: [R] simple matrix division.
In-Reply-To: <BAY106-F16B1B2A8E3A882193836B8BB3D0@phx.gbl>
References: <BAY106-F16B1B2A8E3A882193836B8BB3D0@phx.gbl>
Message-ID: <Pine.LNX.4.64.0609022035260.29016@gannet.stats.ox.ac.uk>

On Sat, 2 Sep 2006, Alexander Nervedi wrote:

> Hi
> 
> I have
> 
> >x<-matrix(c(1,2,3,4), ncol = 2)
> >x
>      [,1] [,2]
> [1,]    1    3
> [2,]    2    4
> 
> I'd like these two be divided by their column totals. so 1/3 and 2/3 and 3/7 
> and 4/7. The obvious
> >x/colSums(x)
>           [,1]      [,2]
> [1,] 0.3333333 1.0000000
> [2,] 0.2857143 0.5714286
> 
> gives me the wrong results for off diagnal ones,  since it divides the first 
> row by 3 and second by 7. The inelegant
> >t(t(x)/colSums(x))
>           [,1]      [,2]
> [1,] 0.3333333 0.4285714
> [2,] 0.6666667 0.5714286
> 
> gives me the right thing. I was wondering if there is any better way of 
> getting what I want.

x/rep(colSums(x), each = nrow(x))


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From mnair at iusb.edu  Sat Sep  2 21:48:48 2006
From: mnair at iusb.edu (Nair, Murlidharan T)
Date: Sat, 2 Sep 2006 15:48:48 -0400
Subject: [R] princomp/biplot
References: <BAY106-F16B1B2A8E3A882193836B8BB3D0@phx.gbl>
	<Pine.LNX.4.64.0609022035260.29016@gannet.stats.ox.ac.uk>
Message-ID: <A32055BDEA88C34BB3DBBCD229380778050FDC@iu-mssg-mbx109.ads.iu.edu>

I am getting the following error when I an trying to use princomp
princomp(unique.data)
Error in cov.wt(z) : 'x' must contain finite values only
 
What do I look for?


From rdbisch at gmail.com  Sat Sep  2 23:12:11 2006
From: rdbisch at gmail.com (Rick Bischoff)
Date: Sat, 2 Sep 2006 17:12:11 -0400
Subject: [R] princomp/biplot
In-Reply-To: <A32055BDEA88C34BB3DBBCD229380778050FDC@iu-mssg-mbx109.ads.iu.edu>
References: <BAY106-F16B1B2A8E3A882193836B8BB3D0@phx.gbl>
	<Pine.LNX.4.64.0609022035260.29016@gannet.stats.ox.ac.uk>
	<A32055BDEA88C34BB3DBBCD229380778050FDC@iu-mssg-mbx109.ads.iu.edu>
Message-ID: <D32828F8-5A77-4FB5-AAD0-4C6F56F0EF31@gmail.com>

Infinite values?

On Sep 2, 2006, at 3:48 PM, Nair, Murlidharan T wrote:

> I am getting the following error when I an trying to use princomp
> princomp(unique.data)
> Error in cov.wt(z) : 'x' must contain finite values only
>
> What do I look for?
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From spencer.graves at pdf.com  Sat Sep  2 23:32:24 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 02 Sep 2006 14:32:24 -0700
Subject: [R] Search for best ARIMA model
In-Reply-To: <7260411F9C32E74A86AD9567E64C3301D2FBD0@LI-HAWK.hag.hilti.com>
References: <7260411F9C32E74A86AD9567E64C3301D2FBD0@LI-HAWK.hag.hilti.com>
Message-ID: <44F9F868.5080107@pdf.com>

      I just got 166 hits from RSiteSearch("best fit arima") and 86 from 
RSiteSearch("best fit arima").  Have you tried that? 

      If that does not get you what you want, I might try 'expand.grid' 
plus some hand massage if necessary to create the list of alternative 
models I wanted to consider.  Then I might add a column "AIC" of NAs to 
that using "cbind" to create a data.frame with all the alternatives with 
the result.  Then I might start with something like "fit0 <- arima(lh, 
order = c(0,0,0))" and use 'update(fit0, order=...)' to evaluate each 
one, storing only the AIC.  Then 'which(aic==max(aic))' would identify 
the best fitting alternative(s).  If you haven't already, I suggest you 
review Venables and Ripley (2002) Modern Applied Statistics with S, 4th 
ed. (Springer) on 'expand.grid', 'cbind', and 'update';  if you don't 
already have this book, I highly recommend it. 

      Hope this helps. 
      Spencer Graves

Schweitzer, Markus wrote:
> Hello,
>
> I have a several time series, which I would like to check for their best
> fitted Arima model (I am checking for the lowest aic value).
> Which lets me raise two questions:
>
> 1) is there are more efficient way, than using 6 for-loops?
> 2) sometimes the system cannot calculate  with given parameters - is
> there a more efficient solution than I found?
>
> I hope, you can help me to make this calculation quicker since I have to
> run this function 450 times...
> Thank you very much in advance,
>
> Markus
>
>
> arima.estim <- function(TS) {
> 	best.model <- arima(TS, order = c(1, 0, 0), seasonal =
> list(order = c(0, 0, 0), period = frequency(TS)) )
>
> # Start value
> # I continue with brute force- p, q, r, s are nested from 0 to 3 and i
> and j are nested from 0 to 2. p and  q are not both allowed to be 0.
>
> for (p in 0:3){
>   for( q in 0:3){
>     if(p==0 && q==0) {}
>       else {
>         for(r in 0:3) {
>           for(s in 0:3) {
>             for (j in 0:2) {
>               for(i in 0:2) {
>       
> # test, if series works
>     if(inherits(try(arima(TS, order = c(p, i, q), seasonal = list(order
> = c(r,  j, s), period = frequency(TS)) ), TRUE), 'try-error')){
>         
> 	print(c(p,i,q))} #shows, which parameters didn't work -> will be
> removed by
>        
> 	 else    {
> 	         tmp <- arima(TS, order = c(p, i, q), seasonal =
> list(order = c(r,  j, s), period = frequency(TS)))     # calculate again
> :(
>
>           if(best.model$aic > tmp$aic)
>           {
>                   best.model <- tmp
>           }
>               }
>                  }
>                  }    
>                  }
>                  } } } } 
>                  
>                  best.model}
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From rvaradhan at jhmi.edu  Sun Sep  3 01:36:02 2006
From: rvaradhan at jhmi.edu (RAVI VARADHAN)
Date: Sat, 02 Sep 2006 19:36:02 -0400
Subject: [R] nonlinear least squares fitting Trust-Region"
In-Reply-To: <44F9C7C5.8020602@pdf.com>
References: <1412650035.23521157187489892.JavaMail.nobody@mail04.abv.bg>
	<Pine.LNX.4.64.0609021030080.14662@gannet.stats.ox.ac.uk>
	<fcd4fdbd13d.44f96002@johnshopkins.edu> <44F9C7C5.8020602@pdf.com>
Message-ID: <f488d5cd5624.44f9dd22@johnshopkins.edu>

I think the idea of parameter and intrinsic nonlinearity is due to Beale (JRSSB 1960).  Was he Doug Bates' thesis advisor?

Ravi.

----- Original Message -----
From: Spencer Graves <spencer.graves at pdf.com>
Date: Saturday, September 2, 2006 2:05 pm
Subject: Re: [R] nonlinear least squares fitting Trust-Region"
To: RAVI VARADHAN <rvaradhan at jhmi.edu>
Cc: Prof Brian Ripley <ripley at stats.ox.ac.uk>, Martin Ivanov <tramni at abv.bg>, r-help at stat.math.ethz.ch

>      May I also suggest Bates and Watts (1988) Nonlinear 
> Regression 
> Analysis and Its Applications (Wiley).  This book carefully 
> explains the 
> difference between "parameter effects" and "intrinsic" curvature 
> in 
> non-linear fitting. I don't know if this idea was original with 
> Bates or 
> Watts, but I believe that Bates' PhD dissertation made important, 
> original contributions to our understanding of it -- and it helped 
> get 
> him the faculty position in Statistics at the University of 
> Wisconsin, 
> where he still is.  Bates is also a leading contributor to R. 
> 
>      hope this helps. 
>      spencer graves
> 
> RAVI VARADHAN wrote:
> > As suggested by Prof. Ripley, you should read a good book in the 
> optimization area.  One that I would highly recommend is the book 
> by Dennis and Schnabel (1983) - Numerical methods for 
> unconstrained optimization, which does a great job of explaining 
> both "line-search" and "trust-region" approaches for achieving 
> globally-convergent versions of a fast numerical scheme such as 
> Gauss-Newton.
> >
> > Best,
> > Ravi.
> >
> > ----- Original Message -----
> > From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
> > Date: Saturday, September 2, 2006 5:51 am
> > Subject: Re: [R] nonlinear least squares fitting Trust-Region"
> > To: Martin Ivanov <tramni at abv.bg>
> > Cc: r-help at stat.math.ethz.ch
> >
> >   
> >> I believe people (including me) did not reply because you 
> appeared 
> >> not to 
> >> have done your homework.  The help page for ?nls _does_ have a 
> >> reference 
> >> to the 'port' documentation, and RSiteSearch("trust region") is 
> >> informative and leads to an R package that does trust-region 
> >> optimization.  
> >> (So would looking in the R FAQ.)
> >>
> >> You say:
> >>
> >>     
> >>> Since I am not an expert in the field of optimization, I am 
> just 
> >>> conforming to what matlab documentation
> >>>       
> >> Please note that some of the R developers are really expert in 
> >> that area, 
> >> and their advice (in the R documentation) should be taken as 
> >> seriously as 
> >> that in some commercial package that is merely commenting about 
> >> the very 
> >> sparse choice it offers.  Or if R is not in your personal trust 
> >> region, 
> >> just use 'matlab'.
> >>
> >> Please
> >>
> >> 1) do not shout at your helpers: using all caps is regarded as 
> >> shouting.
> >> 2) study and follow the posting guide.  People are much more 
> >> likely to 
> >> help you if you demonstrate you have made efforts to help yourself.
> >>
> >> 3) read the literature.  The R FAQ leads to books that cover 
> >> fitting 
> >> non-linear models in S/R in considerable detail.
> >>
> >>
> >> On Sat, 2 Sep 2006, Martin Ivanov wrote:
> >>
> >>     
> >>> Dear Mr Graves,
> >>>       
> >>> Thank you very much for your response. Nobody else from this 
> >>>       
> >> mailing 
> >>     
> >>> list ventured to reply to me for the two weeks since I posted 
> my 
> >>> question. "nlminb" and "optim" are just optimization 
> procedures. 
> >>>       
> >> What I 
> >>     
> >>> need is not just optimization, but a nonlinear CURVE FITTING 
> >>>       
> >> procedure.
> >> Which is just optimization: usually by least squares (although 
> you 
> >> have 
> >> not actually specified that and there are better modern 
> >> statistical 
> >> ideas).
> >>
> >>     
> >>> If there is some way to perform nonlinear curve fitting with 
> the 
> >>> "Trust-Region" algorithm using any of these functions, I would 
> >>>       
> >> me much 
> >>     
> >>> obliged to you if you suggest to me how to achieve that. You 
> >>>       
> >> asked me 
> >>     
> >>> why I do not want Gauss-Newton. Since I am not an expert in 
> the 
> >>>       
> >> field of 
> >>     
> >>> optimization, I am just conforming to what matlab 
> documentation 
> >>> suggests, namely: "Algorithm used for the fitting procedure: 
> >>> Trust-Region -- This is the default algorithm and must be used 
> >>>       
> >> if you 
> >>     
> >>> specify coefficient constraints. Levenberg-Marquardt -- If the 
> >>> trust-region algorithm does not produce a reasonable fit, and 
> >>>       
> >> you do not 
> >>     
> >>> have coefficient constraints, you should try the Levenberg-
> >>>       
> >> Marquardt 
> >>     
> >>> algorithm. Gauss-Newton --THIS ALGORITHM IS POTENTIALLY FASTER 
> >>>       
> >> THAN THE 
> >>     
> >>> OTHER ALGORITHMS, BUT IT ASSUMES THAT THE RESIDUALS ARE CLOSE 
> TO 
> >>>       
> >> ZERO. 
> >>     
> >>> IT IS INCLUDED FOR PEDAGOGICAL REASONS AND SHOULD BE THE LAST 
> >>>       
> >> CHOICE FOR 
> >>     
> >>> MOST MODELS AND DATA SETS. I browsed some literature about the 
> >>>       
> >> garchfit 
> >>     
> >>> function, but I did not see the "Trust-Region" algorithm there 
> >>>       
> >> either: 
> >>     
> >>> algorithm = c("sqp", "nlminb", "lbfgsb", "nlminb+nm", 
> >>>       
> >> "lbfgsb+nm"), 
> >>     
> >>> control = list(), title = NULL, description = NULL, ...)
> >>>
> >>> Thank you for your attention. I am looking forward to your reply.
> >>> Regards,
> >>> Martin
> >>>
> >>> ---------------------------------------------------------------
> --
> >>> vbox7.com - ??????? ????? ???????!
> >>>
> >>> ______________________________________________
> >>> R-help at stat.math.ethz.ch mailing list
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide http://www.R-
> >>>       
> >> project.org/posting-guide.html
> >>     
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>>       
> >> -- 
> >> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> >> Professor of Applied Statistics,  
> http://www.stats.ox.ac.uk/~ripley/>> University of Oxford,         
>    Tel:  +44 1865 272861 (self)
> >> 1 South Parks Road,                     +44 1865 272866 (PA)
> >> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide http://www.R-
> project.org/posting-
> >> guide.htmland provide commented, minimal, self-contained, 
> >> reproducible code.
> >>
> >>     
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-
> project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >   
>


From spencer.graves at pdf.com  Sun Sep  3 02:41:21 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 02 Sep 2006 17:41:21 -0700
Subject: [R] nonlinear least squares fitting Trust-Region"
In-Reply-To: <f488d5cd5624.44f9dd22@johnshopkins.edu>
References: <1412650035.23521157187489892.JavaMail.nobody@mail04.abv.bg>	<Pine.LNX.4.64.0609021030080.14662@gannet.stats.ox.ac.uk>	<fcd4fdbd13d.44f96002@johnshopkins.edu>
	<44F9C7C5.8020602@pdf.com> <f488d5cd5624.44f9dd22@johnshopkins.edu>
Message-ID: <44FA24B1.8050402@pdf.com>

Hi, Ravi: 

      Thanks for the reference.  I believe that Bates' thesis adviser 
was Donald G. Watts.  I'm not certain, but I think Watts had a 
connection with the University of Wisconsin, but I'm not certain of 
that.  Maybe someone else will correct / confirm or amplify on this. 

      Best Wishes,
      Spencer

RAVI VARADHAN wrote:
> I think the idea of parameter and intrinsic nonlinearity is due to Beale (JRSSB 1960).  Was he Doug Bates' thesis advisor?
>
> Ravi.
>
> ----- Original Message -----
> From: Spencer Graves <spencer.graves at pdf.com>
> Date: Saturday, September 2, 2006 2:05 pm
> Subject: Re: [R] nonlinear least squares fitting Trust-Region"
> To: RAVI VARADHAN <rvaradhan at jhmi.edu>
> Cc: Prof Brian Ripley <ripley at stats.ox.ac.uk>, Martin Ivanov <tramni at abv.bg>, r-help at stat.math.ethz.ch
>
>   
>>      May I also suggest Bates and Watts (1988) Nonlinear 
>> Regression 
>> Analysis and Its Applications (Wiley).  This book carefully 
>> explains the 
>> difference between "parameter effects" and "intrinsic" curvature 
>> in 
>> non-linear fitting. I don't know if this idea was original with 
>> Bates or 
>> Watts, but I believe that Bates' PhD dissertation made important, 
>> original contributions to our understanding of it -- and it helped 
>> get 
>> him the faculty position in Statistics at the University of 
>> Wisconsin, 
>> where he still is.  Bates is also a leading contributor to R. 
>>
>>      hope this helps. 
>>      spencer graves
>>
>> RAVI VARADHAN wrote:
>>     
>>> As suggested by Prof. Ripley, you should read a good book in the 
>>>       
>> optimization area.  One that I would highly recommend is the book 
>> by Dennis and Schnabel (1983) - Numerical methods for 
>> unconstrained optimization, which does a great job of explaining 
>> both "line-search" and "trust-region" approaches for achieving 
>> globally-convergent versions of a fast numerical scheme such as 
>> Gauss-Newton.
>>     
>>> Best,
>>> Ravi.
>>>
>>> ----- Original Message -----
>>> From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
>>> Date: Saturday, September 2, 2006 5:51 am
>>> Subject: Re: [R] nonlinear least squares fitting Trust-Region"
>>> To: Martin Ivanov <tramni at abv.bg>
>>> Cc: r-help at stat.math.ethz.ch
>>>
>>>   
>>>       
>>>> I believe people (including me) did not reply because you 
>>>>         
>> appeared 
>>     
>>>> not to 
>>>> have done your homework.  The help page for ?nls _does_ have a 
>>>> reference 
>>>> to the 'port' documentation, and RSiteSearch("trust region") is 
>>>> informative and leads to an R package that does trust-region 
>>>> optimization.  
>>>> (So would looking in the R FAQ.)
>>>>
>>>> You say:
>>>>
>>>>     
>>>>         
>>>>> Since I am not an expert in the field of optimization, I am 
>>>>>           
>> just 
>>     
>>>>> conforming to what matlab documentation
>>>>>       
>>>>>           
>>>> Please note that some of the R developers are really expert in 
>>>> that area, 
>>>> and their advice (in the R documentation) should be taken as 
>>>> seriously as 
>>>> that in some commercial package that is merely commenting about 
>>>> the very 
>>>> sparse choice it offers.  Or if R is not in your personal trust 
>>>> region, 
>>>> just use 'matlab'.
>>>>
>>>> Please
>>>>
>>>> 1) do not shout at your helpers: using all caps is regarded as 
>>>> shouting.
>>>> 2) study and follow the posting guide.  People are much more 
>>>> likely to 
>>>> help you if you demonstrate you have made efforts to help yourself.
>>>>
>>>> 3) read the literature.  The R FAQ leads to books that cover 
>>>> fitting 
>>>> non-linear models in S/R in considerable detail.
>>>>
>>>>
>>>> On Sat, 2 Sep 2006, Martin Ivanov wrote:
>>>>
>>>>     
>>>>         
>>>>> Dear Mr Graves,
>>>>>       
>>>>> Thank you very much for your response. Nobody else from this 
>>>>>       
>>>>>           
>>>> mailing 
>>>>     
>>>>         
>>>>> list ventured to reply to me for the two weeks since I posted 
>>>>>           
>> my 
>>     
>>>>> question. "nlminb" and "optim" are just optimization 
>>>>>           
>> procedures. 
>>     
>>>>>       
>>>>>           
>>>> What I 
>>>>     
>>>>         
>>>>> need is not just optimization, but a nonlinear CURVE FITTING 
>>>>>       
>>>>>           
>>>> procedure.
>>>> Which is just optimization: usually by least squares (although 
>>>>         
>> you 
>>     
>>>> have 
>>>> not actually specified that and there are better modern 
>>>> statistical 
>>>> ideas).
>>>>
>>>>     
>>>>         
>>>>> If there is some way to perform nonlinear curve fitting with 
>>>>>           
>> the 
>>     
>>>>> "Trust-Region" algorithm using any of these functions, I would 
>>>>>       
>>>>>           
>>>> me much 
>>>>     
>>>>         
>>>>> obliged to you if you suggest to me how to achieve that. You 
>>>>>       
>>>>>           
>>>> asked me 
>>>>     
>>>>         
>>>>> why I do not want Gauss-Newton. Since I am not an expert in 
>>>>>           
>> the 
>>     
>>>>>       
>>>>>           
>>>> field of 
>>>>     
>>>>         
>>>>> optimization, I am just conforming to what matlab 
>>>>>           
>> documentation 
>>     
>>>>> suggests, namely: "Algorithm used for the fitting procedure: 
>>>>> Trust-Region -- This is the default algorithm and must be used 
>>>>>       
>>>>>           
>>>> if you 
>>>>     
>>>>         
>>>>> specify coefficient constraints. Levenberg-Marquardt -- If the 
>>>>> trust-region algorithm does not produce a reasonable fit, and 
>>>>>       
>>>>>           
>>>> you do not 
>>>>     
>>>>         
>>>>> have coefficient constraints, you should try the Levenberg-
>>>>>       
>>>>>           
>>>> Marquardt 
>>>>     
>>>>         
>>>>> algorithm. Gauss-Newton --THIS ALGORITHM IS POTENTIALLY FASTER 
>>>>>       
>>>>>           
>>>> THAN THE 
>>>>     
>>>>         
>>>>> OTHER ALGORITHMS, BUT IT ASSUMES THAT THE RESIDUALS ARE CLOSE 
>>>>>           
>> TO 
>>     
>>>>>       
>>>>>           
>>>> ZERO. 
>>>>     
>>>>         
>>>>> IT IS INCLUDED FOR PEDAGOGICAL REASONS AND SHOULD BE THE LAST 
>>>>>       
>>>>>           
>>>> CHOICE FOR 
>>>>     
>>>>         
>>>>> MOST MODELS AND DATA SETS. I browsed some literature about the 
>>>>>       
>>>>>           
>>>> garchfit 
>>>>     
>>>>         
>>>>> function, but I did not see the "Trust-Region" algorithm there 
>>>>>       
>>>>>           
>>>> either: 
>>>>     
>>>>         
>>>>> algorithm = c("sqp", "nlminb", "lbfgsb", "nlminb+nm", 
>>>>>       
>>>>>           
>>>> "lbfgsb+nm"), 
>>>>     
>>>>         
>>>>> control = list(), title = NULL, description = NULL, ...)
>>>>>
>>>>> Thank you for your attention. I am looking forward to your reply.
>>>>> Regards,
>>>>> Martin
>>>>>
>>>>> ---------------------------------------------------------------
>>>>>           
>> --
>>     
>>>>> vbox7.com - ??????? ????? ???????!
>>>>>
>>>>> ______________________________________________
>>>>> R-help at stat.math.ethz.ch mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide http://www.R-
>>>>>       
>>>>>           
>>>> project.org/posting-guide.html
>>>>     
>>>>         
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>
>>>>>       
>>>>>           
>>>> -- 
>>>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>>>> Professor of Applied Statistics,  
>>>>         
>> http://www.stats.ox.ac.uk/~ripley/>> University of Oxford,         
>>    Tel:  +44 1865 272861 (self)
>>     
>>>> 1 South Parks Road,                     +44 1865 272866 (PA)
>>>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>>>
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-
>>>>         
>> project.org/posting-
>>     
>>>> guide.htmland provide commented, minimal, self-contained, 
>>>> reproducible code.
>>>>
>>>>     
>>>>         
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-
>>>       
>> project.org/posting-guide.html
>>     
>>> and provide commented, minimal, self-contained, reproducible code.
>>>   
>>>       
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From spencer.graves at pdf.com  Sun Sep  3 02:58:05 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 02 Sep 2006 17:58:05 -0700
Subject: [R] my error with augPred
In-Reply-To: <44EDAC73.16458.584068@localhost>
References: <44EDAC73.16458.584068@localhost>
Message-ID: <44FA289D.8080105@pdf.com>

<comments in line> 

Petr Pikal wrote:
> Dear all
>
> I try to refine my nlme models and with partial success. The model is 
> refined and fitted (using Pinheiro/Bates book as a tutorial) but when 
> I try to plot
>
> plot(augPred(fit4))
>
> I obtain
> Error in predict.nlme(object, value[1:(nrow(value)/nL), , drop = 
> FALSE],  : 
>         Levels (0,3.5],(3.5,5],(5,7],(7,Inf] not allowed for 
> vykon.fac
>   
>
> Is it due to the fact that I have unbalanced design with not all 
> levels of vykon.fac present in all levels of other explanatory factor 
> variable?
>   
I don't know, but I'm skeptical. 
> I try to repeat 8.19 fig which is OK until I try:
>
> fit4 <- update(fit2, fixed = list(A+B~1,xmid~vykon.fac, scal~1),  
> start = c(57, 100, 700, rep(0,3), 13))
>
> I know I should provide an example but maybe somebody will be clever 
> enough to point me to an explanation without it.
>   
I'm not. 

To answer these questions without an example from you, I'd have to make 
up my own example and try to see if I could replicate the error messages 
you report, and I'm not sufficiently concerned about this right now to 
do that. 

Have you tried taking an example from the book and deleting certain rows 
from the data to see if you can force it to reproduce your error? 

Alternatively, have you tried using 'debug' to trace through the code 
line by line until you learn enough of what it's doing to answer your 
question? 

Spencer Graves
> nlme version 3.1-75
> SSfpl model
> R 2.4.0dev (but is the same in 2.3.1), W2000.
>
> Thank you
> Best regards.
>
> Petr PikalPetr Pikal
> petr.pikal at precheza.cz
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From johannes at huesing.name  Sun Sep  3 07:03:24 2006
From: johannes at huesing.name (Johannes =?iso-8859-1?Q?H=FCsing?=)
Date: Sun, 3 Sep 2006 07:03:24 +0200 (CEST)
Subject: [R] Dividing objects in classes using function sample()
In-Reply-To: <20060902145330.150270@gmx.net>
References: <20060902145330.150270@gmx.net>
Message-ID: <62518.85.216.83.151.1157259804.squirrel@mail.panix.com>

> I want to divide n objects in k classes and need an output with all
> (n+1)(n+2)/2 possibilities.

That's the "set of compositions". You may use the partitions package and
proceed from there (provided the brute-force method suggested by Gabor
is not viable').


From spencer.graves at pdf.com  Sun Sep  3 07:05:15 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 02 Sep 2006 22:05:15 -0700
Subject: [R] lmer(): specifying i.i.d random slopes for multiple
	covariates
In-Reply-To: <20060824135832.52370@gmx.net>
References: <20060824135832.52370@gmx.net>
Message-ID: <44FA628B.9000108@pdf.com>

      The first example in the 'lmer' help page is the following: 

     (fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy))

      If this does not answer your question, please provide commented, 
minimal, self-contained, reproducible code with your explanation of why 
and how it does not. 

      Hope this helps. 
      Spencer Graves

Fabian Scheipl wrote:
> Dear readers,
>
> Is it possible to specify a model
>
> y=X %*% beta + Z %*% b ; b=(b_1,..,b_k) and b_i~N(0,v^2) for i=1,..,k
>
> that is, a model where the random slopes for different covariates are i.i.d., in lmer() and how?
>
> In lme() one needs a constant grouping factor (e.g.: all=rep(1,n)) and would then specify:
> lme(fixed= y~X, random= list(all=pdIdent(~Z-1)) ) ,
> that?s how it's done in the lmeSplines- documentation.
>
> Any hints would be greatly appreciated- I'm trying to write a suite of functions that will transform additive models into their mixed-effects representation like lmeSplines but using lmer() instead of lme().
>
> Thank you for your time,
> Fabian Scheipl
>


From attenka at utu.fi  Sun Sep  3 08:43:57 2006
From: attenka at utu.fi (kone)
Date: Sun, 03 Sep 2006 09:43:57 +0300
Subject: [R] From two plot() to two X11-vindows?
Message-ID: <736c99f5f4dfb09d8ac0006392f14c93@local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060903/db9b916c/attachment.pl 

From berwin at maths.uwa.edu.au  Sun Sep  3 08:49:53 2006
From: berwin at maths.uwa.edu.au (Berwin A Turlach)
Date: Sun, 3 Sep 2006 14:49:53 +0800
Subject: [R] From two plot() to two X11-vindows?
In-Reply-To: <736c99f5f4dfb09d8ac0006392f14c93@local>
References: <736c99f5f4dfb09d8ac0006392f14c93@local>
Message-ID: <17658.31505.875326.883450@bossiaea.maths.uwa.edu.au>

>>>>> "AT" == kone  <attenka at utu.fi> writes:

    AT> but how to send the information from different plot- and
    AT> lines-commands to a certain window?
?dev.set 
?dev.next
?dev.prev
?dev.cur

HTH.

Cheers,

        Berwin


From ripley at stats.ox.ac.uk  Sun Sep  3 09:40:27 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 3 Sep 2006 08:40:27 +0100 (BST)
Subject: [R] princomp/biplot
In-Reply-To: <D32828F8-5A77-4FB5-AAD0-4C6F56F0EF31@gmail.com>
References: <BAY106-F16B1B2A8E3A882193836B8BB3D0@phx.gbl>
	<Pine.LNX.4.64.0609022035260.29016@gannet.stats.ox.ac.uk>
	<A32055BDEA88C34BB3DBBCD229380778050FDC@iu-mssg-mbx109.ads.iu.edu>
	<D32828F8-5A77-4FB5-AAD0-4C6F56F0EF31@gmail.com>
Message-ID: <Pine.LNX.4.64.0609030839001.26618@gannet.stats.ox.ac.uk>

On Sat, 2 Sep 2006, Rick Bischoff wrote:

> Infinite values?

Or NA or NaN values

> On Sep 2, 2006, at 3:48 PM, Nair, Murlidharan T wrote:
> 
> > I am getting the following error when I an trying to use princomp
> > princomp(unique.data)
> > Error in cov.wt(z) : 'x' must contain finite values only
> >
> > What do I look for?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From geoffrey.russell at gmail.com  Sun Sep  3 12:50:38 2006
From: geoffrey.russell at gmail.com (Geoff Russell)
Date: Sun, 3 Sep 2006 20:20:38 +0930
Subject: [R] Running cox models
Message-ID: <93c3eada0609030350v1c6429f3g743d86aab65eda9f@mail.gmail.com>

Hi,

I'm reading van Belle et al "Biostatistics" and trying to run a cox test using
a dataset from:

http://faculty.washington.edu/~heagerty/Books/Biostatistics/chapter16.html

(Primary Biliary Cirrhosis data link at top of the page),

I'm using the following code:

--------------- start of code
library(survival)
liver <- scan("liver2.txt",list(age=0,albumin=0,alkphos=0,ascites=0,bili=0,
        cholest=0,edema=0,edmadj=0,hepmeg=0,obstime=0,platelet=0,protime=0,
        sex=0,sgot=0,spiders=0,stage=0,status=0,treatmnt=0,
        triglyc=0,urinecu=0))
fit<-coxph(Surv(obstime,status)~bili+edmadj+albumin+protime+age,data=liver)
summary(fit)
----------------- End of code

but the answer is rather different from that in the book (p.688 - for
anyone with the book).

The book refers to EDTRT, but the dataset has EDMADJ and EDMEMA, also
the book talks about 312 patients and the dataset has 418 lines.

Has anybody else used this dataset?

Cheers,
Geoff Russell


From johannes at huesing.name  Sun Sep  3 13:07:08 2006
From: johannes at huesing.name (Johannes =?iso-8859-1?Q?H=FCsing?=)
Date: Sun, 3 Sep 2006 13:07:08 +0200 (CEST)
Subject: [R] Dividing objects in classes using function sample()
Message-ID: <63274.85.216.83.151.1157281628.squirrel@mail.panix.com>

> I want to divide n objects in k classes and need an output with all
> (n+1)(n+2)/2 possibilities.

That's the "set of compositions". You may use the partitions package and
proceed from there (provided the brute-force method suggested by Gabor
is not viable').


From p.dalgaard at biostat.ku.dk  Sun Sep  3 13:21:19 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 03 Sep 2006 13:21:19 +0200
Subject: [R] Running cox models
In-Reply-To: <93c3eada0609030350v1c6429f3g743d86aab65eda9f@mail.gmail.com>
References: <93c3eada0609030350v1c6429f3g743d86aab65eda9f@mail.gmail.com>
Message-ID: <x23bb9yz4g.fsf@turmalin.kubism.ku.dk>

"Geoff Russell" <geoffrey.russell at gmail.com> writes:

> Hi,
> 
> I'm reading van Belle et al "Biostatistics" and trying to run a cox test using
> a dataset from:
> 
> http://faculty.washington.edu/~heagerty/Books/Biostatistics/chapter16.html
> 
> (Primary Biliary Cirrhosis data link at top of the page),
> 
> I'm using the following code:
> 
> --------------- start of code
> library(survival)
> liver <- scan("liver2.txt",list(age=0,albumin=0,alkphos=0,ascites=0,bili=0,
>         cholest=0,edema=0,edmadj=0,hepmeg=0,obstime=0,platelet=0,protime=0,
>         sex=0,sgot=0,spiders=0,stage=0,status=0,treatmnt=0,
>         triglyc=0,urinecu=0))
> fit<-coxph(Surv(obstime,status)~bili+edmadj+albumin+protime+age,data=liver)
> summary(fit)
> ----------------- End of code
> 
> but the answer is rather different from that in the book (p.688 - for
> anyone with the book).
> 
> The book refers to EDTRT, but the dataset has EDMADJ and EDMEMA, also
> the book talks about 312 patients and the dataset has 418 lines.
> 
> Has anybody else used this dataset?

Have you looked at the pbc dataset that comes with the survival
package? The documentation says 312 randomised and 108 unrandomised
(which should be 106?).

There are some peculiar differences, e.g.

> table(liver$treatmnt,exclude=NULL)

   1    2 <NA>
 157  153  108
> table(pbc$trt, exclude=NULL)

 -9   1   2
106 158 154


(Thomas Lumley was involve in both the book and the package, so
probably knows better.)



-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From msubianto at gmail.com  Sun Sep  3 14:28:55 2006
From: msubianto at gmail.com (Muhammad Subianto)
Date: Sun, 3 Sep 2006 14:28:55 +0200
Subject: [R] Merge list to list - as list
Message-ID: <c7c17cef0609030528y24584adrb59d93c17266e779@mail.gmail.com>

Dear all,
#Last week, I asked about merge x and y as list.
#Now I have a dataset with list of list like:
x <- list(list(matrix(1:20, 5, 4),matrix(1:20, 5, 4)),
          list(matrix(1:20, 5, 4),matrix(1:20, 5, 4)))
y <- list(list(c(1, -1, -1, 1, 1),c(1, 1, -1, -1, -1)),
          list(c(1, 1, 1, 1, 1),c(1, 1, -1, 1, -1)))
x
y

#I need merge x and y, I have tried with
list.uni <- vector("list", length(x))
for (i in 1:length(x)) {
     for (j in 1:length(x[[1]])) {
          list.uni[[i]][[j]] <- mapply(cbind,
                                       x[[i]][[j]],
                                       y[[i]][[j]],
                                       SIMPLIFY=FALSE)
     }
}
list.uni

I have learn about ?lapply, ?sapply and ?mapply but I still didn't
understand how to use it.
I need the result something like


[[1]]
[[1]][[1]]
     [,1] [,2] [,3] [,4] [5]
[1,]    1    6   11   16  1
[2,]    2    7   12   17  -1
[3,]    3    8   13   18  -1
[4,]    4    9   14   19  1
[5,]    5   10   15   20  1

[[1]][[2]]
     [,1] [,2] [,3] [,4] [5]
[1,]    1    6   11   16  1
[2,]    2    7   12   17  1
[3,]    3    8   13   18  -1
[4,]    4    9   14   19  -1
[5,]    5   10   15   20  -1


[[2]]
[[2]][[1]]
     [,1] [,2] [,3] [,4] [5]
[1,]    1    6   11   16  1
[2,]    2    7   12   17  1
[3,]    3    8   13   18  1
[4,]    4    9   14   19  1
[5,]    5   10   15   20  1

[[2]][[2]]
     [,1] [,2] [,3] [,4] [5]
[1,]    1    6   11   16  1
[2,]    2    7   12   17  1
[3,]    3    8   13   18  -1
[4,]    4    9   14   19  1
[5,]    5   10   15   20  -1

Thanks you for any help.
Best wishes, Muhammad Subianto




#Gabor Grothendieck ggrothendieck at gmail.com
#Mon Aug 28 13:53:52 CEST 2006

Here are two ways:

1. use indexes:

lapply(seq(along = x), function(i) cbind(x[[i]], y[[i]]))

2. use mapply:

mapply(cbind, x, y, SIMPLIFY = FALSE)


On 8/28/06, Muhammad Subianto <msubianto at gmail.com> wrote:
> Dear all,
>
> I have dataset
> x <- list(matrix(1:20, 5, 4),matrix(1:20, 5, 4),matrix(1:20, 5, 4))
> y <- list(matrix(110:114, 5, 1),matrix(110:114, 5, 1),matrix(110:114, 5, 1))
>
> I need merge x and y as list (y put in last column).
> The result is something like
>
> [[1]]
>     [,1] [,2] [,3] [,4]  [,5]
> [1,]    1    6   11   16   110
> [2,]    2    7   12   17   111
> [3,]    3    8   13   18   112
> [4,]    4    9   14   19   113
> [5,]    5   10   15   20   114
>
> [[2]]
>     [,1] [,2] [,3] [,4]  [,5]
> [1,]    1    6   11   16   110
> [2,]    2    7   12   17   111
> [3,]    3    8   13   18   112
> [4,]    4    9   14   19   113
> [5,]    5   10   15   20   114
>
> [[3]]
>     [,1] [,2] [,3] [,4]  [,5]
> [1,]    1    6   11   16   110
> [2,]    2    7   12   17   111
> [3,]    3    8   13   18   112
> [4,]    4    9   14   19   113
> [5,]    5   10   15   20   114
>
> I have tried
> a <- list(x,y)
> as.data.frame(t(sapply(a, rbind)))
> lapply(a, function(x) matrix(unlist(x), nrow = length(x), byrow = TRUE))
> but I don't know how to fix it.
>
> Regards, Muhammad Subianto


From mnair at iusb.edu  Sun Sep  3 14:44:47 2006
From: mnair at iusb.edu (Nair, Murlidharan T)
Date: Sun, 3 Sep 2006 08:44:47 -0400
Subject: [R] princomp/biplot
References: <BAY106-F16B1B2A8E3A882193836B8BB3D0@phx.gbl>
	<Pine.LNX.4.64.0609022035260.29016@gannet.stats.ox.ac.uk>
	<A32055BDEA88C34BB3DBBCD229380778050FDC@iu-mssg-mbx109.ads.iu.edu>
	<D32828F8-5A77-4FB5-AAD0-4C6F56F0EF31@gmail.com>
	<Pine.LNX.4.64.0609030839001.26618@gannet.stats.ox.ac.uk>
Message-ID: <A32055BDEA88C34BB3DBBCD229380778050FDD@iu-mssg-mbx109.ads.iu.edu>

Here is part of the data I am looking at I searched for NA and NaN values and did not find any.
 
unique.data<-structure(list(TargetID = structure(c(5, 4, 9, 7, 1, 2, 3, 10, 
6, 8), .Label = c("BGN-1274", "CES1-0939", "CES1-0942", "CLU-0191", 
"CLU-0196", "DRR1-0754", "EPB41-1096", "FASN-1433", "FGFR2-0101", 
"IGSF4-0725"), class = "factor"), Tumor_50_5 = c(24472.28668, 
14927.30306, 4603.214232, 20353.89504, 10800.10031, 8334.506698, 
12935.86589, 18670.69279, 1834.285319, 8061.017148), Tumor_50_5.1 = c(24694.74653, 
16332.21311, 3026.771736, 19711.89242, 9702.909104, 7527.209484, 
13623.2929, 16251.16879, 1101.827804, 7941.266099), Normal_44 = c(17722.98964, 
9847.459893, 4355.325012, 11987.45188, 6749.556025, 6670.343296, 
18217.32106, 11840.09384, 2637.562385, 2897.246729), Normal_44.1 = c(16906.49727, 
10366.3652, 4664.205184, 13010.55209, 5927.638183, 8580.823581, 
17751.28597, 12749.67995, 2865.991941, 2181.529494), Tumor_70_147 = c(3278.127988, 
1840.267195, 668.2195907, 26654.84126, 11859.62749, 1235.222232, 
5953.22946, 20211.21168, 436.3739766, 9346.450426), Tumor_70_147.1 = c(3302.086188, 
1762.572781, 520.8580179, 26857.4145, 12101.58688, 1247.985431, 
4649.722104, 21520.61196, 810.8623424, 7742.528151), Tumor_70_174 = c(8953.042083, 
4680.937373, 868.4611171, 17589.24888, 13761.53356, 3561.61069, 
10889.29719, 18580.46789, 694.8298796, 7034.296512), Tumor_70_174.1 = c(8785.048314, 
5105.333464, 834.0581933, 17740.60095, 13545.72116, 2974.066553, 
11710.38837, 17029.91958, 851.6388904, 8336.363559), Normal_201 = c(13209.97704, 
5987.558991, 4925.912486, 19035.29531, 8877.366633, 3627.623935, 
13091.06176, 17120.43797, 1393.432353, 992.862678), Normal_201.1 = c(12956.36969, 
6957.632081, 4170.711469, 19436.27011, 7488.596222, 3758.197226, 
14258.70552, 15733.29592, 1333.489392, 2644.183317), Tumor_60_21 = c(9119.076822, 
5424.122627, 2729.085504, 20266.57245, 8175.101635, 6707.572283, 
15383.23941, 18514.51632, 3493.213231, 7212.572147), Tumor_60_21.1 = c(9642.858134, 
5482.593811, 3214.152504, 20381.74091, 7681.749628, 7021.018807, 
14560.42309, 17373.35222, 3083.209781, 8084.917274), Normal_46 = c(14977.87599, 
9553.68047, 4699.713431, 16688.94485, 7479.164014, 9391.987572, 
20236.49557, 13428.83993, 3073.093289, 2633.839836), Normal_46.1 = c(16912.53903, 
8222.321115, 5635.427184, 16668.26626, 9137.175471, 8782.531138, 
21818.68236, 12549.00461, 3163.267961, 2274.174218), Normal_148 = c(15082.8995, 
9914.330671, 2888.589499, 17334.89075, 11699.33778, 8702.306051, 
18436.21715, 12974.84012, 2396.640619, 3601.794474), Normal_148.1 = c(17164.53905, 
10458.76035, 2931.573467, 16093.02971, 10021.50548, 4340.399394, 
16816.04525, 13363.93656, 1818.554592, 5187.103679), Tumor_40_177 = c(11958.54077, 
7057.503614, 4858.98125, 14072.54279, 7228.055277, 8058.988591, 
17254.88914, 14533.74148, 3795.679037, 1960.184449), Tumor_40_177.1 = c(14342.34045, 
7043.882552, 5339.987356, 13434.18696, 7775.267671, 7870.574924, 
15709.66122, 14501.01108, 3135.631252, 2779.670904), Normal_196 = c(12268.70303, 
6822.82457, 3430.226128, 15069.9473, 6847.04561, 7603.957287, 
17434.3659, 14678.38995, 3128.989625, 4107.31109), Normal_196.1 = c(13217.37087, 
6603.63231, 3729.266628, 18720.92104, 8377.203613, 8204.241858, 
20083.21434, 15659.35316, 3143.702381, 3234.881929), Tumor_90_202 = c(10448.33092, 
5622.300331, 2602.578141, 23267.11533, 10894.25795, 3164.504413, 
13420.02824, 16470.94781, 1237.868083, 4305.943142), Tumor_90_202.1 = c(12007.05258, 
4173.245287, 1692.14977, 21977.2182, 10268.23279, 2720.971526, 
11929.71067, 17392.77126, 1055.695487, 3900.654748), Normal_22 = c(14727.02707, 
8613.463608, 4520.130357, 17173.11101, 8281.605012, 6869.527442, 
17604.37338, 14543.36345, 3481.023924, 3593.483199), Normal_22.1 = c(12358.47478, 
7393.050914, 4602.829435, 17294.64275, 9111.618581, 7958.214148, 
15552.31912, 13486.77236, 3059.020875, 4655.274728), Normal_121 = c(13617.79644, 
6916.599185, 6734.349423, 15520.9982, 6011.986779, 7281.902116, 
15190.11366, 13322.95415, 2944.596165, 3743.452326), Normal_121.1 = c(14235.29538, 
7957.407174, 4885.430125, 14009.38995, 6373.165212, 7457.273646, 
14866.5746, 13661.51522, 3621.556214, 4363.737452), Normal_155 = c(14855.68181, 
9721.884441, 3369.152739, 16304.03935, 7522.683416, 7597.04418, 
16292.26537, 14041.3317, 1971.238228, 2586.866619), Normal_155.1 = c(15552.13585, 
9886.682712, 3934.632995, 14075.41372, 7455.163957, 6732.60801, 
15313.37004, 13835.98717, 2654.247033, 3198.654039), Tumor_70_189 = c(11487.07368, 
6104.197796, 363.4218849, 24258.67147, 11967.93369, 2493.534501, 
6588.234571, 8381.475617, 1174.815972, 2398.754423), Tumor_70_189.1 = c(11315.73069, 
7122.757874, 594.2384845, 22518.91339, 11272.67261, 1750.295429, 
7990.200557, 9529.124932, 349.4853029, 1590.842831), Tumor_95_197 = c(10154.19749, 
5496.516306, 318.3023324, 22587.84221, 11248.17799, 3016.333383, 
13060.78465, 20922.22061, 1068.696278, 2472.143693), Tumor_95_197.1 = c(10165.06933, 
6639.023708, 376.4619469, 24990.34567, 12562.08115, 3187.349827, 
12307.80303, 20779.99055, 1528.202321, 2564.929166), Tumor_80_204 = c(11417.25194, 
6992.472687, 2743.618114, 22253.30159, 9490.236273, 3821.5234, 
10974.28451, 15290.12188, 952.8203566, 7181.712809), Tumor_80_204.1 = c(11051.7521, 
5288.909975, 2134.634229, 19815.83464, 10435.51024, 1820.467531, 
11419.71779, 13071.72718, 841.5077374, 8431.494196), Normal_30 = c(13297.02158, 
8500.60518, 5069.140899, 19267.91717, 7780.128587, 8993.584733, 
16493.68202, 18095.89001, 2163.938379, 3872.419352), Normal_30.1 = c(13131.55032, 
7514.89843, 5392.00687, 18689.94053, 7418.376584, 7372.264729, 
12894.74179, 17888.42555, 3145.188571, 3559.457417), Normal_133 = c(15902.03785, 
9071.49091, 2194.626729, 13260.94608, 5571.456125, 13558.9189, 
24557.70389, 14873.38379, 5388.67985, 1993.675897), Normal_133.1 = c(14670.27339, 
9728.760275, 2885.095244, 12833.73823, 5989.897654, 11462.32301, 
22748.38048, 15255.67871, 4363.923499, 1955.846897), Tumor_80_167 = c(11439.36157, 
6228.122019, 2692.045314, 20691.32938, 9891.626221, 8729.717239, 
15545.38006, 19142.44432, 2204.506388, 4003.571549), Tumor_80_167.1 = c(10825.26696, 
5427.656524, 1263.104694, 19959.42376, 9652.942183, 8939.622442, 
16169.75522, 19342.73556, 2011.907997, 4656.370956), Tumor_50_192 = c(10913.85735, 
6324.805237, 2507.744772, 18199.45954, 8972.689135, 6472.622133, 
15066.10139, 16956.34253, 2008.115437, 7163.356898), Tumor_50_192.1 = c(12767.91983, 
5963.186631, 2478.20368, 18487.16846, 9013.104829, 5651.851795, 
13018.48653, 15154.98893, 1585.007661, 5885.536464), Tumor_65_198 = c(9594.249395, 
4856.045923, 1135.982305, 25645.99738, 14005.89222, 1653.380607, 
7129.672901, 17964.53461, 1428.551544, 5444.654951), Tumor_65_198.1 = c(7551.650445, 
4523.688987, 999.1185369, 23825.1397, 12264.90773, 915.536689, 
4511.027484, 16627.71506, 1663.709113, 5311.319382)), .Names = c("TargetID", 
"Tumor_50_5", "Tumor_50_5.1", "Normal_44", "Normal_44.1", "Tumor_70_147", 
"Tumor_70_147.1", "Tumor_70_174", "Tumor_70_174.1", "Normal_201", 
"Normal_201.1", "Tumor_60_21", "Tumor_60_21.1", "Normal_46", 
"Normal_46.1", "Normal_148", "Normal_148.1", "Tumor_40_177", 
"Tumor_40_177.1", "Normal_196", "Normal_196.1", "Tumor_90_202", 
"Tumor_90_202.1", "Normal_22", "Normal_22.1", "Normal_121", "Normal_121.1", 
"Normal_155", "Normal_155.1", "Tumor_70_189", "Tumor_70_189.1", 
"Tumor_95_197", "Tumor_95_197.1", "Tumor_80_204", "Tumor_80_204.1", 
"Normal_30", "Normal_30.1", "Normal_133", "Normal_133.1", "Tumor_80_167", 
"Tumor_80_167.1", "Tumor_50_192", "Tumor_50_192.1", "Tumor_65_198", 
"Tumor_65_198.1"), class = "data.frame", row.names = c("1", "2", 
"3", "4", "5", "6", "7", "8", "9", "10"))
pc<-princomp(t(unique.data))
biplot(pc,xlabs=rep("",nrows(t(unique.data))))

________________________________

From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
Sent: Sun 9/3/2006 2:40 AM
To: Rick Bischoff
Cc: Nair, Murlidharan T; r-help at stat.math.ethz.ch
Subject: Re: [R] princomp/biplot



On Sat, 2 Sep 2006, Rick Bischoff wrote:

> Infinite values?

Or NA or NaN values

> On Sep 2, 2006, at 3:48 PM, Nair, Murlidharan T wrote:
>
> > I am getting the following error when I an trying to use princomp
> > princomp(unique.data)
> > Error in cov.wt(z) : 'x' must contain finite values only
> >
> > What do I look for?

--
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From mnair at iusb.edu  Sun Sep  3 14:57:16 2006
From: mnair at iusb.edu (Nair, Murlidharan T)
Date: Sun, 3 Sep 2006 08:57:16 -0400
Subject: [R] princomp/biplot
References: <BAY106-F16B1B2A8E3A882193836B8BB3D0@phx.gbl>
	<Pine.LNX.4.64.0609022035260.29016@gannet.stats.ox.ac.uk>
	<A32055BDEA88C34BB3DBBCD229380778050FDC@iu-mssg-mbx109.ads.iu.edu>
	<D32828F8-5A77-4FB5-AAD0-4C6F56F0EF31@gmail.com>
	<Pine.LNX.4.64.0609030839001.26618@gannet.stats.ox.ac.uk>
Message-ID: <A32055BDEA88C34BB3DBBCD229380778050FE0@iu-mssg-mbx109.ads.iu.edu>

I figured it out. I had to specify the first column as my row names. Please ignore my previous mail

________________________________

From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
Sent: Sun 9/3/2006 2:40 AM
To: Rick Bischoff
Cc: Nair, Murlidharan T; r-help at stat.math.ethz.ch
Subject: Re: [R] princomp/biplot



On Sat, 2 Sep 2006, Rick Bischoff wrote:

> Infinite values?

Or NA or NaN values

> On Sep 2, 2006, at 3:48 PM, Nair, Murlidharan T wrote:
>
> > I am getting the following error when I an trying to use princomp
> > princomp(unique.data)
> > Error in cov.wt(z) : 'x' must contain finite values only
> >
> > What do I look for?

--
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From jholtman at gmail.com  Sun Sep  3 15:09:00 2006
From: jholtman at gmail.com (jim holtman)
Date: Sun, 3 Sep 2006 09:09:00 -0400
Subject: [R] Merge list to list - as list
In-Reply-To: <c7c17cef0609030528y24584adrb59d93c17266e779@mail.gmail.com>
References: <c7c17cef0609030528y24584adrb59d93c17266e779@mail.gmail.com>
Message-ID: <644e1f320609030609s289cf389r492511b343a2e11f@mail.gmail.com>

a slight modification of your routine should work:

#Last week, I asked about merge x and y as list.
#Now I have a dataset with list of list like:
x <- list(list(matrix(1:20, 5, 4),matrix(1:20, 5, 4)),
         list(matrix(1:20, 5, 4),matrix(1:20, 5, 4)))
y <- list(list(c(1, -1, -1, 1, 1),c(1, 1, -1, -1, -1)),
         list(c(1, 1, 1, 1, 1),c(1, 1, -1, 1, -1)))
x
y

#I need merge x and y, I have tried with
list.uni <- vector("list", length(x))
for (i in 1:length(x)) {
    for (j in 1:length(x[[1]])) {
         list.uni[[i]][[j]] <- cbind(x[[i]][[j]],y[[i]][[j]])
    }
}
list.uni


On 9/3/06, Muhammad Subianto <msubianto at gmail.com> wrote:
> Dear all,
> #Last week, I asked about merge x and y as list.
> #Now I have a dataset with list of list like:
> x <- list(list(matrix(1:20, 5, 4),matrix(1:20, 5, 4)),
>          list(matrix(1:20, 5, 4),matrix(1:20, 5, 4)))
> y <- list(list(c(1, -1, -1, 1, 1),c(1, 1, -1, -1, -1)),
>          list(c(1, 1, 1, 1, 1),c(1, 1, -1, 1, -1)))
> x
> y
>
> #I need merge x and y, I have tried with
> list.uni <- vector("list", length(x))
> for (i in 1:length(x)) {
>     for (j in 1:length(x[[1]])) {
>          list.uni[[i]][[j]] <- mapply(cbind,
>                                       x[[i]][[j]],
>                                       y[[i]][[j]],
>                                       SIMPLIFY=FALSE)
>     }
> }
> list.uni
>
> I have learn about ?lapply, ?sapply and ?mapply but I still didn't
> understand how to use it.
> I need the result something like
>
>
> [[1]]
> [[1]][[1]]
>     [,1] [,2] [,3] [,4] [5]
> [1,]    1    6   11   16  1
> [2,]    2    7   12   17  -1
> [3,]    3    8   13   18  -1
> [4,]    4    9   14   19  1
> [5,]    5   10   15   20  1
>
> [[1]][[2]]
>     [,1] [,2] [,3] [,4] [5]
> [1,]    1    6   11   16  1
> [2,]    2    7   12   17  1
> [3,]    3    8   13   18  -1
> [4,]    4    9   14   19  -1
> [5,]    5   10   15   20  -1
>
>
> [[2]]
> [[2]][[1]]
>     [,1] [,2] [,3] [,4] [5]
> [1,]    1    6   11   16  1
> [2,]    2    7   12   17  1
> [3,]    3    8   13   18  1
> [4,]    4    9   14   19  1
> [5,]    5   10   15   20  1
>
> [[2]][[2]]
>     [,1] [,2] [,3] [,4] [5]
> [1,]    1    6   11   16  1
> [2,]    2    7   12   17  1
> [3,]    3    8   13   18  -1
> [4,]    4    9   14   19  1
> [5,]    5   10   15   20  -1
>
> Thanks you for any help.
> Best wishes, Muhammad Subianto
>
>
>
>
> #Gabor Grothendieck ggrothendieck at gmail.com
> #Mon Aug 28 13:53:52 CEST 2006
>
> Here are two ways:
>
> 1. use indexes:
>
> lapply(seq(along = x), function(i) cbind(x[[i]], y[[i]]))
>
> 2. use mapply:
>
> mapply(cbind, x, y, SIMPLIFY = FALSE)
>
>
> On 8/28/06, Muhammad Subianto <msubianto at gmail.com> wrote:
> > Dear all,
> >
> > I have dataset
> > x <- list(matrix(1:20, 5, 4),matrix(1:20, 5, 4),matrix(1:20, 5, 4))
> > y <- list(matrix(110:114, 5, 1),matrix(110:114, 5, 1),matrix(110:114, 5, 1))
> >
> > I need merge x and y as list (y put in last column).
> > The result is something like
> >
> > [[1]]
> >     [,1] [,2] [,3] [,4]  [,5]
> > [1,]    1    6   11   16   110
> > [2,]    2    7   12   17   111
> > [3,]    3    8   13   18   112
> > [4,]    4    9   14   19   113
> > [5,]    5   10   15   20   114
> >
> > [[2]]
> >     [,1] [,2] [,3] [,4]  [,5]
> > [1,]    1    6   11   16   110
> > [2,]    2    7   12   17   111
> > [3,]    3    8   13   18   112
> > [4,]    4    9   14   19   113
> > [5,]    5   10   15   20   114
> >
> > [[3]]
> >     [,1] [,2] [,3] [,4]  [,5]
> > [1,]    1    6   11   16   110
> > [2,]    2    7   12   17   111
> > [3,]    3    8   13   18   112
> > [4,]    4    9   14   19   113
> > [5,]    5   10   15   20   114
> >
> > I have tried
> > a <- list(x,y)
> > as.data.frame(t(sapply(a, rbind)))
> > lapply(a, function(x) matrix(unlist(x), nrow = length(x), byrow = TRUE))
> > but I don't know how to fix it.
> >
> > Regards, Muhammad Subianto
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From ggrothendieck at gmail.com  Sun Sep  3 15:28:09 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 3 Sep 2006 09:28:09 -0400
Subject: [R] Merge list to list - as list
In-Reply-To: <c7c17cef0609030528y24584adrb59d93c17266e779@mail.gmail.com>
References: <c7c17cef0609030528y24584adrb59d93c17266e779@mail.gmail.com>
Message-ID: <971536df0609030628q604a136ak530ac684c3c59d75@mail.gmail.com>

If z is the result then z[[i]] is formed from x[[i]] and y[[i]] using
the previous solution, viz.

z <- list()
z[[1]] <- mapply(cbind, x[[1]], y[[1]], SIMPLIFY = FALSE)
z[[2]] <- mapply(cbind, x[[2]], y[[2]], SIMPLIFY = FALSE)

or with a for loop (which is similar to the code you posted below except
the extraneous j loop is removed since its already incorporated
in the mapply):

z <- list()
for(i in seq(along = x))
   z[[i]] <- mapply(cbind, x[[1]], y[[1]], SIMPLIFY = FALSE)

or reducing the loop to a lapply:

lapply(seq(along = x), function(i) mapply(cbind, x[[i]], y[[i]],
SIMPLIFY = FALSE))



On 9/3/06, Muhammad Subianto <msubianto at gmail.com> wrote:
> Dear all,
> #Last week, I asked about merge x and y as list.
> #Now I have a dataset with list of list like:
> x <- list(list(matrix(1:20, 5, 4),matrix(1:20, 5, 4)),
>          list(matrix(1:20, 5, 4),matrix(1:20, 5, 4)))
> y <- list(list(c(1, -1, -1, 1, 1),c(1, 1, -1, -1, -1)),
>          list(c(1, 1, 1, 1, 1),c(1, 1, -1, 1, -1)))
> x
> y
>
> #I need merge x and y, I have tried with
> list.uni <- vector("list", length(x))
> for (i in 1:length(x)) {
>     for (j in 1:length(x[[1]])) {
>          list.uni[[i]][[j]] <- mapply(cbind,
>                                       x[[i]][[j]],
>                                       y[[i]][[j]],
>                                       SIMPLIFY=FALSE)
>     }
> }
> list.uni
>
> I have learn about ?lapply, ?sapply and ?mapply but I still didn't
> understand how to use it.
> I need the result something like
>
>
> [[1]]
> [[1]][[1]]
>     [,1] [,2] [,3] [,4] [5]
> [1,]    1    6   11   16  1
> [2,]    2    7   12   17  -1
> [3,]    3    8   13   18  -1
> [4,]    4    9   14   19  1
> [5,]    5   10   15   20  1
>
> [[1]][[2]]
>     [,1] [,2] [,3] [,4] [5]
> [1,]    1    6   11   16  1
> [2,]    2    7   12   17  1
> [3,]    3    8   13   18  -1
> [4,]    4    9   14   19  -1
> [5,]    5   10   15   20  -1
>
>
> [[2]]
> [[2]][[1]]
>     [,1] [,2] [,3] [,4] [5]
> [1,]    1    6   11   16  1
> [2,]    2    7   12   17  1
> [3,]    3    8   13   18  1
> [4,]    4    9   14   19  1
> [5,]    5   10   15   20  1
>
> [[2]][[2]]
>     [,1] [,2] [,3] [,4] [5]
> [1,]    1    6   11   16  1
> [2,]    2    7   12   17  1
> [3,]    3    8   13   18  -1
> [4,]    4    9   14   19  1
> [5,]    5   10   15   20  -1
>
> Thanks you for any help.
> Best wishes, Muhammad Subianto
>
>
>
>
> #Gabor Grothendieck ggrothendieck at gmail.com
> #Mon Aug 28 13:53:52 CEST 2006
>
> Here are two ways:
>
> 1. use indexes:
>
> lapply(seq(along = x), function(i) cbind(x[[i]], y[[i]]))
>
> 2. use mapply:
>
> mapply(cbind, x, y, SIMPLIFY = FALSE)
>
>
> On 8/28/06, Muhammad Subianto <msubianto at gmail.com> wrote:
> > Dear all,
> >
> > I have dataset
> > x <- list(matrix(1:20, 5, 4),matrix(1:20, 5, 4),matrix(1:20, 5, 4))
> > y <- list(matrix(110:114, 5, 1),matrix(110:114, 5, 1),matrix(110:114, 5, 1))
> >
> > I need merge x and y as list (y put in last column).
> > The result is something like
> >
> > [[1]]
> >     [,1] [,2] [,3] [,4]  [,5]
> > [1,]    1    6   11   16   110
> > [2,]    2    7   12   17   111
> > [3,]    3    8   13   18   112
> > [4,]    4    9   14   19   113
> > [5,]    5   10   15   20   114
> >
> > [[2]]
> >     [,1] [,2] [,3] [,4]  [,5]
> > [1,]    1    6   11   16   110
> > [2,]    2    7   12   17   111
> > [3,]    3    8   13   18   112
> > [4,]    4    9   14   19   113
> > [5,]    5   10   15   20   114
> >
> > [[3]]
> >     [,1] [,2] [,3] [,4]  [,5]
> > [1,]    1    6   11   16   110
> > [2,]    2    7   12   17   111
> > [3,]    3    8   13   18   112
> > [4,]    4    9   14   19   113
> > [5,]    5   10   15   20   114
> >
> > I have tried
> > a <- list(x,y)
> > as.data.frame(t(sapply(a, rbind)))
> > lapply(a, function(x) matrix(unlist(x), nrow = length(x), byrow = TRUE))
> > but I don't know how to fix it.
> >
> > Regards, Muhammad Subianto
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mnair at iusb.edu  Sun Sep  3 15:34:07 2006
From: mnair at iusb.edu (Nair, Murlidharan T)
Date: Sun, 3 Sep 2006 09:34:07 -0400
Subject: [R] labels in biplot
Message-ID: <A32055BDEA88C34BB3DBBCD22938077863120F@iu-mssg-mbx109.ads.iu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060903/ef47d240/attachment.pl 

From ggrothendieck at gmail.com  Sun Sep  3 16:02:33 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 3 Sep 2006 10:02:33 -0400
Subject: [R] Merge list to list - as list
In-Reply-To: <971536df0609030628q604a136ak530ac684c3c59d75@mail.gmail.com>
References: <c7c17cef0609030528y24584adrb59d93c17266e779@mail.gmail.com>
	<971536df0609030628q604a136ak530ac684c3c59d75@mail.gmail.com>
Message-ID: <971536df0609030702o2efbe74wc90efacaf1ab4a75@mail.gmail.com>

By the way here is another solution.  This one recursively descends
throught the list structure so that it that works regardless
of the nesting of your lists (provided x and y are compatible)
thus it can be used in answer to both the original post and
to this one:

cbind2 <- function(x, y) {
   if (is.list(x))
      mapply(cbind2, x, y, SIMPLIFY = FALSE)
   else
      cbind(x,y)
}

x <- list(list(matrix(1:20, 5, 4),matrix(1:20, 5, 4)),
         list(matrix(1:20, 5, 4),matrix(1:20, 5, 4)))
y <- list(list(c(1, -1, -1, 1, 1),c(1, 1, -1, -1, -1)),
         list(c(1, 1, 1, 1, 1),c(1, 1, -1, 1, -1)))
cbind2(x,y)
cbind2(x[[1]], y[[1]])
cbind2(x[[1]][[1]], y[[1]][[1]])


On 9/3/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> If z is the result then z[[i]] is formed from x[[i]] and y[[i]] using
> the previous solution, viz.
>
> z <- list()
> z[[1]] <- mapply(cbind, x[[1]], y[[1]], SIMPLIFY = FALSE)
> z[[2]] <- mapply(cbind, x[[2]], y[[2]], SIMPLIFY = FALSE)
>
> or with a for loop (which is similar to the code you posted below except
> the extraneous j loop is removed since its already incorporated
> in the mapply):
>
> z <- list()
> for(i in seq(along = x))
>   z[[i]] <- mapply(cbind, x[[1]], y[[1]], SIMPLIFY = FALSE)
>
> or reducing the loop to a lapply:
>
> lapply(seq(along = x), function(i) mapply(cbind, x[[i]], y[[i]],
> SIMPLIFY = FALSE))
>
>
>
> On 9/3/06, Muhammad Subianto <msubianto at gmail.com> wrote:
> > Dear all,
> > #Last week, I asked about merge x and y as list.
> > #Now I have a dataset with list of list like:
> > x <- list(list(matrix(1:20, 5, 4),matrix(1:20, 5, 4)),
> >          list(matrix(1:20, 5, 4),matrix(1:20, 5, 4)))
> > y <- list(list(c(1, -1, -1, 1, 1),c(1, 1, -1, -1, -1)),
> >          list(c(1, 1, 1, 1, 1),c(1, 1, -1, 1, -1)))
> > x
> > y
> >
> > #I need merge x and y, I have tried with
> > list.uni <- vector("list", length(x))
> > for (i in 1:length(x)) {
> >     for (j in 1:length(x[[1]])) {
> >          list.uni[[i]][[j]] <- mapply(cbind,
> >                                       x[[i]][[j]],
> >                                       y[[i]][[j]],
> >                                       SIMPLIFY=FALSE)
> >     }
> > }
> > list.uni
> >
> > I have learn about ?lapply, ?sapply and ?mapply but I still didn't
> > understand how to use it.
> > I need the result something like
> >
> >
> > [[1]]
> > [[1]][[1]]
> >     [,1] [,2] [,3] [,4] [5]
> > [1,]    1    6   11   16  1
> > [2,]    2    7   12   17  -1
> > [3,]    3    8   13   18  -1
> > [4,]    4    9   14   19  1
> > [5,]    5   10   15   20  1
> >
> > [[1]][[2]]
> >     [,1] [,2] [,3] [,4] [5]
> > [1,]    1    6   11   16  1
> > [2,]    2    7   12   17  1
> > [3,]    3    8   13   18  -1
> > [4,]    4    9   14   19  -1
> > [5,]    5   10   15   20  -1
> >
> >
> > [[2]]
> > [[2]][[1]]
> >     [,1] [,2] [,3] [,4] [5]
> > [1,]    1    6   11   16  1
> > [2,]    2    7   12   17  1
> > [3,]    3    8   13   18  1
> > [4,]    4    9   14   19  1
> > [5,]    5   10   15   20  1
> >
> > [[2]][[2]]
> >     [,1] [,2] [,3] [,4] [5]
> > [1,]    1    6   11   16  1
> > [2,]    2    7   12   17  1
> > [3,]    3    8   13   18  -1
> > [4,]    4    9   14   19  1
> > [5,]    5   10   15   20  -1
> >
> > Thanks you for any help.
> > Best wishes, Muhammad Subianto
> >
> >
> >
> >
> > #Gabor Grothendieck ggrothendieck at gmail.com
> > #Mon Aug 28 13:53:52 CEST 2006
> >
> > Here are two ways:
> >
> > 1. use indexes:
> >
> > lapply(seq(along = x), function(i) cbind(x[[i]], y[[i]]))
> >
> > 2. use mapply:
> >
> > mapply(cbind, x, y, SIMPLIFY = FALSE)
> >
> >
> > On 8/28/06, Muhammad Subianto <msubianto at gmail.com> wrote:
> > > Dear all,
> > >
> > > I have dataset
> > > x <- list(matrix(1:20, 5, 4),matrix(1:20, 5, 4),matrix(1:20, 5, 4))
> > > y <- list(matrix(110:114, 5, 1),matrix(110:114, 5, 1),matrix(110:114, 5, 1))
> > >
> > > I need merge x and y as list (y put in last column).
> > > The result is something like
> > >
> > > [[1]]
> > >     [,1] [,2] [,3] [,4]  [,5]
> > > [1,]    1    6   11   16   110
> > > [2,]    2    7   12   17   111
> > > [3,]    3    8   13   18   112
> > > [4,]    4    9   14   19   113
> > > [5,]    5   10   15   20   114
> > >
> > > [[2]]
> > >     [,1] [,2] [,3] [,4]  [,5]
> > > [1,]    1    6   11   16   110
> > > [2,]    2    7   12   17   111
> > > [3,]    3    8   13   18   112
> > > [4,]    4    9   14   19   113
> > > [5,]    5   10   15   20   114
> > >
> > > [[3]]
> > >     [,1] [,2] [,3] [,4]  [,5]
> > > [1,]    1    6   11   16   110
> > > [2,]    2    7   12   17   111
> > > [3,]    3    8   13   18   112
> > > [4,]    4    9   14   19   113
> > > [5,]    5   10   15   20   114
> > >
> > > I have tried
> > > a <- list(x,y)
> > > as.data.frame(t(sapply(a, rbind)))
> > > lapply(a, function(x) matrix(unlist(x), nrow = length(x), byrow = TRUE))
> > > but I don't know how to fix it.
> > >
> > > Regards, Muhammad Subianto
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From ligges at statistik.uni-dortmund.de  Sun Sep  3 16:13:15 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 03 Sep 2006 16:13:15 +0200
Subject: [R] labels in biplot
In-Reply-To: <A32055BDEA88C34BB3DBBCD22938077863120F@iu-mssg-mbx109.ads.iu.edu>
References: <A32055BDEA88C34BB3DBBCD22938077863120F@iu-mssg-mbx109.ads.iu.edu>
Message-ID: <44FAE2FB.4040105@statistik.uni-dortmund.de>



Nair, Murlidharan T wrote:
> I am unable to give % sign in the label. The percent sign is stripped
> off in the plot. Is there a way to force this in? Also can I color the
> labels differently?
> 
>  
> 
> pc <- princomp(USArrests) 
> 
> 
> biplot(pc, xlabs = rep("", nrow(USArrests)))


What about inserting the "%"???

Uwe Ligges


>  
> 
> Thanks ../Murli
> 
>  
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ligges at statistik.uni-dortmund.de  Sun Sep  3 16:15:54 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 03 Sep 2006 16:15:54 +0200
Subject: [R] Reading many files at once
In-Reply-To: <20060901205549.71305.qmail@web38101.mail.mud.yahoo.com>
References: <20060901205549.71305.qmail@web38101.mail.mud.yahoo.com>
Message-ID: <44FAE39A.5070207@statistik.uni-dortmund.de>



Srinivas Iyyer wrote:
> dear group, 
>  
> i have 100 files starting with 'hsa-*'.
> 
> ex. file:
> 
> fruit   p-value 
> ------------
> apple  0.0003
> orange 0.004
> kiwi   0.0003
> peach  0.0004
> 
> 
> 
> I want to read all these files and create a single
> matrix. here each file may have different fruit names.
>  
> in the matrix i want to have a union of all fruits and
> those should be the rows in the matrix and file names
> should be columns. 
> 
> ex;
>         hsa-1   hsa-2  hsa-3  hsa-4  hsa-5
> apple   0.003   0.01   0.002   0.002  0.002
> orange  0.003   0.01   0.002   0.002  0.002
> kiwi    0.003   0.01   0.002   0.002  0.002
> peach   0.003   0.01   0.002   0.002  0.002
> banana  0.003   0.01   0.008   0.002  0.001
> plum    0.003   0.01   0.009   0.002  0.005
> mango   0.003   0.001  0.002   0.002  0.008
> 
>  
> could any one help me please.  


This is a FAQ, more or less, please read the FAQs as the posting guide 
asks you to do.

Use
   myFiles <- list.files( ..., pattern="^hsa-")
to get the filenames, loop ober the files and insert the values into you 
matrix or data.frame.

Uwe Ligges




> thank you.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ligges at statistik.uni-dortmund.de  Sun Sep  3 16:20:37 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 03 Sep 2006 16:20:37 +0200
Subject: [R] repeating the same procedure with a number of files within
 a	directory
In-Reply-To: <20060901164244.10797.qmail@web25507.mail.ukl.yahoo.com>
References: <20060901164244.10797.qmail@web25507.mail.ukl.yahoo.com>
Message-ID: <44FAE4B5.6040006@statistik.uni-dortmund.de>



Ffenics wrote:
> Hi there I am very new to R so dont know much about the programming
> side of thing yet. I've worked out how to input a data matrix, create
> distance matrices and print them to an external file but only for one
> data matrix at a time.
> 
> I actually have a batch of data matrices for which I want to create
> distance matrices for each in turn and then print these distance
> matrices to their respective external files. (I'm wanting them
> printed as CSV files for processing in excel).
> 
> How easy is it to get R to read in files one at a time from a
> directory and do the same thing with all of those files in turn? And
> could someone please give me some advice as to how I may go about
> achieving this if its possible? It would be much appreciated


This is asked very frequently on this list, please do as the posting 
guide suggests:

Read the FAQs (in particular "How can I save the result of each 
iteration in a loop into a separate file?"),
and search the mailing list archives.

You can read ?list.file for your particular problem.

Uwe Ligges


> [[alternative HTML version deleted]]
> 
> ______________________________________________ 
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the
> posting guide http://www.R-project.org/posting-guide.html and provide
> commented, minimal, self-contained, reproducible code.


From msubianto at gmail.com  Sun Sep  3 16:47:13 2006
From: msubianto at gmail.com (Muhammad Subianto)
Date: Sun, 3 Sep 2006 16:47:13 +0200
Subject: [R] Merge list to list - as list
In-Reply-To: <c7c17cef0609030528y24584adrb59d93c17266e779@mail.gmail.com>
References: <c7c17cef0609030528y24584adrb59d93c17266e779@mail.gmail.com>
Message-ID: <c7c17cef0609030747w1aae3143s120c204af3b6c534@mail.gmail.com>

Dear all,
Many thanks to Gabor Grothendieck and Jim Holtman, both of you always
reply (to answer) my problems.

Regards, Muhammad Subianto


##Gabor Grothendieck
If z is the result then z[[i]] is formed from x[[i]] and y[[i]] using
the previous solution, viz.

z <- list()
z[[1]] <- mapply(cbind, x[[1]], y[[1]], SIMPLIFY = FALSE)
z[[2]] <- mapply(cbind, x[[2]], y[[2]], SIMPLIFY = FALSE)

or with a for loop (which is similar to the code you posted below except
the extraneous j loop is removed since its already incorporated
in the mapply):

z <- list()
for(i in seq(along = x))
   z[[i]] <- mapply(cbind, x[[1]], y[[1]], SIMPLIFY = FALSE)

or reducing the loop to a lapply:

lapply(seq(along = x), function(i) mapply(cbind, x[[i]], y[[i]],
SIMPLIFY = FALSE))

## Gabor Grothendieck
cbind2 <- function(x, y) {
   if (is.list(x))
      mapply(cbind2, x, y, SIMPLIFY = FALSE)
   else
      cbind(x,y)
}

x <- list(list(matrix(1:20, 5, 4),matrix(1:20, 5, 4)),
         list(matrix(1:20, 5, 4),matrix(1:20, 5, 4)))
y <- list(list(c(1, -1, -1, 1, 1),c(1, 1, -1, -1, -1)),
         list(c(1, 1, 1, 1, 1),c(1, 1, -1, 1, -1)))
cbind2(x,y)
cbind2(x[[1]], y[[1]])
cbind2(x[[1]][[1]], y[[1]][[1]])

## Jim Holtman
list.uni <- vector("list", length(x))
for (i in 1:length(x)) {
    for (j in 1:length(x[[1]])) {
         list.uni[[i]][[j]] <- cbind(x[[i]][[j]],y[[i]][[j]])
    }
}
list.uni


On 9/3/06, Muhammad Subianto <msubianto at gmail.com> wrote:
> Dear all,
> #Last week, I asked about merge x and y as list.
> #Now I have a dataset with list of list like:
> x <- list(list(matrix(1:20, 5, 4),matrix(1:20, 5, 4)),
>           list(matrix(1:20, 5, 4),matrix(1:20, 5, 4)))
> y <- list(list(c(1, -1, -1, 1, 1),c(1, 1, -1, -1, -1)),
>           list(c(1, 1, 1, 1, 1),c(1, 1, -1, 1, -1)))
> x
> y
>
> #I need merge x and y, I have tried with
> list.uni <- vector("list", length(x))
> for (i in 1:length(x)) {
>      for (j in 1:length(x[[1]])) {
>           list.uni[[i]][[j]] <- mapply(cbind,
>                                        x[[i]][[j]],
>                                        y[[i]][[j]],
>                                        SIMPLIFY=FALSE)
>      }
> }
> list.uni
>
> I have learn about ?lapply, ?sapply and ?mapply but I still didn't
> understand how to use it.
> I need the result something like
>
>
> [[1]]
> [[1]][[1]]
>      [,1] [,2] [,3] [,4] [5]
> [1,]    1    6   11   16  1
> [2,]    2    7   12   17  -1
> [3,]    3    8   13   18  -1
> [4,]    4    9   14   19  1
> [5,]    5   10   15   20  1
>
> [[1]][[2]]
>      [,1] [,2] [,3] [,4] [5]
> [1,]    1    6   11   16  1
> [2,]    2    7   12   17  1
> [3,]    3    8   13   18  -1
> [4,]    4    9   14   19  -1
> [5,]    5   10   15   20  -1
>
>
> [[2]]
> [[2]][[1]]
>      [,1] [,2] [,3] [,4] [5]
> [1,]    1    6   11   16  1
> [2,]    2    7   12   17  1
> [3,]    3    8   13   18  1
> [4,]    4    9   14   19  1
> [5,]    5   10   15   20  1
>
> [[2]][[2]]
>      [,1] [,2] [,3] [,4] [5]
> [1,]    1    6   11   16  1
> [2,]    2    7   12   17  1
> [3,]    3    8   13   18  -1
> [4,]    4    9   14   19  1
> [5,]    5   10   15   20  -1
>
> Thanks you for any help.
> Best wishes, Muhammad Subianto
>
>
>
>
> #Gabor Grothendieck ggrothendieck at gmail.com
> #Mon Aug 28 13:53:52 CEST 2006
>
> Here are two ways:
>
> 1. use indexes:
>
> lapply(seq(along = x), function(i) cbind(x[[i]], y[[i]]))
>
> 2. use mapply:
>
> mapply(cbind, x, y, SIMPLIFY = FALSE)
>
>
> On 8/28/06, Muhammad Subianto <msubianto at gmail.com> wrote:
> > Dear all,
> >
> > I have dataset
> > x <- list(matrix(1:20, 5, 4),matrix(1:20, 5, 4),matrix(1:20, 5, 4))
> > y <- list(matrix(110:114, 5, 1),matrix(110:114, 5, 1),matrix(110:114, 5, 1))
> >
> > I need merge x and y as list (y put in last column).
> > The result is something like
> >
> > [[1]]
> >     [,1] [,2] [,3] [,4]  [,5]
> > [1,]    1    6   11   16   110
> > [2,]    2    7   12   17   111
> > [3,]    3    8   13   18   112
> > [4,]    4    9   14   19   113
> > [5,]    5   10   15   20   114
> >
> > [[2]]
> >     [,1] [,2] [,3] [,4]  [,5]
> > [1,]    1    6   11   16   110
> > [2,]    2    7   12   17   111
> > [3,]    3    8   13   18   112
> > [4,]    4    9   14   19   113
> > [5,]    5   10   15   20   114
> >
> > [[3]]
> >     [,1] [,2] [,3] [,4]  [,5]
> > [1,]    1    6   11   16   110
> > [2,]    2    7   12   17   111
> > [3,]    3    8   13   18   112
> > [4,]    4    9   14   19   113
> > [5,]    5   10   15   20   114
> >
> > I have tried
> > a <- list(x,y)
> > as.data.frame(t(sapply(a, rbind)))
> > lapply(a, function(x) matrix(unlist(x), nrow = length(x), byrow = TRUE))
> > but I don't know how to fix it.
> >
> > Regards, Muhammad Subianto
>


From laurentRhelp at free.fr  Sun Sep  3 17:41:33 2006
From: laurentRhelp at free.fr (Laurent Rhelp)
Date: Sun, 03 Sep 2006 17:41:33 +0200
Subject: [R] lattice and several groups
In-Reply-To: <971536df0608302113v60d56659lf7c57f86ee2a81c4@mail.gmail.com>
References: <44F4A0FF.2010708@free.fr>	
	<971536df0608291728o7435696ew121a6b3843b51c94@mail.gmail.com>	
	<971536df0608291801i69a47e5bscd5157db05507cb9@mail.gmail.com>	
	<44F60D2C.5050201@free.fr>	
	<971536df0608301844h5ff2305cjb689baaa0895f75d@mail.gmail.com>	
	<971536df0608302004q1f100729ha4b9b73859d2e529@mail.gmail.com>
	<971536df0608302113v60d56659lf7c57f86ee2a81c4@mail.gmail.com>
Message-ID: <44FAF7AD.7060403@free.fr>

Gabor Grothendieck a ?crit :

> In thinking about this a bit more we can use
> panel.superpose/panel.groups to shorten it:
>
> # define data -- df
>
> # note that your val2 and val3 lines had a syntax
> # so we have commented them out and
> # replaced them as shown.
> n <- 18
> x1 <- seq(1,n)
> val1 <- -2*x1+50
> # val2 <- (-2*(x1-8)2)+100
> val2 <- (-2*(x1-8))+100
> # val3 <- (-2*(x1-8)2)+50
> val3 <- (-2*(x1-8))+50
> y <- c(val1,val2,val3)
> x <- rep(x1,3)
> f1 <- rep(c("mod1","mod2","mod3"),each=n/3)
> f1 <- rep(f1,3)
> f2 <- rep(c("g1","g2","g3"),each=n)
> df <- data.frame(x=x,y=y,f1=f1,f2=f2)
> surveys <-
> factor(c(rep("survey1",n*3),rep("survey2",n*3),rep("survey3",n*3)))
> df <- rbind(df,df,df)
> df <- data.frame(df,surveys=surveys)
>
> # create xyplot
>
> library(lattice)
> library(grid)
>
> # set custom col and pch here
> my.col <- 1:nlevels(df$f2)
> my.pch <- 1:nlevels(df$f1)
>
> pnl <- function(x, y, subscripts, pch, type, ...)
>   panel.xyplot(x, y, type = type, pch = my.pch[df[subscripts, "f1"]], 
> ...)
>     
> xyplot(y ~ x | surveys, data = df, groups = df$f2, type = "b",
>        panel = panel.superpose,
>        panel.groups = pnl,
>        par.settings = list(superpose.line = list(col = my.col),
>           superpose.symbol = list(col = my.col))
> )
>
>
> key1 <- list(border = TRUE, colums = 2, text = list(levels(df$f1)),
>       points = list(pch = my.pch)
> )
>
> key2 <- list(border = TRUE, colums = 2, text = list(levels(df$f2)),
>       lines = list(col = my.col)
> )
>
> draw.key(key1, draw = TRUE, vp = viewport(.9, .9))
> draw.key(key2, draw = TRUE, vp = viewport(.75, .9))
>
>
>
> On 8/30/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
>
>> Or maybe this is what you are looking for where pnl below was
>> created by modifying source to the panel.plot.default in the zoo
>> package (there might be a simpler way):
>>
>>
>> pnl <- function (x, y, subscripts, groups, col, pch, type, ...) {
>>    for (g in levels(groups)) {
>>        idx <- g == groups[subscripts]
>>        if (any(idx))
>>            panel.xyplot(x[idx], y[idx], ..., col = col[subscripts][idx],
>>                pch = pch[subscripts][idx], type = type)
>>    }
>> }
>>
>> xyplot(y ~ x | surveys, data = df, groups = df$f2, type = "b",
>>        col = as.numeric(df$f2), pch = as.numeric(df$f1), panel = pnl)
>>
>>
>> key1 <- list(border = TRUE, colums = 2, text = list(levels(df$f1)),
>>       points = list(pch = 1:nlevels(df$f1))
>> )
>>
>> key2 <- list(border = TRUE, colums = 2, text = list(levels(df$f2)),
>>       points = list(pch = 20, col = 1:nlevels(df$f2))
>> )
>>
>> draw.key(key1, draw = TRUE, vp = viewport(.9, .9))
>> draw.key(key2, draw = TRUE, vp = viewport(.75, .9))
>>
>>
>>
>>
>> On 8/30/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
>> > To handle conditioning on survey we provide a panel function
>> > that subsets col and pch:
>> >
>> > # define test data - df
>> >
>> > # note that your val2 and val3 lines had a syntax
>> > # so we have commented them out and
>> > # replaced them as shown.
>> > n <- 18
>> > x1 <- seq(1,n)
>> > val1 <- -2*x1+50
>> > # val2 <- (-2*(x1-8)2)+100
>> > val2 <- (-2*(x1-8))+100
>> > # val3 <- (-2*(x1-8)2)+50
>> > val3 <- (-2*(x1-8))+50
>> > y <- c(val1,val2,val3)
>> > x <- rep(x1,3)
>> > f1 <- rep(c("mod1","mod2","mod3"),each=n/3)
>> > f1 <- rep(f1,3)
>> > f2 <- rep(c("g1","g2","g3"),each=n)
>> > df <- data.frame(x=x,y=y,f1=f1,f2=f2)
>> > surveys <-
>> > factor(c(rep("survey1",n*3),rep("survey2",n*3),rep("survey3",n*3)))
>> > df <- rbind(df,df,df)
>> > df <- data.frame(df,surveys=surveys)
>> >
>> > # create xyplot
>> >
>> > library(lattice)
>> > library(grid)
>> >
>> > pnl <- function(x, y, groups, subscripts, col, pch, ...)
>> >        panel.xyplot(x, y, col = col[subscripts], pch = 
>> pch[subscripts], ...)
>> >
>> > xyplot(y ~ x | surveys, data = df,
>> >        col = as.numeric(df$f1), pch = as.numeric(df$f2), panel = pnl)
>> >
>> >
>> > key1 <- list(border = TRUE, colums = 2, text = list(levels(df$f1)),
>> >       points = list(pch = 1:nlevels(df$f1))
>> > )
>> >
>> > key2 <- list(border = TRUE, colums = 2, text = list(levels(df$f2)),
>> >       points = list(pch = 20, col = 1:nlevels(df$f2))
>> > )
>> >
>> > # add legend
>> >
>> > draw.key(key1, draw = TRUE, vp = viewport(.9, .9))
>> > draw.key(key2, draw = TRUE, vp = viewport(.75, .9))
>> >
>> >
>> > On 8/30/06, Laurent Rhelp <laurentRhelp at free.fr> wrote:
>> > > Gabor Grothendieck a ?crit :
>> > >
>> > > >Note that before entering this you need:
>> > > >
>> > > >library(lattice)
>> > > >library(grid) # to access the viewport function
>> > > >
>> > > >On 8/29/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
>> > > >
>> > > >
>> > > >>Try this:
>> > > >>
>> > > >>xyplot(val ~ x, data = df, type = "p",
>> > > >>       col = as.numeric(df$f1), pch = as.numeric(df$f2))
>> > > >>
>> > > >>key1 <- list(border = TRUE, colums = 2, text = 
>> list(levels(df$f1)),
>> > > >>       points = list(pch = 1:nlevels(df$f1))
>> > > >>)
>> > > >>
>> > > >>key2 <- list(border = TRUE, colums = 2, text = 
>> list(levels(df$f2)),
>> > > >>       points = list(pch = 20, col = 1:nlevels(df$f2))
>> > > >>)
>> > > >>
>> > > >>trellis.focus("panel", 1, 1)
>> > > >>draw.key(key1, draw = TRUE, vp = viewport(.9, .9))
>> > > >>draw.key(key2, draw = TRUE, vp = viewport(.75, .9))
>> > > >>trellis.unfocus()
>> > > >>
>> > > >>
>> > > >>On 8/29/06, Laurent Rhelp <laurentRhelp at free.fr> wrote:
>> > > >>
>> > > >>
>> > > >>>Dear R-list,
>> > > >>>
>> > > >>>    I would like to use the lattice library to show several 
>> groups on
>> > > >>>the same graph. Here's my example :
>> > > >>>
>> > > >>>## the data
>> > > >>>f1 <- 
>> factor(c("mod1","mod2","mod3"),levels=c("mod1","mod2","mod3"))
>> > > >>>f1 <- rep(f1,3)
>> > > >>>f2 <- 
>> factor(rep(c("g1","g2","g3"),each=3),levels=c("g1","g2","g3"))
>> > > >>>df <- data.frame(val=c(4,3,2,5,4,3,6,5,4), 
>> x=rep(c(1,2,3),3),f1=f1,f2=f2)
>> > > >>>#############################################################
>> > > >>>library(lattice)
>> > > >>>
>> > > >>>para.liste <- trellis.par.get()
>> > > >>>superpose.symbol <- para.liste$superpose.symbol
>> > > >>>superpose.symbol$pch <- c(1,2,3)
>> > > >>>trellis.par.set("superpose.symbol",superpose.symbol)
>> > > >>>
>> > > >>># Now I can see the group according to the f1 factor (with a 
>> different
>> > > >>>symbol for every modality)
>> > > >>>xyplot( val~x,
>> > > >>>       data=df,
>> > > >>>       group=f1,
>> > > >>>       auto.key=list(space="right")
>> > > >>>      )
>> > > >>>
>> > > >>># or I can see the group according to the f2 factor
>> > > >>>xyplot( val~x,
>> > > >>>       data=df,
>> > > >>>       type="l",
>> > > >>>       group=f2,
>> > > >>>       auto.key=list(space="right",points=FALSE,lines=TRUE)
>> > > >>>      )
>> > > >>>
>> > > >>>How can I do to highlight both the f1 and f2 factors on one 
>> panel with
>> > > >>>the legends, using the lattice function ?
>> > > >>>
>> > > >>>Thanks
>> > > >>>
>> > > >>>______________________________________________
>> > > >>>R-help at stat.math.ethz.ch mailing list
>> > > >>>https://stat.ethz.ch/mailman/listinfo/r-help
>> > > >>>PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> > > >>>and provide commented, minimal, self-contained, reproducible 
>> code.
>> > > >>>
>> > > >>>
>> > > >>>
>> > > >
>> > > >______________________________________________
>> > > >R-help at stat.math.ethz.ch mailing list
>> > > >https://stat.ethz.ch/mailman/listinfo/r-help
>> > > >PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> > > >and provide commented, minimal, self-contained, reproducible code.
>> > > >
>> > > >
>> > > >
>> > > >
>> > > Thank you, Gabor. The way to put the two legends is very 
>> interesting.
>> > > For the graphs, in fact, my problem is to fit the data for every 
>> level
>> > > of the f2 factor, showing the levels of the f1 factor in each 
>> panel and
>> > > that for several surveys . Here's an example closer to my actual 
>> data :
>> > >
>> > > ## the data
>> > >
>> > > n <- 18
>> > > x1 <- seq(1,n)
>> > > val1 <- -2*x1+50
>> > > val2 <- (-2*(x1-8)2)+100
>> > > val3 <- (-2*(x1-8)2)+50
>> > > y <- c(val1,val2,val3)
>> > > x <- rep(x1,3)
>> > > f1 <- rep(c("mod1","mod2","mod3"),each=n/3)
>> > > f1 <- rep(f1,3)
>> > > f2 <- rep(c("g1","g2","g3"),each=n)
>> > > df <- data.frame(x=x,y=y,f1=f1,f2=f2)
>> > >
>> > > surveys <-
>> > > factor(c(rep("survey1",n*3),rep("survey2",n*3),rep("survey3",n*3)))
>> > > df <- rbind(df,df,df)
>> > > df <- data.frame(df,surveys=surveys)
>> > > 
>> #######################################################################
>> > > library(lattice)
>> > >
>> > > para.liste <- trellis.par.get()
>> > > superpose.symbol <- para.liste$superpose.symbol
>> > > superpose.symbol$pch <- c(1,2,3)
>> > > trellis.par.set("superpose.symbol",superpose.symbol)
>> > >
>> > > xyplot( y~x | surveys,         data=df,
>> > >       group=f1,
>> > >       auto.key=list(space="right")
>> > >      )
>> > >
>> > > xyplot( y~x | surveys  ,
>> > >       data=df,
>> > >       type="l",
>> > >       group=f2,
>> > >       auto.key=list(space="right",points=FALSE,lines=TRUE)
>> > >      )
>> > >
>> > > Certainly, I have to use the panel function but I don't know how 
>> to mark
>> > > the f1 factor in each panel (I want to fit the values according 
>> to the
>> > > f2 factor) !
>> > >
>> > >
>> > >
>> >
>>
>
>
Thank you for the three solutions. Spending time understanding them 
allows me to well-understand the behavior of the lattice functions. The 
last one is nice but the second one gave me the solution to adapt my 
processing according to the groups which was another aim for me : I 
wanted to do an linear regression for the g1 group and an loess 
regression for the g1, g2 group. So I modified your pnl function as below :


pnl <- function (x, y, subscripts, groups, col, pch, type, ...) {
   for (g in levels(groups)) {
       idx <- g == groups[subscripts]
       if (any(idx)){
           panel.xyplot(x[idx], y[idx], ..., col = col[subscripts][idx],
               pch = pch[subscripts][idx], type = type)

      ## to allow for the treatments according the groups
      switch(g,
        g1 = panel.lmline(x[idx], y[idx], ..., col = col[subscripts][idx],
               pch = pch[subscripts][idx]),
        g2 = panel.loess(x[idx], y[idx], ..., col = col[subscripts][idx],
               pch = pch[subscripts][idx]),
        g3 = panel.loess(x[idx], y[idx], ... , col = col[subscripts][idx],
               pch = pch[subscripts][idx])
            
       )
         }
   }
}
##
##  Finally, with these data
##  (I noticed that my paste failed for the syntax so I wrote (x1-8)*(x1-8))
##
n <- 18
x1 <- seq(1,n)
val1 <- jitter(-2*x1+50,amount=10)
val2 <- jitter((-2*(x1-8)*(x1-8))+100,amount=10)
val3 <- jitter((-2*(x1-8)*(x1-8))+50,amount=10)
y <- c(val1,val2,val3)
x <- rep(x1,3)
f1 <- rep(c("mod1","mod2","mod3"),each=n/3)
f1 <- rep(f1,3)
f2 <- rep(c("g1","g2","g3"),each=n)
df <- data.frame(x=x,y=y,f1=f1,f2=f2)
surveys <-
factor(c(rep("survey1",n*3),rep("survey2",n*3),rep("survey3",n*3)))
df <- rbind(df,df,df)
df <- data.frame(df,surveys=surveys)
##



## the graph

xyplot(y ~ x | surveys, data = df, groups = df$f2, type = "b",
    col = as.numeric(df$f2), pch = as.numeric(df$f1), panel = pnl)


key1 <- list(border = TRUE, colums = 2, text = list(levels(df$f1)),
      points = list(pch = 1:nlevels(df$f1))
)

key2 <- list(border = TRUE, colums = 2, text = list(levels(df$f2)),
      points = list(pch = 20, col = 1:nlevels(df$f2))
)

draw.key(key1, draw = TRUE, vp = viewport(.9, .9))
draw.key(key2, draw = TRUE, vp = viewport(.75, .9))

Thank you very much.
Laurent


From ggrothendieck at gmail.com  Sun Sep  3 17:54:58 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 3 Sep 2006 11:54:58 -0400
Subject: [R] lattice and several groups
In-Reply-To: <44FAF7AD.7060403@free.fr>
References: <44F4A0FF.2010708@free.fr>
	<971536df0608291728o7435696ew121a6b3843b51c94@mail.gmail.com>
	<971536df0608291801i69a47e5bscd5157db05507cb9@mail.gmail.com>
	<44F60D2C.5050201@free.fr>
	<971536df0608301844h5ff2305cjb689baaa0895f75d@mail.gmail.com>
	<971536df0608302004q1f100729ha4b9b73859d2e529@mail.gmail.com>
	<971536df0608302113v60d56659lf7c57f86ee2a81c4@mail.gmail.com>
	<44FAF7AD.7060403@free.fr>
Message-ID: <971536df0609030854q723c40d7xa832dab6456ad97b@mail.gmail.com>

Try this version which corresponds to your latest version
but makes use of panel.groups distinguishing the groups
using group.number:

# set custom col and pch here
my.col <- 1:nlevels(df$f2)
my.pch <- 1:nlevels(df$f1)

pnl <- function(x, y, subscripts, pch, group.number, ...) {
  panel <- c(panel.lmline, panel.loess, panel.loess)[[group.number]]
  panel(x, y, ..., pch = pch[subscripts])
  panel.xyplot(x, y, pch = my.pch[df[subscripts, "f1"]], ...)
}


xyplot(y ~ x | surveys, data = df, groups = df$f2, type = "b",
       panel = panel.superpose,
       panel.groups = pnl,
       par.settings = list(superpose.line = list(col = my.col),
          superpose.symbol = list(col = my.col))

)


key1 <- list(border = TRUE, colums = 2, text = list(levels(df$f1)),
      points = list(pch = my.pch)

)

key2 <- list(border = TRUE, colums = 2, text = list(levels(df$f2)),
      lines = list(col = my.col)
)

draw.key(key1, draw = TRUE, vp = viewport(.9, .9))
draw.key(key2, draw = TRUE, vp = viewport(.75, .9))


On 9/3/06, Laurent Rhelp <laurentRhelp at free.fr> wrote:
> Gabor Grothendieck a ?crit :
>
> > In thinking about this a bit more we can use
> > panel.superpose/panel.groups to shorten it:
> >
> > # define data -- df
> >
> > # note that your val2 and val3 lines had a syntax
> > # so we have commented them out and
> > # replaced them as shown.
> > n <- 18
> > x1 <- seq(1,n)
> > val1 <- -2*x1+50
> > # val2 <- (-2*(x1-8)2)+100
> > val2 <- (-2*(x1-8))+100
> > # val3 <- (-2*(x1-8)2)+50
> > val3 <- (-2*(x1-8))+50
> > y <- c(val1,val2,val3)
> > x <- rep(x1,3)
> > f1 <- rep(c("mod1","mod2","mod3"),each=n/3)
> > f1 <- rep(f1,3)
> > f2 <- rep(c("g1","g2","g3"),each=n)
> > df <- data.frame(x=x,y=y,f1=f1,f2=f2)
> > surveys <-
> > factor(c(rep("survey1",n*3),rep("survey2",n*3),rep("survey3",n*3)))
> > df <- rbind(df,df,df)
> > df <- data.frame(df,surveys=surveys)
> >
> > # create xyplot
> >
> > library(lattice)
> > library(grid)
> >
> > # set custom col and pch here
> > my.col <- 1:nlevels(df$f2)
> > my.pch <- 1:nlevels(df$f1)
> >
> > pnl <- function(x, y, subscripts, pch, type, ...)
> >   panel.xyplot(x, y, type = type, pch = my.pch[df[subscripts, "f1"]],
> > ...)
> >
> > xyplot(y ~ x | surveys, data = df, groups = df$f2, type = "b",
> >        panel = panel.superpose,
> >        panel.groups = pnl,
> >        par.settings = list(superpose.line = list(col = my.col),
> >           superpose.symbol = list(col = my.col))
> > )
> >
> >
> > key1 <- list(border = TRUE, colums = 2, text = list(levels(df$f1)),
> >       points = list(pch = my.pch)
> > )
> >
> > key2 <- list(border = TRUE, colums = 2, text = list(levels(df$f2)),
> >       lines = list(col = my.col)
> > )
> >
> > draw.key(key1, draw = TRUE, vp = viewport(.9, .9))
> > draw.key(key2, draw = TRUE, vp = viewport(.75, .9))
> >
> >
> >
> > On 8/30/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> >
> >> Or maybe this is what you are looking for where pnl below was
> >> created by modifying source to the panel.plot.default in the zoo
> >> package (there might be a simpler way):
> >>
> >>
> >> pnl <- function (x, y, subscripts, groups, col, pch, type, ...) {
> >>    for (g in levels(groups)) {
> >>        idx <- g == groups[subscripts]
> >>        if (any(idx))
> >>            panel.xyplot(x[idx], y[idx], ..., col = col[subscripts][idx],
> >>                pch = pch[subscripts][idx], type = type)
> >>    }
> >> }
> >>
> >> xyplot(y ~ x | surveys, data = df, groups = df$f2, type = "b",
> >>        col = as.numeric(df$f2), pch = as.numeric(df$f1), panel = pnl)
> >>
> >>
> >> key1 <- list(border = TRUE, colums = 2, text = list(levels(df$f1)),
> >>       points = list(pch = 1:nlevels(df$f1))
> >> )
> >>
> >> key2 <- list(border = TRUE, colums = 2, text = list(levels(df$f2)),
> >>       points = list(pch = 20, col = 1:nlevels(df$f2))
> >> )
> >>
> >> draw.key(key1, draw = TRUE, vp = viewport(.9, .9))
> >> draw.key(key2, draw = TRUE, vp = viewport(.75, .9))
> >>
> >>
> >>
> >>
> >> On 8/30/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> >> > To handle conditioning on survey we provide a panel function
> >> > that subsets col and pch:
> >> >
> >> > # define test data - df
> >> >
> >> > # note that your val2 and val3 lines had a syntax
> >> > # so we have commented them out and
> >> > # replaced them as shown.
> >> > n <- 18
> >> > x1 <- seq(1,n)
> >> > val1 <- -2*x1+50
> >> > # val2 <- (-2*(x1-8)2)+100
> >> > val2 <- (-2*(x1-8))+100
> >> > # val3 <- (-2*(x1-8)2)+50
> >> > val3 <- (-2*(x1-8))+50
> >> > y <- c(val1,val2,val3)
> >> > x <- rep(x1,3)
> >> > f1 <- rep(c("mod1","mod2","mod3"),each=n/3)
> >> > f1 <- rep(f1,3)
> >> > f2 <- rep(c("g1","g2","g3"),each=n)
> >> > df <- data.frame(x=x,y=y,f1=f1,f2=f2)
> >> > surveys <-
> >> > factor(c(rep("survey1",n*3),rep("survey2",n*3),rep("survey3",n*3)))
> >> > df <- rbind(df,df,df)
> >> > df <- data.frame(df,surveys=surveys)
> >> >
> >> > # create xyplot
> >> >
> >> > library(lattice)
> >> > library(grid)
> >> >
> >> > pnl <- function(x, y, groups, subscripts, col, pch, ...)
> >> >        panel.xyplot(x, y, col = col[subscripts], pch =
> >> pch[subscripts], ...)
> >> >
> >> > xyplot(y ~ x | surveys, data = df,
> >> >        col = as.numeric(df$f1), pch = as.numeric(df$f2), panel = pnl)
> >> >
> >> >
> >> > key1 <- list(border = TRUE, colums = 2, text = list(levels(df$f1)),
> >> >       points = list(pch = 1:nlevels(df$f1))
> >> > )
> >> >
> >> > key2 <- list(border = TRUE, colums = 2, text = list(levels(df$f2)),
> >> >       points = list(pch = 20, col = 1:nlevels(df$f2))
> >> > )
> >> >
> >> > # add legend
> >> >
> >> > draw.key(key1, draw = TRUE, vp = viewport(.9, .9))
> >> > draw.key(key2, draw = TRUE, vp = viewport(.75, .9))
> >> >
> >> >
> >> > On 8/30/06, Laurent Rhelp <laurentRhelp at free.fr> wrote:
> >> > > Gabor Grothendieck a ?crit :
> >> > >
> >> > > >Note that before entering this you need:
> >> > > >
> >> > > >library(lattice)
> >> > > >library(grid) # to access the viewport function
> >> > > >
> >> > > >On 8/29/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> >> > > >
> >> > > >
> >> > > >>Try this:
> >> > > >>
> >> > > >>xyplot(val ~ x, data = df, type = "p",
> >> > > >>       col = as.numeric(df$f1), pch = as.numeric(df$f2))
> >> > > >>
> >> > > >>key1 <- list(border = TRUE, colums = 2, text =
> >> list(levels(df$f1)),
> >> > > >>       points = list(pch = 1:nlevels(df$f1))
> >> > > >>)
> >> > > >>
> >> > > >>key2 <- list(border = TRUE, colums = 2, text =
> >> list(levels(df$f2)),
> >> > > >>       points = list(pch = 20, col = 1:nlevels(df$f2))
> >> > > >>)
> >> > > >>
> >> > > >>trellis.focus("panel", 1, 1)
> >> > > >>draw.key(key1, draw = TRUE, vp = viewport(.9, .9))
> >> > > >>draw.key(key2, draw = TRUE, vp = viewport(.75, .9))
> >> > > >>trellis.unfocus()
> >> > > >>
> >> > > >>
> >> > > >>On 8/29/06, Laurent Rhelp <laurentRhelp at free.fr> wrote:
> >> > > >>
> >> > > >>
> >> > > >>>Dear R-list,
> >> > > >>>
> >> > > >>>    I would like to use the lattice library to show several
> >> groups on
> >> > > >>>the same graph. Here's my example :
> >> > > >>>
> >> > > >>>## the data
> >> > > >>>f1 <-
> >> factor(c("mod1","mod2","mod3"),levels=c("mod1","mod2","mod3"))
> >> > > >>>f1 <- rep(f1,3)
> >> > > >>>f2 <-
> >> factor(rep(c("g1","g2","g3"),each=3),levels=c("g1","g2","g3"))
> >> > > >>>df <- data.frame(val=c(4,3,2,5,4,3,6,5,4),
> >> x=rep(c(1,2,3),3),f1=f1,f2=f2)
> >> > > >>>#############################################################
> >> > > >>>library(lattice)
> >> > > >>>
> >> > > >>>para.liste <- trellis.par.get()
> >> > > >>>superpose.symbol <- para.liste$superpose.symbol
> >> > > >>>superpose.symbol$pch <- c(1,2,3)
> >> > > >>>trellis.par.set("superpose.symbol",superpose.symbol)
> >> > > >>>
> >> > > >>># Now I can see the group according to the f1 factor (with a
> >> different
> >> > > >>>symbol for every modality)
> >> > > >>>xyplot( val~x,
> >> > > >>>       data=df,
> >> > > >>>       group=f1,
> >> > > >>>       auto.key=list(space="right")
> >> > > >>>      )
> >> > > >>>
> >> > > >>># or I can see the group according to the f2 factor
> >> > > >>>xyplot( val~x,
> >> > > >>>       data=df,
> >> > > >>>       type="l",
> >> > > >>>       group=f2,
> >> > > >>>       auto.key=list(space="right",points=FALSE,lines=TRUE)
> >> > > >>>      )
> >> > > >>>
> >> > > >>>How can I do to highlight both the f1 and f2 factors on one
> >> panel with
> >> > > >>>the legends, using the lattice function ?
> >> > > >>>
> >> > > >>>Thanks
> >> > > >>>
> >> > > >>>______________________________________________
> >> > > >>>R-help at stat.math.ethz.ch mailing list
> >> > > >>>https://stat.ethz.ch/mailman/listinfo/r-help
> >> > > >>>PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> > > >>>and provide commented, minimal, self-contained, reproducible
> >> code.
> >> > > >>>
> >> > > >>>
> >> > > >>>
> >> > > >
> >> > > >______________________________________________
> >> > > >R-help at stat.math.ethz.ch mailing list
> >> > > >https://stat.ethz.ch/mailman/listinfo/r-help
> >> > > >PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> > > >and provide commented, minimal, self-contained, reproducible code.
> >> > > >
> >> > > >
> >> > > >
> >> > > >
> >> > > Thank you, Gabor. The way to put the two legends is very
> >> interesting.
> >> > > For the graphs, in fact, my problem is to fit the data for every
> >> level
> >> > > of the f2 factor, showing the levels of the f1 factor in each
> >> panel and
> >> > > that for several surveys . Here's an example closer to my actual
> >> data :
> >> > >
> >> > > ## the data
> >> > >
> >> > > n <- 18
> >> > > x1 <- seq(1,n)
> >> > > val1 <- -2*x1+50
> >> > > val2 <- (-2*(x1-8)2)+100
> >> > > val3 <- (-2*(x1-8)2)+50
> >> > > y <- c(val1,val2,val3)
> >> > > x <- rep(x1,3)
> >> > > f1 <- rep(c("mod1","mod2","mod3"),each=n/3)
> >> > > f1 <- rep(f1,3)
> >> > > f2 <- rep(c("g1","g2","g3"),each=n)
> >> > > df <- data.frame(x=x,y=y,f1=f1,f2=f2)
> >> > >
> >> > > surveys <-
> >> > > factor(c(rep("survey1",n*3),rep("survey2",n*3),rep("survey3",n*3)))
> >> > > df <- rbind(df,df,df)
> >> > > df <- data.frame(df,surveys=surveys)
> >> > >
> >> #######################################################################
> >> > > library(lattice)
> >> > >
> >> > > para.liste <- trellis.par.get()
> >> > > superpose.symbol <- para.liste$superpose.symbol
> >> > > superpose.symbol$pch <- c(1,2,3)
> >> > > trellis.par.set("superpose.symbol",superpose.symbol)
> >> > >
> >> > > xyplot( y~x | surveys,         data=df,
> >> > >       group=f1,
> >> > >       auto.key=list(space="right")
> >> > >      )
> >> > >
> >> > > xyplot( y~x | surveys  ,
> >> > >       data=df,
> >> > >       type="l",
> >> > >       group=f2,
> >> > >       auto.key=list(space="right",points=FALSE,lines=TRUE)
> >> > >      )
> >> > >
> >> > > Certainly, I have to use the panel function but I don't know how
> >> to mark
> >> > > the f1 factor in each panel (I want to fit the values according
> >> to the
> >> > > f2 factor) !
> >> > >
> >> > >
> >> > >
> >> >
> >>
> >
> >
> Thank you for the three solutions. Spending time understanding them
> allows me to well-understand the behavior of the lattice functions. The
> last one is nice but the second one gave me the solution to adapt my
> processing according to the groups which was another aim for me : I
> wanted to do an linear regression for the g1 group and an loess
> regression for the g1, g2 group. So I modified your pnl function as below :
>
>
> pnl <- function (x, y, subscripts, groups, col, pch, type, ...) {
>   for (g in levels(groups)) {
>       idx <- g == groups[subscripts]
>       if (any(idx)){
>           panel.xyplot(x[idx], y[idx], ..., col = col[subscripts][idx],
>               pch = pch[subscripts][idx], type = type)
>
>      ## to allow for the treatments according the groups
>      switch(g,
>        g1 = panel.lmline(x[idx], y[idx], ..., col = col[subscripts][idx],
>               pch = pch[subscripts][idx]),
>        g2 = panel.loess(x[idx], y[idx], ..., col = col[subscripts][idx],
>               pch = pch[subscripts][idx]),
>        g3 = panel.loess(x[idx], y[idx], ... , col = col[subscripts][idx],
>               pch = pch[subscripts][idx])
>
>       )
>         }
>   }
> }
> ##
> ##  Finally, with these data
> ##  (I noticed that my paste failed for the syntax so I wrote (x1-8)*(x1-8))
> ##
> n <- 18
> x1 <- seq(1,n)
> val1 <- jitter(-2*x1+50,amount=10)
> val2 <- jitter((-2*(x1-8)*(x1-8))+100,amount=10)
> val3 <- jitter((-2*(x1-8)*(x1-8))+50,amount=10)
> y <- c(val1,val2,val3)
> x <- rep(x1,3)
> f1 <- rep(c("mod1","mod2","mod3"),each=n/3)
> f1 <- rep(f1,3)
> f2 <- rep(c("g1","g2","g3"),each=n)
> df <- data.frame(x=x,y=y,f1=f1,f2=f2)
> surveys <-
> factor(c(rep("survey1",n*3),rep("survey2",n*3),rep("survey3",n*3)))
> df <- rbind(df,df,df)
> df <- data.frame(df,surveys=surveys)
> ##
>
>
>
> ## the graph
>
> xyplot(y ~ x | surveys, data = df, groups = df$f2, type = "b",
>    col = as.numeric(df$f2), pch = as.numeric(df$f1), panel = pnl)
>
>
> key1 <- list(border = TRUE, colums = 2, text = list(levels(df$f1)),
>      points = list(pch = 1:nlevels(df$f1))
> )
>
> key2 <- list(border = TRUE, colums = 2, text = list(levels(df$f2)),
>      points = list(pch = 20, col = 1:nlevels(df$f2))
> )
>
> draw.key(key1, draw = TRUE, vp = viewport(.9, .9))
> draw.key(key2, draw = TRUE, vp = viewport(.75, .9))
>
> Thank you very much.
> Laurent
>
>


From phhs80 at gmail.com  Sun Sep  3 17:55:53 2006
From: phhs80 at gmail.com (Paul Smith)
Date: Sun, 3 Sep 2006 16:55:53 +0100
Subject: [R] Fitting Pareto distribution to some data
Message-ID: <6ade6f6c0609030855j34154077y9fd32674edd2ece3@mail.gmail.com>

Dear All

I am trying to fit Pareto distribution to some data. MASS package does
not support Pareto distribution. Is there some alternative way?

Thanks in advance,

Paul


From ripley at stats.ox.ac.uk  Sun Sep  3 18:30:53 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 3 Sep 2006 17:30:53 +0100 (BST)
Subject: [R] Fitting Pareto distribution to some data
In-Reply-To: <6ade6f6c0609030855j34154077y9fd32674edd2ece3@mail.gmail.com>
References: <6ade6f6c0609030855j34154077y9fd32674edd2ece3@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0609031726110.25761@gannet.stats.ox.ac.uk>

On Sun, 3 Sep 2006, Paul Smith wrote:

> Dear All
> 
> I am trying to fit Pareto distribution to some data. MASS package does
> not support Pareto distribution. Is there some alternative way?

Actually fitdistr{MASS} does if you supply the pdf for a Pareto.
That is not in base R, but easy to write for yourself.

It seems that Pareto and generalized Pareto is in several packages, 
including POT SoPhy VaR evd evir fExtremes lmomco.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From laurentRhelp at free.fr  Sun Sep  3 19:11:30 2006
From: laurentRhelp at free.fr (Laurent Rhelp)
Date: Sun, 03 Sep 2006 19:11:30 +0200
Subject: [R] lattice and several groups
In-Reply-To: <971536df0609030854q723c40d7xa832dab6456ad97b@mail.gmail.com>
References: <44F4A0FF.2010708@free.fr>	
	<971536df0608291728o7435696ew121a6b3843b51c94@mail.gmail.com>	
	<971536df0608291801i69a47e5bscd5157db05507cb9@mail.gmail.com>	
	<44F60D2C.5050201@free.fr>	
	<971536df0608301844h5ff2305cjb689baaa0895f75d@mail.gmail.com>	
	<971536df0608302004q1f100729ha4b9b73859d2e529@mail.gmail.com>	
	<971536df0608302113v60d56659lf7c57f86ee2a81c4@mail.gmail.com>	
	<44FAF7AD.7060403@free.fr>
	<971536df0609030854q723c40d7xa832dab6456ad97b@mail.gmail.com>
Message-ID: <44FB0CC2.2050309@free.fr>

Gabor Grothendieck a ?crit :

> Try this version which corresponds to your latest version
> but makes use of panel.groups distinguishing the groups
> using group.number:
>
> # set custom col and pch here
> my.col <- 1:nlevels(df$f2)
> my.pch <- 1:nlevels(df$f1)
>
> pnl <- function(x, y, subscripts, pch, group.number, ...) {
>  panel <- c(panel.lmline, panel.loess, panel.loess)[[group.number]]
>  panel(x, y, ..., pch = pch[subscripts])
>  panel.xyplot(x, y, pch = my.pch[df[subscripts, "f1"]], ...)
> }
>
>
> xyplot(y ~ x | surveys, data = df, groups = df$f2, type = "b",
>       panel = panel.superpose,
>       panel.groups = pnl,
>       par.settings = list(superpose.line = list(col = my.col),
>          superpose.symbol = list(col = my.col))
>
> )
>
>
> key1 <- list(border = TRUE, colums = 2, text = list(levels(df$f1)),
>      points = list(pch = my.pch)
>
> )
>
> key2 <- list(border = TRUE, colums = 2, text = list(levels(df$f2)),
>      lines = list(col = my.col)
> )
>
> draw.key(key1, draw = TRUE, vp = viewport(.9, .9))
> draw.key(key2, draw = TRUE, vp = viewport(.75, .9))
>
>
> On 9/3/06, Laurent Rhelp <laurentRhelp at free.fr> wrote:
>
>> Gabor Grothendieck a ?crit :
>>
>> > In thinking about this a bit more we can use
>> > panel.superpose/panel.groups to shorten it:
>> >
>> > # define data -- df
>> >
>> > # note that your val2 and val3 lines had a syntax
>> > # so we have commented them out and
>> > # replaced them as shown.
>> > n <- 18
>> > x1 <- seq(1,n)
>> > val1 <- -2*x1+50
>> > # val2 <- (-2*(x1-8)2)+100
>> > val2 <- (-2*(x1-8))+100
>> > # val3 <- (-2*(x1-8)2)+50
>> > val3 <- (-2*(x1-8))+50
>> > y <- c(val1,val2,val3)
>> > x <- rep(x1,3)
>> > f1 <- rep(c("mod1","mod2","mod3"),each=n/3)
>> > f1 <- rep(f1,3)
>> > f2 <- rep(c("g1","g2","g3"),each=n)
>> > df <- data.frame(x=x,y=y,f1=f1,f2=f2)
>> > surveys <-
>> > factor(c(rep("survey1",n*3),rep("survey2",n*3),rep("survey3",n*3)))
>> > df <- rbind(df,df,df)
>> > df <- data.frame(df,surveys=surveys)
>> >
>> > # create xyplot
>> >
>> > library(lattice)
>> > library(grid)
>> >
>> > # set custom col and pch here
>> > my.col <- 1:nlevels(df$f2)
>> > my.pch <- 1:nlevels(df$f1)
>> >
>> > pnl <- function(x, y, subscripts, pch, type, ...)
>> >   panel.xyplot(x, y, type = type, pch = my.pch[df[subscripts, "f1"]],
>> > ...)
>> >
>> > xyplot(y ~ x | surveys, data = df, groups = df$f2, type = "b",
>> >        panel = panel.superpose,
>> >        panel.groups = pnl,
>> >        par.settings = list(superpose.line = list(col = my.col),
>> >           superpose.symbol = list(col = my.col))
>> > )
>> >
>> >
>> > key1 <- list(border = TRUE, colums = 2, text = list(levels(df$f1)),
>> >       points = list(pch = my.pch)
>> > )
>> >
>> > key2 <- list(border = TRUE, colums = 2, text = list(levels(df$f2)),
>> >       lines = list(col = my.col)
>> > )
>> >
>> > draw.key(key1, draw = TRUE, vp = viewport(.9, .9))
>> > draw.key(key2, draw = TRUE, vp = viewport(.75, .9))
>> >
>> >
>> >
>> > On 8/30/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
>> >
>> >> Or maybe this is what you are looking for where pnl below was
>> >> created by modifying source to the panel.plot.default in the zoo
>> >> package (there might be a simpler way):
>> >>
>> >>
>> >> pnl <- function (x, y, subscripts, groups, col, pch, type, ...) {
>> >>    for (g in levels(groups)) {
>> >>        idx <- g == groups[subscripts]
>> >>        if (any(idx))
>> >>            panel.xyplot(x[idx], y[idx], ..., col = 
>> col[subscripts][idx],
>> >>                pch = pch[subscripts][idx], type = type)
>> >>    }
>> >> }
>> >>
>> >> xyplot(y ~ x | surveys, data = df, groups = df$f2, type = "b",
>> >>        col = as.numeric(df$f2), pch = as.numeric(df$f1), panel = pnl)
>> >>
>> >>
>> >> key1 <- list(border = TRUE, colums = 2, text = list(levels(df$f1)),
>> >>       points = list(pch = 1:nlevels(df$f1))
>> >> )
>> >>
>> >> key2 <- list(border = TRUE, colums = 2, text = list(levels(df$f2)),
>> >>       points = list(pch = 20, col = 1:nlevels(df$f2))
>> >> )
>> >>
>> >> draw.key(key1, draw = TRUE, vp = viewport(.9, .9))
>> >> draw.key(key2, draw = TRUE, vp = viewport(.75, .9))
>> >>
>> >>
>> >>
>> >>
>> >> On 8/30/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
>> >> > To handle conditioning on survey we provide a panel function
>> >> > that subsets col and pch:
>> >> >
>> >> > # define test data - df
>> >> >
>> >> > # note that your val2 and val3 lines had a syntax
>> >> > # so we have commented them out and
>> >> > # replaced them as shown.
>> >> > n <- 18
>> >> > x1 <- seq(1,n)
>> >> > val1 <- -2*x1+50
>> >> > # val2 <- (-2*(x1-8)2)+100
>> >> > val2 <- (-2*(x1-8))+100
>> >> > # val3 <- (-2*(x1-8)2)+50
>> >> > val3 <- (-2*(x1-8))+50
>> >> > y <- c(val1,val2,val3)
>> >> > x <- rep(x1,3)
>> >> > f1 <- rep(c("mod1","mod2","mod3"),each=n/3)
>> >> > f1 <- rep(f1,3)
>> >> > f2 <- rep(c("g1","g2","g3"),each=n)
>> >> > df <- data.frame(x=x,y=y,f1=f1,f2=f2)
>> >> > surveys <-
>> >> > factor(c(rep("survey1",n*3),rep("survey2",n*3),rep("survey3",n*3)))
>> >> > df <- rbind(df,df,df)
>> >> > df <- data.frame(df,surveys=surveys)
>> >> >
>> >> > # create xyplot
>> >> >
>> >> > library(lattice)
>> >> > library(grid)
>> >> >
>> >> > pnl <- function(x, y, groups, subscripts, col, pch, ...)
>> >> >        panel.xyplot(x, y, col = col[subscripts], pch =
>> >> pch[subscripts], ...)
>> >> >
>> >> > xyplot(y ~ x | surveys, data = df,
>> >> >        col = as.numeric(df$f1), pch = as.numeric(df$f2), panel = 
>> pnl)
>> >> >
>> >> >
>> >> > key1 <- list(border = TRUE, colums = 2, text = list(levels(df$f1)),
>> >> >       points = list(pch = 1:nlevels(df$f1))
>> >> > )
>> >> >
>> >> > key2 <- list(border = TRUE, colums = 2, text = list(levels(df$f2)),
>> >> >       points = list(pch = 20, col = 1:nlevels(df$f2))
>> >> > )
>> >> >
>> >> > # add legend
>> >> >
>> >> > draw.key(key1, draw = TRUE, vp = viewport(.9, .9))
>> >> > draw.key(key2, draw = TRUE, vp = viewport(.75, .9))
>> >> >
>> >> >
>> >> > On 8/30/06, Laurent Rhelp <laurentRhelp at free.fr> wrote:
>> >> > > Gabor Grothendieck a ?crit :
>> >> > >
>> >> > > >Note that before entering this you need:
>> >> > > >
>> >> > > >library(lattice)
>> >> > > >library(grid) # to access the viewport function
>> >> > > >
>> >> > > >On 8/29/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
>> >> > > >
>> >> > > >
>> >> > > >>Try this:
>> >> > > >>
>> >> > > >>xyplot(val ~ x, data = df, type = "p",
>> >> > > >>       col = as.numeric(df$f1), pch = as.numeric(df$f2))
>> >> > > >>
>> >> > > >>key1 <- list(border = TRUE, colums = 2, text =
>> >> list(levels(df$f1)),
>> >> > > >>       points = list(pch = 1:nlevels(df$f1))
>> >> > > >>)
>> >> > > >>
>> >> > > >>key2 <- list(border = TRUE, colums = 2, text =
>> >> list(levels(df$f2)),
>> >> > > >>       points = list(pch = 20, col = 1:nlevels(df$f2))
>> >> > > >>)
>> >> > > >>
>> >> > > >>trellis.focus("panel", 1, 1)
>> >> > > >>draw.key(key1, draw = TRUE, vp = viewport(.9, .9))
>> >> > > >>draw.key(key2, draw = TRUE, vp = viewport(.75, .9))
>> >> > > >>trellis.unfocus()
>> >> > > >>
>> >> > > >>
>> >> > > >>On 8/29/06, Laurent Rhelp <laurentRhelp at free.fr> wrote:
>> >> > > >>
>> >> > > >>
>> >> > > >>>Dear R-list,
>> >> > > >>>
>> >> > > >>>    I would like to use the lattice library to show several
>> >> groups on
>> >> > > >>>the same graph. Here's my example :
>> >> > > >>>
>> >> > > >>>## the data
>> >> > > >>>f1 <-
>> >> factor(c("mod1","mod2","mod3"),levels=c("mod1","mod2","mod3"))
>> >> > > >>>f1 <- rep(f1,3)
>> >> > > >>>f2 <-
>> >> factor(rep(c("g1","g2","g3"),each=3),levels=c("g1","g2","g3"))
>> >> > > >>>df <- data.frame(val=c(4,3,2,5,4,3,6,5,4),
>> >> x=rep(c(1,2,3),3),f1=f1,f2=f2)
>> >> > > >>>#############################################################
>> >> > > >>>library(lattice)
>> >> > > >>>
>> >> > > >>>para.liste <- trellis.par.get()
>> >> > > >>>superpose.symbol <- para.liste$superpose.symbol
>> >> > > >>>superpose.symbol$pch <- c(1,2,3)
>> >> > > >>>trellis.par.set("superpose.symbol",superpose.symbol)
>> >> > > >>>
>> >> > > >>># Now I can see the group according to the f1 factor (with a
>> >> different
>> >> > > >>>symbol for every modality)
>> >> > > >>>xyplot( val~x,
>> >> > > >>>       data=df,
>> >> > > >>>       group=f1,
>> >> > > >>>       auto.key=list(space="right")
>> >> > > >>>      )
>> >> > > >>>
>> >> > > >>># or I can see the group according to the f2 factor
>> >> > > >>>xyplot( val~x,
>> >> > > >>>       data=df,
>> >> > > >>>       type="l",
>> >> > > >>>       group=f2,
>> >> > > >>>       auto.key=list(space="right",points=FALSE,lines=TRUE)
>> >> > > >>>      )
>> >> > > >>>
>> >> > > >>>How can I do to highlight both the f1 and f2 factors on one
>> >> panel with
>> >> > > >>>the legends, using the lattice function ?
>> >> > > >>>
>> >> > > >>>Thanks
>> >> > > >>>
>> >> > > >>>______________________________________________
>> >> > > >>>R-help at stat.math.ethz.ch mailing list
>> >> > > >>>https://stat.ethz.ch/mailman/listinfo/r-help
>> >> > > >>>PLEASE do read the posting guide
>> >> http://www.R-project.org/posting-guide.html
>> >> > > >>>and provide commented, minimal, self-contained, reproducible
>> >> code.
>> >> > > >>>
>> >> > > >>>
>> >> > > >>>
>> >> > > >
>> >> > > >______________________________________________
>> >> > > >R-help at stat.math.ethz.ch mailing list
>> >> > > >https://stat.ethz.ch/mailman/listinfo/r-help
>> >> > > >PLEASE do read the posting guide
>> >> http://www.R-project.org/posting-guide.html
>> >> > > >and provide commented, minimal, self-contained, reproducible 
>> code.
>> >> > > >
>> >> > > >
>> >> > > >
>> >> > > >
>> >> > > Thank you, Gabor. The way to put the two legends is very
>> >> interesting.
>> >> > > For the graphs, in fact, my problem is to fit the data for every
>> >> level
>> >> > > of the f2 factor, showing the levels of the f1 factor in each
>> >> panel and
>> >> > > that for several surveys . Here's an example closer to my actual
>> >> data :
>> >> > >
>> >> > > ## the data
>> >> > >
>> >> > > n <- 18
>> >> > > x1 <- seq(1,n)
>> >> > > val1 <- -2*x1+50
>> >> > > val2 <- (-2*(x1-8)2)+100
>> >> > > val3 <- (-2*(x1-8)2)+50
>> >> > > y <- c(val1,val2,val3)
>> >> > > x <- rep(x1,3)
>> >> > > f1 <- rep(c("mod1","mod2","mod3"),each=n/3)
>> >> > > f1 <- rep(f1,3)
>> >> > > f2 <- rep(c("g1","g2","g3"),each=n)
>> >> > > df <- data.frame(x=x,y=y,f1=f1,f2=f2)
>> >> > >
>> >> > > surveys <-
>> >> > > 
>> factor(c(rep("survey1",n*3),rep("survey2",n*3),rep("survey3",n*3)))
>> >> > > df <- rbind(df,df,df)
>> >> > > df <- data.frame(df,surveys=surveys)
>> >> > >
>> >> 
>> #######################################################################
>> >> > > library(lattice)
>> >> > >
>> >> > > para.liste <- trellis.par.get()
>> >> > > superpose.symbol <- para.liste$superpose.symbol
>> >> > > superpose.symbol$pch <- c(1,2,3)
>> >> > > trellis.par.set("superpose.symbol",superpose.symbol)
>> >> > >
>> >> > > xyplot( y~x | surveys,         data=df,
>> >> > >       group=f1,
>> >> > >       auto.key=list(space="right")
>> >> > >      )
>> >> > >
>> >> > > xyplot( y~x | surveys  ,
>> >> > >       data=df,
>> >> > >       type="l",
>> >> > >       group=f2,
>> >> > >       auto.key=list(space="right",points=FALSE,lines=TRUE)
>> >> > >      )
>> >> > >
>> >> > > Certainly, I have to use the panel function but I don't know how
>> >> to mark
>> >> > > the f1 factor in each panel (I want to fit the values according
>> >> to the
>> >> > > f2 factor) !
>> >> > >
>> >> > >
>> >> > >
>> >> >
>> >>
>> >
>> >
>> Thank you for the three solutions. Spending time understanding them
>> allows me to well-understand the behavior of the lattice functions. The
>> last one is nice but the second one gave me the solution to adapt my
>> processing according to the groups which was another aim for me : I
>> wanted to do an linear regression for the g1 group and an loess
>> regression for the g1, g2 group. So I modified your pnl function as 
>> below :
>>
>>
>> pnl <- function (x, y, subscripts, groups, col, pch, type, ...) {
>>   for (g in levels(groups)) {
>>       idx <- g == groups[subscripts]
>>       if (any(idx)){
>>           panel.xyplot(x[idx], y[idx], ..., col = col[subscripts][idx],
>>               pch = pch[subscripts][idx], type = type)
>>
>>      ## to allow for the treatments according the groups
>>      switch(g,
>>        g1 = panel.lmline(x[idx], y[idx], ..., col = 
>> col[subscripts][idx],
>>               pch = pch[subscripts][idx]),
>>        g2 = panel.loess(x[idx], y[idx], ..., col = col[subscripts][idx],
>>               pch = pch[subscripts][idx]),
>>        g3 = panel.loess(x[idx], y[idx], ... , col = 
>> col[subscripts][idx],
>>               pch = pch[subscripts][idx])
>>
>>       )
>>         }
>>   }
>> }
>> ##
>> ##  Finally, with these data
>> ##  (I noticed that my paste failed for the syntax so I wrote 
>> (x1-8)*(x1-8))
>> ##
>> n <- 18
>> x1 <- seq(1,n)
>> val1 <- jitter(-2*x1+50,amount=10)
>> val2 <- jitter((-2*(x1-8)*(x1-8))+100,amount=10)
>> val3 <- jitter((-2*(x1-8)*(x1-8))+50,amount=10)
>> y <- c(val1,val2,val3)
>> x <- rep(x1,3)
>> f1 <- rep(c("mod1","mod2","mod3"),each=n/3)
>> f1 <- rep(f1,3)
>> f2 <- rep(c("g1","g2","g3"),each=n)
>> df <- data.frame(x=x,y=y,f1=f1,f2=f2)
>> surveys <-
>> factor(c(rep("survey1",n*3),rep("survey2",n*3),rep("survey3",n*3)))
>> df <- rbind(df,df,df)
>> df <- data.frame(df,surveys=surveys)
>> ##
>>
>>
>>
>> ## the graph
>>
>> xyplot(y ~ x | surveys, data = df, groups = df$f2, type = "b",
>>    col = as.numeric(df$f2), pch = as.numeric(df$f1), panel = pnl)
>>
>>
>> key1 <- list(border = TRUE, colums = 2, text = list(levels(df$f1)),
>>      points = list(pch = 1:nlevels(df$f1))
>> )
>>
>> key2 <- list(border = TRUE, colums = 2, text = list(levels(df$f2)),
>>      points = list(pch = 20, col = 1:nlevels(df$f2))
>> )
>>
>> draw.key(key1, draw = TRUE, vp = viewport(.9, .9))
>> draw.key(key2, draw = TRUE, vp = viewport(.75, .9))
>>
>> Thank you very much.
>> Laurent
>>
>>
>
>
It is great. I am impressed by the two lines :

 panel <- c(panel.lmline, panel.loess, panel.loess)[[group.number]]
 panel(x, y, ..., pch = pch[subscripts])

It seems magic. Thanks a lot.


From bjorn.grobler at zoo.ox.ac.uk  Sun Sep  3 20:39:26 2006
From: bjorn.grobler at zoo.ox.ac.uk (Chris Grobler)
Date: Sun, 03 Sep 2006 19:39:26 +0100
Subject: [R] How can I fit the secondary y axis legend on my graph?
Message-ID: <44FB215E.3070509@zoo.ox.ac.uk>

Dear All,

   Having a bit of trouble with plotting two y variables on the same 
graph. I cannot manage to get the secondary y axis label on to the right 
of the axis - it gets plotted beyond the graphic window I assume?! The 
way I constructed the graph is thus:

plot(data[,3],data[,2],axes=F, type="b")     ## plots my data from two 
data colums without axes - fine
axis(1, at=data[,3])                                     ## adds my 
primary axis - fine
mtext("Year", side=1, line=2)                     ## Adds the title 
"Year" - fine
axis(2)
mtext("Index of population size", side=2, line=2)    ## Puts the primary 
y axis on and names it correctly
points(data[,3],data[,1],pch=16)                 ## puts my secondary y 
points on - also fine
axis(4)
mtext("Number of sun hours in April", side=4, line=2) ## if I enter 
"line=1" it writes over my inex numbers but if I put                     
                                                                        
        "line=2" it disappears.

I assume that perhaps the graph automatically fills the space and does 
not leave any for 'odd' axes. My questions therefore are: how can I get 
the legend on that axis, how do I remove ylab and xlab to free the space 
that they take (as they spaced by default at "line=3" and thus waste 
space - I would like to set them to "line=2") and perhaps how do I 
scrunch up the x-axis so that it leaves more space at either side?

Sorry for the ignorance!

Chris


From ripley at stats.ox.ac.uk  Sun Sep  3 20:51:49 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 3 Sep 2006 19:51:49 +0100 (BST)
Subject: [R] How can I fit the secondary y axis legend on my graph?
In-Reply-To: <44FB215E.3070509@zoo.ox.ac.uk>
References: <44FB215E.3070509@zoo.ox.ac.uk>
Message-ID: <Pine.LNX.4.64.0609031949270.20502@gannet.stats.ox.ac.uk>

?par, specifically mar and mai, xaxt and yaxt.  Note that there are my 
default on 2.1 lines in the right margin and 4.1 in the left one.

You should find the description in 'An Introduction to R' helpful.

On Sun, 3 Sep 2006, Chris Grobler wrote:

> Dear All,
> 
>    Having a bit of trouble with plotting two y variables on the same 
> graph. I cannot manage to get the secondary y axis label on to the right 
> of the axis - it gets plotted beyond the graphic window I assume?! The 
> way I constructed the graph is thus:
> 
> plot(data[,3],data[,2],axes=F, type="b")     ## plots my data from two 
> data colums without axes - fine
> axis(1, at=data[,3])                                     ## adds my 
> primary axis - fine
> mtext("Year", side=1, line=2)                     ## Adds the title 
> "Year" - fine
> axis(2)
> mtext("Index of population size", side=2, line=2)    ## Puts the primary 
> y axis on and names it correctly
> points(data[,3],data[,1],pch=16)                 ## puts my secondary y 
> points on - also fine
> axis(4)
> mtext("Number of sun hours in April", side=4, line=2) ## if I enter 
> "line=1" it writes over my inex numbers but if I put                     
>                                                                         
>         "line=2" it disappears.
> 
> I assume that perhaps the graph automatically fills the space and does 
> not leave any for 'odd' axes. My questions therefore are: how can I get 
> the legend on that axis, how do I remove ylab and xlab to free the space 
> that they take (as they spaced by default at "line=3" and thus waste 
> space - I would like to set them to "line=2") and perhaps how do I 
> scrunch up the x-axis so that it leaves more space at either side?
> 
> Sorry for the ignorance!
> 
> Chris
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From jdbricker at gmail.com  Sun Sep  3 20:52:50 2006
From: jdbricker at gmail.com (Jeff Bricker)
Date: Sun, 3 Sep 2006 14:52:50 -0400
Subject: [R] How can I fit the secondary y axis legend on my graph?
In-Reply-To: <44FB215E.3070509@zoo.ox.ac.uk>
References: <44FB215E.3070509@zoo.ox.ac.uk>
Message-ID: <3a3063250609031152u25905623jd2cb83d5ae76e728@mail.gmail.com>

I suspect you may need to tweak your margins.  See the "mar" argument
to the "par" command for guidance.

Something like par(mar=c(5,4,4,4)) should probably get you started, though.

On 9/3/06, Chris Grobler <bjorn.grobler at zoo.ox.ac.uk> wrote:
> Dear All,
>
>    Having a bit of trouble with plotting two y variables on the same
> graph. I cannot manage to get the secondary y axis label on to the right
> of the axis - it gets plotted beyond the graphic window I assume?! The
> way I constructed the graph is thus:
>
> plot(data[,3],data[,2],axes=F, type="b")     ## plots my data from two
> data colums without axes - fine
> axis(1, at=data[,3])                                     ## adds my
> primary axis - fine
> mtext("Year", side=1, line=2)                     ## Adds the title
> "Year" - fine
> axis(2)
> mtext("Index of population size", side=2, line=2)    ## Puts the primary
> y axis on and names it correctly
> points(data[,3],data[,1],pch=16)                 ## puts my secondary y
> points on - also fine
> axis(4)
> mtext("Number of sun hours in April", side=4, line=2) ## if I enter
> "line=1" it writes over my inex numbers but if I put
>
>         "line=2" it disappears.
>
> I assume that perhaps the graph automatically fills the space and does
> not leave any for 'odd' axes. My questions therefore are: how can I get
> the legend on that axis, how do I remove ylab and xlab to free the space
> that they take (as they spaced by default at "line=3" and thus waste
> space - I would like to set them to "line=2") and perhaps how do I
> scrunch up the x-axis so that it leaves more space at either side?
>
> Sorry for the ignorance!
>
> Chris
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From h.wickham at gmail.com  Sun Sep  3 21:02:34 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Sun, 3 Sep 2006 14:02:34 -0500
Subject: [R] How can I fit the secondary y axis legend on my graph?
In-Reply-To: <44FB215E.3070509@zoo.ox.ac.uk>
References: <44FB215E.3070509@zoo.ox.ac.uk>
Message-ID: <f8e6ff050609031202q7858ea57n7c5f7878d0c0dcb2@mail.gmail.com>

>    Having a bit of trouble with plotting two y variables on the same
> graph. I cannot manage to get the secondary y axis label on to the right
> of the axis - it gets plotted beyond the graphic window I assume?! The
> way I constructed the graph is thus:

Chris, I would strongly suggest you find a different way of
visualising your data than using two overlayed line plots.  This type
of graphic is not an effective way of comparing two time series as it
can be so easily misread and misused.  See
http://junkcharts.typepad.com/junk_charts/2006/06/illusion_of_suc.html
and http://junkcharts.typepad.com/junk_charts/2006/05/the_crossover_l.html
for examples of this.

Hadley


From tlumley at u.washington.edu  Sun Sep  3 21:50:01 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Sun, 3 Sep 2006 12:50:01 -0700 (PDT)
Subject: [R] Running cox models
In-Reply-To: <93c3eada0609030350v1c6429f3g743d86aab65eda9f@mail.gmail.com>
References: <93c3eada0609030350v1c6429f3g743d86aab65eda9f@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0609031236190.14174@homer21.u.washington.edu>

On Sun, 3 Sep 2006, Geoff Russell wrote:

> Hi,
>
> I'm reading van Belle et al "Biostatistics" and trying to run a cox test 
> using a dataset from:
>
> http://faculty.washington.edu/~heagerty/Books/Biostatistics/chapter16.html
>
> (Primary Biliary Cirrhosis data link at top of the page),
>
> I'm using the following code:
>
> --------------- start of code
> library(survival)
> liver <- scan("liver2.txt",list(age=0,albumin=0,alkphos=0,ascites=0,bili=0,
>        cholest=0,edema=0,edmadj=0,hepmeg=0,obstime=0,platelet=0,protime=0,
>        sex=0,sgot=0,spiders=0,stage=0,status=0,treatmnt=0,
>        triglyc=0,urinecu=0))
> fit<-coxph(Surv(obstime,status)~bili+edmadj+albumin+protime+age,data=liver)
> summary(fit)
> ----------------- End of code
>
> but the answer is rather different from that in the book (p.688 - for
> anyone with the book).

A little further up the page (or on a previous page -- I don't have a 
printed copy with me) the example specifies that in the model BILI, 
ALBUMIN and PROTIME are log transformations of the data and AGE is in 
ten-year units.

I must admit that the need to take the 312 records with value for 
treatment is not explicit in the book, though.

If you try
coxph(formula = Surv(obstime, status) ~ log(bili) + edmadj + log(albumin) 
+ log(protime) + I(age/10), data = liver, subset = !is.na(treatmnt))

you will get something much more like the book.


 	-thomas


From dsohal at gmail.com  Sun Sep  3 22:34:21 2006
From: dsohal at gmail.com (Davendra Sohal)
Date: Sun, 3 Sep 2006 16:34:21 -0400
Subject: [R] Memory issues
Message-ID: <c2f237040609031334od428419w49eccc8b357c6e0f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060903/f8ec731d/attachment.pl 

From murray at math.umass.edu  Mon Sep  4 02:10:33 2006
From: murray at math.umass.edu (Murray Eisenberg)
Date: Sun, 03 Sep 2006 20:10:33 -0400
Subject: [R] RGui problem in Windows XP with demo() and help()
Message-ID: <44FB6EF9.3090806@math.umass.edu>

I just installed R-2.3.1pat under Windows XP as well as the associated
RWinEdt.  If I start RGui from its shortcut (but do _not_ also start
RWinEdt) and then try to execute demo() or help(), I get a RWinEdt
pop-up error window with message:

   File "D:\WP\WinEdtData\WinEdt\D:/WP/WinEdtData/WinEdt/R.ini" does not
exist!
   Qualifier -e/-E does not specify an existing file!

First question: what is causing RWinEdt even to get involved here?

Second: where is the garbled path to R.ini coming from.

Here's the configuration:  Everything R is in D:\Stats\R.  R-2.3.1pat
has the actual R release installed.

The shortcut for R-2.3.1-pat has target
D:\Stats\R\R-2.3.1pat\bin\Rgui.exe and starts in
D:\Stats\R\R-2.3.1pat.

File .Renviron is in D:\Stats\R.  R-2.3.1pat; the only lines in it that
are not commented out are:

  R_USER=e:/Documents/R
  R_LIBS=d:/Stats/R/myRlib

File .Rprofile is in e:/Documents/R (the reference of myR_USER); the
only lines in it not commented out are:

   options(editor="\"d:/WP/winedt/winedt\" -c=\"R-WinEdt\"
        -E=\"D:/WP/WinEdtData/WinEdt/R.ini\" -V")      [on 1 line]
   options(pager="\"d:/WP/winedt/winedt\" -C=\"R-WinEdt\"
        -e=\"D:/WP/WinEdtData/WinEdt/R.ini\" -V")      [on 1 line]
   .First <- function(x) print("Profile read")

Is the root of the problem that I have a single .Rprofile set up for
RWinEdt but need to have a different one for RGui without RWinEdt?
And, if so, how do I tell R which one to use at startup?
-- 
Murray Eisenberg                     murray at math.umass.edu
Mathematics & Statistics Dept.
Lederle Graduate Research Tower      phone 413 549-1020 (H)
University of Massachusetts                413 545-2859 (W)
710 North Pleasant Street            fax   413 545-1801
Amherst, MA 01003-9305


From p.murrell at auckland.ac.nz  Mon Sep  4 02:35:49 2006
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Mon, 04 Sep 2006 12:35:49 +1200
Subject: [R] embed image (png) in postscript (device)
In-Reply-To: <47fce0650609010456l5739f11yc63892e53b94cd61@mail.gmail.com>
References: <47fce0650609010456l5739f11yc63892e53b94cd61@mail.gmail.com>
Message-ID: <44FB74E5.8080807@stat.auckland.ac.nz>

Hi


Hans-Peter wrote:
> Hi,
> 
> I output multiple "grid-package-based" plots to the postscript device.
> Because the graphics are complicated and consists of a lot of datapoints
> (~200'000) the files become really big. To avoid this big files and to
> shorten the creation, I currently print the plots to the png device and
> manually combine them into one multipaged pdf document.


If you want to automate combining png files into a single pdf, you could
take a look at ImageMagick.

Paul


> My question is, it is possible to do this in R directly?
> Probably I would have to create a png plot first, then reimport it into R
> and "put" it on the ps device. Is this a realistic way (in principle) ?
> 
> If there is no package to read png files (I didn't find anything), I
> probably could solve this. But I still don't know if it would be possible to
> "put" this memory representation to a ps device.
> 
> Thanks for your time.
> 

-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/


From ggrothendieck at gmail.com  Mon Sep  4 04:47:31 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 3 Sep 2006 22:47:31 -0400
Subject: [R] How can I fit the secondary y axis legend on my graph?
In-Reply-To: <44FB215E.3070509@zoo.ox.ac.uk>
References: <44FB215E.3070509@zoo.ox.ac.uk>
Message-ID: <971536df0609031947y5899742bkb94cfe9538bc2976@mail.gmail.com>

There is an example here:

http://zoonek2.free.fr/UNIX/48_R/04.html#1.2

(Scroll down until you see the graphic with the two y axes.  The
code is in the box just above it.)

On 9/3/06, Chris Grobler <bjorn.grobler at zoo.ox.ac.uk> wrote:
> Dear All,
>
>   Having a bit of trouble with plotting two y variables on the same
> graph. I cannot manage to get the secondary y axis label on to the right
> of the axis - it gets plotted beyond the graphic window I assume?! The
> way I constructed the graph is thus:
>
> plot(data[,3],data[,2],axes=F, type="b")     ## plots my data from two
> data colums without axes - fine
> axis(1, at=data[,3])                                     ## adds my
> primary axis - fine
> mtext("Year", side=1, line=2)                     ## Adds the title
> "Year" - fine
> axis(2)
> mtext("Index of population size", side=2, line=2)    ## Puts the primary
> y axis on and names it correctly
> points(data[,3],data[,1],pch=16)                 ## puts my secondary y
> points on - also fine
> axis(4)
> mtext("Number of sun hours in April", side=4, line=2) ## if I enter
> "line=1" it writes over my inex numbers but if I put
>
>        "line=2" it disappears.
>
> I assume that perhaps the graph automatically fills the space and does
> not leave any for 'odd' axes. My questions therefore are: how can I get
> the legend on that axis, how do I remove ylab and xlab to free the space
> that they take (as they spaced by default at "line=3" and thus waste
> space - I would like to set them to "line=2") and perhaps how do I
> scrunch up the x-axis so that it leaves more space at either side?
>
> Sorry for the ignorance!
>
> Chris
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ripley at stats.ox.ac.uk  Mon Sep  4 07:26:09 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 4 Sep 2006 06:26:09 +0100 (BST)
Subject: [R] Memory issues
In-Reply-To: <c2f237040609031334od428419w49eccc8b357c6e0f@mail.gmail.com>
References: <c2f237040609031334od428419w49eccc8b357c6e0f@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0609040623540.20146@gannet.stats.ox.ac.uk>

Please do read the rw-FAQ, Q2.9  (and the posting guide).

In particular, Windows never gives 4GB to a single 32-bit user process.

On Sun, 3 Sep 2006, Davendra Sohal wrote:

> Hi,
> I'm using R on Windows and upgraded the computer memory to 4GB, as R was
> telling me that it is out of memory (for making heatmaps).
> It still says that the maximum memory is 1024Mb, even if I increase it using
> memory.limit and memory.size.
> Is there a way to permanently increase R's memory quota to 4GB?
> Please help.
> Many thanks,
> -DS.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From wwguocn at gmail.com  Mon Sep  4 07:33:55 2006
From: wwguocn at gmail.com (Guo Wei-Wei)
Date: Mon, 4 Sep 2006 13:33:55 +0800
Subject: [R]  Question on Chi-square of null model in sem package
Message-ID: <d3677d7d0609032233rf2ecc6ajadeb5edf075df1e2@mail.gmail.com>

Dear all,

I met a problem while doing SEM by sem package. I got a negative
chi-square of null model. Because the theoretical value of chi-square
cannot be negative, I checked the source code of sem.R in sem package
and I found the Chi-square of null model was computed by the following
expression:

result$chisqNull <- (N - 1) * (sum(diag(S %*% diag(1/diag(S)))) +
log(prod(diag(S))))

I think the reason for negative Chi-square is the too small value of
prod(diag(S)) of my data. I'm working on a data.frame named emc.data
from a sample of a 16-item questioinnaire. The variance of items are

> diag(cov(emc.data))
     EMC1      EMC2      EMC3      EMC4      EMC5      EMC6      EMC7      EMC8
0.3622224 0.2350041 0.2488009 0.2901653 0.3195399 0.3107343 0.3436622 0.2345912
     EMC9     EMC10     EMC11     EMC12     EMC13     EMC14     EMC15     EMC16
0.2621680 0.3230400 0.4039245 0.3803105 0.2773370 0.4348342 0.2757216 0.3405252

The fit indices of RMSEA and GFI are good, so I think the problem
might be solve by another way for computing the Chi-square of null
model. I'm not well trained in maths, so I come for help. Any advise
is appreciated.

Best wishes,
Wei-Wei


From spencer.graves at pdf.com  Mon Sep  4 08:28:22 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 03 Sep 2006 23:28:22 -0700
Subject: [R] Need help to estimate the Coef matrices in mAr
In-Reply-To: <d4c57560608292247h2f18d279r75061f0a4eb6cebd@mail.gmail.com>
References: <d4c57560608292247h2f18d279r75061f0a4eb6cebd@mail.gmail.com>
Message-ID: <44FBC786.5010703@pdf.com>

      Have you tried 'RSiteSearch("multivariate autoregression", 
"functions")'?  This produced 14 hits for me just now, the first of 
which mentions a package 'MSBVAR'.  Have you looked at that? 

      If that failed, I don't think it would be too hard to modify 
'mAr.est' to do what you want.  If it were my problem, I might a local 
copy of the function, then add an argument accepting a 2 or 
3-dimensional array with numbers for AR coefficients to be fixed and NAs 
for the coefficients.  Then I'd use 'debug' to walk through the function 
line by line until I figured out how to modify the function to do what I 
wanted.  I haven't checked all the details, so I don't know for sure if 
this would work, but the function contains a line 'R = qr.R(qr((rbind(K, 
diag(scale)))), complete = TRUE)' which I would start by decomposing, 
possibly starting as follows: 

      Z <-     rbind(K, diag(scale)

I'd figure out how the different columns of Z relate to my problem, then 
modify it appropriately to get what I wanted. 

      Another alternative would be to program it from scratch using 
something like 'optim' to minimize the sum of squares of residuals over 
the free parameters in my AR matrices.   I'm confident I could make this 
work, even if the I somehow could not get it with either of the other two. 

      There may be something else  better, e.g., a Kalman filter 
representation, but I can't think how to do that off the top if my head. 

      Hope this helps. 
      Spencer Graves

Arun Kumar Saha wrote:
> Dear R users,
>
> I am using mAr package to fit a Vector autoregressive model to my data. But
> here I want to put some predetermined values for some elements in
> coefficient matrix that mAr.est going to estimate. For example if p=3 then I
> want to put A3[1,3] = 0 and keep rest of the elements of coefficient
> matrices to be determined by mAr.est.
>
> Can anyone please tell me how can I do that?
>
> Sincerely yours,
> Arun
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mskrishnan71 at yahoo.com  Mon Sep  4 03:06:16 2006
From: mskrishnan71 at yahoo.com (Mahesh Krishnan)
Date: Sun, 3 Sep 2006 18:06:16 -0700 (PDT)
Subject: [R] Subsetting vectors based on condition
Message-ID: <20060904010616.70272.qmail@web83104.mail.mud.yahoo.com>

Hello,

I have a question regarding subsetting of vectors. Here's an example of
what I'm trying to do:

vect.1 <- c(76,195, 290, 380)

vect.2 <-  c(63,  95, 133, 170, 215, 253, 285, 299, 325, 375)

I would like to subset vect.2 so that it has the same length as vect.1,
and its numbers are the first corresponging higher value compared to
vect.1.

The output should be:

 final.output = (95, 215, 299, NA)

What is the fastest/most eficient way to accompllish this in R?

Thanks for the help in advance,

Mahesh Krishnan


From s9268716 at mail.inf.tu-dresden.de  Mon Sep  4 08:43:44 2006
From: s9268716 at mail.inf.tu-dresden.de (=?ISO-8859-1?Q?Bj=F6rn_Thalheim?=)
Date: Mon, 04 Sep 2006 08:43:44 +0200
Subject: [R] Command line cut-off
Message-ID: <44FBCB20.2010202@mail.inf.tu-dresden.de>

Hi,

I noticed that, when working with R on a command line, I cannot enter
anything into the command line which is longer than 1023 characters.
If I input sth longer, it'l be just cut off at this lenght, and a "+"
will be presented to me on the command line.

It's not as bad since I can copy'n'paste commands longer than this with
newline characters in between onto the prompt, but still kind of annoying.

Does anybody know how I can make my command line accept lines longer
than 1023 characters?

Regards,

Bj?rn


-- 
Q:	How many lawyers does it take to change a light bulb?
A:	One.  Only it's his light bulb when he's done.

-- 
Important! Please recognize my new GPG Public Key!
                 Bj?rn Thalheim
gpg fingerprint: 2F22 AAEB 1818 1548 EC78  1AE8 9D2E FCB4 0980 28CC
   download key: wget http://www.ifsr.de/~bjoern/gpg/public_key.asc
       See also: http://www.ifsr.de/~bjoern/gpg/key.html

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 252 bytes
Desc: OpenPGP digital signature
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20060904/c17ea1a1/attachment.bin 

From rob.hyndman at buseco.monash.edu.au  Sat Sep  2 23:43:31 2006
From: rob.hyndman at buseco.monash.edu.au (Rob Hyndman)
Date: Sun, 03 Sep 2006 07:43:31 +1000
Subject: [R] [R-pkgs] New forecasting bundle of packages
Message-ID: <44F9FB03.5050703@buseco.monash.edu.au>

v1.0 of the forecasting bundle of packages is now on CRAN and will 
propagate to mirrors shortly.

The forecasting bundle of R packages provides new forecasting methods, 
and graphical tools for displaying and analysing forecasts. It comprises 
the following packages:

     * forecast: Functions and methods for forecasting.

     * fma: All data sets from Makridakis, Wheelwright and Hyndman 
(1998)         Forecasting: methods and applications, Wiley & Sons: New 
York.

     * Mcomp: All data from the M1 and M3 forecast competitions.

Key features:

* a "forecast" method and class which can be applied to Arima, StructTS, 
HoltWinters and other time series models. This is preferred to predict() 
as it provides output in a consistent format (the "forecast" class) that 
can be used by other functions.

* automatic univariate time series forecasting based on exponential 
smoothing state space models. This is much more general and flexible 
than HoltWinters().

* automatic ARIMA forecasting based on minimizing the AIC or BIC.

* several new forecasting methods and time series graphics.

Some features of the forecast package were the subject of my talk at 
UseR! in Vienna in June. Slides of the talk are at 
http://www.robhyndman.info/talks/Hyndman_UseR.pdf

Anyone who has been using earlier versions of the packages from my web 
pages should check out the list of changes at 
http://www.robhyndman.info/Rlibrary/forecast/

__________________________________________________
Professor Rob J Hyndman
Department of Econometrics & Business Statistics,
Monash University, VIC 3800, Australia
http://www.robhyndman.info/

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From nmi13 at ext.canterbury.ac.nz  Mon Sep  4 09:35:17 2006
From: nmi13 at ext.canterbury.ac.nz (nmi13)
Date: Mon, 04 Sep 2006 19:35:17 +1200
Subject: [R] generating loglogistic distribution in R
Message-ID: <44FC372A@webmail>

Hi Dear,

Can someone please inform me on genreating random variables of Loglogistic, 
and PERT beta distributions?

Thanks for your time and help, in advance.

Regards,
Murthy.


From ripley at stats.ox.ac.uk  Mon Sep  4 11:16:28 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 4 Sep 2006 10:16:28 +0100 (BST)
Subject: [R] Command line cut-off
In-Reply-To: <44FBCB20.2010202@mail.inf.tu-dresden.de>
References: <44FBCB20.2010202@mail.inf.tu-dresden.de>
Message-ID: <Pine.LNX.4.64.0609041007500.19611@gannet.stats.ox.ac.uk>

On Mon, 4 Sep 2006, Bj?rn Thalheim wrote:

> Hi,
> 
> I noticed that, when working with R on a command line, I cannot enter
> anything into the command line which is longer than 1023 characters.

Actually, 1000 or 1022 or 1023 bytes, and it depends on your 'command 
line' (and you have not even told us your OS).

You will find details in the R-devel list archives, e.g.

https://stat.ethz.ch/pipermail/r-devel/2006-August/038985.html

> If I input sth longer, it'l be just cut off at this lenght, and a "+"
> will be presented to me on the command line.
> 
> It's not as bad since I can copy'n'paste commands longer than this with
> newline characters in between onto the prompt, but still kind of annoying.

How do you think thousands users of R for a decade have managed?
What are you doing that needs 'commands' (R has function calls) longer 
than 1000 or so characters that need to be on one line?

> Does anybody know how I can make my command line accept lines longer
> than 1023 characters?

See the R-devel version of R for a full description: on some consoles it 
works there.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From gchappi at gmail.com  Mon Sep  4 11:46:03 2006
From: gchappi at gmail.com (Hans-Peter)
Date: Mon, 4 Sep 2006 11:46:03 +0200
Subject: [R] xlsReadWrite 1.0
Message-ID: <47fce0650609040246h25b2078l54acef718a269863@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060904/2b6340b6/attachment.pl 

From dingjia at gmail.com  Mon Sep  4 12:34:12 2006
From: dingjia at gmail.com (jia ding)
Date: Mon, 4 Sep 2006 12:34:12 +0200
Subject: [R] merge files after cor.test
Message-ID: <91ae6e350609040334s50872eb8v346a2d3ebd6cfd68@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060904/034263bb/attachment.pl 

From petr.pikal at precheza.cz  Mon Sep  4 13:50:25 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 04 Sep 2006 13:50:25 +0200
Subject: [R] my error with augPred
In-Reply-To: <44FA289D.8080105@pdf.com>
References: <44EDAC73.16458.584068@localhost>
Message-ID: <44FC2F21.3661.141D7F7@localhost>

Hallo

thank you for your response. I am not sure but maybe fixed effects 
cannot be set to be influenced by a factor to be able to use augPred.

lob<-Loblolly[Loblolly$Seed!=321,]
set.seed(1)
lob<-data.frame(lob, x1=sample(letters[1:3], replace=T)) # add a 
#factor
lob<-groupedData(height~age|Seed, data=lob)
fm1 <- nlme(height ~ SSasymp(age, Asym, R0, lrc),
            data = lob,
            fixed = Asym + R0 + lrc ~ 1,
            random = Asym ~ 1,
            start = c(Asym = 103, R0 = -8.5, lrc = -3.3))

fm2<-update(fm1, fixed=list(Asym~x1, R0+lrc~1), start=c(103,0,-8.5,-
3))
                                             ^^^^^^^
and

plot(augPred(fm2))

Throws an error.
So it is not possible to use augPred with such constructions.

Best regards.
Petr Pikal

On 2 Sep 2006 at 17:58, Spencer Graves wrote:

Date sent:      	Sat, 02 Sep 2006 17:58:05 -0700
From:           	Spencer Graves <spencer.graves at pdf.com>
To:             	Petr Pikal <petr.pikal at precheza.cz>
Copies to:      	r-help at stat.math.ethz.ch
Subject:        	Re: [R] my error with augPred

> <comments in line> 
> 
> Petr Pikal wrote:
> > Dear all
> >
> > I try to refine my nlme models and with partial success. The model
> > is refined and fitted (using Pinheiro/Bates book as a tutorial) but
> > when I try to plot
> >
> > plot(augPred(fit4))
> >
> > I obtain
> > Error in predict.nlme(object, value[1:(nrow(value)/nL), , drop =
> > FALSE],  : 
> >         Levels (0,3.5],(3.5,5],(5,7],(7,Inf] not allowed for 
> > vykon.fac
> >   
> >
> > Is it due to the fact that I have unbalanced design with not all
> > levels of vykon.fac present in all levels of other explanatory
> > factor variable?
> >   
> I don't know, but I'm skeptical. 
> > I try to repeat 8.19 fig which is OK until I try:
> >
> > fit4 <- update(fit2, fixed = list(A+B~1,xmid~vykon.fac, scal~1), 
> > start = c(57, 100, 700, rep(0,3), 13))
> >
> > I know I should provide an example but maybe somebody will be clever
> > enough to point me to an explanation without it.
> >   
> I'm not. 
> 
> To answer these questions without an example from you, I'd have to
> make up my own example and try to see if I could replicate the error
> messages you report, and I'm not sufficiently concerned about this
> right now to do that. 
> 
> Have you tried taking an example from the book and deleting certain
> rows from the data to see if you can force it to reproduce your error?
> 
> 
> Alternatively, have you tried using 'debug' to trace through the code
> line by line until you learn enough of what it's doing to answer your
> question? 
> 
> Spencer Graves
> > nlme version 3.1-75
> > SSfpl model
> > R 2.4.0dev (but is the same in 2.3.1), W2000.
> >
> > Thank you
> > Best regards.
> >
> > Petr PikalPetr Pikal
> > petr.pikal at precheza.cz
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented,
> > minimal, self-contained, reproducible code.
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From petr.pikal at precheza.cz  Mon Sep  4 14:05:41 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 04 Sep 2006 14:05:41 +0200
Subject: [R] abline and plot(augPred) help
Message-ID: <44FC32B5.32197.14FCFC6@localhost>

Dear all

as I did not get any response on my post about abline and 
plot(augPred)) I try again. I hope I do not break some posting guide 
rules. I would try to contact package maintainer directly but there 
is stated to be R-core people, so I feel R-help list shall be OK.

I need to draw straight lines through augPred plotted panels 
(vertical or horizontal) at specified point. I know I shall probably 
use panel.abline but I am missing correct syntax. Below you can see 
my attempts together with results. I hope somebody can point me to 
right direction.

I am probably somewhere close but I have no clue, which parameter I
shall modify to get measured points, fitted lines and vertical lines
in panels together.

Please help

Thank you
Best regards.
Petr Pikal

fm1 <- lme(Orthodont)

# standard plot
plot(augPred(fm1, level = 0:1, length.out = 2))

#plot with vertical but without points and fitted lines
plot(augPred(fm1, level = 0:1, length.out = 2),
panel=function(v,...) {
panel.abline(v=10)}
)

# plot with vertical but without fitted lines
plot(augPred(fm1, level = 0:1, length.out=2),
panel=function(x,y,...) {
panel.xyplot(x,y,...)
panel.abline(v=10)}
)

# plot with vertical and with all points (fitted lines are drawn as 
points)
plot(augPred(fm1, level = 0:1),
panel=function(x,y,...) {
panel.xyplot(x,y,...)
panel.abline(v=10)}
)

Petr Pikal
petr.pikal at precheza.cz


From jholtman at gmail.com  Mon Sep  4 14:18:04 2006
From: jholtman at gmail.com (jim holtman)
Date: Mon, 4 Sep 2006 08:18:04 -0400
Subject: [R] Subsetting vectors based on condition
In-Reply-To: <20060904010616.70272.qmail@web83104.mail.mud.yahoo.com>
References: <20060904010616.70272.qmail@web83104.mail.mud.yahoo.com>
Message-ID: <644e1f320609040518y1bc08ff9g498d988390ec0441@mail.gmail.com>

> vect.1
[1]  76 195 290 380
> vect.2
 [1]  63  95 133 170 215 253 285 299 325 375
> unlist(lapply(vect.1, function(x)vect.2[which(vect.2 > x)[1]]))
[1]  95 215 299  NA
>


On 9/3/06, Mahesh Krishnan <mskrishnan71 at yahoo.com> wrote:
> Hello,
>
> I have a question regarding subsetting of vectors. Here's an example of
> what I'm trying to do:
>
> vect.1 <- c(76,195, 290, 380)
>
> vect.2 <-  c(63,  95, 133, 170, 215, 253, 285, 299, 325, 375)
>
> I would like to subset vect.2 so that it has the same length as vect.1,
> and its numbers are the first corresponging higher value compared to
> vect.1.
>
> The output should be:
>
>  final.output = (95, 215, 299, NA)
>
> What is the fastest/most eficient way to accompllish this in R?
>
> Thanks for the help in advance,
>
> Mahesh Krishnan
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From phhs80 at gmail.com  Mon Sep  4 15:14:39 2006
From: phhs80 at gmail.com (Paul Smith)
Date: Mon, 4 Sep 2006 14:14:39 +0100
Subject: [R] Fitting Pareto distribution to some data
In-Reply-To: <Pine.LNX.4.64.0609031726110.25761@gannet.stats.ox.ac.uk>
References: <6ade6f6c0609030855j34154077y9fd32674edd2ece3@mail.gmail.com>
	<Pine.LNX.4.64.0609031726110.25761@gannet.stats.ox.ac.uk>
Message-ID: <6ade6f6c0609040614j6d52a2d5n9499a4baa98fff02@mail.gmail.com>

On 9/3/06, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> > I am trying to fit Pareto distribution to some data. MASS package does
> > not support Pareto distribution. Is there some alternative way?
>
> Actually fitdistr{MASS} does if you supply the pdf for a Pareto.
> That is not in base R, but easy to write for yourself.
>
> It seems that Pareto and generalized Pareto is in several packages,
> including POT SoPhy VaR evd evir fExtremes lmomco.

Thanks, Prof Brian Ripley.

Paul


From jfox at mcmaster.ca  Mon Sep  4 15:34:37 2006
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 4 Sep 2006 09:34:37 -0400
Subject: [R] Question on Chi-square of null model in sem package
In-Reply-To: <d3677d7d0609032233rf2ecc6ajadeb5edf075df1e2@mail.gmail.com>
Message-ID: <20060904133436.WZMD1747.tomts25-srv.bellnexxia.net@JohnDesktop8300>

Dear Wei-Wei,

As I explained to you in private email yesterday (perhaps you didn't receive
my reply?), the problem that you point out is due to a bug in the sem
function that I fixed some time ago and then inadvertently reintroduced.
Yesterday, I sent a corrected version of the sem package (0.9-5) to CRAN;
the source package is there now and I'm sure that the compiled Windows
package will appear in due course.

Thank you once more for bringing the problem to my attention.

John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Guo Wei-Wei
> Sent: Monday, September 04, 2006 12:34 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Question on Chi-square of null model in sem package
> 
> Dear all,
> 
> I met a problem while doing SEM by sem package. I got a 
> negative chi-square of null model. Because the theoretical 
> value of chi-square cannot be negative, I checked the source 
> code of sem.R in sem package and I found the Chi-square of 
> null model was computed by the following
> expression:
> 
> result$chisqNull <- (N - 1) * (sum(diag(S %*% diag(1/diag(S)))) +
> log(prod(diag(S))))
> 
> I think the reason for negative Chi-square is the too small value of
> prod(diag(S)) of my data. I'm working on a data.frame named 
> emc.data from a sample of a 16-item questioinnaire. The 
> variance of items are
> 
> > diag(cov(emc.data))
>      EMC1      EMC2      EMC3      EMC4      EMC5      EMC6   
>    EMC7      EMC8
> 0.3622224 0.2350041 0.2488009 0.2901653 0.3195399 0.3107343 
> 0.3436622 0.2345912
>      EMC9     EMC10     EMC11     EMC12     EMC13     EMC14   
>   EMC15     EMC16
> 0.2621680 0.3230400 0.4039245 0.3803105 0.2773370 0.4348342 
> 0.2757216 0.3405252
> 
> The fit indices of RMSEA and GFI are good, so I think the 
> problem might be solve by another way for computing the 
> Chi-square of null model. I'm not well trained in maths, so I 
> come for help. Any advise is appreciated.
> 
> Best wishes,
> Wei-Wei
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From amsa36060 at yahoo.com  Mon Sep  4 15:44:07 2006
From: amsa36060 at yahoo.com (Amir Safari)
Date: Mon, 4 Sep 2006 06:44:07 -0700 (PDT)
Subject: [R] library(plgem)
Message-ID: <20060904134407.37859.qmail@web60421.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060904/db33196d/attachment.pl 

From hto at arcor.de  Mon Sep  4 16:14:50 2006
From: hto at arcor.de (Thomas Hunger)
Date: Mon, 4 Sep 2006 16:14:50 +0200
Subject: [R] how to fit gauss beam?
Message-ID: <200609041614.50475.hto@arcor.de>

Hello,

I am having a hard time fitting a gauss beam using R. In 
gnutplot I did something like

$ w(z) = w0 * sqrt(1+(z/z0)**2)
$ fit w(z) 'before_eom.txt' using 1:2 via w0, z0

to obtain w0 and z0. Now I want to do the same in R. I tried 
a linear model like this (r = radius, z = distance):

beam <- function(z) {
  sum(sqrt(1 + z**2))
}

lm(r ~ I(beam(z)), data = before_eom)

Which gives nonsensical answers ...

Then I tried a nonlinear model:

d <- read.table ("before_eom.tab", header=T)
z <- d$d
r <- d$minor * 1e-6

beam<- function(p) {
  M <- 1.1
  sum((r-M*p[1]*sqrt(1 + (z/p[2])**2))^2)
}

out <- nlm(beam, p=c(400, 0.1), hessian=TRUE)
out$estimate

This is very sensitive to the starting values and gives 
nonsensical answers as well...

Is there a simple way to translate the gnuplot fit into R?

Thanks,
Thomas


From john.seers at bbsrc.ac.uk  Mon Sep  4 16:20:17 2006
From: john.seers at bbsrc.ac.uk (john seers (IFR))
Date: Mon, 4 Sep 2006 15:20:17 +0100
Subject: [R] library(plgem)
In-Reply-To: <20060904134407.37859.qmail@web60421.mail.yahoo.com>
Message-ID: <1CF0B26CECD746438AE02DBF7DDE1C7B03055EEA@ifre2ksrv1.ifrxp.bbsrc.ac.uk>



Perhaps here?


http://www.bioconductor.org/packages/bioc/1.6/src/contrib/html/plgem.htm
l


 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Amir Safari
Sent: 04 September 2006 14:44
To: R-help at stat.math.ethz.ch
Subject: [R] library(plgem)





     
  Dear Users,
   
  library(plgem) doesn't exist directly in the list of available
packages of R. Where could it be found?
  Thanks so much for help.
  Amir


 		
---------------------------------

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From flyhyena at yahoo.com.cn  Mon Sep  4 16:38:24 2006
From: flyhyena at yahoo.com.cn (sun)
Date: Mon, 4 Sep 2006 22:38:24 +0800 (CST)
Subject: [R] newbie question about index
In-Reply-To: <44F81553.31541.2CD7BC@localhost>
Message-ID: <20060904143824.45286.qmail@web15607.mail.cnb.yahoo.com>

Thanks for all these replies, all work perfectly.

Sun
--- Petr Pikal <petr.pikal at precheza.cz>????:

> Hallo
> 
> probably there are other options but
> 
> outer(1:3,a, "==")*1
> 
> can do what you want.
> 
> HTH
> Petr
> 
> 
> 
> On 31 Aug 2006 at 22:41, z s wrote:
> 
> Date sent:      	Thu, 31 Aug 2006 22:41:27 +0800
> (CST)
> From:           	z s <flyhyena at yahoo.com.cn>
> To:             	r-help at stat.math.ethz.ch
> Subject:        	[R] newbie question about index
> 
> > Hi,
> >
> >   I am trying to convert a variable a =
> sample(1:3,100,rep = T)
> >   represents choices into a 3X100 dummy varible b
> with corresponding
> >   element set to 1 otherwise 0.
> > eg.
> >
> > a: 1 3 2 1 2 3 1 1....
> >
> > b: 1 0 0 1 0 0 1 1..
> >     0 0 1 0 1 0 0 0...
> >     0 1 0 0 0 1 0 0...
> >
> >  Is there something like b[a] =1 existing? I could
> not figure this out
> >  myself.
> >
> >
> > ---------------------------------
> >  Mp3??????-??????????????
> >  [[alternative HTML version deleted]]
> >
> >
> 
> Petr Pikal
> petr.pikal at precheza.cz
> 
> 



		
___________________________________________________________ 
????????????????-3.5G??????20M??????


From bolker at zoo.ufl.edu  Mon Sep  4 17:17:54 2006
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Mon, 4 Sep 2006 15:17:54 +0000 (UTC)
Subject: [R] generating loglogistic distribution in R
References: <44FC372A@webmail>
Message-ID: <loom.20060904T171459-184@post.gmane.org>

nmi13 <nmi13 <at> ext.canterbury.ac.nz> writes:

> 
> Hi Dear

   ???

> 
> Can someone please inform me on genreating random variables of Loglogistic, 
> and PERT beta distributions?
> 

loglogistic:

exp(n,rlogis(n,...))

I'm not sure about the PERT beta -- a few seconds of web browsing
suggests that it's a shifted, scaled version of the beta.  Hence

a + (b-a)*rbeta(n,shape1,shape2)

  cheers
    Ben Bolker


From Xiao.Zhao at newcastle.ac.uk  Mon Sep  4 17:18:51 2006
From: Xiao.Zhao at newcastle.ac.uk (Xiao Zhao)
Date: Mon, 4 Sep 2006 16:18:51 +0100
Subject: [R] Questions about sort data
Message-ID: <4165CF7A7F12DE4B96622CCBB905864705078646@largo.campus.ncl.ac.uk>

Dear R users,
I am doing my project which I want to plot a piecewise function, I knew
that I can use the command segments to plot. But the problem is I want
to use my real data which needs me to sort of my data by using the 'if
else'command, I use it 
If(t[i]<36) lambda<-0.5
Else lambda<-0.2
The funny thing is when I look at my data set, it did not follow my
command to sort data, also the final numbers do not change either.
The other problem is I have a big data set, sometimes I want to know the
number of some data, such as how many numbers are over 60, how to do
this by using R?
Does anybody got any ideas about my two pros?

Thank you in advance
Best wishes
Nessie^_^


From ggrothendieck at gmail.com  Mon Sep  4 17:32:41 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 4 Sep 2006 11:32:41 -0400
Subject: [R] Questions about sort data
In-Reply-To: <4165CF7A7F12DE4B96622CCBB905864705078646@largo.campus.ncl.ac.uk>
References: <4165CF7A7F12DE4B96622CCBB905864705078646@largo.campus.ncl.ac.uk>
Message-ID: <971536df0609040832t529f0fadv2f175322a09fb68a@mail.gmail.com>

Regarding your other questions
On 9/4/06, Xiao Zhao <Xiao.Zhao at newcastle.ac.uk> wrote:
> Dear R users,
> I am doing my project which I want to plot a piecewise function, I knew
> that I can use the command segments to plot. But the problem is I want
> to use my real data which needs me to sort of my data by using the 'if
> else'command, I use it
> If(t[i]<36) lambda<-0.5
> Else lambda<-0.2
> The funny thing is when I look at my data set, it did not follow my
> command to sort data, also the final numbers do not change either.
> The other problem is I have a big data set, sometimes I want to know the
> number of some data, such as how many numbers are over 60, how to do
> this by using R?
> Does anybody got any ideas about my two pros?


Using the built in data set rivers there are

  sum(rivers > 1000)

rivers greater than 1000 miles long.

See the last two lines on every message to r-help.


From chanchal at biochem.mpg.de  Mon Sep  4 17:59:07 2006
From: chanchal at biochem.mpg.de (Chanchal Kumar)
Date: Mon, 4 Sep 2006 17:59:07 +0200
Subject: [R] Problems with 2-D Kernel Density Estimation using MASS &
	KernSmooth
Message-ID: <512FDBA9F3D0C54E95CBAA9A616BD3A601AB5EC0@msx.w2k.biochem.mpg.de>

Dear R-Users,
   
    I am using the two dimensional Kernel density estimation function in
"MASS" package (specifically "kde2d") and am having a recurrent problem.
The problem is that when my data vectors have less then 30000 entries
then the functions gives me the density estimate. But when the vector
size increases beyond 30000 then I get an error. I have pasted below my
steps and the error message. 

>Pmin<-2
>Pmax<-14
>Mmin<-100
>Mmax<-10000000
>N<-200

>dens<-kde2d(data3[[3]],data3[[4]], h = c(width.SJ(data3[[3]],nb=100,
+method="dpi"), width.SJ(data3[[4]], nb=100,method="dpi")), n=N,
+lims=c(Pmin,Pmax,log10(Mmin),log10(Mmax)))

When data3[[3]] and data3[[4]] are each less then 30000 entries then the
function runs fine else it gives me following error message:

>Error in SDh(cnt, (2.394/(n * TD))^(1/7), n, d) : NA/NaN/Inf in foreign
function call (arg 5)

I also tried using the "bkde2D" from "KernSmooth" package and am getting
the same error for longer vector sizes. 

I shall be thankful if you could suggest what is going wrong.
Alternatively I will be glad to know about other possible ways of fast
density estimation on bigger vectors. Thanks in advance!

Best Regards,
Chanchal 
===============================
Chanchal Kumar, Ph.D. Candidate
Dept. of Proteomics and Signal Transduction
Max Planck Institute of Biochemistry
Am Klopferspitz 18
82152 D-Martinsried (near Munich)
Germany
e-mail: chanchal at biochem.mpg.de
Phone: (Office) +49 (0) 89 8578 2296
Fax:(Office) +49 (0) 89 8578 2219
http://www.biochem.mpg.de/mann/


From spencer.graves at pdf.com  Mon Sep  4 18:05:12 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 04 Sep 2006 09:05:12 -0700
Subject: [R] Cross-correlation between two time series data
In-Reply-To: <BAY108-F295F0A4099FCED365A82C2FF3E0@phx.gbl>
References: <BAY108-F295F0A4099FCED365A82C2FF3E0@phx.gbl>
Message-ID: <44FC4EB8.1040108@pdf.com>

      'ccf' provides a plot with red, dashed lines indicating an 
approximate 95% threshold for the correlation. 

      Beyond that, with any particular model fit, you can get confidence 
intervals and anova tests for any particular parameter estimated. 

      If neither of these are adequate, I suppose one might be able to 
try Markov Chain Monte Carlo, but I've never used that, so I can't 
comment further on that. 

      If you would like more help from this listserve, please provide 
more detail of your application including commented, minimal, 
self-contained, reproducible code, explaining something you've tried and 
why it is not adequate (as suggested in the posting guide 
"www.R-project.org/posting-guide.html"). 

      Hope this helps. 
      Spencer Graves

Juni Joshi wrote:
>    Hi all,
>
>    I  have  two  time  series  data  (say  x  and  y). I am interested to
>    calculate the correlation between them and its confidence interval (or
>    to  test  no  correlation). Function cor.test(x,y) does the test of no
>    correlation. But this test probably is wrong because of autocorrelated
>    data.
>
>    ccf()  calculates the correlation between two series data. But it does
>    not  provide  the  confidence intervals of cross correlation. Is there
>    any  function  that  calculates the confidence interval of correlation
>    between  two  time  series data or performs the test of no correlation
>    between two time series data.
>
>    Thanks.
>
>    Jun
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ffenics2002 at yahoo.co.uk  Mon Sep  4 18:20:39 2006
From: ffenics2002 at yahoo.co.uk (Ffenics)
Date: Mon, 4 Sep 2006 17:20:39 +0100 (BST)
Subject: [R] opening files in directory
Message-ID: <20060904162039.9060.qmail@web25503.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060904/8cb57dd1/attachment.pl 

From ligges at statistik.uni-dortmund.de  Mon Sep  4 18:27:54 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 04 Sep 2006 18:27:54 +0200
Subject: [R] opening files in directory
In-Reply-To: <20060904162039.9060.qmail@web25503.mail.ukl.yahoo.com>
References: <20060904162039.9060.qmail@web25503.mail.ukl.yahoo.com>
Message-ID: <44FC540A.4000709@statistik.uni-dortmund.de>

FAQ ...

Uwe Ligges



Ffenics wrote:
> Hi there
> I want to be able to take all the files in a given directory, read them in one at a time, calculate a distance matrix for them (the files are data matrices) and then print them out to separate files. This is the code I thought I would be able to use
> (all files are in directory data_files)
> for(i in 1:length(files))
> + {
> + x<-read.table("data_files/files[[i]]")
> + dist<-dist(x, method="euclidean", diag=TRUE)
> + mat<-as.matrix(dist)
> + write.table(mat, file="files[[i]]")
> + }
> But I get this error when I try to open the first file using read.table
> Error in file(file, "r") : unable to open connection
> In addition: Warning message:
> cannot open file 'data_files/files[[i]]'
> if I try the read.table command without the quotation marks like so
> x<-read.table(data_matrix_files/files[[i]])
> I get the error 
> Error in read.table(data_matrix_files/files[[i]]) :
>         Object "data_matrix_files" not found
> But if I go to the directory where the files are kept before starting up R, the read.table command without the quotation marks works.
> I don't want to start up R in the same directory as the where the files I will be using reside though so how do I rectify this?
> Any help much appreciated
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From georg.otto at tuebingen.mpg.de  Mon Sep  4 18:52:49 2006
From: georg.otto at tuebingen.mpg.de (Georg Otto)
Date: Mon, 04 Sep 2006 18:52:49 +0200
Subject: [R] read csv
Message-ID: <m1ejurlgke.fsf@tuebingen.mpg.de>


Hi,

I have a csv file where the number of filled columns varies in the
different rows:

Sun 5-Feb-06,15,,,01:30:00,0:06:00,
Mon 6-Feb-06,,,,,,
Tue 7-Feb-06,7,,,00:41:00,0:05:51,
Wed 8-Feb-06,,,,,,

I would like to use read.table (or whatever is appropriate) to read in
only those rows that have two or more columns filled. Any hint will be
appreciated.

Georg


From ffenics2002 at yahoo.co.uk  Mon Sep  4 19:04:42 2006
From: ffenics2002 at yahoo.co.uk (Ffenics)
Date: Mon, 4 Sep 2006 18:04:42 +0100 (BST)
Subject: [R] opening files in directory
Message-ID: <20060904170442.25141.qmail@web25503.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060904/f22b676f/attachment.pl 

From ripley at stats.ox.ac.uk  Mon Sep  4 19:36:53 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 4 Sep 2006 18:36:53 +0100 (BST)
Subject: [R] read csv
In-Reply-To: <m1ejurlgke.fsf@tuebingen.mpg.de>
References: <m1ejurlgke.fsf@tuebingen.mpg.de>
Message-ID: <Pine.LNX.4.64.0609041833200.27343@gannet.stats.ox.ac.uk>

On Mon, 4 Sep 2006, Georg Otto wrote:

> 
> Hi,
> 
> I have a csv file where the number of filled columns varies in the
> different rows:
> 
> Sun 5-Feb-06,15,,,01:30:00,0:06:00,
> Mon 6-Feb-06,,,,,,
> Tue 7-Feb-06,7,,,00:41:00,0:05:51,
> Wed 8-Feb-06,,,,,,
> 
> I would like to use read.table (or whatever is appropriate) to read in
> only those rows that have two or more columns filled. Any hint will be
> appreciated.

See ?read.table is the main hint.

Use read.table(fill=TRUE) and post-process, e.g. by

A <- A[rowSums(!is.na(A)) > 2), ]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From hto at arcor.de  Mon Sep  4 19:42:34 2006
From: hto at arcor.de (Thomas Hunger)
Date: Mon, 4 Sep 2006 19:42:34 +0200
Subject: [R] read csv
In-Reply-To: <m1ejurlgke.fsf@tuebingen.mpg.de>
References: <m1ejurlgke.fsf@tuebingen.mpg.de>
Message-ID: <200609041942.34701.hto@arcor.de>

> I have a csv file where the number of filled columns
> varies in the different rows:

I would hack it like this, but then I am totally new to R, 
which means you should not trust me.:

d <- read.csv("testdata", header=F)

selection <- apply(d, c(1), 
      function(x) {sum(!is.na(x) & x != "") > 2})

as.data.frame(t(as.data.frame(t(d))[selection]))


Tom


From wasquith at austin.rr.com  Mon Sep  4 20:18:57 2006
From: wasquith at austin.rr.com (William Asquith)
Date: Mon, 4 Sep 2006 13:18:57 -0500
Subject: [R] Fitting Pareto distribution to some data
In-Reply-To: <6ade6f6c0609030855j34154077y9fd32674edd2ece3@mail.gmail.com>
References: <6ade6f6c0609030855j34154077y9fd32674edd2ece3@mail.gmail.com>
Message-ID: <2F725E0F-DFA4-4183-9383-0B7F929CA2E7@austin.rr.com>

Paul,
Package lmomco fits generalized pareto (three parameter) using method  
of L-moments.  I suspect that other packages that Brian identified  
use method of moments or other.

William

On Sep 3, 2006, at 10:55 AM, Paul Smith wrote:

> Dear All
>
> I am trying to fit Pareto distribution to some data. MASS package does
> not support Pareto distribution. Is there some alternative way?
>
> Thanks in advance,
>
> Paul
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jh910 at juno.com  Mon Sep  4 20:34:25 2006
From: jh910 at juno.com (J. Hosking)
Date: Mon, 04 Sep 2006 14:34:25 -0400
Subject: [R] Subsetting vectors based on condition
In-Reply-To: <20060904010616.70272.qmail@web83104.mail.mud.yahoo.com>
References: <20060904010616.70272.qmail@web83104.mail.mud.yahoo.com>
Message-ID: <edhrjj$mmv$1@sea.gmane.org>

Mahesh Krishnan wrote:
> Hello,
> 
> I have a question regarding subsetting of vectors. Here's an example of
> what I'm trying to do:
> 
> vect.1 <- c(76,195, 290, 380)
> 
> vect.2 <-  c(63,  95, 133, 170, 215, 253, 285, 299, 325, 375)
> 
> I would like to subset vect.2 so that it has the same length as vect.1,
> and its numbers are the first corresponging higher value compared to
> vect.1.
> 
> The output should be:
> 
>  final.output = (95, 215, 299, NA)
> 
> What is the fastest/most eficient way to accompllish this in R?
> 
> Thanks for the help in advance,
> 
> Mahesh Krishnan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

 > vect.1 <- c(76,195, 290, 380)
 > vect.2 <-  c(63,  95, 133, 170, 215, 253, 285, 299, 325, 375)
 > vect.2[ findInterval(vect.1,vect.2) + 1 ]
[1]  95 215 299  NA

J. R. M. Hosking


From phhs80 at gmail.com  Mon Sep  4 20:55:26 2006
From: phhs80 at gmail.com (Paul Smith)
Date: Mon, 4 Sep 2006 19:55:26 +0100
Subject: [R] Fitting Pareto distribution to some data
In-Reply-To: <2F725E0F-DFA4-4183-9383-0B7F929CA2E7@austin.rr.com>
References: <6ade6f6c0609030855j34154077y9fd32674edd2ece3@mail.gmail.com>
	<2F725E0F-DFA4-4183-9383-0B7F929CA2E7@austin.rr.com>
Message-ID: <6ade6f6c0609041155p112fad2bw42930994a771ac98@mail.gmail.com>

On 9/4/06, William Asquith <wasquith at austin.rr.com> wrote:
> Package lmomco fits generalized pareto (three parameter) using method
> of L-moments.  I suspect that other packages that Brian identified
> use method of moments or other.

That is excellent to learn that, William. Thanks.

Paul


> On Sep 3, 2006, at 10:55 AM, Paul Smith wrote:
>
> > Dear All
> >
> > I am trying to fit Pareto distribution to some data. MASS package does
> > not support Pareto distribution. Is there some alternative way?
> >
> > Thanks in advance,
> >
> > Paul
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> > guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>


From spencer.graves at pdf.com  Mon Sep  4 21:30:44 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 04 Sep 2006 12:30:44 -0700
Subject: [R] Optimization
In-Reply-To: <200608301357.k7UDuodl032724@smtp.unipr.it>
References: <200608301357.k7UDuodl032724@smtp.unipr.it>
Message-ID: <44FC7EE4.70708@pdf.com>

      Have you considered talking logarithms of the expression you 
mentioned: 

      log(Yield) = a1*log(A)+b1*log(B)+c2*log(C)+...

where a1 = a/(a+b+...), etc.  This model has two constraints not present 
in ordinary least squares:  First, the intercept is assumed to be zero.  
Second, the coefficients in this log formulation must sum to 1.  If I 
were you, I might use something like "lm" to test them both. 

      To explain how, I'll modify the notation, replacing A by X1, B by 
X2, ..., up to Xkm1 (= X[k-1]) and Xk for k different environmental 
variables.  Then I might try something like the following: 

      fit0 <- lm(log(Yield) ~ log(X1) + ... + log(Xk)-1 )
      fit1 <- lm(log(Yield) ~ log(X1) + ... + log(Xk) )
      fit.1 <- lm(log(Yield/Xk) ~ log(X1/Xk) + ... + log(Xkm1/Xk) )
      fit.0 <- lm(log(Yield/Xk) ~ log(X1/Xk) + ... + log(Xkm1/Xk)-1 )

      anova(fit1, fit0) would test the no-constant model, and if I 
haven't made a mistake in this, anova(fit0, fit.0) and anova(fit1, 
fit.1) would test the constraint that all the coefficients should sum to 
1. 

      If you would like further help from this listserve, please provide 
commented, minimal, self-contained, reproducible code to help potential 
respondents understand your question and concerns (as suggested in the 
posting guide "www.R-project.org/posting-guide.html"). 

      Hope this helps. 
      Spencer Graves

Simone Vincenzi wrote:
> Dear R-list,
> I'm trying to estimate the relative importance of 6 environmental variables
> in determining clam yield. To estimate clam yield a previous work used the
> function Yield = (A^a*B^b*C^c...)^1/(a+b+c+...) where A,B,C... are the
> values of the environmental variables and the weights a,b,c... have not been
> calibrated on data but taken from literature. Now I'd like to estimate the
> weights a,b,c... by using a dataset with 110 observations of yield and
> values of the environmental variables. I'm wondering if it is feasible or if
> the number of observation is too low, if some data transformation is needed
> and which R function is the most appropriate to try to estimate the weights.
> Any help would be greatly appreciated.
>
> Simone Vincenzi 
>
> _________________________________________
> Simone Vincenzi, PhD Student 
> Department of Environmental Sciences
> University of Parma
> Parco Area delle Scienze, 33/A, 43100 Parma, Italy
> Phone: +39 0521 905696
> Fax: +39 0521 906611
> e.mail: svincenz at nemo.unipr.it 
>
>
>
> --
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mr.blacksheep at gmail.com  Mon Sep  4 21:54:51 2006
From: mr.blacksheep at gmail.com (Mike Nielsen)
Date: Mon, 4 Sep 2006 14:54:51 -0500
Subject: [R] opening files in directory
In-Reply-To: <20060904162039.9060.qmail@web25503.mail.ukl.yahoo.com>
References: <20060904162039.9060.qmail@web25503.mail.ukl.yahoo.com>
Message-ID: <46a360560609041254u1151dd93o6fe34d109a78636a@mail.gmail.com>

R won't do variable interpolation inside quotation marks as perl does.

You could try amending your code with, for e.g.

file.name<-paste(sep="/","data_files",files[[i]])
x<-read.table(file.name)

Regards,

Mike

On 9/4/06, Ffenics <ffenics2002 at yahoo.co.uk> wrote:
> Hi there
> I want to be able to take all the files in a given directory, read them in one at a time, calculate a distance matrix for them (the files are data matrices) and then print them out to separate files. This is the code I thought I would be able to use
> (all files are in directory data_files)
> for(i in 1:length(files))
> + {
> + x<-read.table("data_files/files[[i]]")
> + dist<-dist(x, method="euclidean", diag=TRUE)
> + mat<-as.matrix(dist)
> + write.table(mat, file="files[[i]]")
> + }
> But I get this error when I try to open the first file using read.table
> Error in file(file, "r") : unable to open connection
> In addition: Warning message:
> cannot open file 'data_files/files[[i]]'
> if I try the read.table command without the quotation marks like so
> x<-read.table(data_matrix_files/files[[i]])
> I get the error
> Error in read.table(data_matrix_files/files[[i]]) :
>         Object "data_matrix_files" not found
> But if I go to the directory where the files are kept before starting up R, the read.table command without the quotation marks works.
> I don't want to start up R in the same directory as the where the files I will be using reside though so how do I rectify this?
> Any help much appreciated
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Regards,

Mike Nielsen


From ggrothendieck at gmail.com  Mon Sep  4 22:13:31 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 4 Sep 2006 16:13:31 -0400
Subject: [R] opening files in directory
In-Reply-To: <46a360560609041254u1151dd93o6fe34d109a78636a@mail.gmail.com>
References: <20060904162039.9060.qmail@web25503.mail.ukl.yahoo.com>
	<46a360560609041254u1151dd93o6fe34d109a78636a@mail.gmail.com>
Message-ID: <971536df0609041313n31021c49rf5ff7f66b713bd2d@mail.gmail.com>

On 9/4/06, Mike Nielsen <mr.blacksheep at gmail.com> wrote:
> R won't do variable interpolation inside quotation marks as perl does.

Just as an aside, gsubfn in package gsubfn will do perl-style
(well, sort of) string interpolation:

> library(gsubfn)
> i <- 1
> gsubfn(x = "data_files/file$i")
[1] "data_files/file1"

cati and cati0 in the same package provide for such interpolation
within cat.


From p.murrell at auckland.ac.nz  Mon Sep  4 22:19:02 2006
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Tue, 05 Sep 2006 08:19:02 +1200
Subject: [R] abline and plot(augPred) help
In-Reply-To: <44FC32B5.32197.14FCFC6@localhost>
References: <44FC32B5.32197.14FCFC6@localhost>
Message-ID: <44FC8A36.9010208@stat.auckland.ac.nz>

Hi


Petr Pikal wrote:
> Dear all
> 
> as I did not get any response on my post about abline and 
> plot(augPred)) I try again. I hope I do not break some posting guide 
> rules. I would try to contact package maintainer directly but there 
> is stated to be R-core people, so I feel R-help list shall be OK.
> 
> I need to draw straight lines through augPred plotted panels 
> (vertical or horizontal) at specified point. I know I shall probably 
> use panel.abline but I am missing correct syntax. Below you can see 
> my attempts together with results. I hope somebody can point me to 
> right direction.
> 
> I am probably somewhere close but I have no clue, which parameter I
> shall modify to get measured points, fitted lines and vertical lines
> in panels together.


The problem is that you do not know about the default panel function
that nlme:::plot.augPred() uses, so your panel functions are not
replicating all of the default behaviour as well as adding your vertical
lines.  Some possible solutons suggested below ...


> fm1 <- lme(Orthodont)
> 
> # standard plot
> plot(augPred(fm1, level = 0:1, length.out = 2))
> 
> #plot with vertical but without points and fitted lines
> plot(augPred(fm1, level = 0:1, length.out = 2),
> panel=function(v,...) {
> panel.abline(v=10)}
> )
> 
> # plot with vertical but without fitted lines
> plot(augPred(fm1, level = 0:1, length.out=2),
> panel=function(x,y,...) {
> panel.xyplot(x,y,...)
> panel.abline(v=10)}
> )
> 
> # plot with vertical and with all points (fitted lines are drawn as 
> points)
> plot(augPred(fm1, level = 0:1),
> panel=function(x,y,...) {
> panel.xyplot(x,y,...)
> panel.abline(v=10)}
> )

One option is to take a sneak a peek at nlme:::plot.augPred() to see
what the default panel function is doing.  Here I have replicated the
default panel function and added a call to panel.abline().

plot(augPred(fm1, level = 0:1, length.out = 2),
  panel=function(x, y, subscripts, groups, ...) {
                orig <- groups[subscripts] == "original"
                panel.xyplot(x[orig], y[orig], ...)
                panel.superpose(x[!orig], y[!orig], subscripts[!orig],
                                groups, ..., type = "l")
                panel.abline(v=10)
  })

The problem with this approach is that you need to crawl around in the
code of nlme:::plot.augPred().  An alternative approach is to annotate
the plot after-the-fact.  This is shown below.

plot(augPred(fm1, level = 0:1, length.out = 2))
for (i in 1:5) {
  for (j in 1:6) {
    if (i < 5 || j < 4) {
      trellis.focus("panel", j, i, highlight=FALSE)
      panel.abline(v=10)
    }
  }
}

This avoids crawling around in code, but the problem with this is
knowing how many rows and columns of panels there are.  If you
explicitly controlled the 'layout' of the original plot, you could
guarantee that your annotation works properly.

Hope that helps.

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/


From ggrothendieck at gmail.com  Mon Sep  4 22:37:00 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 4 Sep 2006 16:37:00 -0400
Subject: [R] abline and plot(augPred) help
In-Reply-To: <44FC8A36.9010208@stat.auckland.ac.nz>
References: <44FC32B5.32197.14FCFC6@localhost>
	<44FC8A36.9010208@stat.auckland.ac.nz>
Message-ID: <971536df0609041337y2eba6e45uea85588f5977f8db@mail.gmail.com>

On 9/4/06, Paul Murrell <p.murrell at auckland.ac.nz> wrote:
> Hi
>
>
> Petr Pikal wrote:
> > Dear all
> >
> > as I did not get any response on my post about abline and
> > plot(augPred)) I try again. I hope I do not break some posting guide
> > rules. I would try to contact package maintainer directly but there
> > is stated to be R-core people, so I feel R-help list shall be OK.
> >
> > I need to draw straight lines through augPred plotted panels
> > (vertical or horizontal) at specified point. I know I shall probably
> > use panel.abline but I am missing correct syntax. Below you can see
> > my attempts together with results. I hope somebody can point me to
> > right direction.
> >
> > I am probably somewhere close but I have no clue, which parameter I
> > shall modify to get measured points, fitted lines and vertical lines
> > in panels together.
>
>
> The problem is that you do not know about the default panel function
> that nlme:::plot.augPred() uses, so your panel functions are not
> replicating all of the default behaviour as well as adding your vertical
> lines.  Some possible solutons suggested below ...
>
>
> > fm1 <- lme(Orthodont)
> >
> > # standard plot
> > plot(augPred(fm1, level = 0:1, length.out = 2))
> >
> > #plot with vertical but without points and fitted lines
> > plot(augPred(fm1, level = 0:1, length.out = 2),
> > panel=function(v,...) {
> > panel.abline(v=10)}
> > )
> >
> > # plot with vertical but without fitted lines
> > plot(augPred(fm1, level = 0:1, length.out=2),
> > panel=function(x,y,...) {
> > panel.xyplot(x,y,...)
> > panel.abline(v=10)}
> > )
> >
> > # plot with vertical and with all points (fitted lines are drawn as
> > points)
> > plot(augPred(fm1, level = 0:1),
> > panel=function(x,y,...) {
> > panel.xyplot(x,y,...)
> > panel.abline(v=10)}
> > )
>
> One option is to take a sneak a peek at nlme:::plot.augPred() to see
> what the default panel function is doing.  Here I have replicated the
> default panel function and added a call to panel.abline().
>
> plot(augPred(fm1, level = 0:1, length.out = 2),
>  panel=function(x, y, subscripts, groups, ...) {
>                orig <- groups[subscripts] == "original"
>                panel.xyplot(x[orig], y[orig], ...)
>                panel.superpose(x[!orig], y[!orig], subscripts[!orig],
>                                groups, ..., type = "l")
>                panel.abline(v=10)
>  })
>
> The problem with this approach is that you need to crawl around in the
> code of nlme:::plot.augPred().  An alternative approach is to annotate
> the plot after-the-fact.  This is shown below.
>
> plot(augPred(fm1, level = 0:1, length.out = 2))
> for (i in 1:5) {
>  for (j in 1:6) {
>    if (i < 5 || j < 4) {
>      trellis.focus("panel", j, i, highlight=FALSE)
>      panel.abline(v=10)
>    }
>  }
> }
>
> This avoids crawling around in code, but the problem with this is
> knowing how many rows and columns of panels there are.  If you
> explicitly controlled the 'layout' of the original plot, you could
> guarantee that your annotation works properly.
>

You can find that out with trellis.currentLayout:

tcL <- trellis.currentLayout()
for(i in 1:nrow(tcL))
  for(j in 1:ncol(tcL))
    if (tcL[i,j] > 0) {
        trellis.focus("panel", j, i, highlight = FALSE)
        panel.abline(v = 10)
        trellis.unfocus()
    }


From ThadenJohnJ at uams.edu  Mon Sep  4 23:16:32 2006
From: ThadenJohnJ at uams.edu (Thaden, John J)
Date: Mon, 4 Sep 2006 16:16:32 -0500
Subject: [R] Coercing elements of a matrix from integer to double
Message-ID: <0C6BF3FC506F664F90C8BA3E0160462D04A75A52@EXCHANGE3.ad.uams.edu>

Ive been converting elements of matrices and arrays, e.g.,  from
Integers to double-precision, by vectorizing the matrix and then
remaking it. Alternatively, I can redefine one element as double 
which then redefines them all.  Both methods are quick, so I guess
I shouldn't complain, but I would have thought there'd be something
more obvious. Have I missed it?

Here's my redimensioning example:  

## Matrix M...
M <- 1:2e6 ; dim(Mi) <- c(1e3,2e3)
dim(M)
class(M)
## ...has integer elements, e.g.,
class(M[1,1])

## The as.double() command changes
## these to double-precision, but it
## also strips away dimensions...
Md <- as.double(Mi)
dim(Md)
class(Md)
## ...so I have to put them back.
dim(Md) <- dim(Mi)
dim(Md) 
class(Md)
class(Md[1,1])

Here's my "tail wagging the dog" example:

M[1,1] <- as.double(M[1,1])
class(M[2,2])

Thanks,
-John Thaden

Confidentiality Notice: This e-mail message, including any a...{{dropped}}


From p.dalgaard at biostat.ku.dk  Mon Sep  4 23:28:46 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 04 Sep 2006 23:28:46 +0200
Subject: [R] Coercing elements of a matrix from integer to double
In-Reply-To: <0C6BF3FC506F664F90C8BA3E0160462D04A75A52@EXCHANGE3.ad.uams.edu>
References: <0C6BF3FC506F664F90C8BA3E0160462D04A75A52@EXCHANGE3.ad.uams.edu>
Message-ID: <x2u03nz5gx.fsf@turmalin.kubism.ku.dk>

"Thaden, John J" <ThadenJohnJ at uams.edu> writes:

> Ive been converting elements of matrices and arrays, e.g.,  from
> Integers to double-precision, by vectorizing the matrix and then
> remaking it. Alternatively, I can redefine one element as double 
> which then redefines them all.  Both methods are quick, so I guess
> I shouldn't complain, but I would have thought there'd be something
> more obvious. Have I missed it?

storage.mode(M) <- "double"


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From hto at arcor.de  Mon Sep  4 23:46:48 2006
From: hto at arcor.de (Thomas Hunger)
Date: Mon, 4 Sep 2006 23:46:48 +0200
Subject: [R] how to fit gauss beam?
In-Reply-To: <200609041614.50475.hto@arcor.de>
References: <200609041614.50475.hto@arcor.de>
Message-ID: <200609042346.48469.hto@arcor.de>

Reply to self:
> I am having a hard time fitting a gauss beam using R. In
> gnutplot I did something like
>
> $ w(z) = w0 * sqrt(1+(z/z0)**2)
> $ fit w(z) 'before_eom.txt' using 1:2 via w0, z0
>
> to obtain w0 and z0. Now I want to do the same in R. I
> tried a linear model like this (r = radius, z =

This works fine:

data <- read.table ("nach_eom.tab", header=T)

M <- 1.1
nls (major ~ M*w0*sqrt(1+(d/z0)^2), 
     data = data, 
     start = list(w0 = 460, z0=0.02))


From A.Robinson at ms.unimelb.edu.au  Tue Sep  5 00:06:26 2006
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Tue, 5 Sep 2006 08:06:26 +1000
Subject: [R] Cross-correlation between two time series data
In-Reply-To: <44FC4EB8.1040108@pdf.com>
References: <BAY108-F295F0A4099FCED365A82C2FF3E0@phx.gbl>
	<44FC4EB8.1040108@pdf.com>
Message-ID: <20060904220626.GS12212@ms.unimelb.edu.au>

Jun,

If your interest is to estimate the correlation and either a
confidence interval or a test for no correlation, then you might try
to proceed as follows.  This is a Monte-Carlo significance test, and a
useful strategy.

1) use ccf() to compute the cross-correlation between x and y.

2) repeat the following steps, say, 1000 times.

2a) randomly reorder the values of one of the time series, say x.
    Call the randomly reordered series x'. 

2b) use ccf() to compute the cross-correlation between x' and y.
    Store that cross-correlation.

3) the 1000 cross-correlation estimates computed in step 2 are all
   estimating cross-correlation 0, conditional on the data.  A
   two-tailed test then is: if the cross-correlation computed in step
   1 is outside the (0.025, 0.975) quantiles of the empirical
   distribution of the cross-correlations computed in step 2, then,
   reject the null hypothesis that x and y are uncorrelated, with size
   0.05.

I hope that this helps.

Andrew


Juni Joshi wrote:
>    Hi all,
>
>    I  have  two  time  series  data  (say  x  and  y). I am interested to
>    calculate the correlation between them and its confidence interval (or
>    to  test  no  correlation). Function cor.test(x,y) does the test of no
>    correlation. But this test probably is wrong because of autocorrelated
>    data.
>
>    ccf()  calculates the correlation between two series data. But it does
>    not  provide  the  confidence intervals of cross correlation. Is there
>    any  function  that  calculates the confidence interval of correlation
>    between  two  time  series data or performs the test of no correlation
>    between two time series data.
>
>    Thanks.
>
>    Jun
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
Email: a.robinson at ms.unimelb.edu.au         http://www.ms.unimelb.edu.au


From mskrishnan71 at yahoo.com  Tue Sep  5 00:30:01 2006
From: mskrishnan71 at yahoo.com (Mahesh Krishnan)
Date: Mon, 4 Sep 2006 15:30:01 -0700 (PDT)
Subject: [R] Subsetting vectors based on condition
In-Reply-To: <644e1f320609040518y1bc08ff9g498d988390ec0441@mail.gmail.com>
Message-ID: <20060904223002.52964.qmail@web83109.mail.mud.yahoo.com>



Sincere thanks to Jim Holtman and J. Hosking for their suggestions.
both their solutions work perfectly,
in particular the "findInterval" function is  what I was looking for.

Cheers.

Mahesh Krishnan


From dreiss.isb at gmail.com  Tue Sep  5 00:39:12 2006
From: dreiss.isb at gmail.com (David Reiss)
Date: Mon, 4 Sep 2006 15:39:12 -0700
Subject: [R] Fitting generalized additive models with constraints?
Message-ID: <fd913b0d0609041539o545e7e0bx8bbe4ac9f369976e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060904/5b7d61f6/attachment.pl 

From t.gardner at uea.ac.uk  Tue Sep  5 00:57:40 2006
From: t.gardner at uea.ac.uk (Toby Gardner)
Date: Mon, 4 Sep 2006 23:57:40 +0100
Subject: [R] Problem with Variance Components (and general glmm confusion)
Message-ID: <01b001c6d075$86fb3940$5401a8c0@Toby>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060904/d85904d6/attachment.pl 

From spencer.graves at pdf.com  Tue Sep  5 01:00:44 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 04 Sep 2006 16:00:44 -0700
Subject: [R] Cross-correlation between two time series data
In-Reply-To: <20060904220626.GS12212@ms.unimelb.edu.au>
References: <BAY108-F295F0A4099FCED365A82C2FF3E0@phx.gbl>	<44FC4EB8.1040108@pdf.com>
	<20060904220626.GS12212@ms.unimelb.edu.au>
Message-ID: <44FCB01C.1080805@pdf.com>

Hi, Andrew: 

      This will produce a "permutation distribution" for the correlation 
under the null hypothesis of zero correlation between the variables.  
This is a reasonable thing to do, and would probably produce limits more 
accurate than the dashed red lines on the 'ccf' plot.  However, they 
would NOT be confidence interval(s). 

      For a confidence interval on cross correlation, you'd have to 
hypothesize some cross correlation pattern between x and y, preferably 
parameterized parsimoniously, then somehow determine an appropriate 
range of values consistent with the data.  By the time you've done all 
that, you've effectively fit some model and constructed confidence 
intervals on the parameter(s). 

      Best Wishes,
      Spencer

Andrew Robinson wrote:
> Jun,
>
> If your interest is to estimate the correlation and either a
> confidence interval or a test for no correlation, then you might try
> to proceed as follows.  This is a Monte-Carlo significance test, and a
> useful strategy.
>
> 1) use ccf() to compute the cross-correlation between x and y.
>
> 2) repeat the following steps, say, 1000 times.
>
> 2a) randomly reorder the values of one of the time series, say x.
>     Call the randomly reordered series x'. 
>
> 2b) use ccf() to compute the cross-correlation between x' and y.
>     Store that cross-correlation.
>
> 3) the 1000 cross-correlation estimates computed in step 2 are all
>    estimating cross-correlation 0, conditional on the data.  A
>    two-tailed test then is: if the cross-correlation computed in step
>    1 is outside the (0.025, 0.975) quantiles of the empirical
>    distribution of the cross-correlations computed in step 2, then,
>    reject the null hypothesis that x and y are uncorrelated, with size
>    0.05.
>
> I hope that this helps.
>
> Andrew
>
>
> Juni Joshi wrote:
>   
>>    Hi all,
>>
>>    I  have  two  time  series  data  (say  x  and  y). I am interested to
>>    calculate the correlation between them and its confidence interval (or
>>    to  test  no  correlation). Function cor.test(x,y) does the test of no
>>    correlation. But this test probably is wrong because of autocorrelated
>>    data.
>>
>>    ccf()  calculates the correlation between two series data. But it does
>>    not  provide  the  confidence intervals of cross correlation. Is there
>>    any  function  that  calculates the confidence interval of correlation
>>    between  two  time  series data or performs the test of no correlation
>>    between two time series data.
>>
>>    Thanks.
>>
>>    Jun
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>     
>
>


From amstat2006 at gmail.com  Tue Sep  5 01:42:09 2006
From: amstat2006 at gmail.com (Am Stat)
Date: Mon, 4 Sep 2006 19:42:09 -0400
Subject: [R] plot a new picture against an old one to see the difference
	between them
Message-ID: <003201c6d07b$bd896210$0200a8c0@LeonE1405>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060904/a0a33ffd/attachment.pl 

From amstat2006 at gmail.com  Tue Sep  5 02:11:54 2006
From: amstat2006 at gmail.com (Am Stat)
Date: Mon, 4 Sep 2006 20:11:54 -0400
Subject: [R] plot a new picture against an old one to see the difference
	between them
References: <003201c6d07b$bd896210$0200a8c0@LeonE1405>
	<52086EF2-CE2D-4207-A39C-2FDF220A0200@ysidro.econ.uiuc.edu>
Message-ID: <004001c6d07f$e54fe400$0200a8c0@LeonE1405>

Dear Roger,

Thanks, that's really helpful,  do you know how to deal with it if the two 
plots are  generated by plot(), not by contour().

Best,

Leon


----- Original Message ----- 
From: "roger koenker" <roger at ysidro.econ.uiuc.edu>
To: "Am Stat" <amstat2006 at gmail.com>
Sent: Monday, September 04, 2006 8:06 PM
Subject: Re: [R] plot a new picture against an old one to see the difference 
between them


> for the second call to contour  use the argument add=TRUE.
>
> On Sep 4, 2006, at 6:42 PM, Am Stat wrote:
>
>> Hello, useR:,
>>
>> Suppose I have two plots made by using contour() function, say  Cont1 and 
>> Cont2 respectively.
>>
>> They have slightly difference because of the two slightly different  data 
>> I used.
>>
>> I want to see the difference between them so I want to plot Cont2  on 
>> Cont1, are there any methods to plot it without filling the  frame of 
>> Cont1 totally of Cont2.
>> I mean, how I can integreate the two plots together that they kind  of 
>> have weighted colors?
>>
>> Thanks very much in Advance!
>>
>> Leon
>> [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting- 
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>


From krcabrer at une.net.co  Tue Sep  5 03:48:54 2006
From: krcabrer at une.net.co (Kenneth Cabrera)
Date: Mon, 04 Sep 2006 20:48:54 -0500
Subject: [R] Help with plotmath
In-Reply-To: <004001c6d07f$e54fe400$0200a8c0@LeonE1405>
References: <003201c6d07b$bd896210$0200a8c0@LeonE1405>
	<52086EF2-CE2D-4207-A39C-2FDF220A0200@ysidro.econ.uiuc.edu>
	<004001c6d07f$e54fe400$0200a8c0@LeonE1405>
Message-ID: <op.tfdtzshn3mu6w9@davinci.epm.net.co>

Hi R users:

How can I have several subscript number with a comma in a plot.

I would like to have the LaTeX equivalent of

x_{i,j}.

I try:

plot(1:10,1:10,type="n")
text(5,5,expression(x[i,j]))

but it doesn?t work.

Thank you for your help.


From r.darnell at uq.edu.au  Tue Sep  5 04:11:17 2006
From: r.darnell at uq.edu.au (Ross Darnell)
Date: Tue, 05 Sep 2006 12:11:17 +1000
Subject: [R] Sweave and the "[" function
Message-ID: <44FCDCC5.9010903@uq.edu.au>

I am wanting to use the "[" operator in an S-chunk, e.g.


<<>>=
str(women)
women$height
women[,1]
"["(women,1)
@

to show the equivalence of  three methods of extracting an element from 
a data.frame.

However Sweave returns the last of these as

women[1]

in the S input chunk

How can I force it not to do this and return "["(women,1)


From wwguocn at gmail.com  Tue Sep  5 04:58:55 2006
From: wwguocn at gmail.com (Guo Wei-Wei)
Date: Tue, 5 Sep 2006 10:58:55 +0800
Subject: [R] Question on Chi-square of null model in sem package
In-Reply-To: <20060904133436.WZMD1747.tomts25-srv.bellnexxia.net@JohnDesktop8300>
References: <d3677d7d0609032233rf2ecc6ajadeb5edf075df1e2@mail.gmail.com>
	<20060904133436.WZMD1747.tomts25-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <d3677d7d0609041958h2e1806aci1c7d30a10994bc89@mail.gmail.com>

Dear Pref. Fox

Sorry, I didn't receive your reply. I try the new sem package. It's
great. The following is the results that I got. The fit indices are
fine.

 Model Chisquare =  208   Df =  98 Pr(>Chisq) = 6.6e-10
 Chisquare (null model) =  1741   Df =  120
 Goodness-of-fit index =  0.9
 Adjusted goodness-of-fit index =  0.87
 RMSEA index =  0.066   90 % CI: (0.054, 0.079)
 Bentler-Bonnett NFI =  0.88
 Tucker-Lewis NNFI =  0.92
 Bentler CFI =  0.93
 BIC =  -336

Thank you very much. You help me out so many problems.

Best wishes,
Wei-Wei


2006/9/4, John Fox <jfox at mcmaster.ca>:
> Dear Wei-Wei,
>
> As I explained to you in private email yesterday (perhaps you didn't receive
> my reply?), the problem that you point out is due to a bug in the sem
> function that I fixed some time ago and then inadvertently reintroduced.
> Yesterday, I sent a corrected version of the sem package (0.9-5) to CRAN;
> the source package is there now and I'm sure that the compiled Windows
> package will appear in due course.
>
> Thank you once more for bringing the problem to my attention.
>
> John
>


From A.Robinson at ms.unimelb.edu.au  Tue Sep  5 05:04:57 2006
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Tue, 5 Sep 2006 13:04:57 +1000
Subject: [R] Cross-correlation between two time series data
In-Reply-To: <44FCB01C.1080805@pdf.com>
References: <BAY108-F295F0A4099FCED365A82C2FF3E0@phx.gbl>
	<44FC4EB8.1040108@pdf.com>
	<20060904220626.GS12212@ms.unimelb.edu.au>
	<44FCB01C.1080805@pdf.com>
Message-ID: <20060905030457.GT12212@ms.unimelb.edu.au>

Hi Spencer,

you are quite right.  I should have been careful to emphasize that the
strategy I suggested was intended only to produce the "test for no
correlation" clause of the "either a confidence interval or a test for
no correlation" sentence.

Cheers

Andrew

On Mon, Sep 04, 2006 at 04:00:44PM -0700, Spencer Graves wrote:
> Hi, Andrew: 
> 
>      This will produce a "permutation distribution" for the correlation 
> under the null hypothesis of zero correlation between the variables.  
> This is a reasonable thing to do, and would probably produce limits more 
> accurate than the dashed red lines on the 'ccf' plot.  However, they 
> would NOT be confidence interval(s). 
> 
>      For a confidence interval on cross correlation, you'd have to 
> hypothesize some cross correlation pattern between x and y, preferably 
> parameterized parsimoniously, then somehow determine an appropriate 
> range of values consistent with the data.  By the time you've done all 
> that, you've effectively fit some model and constructed confidence 
> intervals on the parameter(s). 
> 
>      Best Wishes,
>      Spencer
> 
> Andrew Robinson wrote:
> >Jun,
> >
> >If your interest is to estimate the correlation and either a
> >confidence interval or a test for no correlation, then you might try
> >to proceed as follows.  This is a Monte-Carlo significance test, and a
> >useful strategy.
> >
> >1) use ccf() to compute the cross-correlation between x and y.
> >
> >2) repeat the following steps, say, 1000 times.
> >
> >2a) randomly reorder the values of one of the time series, say x.
> >    Call the randomly reordered series x'. 
> >
> >2b) use ccf() to compute the cross-correlation between x' and y.
> >    Store that cross-correlation.
> >
> >3) the 1000 cross-correlation estimates computed in step 2 are all
> >   estimating cross-correlation 0, conditional on the data.  A
> >   two-tailed test then is: if the cross-correlation computed in step
> >   1 is outside the (0.025, 0.975) quantiles of the empirical
> >   distribution of the cross-correlations computed in step 2, then,
> >   reject the null hypothesis that x and y are uncorrelated, with size
> >   0.05.
> >
> >I hope that this helps.
> >
> >Andrew
> >
> >
> >Juni Joshi wrote:
> >  
> >>   Hi all,
> >>
> >>   I  have  two  time  series  data  (say  x  and  y). I am interested to
> >>   calculate the correlation between them and its confidence interval (or
> >>   to  test  no  correlation). Function cor.test(x,y) does the test of no
> >>   correlation. But this test probably is wrong because of autocorrelated
> >>   data.
> >>
> >>   ccf()  calculates the correlation between two series data. But it does
> >>   not  provide  the  confidence intervals of cross correlation. Is there
> >>   any  function  that  calculates the confidence interval of correlation
> >>   between  two  time  series data or performs the test of no correlation
> >>   between two time series data.
> >>
> >>   Thanks.
> >>
> >>   Jun
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide 
> >>http://www.R-project.org/posting-guide.html
> >>and provide commented, minimal, self-contained, reproducible code.
> >>
> >>    
> >
> >  

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
Email: a.robinson at ms.unimelb.edu.au         http://www.ms.unimelb.edu.au


From ggrothendieck at gmail.com  Tue Sep  5 05:41:39 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 4 Sep 2006 23:41:39 -0400
Subject: [R] Help with plotmath
In-Reply-To: <op.tfdtzshn3mu6w9@davinci.epm.net.co>
References: <003201c6d07b$bd896210$0200a8c0@LeonE1405>
	<52086EF2-CE2D-4207-A39C-2FDF220A0200@ysidro.econ.uiuc.edu>
	<004001c6d07f$e54fe400$0200a8c0@LeonE1405>
	<op.tfdtzshn3mu6w9@davinci.epm.net.co>
Message-ID: <971536df0609042041j2496a60fp93601132b914f45a@mail.gmail.com>

Try:

text(5,5,expression(x[i * "," * j]))

On 9/4/06, Kenneth Cabrera <krcabrer at une.net.co> wrote:
> Hi R users:
>
> How can I have several subscript number with a comma in a plot.
>
> I would like to have the LaTeX equivalent of
>
> x_{i,j}.
>
> I try:
>
> plot(1:10,1:10,type="n")
> text(5,5,expression(x[i,j]))
>
> but it doesn?t work.
>
> Thank you for your help.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From h.wickham at gmail.com  Tue Sep  5 06:03:32 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Mon, 4 Sep 2006 23:03:32 -0500
Subject: [R] Sweave and the "[" function
In-Reply-To: <44FCDCC5.9010903@uq.edu.au>
References: <44FCDCC5.9010903@uq.edu.au>
Message-ID: <f8e6ff050609042103m6755c62cq3093f692951907e@mail.gmail.com>

> <<>>=
> str(women)
> women$height
> women[,1]
> "["(women,1)
> @
>
> to show the equivalence of  three methods of extracting an element from
> a data.frame.
>
> However Sweave returns the last of these as
>
> women[1]
>
> in the S input chunk
>
> How can I force it not to do this and return "["(women,1)

I don't think you can.  Sweave parses your R code and from then on
uses the internal R representation.  R normalises the parse tree in
certain ways (eg. strips comments, formats source code, and clearly
normalises some function calls).  Since sweave uses this, and not the
original text, I don't think there is anyway to get around this,
unless there is some trick during parsing.

(And don't forget women[[1]])

Hadley


From vincent.goulet at act.ulaval.ca  Tue Sep  5 06:24:00 2006
From: vincent.goulet at act.ulaval.ca (Vincent Goulet)
Date: Tue, 05 Sep 2006 00:24:00 -0400
Subject: [R] Sweave and the "[" function
In-Reply-To: <f8e6ff050609042103m6755c62cq3093f692951907e@mail.gmail.com>
References: <44FCDCC5.9010903@uq.edu.au>
	<f8e6ff050609042103m6755c62cq3093f692951907e@mail.gmail.com>
Message-ID: <200609050024.00272.vincent.goulet@act.ulaval.ca>

Le Mardi 5 Septembre 2006 0:03, hadley wickham a ?crit?:
> > <<>>=
> > str(women)
> > women$height
> > women[,1]
> > "["(women,1)
> > @
> >
> > to show the equivalence of  three methods of extracting an element from
> > a data.frame.
> >
> > However Sweave returns the last of these as
> >
> > women[1]
> >
> > in the S input chunk
> >
> > How can I force it not to do this and return "["(women,1)
>
> I don't think you can.  Sweave parses your R code and from then on
> uses the internal R representation.  R normalises the parse tree in
> certain ways (eg. strips comments, formats source code, and clearly
> normalises some function calls).  Since sweave uses this, and not the
> original text, I don't think there is anyway to get around this,
> unless there is some trick during parsing.
>
> (And don't forget women[[1]])
>
> Hadley

So here's a workaround (untested):

<<echo=TRUE, eval=TRUE>>=
str(women)
women$height
women[,1]
@
<<echo=TRUE, eval=FALSE>>=
"["(women,1)
@
<<echo=FALSE, eval=TRUE>>=
"["(women,1)
@

I often end up doing similar things.

HTH    Vincent

-- 
  Vincent Goulet, Professeur agr?g?
  ?cole d'actuariat
  Universit? Laval, Qu?bec 
  Vincent.Goulet at act.ulaval.ca   http://vgoulet.act.ulaval.ca


From rdbisch at gmail.com  Tue Sep  5 06:27:03 2006
From: rdbisch at gmail.com (Rick Bischoff)
Date: Tue, 5 Sep 2006 00:27:03 -0400
Subject: [R] colorRamp
Message-ID: <DAEC02A4-B7E1-41D3-9239-C43237A09459@gmail.com>

Hi,

I am using colorRamp in the following way.  I am *sure* there is a  
better way to do this, so if you'd be so kind to show me the true R way:

Step 0: Create a new variable, say, "x", that maps some other  
continuous variable I have onto the [0,1] line.
Step 1:  Store the result from colorRamp (a function), into, say, "test"

 > test <- colorRamp(mypalette)

Step 2:  In my data frame, "data"

 > data$colorTemp <- test(data$x)

Step 3: Write a new function

bob <- function(temp) { rgb(temp[1],temp[2],temp[3],maxColorValue=255) }

Step 4:

 > for (i in 1:dim(data)[1]) data[i,"color"] <- bob(data[i,  
"colorTemp"])

Step 5:

map("states", region=data$region, fill=T, col=data$color)

Thanks in advance!
Rick


From wangtong at usc.edu  Tue Sep  5 07:35:01 2006
From: wangtong at usc.edu (Tong Wang)
Date: Mon, 04 Sep 2006 22:35:01 -0700
Subject: [R] Quick question about lm()
Message-ID: <dd788bc85ca3.44fcaa15@usc.edu>

Hi, 
     Feel awkward to ask , but really couldn't find a answer anywhere,   How could I extract the R^2 and t-stat. from the 
result of lm()?
     Thanks a lot. 

best


From christos at nuverabio.com  Tue Sep  5 07:52:29 2006
From: christos at nuverabio.com (Christos Hatzis)
Date: Tue, 5 Sep 2006 01:52:29 -0400
Subject: [R] Quick question about lm()
In-Reply-To: <dd788bc85ca3.44fcaa15@usc.edu>
Message-ID: <000b01c6d0af$79aef080$0202a8c0@headquarters.silicoinsights>

Say,

my.lm <- lm(y ~ x, data=my.data)

Then if you try:

names(summary(my.lm)) 

you will see the components of the summary.lm object.  The coefficients and
t-statistics can be extracted by

summary(my.lm)$coefficients

and similarly for the r-squared and other statistics provided in the summary
report.

-Christos
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Tong Wang
Sent: Tuesday, September 05, 2006 1:35 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Quick question about lm()

Hi, 
     Feel awkward to ask , but really couldn't find a answer anywhere,   How
could I extract the R^2 and t-stat. from the 
result of lm()?
     Thanks a lot. 

best

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From david.schwarz at goodstats.biz  Mon Sep  4 18:52:22 2006
From: david.schwarz at goodstats.biz (David S. Schwarz)
Date: Mon, 4 Sep 2006 12:52:22 -0400
Subject: [R] Residual analysis in GEE
Message-ID: <NEEOKOCNEOMBGMLFMBPBEEDJCMAA.david.schwarz@goodstats.biz>

Does anyone know how to get residuals out of the Generalized Estimating
Equations (GEE) module. I can get the fitted values but not the residuals?

Also, I would like some more extended model diagnostics such as the
studentized residuals, studentized deleted residuals,DFFIT, BFBeta, etc.

Any help would be greatly appreciated.

Dave Schwarz


From aleszib at gmail.com  Tue Sep  5 08:42:27 2006
From: aleszib at gmail.com (Ales Ziberna)
Date: Tue, 05 Sep 2006 08:42:27 +0200
Subject: [R] plot a new picture against an old one to see the difference
 between them
In-Reply-To: <004001c6d07f$e54fe400$0200a8c0@LeonE1405>
References: <003201c6d07b$bd896210$0200a8c0@LeonE1405>	<52086EF2-CE2D-4207-A39C-2FDF220A0200@ysidro.econ.uiuc.edu>
	<004001c6d07f$e54fe400$0200a8c0@LeonE1405>
Message-ID: <44FD1C53.8020302@gmail.com>

Maybe
par(mfrow=c(1,2))

would do the trick. It puts two plots (of any kind) to the same device 
next to each-other. You have to run the command before the plotting 
commands. To have again only one plot per device, use
par(mfrow=c(1,1))

Best,
Ales Ziberna

Am Stat pravi:
> Dear Roger,
> 
> Thanks, that's really helpful,  do you know how to deal with it if the two 
> plots are  generated by plot(), not by contour().
> 
> Best,
> 
> Leon
> 
> 
> ----- Original Message ----- 
> From: "roger koenker" <roger at ysidro.econ.uiuc.edu>
> To: "Am Stat" <amstat2006 at gmail.com>
> Sent: Monday, September 04, 2006 8:06 PM
> Subject: Re: [R] plot a new picture against an old one to see the difference 
> between them
> 
> 
>> for the second call to contour  use the argument add=TRUE.
>>
>> On Sep 4, 2006, at 6:42 PM, Am Stat wrote:
>>
>>> Hello, useR:,
>>>
>>> Suppose I have two plots made by using contour() function, say  Cont1 and 
>>> Cont2 respectively.
>>>
>>> They have slightly difference because of the two slightly different  data 
>>> I used.
>>>
>>> I want to see the difference between them so I want to plot Cont2  on 
>>> Cont1, are there any methods to plot it without filling the  frame of 
>>> Cont1 totally of Cont2.
>>> I mean, how I can integreate the two plots together that they kind  of 
>>> have weighted colors?
>>>
>>> Thanks very much in Advance!
>>>
>>> Leon
>>> [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting- 
>>> guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From info at lancashireclubbers.co.uk  Tue Sep  5 08:51:48 2006
From: info at lancashireclubbers.co.uk (Clubbing-UnknownLocations)
Date: Tue, 05 Sep 2006 07:51:48 +0100
Subject: 2Kinky @ The View, Frodsham - Vengabus 15.00 ticket travel package
Message-ID: <20060905075051.79328346@lancashireclubbers.co.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060905/14fe1a23/attachment.pl 

From renaud.lancelot at gmail.com  Tue Sep  5 10:53:34 2006
From: renaud.lancelot at gmail.com (Renaud Lancelot)
Date: Tue, 5 Sep 2006 10:53:34 +0200
Subject: [R] Residual analysis in GEE
In-Reply-To: <NEEOKOCNEOMBGMLFMBPBEEDJCMAA.david.schwarz@goodstats.biz>
References: <NEEOKOCNEOMBGMLFMBPBEEDJCMAA.david.schwarz@goodstats.biz>
Message-ID: <c2ee56800609050153y409ff7a6u567dd62f796be2f4@mail.gmail.com>

See the function geeglm in package geese. It has response, Pearson an
working residuals.

Best,

Renaud

2006/9/4, David S. Schwarz <david.schwarz at goodstats.biz>:
> Does anyone know how to get residuals out of the Generalized Estimating
> Equations (GEE) module. I can get the fitted values but not the residuals?
>
> Also, I would like some more extended model diagnostics such as the
> studentized residuals, studentized deleted residuals,DFFIT, BFBeta, etc.
>
> Any help would be greatly appreciated.
>
> Dave Schwarz
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Renaud LANCELOT
D?partement Elevage et M?decine V?t?rinaire (EMVT) du CIRAD
Directeur adjoint charg? des affaires scientifiques

CIRAD, Animal Production and Veterinary Medicine Department
Deputy director for scientific affairs

Campus international de Baillarguet
TA 30 / B (B?t. B, Bur. 214)
34398 Montpellier Cedex 5 - France
T?l   +33 (0)4 67 59 37 17
Secr. +33 (0)4 67 59 39 04
Fax   +33 (0)4 67 59 37 95


From petr.pikal at precheza.cz  Tue Sep  5 10:59:31 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 05 Sep 2006 10:59:31 +0200
Subject: [R] abline and plot(augPred) help
In-Reply-To: <971536df0609041337y2eba6e45uea85588f5977f8db@mail.gmail.com>
References: <44FC8A36.9010208@stat.auckland.ac.nz>
Message-ID: <44FD5893.21471.403263@localhost>

I would like to thank to Gabor Grothendieck, Paul Murrel and Maria 
Gabriela Cendoya for their helpful answers.

Based on Gabors code here is a solution for adding lines to lattice 
plots which works smoothly on augPred plots.

addLine<- function(a, b=NULL, v = NULL, h = NULL, ...) {
tcL <- trellis.currentLayout()
for(i in 1:nrow(tcL))
  for(j in 1:ncol(tcL))
    if (tcL[i,j] > 0) {
        trellis.focus("panel", j, i, highlight = FALSE)
        panel.abline(a=a, b=b, v=v, h=h, ...)
        trellis.unfocus()
        }
}

Best regards.
Petr Pikal


On 4 Sep 2006 at 16:37, Gabor Grothendieck wrote:

Date sent:      	Mon, 4 Sep 2006 16:37:00 -0400
From:           	"Gabor Grothendieck" <ggrothendieck at gmail.com>
To:             	"Paul Murrell" <p.murrell at auckland.ac.nz>
Copies to:      	Petr Pikal <petr.pikal at precheza.cz>, r-help at stat.math.ethz.ch
Subject:        	Re: [R] abline and plot(augPred) help

> On 9/4/06, Paul Murrell <p.murrell at auckland.ac.nz> wrote:
> > Hi
> >
> >
> > Petr Pikal wrote:
> > > Dear all
> > >
> > > as I did not get any response on my post about abline and
> > > plot(augPred)) I try again. I hope I do not break some posting
> > > guide rules. I would try to contact package maintainer directly
> > > but there is stated to be R-core people, so I feel R-help list
> > > shall be OK.
> > >
> > > I need to draw straight lines through augPred plotted panels
> > > (vertical or horizontal) at specified point. I know I shall
> > > probably use panel.abline but I am missing correct syntax. Below
> > > you can see my attempts together with results. I hope somebody can
> > > point me to right direction.
> > >
> > > I am probably somewhere close but I have no clue, which parameter
> > > I shall modify to get measured points, fitted lines and vertical
> > > lines in panels together.
> >
> >
> > The problem is that you do not know about the default panel function
> > that nlme:::plot.augPred() uses, so your panel functions are not
> > replicating all of the default behaviour as well as adding your
> > vertical lines.  Some possible solutons suggested below ...
> >
> >
> > > fm1 <- lme(Orthodont)
> > >
> > > # standard plot
> > > plot(augPred(fm1, level = 0:1, length.out = 2))
> > >
> > > #plot with vertical but without points and fitted lines
> > > plot(augPred(fm1, level = 0:1, length.out = 2),
> > > panel=function(v,...) {
> > > panel.abline(v=10)}
> > > )
> > >
> > > # plot with vertical but without fitted lines
> > > plot(augPred(fm1, level = 0:1, length.out=2),
> > > panel=function(x,y,...) {
> > > panel.xyplot(x,y,...)
> > > panel.abline(v=10)}
> > > )
> > >
> > > # plot with vertical and with all points (fitted lines are drawn
> > > # as
> > > points)
> > > plot(augPred(fm1, level = 0:1),
> > > panel=function(x,y,...) {
> > > panel.xyplot(x,y,...)
> > > panel.abline(v=10)}
> > > )
> >
> > One option is to take a sneak a peek at nlme:::plot.augPred() to see
> > what the default panel function is doing.  Here I have replicated
> > the default panel function and added a call to panel.abline().
> >
> > plot(augPred(fm1, level = 0:1, length.out = 2),
> >  panel=function(x, y, subscripts, groups, ...) {
> >                orig <- groups[subscripts] == "original"
> >                panel.xyplot(x[orig], y[orig], ...)
> >                panel.superpose(x[!orig], y[!orig],
> >                subscripts[!orig],
> >                                groups, ..., type = "l")
> >                panel.abline(v=10)
> >  })
> >
> > The problem with this approach is that you need to crawl around in
> > the code of nlme:::plot.augPred().  An alternative approach is to
> > annotate the plot after-the-fact.  This is shown below.
> >
> > plot(augPred(fm1, level = 0:1, length.out = 2))
> > for (i in 1:5) {
> >  for (j in 1:6) {
> >    if (i < 5 || j < 4) {
> >      trellis.focus("panel", j, i, highlight=FALSE)
> >      panel.abline(v=10)
> >    }
> >  }
> > }
> >
> > This avoids crawling around in code, but the problem with this is
> > knowing how many rows and columns of panels there are.  If you
> > explicitly controlled the 'layout' of the original plot, you could
> > guarantee that your annotation works properly.
> >
> 
> You can find that out with trellis.currentLayout:
> 
> tcL <- trellis.currentLayout()
> for(i in 1:nrow(tcL))
>   for(j in 1:ncol(tcL))
>     if (tcL[i,j] > 0) {
>         trellis.focus("panel", j, i, highlight = FALSE)
>         panel.abline(v = 10)
>         trellis.unfocus()
>     }
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From saeedeh_m_d at yahoo.co.uk  Tue Sep  5 11:01:57 2006
From: saeedeh_m_d at yahoo.co.uk (saeedeh maleki)
Date: Tue, 5 Sep 2006 10:01:57 +0100 (BST)
Subject: [R] Reserve and biobase
Message-ID: <20060905090158.77583.qmail@web25508.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060905/7e99e28b/attachment.pl 

From petr.pikal at precheza.cz  Tue Sep  5 11:02:34 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 05 Sep 2006 11:02:34 +0200
Subject: [R] plot a new picture against an old one to see the difference
	between them
In-Reply-To: <44FD1C53.8020302@gmail.com>
References: <004001c6d07f$e54fe400$0200a8c0@LeonE1405>
Message-ID: <44FD594A.24720.42FD2B@localhost>

Hi

On 5 Sep 2006 at 8:42, Ales Ziberna wrote:

Date sent:      	Tue, 05 Sep 2006 08:42:27 +0200
From:           	Ales Ziberna <aleszib at gmail.com>
To:             	Am Stat <amstat2006 at gmail.com>
Copies to:      	roger koenker <roger at ysidro.econ.uiuc.edu>,
	R-help at stat.math.ethz.ch
Subject:        	Re: [R] plot a new picture against an old one to see the difference
	between them

> Maybe
> par(mfrow=c(1,2))
> 
> would do the trick. It puts two plots (of any kind) to the same device
> next to each-other. You have to run the command before the plotting
> commands. To have again only one plot per device, use
> par(mfrow=c(1,1))

see also ?lines or ?points to put lines or points over previously 
plotted image.

HTH
Petr

> 
> Best,
> Ales Ziberna
> 
> Am Stat pravi:
> > Dear Roger,
> > 
> > Thanks, that's really helpful,  do you know how to deal with it if
> > the two plots are  generated by plot(), not by contour().
> > 
> > Best,
> > 
> > Leon
> > 
> > 
> > ----- Original Message ----- 
> > From: "roger koenker" <roger at ysidro.econ.uiuc.edu>
> > To: "Am Stat" <amstat2006 at gmail.com>
> > Sent: Monday, September 04, 2006 8:06 PM
> > Subject: Re: [R] plot a new picture against an old one to see the
> > difference between them
> > 
> > 
> >> for the second call to contour  use the argument add=TRUE.
> >>
> >> On Sep 4, 2006, at 6:42 PM, Am Stat wrote:
> >>
> >>> Hello, useR:,
> >>>
> >>> Suppose I have two plots made by using contour() function, say 
> >>> Cont1 and Cont2 respectively.
> >>>
> >>> They have slightly difference because of the two slightly
> >>> different  data I used.
> >>>
> >>> I want to see the difference between them so I want to plot Cont2 
> >>> on Cont1, are there any methods to plot it without filling the 
> >>> frame of Cont1 totally of Cont2. I mean, how I can integreate the
> >>> two plots together that they kind  of have weighted colors?
> >>>
> >>> Thanks very much in Advance!
> >>>
> >>> Leon
> >>> [[alternative HTML version deleted]]
> >>>
> >>> ______________________________________________
> >>> R-help at stat.math.ethz.ch mailing list
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide http://www.R-project.org/posting-
> >>> guide.html and provide commented, minimal, self-contained,
> >>> reproducible code.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented,
> > minimal, self-contained, reproducible code.
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From s.wood at bath.ac.uk  Tue Sep  5 11:57:07 2006
From: s.wood at bath.ac.uk (Simon Wood)
Date: Tue, 5 Sep 2006 10:57:07 +0100
Subject: [R] Fitting generalized additive models with constraints?
In-Reply-To: <fd913b0d0609041539o545e7e0bx8bbe4ac9f369976e@mail.gmail.com>
References: <fd913b0d0609041539o545e7e0bx8bbe4ac9f369976e@mail.gmail.com>
Message-ID: <200609051057.07970.s.wood@bath.ac.uk>


> I am trying to fit a GAM for a simple model, a simple model, y ~ s(x0) +
> s(x1) ; with a constraint that the fitted smooth functions s(x0) and s(x1)
> have to each always be >0.
>
> >From the library documentation and a search of the R-site and R-help
>
> archives I have not been able to decipher whether the following is possible
> using this, or other GAM libraries, or whether I will have to try to "roll
> my own". I see from the mgcv docs that GAMs need to be constrained such
> that the smooth functions have zero mean. Is there a way around this?
>
> Is such a constraint possible?
It is possible to estimate a GAM subject to this constraint, but be aware that 
the mean levels of your component smooths are not identifiable, so there is 
an unavoidable abitrariness in the estimate....

You have to have some sort of constraint on the smooths in a GAM to ensure 
identifiability, and a convenient way to set the model up is to write it as 
e.g.

E(y) = a + f0(x0) + f1(x1)

where `a' is the intercept and f0 and f1 are smooth functions which sum to 
zero over their respective covariate values. In this parameterization your 
constraint implies that 

a + f0(x0) + f1(x1)  > 0

for all x0, x1. If this constraint is met then you can find constants b and c 
such that b+c=a such that f0(x0)+b>0 and f1(x1)+c>0 for all x0,x1. i.e. you 
redefine f0 as f0+b and f1 as f1+c, and you have a fitted model meeting the 
constraints.

To fit the GAM subject to the constraints you can use mgcv:::pcls... ?pcls has 
some examples, but it does involve moderately low level programming. It's 
hard to impose the constraint exactly, so the usual approach would be to 
impose the constraint over a fairly fine grid of x0, x1 values. Also, you'll 
need to figure out how to select smoothing parameters. For many problems it 
suffices to estimate smoothing parameters on the unconstrained fit, and then 
use these to fit subject to constraints, but it depends on the problem.... 

Hope that's some use.

Simon

> thanks very much for any advice or pointers.
> -David
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented, minimal,
> self-contained, reproducible code.

-- 
> Simon Wood, Mathematical Sciences, University of Bath, Bath, BA2 7AY UK
> +44 1225 386603  www.maths.bath.ac.uk/~sw283


From wangtong at usc.edu  Tue Sep  5 12:02:43 2006
From: wangtong at usc.edu (Tong Wang)
Date: Tue, 05 Sep 2006 03:02:43 -0700
Subject: [R] A question about gc()
Message-ID: <dddff55b4e4a.44fce8d3@usc.edu>

Hi everyone, 
    I am doing some intensive computation:  50000 regressions of the form Y~X with each y of size (1,1000) ,
even if I break invoke gc() for a few time in the loop, it still breaks down at some point with the error message:

Error in .signalSimpleWarning("Reached total allocation of 1024Mb: see help(memory.size)",  : 
        recursive default argument reference
 
After getting this, even if I call gc() and resume the computation, it won't move at all.
May I get some suggestions what should I do to get around this problem ?
Thanks a lot. 

tong


From saeedeh_m_d at yahoo.co.uk  Tue Sep  5 12:19:21 2006
From: saeedeh_m_d at yahoo.co.uk (saeedeh maleki)
Date: Tue, 5 Sep 2006 11:19:21 +0100 (BST)
Subject: [R] Rserve and bibase
Message-ID: <20060905101921.26585.qmail@web25512.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060905/1194c8c3/attachment.pl 

From ripley at stats.ox.ac.uk  Tue Sep  5 12:42:14 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 5 Sep 2006 11:42:14 +0100 (BST)
Subject: [R] A question about gc()
In-Reply-To: <dddff55b4e4a.44fce8d3@usc.edu>
References: <dddff55b4e4a.44fce8d3@usc.edu>
Message-ID: <Pine.LNX.4.64.0609051136360.818@gannet.stats.ox.ac.uk>

On Tue, 5 Sep 2006, Tong Wang wrote:

> Hi everyone, 

>     I am doing some intensive computation:  50000 regressions of the 
> form Y~X with each y of size (1,1000) , even if I break invoke gc() for 
> a few time in the loop, it still breaks down at some point with the 
> error message:

gc() does not reclaim memory for you beyond what R has already done.

> Error in .signalSimpleWarning("Reached total allocation of 1024Mb: see help(memory.size)",  : 
>         recursive default argument reference

>  After getting this, even if I call gc() and resume the computation, it 
> won't move at all. May I get some suggestions what should I do to get 
> around this problem ?

Consult the rw-FAQ (since you seem to be using Windows without telling us) 
and the help page the message mentions.

It looks as if you are trying to store too many objects.  If you have lots 
of RAM you can increase that limit (see the previous para), but you are 
getting uncomfortably close to the address space limit of your OS.

Perhaps you can only save the part of the fit you need, or save() the 
objects to separate files and rm() them, and postprocess them in a later 
session?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From r.darnell at uq.edu.au  Tue Sep  5 12:56:33 2006
From: r.darnell at uq.edu.au (Ross Darnell)
Date: Tue, 05 Sep 2006 20:56:33 +1000
Subject: [R] Sweave and the "[" function
In-Reply-To: <200609050024.00272.vincent.goulet@act.ulaval.ca>
References: <44FCDCC5.9010903@uq.edu.au>
	<f8e6ff050609042103m6755c62cq3093f692951907e@mail.gmail.com>
	<200609050024.00272.vincent.goulet@act.ulaval.ca>
Message-ID: <44FD57E1.802@uq.edu.au>

Hi Vincent

This would seem logical but in this case doesn't work.
It doesn't seem to be a Sweave problem (feature) at all  but within R as 
Hadley stated.
Within R try

 > quote(women[1])
women[1]

now try

 > quote("["(women,1))
women[1]

So it's parsed and "normalised" (there's a familiar term); in this case 
to women[1] before its quoted.

Curiously in the "R Language Definition Guide" in 10.4.3  it states

R has three indexing constructs, two of which are syntactically similar 
although with somewhat
different semantics:
object [ arg1, ...... , argn ]
object [[ arg1, ...... , argn ]]
The object can formally be any valid expression, but it is understood to 
denote or evaluate
to a subsettable object. The arguments generally evaluate to numerical 
or character indices,
but other kinds of arguments are possible (notably drop = FALSE).
Internally, these index constructs are stored as function calls with 
function name "[" respectively
"[[".

So I'm lost now. Can some one hand me a  map and compass?
 




Vincent Goulet wrote:
> Le Mardi 5 Septembre 2006 0:03, hadley wickham a ?crit :
>   
>>> <<>>=
>>> str(women)
>>> women$height
>>> women[,1]
>>> "["(women,1)
>>> @
>>>
>>> to show the equivalence of  three methods of extracting an element from
>>> a data.frame.
>>>
>>> However Sweave returns the last of these as
>>>
>>> women[1]
>>>
>>> in the S input chunk
>>>
>>> How can I force it not to do this and return "["(women,1)
>>>       
>> I don't think you can.  Sweave parses your R code and from then on
>> uses the internal R representation.  R normalises the parse tree in
>> certain ways (eg. strips comments, formats source code, and clearly
>> normalises some function calls).  Since sweave uses this, and not the
>> original text, I don't think there is anyway to get around this,
>> unless there is some trick during parsing.
>>
>> (And don't forget women[[1]])
>>
>> Hadley
>>     
>
> So here's a workaround (untested):
>
> <<echo=TRUE, eval=TRUE>>=
> str(women)
> women$height
> women[,1]
> @
> <<echo=TRUE, eval=FALSE>>=
> "["(women,1)
> @
> <<echo=FALSE, eval=TRUE>>=
> "["(women,1)
> @
>
> I often end up doing similar things.
>
> HTH    Vincent
>
>


From berwin at maths.uwa.edu.au  Tue Sep  5 13:22:18 2006
From: berwin at maths.uwa.edu.au (Berwin A Turlach)
Date: Tue, 5 Sep 2006 19:22:18 +0800
Subject: [R] Sweave and the "[" function
In-Reply-To: <44FD57E1.802@uq.edu.au>
References: <44FCDCC5.9010903@uq.edu.au>
	<f8e6ff050609042103m6755c62cq3093f692951907e@mail.gmail.com>
	<200609050024.00272.vincent.goulet@act.ulaval.ca>
	<44FD57E1.802@uq.edu.au>
Message-ID: <17661.24042.524273.443676@bossiaea.maths.uwa.edu.au>

G'day all,

>>>>> "RD" == Ross Darnell <r.darnell at uq.edu.au> writes:

    RD> Hi Vincent This would seem logical but in this case doesn't
    RD> work.
Well, he said that it was untested. :)

Same idea, but with a slightly different implementation (and this one
works, I have tested it):

<<>>=
str(women)
women$height
women[,1]
@
\begin{Sinput}
> "["(women,1)
\end{Sinput}
<<echo=FALSE, eval=TRUE>>=
"["(women,1)
@

HTH

Cheers,

        Berwin

========================== Full address ============================
Berwin A Turlach                      Tel.: +61 (8) 6488 3338 (secr)   
School of Mathematics and Statistics        +61 (8) 6488 3383 (self)      
The University of Western Australia   FAX : +61 (8) 6488 1028
35 Stirling Highway                   
Crawley WA 6009                e-mail: berwin at maths.uwa.edu.au
Australia                        http://www.maths.uwa.edu.au/~berwin


From Friedrich.Leisch at stat.uni-muenchen.de  Tue Sep  5 13:27:46 2006
From: Friedrich.Leisch at stat.uni-muenchen.de (Friedrich Leisch)
Date: Tue, 5 Sep 2006 13:27:46 +0200
Subject: [R] Sweave and the "[" function
In-Reply-To: <44FD57E1.802@uq.edu.au>
References: <44FCDCC5.9010903@uq.edu.au>
	<f8e6ff050609042103m6755c62cq3093f692951907e@mail.gmail.com>
	<200609050024.00272.vincent.goulet@act.ulaval.ca>
	<44FD57E1.802@uq.edu.au>
Message-ID: <17661.24370.849415.908176@celebrian.ci.tuwien.ac.at>

>>>>> On Tue, 05 Sep 2006 20:56:33 +1000,
>>>>> Ross Darnell (RD) wrote:

  > Hi Vincent
  > This would seem logical but in this case doesn't work.
  > It doesn't seem to be a Sweave problem (feature) at all  but within R as 
  > Hadley stated.

Yes, Sweave parses & deparses the code in oreder to make full
expressions out of lines of text, i.e., to "know" where open
parentheses are closed etc. This is necessary in order to know where
to insert output when multiple lines are contained in one code chunk,
but unfortunately looses the original formatting as has correctly been
explained earlier in this thread.

Best,
Fritz

-- 
-----------------------------------------------------------------------
Prof. Dr. Friedrich Leisch 

Institut f?r Statistik                          Tel: (+49 89) 2180 3165
Ludwig-Maximilians-Universit?t                  Fax: (+49 89) 2180 5308
Ludwigstra?e 33
D-80539 M?nchen                 http://www.stat.uni-muenchen.de/~leisch


From p.dalgaard at biostat.ku.dk  Tue Sep  5 13:46:06 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 05 Sep 2006 13:46:06 +0200
Subject: [R] Sweave and the "[" function
In-Reply-To: <44FD57E1.802@uq.edu.au>
References: <44FCDCC5.9010903@uq.edu.au>
	<f8e6ff050609042103m6755c62cq3093f692951907e@mail.gmail.com>
	<200609050024.00272.vincent.goulet@act.ulaval.ca>
	<44FD57E1.802@uq.edu.au>
Message-ID: <x2veo2fse9.fsf@viggo.kubism.ku.dk>

Ross Darnell <r.darnell at uq.edu.au> writes:

> Hi Vincent
> 
> This would seem logical but in this case doesn't work.
> It doesn't seem to be a Sweave problem (feature) at all  but within R as 
> Hadley stated.
> Within R try
> 
>  > quote(women[1])
> women[1]
> 
> now try
> 
>  > quote("["(women,1))
> women[1]
> 
> So it's parsed and "normalised" (there's a familiar term); in this case 
> to women[1] before its quoted.
> 
> Curiously in the "R Language Definition Guide" in 10.4.3  it states
> 
> R has three indexing constructs, two of which are syntactically similar 
> although with somewhat
> different semantics:
> object [ arg1, ...... , argn ]
> object [[ arg1, ...... , argn ]]
> The object can formally be any valid expression, but it is understood to 
> denote or evaluate
> to a subsettable object. The arguments generally evaluate to numerical 
> or character indices,
> but other kinds of arguments are possible (notably drop = FALSE).
> Internally, these index constructs are stored as function calls with 
> function name "[" respectively
> "[[".

Clear enough to me, but then again, I probably wrote it... 5 years ago
or so.

> So I'm lost now. Can some one hand me a  map and compass?

Er, maybe, if you tell us which woods you are lost in so that we can
find you...

What may be confusing you is that it is precisely the other way
around: women[1] is parsed to "["(women,1) which deparses to women[1].

This may be illuminating:

for(i in 1:3) print(quote(foo(women,1))[[i]])
for(i in 1:3) print(quote("["(women,1))[[i]])
for(i in 1:3) print(quote(women[i])[[i]])

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From info at lancashireclubbers.co.uk  Tue Sep  5 14:45:19 2006
From: info at lancashireclubbers.co.uk (Clubbing-UnknownLocations)
Date: Tue, 05 Sep 2006 13:45:19 +0100
Subject: [R] Clubbing-UnknownLocations Mailing List Confirmation
Message-ID: <20060905134518.09905421@lancashireclubbers.co.uk>


This message has been sent to you as the final step to confirm your
email *removal* for the following list: 

Clubbing-UnknownLocations

To confirm this unsubscription, please follow the URL below:

<http://www.lancashireclubbers.co.uk/cgi-bin/dada/mail.cgi/u/loc_unknown/r-help/stat.math.ethz.ch/1835582/>

(Click the URL above, or copy and paste the URL into your browser. 
Doing so will remove you to this list.)

-----------------------------------------------------------------------

The following is the description given for this list: 

Lancashire based list, for informing the community of the latest clubbing events in and around Burnley, along with special promotions and news.

These members have not been location verified.

-----------------------------------------------------------------------

This double opt-out confirmation email was sent to protect the privacy
of the owner of this email address. 

Furthermore, the following privacy policy is associated with this list: 

Your email address will be used by Lancashireclubbers to inform you about Lancashire based Clubbing Events, Promotions and News related to clubbing. Your email address will not be sold on, and will in no way be used to advertise non-clubbing / music related information. 



Please read and understand this privacy policy. 

If you did not ask to be removed from this particular list, please
do not visit the confirmation URL above. The confirmation for removal 
will not go through and no other action on your part will be needed.

To contact the owner of this email list, please use the address below: 

<mailto:info at lancashireclubbers.co.uk>

The following physical address is associated with this mailing list: 

168 Sycamore Avenue, Burnley


- <mailto:info at lancashireclubbers.co.uk>


From info at lancashireclubbers.co.uk  Tue Sep  5 14:52:48 2006
From: info at lancashireclubbers.co.uk (Clubbing-UnknownLocations)
Date: Tue, 05 Sep 2006 13:52:48 +0100
Subject: [R] Clubbing-UnknownLocations Unsubscription
Message-ID: <20060905135248.46554226@lancashireclubbers.co.uk>


The removal of the email address:

	r-help at stat.math.ethz.ch
	
from the mailing list: 

	Clubbing-UnknownLocations 

is complete.

You may wish to save this email message for future reference.

-----------------------------------------------------------------------

Date of this removal: Tue Sep  5 13:52:48 2006

You may automatically re-subscribe to this list at any time by 
visiting the following URL:

<http://www.lancashireclubbers.co.uk/cgi-bin/dada/mail.cgi/s/loc_unknown/r-help/stat.math.ethz.ch/>

If the above URL is inoperable, make sure that you have copied the 
entire address. Some mail readers will wrap a long URL and thus break
this automatic unsubscribe mechanism. 

You may also change your subscription by visiting this list's main screen: 

<http://www.lancashireclubbers.co.uk/cgi-bin/dada/mail.cgi/list/loc_unknown>

If you're still having trouble, please contact the list owner at: 

	<mailto:info at lancashireclubbers.co.uk>

The following physical address is associated with this mailing list: 

168 Sycamore Avenue, Burnley

- <mailto:info at lancashireclubbers.co.uk>


From ligges at statistik.uni-dortmund.de  Tue Sep  5 15:03:24 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 05 Sep 2006 15:03:24 +0200
Subject: [R] RGui problem in Windows XP with demo() and help()
In-Reply-To: <44FB6EF9.3090806@math.umass.edu>
References: <44FB6EF9.3090806@math.umass.edu>
Message-ID: <44FD759C.1070105@statistik.uni-dortmund.de>



Murray Eisenberg wrote:
> I just installed R-2.3.1pat under Windows XP as well as the associated
> RWinEdt.  If I start RGui from its shortcut (but do _not_ also start
> RWinEdt) and then try to execute demo() or help(), I get a RWinEdt
> pop-up error window with message:
> 
>    File "D:\WP\WinEdtData\WinEdt\D:/WP/WinEdtData/WinEdt/R.ini" does not
> exist!
>    Qualifier -e/-E does not specify an existing file!
> 
> First question: what is causing RWinEdt even to get involved here?
> 
> Second: where is the garbled path to R.ini coming from.
> 
> Here's the configuration:  Everything R is in D:\Stats\R.  R-2.3.1pat
> has the actual R release installed.
> 
> The shortcut for R-2.3.1-pat has target
> D:\Stats\R\R-2.3.1pat\bin\Rgui.exe and starts in
> D:\Stats\R\R-2.3.1pat.
> 
> File .Renviron is in D:\Stats\R.  R-2.3.1pat; the only lines in it that
> are not commented out are:
> 
>   R_USER=e:/Documents/R
>   R_LIBS=d:/Stats/R/myRlib
> 
> File .Rprofile is in e:/Documents/R (the reference of myR_USER); the
> only lines in it not commented out are:
> 
>    options(editor="\"d:/WP/winedt/winedt\" -c=\"R-WinEdt\"
>         -E=\"D:/WP/WinEdtData/WinEdt/R.ini\" -V")      [on 1 line]
>    options(pager="\"d:/WP/winedt/winedt\" -C=\"R-WinEdt\"
>         -e=\"D:/WP/WinEdtData/WinEdt/R.ini\" -V")      [on 1 line]
>    .First <- function(x) print("Profile read")


Just delete those options, they are not required in order to have 
RWinEdt working with R (I'm not using them).
The usual way is to start RWinEdt separately in order to edit .R files 
and source() or paste parts of them to R.

Uwe Ligges


> Is the root of the problem that I have a single .Rprofile set up for
> RWinEdt but need to have a different one for RGui without RWinEdt?
> And, if so, how do I tell R which one to use at startup?


From h.wickham at gmail.com  Tue Sep  5 15:18:37 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Tue, 5 Sep 2006 08:18:37 -0500
Subject: [R] colorRamp
In-Reply-To: <DAEC02A4-B7E1-41D3-9239-C43237A09459@gmail.com>
References: <DAEC02A4-B7E1-41D3-9239-C43237A09459@gmail.com>
Message-ID: <f8e6ff050609050618w25cac1d2t10afd113dc877002@mail.gmail.com>

> I am using colorRamp in the following way.  I am *sure* there is a
> better way to do this, so if you'd be so kind to show me the true R way:
>

Another possibility is to use map_colour_gradient from ggplot, which
takes care of most of that for you (although it doesn't use colorRamp,
that could be easily fixed)

Hadley


From therneau at mayo.edu  Tue Sep  5 15:24:54 2006
From: therneau at mayo.edu (Terry Therneau)
Date: Tue, 5 Sep 2006 08:24:54 -0500 (CDT)
Subject: [R] terms.inner
Message-ID: <200609051324.k85DOsq19773@rocky.mayo.edu>

Question:
  I am trying to impliment a function in R that we use quite regularly in
Splus, and it fails due to a lack of the "terms.inner" function in R.
The substitute is?

Part question and part soapbox:
Why remove terms.inner from R?  It's little used, but rather innocuous.

Mostly soapbox:
  I figured it was no big deal, as I originally discovered the use of
terms.inner from reading the plot.gam function.  So I'd just see what
plot.gam does in R.
 > plot.gam
 Error: object "plot.gam" not found

  Ok, I know this function has to exist.  I even remember that there is some
sort of multi-colon secret handshake that will convince R to let you look
at it, although I don't remember the form.  This Nixonesque passion with
hiding things is one of the reasons I still prefer Splus.

  
  	Terry Therneau
  	therneau.terry at mayo.edu


From stecalza at tiscali.it  Tue Sep  5 15:42:19 2006
From: stecalza at tiscali.it (Stefano Calza)
Date: Tue, 5 Sep 2006 15:42:19 +0200
Subject: [R] terms.inner
In-Reply-To: <200609051324.k85DOsq19773@rocky.mayo.edu>
References: <200609051324.k85DOsq19773@rocky.mayo.edu>
Message-ID: <20060905134219.GE4221@med.unibs.it>

On Tue, Sep 05, 2006 at 08:24:54AM -0500, Terry Therneau wrote:

...
...
<Terry> > plot.gam
<Terry> Error: object "plot.gam" not found

if I do

library(gam)

plot.gam

it prints the function

<Terry>
<Terry>  Ok, I know this function has to exist.  I even remember that there is some
<Terry>sort of multi-colon secret handshake that will convince R to let you look
<Terry>at it, although I don't remember the form.  This Nixonesque passion with
<Terry>hiding things is one of the reasons I still prefer Splus.


afaik getAnywhere(function name) usually works for the hidden ones...though I don't quite understand why one should hide them


HIH,
Stefano



<Terry>
<Terry>  
<Terry>  	Terry Therneau
<Terry>  	therneau.terry a mayo.edu
<Terry>
<Terry>______________________________________________
<Terry>R-help a stat.math.ethz.ch mailing list
<Terry>https://stat.ethz.ch/mailman/listinfo/r-help
<Terry>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
<Terry>and provide commented, minimal, self-contained, reproducible code.


From ggrothendieck at gmail.com  Tue Sep  5 15:49:03 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 5 Sep 2006 09:49:03 -0400
Subject: [R] terms.inner
In-Reply-To: <200609051324.k85DOsq19773@rocky.mayo.edu>
References: <200609051324.k85DOsq19773@rocky.mayo.edu>
Message-ID: <971536df0609050649h7d2c17c8oecc6ead71af3a33d@mail.gmail.com>

Check out:

http://tolstoy.newcastle.edu.au/R/help/01c/0340.html


On 9/5/06, Terry Therneau <therneau at mayo.edu> wrote:
> Question:
>  I am trying to impliment a function in R that we use quite regularly in
> Splus, and it fails due to a lack of the "terms.inner" function in R.
> The substitute is?
>
> Part question and part soapbox:
> Why remove terms.inner from R?  It's little used, but rather innocuous.
>
> Mostly soapbox:
>  I figured it was no big deal, as I originally discovered the use of
> terms.inner from reading the plot.gam function.  So I'd just see what
> plot.gam does in R.
>  > plot.gam
>  Error: object "plot.gam" not found
>
>  Ok, I know this function has to exist.  I even remember that there is some
> sort of multi-colon secret handshake that will convince R to let you look
> at it, although I don't remember the form.  This Nixonesque passion with
> hiding things is one of the reasons I still prefer Splus.
>
>
>        Terry Therneau
>        therneau.terry at mayo.edu
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From S.Pickett at exeter.ac.uk  Tue Sep  5 16:01:30 2006
From: S.Pickett at exeter.ac.uk (Simon Pickett)
Date: Tue, 5 Sep 2006 15:01:30 +0100 (BST)
Subject: [R] help: advice on the structuring of ReML models for analysing
 growth curves
Message-ID: <1173.144.173.76.117.1157464890.squirrel@www.webmail.ex.ac.uk>

Hi R experts,
I am interested on the effects of two dietry compunds on the growth of
chicks. Rather than extracting linear growth functions for each chick and
using these in an analysis I thought using ReML might provide a neater and
better way of doing this. (I have read the pdf vignette("MlmSoftRev") and
"Fitting linear mixed models in R" by Douglas Bates but I am not entirely
sure that I have the right solution).

Basically I fed chicks in nest boxes over a period of time and weighed
them each time I fed them. I presume that "chick id" should be a random
factor and should be nested within "nest box number"? (Chicks were not
moved around so this should make things more simple). Also since the
chicks were measured repeatedly over time I presume that this should be a
random factor? Growth is not linear exactly (more quadratic), so I thought
rather than put time in the fixed model I want to control for the effects
of time as a random factor....
The resulting model is this
where id=chick identity and brood=nest box
model1<-lmer(weight~treatment1*treatment2*brood
size*sex+(id|brood)+(1|brood)+(1|age), data=H)

Is this the "right" approach or am I barking up the wrong tree?
Any suggestions much appreciated,
Simon


Simon Pickett
PhD student
Centre For Ecology and Conservation
Tremough Campus
University of Exeter in Cornwall
TR109EZ
Tel 01326371852


From A.Robinson at ms.unimelb.edu.au  Tue Sep  5 16:25:22 2006
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Wed, 6 Sep 2006 00:25:22 +1000
Subject: [R] help: advice on the structuring of ReML models for
	analysing growth curves
In-Reply-To: <1173.144.173.76.117.1157464890.squirrel@www.webmail.ex.ac.uk>
References: <1173.144.173.76.117.1157464890.squirrel@www.webmail.ex.ac.uk>
Message-ID: <20060905142522.GF12212@ms.unimelb.edu.au>

Hi Simon,

overall I think that lmer is a good tool for this problem.  It's
impossible to reply definitively without the full details on the
experimental design.

Caveat in place, I have questions and some suggestions.  Are
treatment1 and treatment2 distinct factors, or two levels of a
treatment, the dietary compound?  Also, what is broodsize?

If you want to nest chick id within brood, I think that you should
include the interaction as a random factor.  If you'd like the age
effects to differ between chicks then age should be on the left of id.

Thus, start with something like ...

model1 <- lmer(weight ~ treatment +  broodsize + sex + age
       + (1|brood) + (age|id:brood), data=H) 

You might also like to consider a quadratic term in age, if you think
that growth is quadratic. 

Cheers

Andrew

ps spaces enhance legibility :)

On Tue, Sep 05, 2006 at 03:01:30PM +0100, Simon Pickett wrote:
> Hi R experts,
> I am interested on the effects of two dietry compunds on the growth of
> chicks. Rather than extracting linear growth functions for each chick and
> using these in an analysis I thought using ReML might provide a neater and
> better way of doing this. (I have read the pdf vignette("MlmSoftRev") and
> "Fitting linear mixed models in R" by Douglas Bates but I am not entirely
> sure that I have the right solution).
> 
> Basically I fed chicks in nest boxes over a period of time and weighed
> them each time I fed them. I presume that "chick id" should be a random
> factor and should be nested within "nest box number"? (Chicks were not
> moved around so this should make things more simple). Also since the
> chicks were measured repeatedly over time I presume that this should be a
> random factor? Growth is not linear exactly (more quadratic), so I thought
> rather than put time in the fixed model I want to control for the effects
> of time as a random factor....
> The resulting model is this
> where id=chick identity and brood=nest box
> model1<-lmer(weight~treatment1*treatment2*brood
> size*sex+(id|brood)+(1|brood)+(1|age), data=H)
> 
> Is this the "right" approach or am I barking up the wrong tree?
> Any suggestions much appreciated,
> Simon
> 
> 
> Simon Pickett
> PhD student
> Centre For Ecology and Conservation
> Tremough Campus
> University of Exeter in Cornwall
> TR109EZ
> Tel 01326371852
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
Email: a.robinson at ms.unimelb.edu.au         http://www.ms.unimelb.edu.au


From tlumley at u.washington.edu  Tue Sep  5 16:31:39 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 5 Sep 2006 07:31:39 -0700 (PDT)
Subject: [R] terms.inner
In-Reply-To: <200609051324.k85DOsq19773@rocky.mayo.edu>
References: <200609051324.k85DOsq19773@rocky.mayo.edu>
Message-ID: <Pine.LNX.4.64.0609050717070.14585@homer21.u.washington.edu>

On Tue, 5 Sep 2006, Terry Therneau wrote:

> Question:
>  I am trying to impliment a function in R that we use quite regularly in
> Splus, and it fails due to a lack of the "terms.inner" function in R.
> The substitute is?
>
> Part question and part soapbox:
> Why remove terms.inner from R?  It's little used, but rather innocuous.

AFAIK it wasn't removed, it just hasn't ever been implemented. Probably 
no-one in the early years of R tried to port any code that used it -- 
there were a number of functions added back then just because the survival 
package used them, such as subscripting on terms objects.

> Mostly soapbox:
>  I figured it was no big deal, as I originally discovered the use of
> terms.inner from reading the plot.gam function.  So I'd just see what
> plot.gam does in R.
> > plot.gam
> Error: object "plot.gam" not found
>
>  Ok, I know this function has to exist.  I even remember that there is some
> sort of multi-colon secret handshake that will convince R to let you look
> at it, although I don't remember the form.  This Nixonesque passion with
> hiding things is one of the reasons I still prefer Splus.

Perhaps not the best soapbox example. There are at least two gam 
implementations, but they are both in add-on packages. Neither of them 
hides its plot.gam in a namespace, but you do need to install and load the 
package.  You presumably want Trevor Hastie's one, which is in the "gam" 
package, rather than Simon Woods' one in the "mgcv" package.

BTW, termplot() does many of the same things as Hastie's plot.gam(). It is 
also not hidden. If I understand correctly what terms.inner does, 
termplot() uses two functions:
     pf <- envir
     carrier <- function(term) {
         if (length(term) > 1)
             carrier(term[[2]])
         else eval(term, data, enclos = pf)
     }
     carrier.name <- function(term) {
         if (length(term) > 1)
             carrier.name(term[[2]])
         else as.character(term)
     }
to do the job.


 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From rgentlem at fhcrc.org  Tue Sep  5 17:21:55 2006
From: rgentlem at fhcrc.org (Robert Gentleman)
Date: Tue, 05 Sep 2006 08:21:55 -0700
Subject: [R] Reserve and biobase
In-Reply-To: <20060905090158.77583.qmail@web25508.mail.ukl.yahoo.com>
References: <20060905090158.77583.qmail@web25508.mail.ukl.yahoo.com>
Message-ID: <44FD9613.8040103@fhcrc.org>

You should ask questions about Bioconductor software on the Bioconductor 
list, you might also read the posting guide and provide version numbers 
for all packages you are using (the output of sessionInfo is useful).

Biobase does not do any plotting, so where ever the action is coming 
from it is unlikely to be Biobase that is doing it. I don't use Rserve 
so I cannot comment on how that might interact with other software. I 
have no idea what you are trying to do, but somehow you seem to be doing 
a lot more than just loading Biobase - so why not try just doing that 
and leave out all the other code opening devices (and why open two 
postscript devices and then close them?).

Robert


saeedeh maleki wrote:
> Hi 
>    
>   I am using Rserve for R2.3.1.
>   every time after I load Biobase library, a new Graphics window  frame pops up. Could any onw know how can avoid it.
>    
>   Best
>   Saeede 
>    
>   class testReserve {
>     public static void main(String[] args) {
>     RServeConnection rsCon = null;
>     Rconnection c = null;
>     Process proc = null;
>       try {
>       Runtime rt = Runtime.getRuntime();
>       proc = rt.exec(generalMetaData.rserveDir);
>       try {
>         c = new Rconnection();
>         c.eval("library(grDevices)");
> //        c.eval("graphics.off()");
>         c.eval("postscript()");
>         //load library
>         c.eval("library(tools)");
>         System.out.println(" load library tools");
>         c.eval(" postscript('foo2.ps')");
>         c.eval(" library(Biobase)");
>         c.eval("graphics.off()");
>         System.out.println(" load library Biobase");
>        
>         }
>       catch (RSrvException ex1) {
>         System.out.println(ex1.getMessage());
>       }
>       }
>     catch (Exception e) {
>       System.out.print("cannot run rserve");
>     }
>     //end of testing
>     }
> }
>    
> 
>  		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Robert Gentleman, PhD
Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M2-B876
PO Box 19024
Seattle, Washington 98109-1024
206-667-7700
rgentlem at fhcrc.org


From svincenz at nemo.unipr.it  Tue Sep  5 16:10:34 2006
From: svincenz at nemo.unipr.it (Simone Vincenzi)
Date: Tue, 5 Sep 2006 16:10:34 +0200
Subject: [R] R:  Optimization
In-Reply-To: <44FC7EE4.70708@pdf.com>
Message-ID: <200609051411.k85EAYcj030681@smtp.unipr.it>

Thanks for the help.
I thought about taking the logarithms of the expression but to cope with the
problem of zero values both in yield and the values of the environmental
variables another transformation is needed (I simply add one to the yield
value and to the environmental variables values). I follow the suggestion
kindly provided by Spencer Graves but I found the following problems, which
depend probably on my understanding:
1) I don't understand why it is needed to test the no-constant model (which
provides a better fitting of the model). Can I simply assume a no-constant
model? 
2) Here comes the big problem. I don't understand what it is done with the
models fit.0 and fit.1. In both cases I have to leave out one variable from
the linear model but I don't understand why in this way I would test the
constraint that all the coefficients should sum to 1. And it is not possible
to apply anova(fit0,fit.0) and anova(fit1,fit.1) because I have different
response variables. 

In conclusion, I don't actually know how to proceed and if in R this kind of
analysis is possible.
Any help and any further explanation would be very appreciated. Below I
report part of the data frame I'm using for the analysis. The environmental
variables (Salinity, Hydrodynamism, Sediment, Oxygen, Chlorophyll and
Bathymetry) values have been transformed using suitability function (and
thus are bounded between 0 and 1) while Yield is in kg/m2.

   Sedi Sal Bathy Chl Hydro Oxy  Yield
1  1.00 1.00 0.51   1 0.17 0.75 1.5
2  0.50 0.95 1.00   1 0.09 0.94 0.4
3  0.50 1.00 0.17   1 0.44 0.90 1.8
4  1.00 0.98 1.00   1 0.10 0.89 4.5
5  0.13 0.84 0.73   1 0.16 0.84 0.4
6  0.50 0.90 0.91   1 0.22 0.84 0.4
7  0.13 0.75 1.00   1 0.14 0.86 0.2
8  0.13 0.84 0.75   1 0.10 0.83 0.3
9  0.13 0.78 0.97   1 0.06 0.84 0.5
10 0.13 0.87 0.70   1 0.45 0.85 1.0
11 1.00 0.77 1.00   1 0.19 0.86 1.5
12 1.00 0.94 0.81   1 0.47 0.86 3.0
13 1.00 0.93 1.00   1 0.45 0.89 2.5
14 0.50 1.00 1.00   1 0.54 0.84 4.0
15 0.50 1.00 1.00   1 0.25 0.88 2.2
16 1.00 1.00 0.56   1 0.25 0.90 5.0
17 1.00 0.90 0.56   1 0.40 0.90 1.5
18 0.50 0.97 1.00   1 0.22 0.95 1.0
19 0.54 0.96 1.00   1 0.18 0.91 0.3
20 1.00 0.97 0.33   1 0.39 0.90 3.0


And here the results of the fitted models so far.

summary(fit0)

Call:
lm(formula = log(Yield + 1) ~ log(Sal + 1) + log(Bathy + 1) + log(Chl + 
    1) + log(Hydro + 1) + log(Oxy + 1) + log(Sedi + 1) - 1, data = data.df)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.05485 -0.23759  0.01331  0.18692  1.23803 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
log(Sal + 1)   5.07193    1.00584   5.042 1.98e-06 ***
log(Bathy + 1)  0.05804    0.20684   0.281 0.779561    
log(Chl + 1)  -8.53941    2.11720  -4.033 0.000106 ***
log(Hydro + 1)  1.73835    0.28815   6.033 2.56e-08 ***
log(Oxy + 1)   4.19951    1.98459   2.116 0.036750 *  
log(Sedi + 1)  1.15953    0.14807   7.831 4.51e-12 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Residual standard error: 0.3735 on 103 degrees of freedom
Multiple R-Squared: 0.9148,     Adjusted R-squared: 0.9098 
F-statistic: 184.2 on 6 and 103 DF,  p-value: < 2.2e-16 

> summary(fit1)

Call:
lm(formula = log(Yield + 1) ~ log(Sal + 1) + log(Bathy + 1) + log(Chl + 
    1) + log(Hydro + 1) + log(Oxy + 1) + log(Sedi + 1), data = (data.df))

Residuals:
      Min        1Q    Median        3Q       Max 
-1.057766 -0.227720  0.006146  0.192200  1.222543 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept)   -4.49400    4.76603  -0.943   0.3479    
log(Sal + 1)   5.13499    1.00861   5.091 1.63e-06 ***
log(Bathy + 1)  0.06685    0.20716   0.323   0.7476    
log(Chl + 1)  -2.48909    6.75718  -0.368   0.7134    
log(Hydro + 1)  1.70674    0.29025   5.880 5.22e-08 ***
log(Oxy + 1)   4.63758    2.03928   2.274   0.0251 *  
log(Sedi + 1)  1.14005    0.14959   7.621 1.34e-11 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Residual standard error: 0.3737 on 102 degrees of freedom
Multiple R-Squared: 0.7589,     Adjusted R-squared: 0.7447 
F-statistic: 53.51 on 6 and 102 DF,  p-value: < 2.2e-16 

> summary(fit.0)

Call:
lm(formula = log((Yield + 1)/(Sedi + 1)) ~ log((Sal + 1)/(Sedi + 
    1)) + log((Bathy + 1)/(Sedi + 1)) + log((Chl + 1)/(Sedi + 
    1)) + log((Hydro + 1)/(Sedi + 1)) + log((Oxy + 1)/(Sedi + 
    1)) - 1, data = subset(data.df))

Residuals:
    Min      1Q  Median      3Q     Max 
-1.5762 -0.2591  0.1065  0.5023  1.4533 

Coefficients:
                           Estimate Std. Error t value Pr(>|t|)    
log((Sal + 1)/(Sedi + 1))    3.0170     1.5304   1.971  0.05134 .  
log((Bathy + 1)/(Sedi + 1))  -0.3982     0.3139  -1.268  0.20748    
log((Chl + 1)/(Sedi + 1))    8.8137     2.3941   3.681  0.00037 ***
log((Hydro + 1)/(Sedi + 1))   0.3309     0.4066   0.814  0.41760    
log((Oxy + 1)/(Sedi + 1))  -12.5242     2.1881  -5.724 1.01e-07 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Residual standard error: 0.5766 on 104 degrees of freedom
Multiple R-Squared: 0.523,      Adjusted R-squared: 0.5001 
F-statistic:  22.8 on 5 and 104 DF,  p-value: 2.179e-15 


> summary(fit.1)

Call:
lm(formula = log((Yield + 1)/(Sedi + 1)) ~ log((Sal + 1)/(Sedi + 
    1)) + log((Bathy + 1)/(Sedi + 1)) + log((Chl + 1)/(Sedi + 
    1)) + log((Hydro + 1)/(Sedi + 1)) + log((Oxy + 1)/(Sedi + 
    1)), data = subset(data.df))

Residuals:
     Min       1Q   Median       3Q      Max 
-1.05552 -0.23441  0.01263  0.18355  1.24473 

Coefficients:
                            Estimate Std. Error t value Pr(>|t|)    
(Intercept)                  1.84916    0.15474  11.950  < 2e-16 ***
log((Sal + 1)/(Sedi + 1))    5.03863    1.00977   4.990 2.46e-06 ***
log((Bathy + 1)/(Sedi + 1))   0.05279    0.20767   0.254   0.7999    
log((Chl + 1)/(Sedi + 1))  -10.96682    2.27270  -4.825 4.86e-06 ***
log((Hydro + 1)/(Sedi + 1))   1.74631    0.28980   6.026 2.64e-08 ***
log((Oxy + 1)/(Sedi + 1))    3.95938    1.98206   1.998   0.0484 *  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Residual standard error: 0.3751 on 103 degrees of freedom
Multiple R-Squared: 0.5606,     Adjusted R-squared: 0.5393 
F-statistic: 26.28 on 5 and 103 DF,  p-value: < 2.2e-16

> anova(fit1,fit0)
Analysis of Variance Table

  Res.Df     RSS  Df Sum of Sq      F Pr(>F)
1    102 14.2409                            
2    103 14.3650  -1   -0.1241 0.8891 0.3479

Thanks for the help

Simone Vincenzi







_________________________________________
Simone Vincenzi, PhD Student 
Department of Environmental Sciences
University of Parma
Parco Area delle Scienze, 33/A, 43100 Parma, Italy
Phone: +39 0521 905696
Fax: +39 0521 906611
e.mail: svincenz at nemo.unipr.it 


-----Messaggio originale-----
Da: Spencer Graves [mailto:spencer.graves at pdf.com] 
Inviato: luned? 4 settembre 2006 21.31
A: Simone Vincenzi
Cc: r-help at stat.math.ethz.ch
Oggetto: Re: [R] Optimization

      Have you considered talking logarithms of the expression you 
mentioned: 

      log(Yield) = a1*log(A)+b1*log(B)+c2*log(C)+...

where a1 = a/(a+b+...), etc.  This model has two constraints not present 
in ordinary least squares:  First, the intercept is assumed to be zero.  
Second, the coefficients in this log formulation must sum to 1.  If I 
were you, I might use something like "lm" to test them both. 

      To explain how, I'll modify the notation, replacing A by X1, B by 
X2, ..., up to Xkm1 (= X[k-1]) and Xk for k different environmental 
variables.  Then I might try something like the following: 

      fit0 <- lm(log(Yield) ~ log(X1) + ... + log(Xk)-1 )
      fit1 <- lm(log(Yield) ~ log(X1) + ... + log(Xk) )
      fit.1 <- lm(log(Yield/Xk) ~ log(X1/Xk) + ... + log(Xkm1/Xk) )
      fit.0 <- lm(log(Yield/Xk) ~ log(X1/Xk) + ... + log(Xkm1/Xk)-1 )

      anova(fit1, fit0) would test the no-constant model, and if I 
haven't made a mistake in this, anova(fit0, fit.0) and anova(fit1, 
fit.1) would test the constraint that all the coefficients should sum to 
1. 

      If you would like further help from this listserve, please provide 
commented, minimal, self-contained, reproducible code to help potential 
respondents understand your question and concerns (as suggested in the 
posting guide "www.R-project.org/posting-guide.html"). 

      Hope this helps. 
      Spencer Graves

Simone Vincenzi wrote:
> Dear R-list,
> I'm trying to estimate the relative importance of 6 environmental
variables
> in determining clam yield. To estimate clam yield a previous work used the
> function Yield = (A^a*B^b*C^c...)^1/(a+b+c+...) where A,B,C... are the
> values of the environmental variables and the weights a,b,c... have not
been
> calibrated on data but taken from literature. Now I'd like to estimate the
> weights a,b,c... by using a dataset with 110 observations of yield and
> values of the environmental variables. I'm wondering if it is feasible or
if
> the number of observation is too low, if some data transformation is
needed
> and which R function is the most appropriate to try to estimate the
weights.
> Any help would be greatly appreciated.
>
> Simone Vincenzi 
>
> _________________________________________
> Simone Vincenzi, PhD Student 
> Department of Environmental Sciences
> University of Parma
> Parco Area delle Scienze, 33/A, 43100 Parma, Italy
> Phone: +39 0521 905696
> Fax: +39 0521 906611
> e.mail: svincenz at nemo.unipr.it 
>
>
>
> --
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   

-- 
No virus found in this incoming message.


 

--


From mtmorgan at fhcrc.org  Tue Sep  5 18:14:36 2006
From: mtmorgan at fhcrc.org (Martin Morgan)
Date: Tue, 05 Sep 2006 09:14:36 -0700
Subject: [R] Reserve and biobase
In-Reply-To: <44FD9613.8040103@fhcrc.org> (Robert Gentleman's message of
	"Tue, 05 Sep 2006 08:21:55 -0700")
References: <20060905090158.77583.qmail@web25508.mail.ukl.yahoo.com>
	<44FD9613.8040103@fhcrc.org>
Message-ID: <6phk64imgsz.fsf@gopher4.fhcrc.org>

Wild speculation on my part, but I wonder if the 'graphics window'
that pops up is the 'welcome' message in Biobase? This is invoked by a
call to 'message' in the .onAttach function...

.onAttach <- function(libname, pkgname) {
   message(paste("\nWelcome to Bioconductor\n",
   [etc]

If so,

suppressMessages(library(Biobase))

might help. It would be great to hear confirmation of this as the
problem. If not, a completely streamlined example (no extra packages
loaded / actions taken) would aid in debugging.

Martin
-- 
Bioconductor

Robert Gentleman <rgentlem at fhcrc.org> writes:

> You should ask questions about Bioconductor software on the Bioconductor 
> list, you might also read the posting guide and provide version numbers 
> for all packages you are using (the output of sessionInfo is useful).
>
> Biobase does not do any plotting, so where ever the action is coming 
> from it is unlikely to be Biobase that is doing it. I don't use Rserve 
> so I cannot comment on how that might interact with other software. I 
> have no idea what you are trying to do, but somehow you seem to be doing 
> a lot more than just loading Biobase - so why not try just doing that 
> and leave out all the other code opening devices (and why open two 
> postscript devices and then close them?).
>
> Robert
>
>
> saeedeh maleki wrote:
>> Hi 
>>    
>>   I am using Rserve for R2.3.1.  every time after I load Biobase
>>   library, a new Graphics window frame pops up. Could any onw know
>>   how can avoid it.
>>    
>>   Best
>>   Saeede 
>>    
>>   class testReserve {
>>     public static void main(String[] args) {
>>     RServeConnection rsCon = null;
>>     Rconnection c = null;
>>     Process proc = null;
>>       try {
>>       Runtime rt = Runtime.getRuntime();
>>       proc = rt.exec(generalMetaData.rserveDir);
>>       try {
>>         c = new Rconnection();
>>         c.eval("library(grDevices)");
>> //        c.eval("graphics.off()");
>>         c.eval("postscript()");
>>         //load library
>>         c.eval("library(tools)");
>>         System.out.println(" load library tools");
>>         c.eval(" postscript('foo2.ps')");
>>         c.eval(" library(Biobase)");
>>         c.eval("graphics.off()");
>>         System.out.println(" load library Biobase");
>>        
>>         }
>>       catch (RSrvException ex1) {
>>         System.out.println(ex1.getMessage());
>>       }
>>       }
>>     catch (Exception e) {
>>       System.out.print("cannot run rserve");
>>     }
>>     //end of testing
>>     }
>> }
>>    
>> 
>>  		
>> ---------------------------------
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 
>
> -- 
> Robert Gentleman, PhD
> Program in Computational Biology
> Division of Public Health Sciences
> Fred Hutchinson Cancer Research Center
> 1100 Fairview Ave. N, M2-B876
> PO Box 19024
> Seattle, Washington 98109-1024
> 206-667-7700
> rgentlem at fhcrc.org
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From gunter.berton at gene.com  Tue Sep  5 18:14:20 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 5 Sep 2006 09:14:20 -0700
Subject: [R] help: advice on the structuring of ReML models foranalysing
	growth curves
In-Reply-To: <20060905142522.GF12212@ms.unimelb.edu.au>
Message-ID: <003501c6d106$585df780$711f210a@gne.windows.gene.com>


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Andrew Robinson
> Sent: Tuesday, September 05, 2006 7:25 AM
> To: Simon Pickett
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] help: advice on the structuring of ReML 
> models foranalysing growth curves
> 
> Hi Simon,
> 
> overall I think that lmer is a good tool for this problem.  It's
> impossible to reply definitively without the full details on the
> experimental design.
> 
> Caveat in place, I have questions and some suggestions.  Are
> treatment1 and treatment2 distinct factors, or two levels of a
> treatment, the dietary compound?  Also, what is broodsize?
> 
> If you want to nest chick id within brood, I think that you should
> include the interaction as a random factor.  If you'd like the age
> effects to differ between chicks then age should be on the left of id.
> 
> Thus, start with something like ...
> 
> model1 <- lmer(weight ~ treatment +  broodsize + sex + age
>        + (1|brood) + (age|id:brood), data=H) 


FWIW, this model can also be easily fit with the lme() function (in the nlme
package) as the random effects are strictly nested. The only advantage in
doing so is that the lme tools for examining the model are somewhat more
developed and extensive (or am I just more familiar with them?)

Cheers,
Bert

- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box


From gunter.berton at gene.com  Tue Sep  5 18:26:01 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 5 Sep 2006 09:26:01 -0700
Subject: [R] terms.inner
In-Reply-To: <Pine.LNX.4.64.0609050717070.14585@homer21.u.washington.edu>
Message-ID: <003f01c6d107$fa707c90$711f210a@gne.windows.gene.com>

Terry:

errr...

> > at it, although I don't remember the form.  This Nixonesque 
> passion with
> > hiding things is one of the reasons I still prefer Splus.
> 

Two comments:

1) The use of namespaces is a well-established appoach in computer science
to avoid naming conflicts (and probably other stuff I don't understand).

2) To be fair, S-Plus is a proprietary closed system, and so can and
presumably does control its naming of new functions so that naming conflicts
are avoided. R, which is an open system with literally hundreds of
contributed packages cannot do this, and so must use some methodology like
namespaces to do so. 


Cheers,
Bert

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box


From A.Robinson at ms.unimelb.edu.au  Tue Sep  5 18:54:37 2006
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Wed, 6 Sep 2006 02:54:37 +1000
Subject: [R] help: advice on the structuring of ReML models foranalysing
	growth curves
In-Reply-To: <003501c6d106$585df780$711f210a@gne.windows.gene.com>
References: <20060905142522.GF12212@ms.unimelb.edu.au>
	<003501c6d106$585df780$711f210a@gne.windows.gene.com>
Message-ID: <20060905165437.GH12212@ms.unimelb.edu.au>

I agree with Bert.  The lme() helper functions are much more developed
than the lmer() helper functions.  This is probably relevant for
Simon's data because temporal autocorrelation is likely for the
measurements within chicks, and is easily handled in lme().  I'm not
sure if it can be done yet in lmer().

Cheers

Andrew

On Tue, Sep 05, 2006 at 09:14:20AM -0700, Berton Gunter wrote:
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Andrew Robinson
> > Sent: Tuesday, September 05, 2006 7:25 AM
> > To: Simon Pickett
> > Cc: r-help at stat.math.ethz.ch
> > Subject: Re: [R] help: advice on the structuring of ReML 
> > models foranalysing growth curves
> > 
> > Hi Simon,
> > 
> > overall I think that lmer is a good tool for this problem.  It's
> > impossible to reply definitively without the full details on the
> > experimental design.
> > 
> > Caveat in place, I have questions and some suggestions.  Are
> > treatment1 and treatment2 distinct factors, or two levels of a
> > treatment, the dietary compound?  Also, what is broodsize?
> > 
> > If you want to nest chick id within brood, I think that you should
> > include the interaction as a random factor.  If you'd like the age
> > effects to differ between chicks then age should be on the left of id.
> > 
> > Thus, start with something like ...
> > 
> > model1 <- lmer(weight ~ treatment +  broodsize + sex + age
> >        + (1|brood) + (age|id:brood), data=H) 
> 
> 
> FWIW, this model can also be easily fit with the lme() function (in the nlme
> package) as the random effects are strictly nested. The only advantage in
> doing so is that the lme tools for examining the model are somewhat more
> developed and extensive (or am I just more familiar with them?)
> 
> Cheers,
> Bert
> 
> - Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
>  
> "The business of the statistician is to catalyze the scientific learning
> process."  - George E. P. Box

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
Email: a.robinson at ms.unimelb.edu.au         http://www.ms.unimelb.edu.au


From therneau at mayo.edu  Tue Sep  5 18:56:10 2006
From: therneau at mayo.edu (Terry Therneau)
Date: Tue, 5 Sep 2006 11:56:10 -0500 (CDT)
Subject: [R] terms.inner() reprise
Message-ID: <200609051656.k85GuAM02775@rocky.mayo.edu>

 I quickly recieved several replies to my terms.inner question.  To
summarize:

1. "It was never in R"
Clearly I was mistaken in my assumptions.  Since gam used it, and gam was
ported, I assumed terms.inner had to have been in R at some point.  But that
version of gam must never have made it to R.

2. Given #1, it's not too surprising that I didn't (yet) get any answers to the
main question, which is what to do instead.  How do you know what can
replace something, when you've never seen the something, after all.  A
bit more digging on my own found that
	all.vars(delete.repsonse(formula))
seems to be what I want.  Something that returns "x z" from the formula
	y ~ log(x) + cos(z)
	

3. "gam has to attached"
This surprised me.  I think of lm, glm, and gam as the "big 3" models from
the Chambers and Hastie book, in the sense of using them all the time in
my consulting work.  I would never have guessed that the first 2 would
make it into base R and the last not so.  
  If I had downloaded and attached the right thing, it seems that I would have
found print.gam.

4. Hiding names
  The soapbox part of my note is getting the most response, and very
entertaining it is.  getAnywhere() seems to be the most general fix to my
frustration.  I may come back with another note summarizing some of the
philosophical debate.

	Terry Therneau


From pbautist2 at yahoo.com.mx  Tue Sep  5 19:37:39 2006
From: pbautist2 at yahoo.com.mx (Patricia Bautista)
Date: Tue, 5 Sep 2006 12:37:39 -0500 (CDT)
Subject: [R] R object code
Message-ID: <20060905173739.69064.qmail@web32713.mail.mud.yahoo.com>

Hi!, I wonder to know if someone can explain me how I
can access R object code. Briefly, what I need to do
is: given a user defined function in R I want to
access its object code with C because I need evaluate
the function but using C. 

Thanks in advance for any help.

Patricia. 


	
	
		
___________________________________________________________ 
Do You Yahoo!? 
La mejor conexi?n a Internet y <b >2GB</b> extra a tu correo por $100 al mes. http://net.yahoo.com.mx


From dreiss.isb at gmail.com  Tue Sep  5 21:32:59 2006
From: dreiss.isb at gmail.com (David Reiss)
Date: Tue, 5 Sep 2006 12:32:59 -0700
Subject: [R] Fitting generalized additive models with constraints?
Message-ID: <fd913b0d0609051232s4c258486qa208ed7ea5c3ae4a@mail.gmail.com>

>> I am trying to fit a GAM for a simple model, a simple model, y ~ s(x0) +
>> s(x1) ; with a constraint that the fitted smooth functions s(x0) and s(x1)
>> have to each always be >0.
>>
>> From the library documentation and a search of the R-site and R-help
>> archives I have not been able to decipher whether the following is possible
>> using this, or other GAM libraries, or whether I will have to try to "roll
>> my own". I see from the mgcv docs that GAMs need to be constrained such
>> that the smooth functions have zero mean. Is there a way around this?
>>
>> Is such a constraint possible?

> It is possible to estimate a GAM subject to this constraint, but be aware that
> the mean levels of your component smooths are not identifiable, so there is
> an unavoidable abitrariness in the estimate....

> You have to have some sort of constraint on the smooths in a GAM to ensure
> identifiability, and a convenient way to set the model up is to write it as
> e.g.
>
> E(y) = a + f0(x0) + f1(x1)
>
> where `a' is the intercept and f0 and f1 are smooth functions which sum to
> zero over their respective covariate values. In this parameterization your
> constraint implies that
>
> a + f0(x0) + f1(x1)  > 0
>
> for all x0, x1. If this constraint is met then you can find constants b and c
> such that b+c=a such that f0(x0)+b>0 and f1(x1)+c>0 for all x0,x1. i.e. you
> redefine f0 as f0+b and f1 as f1+c, and you have a fitted model meeting the
> constraints.
>
> To fit the GAM subject to the constraints you can use mgcv:::pcls... ?pcls has
> some examples, but it does involve moderately low level programming. It's
> hard to impose the constraint exactly, so the usual approach would be to
> impose the constraint over a fairly fine grid of x0, x1 values. Also, you'll
> need to figure out how to select smoothing parameters. For many problems it
> suffices to estimate smoothing parameters on the unconstrained fit, and then
> use these to fit subject to constraints, but it depends on the problem....
>
> Hope that's some use.
>
> Simon

Hi Simon,
thanks very much for the advice. I will try to parse your response and
the pcls docs and see if I can get this to work. In the meantime, I
found a paper that tries to achieve a similar thing with the same
constraints as I am working with, using quadprog:
http://www.esajournals.org/esaonline/?request=get-abstract&issn=0012-9658&volume=083&issue=08&page=2256
-David


From justin_bem at yahoo.fr  Tue Sep  5 22:52:45 2006
From: justin_bem at yahoo.fr (justin bem)
Date: Tue, 5 Sep 2006 20:52:45 +0000 (GMT)
Subject: [R] Re :  merge files after cor.test
In-Reply-To: <91ae6e350609040334s50872eb8v346a2d3ebd6cfd68@mail.gmail.com>
Message-ID: <20060905205245.7629.qmail@web25715.mail.ukl.yahoo.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20060905/08644d01/attachment.pl 

From r.mueller at oeko-sorpe.de  Tue Sep  5 23:10:41 2006
From: r.mueller at oeko-sorpe.de (Richard =?iso-8859-1?q?M=FCller?=)
Date: Tue, 5 Sep 2006 23:10:41 +0200
Subject: [R] winDialog UNIX equivalent?
Message-ID: <200609052310.41627.r.mueller@oeko-sorpe.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060905/f72d32c1/attachment.pl 

From ripley at stats.ox.ac.uk  Tue Sep  5 23:36:19 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 5 Sep 2006 22:36:19 +0100 (BST)
Subject: [R] R object code
In-Reply-To: <20060905173739.69064.qmail@web32713.mail.mud.yahoo.com>
References: <20060905173739.69064.qmail@web32713.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.64.0609052231090.12320@gannet.stats.ox.ac.uk>

On Tue, 5 Sep 2006, Patricia Bautista wrote:

> Hi!, I wonder to know if someone can explain me how I
> can access R object code. Briefly, what I need to do
> is: given a user defined function in R I want to
> access its object code with C because I need evaluate
> the function but using C. 

Are you doing this from a package in R, or from another application?
Both are covered in 'Writing R Extensions': the first uses eval in C, and 
the second needs embedded R or an R server.

Depending on your OS, there are other possibilities like (D)COM (also in 
that manual) and Rserve (www.rosuda.org/Rserve).

(The details are more appropriate for the R-devel list, according to the 
posting guide.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From mnair at iusb.edu  Wed Sep  6 02:24:55 2006
From: mnair at iusb.edu (Nair, Murlidharan T)
Date: Tue, 5 Sep 2006 20:24:55 -0400
Subject: [R] biplot label size
Message-ID: <A32055BDEA88C34BB3DBBCD2293807786E30D4@iu-mssg-mbx109.ads.iu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060905/39a5c219/attachment.pl 

From ggrothendieck at gmail.com  Wed Sep  6 02:59:36 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 5 Sep 2006 20:59:36 -0400
Subject: [R] biplot label size
In-Reply-To: <A32055BDEA88C34BB3DBBCD2293807786E30D4@iu-mssg-mbx109.ads.iu.edu>
References: <A32055BDEA88C34BB3DBBCD2293807786E30D4@iu-mssg-mbx109.ads.iu.edu>
Message-ID: <971536df0609051759m62f482fdw3374e9c81885cca6@mail.gmail.com>

?biplot indicates cex= controls the size of the point labels.  Is
that what you want?   I suspect not since you say you've tried
it so if its something that ?biplot does not answer you may need
to examine the source.   Enter this into R to display the source:

   stats:::biplot.default


On 9/5/06, Nair, Murlidharan T <mnair at iusb.edu> wrote:
> Which is the parameter that is used to decrease the size of ylabs
> plotted in biplot? I tried playing with cex and cex.lab I am not getting
> it right
>
>
>
> pc <- princomp(USArrests)
>
> biplot(pc, xlabs = rep("", nrow(USArrests)),ylabs=(colnames(USArrests)))
>
>
>
>  Thanks../Murli
>
>
>
>
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From spencer.graves at pdf.com  Wed Sep  6 04:24:23 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 05 Sep 2006 19:24:23 -0700
Subject: [R] R:  Optimization
In-Reply-To: <200609051411.k85EAYcj030681@smtp.unipr.it>
References: <200609051411.k85EAYcj030681@smtp.unipr.it>
Message-ID: <44FE3157.5050105@pdf.com>

<comments in line> 

Simone Vincenzi wrote:
> Thanks for the help.
> I thought about taking the logarithms of the expression but to cope with the
> problem of zero values both in yield and the values of the environmental
> variables another transformation is needed (I simply add one to the yield
> value and to the environmental variables values). 
Do you have 0 yield in any cases where the values of all the 
environmental variables were non-zero?   I didn't see any 0's in the 
data you have below.  I would not worry about 0's unless they actually 
occur.  If you have a larger data set with some 0's then I suggest you 
consider the cases where the 0's actually occur. 

      * If every 0 yield occurs when at least one of the environmental 
variables is 0, you can safely delete all cases with 0 yield, because 
they provide zero information to estimate any of the parameters in your 
model.  I would rerun the model without the 1's, as they make no sense 
to me. 

      * Otherwise, you need to evaluate the noise in the model.  In 
other words, the equation you wrote will not fit exactly.  The standard 
regression model (linear or nonlinear) assumes that y = f(X, b) + e, 
where "X" here is a vector of A, B, ..., b is a vector of the parameters 
to be estimated, b0, b1, ..., in the model I wrote, and the errors "e" 
are normally distributed with constant variance.  If you have cases 
where the yield is zero but none of the X's are, you have problems with 
this assumption no matter what.  You could use "nls" with the model as 
you specified it.  However, to get starting values for "nls" I suggest 
you run "lm" on the logarithms, adding 1 if you like, as you suggested. 
> I follow the suggestion
> kindly provided by Spencer Graves but I found the following problems, which
> depend probably on my understanding:
> 1) I don't understand why it is needed to test the no-constant model (which
> provides a better fitting of the model). Can I simply assume a no-constant
> model? 
>   
I was assuming yield would be a number between 0 and 1, similar to A, B, 
... .  If your inputs A, B, .., are all between 0 and 1 but yield is 
not. I suggest you carefully examine the source for that equation, 
because it makes no physical sense to me.  Your regression results below 
report that the intercept is not significantly different from 0.  
However, I would suspect that might be just an accident.  If you change 
units from kg/m2 to psi or something else, you should get a 
statistically significant intercept. 

      * If yield is between 0 and 1, then the yield equation you wrote 
makes physical sense.  And then it makes sense to test whether b0 = 0, 
because that is a test for whether there are other environmental that 
impact yield that are not in the model. 

      * Similarly, the comparisons of fit0 and fit.0 plus fit1 and fit.1 
are designed to test for other environmental variable(s) not in the 
model that affect yield but are correlated somewhat with the variables 
you already have. 

      For more, I suggest you consult a statistician.  There must be 
several at Uni Parma. 

      Hope this helps. 
      Spencer Graves
> 2) Here comes the big problem. I don't understand what it is done with the
> models fit.0 and fit.1. In both cases I have to leave out one variable from
> the linear model but I don't understand why in this way I would test the
> constraint that all the coefficients should sum to 1. And it is not possible
> to apply anova(fit0,fit.0) and anova(fit1,fit.1) because I have different
> response variables. 
>
> In conclusion, I don't actually know how to proceed and if in R this kind of
> analysis is possible.
> Any help and any further explanation would be very appreciated. Below I
> report part of the data frame I'm using for the analysis. The environmental
> variables (Salinity, Hydrodynamism, Sediment, Oxygen, Chlorophyll and
> Bathymetry) values have been transformed using suitability function (and
> thus are bounded between 0 and 1) while Yield is in kg/m2.
>
>    Sedi Sal Bathy Chl Hydro Oxy  Yield
> 1  1.00 1.00 0.51   1 0.17 0.75 1.5
> 2  0.50 0.95 1.00   1 0.09 0.94 0.4
> 3  0.50 1.00 0.17   1 0.44 0.90 1.8
> 4  1.00 0.98 1.00   1 0.10 0.89 4.5
> 5  0.13 0.84 0.73   1 0.16 0.84 0.4
> 6  0.50 0.90 0.91   1 0.22 0.84 0.4
> 7  0.13 0.75 1.00   1 0.14 0.86 0.2
> 8  0.13 0.84 0.75   1 0.10 0.83 0.3
> 9  0.13 0.78 0.97   1 0.06 0.84 0.5
> 10 0.13 0.87 0.70   1 0.45 0.85 1.0
> 11 1.00 0.77 1.00   1 0.19 0.86 1.5
> 12 1.00 0.94 0.81   1 0.47 0.86 3.0
> 13 1.00 0.93 1.00   1 0.45 0.89 2.5
> 14 0.50 1.00 1.00   1 0.54 0.84 4.0
> 15 0.50 1.00 1.00   1 0.25 0.88 2.2
> 16 1.00 1.00 0.56   1 0.25 0.90 5.0
> 17 1.00 0.90 0.56   1 0.40 0.90 1.5
> 18 0.50 0.97 1.00   1 0.22 0.95 1.0
> 19 0.54 0.96 1.00   1 0.18 0.91 0.3
> 20 1.00 0.97 0.33   1 0.39 0.90 3.0
>
>
> And here the results of the fitted models so far.
>
> summary(fit0)
>
> Call:
> lm(formula = log(Yield + 1) ~ log(Sal + 1) + log(Bathy + 1) + log(Chl + 
>     1) + log(Hydro + 1) + log(Oxy + 1) + log(Sedi + 1) - 1, data = data.df)
>
> Residuals:
>      Min       1Q   Median       3Q      Max 
> -1.05485 -0.23759  0.01331  0.18692  1.23803 
>
> Coefficients:
>               Estimate Std. Error t value Pr(>|t|)    
> log(Sal + 1)   5.07193    1.00584   5.042 1.98e-06 ***
> log(Bathy + 1)  0.05804    0.20684   0.281 0.779561    
> log(Chl + 1)  -8.53941    2.11720  -4.033 0.000106 ***
> log(Hydro + 1)  1.73835    0.28815   6.033 2.56e-08 ***
> log(Oxy + 1)   4.19951    1.98459   2.116 0.036750 *  
> log(Sedi + 1)  1.15953    0.14807   7.831 4.51e-12 ***
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
>
> Residual standard error: 0.3735 on 103 degrees of freedom
> Multiple R-Squared: 0.9148,     Adjusted R-squared: 0.9098 
> F-statistic: 184.2 on 6 and 103 DF,  p-value: < 2.2e-16 
>
>   
>> summary(fit1)
>>     
>
> Call:
> lm(formula = log(Yield + 1) ~ log(Sal + 1) + log(Bathy + 1) + log(Chl + 
>     1) + log(Hydro + 1) + log(Oxy + 1) + log(Sedi + 1), data = (data.df))
>
> Residuals:
>       Min        1Q    Median        3Q       Max 
> -1.057766 -0.227720  0.006146  0.192200  1.222543 
>
> Coefficients:
>               Estimate Std. Error t value Pr(>|t|)    
> (Intercept)   -4.49400    4.76603  -0.943   0.3479    
> log(Sal + 1)   5.13499    1.00861   5.091 1.63e-06 ***
> log(Bathy + 1)  0.06685    0.20716   0.323   0.7476    
> log(Chl + 1)  -2.48909    6.75718  -0.368   0.7134    
> log(Hydro + 1)  1.70674    0.29025   5.880 5.22e-08 ***
> log(Oxy + 1)   4.63758    2.03928   2.274   0.0251 *  
> log(Sedi + 1)  1.14005    0.14959   7.621 1.34e-11 ***
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
>
> Residual standard error: 0.3737 on 102 degrees of freedom
> Multiple R-Squared: 0.7589,     Adjusted R-squared: 0.7447 
> F-statistic: 53.51 on 6 and 102 DF,  p-value: < 2.2e-16 
>
>   
>> summary(fit.0)
>>     
>
> Call:
> lm(formula = log((Yield + 1)/(Sedi + 1)) ~ log((Sal + 1)/(Sedi + 
>     1)) + log((Bathy + 1)/(Sedi + 1)) + log((Chl + 1)/(Sedi + 
>     1)) + log((Hydro + 1)/(Sedi + 1)) + log((Oxy + 1)/(Sedi + 
>     1)) - 1, data = subset(data.df))
>
> Residuals:
>     Min      1Q  Median      3Q     Max 
> -1.5762 -0.2591  0.1065  0.5023  1.4533 
>
> Coefficients:
>                            Estimate Std. Error t value Pr(>|t|)    
> log((Sal + 1)/(Sedi + 1))    3.0170     1.5304   1.971  0.05134 .  
> log((Bathy + 1)/(Sedi + 1))  -0.3982     0.3139  -1.268  0.20748    
> log((Chl + 1)/(Sedi + 1))    8.8137     2.3941   3.681  0.00037 ***
> log((Hydro + 1)/(Sedi + 1))   0.3309     0.4066   0.814  0.41760    
> log((Oxy + 1)/(Sedi + 1))  -12.5242     2.1881  -5.724 1.01e-07 ***
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
>
> Residual standard error: 0.5766 on 104 degrees of freedom
> Multiple R-Squared: 0.523,      Adjusted R-squared: 0.5001 
> F-statistic:  22.8 on 5 and 104 DF,  p-value: 2.179e-15 
>
>
>   
>> summary(fit.1)
>>     
>
> Call:
> lm(formula = log((Yield + 1)/(Sedi + 1)) ~ log((Sal + 1)/(Sedi + 
>     1)) + log((Bathy + 1)/(Sedi + 1)) + log((Chl + 1)/(Sedi + 
>     1)) + log((Hydro + 1)/(Sedi + 1)) + log((Oxy + 1)/(Sedi + 
>     1)), data = subset(data.df))
>
> Residuals:
>      Min       1Q   Median       3Q      Max 
> -1.05552 -0.23441  0.01263  0.18355  1.24473 
>
> Coefficients:
>                             Estimate Std. Error t value Pr(>|t|)    
> (Intercept)                  1.84916    0.15474  11.950  < 2e-16 ***
> log((Sal + 1)/(Sedi + 1))    5.03863    1.00977   4.990 2.46e-06 ***
> log((Bathy + 1)/(Sedi + 1))   0.05279    0.20767   0.254   0.7999    
> log((Chl + 1)/(Sedi + 1))  -10.96682    2.27270  -4.825 4.86e-06 ***
> log((Hydro + 1)/(Sedi + 1))   1.74631    0.28980   6.026 2.64e-08 ***
> log((Oxy + 1)/(Sedi + 1))    3.95938    1.98206   1.998   0.0484 *  
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
>
> Residual standard error: 0.3751 on 103 degrees of freedom
> Multiple R-Squared: 0.5606,     Adjusted R-squared: 0.5393 
> F-statistic: 26.28 on 5 and 103 DF,  p-value: < 2.2e-16
>
>   
>> anova(fit1,fit0)
>>     
> Analysis of Variance Table
>
>   Res.Df     RSS  Df Sum of Sq      F Pr(>F)
> 1    102 14.2409                            
> 2    103 14.3650  -1   -0.1241 0.8891 0.3479
>
> Thanks for the help
>
> Simone Vincenzi
>
>
>
>
>
>
>
> _________________________________________
> Simone Vincenzi, PhD Student 
> Department of Environmental Sciences
> University of Parma
> Parco Area delle Scienze, 33/A, 43100 Parma, Italy
> Phone: +39 0521 905696
> Fax: +39 0521 906611
> e.mail: svincenz at nemo.unipr.it 
>
>
> -----Messaggio originale-----
> Da: Spencer Graves [mailto:spencer.graves at pdf.com] 
> Inviato: luned? 4 settembre 2006 21.31
> A: Simone Vincenzi
> Cc: r-help at stat.math.ethz.ch
> Oggetto: Re: [R] Optimization
>
>       Have you considered talking logarithms of the expression you 
> mentioned: 
>
>       log(Yield) = a1*log(A)+b1*log(B)+c2*log(C)+...
>
> where a1 = a/(a+b+...), etc.  This model has two constraints not present 
> in ordinary least squares:  First, the intercept is assumed to be zero.  
> Second, the coefficients in this log formulation must sum to 1.  If I 
> were you, I might use something like "lm" to test them both. 
>
>       To explain how, I'll modify the notation, replacing A by X1, B by 
> X2, ..., up to Xkm1 (= X[k-1]) and Xk for k different environmental 
> variables.  Then I might try something like the following: 
>
>       fit0 <- lm(log(Yield) ~ log(X1) + ... + log(Xk)-1 )
>       fit1 <- lm(log(Yield) ~ log(X1) + ... + log(Xk) )
>       fit.1 <- lm(log(Yield/Xk) ~ log(X1/Xk) + ... + log(Xkm1/Xk) )
>       fit.0 <- lm(log(Yield/Xk) ~ log(X1/Xk) + ... + log(Xkm1/Xk)-1 )
>
>       anova(fit1, fit0) would test the no-constant model, and if I 
> haven't made a mistake in this, anova(fit0, fit.0) and anova(fit1, 
> fit.1) would test the constraint that all the coefficients should sum to 
> 1. 
>
>       If you would like further help from this listserve, please provide 
> commented, minimal, self-contained, reproducible code to help potential 
> respondents understand your question and concerns (as suggested in the 
> posting guide "www.R-project.org/posting-guide.html"). 
>
>       Hope this helps. 
>       Spencer Graves
>
> Simone Vincenzi wrote:
>   
>> Dear R-list,
>> I'm trying to estimate the relative importance of 6 environmental
>>     
> variables
>   
>> in determining clam yield. To estimate clam yield a previous work used the
>> function Yield = (A^a*B^b*C^c...)^1/(a+b+c+...) where A,B,C... are the
>> values of the environmental variables and the weights a,b,c... have not
>>     
> been
>   
>> calibrated on data but taken from literature. Now I'd like to estimate the
>> weights a,b,c... by using a dataset with 110 observations of yield and
>> values of the environmental variables. I'm wondering if it is feasible or
>>     
> if
>   
>> the number of observation is too low, if some data transformation is
>>     
> needed
>   
>> and which R function is the most appropriate to try to estimate the
>>     
> weights.
>   
>> Any help would be greatly appreciated.
>>
>> Simone Vincenzi 
>>
>> _________________________________________
>> Simone Vincenzi, PhD Student 
>> Department of Environmental Sciences
>> University of Parma
>> Parco Area delle Scienze, 33/A, 43100 Parma, Italy
>> Phone: +39 0521 905696
>> Fax: +39 0521 906611
>> e.mail: svincenz at nemo.unipr.it 
>>
>>
>>
>> --
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>>     
> http://www.R-project.org/posting-guide.html
>   
>> and provide commented, minimal, self-contained, reproducible code.
>>   
>>     
>
>


From ripley at stats.ox.ac.uk  Wed Sep  6 08:16:39 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 6 Sep 2006 07:16:39 +0100 (BST)
Subject: [R] biplot label size
In-Reply-To: <A32055BDEA88C34BB3DBBCD2293807786E30D4@iu-mssg-mbx109.ads.iu.edu>
References: <A32055BDEA88C34BB3DBBCD2293807786E30D4@iu-mssg-mbx109.ads.iu.edu>
Message-ID: <Pine.LNX.4.64.0609060715450.1499@gannet.stats.ox.ac.uk>

cex works for me.

On Tue, 5 Sep 2006, Nair, Murlidharan T wrote:

> Which is the parameter that is used to decrease the size of ylabs
> plotted in biplot? I tried playing with cex and cex.lab I am not getting
> it right
> 
> pc <- princomp(USArrests) 
> 
> biplot(pc, xlabs = rep("", nrow(USArrests)),ylabs=(colnames(USArrests)))
> 
>  Thanks../Murli


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From gunther.hoening at ukmainz.de  Wed Sep  6 09:34:14 2006
From: gunther.hoening at ukmainz.de (=?iso-8859-1?Q?Gunther_H=F6ning?=)
Date: Wed, 6 Sep 2006 09:34:14 +0200
Subject: [R] Question on AffyBatch
In-Reply-To: <44FE3157.5050105@pdf.com>
Message-ID: <001201c6d186$da1bddc0$0f1e0b0a@3med.klinik.unimainz.de>

Dear list,

I'm trying to find out the following in an AffyBatch.

To get the indices from a loction on a chip I use the function xy2i() for
the hgu133plus2 by Affymetrix.
But now I want to know the name of the probe located at this spot. How can
this be done?
And what about the locations used as QC?

Gunther


From dimitris.rizopoulos at med.kuleuven.be  Wed Sep  6 09:33:12 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 6 Sep 2006 09:33:12 +0200
Subject: [R] [R-pkgs] package ltm -- version 0.6-0
Message-ID: <00ad01c6d186$b5597740$0540210a@www.domain>

Dear R-users,

I'd like to announce the release of the new version of package 'ltm' 
for analyzing multivariate dichotomous and polytomous data under the 
Item Response Theory approach.

New features:

* function tpm() (along with supporting methods, i.e., anova, plot, 
margins, factor.scores, etc.) has been added for fitting Birnbaum's 
Three Parameter Model.

* grm() can now handle mix of dichotomous and polytomous items.

* descript() returns more output, especially for dichotomous manifest 
variables.

For more details check the CHANGES file. Future plans include 
development of functions for goodness-of-fit, and the Partial Credit 
Model.

Any kind of feedback (questions, suggestions, bug-reports, etc.) is 
more than welcome.


Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm



Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From wangtong at usc.edu  Wed Sep  6 10:21:10 2006
From: wangtong at usc.edu (Tong Wang)
Date: Wed, 06 Sep 2006 01:21:10 -0700
Subject: [R] Quick question about lm()
In-Reply-To: <000b01c6d0af$79aef080$0202a8c0@headquarters.silicoinsights>
References: <dd788bc85ca3.44fcaa15@usc.edu>
	<000b01c6d0af$79aef080$0202a8c0@headquarters.silicoinsights>
Message-ID: <f683f1fdc1ae.44fe2286@usc.edu>

Thanks a lot for your help. 

tong

----- Original Message -----
From: Christos Hatzis <christos at nuverabio.com>
Date: Monday, September 4, 2006 10:54 pm
Subject: RE: [R] Quick question about lm()
To: 'Tong Wang' <wangtong at usc.edu>, r-help at stat.math.ethz.ch

> Say,
> 
> my.lm <- lm(y ~ x, data=my.data)
> 
> Then if you try:
> 
> names(summary(my.lm)) 
> 
> you will see the components of the summary.lm object.  The 
> coefficients and
> t-statistics can be extracted by
> 
> summary(my.lm)$coefficients
> 
> and similarly for the r-squared and other statistics provided in 
> the summary
> report.
> 
> -Christos
> 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Tong Wang
> Sent: Tuesday, September 05, 2006 1:35 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Quick question about lm()
> 
> Hi, 
>     Feel awkward to ask , but really couldn't find a answer 
> anywhere,   How
> could I extract the R^2 and t-stat. from the 
> result of lm()?
>     Thanks a lot. 
> 
> best
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.htmland provide commented, minimal, self-contained, 
> reproducible code.
> 
> 
> 
>


From wangtong at usc.edu  Wed Sep  6 10:24:46 2006
From: wangtong at usc.edu (Tong Wang)
Date: Wed, 06 Sep 2006 01:24:46 -0700
Subject: [R] A question about gc()
In-Reply-To: <Pine.LNX.4.64.0609051136360.818@gannet.stats.ox.ac.uk>
References: <dddff55b4e4a.44fce8d3@usc.edu>
	<Pine.LNX.4.64.0609051136360.818@gannet.stats.ox.ac.uk>
Message-ID: <dde59359952a.44fe235e@usc.edu>

Yes, I am using R in windows, sorry for not being specific. 
You are right, I have stored too much stuff, and I need to trash some data in the process. 
Problem solved for me, Thank you very much. 

tong

----- Original Message -----
From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
Date: Tuesday, September 5, 2006 3:42 am
Subject: Re: [R] A question about gc()
To: Tong Wang <wangtong at usc.edu>
Cc: R help <r-help at stat.math.ethz.ch>

> On Tue, 5 Sep 2006, Tong Wang wrote:
> 
> > Hi everyone, 
> 
> >     I am doing some intensive computation:  50000 regressions of 
> the 
> > form Y~X with each y of size (1,1000) , even if I break invoke 
> gc() for 
> > a few time in the loop, it still breaks down at some point with 
> the 
> > error message:
> 
> gc() does not reclaim memory for you beyond what R has already done.
> 
> > Error in .signalSimpleWarning("Reached total allocation of 
> 1024Mb: see help(memory.size)",  : 
> >         recursive default argument reference
> 
> >  After getting this, even if I call gc() and resume the 
> computation, it 
> > won't move at all. May I get some suggestions what should I do to 
> get 
> > around this problem ?
> 
> Consult the rw-FAQ (since you seem to be using Windows without 
> telling us) 
> and the help page the message mentions.
> 
> It looks as if you are trying to store too many objects.  If you 
> have lots 
> of RAM you can increase that limit (see the previous para), but you 
> are 
> getting uncomfortably close to the address space limit of your OS.
> 
> Perhaps you can only save the part of the fit you need, or save() 
> the 
> objects to separate files and rm() them, and postprocess them in a 
> later 
> session?
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>


From wangtong at usc.edu  Wed Sep  6 10:37:22 2006
From: wangtong at usc.edu (Tong Wang)
Date: Wed, 06 Sep 2006 01:37:22 -0700
Subject: [R] What is the matrix version of min()
Message-ID: <f6b9cba2d838.44fe2652@usc.edu>

Hi, 
    Is there a function which operates on a matrix and return a vector of min/max of each rol/col ?
say,  X=  2,  1
                3,  4 
min.col(X)=c(2,1)

thanks a lot.

tong


From s.wood at bath.ac.uk  Wed Sep  6 10:42:19 2006
From: s.wood at bath.ac.uk (Simon Wood)
Date: Wed, 6 Sep 2006 09:42:19 +0100
Subject: [R] Fitting generalized additive models with constraints?
In-Reply-To: <fd913b0d0609051232s4c258486qa208ed7ea5c3ae4a@mail.gmail.com>
References: <fd913b0d0609051232s4c258486qa208ed7ea5c3ae4a@mail.gmail.com>
Message-ID: <200609060942.19308.s.wood@bath.ac.uk>

On Tuesday 05 September 2006 20:32, David Reiss wrote:
> >> I am trying to fit a GAM for a simple model, a simple model, y ~ s(x0) +
> >> s(x1) ; with a constraint that the fitted smooth functions s(x0) and
> >> s(x1) have to each always be >0.
> >>
> >> From the library documentation and a search of the R-site and R-help
> >> archives I have not been able to decipher whether the following is
> >> possible using this, or other GAM libraries, or whether I will have to
> >> try to "roll my own". I see from the mgcv docs that GAMs need to be
> >> constrained such that the smooth functions have zero mean. Is there a
> >> way around this?
> >>
> >> Is such a constraint possible?
> >
> > It is possible to estimate a GAM subject to this constraint, but be aware
> > that the mean levels of your component smooths are not identifiable, so
> > there is an unavoidable abitrariness in the estimate....
> >
> > You have to have some sort of constraint on the smooths in a GAM to
> > ensure identifiability, and a convenient way to set the model up is to
> > write it as e.g.
> >
> > E(y) = a + f0(x0) + f1(x1)
> >
> > where `a' is the intercept and f0 and f1 are smooth functions which sum
> > to zero over their respective covariate values. In this parameterization
> > your constraint implies that
> >
> > a + f0(x0) + f1(x1)  > 0
> >
> > for all x0, x1. If this constraint is met then you can find constants b
> > and c such that b+c=a such that f0(x0)+b>0 and f1(x1)+c>0 for all x0,x1.
> > i.e. you redefine f0 as f0+b and f1 as f1+c, and you have a fitted model
> > meeting the constraints.
> >
> > To fit the GAM subject to the constraints you can use mgcv:::pcls...
> > ?pcls has some examples, but it does involve moderately low level
> > programming. It's hard to impose the constraint exactly, so the usual
> > approach would be to impose the constraint over a fairly fine grid of x0,
> > x1 values. Also, you'll need to figure out how to select smoothing
> > parameters. For many problems it suffices to estimate smoothing
> > parameters on the unconstrained fit, and then use these to fit subject to
> > constraints, but it depends on the problem....
> >
> > Hope that's some use.
> >
> > Simon
>
> Hi Simon,
> thanks very much for the advice. I will try to parse your response and
> the pcls docs and see if I can get this to work. In the meantime, I
> found a paper that tries to achieve a similar thing with the same
> constraints as I am working with, using quadprog:
> http://www.esajournals.org/esaonline/?request=get-abstract&issn=0012-9658&v
>olume=083&issue=08&page=2256 -David

- The additional wrinkle in gradient matching is that you have the equality 
constraints that the functions pass through zero at zero (a zero population 
can't produce offspring or corpses! ), which you don't have do you? If you do 
then it removes the ambiguity in the model, and makes everything a bit 
easier.
best,
Simon

-- 
> Simon Wood, Mathematical Sciences, University of Bath, Bath, BA2 7AY UK
> +44 1225 386603  www.maths.bath.ac.uk/~sw283


From r.hankin at noc.soton.ac.uk  Wed Sep  6 10:42:11 2006
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Wed, 6 Sep 2006 09:42:11 +0100
Subject: [R] What is the matrix version of min()
In-Reply-To: <f6b9cba2d838.44fe2652@usc.edu>
References: <f6b9cba2d838.44fe2652@usc.edu>
Message-ID: <5108D474-804D-4204-A2D7-E1B07A224503@soc.soton.ac.uk>

Tong

you need to use apply().  The second argument specifies whether
you want to work with rows or columns.  The point of this is that
min() and max() operate on vectors and give a single value,
and you want to "apply" this function to all rows or all columns:

 > a <- matrix(rnorm(30),5,6)
 > apply(a,2,max)
[1] 2.6413241 0.9842076 1.7989560 0.6999855 2.0542201 0.1162821
 > apply(a,1,max)
[1] 1.1771370 0.9811693 2.6413241 0.9842076 2.0542201
 >

HTH

rksh


On 6 Sep 2006, at 09:37, Tong Wang wrote:

> Hi,
>     Is there a function which operates on a matrix and return a  
> vector of min/max of each rol/col ?
> say,  X=  2,  1
>                 3,  4
> min.col(X)=c(2,1)
>
> thanks a lot.
>
> tong
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From dimitris.rizopoulos at med.kuleuven.be  Wed Sep  6 10:48:02 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 6 Sep 2006 10:48:02 +0200
Subject: [R] What is the matrix version of min()
References: <f6b9cba2d838.44fe2652@usc.edu>
Message-ID: <011001c6d191$29a01d20$0540210a@www.domain>

you could use something like:

# for row min and max
apply(X, 1, min)
apply(X, 1, max)

# for column min and max
apply(X, 2, min)
apply(X, 2, max)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Tong Wang" <wangtong at usc.edu>
To: "R help" <r-help at stat.math.ethz.ch>
Sent: Wednesday, September 06, 2006 10:37 AM
Subject: [R] What is the matrix version of min()


> Hi,
>    Is there a function which operates on a matrix and return a 
> vector of min/max of each rol/col ?
> say,  X=  2,  1
>                3,  4
> min.col(X)=c(2,1)
>
> thanks a lot.
>
> tong
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From mel at altk.com  Wed Sep  6 10:48:58 2006
From: mel at altk.com (mel)
Date: Wed, 06 Sep 2006 10:48:58 +0200
Subject: [R] What is the matrix version of min()
In-Reply-To: <f6b9cba2d838.44fe2652@usc.edu>
References: <f6b9cba2d838.44fe2652@usc.edu>
Message-ID: <44FE8B7A.6070304@altk.com>

Tong Wang a ?crit :

> Hi, 
>     Is there a function which operates on a matrix and return a vector of min/max of each rol/col ?
> say,  X=  2,  1
>                 3,  4 
> min.col(X)=c(2,1)
> thanks a lot.
> tong

see ?pmin, which.min, which.max, max.col
hih


From JeeBee at troefpunt.nl  Wed Sep  6 10:58:55 2006
From: JeeBee at troefpunt.nl (JeeBee)
Date: Wed, 06 Sep 2006 10:58:55 +0200
Subject: [R] What is the matrix version of min()
References: <f6b9cba2d838.44fe2652@usc.edu>
Message-ID: <pan.2006.09.06.08.58.52.285403@troefpunt.nl>


see ?apply

min.row <- apply(X, 1, min)
min.col <- apply(X, 2, min)

JeeBee

On Wed, 06 Sep 2006 01:37:22 -0700, Tong Wang wrote:

> Hi,
>     Is there a function which operates on a matrix and return a vector of
>     min/max of each rol/col ?
> say,  X=  2,  1
>                 3,  4
> min.col(X)=c(2,1)
> 
> thanks a lot.
> 
> tong
> 
> ______________________________________________ R-help at stat.math.ethz.ch
> mailing list https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read
> the posting guide http://www.R-project.org/posting-guide.html and provide
> commented, minimal, self-contained, reproducible code.


From d.scott at auckland.ac.nz  Wed Sep  6 11:09:17 2006
From: d.scott at auckland.ac.nz (David Scott)
Date: Wed, 6 Sep 2006 21:09:17 +1200 (NZST)
Subject: [R] What is the matrix version of min()
In-Reply-To: <5108D474-804D-4204-A2D7-E1B07A224503@soc.soton.ac.uk>
References: <f6b9cba2d838.44fe2652@usc.edu>
	<5108D474-804D-4204-A2D7-E1B07A224503@soc.soton.ac.uk>
Message-ID: <Pine.LNX.4.64.0609062106380.19339@stat12.stat.auckland.ac.nz>

On Wed, 6 Sep 2006, Robin Hankin wrote:

> Tong
>
> you need to use apply().  The second argument specifies whether
> you want to work with rows or columns.  The point of this is that
> min() and max() operate on vectors and give a single value,
> and you want to "apply" this function to all rows or all columns:
>
> > a <- matrix(rnorm(30),5,6)
> > apply(a,2,max)
> [1] 2.6413241 0.9842076 1.7989560 0.6999855 2.0542201 0.1162821
> > apply(a,1,max)
> [1] 1.1771370 0.9811693 2.6413241 0.9842076 2.0542201
> >
>
> HTH
>
> rksh
>

Or in some circumstances you can use pmin (or pmax):

> a<-matrix(rnorm(15),5,3)
> a
            [,1]       [,2]        [,3]
[1,]  1.5175319 -0.4428964 -0.55473327
[2,] -0.2235937  1.0157411  0.08653748
[3,]  0.3240530 -0.4251498 -0.28565732
[4,]  0.4663556  1.1933213  0.60395935
[5,]  0.4078475  0.1739074  1.85645664
> pmin(a[,1],a[,2],a[,3])
[1] -0.5547333 -0.2235937 -0.4251498  0.4663556  0.1739074


David Scott
_____________________________________________________________
David Scott	Visiting (July 06 to January 07)
 		Department of Probability and Statistics
 		The University of Sheffield
 		The Hicks Building
 		Hounsfield Road
 		Sheffield S3 7RH
 		United Kingdom
Phone:	+44 114 222 3908
Email:	d.scott at auckland.ac.nz


From wangtong at usc.edu  Wed Sep  6 11:14:11 2006
From: wangtong at usc.edu (Tong Wang)
Date: Wed, 06 Sep 2006 02:14:11 -0700
Subject: [R] What is the matrix version of min()
In-Reply-To: <5108D474-804D-4204-A2D7-E1B07A224503@soc.soton.ac.uk>
References: <f6b9cba2d838.44fe2652@usc.edu>
	<5108D474-804D-4204-A2D7-E1B07A224503@soc.soton.ac.uk>
Message-ID: <de56d2458f32.44fe2ef3@usc.edu>

Hi, 
   THANK YOU ALL for the prompt reply. 

cheers.

----- Original Message -----
From: Robin Hankin <r.hankin at noc.soton.ac.uk>
Date: Wednesday, September 6, 2006 1:42 am
Subject: Re: [R] What is the matrix version of min()
To: Tong Wang <wangtong at usc.edu>
Cc: R help <r-help at stat.math.ethz.ch>

> Tong
> 
> you need to use apply().  The second argument specifies whether
> you want to work with rows or columns.  The point of this is that
> min() and max() operate on vectors and give a single value,
> and you want to "apply" this function to all rows or all columns:
> 
> > a <- matrix(rnorm(30),5,6)
> > apply(a,2,max)
> [1] 2.6413241 0.9842076 1.7989560 0.6999855 2.0542201 0.1162821
> > apply(a,1,max)
> [1] 1.1771370 0.9811693 2.6413241 0.9842076 2.0542201
> >
> 
> HTH
> 
> rksh
> 
> 
> On 6 Sep 2006, at 09:37, Tong Wang wrote:
> 
> > Hi,
> >     Is there a function which operates on a matrix and return a  
> > vector of min/max of each rol/col ?
> > say,  X=  2,  1
> >                 3,  4
> > min.col(X)=c(2,1)
> >
> > thanks a lot.
> >
> > tong
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> 
> > guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>  tel  023-8059-7743
> 
> 
> 
> 
>


From gallon.li at gmail.com  Wed Sep  6 12:10:33 2006
From: gallon.li at gmail.com (gallon li)
Date: Wed, 6 Sep 2006 18:10:33 +0800
Subject: [R] plot axises on both sides of a graph
Message-ID: <54f7e7c30609060310s6d54dfb3x2a594c54ccfa774e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060906/41c15ec1/attachment.pl 

From gallon.li at gmail.com  Wed Sep  6 12:19:28 2006
From: gallon.li at gmail.com (gallon li)
Date: Wed, 6 Sep 2006 18:19:28 +0800
Subject: [R] histogram in the background?
Message-ID: <54f7e7c30609060319p7244d360r5127f9902961f3ae@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060906/d486b680/attachment.pl 

From JeeBee at troefpunt.nl  Wed Sep  6 12:25:20 2006
From: JeeBee at troefpunt.nl (JeeBee)
Date: Wed, 06 Sep 2006 12:25:20 +0200
Subject: [R] winDialog UNIX equivalent?
References: <200609052310.41627.r.mueller@oeko-sorpe.de>
Message-ID: <pan.2006.09.06.10.25.18.590736@troefpunt.nl>


Yes, there are many.
To give one example, you could consider using tcltk.

    library(tcltk)
    tkmessageBox(title="This is terrible", 
      message="What did you do?\nPromise not to do this again!", 
      icon="error", type="ok")

On Tue, 05 Sep 2006 23:10:41 +0200, Richard M?ller wrote:

> Hi all,
> I'm using winDialog and winDialogString in scripts running on a
> XP-machine. Since we're using some Linux-machines (Suse 10.0 and 10.1 on
> x86) I'm interested in equivalents of the above functions usable under
> Linux-OS. Are there any? Thanks, Richard


From JeeBee at troefpunt.nl  Wed Sep  6 12:29:51 2006
From: JeeBee at troefpunt.nl (JeeBee)
Date: Wed, 06 Sep 2006 12:29:51 +0200
Subject: [R] histogram in the background?
References: <54f7e7c30609060319p7244d360r5127f9902961f3ae@mail.gmail.com>
Message-ID: <pan.2006.09.06.10.29.50.114921@troefpunt.nl>


How about this?

http://addictedtor.free.fr/graphiques/RGraphGallery.php?graph=78

JeeBee

On Wed, 06 Sep 2006 18:19:28 +0800, gallon li wrote:

> I intend to draw a plot of y against x. In the background of this graph I
> wish to creat a histogram of the horizontal variable x. Does any expert
> know how to produce such a plot?
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________ R-help at stat.math.ethz.ch
> mailing list https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read
> the posting guide http://www.R-project.org/posting-guide.html and provide
> commented, minimal, self-contained, reproducible code.


From JeeBee at troefpunt.nl  Wed Sep  6 12:44:05 2006
From: JeeBee at troefpunt.nl (JeeBee)
Date: Wed, 06 Sep 2006 12:44:05 +0200
Subject: [R] plot axises on both sides of a graph
References: <54f7e7c30609060310s6d54dfb3x2a594c54ccfa774e@mail.gmail.com>
Message-ID: <pan.2006.09.06.10.44.02.536836@troefpunt.nl>


Look at:
?axis (try the examples)

Further, a nice example I found on this mailing lists archive,
from somebody who says this has been asked many times already :)

x <- 1:10
y1 <- 1:10
y2 <- rev(seq(1,1000, length=10))
plot(x,y1,ann=FALSE)
axis(2, at=c(2,4,6,8), labels=as.character(c(2,4,6,8)))

points(x,y2/100,col="red")
axis(4, at=c(2,4,6,8), labels=as.character(c(200, 400, 600, 800))) 


On Wed, 06 Sep 2006 18:10:33 +0800, gallon li wrote:

> Usually the y-axis is shown on the left-hand-side of a graph, is it
> possible to artifically creat one more y-axis on the right-hand-side in R?
> What is the main reference? Thank you in advance.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________ R-help at stat.math.ethz.ch
> mailing list https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read
> the posting guide http://www.R-project.org/posting-guide.html and provide
> commented, minimal, self-contained, reproducible code.


From JeeBee at troefpunt.nl  Wed Sep  6 13:59:51 2006
From: JeeBee at troefpunt.nl (JeeBee)
Date: Wed, 06 Sep 2006 13:59:51 +0200
Subject: [R] histogram in the background?
References: <54f7e7c30609060319p7244d360r5127f9902961f3ae@mail.gmail.com>
	<pan.2006.09.06.10.29.50.114921@troefpunt.nl>
Message-ID: <pan.2006.09.06.11.59.50.115587@troefpunt.nl>

gallon li wrote:

I have found this one before. However, my intension is slightly differing
from this plot: I wish to plot the histogram in the backgroun instead of
in the margin. Thanks anyway!

On Wed, 06 Sep 2006 12:29:51 +0200, JeeBee wrote:

> 
> How about this?
> 
> http://addictedtor.free.fr/graphiques/RGraphGallery.php?graph=78
> 
> JeeBee
>


From gallon.li at gmail.com  Wed Sep  6 14:08:43 2006
From: gallon.li at gmail.com (gallon li)
Date: Wed, 6 Sep 2006 20:08:43 +0800
Subject: [R] Fwd:  plot axises on both sides of a graph
In-Reply-To: <54f7e7c30609060448i49916e06nb40992bffb8a4247@mail.gmail.com>
References: <54f7e7c30609060310s6d54dfb3x2a594c54ccfa774e@mail.gmail.com>
	<44FF72B2.4040506@bitwrit.com.au>
	<54f7e7c30609060448i49916e06nb40992bffb8a4247@mail.gmail.com>
Message-ID: <54f7e7c30609060508o6fb7a2aei594bb3ea7aca5b38@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060906/7e0c304b/attachment.pl 

From r.mueller at oeko-sorpe.de  Wed Sep  6 14:10:31 2006
From: r.mueller at oeko-sorpe.de (Richard =?utf-8?q?M=C3=BCller?=)
Date: Wed, 6 Sep 2006 14:10:31 +0200
Subject: [R] winDialog UNIX equivalent?
In-Reply-To: <200609052310.41627.r.mueller@oeko-sorpe.de>
References: <200609052310.41627.r.mueller@oeko-sorpe.de>
Message-ID: <200609061410.31181.r.mueller@oeko-sorpe.de>

> I'm using winDialog and winDialogString in scripts running on a XP-machine.
> Since we're using some Linux-machines (Suse 10.0 and 10.1 on x86) I'm
Thanks to the respondents. Use of TCl/Tk is a fine idea, because you can use 
the same scripts on Win and Linux OS.
Richard
-- 
Richard M?ller - Am Spring 9 - D-58802 Balve-Eisborn
www.oeko-sorpe.de


From rkrug at sun.ac.za  Wed Sep  6 14:31:21 2006
From: rkrug at sun.ac.za (Rainer M Krug)
Date: Wed, 06 Sep 2006 14:31:21 +0200
Subject: [R] problem with putting objects in list
Message-ID: <44FEBF99.3060401@sun.ac.za>

Hi

I use the following code and it stores the results of density() in the
list dr:

dens <- function(run) { density( positions$X[positions$run==run], bw=3,
cut=-2 ) }
dr <- lapply(1:5, dens)

but the results are stored in dr[[i]] and not dr[i], i.e. plot(dr[[1]])
works, but plot([1]) doesn't.

Is there any way that I can store them in dr[i]?

Thanks a lot,

Rainer



-- 
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
Biology (UCT)

Department of Conservation Ecology and Entomology
University of Stellenbosch
Matieland 7602
South Africa

Tel:		+27 - (0)72 808 2975 (w)
Fax:		+27 - (0)21 808 3304
Cell:		+27 - (0)83 9479 042

email:	RKrug at sun.ac.za
      	Rainer at krugs.de


From JeeBee at troefpunt.nl  Wed Sep  6 14:36:08 2006
From: JeeBee at troefpunt.nl (JeeBee)
Date: Wed, 06 Sep 2006 14:36:08 +0200
Subject: [R] Fwd:  plot axises on both sides of a graph
References: <54f7e7c30609060310s6d54dfb3x2a594c54ccfa774e@mail.gmail.com>
	<44FF72B2.4040506@bitwrit.com.au>
	<54f7e7c30609060448i49916e06nb40992bffb8a4247@mail.gmail.com>
	<54f7e7c30609060508o6fb7a2aei594bb3ea7aca5b38@mail.gmail.com>
Message-ID: <pan.2006.09.06.12.36.06.70382@troefpunt.nl>

See these two examples.

plot(1:2)
axis(4)
mtext("right y axis", side=4, line=-1.5)

par(mar=c(5,4,4,5)+.1)
plot(1:2)
axis(4)
mtext("right y axis", side=4, line=3) 

Good luck finding the right combination again ;)


On Wed, 06 Sep 2006 20:08:43 +0800, gallon li wrote:

> ---------- Forwarded message ---------- From: gallon li
> <gallon.li at gmail.com> Date: Sep 6, 2006 7:48 PM
> Subject: Re: [R] plot axises on both sides of a graph To: Jim Lemon
> <jim at bitwrit.com.au>
> 
> Both of your suggestions are so helpful. By combining what you told me,
> now I am able to produce a second y-axis on the right-hand-side. Still one
> problem remains: how can I put a definition of this y-axis in the space
> left? Clearly there is enough room and I have to check some functions for
> defining the margin of a plot. Moreoever, it seems not straight forward to
> put some vertical text directly on the plot for this second ylab.


From jrkrideau at yahoo.ca  Wed Sep  6 15:17:28 2006
From: jrkrideau at yahoo.ca (John Kane)
Date: Wed, 6 Sep 2006 09:17:28 -0400 (EDT)
Subject: [R] histogram in the background?
In-Reply-To: <pan.2006.09.06.11.59.50.115587@troefpunt.nl>
Message-ID: <20060906131728.69433.qmail@web32805.mail.mud.yahoo.com>


--- JeeBee <JeeBee at troefpunt.nl> wrote:

> gallon li wrote:
> 
> I have found this one before. However, my intension
> is slightly differing
> from this plot: I wish to plot the histogram in the
> backgroun instead of
> in the margin. Thanks anyway!
> 
> On Wed, 06 Sep 2006 12:29:51 +0200, JeeBee wrote:
> 
> > 
> > How about this?
> > 
> >
>
http://addictedtor.free.fr/graphiques/RGraphGallery.php?graph=78
> > 
> > JeeBee

Can you not just plot the histogram and then the main
plot?  

Something like this, although it is a barplot rather
than a histogram.

Hours <- c(0,1,2,3,4,5)
Nam <- c( "alf", "bet", "cet", "det", NA , fet)
 En <- c(5, 7, 8, 9, NA, 9)
 L1 <- c(10, 9, 7, 5, 3 ,6)
 L2 <- c(7, 4, 3, 2, 5, 7)
 
 
  MyLabels <- seq(200, 500, length=13)
 
 mp <-barplot(En)
 
# adjust margins to accommodate titles and labels
especially the 4 axis label.
par(mar=c(5,5,5,5))
 
 barplot(En, ylim=c(0, 12),axes =FALSE, ann=FALSE,
xlab="Hours", ylab="Volume", col.lab="blue")
 points (mp,L1, type="p", pch=19, col = "red")
 points (mp,L2, type = "l", col="blue")
 axis ( 2, 0:12, las=1, font=2) 
 axis (3, at=1:6, tick =F,  labels =c( "alf", "bet",
"cet", "det", NA , "fet"))
 axis (4, at = 0:12, labels = MyLabels, font=2)
 mtext("Number of Units Detected", 4,
line=3,col="blue")
 title ( main="A combined line and bar chart \n with
different x-axis labels", line=3, 
       font=3, col.main="blue")
 box()


From jmacdon at med.umich.edu  Wed Sep  6 15:34:16 2006
From: jmacdon at med.umich.edu (James W. MacDonald)
Date: Wed, 06 Sep 2006 09:34:16 -0400
Subject: [R] Question on AffyBatch
In-Reply-To: <001201c6d186$da1bddc0$0f1e0b0a@3med.klinik.unimainz.de>
References: <001201c6d186$da1bddc0$0f1e0b0a@3med.klinik.unimainz.de>
Message-ID: <44FECE58.1030407@med.umich.edu>

Hi Gunther,

Gunther H?ning wrote:
> Dear list,
> 
> I'm trying to find out the following in an AffyBatch.

This question is related specifically to a Bioconductor package, so 
should be asked on the bioconductor listserv rather than R-help.

Best,

Jim


> 
> To get the indices from a loction on a chip I use the function xy2i() for
> the hgu133plus2 by Affymetrix.
> But now I want to know the name of the probe located at this spot. How can
> this be done?
> And what about the locations used as QC?
> 
> Gunther
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


-- 
James W. MacDonald, M.S.
Biostatistician
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623


**********************************************************
Electronic Mail is not secure, may not be read every day, and should not be used for urgent or sensitive issues.


From h.wickham at gmail.com  Wed Sep  6 15:34:02 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Wed, 6 Sep 2006 08:34:02 -0500
Subject: [R] histogram in the background?
In-Reply-To: <54f7e7c30609060319p7244d360r5127f9902961f3ae@mail.gmail.com>
References: <54f7e7c30609060319p7244d360r5127f9902961f3ae@mail.gmail.com>
Message-ID: <f8e6ff050609060634q69582697s3394ef53a9b31121@mail.gmail.com>

> I intend to draw a plot of y against x. In the background of this graph I
> wish to creat a histogram of the horizontal variable x. Does any expert know
> how to produce such a plot?

When constructing such a plot, you need to be careful that you don't
end up constructing a pretty picture instead of a statistical graphic.
 In this case you need to ask yourself, what would the y-axis
represent?  What scale would it have?

Hadley


From antonio.fabio at gmail.com  Wed Sep  6 15:48:15 2006
From: antonio.fabio at gmail.com (Antonio, Fabio Di Narzo)
Date: Wed, 6 Sep 2006 15:48:15 +0200
Subject: [R] problem with putting objects in list
In-Reply-To: <44FEBF99.3060401@sun.ac.za>
References: <44FEBF99.3060401@sun.ac.za>
Message-ID: <b0808fdc0609060648k37e74aedx861b28439c11cb99@mail.gmail.com>

Use 'sapply' instead of 'lapply'. Type
>?lapply
for details

Antonio, Fabio Di Narzo.
University of Bologna, Italy

2006/9/6, Rainer M Krug <rkrug a sun.ac.za>:
> Hi
>
> I use the following code and it stores the results of density() in the
> list dr:
>
> dens <- function(run) { density( positions$X[positions$run==run], bw=3,
> cut=-2 ) }
> dr <- lapply(1:5, dens)
>
> but the results are stored in dr[[i]] and not dr[i], i.e. plot(dr[[1]])
> works, but plot([1]) doesn't.
>
> Is there any way that I can store them in dr[i]?
>
> Thanks a lot,
>
> Rainer
>
>
>
> --
> Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
> Biology (UCT)
>
> Department of Conservation Ecology and Entomology
> University of Stellenbosch
> Matieland 7602
> South Africa
>
> Tel:            +27 - (0)72 808 2975 (w)
> Fax:            +27 - (0)21 808 3304
> Cell:           +27 - (0)83 9479 042
>
> email:  RKrug a sun.ac.za
>         Rainer a krugs.de
>
> ______________________________________________
> R-help a stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From rkrug at sun.ac.za  Wed Sep  6 16:04:06 2006
From: rkrug at sun.ac.za (Rainer M Krug)
Date: Wed, 06 Sep 2006 16:04:06 +0200
Subject: [R] problem with putting objects in list
In-Reply-To: <b0808fdc0609060648k37e74aedx861b28439c11cb99@mail.gmail.com>
References: <44FEBF99.3060401@sun.ac.za>
	<b0808fdc0609060648k37e74aedx861b28439c11cb99@mail.gmail.com>
Message-ID: <44FED556.5010809@sun.ac.za>

Antonio, Fabio Di Narzo wrote:
> Use 'sapply' instead of 'lapply'. Type
If I use sapply it seems to simplify / collapse to much.

>> ?lapply
> for details


> 
> Antonio, Fabio Di Narzo.
> University of Bologna, Italy
> 
> 2006/9/6, Rainer M Krug <rkrug at sun.ac.za>:
>> Hi
>>
>> I use the following code and it stores the results of density() in the
>> list dr:
>>
>> dens <- function(run) { density( positions$X[positions$run==run], bw=3,
>> cut=-2 ) }
>> dr <- lapply(1:5, dens)
>>
>> but the results are stored in dr[[i]] and not dr[i], i.e. plot(dr[[1]])
>> works, but plot([1]) doesn't.
>>
>> Is there any way that I can store them in dr[i]?
>>
>> Thanks a lot,
>>
>> Rainer
>>
>>
>>
>> -- 
>> Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
>> Biology (UCT)
>>
>> Department of Conservation Ecology and Entomology
>> University of Stellenbosch
>> Matieland 7602
>> South Africa
>>
>> Tel:            +27 - (0)72 808 2975 (w)
>> Fax:            +27 - (0)21 808 3304
>> Cell:           +27 - (0)83 9479 042
>>
>> email:  RKrug at sun.ac.za
>>         Rainer at krugs.de
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>


-- 
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
Biology (UCT)

Department of Conservation Ecology and Entomology
University of Stellenbosch
Matieland 7602
South Africa

Tel:		+27 - (0)72 808 2975 (w)
Fax:		+27 - (0)21 808 3304
Cell:		+27 - (0)83 9479 042

email:	RKrug at sun.ac.za
      	Rainer at krugs.de


From tania.oh at brasenose.oxford.ac.uk  Wed Sep  6 16:16:21 2006
From: tania.oh at brasenose.oxford.ac.uk (Tania Oh)
Date: Wed, 6 Sep 2006 15:16:21 +0100
Subject: [R] how to loop through 2 lists with different indexes
Message-ID: <62AA4AFF-BE9B-45CF-B3A0-2DE1C8758DB3@bnc.ox.ac.uk>

Dear all,

I am a newbie in R and need some help please.  (I do apologise if my  
email is not as informative as it should be, I've tried to include  
the relevant details without overcrowding it with the rest of the code)

I would like to sample (without replacement) Y objects based on the  
number of objects in X in 5 different bins. I'm having trouble  
because the list object in which the number of objects of X is stored  
in doesn't start its index from 0.

# number of X objects in each of the 5 bins
   x.bin.size <- lapply(x.by.bins, nrow)

x.bin.size
$`3`
[1] 1

$`4`
[1] 3

$`5`
[1] 10


# no. of objects in each of the 5 bins of Y
  y.bin.size <- lapply(y.by.bins, nrow)

 >   y.bin.size
$`2`
[1] 4

$`3`
[1] 42

$`4`
[1] 253

$`5`
[1] 945

how do I loop through Y and sample from X when the index of Y starts  
from 2 and that of X starts from 3? in X, the missing index $`2` is  
assumed to have 0 objects in it. hence, I would only sample from Y  
and X starting from index 3.

sample (y.bin.size$`3`, x.bin.size$`3`, replace=FALSE)

but how should I do this in an R command without knowing which  
indexes (of X ) are empty? Any pointers would be greatly appreciated.


Many thanks in advance,
tania


From tania.oh at brasenose.oxford.ac.uk  Wed Sep  6 17:03:34 2006
From: tania.oh at brasenose.oxford.ac.uk (Tania Oh)
Date: Wed, 6 Sep 2006 16:03:34 +0100
Subject: [R] how to loop through 2 lists with different indexes
Message-ID: <0CE0D68B-B0A3-46B6-9593-9F11F2234327@bnc.ox.ac.uk>

Very sorry if this mail is sent out twice to the list, I wasn't sure  
if the email address I used in the first go was correct. 	



	From: 	  tania.oh at bnc.ox.ac.uk
	Subject: 	how to loop through 2 lists with different indexes
	Date: 	6 September 2006 15:16:21 BDT
	To: 	  r-help at lists.R-project.org

Dear all,

I am a newbie in R and need some help please.  (I do apologise if my  
email is not as informative as it should be, I've tried to include  
the relevant details without overcrowding it with the rest of the code)

I would like to sample (without replacement) Y objects based on the  
number of objects in X in 5 different bins. I'm having trouble  
because the list object in which the number of objects of X is stored  
in doesn't start its index from 0.

# number of X objects in each of the 5 bins
   x.bin.size <- lapply(x.by.bins, nrow)

x.bin.size
$`3`
[1] 1

$`4`
[1] 3

$`5`
[1] 10


# no. of objects in each of the 5 bins of Y
  y.bin.size <- lapply(y.by.bins, nrow)

 >   y.bin.size
$`2`
[1] 4

$`3`
[1] 42

$`4`
[1] 253

$`5`
[1] 945

how do I loop through Y and sample from X when the index of Y starts  
from 2 and that of X starts from 3? in X, the missing index $`2` is  
assumed to have 0 objects in it. hence, I would only sample from Y  
and X starting from index 3.

sample (y.bin.size$`3`, x.bin.size$`3`, replace=FALSE)

but how should I do this in an R command without knowing which  
indexes (of X ) are empty? Any pointers would be greatly appreciated.


Many thanks in advance,
tania

  ---
Tania Oh
D.Phil student
Department of Physiology, Anatomy and Genetics
University of Oxford
OX1 3TU


From jholtman at gmail.com  Wed Sep  6 17:04:27 2006
From: jholtman at gmail.com (jim holtman)
Date: Wed, 6 Sep 2006 11:04:27 -0400
Subject: [R] how to loop through 2 lists with different indexes
In-Reply-To: <62AA4AFF-BE9B-45CF-B3A0-2DE1C8758DB3@bnc.ox.ac.uk>
References: <62AA4AFF-BE9B-45CF-B3A0-2DE1C8758DB3@bnc.ox.ac.uk>
Message-ID: <644e1f320609060804o56440ee5t7a6786120b218c92@mail.gmail.com>

To find what names are common to both, use 'intersect'

> x.bin.size <- list('3'=1, '4'=4, '5'=10)
> y.bin.size <- list('2'=4, '3'=42, '4'=253, '5'=954)
> sameNames <- intersect(names(x.bin.size), names(y.bin.size))
> sameNames
[1] "3" "4" "5"
> lapply(sameNames, function(x) sample(seq(y.bin.size[[x]]), x.bin.size[[x]]))
[[1]]
[1] 12

[[2]]
[1]  95 145 228  51

[[3]]
 [1] 858 901 630 599  59 196 168 651 364 728

>


On 9/6/06, Tania Oh <tania.oh at brasenose.oxford.ac.uk> wrote:
> Dear all,
>
> I am a newbie in R and need some help please.  (I do apologise if my
> email is not as informative as it should be, I've tried to include
> the relevant details without overcrowding it with the rest of the code)
>
> I would like to sample (without replacement) Y objects based on the
> number of objects in X in 5 different bins. I'm having trouble
> because the list object in which the number of objects of X is stored
> in doesn't start its index from 0.
>
> # number of X objects in each of the 5 bins
>   x.bin.size <- lapply(x.by.bins, nrow)
>
> x.bin.size
> $`3`
> [1] 1
>
> $`4`
> [1] 3
>
> $`5`
> [1] 10
>
>
> # no. of objects in each of the 5 bins of Y
>  y.bin.size <- lapply(y.by.bins, nrow)
>
>  >   y.bin.size
> $`2`
> [1] 4
>
> $`3`
> [1] 42
>
> $`4`
> [1] 253
>
> $`5`
> [1] 945
>
> how do I loop through Y and sample from X when the index of Y starts
> from 2 and that of X starts from 3? in X, the missing index $`2` is
> assumed to have 0 objects in it. hence, I would only sample from Y
> and X starting from index 3.
>
> sample (y.bin.size$`3`, x.bin.size$`3`, replace=FALSE)
>
> but how should I do this in an R command without knowing which
> indexes (of X ) are empty? Any pointers would be greatly appreciated.
>
>
> Many thanks in advance,
> tania
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From pierreclauss at yahoo.fr  Wed Sep  6 17:27:39 2006
From: pierreclauss at yahoo.fr (pierre clauss)
Date: Wed, 6 Sep 2006 15:27:39 +0000 (GMT)
Subject: [R] About the Skew Student distribution
Message-ID: <20060906152739.49058.qmail@web26301.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060906/ad61d498/attachment.pl 

From toby_marks at americancentury.com  Wed Sep  6 17:48:54 2006
From: toby_marks at americancentury.com (toby_marks at americancentury.com)
Date: Wed, 6 Sep 2006 10:48:54 -0500
Subject: [R] Matrix multiplication using apply() or lappy() ?
Message-ID: <OF105EBAB0.BB554C4F-ON862571E1.0054701A-862571E1.0056E02E@americancentury.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060906/8939eacf/attachment.pl 

From barreiro at pasteur.fr  Wed Sep  6 17:58:44 2006
From: barreiro at pasteur.fr (Luis Barreiro)
Date: Wed, 06 Sep 2006 17:58:44 +0200
Subject: [R] density plots????
Message-ID: <5.0.2.1.2.20060906175755.02462a60@mail.pasteur.fr>

Dear all,

I arrive to do density plots using the function "kde2d" , and from this do 
a countour plot. My problem is that I do not really understand what the 
labels for the different levels mean??? What I would like to obtain is a 
surface encompassing the 95 percentile of my values. In other words I would 
like the levels to represent, for example, the 90th, 95th and 99th 
percentiles of my values. I hope I have been clear.
Do you think you can help me??? I would be VERY grateful.


Thanks in advance

Luis Barreiro


From christos at nuverabio.com  Wed Sep  6 17:56:36 2006
From: christos at nuverabio.com (Christos Hatzis)
Date: Wed, 6 Sep 2006 11:56:36 -0400
Subject: [R] Matrix multiplication using apply() or lappy() ?
In-Reply-To: <OF105EBAB0.BB554C4F-ON862571E1.0054701A-862571E1.0056E02E@americancentury.com>
Message-ID: <003901c6d1cd$091093a0$0e010a0a@headquarters.silicoinsights>

See ?sweep

sweep(a, 2, a[1,],"/")

-Christos  

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
toby_marks at americancentury.com
Sent: Wednesday, September 06, 2006 11:49 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Matrix multiplication using apply() or lappy() ?

I am trying to divide the columns of a matrix by the first row in the
matrix.

I have tried to get this using apply and I seem to be missing a concept
regarding the apply w/o calling a function but rather command args %*% /
etc.  Would using apply be more efficient than this approach? 

I have observed examples in the archives using this type of approach. Does
anybody have a snippet of a call to apply() that would accomplish this as
well?

Thanks!


seed=50
$a = array(rnorm(20),dim=c(4,5))
$b = matrix(a[1,],dim(a)[1],dim(a)[2],byrow=T)
$a
           [,1]       [,2]       [,3]        [,4]       [,5]
[1,] -1.3682810 -0.4314462 1.57572752  0.67928882 -0.3672346 [2,]  0.4328180
0.6556479 0.64289931  0.08983289  0.1852306 [3,] -0.8113932  0.3219253
0.08976065 -2.99309008  0.5818237 [4,]  1.4441013 -0.7838389 0.27655075
0.28488295  1.3997368

$a/b
           [,1]       [,2]       [,3]       [,4]      [,5]
[1,]  1.0000000  1.0000000 1.00000000  1.0000000  1.000000 [2,] -0.3163225
-1.5196515 0.40800157  0.1322455 -0.504393 [3,]  0.5930018 -0.7461539
0.05696457 -4.4062113 -1.584338 [4,] -1.0554128  1.8167710 0.17550671
0.4193841 -3.811560



------------------------------------------------------------
CONFIDENTIALITY NOTICE: This electronic mail transmission (i...{{dropped}}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From afshart at exchange.sba.miami.edu  Wed Sep  6 17:59:04 2006
From: afshart at exchange.sba.miami.edu (Afshartous, David)
Date: Wed, 6 Sep 2006 11:59:04 -0400
Subject: [R] Covariance/Correlation matrix for repeated measures data frame
Message-ID: <6BCB4D493A447546A8126F24332056E80401F9BD@school1.business.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060906/cd9b652c/attachment.pl 

From ggrothendieck at gmail.com  Wed Sep  6 18:03:56 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 6 Sep 2006 12:03:56 -0400
Subject: [R] Matrix multiplication using apply() or lappy() ?
In-Reply-To: <OF105EBAB0.BB554C4F-ON862571E1.0054701A-862571E1.0056E02E@americancentury.com>
References: <OF105EBAB0.BB554C4F-ON862571E1.0054701A-862571E1.0056E02E@americancentury.com>
Message-ID: <971536df0609060903r292b1a82u9347c03252cb7a68@mail.gmail.com>

Here are a few possibilities:

a <- matrix(1:24, 4) # test data

a / rep(a[1,], each = 4)

a / outer(rep(1, nrow(a)), a[1,])

a %*% diag(1/a[1,])

sweep(a, 2, a[1,], "/")


On 9/6/06, toby_marks at americancentury.com
<toby_marks at americancentury.com> wrote:
> I am trying to divide the columns of a matrix by the first row in the
> matrix.
>
> I have tried to get this using apply and I seem to be missing a concept
> regarding the apply w/o calling a function but rather command args %*% /
> etc.  Would using apply be more efficient than this approach?
>
> I have observed examples in the archives using this type of approach. Does
> anybody have a snippet of a call to apply() that would accomplish this as
> well?
>
> Thanks!
>
>
> seed=50
> $a = array(rnorm(20),dim=c(4,5))
> $b = matrix(a[1,],dim(a)[1],dim(a)[2],byrow=T)
> $a
>           [,1]       [,2]       [,3]        [,4]       [,5]
> [1,] -1.3682810 -0.4314462 1.57572752  0.67928882 -0.3672346
> [2,]  0.4328180  0.6556479 0.64289931  0.08983289  0.1852306
> [3,] -0.8113932  0.3219253 0.08976065 -2.99309008  0.5818237
> [4,]  1.4441013 -0.7838389 0.27655075  0.28488295  1.3997368
>
> $a/b
>           [,1]       [,2]       [,3]       [,4]      [,5]
> [1,]  1.0000000  1.0000000 1.00000000  1.0000000  1.000000
> [2,] -0.3163225 -1.5196515 0.40800157  0.1322455 -0.504393
> [3,]  0.5930018 -0.7461539 0.05696457 -4.4062113 -1.584338
> [4,] -1.0554128  1.8167710 0.17550671  0.4193841 -3.811560
>
>
>
> ------------------------------------------------------------
> CONFIDENTIALITY NOTICE: This electronic mail transmission (i...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Wed Sep  6 18:08:35 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 6 Sep 2006 12:08:35 -0400
Subject: [R] Matrix multiplication using apply() or lappy() ?
In-Reply-To: <971536df0609060903r292b1a82u9347c03252cb7a68@mail.gmail.com>
References: <OF105EBAB0.BB554C4F-ON862571E1.0054701A-862571E1.0056E02E@americancentury.com>
	<971536df0609060903r292b1a82u9347c03252cb7a68@mail.gmail.com>
Message-ID: <971536df0609060908p5bcf2414qc93393f2dfe75c15@mail.gmail.com>

And here is one more:

t(apply(a, 1, function(x) x/a[1,]))

On 9/6/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Here are a few possibilities:
>
> a <- matrix(1:24, 4) # test data
>
> a / rep(a[1,], each = 4)
>
> a / outer(rep(1, nrow(a)), a[1,])
>
> a %*% diag(1/a[1,])
>
> sweep(a, 2, a[1,], "/")
>
>
> On 9/6/06, toby_marks at americancentury.com
> <toby_marks at americancentury.com> wrote:
> > I am trying to divide the columns of a matrix by the first row in the
> > matrix.
> >
> > I have tried to get this using apply and I seem to be missing a concept
> > regarding the apply w/o calling a function but rather command args %*% /
> > etc.  Would using apply be more efficient than this approach?
> >
> > I have observed examples in the archives using this type of approach. Does
> > anybody have a snippet of a call to apply() that would accomplish this as
> > well?
> >
> > Thanks!
> >
> >
> > seed=50
> > $a = array(rnorm(20),dim=c(4,5))
> > $b = matrix(a[1,],dim(a)[1],dim(a)[2],byrow=T)
> > $a
> >           [,1]       [,2]       [,3]        [,4]       [,5]
> > [1,] -1.3682810 -0.4314462 1.57572752  0.67928882 -0.3672346
> > [2,]  0.4328180  0.6556479 0.64289931  0.08983289  0.1852306
> > [3,] -0.8113932  0.3219253 0.08976065 -2.99309008  0.5818237
> > [4,]  1.4441013 -0.7838389 0.27655075  0.28488295  1.3997368
> >
> > $a/b
> >           [,1]       [,2]       [,3]       [,4]      [,5]
> > [1,]  1.0000000  1.0000000 1.00000000  1.0000000  1.000000
> > [2,] -0.3163225 -1.5196515 0.40800157  0.1322455 -0.504393
> > [3,]  0.5930018 -0.7461539 0.05696457 -4.4062113 -1.584338
> > [4,] -1.0554128  1.8167710 0.17550671  0.4193841 -3.811560
> >
> >
> >
> > ------------------------------------------------------------
> > CONFIDENTIALITY NOTICE: This electronic mail transmission (i...{{dropped}}
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From ripley at stats.ox.ac.uk  Wed Sep  6 18:10:21 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 6 Sep 2006 17:10:21 +0100 (BST)
Subject: [R] Matrix multiplication using apply() or lappy() ?
In-Reply-To: <003901c6d1cd$091093a0$0e010a0a@headquarters.silicoinsights>
References: <003901c6d1cd$091093a0$0e010a0a@headquarters.silicoinsights>
Message-ID: <Pine.LNX.4.64.0609061706340.23583@gannet.stats.ox.ac.uk>

On Wed, 6 Sep 2006, Christos Hatzis wrote:

> See ?sweep
> 
> sweep(a, 2, a[1,],"/")

That is less efficient than

a/rep(a[1,], each=nrow(a))


> 
> -Christos  
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
> toby_marks at americancentury.com
> Sent: Wednesday, September 06, 2006 11:49 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Matrix multiplication using apply() or lappy() ?
> 
> I am trying to divide the columns of a matrix by the first row in the
> matrix.
> 
> I have tried to get this using apply and I seem to be missing a concept
> regarding the apply w/o calling a function but rather command args %*% /
> etc.  Would using apply be more efficient than this approach? 
> 
> I have observed examples in the archives using this type of approach. Does
> anybody have a snippet of a call to apply() that would accomplish this as
> well?
> 
> Thanks!
> 
> 
> seed=50
> $a = array(rnorm(20),dim=c(4,5))
> $b = matrix(a[1,],dim(a)[1],dim(a)[2],byrow=T)
> $a
>            [,1]       [,2]       [,3]        [,4]       [,5]
> [1,] -1.3682810 -0.4314462 1.57572752  0.67928882 -0.3672346 [2,]  0.4328180
> 0.6556479 0.64289931  0.08983289  0.1852306 [3,] -0.8113932  0.3219253
> 0.08976065 -2.99309008  0.5818237 [4,]  1.4441013 -0.7838389 0.27655075
> 0.28488295  1.3997368
> 
> $a/b
>            [,1]       [,2]       [,3]       [,4]      [,5]
> [1,]  1.0000000  1.0000000 1.00000000  1.0000000  1.000000 [2,] -0.3163225
> -1.5196515 0.40800157  0.1322455 -0.504393 [3,]  0.5930018 -0.7461539
> 0.05696457 -4.4062113 -1.584338 [4,] -1.0554128  1.8167710 0.17550671
> 0.4193841 -3.811560
> 
> 
> 
> ------------------------------------------------------------
> CONFIDENTIALITY NOTICE: This electronic mail transmission (i...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ggrothendieck at gmail.com  Wed Sep  6 18:11:30 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 6 Sep 2006 12:11:30 -0400
Subject: [R] Matrix multiplication using apply() or lappy() ?
In-Reply-To: <971536df0609060908p5bcf2414qc93393f2dfe75c15@mail.gmail.com>
References: <OF105EBAB0.BB554C4F-ON862571E1.0054701A-862571E1.0056E02E@americancentury.com>
	<971536df0609060903r292b1a82u9347c03252cb7a68@mail.gmail.com>
	<971536df0609060908p5bcf2414qc93393f2dfe75c15@mail.gmail.com>
Message-ID: <971536df0609060911y78748429s92f5d9a6cf82da2f@mail.gmail.com>

This last one could also be written slightly shorter as:

t(apply(a, 1, "/", a[1,]))

On 9/6/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> And here is one more:
>
> t(apply(a, 1, function(x) x/a[1,]))
>
> On 9/6/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > Here are a few possibilities:
> >
> > a <- matrix(1:24, 4) # test data
> >
> > a / rep(a[1,], each = 4)
> >
> > a / outer(rep(1, nrow(a)), a[1,])
> >
> > a %*% diag(1/a[1,])
> >
> > sweep(a, 2, a[1,], "/")
> >
> >
> > On 9/6/06, toby_marks at americancentury.com
> > <toby_marks at americancentury.com> wrote:
> > > I am trying to divide the columns of a matrix by the first row in the
> > > matrix.
> > >
> > > I have tried to get this using apply and I seem to be missing a concept
> > > regarding the apply w/o calling a function but rather command args %*% /
> > > etc.  Would using apply be more efficient than this approach?
> > >
> > > I have observed examples in the archives using this type of approach. Does
> > > anybody have a snippet of a call to apply() that would accomplish this as
> > > well?
> > >
> > > Thanks!
> > >
> > >
> > > seed=50
> > > $a = array(rnorm(20),dim=c(4,5))
> > > $b = matrix(a[1,],dim(a)[1],dim(a)[2],byrow=T)
> > > $a
> > >           [,1]       [,2]       [,3]        [,4]       [,5]
> > > [1,] -1.3682810 -0.4314462 1.57572752  0.67928882 -0.3672346
> > > [2,]  0.4328180  0.6556479 0.64289931  0.08983289  0.1852306
> > > [3,] -0.8113932  0.3219253 0.08976065 -2.99309008  0.5818237
> > > [4,]  1.4441013 -0.7838389 0.27655075  0.28488295  1.3997368
> > >
> > > $a/b
> > >           [,1]       [,2]       [,3]       [,4]      [,5]
> > > [1,]  1.0000000  1.0000000 1.00000000  1.0000000  1.000000
> > > [2,] -0.3163225 -1.5196515 0.40800157  0.1322455 -0.504393
> > > [3,]  0.5930018 -0.7461539 0.05696457 -4.4062113 -1.584338
> > > [4,] -1.0554128  1.8167710 0.17550671  0.4193841 -3.811560
> > >
> > >
> > >
> > > ------------------------------------------------------------
> > > CONFIDENTIALITY NOTICE: This electronic mail transmission (i...{{dropped}}
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
>


From dimitris.rizopoulos at med.kuleuven.be  Wed Sep  6 18:12:21 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 6 Sep 2006 18:12:21 +0200
Subject: [R] Covariance/Correlation matrix for repeated measures data
	frame
References: <6BCB4D493A447546A8126F24332056E80401F9BD@school1.business.edu>
Message-ID: <010301c6d1cf$3b7cfc00$0540210a@www.domain>

try the following:

dat <- data.frame(id = rep(1:100, each = 6), time = rep(1:6, 100), pot 
= rnorm(600))

# for a balanced data-set
mat <- matrix(dat$pot, ncol = 6, byrow = TRUE)
cor(mat)

# for a unbalanced data-set
dat <- dat[-sample(600, 100), ]
mat <- t(sapply(split(dat, dat$id), function(x){
    out <- rep(NA, 6)
    out[x$time] <- x$pot
    out
}))

cor(mat, use = "pairwise.complete.obs")


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Afshartous, David" <afshart at exchange.sba.miami.edu>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, September 06, 2006 5:59 PM
Subject: [R] Covariance/Correlation matrix for repeated measures data 
frame


> All,
>
> I have a repeated measures data frame and was wondering if the
> covariance matrix can be
> calculated via some created indexing or built-in R function.
>
> Specifically, say there are 3 variables, where potassium 
> concentration
> is measured 6 times on each patient.
> Patient number (discrete)
> Time (1 to 6, discrete)
> Potassium (continuous variable)
>
> I want the covariance/correlation matrix for the cov/corr between
> Potassium at time i and time j.
>
> Is this possible in the current dataframe format?  Or do I have to
> define new varialbes, say Time i and Time j,
> and then compute the cov/corr between Time i and Time j for all
> combinations?
>
> Cheers,
> Dave
>
>
>
>
> David Afshartous, PhD
> University of Miami
> School of Business
> Rm KE-408
> Coral Gables, FL 33124
>
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From ggrothendieck at gmail.com  Wed Sep  6 18:19:46 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 6 Sep 2006 12:19:46 -0400
Subject: [R] Matrix multiplication using apply() or lappy() ?
In-Reply-To: <971536df0609060911y78748429s92f5d9a6cf82da2f@mail.gmail.com>
References: <OF105EBAB0.BB554C4F-ON862571E1.0054701A-862571E1.0056E02E@americancentury.com>
	<971536df0609060903r292b1a82u9347c03252cb7a68@mail.gmail.com>
	<971536df0609060908p5bcf2414qc93393f2dfe75c15@mail.gmail.com>
	<971536df0609060911y78748429s92f5d9a6cf82da2f@mail.gmail.com>
Message-ID: <971536df0609060919p1c27fa56v7c61ed299e36e5ce@mail.gmail.com>

Yet another one using the idempotent apply in reshape package
that eliminates the transpose:

library(reshape)
iapply(a, 1, "/", a[1,])

On 9/6/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> This last one could also be written slightly shorter as:
>
> t(apply(a, 1, "/", a[1,]))
>
> On 9/6/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > And here is one more:
> >
> > t(apply(a, 1, function(x) x/a[1,]))
> >
> > On 9/6/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > > Here are a few possibilities:
> > >
> > > a <- matrix(1:24, 4) # test data
> > >
> > > a / rep(a[1,], each = 4)
> > >
> > > a / outer(rep(1, nrow(a)), a[1,])
> > >
> > > a %*% diag(1/a[1,])
> > >
> > > sweep(a, 2, a[1,], "/")
> > >
> > >
> > > On 9/6/06, toby_marks at americancentury.com
> > > <toby_marks at americancentury.com> wrote:
> > > > I am trying to divide the columns of a matrix by the first row in the
> > > > matrix.
> > > >
> > > > I have tried to get this using apply and I seem to be missing a concept
> > > > regarding the apply w/o calling a function but rather command args %*% /
> > > > etc.  Would using apply be more efficient than this approach?
> > > >
> > > > I have observed examples in the archives using this type of approach. Does
> > > > anybody have a snippet of a call to apply() that would accomplish this as
> > > > well?
> > > >
> > > > Thanks!
> > > >
> > > >
> > > > seed=50
> > > > $a = array(rnorm(20),dim=c(4,5))
> > > > $b = matrix(a[1,],dim(a)[1],dim(a)[2],byrow=T)
> > > > $a
> > > >           [,1]       [,2]       [,3]        [,4]       [,5]
> > > > [1,] -1.3682810 -0.4314462 1.57572752  0.67928882 -0.3672346
> > > > [2,]  0.4328180  0.6556479 0.64289931  0.08983289  0.1852306
> > > > [3,] -0.8113932  0.3219253 0.08976065 -2.99309008  0.5818237
> > > > [4,]  1.4441013 -0.7838389 0.27655075  0.28488295  1.3997368
> > > >
> > > > $a/b
> > > >           [,1]       [,2]       [,3]       [,4]      [,5]
> > > > [1,]  1.0000000  1.0000000 1.00000000  1.0000000  1.000000
> > > > [2,] -0.3163225 -1.5196515 0.40800157  0.1322455 -0.504393
> > > > [3,]  0.5930018 -0.7461539 0.05696457 -4.4062113 -1.584338
> > > > [4,] -1.0554128  1.8167710 0.17550671  0.4193841 -3.811560
> > > >
> > > >
> > > >
> > > > ------------------------------------------------------------
> > > > CONFIDENTIALITY NOTICE: This electronic mail transmission (i...{{dropped}}
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible code.
> > > >
> > >
> >
>


From jerome.lemaitre.1 at ulaval.ca  Wed Sep  6 18:41:04 2006
From: jerome.lemaitre.1 at ulaval.ca (jerome lemaitre)
Date: Wed, 6 Sep 2006 12:41:04 -0400
Subject: [R] Help on estimated variance in lme4
Message-ID: <001201c6d1d3$3f507560$36cacb84@lemaitrej>

Dear all,

I get an error message when I run my model and I am not sure what to do
about it.

I try to determine what factors influence the survival of voles. I use a
mixed-model because I have several voles per site (varying from 2 to 19
voles).

Here is the model:
###
fm5 <-lmer(data=cdrgsaou2,
alive~factor(pacut)+factor(agecamp)+factor(sex)+ResCondCorp+(1|factor(cdrgsa
ou2$ids)),
	family=binomial,
	method="Laplace",
	)
###
Description of variables
Alive: 0 or 1; dead or alive
pacut: 0 or 1; presence of parasites
agecamp: a or j; adult or juvenile
sex: m or f; male or female
ResCondCorp: body condition, continuous;
cdrgsaou2$ids: name of the site.


Here is the output:

###
Generalized linear mixed model fit using Laplace 
Formula: alive ~ factor(pacut) + factor(agecamp) + factor(sex) + ResCondCorp
+      (1 | factor(cdrgsaou2$ids)) 
   Data: cdrgsaou2 
 Family: binomial(logit link)
      AIC      BIC    logLik deviance
 305.7418 328.7331 -146.8709 293.7418
Random effects:
 Groups                Name        Variance Std.Dev.
 factor(cdrgsaou2$ids) (Intercept) 0.034382 0.18542 
number of obs: 341, groups: factor(cdrgsaou2$ids), 36

Estimated scale (compare to 1)  2.174681 

Fixed effects:
                  Estimate Std. Error z value  Pr(>|z|)    
(Intercept)       0.971458   0.250951  3.8711 0.0001083 ***
factor(pacut)1   -0.831888   0.358583 -2.3199 0.0203447 *  
factor(agecamp)j -1.294236   0.330638 -3.9144 9.065e-05 ***
factor(sex)m      0.581713   0.296229  1.9637 0.0495616 *  
ResCondCorp      -0.176251   0.020263 -8.6982 < 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Correlation of Fixed Effects:
            (Intr) fct()1 fctr(g) fctr(s)
factr(pct)1 -0.334                       
fctr(gcmp)j -0.417  0.066                
factor(sx)m -0.505 -0.002 -0.173         
ResCondCorp -0.309 -0.010  0.302  -0.032 
###

Here is the error message:

###
Warning message:
Estimated variance for factor 'factor(cdrgsaou2$ids)' is effectively zero
 in: LMEopt(x = mer, value = cv)
###

Thank you very much by advance for any help.



J?r?me Lema?tre


Ph.D. student
Silviculture-wildlife research chair in irregular boreal forests
& D?partment of biology,
Faculty of Sciences and Engineering
Alexandre-Vachon building
University Laval
Quebec, QC  G1K 7P4
Phone : (418) 656-2131 poste 2917
Office : VCH-2044
Email: jerome.lemaitre.1 at ulaval.ca


From rolf at erdos.math.unb.ca  Wed Sep  6 18:52:21 2006
From: rolf at erdos.math.unb.ca (Rolf Turner)
Date: Wed, 6 Sep 2006 13:52:21 -0300 (ADT)
Subject: [R] Matrix multiplication using apply() or lappy() ?
Message-ID: <200609061652.k86GqLoD010185@erdos.math.unb.ca>

Prof. Brian Ripley wrote:

> On Wed, 6 Sep 2006, Christos Hatzis wrote:
> 
> > See ?sweep
> > 
> > sweep(a, 2, a[1,],"/")
> 
> That is less efficient than
> 
> a/rep(a[1,], each=nrow(a))

*My* first instinct was to use

	t(t(a)/a[1,])

(which has not heretofore been suggested).

This seems to be more efficient still (at least in respect of Prof.
Grothendieck's toy example) by between 20 and 25 percent:

	> a <- matrix(1:24,4)
	> system.time(for(i in 1:1000) junk <- a / rep(a[1,], each = 4))
	[1] 0.690 0.080 1.051 0.000 0.000
	> system.time(for(i in 1:1000) junk <- t(t(a)/a[1,]))
	[1] 0.520 0.120 0.647 0.000 0.000
	> system.time(for(i in 1:10000) junk <- a / rep(a[1,], each = 4))
	[1]  7.08  0.99 10.08  0.00  0.00
	> system.time(for(i in 1:10000) junk <- t(t(a)/a[1,]))
	[1] 5.530 0.940 7.856 0.000 0.000

			cheers,

				Rolf Turner


From toby_marks at americancentury.com  Wed Sep  6 19:08:55 2006
From: toby_marks at americancentury.com (toby_marks at americancentury.com)
Date: Wed, 6 Sep 2006 12:08:55 -0500
Subject: [R] Matrix multiplication using apply() or lappy() ?
In-Reply-To: <971536df0609060911y78748429s92f5d9a6cf82da2f@mail.gmail.com>
Message-ID: <OF60BE23C7.98F8AAB3-ON862571E1.005DF85F-862571E1.005E3373@americancentury.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060906/28805a9e/attachment.pl 

From ggrothendieck at gmail.com  Wed Sep  6 19:21:36 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 6 Sep 2006 13:21:36 -0400
Subject: [R] Matrix multiplication using apply() or lappy() ?
In-Reply-To: <200609061652.k86GqLoD010185@erdos.math.unb.ca>
References: <200609061652.k86GqLoD010185@erdos.math.unb.ca>
Message-ID: <971536df0609061021m56a5be8ap3419b95ebf853ceb@mail.gmail.com>

In terms of speed Toby's original idea was actually the fastest.
Here they are decreasing order of the largest timing in each
row of system.time.  I also tried it with a 100x10 matrix and
got almost the same order:

> library(reshape)
> system.time(for(i in 1:1000) iapply(a, 1, "/", a[1,]))
[1] 11.51  0.01 18.65    NA    NA
> system.time(for(i in 1:1000) t(apply(a, 1, "/", a[1,])))
[1] 0.83 0.00 1.36   NA   NA
> system.time(for(i in 1:1000) sweep(a, 2, a[1,], "/"))
[1] 0.27 0.00 0.39   NA   NA
> system.time(for(i in 1:1000) a/outer(rep(1, nrow(a)), a[1,]))
[1] 0.23 0.00 0.39   NA   NA
> system.time(for(i in 1:1000) a %*% diag(1/a[1,]))
[1] 0.25 0.00 0.38   NA   NA
> system.time(for(i in 1:1000) a/rep(a[1,], each = nrow(a)))
[1] 0.09 0.00 0.16   NA   NA
> system.time(for(i in 1:1000) t(t(a)/a[1,]))
[1] 0.10 0.00 0.13   NA   NA
> system.time(for(i in 1:1000) a/matrix(a[1,], nrow(a), ncol(a), byrow = TRUE))
[1] 0.05 0.00 0.12   NA   NA


> On 9/6/06, Rolf Turner <rolf at erdos.math.unb.ca> wrote:
> Prof. Brian Ripley wrote:
>
> > On Wed, 6 Sep 2006, Christos Hatzis wrote:
> >
> > > See ?sweep
> > >
> > > sweep(a, 2, a[1,],"/")
> >
> > That is less efficient than
> >
> > a/rep(a[1,], each=nrow(a))
>
> *My* first instinct was to use
>
>        t(t(a)/a[1,])
>
> (which has not heretofore been suggested).
>
> This seems to be more efficient still (at least in respect of Prof.
> Grothendieck's toy example) by between 20 and 25 percent:
>
>        > a <- matrix(1:24,4)
>        > system.time(for(i in 1:1000) junk <- a / rep(a[1,], each = 4))
>        [1] 0.690 0.080 1.051 0.000 0.000
>        > system.time(for(i in 1:1000) junk <- t(t(a)/a[1,]))
>        [1] 0.520 0.120 0.647 0.000 0.000
>        > system.time(for(i in 1:10000) junk <- a / rep(a[1,], each = 4))
>        [1]  7.08  0.99 10.08  0.00  0.00
>        > system.time(for(i in 1:10000) junk <- t(t(a)/a[1,]))
>        [1] 5.530 0.940 7.856 0.000 0.000
>
>                        cheers,
>
>                                Rolf Turner
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ripley at stats.ox.ac.uk  Wed Sep  6 19:30:31 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 6 Sep 2006 18:30:31 +0100 (BST)
Subject: [R] Matrix multiplication using apply() or lappy() ?
In-Reply-To: <200609061652.k86GqLoD010185@erdos.math.unb.ca>
References: <200609061652.k86GqLoD010185@erdos.math.unb.ca>
Message-ID: <Pine.LNX.4.64.0609061825580.11591@gannet.stats.ox.ac.uk>

What version of R was this?

In 2.4.0 alpha

> a <- matrix(1:24,4)
> system.time(for(i in 1:1000) junk <- a / rep(a[1,], each = 4))
[1] 0.014 0.000 0.014 0.000 0.000
> system.time(for(i in 1:1000) junk <- t(t(a)/a[1,]))
[1] 0.057 0.000 0.058 0.000 0.000

shows a large margin the other way, which increases with bigger matrices

> a <- matrix(pi*1:100, 100, 1000)
> system.time(for(i in 1:1000) junk <- t(t(a)/a[1,]))
[1] 18.329  2.238 20.595  0.000  0.000
> system.time(for(i in 1:1000) junk <- a / rep(a[1,], each = 4))
[1] 2.589 1.021 3.610 0.000 0.000


On Wed, 6 Sep 2006, Rolf Turner wrote:

> Prof. Brian Ripley wrote:
> 
> > On Wed, 6 Sep 2006, Christos Hatzis wrote:
> > 
> > > See ?sweep
> > > 
> > > sweep(a, 2, a[1,],"/")
> > 
> > That is less efficient than
> > 
> > a/rep(a[1,], each=nrow(a))
> 
> *My* first instinct was to use
> 
> 	t(t(a)/a[1,])
> 
> (which has not heretofore been suggested).
> 
> This seems to be more efficient still (at least in respect of Prof.
> Grothendieck's toy example) by between 20 and 25 percent:
> 
> 	> a <- matrix(1:24,4)
> 	> system.time(for(i in 1:1000) junk <- a / rep(a[1,], each = 4))
> 	[1] 0.690 0.080 1.051 0.000 0.000
> 	> system.time(for(i in 1:1000) junk <- t(t(a)/a[1,]))
> 	[1] 0.520 0.120 0.647 0.000 0.000
> 	> system.time(for(i in 1:10000) junk <- a / rep(a[1,], each = 4))
> 	[1]  7.08  0.99 10.08  0.00  0.00
> 	> system.time(for(i in 1:10000) junk <- t(t(a)/a[1,]))
> 	[1] 5.530 0.940 7.856 0.000 0.000
> 
> 			cheers,
> 
> 				Rolf Turner
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From spencer.graves at pdf.com  Wed Sep  6 19:59:19 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 06 Sep 2006 10:59:19 -0700
Subject: [R] singular factor analysis
Message-ID: <44FF0C77.5030309@pdf.com>

      Are there any functions available to do a factor analysis with 
fewer observations than variables?  As long as you have more than 3 
observations, my computations suggest you have enough data to estimate a 
factor analysis covariance matrix, even though the sample covariance 
matrix is singular.  I tried the naive thing and got an error: 

 > set.seed(1)
 > X <- array(rnorm(50), dim=c(5, 10))
 > factanal(X, factors=1)
Error in solve.default(cv) : system is computationally singular: 
reciprocal condition number = 4.8982e-018

      I can write a likelihood for a multivariate normal and solve it, 
but I wondered if there is anything else available that could do this? 

      Thanks,
      Spencer Graves


From tplate at acm.org  Wed Sep  6 20:11:19 2006
From: tplate at acm.org (Tony Plate)
Date: Wed, 06 Sep 2006 12:11:19 -0600
Subject: [R] problem with putting objects in list
In-Reply-To: <44FEBF99.3060401@sun.ac.za>
References: <44FEBF99.3060401@sun.ac.za>
Message-ID: <44FF0F47.2070703@acm.org>

I suspect you are not thinking about the list and the 
subsetting/extraction operators in the right way.

A list contains a number of components.

To get a subset of the list, use the '[' operator.  The subset can 
contain zero or more components of the list, and it is a list itself. 
So, if x is a list, then x[2] is a list containing a single component.

To extract a component from the list, use the '[[' operator.  You can 
only extract one component at a time.  If you supply a vector index with 
more than one element, it will index recursively.

 > x <- list(1,2:3,letters[1:3])
 > x
[[1]]
[1] 1

[[2]]
[1] 2 3

[[3]]
[1] "a" "b" "c"

 > # a subset of the list
 > x[2:3]
[[1]]
[1] 2 3

[[2]]
[1] "a" "b" "c"

 > # a list with one component:
 > x[2]
[[1]]
[1] 2 3

 > # the second component itself
 > x[[2]]
[1] 2 3
 > # recursive indexing
 > x[[c(2,1)]]
[1] 2
 > x[[c(3,2)]]
[1] "b"
 >

Rainer M Krug wrote:
> Hi
> 
> I use the following code and it stores the results of density() in the
> list dr:
> 
> dens <- function(run) { density( positions$X[positions$run==run], bw=3,
> cut=-2 ) }
> dr <- lapply(1:5, dens)
> 
> but the results are stored in dr[[i]] and not dr[i], i.e. plot(dr[[1]])
> works, but plot([1]) doesn't.
> 
> Is there any way that I can store them in dr[i]?
> 
> Thanks a lot,
> 
> Rainer
> 
> 
>


From myotisone at gmail.com  Wed Sep  6 21:04:52 2006
From: myotisone at gmail.com (Graham Smith)
Date: Wed, 6 Sep 2006 20:04:52 +0100
Subject: [R] deleting an arow added to a graphic
Message-ID: <2c75873c0609061204h2663be8dl4737106ea5d1f298@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060906/7ff4544c/attachment.pl 

From murdoch at stats.uwo.ca  Wed Sep  6 21:33:52 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 06 Sep 2006 15:33:52 -0400
Subject: [R] deleting an arow added to a graphic
In-Reply-To: <2c75873c0609061204h2663be8dl4737106ea5d1f298@mail.gmail.com>
References: <2c75873c0609061204h2663be8dl4737106ea5d1f298@mail.gmail.com>
Message-ID: <44FF22A0.5060404@stats.uwo.ca>

On 9/6/2006 3:04 PM, Graham Smith wrote:
> I know this has got to be simple, but I have a added  an arrow to a graph
> with:
> 
> arrows(5,8,8, predict(lmfit,data.frame(x=8)), length=0.1)
> 
> but its in the wrong position, correcting it and running again adds an new
> arrow (which is what you would expect) so how do I
> 
> a) edit the existing arrow, and
> b) delete it all together
> 
> As so often seems to be the case, some of the simplist things seem also to
> be the most difficult to find the answer to.
> 

Generally "classic" graphics in R are like drawing in ink on paper:  you 
can't remove items that you've drawn there.

So the way to do what you want is to save the commands that produced the 
  entire graph, and edit them until you get them right.  Then you can 
run them and produce a new graph that's just right.

grid allows items to be removed from an existing graph, so lattice and 
ggplot inherit this nice property.  rgl (for 3d graphics) also allows 
items to be removed in a fairly inflexible way (only in the reverse 
order of the order drawn); the next release will make this more flexible.

Duncan Murdoch


From ggrothendieck at gmail.com  Wed Sep  6 21:47:12 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 6 Sep 2006 15:47:12 -0400
Subject: [R] deleting an arow added to a graphic
In-Reply-To: <2c75873c0609061204h2663be8dl4737106ea5d1f298@mail.gmail.com>
References: <2c75873c0609061204h2663be8dl4737106ea5d1f298@mail.gmail.com>
Message-ID: <971536df0609061247x17af045ak1edbe0926c3d48d7@mail.gmail.com>

This does not actually remove it but you could overwrite it with
an arrow the same color as the background and then plot a
new arrow:

x <- 1:10
plot(x ~ x)
arrows(1, 1, 2, 2)

# revise it
arrows(1, 1, 2, 2, col = "white")
arrows(2, 2, 3, 3)


On 9/6/06, Graham Smith <myotisone at gmail.com> wrote:
> I know this has got to be simple, but I have a added  an arrow to a graph
> with:
>
> arrows(5,8,8, predict(lmfit,data.frame(x=8)), length=0.1)
>
> but its in the wrong position, correcting it and running again adds an new
> arrow (which is what you would expect) so how do I
>
> a) edit the existing arrow, and
> b) delete it all together
>
> As so often seems to be the case, some of the simplist things seem also to
> be the most difficult to find the answer to.
>
> Many thanks,
>
> Graham
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From cooch17 at verizon.net  Wed Sep  6 22:20:24 2006
From: cooch17 at verizon.net (Evan Cooch)
Date: Wed, 06 Sep 2006 16:20:24 -0400
Subject: [R] continuation lines in R script files
Message-ID: <44FF2D88.5080902@verizon.net>

When I have to enter a very large matrix into the R console, I can make 
use of the continuation feature in the console to enter the matrix in 
pieces (e.g.,  on a row by row basis). So, for example, the console 
would show the "+" sign for continuation lines - something like what 
I've written below:

a=matrix(c(0,20,50,
+  0.05,0,0,
+ 0,0.1,0),
+ 3,3,byrow=T)

(obviously, for a matrix this small - 3x3 - I could enter it all on a 
single line, this is just to demonstrate)

My question is - how do you accomplish the same thing in an R script 
file? I've tried literally copying the preceding - syntax error at the 
second line. I've also tried


a=matrix(c(0,20,50,
0.05,0,0,
0,0.1,0),
3,3,byrow=T)

Again, syntax error, at the second line...

After multiple searches for 'continuation line', with no luck 
(everything I found refers to the R console, no a script file), I'll ask 
here. Basically, I want to know how to get an R script to handle a 
structure entered over multiple lines (e.g., a matrix). This is default 
behaviour in .m files in Matlab, and most other environments I've ever 
worked in (e.g., SAS looks for the ; to indicate end of a line).

Thanks in advance...


From mikem at salter-point.com  Wed Sep  6 22:24:58 2006
From: mikem at salter-point.com (Mike Meyer)
Date: Wed, 6 Sep 2006 13:24:58 -0700
Subject: [R] continuation lines in R script files
In-Reply-To: <44FF2D88.5080902@verizon.net>
References: <44FF2D88.5080902@verizon.net>
Message-ID: <918fde5a0609061324k57feecfat9cd5d06177324612@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060906/1b14862a/attachment.pl 

From joris.dewolf at cropdesign.com  Wed Sep  6 22:26:38 2006
From: joris.dewolf at cropdesign.com (Joris De Wolf)
Date: Wed, 06 Sep 2006 22:26:38 +0200
Subject: [R] continuation lines in R script files
In-Reply-To: <44FF2D88.5080902@verizon.net>
References: <44FF2D88.5080902@verizon.net>
Message-ID: <44FF2EFE.3070707@cropdesign.com>

Are your sure your second solution does not work? Try again...

Evan Cooch wrote:
> When I have to enter a very large matrix into the R console, I can make 
> use of the continuation feature in the console to enter the matrix in 
> pieces (e.g.,  on a row by row basis). So, for example, the console 
> would show the "+" sign for continuation lines - something like what 
> I've written below:
> 
> a=matrix(c(0,20,50,
> +  0.05,0,0,
> + 0,0.1,0),
> + 3,3,byrow=T)
> 
> (obviously, for a matrix this small - 3x3 - I could enter it all on a 
> single line, this is just to demonstrate)
> 
> My question is - how do you accomplish the same thing in an R script 
> file? I've tried literally copying the preceding - syntax error at the 
> second line. I've also tried
> 
> 
> a=matrix(c(0,20,50,
> 0.05,0,0,
> 0,0.1,0),
> 3,3,byrow=T)
> 
> Again, syntax error, at the second line...
> 
> After multiple searches for 'continuation line', with no luck 
> (everything I found refers to the R console, no a script file), I'll ask 
> here. Basically, I want to know how to get an R script to handle a 
> structure entered over multiple lines (e.g., a matrix). This is default 
> behaviour in .m files in Matlab, and most other environments I've ever 
> worked in (e.g., SAS looks for the ; to indicate end of a line).
> 
> Thanks in advance...
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


confidentiality notice:
The information contained in this e-mail is confidential and...{{dropped}}


From bolker at zoo.ufl.edu  Thu Sep  7 00:35:17 2006
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Wed, 6 Sep 2006 22:35:17 +0000 (UTC)
Subject: [R] About the Skew Student distribution
References: <20060906152739.49058.qmail@web26301.mail.ukl.yahoo.com>
Message-ID: <loom.20060907T001919-625@post.gmane.org>

pierre clauss <pierreclauss <at> yahoo.fr> writes:

> 
> Hello everybody,
> I need your help about the package SN and the skew student distribution. Il
will be very grateful if I have the solution.
> 
> I construct a stochastic model with a white noise not gaussian but following a
skew student distribution. I
> fit the noise on monthly data to obtain the four parameters. The question is :
how to annualize the
> parameters to use my model for simulate daily data for example ? 
> 
> If the volatility is estimated to 3 for example, I need to multiply this by
sqrt(12) to have for the parameter
> of volatility of the skew student : 3*sqrt(12)*sqrt(dt) with "dt" the time
increment parameter (1/12 for
> monthly data, 1/261 for daily data, and so on). Do I do the same thing (and
what is the multiplicative factor
> ?) for the parameters of asymmetry and the degree of freedom ?
> 

  I'm not sure about your application, but I'll take a crack at it.
It sounds like you've got a continuous-time stochastic process (you
don't say explicitly).  For a Brownian motion, you would just do what
you suggest -- scale the variance by sqrt(dt) (I don't see exactly
why daily data have dt=1/261 -- although 365*5/7 = 261, so I guess
you're counting weekdays (trading days??) only).  Unfortunately, it's
not nearly as transparent (to me) what stochastic differential equation
would lead to aggregated data that were skew-Student.  The review
paper on Azzalini (SN's author)'s web site (
http://azzalini.stat.unipd.it/SN/review-web.ps ) cites some papers in
computational finance, which I'm guessing is your area -- your best
bet is probably to go back to those papers and see if they deal
with the effects of temporal aggregation.

  VERY crudely, you can just experiment with this yourself
by simulating values from a particular skew-Student distribution,
aggregating them, and then looking at the properties of the
resulting distribution -- *do* the asymmetry and df change?
(I bet they do -- in some sense temporal aggregation must lead
to a distribution that is "more normal", larger df and smaller
skew -- but reversing this could be quite ugly).

  good luck
   Ben Bolker
finance


From sltucker15 at yahoo.com  Thu Sep  7 01:39:38 2006
From: sltucker15 at yahoo.com (Sarah Tucker)
Date: Wed, 6 Sep 2006 16:39:38 -0700 (PDT)
Subject: [R] How to get multiple partial matches?
Message-ID: <20060906233938.89854.qmail@web38504.mail.mud.yahoo.com>

Hi, 

I'm very new to R, and am not at all a software
programmer of any sort.    I appreciate any help you
may have.  I have figured out how to get my data into
a dataframe and order it alphabetically according to a
particular column.  Now, I would like to seperate out
certain rows based on partial character matches.  Here
is an (extremely) abreviated example of my data set

        Probe Ch1 Median - B Ch1 Mean - B
72     5S_F_1            501          567
7700   5S_F_2            338          611
7517   5S_F_3            412          467
10687  5S_F_4            380          428
4870   5S_F_5            315          368
6035   5S_F_6            300          359
3826   5S_F_7            350          386
8754   5S_F_8            450          473
6399   5S_F_9            439          494
749   5S_F_10            334          384

I would like to be able to select out all rows with,
for example, "5S_F_" in the Probe column (there are
non-"5S_F_" containing values in the real, larger data
set).

I think pmatch does this for instances where there is
only 1 match, but I would like to recover all the
matches.  I have tried to use charmatch, match,
pmatch, agrep and grep for this purpose, but with no
luck.

When I grep for "5S_F_" with value = T, I get
"character(0)"
Adding wildcards (either "*" or ".") does not change
this outcome.

I thought maybe the underscores were messing it up, so
I tried to grep "5S*" with value = T, and I get a long
list of numbers back

[1] "55"   "95"   "56"   "57"   "58"   "59"   "65"  
"75"   "85"   "105" 
  [11] "115"  "125"  "135"  "5"    "5"    "5"    "5"  
 "5"    "5"    "5"  

These numbers make no sense to me.  They don't seem to
correlate with where the "5S"'s occur in the
dataframe, and they don't look like any values in the
Probe column (there are no numeric vaules in the Probe
column, just strings of character digit combinations).

How can I select out all the rows with the same
partial character match?


From jholtman at gmail.com  Thu Sep  7 02:01:38 2006
From: jholtman at gmail.com (jim holtman)
Date: Wed, 6 Sep 2006 20:01:38 -0400
Subject: [R] How to get multiple partial matches?
In-Reply-To: <20060906233938.89854.qmail@web38504.mail.mud.yahoo.com>
References: <20060906233938.89854.qmail@web38504.mail.mud.yahoo.com>
Message-ID: <644e1f320609061701h71c4f8c3x185987c87193e693@mail.gmail.com>

Try using 'grep' and regular expressions:

> x <- "72     5S_F_1            501          567
+ 7700   5S_F_2            338          611
+ 7517   5S_F_3            412          467
+ 10687  5S_F_4            380          428
+ 4870   5S_F_5            315          368
+ 6035   5S_F_6            300          359
+ 3826   5S_F_7            350          386
+ 8754   5S_F_8            450          473
+ 6399   5S_F_9            439          494
+ 749   5S_F_10            334          384
+ "
> df <- read.table(textConnection(x))
> df
      V1      V2  V3  V4
1     72  5S_F_1 501 567
2   7700  5S_F_2 338 611
3   7517  5S_F_3 412 467
4  10687  5S_F_4 380 428
5   4870  5S_F_5 315 368
6   6035  5S_F_6 300 359
7   3826  5S_F_7 350 386
8   8754  5S_F_8 450 473
9   6399  5S_F_9 439 494
10   749 5S_F_10 334 384
> # select only ones with '5S_F_1'
> df[grep('5S_F_1', as.character(df$V2)),]
    V1      V2  V3  V4
1   72  5S_F_1 501 567
10 749 5S_F_10 334 384
>
>


On 9/6/06, Sarah Tucker <sltucker15 at yahoo.com> wrote:
> Hi,
>
> I'm very new to R, and am not at all a software
> programmer of any sort.    I appreciate any help you
> may have.  I have figured out how to get my data into
> a dataframe and order it alphabetically according to a
> particular column.  Now, I would like to seperate out
> certain rows based on partial character matches.  Here
> is an (extremely) abreviated example of my data set
>
>        Probe Ch1 Median - B Ch1 Mean - B
> 72     5S_F_1            501          567
> 7700   5S_F_2            338          611
> 7517   5S_F_3            412          467
> 10687  5S_F_4            380          428
> 4870   5S_F_5            315          368
> 6035   5S_F_6            300          359
> 3826   5S_F_7            350          386
> 8754   5S_F_8            450          473
> 6399   5S_F_9            439          494
> 749   5S_F_10            334          384
>
> I would like to be able to select out all rows with,
> for example, "5S_F_" in the Probe column (there are
> non-"5S_F_" containing values in the real, larger data
> set).
>
> I think pmatch does this for instances where there is
> only 1 match, but I would like to recover all the
> matches.  I have tried to use charmatch, match,
> pmatch, agrep and grep for this purpose, but with no
> luck.
>
> When I grep for "5S_F_" with value = T, I get
> "character(0)"
> Adding wildcards (either "*" or ".") does not change
> this outcome.
>
> I thought maybe the underscores were messing it up, so
> I tried to grep "5S*" with value = T, and I get a long
> list of numbers back
>
> [1] "55"   "95"   "56"   "57"   "58"   "59"   "65"
> "75"   "85"   "105"
>  [11] "115"  "125"  "135"  "5"    "5"    "5"    "5"
>  "5"    "5"    "5"
>
> These numbers make no sense to me.  They don't seem to
> correlate with where the "5S"'s occur in the
> dataframe, and they don't look like any values in the
> Probe column (there are no numeric vaules in the Probe
> column, just strings of character digit combinations).
>
> How can I select out all the rows with the same
> partial character match?
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From bates at stat.wisc.edu  Thu Sep  7 02:12:25 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 6 Sep 2006 19:12:25 -0500
Subject: [R] Help on estimated variance in lme4
In-Reply-To: <001201c6d1d3$3f507560$36cacb84@lemaitrej>
References: <001201c6d1d3$3f507560$36cacb84@lemaitrej>
Message-ID: <40e66e0b0609061712jcb80f32ve71ad9736389c05b@mail.gmail.com>

Could you try this model fit again adding control = list(usePQL =
FALSE, msVerbose=TRUE) to the argument list of the call to lmer?  By
default PQL iterations are used at the beginning of a generalized
linear mixed model fit followed by optimization of the Laplace
approximation to the log-likelihood when method = "Laplace".
Sometimes the PQL iterations do more harm than good and you do better
going straight to the optimization of the Laplace approximation.

On 9/6/06, jerome lemaitre <jerome.lemaitre.1 at ulaval.ca> wrote:
> Dear all,
>
> I get an error message when I run my model and I am not sure what to do
> about it.
>
> I try to determine what factors influence the survival of voles. I use a
> mixed-model because I have several voles per site (varying from 2 to 19
> voles).
>
> Here is the model:
> ###
> fm5 <-lmer(data=cdrgsaou2,
> alive~factor(pacut)+factor(agecamp)+factor(sex)+ResCondCorp+(1|factor(cdrgsa
> ou2$ids)),
>         family=binomial,
>         method="Laplace",
>         )
> ###
> Description of variables
> Alive: 0 or 1; dead or alive
> pacut: 0 or 1; presence of parasites
> agecamp: a or j; adult or juvenile
> sex: m or f; male or female
> ResCondCorp: body condition, continuous;
> cdrgsaou2$ids: name of the site.
>
>
> Here is the output:
>
> ###
> Generalized linear mixed model fit using Laplace
> Formula: alive ~ factor(pacut) + factor(agecamp) + factor(sex) + ResCondCorp
> +      (1 | factor(cdrgsaou2$ids))
>    Data: cdrgsaou2
>  Family: binomial(logit link)
>       AIC      BIC    logLik deviance
>  305.7418 328.7331 -146.8709 293.7418
> Random effects:
>  Groups                Name        Variance Std.Dev.
>  factor(cdrgsaou2$ids) (Intercept) 0.034382 0.18542
> number of obs: 341, groups: factor(cdrgsaou2$ids), 36
>
> Estimated scale (compare to 1)  2.174681
>
> Fixed effects:
>                   Estimate Std. Error z value  Pr(>|z|)
> (Intercept)       0.971458   0.250951  3.8711 0.0001083 ***
> factor(pacut)1   -0.831888   0.358583 -2.3199 0.0203447 *
> factor(agecamp)j -1.294236   0.330638 -3.9144 9.065e-05 ***
> factor(sex)m      0.581713   0.296229  1.9637 0.0495616 *
> ResCondCorp      -0.176251   0.020263 -8.6982 < 2.2e-16 ***
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>
> Correlation of Fixed Effects:
>             (Intr) fct()1 fctr(g) fctr(s)
> factr(pct)1 -0.334
> fctr(gcmp)j -0.417  0.066
> factor(sx)m -0.505 -0.002 -0.173
> ResCondCorp -0.309 -0.010  0.302  -0.032
> ###
>
> Here is the error message:
>
> ###
> Warning message:
> Estimated variance for factor 'factor(cdrgsaou2$ids)' is effectively zero
>  in: LMEopt(x = mer, value = cv)
> ###
>
> Thank you very much by advance for any help.
>
>
>
> J?r?me Lema?tre
>
>
> Ph.D. student
> Silviculture-wildlife research chair in irregular boreal forests
> & D?partment of biology,
> Faculty of Sciences and Engineering
> Alexandre-Vachon building
> University Laval
> Quebec, QC  G1K 7P4
> Phone : (418) 656-2131 poste 2917
> Office : VCH-2044
> Email: jerome.lemaitre.1 at ulaval.ca
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From bates at stat.wisc.edu  Thu Sep  7 02:23:54 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 6 Sep 2006 19:23:54 -0500
Subject: [R] Problem with Variance Components (and general glmm
	confusion)
In-Reply-To: <01b001c6d075$86fb3940$5401a8c0@Toby>
References: <01b001c6d075$86fb3940$5401a8c0@Toby>
Message-ID: <40e66e0b0609061723l494b9990n602939aa2f6ac152@mail.gmail.com>

On 9/4/06, Toby Gardner <t.gardner at uea.ac.uk> wrote:
> Dear list,
>
> I am having some problems with extracting Variance Components from a random-effects model:
>
> I am running a simple random-effects model using lme:
>
> model<-lme(y~1,random=~1|groupA/groupB)
>
> which returns the output for the StdDev of the Random effects, and model AIC etc as expected.
>
> Until yesterday I was using R v. 2.0, and had no problem in calling the variance components of the above model using VarCorr(model), together with their 95% confidence intervals using intervals() - although for some response variables a call to intervals() returns the error: Cannot get confidence intervals on var-cov components: Non-positive definite approximate variance-covariance.
>
> I have now installed R v. 2.3.1 and am now experiencing odd behaviour with VarCorr(lme.object), with an error message typically being returned:
>
> Error in VarCorr(model) : no direct or inherited method for function 'VarCorr' for this call
>
> Is this known to happen? For instance could it be due to the subsequent loading of new packages? (lme4 for instance?).

Yes.  Avoid loading lme4 and nlme simultaneously.

>
> To get around this problem I have tried running the same model using lmer:
>
> model2<-lmer(y~1 + (1|groupA) + (1|groupB))

In recent versions of lme4 you can use the specification

model2 <- lmer(y ~ 1 + (1|groupA/groupB))

Your version may be correct or not.  It depends on what the distinct
levels of groupB correspond to.  The version with the / is more
reliable.

>
> Should this not produce the same model? The variance components are very similar but not identical, making me think that I am doing something wrong. I am also correct in thinking that intervals() does not work with lmer? I get: Error in intervals(model2) : no applicable method for "intervals"

That is correct.  Currently there is no intervals method for an lmer
model.  You can use mcmcsamp to get a Markov chain Monte Carlo sample
to which you can apply HPDinterval from the "coda" package.  However,
these are stochastic intervals so it is best to try on a couple of
chains to check on the reproducibility or the intervals.

>
> GLMM
>
> I have a general application question - please excuse my ignorance, I am relatively new to this and trying to find a way through the maze.  In short I need to compile generalized linear mixed models both for (a) Poisson data and (b) binonial data incorporating a two nested random factors, and I need to be able to extract AIC values as I am taking an information-theoretic approach to model selection.  Prior to sending an email to the list I have spent quite a few days reading the background on a number of functions, all of which offer potential for this; glmmML, glmmPQL, lmer, and glmmADMB.  I can understand that glmmPQL is unsuitable because there is no way of knowing the maximised likelihood, but is there much difference between the remaining three options? I have seen simulation comparisons published on this list between glmmADMB and glmmPQL and lmer, but it seems these are before the latest release of lmer, and also they do not evaluate glmmML.  To a newcomer this myriad !
>  of options is bewildering, can anyone offer advice as to the most robust approach?

Goran can correct me if I am wrong but I don't believe that glmmML can
be used with multiple levels of random effects.

I'm not sure what the status of glmmADMB is these days.  There was
some controversy regarding the license applied to some of that code a
while back.  I don't know if it has been resolved to everyone's
satisfaction.

When using lmer I would suggest using method = "Laplace" and perhaps
control = list(usePQL = FALSE, msVerbose = 1) as I mentioned in
another reply to the list a few minutes ago.

Let us know how it works out.

>
> Many thanks for your time and patience,
>
> Toby Gardner
>
> School of Environmental Sciences
> University of East Anglia
> Norwich, NR4 7TJ
> United Kingdom
> Email: t.gardner at uea.ac.uk
> Website: www.uea.ac.uk/~e387495
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mcpung at gmail.com  Thu Sep  7 03:32:23 2006
From: mcpung at gmail.com (Murray Pung)
Date: Thu, 7 Sep 2006 11:32:23 +1000
Subject: [R] graphics - joining repeated measures with a line
Message-ID: <8d6f66050609061832k7e61d8d9tcffd98f7e9ca09c3@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060907/9ebf4fcd/attachment.pl 

From ggrothendieck at gmail.com  Thu Sep  7 03:55:35 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 6 Sep 2006 21:55:35 -0400
Subject: [R] graphics - joining repeated measures with a line
In-Reply-To: <8d6f66050609061832k7e61d8d9tcffd98f7e9ca09c3@mail.gmail.com>
References: <8d6f66050609061832k7e61d8d9tcffd98f7e9ca09c3@mail.gmail.com>
Message-ID: <971536df0609061855jb803196md9cfcc1987bed7ce@mail.gmail.com>

Make each pair of points a separate group using group= and specify
that both points and lines be used via type = "b".  Also set the
symbols in par.settings= so that they are accessed by both
the main plot and the legend:

xyplot(var ~ visit, group = symbols[patient], type = "b",
   auto.key = list(space = "right"),
   par.settings = list(superpose.symbol = list(pch = symbols)))


On 9/6/06, Murray Pung <mcpung at gmail.com> wrote:
> I would like to join repeated measures for patients across two visits using
> a line. The program below uses symbols to represent each patient. Basically,
> I would like to join each pair of symbols.
>
>
>
> library(lattice)
>
> patient <- c(1,2,3,4,5,6,7,8,9,1,2,3,4,5,6,7,8,9)
> var <-
> c(826,119,168,90,572,323,122,10,42,900,250,180,120,650,400,130,12,33)
> visit <- c(1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2)
> symbols <- c(1,2,3,4,5,6,7,8,9)
>
> xyplot(var ~ visit, pch = symbols[patient], key = list(points = list(pch =
> symbols), space = list("right"),text =
> list(c("1","2","3","4","5","6","7","8","9"))))
>
> # grid.lines(x = visit,y = var,draw = TRUE) ??
>
> I am thinking I may need to use a function that joins coordinates (for
> example join (1,826) with (2,900)), but am hoping there may be a better way.
>
>
>
> Thanks for any help.
>
> Murray
>
>
> --
> Murray Pung
> Statistician, Datapharm Australia Pty Ltd
> 0404 273 283
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Thu Sep  7 04:01:30 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 6 Sep 2006 22:01:30 -0400
Subject: [R] graphics - joining repeated measures with a line
In-Reply-To: <971536df0609061855jb803196md9cfcc1987bed7ce@mail.gmail.com>
References: <8d6f66050609061832k7e61d8d9tcffd98f7e9ca09c3@mail.gmail.com>
	<971536df0609061855jb803196md9cfcc1987bed7ce@mail.gmail.com>
Message-ID: <971536df0609061901s2136affbm9f03e685f81a6edb@mail.gmail.com>

Just one correction (although in this case it does not change the
output) -- use group = patient rather than group = symbol[patient]:

xyplot(var ~ visit, group = patient, type = "b", auto.key = list(space
= "right"),
   par.settings = list(superpose.symbol = list(pch = symbols)))


On 9/6/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Make each pair of points a separate group using group= and specify
> that both points and lines be used via type = "b".  Also set the
> symbols in par.settings= so that they are accessed by both
> the main plot and the legend:
>
> xyplot(var ~ visit, group = symbols[patient], type = "b",
>   auto.key = list(space = "right"),
>   par.settings = list(superpose.symbol = list(pch = symbols)))
>
>
> On 9/6/06, Murray Pung <mcpung at gmail.com> wrote:
> > I would like to join repeated measures for patients across two visits using
> > a line. The program below uses symbols to represent each patient. Basically,
> > I would like to join each pair of symbols.
> >
> >
> >
> > library(lattice)
> >
> > patient <- c(1,2,3,4,5,6,7,8,9,1,2,3,4,5,6,7,8,9)
> > var <-
> > c(826,119,168,90,572,323,122,10,42,900,250,180,120,650,400,130,12,33)
> > visit <- c(1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2)
> > symbols <- c(1,2,3,4,5,6,7,8,9)
> >
> > xyplot(var ~ visit, pch = symbols[patient], key = list(points = list(pch =
> > symbols), space = list("right"),text =
> > list(c("1","2","3","4","5","6","7","8","9"))))
> >
> > # grid.lines(x = visit,y = var,draw = TRUE) ??
> >
> > I am thinking I may need to use a function that joins coordinates (for
> > example join (1,826) with (2,900)), but am hoping there may be a better way.
> >
> >
> >
> > Thanks for any help.
> >
> > Murray
> >
> >
> > --
> > Murray Pung
> > Statistician, Datapharm Australia Pty Ltd
> > 0404 273 283
> >
> >        [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From hannah.murdoch at student.adelaide.edu.au  Thu Sep  7 04:20:25 2006
From: hannah.murdoch at student.adelaide.edu.au (Hannah Murdoch)
Date: Thu,  7 Sep 2006 11:50:25 +0930
Subject: [R] stratified poisson regression
Message-ID: <1157595625.44ff81e9cd7ff@webmail.adelaide.edu.au>

Hello,

I'm fitting poisson regression to mortality data and wish to stratify by age.
Is there any way to perform this stratification and use the glm function in R?

Thanks,
Hannah Murdoch


From spencer.graves at pdf.com  Thu Sep  7 08:10:17 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 06 Sep 2006 23:10:17 -0700
Subject: [R] my error with augPred
In-Reply-To: <44FC2F21.3661.141D7F7@localhost>
References: <44EDAC73.16458.584068@localhost> <44FC2F21.3661.141D7F7@localhost>
Message-ID: <44FFB7C9.7060400@pdf.com>

      Thank you for providing such a complete, self contained example.  
I found that 'predict.nlme' does not like a factor in the 'fixed' 
argument as you used it, "fixed=list(Asym~x1, R0+lrc~1)".  To see this, 
I added 'x1.' as a numeric version of the factor 'x1' and reran it 
successfully: 

fm2.<-update(fm1, fixed=list(Asym~x1., R0+lrc~1), start=c(103,0,-8.5,-3))
aP2. <- augPred(fm2.)
plot(aP2.)

      Unfortunately, it looks like this work-around won't help you with 
your original problem, because there, the counterpart to 'x1' is an 
ordered factor with more than 2 levels. 

      The error message refers to 'predict.nlme'.  I know no reason why 
'predict.nlme' shouldn't work with a factor with more than 2 levels in 
this context.  If it were my problem and it was sufficiently important, 
I would make a local copy of 'predict.nlme' as follows: 

      predict.nlme <- getAnywhere("predict.nlme")

      Then I'd use 'debug(nlme:::predict.nlme)' to walk through the 
problem example line by line until I figured out what I had to change to 
make this work. 

      I hesitate to use the "B" word, but I think it might be 
appropriate to file a bug report on this;  perhaps someone else will do 
that.  

      I'm sorry I couldn't solve your original problem.  With luck, 
someone else will convert this example into a fix to the code. 
      Spencer Graves
     
Petr Pikal wrote:
> Hallo
>
> thank you for your response. I am not sure but maybe fixed effects 
> cannot be set to be influenced by a factor to be able to use augPred.
>
> lob<-Loblolly[Loblolly$Seed!=321,]
> set.seed(1)
> lob<-data.frame(lob, x1=sample(letters[1:3], replace=T)) # add a 
> #factor
> lob<-groupedData(height~age|Seed, data=lob)
> fm1 <- nlme(height ~ SSasymp(age, Asym, R0, lrc),
>             data = lob,
>             fixed = Asym + R0 + lrc ~ 1,
>             random = Asym ~ 1,
>             start = c(Asym = 103, R0 = -8.5, lrc = -3.3))
>
> fm2<-update(fm1, fixed=list(Asym~x1, R0+lrc~1), start=c(103,0,-8.5,-3))
>                                              ^^^^^^^
> and
>
> plot(augPred(fm2))
>
> Throws an error.
> So it is not possible to use augPred with such constructions.
>
> Best regards.
> Petr Pikal
>
> On 2 Sep 2006 at 17:58, Spencer Graves wrote:
>
> Date sent:      	Sat, 02 Sep 2006 17:58:05 -0700
> From:           	Spencer Graves <spencer.graves at pdf.com>
> To:             	Petr Pikal <petr.pikal at precheza.cz>
> Copies to:      	r-help at stat.math.ethz.ch
> Subject:        	Re: [R] my error with augPred
>
>   
>> <comments in line> 
>>
>> Petr Pikal wrote:
>>     
>>> Dear all
>>>
>>> I try to refine my nlme models and with partial success. The model
>>> is refined and fitted (using Pinheiro/Bates book as a tutorial) but
>>> when I try to plot
>>>
>>> plot(augPred(fit4))
>>>
>>> I obtain
>>> Error in predict.nlme(object, value[1:(nrow(value)/nL), , drop =
>>> FALSE],  : 
>>>         Levels (0,3.5],(3.5,5],(5,7],(7,Inf] not allowed for 
>>> vykon.fac
>>>   
>>>
>>> Is it due to the fact that I have unbalanced design with not all
>>> levels of vykon.fac present in all levels of other explanatory
>>> factor variable?
>>>   
>>>       
>> I don't know, but I'm skeptical. 
>>     
>>> I try to repeat 8.19 fig which is OK until I try:
>>>
>>> fit4 <- update(fit2, fixed = list(A+B~1,xmid~vykon.fac, scal~1), 
>>> start = c(57, 100, 700, rep(0,3), 13))
>>>
>>> I know I should provide an example but maybe somebody will be clever
>>> enough to point me to an explanation without it.
>>>   
>>>       
>> I'm not. 
>>
>> To answer these questions without an example from you, I'd have to
>> make up my own example and try to see if I could replicate the error
>> messages you report, and I'm not sufficiently concerned about this
>> right now to do that. 
>>
>> Have you tried taking an example from the book and deleting certain
>> rows from the data to see if you can force it to reproduce your error?
>>
>>
>> Alternatively, have you tried using 'debug' to trace through the code
>> line by line until you learn enough of what it's doing to answer your
>> question? 
>>
>> Spencer Graves
>>     
>>> nlme version 3.1-75
>>> SSfpl model
>>> R 2.4.0dev (but is the same in 2.3.1), W2000.
>>>
>>> Thank you
>>> Best regards.
>>>
>>> Petr PikalPetr Pikal
>>> petr.pikal at precheza.cz
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html and provide commented,
>>> minimal, self-contained, reproducible code.
>>>
>>>       
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html and provide commented,
>> minimal, self-contained, reproducible code.
>>     
>
> Petr Pikal
> petr.pikal at precheza.cz
>
>


From adik at ilovebacon.org  Thu Sep  7 08:12:52 2006
From: adik at ilovebacon.org (Adam D. I. Kramer)
Date: Wed, 6 Sep 2006 23:12:52 -0700 (PDT)
Subject: [R] Alternatives to merge for large data sets?
In-Reply-To: <mailman.13.1157536804.28705.r-help@stat.math.ethz.ch>
References: <mailman.13.1157536804.28705.r-help@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.64.0609062304380.4867@parser.ilovebacon.org>

Hello,

I am trying to merge two very large data sets, via

pubbounds.prof <-
merge(x=pubbounds,y=prof,by.x="user",by.y="userid",all=TRUE,sort=FALSE)

which gives me an error of

Error: cannot allocate vector of size 2962 Kb

I am reasonably sure that this is correct syntax.

The trouble is that pubbounds and prof are large; they are data frames which
take up 70M and 11M respectively when saved as .Rdata files.

I understand from various archive searches that "merge can't handle that,"
because merge takes n^2 memory, which I do not have.

My question is whether there is an alternative to merge which would carry
out the process in a slower, iterative manner...or if I should just bite the
bullet, write.table, and use a perl script to do the job.

Thankful as always,
Adam D. I. Kramer


From gyadav at ccilindia.co.in  Wed Sep  6 10:36:08 2006
From: gyadav at ccilindia.co.in (gyadav at ccilindia.co.in)
Date: Wed, 6 Sep 2006 14:06:08 +0530
Subject: [R] how to create time series object
In-Reply-To: <001201c6d186$da1bddc0$0f1e0b0a@3med.klinik.unimainz.de>
Message-ID: <OF9B469CA2.5BAC0EB0-ON652571E1.002F0F67-652571E2.002F6475@ccilindia.co.in>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060906/89ea7e2a/attachment.pl 

From Max.Kuhn at pfizer.com  Wed Sep  6 21:03:35 2006
From: Max.Kuhn at pfizer.com (Kuhn, Max)
Date: Wed, 6 Sep 2006 15:03:35 -0400
Subject: [R] [R-pkgs] odfWeave Version 0.4.4
Message-ID: <71257D09F114DA4A8E134DEAC70F25D3060D991B@groamrexm03.amer.pfizer.com>

Version 0.4.4 of odfWeave is available from CRAN. A Windows binary
should be available shortly.

This version requires base R version 2.3.1 or greater.

Changes from the last version include

  - Non-English character sets are handled better. For example, Chinese
characters can be included in R code. See the file "testCases.odt" in
the examples directory for an example

  - Image specifications, such as format and size, have been moved out
of odfWeaveControl. They are now controlled by the functions
getImageDefs and setImageDefs. This change allows the user to easily
modify the image properties in code chunks so that figures can have
different sizes or types throughout the document.

  - When odfWeave is invoked, a check for a zip program is done and a
more meaningful error is reported. This should help users better
understand the odfWeave software dependencies. 

  - A new XML parser was written so that users no longer need to turn
off the "size optimization" feature in OpenOffice. 

  - Three bugs were fixed:

    o If the user specified a relative path to the source file, an error
occurred

    o Fonts contained in the style definitions are automatically
registered in the ODF document. Previously, fonts that specified using
setStyleDefs but were not used in the document were ignored. Now, the
fonts found using getStyleDefs() at the start of odfWeave execution are
added to the document.

    o A new function, odfTmpDir, is now used to set the path to the
working directory. A new directory is created in the location of
tempdir().

As always, please send any comments, suggestions or bug reports to
max.kuhn at pfizer.com.

Max
----------------------------------------------------------------------
LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From ripley at stats.ox.ac.uk  Thu Sep  7 10:57:58 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 7 Sep 2006 09:57:58 +0100 (BST)
Subject: [R] Alternatives to merge for large data sets?
In-Reply-To: <Pine.LNX.4.64.0609062304380.4867@parser.ilovebacon.org>
References: <mailman.13.1157536804.28705.r-help@stat.math.ethz.ch>
	<Pine.LNX.4.64.0609062304380.4867@parser.ilovebacon.org>
Message-ID: <Pine.LNX.4.64.0609070954560.10893@gannet.stats.ox.ac.uk>

Which version of R?

Please try 2.4.0 alpha, as it has a different and more efficient 
algorithm for the case of 1-1 matches.

On Wed, 6 Sep 2006, Adam D. I. Kramer wrote:

> Hello,
> 
> I am trying to merge two very large data sets, via
> 
> pubbounds.prof <-
> merge(x=pubbounds,y=prof,by.x="user",by.y="userid",all=TRUE,sort=FALSE)
> 
> which gives me an error of
> 
> Error: cannot allocate vector of size 2962 Kb
> 
> I am reasonably sure that this is correct syntax.
> 
> The trouble is that pubbounds and prof are large; they are data frames which
> take up 70M and 11M respectively when saved as .Rdata files.
> 
> I understand from various archive searches that "merge can't handle that,"
> because merge takes n^2 memory, which I do not have.

Not really true (it has been changed since those days).  Of course, if you 
have multiple matches it must do so.

> My question is whether there is an alternative to merge which would carry
> out the process in a slower, iterative manner...or if I should just bite the
> bullet, write.table, and use a perl script to do the job.
> 
> Thankful as always,
> Adam D. I. Kramer

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From pburns at pburns.seanet.com  Thu Sep  7 11:05:39 2006
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Thu, 07 Sep 2006 10:05:39 +0100
Subject: [R] singular factor analysis
In-Reply-To: <44FF0C77.5030309@pdf.com>
References: <44FF0C77.5030309@pdf.com>
Message-ID: <44FFE0E3.6040005@pburns.seanet.com>

This is a very common computation in finance.

On the public domain page of the Burns Statistics website
in the financial part is the code and R help file for
'factor.model.stat'.  Most of the complication of the code
is to deal with missing values.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Spencer Graves wrote:

>      Are there any functions available to do a factor analysis with 
>fewer observations than variables?  As long as you have more than 3 
>observations, my computations suggest you have enough data to estimate a 
>factor analysis covariance matrix, even though the sample covariance 
>matrix is singular.  I tried the naive thing and got an error: 
>
> > set.seed(1)
> > X <- array(rnorm(50), dim=c(5, 10))
> > factanal(X, factors=1)
>Error in solve.default(cv) : system is computationally singular: 
>reciprocal condition number = 4.8982e-018
>
>      I can write a likelihood for a multivariate normal and solve it, 
>but I wondered if there is anything else available that could do this? 
>
>      Thanks,
>      Spencer Graves
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>
>  
>


From cg.pettersson at vpe.slu.se  Thu Sep  7 11:35:40 2006
From: cg.pettersson at vpe.slu.se (CG Pettersson)
Date: Thu, 07 Sep 2006 11:35:40 +0200
Subject: [R] Model vs. Observed for a lme() regression fit using two
	variables
Message-ID: <44FFE7EC.7000206@vpe.slu.se>

Dear all.

R 2.3.1, W2k.

I am working with a field trial series where, for the moment, I do 
regressions using more than one covariate to explain the protein levels 
in malting barley.

To do this I use lme() and a mixed call, structured by both experiment 
(trial) and repetition in each experiment (block). Everything works 
fine, resulting in nice working linear models using two covariates. But 
how do I visualize this in an efficient and clear way?

What I want is something like the standard output from all multivariate 
tools I have worked with (Observed vs. Predicted) with the least square 
line in the middle. It is naturally possible to plot each covariate 
separate, and also to use the 3d- sqatterplot in Rcmdr to plot both at 
the same time, but I want a plain 2d plot.

Who has made a plotting method for this and where do I find it?
Or am I missing something obvious here, that this plot is easy to 
achieve without any ready made methods?

Cheers
/CG

-- 
CG Pettersson, MSci, PhD Stud.
Swedish University of Agricultural Sciences (SLU)
Dept. of Crop Production Ecology. Box 7043.
SE-750 07 UPPSALA, Sweden.
+46 18 671428, +46 70 3306685
cg.pettersson at vpe.slu.se


From A.Robinson at ms.unimelb.edu.au  Thu Sep  7 12:03:28 2006
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Thu, 7 Sep 2006 20:03:28 +1000
Subject: [R] Model vs. Observed for a lme() regression fit using
	two	variables
In-Reply-To: <44FFE7EC.7000206@vpe.slu.se>
References: <44FFE7EC.7000206@vpe.slu.se>
Message-ID: <20060907100328.GJ12212@ms.unimelb.edu.au>

Hi CG,

I think that the best pair of summary plots are 

1) the fitted values without random effects against the observed
   response variable, and

2) fitted values with random effects against the observed response
   variable.

The first plot gives a summary of the overall quality of the fixed
effects of the model, the second gives a summary of the overall
quality of the fixed effects and random effects of the model.

eg

fm1 <- lme(distance ~ age, data = Orthodont)

plot(fitted(fm1, level=0), Orthodont$distance)
abline(0, 1, col="red")

plot(fitted(fm1, level=1), Orthodont$distance)
abline(0, 1, col="red")

I hope that this helps.

Andrew

On Thu, Sep 07, 2006 at 11:35:40AM +0200, CG Pettersson wrote:
> Dear all.
> 
> R 2.3.1, W2k.
> 
> I am working with a field trial series where, for the moment, I do 
> regressions using more than one covariate to explain the protein levels 
> in malting barley.
> 
> To do this I use lme() and a mixed call, structured by both experiment 
> (trial) and repetition in each experiment (block). Everything works 
> fine, resulting in nice working linear models using two covariates. But 
> how do I visualize this in an efficient and clear way?
> 
> What I want is something like the standard output from all multivariate 
> tools I have worked with (Observed vs. Predicted) with the least square 
> line in the middle. It is naturally possible to plot each covariate 
> separate, and also to use the 3d- sqatterplot in Rcmdr to plot both at 
> the same time, but I want a plain 2d plot.
> 
> Who has made a plotting method for this and where do I find it?
> Or am I missing something obvious here, that this plot is easy to 
> achieve without any ready made methods?
> 
> Cheers
> /CG
> 
> -- 
> CG Pettersson, MSci, PhD Stud.
> Swedish University of Agricultural Sciences (SLU)
> Dept. of Crop Production Ecology. Box 7043.
> SE-750 07 UPPSALA, Sweden.
> +46 18 671428, +46 70 3306685
> cg.pettersson at vpe.slu.se
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
Email: a.robinson at ms.unimelb.edu.au         http://www.ms.unimelb.edu.au


From k00aher at student.chalmers.se  Thu Sep  7 12:46:06 2006
From: k00aher at student.chalmers.se (Ernst O Ahlberg Helgee)
Date: Thu, 7 Sep 2006 12:46:06 +0200 (CEST)
Subject: [R] legend problems in lattice
Message-ID: <Pine.LNX.4.64.0609071240470.759@gamma02.me.chalmers.se>

Hi!
Im sorry to bother you but I cant fix this.
I use the lattice function levelplot and I want the colorkey at the 
bottom, how do I get it there? I have tried changing colorkey.space and 
changing in legend but I cant get it right, plz help

btw I'd like to speceify strings to appear at the tick marks and also 
there I fail any thoughts?

cheers
Ernst


From hchen at utmem.edu  Thu Sep  7 13:18:03 2006
From: hchen at utmem.edu (Hao Chen)
Date: Thu, 7 Sep 2006 06:18:03 -0500
Subject: [R] barplot: different colors for the bar and the strips
Message-ID: <20060907111803.GA7222@utmail.utmem.edu>

Hi,

I am using barplot and would like to know if it is possible to have bars
filled with one color while use a different color for the shading lines. 

The following code colors the shading lines, leaving the bars in white:

 barplot(1:5, col=c(1:5), density=c(1:5)*5)

while the colors are applied to the bars when density is removed.

 barplot(1:5, col=c(1:5))

I did check ?barplot and found the following: 

	col: a vector of colors for the bars or bar components. 
 
 Thanks,

 Hao
 --


From sundar.dorai-raj at pdf.com  Thu Sep  7 13:26:57 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 07 Sep 2006 06:26:57 -0500
Subject: [R] legend problems in lattice
In-Reply-To: <Pine.LNX.4.64.0609071240470.759@gamma02.me.chalmers.se>
References: <Pine.LNX.4.64.0609071240470.759@gamma02.me.chalmers.se>
Message-ID: <45000201.6020406@pdf.com>



Ernst O Ahlberg Helgee wrote:
> Hi!
> Im sorry to bother you but I cant fix this.
> I use the lattice function levelplot and I want the colorkey at the 
> bottom, how do I get it there? I have tried changing colorkey.space and 
> changing in legend but I cant get it right, plz help
> 
> btw I'd like to speceify strings to appear at the tick marks and also 
> there I fail any thoughts?
> 
> cheers
> Ernst
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

Hi, Ernst,

Please read ?levelplot. Under the argument for "colorkey" you will see:

colorkey: logical specifying whether a color key is to be drawn
           alongside the plot, or a list describing the color key. The
           list may contain the following components:


           'space': location of the colorkey, can be one of '"left"',
                '"right"', '"top"' and '"bottom"'.  Defaults to
                '"right"'.


So the answer to your first question is:

levelplot(..., colorkey = list(space = "bottom"))

For your second question, use the "scale" argument. See ?xyplot for 
details. For example,

levelplot(..., scale = list(x = list(at = 1:4, labels = letters[1:4])))

HTH,

--sundar


From hstevens at muohio.edu  Thu Sep  7 13:46:24 2006
From: hstevens at muohio.edu (Martin Henry H. Stevens)
Date: Thu, 7 Sep 2006 07:46:24 -0400
Subject: [R] Conservative "ANOVA tables" in lmer
Message-ID: <1A8FE5B0-3FE3-4CA9-9224-9CACB9AF4854@muohio.edu>

Dear lmer-ers,
My thanks for all of you who are sharing your trials and tribulations  
publicly.

I was hoping to elicit some feedback on my thoughts on denominator  
degrees of freedom for F ratios in mixed models. These thoughts and  
practices result from my reading of previous postings by Doug Bates  
and others.

- I start by assuming that the appropriate denominator degrees lies  
between n - p and and n - q, where n=number of observations, p=number  
of fixed effects (rank of model matrix X), and q=rank of Z:X.
- I then conclude that good estimates of P values on the F ratios lie  
between 1 - pf(F.ratio, numDF, n-p) and 1 - pf(F.ratio, numDF, n-q).
- I further surmise that the latter of these (1 - pf(F.ratio, numDF,  
n-q)) is the more conservative estimate.

When I use these criteria and compare my "ANOVA" table to the results  
of analysis of Helmert contrasts using MCMC sample with highest  
posterior density intervals, I find that my conclusions (e.g. factor  
A, with three levels, has a "significant effect" on the response  
variable) are qualitatively the same.

Comments?

Hank


Dr. Hank Stevens, Assistant Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/~stevenmh/
http://www.muohio.edu/ecology/
http://www.muohio.edu/botany/
"E Pluribus Unum"


From TAPO at novozymes.com  Thu Sep  7 13:56:56 2006
From: TAPO at novozymes.com (TAPO (Thomas Agersten Poulsen))
Date: Thu, 7 Sep 2006 13:56:56 +0200
Subject: [R] Stacking a list of data.frames
Message-ID: <934F95E71B6C9347A873C42AE3C19619117E680A@NZT0004E.dknz.nzcorp.net>

Dear list,

I have a list of data.frames (generated by "by"), that I want to "stack" into a single data.frame.

I can do this by cbind, but only by subsetting the list explicitly like this:

	cbind(l[[1]],l[[2]],l[[3]],l[[4]])

I find this ugly and not very general.

I tried
	cbind(l)
	cbind(l[[1:4]])
but they do not give the right result.

Please help!

Best regards
Thomas
--
Thomas A Poulsen        Scientist, Ph.D.
Novozymes A/S           Protein Design / Bioinformatics
Brudelysvej 26, 1US.24  Phone: +45 44 42 27 23
DK-2880 Bagsv?rd.       Fax:   +45 44 98 02 46


From dimitris.rizopoulos at med.kuleuven.be  Thu Sep  7 14:08:44 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Thu, 7 Sep 2006 14:08:44 +0200
Subject: [R] Stacking a list of data.frames
References: <934F95E71B6C9347A873C42AE3C19619117E680A@NZT0004E.dknz.nzcorp.net>
Message-ID: <009401c6d276$5d7d8980$0540210a@www.domain>

try this:

do.call(cbind, l)


Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "TAPO (Thomas Agersten Poulsen)" <TAPO at novozymes.com>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, September 07, 2006 1:56 PM
Subject: [R] Stacking a list of data.frames


Dear list,

I have a list of data.frames (generated by "by"), that I want to 
"stack" into a single data.frame.

I can do this by cbind, but only by subsetting the list explicitly 
like this:

cbind(l[[1]],l[[2]],l[[3]],l[[4]])

I find this ugly and not very general.

I tried
cbind(l)
cbind(l[[1:4]])
but they do not give the right result.

Please help!

Best regards
Thomas
--
Thomas A Poulsen        Scientist, Ph.D.
Novozymes A/S           Protein Design / Bioinformatics
Brudelysvej 26, 1US.24  Phone: +45 44 42 27 23
DK-2880 Bagsv?rd.       Fax:   +45 44 98 02 46

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide 
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From isidora10 at yahoo.com  Thu Sep  7 14:10:02 2006
From: isidora10 at yahoo.com (isidora k)
Date: Thu, 7 Sep 2006 05:10:02 -0700 (PDT)
Subject: [R] merging tables by columns AND rows
Message-ID: <20060907121002.63543.qmail@web52115.mail.yahoo.com>

Hi everyone!
I have 100 tables of the form:
XCOORD,YCOORD,OBSERVATION
27.47500,42.52641,177
27.48788,42.52641,177
27.50075,42.52641,179
27.51362,42.52641,178
27.52650,42.52641,180
27.53937,42.52641,178
27.55225,42.52641,181
27.56512,42.52641,177
27.57800,42.52641,181
27.59087,42.52641,181
27.60375,42.52641,180
27.61662,42.52641,181
..., ..., ...
with approximately 1000000 observations for each. All
these tables have the same xcoord and ycoord and I
would like to get a table of the form
XCOORD,YCOORD,OBSERVATION1,OBSERVATION2,... 
27.47500,42.52641,177,233,...
27.48788,42.52641,177,345,...
27.50075,42.52641,179,233,...
27.51362,42.52641,178,123,...
27.52650,42.52641,180,178,...
27.53937,42.52641,178,...,...
27.55225,42.52641,181,...
27.56512,42.52641,177,...
27.57800,42.52641,181,...
27.59087,42.52641,181,...
27.60375,42.52641,180,...
27.61662,42.52641,181,...
In other words I would like to merge all the tables
taking into account the common row names of their
xcoords AND ycoords.
Is there any way to do this in R?
I would be grateful for any advice.
Many Thanks
Isidora


From Roger.Bivand at nhh.no  Thu Sep  7 14:33:21 2006
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 7 Sep 2006 14:33:21 +0200 (CEST)
Subject: [R] merging tables by columns AND rows
In-Reply-To: <20060907121002.63543.qmail@web52115.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0609071422250.7688-100000@reclus.nhh.no>

On Thu, 7 Sep 2006, isidora k wrote:

> Hi everyone!
> I have 100 tables of the form:
> XCOORD,YCOORD,OBSERVATION
> 27.47500,42.52641,177
> 27.48788,42.52641,177
> 27.50075,42.52641,179
> 27.51362,42.52641,178
> 27.52650,42.52641,180
> 27.53937,42.52641,178
> 27.55225,42.52641,181
> 27.56512,42.52641,177
> 27.57800,42.52641,181
> 27.59087,42.52641,181
> 27.60375,42.52641,180
> 27.61662,42.52641,181
> ..., ..., ...
> with approximately 1000000 observations for each. All
> these tables have the same xcoord and ycoord and I
> would like to get a table of the form
> XCOORD,YCOORD,OBSERVATION1,OBSERVATION2,... 
> 27.47500,42.52641,177,233,...
> 27.48788,42.52641,177,345,...
> 27.50075,42.52641,179,233,...
> 27.51362,42.52641,178,123,...
> 27.52650,42.52641,180,178,...
> 27.53937,42.52641,178,...,...
> 27.55225,42.52641,181,...
> 27.56512,42.52641,177,...
> 27.57800,42.52641,181,...
> 27.59087,42.52641,181,...
> 27.60375,42.52641,180,...
> 27.61662,42.52641,181,...
> In other words I would like to merge all the tables
> taking into account the common row names of their
> xcoords AND ycoords.

Your data look very much like a rectangular grid. If you had either posted
from an identifiable institution or included an informative signature,
then we'd have known which field you're in, so the following is guesswork.

If all of your data is for a full grid, with the same coordinates always
in the same order, any missing values fully represented in the data, then
reading the first data set in as a data.frame or matrix, and converting it
to a SpatialGridDataFrame object (defined in the sp contributed package)
will give you a base to start from. 

>From that you just add columns, one column for each data set, by reading
in just the data you need (for example using scan). This depends crucially
on the same grid being used each time, with the data in the same order. If
the coordinates differ between data sets, bets are off.

If these are spatial data, please consider the R-sig-geo mailing list for 
more targetted help.

> Is there any way to do this in R?
> I would be grateful for any advice.
> Many Thanks
> Isidora
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From ggrothendieck at gmail.com  Thu Sep  7 14:34:10 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 7 Sep 2006 08:34:10 -0400
Subject: [R] how to create time series object
In-Reply-To: <OF9B469CA2.5BAC0EB0-ON652571E1.002F0F67-652571E2.002F6475@ccilindia.co.in>
References: <001201c6d186$da1bddc0$0f1e0b0a@3med.klinik.unimainz.de>
	<OF9B469CA2.5BAC0EB0-ON652571E1.002F0F67-652571E2.002F6475@ccilindia.co.in>
Message-ID: <971536df0609070534x162c7a5awc083cc3cd408a61b@mail.gmail.com>

You can use the 'zoo' or 'its' packages.  For 'zoo' see the
documents listed at the end of:

http://cran.r-project.org/src/contrib/Descriptions/zoo.html

On 9/6/06, gyadav at ccilindia.co.in <gyadav at ccilindia.co.in> wrote:
>
> hi all
>
> i have date and the return series like below, but the dates are not in
> uniform intervals. Please show me the way how to create a time series in
> 'R' so that dates are also associated with the returns.
>
> thanks in advance
>
>   Sayonara With Smile & With Warm Regards :-)
>
>  G a u r a v   Y a d a v
>  Senior Executive Officer,
>  Economic Research & Surveillance Department,
>  Clearing Corporation Of India Limited.
>
>  Address: 5th, 6th, 7th Floor, Trade Wing 'C',  Kamala City, S.B. Marg,
> Mumbai - 400 013
>  Telephone(Office): - +91 022 6663 9398 ,  Mobile(Personal) (0)9821286118
>  Email(Office) :- gyadav at ccilindia.co.in ,  Email(Personal) :-
> emailtogauravyadav at gmail.com
>
>
> ============================================================================================
> DISCLAIMER AND CONFIDENTIALITY CAUTION:\ \ This message and ...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From kungfista at gmx.de  Thu Sep  7 14:35:13 2006
From: kungfista at gmx.de (Rotkiv, Rehceb)
Date: Thu, 07 Sep 2006 14:35:13 +0200
Subject: [R] Axes of a histogram
Message-ID: <1157632513.11871.16.camel@vannili-desktop>

Hello everyone,

I would be glad if you could help out an R-beginner here... I have a
vector of categorial data like this

> v <- c(1, 1, 2, 2, 2, 3, 3, 4, 4, 4)

When I do

> hist(v)

I get the x-axis of the histogram with floating point labels: 1.0, 1.5,
2.0, etc. Is it possible to tell R that the data consists of categories,
i.e. that I only want the category names (1, 2, 3, 4) on my x-axis?

Thanks in advance,
Rehceb Rotkiv


From dimitris.rizopoulos at med.kuleuven.be  Thu Sep  7 14:47:32 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Thu, 7 Sep 2006 14:47:32 +0200
Subject: [R] Axes of a histogram
References: <1157632513.11871.16.camel@vannili-desktop>
Message-ID: <00c401c6d27b$c8fbe6c0$0540210a@www.domain>

probably you're looking for a barplot, e.g.,

v <- c(1, 1, 2, 2, 2, 3, 3, 4, 4, 4)
plot(factor(v))


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Rotkiv, Rehceb" <kungfista at gmx.de>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, September 07, 2006 2:35 PM
Subject: [R] Axes of a histogram


> Hello everyone,
>
> I would be glad if you could help out an R-beginner here... I have a
> vector of categorial data like this
>
>> v <- c(1, 1, 2, 2, 2, 3, 3, 4, 4, 4)
>
> When I do
>
>> hist(v)
>
> I get the x-axis of the histogram with floating point labels: 1.0, 
> 1.5,
> 2.0, etc. Is it possible to tell R that the data consists of 
> categories,
> i.e. that I only want the category names (1, 2, 3, 4) on my x-axis?
>
> Thanks in advance,
> Rehceb Rotkiv
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From isidora10 at yahoo.com  Thu Sep  7 14:51:26 2006
From: isidora10 at yahoo.com (isidora k)
Date: Thu, 7 Sep 2006 05:51:26 -0700 (PDT)
Subject: [R] merging tables by columns AND rows
In-Reply-To: <Pine.LNX.4.44.0609071422250.7688-100000@reclus.nhh.no>
Message-ID: <20060907125127.82474.qmail@web52101.mail.yahoo.com>

Some of the coordinates might not match and also I do
not have the same number of observations in every
table but I want to get only the common ones back.
This is where it gets tricky!I have tried merge, scan
and every joining function I could find but nothing
seems to do what I want.
the R-sig-geo mailing list sounds like a good idea!
Thank you!


From MSchwartz at mn.rr.com  Thu Sep  7 14:54:05 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Thu, 07 Sep 2006 07:54:05 -0500
Subject: [R] barplot: different colors for the bar and the strips
In-Reply-To: <20060907111803.GA7222@utmail.utmem.edu>
References: <20060907111803.GA7222@utmail.utmem.edu>
Message-ID: <1157633645.5418.9.camel@localhost.localdomain>

On Thu, 2006-09-07 at 06:18 -0500, Hao Chen wrote:
> Hi,
> 
> I am using barplot and would like to know if it is possible to have bars
> filled with one color while use a different color for the shading lines. 
> 
> The following code colors the shading lines, leaving the bars in white:
> 
>  barplot(1:5, col=c(1:5), density=c(1:5)*5)
> 
> while the colors are applied to the bars when density is removed.
> 
>  barplot(1:5, col=c(1:5))
> 
> I did check ?barplot and found the following: 
> 
> 	col: a vector of colors for the bars or bar components. 
>  
>  Thanks,
> 
>  Hao

Note the key word 'or' in the description of the 'col' argument.

You need to make two separate calls to barplot(). The first using the
fill colors, then the second using the shading lines AND setting 'add =
TRUE', so that the second plot overwrites the first without clearing the
plot device.

 barplot(1:5, col=c(1:5))

 barplot(1:5, col = "black", density=c(1:5), add = TRUE)

Just be sure that any other arguments, such as axis limits, are
identical between the two calls.

HTH,

Marc Schwartz


From MSchwartz at mn.rr.com  Thu Sep  7 14:57:07 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Thu, 07 Sep 2006 07:57:07 -0500
Subject: [R] Axes of a histogram
In-Reply-To: <1157632513.11871.16.camel@vannili-desktop>
References: <1157632513.11871.16.camel@vannili-desktop>
Message-ID: <1157633827.5418.13.camel@localhost.localdomain>

On Thu, 2006-09-07 at 14:35 +0200, Rotkiv, Rehceb wrote:
> Hello everyone,
> 
> I would be glad if you could help out an R-beginner here... I have a
> vector of categorial data like this
> 
> > v <- c(1, 1, 2, 2, 2, 3, 3, 4, 4, 4)
> 
> When I do
> 
> > hist(v)
> 
> I get the x-axis of the histogram with floating point labels: 1.0, 1.5,
> 2.0, etc. Is it possible to tell R that the data consists of categories,
> i.e. that I only want the category names (1, 2, 3, 4) on my x-axis?
> 
> Thanks in advance,
> Rehceb Rotkiv

You don't want a histogram, but a barplot:

  barplot(table(v))

See ?barplot and ?table

HTH,

Marc Schwartz


From bates at stat.wisc.edu  Thu Sep  7 14:59:58 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 7 Sep 2006 07:59:58 -0500
Subject: [R] Conservative "ANOVA tables" in lmer
In-Reply-To: <1A8FE5B0-3FE3-4CA9-9224-9CACB9AF4854@muohio.edu>
References: <1A8FE5B0-3FE3-4CA9-9224-9CACB9AF4854@muohio.edu>
Message-ID: <40e66e0b0609070559u656463c1l52a60eda9a095237@mail.gmail.com>

Thanks for your summary, Hank.

On 9/7/06, Martin Henry H. Stevens <hstevens at muohio.edu> wrote:
> Dear lmer-ers,
> My thanks for all of you who are sharing your trials and tribulations
> publicly.

> I was hoping to elicit some feedback on my thoughts on denominator
> degrees of freedom for F ratios in mixed models. These thoughts and
> practices result from my reading of previous postings by Doug Bates
> and others.

> - I start by assuming that the appropriate denominator degrees lies
> between n - p and and n - q, where n=number of observations, p=number
> of fixed effects (rank of model matrix X), and q=rank of Z:X.

I agree with this but the opinion is by no means universal.  Initially
I misread the statement because I usually write the number of columns
of Z as q.

It is not easy to assess rank of Z:X numerically.  In many cases one
can reason what it should be from the form of the model but a general
procedure to assess the rank of a matrix, especially a sparse matrix,
is difficult.

An alternative which can be easily calculated is n - t where t is the
trace of the 'hat matrix'.  The function 'hatTrace' applied to a
fitted lmer model evaluates this trace (conditional on the estimates
of the relative variances of the random effects).

> - I then conclude that good estimates of P values on the F ratios lie
> between 1 - pf(F.ratio, numDF, n-p) and 1 - pf(F.ratio, numDF, n-q).
> - I further surmise that the latter of these (1 - pf(F.ratio, numDF,
> n-q)) is the more conservative estimate.
>
> When I use these criteria and compare my "ANOVA" table to the results
> of analysis of Helmert contrasts using MCMC sample with highest
> posterior density intervals, I find that my conclusions (e.g. factor
> A, with three levels, has a "significant effect" on the response
> variable) are qualitatively the same.

> Comments?

I would be happy to re-institute p-values for fixed effects in the
summary and anova methods for lmer objects using a denominator degrees
of freedom based on the trace of the hat matrix or the rank of Z:X if
others will volunteer to respond to the "these answers are obviously
wrong because they don't agree with <whatever> and the idiot who wrote
this software should be thrashed to within an inch of his life"
messages.  I don't have the patience.


From h.wickham at gmail.com  Thu Sep  7 15:41:10 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Thu, 7 Sep 2006 08:41:10 -0500
Subject: [R] graphics - joining repeated measures with a line
In-Reply-To: <8d6f66050609061832k7e61d8d9tcffd98f7e9ca09c3@mail.gmail.com>
References: <8d6f66050609061832k7e61d8d9tcffd98f7e9ca09c3@mail.gmail.com>
Message-ID: <f8e6ff050609070641n297a6e6cw937a96bc4aff4a85@mail.gmail.com>

> I would like to join repeated measures for patients across two visits using
> a line. The program below uses symbols to represent each patient. Basically,
> I would like to join each pair of symbols.

This is easy in ggplot:

install.packages("ggplot")
library(ggplot)

qplot(visit, var, id=patient, type=c("line", "point"), colour=factor(patient))

Regards,

Hadley


From z.dalton at lancaster.ac.uk  Thu Sep  7 15:43:40 2006
From: z.dalton at lancaster.ac.uk (z.dalton at lancaster.ac.uk)
Date: Thu, 7 Sep 2006 14:43:40 +0100 (BST)
Subject: [R] counting process form of a cox model (cluster(id))?
Message-ID: <E1GLKAO-00015y-00@wing0.lancs.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060907/6cee4240/attachment.pl 

From lorenz.gygax at art.admin.ch  Thu Sep  7 15:45:40 2006
From: lorenz.gygax at art.admin.ch (lorenz.gygax at art.admin.ch)
Date: Thu, 7 Sep 2006 15:45:40 +0200
Subject: [R] Conservative "ANOVA tables" in lmer
In-Reply-To: <40e66e0b0609070559u656463c1l52a60eda9a095237@mail.gmail.com>
Message-ID: <145C63777EF3ED41A5A99035845F7DD95C8D8B@EVD-C8001.bk.evdad.admin.ch>

Dear Douglas,

> I would be happy to re-institute p-values for fixed effects in the
> summary and anova methods for lmer objects using a denominator
> degrees of freedom based on the trace of the hat matrix or the rank
> of Z:X

Please do!

> if others will volunteer to respond to the "these answers are
> obviously wrong because they don't agree with <whatever> and the
> idiot who wrote this software should be thrashed to within an inch
> of his life" messages.  I don't have the patience.

I would try to take up my shares of these type or questions.

Best regards, Lorenz
- 
Lorenz Gygax
Dr. sc. nat., postdoc

Centre for proper housing of ruminants and pigs
Swiss Federal Veterinary Office
Agroscope Reckenholz-T?nikon Research Station ART

T?nikon, CH-8356 Ettenhausen / Switzerland
Tel: +41 052 368 33 84
Fax: +41 052 365 11 90
lorenz.gygax at art.admin.ch
www.art.admin.ch


From cooch17 at verizon.net  Thu Sep  7 15:56:59 2006
From: cooch17 at verizon.net (Evan Cooch)
Date: Thu, 07 Sep 2006 09:56:59 -0400
Subject: [R] continuation lines in R script files
In-Reply-To: <44FF2EFE.3070707@cropdesign.com>
References: <44FF2D88.5080902@verizon.net> <44FF2EFE.3070707@cropdesign.com>
Message-ID: <4500252B.7010302@verizon.net>

Joris De Wolf wrote:
> Are your sure your second solution does not work? Try again...
>
>   

Turns out the second approach did work - but only once I stopped 
cutting-and-pasting between two different operating systems (Linux and 
Windows under Linux). Apparently, some of the cut-and-paste things I was 
doing added weird EOL characters (unseen) or some such...

Ah well.


From alex.lam at bbsrc.ac.uk  Thu Sep  7 16:20:25 2006
From: alex.lam at bbsrc.ac.uk (alex lam (RI))
Date: Thu, 7 Sep 2006 15:20:25 +0100
Subject: [R] Memory allocation
Message-ID: <84DA9D8AC9B05F4B889E7C70238CB4510427B99F@rie2ksrv1.ri.bbsrc.ac.uk>

Dear list,

I have been trying to run the function "qvalue" under the package qvalue
on a vector with about 20 million values.

> asso_p.qvalue<-qvalue(asso_p.vector)
Error: cannot allocate vector of size 156513 Kb
> sessionInfo()
Version 2.3.1 (2006-06-01)
i686-pc-linux-gnu

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"
"datasets"
[7] "base"

other attached packages:
qvalue
 "1.1"
> gc()
            used  (Mb) gc trigger   (Mb)  max used   (Mb)
Ncells    320188   8.6   23540643  628.7  20464901  546.5
Vcells 101232265 772.4  294421000 2246.3 291161136 2221.4

I have been told that the linux box has 4Gb of RAM, so it should be able
to do better than this.
I searched the FAQ and found some tips on increasing memory size, but
they seem to be windows specific, such as memory.size() and the
-max-mem-size flag. On my linux box R didn't recognise them.

I don't understand the meaning of max-vsize, max-nsize and max-ppsize.
Any help on how to increase the memory allocation on linux is much
appreciated.

Many thanks,
Alex

------------------------------------
Alex Lam
PhD student
Department of Genetics and Genomics
Roslin Institute (Edinburgh)
Roslin
Midlothian EH25 9PS

Phone +44 131 5274471
Web   http://www.roslin.ac.uk


From t.gardner at uea.ac.uk  Thu Sep  7 16:22:22 2006
From: t.gardner at uea.ac.uk (Toby Gardner)
Date: Thu, 7 Sep 2006 15:22:22 +0100
Subject: [R] Problem with Variance Components (and general glmmconfusion)
References: <01b001c6d075$86fb3940$5401a8c0@Toby>
	<40e66e0b0609061723l494b9990n602939aa2f6ac152@mail.gmail.com>
Message-ID: <00da01c6d289$08dafe40$8262de8b@Toby>

Dear Dr Bates,



Many thanks for such a useful response to my problem.



Regarding Variance Components .....



The VarCorr function runs fine for lmer objects once the nlme package is 
removed.



Regarding the format of the nested random effects for an lmer object, you 
said:



In recent versions of lme4 you can use the specification

model2 <- lmer(y ~ 1 + (1|groupA/groupB))

Your version may be correct or not.  It depends on what the distinct
levels of groupB correspond to.  The version with the / is more
reliable.



This works well. These are environmental data measured at plots within sites 
(B), within forests (A).



Here is the model (I have put a dump of the data file used for these 
analyses at the end of this email):



> modelusd<-lmer(USD~1 + (1|forest/site))

> summary(modelusd)

Linear mixed-effects model fit by REML

Formula: USD ~ 1 + (1 | forest/site)

      AIC      BIC    logLik MLdeviance REMLdeviance

 816.7469 825.7788 -405.3734   815.0236     810.7469

Random effects:

 Groups      Name        Variance Std.Dev.

 site:forest (Intercept)  6.2099  2.4920

 forest      (Intercept) 33.0435  5.7483

 Residual                10.4335  3.2301

number of obs: 150, groups: site:forest, 15; forest, 3



Fixed effects:

            Estimate Std. Error t value

(Intercept)   9.8033     3.3909  2.8911



And VarCorr confirms the variance components:



> VarCorr(modelusd)

$`site:forest`

1 x 1 Matrix of class "dpoMatrix"

            (Intercept)

(Intercept)    6.209851



$forest

1 x 1 Matrix of class "dpoMatrix"

            (Intercept)

(Intercept)    33.04345



attr(,"sc")

[1] 3.230100



And following your suggestion I used the HPDinterval to obtain a measure of 
error around the random effects:



> MC.modelusd<-mcmcsamp(modelusd, 50000)

> HPDinterval(MC.modelusd)

                     lower      upper

(Intercept)     -3.5815626  23.202750

log(sigma^2)     2.1168335   2.594976

log(st:f.(In))   0.8209994   3.250159

log(frst.(In))   1.0676778   8.676852

deviance       814.8747050 829.055630

attr(,"Probability")

[1] 0.95



What I am really after are the intra-class correlation coefficients so I can 
demonstrate the variability in a given environmental variable at different 
spatial scales.  I can of course calculate the % variance explained for each 
random effect from the summary(lmer).  However - and this may be a stupid 
question! - but can the intervals for the StDev of the random effects also 
just be transformed to intervals of the variance (and then converted to % 
values for the intra-class correlation coefficients) by squaring?



Ideally I would like to partition the variance explained by all (three) 
spatially nested scales - forest / site / array - where array is the sample 
unit.  Using lmer produces the model summary I want:



> modelusd2<-lmer(USD~1 + (1|forest/site/array))

> summary(modelusd2)



Linear mixed-effects model fit by REML

Formula: USD ~ 1 + (1 | forest/site/array)

      AIC      BIC    logLik MLdeviance REMLdeviance

 818.7469 830.7894 -405.3734   815.0236     810.7469

Random effects:

 Groups              Name        Variance Std.Dev.

 array:(site:forest) (Intercept)  7.5559  2.7488

 site:forest         (Intercept)  6.2099  2.4920

 forest              (Intercept) 33.0435  5.7484

 Residual                         2.8776  1.6963

number of obs: 150, groups: array:(site:forest), 150; site:forest, 15; 
forest, 3



Fixed effects:

            Estimate Std. Error t value

(Intercept)   9.8033     3.3909  2.8911



However - the mcmcsamp process fails



> MC.modelusd2<-mcmcsamp(modelusd2, 50000)



with this error message:



Error: Leading minor of order 1 in downdated X'X is not positive definite

Error in t(.Call(mer_MCMCsamp, object, saveb, n, trans, verbose)) :

            unable to find the argument 'x' in selecting a method for 
function 't'

>



Am I trying something impossible here?



Regarding GLMMs..(now with species count data, blocking random factors and 
multiple fixed factors)



When using lmer I would suggest using method = "Laplace" and perhaps
control = list(usePQL = FALSE, msVerbose = 1) as I mentioned in
another reply to the list a few minutes ago.



This seems to work well, thanks.



With the greatest respect to all concerned, if I could I would like to echo 
the request by Martin Maechler on the list a few weeks ago that it would be 
extremely useful (especially for newcomers like me - and likely would 
greatly reduce the traffic on this list looking at many of the past threads) 
if authors of packages were able to be explicit in the help files about how 
functions differ (key advantages and disadvantages) from packages offering 
otherwise very similar functions (e.g. lmer/glmmML - although the subsequent 
comment by Dr Bates on this helped a lot).



Many thanks!



Toby Gardner



platform       i386-pc-mingw32
arch           i386
os             mingw32
system         i386, mingw32
status
major          2
minor          3.1
year           2006
month          06
day            01
svn rev        38247
language       R
version.string Version 2.3.1 (2006-06-01)



> dump("testdata", file=stdout())
testdata <-
structure(list(forest = structure(as.integer(c(2, 2, 2, 2, 2,
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)), .Label = c("EUC",
"PF", "SF"), class = "factor"), site = as.integer(c(1, 1, 1,
1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3,
3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5,
5, 5, 5, 5, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,
2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4,
4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 1, 1, 1, 1, 1, 1, 1,
1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3,
3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5
)), Array = as.integer(c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2,
3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2,
3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2,
3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2,
3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2,
3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2,
3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2,
3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)), USD = c(13.67,
13, 9.33, 10.67, 11, 10, 11.67, 10.33, 7, 8, 5.67, 7.67, 13.67,
10.33, 10.67, 7, 10.33, 12, 14, 7.67, 6.67, 4.33, 9, 13.67, 11,
19, 14, 19.67, 18.67, 8.33, 4.67, 10, 5, 11, 8, 7.67, 11, 12,
11, 7.67, 13.67, 15.33, 12, 11.33, 14.67, 13.33, 7, 12, 11.33,
11.33, 0.67, 3.33, 2, 2.67, 0.33, 1.33, 1.33, 1, 0.67, 0, 3.33,
3.33, 5.67, 4.67, 1.33, 3.67, 1, 6.33, 3, 1.67, 3.67, 5.67, 5.33,
2.67, 1.67, 2.33, 3.67, 6.67, 5.33, 9, 8, 5.67, 2.67, 0, 4.33,
8, 6, 3.67, 10.67, 9, 0.67, 1, 1.33, 0.67, 3, 1.5, 0.67, 0.33,
7.67, 7.33, 22, 18.67, 16.67, 21.33, 22.33, 22.33, 17.67, 16.33,
19.67, 20.67, 21.33, 17.33, 19, 19.33, 18.33, 18.67, 18.33, 19,
18.33, 19.33, 17.33, 12.33, 9.33, 8.33, 6, 17, 18, 8, 12, 15.67,
6, 1.33, 19, 11.67, 7, 16.33, 16, 14, 10.33, 4, 19, 19.67, 14,
15.33, 14, 6.33, 11.33, 11.67, 14, 15.33)), .Names = c("forest",
"site", "Array", "USD"), class = "data.frame", row.names = c("1",
"2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13",
"14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24",
"25", "26", "27", "28", "29", "30", "31", "32", "33", "34", "35",
"36", "37", "38", "39", "40", "41", "42", "43", "44", "45", "46",
"47", "48", "49", "50", "51", "52", "53", "54", "55", "56", "57",
"58", "59", "60", "61", "62", "63", "64", "65", "66", "67", "68",
"69", "70", "71", "72", "73", "74", "75", "76", "77", "78", "79",
"80", "81", "82", "83", "84", "85", "86", "87", "88", "89", "90",
"91", "92", "93", "94", "95", "96", "97", "98", "99", "100",
"101", "102", "103", "104", "105", "106", "107", "108", "109",
"110", "111", "112", "113", "114", "115", "116", "117", "118",
"119", "120", "121", "122", "123", "124", "125", "126", "127",
"128", "129", "130", "131", "132", "133", "134", "135", "136",
"137", "138", "139", "140", "141", "142", "143", "144", "145",
"146", "147", "148", "149", "150"))







----- Original Message ----- 
From: "Douglas Bates" <bates at stat.wisc.edu>
To: "Toby Gardner" <t.gardner at uea.ac.uk>
Cc: "R-help" <r-help at stat.math.ethz.ch>
Sent: Thursday, September 07, 2006 1:23 AM
Subject: Re: [R] Problem with Variance Components (and general 
glmmconfusion)


> On 9/4/06, Toby Gardner <t.gardner at uea.ac.uk> wrote:
>> Dear list,
>>
>> I am having some problems with extracting Variance Components from a 
>> random-effects model:
>>
>> I am running a simple random-effects model using lme:
>>
>> model<-lme(y~1,random=~1|groupA/groupB)
>>
>> which returns the output for the StdDev of the Random effects, and model 
>> AIC etc as expected.
>>
>> Until yesterday I was using R v. 2.0, and had no problem in calling the 
>> variance components of the above model using VarCorr(model), together 
>> with their 95% confidence intervals using intervals() - although for some 
>> response variables a call to intervals() returns the error: Cannot get 
>> confidence intervals on var-cov components: Non-positive definite 
>> approximate variance-covariance.
>>
>> I have now installed R v. 2.3.1 and am now experiencing odd behaviour 
>> with VarCorr(lme.object), with an error message typically being returned:
>>
>> Error in VarCorr(model) : no direct or inherited method for function 
>> 'VarCorr' for this call
>>
>> Is this known to happen? For instance could it be due to the subsequent 
>> loading of new packages? (lme4 for instance?).
>
> Yes.  Avoid loading lme4 and nlme simultaneously.
>
>>
>> To get around this problem I have tried running the same model using 
>> lmer:
>>
>> model2<-lmer(y~1 + (1|groupA) + (1|groupB))
>
> In recent versions of lme4 you can use the specification
>
> model2 <- lmer(y ~ 1 + (1|groupA/groupB))
>
> Your version may be correct or not.  It depends on what the distinct
> levels of groupB correspond to.  The version with the / is more
> reliable.
>
>>
>> Should this not produce the same model? The variance components are very 
>> similar but not identical, making me think that I am doing something 
>> wrong. I am also correct in thinking that intervals() does not work with 
>> lmer? I get: Error in intervals(model2) : no applicable method for 
>> "intervals"
>
> That is correct.  Currently there is no intervals method for an lmer
> model.  You can use mcmcsamp to get a Markov chain Monte Carlo sample
> to which you can apply HPDinterval from the "coda" package.  However,
> these are stochastic intervals so it is best to try on a couple of
> chains to check on the reproducibility or the intervals.
>
>>
>> GLMM
>>
>> I have a general application question - please excuse my ignorance, I am 
>> relatively new to this and trying to find a way through the maze.  In 
>> short I need to compile generalized linear mixed models both for (a) 
>> Poisson data and (b) binonial data incorporating a two nested random 
>> factors, and I need to be able to extract AIC values as I am taking an 
>> information-theoretic approach to model selection.  Prior to sending an 
>> email to the list I have spent quite a few days reading the background on 
>> a number of functions, all of which offer potential for this; glmmML, 
>> glmmPQL, lmer, and glmmADMB.  I can understand that glmmPQL is unsuitable 
>> because there is no way of knowing the maximised likelihood, but is there 
>> much difference between the remaining three options? I have seen 
>> simulation comparisons published on this list between glmmADMB and 
>> glmmPQL and lmer, but it seems these are before the latest release of 
>> lmer, and also they do not evaluate glmmML.  To a newcomer this myria!
> d !
>>  of options is bewildering, can anyone offer advice as to the most robust 
>> approach?
>
> Goran can correct me if I am wrong but I don't believe that glmmML can
> be used with multiple levels of random effects.
>
> I'm not sure what the status of glmmADMB is these days.  There was
> some controversy regarding the license applied to some of that code a
> while back.  I don't know if it has been resolved to everyone's
> satisfaction.
>
> When using lmer I would suggest using method = "Laplace" and perhaps
> control = list(usePQL = FALSE, msVerbose = 1) as I mentioned in
> another reply to the list a few minutes ago.
>
> Let us know how it works out.
>
>>
>> Many thanks for your time and patience,
>>
>> Toby Gardner
>>
>> School of Environmental Sciences
>> University of East Anglia
>> Norwich, NR4 7TJ
>> United Kingdom
>> Email: t.gardner at uea.ac.uk
>> Website: www.uea.ac.uk/~e387495
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ripley at stats.ox.ac.uk  Thu Sep  7 16:44:41 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 7 Sep 2006 15:44:41 +0100 (BST)
Subject: [R] Memory allocation
In-Reply-To: <84DA9D8AC9B05F4B889E7C70238CB4510427B99F@rie2ksrv1.ri.bbsrc.ac.uk>
References: <84DA9D8AC9B05F4B889E7C70238CB4510427B99F@rie2ksrv1.ri.bbsrc.ac.uk>
Message-ID: <Pine.LNX.4.64.0609071542030.32691@gannet.stats.ox.ac.uk>

On Thu, 7 Sep 2006, alex lam (RI) wrote:

> Dear list,
> 
> I have been trying to run the function "qvalue" under the package qvalue
> on a vector with about 20 million values.
> 
> > asso_p.qvalue<-qvalue(asso_p.vector)
> Error: cannot allocate vector of size 156513 Kb
> > sessionInfo()
> Version 2.3.1 (2006-06-01)
> i686-pc-linux-gnu
> 
> attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"
> "datasets"
> [7] "base"
> 
> other attached packages:
> qvalue
>  "1.1"
> > gc()
>             used  (Mb) gc trigger   (Mb)  max used   (Mb)
> Ncells    320188   8.6   23540643  628.7  20464901  546.5
> Vcells 101232265 772.4  294421000 2246.3 291161136 2221.4
> 
> I have been told that the linux box has 4Gb of RAM, so it should be able
> to do better than this.

But it also has a 4Gb/process address space, and of that some (1Gb?) is 
reserved for the system.  So it is quite possible that with 2.2Gb used you 
are unable to find any large blocks.

> I searched the FAQ and found some tips on increasing memory size, but
> they seem to be windows specific, such as memory.size() and the
> -max-mem-size flag. On my linux box R didn't recognise them.

?"Memory-limits" is the key

     Error messages beginning 'cannot allocate vector of size' indicate
     a failure to obtain memory, either because the size exceeded the
     address-space limit for a process or, more likely, because the
     system was unable to provide the memory.  Note that on a 32-bit OS
     there may well be enough free memory available, but not a large
     enough contiguous block of address space into which to map it.

> I don't understand the meaning of max-vsize, max-nsize and max-ppsize.
> Any help on how to increase the memory allocation on linux is much
> appreciated.

Get a 64-bit OS.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From maechler at stat.math.ethz.ch  Thu Sep  7 16:52:32 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 7 Sep 2006 16:52:32 +0200
Subject: [R] Conservative "ANOVA tables" in lmer
In-Reply-To: <40e66e0b0609070559u656463c1l52a60eda9a095237@mail.gmail.com>
References: <1A8FE5B0-3FE3-4CA9-9224-9CACB9AF4854@muohio.edu>
	<40e66e0b0609070559u656463c1l52a60eda9a095237@mail.gmail.com>
Message-ID: <17664.12848.693133.495312@stat.math.ethz.ch>

>>>>> "DB" == Douglas Bates <bates at stat.wisc.edu>
>>>>>     on Thu, 7 Sep 2006 07:59:58 -0500 writes:

    DB> Thanks for your summary, Hank.
    DB> On 9/7/06, Martin Henry H. Stevens <hstevens at muohio.edu> wrote:
    >> Dear lmer-ers,
    >> My thanks for all of you who are sharing your trials and tribulations
    >> publicly.

    >> I was hoping to elicit some feedback on my thoughts on denominator
    >> degrees of freedom for F ratios in mixed models. These thoughts and
    >> practices result from my reading of previous postings by Doug Bates
    >> and others.

    >> - I start by assuming that the appropriate denominator degrees lies
    >> between n - p and and n - q, where n=number of observations, p=number
    >> of fixed effects (rank of model matrix X), and q=rank of Z:X.

    DB> I agree with this but the opinion is by no means universal.  Initially
    DB> I misread the statement because I usually write the number of columns
    DB> of Z as q.

    DB> It is not easy to assess rank of Z:X numerically.  In many cases one
    DB> can reason what it should be from the form of the model but a general
    DB> procedure to assess the rank of a matrix, especially a sparse matrix,
    DB> is difficult.

    DB> An alternative which can be easily calculated is n - t where t is the
    DB> trace of the 'hat matrix'.  The function 'hatTrace' applied to a
    DB> fitted lmer model evaluates this trace (conditional on the estimates
    DB> of the relative variances of the random effects).

    >> - I then conclude that good estimates of P values on the F ratios lie
    >>   between 1 - pf(F.ratio, numDF, n-p) and 1 - pf(F.ratio, numDF, n-q).
    >>   -- I further surmise that the latter of these (1 - pf(F.ratio, numDF,
    >>   n-q)) is the more conservative estimate.

This assumes that the true distribution (under H0) of that "F ratio"
*is*  F_{n1,n2}  for some (possibly non-integer)  n1 and n2.
But AFAIU, this is only approximately true at best, and AFAIU,
the quality of this approximation has only been investigated
empirically for some situations. 
Hence, even your conservative estimate of the P value could be
wrong (I mean "wrong on the wrong side" instead of just
"conservatively wrong").  Consequently, such a P-value is only
``approximately conservative'' ...
I agree howevert that in some situations, it might be a very
useful "descriptive statistic" about the fitted model.

Martin

    >> When I use these criteria and compare my "ANOVA" table to the results
    >> of analysis of Helmert contrasts using MCMC sample with highest
    >> posterior density intervals, I find that my conclusions (e.g. factor
    >> A, with three levels, has a "significant effect" on the response
    >> variable) are qualitatively the same.

    >> Comments?

    DB> I would be happy to re-institute p-values for fixed effects in the
    DB> summary and anova methods for lmer objects using a denominator degrees
    DB> of freedom based on the trace of the hat matrix or the rank of Z:X if
    DB> others will volunteer to respond to the "these answers are obviously
    DB> wrong because they don't agree with <whatever> and the idiot who wrote
    DB> this software should be thrashed to within an inch of his life"
    DB> messages.  I don't have the patience.

    DB> ______________________________________________
    DB> R-help at stat.math.ethz.ch mailing list
    DB> https://stat.ethz.ch/mailman/listinfo/r-help
    DB> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    DB> and provide commented, minimal, self-contained, reproducible code.


From bates at stat.wisc.edu  Thu Sep  7 17:11:47 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 7 Sep 2006 10:11:47 -0500
Subject: [R] Conservative "ANOVA tables" in lmer
In-Reply-To: <17664.12848.693133.495312@stat.math.ethz.ch>
References: <1A8FE5B0-3FE3-4CA9-9224-9CACB9AF4854@muohio.edu>
	<40e66e0b0609070559u656463c1l52a60eda9a095237@mail.gmail.com>
	<17664.12848.693133.495312@stat.math.ethz.ch>
Message-ID: <40e66e0b0609070811k2f462ca5k53fcdeaf271f7fd9@mail.gmail.com>

On 9/7/06, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
> >>>>> "DB" == Douglas Bates <bates at stat.wisc.edu>
> >>>>>     on Thu, 7 Sep 2006 07:59:58 -0500 writes:
>
>     DB> Thanks for your summary, Hank.
>     DB> On 9/7/06, Martin Henry H. Stevens <hstevens at muohio.edu> wrote:
>     >> Dear lmer-ers,
>     >> My thanks for all of you who are sharing your trials and tribulations
>     >> publicly.
>
>     >> I was hoping to elicit some feedback on my thoughts on denominator
>     >> degrees of freedom for F ratios in mixed models. These thoughts and
>     >> practices result from my reading of previous postings by Doug Bates
>     >> and others.
>
>     >> - I start by assuming that the appropriate denominator degrees lies
>     >> between n - p and and n - q, where n=number of observations, p=number
>     >> of fixed effects (rank of model matrix X), and q=rank of Z:X.
>
>     DB> I agree with this but the opinion is by no means universal.  Initially
>     DB> I misread the statement because I usually write the number of columns
>     DB> of Z as q.
>
>     DB> It is not easy to assess rank of Z:X numerically.  In many cases one
>     DB> can reason what it should be from the form of the model but a general
>     DB> procedure to assess the rank of a matrix, especially a sparse matrix,
>     DB> is difficult.
>
>     DB> An alternative which can be easily calculated is n - t where t is the
>     DB> trace of the 'hat matrix'.  The function 'hatTrace' applied to a
>     DB> fitted lmer model evaluates this trace (conditional on the estimates
>     DB> of the relative variances of the random effects).
>
>     >> - I then conclude that good estimates of P values on the F ratios lie
>     >>   between 1 - pf(F.ratio, numDF, n-p) and 1 - pf(F.ratio, numDF, n-q).
>     >>   -- I further surmise that the latter of these (1 - pf(F.ratio, numDF,
>     >>   n-q)) is the more conservative estimate.
>
> This assumes that the true distribution (under H0) of that "F ratio"
> *is*  F_{n1,n2}  for some (possibly non-integer)  n1 and n2.
> But AFAIU, this is only approximately true at best, and AFAIU,
> the quality of this approximation has only been investigated
> empirically for some situations.
> Hence, even your conservative estimate of the P value could be
> wrong (I mean "wrong on the wrong side" instead of just
> "conservatively wrong").  Consequently, such a P-value is only
> ``approximately conservative'' ...
> I agree howevert that in some situations, it might be a very
> useful "descriptive statistic" about the fitted model.

Thank you for pointing that out Martin.  I agree.  As I mentioned a
value of the denominator degrees of freedom based on the trace of the
hat matrix is conditional on the estimates of the relative variances
of the random effects.  I think an argument could still be made for
the upper bound on the dimension of the model space being rank of Z:X
and hence a lower bound on the dimension of the space in which the
residuals lie as being n - rank[Z:X].  One possible approach would be
to use the squared length of the projection of the data vector into
the orthogonal complement of Z:X as the "sum of squares" and n -
rank(Z:X) as the degrees of freedom and base tests on that.  Under the
assumptions on the model I think an F ratio calculated using that
actually would have an F distribution.

>
> Martin
>
>     >> When I use these criteria and compare my "ANOVA" table to the results
>     >> of analysis of Helmert contrasts using MCMC sample with highest
>     >> posterior density intervals, I find that my conclusions (e.g. factor
>     >> A, with three levels, has a "significant effect" on the response
>     >> variable) are qualitatively the same.
>
>     >> Comments?
>
>     DB> I would be happy to re-institute p-values for fixed effects in the
>     DB> summary and anova methods for lmer objects using a denominator degrees
>     DB> of freedom based on the trace of the hat matrix or the rank of Z:X if
>     DB> others will volunteer to respond to the "these answers are obviously
>     DB> wrong because they don't agree with <whatever> and the idiot who wrote
>     DB> this software should be thrashed to within an inch of his life"
>     DB> messages.  I don't have the patience.
>
>     DB> ______________________________________________
>     DB> R-help at stat.math.ethz.ch mailing list
>     DB> https://stat.ethz.ch/mailman/listinfo/r-help
>     DB> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>     DB> and provide commented, minimal, self-contained, reproducible code.
>


From p.dalgaard at biostat.ku.dk  Thu Sep  7 17:20:29 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 07 Sep 2006 17:20:29 +0200
Subject: [R] Conservative "ANOVA tables" in lmer
In-Reply-To: <17664.12848.693133.495312@stat.math.ethz.ch>
References: <1A8FE5B0-3FE3-4CA9-9224-9CACB9AF4854@muohio.edu>
	<40e66e0b0609070559u656463c1l52a60eda9a095237@mail.gmail.com>
	<17664.12848.693133.495312@stat.math.ethz.ch>
Message-ID: <x2fyf3g0ua.fsf@viggo.kubism.ku.dk>

Martin Maechler <maechler at stat.math.ethz.ch> writes:

> >>>>> "DB" == Douglas Bates <bates at stat.wisc.edu>
> >>>>>     on Thu, 7 Sep 2006 07:59:58 -0500 writes:
> 
>     DB> Thanks for your summary, Hank.
>     DB> On 9/7/06, Martin Henry H. Stevens <hstevens at muohio.edu> wrote:
>     >> Dear lmer-ers,
>     >> My thanks for all of you who are sharing your trials and tribulations
>     >> publicly.
> 
>     >> I was hoping to elicit some feedback on my thoughts on denominator
>     >> degrees of freedom for F ratios in mixed models. These thoughts and
>     >> practices result from my reading of previous postings by Doug Bates
>     >> and others.
> 
>     >> - I start by assuming that the appropriate denominator degrees lies
>     >> between n - p and and n - q, where n=number of observations, p=number
>     >> of fixed effects (rank of model matrix X), and q=rank of Z:X.
> 
>     DB> I agree with this but the opinion is by no means universal.  Initially
>     DB> I misread the statement because I usually write the number of columns
>     DB> of Z as q.
> 
>     DB> It is not easy to assess rank of Z:X numerically.  In many cases one
>     DB> can reason what it should be from the form of the model but a general
>     DB> procedure to assess the rank of a matrix, especially a sparse matrix,
>     DB> is difficult.
> 
>     DB> An alternative which can be easily calculated is n - t where t is the
>     DB> trace of the 'hat matrix'.  The function 'hatTrace' applied to a
>     DB> fitted lmer model evaluates this trace (conditional on the estimates
>     DB> of the relative variances of the random effects).
> 
>     >> - I then conclude that good estimates of P values on the F ratios lie
>     >>   between 1 - pf(F.ratio, numDF, n-p) and 1 - pf(F.ratio, numDF, n-q).
>     >>   -- I further surmise that the latter of these (1 - pf(F.ratio, numDF,
>     >>   n-q)) is the more conservative estimate.
> 
> This assumes that the true distribution (under H0) of that "F ratio"
> *is*  F_{n1,n2}  for some (possibly non-integer)  n1 and n2.
> But AFAIU, this is only approximately true at best, and AFAIU,
> the quality of this approximation has only been investigated
> empirically for some situations. 
> Hence, even your conservative estimate of the P value could be
> wrong (I mean "wrong on the wrong side" instead of just
> "conservatively wrong").  Consequently, such a P-value is only
> ``approximately conservative'' ...
> I agree howevert that in some situations, it might be a very
> useful "descriptive statistic" about the fitted model.

I'm very wary of ANY attempt at guesswork in these matters. 

I may be understanding the post wrongly, but consider this case: Y_ij
= mu + z_i + eps_ij, i = 1..3, j=1..100

I get rank(X)=1, rank(X:Z)=3,  n=300

It is well known that the test for mu=0 in this case is obtained by
reducing data to group means, xbar_i, and then do a one-sample t test,
the square of which is F(1, 2), but it seems to be suggested that
F(1, 297) is a conservative test???!

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From bates at stat.wisc.edu  Thu Sep  7 17:32:06 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 7 Sep 2006 10:32:06 -0500
Subject: [R] Conservative "ANOVA tables" in lmer
In-Reply-To: <x2fyf3g0ua.fsf@viggo.kubism.ku.dk>
References: <1A8FE5B0-3FE3-4CA9-9224-9CACB9AF4854@muohio.edu>
	<40e66e0b0609070559u656463c1l52a60eda9a095237@mail.gmail.com>
	<17664.12848.693133.495312@stat.math.ethz.ch>
	<x2fyf3g0ua.fsf@viggo.kubism.ku.dk>
Message-ID: <40e66e0b0609070832x28bb5365w6f4bfd8a6a72078a@mail.gmail.com>

On 07 Sep 2006 17:20:29 +0200, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> Martin Maechler <maechler at stat.math.ethz.ch> writes:
>
> > >>>>> "DB" == Douglas Bates <bates at stat.wisc.edu>
> > >>>>>     on Thu, 7 Sep 2006 07:59:58 -0500 writes:
> >
> >     DB> Thanks for your summary, Hank.
> >     DB> On 9/7/06, Martin Henry H. Stevens <hstevens at muohio.edu> wrote:
> >     >> Dear lmer-ers,
> >     >> My thanks for all of you who are sharing your trials and tribulations
> >     >> publicly.
> >
> >     >> I was hoping to elicit some feedback on my thoughts on denominator
> >     >> degrees of freedom for F ratios in mixed models. These thoughts and
> >     >> practices result from my reading of previous postings by Doug Bates
> >     >> and others.
> >
> >     >> - I start by assuming that the appropriate denominator degrees lies
> >     >> between n - p and and n - q, where n=number of observations, p=number
> >     >> of fixed effects (rank of model matrix X), and q=rank of Z:X.
> >
> >     DB> I agree with this but the opinion is by no means universal.  Initially
> >     DB> I misread the statement because I usually write the number of columns
> >     DB> of Z as q.
> >
> >     DB> It is not easy to assess rank of Z:X numerically.  In many cases one
> >     DB> can reason what it should be from the form of the model but a general
> >     DB> procedure to assess the rank of a matrix, especially a sparse matrix,
> >     DB> is difficult.
> >
> >     DB> An alternative which can be easily calculated is n - t where t is the
> >     DB> trace of the 'hat matrix'.  The function 'hatTrace' applied to a
> >     DB> fitted lmer model evaluates this trace (conditional on the estimates
> >     DB> of the relative variances of the random effects).
> >
> >     >> - I then conclude that good estimates of P values on the F ratios lie
> >     >>   between 1 - pf(F.ratio, numDF, n-p) and 1 - pf(F.ratio, numDF, n-q).
> >     >>   -- I further surmise that the latter of these (1 - pf(F.ratio, numDF,
> >     >>   n-q)) is the more conservative estimate.
> >
> > This assumes that the true distribution (under H0) of that "F ratio"
> > *is*  F_{n1,n2}  for some (possibly non-integer)  n1 and n2.
> > But AFAIU, this is only approximately true at best, and AFAIU,
> > the quality of this approximation has only been investigated
> > empirically for some situations.
> > Hence, even your conservative estimate of the P value could be
> > wrong (I mean "wrong on the wrong side" instead of just
> > "conservatively wrong").  Consequently, such a P-value is only
> > ``approximately conservative'' ...
> > I agree howevert that in some situations, it might be a very
> > useful "descriptive statistic" about the fitted model.
>
> I'm very wary of ANY attempt at guesswork in these matters.
>
> I may be understanding the post wrongly, but consider this case: Y_ij
> = mu + z_i + eps_ij, i = 1..3, j=1..100
>
> I get rank(X)=1, rank(X:Z)=3,  n=300
>
> It is well known that the test for mu=0 in this case is obtained by
> reducing data to group means, xbar_i, and then do a one-sample t test,
> the square of which is F(1, 2), but it seems to be suggested that
> F(1, 297) is a conservative test???!

It's a different test, isn't it?  Your test is based upon the between
group sum of squares with 2 df.  I am proposing to use the within
group sum of squares or its generalization.


From spencer.graves at pdf.com  Thu Sep  7 17:49:30 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 07 Sep 2006 08:49:30 -0700
Subject: [R] singular factor analysis
In-Reply-To: <44FFE0E3.6040005@pburns.seanet.com>
References: <44FF0C77.5030309@pdf.com> <44FFE0E3.6040005@pburns.seanet.com>
Message-ID: <45003F8A.7060904@pdf.com>

Hi, Patrick:  Thanks very much.  I'll try it.  Spencer Graves

Patrick Burns wrote:
> This is a very common computation in finance.
>
> On the public domain page of the Burns Statistics website
> in the financial part is the code and R help file for
> 'factor.model.stat'.  Most of the complication of the code
> is to deal with missing values.
>
> Patrick Burns
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
>
> Spencer Graves wrote:
>
>>      Are there any functions available to do a factor analysis with 
>> fewer observations than variables?  As long as you have more than 3 
>> observations, my computations suggest you have enough data to 
>> estimate a factor analysis covariance matrix, even though the sample 
>> covariance matrix is singular.  I tried the naive thing and got an 
>> error:
>> > set.seed(1)
>> > X <- array(rnorm(50), dim=c(5, 10))
>> > factanal(X, factors=1)
>> Error in solve.default(cv) : system is computationally singular: 
>> reciprocal condition number = 4.8982e-018
>>
>>      I can write a likelihood for a multivariate normal and solve it, 
>> but I wondered if there is anything else available that could do this?
>>      Thanks,
>>      Spencer Graves
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>  
>>


From timh at insightful.com  Thu Sep  7 18:47:10 2006
From: timh at insightful.com (Tim Hesterberg)
Date: 7 Sep 2006 09:47:10 -0700
Subject: [R] Matrix multiplication using apply() or lappy() ?
In-Reply-To: <Pine.LNX.4.64.0609061825580.11591@gannet.stats.ox.ac.uk>
	(message from Prof Brian Ripley on Wed, 6 Sep 2006 18:30:31 +0100
	(BST))
References: <Pine.LNX.4.64.0609061825580.11591@gannet.stats.ox.ac.uk>
Message-ID: <SEWINEXCH00gkJXrRjb000000ef@sewinexch00.insightful.com>

toby_marks at americancentury.com asked:
>I am trying to divide the columns of a matrix by the first row in the 
>matrix.

Dividing columns of a matrix by a vector is a pretty fundamental
operation, and the query resulted in a large number of suggestions:

x/matrix(v, nrow(x), ncol(x), byrow = TRUE))
sweep(x, 2, v, "/")
x / rep(v, each = nrow(x))
x / outer(rep(1, nrow(x)), v)
x %*% diag(1/v)
t(apply(x, 1, function(x) x/v))
x/rep(v, each=nrow(x))
t(apply(x, 1, "/", v))
library(reshape); iapply(x, 1, "/", v)  # R only
t(t(x)/v)
scale(x, center = FALSE, v)  # not previously suggested


It is unsatisfactory when such a fundamental operation is
done in so many different ways.  
* It makes it hard to read other people's code.  
* Some of these are very inefficient.

I propose to create standard functions and possibly operator forms
for this and similar operators:

	colPlus(x, v)		x %c+% v
	colMinus(x, v)		x %c-% v
	colTimes(x, v)		x %c*% v
	colDivide(x, v)		x %c/% v
	colPower(x, v)		x %c^% v

Goals are:
* more readable code
* generic functions, with methods for objects such as data frames
  and S-PLUS bigdata objects  (this would be for both S-PLUS and R)
* efficiency -- use the fastest of the above methods, or drop to C
  to avoid replicating v.
* allow error checking (that length of v matches number of columns of x)

I'd like feedback (to me, I'll summarize for the list) on:
* the suggestion in general
* are names like "colPlus" OK, or do you have other suggestions?
* create both functions and operators, or just the functions?
* should there be similar operations for rows?  

Note:  similar operations for rows are not usually needed, because
	x * v  # e.g. where v = colMeans(x)
is equivalent to (but faster than)
	x * rep(v, length = length(x))
The advantage would be that
	colTimes(x, v)
could throw an error if length(v) != nrow(x)

Tim Hesterberg

P.S.  Of the suggestions, my preference is
	a / rep(v, each=nrow(a))
It was to support this and similar +-*^ operations that I originally
added the "each" argument to rep.

========================================================
| Tim Hesterberg       Research Scientist              |
| timh at insightful.com  Insightful Corp.                |
| (206)802-2319        1700 Westlake Ave. N, Suite 500 |
| (206)283-8691 (fax)  Seattle, WA 98109-3044, U.S.A.  |
|                      www.insightful.com/Hesterberg   |
========================================================
Download the S+Resample library from www.insightful.com/downloads/libraries


From axmeyer at googlemail.com  Thu Sep  7 18:49:06 2006
From: axmeyer at googlemail.com (Anton Meyer)
Date: Thu, 7 Sep 2006 18:49:06 +0200
Subject: [R] area between two curves, but one is not continuous
Message-ID: <60ab48ec0609070949q2563665cw9aced860bc8891aa@mail.gmail.com>

Hello,

I want to colorize the area between two curves, but one of these
curves isn't continuous.

The best solution I found is the 2nd example in the help of polygon,
but how can I get no area filling for the missing data in the 2nd curve.

example:

x1 = c(1:8)
x2 = c(1:8)
y1 = c(1,5,6,1,4,5,5,5)
y2 = c(0,3,3,NA,NA,1,3,4)

plot(x1,y1,type="l")
lines(x2,y2)

for the missing parts I want no filling.

so for this examples the code would be:
polygon(c(1:3,3:1),c(y1[1:3],rev(y2[1:3])),col="green")
polygon(c(6:8,8:6),c(y1[6:8],rev(y2[6:8])),col="green")

How can I generalize this for a longer curve with more data?

AxM


From hodgess at gator.dt.uh.edu  Thu Sep  7 18:57:23 2006
From: hodgess at gator.dt.uh.edu (Erin Hodgess)
Date: Thu, 7 Sep 2006 11:57:23 -0500
Subject: [R]  [OT] Important stat dates
Message-ID: <200609071657.k87GvN0T011573@gator.dt.uh.edu>

Dear R People:

Way Off Topic:

Is anyone aware of a website that contains important dates
in statistics history, please?

Maybe a sort of "This Day in Statistics", please?

I thought that my students might get a kick out of that.

(actually I will probably enjoy it more than them!)

Thanks for any help!

I tried (via Google) "today in statistics" and "today in statistics
history" but nothing worthwhile appeared.

Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu


From hchen at utmem.edu  Thu Sep  7 19:14:38 2006
From: hchen at utmem.edu (Hao Chen)
Date: Thu, 7 Sep 2006 12:14:38 -0500
Subject: [R] barplot: different colors for the bar and the strips
In-Reply-To: <1157633645.5418.9.camel@localhost.localdomain>
References: <20060907111803.GA7222@utmail.utmem.edu>
	<1157633645.5418.9.camel@localhost.localdomain>
Message-ID: <20060907171438.GA25555@utmail.utmem.edu>

Hello Marc Schwartz

On Thu, Sep 07, 2006 at 07:54:05AM -0500, Marc Schwartz wrote:
> On Thu, 2006-09-07 at 06:18 -0500, Hao Chen wrote:
> > Hi,
> > 
> > I am using barplot and would like to know if it is possible to have bars
> > filled with one color while use a different color for the shading lines. 
> > 
> > The following code colors the shading lines, leaving the bars in white:
> > 
> >  barplot(1:5, col=c(1:5), density=c(1:5)*5)
> > 
> > while the colors are applied to the bars when density is removed.
> > 
> >  barplot(1:5, col=c(1:5))
> > 
> > I did check ?barplot and found the following: 
> > 
> > 	col: a vector of colors for the bars or bar components. 
> >  
> >  Thanks,
> > 
> >  Hao
> 
> Note the key word 'or' in the description of the 'col' argument.
> 
> You need to make two separate calls to barplot(). The first using the
> fill colors, then the second using the shading lines AND setting 'add =
> TRUE', so that the second plot overwrites the first without clearing the
> plot device.
> 
>  barplot(1:5, col=c(1:5))
> 
>  barplot(1:5, col = "black", density=c(1:5), add = TRUE)
> 
> Just be sure that any other arguments, such as axis limits, are
> identical between the two calls.
> 
> HTH,
> 
> Marc Schwartz
> 


Thank you very much for your help. It works but only in the order as you
put it, since the following code only shows the color, but not the
shading lines:

barplot(1:5, col = "black", density=c(1:5))
barplot(1:5, col=c(1:5), add = TRUE)

Hao Chen

---
Mining PubMed: http://www.chilibot.net
-


From mschwartz at mn.rr.com  Thu Sep  7 19:25:50 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Thu, 07 Sep 2006 12:25:50 -0500
Subject: [R] barplot: different colors for the bar and the strips
In-Reply-To: <20060907171438.GA25555@utmail.utmem.edu>
References: <20060907111803.GA7222@utmail.utmem.edu>
	<1157633645.5418.9.camel@localhost.localdomain>
	<20060907171438.GA25555@utmail.utmem.edu>
Message-ID: <1157649950.4023.6.camel@localhost.localdomain>

On Thu, 2006-09-07 at 12:14 -0500, Hao Chen wrote:
> Hello Marc Schwartz
> 
> On Thu, Sep 07, 2006 at 07:54:05AM -0500, Marc Schwartz wrote:
> > On Thu, 2006-09-07 at 06:18 -0500, Hao Chen wrote:
> > > Hi,
> > > 
> > > I am using barplot and would like to know if it is possible to have bars
> > > filled with one color while use a different color for the shading lines. 
> > > 
> > > The following code colors the shading lines, leaving the bars in white:
> > > 
> > >  barplot(1:5, col=c(1:5), density=c(1:5)*5)
> > > 
> > > while the colors are applied to the bars when density is removed.
> > > 
> > >  barplot(1:5, col=c(1:5))
> > > 
> > > I did check ?barplot and found the following: 
> > > 
> > > 	col: a vector of colors for the bars or bar components. 
> > >  
> > >  Thanks,
> > > 
> > >  Hao
> > 
> > Note the key word 'or' in the description of the 'col' argument.
> > 
> > You need to make two separate calls to barplot(). The first using the
> > fill colors, then the second using the shading lines AND setting 'add =
> > TRUE', so that the second plot overwrites the first without clearing the
> > plot device.
> > 
> >  barplot(1:5, col=c(1:5))
> > 
> >  barplot(1:5, col = "black", density=c(1:5), add = TRUE)
> > 
> > Just be sure that any other arguments, such as axis limits, are
> > identical between the two calls.
> > 
> > HTH,
> > 
> > Marc Schwartz
> > 
> 
> 
> Thank you very much for your help. It works but only in the order as you
> put it, since the following code only shows the color, but not the
> shading lines:
> 
> barplot(1:5, col = "black", density=c(1:5))
> barplot(1:5, col=c(1:5), add = TRUE)
> 
> Hao Chen

That is correct. The sequence is important, as the shading lines are
drawn with a transparent background, enabling the original color to be
seen.

Reversing the order, you are overplotting the shading lines with opaque
colored rectangles. Hence, the lines are lost.

HTH,

Marc


From jonthayn at ku.edu  Thu Sep  7 19:47:36 2006
From: jonthayn at ku.edu (Jonathan Boyd Thayn)
Date: Thu, 7 Sep 2006 12:47:36 -0500
Subject: [R] rgdal on a Mac
Message-ID: <8be5465a90166b7461546de1d2b5fa80@ku.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060907/b08696e7/attachment.pl 

From mschwartz at mn.rr.com  Thu Sep  7 19:35:03 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Thu, 07 Sep 2006 12:35:03 -0500
Subject: [R] [OT] Important stat dates
In-Reply-To: <200609071657.k87GvN0T011573@gator.dt.uh.edu>
References: <200609071657.k87GvN0T011573@gator.dt.uh.edu>
Message-ID: <1157650503.4023.10.camel@localhost.localdomain>

On Thu, 2006-09-07 at 11:57 -0500, Erin Hodgess wrote:
> Dear R People:
> 
> Way Off Topic:
> 
> Is anyone aware of a website that contains important dates
> in statistics history, please?
> 
> Maybe a sort of "This Day in Statistics", please?
> 
> I thought that my students might get a kick out of that.
> 
> (actually I will probably enjoy it more than them!)
> 
> Thanks for any help!
> 
> I tried (via Google) "today in statistics" and "today in statistics
> history" but nothing worthwhile appeared.


Here are two pages that you might find helpful:

  http://www.york.ac.uk/depts/maths/histstat/welcome.htm

  http://www.economics.soton.ac.uk/staff/aldrich/Figures.htm

Both have additional references and reciprocal links.

HTH,

Marc Schwartz


From Roger.Bivand at nhh.no  Thu Sep  7 19:47:04 2006
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 7 Sep 2006 19:47:04 +0200 (CEST)
Subject: [R] rgdal on a Mac
In-Reply-To: <8be5465a90166b7461546de1d2b5fa80@ku.edu>
Message-ID: <Pine.LNX.4.44.0609071934300.7886-100000@reclus.nhh.no>

On Thu, 7 Sep 2006, Jonathan Boyd Thayn wrote:

> I am trying to install the rgdal package on my Mac OS X 3.9.  I 
> downloaded and installed the GDAL libraries from Fink and then tried to 
> install rgdal and got the following message.  I tried to determine if 
> the GDAL libraries were in my path but I'm not sure how to do that.  
> Any ideas?  Thanks.
> 

(The R-sig-geo mailing list may be a more appropriate place to look for an 
answer)

Unfortunately, I as maintainer of the package have no access to OSX. I do 
know that OSX users have installed rgdal successfully, and installation 
instructions are on the Rgeo website:

http://www.sal.uiuc.edu/tools/tools-sum/rgeo/rgeo-detail/map-packages-on-cran

"OSX: The rgdal source package from CRAN can be installed on OSX by first 
installing PROJ.4 and GDAL, then installing sp, and finally download the 
source package tarball to a suitable temporary location, and install with 
R CMD INSTALL ... your options ... rgdal*.tar.gz. Your options give the 
locations, if required, of --with-gdal-config=, --with-proj-include=, 
and/or --with-proj-lib=, all within --configure-args='' as described in 
section 1.2.2 of the ''Writing R extensions'' manual."

But this presupposes that you can find the installed software on your 
system yourself, something that is difficult to do at a distance. If OSX 
has the locate utility, you could run it in a terminal, or search for the 
files needed (in Finder??), but an OSX user would know the correct way 
forward. I expect that you have installed PROJ.4 too - do either of 
proj -lp or gdalinfo --formats or ogrinfo --formats at a terminal prompt 
say anything useful to indicate that the applications using the libraries 
are available and working?

> 
> trying URL 
> 'http://www.biometrics.mtu.edu/CRAN/src/contrib/rgdal_0.4-10.tar.gz'
> Content type 'application/x-gzip' length 4009531 bytes
> opened URL
> ==================================================
> downloaded 3915Kb
> 
> * Installing *source* package 'rgdal' ...
> gdal-config: gdal-config
> ./configure: line 1: gdal-config: command not found
> 
> The gdal-config script distributed with GDAL could not be found.
> If you have not installed the GDAL libraries, you can
> download the source from  http://www.gdal.org/
> If you have installed the GDAL libraries, then make sure that
> gdal-config is in your path. Try typing gdal-config at a
> shell prompt and see if it runs. If not, use:
>   --configure-args='--with-gdal-config=/usr/local/bin/gdal-config' echo 
> with appropriate values for your installation.
> 
> 
> The downloaded packages are in
> 	/private/tmp/Rtmp9zhfAK/downloaded_packages
> ** Removing 
> '/Library/Frameworks/R.framework/Versions/2.2/Resources/library/rgdal'
> ** Restoring previous 
> '/Library/Frameworks/R.framework/Versions/2.2/Resources/library/rgdal'
> ERROR: configuration failed for package 'rgdal'
> 
> 
> Jonathan B. Thayn
> Kansas Applied Remote Sensing (KARS) Program
> University of Kansas
> Higuchi Hall
> 2101 Constant Avenue
> Lawrence, Kansas 66047-3759
> jonthayn at ku.edu
> www.kars.ku.edu/about/people/thayn/JonSite/Welcome.html
> 	[[alternative text/enriched version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From friendly at yorku.ca  Thu Sep  7 19:59:50 2006
From: friendly at yorku.ca (Michael Friendly)
Date: Thu, 07 Sep 2006 13:59:50 -0400
Subject: [R] plot image matrix with row/col labels
Message-ID: <45005E16.10509@yorku.ca>

I'm working with an historical image that may be (one of?) the first 
uses of gray-scale shading to show the pattern of values in a 
matrix/table, later used by Bertin in his 'reorderable matrix'
and sometimes called a scalogram.

The image is at
http://euclid.psych.yorku.ca/SCS/Gallery/images/Private/scalogram.jpg
The rows refer to the arrondisements of Paris, the cols to various
population characteristics.

I want to read it into R with rimage(read.jpeg), calcualte avg. shading 
values,
and recreate an approximation to the image with the row and column 
labels.  I'm stuck at this last step,
plot(imagematrix(mat))
(and also on how to read the image from a URL).

Can someone help?  My code is below

library(rimage)
image <- read.jpeg("C:/Documents/milestone/images/scalogram.jpg")
## how to read from web?
#image <- 
read.jpeg("http://euclid.psych.yorku.ca/SCS/Gallery/images/Private/scalogram.jpg")

# remove row/col headers
img2 <- image[480:1740, 470:2350]
str(img2)

# size of each blob
ht <-floor(nrow(img2)/20);
wd <-floor(ncol(img2)/40)

# calculate trimmed mean of pixel values
mat <- matrix(nrow=20,ncol=40,0)
for (i in 1:20) {
	for (j in 1:40) {
		rows <- seq(1+(i-1)*ht, i*ht)
		cols <- seq(1+(j-1)*wd, j*wd)
		blob <- img2[ rows,  cols ]
		mat[i,j] <- mean(blob, trim=0.1)
		}
	}


# names for arrrondisements
rnames <- c(
	"01 Louvre", "02 Bourse", "03 Temple", "04 Hotel de Ville", "05 Pantheon",
	"06 Luxembourg", "07 Palais", "08 Eglise", "09 Opera", "10 St. Laurent",
	"11 Popincourt", "12 Reuilly", "13 Goeblins", "14 Observatoire", "15 
Vaurigard",
	"16 Passy", "17 Batingnoles", "18 Montmartre", "19 B. Chaumont", "20 
Menilmontant")

#names for population characteristics
cnames <- c("01 Accrois. pop", "02 Pop specifique", "03 
Habitants/menage", "04 Maisons/hectare", "05 Habitants/maison",
	"06 Appart./maison", "07 Appart. vacantes", "08 Locaux Indust.&C", "09 
Garnisson,", "10 Parisiens",
	"11 Provinseaux", "12 Etrangers", "13 Calvinistes", "14 Lutheriens", 
"15 Isrealites",
	"16 Libres penseurs", "17 Illettres", "18 Enfants", "19 Mineurs", "20 
Adultes",
	"21 Vieillards", "22 Electeurs", "23 Horticulture", "24 Industrie", "25 
Commerce",
	"26 Transports", "27 Prof. diverses", "28 Prof. liberales", "29 Forces 
publiques", "30 Admin. publique",
	"31 Clerge",  "32 Proprietaires rentiers", "33 Pop. aisee", "34 
Employees", "35 Ouvriers",
	"36 Journaliers", "37 Domestiques", "38 Chevaux", "39 Chiens", "40 
Moralite"
	)

dimnames(mat) <- list(rnames, cnames)

# how to plot the image matrix with row/col names???

# show the image matrix
plot(imagematrix(mat))


-- 
Michael Friendly     Email: friendly AT yorku DOT ca
Professor, Psychology Dept.
York University      Voice: 416 736-5115 x66249 Fax: 416 736-5814
4700 Keele Street    http://www.math.yorku.ca/SCS/friendly.html
Toronto, ONT  M3J 1P3 CANADA


From Marc.Zodet at ahrq.hhs.gov  Thu Sep  7 20:39:56 2006
From: Marc.Zodet at ahrq.hhs.gov (Zodet, Marc W. (AHRQ))
Date: Thu, 7 Sep 2006 14:39:56 -0400
Subject: [R] Running/submitting script files
Message-ID: <1F809F62E3CEA04881B4644029484B450316CEB3@AVN3VS004.ees.hhs.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060907/dc8f0957/attachment.pl 

From adik at ilovebacon.org  Thu Sep  7 20:46:04 2006
From: adik at ilovebacon.org (Adam D. I. Kramer)
Date: Thu, 7 Sep 2006 11:46:04 -0700 (PDT)
Subject: [R] Alternatives to merge for large data sets?
In-Reply-To: <Pine.LNX.4.64.0609070954560.10893@gannet.stats.ox.ac.uk>
References: <mailman.13.1157536804.28705.r-help@stat.math.ethz.ch>
	<Pine.LNX.4.64.0609062304380.4867@parser.ilovebacon.org>
	<Pine.LNX.4.64.0609070954560.10893@gannet.stats.ox.ac.uk>
Message-ID: <Pine.LNX.4.64.0609071144370.4867@parser.ilovebacon.org>


On Thu, 7 Sep 2006, Prof Brian Ripley wrote:

> Which version of R?

Previously, 2.3.1.

> Please try 2.4.0 alpha, as it has a different and more efficient
> algorithm for the case of 1-1 matches.

I downloaded and installed R-latest, but got the same error message:

Error: cannot allocate vector of size 7301 Kb

...though at least the too-big size was larger this time.

My data set is not exactly 1-1; every item in "prof" may have one or more
matches in "pubbounds," though every item in "pubbounds" corrosponds only to
one "prof."

--Adam

>
> On Wed, 6 Sep 2006, Adam D. I. Kramer wrote:
>
>> Hello,
>>
>> I am trying to merge two very large data sets, via
>>
>> pubbounds.prof <-
>> merge(x=pubbounds,y=prof,by.x="user",by.y="userid",all=TRUE,sort=FALSE)
>>
>> which gives me an error of
>>
>> Error: cannot allocate vector of size 2962 Kb
>>
>> I am reasonably sure that this is correct syntax.
>>
>> The trouble is that pubbounds and prof are large; they are data frames which
>> take up 70M and 11M respectively when saved as .Rdata files.
>>
>> I understand from various archive searches that "merge can't handle that,"
>> because merge takes n^2 memory, which I do not have.
>
> Not really true (it has been changed since those days).  Of course, if you
> have multiple matches it must do so.
>
>> My question is whether there is an alternative to merge which would carry
>> out the process in a slower, iterative manner...or if I should just bite the
>> bullet, write.table, and use a perl script to do the job.
>>
>> Thankful as always,
>> Adam D. I. Kramer
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>


From elvis at xlsolutions-corp.com  Thu Sep  7 20:53:57 2006
From: elvis at xlsolutions-corp.com (elvis at xlsolutions-corp.com)
Date: Thu, 07 Sep 2006 11:53:57 -0700
Subject: [R] October R/Splus course @ 3 locations *** R/Splus Fundamentals
	and Programming Techniques
Message-ID: <20060907115357.9f08cc34deb45d78e54b3b5664e21546.37ba8ba3f1.wbe@email.secureserver.net>

XLSolutions Corporation (www.xlsolutions-corp.com) is proud to
announce our 2-day October 2006 "R/S-plus Fundamentals and Programming
Techniques" : www.xlsolutions-corp.com/Rfund.htm

*** Washington DC / October 12-13, 2006
*** Seattle Wa  / October 19-20
*** San Francisco / October 26-27      

Reserve your seat now at the early bird rates! Payment due AFTER
the class

Course Description:

This two-day beginner to intermediate R/S-plus course focuses on a
broad spectrum of topics, from reading raw data to a comparison of R
and S. We will learn the essentials of data manipulation, graphical
visualization and R/S-plus programming. We will explore statistical
data analysis tools,including graphics with data sets. How to enhance
your plots, build your own packages (librairies) and connect via
ODBC,etc.
We will perform some statistical modeling and fit linear regression
models. Participants are encouraged to bring data for interactive
sessions

With the following outline:

- An Overview of R and S
- Data Manipulation and Graphics
- Using Lattice Graphics
- A Comparison of R and S-Plus
- How can R Complement SAS?
- Writing Functions
- Avoiding Loops
- Vectorization
- Statistical Modeling
- Project Management
- Techniques for Effective use of R and S
- Enhancing Plots
- Using High-level Plotting Functions
- Building and Distributing Packages (libraries)
- Connecting; ODBC, Rweb, Orca via sockets and via Rjava


Email us for group discounts.
Email Sue Turner: sue at xlsolutions-corp.com
Phone: 206-686-1578
Visit us: www.xlsolutions-corp.com/training.htm
Please let us know if you and your colleagues are interested in this
classto take advantage of group discount. Register now to secure your
seat!

Interested in R/Splus Advanced course? email us.


Cheers,
Elvis Miller, PhD
Manager Training.
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com
elvis at xlsolutions-corp.com


From br44114 at gmail.com  Thu Sep  7 21:05:05 2006
From: br44114 at gmail.com (bogdan romocea)
Date: Thu, 7 Sep 2006 15:05:05 -0400
Subject: [R] Alternatives to merge for large data sets?
Message-ID: <8d5a36350609071205x5dc62ba5qb1a6b679059b6d4@mail.gmail.com>

One obvious alternative is an SQL join, which you could do directly in
a DBMS, or from R via RMySQL / RSQLite /... Keep in mind that creating
indexes on user/userid before the join may save a lot of time.


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Adam
> D. I. Kramer
> Sent: Thursday, September 07, 2006 2:46 PM
> To: Prof Brian Ripley
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Alternatives to merge for large data sets?
>
>
> On Thu, 7 Sep 2006, Prof Brian Ripley wrote:
>
> > Which version of R?
>
> Previously, 2.3.1.
>
> > Please try 2.4.0 alpha, as it has a different and more efficient
> > algorithm for the case of 1-1 matches.
>
> I downloaded and installed R-latest, but got the same error message:
>
> Error: cannot allocate vector of size 7301 Kb
>
> ...though at least the too-big size was larger this time.
>
> My data set is not exactly 1-1; every item in "prof" may have
> one or more
> matches in "pubbounds," though every item in "pubbounds"
> corrosponds only to
> one "prof."
>
> --Adam
>
> >
> > On Wed, 6 Sep 2006, Adam D. I. Kramer wrote:
> >
> >> Hello,
> >>
> >> I am trying to merge two very large data sets, via
> >>
> >> pubbounds.prof <-
> >>
> merge(x=pubbounds,y=prof,by.x="user",by.y="userid",all=TRUE,so
> rt=FALSE)
> >>
> >> which gives me an error of
> >>
> >> Error: cannot allocate vector of size 2962 Kb
> >>
> >> I am reasonably sure that this is correct syntax.
> >>
> >> The trouble is that pubbounds and prof are large; they are
> data frames which
> >> take up 70M and 11M respectively when saved as .Rdata files.
> >>
> >> I understand from various archive searches that "merge
> can't handle that,"
> >> because merge takes n^2 memory, which I do not have.
> >
> > Not really true (it has been changed since those days).  Of
> course, if you
> > have multiple matches it must do so.
> >
> >> My question is whether there is an alternative to merge
> which would carry
> >> out the process in a slower, iterative manner...or if I
> should just bite the
> >> bullet, write.table, and use a perl script to do the job.
> >>
> >> Thankful as always,
> >> Adam D. I. Kramer
> >
> > --
> > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865 272861 (self)
> > 1 South Parks Road,                     +44 1865 272866 (PA)
> > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From edd at debian.org  Thu Sep  7 21:13:37 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 7 Sep 2006 14:13:37 -0500
Subject: [R] Running/submitting script files
In-Reply-To: <1F809F62E3CEA04881B4644029484B450316CEB3@AVN3VS004.ees.hhs.gov>
References: <1F809F62E3CEA04881B4644029484B450316CEB3@AVN3VS004.ees.hhs.gov>
Message-ID: <17664.28513.141653.35504@basebud.nulle.part>


On 7 September 2006 at 14:39, Zodet, Marc W. (AHRQ) wrote:
| Is there any way to run an R script file (i.e., *.R) from the command
| prompt in the console window.  Ultimately, I'm looking to put such code
| in a script file so that it can set off other R scripts/programs as
| needed.

Which platform?  On Linux/Unix, Jeffey Horner's interp does just that.
Currently at version 0.0.4 and may undergo a renaming in the near future ...

http://wiki.r-project.org/rwiki/doku.php?id=developers:rinterp

Hth, Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From roger.bos at gmail.com  Thu Sep  7 21:34:34 2006
From: roger.bos at gmail.com (roger bos)
Date: Thu, 7 Sep 2006 15:34:34 -0400
Subject: [R] Running/submitting script files
In-Reply-To: <17664.28513.141653.35504@basebud.nulle.part>
References: <1F809F62E3CEA04881B4644029484B450316CEB3@AVN3VS004.ees.hhs.gov>
	<17664.28513.141653.35504@basebud.nulle.part>
Message-ID: <1db726800609071234m48a270a1r350426579364b143@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060907/3b685aaf/attachment.pl 

From trajnp at gmail.com  Thu Sep  7 21:54:04 2006
From: trajnp at gmail.com (Raj, Towfique)
Date: Thu, 7 Sep 2006 20:54:04 +0100
Subject: [R] Running wilcox.test function on two lists
Message-ID: <bcb1e1da0609071254x342d6831gb19d77f4c45701c6@mail.gmail.com>

Dear all,
I'm a newbie to R and I would really apperciate any help with the following:

I have two lists, l1 and l2:

l1:
$"A*0101"
[1] 0.076 0.109 0.155  0.077 0.09 0  0  0.073
[9] 0.33  0.0034 0.0053


$"A*0247"
[1] 0 0 0.5 .004 0 0 0

$"A*0248"
[1] 0 0 0.3 0 0.06

....

l2:

$"A*1101"
[1] 0.17  0.24  0.097  0.075  0.067

$"A*0247"
numeric(0)

$"A*0248"
[1] 0.031

....

Basically, what I want to do is run wilcox.test() on each entry pair
in the list.

1) I want to loop through the list to run wilcox.test for each entry
of the list. How would I do that? mapply()?

          wilcox.test(l0[[1]],l1[[1]]) for the first one and so on....

2) I want to exclude the list entry which has no values (i.e. A*0247).

3) Finally, I only want the to see the 'p-value' for each list names.
The output I want capture is only the 'p-value' object from
wilcox.test.

        name    p-value
       A*0101  0.8329
        ....       ....

I'm grateful for any help, or any pointers to a good online tutorial.

Thanks a lot in advance,

-T.


From cg.pettersson at evp.slu.se  Thu Sep  7 21:49:18 2006
From: cg.pettersson at evp.slu.se (CG Pettersson)
Date: Thu, 7 Sep 2006 21:49:18 +0200 (CEST)
Subject: [R] Model vs. Observed for a lme() regression fit using
 two	variables
In-Reply-To: <20060907100328.GJ12212@ms.unimelb.edu.au>
References: <44FFE7EC.7000206@vpe.slu.se>
	<20060907100328.GJ12212@ms.unimelb.edu.au>
Message-ID: <60579.212.112.39.217.1157658558.squirrel@webmail.slu.se>

Hi Andrew,

Thanks a lot, That would give me what I want.
But using my own data and models resulted in this:

> plot(fitted(tcos31.c.cp, level=1), FCR.c$g.cp)
Error in xy.coords(x, y, xlabel, ylabel, log) :
        'x' and 'y' lengths differ

This is quite correct, as there are some missing values in the covariate
and I made the model using the 'na.action=na.omit' option.

I know there is a way of using the model to fix this, but haven?t been
able to get the code right during the afternoon.

How do I code this and where should I have looked?

Cheers
/CG




On Thu, September 7, 2006 12:03 pm, Andrew Robinson said:
> Hi CG,
>
> I think that the best pair of summary plots are
>
> 1) the fitted values without random effects against the observed
>    response variable, and
>
> 2) fitted values with random effects against the observed response
>    variable.
>
> The first plot gives a summary of the overall quality of the fixed
> effects of the model, the second gives a summary of the overall
> quality of the fixed effects and random effects of the model.
>
> eg
>
> fm1 <- lme(distance ~ age, data = Orthodont)
>
> plot(fitted(fm1, level=0), Orthodont$distance)
> abline(0, 1, col="red")
>
> plot(fitted(fm1, level=1), Orthodont$distance)
> abline(0, 1, col="red")
>
> I hope that this helps.
>
> Andrew
>
> On Thu, Sep 07, 2006 at 11:35:40AM +0200, CG Pettersson wrote:
>> Dear all.
>>
>> R 2.3.1, W2k.
>>
>> I am working with a field trial series where, for the moment, I do
>> regressions using more than one covariate to explain the protein levels
>> in malting barley.
>>
>> To do this I use lme() and a mixed call, structured by both experiment
>> (trial) and repetition in each experiment (block). Everything works
>> fine, resulting in nice working linear models using two covariates. But
>> how do I visualize this in an efficient and clear way?
>>
>> What I want is something like the standard output from all multivariate
>> tools I have worked with (Observed vs. Predicted) with the least square
>> line in the middle. It is naturally possible to plot each covariate
>> separate, and also to use the 3d- sqatterplot in Rcmdr to plot both at
>> the same time, but I want a plain 2d plot.
>>
>> Who has made a plotting method for this and where do I find it?
>> Or am I missing something obvious here, that this plot is easy to
>> achieve without any ready made methods?
>>
>> Cheers
>> /CG
>>
>> --
>> CG Pettersson, MSci, PhD Stud.
>> Swedish University of Agricultural Sciences (SLU)
>> Dept. of Crop Production Ecology. Box 7043.
>> SE-750 07 UPPSALA, Sweden.
>> +46 18 671428, +46 70 3306685
>> cg.pettersson at vpe.slu.se
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> --
> Andrew Robinson
> Department of Mathematics and Statistics            Tel: +61-3-8344-9763
> University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
> Email: a.robinson at ms.unimelb.edu.au         http://www.ms.unimelb.edu.au
>


-- 
CG Pettersson, MSci, PhD Stud.
Swedish University of Agricultural Sciences (SLU)
Dep. of Crop Production Ekology. Box 7043.
SE-750 07 Uppsala, Sweden
cg.pettersson at vpe.slu.se


From Dimitris.Rizopoulos at med.kuleuven.be  Thu Sep  7 22:11:39 2006
From: Dimitris.Rizopoulos at med.kuleuven.be (Dimitrios Rizopoulos)
Date: Thu, 07 Sep 2006 22:11:39 +0200
Subject: [R] Running wilcox.test function on two lists
In-Reply-To: <bcb1e1da0609071254x342d6831gb19d77f4c45701c6@mail.gmail.com>
References: <bcb1e1da0609071254x342d6831gb19d77f4c45701c6@mail.gmail.com>
Message-ID: <20060907221139.s4gm6g8ezdokkcwo@webmail3.kuleuven.be>

try something like the following:

lis1 <- c(lapply(1:10, rnorm, n = 10))
lis2 <- c(lapply(1:10, rnorm, n = 10))
lis1[[5]] <- lis2[[8]] <- numeric(0)
############################################
ind <- sapply(lis1, length) > 0 & sapply(lis2, length) > 0
lis1 <- lis1[ind]
lis2 <- lis2[ind]

mapply(function(x, y) wilcox.test(x, y)$p.value, lis1, lis2)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
      http://www.student.kuleuven.be/~m0390867/dimitris.htm


Quoting "Raj, Towfique" <trajnp at gmail.com>:

> Dear all,
> I'm a newbie to R and I would really apperciate any help with the following:
>
> I have two lists, l1 and l2:
>
> l1:
> $"A*0101"
> [1] 0.076 0.109 0.155  0.077 0.09 0  0  0.073
> [9] 0.33  0.0034 0.0053
>
>
> $"A*0247"
> [1] 0 0 0.5 .004 0 0 0
>
> $"A*0248"
> [1] 0 0 0.3 0 0.06
>
> ....
>
> l2:
>
> $"A*1101"
> [1] 0.17  0.24  0.097  0.075  0.067
>
> $"A*0247"
> numeric(0)
>
> $"A*0248"
> [1] 0.031
>
> ....
>
> Basically, what I want to do is run wilcox.test() on each entry pair
> in the list.
>
> 1) I want to loop through the list to run wilcox.test for each entry
> of the list. How would I do that? mapply()?
>
>           wilcox.test(l0[[1]],l1[[1]]) for the first one and so on....
>
> 2) I want to exclude the list entry which has no values (i.e. A*0247).
>
> 3) Finally, I only want the to see the 'p-value' for each list names.
> The output I want capture is only the 'p-value' object from
> wilcox.test.
>
>         name    p-value
>        A*0101  0.8329
>         ....       ....
>
> I'm grateful for any help, or any pointers to a good online tutorial.
>
> Thanks a lot in advance,
>
> -T.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>



Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From Qinghong.Li at rdmo.nestle.com  Thu Sep  7 23:07:10 2006
From: Qinghong.Li at rdmo.nestle.com (Li,Qinghong,ST.LOUIS,Molecular Biology)
Date: Thu, 7 Sep 2006 16:07:10 -0500
Subject: [R] pairwise.t.test vs. t. test
Message-ID: <2BA2B7291D5DC6409FD53CB7C01F0D990183B8D4@usslre00.nestle.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060907/fddd5f14/attachment.pl 

From mnair at iusb.edu  Thu Sep  7 23:38:42 2006
From: mnair at iusb.edu (Nair, Murlidharan T)
Date: Thu, 7 Sep 2006 17:38:42 -0400
Subject: [R] reading images in R
Message-ID: <A32055BDEA88C34BB3DBBCD2293807786E3339@iu-mssg-mbx109.ads.iu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060907/51f62424/attachment.pl 

From mnair at iusb.edu  Fri Sep  8 00:04:06 2006
From: mnair at iusb.edu (Nair, Murlidharan T)
Date: Thu, 7 Sep 2006 18:04:06 -0400
Subject: [R] labeling graphs
Message-ID: <A32055BDEA88C34BB3DBBCD2293807786E3346@iu-mssg-mbx109.ads.iu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060907/4389a431/attachment.pl 

From afshart at exchange.sba.miami.edu  Fri Sep  8 00:17:54 2006
From: afshart at exchange.sba.miami.edu (Afshartous, David)
Date: Thu, 7 Sep 2006 18:17:54 -0400
Subject: [R] augPred plot in nlme library
Message-ID: <6BCB4D493A447546A8126F24332056E80415E58C@school1.business.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060907/a09d5f3d/attachment.pl 

From markleeds at verizon.net  Fri Sep  8 00:22:36 2006
From: markleeds at verizon.net (MARK LEEDS)
Date: Thu, 07 Sep 2006 18:22:36 -0400
Subject: [R] pairwise.t.test vs. t. test
References: <2BA2B7291D5DC6409FD53CB7C01F0D990183B8D4@usslre00.nestle.com>
Message-ID: <000701c6d2cc$1f852a60$2e01a8c0@m8d4477f3de884>

no, because the formula for the test statistics ( even assuming that 
variances are equal ) of the two different tests are different. in the 
pairwise t test, the pairwise differences are
viewed as one sample so it turns into a one sample test. any intro stat book 
will have the formulas.

                                                                             
                                                                             
     mark





----- Original Message ----- 
From: "Li,Qinghong,ST.LOUIS,Molecular Biology" <Qinghong.Li at rdmo.nestle.com>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, September 07, 2006 5:07 PM
Subject: [R] pairwise.t.test vs. t. test


> Hi,
>
> If I set the p.adjust="none", does it meant that the output p values from 
> the pairwise.t.test will be the same as those from individual t.tests (set 
> var.equal=T, alternative="t")?
>
> I actually got different p values from the two tests. See below. Is it 
> supposed to be this way?
>
> Thanks
> Johnny
>
>> x
> [1] 61.6 52.7 61.3 65.2 62.8 63.7 64.8 58.7 44.9 57.0 64.3 55.1 50.0 41.0
> [15] 43.0 45.9 52.2 45.5 46.9 31.6 40.6 44.8 39.4 31.0 37.5 32.6 23.2 34.6
> [29] 38.3 38.1 19.5 21.2 15.8 33.3 28.6 25.8
>> Grp
> [1] Yng Yng Yng Yng Yng Yng Yng Yng Yng Yng Yng Yng Med Med Med Med Med 
> Med
> [19] Med Med Med Med Med Med Old Old Old Old Old Old Old Old Old Old Old 
> Old
> Levels: Yng Med Old
>>  pairwise.t.test(x=x,g=Grp,p.adjust.method="none")
>
>        Pairwise comparisons using t tests with pooled SD
>
> data:  x and Grp
>
>    Yng     Med
> Med 1.0e-06 -
> Old 2.0e-12 2.6e-05
>
> P value adjustment method: none
>
>
>> t.test(x=x[1:12],y=x[25:36],var.equal=T, alternative="t")
>
>        Two Sample t-test
>
> data:  x[1:12] and x[25:36]
> t = 10.5986, df = 22, p-value = 4.149e-10
> alternative hypothesis: true difference in means is not equal to 0
> 95 percent confidence interval:
> 24.37106 36.22894
> sample estimates:
> mean of x mean of y
> 59.34167  29.04167
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ccleland at optonline.net  Fri Sep  8 00:44:00 2006
From: ccleland at optonline.net (Chuck Cleland)
Date: Thu, 07 Sep 2006 18:44:00 -0400
Subject: [R] pairwise.t.test vs. t. test
In-Reply-To: <000701c6d2cc$1f852a60$2e01a8c0@m8d4477f3de884>
References: <2BA2B7291D5DC6409FD53CB7C01F0D990183B8D4@usslre00.nestle.com>
	<000701c6d2cc$1f852a60$2e01a8c0@m8d4477f3de884>
Message-ID: <4500A0B0.2050401@optonline.net>

MARK LEEDS wrote:
> no, because the formula for the test statistics ( even assuming that 
> variances are equal ) of the two different tests are different. in the 
> pairwise t test, the pairwise differences are
> viewed as one sample so it turns into a one sample test. any intro stat book 
> will have the formulas.
>                                                                              
>      mark

  Actually, I think the difference is due to the SD being pooled across
all 3 groups in the pairwise.t.test, but just 2 groups in t.test.

> ----- Original Message ----- 
> From: "Li,Qinghong,ST.LOUIS,Molecular Biology" <Qinghong.Li at rdmo.nestle.com>
> To: <r-help at stat.math.ethz.ch>
> Sent: Thursday, September 07, 2006 5:07 PM
> Subject: [R] pairwise.t.test vs. t. test
> 
> 
>> Hi,
>>
>> If I set the p.adjust="none", does it meant that the output p values from 
>> the pairwise.t.test will be the same as those from individual t.tests (set 
>> var.equal=T, alternative="t")?
>>
>> I actually got different p values from the two tests. See below. Is it 
>> supposed to be this way?
>>
>> Thanks
>> Johnny
>>
>>> x
>> [1] 61.6 52.7 61.3 65.2 62.8 63.7 64.8 58.7 44.9 57.0 64.3 55.1 50.0 41.0
>> [15] 43.0 45.9 52.2 45.5 46.9 31.6 40.6 44.8 39.4 31.0 37.5 32.6 23.2 34.6
>> [29] 38.3 38.1 19.5 21.2 15.8 33.3 28.6 25.8
>>> Grp
>> [1] Yng Yng Yng Yng Yng Yng Yng Yng Yng Yng Yng Yng Med Med Med Med Med 
>> Med
>> [19] Med Med Med Med Med Med Old Old Old Old Old Old Old Old Old Old Old 
>> Old
>> Levels: Yng Med Old
>>>  pairwise.t.test(x=x,g=Grp,p.adjust.method="none")
>>        Pairwise comparisons using t tests with pooled SD
>>
>> data:  x and Grp
>>
>>    Yng     Med
>> Med 1.0e-06 -
>> Old 2.0e-12 2.6e-05
>>
>> P value adjustment method: none
>>
>>
>>> t.test(x=x[1:12],y=x[25:36],var.equal=T, alternative="t")
>>        Two Sample t-test
>>
>> data:  x[1:12] and x[25:36]
>> t = 10.5986, df = 22, p-value = 4.149e-10
>> alternative hypothesis: true difference in means is not equal to 0
>> 95 percent confidence interval:
>> 24.37106 36.22894
>> sample estimates:
>> mean of x mean of y
>> 59.34167  29.04167
>>
>> [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From markleeds at verizon.net  Fri Sep  8 01:25:31 2006
From: markleeds at verizon.net (MARK LEEDS)
Date: Thu, 07 Sep 2006 19:25:31 -0400
Subject: [R] pairwise.t.test vs. t. test
References: <2BA2B7291D5DC6409FD53CB7C01F0D990183B8D4@usslre00.nestle.com>
	<000701c6d2cc$1f852a60$2e01a8c0@m8d4477f3de884>
	<4500A0B0.2050401@optonline.net>
Message-ID: <000501c6d2d4$e9d8d840$2e01a8c0@m8d4477f3de884>

thanks. i assumed we we were talking about the standard textbook difference 
between the t test and pairwise t test.
my bad.



----- Original Message ----- 
From: "Chuck Cleland" <ccleland at optonline.net>
To: "MARK LEEDS" <markleeds at verizon.net>
Cc: "Li,Qinghong,ST.LOUIS,Molecular Biology" <Qinghong.Li at rdmo.nestle.com>; 
<r-help at stat.math.ethz.ch>
Sent: Thursday, September 07, 2006 6:44 PM
Subject: Re: [R] pairwise.t.test vs. t. test


> MARK LEEDS wrote:
>> no, because the formula for the test statistics ( even assuming that
>> variances are equal ) of the two different tests are different. in the
>> pairwise t test, the pairwise differences are
>> viewed as one sample so it turns into a one sample test. any intro stat 
>> book
>> will have the formulas.
>>
>>      mark
>
>  Actually, I think the difference is due to the SD being pooled across
> all 3 groups in the pairwise.t.test, but just 2 groups in t.test.
>
>> ----- Original Message ----- 
>> From: "Li,Qinghong,ST.LOUIS,Molecular Biology" 
>> <Qinghong.Li at rdmo.nestle.com>
>> To: <r-help at stat.math.ethz.ch>
>> Sent: Thursday, September 07, 2006 5:07 PM
>> Subject: [R] pairwise.t.test vs. t. test
>>
>>
>>> Hi,
>>>
>>> If I set the p.adjust="none", does it meant that the output p values 
>>> from
>>> the pairwise.t.test will be the same as those from individual t.tests 
>>> (set
>>> var.equal=T, alternative="t")?
>>>
>>> I actually got different p values from the two tests. See below. Is it
>>> supposed to be this way?
>>>
>>> Thanks
>>> Johnny
>>>
>>>> x
>>> [1] 61.6 52.7 61.3 65.2 62.8 63.7 64.8 58.7 44.9 57.0 64.3 55.1 50.0 
>>> 41.0
>>> [15] 43.0 45.9 52.2 45.5 46.9 31.6 40.6 44.8 39.4 31.0 37.5 32.6 23.2 
>>> 34.6
>>> [29] 38.3 38.1 19.5 21.2 15.8 33.3 28.6 25.8
>>>> Grp
>>> [1] Yng Yng Yng Yng Yng Yng Yng Yng Yng Yng Yng Yng Med Med Med Med Med
>>> Med
>>> [19] Med Med Med Med Med Med Old Old Old Old Old Old Old Old Old Old Old
>>> Old
>>> Levels: Yng Med Old
>>>>  pairwise.t.test(x=x,g=Grp,p.adjust.method="none")
>>>        Pairwise comparisons using t tests with pooled SD
>>>
>>> data:  x and Grp
>>>
>>>    Yng     Med
>>> Med 1.0e-06 -
>>> Old 2.0e-12 2.6e-05
>>>
>>> P value adjustment method: none
>>>
>>>
>>>> t.test(x=x[1:12],y=x[25:36],var.equal=T, alternative="t")
>>>        Two Sample t-test
>>>
>>> data:  x[1:12] and x[25:36]
>>> t = 10.5986, df = 22, p-value = 4.149e-10
>>> alternative hypothesis: true difference in means is not equal to 0
>>> 95 percent confidence interval:
>>> 24.37106 36.22894
>>> sample estimates:
>>> mean of x mean of y
>>> 59.34167  29.04167
>>>
>>> [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> -- 
> Chuck Cleland, Ph.D.
> NDRI, Inc.
> 71 West 23rd Street, 8th floor
> New York, NY 10010
> tel: (212) 845-4495 (Tu, Th)
> tel: (732) 512-0171 (M, W, F)
> fax: (917) 438-0894
>


From p.dalgaard at biostat.ku.dk  Fri Sep  8 01:52:09 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 08 Sep 2006 01:52:09 +0200
Subject: [R] pairwise.t.test vs. t. test
In-Reply-To: <000501c6d2d4$e9d8d840$2e01a8c0@m8d4477f3de884>
References: <2BA2B7291D5DC6409FD53CB7C01F0D990183B8D4@usslre00.nestle.com>
	<000701c6d2cc$1f852a60$2e01a8c0@m8d4477f3de884>
	<4500A0B0.2050401@optonline.net>
	<000501c6d2d4$e9d8d840$2e01a8c0@m8d4477f3de884>
Message-ID: <x2y7svdyl2.fsf@turmalin.kubism.ku.dk>

"MARK LEEDS" <markleeds at verizon.net> writes:

> thanks. i assumed we we were talking about the standard textbook difference 
> between the t test and pairwise t test.
> my bad.

Notice the difference between paired and pairwise...


> ----- Original Message ----- 
> From: "Chuck Cleland" <ccleland at optonline.net>
> To: "MARK LEEDS" <markleeds at verizon.net>
> Cc: "Li,Qinghong,ST.LOUIS,Molecular Biology" <Qinghong.Li at rdmo.nestle.com>; 
> <r-help at stat.math.ethz.ch>
> Sent: Thursday, September 07, 2006 6:44 PM
> Subject: Re: [R] pairwise.t.test vs. t. test
> 
> 
> > MARK LEEDS wrote:
> >> no, because the formula for the test statistics ( even assuming that
> >> variances are equal ) of the two different tests are different. in the
> >> pairwise t test, the pairwise differences are
> >> viewed as one sample so it turns into a one sample test. any intro stat 
> >> book
> >> will have the formulas.
> >>
> >>      mark
> >
> >  Actually, I think the difference is due to the SD being pooled across
> > all 3 groups in the pairwise.t.test, but just 2 groups in t.test.
> >
> >> ----- Original Message ----- 
> >> From: "Li,Qinghong,ST.LOUIS,Molecular Biology" 
> >> <Qinghong.Li at rdmo.nestle.com>
> >> To: <r-help at stat.math.ethz.ch>
> >> Sent: Thursday, September 07, 2006 5:07 PM
> >> Subject: [R] pairwise.t.test vs. t. test
> >>
> >>
> >>> Hi,
> >>>
> >>> If I set the p.adjust="none", does it meant that the output p values 
> >>> from
> >>> the pairwise.t.test will be the same as those from individual t.tests 
> >>> (set
> >>> var.equal=T, alternative="t")?
> >>>
> >>> I actually got different p values from the two tests. See below. Is it
> >>> supposed to be this way?
> >>>
> >>> Thanks
> >>> Johnny
> >>>
> >>>> x
> >>> [1] 61.6 52.7 61.3 65.2 62.8 63.7 64.8 58.7 44.9 57.0 64.3 55.1 50.0 
> >>> 41.0
> >>> [15] 43.0 45.9 52.2 45.5 46.9 31.6 40.6 44.8 39.4 31.0 37.5 32.6 23.2 
> >>> 34.6
> >>> [29] 38.3 38.1 19.5 21.2 15.8 33.3 28.6 25.8
> >>>> Grp
> >>> [1] Yng Yng Yng Yng Yng Yng Yng Yng Yng Yng Yng Yng Med Med Med Med Med
> >>> Med
> >>> [19] Med Med Med Med Med Med Old Old Old Old Old Old Old Old Old Old Old
> >>> Old
> >>> Levels: Yng Med Old
> >>>>  pairwise.t.test(x=x,g=Grp,p.adjust.method="none")
> >>>        Pairwise comparisons using t tests with pooled SD
> >>>
> >>> data:  x and Grp
> >>>
> >>>    Yng     Med
> >>> Med 1.0e-06 -
> >>> Old 2.0e-12 2.6e-05
> >>>
> >>> P value adjustment method: none
> >>>
> >>>
> >>>> t.test(x=x[1:12],y=x[25:36],var.equal=T, alternative="t")
> >>>        Two Sample t-test
> >>>
> >>> data:  x[1:12] and x[25:36]
> >>> t = 10.5986, df = 22, p-value = 4.149e-10
> >>> alternative hypothesis: true difference in means is not equal to 0
> >>> 95 percent confidence interval:
> >>> 24.37106 36.22894
> >>> sample estimates:
> >>> mean of x mean of y
> >>> 59.34167  29.04167
> >>>
> >>> [[alternative HTML version deleted]]
> >>>
> >>> ______________________________________________
> >>> R-help at stat.math.ethz.ch mailing list
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide
> >>> http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide 
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >
> > -- 
> > Chuck Cleland, Ph.D.
> > NDRI, Inc.
> > 71 West 23rd Street, 8th floor
> > New York, NY 10010
> > tel: (212) 845-4495 (Tu, Th)
> > tel: (732) 512-0171 (M, W, F)
> > fax: (917) 438-0894
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From lxweix at wm.edu  Fri Sep  8 02:35:11 2006
From: lxweix at wm.edu (Liang Wei)
Date: Thu, 7 Sep 2006 20:35:11 -0400
Subject: [R] Probabilites for all groups using knn function in R
Message-ID: <003f01c6d2de$a567be10$0200a8c0@LeonE1405>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060907/68fd1944/attachment.pl 

From ggrothendieck at gmail.com  Fri Sep  8 03:55:42 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 7 Sep 2006 21:55:42 -0400
Subject: [R] labeling graphs
In-Reply-To: <A32055BDEA88C34BB3DBBCD2293807786E3346@iu-mssg-mbx109.ads.iu.edu>
References: <A32055BDEA88C34BB3DBBCD2293807786E3346@iu-mssg-mbx109.ads.iu.edu>
Message-ID: <971536df0609071855k33f04272i8032a2db5897cb32@mail.gmail.com>

Issue this command and then click anywhere on the plot.

loc <- locator(1); do.call(text, c(loc, "abc"))


On 9/7/06, Nair, Murlidharan T <mnair at iusb.edu> wrote:
> I am trying to add text at specific location on my graph. I know this
> can be done in R but I can't recollect.
>
> I was trying using locator() to identify the position and use identify()
> but I can get it to work. Can someone jog my memory?
>
> Thanks ../Murli
>
>
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Fri Sep  8 06:23:09 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 8 Sep 2006 00:23:09 -0400
Subject: [R] area between two curves, but one is not continuous
In-Reply-To: <60ab48ec0609070949q2563665cw9aced860bc8891aa@mail.gmail.com>
References: <60ab48ec0609070949q2563665cw9aced860bc8891aa@mail.gmail.com>
Message-ID: <971536df0609072123j2a72858emfefe5865a8d3a3be@mail.gmail.com>

If you don't need borders on the polygons then it can be simply done two points
at a time checking that neither point is an NA:

# data
x1 <- x2 <- 1:8
y1 <- c(1,5,6,1,4,5,5,5)
y2 <- c(0,3,3,NA,NA,1,3,4)

# plot
plot(x1,y1,type="l")
lines(x2,y2)

# fill in area between curves with green two points at a time
for(i in seq(2, length(x1)))
   if (!any(is.na(y2[c(i-1, i)])))
      polygon(c(x1[i-1], x1[i], x2[i], x2[i-1]),
         c(y1[i-1], y1[i], y2[i], y2[i-1]),
         col = "green", border = 0)



On 9/7/06, Anton Meyer <axmeyer at googlemail.com> wrote:
> Hello,
>
> I want to colorize the area between two curves, but one of these
> curves isn't continuous.
>
> The best solution I found is the 2nd example in the help of polygon,
> but how can I get no area filling for the missing data in the 2nd curve.
>
> example:
>
> x1 = c(1:8)
> x2 = c(1:8)
> y1 = c(1,5,6,1,4,5,5,5)
> y2 = c(0,3,3,NA,NA,1,3,4)
>
> plot(x1,y1,type="l")
> lines(x2,y2)
>
> for the missing parts I want no filling.
>
> so for this examples the code would be:
> polygon(c(1:3,3:1),c(y1[1:3],rev(y2[1:3])),col="green")
> polygon(c(6:8,8:6),c(y1[6:8],rev(y2[6:8])),col="green")
>
> How can I generalize this for a longer curve with more data?


From attenka at utu.fi  Fri Sep  8 06:54:30 2006
From: attenka at utu.fi (kone)
Date: Fri, 08 Sep 2006 07:54:30 +0300
Subject: [R] Weighted association map
Message-ID: <38929107-8CB9-4C22-827F-952D371B094E@local>

Could somebody program this kind of plot type to R, if none exists,  
based on mds or correlation tables or some more suitable method? What  
do you think about idea? Does it work? None similar or better exists?

http://weightedassociationmap.blogspot.com/


Atte Tenkanen
University of Turku, Finland


From kungfista at gmx.de  Fri Sep  8 08:19:33 2006
From: kungfista at gmx.de (Rotkiv, Rehceb)
Date: Fri, 08 Sep 2006 08:19:33 +0200
Subject: [R] Axes of a histogram
In-Reply-To: <00c401c6d27b$c8fbe6c0$0540210a@www.domain>
References: <1157632513.11871.16.camel@vannili-desktop>
	<00c401c6d27b$c8fbe6c0$0540210a@www.domain>
Message-ID: <1157696373.5115.3.camel@vannili-desktop>

You're right, that's exactly what I needed! Thanks!

Rehceb

On Thu, 2006-09-07 at 14:47 +0200, Dimitris Rizopoulos wrote:
> probably you're looking for a barplot, e.g.,
> 
> v <- c(1, 1, 2, 2, 2, 3, 3, 4, 4, 4)
> plot(factor(v))
> 
> 
> I hope it helps.
> 
> Best,
> Dimitris
> 
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
> 
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/(0)16/336899
> Fax: +32/(0)16/337015
> Web: http://med.kuleuven.be/biostat/
>      http://www.student.kuleuven.be/~m0390867/dimitris.htm
> 
> 
> ----- Original Message ----- 
> From: "Rotkiv, Rehceb" <kungfista at gmx.de>
> To: <r-help at stat.math.ethz.ch>
> Sent: Thursday, September 07, 2006 2:35 PM
> Subject: [R] Axes of a histogram
> 
> 
> > Hello everyone,
> >
> > I would be glad if you could help out an R-beginner here... I have a
> > vector of categorial data like this
> >
> >> v <- c(1, 1, 2, 2, 2, 3, 3, 4, 4, 4)
> >
> > When I do
> >
> >> hist(v)
> >
> > I get the x-axis of the histogram with floating point labels: 1.0, 
> > 1.5,
> > 2.0, etc. Is it possible to tell R that the data consists of 
> > categories,
> > i.e. that I only want the category names (1, 2, 3, 4) on my x-axis?
> >
> > Thanks in advance,
> > Rehceb Rotkiv
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide 
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> > 
> 
> 
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
>


From petr.pikal at precheza.cz  Fri Sep  8 08:52:36 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 08 Sep 2006 08:52:36 +0200
Subject: [R] my error with augPred
In-Reply-To: <44FFB7C9.7060400@pdf.com>
References: <44FC2F21.3661.141D7F7@localhost>
Message-ID: <45012F54.23505.2E8DD8@localhost>

Hi Spencer

Thank you for your reply. I tried as you shad suggested and it seems 
to me that problem comes from this piece of code

contr <- object$contrasts
Browse[1]> 
debug: for (i in names(dataMix)) {
    if (inherits(dataMix[, i], "factor") && !is.null(contr[[i]])) {
        levs <- levels(dataMix[, i])
        levsC <- dimnames(contr[[i]])[[1]]
        if (any(wch <- is.na(match(levs, levsC)))) {
            stop(paste("Levels", paste(levs[wch], collapse = ","), 
                "not allowed for", i))
        }
        attr(dataMix[, i], "contrasts") <- contr[[i]][levs, , 
            drop = FALSE]
    }
}

especially from levs and levsC comparison.

levs are letters[1:3] and levsC is NULL because object$contrasts is

Browse[1]> object$contrasts
$x1
[1] "contr.treatment"

As a statistian amateur I can not say if it si a bug or not, but it 
seems to me that if in case of factors object$contrasts is always 
"contr.treatment" there is no way how to match it with actual levels 
of contrasts.

Someone more experienced has to decide about nature of this feature.

Best regards
Petr


On 6 Sep 2006 at 23:10, Spencer Graves wrote:

Date sent:      	Wed, 06 Sep 2006 23:10:17 -0700
From:           	Spencer Graves <spencer.graves at pdf.com>
To:             	Petr Pikal <petr.pikal at precheza.cz>
Copies to:      	r-help at stat.math.ethz.ch, Douglas Bates <bates at stat.wisc.edu>
Subject:        	Re: [R] my error with augPred

>       Thank you for providing such a complete, self contained example.
>        
> I found that 'predict.nlme' does not like a factor in the 'fixed'
> argument as you used it, "fixed=list(Asym~x1, R0+lrc~1)".  To see
> this, I added 'x1.' as a numeric version of the factor 'x1' and reran
> it successfully: 
> 
> fm2.<-update(fm1, fixed=list(Asym~x1., R0+lrc~1),
> start=c(103,0,-8.5,-3)) aP2. <- augPred(fm2.) plot(aP2.)
> 
>       Unfortunately, it looks like this work-around won't help you
>       with 
> your original problem, because there, the counterpart to 'x1' is an
> ordered factor with more than 2 levels. 
> 
>       The error message refers to 'predict.nlme'.  I know no reason
>       why 
> 'predict.nlme' shouldn't work with a factor with more than 2 levels in
> this context.  If it were my problem and it was sufficiently
> important, I would make a local copy of 'predict.nlme' as follows: 
> 
>       predict.nlme <- getAnywhere("predict.nlme")
> 
>       Then I'd use 'debug(nlme:::predict.nlme)' to walk through the
> problem example line by line until I figured out what I had to change
> to make this work. 
> 
>       I hesitate to use the "B" word, but I think it might be 
> appropriate to file a bug report on this;  perhaps someone else will
> do that.  
> 
>       I'm sorry I couldn't solve your original problem.  With luck,
> someone else will convert this example into a fix to the code. 
>       Spencer Graves
> 
> Petr Pikal wrote:
> > Hallo
> >
> > thank you for your response. I am not sure but maybe fixed effects
> > cannot be set to be influenced by a factor to be able to use
> > augPred.
> >
> > lob<-Loblolly[Loblolly$Seed!=321,]
> > set.seed(1)
> > lob<-data.frame(lob, x1=sample(letters[1:3], replace=T)) # add a 
> > #factor
> > lob<-groupedData(height~age|Seed, data=lob)
> > fm1 <- nlme(height ~ SSasymp(age, Asym, R0, lrc),
> >             data = lob,
> >             fixed = Asym + R0 + lrc ~ 1,
> >             random = Asym ~ 1,
> >             start = c(Asym = 103, R0 = -8.5, lrc = -3.3))
> >
> > fm2<-update(fm1, fixed=list(Asym~x1, R0+lrc~1),
> > start=c(103,0,-8.5,-3))
> >                                  ^^^^^^^
> > and
> >
> > plot(augPred(fm2))
> >
> > Throws an error.
> > So it is not possible to use augPred with such constructions.
> >
> > Best regards.
> > Petr Pikal
> >
> > On 2 Sep 2006 at 17:58, Spencer Graves wrote:
> >
> > Date sent:      	Sat, 02 Sep 2006 17:58:05 -0700
> > From:           	Spencer Graves <spencer.graves at pdf.com>
> > To:             	Petr Pikal <petr.pikal at precheza.cz>
> > Copies to:      	r-help at stat.math.ethz.ch
> > Subject:        	Re: [R] my error with augPred
> >
> >   
> >> <comments in line> 
> >>
> >> Petr Pikal wrote:
> >>     
> >>> Dear all
> >>>
> >>> I try to refine my nlme models and with partial success. The model
> >>> is refined and fitted (using Pinheiro/Bates book as a tutorial)
> >>> but when I try to plot
> >>>
> >>> plot(augPred(fit4))
> >>>
> >>> I obtain
> >>> Error in predict.nlme(object, value[1:(nrow(value)/nL), , drop =
> >>> FALSE],  : 
> >>>         Levels (0,3.5],(3.5,5],(5,7],(7,Inf] not allowed for 
> >>> vykon.fac
> >>>   
> >>>
> >>> Is it due to the fact that I have unbalanced design with not all
> >>> levels of vykon.fac present in all levels of other explanatory
> >>> factor variable?
> >>>   
> >>>       
> >> I don't know, but I'm skeptical. 
> >>     
> >>> I try to repeat 8.19 fig which is OK until I try:
> >>>
> >>> fit4 <- update(fit2, fixed = list(A+B~1,xmid~vykon.fac, scal~1),
> >>> start = c(57, 100, 700, rep(0,3), 13))
> >>>
> >>> I know I should provide an example but maybe somebody will be
> >>> clever enough to point me to an explanation without it.
> >>>   
> >>>       
> >> I'm not. 
> >>
> >> To answer these questions without an example from you, I'd have to
> >> make up my own example and try to see if I could replicate the
> >> error messages you report, and I'm not sufficiently concerned about
> >> this right now to do that. 
> >>
> >> Have you tried taking an example from the book and deleting certain
> >> rows from the data to see if you can force it to reproduce your
> >> error?
> >>
> >>
> >> Alternatively, have you tried using 'debug' to trace through the
> >> code line by line until you learn enough of what it's doing to
> >> answer your question? 
> >>
> >> Spencer Graves
> >>     
> >>> nlme version 3.1-75
> >>> SSfpl model
> >>> R 2.4.0dev (but is the same in 2.3.1), W2000.
> >>>
> >>> Thank you
> >>> Best regards.
> >>>
> >>> Petr PikalPetr Pikal
> >>> petr.pikal at precheza.cz
> >>>
> >>> ______________________________________________
> >>> R-help at stat.math.ethz.ch mailing list
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide
> >>> http://www.R-project.org/posting-guide.html and provide commented,
> >>> minimal, self-contained, reproducible code.
> >>>
> >>>       
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html and provide commented,
> >> minimal, self-contained, reproducible code.
> >>     
> >
> > Petr Pikal
> > petr.pikal at precheza.cz
> >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From myotisone at gmail.com  Fri Sep  8 09:06:16 2006
From: myotisone at gmail.com (Graham Smith)
Date: Fri, 8 Sep 2006 08:06:16 +0100
Subject: [R] subsetting a data set
Message-ID: <2c75873c0609080006n1ec66f87t9c1c0b01cfa6eaf6@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060908/dd7ad568/attachment.pl 

From rkrug at sun.ac.za  Fri Sep  8 09:52:13 2006
From: rkrug at sun.ac.za (Rainer M Krug)
Date: Fri, 08 Sep 2006 09:52:13 +0200
Subject: [R] problem with putting objects in list
In-Reply-To: <644e1f320609060817t42660f9p786a6a0734f15d94@mail.gmail.com>
References: <44FEBF99.3060401@sun.ac.za>	
	<b0808fdc0609060648k37e74aedx861b28439c11cb99@mail.gmail.com>	
	<44FED556.5010809@sun.ac.za>
	<644e1f320609060817t42660f9p786a6a0734f15d94@mail.gmail.com>
Message-ID: <4501212D.6010703@sun.ac.za>

Well - it must be that the whole concept of lists, vectors / matrices,
objects, data frames is not that clear to me. From my background
(Delphi, Basic), I am used to that, when referencing dr[i] I get the
object which is stored in the list.

Is there any manual / technical manual / reference available which
explains all this? The normal manual (Introduction into R) didn't made
it clear to me.

Rainer

jim holtman wrote:
> What is the problem with referencing it with 'dr[[i]]', the way a list
> is supposed to be referenced?
> 
> On 9/6/06, Rainer M Krug <rkrug at sun.ac.za> wrote:
>> Antonio, Fabio Di Narzo wrote:
>> > Use 'sapply' instead of 'lapply'. Type
>> If I use sapply it seems to simplify / collapse to much.
>>
>> >> ?lapply
>> > for details
>>
>>
>> >
>> > Antonio, Fabio Di Narzo.
>> > University of Bologna, Italy
>> >
>> > 2006/9/6, Rainer M Krug <rkrug at sun.ac.za>:
>> >> Hi
>> >>
>> >> I use the following code and it stores the results of density() in the
>> >> list dr:
>> >>
>> >> dens <- function(run) { density( positions$X[positions$run==run],
>> bw=3,
>> >> cut=-2 ) }
>> >> dr <- lapply(1:5, dens)
>> >>
>> >> but the results are stored in dr[[i]] and not dr[i], i.e.
>> plot(dr[[1]])
>> >> works, but plot([1]) doesn't.
>> >>
>> >> Is there any way that I can store them in dr[i]?
>> >>
>> >> Thanks a lot,
>> >>
>> >> Rainer
>> >>
>> >>
>> >>
>> >> --
>> >> Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
>> >> Biology (UCT)
>> >>
>> >> Department of Conservation Ecology and Entomology
>> >> University of Stellenbosch
>> >> Matieland 7602
>> >> South Africa
>> >>
>> >> Tel:            +27 - (0)72 808 2975 (w)
>> >> Fax:            +27 - (0)21 808 3304
>> >> Cell:           +27 - (0)83 9479 042
>> >>
>> >> email:  RKrug at sun.ac.za
>> >>         Rainer at krugs.de
>> >>
>> >> ______________________________________________
>> >> R-help at stat.math.ethz.ch mailing list
>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>> >> PLEASE do read the posting guide
>> >> http://www.R-project.org/posting-guide.html
>> >> and provide commented, minimal, self-contained, reproducible code.
>> >>
>>
>>
>> -- 
>> Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
>> Biology (UCT)
>>
>> Department of Conservation Ecology and Entomology
>> University of Stellenbosch
>> Matieland 7602
>> South Africa
>>
>> Tel:            +27 - (0)72 808 2975 (w)
>> Fax:            +27 - (0)21 808 3304
>> Cell:           +27 - (0)83 9479 042
>>
>> email:  RKrug at sun.ac.za
>>        Rainer at krugs.de
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> 


-- 
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
Biology (UCT)

Department of Conservation Ecology and Entomology
University of Stellenbosch
Matieland 7602
South Africa

Tel:		+27 - (0)72 808 2975 (w)
Fax:		+27 - (0)21 808 3304
Cell:		+27 - (0)83 9479 042

email:	RKrug at sun.ac.za
      	Rainer at krugs.de


From rkrug at sun.ac.za  Fri Sep  8 10:01:43 2006
From: rkrug at sun.ac.za (Rainer M Krug)
Date: Fri, 08 Sep 2006 10:01:43 +0200
Subject: [R] problem with putting objects in list
In-Reply-To: <44FEF529.4000307@pburns.seanet.com>
References: <44FEBF99.3060401@sun.ac.za> <44FEF529.4000307@pburns.seanet.com>
Message-ID: <45012367.4010601@sun.ac.za>

Thanks a lot - it looks like the sort of manual I was looking for -
Could I suggest of including it into the list of manuals for R? (If it
is already there, my apologies)

Rainer

Patrick Burns wrote:
> S Poetry should help you understand this.  See
> especially the section of chapter 1 on subscripting.
> 
> Patrick Burns
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
> 
> Rainer M Krug wrote:
> 
>> Hi
>>
>> I use the following code and it stores the results of density() in the
>> list dr:
>>
>> dens <- function(run) { density( positions$X[positions$run==run], bw=3,
>> cut=-2 ) }
>> dr <- lapply(1:5, dens)
>>
>> but the results are stored in dr[[i]] and not dr[i], i.e. plot(dr[[1]])
>> works, but plot([1]) doesn't.
>>
>> Is there any way that I can store them in dr[i]?
>>
>> Thanks a lot,
>>
>> Rainer
>>
>>
>>
>>  
>>


-- 
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
Biology (UCT)

Department of Conservation Ecology and Entomology
University of Stellenbosch
Matieland 7602
South Africa

Tel:		+27 - (0)72 808 2975 (w)
Fax:		+27 - (0)21 808 3304
Cell:		+27 - (0)83 9479 042

email:	RKrug at sun.ac.za
      	Rainer at krugs.de


From rkrug at sun.ac.za  Fri Sep  8 10:07:49 2006
From: rkrug at sun.ac.za (Rainer M Krug)
Date: Fri, 08 Sep 2006 10:07:49 +0200
Subject: [R] problem with putting objects in list
In-Reply-To: <44FF0F47.2070703@acm.org>
References: <44FEBF99.3060401@sun.ac.za> <44FF0F47.2070703@acm.org>
Message-ID: <450124D5.9080704@sun.ac.za>

Thanks a lot Tony

this explains a lot.
I am not aware to have read such a good explanation about this issue
before. I would suggest to add it to the FAQs.

Rainer

Tony Plate wrote:
> I suspect you are not thinking about the list and the
> subsetting/extraction operators in the right way.
> 
> A list contains a number of components.
> 
> To get a subset of the list, use the '[' operator.  The subset can
> contain zero or more components of the list, and it is a list itself.
> So, if x is a list, then x[2] is a list containing a single component.
> 
> To extract a component from the list, use the '[[' operator.  You can
> only extract one component at a time.  If you supply a vector index with
> more than one element, it will index recursively.
> 
>> x <- list(1,2:3,letters[1:3])
>> x
> [[1]]
> [1] 1
> 
> [[2]]
> [1] 2 3
> 
> [[3]]
> [1] "a" "b" "c"
> 
>> # a subset of the list
>> x[2:3]
> [[1]]
> [1] 2 3
> 
> [[2]]
> [1] "a" "b" "c"
> 
>> # a list with one component:
>> x[2]
> [[1]]
> [1] 2 3
> 
>> # the second component itself
>> x[[2]]
> [1] 2 3
>> # recursive indexing
>> x[[c(2,1)]]
> [1] 2
>> x[[c(3,2)]]
> [1] "b"
>>
> 
> Rainer M Krug wrote:
>> Hi
>>
>> I use the following code and it stores the results of density() in the
>> list dr:
>>
>> dens <- function(run) { density( positions$X[positions$run==run], bw=3,
>> cut=-2 ) }
>> dr <- lapply(1:5, dens)
>>
>> but the results are stored in dr[[i]] and not dr[i], i.e. plot(dr[[1]])
>> works, but plot([1]) doesn't.
>>
>> Is there any way that I can store them in dr[i]?
>>
>> Thanks a lot,
>>
>> Rainer
>>
>>
>>
> 


-- 
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
Biology (UCT)

Department of Conservation Ecology and Entomology
University of Stellenbosch
Matieland 7602
South Africa

Tel:		+27 - (0)72 808 2975 (w)
Fax:		+27 - (0)21 808 3304
Cell:		+27 - (0)83 9479 042

email:	RKrug at sun.ac.za
      	Rainer at krugs.de


From attenka at utu.fi  Fri Sep  8 10:16:45 2006
From: attenka at utu.fi (kone)
Date: Fri, 08 Sep 2006 11:16:45 +0300
Subject: [R] Weighted association map
In-Reply-To: <38929107-8CB9-4C22-827F-952D371B094E@local>
References: <38929107-8CB9-4C22-827F-952D371B094E@local>
Message-ID: <D979C34B-0421-4DD5-8491-0CBDE06369B5@local>


> Could somebody program this kind of plot type to R, if none exists,  
> based on mds or correlation tables or some more suitable method?  
> What do you think about idea? Does it work? None similar or better  
> exists?
>
> http://weightedassociationmap.blogspot.com/
>
>
> Atte Tenkanen
> University of Turku, Finland



I got a hint of package ade4 and function dudi.pca. Something like  
this, though I can't immediately interpret those dudi.pca-pictures. I  
originally saw this as a more general idea, which can, if wanted, be  
applied with statistical methods too. If you do multidimensional  
scaling, you can see, how "near" all components are each other, but  
you can't see the connections or directions. If you have a  
correlation matrix, you can't see the connections graphically.

Atte


From petr.pikal at precheza.cz  Fri Sep  8 10:31:12 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 08 Sep 2006 10:31:12 +0200
Subject: [R] subsetting a data set
In-Reply-To: <2c75873c0609080006n1ec66f87t9c1c0b01cfa6eaf6@mail.gmail.com>
Message-ID: <45014670.8509.88D471@localhost>

Hi

I am not sure if your Max is the same as max so I am not sure what 
you exactly want from your data. However you shall consult ?tapply, 
?by, ?aggregate and maybe also ?"[" together with chapter 2 in intro 
manual in docs directory.

aggregate(data[, some.columns], list(data$factor1, data$factor2), 
max)

will give you maximum for specified columns based on spliting the 
data according to both factors

Also connection summary with max is not common and I wonder what is 
your output in this case. I believe that there are six same numbers. 
However R is case sensitive and maybe Max does something different 
from max. In my case it throws an error.

HTH
Petr

On 8 Sep 2006 at 8:06, Graham Smith wrote:

Date sent:      	Fri, 8 Sep 2006 08:06:16 +0100
From:           	"Graham Smith" <myotisone at gmail.com>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] subsetting a data set

> I have a data set called GQ1, which has 20 variables one of which is a
> factor called Status at thre levels "Expert", "Ecol" and "Stake"
> 
> I have managed to evaluate some of the data split by status using
> commands like:
> 
> summary (Max[Status=="Ecol"])
> 
> BUT how do I produce  asummary for Ecol and Expert combined, the only
> example I can find suggsts I could use
> 
> summary (Max[Status=="Ecol"& Status=="Expert"]) but that doesn't work.
> 
> Additionally on the same vein, if I cannot work out how to create a
> new data set that would contain all the data for all the variables but
> only for the data where Status = Ecol, or where status equalles Ecol
> and Expert.
> 
> I know this is yet again a very simple problem, but I really can't
> find the solution in the help or the books I have.
> 
> Many thanks,
> 
> Graham
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From myotisone at gmail.com  Fri Sep  8 11:03:51 2006
From: myotisone at gmail.com (Graham Smith)
Date: Fri, 8 Sep 2006 10:03:51 +0100
Subject: [R] subsetting a data set
In-Reply-To: <45014670.8509.88D471@localhost>
References: <2c75873c0609080006n1ec66f87t9c1c0b01cfa6eaf6@mail.gmail.com>
	<45014670.8509.88D471@localhost>
Message-ID: <2c75873c0609080203s5d2cbeycbbb60812b613527@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060908/a82a9314/attachment.pl 

From petr.pikal at precheza.cz  Fri Sep  8 11:13:49 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 08 Sep 2006 11:13:49 +0200
Subject: [R] subsetting a data set
In-Reply-To: <45014670.8509.88D471@localhost>
References: <2c75873c0609080006n1ec66f87t9c1c0b01cfa6eaf6@mail.gmail.com>
Message-ID: <4501506D.27026.AFE607@localhost>

Sorry, I did not notice that in your case Max is not a function but 
your data. So probably

by(Max[, your.columns], list(Max$status), summary)

is maybe what you want.
HTH
Petr


On 8 Sep 2006 at 10:31, Petr Pikal wrote:

From:           	"Petr Pikal" <petr.pikal at precheza.cz>
To:             	"Graham Smith" <myotisone at gmail.com>, r-help at stat.math.ethz.ch
Date sent:      	Fri, 08 Sep 2006 10:31:12 +0200
Priority:       	normal
Subject:        	Re: [R] subsetting a data set

> Hi
> 
> I am not sure if your Max is the same as max so I am not sure what you
> exactly want from your data. However you shall consult ?tapply, ?by,
> ?aggregate and maybe also ?"[" together with chapter 2 in intro manual
> in docs directory.
> 
> aggregate(data[, some.columns], list(data$factor1, data$factor2), max)
> 
> will give you maximum for specified columns based on spliting the data
> according to both factors
> 
> Also connection summary with max is not common and I wonder what is
> your output in this case. I believe that there are six same numbers.
> However R is case sensitive and maybe Max does something different
> from max. In my case it throws an error.
> 
> HTH
> Petr
> 
> On 8 Sep 2006 at 8:06, Graham Smith wrote:
> 
> Date sent:      	Fri, 8 Sep 2006 08:06:16 +0100
> From:           	"Graham Smith" <myotisone at gmail.com>
> To:             	r-help at stat.math.ethz.ch
> Subject:        	[R] subsetting a data set
> 
> > I have a data set called GQ1, which has 20 variables one of which is
> > a factor called Status at thre levels "Expert", "Ecol" and "Stake"
> > 
> > I have managed to evaluate some of the data split by status using
> > commands like:
> > 
> > summary (Max[Status=="Ecol"])
> > 
> > BUT how do I produce  asummary for Ecol and Expert combined, the
> > only example I can find suggsts I could use
> > 
> > summary (Max[Status=="Ecol"& Status=="Expert"]) but that doesn't
> > work.
> > 
> > Additionally on the same vein, if I cannot work out how to create a
> > new data set that would contain all the data for all the variables
> > but only for the data where Status = Ecol, or where status equalles
> > Ecol and Expert.
> > 
> > I know this is yet again a very simple problem, but I really can't
> > find the solution in the help or the books I have.
> > 
> > Many thanks,
> > 
> > Graham
> > 
> >  [[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented,
> > minimal, self-contained, reproducible code.
> 
> Petr Pikal
> petr.pikal at precheza.cz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From petr.pikal at precheza.cz  Fri Sep  8 11:26:53 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 08 Sep 2006 11:26:53 +0200
Subject: [R] subsetting a data set
In-Reply-To: <2c75873c0609080203s5d2cbeycbbb60812b613527@mail.gmail.com>
References: <45014670.8509.88D471@localhost>
Message-ID: <4501537D.28013.BBDB9D@localhost>

Hi

if you use summary aggregate probably will not work and tapply have 
to be called differently

tapply(seq(along=Max[,1]), list(Max$Status), function(i, x) 
summary(x[i]), x=Max[,one.column])

or you can use by

by(Max[,1:5]), list(Max$Status), summary)

or if you do not like the output  something like that

lll <- lapply(as.list(Max[,your.columns]), function(x) 
sapply(split(x,Max$Status),summary))
do.call("rbind",lll)
or
do.call("data.frame",lll)

HTH
Petr

On 8 Sep 2006 at 10:03, Graham Smith wrote:

Date sent:      	Fri, 8 Sep 2006 10:03:51 +0100
From:           	"Graham Smith" <myotisone at gmail.com>
To:             	"Petr Pikal" <petr.pikal at precheza.cz>
Copies to:      	r-help at stat.math.ethz.ch
Subject:        	Re: [R] subsetting a data set

> Petr,
> 
> Thanks I shall have at look at these options.
> 
> Sorry about the confusion with the "Max", in my example "Max" is the
> name of the variable that I am summarising. I chose a poor example to
> cut and paste form R, not thinking about the obvious confusion this
> would cause.
> 
> Thanks again
> 
> Graham
> 
> On 08/09/06, Petr Pikal <petr.pikal at precheza.cz> wrote:
> >
> > Hi
> >
> > I am not sure if your Max is the same as max so I am not sure what
> > you exactly want from your data. However you shall consult ?tapply,
> > ?by, ?aggregate and maybe also ?"[" together with chapter 2 in intro
> > manual in docs directory.
> >
> > aggregate(data[, some.columns], list(data$factor1, data$factor2),
> > max)
> >
> > will give you maximum for specified columns based on spliting the
> > data according to both factors
> >
> > Also connection summary with max is not common and I wonder what is
> > your output in this case. I believe that there are six same numbers.
> > However R is case sensitive and maybe Max does something different
> > from max. In my case it throws an error.
> >
> > HTH
> > Petr
> >
> > On 8 Sep 2006 at 8:06, Graham Smith wrote:
> >
> > Date sent:              Fri, 8 Sep 2006 08:06:16 +0100
> > From:                   "Graham Smith" < myotisone at gmail.com>
> > To:                     r-help at stat.math.ethz.ch
> > Subject:                [R] subsetting a data set
> >
> > > I have a data set called GQ1, which has 20 variables one of which
> > > is a factor called Status at thre levels "Expert", "Ecol" and
> > > "Stake"
> > >
> > > I have managed to evaluate some of the data split by status using
> > > commands like:
> > >
> > > summary (Max[Status=="Ecol"])
> > >
> > > BUT how do I produce  asummary for Ecol and Expert combined, the
> > > only example I can find suggsts I could use
> > >
> > > summary (Max[Status=="Ecol"& Status=="Expert"]) but that doesn't
> > > work.
> > >
> > > Additionally on the same vein, if I cannot work out how to create
> > > a new data set that would contain all the data for all the
> > > variables but only for the data where Status = Ecol, or where
> > > status equalles Ecol and Expert.
> > >
> > > I know this is yet again a very simple problem, but I really can't
> > > find the solution in the help or the books I have.
> > >
> > > Many thanks,
> > >
> > > Graham
> > >
> > >  [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html and provide commented,
> > > minimal, self-contained, reproducible code.
> >
> > Petr Pikal
> > petr.pikal at precheza.cz
> >
> >
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From myotisone at gmail.com  Fri Sep  8 11:33:49 2006
From: myotisone at gmail.com (Graham Smith)
Date: Fri, 8 Sep 2006 10:33:49 +0100
Subject: [R] subsetting a data set
In-Reply-To: <4501506D.27026.AFE607@localhost>
References: <2c75873c0609080006n1ec66f87t9c1c0b01cfa6eaf6@mail.gmail.com>
	<45014670.8509.88D471@localhost> <4501506D.27026.AFE607@localhost>
Message-ID: <2c75873c0609080233t7ffabd57ld69cb597a16a182@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060908/51535948/attachment.pl 

From ggrothendieck at gmail.com  Fri Sep  8 11:37:22 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 8 Sep 2006 05:37:22 -0400
Subject: [R] Weighted association map
In-Reply-To: <38929107-8CB9-4C22-827F-952D371B094E@local>
References: <38929107-8CB9-4C22-827F-952D371B094E@local>
Message-ID: <971536df0609080237q12aa5bc7pb9069d0a2dca57a5@mail.gmail.com>

Try the sna package.  Below we calculate the
correlation matrix, kor, of the numeric cols of builtin iris
dataset.  Zap negative ones and discretize rest to
get lwd width matrix, lwd, used for edge widths.  From
that create the adjacency matrix, sign(lwd), and plot it
using indicated layout mode.  Seems like three of
the variables are correlated and Sepal.Width is uncorrelated
or negatively correlated to those. Try playing around with
gplot args to create variations.

library(sna)
set.seed(123) # layout uses random numbers
kor <- cor(iris[1:4])
lwd <- replace(kor, TRUE, 10 * round(pmax(0, kor), 1))
gplot(sign(lwd), edge.lwd = lwd, displaylabels = TRUE, label = rownames(kor))

On 9/8/06, kone <attenka at utu.fi> wrote:
> Could somebody program this kind of plot type to R, if none exists,
> based on mds or correlation tables or some more suitable method? What
> do you think about idea? Does it work? None similar or better exists?
>
> http://weightedassociationmap.blogspot.com/
>
>
> Atte Tenkanen
> University of Turku, Finland
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From rleigh at whinlatter.ukfsn.org  Fri Sep  8 11:41:00 2006
From: rleigh at whinlatter.ukfsn.org (Roger Leigh)
Date: Fri, 08 Sep 2006 10:41:00 +0100
Subject: [R] Computing skewness and kurtosis with the moments package
Message-ID: <871wqmbsr7.fsf@hardknott.home>

Hi,

I'm a newcomer to R, having previously used SPSS.  One problem I have
run into is computing kurtosis.  A test dataset is here:

http://www.whinlatter.ukfsn.org/2401.dat

> library(moments)
> data <- read.table("2401.dat", header=T)
> attach(data)
> loglen <- log10(Length)

With SPSS, I get
  Skewness -0.320
  Kurtosis -1.138

With R:
> skewness(loglen)
[1] -0.317923
> kurtosis(loglen)
[1] 1.860847

Using the example skew and kurtosis functions from M. J. Crawley's
"Statistics: An introduction using R": pp 69 and 72:

> mskew(loglen)
[1] -0.3158337
> mkurtosis(loglen)
[1] -1.155441

The kurtosis value here matches the SPSS calculation somewhat more
closely, but is still not exactly the same.

Looking at the functions, there is some difference between them:

> skewness
function (x, na.rm = FALSE) 
{
    if (is.matrix(x)) 
        apply(x, 2, skewness, na.rm = na.rm)
    else if (is.vector(x)) {
        if (na.rm) 
            x <- x[!is.na(x)]
        n <- length(x)
        (sum((x - mean(x))^3)/n)/(sum((x - mean(x))^2)/n)^(3/2)
    }
    else if (is.data.frame(x)) 
        sapply(x, skewness, na.rm = na.rm)
    else skewness(as.vector(x), na.rm = na.rm)
}
> mskew
function(x) {
m3 <- sum((x - mean(x))^3)/length(x)
s3 <- sqrt(var(x))^3
m3/s3
}
> kurtosis
function (x, na.rm = FALSE) 
{
    if (is.matrix(x)) 
        apply(x, 2, kurtosis, na.rm = na.rm)
    else if (is.vector(x)) {
        if (na.rm) 
            x <- x[!is.na(x)]
        n <- length(x)
        n * sum((x - mean(x))^4)/(sum((x - mean(x))^2)^2)
    }
    else if (is.data.frame(x)) 
        sapply(x, kurtosis, na.rm = na.rm)
    else kurtosis(as.vector(x), na.rm = na.rm)
}
> mkurtosis
function(x) {
m4 <- sum((x - mean(x))^4)/length(x)
s4 <- var(x)^2
m4/s4 - 3
}

Are any of these functions incorrect, or are there several different
methods of computing the skew and kurtosis values?

Are there any more appropriate R packages I should consider using?


Many thanks,
Roger

-- 
  .''`.  Roger Leigh
 : :' :  Debian GNU/Linux             http://people.debian.org/~rleigh/
 `. `'   Printing on GNU/Linux?       http://gutenprint.sourceforge.net/
   `-    GPG Public Key: 0x25BFB848   Please GPG sign your mail.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 188 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20060908/a99f6758/attachment.bin 

From balajis at stanford.edu  Fri Sep  8 11:51:47 2006
From: balajis at stanford.edu (Balaji S. Srinivasan)
Date: Fri, 8 Sep 2006 02:51:47 -0700
Subject: [R] R drop behavior -- set as option in later version?
Message-ID: <39422c340609080251u6c367b5ch3ad0cbe33a87d015@mail.gmail.com>

Hi,

I know the topic of drop=TRUE/FALSE has been discussed quite a bit, but
I was wondering whether it might be possible to set "drop=FALSE" as a
global setting (e.g. as an option in options()) so that one does not
have to remember
to write it every time you do an operation which might return a 1
column or 1 row matrix.

I searched in R-help and did not see any previous proposals along these lines,
though I may be mistaken. I think this might be a solution that doesn't break
existing code, yet does allow the use of native matrices & data frames (rather
than using the Matrix package, for example) without tons of drop=FALSE
statements.

I may be mistaken, but it seems like this would only require changes
to a few lines of code.

In main/subset.c, surrounding line 505, there is the following code:

------------
/* Extracts the drop argument, if present, from the argument list.
   The object being subsetted must be the first argument. */
static void ExtractDropArg(SEXP el, int *drop)
{
    SEXP last = el;
    for (el = CDR(el); el != R_NilValue; el = CDR(el)) {
        if(TAG(el) == R_DropSymbol) {
            *drop = asLogical(CAR(el));
            if (*drop == NA_LOGICAL) *drop = 1;
            SETCDR(last, CDR(el));
        }
        else last = el;
    }
}
------------

This is not exactly the right syntax for a GetOption call, but
something along the following lines
should allow globally settable drop  by modifying line 505:


------------
if(*drop == NA_LOGICAL) {
    GetOption("GlobalDrop",drop);   //assume drop is being modified by
reference to be set to whatever the GlobalDrop option is set to.
}
------------


The possible caveats are:

1) Performance issues associated with GetOption call (this is probably
not a showstopper)
2) Breaking old code which assumes drop=TRUE if the drop=FALSE option is set.

For the latter situation, it would be possible to wrap any new code
with an options setting as follows:

og <- options("GlobalDrop")
options(GlobalDrop=FALSE)
#new code here which assumes drop=FALSE
options(og)

This does require three more lines of code, but still much less typing
(and probably a lower error rate) than including drop=FALSE with every
matrix subset call which might result in a 1D row or column vector.

Of course, in the case that you were calling subroutines and/or
libraries in which drop=TRUE was
assumed, then you might not want to do this as you would have to wrap
every call to these subroutines with an explicit setting of the drop
option.

Even still, I think the reduction in error rate/repetitive typing
might be worth it. You could imagine just setting the global option,
running through a few hundred lines of code with a lot of matrix
subsetting, and then resetting it. Alternatives like subset and
Extract are, like drop=FALSE, more verbose than the simple bracket
notation.

Anyway, this is just a thought, though I would be very happy if the R
maintainers decided to incorporate this change.


-- 
Balaji S. Srinivasan
Stanford University
Depts. of Statistics and Computer Science
318 Campus Drive, Clark Center S251
(650) 380-0695
balajis at stanford.edu
http://jinome.stanford.edu


From seanpor at acm.org  Fri Sep  8 13:05:44 2006
From: seanpor at acm.org (Sean O'Riordain)
Date: Fri, 8 Sep 2006 11:05:44 +0000
Subject: [R] subsetting a data set
In-Reply-To: <2c75873c0609080233t7ffabd57ld69cb597a16a182@mail.gmail.com>
References: <2c75873c0609080006n1ec66f87t9c1c0b01cfa6eaf6@mail.gmail.com>
	<45014670.8509.88D471@localhost> <4501506D.27026.AFE607@localhost>
	<2c75873c0609080233t7ffabd57ld69cb597a16a182@mail.gmail.com>
Message-ID: <8ed68eed0609080405q2e8b5874m6a22b43ec7fd207b@mail.gmail.com>

Hi Graham,
Try creating a new column with the two levels that you want...

something along the lines of (warning untested!!!)

GQ1[(GQ1$Status == "Expert) | (GQ1$Status == "Ecol"),]$newColumn <- "AllEcol"
GQ1[GQ1$Status == "Stake",]$newColumn <- "Stake"

and then do the
by(GQ1[,"Max"], list(GQ1$NewColumn), summary)

when in doubt... break the problem into smaller chunks... :-)

cheers,
Sean

On 08/09/06, Graham Smith <myotisone at gmail.com> wrote:
> Petr,
>
> Thanks again, but the data is GQ1, Max is a variable (column)
>
> So I have used
>
>  by(GQ1[,"Max"], list(GQ1$Status), summary)
>
> Which is very good,  and is better than the way I did it before by
> summarising for each status level individually, but that still isn't combing
> the data for Status == "Expert" and Status = "Ecol"
>
> So at the moment the status variable has 3 levels Expert, Ecol and Stake,
>
> I want to analsye that at two levels: Expert and Ecol combined into a new
> level called "AllEcol" and the exsiting level "Stake"
>
> It is this combining the levels that has got me stuck.
>
> Thanks again,
>
> Graham
>
> On 08/09/06, Petr Pikal <petr.pikal at precheza.cz> wrote:
> >
> > Sorry, I did not notice that in your case Max is not a function but
> > your data. So probably
> >
> > by(Max[, your.columns], list(Max$status), summary)
> >
> > is maybe what you want.
> > HTH
> > Petr
> >
> >
> > On 8 Sep 2006 at 10:31, Petr Pikal wrote:
> >
> > From:                   "Petr Pikal" <petr.pikal at precheza.cz>
> > To:                     "Graham Smith" <myotisone at gmail.com>,
> > r-help at stat.math.ethz.ch
> > Date sent:              Fri, 08 Sep 2006 10:31:12 +0200
> > Priority:               normal
> > Subject:                Re: [R] subsetting a data set
> >
> > > Hi
> > >
> > > I am not sure if your Max is the same as max so I am not sure what you
> > > exactly want from your data. However you shall consult ?tapply, ?by,
> > > ?aggregate and maybe also ?"[" together with chapter 2 in intro manual
> > > in docs directory.
> > >
> > > aggregate(data[, some.columns], list(data$factor1, data$factor2), max)
> > >
> > > will give you maximum for specified columns based on spliting the data
> > > according to both factors
> > >
> > > Also connection summary with max is not common and I wonder what is
> > > your output in this case. I believe that there are six same numbers.
> > > However R is case sensitive and maybe Max does something different
> > > from max. In my case it throws an error.
> > >
> > > HTH
> > > Petr
> > >
> > > On 8 Sep 2006 at 8:06, Graham Smith wrote:
> > >
> > > Date sent:            Fri, 8 Sep 2006 08:06:16 +0100
> > > From:                 "Graham Smith" <myotisone at gmail.com>
> > > To:                   r-help at stat.math.ethz.ch
> > > Subject:              [R] subsetting a data set
> > >
> > > > I have a data set called GQ1, which has 20 variables one of which is
> > > > a factor called Status at thre levels "Expert", "Ecol" and "Stake"
> > > >
> > > > I have managed to evaluate some of the data split by status using
> > > > commands like:
> > > >
> > > > summary (Max[Status=="Ecol"])
> > > >
> > > > BUT how do I produce  asummary for Ecol and Expert combined, the
> > > > only example I can find suggsts I could use
> > > >
> > > > summary (Max[Status=="Ecol"& Status=="Expert"]) but that doesn't
> > > > work.
> > > >
> > > > Additionally on the same vein, if I cannot work out how to create a
> > > > new data set that would contain all the data for all the variables
> > > > but only for the data where Status = Ecol, or where status equalles
> > > > Ecol and Expert.
> > > >
> > > > I know this is yet again a very simple problem, but I really can't
> > > > find the solution in the help or the books I have.
> > > >
> > > > Many thanks,
> > > >
> > > > Graham
> > > >
> > > >  [[alternative HTML version deleted]]
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide
> > > > http://www.R-project.org/posting-guide.html and provide commented,
> > > > minimal, self-contained, reproducible code.
> > >
> > > Petr Pikal
> > > petr.pikal at precheza.cz
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html and provide commented,
> > > minimal, self-contained, reproducible code.
> >
> > Petr Pikal
> > petr.pikal at precheza.cz
> >
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From myotisone at gmail.com  Fri Sep  8 13:11:23 2006
From: myotisone at gmail.com (Graham Smith)
Date: Fri, 8 Sep 2006 12:11:23 +0100
Subject: [R] subsetting a data set
In-Reply-To: <8ed68eed0609080405q2e8b5874m6a22b43ec7fd207b@mail.gmail.com>
References: <2c75873c0609080006n1ec66f87t9c1c0b01cfa6eaf6@mail.gmail.com>
	<45014670.8509.88D471@localhost> <4501506D.27026.AFE607@localhost>
	<2c75873c0609080233t7ffabd57ld69cb597a16a182@mail.gmail.com>
	<8ed68eed0609080405q2e8b5874m6a22b43ec7fd207b@mail.gmail.com>
Message-ID: <2c75873c0609080411r63c282ffn4fbed30e38850bf5@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060908/e3d1a4e8/attachment.pl 

From ffenics2002 at yahoo.co.uk  Fri Sep  8 13:16:51 2006
From: ffenics2002 at yahoo.co.uk (Ffenics)
Date: Fri, 8 Sep 2006 12:16:51 +0100 (BST)
Subject: [R] Connecting to R using Perl?
Message-ID: <20060908111651.71091.qmail@web25505.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060908/206116d6/attachment.pl 

From petr.pikal at precheza.cz  Fri Sep  8 13:19:52 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 08 Sep 2006 13:19:52 +0200
Subject: [R] subsetting a data set
In-Reply-To: <2c75873c0609080233t7ffabd57ld69cb597a16a182@mail.gmail.com>
References: <4501506D.27026.AFE607@localhost>
Message-ID: <45016DF8.2655.1234B4A@localhost>

Hi

On 8 Sep 2006 at 10:33, Graham Smith wrote:

Date sent:      	Fri, 8 Sep 2006 10:33:49 +0100
From:           	"Graham Smith" <myotisone at gmail.com>
To:             	"Petr Pikal" <petr.pikal at precheza.cz>
Copies to:      	r-help at stat.math.ethz.ch
Subject:        	Re: [R] subsetting a data set

> Petr,
> 
> Thanks again, but the data is GQ1, Max is a variable (column)
> 
> So I have used
> 
>  by(GQ1[,"Max"], list(GQ1$Status), summary)
> 
> Which is very good,  and is better than the way I did it before by
> summarising for each status level individually, but that still isn't
> combing the data for Status == "Expert" and Status = "Ecol"
> 
> So at the moment the status variable has 3 levels Expert, Ecol and
> Stake,

look at ?factors how to deal with factors, if your variable is not a 
factor (see ?str) than turn it to one.

x<-sample(letters[1:3], 20, replace=T) #character
x.f<-as.factor(x) #turn to factor
> x.f
 [1] b c b a c a c a a a a a b c c c b b c b
Levels: a b c
> levels(x.f)<-c("x","x","y") #rename levels
> x.f
 [1] x y x x y x y x x x x x x y y y x x y x
Levels: x y
>
> 
> I want to analsye that at two levels: Expert and Ecol combined into a
> new level called "AllEcol" and the exsiting level "Stake"

so in your case something like 

GQ1$statusComb<-factor(GQ1$status, labels=c("AllEcol","AllEcol", 
"Stake"))

shall do it. Beware of label ordering!!!

BTW. It had been good if you provided a usable example, as stated in 
posting guide. Many times trying to elaborate an example I will solve 
the problem myself.

HTH
Petr

> 
> It is this combining the levels that has got me stuck.
> 
> Thanks again,
> 
> Graham
> 
> On 08/09/06, Petr Pikal <petr.pikal at precheza.cz> wrote:
> >
> > Sorry, I did not notice that in your case Max is not a function but
> > your data. So probably
> >
> > by(Max[, your.columns], list(Max$status), summary)
> >
> > is maybe what you want.
> > HTH
> > Petr
> >
> >
> > On 8 Sep 2006 at 10:31, Petr Pikal wrote:
> >
> > From:                   "Petr Pikal" <petr.pikal at precheza.cz>
> > To:                     "Graham Smith" <myotisone at gmail.com>,
> > r-help at stat.math.ethz.ch
> > Date sent:              Fri, 08 Sep 2006 10:31:12 +0200
> > Priority:               normal
> > Subject:                Re: [R] subsetting a data set
> >
> > > Hi
> > >
> > > I am not sure if your Max is the same as max so I am not sure what
> > > you exactly want from your data. However you shall consult
> > > ?tapply, ?by, ?aggregate and maybe also ?"[" together with chapter
> > > 2 in intro manual in docs directory.
> > >
> > > aggregate(data[, some.columns], list(data$factor1, data$factor2),
> > > max)
> > >
> > > will give you maximum for specified columns based on spliting the
> > > data according to both factors
> > >
> > > Also connection summary with max is not common and I wonder what
> > > is your output in this case. I believe that there are six same
> > > numbers. However R is case sensitive and maybe Max does something
> > > different from max. In my case it throws an error.
> > >
> > > HTH
> > > Petr
> > >
> > > On 8 Sep 2006 at 8:06, Graham Smith wrote:
> > >
> > > Date sent:            Fri, 8 Sep 2006 08:06:16 +0100
> > > From:                 "Graham Smith" <myotisone at gmail.com>
> > > To:                   r-help at stat.math.ethz.ch
> > > Subject:              [R] subsetting a data set
> > >
> > > > I have a data set called GQ1, which has 20 variables one of
> > > > which is a factor called Status at thre levels "Expert", "Ecol"
> > > > and "Stake"
> > > >
> > > > I have managed to evaluate some of the data split by status
> > > > using commands like:
> > > >
> > > > summary (Max[Status=="Ecol"])
> > > >
> > > > BUT how do I produce  asummary for Ecol and Expert combined, the
> > > > only example I can find suggsts I could use
> > > >
> > > > summary (Max[Status=="Ecol"& Status=="Expert"]) but that doesn't
> > > > work.
> > > >
> > > > Additionally on the same vein, if I cannot work out how to
> > > > create a new data set that would contain all the data for all
> > > > the variables but only for the data where Status = Ecol, or
> > > > where status equalles Ecol and Expert.
> > > >
> > > > I know this is yet again a very simple problem, but I really
> > > > can't find the solution in the help or the books I have.
> > > >
> > > > Many thanks,
> > > >
> > > > Graham
> > > >
> > > >  [[alternative HTML version deleted]]
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide
> > > > http://www.R-project.org/posting-guide.html and provide
> > > > commented, minimal, self-contained, reproducible code.
> > >
> > > Petr Pikal
> > > petr.pikal at precheza.cz
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html and provide commented,
> > > minimal, self-contained, reproducible code.
> >
> > Petr Pikal
> > petr.pikal at precheza.cz
> >
> >
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From myotisone at gmail.com  Fri Sep  8 13:31:37 2006
From: myotisone at gmail.com (Graham Smith)
Date: Fri, 8 Sep 2006 12:31:37 +0100
Subject: [R] subsetting a data set
In-Reply-To: <8ed68eed0609080405q2e8b5874m6a22b43ec7fd207b@mail.gmail.com>
References: <2c75873c0609080006n1ec66f87t9c1c0b01cfa6eaf6@mail.gmail.com>
	<45014670.8509.88D471@localhost> <4501506D.27026.AFE607@localhost>
	<2c75873c0609080233t7ffabd57ld69cb597a16a182@mail.gmail.com>
	<8ed68eed0609080405q2e8b5874m6a22b43ec7fd207b@mail.gmail.com>
Message-ID: <2c75873c0609080431g7ad901dfp68bd00f008254160@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060908/6e5b2a77/attachment.pl 

From gregor.gorjanc at bfro.uni-lj.si  Fri Sep  8 13:33:05 2006
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Fri, 8 Sep 2006 11:33:05 +0000 (UTC)
Subject: [R] R drop behavior -- set as option in later version?
References: <39422c340609080251u6c367b5ch3ad0cbe33a87d015@mail.gmail.com>
Message-ID: <loom.20060908T132749-283@post.gmane.org>

Balaji S. Srinivasan <balajis <at> stanford.edu> writes:
> ...
Hello,

I agree with you that you find yourself typing the same constructs over and
over. I think that we need to distinguish two modes of working with R. If you do
analysis, then you can really get tired of typing drop=FALSE, na.rm=TRUE etc.
But this things are very important when you are programming in R. Since these
two modes are not really separated in R I do not think this is an easy task, but
it would be great to have it. I had recently the same question for na.rm=TRUE.
But imagine how hard would it be to have two separate modes ... argh, probably a
mess^2 or have I missed something obvious.

> The possible caveats are:
> 
> 1) Performance issues associated with GetOption call (this is probably
> not a showstopper)
> 2) Breaking old code which assumes drop=TRUE if the drop=FALSE option is set.
> 
> For the latter situation, it would be possible to wrap any new code
> with an options setting as follows:
> 
> og <- options("GlobalDrop")
> options(GlobalDrop=FALSE)
> #new code here which assumes drop=FALSE
> options(og)

I think that local options i.e. drop=TRUE in code should override global ones.

Gregor


From myotisone at gmail.com  Fri Sep  8 13:38:45 2006
From: myotisone at gmail.com (Graham Smith)
Date: Fri, 8 Sep 2006 12:38:45 +0100
Subject: [R] subsetting a data set
In-Reply-To: <45016DF8.2655.1234B4A@localhost>
References: <4501506D.27026.AFE607@localhost>
	<2c75873c0609080233t7ffabd57ld69cb597a16a182@mail.gmail.com>
	<45016DF8.2655.1234B4A@localhost>
Message-ID: <2c75873c0609080438u6cfae70ve5e9b97bfb57e407@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060908/ebfbed74/attachment.pl 

From ggrothendieck at gmail.com  Fri Sep  8 13:53:21 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 8 Sep 2006 07:53:21 -0400
Subject: [R] Weighted association map
In-Reply-To: <971536df0609080237q12aa5bc7pb9069d0a2dca57a5@mail.gmail.com>
References: <38929107-8CB9-4C22-827F-952D371B094E@local>
	<971536df0609080237q12aa5bc7pb9069d0a2dca57a5@mail.gmail.com>
Message-ID: <971536df0609080453p2565c7d5o69d07e895fde4ba4@mail.gmail.com>

Actually the discretization does not appear to be needed.  This
works just as well:

set.seed(123)
kor <- cor(iris[1:4])
gplot(sign(kor), edge.lwd = 10*kor, displaylabels = TRUE, label = rownames(kor))

On 9/8/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Try the sna package.  Below we calculate the
> correlation matrix, kor, of the numeric cols of builtin iris
> dataset.  Zap negative ones and discretize rest to
> get lwd width matrix, lwd, used for edge widths.  From
> that create the adjacency matrix, sign(lwd), and plot it
> using indicated layout mode.  Seems like three of
> the variables are correlated and Sepal.Width is uncorrelated
> or negatively correlated to those. Try playing around with
> gplot args to create variations.
>
> library(sna)
> set.seed(123) # layout uses random numbers
> kor <- cor(iris[1:4])
> lwd <- replace(kor, TRUE, 10 * round(pmax(0, kor), 1))
> gplot(sign(lwd), edge.lwd = lwd, displaylabels = TRUE, label = rownames(kor))
>
> On 9/8/06, kone <attenka at utu.fi> wrote:
> > Could somebody program this kind of plot type to R, if none exists,
> > based on mds or correlation tables or some more suitable method? What
> > do you think about idea? Does it work? None similar or better exists?
> >
> > http://weightedassociationmap.blogspot.com/
> >
> >
> > Atte Tenkanen
> > University of Turku, Finland
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From f.abian at gmx.net  Fri Sep  8 14:25:05 2006
From: f.abian at gmx.net (Fabian Scheipl)
Date: Fri, 08 Sep 2006 14:25:05 +0200
Subject: [R]  Conservative "ANOVA tables" in lmer
In-Reply-To: <mailman.9.1157709603.6524.r-help@stat.math.ethz.ch>
References: <mailman.9.1157709603.6524.r-help@stat.math.ethz.ch>
Message-ID: <20060908122505.104960@gmx.net>


Dear list,

I have written functions to perform simulation-based tests of 
HO: Var(Random Effects)=0, beta_foo=0 
in linear mixed models based on the exact distribution of the LR- and
Restricted LR-test statistic (see research presented in
Crainiceanu, Ruppert: "LRT in LMM with 1 variance component",
J. R. Statist. Soc. B (2004), 66, Part 1, pp. 165-185 )
-they are about 15-20 times faster than the parametric bootstrap.

At the moment, the exact distributions are only easily simulated for the
case of 1 single variance component/random effect and i.i.d. errors; feasible approximations for the "multivariate" case are currently
being investigated and will be implemented soon.

the syntax looks something like this:

#begin code:

data(sleepstudy)
summary(sleepstudy)  #Effect of sleep deprivation on reaction time
xyplot(Reaction~Days|Subject, data=sleepstudy)
m<-lmer(Reaction~Days+(Days-1|Subject),data=sleepstudy) 
#random slopes, but no random intercept  
#doesna make sense, but it's just an example
summary(m)

#test for individual heterogeneity based on RLRT
#(No restrictions on fixed effects under H0)
#HO: lambda=Var(RandomSlopes)/Var(error)==0 <==> Var(RandomSlopes)==0

t3<-RLRT1SimTest(m, lambda0=0, seed=5, nsim=10000)

#will produce output:
#HO: lambda = 0 ; p-value = 0 
# observed lambda = 0.06259639 

#test for influence of Days based on LRT 
#(restriction on fixed efects: beta_Days==0)

m0<-lm(Reaction~1,data=sleepstudy)
t4<-LRT1SimTest(m, m0, seed=10, nsim=10000)

#will produce output:
#Model under HO:  Reaction ~ 1 ;
#Model under HA:  Reaction ~ Days + (Days - 1 | Subject) ;
# p-value = 0 
# observed lambda = 0.06259639 

#end code 
                      
If you are interested in using these functions i'll be glad
to send them to you-
be aware, however, that you can only use them for testing
"1 Random Effect" vs. "no Random Effect" in a model with i.i.d. errors!!

The plan is to put them in a package beginning next year and
use them as a basis for an (exact) anova.lmer() method.

Greetings,
Fabian Scheipl


--


From ibanez at bioef.org  Fri Sep  8 10:53:47 2006
From: ibanez at bioef.org (Berta)
Date: Fri, 8 Sep 2006 10:53:47 +0200
Subject: [R] how to construct stripchart with coincident points centered
References: <38929107-8CB9-4C22-827F-952D371B094E@local>
	<D979C34B-0421-4DD5-8491-0CBDE06369B5@local>
Message-ID: <007b01c6d324$4d55cd70$3d01a8c0@BIOEF.ORG>

Hi R-users,
I am using stripchart with coincident points,

y <-rbinom(100, 3, 0.5)
stripchart(y, method="stack", pch="o", vertical=TRUE)

But the result is not centered in the sense that if a value (say value 0) is 
repeated 7 times, the first point is ploted in the middle and the rest at 
its right side, in stead of ploting 3 at its right and 3 at its left.  Can 
anybody help?

Thanks,
Berta.


From maechler at stat.math.ethz.ch  Fri Sep  8 14:48:01 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 8 Sep 2006 14:48:01 +0200
Subject: [R] R drop behavior -- set as option in later version?
In-Reply-To: <loom.20060908T132749-283@post.gmane.org>
References: <39422c340609080251u6c367b5ch3ad0cbe33a87d015@mail.gmail.com>
	<loom.20060908T132749-283@post.gmane.org>
Message-ID: <17665.26241.844129.602064@stat.math.ethz.ch>

Wrong Mailing List !!

Proposals for changes to R should be discussed on R-devel, see
the posting guide.
I'll reply separately, but only CC to R-devel.

Martin Maechler, ETH Zurich


From gregor.gorjanc at bfro.uni-lj.si  Fri Sep  8 14:52:31 2006
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Fri, 08 Sep 2006 14:52:31 +0200
Subject: [R] R drop behavior -- set as option in later version?
In-Reply-To: <17665.26241.844129.602064@stat.math.ethz.ch>
References: <39422c340609080251u6c367b5ch3ad0cbe33a87d015@mail.gmail.com>	<loom.20060908T132749-283@post.gmane.org>
	<17665.26241.844129.602064@stat.math.ethz.ch>
Message-ID: <4501678F.2080305@bfro.uni-lj.si>

Martin Maechler wrote:
> Wrong Mailing List !!
> 
> Proposals for changes to R should be discussed on R-devel, see
> the posting guide.
> I'll reply separately, but only CC to R-devel.

You are right, but I think that this might be an overkill. But, who am I
to decide that anyway! I actually excited that you noticed this thread
and find it usefull for r-devel discussion.

-- 
Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana     PhD student
Biotechnical Faculty
Zootechnical Department     URI: http://www.bfro.uni-lj.si/MR/ggorjan
Groblje 3                   mail: gregor.gorjanc <at> bfro.uni-lj.si

SI-1230 Domzale             tel: +386 (0)1 72 17 861
Slovenia, Europe            fax: +386 (0)1 72 17 888

----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.


From Sophie.Baillargeon at mat.ulaval.ca  Fri Sep  8 15:21:53 2006
From: Sophie.Baillargeon at mat.ulaval.ca (Sophie Baillargeon)
Date: Fri, 08 Sep 2006 09:21:53 -0400
Subject: [R] Multiple matrix multiplication with two 3-dimensional arrays
Message-ID: <6.2.3.4.2.20060908091110.01e42978@archimede.mat.ulaval.ca>


Hi,

I need to do several matrix multiplications with 
the corresponding matrices forming two 
3-dimentional arrays. To illustrate my problem, 
let's say I have the following 3-dimensional arrays:

array1 <- array(1:30,dim=c(3,2,5))
array2 <- array(1:20,dim=c(2,2,5))

I know that I can get what I want with the following computation :

result <- array(dim=c(dim(array1)[1], dim(array2)[2], dim(array1)[3]))
for (i in 1: dim(array1)[3])
{
             result[,,i] <- array1[,,i]%*%array2[,,i]
}

My question is :
Is there a more efficient way to do that computation, i.e. without a loop?

Maybe I could use an "apply" or something but I 
can't figure out how. I would have hoped that simply doing

array1%*%array2

would work, but it doesn?t



Thank you very much!


Sophie Baillargeon

__________________________________________

Sophie Baillargeon, M.Sc.

Professionnelle de recherche en statistique
D?partement de math?matiques et de statistique
Universit? Laval
t?l?phone: (418) 656-2131 poste 2333
courriel: Sophie.Baillargeon at mat.ulaval.ca


From anders.bjorgesater at bio.uio.no  Fri Sep  8 15:32:01 2006
From: anders.bjorgesater at bio.uio.no (=?ISO-8859-1?Q?Anders_Bj=F8rges=E6ter?=)
Date: Fri, 08 Sep 2006 15:32:01 +0200
Subject: [R] repeating values
Message-ID: <450170D1.9050209@ulrik.uio.no>

Hello

I have 2 two vectors like

x	N
2	3
4	1
5	2

and want to make a new vector

x
2
2
2
4
5
5

i.e. repeat the values in x according to N


Thanks for your help.

Best Regards
Anders


From dimitris.rizopoulos at med.kuleuven.be  Fri Sep  8 15:47:01 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Fri, 8 Sep 2006 15:47:01 +0200
Subject: [R] repeating values
References: <450170D1.9050209@ulrik.uio.no>
Message-ID: <005501c6d34d$42cecef0$0540210a@www.domain>

just use rep(), e.g.,

x <- c(2, 4, 5)
N <- c(3, 1, 2)
rep(x, N)


Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Anders Bj?rges?ter" <anders.bjorgesater at bio.uio.no>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, September 08, 2006 3:32 PM
Subject: [R] repeating values


> Hello
>
> I have 2 two vectors like
>
> x N
> 2 3
> 4 1
> 5 2
>
> and want to make a new vector
>
> x
> 2
> 2
> 2
> 4
> 5
> 5
>
> i.e. repeat the values in x according to N
>
>
> Thanks for your help.
>
> Best Regards
> Anders
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From rbaer at atsu.edu  Fri Sep  8 15:59:03 2006
From: rbaer at atsu.edu (Robert Baer)
Date: Fri, 8 Sep 2006 08:59:03 -0500
Subject: [R] how to construct stripchart with coincident points centered
References: <38929107-8CB9-4C22-827F-952D371B094E@local><D979C34B-0421-4DD5-8491-0CBDE06369B5@local>
	007b01c6d324$4d55cd70$3d01a8c0@BIOEF.ORG
Message-ID: <003901c6d34e$f1814ad0$a00c010a@BigBaer>

You may be looking for:
 stripchart(y, method="jitter", pch="o", vertical=TRUE,jitter=.5)

Rob
____________________________
Robert W. Baer, Ph.D.
Associate Professor
Department of Physiology
A. T. Still University of Health Science
800 W. Jefferson St.
Kirksville, MO 63501-1497 USA

----- Original Message ----- 
From: "Berta" <ibanez at bioef.org>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, September 08, 2006 3:53 AM
Subject: [R] how to construct stripchart with coincident points centered


> Hi R-users,
> I am using stripchart with coincident points,
>
> y <-rbinom(100, 3, 0.5)
> stripchart(y, method="stack", pch="o", vertical=TRUE)
>
> But the result is not centered in the sense that if a value (say value 0)
is
> repeated 7 times, the first point is ploted in the middle and the rest at
> its right side, in stead of ploting 3 at its right and 3 at its left.  Can
> anybody help?
>
> Thanks,
> Berta.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Fri Sep  8 15:59:38 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 8 Sep 2006 09:59:38 -0400
Subject: [R] Multiple matrix multiplication with two 3-dimensional arrays
In-Reply-To: <6.2.3.4.2.20060908091110.01e42978@archimede.mat.ulaval.ca>
References: <6.2.3.4.2.20060908091110.01e42978@archimede.mat.ulaval.ca>
Message-ID: <971536df0609080659v544a192ek52c1c3bbe528cc7b@mail.gmail.com>

If the arrays are a1 and a2 then:

library(abind)
abind(lapply(1:dim(a1)[3], function(i) a1[,,i] %*% a2[,,i]), along = 3)


On 9/8/06, Sophie Baillargeon <Sophie.Baillargeon at mat.ulaval.ca> wrote:
>
> Hi,
>
> I need to do several matrix multiplications with
> the corresponding matrices forming two
> 3-dimentional arrays. To illustrate my problem,
> let's say I have the following 3-dimensional arrays:
>
> array1 <- array(1:30,dim=c(3,2,5))
> array2 <- array(1:20,dim=c(2,2,5))
>
> I know that I can get what I want with the following computation :
>
> result <- array(dim=c(dim(array1)[1], dim(array2)[2], dim(array1)[3]))
> for (i in 1: dim(array1)[3])
> {
>             result[,,i] <- array1[,,i]%*%array2[,,i]
> }
>
> My question is :
> Is there a more efficient way to do that computation, i.e. without a loop?
>
> Maybe I could use an "apply" or something but I
> can't figure out how. I would have hoped that simply doing
>
> array1%*%array2
>
> would work, but it doesn't?
>
>
> Thank you very much!
>
>
> Sophie Baillargeon
>
> __________________________________________
>
> Sophie Baillargeon, M.Sc.
>
> Professionnelle de recherche en statistique
> D?partement de math?matiques et de statistique
> Universit? Laval
> t?l?phone: (418) 656-2131 poste 2333
> courriel: Sophie.Baillargeon at mat.ulaval.ca
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From f.abian at gmx.net  Fri Sep  8 16:24:30 2006
From: f.abian at gmx.net (Fabian Scheipl)
Date: Fri, 08 Sep 2006 16:24:30 +0200
Subject: [R] Conservative "ANOVA tables" in lmer
In-Reply-To: <mailman.9.1157709603.6524.r-help@stat.math.ethz.ch>
References: <mailman.9.1157709603.6524.r-help@stat.math.ethz.ch>
Message-ID: <20060908142430.104960@gmx.net>

Dear list,

I have written functions to perform simulation-based tests of 
HO: Var(Random Effects)=0, beta_foo=0 
in linear mixed models based on the exact distribution of the LR- and
Restricted LR-test statistic (see research presented in
Crainiceanu, Ruppert: "LRT in LMM with 1 variance component",
J. R. Statist. Soc. B (2004), 66, Part 1, pp. 165-185 )
-they are about 15-20 times faster than the parametric bootstrap.

At the moment, the exact distributions are only easily simulated for the
case of 1 single variance component/random effect and i.i.d. errors; feasible approximations for the "multivariate" case are currently
being investigated and will be implemented soon.

the syntax looks something like this:

#begin code:

data(sleepstudy)
summary(sleepstudy)  #Effect of sleep deprivation on reaction time
xyplot(Reaction~Days|Subject, data=sleepstudy)
m<-lmer(Reaction~Days+(Days-1|Subject),data=sleepstudy) 
#random slopes, but no random intercept  
#doesna make sense, but it's just an example
summary(m)

#test for individual heterogeneity based on RLRT
#(No restrictions on fixed effects under H0)
#HO: lambda=Var(RandomSlopes)/Var(error)==0 <==> Var(RandomSlopes)==0

t3<-RLRT1SimTest(m, lambda0=0, seed=5, nsim=10000)

#will produce output:
#HO: lambda = 0 ; p-value = 0 
# observed lambda = 0.06259639 

#test for influence of Days based on LRT 
#(restriction on fixed efects: beta_Days==0)

m0<-lm(Reaction~1,data=sleepstudy)
t4<-LRT1SimTest(m, m0, seed=10, nsim=10000)

#will produce output:
#Model under HO:  Reaction ~ 1 ;
#Model under HA:  Reaction ~ Days + (Days - 1 | Subject) ;
# p-value = 0 
# observed lambda = 0.06259639 

#end code 
                      
If you are interested in using these functions i'll be glad
to send them to you-
be aware, however, that you can only use them for testing
"1 Random Effect" vs. "no Random Effect" in a model with i.i.d. errors!!

The plan is to put them in a package beginning next year and
use them as a basis for an (exact) anova.lmer() method.

Greetings,
Fabian Scheipl


-- 





--


From mike.prager at noaa.gov  Fri Sep  8 16:35:40 2006
From: mike.prager at noaa.gov (Mike Prager)
Date: Fri, 08 Sep 2006 10:35:40 -0400
Subject: [R] problem with putting objects in list
References: <44FEBF99.3060401@sun.ac.za>	
	<b0808fdc0609060648k37e74aedx861b28439c11cb99@mail.gmail.com>	
	<44FED556.5010809@sun.ac.za>
	<644e1f320609060817t42660f9p786a6a0734f15d94@mail.gmail.com>
	<4501212D.6010703@sun.ac.za>
Message-ID: <vlv2g2lkt2khotd67ksboramm1e6glvdt3@4ax.com>

Rainer M Krug <rkrug at sun.ac.za> wrote:

> Is there any manual / technical manual / reference available which
> explains all this? The normal manual (Introduction into R) didn't made
> it clear to me.

Go to http://www.r-project.org/

and on the left side of the page is a heading, "Documentation,"
with references to many fine manuals and references. 

Many are free.  Among the fine ones you can buy are introductory
texts by Dalgaard; Verzani; and Maindonald & Braun.  At a
slightly more advanced level, the classic text "MASS" by
Venables and Ripley is often considered the canonical reference
of S and R users.

Hope that helps.

-- 
Mike Prager, NOAA, Beaufort, NC
* Opinions expressed are personal and not represented otherwise.
* Any use of tradenames does not constitute a NOAA endorsement.


From Greg.Snow at intermountainmail.org  Fri Sep  8 17:17:23 2006
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Fri, 8 Sep 2006 09:17:23 -0600
Subject: [R] reading images in R
References: <A32055BDEA88C34BB3DBBCD2293807786E3339@iu-mssg-mbx109.ads.iu.edu>
Message-ID: <07E228A5BE53C24CAD490193A7381BBB12A0BE@LP-EXCHVS07.CO.IHC.COM>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060908/93c680a5/attachment.pl 

From bbands at gmail.com  Fri Sep  8 17:25:43 2006
From: bbands at gmail.com (BBands)
Date: Fri, 8 Sep 2006 08:25:43 -0700
Subject: [R] Connecting to R using Perl?
In-Reply-To: <20060908111651.71091.qmail@web25505.mail.ukl.yahoo.com>
References: <20060908111651.71091.qmail@web25505.mail.ukl.yahoo.com>
Message-ID: <6e8360ad0609080825j7cb8bae4n23623d7929fea03a@mail.gmail.com>

On 9/8/06, Ffenics <ffenics2002 at yahoo.co.uk> wrote:
> Hi there
> Can anyone tell me please if I can access R though a perl script?

http://www.omegahat.org/RSPerl/

And there is always:

http://rpy.sourceforge.net/

    jab
-- 
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.


From therneau at mayo.edu  Fri Sep  8 18:02:33 2006
From: therneau at mayo.edu (Terry Therneau)
Date: Fri, 8 Sep 2006 11:02:33 -0500 (CDT)
Subject: [R] counting process form of a cox model (cluster(id))
Message-ID: <200609081602.k88G2X924772@prolapse.mayo.edu>

Zoe writes: 
My question is quick.  I am looking at 1 event (death), and repeated 
measurements (the time dependent covariate 'lqol') are frequently taken on a 
subject, so I assume that measurements on the same subject will be correlated. 

The answer is: no, it's not a problem
When the time intervals for a subject are disjoint, e.g, 0-10, 10-49, 49-127,
etc, like they will be on this data, the mulitple lines are just a computational
trick.  Any given term in the likelihood will select the right line of 
data for each person, but only one line.  
   Since the multiple rows of data for a person never appear together, it
does not matter if they are correlated or not.  The set of lines that are
chosen for the likelihood have only 1 (or zero) appearances for each person,
hence are an independent set of observations.  So you don't need the robust
variance.
   However, if you allow time travel, e.g. a person returns to time zero after
an event, that is another kettle of fish.  You then have two copies of the
same person at the same party at the same time, and they can interact.  You 
will need a robust variance, but also want to think hard about whether the
model itself makes any sense.
   If there are multiple events per person then one needs the sandwich variance,
but for a somewhat different reason. 

Terry Therneau


From tomas.willebrand at szooek.slu.se  Fri Sep  8 21:01:24 2006
From: tomas.willebrand at szooek.slu.se (Tomas Willebrand)
Date: Fri, 8 Sep 2006 21:01:24 +0200
Subject: [R] JGR on SUSE 10.1 AMD 64
Message-ID: <200609082101.24617.tomas.willebrand@szooek.slu.se>

Dear list;

I have R Version 2.3.1 (2006-06-01) installed on a AMD 64 machine with SUSE 
10.1. 

I have Sun Java version 1.5.0-sun installed. 

I have used "install.packages("JGR", dep=TRUE) to install rJava, iplots and 
JGR. I would like to do a presentation of linux and R for the department!

Compiling rJava do not give any error messages!

But when trying to load library("JGR") or just library("rJava") I get the 
following error message:
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
> unable to load shared library  '/usr/lib64/R/library/rJava/libs/rJava.so':
> libjvm.so: kan inte ?ppna delad objektfil: Filen eller katalogen finns inte
> Error in library("rJava") : .First.lib failed for 'rJava'

I desperately tried to symlink libjvm.so from 
>	/usr/lib64/jvm/java-1.5.0-sun-1.5.0_07/jre/lib/amd64/server
to
>	/usr/lib64/R/library/rJava/libs
but without success.

Any suggestions appreciated!

Regards,

Tomas


From vincent.spiesser at cg29.fr  Fri Sep  8 15:53:22 2006
From: vincent.spiesser at cg29.fr (SPIESSER Vincent)
Date: Fri, 8 Sep 2006 15:53:22 +0200
Subject: [R] Connecting to a SQLBASE database with R
Message-ID: <D43E3D44FCFBB44EA2E2306F7FE48114147494@qp10exch02.cg29.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060908/1c72a285/attachment.pl 

From mdalphin at amgen.com  Fri Sep  8 21:38:06 2006
From: mdalphin at amgen.com (Dalphin, Mark)
Date: Fri, 8 Sep 2006 12:38:06 -0700 
Subject: [R] Confidence intervals on Lowess (eg SiZer from J.S.Marron)
Message-ID: <567ACB2E39C83543B746F1AD7F5E5E0409EDBD4C@wa-mb2-sea.amgen.com>

Hi,

I have some very noisy, relatively sparse data; a biological response of
roughly
~8 subjects at ~8 times points). I've been following the data trend using a
lowess
line, over-plotted with several values of bandwidth, 'f <- seq(0.3, 0.9,
by=0.1)'.
At this point, we have no models for these data.

I wonder if there is any way under R to assign some sort of confidence
interval to the
lowess line. For example, I have seen a method from the lab of J.S.Marron,
which
his group has implemented in the Matlab program, SiZer.
	http://www.stat.unc.edu/faculty/marron/DataAnalyses/SiZer_Intro.html
and, the more interesting:
	
http://www.stat.unc.edu/faculty/marron/DataAnalyses/SiZer/SiZer_Basics.html

An implementation under R of SiZer would obviously answer this question;
does it exist?
Suggestions of alternative approaches would also be welcome.

My searchs of the R-maillist archive for 'sizer', 'Marron' and 'scale-space'
didn't return anything
that I recognized as useful here, but I am new to the idea of confidence in
a lowess line
so I may not be using the appropriate vocabulary.

Thank-you,
Mark

----------------------
Mark Dalphin
Dept Comp Biol, M/S AW2/D3262
Amgen, Inc.
1201 Amgen Court W
Seattle, WA 98119
Phone: +1-206-265-7951


From bolker at zoo.ufl.edu  Fri Sep  8 21:59:48 2006
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Fri, 8 Sep 2006 19:59:48 +0000 (UTC)
Subject: [R] Confidence intervals on Lowess (eg SiZer from J.S.Marron)
References: <567ACB2E39C83543B746F1AD7F5E5E0409EDBD4C@wa-mb2-sea.amgen.com>
Message-ID: <loom.20060908T215821-562@post.gmane.org>

Dalphin, Mark <mdalphin <at> amgen.com> writes:


> I wonder if there is any way under R to assign some sort of confidence
> interval to the
> lowess line. 

  bootstrap (e.g. in the simpleboot package)?

  Ben Bolker


From afshart at exchange.sba.miami.edu  Fri Sep  8 22:24:27 2006
From: afshart at exchange.sba.miami.edu (Afshartous, David)
Date: Fri, 8 Sep 2006 16:24:27 -0400
Subject: [R] augPred plot in nlme library
Message-ID: <6BCB4D493A447546A8126F24332056E80415E7FA@school1.business.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060908/3891bb91/attachment.pl 

From bolker at zoo.ufl.edu  Fri Sep  8 22:24:45 2006
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Fri, 8 Sep 2006 20:24:45 +0000 (UTC)
Subject: [R] Computing skewness and kurtosis with the moments package
References: <871wqmbsr7.fsf@hardknott.home>
Message-ID: <loom.20060908T222003-930@post.gmane.org>

Roger Leigh <rleigh <at> whinlatter.ukfsn.org> writes:

>
> With SPSS, I get
>   Skewness -0.320
>   Kurtosis -1.138
> 
> With R:
> > skewness(loglen)
> [1] -0.317923
> > kurtosis(loglen)
> [1] 1.860847
> 
> Using the example skew and kurtosis functions from M. J. Crawley's
> "Statistics: An introduction using R": pp 69 and 72:
> 
> > mskew(loglen)
> [1] -0.3158337
> > mkurtosis(loglen)
> [1] -1.155441
> 


  There are two differences between the R functions;
(1) Crawley subtracts 3 from E[x^4]/E[x^2]^2, the
kurtosis function in the moments package doesn't.

(2) Crawley uses var(x), which is sum((x-mean(x))^2)/(n-1),
rather than sum((x-mean(x))^2)/n  [I'm not sure I got all
the parentheses in the right place.]

 With these differences corrected the two sets of functions
give the same answers.

   They still don't give exactly the same answers as SPSS, but ...
I wouldn't worry about it too much since the uncertainties in
the skew and kurtosis are likely to be much larger.

from _Numerical Recipes_:

if the difference between n and n?1 ever matters to you, then you are probably
up to no good anyway - e.g., trying to substantiate a questionable hypothesis
with marginal data.

  cheers
    Ben Bolker


From ripley at stats.ox.ac.uk  Fri Sep  8 22:28:55 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 8 Sep 2006 21:28:55 +0100 (BST)
Subject: [R] Connecting to a SQLBASE database with R
In-Reply-To: <D43E3D44FCFBB44EA2E2306F7FE48114147494@qp10exch02.cg29.local>
References: <D43E3D44FCFBB44EA2E2306F7FE48114147494@qp10exch02.cg29.local>
Message-ID: <Pine.LNX.4.64.0609082126170.17940@gannet.stats.ox.ac.uk>

You haven't told us your OS.

SQLBase would appear to have a Windows ODBC driver, so RODBC should be 
usable. I doubt it any other solution is (and RSQLite works only with 
SQLite).

On Fri, 8 Sep 2006, SPIESSER Vincent wrote:

> I am trying to extract data from a database with R in order to produce monthly statistics.
> 
> I found in the R Website, the package RODBC, RSQLite and others ones which permit this kind of extraction.
> 
> The database I want to be connected with is a SQLBASE 7.0 database.
> So, I would like to know if, using one of these package or another one, I could be able to connect with this type of database.
> 
> 
> I hope I would be understood. I am not a database specialist and, being french, my english is a little bit poor.
> 
> Thanks for your response.
> 
> Vincent Spiesser

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From antonio.fabio at gmail.com  Fri Sep  8 22:34:24 2006
From: antonio.fabio at gmail.com (Antonio, Fabio Di Narzo)
Date: Fri, 8 Sep 2006 22:34:24 +0200
Subject: [R] Connecting to a SQLBASE database with R
In-Reply-To: <D43E3D44FCFBB44EA2E2306F7FE48114147494@qp10exch02.cg29.local>
References: <D43E3D44FCFBB44EA2E2306F7FE48114147494@qp10exch02.cg29.local>
Message-ID: <b0808fdc0609081334v5c8dabfdtbcebc125de643582@mail.gmail.com>

The ODBC protocol is a widely used standard for accessing databases
sources, especially under m$ windows.
I think you can use RODBC for accessing your SQLBASE data, at least
after some machine configuration...

Antonio.


2006/9/8, SPIESSER Vincent <vincent.spiesser a cg29.fr>:
>
> Hi,
>
> I am trying to extract data from a database with R in order to produce monthly statistics.
>
> I found in the R Website, the package RODBC, RSQLite and others ones which permit this kind of extraction.
>
> The database I want to be connected with is a SQLBASE 7.0 database.
> So, I would like to know if, using one of these package or another one, I could be able to connect with this type of database.
>
>
> I hope I would be understood. I am not a database specialist and, being french, my english is a little bit poor.
>
> Thanks for your response.
>
> Vincent Spiesser
>
>
> -------------------------------------------------------------------------
> Ce message a ete scanne par l'anti-virus du Conseil General du Finistere.
> -------------------------------------------------------------------------
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help a stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From deepayan.sarkar at gmail.com  Fri Sep  8 22:36:49 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Fri, 8 Sep 2006 13:36:49 -0700
Subject: [R] augPred plot in nlme library
In-Reply-To: <6BCB4D493A447546A8126F24332056E80415E7FA@school1.business.edu>
References: <6BCB4D493A447546A8126F24332056E80415E7FA@school1.business.edu>
Message-ID: <eb555e660609081336t5bd8616dx41a0d7a23e6e5dd1@mail.gmail.com>

On 9/8/06, Afshartous, David <afshart at exchange.sba.miami.edu> wrote:
> All,
>
> I've solved part of the problem below by making sure that the formula in
> the grouped data object is the same as the formula specified within lme
> (this isn't the case in the cited example from Pinheiro & Bates).
>
> However, augPred seems to plot only a linear model instead of the
> polynomial model.  Does anyone know how to make sure that augPred plots
> the same model as that specified in the model (as below)?

You are unlikely to get any helpful answers unless you give us more
information, as every r-help message asks you to do:

> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-Deepayan


From afshart at exchange.sba.miami.edu  Fri Sep  8 23:18:13 2006
From: afshart at exchange.sba.miami.edu (Afshartous, David)
Date: Fri, 8 Sep 2006 17:18:13 -0400
Subject: [R] augPred plot in nlme library
Message-ID: <6BCB4D493A447546A8126F24332056E80415E81A@school1.business.edu>

 
Deepayan,

Thanks for your suggestion.  Here are more details:

I have a grouped data object for repeated measures data just like the
Pixel grouped data object on p.42 of Pinheiro and Bates (2000).

comp.adj.UKV.3 <- groupedData(adj.UKV ~ Time | Patient_no/Lisinopril, 
	data = comp.adj.UKV.frm, order.groups = F
	#labels = list(x = "Hour", y = "adj.UKV") 
)

i.e., the response is continuous, Time is not treated as a factor, and
there exists two factors, one nested within the other (Lisinopril nested

witin patient, similar to Side within Dog on p.42).

I also fit a model very similar to their model:

fm1comp = lme(adj.UKV ~ Time + Time.sq, data = comp.adj.UKV.3, random =
list(Patient_no = ~ 1 , Lisinopril = ~ 1) )


However, the command below does not produce the fitted curves from this
model,
but rather it seems to be the fitted curves from a linear model.

plot(augPred(fm3comp))

Possibly augPred behaves differently in R than in S, but reading the R
help and 
trying various other approaches has not solved this.

Thanks!
Dave




-----Original Message-----
From: Deepayan Sarkar [mailto:deepayan.sarkar at gmail.com] 
Sent: Friday, September 08, 2006 4:37 PM
To: Afshartous, David
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] augPred plot in nlme library

On 9/8/06, Afshartous, David <afshart at exchange.sba.miami.edu> wrote:
> All,
>
> I've solved part of the problem below by making sure that the formula 
> in the grouped data object is the same as the formula specified within

> lme (this isn't the case in the cited example from Pinheiro & Bates).
>
> However, augPred seems to plot only a linear model instead of the 
> polynomial model.  Does anyone know how to make sure that augPred 
> plots the same model as that specified in the model (as below)?

You are unlikely to get any helpful answers unless you give us more
information, as every r-help message asks you to do:

> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-Deepayan


From deepayan.sarkar at gmail.com  Fri Sep  8 23:35:26 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Fri, 8 Sep 2006 14:35:26 -0700
Subject: [R] augPred plot in nlme library
In-Reply-To: <6BCB4D493A447546A8126F24332056E80415E81A@school1.business.edu>
References: <6BCB4D493A447546A8126F24332056E80415E81A@school1.business.edu>
Message-ID: <eb555e660609081435n65e25b41gf4ed4c558984d6ce@mail.gmail.com>

On 9/8/06, Afshartous, David <afshart at exchange.sba.miami.edu> wrote:
>
> Deepayan,
>
> Thanks for your suggestion.  Here are more details:

Yes, but none of it qualifies as "commented, minimal, self-contained,
reproducible code".

Let's hope someone else on this list is kind enough and has enough
free time to dream up an example that reproduces your vaguely
described ``problem'' and then figure out a solution to it.

Deepayan

> I have a grouped data object for repeated measures data just like the
> Pixel grouped data object on p.42 of Pinheiro and Bates (2000).
>
> comp.adj.UKV.3 <- groupedData(adj.UKV ~ Time | Patient_no/Lisinopril,
>         data = comp.adj.UKV.frm, order.groups = F
>         #labels = list(x = "Hour", y = "adj.UKV")
> )
>
> i.e., the response is continuous, Time is not treated as a factor, and
> there exists two factors, one nested within the other (Lisinopril nested
>
> witin patient, similar to Side within Dog on p.42).
>
> I also fit a model very similar to their model:
>
> fm1comp = lme(adj.UKV ~ Time + Time.sq, data = comp.adj.UKV.3, random =
> list(Patient_no = ~ 1 , Lisinopril = ~ 1) )
>
>
> However, the command below does not produce the fitted curves from this
> model,
> but rather it seems to be the fitted curves from a linear model.
>
> plot(augPred(fm3comp))
>
> Possibly augPred behaves differently in R than in S, but reading the R
> help and
> trying various other approaches has not solved this.
>
> Thanks!
> Dave
>
>
>
>
> -----Original Message-----
> From: Deepayan Sarkar [mailto:deepayan.sarkar at gmail.com]
> Sent: Friday, September 08, 2006 4:37 PM
> To: Afshartous, David
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] augPred plot in nlme library
>
> On 9/8/06, Afshartous, David <afshart at exchange.sba.miami.edu> wrote:
> > All,
> >
> > I've solved part of the problem below by making sure that the formula
> > in the grouped data object is the same as the formula specified within
>
> > lme (this isn't the case in the cited example from Pinheiro & Bates).
> >
> > However, augPred seems to plot only a linear model instead of the
> > polynomial model.  Does anyone know how to make sure that augPred
> > plots the same model as that specified in the model (as below)?
>
> You are unlikely to get any helpful answers unless you give us more
> information, as every r-help message asks you to do:
>
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> -Deepayan


From wsheffle at u.washington.edu  Fri Sep  8 23:44:49 2006
From: wsheffle at u.washington.edu (Will Sheffler)
Date: Fri, 08 Sep 2006 14:44:49 -0700
Subject: [R] boundary constraints with smooth.spline
Message-ID: <4501E451.30300@u.washington.edu>

Hi R Community.

I would like to use smooth.spline to fit a set of data and constrain the 
endpoints of the fit to have specific derivatives. I know this is 
possible with cubic splines, but I can't figure out how to specify this 
with arguments to the smooth.spline function. In general, is it possible 
to specify a set of "knots" w/locations and derivatives to constrain the 
fit? I can't find anything about this is the documentation.
Thanks!

***********************************
Will Sheffler
Baker Lab
Dept of Genome Sciences
University of Washington
Seattle, WA 98195


From A.Robinson at ms.unimelb.edu.au  Fri Sep  8 23:46:00 2006
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Sat, 9 Sep 2006 07:46:00 +1000
Subject: [R] augPred plot in nlme library
In-Reply-To: <6BCB4D493A447546A8126F24332056E80415E81A@school1.business.edu>
References: <6BCB4D493A447546A8126F24332056E80415E81A@school1.business.edu>
Message-ID: <20060908214600.GS12212@ms.unimelb.edu.au>

Hi David,

this is the sort of thing that Deepayan meant.  Make a dataset
available to us, or use one that will be installed by default on R.

eg

require(nlme)
fm1 <- lme(distance ~ age, data = Orthodont)
plot(augPred(fm1))

#  All linear

fm2a <- lme(distance ~ age + age.2, data = Orthodont)
plot(augPred(fm2a))

# Still linear

fm2b <- lme(distance ~ age + I(age^2), data = Orthodont)
plot(augPred(fm2b))

# Quadratic!

I hope that this helps you resolve the problem.

Andrew



On Fri, Sep 08, 2006 at 05:18:13PM -0400, Afshartous, David wrote:
>  
> Deepayan,
> 
> Thanks for your suggestion.  Here are more details:
> 
> I have a grouped data object for repeated measures data just like the
> Pixel grouped data object on p.42 of Pinheiro and Bates (2000).
> 
> comp.adj.UKV.3 <- groupedData(adj.UKV ~ Time | Patient_no/Lisinopril, 
> 	data = comp.adj.UKV.frm, order.groups = F
> 	#labels = list(x = "Hour", y = "adj.UKV") 
> )
> 
> i.e., the response is continuous, Time is not treated as a factor, and
> there exists two factors, one nested within the other (Lisinopril nested
> 
> witin patient, similar to Side within Dog on p.42).
> 
> I also fit a model very similar to their model:
> 
> fm1comp = lme(adj.UKV ~ Time + Time.sq, data = comp.adj.UKV.3, random =
> list(Patient_no = ~ 1 , Lisinopril = ~ 1) )
> 
> 
> However, the command below does not produce the fitted curves from this
> model,
> but rather it seems to be the fitted curves from a linear model.
> 
> plot(augPred(fm3comp))
> 
> Possibly augPred behaves differently in R than in S, but reading the R
> help and 
> trying various other approaches has not solved this.
> 
> Thanks!
> Dave
> 
> 
> 
> 
> -----Original Message-----
> From: Deepayan Sarkar [mailto:deepayan.sarkar at gmail.com] 
> Sent: Friday, September 08, 2006 4:37 PM
> To: Afshartous, David
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] augPred plot in nlme library
> 
> On 9/8/06, Afshartous, David <afshart at exchange.sba.miami.edu> wrote:
> > All,
> >
> > I've solved part of the problem below by making sure that the formula 
> > in the grouped data object is the same as the formula specified within
> 
> > lme (this isn't the case in the cited example from Pinheiro & Bates).
> >
> > However, augPred seems to plot only a linear model instead of the 
> > polynomial model.  Does anyone know how to make sure that augPred 
> > plots the same model as that specified in the model (as below)?
> 
> You are unlikely to get any helpful answers unless you give us more
> information, as every r-help message asks you to do:
> 
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide 
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> -Deepayan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
Email: a.robinson at ms.unimelb.edu.au         http://www.ms.unimelb.edu.au


From murdoch at stats.uwo.ca  Fri Sep  8 23:50:59 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 08 Sep 2006 17:50:59 -0400
Subject: [R] Connecting to a SQLBASE database with R
In-Reply-To: <D43E3D44FCFBB44EA2E2306F7FE48114147494@qp10exch02.cg29.local>
References: <D43E3D44FCFBB44EA2E2306F7FE48114147494@qp10exch02.cg29.local>
Message-ID: <4501E5C3.6000601@stats.uwo.ca>

On 9/8/2006 9:53 AM, SPIESSER Vincent wrote:
> Hi,
> 
> I am trying to extract data from a database with R in order to produce monthly statistics.
> 
> I found in the R Website, the package RODBC, RSQLite and others ones which permit this kind of extraction.
> 
> The database I want to be connected with is a SQLBASE 7.0 database.
> So, I would like to know if, using one of these package or another one, I could be able to connect with this type of database.

I don't know SQLBASE, but I'm pretty sure there is no package that 
specifically supports that database.

So what you need to do is to find out if SQLBASE supports ODBC 
connections.  If it does, then you can use RODBC.  If not, then you're 
probably out of luck.

Duncan Murdoch
> 
> 
> I hope I would be understood. I am not a database specialist and, being french, my english is a little bit poor.
> 
> Thanks for your response.
> 
> Vincent Spiesser
> 
> 
> -------------------------------------------------------------------------
> Ce message a ete scanne par l'anti-virus du Conseil General du Finistere.
> -------------------------------------------------------------------------
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bbands at gmail.com  Fri Sep  8 23:53:42 2006
From: bbands at gmail.com (BBands)
Date: Fri, 8 Sep 2006 14:53:42 -0700
Subject: [R] Connecting to a SQLBASE database with R
In-Reply-To: <b0808fdc0609081334v5c8dabfdtbcebc125de643582@mail.gmail.com>
References: <D43E3D44FCFBB44EA2E2306F7FE48114147494@qp10exch02.cg29.local>
	<b0808fdc0609081334v5c8dabfdtbcebc125de643582@mail.gmail.com>
Message-ID: <6e8360ad0609081453r69628884s31a9dc5a5651a714@mail.gmail.com>

for RODBC you must make a connection SQLBASE first:

For XP: control panel > Admin tools > Data sources > Add...

You may have to install the SQLBASE ODBC driver first if it is not on the list.

The rest should be easy just supply the connection name and details to RODBC.

    jab
-- 
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.


From bates at stat.wisc.edu  Sat Sep  9 00:00:00 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 8 Sep 2006 17:00:00 -0500
Subject: [R] Conservative "ANOVA tables" in lmer
In-Reply-To: <40e66e0b0609070832x28bb5365w6f4bfd8a6a72078a@mail.gmail.com>
References: <1A8FE5B0-3FE3-4CA9-9224-9CACB9AF4854@muohio.edu>
	<40e66e0b0609070559u656463c1l52a60eda9a095237@mail.gmail.com>
	<17664.12848.693133.495312@stat.math.ethz.ch>
	<x2fyf3g0ua.fsf@viggo.kubism.ku.dk>
	<40e66e0b0609070832x28bb5365w6f4bfd8a6a72078a@mail.gmail.com>
Message-ID: <40e66e0b0609081500s50cc0e1cq3a899ded57b8e428@mail.gmail.com>

On 9/7/06, Douglas Bates <bates at stat.wisc.edu> wrote:
> On 07 Sep 2006 17:20:29 +0200, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> > Martin Maechler <maechler at stat.math.ethz.ch> writes:
> >
> > > >>>>> "DB" == Douglas Bates <bates at stat.wisc.edu>
> > > >>>>>     on Thu, 7 Sep 2006 07:59:58 -0500 writes:
> > >
> > >     DB> Thanks for your summary, Hank.
> > >     DB> On 9/7/06, Martin Henry H. Stevens <hstevens at muohio.edu> wrote:
> > >     >> Dear lmer-ers,
> > >     >> My thanks for all of you who are sharing your trials and tribulations
> > >     >> publicly.
> > >
> > >     >> I was hoping to elicit some feedback on my thoughts on denominator
> > >     >> degrees of freedom for F ratios in mixed models. These thoughts and
> > >     >> practices result from my reading of previous postings by Doug Bates
> > >     >> and others.
> > >
> > >     >> - I start by assuming that the appropriate denominator degrees lies
> > >     >> between n - p and and n - q, where n=number of observations, p=number
> > >     >> of fixed effects (rank of model matrix X), and q=rank of Z:X.
> > >
> > >     DB> I agree with this but the opinion is by no means universal.  Initially
> > >     DB> I misread the statement because I usually write the number of columns
> > >     DB> of Z as q.
> > >
> > >     DB> It is not easy to assess rank of Z:X numerically.  In many cases one
> > >     DB> can reason what it should be from the form of the model but a general
> > >     DB> procedure to assess the rank of a matrix, especially a sparse matrix,
> > >     DB> is difficult.
> > >
> > >     DB> An alternative which can be easily calculated is n - t where t is the
> > >     DB> trace of the 'hat matrix'.  The function 'hatTrace' applied to a
> > >     DB> fitted lmer model evaluates this trace (conditional on the estimates
> > >     DB> of the relative variances of the random effects).
> > >
> > >     >> - I then conclude that good estimates of P values on the F ratios lie
> > >     >>   between 1 - pf(F.ratio, numDF, n-p) and 1 - pf(F.ratio, numDF, n-q).
> > >     >>   -- I further surmise that the latter of these (1 - pf(F.ratio, numDF,
> > >     >>   n-q)) is the more conservative estimate.
> > >
> > > This assumes that the true distribution (under H0) of that "F ratio"
> > > *is*  F_{n1,n2}  for some (possibly non-integer)  n1 and n2.
> > > But AFAIU, this is only approximately true at best, and AFAIU,
> > > the quality of this approximation has only been investigated
> > > empirically for some situations.
> > > Hence, even your conservative estimate of the P value could be
> > > wrong (I mean "wrong on the wrong side" instead of just
> > > "conservatively wrong").  Consequently, such a P-value is only
> > > ``approximately conservative'' ...
> > > I agree howevert that in some situations, it might be a very
> > > useful "descriptive statistic" about the fitted model.
> >
> > I'm very wary of ANY attempt at guesswork in these matters.
> >
> > I may be understanding the post wrongly, but consider this case: Y_ij
> > = mu + z_i + eps_ij, i = 1..3, j=1..100
> >
> > I get rank(X)=1, rank(X:Z)=3,  n=300
> >
> > It is well known that the test for mu=0 in this case is obtained by
> > reducing data to group means, xbar_i, and then do a one-sample t test,
> > the square of which is F(1, 2), but it seems to be suggested that
> > F(1, 297) is a conservative test???!
>
> It's a different test, isn't it?  Your test is based upon the between
> group sum of squares with 2 df.  I am proposing to use the within
> group sum of squares or its generalization.

On closer examination I see that you are indeed correct.  I have heard
that "well-known" result many times and finally sat down to prove it
to myself.  For a balanced design the standard error of the intercept
using the REML estimates is the same as the standard error of the mean
calculated from the group means.

> data(Rail, package = 'nlme')
> library(lme4)
> summary(fm1 <- lmer(travel ~ 1 + (1|Rail), Rail))
Linear mixed-effects model fit by REML
Formula: travel ~ 1 + (1 | Rail)
   Data: Rail
   AIC   BIC logLik MLdeviance REMLdeviance
 126.2 128.0 -61.09      128.6        122.2
Random effects:
 Groups   Name        Variance Std.Dev.
 Rail     (Intercept) 615.286  24.8050
 Residual              16.167   4.0208
number of obs: 18, groups: Rail, 6

Fixed effects:
            Estimate Std. Error t value
(Intercept)    66.50      10.17   6.538
> mns <- with(Rail, tapply(travel, Rail, mean)) # group means
> sd(mns)/sqrt(length(mns))  # standard error matches that from lmer
[1] 10.17104
> t.test(mns)

	One Sample t-test

data:  mns
t = 6.5382, df = 5, p-value = 0.001253
alternative hypothesis: true mean is not equal to 0
95 percent confidence interval:
 40.35452 92.64548
sample estimates:
mean of x
     66.5

> ctab <- summary(fm1)@coefs  # coefficient table
> ctab[,1] + c(-1,1) * qt(0.975, 15) * ctab[,2] # 95% conf. int.
[1] 44.82139 88.17861
> ## interval using df = # of obs - rank of [Z:X] is too narrow

So my proposal of using either the trace of the hat matrix or the rank
of the combined model matrices as the degrees of freedom for the model
is not conservative.

However, look at the following

> set.seed(123454321)  # for reproducibility
> sm1 <- mcmcsamp(fm1, 50000)
> library(coda)
> HPDinterval(sm1)
                    lower      upper
(Intercept)     40.470663  92.608514
log(sigma^2)     2.060179   3.716326
log(Rail.(In))   5.371858   8.056897
deviance       128.567329 137.487455
attr(,"Probability")
[1] 0.95

The HPD interval calculated from a MCMC sample reproduce the interval
from the group means almost exactly.  This makes sense in that the
MCMC sample takes into account the variation in the estimates of the
variance components, just as defining intervals based on the Student's
t does.

So for this case where the distribution of the estimate of the mean
has a known distribution the correct degrees of freedom and the MCMC
sample produce similar answers.

This gives me more confidence in the results from the MCMC sample in
general cases.

The problem I have with trying to work out what the degrees of freedom
"should be" is that the rules seem rather arbitrary.  For example, the
"between-within" rule used in SAS PROC Mixed is popular (many accept
it as the "correct" answer) but it assumes that the degrees of freedom
associated with a random effect grouped by a factor with k levels is
always k - 1.  This value is used even when there is a random
intercept and a random slope for each group.  In fact you could have
an arbitrary number of random effects for each level of the grouping
factor and it would still apparently only cost you k - 1 degrees of
freedom.  That doesn't make sense to me.

Anyway, I thank you for pointing out the errors of my ways Peter.


From qxsr at yahoo.com  Sat Sep  9 01:34:58 2006
From: qxsr at yahoo.com (Sam Wong)
Date: Fri, 8 Sep 2006 16:34:58 -0700 (PDT)
Subject: [R] maximizing a likelihood function containing an integral
Message-ID: <20060908233458.77728.qmail@web34711.mail.mud.yahoo.com>


Hi, R Users;

I am trying to maximize a likelihood function which
contains an integral.  The integral contains the
unknown parameter as well.  I am trying to use the
following code to do the maximization:

ll<-function(b.vec){
       b0<-b.vec[1]
       b1<-b.vec[2]
       b2<-b.vec[3]
      p<-1/(1+exp(-b0-b1*z1-b2*x2))
      
lik1<-p^y*(1-p)^(1-y)*exp(-(z1^2+x2^2-2*rho*z1*x2)/(2*(1-rho^2)))
            log.lik1<-sum(log(lik1[1:n1]))
       log.lik2<-0
     for(j in (n1+1):n){
                    integrand<-function(u,B0,B1,B2){

exp(-y[j]*(B0+B1*u+B2*x2[j])-(u-rho*x2[j])^2/2)/(1+exp(B0+B1*u+B2*x2[j]))
}
  
log.lik2<-log.lik2+log(integrate(integrand,lower=1,upper=Inf,B0=b0,B1=b1,B2=b2)$integral)
   }
 
   log.lik<-log.lik1+log.lik2
}


 
start<-c(0,0,0)

nlminb(start,ll)

The error message is: 
Error in log(x) : Non-numeric argument to mathematical
function

Suggestions are welcome.

Thanks

Ming Ji


From gyadav at ccilindia.co.in  Sat Sep  9 05:47:03 2006
From: gyadav at ccilindia.co.in (gyadav at ccilindia.co.in)
Date: Sat, 9 Sep 2006 09:17:03 +0530
Subject: [R] How to actively join you all
In-Reply-To: <4501FDF7.9020006@itp.phys.ethz.ch>
Message-ID: <OF5ECFF221.90523454-ON652571E4.00144EA9-652571E4.0014EB8C@ccilindia.co.in>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060909/f30f84d9/attachment.pl 

From gyadav at ccilindia.co.in  Sat Sep  9 06:28:18 2006
From: gyadav at ccilindia.co.in (gyadav at ccilindia.co.in)
Date: Sat, 9 Sep 2006 09:58:18 +0530
Subject: [R] How to actively join you all
In-Reply-To: <971536df0609082106m6673e677rde16cb82a015eb7b@mail.gmail.com>
Message-ID: <OFB30DE6D4.A23B64C0-ON652571E4.001760E5-652571E4.0018B23C@ccilindia.co.in>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060909/5daa2cf1/attachment.pl 

From dkgunter at lbl.gov  Sat Sep  9 07:04:59 2006
From: dkgunter at lbl.gov (Dan Gunter)
Date: Fri, 08 Sep 2006 22:04:59 -0700
Subject: [R] converting decimal - hexadecimal
In-Reply-To: <451E7762.8000307@ese.u-psud.fr>
References: <451E7762.8000307@ese.u-psud.fr>
Message-ID: <45024B7B.90509@lbl.gov>

Romain Lorrilliere wrote:
> Hi,
>
> do you know, a method to convert an decimal value (integer) to the 
> corresponding hexadecimal value ?
>
> thinks for help.
>
> Romain
>
>   
You mean "representation", not "value"; the value doesn't change. One
method is sprintf with the "%x" formatting code.

-Dan

-- 
Dan Gunter. voice:510-495-2504 fax:510-486-6363 dsd.lbl.gov/~dang


From AnupTyagi at yahoo.com  Sat Sep  9 07:11:57 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Sat, 9 Sep 2006 05:11:57 +0000 (UTC)
Subject: [R] Multiple matrix multiplication with two 3-dimensional arrays
References: <6.2.3.4.2.20060908091110.01e42978@archimede.mat.ulaval.ca>
Message-ID: <loom.20060909T070618-433@post.gmane.org>

Sophie Baillargeon <Sophie.Baillargeon <at> mat.ulaval.ca> writes:


> Maybe I could use an "apply" or something but I 
> can't figure out how. I would have hoped that simply doing
> 
> array1%*%array2
> 
> would work, but it doesn?t


I think one of the issues is that algebra for N-Dimentional arrays are not well
defined. Think how would you define the above operartion on two 3x3x3 arrays. I
had seen a paper a couple of years ago in some math journal that had proposed an
algebra. I will be interested in knowing a reference to some source that defines
3-dimentional array algebra. Then perhaps it could be implemented in R.

I will also be interested in knowing what is the equivalent of "cell" array in
MATLAB in R---that is something that is equivalent to the "cell" functions in a
spreadsheet---sometimes this can be useful.

Anupam.


From prospardgondwe at yahoo.com  Sat Sep  9 11:20:06 2006
From: prospardgondwe at yahoo.com (Prospard Gondwe)
Date: Sat, 9 Sep 2006 02:20:06 -0700 (PDT)
Subject: [R] Use of  "ks.test()" function
Message-ID: <20060909092006.94440.qmail@web60214.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060909/fc687dae/attachment.pl 

From murdoch at stats.uwo.ca  Sat Sep  9 12:31:49 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 09 Sep 2006 06:31:49 -0400
Subject: [R] How to actively join you all
In-Reply-To: <OF5ECFF221.90523454-ON652571E4.00144EA9-652571E4.0014EB8C@ccilindia.co.in>
References: <OF5ECFF221.90523454-ON652571E4.00144EA9-652571E4.0014EB8C@ccilindia.co.in>
Message-ID: <45029815.8050907@stats.uwo.ca>

On 9/8/2006 11:47 PM, gyadav at ccilindia.co.in wrote:
> Hello R Community,
> 
> I am really impressed by the 'R'. I am a computer engineer. Further, I 
> have done work on Linux Code at my home and Worked on Cisco IOS platform. 
> I would like to contribute to the software i.e. i would like to devote my 
> free time for its development, but i do not know how to join the 
> development team. If anybody can tell me the way then please show me the 
> way. I would be really grateful to you all.

Spend some time reading this mailing list and the R-devel mailing list 
until you get a feeling for what is already there, and what is missing. 
  Then try to put together a package to provide some part of what is 
missing.  Read the "Writing R Extensions" manual to see what is involved 
in that.  When the package is done, send it to CRAN, and if appropriate, 
write up a description of it and submit the package and description to a 
journal, e.g. JSS.

If you want to contribute to the R internals, then you should read the 
development.r-project.org web page, which gives details about Subversion 
access to the source, etc.  There's also a new manual "R Internals" 
which will be in 2.4.0. You won't have write permission on the 
repository, but you can suggest changes on the R-devel list, prepare 
patches, etc.  Be prepared to defend the changes you suggest:  R has a 
long history, and we're reluctant to break old code unless there's a 
clear benefit, and reluctant to commit to dead-ends.

To get write permission on the repository you need to convince the 
"core" group that you should be invited to join; there are currently 17 
members, and I think there are 17 conflicting criteria for membership, 
so I don't know any rule other than "make it obvious that it's better 
you're a member than not".

Duncan Murdoch

> 
> thanks
>    Sayonara With Smile & With Warm Regards :-)
> 
>   G a u r a v   Y a d a v
>   Senior Executive Officer,
>   Economic Research & Surveillance Department,
>   Clearing Corporation Of India Limited.
> 
>   Address: 5th, 6th, 7th Floor, Trade Wing 'C',  Kamala City, S.B. Marg, 
> Mumbai - 400 013
>   Telephone(Office): - +91 022 6663 9398 ,  Mobile(Personal) (0)9821286118
>   Email(Office) :- gyadav at ccilindia.co.in ,  Email(Personal) :- 
> emailtogauravyadav at gmail.com
> 
> 
> ============================================================================================
> DISCLAIMER AND CONFIDENTIALITY CAUTION:\ \ This message and ...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From AnupTyagi at yahoo.com  Sat Sep  9 11:42:39 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Sat, 9 Sep 2006 09:42:39 +0000 (UTC)
Subject: [R] reading and formating irregular time series for VaR
Message-ID: <loom.20060909T113937-403@post.gmane.org>

Hi, I am trying to read the following type of data from a .csv file to form an
irregular time series object. I want to use it with the VaR package. How do I
read it in correctly to an irregular time series object? Anupam.

date,Open,High,Low,Close,Volume,OpenInterest,Contract
1972-08-16,54.25,54.25,54.25,54.25,1,1,KC1973H
1972-08-17,54.25,54.25,54.25,54.25,0,1,KC1973H
1972-08-18,54.25,54.25,54.25,54.25,0,1,KC1973H
1972-08-21,54.25,54.25,54.25,54.25,0,1,KC1973H
1972-08-22,54.25,54.25,54.25,54.25,0,1,KC1973H
1972-08-23,54.25,54.25,54.25,54.25,0,1,KC1973H


From wuertz at itp.phys.ethz.ch  Sat Sep  9 14:24:59 2006
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Sat, 09 Sep 2006 14:24:59 +0200
Subject: [R] converting decimal - hexadecimal
In-Reply-To: <451E7762.8000307@ese.u-psud.fr>
References: <451E7762.8000307@ese.u-psud.fr>
Message-ID: <4502B29B.4040103@itp.phys.ethz.ch>

Romain Lorrilliere wrote:

>Hi,
>
>do you know, a method to convert an decimal value (integer) to the 
>corresponding hexadecimal value ?
>
>thinks for help.
>
>Romain
>
>  
>
The fBasics package has two hidden functions:


 > .dec.to.hex(145678)
[1] "02390E"

 > .hex.to.dec("02390E")
[1] 145678


DW


**


From ggrothendieck at gmail.com  Sat Sep  9 15:27:02 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 9 Sep 2006 09:27:02 -0400
Subject: [R] reading and formating irregular time series for VaR
In-Reply-To: <loom.20060909T113937-403@post.gmane.org>
References: <loom.20060909T113937-403@post.gmane.org>
Message-ID: <971536df0609090627m1567ed86pa256c8dfed2cc891@mail.gmail.com>

If you are using the zoo package and if we assume:
- your file contains a single time series and
- the last column is junk
then try this:

library(zoo)
DF <- read.csv("myfile.dat")
z <- zoo(as.matrix(DF[2:7]), as.Date(DF[[1]]))

Read the documents at the end of
   http://cran.r-project.org/src/contrib/Descriptions/zoo.html
for more info.

On 9/9/06, Anupam Tyagi <AnupTyagi at yahoo.com> wrote:
> Hi, I am trying to read the following type of data from a .csv file to form an
> irregular time series object. I want to use it with the VaR package. How do I
> read it in correctly to an irregular time series object? Anupam.
>
> date,Open,High,Low,Close,Volume,OpenInterest,Contract
> 1972-08-16,54.25,54.25,54.25,54.25,1,1,KC1973H
> 1972-08-17,54.25,54.25,54.25,54.25,0,1,KC1973H
> 1972-08-18,54.25,54.25,54.25,54.25,0,1,KC1973H
> 1972-08-21,54.25,54.25,54.25,54.25,0,1,KC1973H
> 1972-08-22,54.25,54.25,54.25,54.25,0,1,KC1973H
> 1972-08-23,54.25,54.25,54.25,54.25,0,1,KC1973H
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Sat Sep  9 15:37:23 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 9 Sep 2006 09:37:23 -0400
Subject: [R] How to actively join you all
In-Reply-To: <OFB30DE6D4.A23B64C0-ON652571E4.001760E5-652571E4.0018B23C@ccilindia.co.in>
References: <971536df0609082106m6673e677rde16cb82a015eb7b@mail.gmail.com>
	<OFB30DE6D4.A23B64C0-ON652571E4.001760E5-652571E4.0018B23C@ccilindia.co.in>
Message-ID: <971536df0609090637x3d1bd0an8aea5c72f1f1cfd3@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060909/2044840b/attachment.pl 

From tlumley at u.washington.edu  Sat Sep  9 15:44:42 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Sat, 9 Sep 2006 06:44:42 -0700 (PDT)
Subject: [R] Multiple matrix multiplication with two 3-dimensional arrays
In-Reply-To: <loom.20060909T070618-433@post.gmane.org>
Message-ID: <Pine.LNX.4.43.0609090644420.9667@hymn04.u.washington.edu>

On Sat, 9 Sep 2006, Anupam Tyagi wrote:

> Sophie Baillargeon <Sophie.Baillargeon <at> mat.ulaval.ca> writes:


>> Maybe I could use an "apply" or something but I 
>> can't figure out how. I would have hoped that simply doing
>> 
>> array1%*%array2
> 
> would work, but it doesn?t


>I think one of the issues is that algebra for N-Dimentional arrays are not well
>defined. Think how would you define the above operartion on two 3x3x3 arrays.


We have the tensor package to handle this.  A simple binary operator has non-uniqueness problems, so you do have to specify which margins to sum over. With two-dimensions (matrices) there are only four possible sums over one margin: x %*% y, y%*%x, crossprod(x,y), tcrossprod(x,y), but with rank-3 tensors there are a lot more options.


         -thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From stat700004 at yahoo.co.in  Sat Sep  9 16:08:54 2006
From: stat700004 at yahoo.co.in (stat stat)
Date: Sat, 9 Sep 2006 15:08:54 +0100 (BST)
Subject: [R] How to rotate any plot in R
Message-ID: <20060909140854.83227.qmail@web7609.mail.in.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060909/dfe99466/attachment.pl 

From afshart at exchange.sba.miami.edu  Sat Sep  9 17:13:28 2006
From: afshart at exchange.sba.miami.edu (Afshartous, David)
Date: Sat, 9 Sep 2006 11:13:28 -0400
Subject: [R] augPred plot in nlme library
Message-ID: <6BCB4D493A447546A8126F24332056E80415E85F@school1.business.edu>

Hi Andrew,

Thanks for your email.  I assume you mean age^2 instead of age.2 for
fm2a,
and for fm2b, I get the following error:

> fm2b <- lme(distance ~ age + I(age^2), data = Orthodont)
Error in lme.formula(distance ~ age + I(age^2), data = Orthodont) : 
        iteration limit reached without convergence (9)

do you get his error as well?  

Finally, the Pixel example on p.42 of Pinheiro & Bates gets the 
quadratic plot w/o using I() as you do below; is this due to 
a difference between S and R?

thanks!
dave

ps - sorry for not making the data available; if anyone is interested
please
let me know and I'll send it directly.  




-----Original Message-----
From: Andrew Robinson [mailto:A.Robinson at ms.unimelb.edu.au] 
Sent: Friday, September 08, 2006 5:46 PM
To: Afshartous, David
Cc: Deepayan Sarkar; r-help at stat.math.ethz.ch
Subject: Re: [R] augPred plot in nlme library

Hi David,

this is the sort of thing that Deepayan meant.  Make a dataset available
to us, or use one that will be installed by default on R.

eg

require(nlme)
fm1 <- lme(distance ~ age, data = Orthodont)
plot(augPred(fm1))

#  All linear

fm2a <- lme(distance ~ age + age.2, data = Orthodont)
plot(augPred(fm2a))

# Still linear

fm2b <- lme(distance ~ age + I(age^2), data = Orthodont)
plot(augPred(fm2b))

# Quadratic!

I hope that this helps you resolve the problem.

Andrew



On Fri, Sep 08, 2006 at 05:18:13PM -0400, Afshartous, David wrote:
>  
> Deepayan,
> 
> Thanks for your suggestion.  Here are more details:
> 
> I have a grouped data object for repeated measures data just like the 
> Pixel grouped data object on p.42 of Pinheiro and Bates (2000).
> 
> comp.adj.UKV.3 <- groupedData(adj.UKV ~ Time | Patient_no/Lisinopril, 
> 	data = comp.adj.UKV.frm, order.groups = F
> 	#labels = list(x = "Hour", y = "adj.UKV")
> )
> 
> i.e., the response is continuous, Time is not treated as a factor, and

> there exists two factors, one nested within the other (Lisinopril 
> nested
> 
> witin patient, similar to Side within Dog on p.42).
> 
> I also fit a model very similar to their model:
> 
> fm1comp = lme(adj.UKV ~ Time + Time.sq, data = comp.adj.UKV.3, random 
> = list(Patient_no = ~ 1 , Lisinopril = ~ 1) )
> 
> 
> However, the command below does not produce the fitted curves from 
> this model, but rather it seems to be the fitted curves from a linear 
> model.
> 
> plot(augPred(fm3comp))
> 
> Possibly augPred behaves differently in R than in S, but reading the R

> help and trying various other approaches has not solved this.
> 
> Thanks!
> Dave
> 
> 
> 
> 
> -----Original Message-----
> From: Deepayan Sarkar [mailto:deepayan.sarkar at gmail.com]
> Sent: Friday, September 08, 2006 4:37 PM
> To: Afshartous, David
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] augPred plot in nlme library
> 
> On 9/8/06, Afshartous, David <afshart at exchange.sba.miami.edu> wrote:
> > All,
> >
> > I've solved part of the problem below by making sure that the 
> > formula in the grouped data object is the same as the formula 
> > specified within
> 
> > lme (this isn't the case in the cited example from Pinheiro &
Bates).
> >
> > However, augPred seems to plot only a linear model instead of the 
> > polynomial model.  Does anyone know how to make sure that augPred 
> > plots the same model as that specified in the model (as below)?
> 
> You are unlikely to get any helpful answers unless you give us more 
> information, as every r-help message asks you to do:
> 
> > R-help at stat.math.ethz.ch mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> -Deepayan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

--
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
Email: a.robinson at ms.unimelb.edu.au         http://www.ms.unimelb.edu.au


From jhallman at frb.gov  Fri Sep  8 21:46:14 2006
From: jhallman at frb.gov (Jeffrey J. Hallman)
Date: 08 Sep 2006 15:46:14 -0400
Subject: [R] Matrix multiplication using apply() or lappy() ?
References: <Pine.LNX.4.64.0609061825580.11591@gannet.stats.ox.ac.uk>
	<SEWINEXCH00gkJXrRjb000000ef@sewinexch00.insightful.com>
Message-ID: <xmrd5a6rvjt.fsf@mralx2.rsma.frb.gov>

Tim Hesterberg <timh at insightful.com> writes:

> toby_marks at americancentury.com asked:
> >I am trying to divide the columns of a matrix by the first row in the 
> >matrix.
> 
> Dividing columns of a matrix by a vector is a pretty fundamental
> operation, and the query resulted in a large number of suggestions:
> 
> 
> It is unsatisfactory when such a fundamental operation is
> done in so many different ways.  
> * It makes it hard to read other people's code.  
> * Some of these are very inefficient.

But since you have no way to force people to use your new 'standard'
operators, people can and will still use any of those myriad ways you decry to
do the same thing.  It's part of the price you pay for working with a flexible
system. 

Besides, nothing will ever make it easy to read other people's code. :-)

Jeff


From deepayan.sarkar at gmail.com  Sat Sep  9 17:45:43 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Sat, 9 Sep 2006 08:45:43 -0700
Subject: [R] augPred plot in nlme library
In-Reply-To: <6BCB4D493A447546A8126F24332056E80415E85F@school1.business.edu>
References: <6BCB4D493A447546A8126F24332056E80415E85F@school1.business.edu>
Message-ID: <eb555e660609090845g4cfd7930h3fe3d0827df85f6b@mail.gmail.com>

On 9/9/06, Afshartous, David <afshart at exchange.sba.miami.edu> wrote:
> Hi Andrew,
>
> Thanks for your email.  I assume you mean age^2 instead of age.2 for
> fm2a,
> and for fm2b, I get the following error:
>
> > fm2b <- lme(distance ~ age + I(age^2), data = Orthodont)
> Error in lme.formula(distance ~ age + I(age^2), data = Orthodont) :
>         iteration limit reached without convergence (9)
>
> do you get his error as well?

For me, adding 'control = list(msMaxIter = 500)' worked. I'm writing
from memory, so the name may not be exactly right, see ?nlmeControl.

> Finally, the Pixel example on p.42 of Pinheiro & Bates gets the
> quadratic plot w/o using I() as you do below; is this due to
> a difference between S and R?

Yes.

>
> thanks!
> dave
>
> ps - sorry for not making the data available; if anyone is interested
> please
> let me know and I'll send it directly.
>
>
>
>
> -----Original Message-----
> From: Andrew Robinson [mailto:A.Robinson at ms.unimelb.edu.au]
> Sent: Friday, September 08, 2006 5:46 PM
> To: Afshartous, David
> Cc: Deepayan Sarkar; r-help at stat.math.ethz.ch
> Subject: Re: [R] augPred plot in nlme library
>
> Hi David,
>
> this is the sort of thing that Deepayan meant.  Make a dataset available
> to us, or use one that will be installed by default on R.
>
> eg
>
> require(nlme)
> fm1 <- lme(distance ~ age, data = Orthodont)
> plot(augPred(fm1))
>
> #  All linear
>
> fm2a <- lme(distance ~ age + age.2, data = Orthodont)
> plot(augPred(fm2a))
>
> # Still linear
>
> fm2b <- lme(distance ~ age + I(age^2), data = Orthodont)
> plot(augPred(fm2b))
>
> # Quadratic!
>
> I hope that this helps you resolve the problem.
>
> Andrew
>
>
>
> On Fri, Sep 08, 2006 at 05:18:13PM -0400, Afshartous, David wrote:
> >
> > Deepayan,
> >
> > Thanks for your suggestion.  Here are more details:
> >
> > I have a grouped data object for repeated measures data just like the
> > Pixel grouped data object on p.42 of Pinheiro and Bates (2000).
> >
> > comp.adj.UKV.3 <- groupedData(adj.UKV ~ Time | Patient_no/Lisinopril,
> >       data = comp.adj.UKV.frm, order.groups = F
> >       #labels = list(x = "Hour", y = "adj.UKV")
> > )
> >
> > i.e., the response is continuous, Time is not treated as a factor, and
>
> > there exists two factors, one nested within the other (Lisinopril
> > nested
> >
> > witin patient, similar to Side within Dog on p.42).
> >
> > I also fit a model very similar to their model:
> >
> > fm1comp = lme(adj.UKV ~ Time + Time.sq, data = comp.adj.UKV.3, random
> > = list(Patient_no = ~ 1 , Lisinopril = ~ 1) )
> >
> >
> > However, the command below does not produce the fitted curves from
> > this model, but rather it seems to be the fitted curves from a linear
> > model.
> >
> > plot(augPred(fm3comp))
> >
> > Possibly augPred behaves differently in R than in S, but reading the R
>
> > help and trying various other approaches has not solved this.
> >
> > Thanks!
> > Dave
> >
> >
> >
> >
> > -----Original Message-----
> > From: Deepayan Sarkar [mailto:deepayan.sarkar at gmail.com]
> > Sent: Friday, September 08, 2006 4:37 PM
> > To: Afshartous, David
> > Cc: r-help at stat.math.ethz.ch
> > Subject: Re: [R] augPred plot in nlme library
> >
> > On 9/8/06, Afshartous, David <afshart at exchange.sba.miami.edu> wrote:
> > > All,
> > >
> > > I've solved part of the problem below by making sure that the
> > > formula in the grouped data object is the same as the formula
> > > specified within
> >
> > > lme (this isn't the case in the cited example from Pinheiro &
> Bates).
> > >
> > > However, augPred seems to plot only a linear model instead of the
> > > polynomial model.  Does anyone know how to make sure that augPred
> > > plots the same model as that specified in the model (as below)?
> >
> > You are unlikely to get any helpful answers unless you give us more
> > information, as every r-help message asks you to do:
> >
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> > -Deepayan


From fjbuch at gmail.com  Sat Sep  9 18:34:45 2006
From: fjbuch at gmail.com (Farrel Buchinsky)
Date: Sat, 9 Sep 2006 12:34:45 -0400
Subject: [R] Contacting a Wiki Author
Message-ID: <eduqf6$3kf$1@sea.gmane.org>

How do I contact a Wiki author?
for instance I was referring to tips:data-frames:generate_index_vectors and 
encountered an error when I input
ChickWeight$indexchick <- rep(1:(length(newchick)-1),
                              diff(newchick))
Error in "$<-.data.frame"(`*tmp*`, "indexchick", value = c(1, 1, 1, 1,  :
        replacement has 566 rows, data has 578

I can see that the author of the page is derwisch

But how does one e-mail derwisch? Does the wiki have a way of contacting a 
user?

-- 
Farrel Buchinsky, MD
Pediatric Otolaryngologist
Allegheny General Hospital
Pittsburgh, PA


From fjbuch at gmail.com  Sat Sep  9 19:01:51 2006
From: fjbuch at gmail.com (Farrel Buchinsky)
Date: Sat, 9 Sep 2006 13:01:51 -0400
Subject: [R] Indexing on a variable that is a factor
Message-ID: <edus20$80i$1@sea.gmane.org>

It is easy to index on numeric variables.
How does one index on a variable that is a factor?

-- 
Farrel Buchinsky, MD
Pediatric Otolaryngologist
Allegheny General Hospital
Pittsburgh, PA


From p.dalgaard at biostat.ku.dk  Sat Sep  9 19:08:39 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 09 Sep 2006 19:08:39 +0200
Subject: [R] Indexing on a variable that is a factor
In-Reply-To: <edus20$80i$1@sea.gmane.org>
References: <edus20$80i$1@sea.gmane.org>
Message-ID: <x2d5a5ezmw.fsf@turmalin.kubism.ku.dk>

"Farrel Buchinsky" <fjbuch at gmail.com> writes:

> It is easy to index on numeric variables.
> How does one index on a variable that is a factor?

Er, same way. Unless you attach some novel meaning to "index on".
Example?


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From ligges at statistik.uni-dortmund.de  Sat Sep  9 19:12:11 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 09 Sep 2006 19:12:11 +0200
Subject: [R] Indexing on a variable that is a factor
In-Reply-To: <edus20$80i$1@sea.gmane.org>
References: <edus20$80i$1@sea.gmane.org>
Message-ID: <4502F5EB.7060304@statistik.uni-dortmund.de>



Farrel Buchinsky wrote:
> It is easy to index on numeric variables.
> How does one index on a variable that is a factor?
> 


Same as for numeric objects. Example:

R> x <- factor(c("A", "B"))
R> x
[1] A B
Levels: A B

R> x[1]
[1] A
Levels: A B


From fjbuch at gmail.com  Sat Sep  9 19:12:18 2006
From: fjbuch at gmail.com (Farrel Buchinsky)
Date: Sat, 9 Sep 2006 13:12:18 -0400
Subject: [R] Indexing on a variable that is a factor
In-Reply-To: <x2d5a5ezmw.fsf@turmalin.kubism.ku.dk>
References: <edus20$80i$1@sea.gmane.org> <x2d5a5ezmw.fsf@turmalin.kubism.ku.dk>
Message-ID: <bd93cdad0609091012s7b79e9e6o94efb97800913027@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060909/e42c97ca/attachment.pl 

From ggrothendieck at gmail.com  Sat Sep  9 19:26:29 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 9 Sep 2006 13:26:29 -0400
Subject: [R] Contacting a Wiki Author
In-Reply-To: <eduqf6$3kf$1@sea.gmane.org>
References: <eduqf6$3kf$1@sea.gmane.org>
Message-ID: <971536df0609091026t70e1ec62x38501db370c66f7a@mail.gmail.com>

The idea of a wiki is that you modify the page yourself.  No need to email
anyone.  Just log in near the bottom right and change it or add a comment.

On 9/9/06, Farrel Buchinsky <fjbuch at gmail.com> wrote:
> How do I contact a Wiki author?
> for instance I was referring to tips:data-frames:generate_index_vectors and
> encountered an error when I input
> ChickWeight$indexchick <- rep(1:(length(newchick)-1),
>                              diff(newchick))
> Error in "$<-.data.frame"(`*tmp*`, "indexchick", value = c(1, 1, 1, 1,  :
>        replacement has 566 rows, data has 578
>
> I can see that the author of the page is derwisch
>
> But how does one e-mail derwisch? Does the wiki have a way of contacting a
> user?
>
> --
> Farrel Buchinsky, MD
> Pediatric Otolaryngologist
> Allegheny General Hospital
> Pittsburgh, PA
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From afshart at exchange.sba.miami.edu  Sat Sep  9 19:39:04 2006
From: afshart at exchange.sba.miami.edu (Afshartous, David)
Date: Sat, 9 Sep 2006 13:39:04 -0400
Subject: [R] augPred plot in nlme library
Message-ID: <6BCB4D493A447546A8126F24332056E80415E878@school1.business.edu>


Thanks Deepayan and Andrew.

msMaxIter solved the convergence problem and plot(augPred) works 
with my data when I employ I() in the function call.

One other strange thing I noticed is that when I take logs of
dependent variable in the function call, the plot of augPred 
doesn't graph any prediction line at all.  

contr=nlmeControl(msMaxIter = 500)
fm2c <- lme(log(distance) ~ age + I(age^2), data = Orthodont,
control=contr)
plot(augPred(fm2c)) 

However, this is fixed by hard coding the dependent variable:

log.dist = log(distance)
fm2c <- lme(log.dist ~ age + I(age^2), data = Orthodont, control=contr)
plot(augPred(fm2c))



-----Original Message-----
From: Deepayan Sarkar [mailto:deepayan.sarkar at gmail.com] 
Sent: Saturday, September 09, 2006 11:46 AM
To: Afshartous, David
Cc: Andrew Robinson; r-help at stat.math.ethz.ch
Subject: Re: [R] augPred plot in nlme library

On 9/9/06, Afshartous, David <afshart at exchange.sba.miami.edu> wrote:
> Hi Andrew,
>
> Thanks for your email.  I assume you mean age^2 instead of age.2 for 
> fm2a, and for fm2b, I get the following error:
>
> > fm2b <- lme(distance ~ age + I(age^2), data = Orthodont)
> Error in lme.formula(distance ~ age + I(age^2), data = Orthodont) :
>         iteration limit reached without convergence (9)
>
> do you get his error as well?

For me, adding 'control = list(msMaxIter = 500)' worked. I'm writing
from memory, so the name may not be exactly right, see ?nlmeControl.

> Finally, the Pixel example on p.42 of Pinheiro & Bates gets the 
> quadratic plot w/o using I() as you do below; is this due to a 
> difference between S and R?

Yes.

>
> thanks!
> dave
>
> ps - sorry for not making the data available; if anyone is interested 
> please let me know and I'll send it directly.
>
>
>
>
> -----Original Message-----
> From: Andrew Robinson [mailto:A.Robinson at ms.unimelb.edu.au]
> Sent: Friday, September 08, 2006 5:46 PM
> To: Afshartous, David
> Cc: Deepayan Sarkar; r-help at stat.math.ethz.ch
> Subject: Re: [R] augPred plot in nlme library
>
> Hi David,
>
> this is the sort of thing that Deepayan meant.  Make a dataset 
> available to us, or use one that will be installed by default on R.
>
> eg
>
> require(nlme)
> fm1 <- lme(distance ~ age, data = Orthodont)
> plot(augPred(fm1))
>
> #  All linear
>
> fm2a <- lme(distance ~ age + age.2, data = Orthodont)
> plot(augPred(fm2a))
>
> # Still linear
>
> fm2b <- lme(distance ~ age + I(age^2), data = Orthodont)
> plot(augPred(fm2b))
>
> # Quadratic!
>
> I hope that this helps you resolve the problem.
>
> Andrew
>
>
>
> On Fri, Sep 08, 2006 at 05:18:13PM -0400, Afshartous, David wrote:
> >
> > Deepayan,
> >
> > Thanks for your suggestion.  Here are more details:
> >
> > I have a grouped data object for repeated measures data just like 
> > the Pixel grouped data object on p.42 of Pinheiro and Bates (2000).
> >
> > comp.adj.UKV.3 <- groupedData(adj.UKV ~ Time |
Patient_no/Lisinopril,
> >       data = comp.adj.UKV.frm, order.groups = F
> >       #labels = list(x = "Hour", y = "adj.UKV")
> > )
> >
> > i.e., the response is continuous, Time is not treated as a factor, 
> > and
>
> > there exists two factors, one nested within the other (Lisinopril 
> > nested
> >
> > witin patient, similar to Side within Dog on p.42).
> >
> > I also fit a model very similar to their model:
> >
> > fm1comp = lme(adj.UKV ~ Time + Time.sq, data = comp.adj.UKV.3, 
> > random = list(Patient_no = ~ 1 , Lisinopril = ~ 1) )
> >
> >
> > However, the command below does not produce the fitted curves from 
> > this model, but rather it seems to be the fitted curves from a 
> > linear model.
> >
> > plot(augPred(fm3comp))
> >
> > Possibly augPred behaves differently in R than in S, but reading the

> > R
>
> > help and trying various other approaches has not solved this.
> >
> > Thanks!
> > Dave
> >
> >
> >
> >
> > -----Original Message-----
> > From: Deepayan Sarkar [mailto:deepayan.sarkar at gmail.com]
> > Sent: Friday, September 08, 2006 4:37 PM
> > To: Afshartous, David
> > Cc: r-help at stat.math.ethz.ch
> > Subject: Re: [R] augPred plot in nlme library
> >
> > On 9/8/06, Afshartous, David <afshart at exchange.sba.miami.edu> wrote:
> > > All,
> > >
> > > I've solved part of the problem below by making sure that the 
> > > formula in the grouped data object is the same as the formula 
> > > specified within
> >
> > > lme (this isn't the case in the cited example from Pinheiro &
> Bates).
> > >
> > > However, augPred seems to plot only a linear model instead of the 
> > > polynomial model.  Does anyone know how to make sure that augPred 
> > > plots the same model as that specified in the model (as below)?
> >
> > You are unlikely to get any helpful answers unless you give us more 
> > information, as every r-help message asks you to do:
> >
> > > R-help at stat.math.ethz.ch mailing list 
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> > -Deepayan


From dwarnold45 at cox.net  Sat Sep  9 20:45:52 2006
From: dwarnold45 at cox.net (David Arnold)
Date: Sat, 9 Sep 2006 11:45:52 -0700
Subject: [R] (no subject)
Message-ID: <47C16492-C043-475D-AA0A-1493264C244B@cox.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060909/f56a64ad/attachment.pl 

From ggrothendieck at gmail.com  Sat Sep  9 21:05:54 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 9 Sep 2006 15:05:54 -0400
Subject: [R] How to rotate any plot in R
In-Reply-To: <20060909140854.83227.qmail@web7609.mail.in.yahoo.com>
References: <20060909140854.83227.qmail@web7609.mail.in.yahoo.com>
Message-ID: <971536df0609091205q228d33dj5a7df7ea821414d5@mail.gmail.com>

With lattice or any grid graphics-based plot we can
draw it in a rotated viewport:

library(lattice)
library(grid)
grid.newpage()
pushViewport(viewport(angle = 90, name = "VP"))
upViewport()
x <- 1:10
print(xyplot(x ~ x, groups = gl(2, 5), auto.key = TRUE), draw.in = "VP")

or

grid.newpage()
pushViewport(viewport(angle = 90))
x <- 1:10
print(xyplot(x ~ x, groups = gl(2, 5), auto.key = TRUE), newpage = FALSE)
upViewport()

On 9/9/06, stat stat <stat700004 at yahoo.co.in> wrote:
> Dear all R users,
>
>  I am wondering whether it is possible in R to rotate any plot like Histogram, scatter plot, correlogram etc along with their legend, comments etc.
>
>  Thanks and regrads,
>  stat
>
>
> ---------------------------------
>  Find out what India is talking about on  - Yahoo! Answers India
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From scip1121 at yahoo.com.cn  Sat Sep  9 21:49:58 2006
From: scip1121 at yahoo.com.cn (w)
Date: Sun, 10 Sep 2006 03:49:58 +0800 (CST)
Subject: [R] duplication matrix
Message-ID: <20060909194958.33233.qmail@web92014.mail.cnb.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060910/590e5325/attachment.pl 

From cuauv at yahoo.com  Sat Sep  9 22:00:04 2006
From: cuauv at yahoo.com (Cuau)
Date: Sat, 9 Sep 2006 13:00:04 -0700 (PDT)
Subject: [R] clustering coefficient
Message-ID: <20060909200004.15163.qmail@web52301.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060909/db5a0244/attachment.pl 

From fjbuch at gmail.com  Sat Sep  9 23:06:37 2006
From: fjbuch at gmail.com (Farrel Buchinsky)
Date: Sat, 9 Sep 2006 17:06:37 -0400
Subject: [R] Contacting a Wiki Author
In-Reply-To: <971536df0609091026t70e1ec62x38501db370c66f7a@mail.gmail.com>
References: <eduqf6$3kf$1@sea.gmane.org>
	<971536df0609091026t70e1ec62x38501db370c66f7a@mail.gmail.com>
Message-ID: <bd93cdad0609091406r75672fc5nc1b0211247445351@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060909/8ed2f1fe/attachment.pl 

From cberry at tajo.ucsd.edu  Sun Sep 10 01:23:18 2006
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Sat, 9 Sep 2006 16:23:18 -0700
Subject: [R] duplication matrix
In-Reply-To: <20060909194958.33233.qmail@web92014.mail.cnb.yahoo.com>
References: <20060909194958.33233.qmail@web92014.mail.cnb.yahoo.com>
Message-ID: <Pine.LNX.4.64.0609091619320.28922@tajo.ucsd.edu>

On Sun, 10 Sep 2006, w wrote:

> Dear R-list members,
> Just wondering if there is any way to compute the duplication matrix in R.
> I tried to search for it but only found functions "xpnd" and "vech".
>

Something like this:

Dn <- function(x){
 	mat <- diag(x)
 	index <- seq(x*(x+1)/2)
 	mat[ lower.tri( mat , TRUE ) ] <- index
 	mat[ upper.tri( mat ) ] <- t( mat )[ upper.tri( mat ) ]
 	outer(c(mat), index , function( x , y ) ifelse(x==y, 1, 0 ) )
}

Dn(4) returns what you describe for a 4 x 4 matrix

> Basically for a symmetric n by n matrix A, the duplication matrix D_n is
> a matrix of dimension n^2 by n(n+1)/2 such that
> D_n vech(A)= c(A), where c(A) just vectorizes A.
>
> The duplication matrix is defined on page 49 of the book "Matrix
> differential calculus with applications in statistics and econometrics"
> by Magnus and Neudecker (1988 )
>
> Thanks a lot!
>
>
>
>
>
> ---------------------------------
> ????????????-3.5G??????20M????
> 	[[alternative HTML version deleted]]
>
>

Charles C. Berry                        (858) 534-2098
                                          Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	         UC San Diego
http://biostat.ucsd.edu/~cberry/         La Jolla, San Diego 92093-0717


From ggrothendieck at gmail.com  Sun Sep 10 01:41:26 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 9 Sep 2006 19:41:26 -0400
Subject: [R] duplication matrix
In-Reply-To: <20060909194958.33233.qmail@web92014.mail.cnb.yahoo.com>
References: <20060909194958.33233.qmail@web92014.mail.cnb.yahoo.com>
Message-ID: <971536df0609091641y2b097cb1k34ec46acf4844a20@mail.gmail.com>

Note that xpnd(v), which you mentioned in your post, is a linear transform
whose effect on v is the same as D_n %*% v so just pump an identity
matrix through xpnd to get its matrix:

library(MCMCpack)
D_n <- function(n) apply(diag(n*(n+1)/2), 2, xpnd, n)

# test
D_n(3)

On 9/9/06, w <scip1121 at yahoo.com.cn> wrote:
> Dear R-list members,
> Just wondering if there is any way to compute the duplication matrix in R.
> I tried to search for it but only found functions "xpnd" and "vech".
>
> Basically for a symmetric n by n matrix A, the duplication matrix D_n is
> a matrix of dimension n^2 by n(n+1)/2 such that
> D_n vech(A)= c(A), where c(A) just vectorizes A.
>
> The duplication matrix is defined on page 49 of the book "Matrix
> differential calculus with applications in statistics and econometrics"
> by Magnus and Neudecker (1988 )
>
> Thanks a lot!
>
>
>
>
>
> ---------------------------------
>  ????????????-3.5G??????20M????
>        [[alternative HTML version deleted]]
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


From A.Robinson at ms.unimelb.edu.au  Sun Sep 10 07:56:24 2006
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Sun, 10 Sep 2006 15:56:24 +1000
Subject: [R] Conservative "ANOVA tables" in lmer
In-Reply-To: <40e66e0b0609070559u656463c1l52a60eda9a095237@mail.gmail.com>
References: <1A8FE5B0-3FE3-4CA9-9224-9CACB9AF4854@muohio.edu>
	<40e66e0b0609070559u656463c1l52a60eda9a095237@mail.gmail.com>
Message-ID: <20060910055624.GA12212@ms.unimelb.edu.au>

On Thu, Sep 07, 2006 at 07:59:58AM -0500, Douglas Bates wrote:

> I would be happy to re-institute p-values for fixed effects in the
> summary and anova methods for lmer objects using a denominator degrees
> of freedom based on the trace of the hat matrix or the rank of Z:X if
> others will volunteer to respond to the "these answers are obviously
> wrong because they don't agree with <whatever> and the idiot who wrote
> this software should be thrashed to within an inch of his life"
> messages.  I don't have the patience.

This seems to be more than fair to me.  I'll volunteer to help explain
why the anova.lmer() output doesn't match SAS, etc.  Is it worth
putting a caveat in the output and the help files?  Is it even worth
writing a FAQ about this?

Cheers

Andrew
-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
Email: a.robinson at ms.unimelb.edu.au         http://www.ms.unimelb.edu.au


From spencer.graves at pdf.com  Sun Sep 10 08:54:50 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 09 Sep 2006 23:54:50 -0700
Subject: [R] Conservative "ANOVA tables" in lmer
In-Reply-To: <20060910055624.GA12212@ms.unimelb.edu.au>
References: <1A8FE5B0-3FE3-4CA9-9224-9CACB9AF4854@muohio.edu>	<40e66e0b0609070559u656463c1l52a60eda9a095237@mail.gmail.com>
	<20060910055624.GA12212@ms.unimelb.edu.au>
Message-ID: <4503B6BA.4030500@pdf.com>

Hi, Doug, et al.: 

      I'll volunteer to do the same, which is an extension of much of 
what I've been doing for R Help for a while now. 

      Regarding writing a FAQ, what about a Wiki entry (and maybe 
ultimately a vignette)?  This thread provides notes around which such 
could be built.  Another piece might be an example from Scheff? (1958), 
which I sent as a reply to an earlier comment on this thread, (foolishly 
sent without reducing the "cc" list, which means it "awaits moderator 
approval").   Each time a question of this nature arises, someone checks 
the Wiki, edits adds something to it if necessary, then replies to the 
list with the reference to the appropriate Wiki entry. 

      Spencer Graves

Andrew Robinson wrote:
> On Thu, Sep 07, 2006 at 07:59:58AM -0500, Douglas Bates wrote:
>
>   
>> I would be happy to re-institute p-values for fixed effects in the
>> summary and anova methods for lmer objects using a denominator degrees
>> of freedom based on the trace of the hat matrix or the rank of Z:X if
>> others will volunteer to respond to the "these answers are obviously
>> wrong because they don't agree with <whatever> and the idiot who wrote
>> this software should be thrashed to within an inch of his life"
>> messages.  I don't have the patience.
>>     
>
> This seems to be more than fair to me.  I'll volunteer to help explain
> why the anova.lmer() output doesn't match SAS, etc.  Is it worth
> putting a caveat in the output and the help files?  Is it even worth
> writing a FAQ about this?
>
> Cheers
>
> Andrew
>


From csardi at rmki.kfki.hu  Sun Sep 10 09:28:53 2006
From: csardi at rmki.kfki.hu (Gabor Csardi)
Date: Sun, 10 Sep 2006 10:28:53 +0300
Subject: [R] clustering coefficient
In-Reply-To: <20060909200004.15163.qmail@web52301.mail.yahoo.com>
References: <20060909200004.15163.qmail@web52301.mail.yahoo.com>
Message-ID: <20060910072853.GB5297@localdomain>

Cuau,

the igraph package has it, the function is called transitivity.
At least i think this is what you're looking for.

G.

On Sat, Sep 09, 2006 at 01:00:04PM -0700, Cuau wrote:
> 
>  Hi all,
> 
>  I've been looking for a function that calculates "clustering coefficient" either Newman's or Wattson's.
> 
>  I have found different functions to calculate different "cluster" but I haven't found yet anything for clustering coefficient, it might be under a different name.
> 
> Any ideas?
> 
> Cuau
> 
>  		
> ---------------------------------
> Get your email and more, right on the  new Yahoo.com 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Csardi Gabor <csardi at rmki.kfki.hu>    MTA RMKI, ELTE TTK


From jsorkin at grecc.umaryland.edu  Sun Sep 10 09:50:54 2006
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Sun, 10 Sep 2006 03:50:54 -0400
Subject: [R] Conservative "ANOVA tables" in lmer
In-Reply-To: <4503B6BA.4030500@pdf.com>
References: <1A8FE5B0-3FE3-4CA9-9224-9CACB9AF4854@muohio.edu>	<40e66e0b0609070559u656463c1l52a60eda9a095237@mail.gmail.com>
	<20060910055624.GA12212@ms.unimelb.edu.au><20060910055624.GA12212@ms.unimelb.edu.au>
	<4503B6BA.4030500@pdf.com>
Message-ID: <45038B9E020000CB00001254@MEDICINE.umaryland.edu>

Spencer,
Would you add the reference to the WIKI? I think it would help round out this thread.
John

John Sorkin M.D., Ph.D.
Chief, Biostatistics and Informatics
Baltimore VA Medical Center GRECC,
University of Maryland School of Medicine Claude D. Pepper OAIC,
University of Maryland Clinical Nutrition Research Unit, and
Baltimore VA Center Stroke of Excellence

University of Maryland School of Medicine
Division of Gerontology
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524

(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing)
jsorkin at grecc.umaryland.edu

>>> Spencer Graves <spencer.graves at pdf.com> 09/10/06 2:54 AM >>>
Hi, Doug, et al.: 

      I'll volunteer to do the same, which is an extension of much of 
what I've been doing for R Help for a while now. 

      Regarding writing a FAQ, what about a Wiki entry (and maybe 
ultimately a vignette)?  This thread provides notes around which such 
could be built.  Another piece might be an example from Scheff? (1958), 
which I sent as a reply to an earlier comment on this thread, (foolishly 
sent without reducing the "cc" list, which means it "awaits moderator 
approval").   Each time a question of this nature arises, someone checks 
the Wiki, edits adds something to it if necessary, then replies to the 
list with the reference to the appropriate Wiki entry. 

      Spencer Graves

Andrew Robinson wrote:
> On Thu, Sep 07, 2006 at 07:59:58AM -0500, Douglas Bates wrote:
>
>   
>> I would be happy to re-institute p-values for fixed effects in the
>> summary and anova methods for lmer objects using a denominator degrees
>> of freedom based on the trace of the hat matrix or the rank of Z:X if
>> others will volunteer to respond to the "these answers are obviously
>> wrong because they don't agree with <whatever> and the idiot who wrote
>> this software should be thrashed to within an inch of his life"
>> messages.  I don't have the patience.
>>     
>
> This seems to be more than fair to me.  I'll volunteer to help explain
> why the anova.lmer() output doesn't match SAS, etc.  Is it worth
> putting a caveat in the output and the help files?  Is it even worth
> writing a FAQ about this?
>
> Cheers
>
> Andrew
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html 
and provide commented, minimal, self-contained, reproducible code.

Confidentiality Statement:
This email message, including any attachments, is\ for the s...{{dropped}}


From mail at vrtprj.com  Sun Sep 10 10:56:51 2006
From: mail at vrtprj.com (Rainer Volz)
Date: Sun, 10 Sep 2006 08:56:51 +0000 (UTC)
Subject: [R] JGR on SUSE 10.1 AMD 64
References: <200609082101.24617.tomas.willebrand@szooek.slu.se>
Message-ID: <loom.20060910T104538-970@post.gmane.org>

Tomas Willebrand <tomas.willebrand <at> szooek.slu.se> writes:

> 
> Dear list;
> 
> I have R Version 2.3.1 (2006-06-01) installed on a AMD 64 machine with SUSE 
> 10.1. 
> 
> I have Sun Java version 1.5.0-sun installed. 
> 
> I have used "install.packages("JGR", dep=TRUE) to install rJava, iplots and 
> JGR. I would like to do a presentation of linux and R for the department!
> 
> Compiling rJava do not give any error messages!
> 
> But when trying to load library("JGR") or just library("rJava") I get the 
> following error message:
> > Error in dyn.load(x, as.logical(local), as.logical(now)) :
> > unable to load shared library  '/usr/lib64/R/library/rJava/libs/rJava.so':
> > libjvm.so: kan inte ?ppna delad objektfil: Filen eller katalogen finns inte
> > Error in library("rJava") : .First.lib failed for 'rJava'
> 
I have the same configuration: adding various library paths for the 
missing libs before calling R made the installation succeed:

export
LD_LIBRARY_PATH=/usr/lib64/R/lib:
/usr/lib64/jvm/java-1.5.0-sun-1.5.0_07/jre/lib/amd64/server:
/usr/lib64/jvm/java-1.5.0-sun-1.5.0_07/jre/lib/amd64:
$LD_LIBRARY_PATH
   

HTH
Rainer


From ripley at stats.ox.ac.uk  Sun Sep 10 11:29:01 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 10 Sep 2006 10:29:01 +0100 (BST)
Subject: [R] JGR on SUSE 10.1 AMD 64
In-Reply-To: <loom.20060910T104538-970@post.gmane.org>
References: <200609082101.24617.tomas.willebrand@szooek.slu.se>
	<loom.20060910T104538-970@post.gmane.org>
Message-ID: <Pine.LNX.4.64.0609101021490.17390@gannet.stats.ox.ac.uk>

On Sun, 10 Sep 2006, Rainer Volz wrote:

> Tomas Willebrand <tomas.willebrand <at> szooek.slu.se> writes:
> 
> > I have R Version 2.3.1 (2006-06-01) installed on a AMD 64 machine with 
> > SUSE 10.1.

Installed how is the usual question: from sources or an RPM?

> > I have Sun Java version 1.5.0-sun installed. 
> > 
> > I have used "install.packages("JGR", dep=TRUE) to install rJava, 
> > iplots and JGR. I would like to do a presentation of linux and R for 
> > the department!
> > 
> > Compiling rJava do not give any error messages!
> > 
> > But when trying to load library("JGR") or just library("rJava") I get 
> > the following error message:
> > > Error in dyn.load(x, as.logical(local), as.logical(now)) :
> > > unable to load shared library  '/usr/lib64/R/library/rJava/libs/rJava.so':
> > > libjvm.so: kan inte ?ppna delad objektfil: Filen eller katalogen finns inte
> > > Error in library("rJava") : .First.lib failed for 'rJava'
> > 
> I have the same configuration: adding various library paths for the 
> missing libs before calling R made the installation succeed:
> 
> export
> LD_LIBRARY_PATH=/usr/lib64/R/lib:
> /usr/lib64/jvm/java-1.5.0-sun-1.5.0_07/jre/lib/amd64/server:
> /usr/lib64/jvm/java-1.5.0-sun-1.5.0_07/jre/lib/amd64:
> $LD_LIBRARY_PATH

This is something that is set up when R is configured, and stored in 
R_HOME/etc/ldpaths.  Has perchance your Java subsystem been updated since 
R was installed?  In which case update R_HOME/etc/ldpaths as appropriate, 
or set R_JAVA_LD_LIBRARY_PATH (not LD_LIBRARY_PATH) to override the 
setting there.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From mail at vrtprj.com  Sun Sep 10 14:02:15 2006
From: mail at vrtprj.com (Rainer Volz)
Date: Sun, 10 Sep 2006 12:02:15 +0000 (UTC)
Subject: [R] JGR on SUSE 10.1 AMD 64
References: <200609082101.24617.tomas.willebrand@szooek.slu.se>
	<loom.20060910T104538-970@post.gmane.org>
	<Pine.LNX.4.64.0609101021490.17390@gannet.stats.ox.ac.uk>
Message-ID: <loom.20060910T135902-950@post.gmane.org>

Prof Brian Ripley <ripley <at> stats.ox.ac.uk> writes:

> This is something that is set up when R is configured, and stored in 
> R_HOME/etc/ldpaths.  Has perchance your Java subsystem been updated since 
> R was installed?  

True.

> In which case update R_HOME/etc/ldpaths as appropriate, 
> or set R_JAVA_LD_LIBRARY_PATH (not LD_LIBRARY_PATH) to override the 
> setting there.

That's much better. Thank you.


From tomas.willebrand at szooek.slu.se  Sun Sep 10 18:51:51 2006
From: tomas.willebrand at szooek.slu.se (Tomas Willebrand)
Date: Sun, 10 Sep 2006 18:51:51 +0200
Subject: [R] JGR on SUSE 10.1 AMD 64
In-Reply-To: <loom.20060910T135902-950@post.gmane.org>
References: <200609082101.24617.tomas.willebrand@szooek.slu.se>
	<Pine.LNX.4.64.0609101021490.17390@gannet.stats.ox.ac.uk>
	<loom.20060910T135902-950@post.gmane.org>
Message-ID: <200609101851.51850.tomas.willebrand@szooek.slu.se>

Prof Ripley and Rainer;

Thanks for the help. 

I changed the file "/usr/lib64/R/etc/ldpaths" so that the first line looked 
like

:${R_JAVA_LD_LIBRARY_PATH= 
/usr/lib64/jvm/java-1.5.0-sun-1.5.0_07/jre/lib/amd64/server:
/usr/lib64/jvm/java-1.5.0-sun-1.5.0_07/jre/lib/amd64:} 

and then everything worked OK!

But when I tested a fresh install of the R-base*rpm and changed the ldpaths 
file before attempting to install JGR it did not install properly!! Not the 
same error as previously when iplots was suppose to load! (ldpaths identical 
in both cases)

-- Install R-base -> failed installation of rJava -> edit ldpaths -> reinstall 
rJava,iplots, JGR -> SUCCESS

-- Install R-base -> edit ldpaths -> loading rJava fails after compiling  
-> ????

Not sure what is going on and do not need to know! 
But maybe someone would find this note useful.

Thanks again

Tomas



s?ndag 10 september 2006 14:02 skrev Rainer Volz:
> Prof Brian Ripley <ripley <at> stats.ox.ac.uk> writes:
> > This is something that is set up when R is configured, and stored in
> > R_HOME/etc/ldpaths.  Has perchance your Java subsystem been updated since
> > R was installed?
>
> True.
>
> > In which case update R_HOME/etc/ldpaths as appropriate,
> > or set R_JAVA_LD_LIBRARY_PATH (not LD_LIBRARY_PATH) to override the
> > setting there.
>
> That's much better. Thank you.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented, minimal,
> self-contained, reproducible code.

-- 
+46 70 288 02 03


From hydinghua at gmail.com  Sun Sep 10 18:57:08 2006
From: hydinghua at gmail.com (Philip He)
Date: Sun, 10 Sep 2006 11:57:08 -0500
Subject: [R] R number output format
Message-ID: <8727b8b60609100957x43b3b95cy329934b295656210@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060910/a8cca4ce/attachment.pl 

From m.blizinski at wit.edu.pl  Sun Sep 10 19:09:42 2006
From: m.blizinski at wit.edu.pl (Maciej =?UTF-8?Q?Blizi=C5=84ski?=)
Date: Sun, 10 Sep 2006 19:09:42 +0200
Subject: [R] Rpart, custom penalty for an error
Message-ID: <1157908182.21564.13.camel@localhost.localnet>

Hello all R-help list subscribers,

I'd like to create a regression tree of a data set with binary response
variable. Only 5% of observations are a success, so the regression tree
will not find really any variable value combinations that will yield
more than 50% of probability of success. I am however interested in
areas where the probability of success is noticeably higher than 5%, for
example 20%. I've tried rpart and the weights option, increasing the
weights of the success-observations.

It works as expected in terms of the tree creation: instead of a single
root, a tree is being built. But the tree plot() and text() are somewhat
misleading. I'm interested in the observation counts inside each leaf.
I use the "use.n = TRUE" parameter. The counts displayed are misleading,
the numbers of successes are not the original numbers from the sample,
they seem to be cloned success-observations.

I'd like to split the tree just as weights parameter allows me to,
keeping the original number of observations in the tree plot. Is it
possible? If yes, how?

Kind regards,
Maciej

-- 
Maciej Blizi?ski <m.blizinski at wit.edu.pl>
http://automatthias.wordpress.com


From jholtman at gmail.com  Sun Sep 10 19:27:55 2006
From: jholtman at gmail.com (jim holtman)
Date: Sun, 10 Sep 2006 13:27:55 -0400
Subject: [R] R number output format
In-Reply-To: <8727b8b60609100957x43b3b95cy329934b295656210@mail.gmail.com>
References: <8727b8b60609100957x43b3b95cy329934b295656210@mail.gmail.com>
Message-ID: <644e1f320609101027j4874b211q16a3ef69a4923301@mail.gmail.com>

?formatC

> formatC(.000000012, format='fg')
[1] "0.000000012"
>


also

?sprintf

> sprintf("%.10f", 0.0000000012)
[1] "0.0000000012"
>


On 9/10/06, Philip He <hydinghua at gmail.com> wrote:
> I'd like to save the number 0.0000012 to a file just as it appears, but when
> I dput(0.0000012, "t.txt"), I got 1.2e-06, NOT 0.0000012.
> Anybody knows how I can do this?
>
> > 0.0000012
> [1] 1.2e-06
>
> > as.character(0.0000012)
> [1] "1.2e-06"
>
> > dput(0.0000012, "t.txt")
>
>
> Thanks.
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From ethan.johnsons at gmail.com  Sun Sep 10 20:48:21 2006
From: ethan.johnsons at gmail.com (Ethan Johnsons)
Date: Sun, 10 Sep 2006 14:48:21 -0400
Subject: [R] rownumber
Message-ID: <5cd96f050609101148i280cc0e2j2d17ad7843ee8fc9@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060910/2b333348/attachment.pl 

From Dimitris.Rizopoulos at med.kuleuven.be  Sun Sep 10 20:58:18 2006
From: Dimitris.Rizopoulos at med.kuleuven.be (Dimitrios Rizopoulos)
Date: Sun, 10 Sep 2006 20:58:18 +0200
Subject: [R] rownumber
In-Reply-To: <5cd96f050609101148i280cc0e2j2d17ad7843ee8fc9@mail.gmail.com>
References: <5cd96f050609101148i280cc0e2j2d17ad7843ee8fc9@mail.gmail.com>
Message-ID: <20060910205818.idprb7x4rn3kosoo@webmail4.kuleuven.be>

maybe the following is what you're looking for:

lead.dat[!duplicate(rownames(lead.dat)), ]

lead.dat[duplicate(rownames(lead.dat)), ]


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
      http://www.student.kuleuven.be/~m0390867/dimitris.htm


Quoting Ethan Johnsons <ethan.johnsons at gmail.com>:

> # R 2.3.1
>
> I read the data set into R, but the data set have two rows per record.
>
> i.e.
>
> .........
> 105     1       1       1103    1       5       4       8       5       22
> 105     2       2       2       2       1       2       17      16      26
> 106     1       2       606     1       5       12      11      9       37
> 106     2       2       2       2       2       2       16      14      29
> 107     1       3       611     1       7       9       10      7       33
> 107     2       2       2       2       2       2       10      13      29
> ...........
>
> How do you extract the certain columns on the first row set of each?
>
> Tthe first row set are like:
> 105     1       1       1103    1       5       4       8       5       22
> 106     1       2       606     1       5       12      11      9       37
> 107     1       3       611     1       7       9       10      7       33
>
> so, nothing gets imported from the second row set.
>
> 105     2       2       2       2       1       2       17      16      26
> 106     2       2       2       2       2       2       16      14      29
> 107     2       2       2       2       2       2       10      13      29
> I tried:
>
>> a <- subset(lead.dat, rownumber==1)
> Error in eval(expr, envir, enclos) : object "rownumber" not found
>> b <- subset(lead.dat, rownumber==2)
> Error in eval(expr, envir, enclos) : object "rownumber" not found
>
> What is wrong with this?
> thx much
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>



Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From mothsailor at googlemail.com  Sun Sep 10 21:18:49 2006
From: mothsailor at googlemail.com (David Barron)
Date: Sun, 10 Sep 2006 20:18:49 +0100
Subject: [R] R number output format
In-Reply-To: <644e1f320609101027j4874b211q16a3ef69a4923301@mail.gmail.com>
References: <8727b8b60609100957x43b3b95cy329934b295656210@mail.gmail.com>
	<644e1f320609101027j4874b211q16a3ef69a4923301@mail.gmail.com>
Message-ID: <815b70590609101218m243165dcyd37e3ebae31c7586@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060910/6566beb9/attachment.pl 

From mothsailor at googlemail.com  Sun Sep 10 21:32:45 2006
From: mothsailor at googlemail.com (David Barron)
Date: Sun, 10 Sep 2006 20:32:45 +0100
Subject: [R] rownumber
In-Reply-To: <5cd96f050609101148i280cc0e2j2d17ad7843ee8fc9@mail.gmail.com>
References: <5cd96f050609101148i280cc0e2j2d17ad7843ee8fc9@mail.gmail.com>
Message-ID: <815b70590609101232x75e63266xe86743d5eb713c1@mail.gmail.com>

This will work too:

> n <- dim(lead.dat)[1]
> n
[1] 6
> ix <- seq(1,n,by=2)
> ix
[1] 1 3 5
> lead.dat[ix,]
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
[1,]  105    1    1 1103    1    5    4    8    5    22
[2,]  106    1    2  606    1    5   12   11    9    37
[3,]  107    1    3  611    1    7    9   10    7    33

-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From ripley at stats.ox.ac.uk  Sun Sep 10 21:36:10 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 10 Sep 2006 20:36:10 +0100 (BST)
Subject: [R] Rpart, custom penalty for an error
In-Reply-To: <1157908182.21564.13.camel@localhost.localnet>
References: <1157908182.21564.13.camel@localhost.localnet>
Message-ID: <Pine.LNX.4.64.0609102029370.17500@gannet.stats.ox.ac.uk>

On Sun, 10 Sep 2006, Maciej Blizi?ski wrote:

> Hello all R-help list subscribers,
> 
> I'd like to create a regression tree of a data set with binary response
> variable. Only 5% of observations are a success, so the regression tree
> will not find really any variable value combinations that will yield
> more than 50% of probability of success. 

This would be a misuse of a regression tree, for the exact problem for 
which classification trees were designed.

> I am however interested in areas where the probability of success is 
> noticeably higher than 5%, for example 20%. I've tried rpart and the 
> weights option, increasing the weights of the success-observations.

You are 'misleading' rpart by using 'weights', claiming to have case
weights for cases you do not have.  You need to use 'cost' instead.

This is a standard issue, discussed in all good books on classification
(including mine).

> It works as expected in terms of the tree creation: instead of a single
> root, a tree is being built. But the tree plot() and text() are somewhat
> misleading. I'm interested in the observation counts inside each leaf.
> I use the "use.n = TRUE" parameter. The counts displayed are misleading,
> the numbers of successes are not the original numbers from the sample,
> they seem to be cloned success-observations.

They _are_ the original numbers, for that is what 'case weights' means.

> I'd like to split the tree just as weights parameter allows me to,
> keeping the original number of observations in the tree plot. Is it
> possible? If yes, how?
> 
> Kind regards,
> Maciej

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From bgreen at dyson.brisnet.org.au  Sun Sep 10 22:24:16 2006
From: bgreen at dyson.brisnet.org.au (Bob Green)
Date: Mon, 11 Sep 2006 06:24:16 +1000
Subject: [R] formatting data to be analysed using multinomial logistic
 regression (nnet)
In-Reply-To: <mailman.13.1157623203.23523.r-help@stat.math.ethz.ch>
Message-ID: <5.1.0.14.0.20060910205402.00c10ba8@pop3.brisnet.org.au>



I am looking into using the multinomial logistic regression option in the 
nnet library and have two questions about formatting the data.

1. Can data be analysed in the following format  or does it need to be 
transformed into count data, such as the housing data in MASS?

Id	Crime	paranoia	hallucinate	toc	disorg	crimhist		age
1	2	1		0		1	0	1		25
2	2	0		1		1	1	1		37
3	1	1		0		1	1	0		42			
4	3	0		0		0	0	1		25
5	2	1		0		1	0	0		49


2.    Can a ratio variable such as $age be included into a model, such as 
the one below?


crimepred <- glm  (crime ~ paranoia + hallucinate + toc  + crimhist, family 
= poisson, data = mht )


Any assistance with the above is appreciated,


regards

Bob Green


From dwarnold45 at cox.net  Sun Sep 10 22:58:57 2006
From: dwarnold45 at cox.net (David Arnold)
Date: Sun, 10 Sep 2006 13:58:57 -0700
Subject: [R] Bargraphs by a Newbie
Message-ID: <A801E22C-1CDB-4C0F-9E48-7FBDCBB72F29@cox.net>

All,

I have this data saved in the file bargraph.dat:

Month Births Deaths
January 305 218
February 289 191
March 313 198
April 342 189
May 311 195
June 324 182
July 345 192
August 341 178
September 353 176
October 329 193
November 304 189
December 324 192

I start an R session, change to the directory containin bargraph.dat,  
and:

 > bd=read.table("bargraph.dat",header=T)
 > bd
        Month Births Deaths
1    January    305    218
2   February    289    191
3      March    313    198
4      April    342    189
5        May    311    195
6       June    324    182
7       July    345    192
8     August    341    178
9  September    353    176
10   October    329    193
11  November    304    189
12  December    324    192

Seems successful. But when I try:

 > attach(bd)
 > barplot(Births,names.arg=Month)

I get a barplot with labels on the bottom that are the "numbers" of  
the months, and they are unordered. Can the above code be used to get  
the actual names of the months?

I did get this to work.

 > b=Births
 > names(b)=Month
 > barplot(b)

This does show the names of the months below each bar. However,  
because the names are so long, not all show up unless I manually  
enlarge the size of the window with the mouse. This raises a second  
question. How can I rotate the names of the months in this last graph  
90 degrees?

Thanks.


From dwarnold45 at cox.net  Sun Sep 10 23:00:12 2006
From: dwarnold45 at cox.net (David Arnold)
Date: Sun, 10 Sep 2006 14:00:12 -0700
Subject: [R] Removing a graphic from plot
Message-ID: <DA8D2357-86F5-4727-8839-F79619D7D0B3@cox.net>

All,

What is the standard procedure for removing a graphic from an  
interactive plot? For example, suppose I use abline to add a line,  
then later decide that I want to remove it?

Thanks.


From jholtman at gmail.com  Sun Sep 10 23:47:08 2006
From: jholtman at gmail.com (jim holtman)
Date: Sun, 10 Sep 2006 17:47:08 -0400
Subject: [R] Bargraphs by a Newbie
In-Reply-To: <A801E22C-1CDB-4C0F-9E48-7FBDCBB72F29@cox.net>
References: <A801E22C-1CDB-4C0F-9E48-7FBDCBB72F29@cox.net>
Message-ID: <644e1f320609101447o51105950h778e35e68c8e94c3@mail.gmail.com>

Month is a factor so you have to change it to character:

> barplot(x$Births, names.arg=as.character(x$Month))


On 9/10/06, David Arnold <dwarnold45 at cox.net> wrote:
> All,
>
> I have this data saved in the file bargraph.dat:
>
> Month Births Deaths
> January 305 218
> February 289 191
> March 313 198
> April 342 189
> May 311 195
> June 324 182
> July 345 192
> August 341 178
> September 353 176
> October 329 193
> November 304 189
> December 324 192
>
> I start an R session, change to the directory containin bargraph.dat,
> and:
>
>  > bd=read.table("bargraph.dat",header=T)
>  > bd
>        Month Births Deaths
> 1    January    305    218
> 2   February    289    191
> 3      March    313    198
> 4      April    342    189
> 5        May    311    195
> 6       June    324    182
> 7       July    345    192
> 8     August    341    178
> 9  September    353    176
> 10   October    329    193
> 11  November    304    189
> 12  December    324    192
>
> Seems successful. But when I try:
>
>  > attach(bd)
>  > barplot(Births,names.arg=Month)
>
> I get a barplot with labels on the bottom that are the "numbers" of
> the months, and they are unordered. Can the above code be used to get
> the actual names of the months?
>
> I did get this to work.
>
>  > b=Births
>  > names(b)=Month
>  > barplot(b)
>
> This does show the names of the months below each bar. However,
> because the names are so long, not all show up unless I manually
> enlarge the size of the window with the mouse. This raises a second
> question. How can I rotate the names of the months in this last graph
> 90 degrees?
>
> Thanks.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From ggrothendieck at gmail.com  Mon Sep 11 00:56:53 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 10 Sep 2006 18:56:53 -0400
Subject: [R] Removing a graphic from plot
In-Reply-To: <DA8D2357-86F5-4727-8839-F79619D7D0B3@cox.net>
References: <DA8D2357-86F5-4727-8839-F79619D7D0B3@cox.net>
Message-ID: <971536df0609101556r5404ec3bk716ffafc2b72e672@mail.gmail.com>

This was discussed just this week:

http://www.nabble.com/-R--deleting-an-arow-added-to-a-graphic-t2228973.html

On 9/10/06, David Arnold <dwarnold45 at cox.net> wrote:
> All,
>
> What is the standard procedure for removing a graphic from an
> interactive plot? For example, suppose I use abline to add a line,
> then later decide that I want to remove it?
>
> Thanks.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From john.maindonald at anu.edu.au  Mon Sep 11 01:24:08 2006
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Mon, 11 Sep 2006 09:24:08 +1000
Subject: [R] Conservative "ANOVA tables" in lmer
In-Reply-To: <mailman.8.1157882403.15598.r-help@stat.math.ethz.ch>
References: <mailman.8.1157882403.15598.r-help@stat.math.ethz.ch>
Message-ID: <88946556-D1DC-4752-92C8-F730C4178C05@anu.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060911/edbec6e6/attachment.pl 

From rdbisch at gmail.com  Mon Sep 11 05:45:38 2006
From: rdbisch at gmail.com (Rick Bischoff)
Date: Sun, 10 Sep 2006 23:45:38 -0400
Subject: [R] faster way?
Message-ID: <E65F8139-8B7F-48B4-8CC8-79D0E34A325D@gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060910/d57df41d/attachment.pl 

From mcpung at gmail.com  Mon Sep 11 06:32:58 2006
From: mcpung at gmail.com (Murray Pung)
Date: Mon, 11 Sep 2006 14:32:58 +1000
Subject: [R] graphics: y limit on xyplot
Message-ID: <8d6f66050609102132g2998d06p7bf4f1e1176af171@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060911/d4e6eed1/attachment.pl 

From ggrothendieck at gmail.com  Mon Sep 11 06:42:21 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 11 Sep 2006 00:42:21 -0400
Subject: [R] graphics: y limit on xyplot
In-Reply-To: <8d6f66050609102132g2998d06p7bf4f1e1176af171@mail.gmail.com>
References: <8d6f66050609102132g2998d06p7bf4f1e1176af171@mail.gmail.com>
Message-ID: <971536df0609102142s66b35d5eq1f496d2e2cab51f3@mail.gmail.com>

It should have length 2, i.e. lower limit and upper limit.

On 9/11/06, Murray Pung <mcpung at gmail.com> wrote:
> I would like to set the y axis limit of an xyplot using the object 'ylimit',
> but receive this error:
> [1] 990
> Error in extend.limits(limitlist[[i]], axs = axs) :
>   improper length of lim
>
> I get the same error if I use ylim.
>
> library(lattice)
> trellis.device(col = FALSE, theme = lattice.getOption("col.whitebg"))
> name <- "Variable name"
> symbols <- c(1,2,3,4,5,6,7,8,9)
> patientp <- c(1,2,3,4,5,6,7,8,9,1,2,3,4,5,6,7,8,9)
> varp <-
> c(826,119,168,90,572,323,122,10,42,900,250,180,120,650,400,130,12,33)
> visitp <- c(1,1,1,1,1,1,1,1,1,3,3,3,3,3,3,3,3,3)
> yall <- c(varp,varl,varm,varh)
> ylimit <- max(yall)*1.1
> xyplot(varp ~ visitp,
>            xlab = "Visit",
>            ylab = name,
>            group = patientp,
>            type = "b",
>            lty = 1,
>            as.table = TRUE,
>            main = list("Placebo",cex = 1.0),
>            scales = list(relation = "free",x = list(tick.number = 1,at =
> c(1,3)),y = list(limits = ylimit)),
>            auto.key = list(space = "right",cex = 1.1),
>            par.settings = list(superpose.symbol = list(pch = symbols,cex =
> 1.1)))
>
>
>
>
> --
> Murray Pung
> Statistician, Datapharm Australia Pty Ltd
> 0404 273 283
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From rroa at udec.cl  Mon Sep 11 06:59:48 2006
From: rroa at udec.cl (Ruben Roa Ureta)
Date: Mon, 11 Sep 2006 00:59:48 -0400 (CLT)
Subject: [R] Extracting mean level of factor for all levels from a glm
Message-ID: <3457.201.223.234.194.1157950788.squirrel@webmail.udec.cl>

ComRades:

I have a glm where interest lies in the estimated mean levels of the
response Y for every level of factor Year (regardeless of statistical
significance of the factor), when the effect of other factors have been
accounted for. The model is of the form:

fit<-glm(Y~Year+Mo+Area+Ship+Eff,family=gaussian(link="log"),data=data)

where Year is categorical and has T levels (labeled 1 to T in the data).
Help much appreciated on how to extract these 'standardised' annual
estimates of Y and their standard errors.

Rub?n


From gyadav at ccilindia.co.in  Mon Sep 11 07:44:58 2006
From: gyadav at ccilindia.co.in (gyadav at ccilindia.co.in)
Date: Mon, 11 Sep 2006 11:14:58 +0530
Subject: [R] How to actively join you all
In-Reply-To: <971536df0609090637x3d1bd0an8aea5c72f1f1cfd3@mail.gmail.com>
Message-ID: <OFA9B94E17.158470F6-ON652571E6.001E529C-652571E6.001FB75F@ccilindia.co.in>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060911/a07b2ea3/attachment.pl 

From attenka at utu.fi  Mon Sep 11 08:23:44 2006
From: attenka at utu.fi (kone)
Date: Mon, 11 Sep 2006 09:23:44 +0300
Subject: [R] Weighted association map
In-Reply-To: <971536df0609080453p2565c7d5o69d07e895fde4ba4@mail.gmail.com>
References: <38929107-8CB9-4C22-827F-952D371B094E@local>
	<971536df0609080237q12aa5bc7pb9069d0a2dca57a5@mail.gmail.com>
	<971536df0609080453p2565c7d5o69d07e895fde4ba4@mail.gmail.com>
Message-ID: <8688BD6B-8649-4CF2-A5C4-AAD86E0F380D@mus.utu.fi>

Sorry to answer so late, but this is just what I want ;-)

-Atte

Gabor Grothendieck kirjoitti 8.9.2006 kello 14.53:

> Actually the discretization does not appear to be needed.  This
> works just as well:
>
> set.seed(123)
> kor <- cor(iris[1:4])
> gplot(sign(kor), edge.lwd = 10*kor, displaylabels = TRUE, label =  
> rownames(kor))
>
> On 9/8/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
>> Try the sna package.  Below we calculate the
>> correlation matrix, kor, of the numeric cols of builtin iris
>> dataset.  Zap negative ones and discretize rest to
>> get lwd width matrix, lwd, used for edge widths.  From
>> that create the adjacency matrix, sign(lwd), and plot it
>> using indicated layout mode.  Seems like three of
>> the variables are correlated and Sepal.Width is uncorrelated
>> or negatively correlated to those. Try playing around with
>> gplot args to create variations.
>>
>> library(sna)
>> set.seed(123) # layout uses random numbers
>> kor <- cor(iris[1:4])
>> lwd <- replace(kor, TRUE, 10 * round(pmax(0, kor), 1))
>> gplot(sign(lwd), edge.lwd = lwd, displaylabels = TRUE, label =  
>> rownames(kor))
>>
>> On 9/8/06, kone <attenka at utu.fi> wrote:
>> > Could somebody program this kind of plot type to R, if none exists,
>> > based on mds or correlation tables or some more suitable method?  
>> What
>> > do you think about idea? Does it work? None similar or better  
>> exists?
>> >
>> > http://weightedassociationmap.blogspot.com/
>> >
>> >
>> > Atte Tenkanen
>> > University of Turku, Finland
>> >
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide http://www.R-project.org/ 
>> posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>> >
>>


From ligges at statistik.uni-dortmund.de  Mon Sep 11 08:47:21 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 11 Sep 2006 08:47:21 +0200
Subject: [R] faster way?
In-Reply-To: <E65F8139-8B7F-48B4-8CC8-79D0E34A325D@gmail.com>
References: <E65F8139-8B7F-48B4-8CC8-79D0E34A325D@gmail.com>
Message-ID: <45050679.4040800@statistik.uni-dortmund.de>



Rick Bischoff wrote:
> Hi,
> 
> Is there a faster way to do this? It takes forever, even on a  
> moderately sized dataset.
> 
> 
> n          <- dim(dsn)[1]
> dsn2 <- dsn[order(-dsn$xhat),]
> dsn2[1, "cumx"] <- dsn2[1, "xhat"]
> 
> for (i in 2:n) {
> 	dsn2[i, "cumx"] <- dsn2[i - 1, "cumx"] + dsn2[i, "xhat"]
> }



dsn2 <- dsn[order(-dsn$xhat),]
dsn2[,"cumx"] <- cumsum(dsn2[,"xhat"])




> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From petr.pikal at precheza.cz  Mon Sep 11 08:51:31 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 11 Sep 2006 08:51:31 +0200
Subject: [R] augPred plot in nlme library
In-Reply-To: <6BCB4D493A447546A8126F24332056E80415E878@school1.business.edu>
Message-ID: <45052393.4461.34DFE3@localhost>

Hi

please try not to hide an information

> plot(augPred(fm2c)) 
Error in log(distance) : object "distance" not found

Is it what you have got and what you mean by "does not graph any 
prediction at all"?

If not did you attached Orthodont before?

I suppose plot.augPred probably expects the same name for original 
and and fitted data. You can go through source code to see what 
happens by

nlme:::plot.augPred

HTH
Petr



On 9 Sep 2006 at 13:39, Afshartous, David wrote:

Date sent:      	Sat, 9 Sep 2006 13:39:04 -0400
From:           	"Afshartous, David" <afshart at exchange.sba.miami.edu>
To:             	"Deepayan Sarkar" <deepayan.sarkar at gmail.com>
Copies to:      	r-help at stat.math.ethz.ch
Subject:        	Re: [R] augPred plot in nlme library

> 
> Thanks Deepayan and Andrew.
> 
> msMaxIter solved the convergence problem and plot(augPred) works with
> my data when I employ I() in the function call.
> 
> One other strange thing I noticed is that when I take logs of
> dependent variable in the function call, the plot of augPred 
> doesn't graph any prediction line at all.  
> 
> contr=nlmeControl(msMaxIter = 500)
> fm2c <- lme(log(distance) ~ age + I(age^2), data = Orthodont,
> control=contr)
> plot(augPred(fm2c)) 
> 
> However, this is fixed by hard coding the dependent variable:
> 
> log.dist = log(distance)
> fm2c <- lme(log.dist ~ age + I(age^2), data = Orthodont,
> control=contr) plot(augPred(fm2c))
> 
> 
> 
> -----Original Message-----
> From: Deepayan Sarkar [mailto:deepayan.sarkar at gmail.com] 
> Sent: Saturday, September 09, 2006 11:46 AM
> To: Afshartous, David
> Cc: Andrew Robinson; r-help at stat.math.ethz.ch
> Subject: Re: [R] augPred plot in nlme library
> 
> On 9/9/06, Afshartous, David <afshart at exchange.sba.miami.edu> wrote: >
> Hi Andrew, > > Thanks for your email.  I assume you mean age^2 instead
> of age.2 for > fm2a, and for fm2b, I get the following error: > > >
> fm2b <- lme(distance ~ age + I(age^2), data = Orthodont) > Error in
> lme.formula(distance ~ age + I(age^2), data = Orthodont) : >        
> iteration limit reached without convergence (9) > > do you get his
> error as well?
> 
> For me, adding 'control = list(msMaxIter = 500)' worked. I'm writing
> from memory, so the name may not be exactly right, see ?nlmeControl.
> 
> > Finally, the Pixel example on p.42 of Pinheiro & Bates gets the
> > quadratic plot w/o using I() as you do below; is this due to a
> > difference between S and R?
> 
> Yes.
> 
> >
> > thanks!
> > dave
> >
> > ps - sorry for not making the data available; if anyone is
> > interested please let me know and I'll send it directly.
> >
> >
> >
> >
> > -----Original Message-----
> > From: Andrew Robinson [mailto:A.Robinson at ms.unimelb.edu.au]
> > Sent: Friday, September 08, 2006 5:46 PM
> > To: Afshartous, David
> > Cc: Deepayan Sarkar; r-help at stat.math.ethz.ch
> > Subject: Re: [R] augPred plot in nlme library
> >
> > Hi David,
> >
> > this is the sort of thing that Deepayan meant.  Make a dataset
> > available to us, or use one that will be installed by default on R.
> >
> > eg
> >
> > require(nlme)
> > fm1 <- lme(distance ~ age, data = Orthodont)
> > plot(augPred(fm1))
> >
> > #  All linear
> >
> > fm2a <- lme(distance ~ age + age.2, data = Orthodont)
> > plot(augPred(fm2a))
> >
> > # Still linear
> >
> > fm2b <- lme(distance ~ age + I(age^2), data = Orthodont)
> > plot(augPred(fm2b))
> >
> > # Quadratic!
> >
> > I hope that this helps you resolve the problem.
> >
> > Andrew
> >
> >
> >
> > On Fri, Sep 08, 2006 at 05:18:13PM -0400, Afshartous, David wrote:
> > >
> > > Deepayan,
> > >
> > > Thanks for your suggestion.  Here are more details:
> > >
> > > I have a grouped data object for repeated measures data just like
> > > the Pixel grouped data object on p.42 of Pinheiro and Bates
> > > (2000).
> > >
> > > comp.adj.UKV.3 <- groupedData(adj.UKV ~ Time |
> Patient_no/Lisinopril,
> > >       data = comp.adj.UKV.frm, order.groups = F
> > >       #labels = list(x = "Hour", y = "adj.UKV")
> > > )
> > >
> > > i.e., the response is continuous, Time is not treated as a factor,
> > > and
> >
> > > there exists two factors, one nested within the other (Lisinopril
> > > nested
> > >
> > > witin patient, similar to Side within Dog on p.42).
> > >
> > > I also fit a model very similar to their model:
> > >
> > > fm1comp = lme(adj.UKV ~ Time + Time.sq, data = comp.adj.UKV.3,
> > > random = list(Patient_no = ~ 1 , Lisinopril = ~ 1) )
> > >
> > >
> > > However, the command below does not produce the fitted curves from
> > > this model, but rather it seems to be the fitted curves from a
> > > linear model.
> > >
> > > plot(augPred(fm3comp))
> > >
> > > Possibly augPred behaves differently in R than in S, but reading
> > > the
> 
> > > R
> >
> > > help and trying various other approaches has not solved this.
> > >
> > > Thanks!
> > > Dave
> > >
> > >
> > >
> > >
> > > -----Original Message-----
> > > From: Deepayan Sarkar [mailto:deepayan.sarkar at gmail.com]
> > > Sent: Friday, September 08, 2006 4:37 PM
> > > To: Afshartous, David
> > > Cc: r-help at stat.math.ethz.ch
> > > Subject: Re: [R] augPred plot in nlme library
> > >
> > > On 9/8/06, Afshartous, David <afshart at exchange.sba.miami.edu>
> > > wrote:
> > > > All,
> > > >
> > > > I've solved part of the problem below by making sure that the
> > > > formula in the grouped data object is the same as the formula
> > > > specified within
> > >
> > > > lme (this isn't the case in the cited example from Pinheiro &
> > Bates).
> > > >
> > > > However, augPred seems to plot only a linear model instead of
> > > > the polynomial model.  Does anyone know how to make sure that
> > > > augPred plots the same model as that specified in the model (as
> > > > below)?
> > >
> > > You are unlikely to get any helpful answers unless you give us
> > > more information, as every r-help message asks you to do:
> > >
> > > > R-help at stat.math.ethz.ch mailing list 
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide
> > > > http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible
> > > > code.
> > >
> > > -Deepayan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From petr.pikal at precheza.cz  Mon Sep 11 09:01:40 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 11 Sep 2006 09:01:40 +0200
Subject: [R] faster way?
In-Reply-To: <E65F8139-8B7F-48B4-8CC8-79D0E34A325D@gmail.com>
Message-ID: <450525F4.10543.3E2D8A@localhost>

Hi

not knowing dsn do you by chance looking for cumsum?

Or maybe something like this?

> x
 [1]  1  2  3  4  5  6  7  8  9 10
> y
 [1]  2  3  4  5  6  7  8  9 10 11
> x[1:9]+y[2:10]
[1]  4  6  8 10 12 14 16 18 20
>

HTH
Petr



On 10 Sep 2006 at 23:45, Rick Bischoff wrote:

To:             	R-Help <r-help at stat.math.ethz.ch>
From:           	Rick Bischoff <rdbisch at gmail.com>
Date sent:      	Sun, 10 Sep 2006 23:45:38 -0400
Subject:        	[R] faster way?

> Hi,
> 
> Is there a faster way to do this? It takes forever, even on a  
> moderately sized dataset.
> 
> 
> n          <- dim(dsn)[1]
> dsn2 <- dsn[order(-dsn$xhat),]
> dsn2[1, "cumx"] <- dsn2[1, "xhat"]
> 
> for (i in 2:n) {
>  dsn2[i, "cumx"] <- dsn2[i - 1, "cumx"] + dsn2[i, "xhat"]
> }
> 
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From ibanez at bioef.org  Mon Sep 11 09:39:26 2006
From: ibanez at bioef.org (Berta)
Date: Mon, 11 Sep 2006 09:39:26 +0200
Subject: [R] how to construct stripchart with coincident points centered
References: <38929107-8CB9-4C22-827F-952D371B094E@local><D979C34B-0421-4DD5-8491-0CBDE06369B5@local>007b01c6d324$4d55cd70$3d01a8c0@BIOEF.ORG
	<003901c6d34e$f1814ad0$a00c010a@BigBaer>
Message-ID: <003401c6d575$69e31ee0$6601a8c0@BIOEF.ORG>

Thanks Rob for the clue, it is nearly  but not exactly what I am looking 
for.
If we plot the first figure as I want (but not centered) and the second 
figure as you suggest:

 win.graph(); par(mfrow=c(2,1));  y <-rbinom(100, 3, 0.5)
 stripchart(y, method="stack", pch="o", vertical=TRUE); abline(v=1)
 stripchart(y, method="jitter", pch="o", vertical=TRUE,jitter=.5); 
abline(v=1)

we obtain "more or less" a symmetric plot in the second figure but points 
within the same value are not equally distanced as I wish (sorry, I did not 
tell this in the previous message). Is there a method in which I can obtain 
the first figure but with coincident points centered and equally distanced?

Thanks again,

Berta.

> You may be looking for:
> stripchart(y, method="jitter", pch="o", vertical=TRUE,jitter=.5)
>
> Rob

> ----- Original Message ----- 

>> Hi R-users,
>> I am using stripchart with coincident points,
>>
>> y <-rbinom(100, 3, 0.5)
>> stripchart(y, method="stack", pch="o", vertical=TRUE)
>>
>> But the result is not centered in the sense that if a value (say value 0)
> is
>> repeated 7 times, the first point is ploted in the middle and the rest at
>> its right side, in stead of ploting 3 at its right and 3 at its left. 
>> Can
>> anybody help?
>>
>> Thanks,
>> Berta.


From spencer.graves at pdf.com  Sun Sep 10 05:36:38 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 09 Sep 2006 20:36:38 -0700
Subject: [R] Conservative "ANOVA tables" in lmer
In-Reply-To: <40e66e0b0609081500s50cc0e1cq3a899ded57b8e428@mail.gmail.com>
References: <1A8FE5B0-3FE3-4CA9-9224-9CACB9AF4854@muohio.edu>	<40e66e0b0609070559u656463c1l52a60eda9a095237@mail.gmail.com>	<17664.12848.693133.495312@stat.math.ethz.ch>	<x2fyf3g0ua.fsf@viggo.kubism.ku.dk>	<40e66e0b0609070832x28bb5365w6f4bfd8a6a72078a@mail.gmail.com>
	<40e66e0b0609081500s50cc0e1cq3a899ded57b8e428@mail.gmail.com>
Message-ID: <45038846.20002@pdf.com>

      Peter's example and Doug's "different test" reply sent me 
Scheff?'s discussion of the balanced and replicated mixed-effect 2-away 
layout.  As I note below, the obvious F test for the fixed effect does 
not appear to be likelihood ratio for anything. 

Douglas Bates wrote:
> On 9/7/06, Douglas Bates <bates at stat.wisc.edu> wrote:
>   
>> On 07 Sep 2006 17:20:29 +0200, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
>>     
>>> Martin Maechler <maechler at stat.math.ethz.ch> writes:
>>>
>>>       
>>>>>>>>> "DB" == Douglas Bates <bates at stat.wisc.edu>
>>>>>>>>>     on Thu, 7 Sep 2006 07:59:58 -0500 writes:
>>>>>>>>>                   
>>>>     DB> Thanks for your summary, Hank.
>>>>     DB> On 9/7/06, Martin Henry H. Stevens <hstevens at muohio.edu> wrote:
>>>>     >> Dear lmer-ers,
>>>>     >> My thanks for all of you who are sharing your trials and tribulations
>>>>     >> publicly.
>>>>
>>>>     >> I was hoping to elicit some feedback on my thoughts on denominator
>>>>     >> degrees of freedom for F ratios in mixed models. These thoughts and
>>>>     >> practices result from my reading of previous postings by Doug Bates
>>>>     >> and others.
>>>>
>>>>     >> - I start by assuming that the appropriate denominator degrees lies
>>>>     >> between n - p and and n - q, where n=number of observations, p=number
>>>>     >> of fixed effects (rank of model matrix X), and q=rank of Z:X.
>>>>
>>>>     DB> I agree with this but the opinion is by no means universal.  Initially
>>>>     DB> I misread the statement because I usually write the number of columns
>>>>     DB> of Z as q.
>>>>
>>>>     DB> It is not easy to assess rank of Z:X numerically.  In many cases one
>>>>     DB> can reason what it should be from the form of the model but a general
>>>>     DB> procedure to assess the rank of a matrix, especially a sparse matrix,
>>>>     DB> is difficult.
>>>>
>>>>     DB> An alternative which can be easily calculated is n - t where t is the
>>>>     DB> trace of the 'hat matrix'.  The function 'hatTrace' applied to a
>>>>     DB> fitted lmer model evaluates this trace (conditional on the estimates
>>>>     DB> of the relative variances of the random effects).
>>>>
>>>>     >> - I then conclude that good estimates of P values on the F ratios lie
>>>>     >>   between 1 - pf(F.ratio, numDF, n-p) and 1 - pf(F.ratio, numDF, n-q).
>>>>     >>   -- I further surmise that the latter of these (1 - pf(F.ratio, numDF,
>>>>     >>   n-q)) is the more conservative estimate.
>>>>
>>>> This assumes that the true distribution (under H0) of that "F ratio"
>>>> *is*  F_{n1,n2}  for some (possibly non-integer)  n1 and n2.
>>>> But AFAIU, this is only approximately true at best, and AFAIU,
>>>> the quality of this approximation has only been investigated
>>>> empirically for some situations.
>>>> Hence, even your conservative estimate of the P value could be
>>>> wrong (I mean "wrong on the wrong side" instead of just
>>>> "conservatively wrong").  Consequently, such a P-value is only
>>>> ``approximately conservative'' ...
>>>> I agree howevert that in some situations, it might be a very
>>>> useful "descriptive statistic" about the fitted model.
>>>>         
>>> I'm very wary of ANY attempt at guesswork in these matters.
>>>
>>> I may be understanding the post wrongly, but consider this case: Y_ij
>>> = mu + z_i + eps_ij, i = 1..3, j=1..100
>>>
>>> I get rank(X)=1, rank(X:Z)=3,  n=300
>>>
>>> It is well known that the test for mu=0 in this case is obtained by
>>> reducing data to group means, xbar_i, and then do a one-sample t test,
>>> the square of which is F(1, 2), but it seems to be suggested that
>>> F(1, 297) is a conservative test???!
>>>       
>> It's a different test, isn't it?  Your test is based upon the between
>> group sum of squares with 2 df.  I am proposing to use the within
>> group sum of squares or its generalization.
>>     
>
> On closer examination I see that you are indeed correct.  I have heard
> that "well-known" result many times and finally sat down to prove it
> to myself.  For a balanced design the standard error of the intercept
> using the REML estimates is the same as the standard error of the mean
> calculated from the group means.
>
>   
>> data(Rail, package = 'nlme')
>> library(lme4)
>> summary(fm1 <- lmer(travel ~ 1 + (1|Rail), Rail))
>>     
> Linear mixed-effects model fit by REML
> Formula: travel ~ 1 + (1 | Rail)
>    Data: Rail
>    AIC   BIC logLik MLdeviance REMLdeviance
>  126.2 128.0 -61.09      128.6        122.2
> Random effects:
>  Groups   Name        Variance Std.Dev.
>  Rail     (Intercept) 615.286  24.8050
>  Residual              16.167   4.0208
> number of obs: 18, groups: Rail, 6
>
> Fixed effects:
>             Estimate Std. Error t value
> (Intercept)    66.50      10.17   6.538
>   
>> mns <- with(Rail, tapply(travel, Rail, mean)) # group means
>> sd(mns)/sqrt(length(mns))  # standard error matches that from lmer
>>     
> [1] 10.17104
>   
>> t.test(mns)
>>     
>
> 	One Sample t-test
>
> data:  mns
> t = 6.5382, df = 5, p-value = 0.001253
> alternative hypothesis: true mean is not equal to 0
> 95 percent confidence interval:
>  40.35452 92.64548
> sample estimates:
> mean of x
>      66.5
>
>   
>> ctab <- summary(fm1)@coefs  # coefficient table
>> ctab[,1] + c(-1,1) * qt(0.975, 15) * ctab[,2] # 95% conf. int.
>>     
> [1] 44.82139 88.17861
>   
>> ## interval using df = # of obs - rank of [Z:X] is too narrow
>>     
>
> So my proposal of using either the trace of the hat matrix or the rank
> of the combined model matrices as the degrees of freedom for the model
> is not conservative.
>
> However, look at the following
>
>   
>> set.seed(123454321)  # for reproducibility
>> sm1 <- mcmcsamp(fm1, 50000)
>> library(coda)
>> HPDinterval(sm1)
>>     
>                     lower      upper
> (Intercept)     40.470663  92.608514
> log(sigma^2)     2.060179   3.716326
> log(Rail.(In))   5.371858   8.056897
> deviance       128.567329 137.487455
> attr(,"Probability")
> [1] 0.95
>
> The HPD interval calculated from a MCMC sample reproduce the interval
> from the group means almost exactly.  This makes sense in that the
> MCMC sample takes into account the variation in the estimates of the
> variance components, just as defining intervals based on the Student's
> t does.
>
> So for this case where the distribution of the estimate of the mean
> has a known distribution the correct degrees of freedom and the MCMC
> sample produce similar answers.
>
> This gives me more confidence in the results from the MCMC sample in
> general cases.
>
> The problem I have with trying to work out what the degrees of freedom
> "should be" is that the rules seem rather arbitrary.  For example, the
> "between-within" rule used in SAS PROC Mixed is popular (many accept
> it as the "correct" answer) but it assumes that the degrees of freedom
> associated with a random effect grouped by a factor with k levels is
> always k - 1.  This value is used even when there is a random
> intercept and a random slope for each group.  In fact you could have
> an arbitrary number of random effects for each level of the grouping
> factor and it would still apparently only cost you k - 1 degrees of
> freedom.  That doesn't make sense to me.
>
> Anyway, I thank you for pointing out the errors of my ways Peter.
>   
      For the traditional, balanced, replicated, 2-way mixed-effects 
analysis, Scheff? (1959, Table 8.1.1, p. 269) gives the expected mean 
squares for a two-way layout with "I" levels of a fixed effect A, "J" 
levels of a random effect B, and "K" replicates, as follows: 

EMS(A: fixed) = var(e) + K*var(A:B) + J*K*MeanSquareA
EMS(B: random) = var(e) + I*K*var(B)
EMS(A:B; random)=var(e)+K*var(A:B)
EMSE = var(e). 

      In this case, the "obvious" test for A is MS(A: fixed) / MS(A:B, 
random), because this gives us a standard F statistic to test 
MeanSquareA = 0.  However, it doesn't make sense to me to test A without 
simultaneously assuming var(A:B) = 0.  The same argument applies to 
Peter's "simpler" case discussed above:  With "Y_ij = mu + z_i + 
eps_ij", it only rarely makes sense to test mu=0 while assuming var(z) 
!= 0.  In the balanced 2-way, mixed-effects analysis, the Neyman-Pearson 
thing to do, I would think, would be to test simultaneously MeanSquareA 
= 0 with var(A:B) = 0.  In lmer, I might write this as follows: 

      anova(lmer(y~A+(A|B)), lmer(y~1+(1|B)). 

      However, this does NOT match the standard analysis associated with 
this design, does it?  To check this, I considered problem 8.1 in 
Scheff? (p. 289), which compares 3 different nozzles (fixed effect) 
tested by 5 different operators (random effect).  The data are as follows: 

y <- c(6,6,-15,   26,12,5,     11,4,4,   21,14,7, 25,18,25,
       13,6,13,    4,4,11,   17,10,17,   -5,2,-5, 15,8,1,
     10,10,-11, -35,0,-14, 11,-10,-17, 12,-2,-16, -4,10,24)

Nozzle <- data.frame(Nozzle=rep(LETTERS[1:3], e=15),
      Operator=rep(letters[1:5], e=3), flowRate=y)

      The traditional analysis can be obtained from 
anova(lm(flowRate~Nozzle*Operator, ...)), but comparing MeanSq.Nozzle to 
MeanSq.Nozzle:Operator rather than MeanSquareResidual, as follows: 

 > fitAB0 <- lm(flowRate~Nozzle*Operator, data=Nozzle)
 > (aov.AB0 <- anova(fitAB0))
Analysis of Variance Table

Response: flowRate
                Df  Sum Sq Mean Sq F value   Pr(>F)  
Nozzle           2 1426.98  713.49  7.0456 0.003101 **
Operator         4  798.80  199.70  1.9720 0.124304  
Nozzle:Operator  8 1821.47  227.68  2.2484 0.051640 .
Residuals       30 3038.00  101.27                   
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

      Scheff? must have computed the following: 

 > (F.A.Scheffe <- aov.AB0[1, "Mean Sq"]/aov.AB0[3, "Mean Sq"])
[1] 3.133690
 > pf(F.A.Scheffe, 1, 2, lower.tail=FALSE)
[1] 0.2187083

      However, I think I prefer the likelihood ratio answer to this, 
because I think it is better to have an approximate solution to the 
exact problem than an exact solution to the approximate problem.  [I got 
this from someone else like Tukey, but I don't have a citation.])  I can 
get this likelihood ratio answer from either lme or lmer. 

      When I tried to fit this model with 'mle'; it didn't want to 
converge: 

library(nlme)
fitAB. <- lme(flowRate~Nozzle, random=~Nozzle|Operator,
              data=Nozzle, method="ML")
Error in lme.formula(flowRate ~ Nozzle, random = ~Nozzle | Operator, 
data = Nozzle,  :
    nlminb problem, convergence error code = 1; message = iteration 
limit reached without convergence (9)

      After several false starts, I got the following to work: 

fitAB. <- lme(flowRate~Nozzle, random=~Nozzle|Operator,
              data=Nozzle, method="ML",
              control=lmeControl(opt="optim"))

 > anova(fitAB., fitB.)
       Model df      AIC      BIC    logLik   Test  L.Ratio p-value
fitAB.     1 10 361.9022 379.9688 -170.9511                       
fitB.      2  3 361.3637 366.7837 -177.6819 1 vs 2 13.46153  0.0616


      I got essentially the same answer from lmer (without the 
convergence problem, but quitting R in between: 

 > fitAB <- lmer(flowRate~Nozzle+(Nozzle|Operator),
+               data=Nozzle, method="ML")
 > fitB <- lmer(flowRate~1+(1|Operator), data=Nozzle,
+              method="ML")
 > anova(fitAB, fitB)
Data: Nozzle
Models:
fitB: flowRate ~ 1 + (1 | Operator)
fitAB: flowRate ~ Nozzle + (Nozzle | Operator)
      Df     AIC     BIC  logLik  Chisq Chi Df Pr(>Chisq) 
fitB   2  359.36  362.98 -177.68                          
fitAB  9  359.88  376.14 -170.94 13.479      7    0.06126 .
     
      Comments? 
      Spencer Graves
p.s.  For the lme fit: 
 > sessionInfo()
Version 2.3.1 Patched (2006-08-13 r38872)
i386-pc-mingw32

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
[7] "base"    

other attached packages:
    nlme
"3.1-75"
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From stevenmh at muohio.edu  Sun Sep 10 12:54:30 2006
From: stevenmh at muohio.edu (Martin Henry H. Stevens)
Date: Sun, 10 Sep 2006 06:54:30 -0400
Subject: [R] Conservative "ANOVA tables" in lmer
In-Reply-To: <45038846.20002@pdf.com>
References: <1A8FE5B0-3FE3-4CA9-9224-9CACB9AF4854@muohio.edu>	<40e66e0b0609070559u656463c1l52a60eda9a095237@mail.gmail.com>	<17664.12848.693133.495312@stat.math.ethz.ch>	<x2fyf3g0ua.fsf@viggo.kubism.ku.dk>	<40e66e0b0609070832x28bb5365w6f4bfd8a6a72078a@mail.gmail.com>
	<40e66e0b0609081500s50cc0e1cq3a899ded57b8e428@mail.gmail.com>
	<45038846.20002@pdf.com>
Message-ID: <60AD184A-1EFC-4BCB-A49D-CD837235B00B@muohio.edu>

Hi Spencer,
I would like to make sure I understand Spencer's question and  
doubt's, below.
On Sep 9, 2006, at 11:36 PM, Spencer Graves wrote:

>      Peter's example and Doug's "different test" reply sent me  
> Scheff?'s discussion of the balanced and replicated mixed-effect 2- 
> away layout.  As I note below, the obvious F test for the fixed  
> effect does not appear to be likelihood ratio for anything.
> Douglas Bates wrote:
>> On 9/7/06, Douglas Bates <bates at stat.wisc.edu> wrote:
>>
>>> On 07 Sep 2006 17:20:29 +0200, Peter Dalgaard  
>>> <p.dalgaard at biostat.ku.dk> wrote:
>>>
>>>> Martin Maechler <maechler at stat.math.ethz.ch> writes:
>>>>
>>>>
>>>>>>>>>> "DB" == Douglas Bates <bates at stat.wisc.edu>
>>>>>>>>>>     on Thu, 7 Sep 2006 07:59:58 -0500 writes:
>>>>>>>>>>
>>>>>     DB> Thanks for your summary, Hank.
>>>>>     DB> On 9/7/06, Martin Henry H. Stevens  
>>>>> <hstevens at muohio.edu> wrote:
>>>>>     >> Dear lmer-ers,
>>>>>     >> My thanks for all of you who are sharing your trials and  
>>>>> tribulations
>>>>>     >> publicly.
>>>>>
>>>>>     >> I was hoping to elicit some feedback on my thoughts on  
>>>>> denominator
>>>>>     >> degrees of freedom for F ratios in mixed models. These  
>>>>> thoughts and
>>>>>     >> practices result from my reading of previous postings by  
>>>>> Doug Bates
>>>>>     >> and others.
>>>>>
>>>>>     >> - I start by assuming that the appropriate denominator  
>>>>> degrees lies
>>>>>     >> between n - p and and n - q, where n=number of  
>>>>> observations, p=number
>>>>>     >> of fixed effects (rank of model matrix X), and q=rank of  
>>>>> Z:X.
>>>>>
>>>>>     DB> I agree with this but the opinion is by no means  
>>>>> universal.  Initially
>>>>>     DB> I misread the statement because I usually write the  
>>>>> number of columns
>>>>>     DB> of Z as q.
>>>>>
>>>>>     DB> It is not easy to assess rank of Z:X numerically.  In  
>>>>> many cases one
>>>>>     DB> can reason what it should be from the form of the model  
>>>>> but a general
>>>>>     DB> procedure to assess the rank of a matrix, especially a  
>>>>> sparse matrix,
>>>>>     DB> is difficult.
>>>>>
>>>>>     DB> An alternative which can be easily calculated is n - t  
>>>>> where t is the
>>>>>     DB> trace of the 'hat matrix'.  The function 'hatTrace'  
>>>>> applied to a
>>>>>     DB> fitted lmer model evaluates this trace (conditional on  
>>>>> the estimates
>>>>>     DB> of the relative variances of the random effects).
>>>>>
>>>>>     >> - I then conclude that good estimates of P values on the  
>>>>> F ratios lie
>>>>>     >>   between 1 - pf(F.ratio, numDF, n-p) and 1 - pf 
>>>>> (F.ratio, numDF, n-q).
>>>>>     >>   -- I further surmise that the latter of these (1 - pf 
>>>>> (F.ratio, numDF,
>>>>>     >>   n-q)) is the more conservative estimate.
>>>>>
>>>>> This assumes that the true distribution (under H0) of that "F  
>>>>> ratio"
>>>>> *is*  F_{n1,n2}  for some (possibly non-integer)  n1 and n2.
>>>>> But AFAIU, this is only approximately true at best, and AFAIU,
>>>>> the quality of this approximation has only been investigated
>>>>> empirically for some situations.
>>>>> Hence, even your conservative estimate of the P value could be
>>>>> wrong (I mean "wrong on the wrong side" instead of just
>>>>> "conservatively wrong").  Consequently, such a P-value is only
>>>>> ``approximately conservative'' ...
>>>>> I agree howevert that in some situations, it might be a very
>>>>> useful "descriptive statistic" about the fitted model.
>>>>>
>>>> I'm very wary of ANY attempt at guesswork in these matters.
>>>>
>>>> I may be understanding the post wrongly, but consider this case:  
>>>> Y_ij
>>>> = mu + z_i + eps_ij, i = 1..3, j=1..100
>>>>
>>>> I get rank(X)=1, rank(X:Z)=3,  n=300
>>>>
>>>> It is well known that the test for mu=0 in this case is obtained by
>>>> reducing data to group means, xbar_i, and then do a one-sample t  
>>>> test,
>>>> the square of which is F(1, 2), but it seems to be suggested that
>>>> F(1, 297) is a conservative test???!
>>>>
>>> It's a different test, isn't it?  Your test is based upon the  
>>> between
>>> group sum of squares with 2 df.  I am proposing to use the within
>>> group sum of squares or its generalization.
>>>
>>
>> On closer examination I see that you are indeed correct.  I have  
>> heard
>> that "well-known" result many times and finally sat down to prove it
>> to myself.  For a balanced design the standard error of the intercept
>> using the REML estimates is the same as the standard error of the  
>> mean
>> calculated from the group means.
>>
>>
>>> data(Rail, package = 'nlme')
>>> library(lme4)
>>> summary(fm1 <- lmer(travel ~ 1 + (1|Rail), Rail))
>>>
>> Linear mixed-effects model fit by REML
>> Formula: travel ~ 1 + (1 | Rail)
>>    Data: Rail
>>    AIC   BIC logLik MLdeviance REMLdeviance
>>  126.2 128.0 -61.09      128.6        122.2
>> Random effects:
>>  Groups   Name        Variance Std.Dev.
>>  Rail     (Intercept) 615.286  24.8050
>>  Residual              16.167   4.0208
>> number of obs: 18, groups: Rail, 6
>>
>> Fixed effects:
>>             Estimate Std. Error t value
>> (Intercept)    66.50      10.17   6.538
>>
>>> mns <- with(Rail, tapply(travel, Rail, mean)) # group means
>>> sd(mns)/sqrt(length(mns))  # standard error matches that from lmer
>>>
>> [1] 10.17104
>>
>>> t.test(mns)
>>>
>>
>> 	One Sample t-test
>>
>> data:  mns
>> t = 6.5382, df = 5, p-value = 0.001253
>> alternative hypothesis: true mean is not equal to 0
>> 95 percent confidence interval:
>>  40.35452 92.64548
>> sample estimates:
>> mean of x
>>      66.5
>>
>>
>>> ctab <- summary(fm1)@coefs  # coefficient table
>>> ctab[,1] + c(-1,1) * qt(0.975, 15) * ctab[,2] # 95% conf. int.
>>>
>> [1] 44.82139 88.17861
>>
>>> ## interval using df = # of obs - rank of [Z:X] is too narrow
>>>
>>
>> So my proposal of using either the trace of the hat matrix or the  
>> rank
>> of the combined model matrices as the degrees of freedom for the  
>> model
>> is not conservative.
>>
>> However, look at the following
>>
>>
>>> set.seed(123454321)  # for reproducibility
>>> sm1 <- mcmcsamp(fm1, 50000)
>>> library(coda)
>>> HPDinterval(sm1)
>>>
>>                     lower      upper
>> (Intercept)     40.470663  92.608514
>> log(sigma^2)     2.060179   3.716326
>> log(Rail.(In))   5.371858   8.056897
>> deviance       128.567329 137.487455
>> attr(,"Probability")
>> [1] 0.95
>>
>> The HPD interval calculated from a MCMC sample reproduce the interval
>> from the group means almost exactly.  This makes sense in that the
>> MCMC sample takes into account the variation in the estimates of the
>> variance components, just as defining intervals based on the  
>> Student's
>> t does.
>>
>> So for this case where the distribution of the estimate of the mean
>> has a known distribution the correct degrees of freedom and the MCMC
>> sample produce similar answers.
>>
>> This gives me more confidence in the results from the MCMC sample in
>> general cases.
>>
>> The problem I have with trying to work out what the degrees of  
>> freedom
>> "should be" is that the rules seem rather arbitrary.  For example,  
>> the
>> "between-within" rule used in SAS PROC Mixed is popular (many accept
>> it as the "correct" answer) but it assumes that the degrees of  
>> freedom
>> associated with a random effect grouped by a factor with k levels is
>> always k - 1.  This value is used even when there is a random
>> intercept and a random slope for each group.  In fact you could have
>> an arbitrary number of random effects for each level of the grouping
>> factor and it would still apparently only cost you k - 1 degrees of
>> freedom.  That doesn't make sense to me.
>>
>> Anyway, I thank you for pointing out the errors of my ways Peter.
>>
>      For the traditional, balanced, replicated, 2-way mixed-effects  
> analysis, Scheff? (1959, Table 8.1.1, p. 269) gives the expected  
> mean squares for a two-way layout with "I" levels of a fixed effect  
> A, "J" levels of a random effect B, and "K" replicates, as follows:
> EMS(A: fixed) = var(e) + K*var(A:B) + J*K*MeanSquareA
> EMS(B: random) = var(e) + I*K*var(B)
> EMS(A:B; random)=var(e)+K*var(A:B)
> EMSE = var(e).
>      In this case, the "obvious" test for A is MS(A: fixed) / MS 
> (A:B, random), because this gives us a standard F statistic to test  
> MeanSquareA = 0.  However, it doesn't make sense to me to test A  
> without simultaneously assuming var(A:B) = 0.

Does one want to test whether var(A:B)=0 because the F-test assumes  
it? That is, that as var(A:B) increases, the var ratio MS{A:fixed)/MS 
(A:B, random) decreases, artifactually reducing the "significance" of  
J:K:MeanSquareA?


> The same argument applies to Peter's "simpler" case discussed  
> above:  With "Y_ij = mu + z_i + eps_ij", it only rarely makes sense  
> to test mu=0 while assuming var(z) != 0.  In the balanced 2-way,  
> mixed-effects analysis, the Neyman-Pearson thing to do, I would  
> think, would be to test simultaneously MeanSquareA = 0 with var 
> (A:B) = 0.  In lmer, I might write this as follows:
>      anova(lmer(y~A+(A|B)), lmer(y~1+(1|B)).
>      However, this does NOT match the standard analysis associated  
> with this design, does it?  To check this, I considered problem 8.1  
> in Scheff? (p. 289), which compares 3 different nozzles (fixed  
> effect) tested by 5 different operators (random effect).  The data  
> are as follows:
> y <- c(6,6,-15,   26,12,5,     11,4,4,   21,14,7, 25,18,25,
>       13,6,13,    4,4,11,   17,10,17,   -5,2,-5, 15,8,1,
>     10,10,-11, -35,0,-14, 11,-10,-17, 12,-2,-16, -4,10,24)
>
> Nozzle <- data.frame(Nozzle=rep(LETTERS[1:3], e=15),
>      Operator=rep(letters[1:5], e=3), flowRate=y)
>
>      The traditional analysis can be obtained from anova(lm 
> (flowRate~Nozzle*Operator, ...)), but comparing MeanSq.Nozzle to  
> MeanSq.Nozzle:Operator rather than MeanSquareResidual, as follows:
> > fitAB0 <- lm(flowRate~Nozzle*Operator, data=Nozzle)
> > (aov.AB0 <- anova(fitAB0))
> Analysis of Variance Table
>
> Response: flowRate
>                Df  Sum Sq Mean Sq F value   Pr(>F)   
> Nozzle           2 1426.98  713.49  7.0456 0.003101 **
> Operator         4  798.80  199.70  1.9720 0.124304   
> Nozzle:Operator  8 1821.47  227.68  2.2484 0.051640 .
> Residuals       30 3038.00  101.27                   ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>
>      Scheff? must have computed the following:
> > (F.A.Scheffe <- aov.AB0[1, "Mean Sq"]/aov.AB0[3, "Mean Sq"])
> [1] 3.133690
> > pf(F.A.Scheffe, 1, 2, lower.tail=FALSE)
> [1] 0.2187083
>
>      However, I think I prefer the likelihood ratio answer to this,  
> because I think it is better to have an approximate solution to the  
> exact problem than an exact solution to the approximate problem.   
> [I got this from someone else like Tukey, but I don't have a  
> citation.])  I can get this likelihood ratio answer from either lme  
> or lmer.
>      When I tried to fit this model with 'mle'; it didn't want to  
> converge:
> library(nlme)
> fitAB. <- lme(flowRate~Nozzle, random=~Nozzle|Operator,
>              data=Nozzle, method="ML")
> Error in lme.formula(flowRate ~ Nozzle, random = ~Nozzle |  
> Operator, data = Nozzle,  :
>    nlminb problem, convergence error code = 1; message = iteration  
> limit reached without convergence (9)
>
>      After several false starts, I got the following to work:
> fitAB. <- lme(flowRate~Nozzle, random=~Nozzle|Operator,
>              data=Nozzle, method="ML",
>              control=lmeControl(opt="optim"))
>
> > anova(fitAB., fitB.)
>       Model df      AIC      BIC    logLik   Test  L.Ratio p-value
> fitAB.     1 10 361.9022 379.9688 -170.9511                        
> fitB.      2  3 361.3637 366.7837 -177.6819 1 vs 2 13.46153  0.0616
>
>
>      I got essentially the same answer from lmer (without the  
> convergence problem, but quitting R in between:
> > fitAB <- lmer(flowRate~Nozzle+(Nozzle|Operator),
> +               data=Nozzle, method="ML")
> > fitB <- lmer(flowRate~1+(1|Operator), data=Nozzle,
> +              method="ML")
> > anova(fitAB, fitB)
> Data: Nozzle
> Models:
> fitB: flowRate ~ 1 + (1 | Operator)
> fitAB: flowRate ~ Nozzle + (Nozzle | Operator)
>      Df     AIC     BIC  logLik  Chisq Chi Df Pr(>Chisq) fitB   2   
> 359.36  362.98 -177.68                          fitAB  9  359.88   
> 376.14 -170.94 13.479      7    0.06126 .
>          Comments?      Spencer Graves
> p.s.  For the lme fit: > sessionInfo()
> Version 2.3.1 Patched (2006-08-13 r38872)
> i386-pc-mingw32
>
> attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"      
> "datasets"
> [7] "base"
> other attached packages:
>    nlme
> "3.1-75"
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting- 
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>


From spencer.graves at pdf.com  Sun Sep 10 14:06:47 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 10 Sep 2006 14:06:47 +0200
Subject: [R] Conservative "ANOVA tables" in lmer
In-Reply-To: <60AD184A-1EFC-4BCB-A49D-CD837235B00B@muohio.edu>
References: <1A8FE5B0-3FE3-4CA9-9224-9CACB9AF4854@muohio.edu>	<40e66e0b0609070559u656463c1l52a60eda9a095237@mail.gmail.com>	<17664.12848.693133.495312@stat.math.ethz.ch>	<x2fyf3g0ua.fsf@viggo.kubism.ku.dk>	<40e66e0b0609070832x28bb5365w6f4bfd8a6a72078a@mail.gmail.com>
	<40e66e0b0609081500s50cc0e1cq3a899ded57b8e428@mail.gmail.com>
	<45038846.20002@pdf.com>
	<60AD184A-1EFC-4BCB-A49D-CD837235B00B@muohio.edu>
Message-ID: <4503FFD7.7010301@pdf.com>



Martin Henry H. Stevens wrote:
> Hi Spencer,
> I would like to make sure I understand Spencer's question and doubt's, 
> below.
> On Sep 9, 2006, at 11:36 PM, Spencer Graves wrote:
>
>>      Peter's example and Doug's "different test" reply sent me 
>> Scheff?'s discussion of the balanced and replicated mixed-effect 
>> 2-away layout.  As I note below, the obvious F test for the fixed 
>> effect does not appear to be likelihood ratio for anything.
>> Douglas Bates wrote:
>>> On 9/7/06, Douglas Bates <bates at stat.wisc.edu> wrote:
>>>
>>>> On 07 Sep 2006 17:20:29 +0200, Peter Dalgaard 
>>>> <p.dalgaard at biostat.ku.dk> wrote:
<snip>

>>>>> I'm very wary of ANY attempt at guesswork in these matters.
>>>>>
>>>>> I may be understanding the post wrongly, but consider this case: Y_ij
>>>>> = mu + z_i + eps_ij, i = 1..3, j=1..100
>>>>>
>>>>> I get rank(X)=1, rank(X:Z)=3,  n=300
>>>>>
>>>>> It is well known that the test for mu=0 in this case is obtained by
>>>>> reducing data to group means, xbar_i, and then do a one-sample t 
>>>>> test,
>>>>> the square of which is F(1, 2), but it seems to be suggested that
>>>>> F(1, 297) is a conservative test???!
>>>>>
>>>> It's a different test, isn't it?  Your test is based upon the between
>>>> group sum of squares with 2 df.  I am proposing to use the within
>>>> group sum of squares or its generalization.
<snip>
>>      For the traditional, balanced, replicated, 2-way mixed-effects 
>> analysis, Scheff? (1959, Table 8.1.1, p. 269) gives the expected mean 
>> squares for a two-way layout with "I" levels of a fixed effect A, "J" 
>> levels of a random effect B, and "K" replicates, as follows:
>> EMS(A: fixed) = var(e) + K*var(A:B) + J*K*MeanSquareA
>> EMS(B: random) = var(e) + I*K*var(B)
>> EMS(A:B; random)=var(e)+K*var(A:B)
>> EMSE = var(e).
>>      In this case, the "obvious" test for A is MS(A: fixed) / MS(A:B, 
>> random), because this gives us a standard F statistic to test 
>> MeanSquareA = 0.  However, it doesn't make sense to me to test A 
>> without simultaneously assuming var(A:B) = 0.
>
> Does one want to test whether var(A:B)=0 because the F-test assumes 
> it? That is, that as var(A:B) increases, the var ratio 
> MS{A:fixed)/MS(A:B, random) decreases, artifactually reducing the 
> "significance" of J:K:MeanSquareA?

SG:  Scheffe recommends testing var(A:B)=0 using MS(A:B)/MSE, which 
follows an F distribution under both null and alternative hypotheses 
(but scaled differently under the alternative).  This is a monotonic 
transformation of the standard likelihood ratio test, and makes sense to 
me. 

SG:  Scheffe's recommended test for MeanSquareA = 0 is MSA/MS(A:B).  I 
haven't worked out the details, but it looks like this assumes that the 
A:B interaction may be real without an A effect, and this violates a 
basic principle of hierarchy, I think.  There are situations where an 
A:B interaction can exist without the main effect, but that rests on a 
particular parameterization, and we have not assumed that in this context.

SG:  I think this is an argument for using likelihood ratio over 
Scheffe's traditional answer.  Similar to Doug's comment about different 
tests, we should be clear about what each procedure tests, and then 
select the procedure that seems to be the closest to the problem we 
really want to solve, rather than simply relying on a test statistic 
whose distribution is better understood.  I'm not criticizing Scheffe:  
He didn't have the computer tools available in 1959 that we have today. 

>> The same argument applies to Peter's "simpler" case discussed above:  
>> With "Y_ij = mu + z_i + eps_ij", it only rarely makes sense to test 
>> mu=0 while assuming var(z) != 0.  In the balanced 2-way, 
>> mixed-effects analysis, the Neyman-Pearson thing to do, I would 
>> think, would be to test simultaneously MeanSquareA = 0 with var(A:B) 
>> = 0.  In lmer, I might write this as follows:
>>      anova(lmer(y~A+(A|B)), lmer(y~1+(1|B)).
>>      However, this does NOT match the standard analysis associated 
>> with this design, does it?  To check this, I considered problem 8.1 
>> in Scheff? (p. 289), which compares 3 different nozzles (fixed 
>> effect) tested by 5 different operators (random effect).  The data 
>> are as follows:
>> y <- c(6,6,-15,   26,12,5,     11,4,4,   21,14,7, 25,18,25,
>>       13,6,13,    4,4,11,   17,10,17,   -5,2,-5, 15,8,1,
>>     10,10,-11, -35,0,-14, 11,-10,-17, 12,-2,-16, -4,10,24)
>>
>> Nozzle <- data.frame(Nozzle=rep(LETTERS[1:3], e=15),
>>      Operator=rep(letters[1:5], e=3), flowRate=y)
>>
>>      The traditional analysis can be obtained from 
>> anova(lm(flowRate~Nozzle*Operator, ...)), but comparing MeanSq.Nozzle 
>> to MeanSq.Nozzle:Operator rather than MeanSquareResidual, as follows:
>> > fitAB0 <- lm(flowRate~Nozzle*Operator, data=Nozzle)
>> > (aov.AB0 <- anova(fitAB0))
>> Analysis of Variance Table
>>
>> Response: flowRate
>>                Df  Sum Sq Mean Sq F value   Pr(>F)  Nozzle           
>> 2 1426.98  713.49  7.0456 0.003101 **
>> Operator         4  798.80  199.70  1.9720 0.124304  Nozzle:Operator  
>> 8 1821.47  227.68  2.2484 0.051640 .
>> Residuals       30 3038.00  101.27                   ---
>> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>>
>>      Scheff? must have computed the following:
>> > (F.A.Scheffe <- aov.AB0[1, "Mean Sq"]/aov.AB0[3, "Mean Sq"])
>> [1] 3.133690
>> > pf(F.A.Scheffe, 1, 2, lower.tail=FALSE)
>> [1] 0.2187083
>>
>>      However, I think I prefer the likelihood ratio answer to this, 
>> because I think it is better to have an approximate solution to the 
>> exact problem than an exact solution to the approximate problem.  [I 
>> got this from someone else like Tukey, but I don't have a 
>> citation.])  I can get this likelihood ratio answer from either lme 
>> or lmer.
>>      When I tried to fit this model with 'mle'; it didn't want to 
>> converge:
>> library(nlme)
>> fitAB. <- lme(flowRate~Nozzle, random=~Nozzle|Operator,
>>              data=Nozzle, method="ML")
>> Error in lme.formula(flowRate ~ Nozzle, random = ~Nozzle | Operator, 
>> data = Nozzle,  :
>>    nlminb problem, convergence error code = 1; message = iteration 
>> limit reached without convergence (9)
>>
>>      After several false starts, I got the following to work:
>> fitAB. <- lme(flowRate~Nozzle, random=~Nozzle|Operator,
>>              data=Nozzle, method="ML",
>>              control=lmeControl(opt="optim"))
>>
>> > anova(fitAB., fitB.)
>>       Model df      AIC      BIC    logLik   Test  L.Ratio p-value
>> fitAB.     1 10 361.9022 379.9688 -170.9511                       
>> fitB.      2  3 361.3637 366.7837 -177.6819 1 vs 2 13.46153  0.0616
>>
>>
>>      I got essentially the same answer from lmer (without the 
>> convergence problem, but quitting R in between:
>> > fitAB <- lmer(flowRate~Nozzle+(Nozzle|Operator),
>> +               data=Nozzle, method="ML")
>> > fitB <- lmer(flowRate~1+(1|Operator), data=Nozzle,
>> +              method="ML")
>> > anova(fitAB, fitB)
>> Data: Nozzle
>> Models:
>> fitB: flowRate ~ 1 + (1 | Operator)
>> fitAB: flowRate ~ Nozzle + (Nozzle | Operator)
>>      Df     AIC     BIC  logLik  Chisq Chi Df Pr(>Chisq) fitB   2  
>> 359.36  362.98 -177.68                          fitAB  9  359.88  
>> 376.14 -170.94 13.479      7    0.06126 .
>>          Comments?      Spencer Graves
>> p.s.  For the lme fit: > sessionInfo()
>> Version 2.3.1 Patched (2006-08-13 r38872)
>> i386-pc-mingw32
>>
>> attached base packages:
>> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"     
>> "datasets"
>> [7] "base"
>> other attached packages:
>>    nlme
>> "3.1-75"
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide 
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>


From gregor.gorjanc at bfro.uni-lj.si  Mon Sep 11 09:57:53 2006
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Mon, 11 Sep 2006 07:57:53 +0000 (UTC)
Subject: [R] Matrix multiplication using apply() or lappy() ?
References: <Pine.LNX.4.64.0609061825580.11591@gannet.stats.ox.ac.uk>
	<SEWINEXCH00gkJXrRjb000000ef@sewinexch00.insightful.com>
	<xmrd5a6rvjt.fsf@mralx2.rsma.frb.gov>
Message-ID: <loom.20060911T095319-593@post.gmane.org>

Jeffrey J. Hallman <jhallman <at> frb.gov> writes:
> Tim Hesterberg <timh <at> insightful.com> writes:
> 
> > toby_marks <at> americancentury.com asked:
> > >I am trying to divide the columns of a matrix by the first row in the 
> > >matrix.
> > 
> > Dividing columns of a matrix by a vector is a pretty fundamental
> > operation, and the query resulted in a large number of suggestions:
> > 
> > It is unsatisfactory when such a fundamental operation is
> > done in so many different ways.  
> > * It makes it hard to read other people's code.  
> > * Some of these are very inefficient.
>
> But since you have no way to force people to use your new 'standard'
> operators, people can and will still use any of those myriad ways you decry to
> do the same thing.  It's part of the price you pay for working with a flexible
> system. 
> 
> Besides, nothing will ever make it easy to read other people's code. 

Reading others code is never an easy task, but there are huge differences
between various "authors" and I agree with Tim, that it is nice to have some
standardized operators for common/fundamental operations. If there are standard
operators, people will use it more an more.


From gregor.gorjanc at bfro.uni-lj.si  Mon Sep 11 10:20:21 2006
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Mon, 11 Sep 2006 10:20:21 +0200
Subject: [R] Test internet presence
Message-ID: <45051C45.1000909@bfro.uni-lj.si>

Hello useRs!

I have a small package and I need internet access for examples. Of
course it works fine when I have internet access, but not otherwise. I
remember I saw a way to test availability of internet access, but I can
not find it now in archives. Can anyone here help me?

Thanks!

-- 
Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana     PhD student
Biotechnical Faculty
Zootechnical Department     URI: http://www.bfro.uni-lj.si/MR/ggorjan
Groblje 3                   mail: gregor.gorjanc <at> bfro.uni-lj.si

SI-1230 Domzale             tel: +386 (0)1 72 17 861
Slovenia, Europe            fax: +386 (0)1 72 17 888

----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.


From Andreas.Moeltner at med.uni-heidelberg.de  Mon Sep 11 10:49:26 2006
From: Andreas.Moeltner at med.uni-heidelberg.de (Moeltner, Andreas)
Date: Mon, 11 Sep 2006 10:49:26 +0200
Subject: [R] function changes argument
Message-ID: <B8EFAF3B18CD4A459A06022B3DEF117852567F@CEX20.ads.krz.uni-heidelberg.de>

Dear R-list,

the following function f changes L. I thought, assignments within
functions are only local?


f<-function(LL)
{ for (ll in LL)
  { ll$txt<-"changed in f"
  }
}

l<-list(txt="original value")
L<-list(l)
L[[1]]$txt 
f(L)
L[[1]]$txt


gives (using R 2.3.1):

...
> L[[1]]$txt
[1] "original value"
> f(L)
> L[[1]]$txt
[1] "changed in f"
> 

Thanks in advance

Andreas


From r.hankin at noc.soton.ac.uk  Mon Sep 11 10:52:03 2006
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Mon, 11 Sep 2006 09:52:03 +0100
Subject: [R] exactly representable numbers
Message-ID: <71115B0F-CF1B-4A63-9A58-106947D19F69@soc.soton.ac.uk>

Hi

Given a real number x,  I want to know how accurately R can represent  
numbers near x.

In particular, I want to know the infinum of exactly representable
numbers greater than x, and the supremum of exactly representable  
numbers
less than x.  And then the interesting thing is the difference  
between these two.


I have a little function that does some of this:


f <- function(x,FAC=1.1){
   delta <- x
while(x+delta > x){
   delta <- delta/FAC
}
return(delta*FAC)
}

But this can't be optimal.

Is there a better way?



--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From mrolland at grignon.inra.fr  Mon Sep 11 11:27:39 2006
From: mrolland at grignon.inra.fr (=?ISO-8859-1?Q?Marie-No=EBlle_Rolland?=)
Date: Mon, 11 Sep 2006 11:27:39 +0200
Subject: [R] Export data
Message-ID: <45052C0B.1010108@grignon.inra.fr>

Hi there,

I would like to write a data output. In first time, a data frame (called 
"res") is written in a file *csv such as:

A    B    C    # names of data
1.5  4.5  7    # values

So I use "write.table" which prints "res" to a file "file.csv"
write.table(res,file="file.csv",sep="",row.names=TRUE,col.names=TRUE,eol 
= "\n")

After, I would like to append other data to the same file,

A    B    C
1.5  4.5  7
3    6    9   # appended data

I use then the same function but with different arguments:
write.table(res,file="file.csv",append=TRUE,sep="",row.names=TRUE,col.names=FALSE,eol 
= "\n")

Actually, it doesn't work, the file is overwritten by the new one.

I got something like that:
A    B    C
3    6    9

Coul you help me? Thank you in advance

Kind regards,
Marie-No?lle


From bren at juanantonio.info  Mon Sep 11 11:44:38 2006
From: bren at juanantonio.info (=?UTF-8?Q?Juan_Antonio_Bre=C3=B1a_Moral?=)
Date: Mon, 11 Sep 2006 02:44:38 -0700 (PDT)
Subject: [R] Search for best ARIMA model
In-Reply-To: <7260411F9C32E74A86AD9567E64C3301D2FBD0@LI-HAWK.hag.hilti.com>
References: <7260411F9C32E74A86AD9567E64C3301D2FBD0@LI-HAWK.hag.hilti.com>
Message-ID: <6243692.post@talk.nabble.com>


Hi, to create best ARIMA model, I use the package forecast.

You can search it in the following URL:

http://www-personal.buseco.monash.edu.au/~hyndman/Rlibrary/forecast/

You should use the method "best.arima"

further information, here:
http://www-personal.buseco.monash.edu.au/~hyndman/Rlibrary/forecast/forecast/best.arima.html

Best Regards.


Schweitzer, Markus wrote:
> 
> Hello,
> 
> I have a several time series, which I would like to check for their best
> fitted Arima model (I am checking for the lowest aic value).
> Which lets me raise two questions:
> 
> 1) is there are more efficient way, than using 6 for-loops?
> 2) sometimes the system cannot calculate  with given parameters - is
> there a more efficient solution than I found?
> 
> I hope, you can help me to make this calculation quicker since I have to
> run this function 450 times...
> Thank you very much in advance,
> 
> Markus
> 
> 
> arima.estim <- function(TS) {
> 	best.model <- arima(TS, order = c(1, 0, 0), seasonal =
> list(order = c(0, 0, 0), period = frequency(TS)) )
> 
> # Start value
> # I continue with brute force- p, q, r, s are nested from 0 to 3 and i
> and j are nested from 0 to 2. p and  q are not both allowed to be 0.
> 
> for (p in 0:3){
>   for( q in 0:3){
>     if(p==0 && q==0) {}
>       else {
>         for(r in 0:3) {
>           for(s in 0:3) {
>             for (j in 0:2) {
>               for(i in 0:2) {
>       
> # test, if series works
>     if(inherits(try(arima(TS, order = c(p, i, q), seasonal = list(order
> = c(r,  j, s), period = frequency(TS)) ), TRUE), 'try-error')){
>         
> 	print(c(p,i,q))} #shows, which parameters didn't work -> will be
> removed by
>        
> 	 else    {
> 	         tmp <- arima(TS, order = c(p, i, q), seasonal =
> list(order = c(r,  j, s), period = frequency(TS)))     # calculate again
> :(
> 
>           if(best.model$aic > tmp$aic)
>           {
>                   best.model <- tmp
>           }
>               }
>                  }
>                  }    
>                  }
>                  } } } } 
>                  
>                  best.model}
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help en stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/Search-for-best-ARIMA-model-tf2156910.html#a6243692
Sent from the R help forum at Nabble.com.


From jacques.veslot at good.ibl.fr  Mon Sep 11 11:45:27 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Mon, 11 Sep 2006 11:45:27 +0200
Subject: [R] Export data
In-Reply-To: <45052C0B.1010108@grignon.inra.fr>
References: <45052C0B.1010108@grignon.inra.fr>
Message-ID: <45053037.6010107@good.ibl.fr>

 > res <- as.data.frame(t(c(A=1,B=2,C=3)))
 > res
   A B C
1 1 2 3

 > write.table(res, file="file.csv", sep="\t", row.names=F, col.names=T)
 > write.table(t(4:6), append=T, file="file.csv", sep="\t", row.names=F, col.names=F)

 > read.csv("file.csv", sep="\t")
   A B C
1 1 2 3
2 4 5 6

-------------------------------------------------------------------
Jacques VESLOT

CNRS UMR 8090
I.B.L (2?me ?tage)
1 rue du Professeur Calmette
B.P. 245
59019 Lille Cedex

Tel : 33 (0)3.20.87.10.44
Fax : 33 (0)3.20.87.10.31

http://www-good.ibl.fr
-------------------------------------------------------------------


Marie-No?lle Rolland a ?crit :
> Hi there,
> 
> I would like to write a data output. In first time, a data frame (called 
> "res") is written in a file *csv such as:
> 
> A    B    C    # names of data
> 1.5  4.5  7    # values
> 
> So I use "write.table" which prints "res" to a file "file.csv"
> write.table(res,file="file.csv",sep="",row.names=TRUE,col.names=TRUE,eol 
> = "\n")
> 
> After, I would like to append other data to the same file,
> 
> A    B    C
> 1.5  4.5  7
> 3    6    9   # appended data
> 
> I use then the same function but with different arguments:
> write.table(res,file="file.csv",append=TRUE,sep="",row.names=TRUE,col.names=FALSE,eol 
> = "\n")
> 
> Actually, it doesn't work, the file is overwritten by the new one.
> 
> I got something like that:
> A    B    C
> 3    6    9
> 
> Coul you help me? Thank you in advance
> 
> Kind regards,
> Marie-No?lle
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From sundar.dorai-raj at pdf.com  Mon Sep 11 11:50:15 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Mon, 11 Sep 2006 04:50:15 -0500
Subject: [R] exactly representable numbers
In-Reply-To: <71115B0F-CF1B-4A63-9A58-106947D19F69@soc.soton.ac.uk>
References: <71115B0F-CF1B-4A63-9A58-106947D19F69@soc.soton.ac.uk>
Message-ID: <45053157.3060107@pdf.com>



Robin Hankin said the following on 9/11/2006 3:52 AM:
> Hi
> 
> Given a real number x,  I want to know how accurately R can represent  
> numbers near x.
> 
> In particular, I want to know the infinum of exactly representable
> numbers greater than x, and the supremum of exactly representable  
> numbers
> less than x.  And then the interesting thing is the difference  
> between these two.
> 
> 
> I have a little function that does some of this:
> 
> 
> f <- function(x,FAC=1.1){
>    delta <- x
> while(x+delta > x){
>    delta <- delta/FAC
> }
> return(delta*FAC)
> }
> 
> But this can't be optimal.
> 
> Is there a better way?
> 
> 
> 

I believe this is what .Machine$double.eps is. From ?.Machine

double.eps: the smallest positive floating-point number 'x' such that
           '1 + x != 1'.  It equals 'base^ulp.digits' if either 'base'
           is 2 or 'rounding' is 0;  otherwise, it is '(base^ulp.digits)
           / 2'.

See also .Machine$double.neg.eps. Is this what you need?

--sundar


From mothsailor at googlemail.com  Mon Sep 11 11:52:54 2006
From: mothsailor at googlemail.com (David Barron)
Date: Mon, 11 Sep 2006 10:52:54 +0100
Subject: [R] Export data
In-Reply-To: <45052C0B.1010108@grignon.inra.fr>
References: <45052C0B.1010108@grignon.inra.fr>
Message-ID: <815b70590609110252q61000b65o810f77d5c0cdb321@mail.gmail.com>

Difficult to say exactly why it didn't work without knowing exactly
what you did.  The following works for me:

> res<-data.frame(A=1.5,B=4.5,C=7)
 > write.table(res,file="file.csv",sep="
",row.names=TRUE,col.names=TRUE,eol = "\n")
 > res2<-data.frame(3,6,9)
 > write.table(res2,file="file.csv",append=TRUE,sep="
",row.names=TRUE,col.names=FALSE,eol="\n")

By the way, the extension .csv is usually used for comma delimited format.

On 11/09/06, Marie-No?lle Rolland <mrolland at grignon.inra.fr> wrote:
> Hi there,
>
> I would like to write a data output. In first time, a data frame (called
> "res") is written in a file *csv such as:
>
> A    B    C    # names of data
> 1.5  4.5  7    # values
>
> So I use "write.table" which prints "res" to a file "file.csv"
> write.table(res,file="file.csv",sep="",row.names=TRUE,col.names=TRUE,eol
> = "\n")
>
> After, I would like to append other data to the same file,
>
> A    B    C
> 1.5  4.5  7
> 3    6    9   # appended data
>
> I use then the same function but with different arguments:
> write.table(res,file="file.csv",append=TRUE,sep="",row.names=TRUE,col.names=FALSE,eol
> = "\n")
>
> Actually, it doesn't work, the file is overwritten by the new one.
>
> I got something like that:
> A    B    C
> 3    6    9
>
> Coul you help me? Thank you in advance
>
> Kind regards,
> Marie-No?lle
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From wangtong at usc.edu  Mon Sep 11 11:57:04 2006
From: wangtong at usc.edu (Tong Wang)
Date: Mon, 11 Sep 2006 02:57:04 -0700
Subject: [R] Question about do.call()
Message-ID: <dceeecb1a777.4504d080@usc.edu>

Hi,
    I'm writing a function f()  which includes probability density function computation,  I would like to make this generic, that is , I will specify  the density when I call f(),   here's what I wrote:  do.call(den, args=list(d, arg)) 
    the question is , say, if I let den=dnorm, I have to replace arg with  mean=??, sd=??, etc.  but I tried arg=c(mean=..,sd=..)
and arg="mean=..,sd=..",  neither of these worked.   What is the right way to do this ?

Thanks a lot

tong


From mothsailor at googlemail.com  Mon Sep 11 12:07:10 2006
From: mothsailor at googlemail.com (David Barron)
Date: Mon, 11 Sep 2006 11:07:10 +0100
Subject: [R] Test internet presence
In-Reply-To: <45051C45.1000909@bfro.uni-lj.si>
References: <45051C45.1000909@bfro.uni-lj.si>
Message-ID: <815b70590609110307k30996ef7s281938514ff31512@mail.gmail.com>

The function download.file (invisibly) returns a non-negative integer
if it fails, so I suppose you could use this as a test.

On 11/09/06, Gregor Gorjanc <gregor.gorjanc at bfro.uni-lj.si> wrote:
> Hello useRs!
>
> I have a small package and I need internet access for examples. Of
> course it works fine when I have internet access, but not otherwise. I
> remember I saw a way to test availability of internet access, but I can
> not find it now in archives. Can anyone here help me?
>
> Thanks!
>
> --
> Lep pozdrav / With regards,
>     Gregor Gorjanc
>
> ----------------------------------------------------------------------
> University of Ljubljana     PhD student
> Biotechnical Faculty
> Zootechnical Department     URI: http://www.bfro.uni-lj.si/MR/ggorjan
> Groblje 3                   mail: gregor.gorjanc <at> bfro.uni-lj.si
>
> SI-1230 Domzale             tel: +386 (0)1 72 17 861
> Slovenia, Europe            fax: +386 (0)1 72 17 888
>
> ----------------------------------------------------------------------
> "One must learn by doing the thing; for though you think you know it,
>  you have no certainty until you try." Sophocles ~ 450 B.C.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From mothsailor at googlemail.com  Mon Sep 11 12:08:37 2006
From: mothsailor at googlemail.com (David Barron)
Date: Mon, 11 Sep 2006 11:08:37 +0100
Subject: [R] Test internet presence
In-Reply-To: <815b70590609110307k30996ef7s281938514ff31512@mail.gmail.com>
References: <45051C45.1000909@bfro.uni-lj.si>
	<815b70590609110307k30996ef7s281938514ff31512@mail.gmail.com>
Message-ID: <815b70590609110308o564e78a4rf8b0b41ad021dbb8@mail.gmail.com>

I means non-zero, not non-negative.

On 11/09/06, David Barron <mothsailor at googlemail.com> wrote:
> The function download.file (invisibly) returns a non-negative integer
> if it fails, so I suppose you could use this as a test.
>
> On 11/09/06, Gregor Gorjanc <gregor.gorjanc at bfro.uni-lj.si> wrote:
> > Hello useRs!
> >
> > I have a small package and I need internet access for examples. Of
> > course it works fine when I have internet access, but not otherwise. I
> > remember I saw a way to test availability of internet access, but I can
> > not find it now in archives. Can anyone here help me?
> >
> > Thanks!
> >
> > --
> > Lep pozdrav / With regards,
> >     Gregor Gorjanc
> >
> > ----------------------------------------------------------------------
> > University of Ljubljana     PhD student
> > Biotechnical Faculty
> > Zootechnical Department     URI: http://www.bfro.uni-lj.si/MR/ggorjan
> > Groblje 3                   mail: gregor.gorjanc <at> bfro.uni-lj.si
> >
> > SI-1230 Domzale             tel: +386 (0)1 72 17 861
> > Slovenia, Europe            fax: +386 (0)1 72 17 888
> >
> > ----------------------------------------------------------------------
> > "One must learn by doing the thing; for though you think you know it,
> >  you have no certainty until you try." Sophocles ~ 450 B.C.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
> --
> =================================
> David Barron
> Said Business School
> University of Oxford
> Park End Street
> Oxford OX1 1HP
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From jochen.einbeck at nuigalway.ie  Mon Sep 11 12:04:59 2006
From: jochen.einbeck at nuigalway.ie (Jochen Einbeck)
Date: Mon, 11 Sep 2006 11:04:59 +0100
Subject: [R] cite this R help list
Message-ID: <450534CB.5070200@nuigalway.ie>

Dear members,

just a technical question: Is there a canonical way of citing this R 
help list?   I'm aware that a help list comment is probably not quotable 
in peer-reviewed statistical journals, but it might be of interest e.g. 
in vignettes, R Documentations,  or software newsletters.

Cheers,

Jochen

-- 

Dr. Jochen Einbeck
Department of Mathematics
National University of Ireland, Galway
Galway
Ireland

Tel +353 91 492327
Fax +353 91 494542


From gregor.gorjanc at bfro.uni-lj.si  Mon Sep 11 12:14:42 2006
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Mon, 11 Sep 2006 12:14:42 +0200
Subject: [R] Test internet presence
In-Reply-To: <815b70590609110308o564e78a4rf8b0b41ad021dbb8@mail.gmail.com>
References: <45051C45.1000909@bfro.uni-lj.si>	
	<815b70590609110307k30996ef7s281938514ff31512@mail.gmail.com>
	<815b70590609110308o564e78a4rf8b0b41ad021dbb8@mail.gmail.com>
Message-ID: <45053712.5020000@bfro.uni-lj.si>

David Barron wrote:
> I means non-zero, not non-negative.
> 
...

Thanks. download.file() is fine!

-- 
Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana     PhD student
Biotechnical Faculty
Zootechnical Department     URI: http://www.bfro.uni-lj.si/MR/ggorjan
Groblje 3                   mail: gregor.gorjanc <at> bfro.uni-lj.si

SI-1230 Domzale             tel: +386 (0)1 72 17 861
Slovenia, Europe            fax: +386 (0)1 72 17 888

----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.


From r.hankin at noc.soton.ac.uk  Mon Sep 11 12:15:04 2006
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Mon, 11 Sep 2006 11:15:04 +0100
Subject: [R] exactly representable numbers
In-Reply-To: <45053157.3060107@pdf.com>
References: <71115B0F-CF1B-4A63-9A58-106947D19F69@soc.soton.ac.uk>
	<45053157.3060107@pdf.com>
Message-ID: <5726D19C-9ACB-4299-812B-3DD41CAA6B8D@soc.soton.ac.uk>

Hi Sundar


thanks for this.  But I didn't make it clear that I'm interested in  
extreme numbers
such as 1e300 and 1e-300.

Then

 > f(1e300)
[1] 7.911257e+283

is different from

1e300*.Machine$double.eps


[I'm interested in the gap between successive different exactly  
representable
numbers right across the IEEE range]



rksh





On 11 Sep 2006, at 10:50, Sundar Dorai-Raj wrote:

>
>
> Robin Hankin said the following on 9/11/2006 3:52 AM:
>> Hi
>> Given a real number x,  I want to know how accurately R can  
>> represent  numbers near x.
>> In particular, I want to know the infimum of exactly representable
>> numbers greater than x, and the supremum of exactly representable   
>> numbers
>> less than x.  And then the interesting thing is the difference   
>> between these two.
>> I have a little function that does some of this:
>> f <- function(x,FAC=1.1){
>>    delta <- x
>> while(x+delta > x){
>>    delta <- delta/FAC
>> }
>> return(delta*FAC)
>> }
>> But this can't be optimal.
>> Is there a better way?
>
> I believe this is what .Machine$double.eps is. From ?.Machine
>
> double.eps: the smallest positive floating-point number 'x' such that
>           '1 + x != 1'.  It equals 'base^ulp.digits' if either 'base'
>           is 2 or 'rounding' is 0;  otherwise, it is  
> '(base^ulp.digits)
>           / 2'.
>
> See also .Machine$double.neg.eps. Is this what you need?
>
> --sundar

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From jrkrideau at yahoo.ca  Mon Sep 11 12:29:22 2006
From: jrkrideau at yahoo.ca (John Kane)
Date: Mon, 11 Sep 2006 06:29:22 -0400 (EDT)
Subject: [R] Export data
In-Reply-To: <45052C0B.1010108@grignon.inra.fr>
Message-ID: <20060911102922.67962.qmail@web32815.mail.mud.yahoo.com>


?write.table  >> 
             append = TRUE ?





--- Marie-No?lle Rolland <mrolland at grignon.inra.fr>
wrote:

> Hi there,
> 
> I would like to write a data output. In first time,
> a data frame (called 
> "res") is written in a file *csv such as:
> 
> A    B    C    # names of data
> 1.5  4.5  7    # values
> 
> So I use "write.table" which prints "res" to a file
> "file.csv"
>
write.table(res,file="file.csv",sep="",row.names=TRUE,col.names=TRUE,eol
> 
> = "\n")
> 
> After, I would like to append other data to the same
> file,
> 
> A    B    C
> 1.5  4.5  7
> 3    6    9   # appended data
> 
> I use then the same function but with different
> arguments:
>
write.table(res,file="file.csv",append=TRUE,sep="",row.names=TRUE,col.names=FALSE,eol
> 
> = "\n")
> 
> Actually, it doesn't work, the file is overwritten
> by the new one.
> 
> I got something like that:
> A    B    C
> 3    6    9
> 
> Coul you help me? Thank you in advance
> 
> Kind regards,
> Marie-No?lle
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From gregor.gorjanc at bfro.uni-lj.si  Mon Sep 11 12:33:22 2006
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Mon, 11 Sep 2006 12:33:22 +0200
Subject: [R] Test internet presence
In-Reply-To: <45053712.5020000@bfro.uni-lj.si>
References: <45051C45.1000909@bfro.uni-lj.si>	
	<815b70590609110307k30996ef7s281938514ff31512@mail.gmail.com>
	<815b70590609110308o564e78a4rf8b0b41ad021dbb8@mail.gmail.com>
	<45053712.5020000@bfro.uni-lj.si>
Message-ID: <45053B72.7010006@bfro.uni-lj.si>

Gregor Gorjanc wrote:
> Hello useRs!
> 
> I have a small package and I need internet access for examples. Of
> course it works fine when I have internet access, but not otherwise. I
> remember I saw a way to test availability of internet access, but I can
> not find it now in archives. Can anyone here help me?
> 
> David Barron wrote:
>> I means non-zero, not non-negative.
>>
> ...
> 
> Thanks. download.file() is fine!
> 

Thinking a bit more, download.file() is not really general as I need to
specify an url and it might happen that I have internet connection, but
server that provides url is down. I am to picky on this?

-- 
Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana     PhD student
Biotechnical Faculty
Zootechnical Department     URI: http://www.bfro.uni-lj.si/MR/ggorjan
Groblje 3                   mail: gregor.gorjanc <at> bfro.uni-lj.si

SI-1230 Domzale             tel: +386 (0)1 72 17 861
Slovenia, Europe            fax: +386 (0)1 72 17 888

----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.


From murdoch at stats.uwo.ca  Mon Sep 11 12:53:41 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 11 Sep 2006 06:53:41 -0400
Subject: [R] function changes argument
In-Reply-To: <B8EFAF3B18CD4A459A06022B3DEF117852567F@CEX20.ads.krz.uni-heidelberg.de>
References: <B8EFAF3B18CD4A459A06022B3DEF117852567F@CEX20.ads.krz.uni-heidelberg.de>
Message-ID: <45054035.6000703@stats.uwo.ca>

On 9/11/2006 4:49 AM, Moeltner, Andreas wrote:
> Dear R-list,
> 
> the following function f changes L. I thought, assignments within
> functions are only local?

That looks like a bug, still present in R-patched and R-devel.  (I 
haven't got the latest pre-release built yet today, but I expect it's 
there, too.)  Thanks for the report.

I'll send a copy of this to the bugs list, but I won't be able to 
attempt to fix it.

Duncan Murdoch

> 
> 
> f<-function(LL)
> { for (ll in LL)
>   { ll$txt<-"changed in f"
>   }
> }
> 
> l<-list(txt="original value")
> L<-list(l)
> L[[1]]$txt 
> f(L)
> L[[1]]$txt
> 
> 
> gives (using R 2.3.1):
> 
> ...
>> L[[1]]$txt
> [1] "original value"
>> f(L)
>> L[[1]]$txt
> [1] "changed in f"
> 
> Thanks in advance
> 
> Andreas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From murdoch at stats.uwo.ca  Mon Sep 11 13:12:03 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 11 Sep 2006 07:12:03 -0400
Subject: [R] exactly representable numbers
In-Reply-To: <5726D19C-9ACB-4299-812B-3DD41CAA6B8D@soc.soton.ac.uk>
References: <71115B0F-CF1B-4A63-9A58-106947D19F69@soc.soton.ac.uk>	<45053157.3060107@pdf.com>
	<5726D19C-9ACB-4299-812B-3DD41CAA6B8D@soc.soton.ac.uk>
Message-ID: <45054483.4040200@stats.uwo.ca>

On 9/11/2006 6:15 AM, Robin Hankin wrote:
> Hi Sundar
> 
> 
> thanks for this.  But I didn't make it clear that I'm interested in  
> extreme numbers
> such as 1e300 and 1e-300.
> 
> Then
> 
>  > f(1e300)
> [1] 7.911257e+283
> 
> is different from
> 
> 1e300*.Machine$double.eps
> 
> 
> [I'm interested in the gap between successive different exactly  
> representable
> numbers right across the IEEE range]

I'm not sure the result you're looking for is well defined, because on 
at least the Windows platform, R makes use of 80 bit temporaries as well 
as 64 bit double precision reals.  I don't know any, but would guess 
there exist examples of apparently equivalent formulations of your 
question that give different answers because one uses the temporaries 
and the other doesn't.

But in answer to your question:  a bisection search is what I'd use: 
you start with x+delta > x, and you know x+0 == x, so use 0 and delta as 
bracketing points.  You should be able to find the value in about 50-60 
bisections if you start with delta == x, many fewer if you make use of 
the double.eps value.  Here's my version:  not tested too much.

f <- function(x) {
   u <- x
   l <- 0
   mid <- u/2
   while (l < mid && mid < u) {
     if (x < x + mid) u <- mid
     else l <- mid
     mid <- (l + u)/2
   }
   u
}

 > f(1e300)
[1] 7.438715e+283
 > 1e300 + 7.438715e+283  > 1e300
[1] TRUE
 > 1e300 + 7.438714e+283  > 1e300
[1] FALSE


Duncan Murdoch

> 
> 
> 
> rksh
> 
> 
> 
> 
> 
> On 11 Sep 2006, at 10:50, Sundar Dorai-Raj wrote:
> 
>>
>> Robin Hankin said the following on 9/11/2006 3:52 AM:
>>> Hi
>>> Given a real number x,  I want to know how accurately R can  
>>> represent  numbers near x.
>>> In particular, I want to know the infimum of exactly representable
>>> numbers greater than x, and the supremum of exactly representable   
>>> numbers
>>> less than x.  And then the interesting thing is the difference   
>>> between these two.
>>> I have a little function that does some of this:
>>> f <- function(x,FAC=1.1){
>>>    delta <- x
>>> while(x+delta > x){
>>>    delta <- delta/FAC
>>> }
>>> return(delta*FAC)
>>> }
>>> But this can't be optimal.
>>> Is there a better way?
>> I believe this is what .Machine$double.eps is. From ?.Machine
>>
>> double.eps: the smallest positive floating-point number 'x' such that
>>           '1 + x != 1'.  It equals 'base^ulp.digits' if either 'base'
>>           is 2 or 'rounding' is 0;  otherwise, it is  
>> '(base^ulp.digits)
>>           / 2'.
>>
>> See also .Machine$double.neg.eps. Is this what you need?
>>
>> --sundar
> 
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>   tel  023-8059-7743
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From gregor.gorjanc at bfro.uni-lj.si  Mon Sep 11 13:47:24 2006
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Mon, 11 Sep 2006 13:47:24 +0200
Subject: [R] Test internet presence
In-Reply-To: <45053B72.7010006@bfro.uni-lj.si>
References: <45051C45.1000909@bfro.uni-lj.si>	
	<815b70590609110307k30996ef7s281938514ff31512@mail.gmail.com>
	<815b70590609110308o564e78a4rf8b0b41ad021dbb8@mail.gmail.com>
	<45053712.5020000@bfro.uni-lj.si> <45053B72.7010006@bfro.uni-lj.si>
Message-ID: <45054CCC.9050600@bfro.uni-lj.si>

Gregor Gorjanc wrote:
> Gregor Gorjanc wrote:
>> Hello useRs!
>>
>> I have a small package and I need internet access for examples. Of
>> course it works fine when I have internet access, but not otherwise. I
>> remember I saw a way to test availability of internet access, but I can
>> not find it now in archives. Can anyone here help me?
>>
>> David Barron wrote:
>>> I means non-zero, not non-negative.
>>>
>> ...
>>
>> Thanks. download.file() is fine!
>>
> 
> Thinking a bit more, download.file() is not really general as I need to
> specify an url and it might happen that I have internet connection, but
> server that provides url is down. I am to picky on this?

Additionally, download.file throws an error if url is not available i.e.
if I do not have internet access. I have used the following example:

if(download.file(url="http://www.r-project.org/index.html",
                 destfile=tempfile()) == 0) {
  print("Yuhuu")
}

unpluged the net cable and got the following message during R CMD check
(in examples part)

trying URL 'http://www.r-project.org/index.html'
Warning: unable to resolve 'www.r-project.org'.
Error in download.file(url = "http://www.r-project.org/index.html",
destfile = tempfile()) :
        cannot open URL 'http://www.r-project.org/index.html'
Execution halted

It seems that 'internal' method was used (I use R 2.3.1 under Linux) as
indicated in help page of download.file. I could use wget or lynx
methods, but these two must be available, so this is not really
portable. Are there any other options for testing internet access? I am
thinking that this might be more relevant for R-devel. I will wait a bit
before moving there.

-- 
Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana     PhD student
Biotechnical Faculty
Zootechnical Department     URI: http://www.bfro.uni-lj.si/MR/ggorjan
Groblje 3                   mail: gregor.gorjanc <at> bfro.uni-lj.si

SI-1230 Domzale             tel: +386 (0)1 72 17 861
Slovenia, Europe            fax: +386 (0)1 72 17 888

----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.


From arinbasu at cashette.com  Mon Sep 11 13:51:17 2006
From: arinbasu at cashette.com (arin basu)
Date: Mon, 11 Sep 2006 04:51:17 -0700 (PDT)
Subject: [R] Comparison of R with proprietary software
Message-ID: <4020369.1157975477296.JavaMail.Administrator@appsrv>

Hi All:

Recently, I got a query how R compared cost-wise (cost of purchase, maintenance, and training manuals) with other proprietary statistical software programs (or programming environments). The request, verbatim:

<--- begin quote ---->

>> dear arin,
>>
>> can you please find out the price of the various
>> prop. statistical
>> packages that are in general use + their per user
>> license fee + annual
>> fees etc at various scales?
>>
>> this will help us to prepare a matrix as to why
>> people should spend
>> money on R or similar training ...

< --- end quote --->

I searched the web and got some quotes. I believe someone/group in this list may already have compiled a nice comparison chart with more detailed research than I can pull up with my one hour surfing. I'd greatly appreciate if anyone can share price-wise comparison between R versus popular proprietary software (SAS, SPSS, STATA, etc). 

TIA,
Arin Basu


From oyvfos at yahoo.no  Mon Sep 11 13:52:43 2006
From: oyvfos at yahoo.no (yvind Foshaug)
Date: Mon, 11 Sep 2006 04:52:43 -0700 (PDT)
Subject: [R] estimating state space with exogenous input in measurement eq.
Message-ID: <20060911115244.90041.qmail@web25505.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060911/43a172c7/attachment.pl 

From markleeds at verizon.net  Mon Sep 11 14:00:44 2006
From: markleeds at verizon.net (MARK LEEDS)
Date: Mon, 11 Sep 2006 08:00:44 -0400
Subject: [R] estimating state space with exogenous input in measurement
	eq.
References: <20060911115244.90041.qmail@web25505.mail.ukl.yahoo.com>
Message-ID: <00a401c6d599$e989c2b0$2e01a8c0@m8d4477f3de884>

below is very close to a standard kalman filter setup except for the 
exogenous u[k] so i would check on www.r-project.org
for any packages that do kalman filtering. (  also, good reference for state 
space is a book by durbin and koopman but i forget the title exactly ).




----- Original Message ----- 
From: "?yvind Foshaug" <oyvfos at yahoo.no>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, September 11, 2006 7:52 AM
Subject: [R] estimating state space with exogenous input in measurement eq.


> Anyone know how to esimate parameters in the system:
>
>  x[k]=Ax[k-1]+ B + Gv[k-1]
>  y[k]=x[k]+Du[k]+Hw[k]
>
>  a system with exogenous u[k] in the measurement eq., v,w are iid, both 
> eq. are gaussian.
>
>  Thanks,
>  Oyvind
>
>
>
> ---------------------------------
>
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ripley at stats.ox.ac.uk  Mon Sep 11 14:20:53 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 11 Sep 2006 13:20:53 +0100 (BST)
Subject: [R] exactly representable numbers
In-Reply-To: <45054483.4040200@stats.uwo.ca>
References: <71115B0F-CF1B-4A63-9A58-106947D19F69@soc.soton.ac.uk>
	<45053157.3060107@pdf.com>
	<5726D19C-9ACB-4299-812B-3DD41CAA6B8D@soc.soton.ac.uk>
	<45054483.4040200@stats.uwo.ca>
Message-ID: <Pine.LNX.4.64.0609111303560.12949@gannet.stats.ox.ac.uk>

On Mon, 11 Sep 2006, Duncan Murdoch wrote:

> On 9/11/2006 6:15 AM, Robin Hankin wrote:
> > Hi Sundar
> > 
> > 
> > thanks for this.  But I didn't make it clear that I'm interested in  
> > extreme numbers
> > such as 1e300 and 1e-300.

That's not relevant, unless you are interested in extremely small numbers.

> > Then
> > 
> >  > f(1e300)
> > [1] 7.911257e+283

(That is inaccurate)

> > is different from
> > 
> > 1e300*.Machine$double.eps

Yes, since 1e300 is not a power of two.  However, Sundar is right in the 
sense that this is an upper bound for normalized numbers.

f(1) is .Machine$double.neg.eps, but so it is for all 1 <= x < 2.
This gives you the answer:  .Machine$double.neg.eps * 2^floor(log2(x))

Similarly for going below (but carefully as you get an extra halving on 
the powers of two).

These results hold for all but denormalized numbers (those below 1e-308).


> > [I'm interested in the gap between successive different exactly  
> > representable
> > numbers right across the IEEE range]
> 
> I'm not sure the result you're looking for is well defined, because on 
> at least the Windows platform, R makes use of 80 bit temporaries as well 
> as 64 bit double precision reals.  I don't know any, but would guess 
> there exist examples of apparently equivalent formulations of your 
> question that give different answers because one uses the temporaries 
> and the other doesn't.

Not at R level.  For something to get stored in a real vector, it will be 
a standard 64-bit double.

> But in answer to your question:  a bisection search is what I'd use: 
> you start with x+delta > x, and you know x+0 == x, so use 0 and delta as 
> bracketing points.  You should be able to find the value in about 50-60 
> bisections if you start with delta == x, many fewer if you make use of 
> the double.eps value.  Here's my version:  not tested too much.
> 
> f <- function(x) {
>    u <- x
>    l <- 0
>    mid <- u/2
>    while (l < mid && mid < u) {
>      if (x < x + mid) u <- mid
>      else l <- mid
>      mid <- (l + u)/2
>    }
>    u
> }
> 
>  > f(1e300)
> [1] 7.438715e+283
>  > 1e300 + 7.438715e+283  > 1e300
> [1] TRUE
>  > 1e300 + 7.438714e+283  > 1e300
> [1] FALSE
> 
> 
> Duncan Murdoch
> 
> > 
> > 
> > 
> > rksh
> > 
> > 
> > 
> > 
> > 
> > On 11 Sep 2006, at 10:50, Sundar Dorai-Raj wrote:
> > 
> >>
> >> Robin Hankin said the following on 9/11/2006 3:52 AM:
> >>> Hi
> >>> Given a real number x,  I want to know how accurately R can  
> >>> represent  numbers near x.
> >>> In particular, I want to know the infimum of exactly representable
> >>> numbers greater than x, and the supremum of exactly representable   
> >>> numbers
> >>> less than x.  And then the interesting thing is the difference   
> >>> between these two.
> >>> I have a little function that does some of this:
> >>> f <- function(x,FAC=1.1){
> >>>    delta <- x
> >>> while(x+delta > x){
> >>>    delta <- delta/FAC
> >>> }
> >>> return(delta*FAC)
> >>> }
> >>> But this can't be optimal.
> >>> Is there a better way?
> >> I believe this is what .Machine$double.eps is. From ?.Machine
> >>
> >> double.eps: the smallest positive floating-point number 'x' such that
> >>           '1 + x != 1'.  It equals 'base^ulp.digits' if either 'base'
> >>           is 2 or 'rounding' is 0;  otherwise, it is  
> >> '(base^ulp.digits)
> >>           / 2'.
> >>
> >> See also .Machine$double.neg.eps. Is this what you need?
> >>
> >> --sundar
> > 
> > --
> > Robin Hankin
> > Uncertainty Analyst
> > National Oceanography Centre, Southampton
> > European Way, Southampton SO14 3ZH, UK
> >   tel  023-8059-7743
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From c.oswald at matsci.uni-sb.de  Mon Sep 11 14:44:24 2006
From: c.oswald at matsci.uni-sb.de (Christian Oswald)
Date: Mon, 11 Sep 2006 14:44:24 +0200
Subject: [R] Description of x-axis
Message-ID: <45055A28.30302@matsci.uni-sb.de>

Hello,

I have made a barplot with some data and need a description below the
x-axis. For example there are 20 values and I need a description "2003"
for the first four values, then "2005" for the next eleven and "2006"
for the last five values.

I want the description below the x-axis-labels and above the x-axis title.

Thanks,

Christian


From murdoch at stats.uwo.ca  Mon Sep 11 14:46:25 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 11 Sep 2006 08:46:25 -0400
Subject: [R] exactly representable numbers
In-Reply-To: <Pine.LNX.4.64.0609111303560.12949@gannet.stats.ox.ac.uk>
References: <71115B0F-CF1B-4A63-9A58-106947D19F69@soc.soton.ac.uk>
	<45053157.3060107@pdf.com>
	<5726D19C-9ACB-4299-812B-3DD41CAA6B8D@soc.soton.ac.uk>
	<45054483.4040200@stats.uwo.ca>
	<Pine.LNX.4.64.0609111303560.12949@gannet.stats.ox.ac.uk>
Message-ID: <45055AA1.6020101@stats.uwo.ca>

On 9/11/2006 8:20 AM, Prof Brian Ripley wrote:
> On Mon, 11 Sep 2006, Duncan Murdoch wrote:
> 
>> On 9/11/2006 6:15 AM, Robin Hankin wrote:
>> > Hi Sundar
>> > 
>> > 
>> > thanks for this.  But I didn't make it clear that I'm interested in  
>> > extreme numbers
>> > such as 1e300 and 1e-300.
> 
> That's not relevant, unless you are interested in extremely small numbers.
> 
>> > Then
>> > 
>> >  > f(1e300)
>> > [1] 7.911257e+283
> 
> (That is inaccurate)
> 
>> > is different from
>> > 
>> > 1e300*.Machine$double.eps
> 
> Yes, since 1e300 is not a power of two.  However, Sundar is right in the 
> sense that this is an upper bound for normalized numbers.
> 
> f(1) is .Machine$double.neg.eps, but so it is for all 1 <= x < 2.
> This gives you the answer:  .Machine$double.neg.eps * 2^floor(log2(x))

I'm not sure what is going wrong, but that is too small (on my machine, 
at least):

 > f1 <- function(x) .Machine$double.neg.eps * 2^floor(log2(x))
 > f1(1e300)
[1] 7.435085e+283
 > 1e300 + f1(1e300) == 1e300
[1] TRUE

Notice the difference in the 3rd decimal place from the empirical answer 
from my bisection search below.

> 
> Similarly for going below (but carefully as you get an extra halving on 
> the powers of two).
> 
> These results hold for all but denormalized numbers (those below 1e-308).
> 
> 
>> > [I'm interested in the gap between successive different exactly  
>> > representable
>> > numbers right across the IEEE range]
>> 
>> I'm not sure the result you're looking for is well defined, because on 
>> at least the Windows platform, R makes use of 80 bit temporaries as well 
>> as 64 bit double precision reals.  I don't know any, but would guess 
>> there exist examples of apparently equivalent formulations of your 
>> question that give different answers because one uses the temporaries 
>> and the other doesn't.
> 
> Not at R level.  For something to get stored in a real vector, it will be 
> a standard 64-bit double.

I don't think that's a proof, since R level code can call C functions, 
and there are an awful lot of callable functions in R, but I don't have 
a counter-example.

Duncan Murdoch


> 
>> But in answer to your question:  a bisection search is what I'd use: 
>> you start with x+delta > x, and you know x+0 == x, so use 0 and delta as 
>> bracketing points.  You should be able to find the value in about 50-60 
>> bisections if you start with delta == x, many fewer if you make use of 
>> the double.eps value.  Here's my version:  not tested too much.
>> 
>> f <- function(x) {
>>    u <- x
>>    l <- 0
>>    mid <- u/2
>>    while (l < mid && mid < u) {
>>      if (x < x + mid) u <- mid
>>      else l <- mid
>>      mid <- (l + u)/2
>>    }
>>    u
>> }
>> 
>>  > f(1e300)
>> [1] 7.438715e+283
>>  > 1e300 + 7.438715e+283  > 1e300
>> [1] TRUE
>>  > 1e300 + 7.438714e+283  > 1e300
>> [1] FALSE
>> 
>> 
>> Duncan Murdoch
>> 
>> > 
>> > 
>> > 
>> > rksh
>> > 
>> > 
>> > 
>> > 
>> > 
>> > On 11 Sep 2006, at 10:50, Sundar Dorai-Raj wrote:
>> > 
>> >>
>> >> Robin Hankin said the following on 9/11/2006 3:52 AM:
>> >>> Hi
>> >>> Given a real number x,  I want to know how accurately R can  
>> >>> represent  numbers near x.
>> >>> In particular, I want to know the infimum of exactly representable
>> >>> numbers greater than x, and the supremum of exactly representable   
>> >>> numbers
>> >>> less than x.  And then the interesting thing is the difference   
>> >>> between these two.
>> >>> I have a little function that does some of this:
>> >>> f <- function(x,FAC=1.1){
>> >>>    delta <- x
>> >>> while(x+delta > x){
>> >>>    delta <- delta/FAC
>> >>> }
>> >>> return(delta*FAC)
>> >>> }
>> >>> But this can't be optimal.
>> >>> Is there a better way?
>> >> I believe this is what .Machine$double.eps is. From ?.Machine
>> >>
>> >> double.eps: the smallest positive floating-point number 'x' such that
>> >>           '1 + x != 1'.  It equals 'base^ulp.digits' if either 'base'
>> >>           is 2 or 'rounding' is 0;  otherwise, it is  
>> >> '(base^ulp.digits)
>> >>           / 2'.
>> >>
>> >> See also .Machine$double.neg.eps. Is this what you need?
>> >>
>> >> --sundar
>> > 
>> > --
>> > Robin Hankin
>> > Uncertainty Analyst
>> > National Oceanography Centre, Southampton
>> > European Way, Southampton SO14 3ZH, UK
>> >   tel  023-8059-7743
>> > 
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 
>


From Ted.Harding at nessie.mcc.ac.uk  Mon Sep 11 14:48:21 2006
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 11 Sep 2006 13:48:21 +0100 (BST)
Subject: [R] Test internet presence
In-Reply-To: <45054CCC.9050600@bfro.uni-lj.si>
Message-ID: <XFMail.060911134821.Ted.Harding@nessie.mcc.ac.uk>

On 11-Sep-06 Gregor Gorjanc wrote:
> It seems that 'internal' method was used (I use R 2.3.1 under
> Linux) as indicated in help page of download.file. I could
> use wget or lynx methods, but these two must be available,
> so this is not really portable. Are there any other options
> for testing internet access? I am thinking that this might be
> more relevant for R-devel. I will wait a bit
> before moving there.
> 
> -- 
> Lep pozdrav / With regards,
>     Gregor Gorjanc

Hi Gorjanc,
Since you are using Linux, I think you should ask R to delegate
the test to the system.

If you have a script, in executable file ("755") say "test.inet.sh",
which says something like

if ping -c 1 <something> ; then
    export NET_UP="YES"
  else
    export NETP_UP="NO"
fi

where "<something>" is the IP address or name of an external host
which responds to 'ping' (some will not, depending on their firewall
settings), then you can use on R:

system("test.inet")
if( sys.getenv(NET_UP") == "YES" ) { ... } else { ... }

For example (nothing to do with R, but shows the principle),
I have the following script to set my system time and hardware
clock from whichever one of 3 NTP servers is willing to respond:

if /bin/ping -c 1 ntp0.zen.co.uk ; then
    export NETTIME="/usr/sbin/ntpdate -u ntp0.zen.co.uk"
  elif /bin/ping -c 1 ntp2b.mcc.ac.uk ; then
    export NETTIME="/usr/sbin/ntpdate -u ntp2b.mcc.ac.uk"
  elif /bin/ping -c 1 ntp2c.mcc.ac.uk ; then
    export NETTIME="/usr/sbin/ntpdate -u ntp2c.mcc.ac.uk"
  else
    export NETTIME=""
fi
if [ "$NETTIME" != "" ] ; then
  sleep 1
  sleep 1
  $NETTIME
  /sbin/clock -u -w
  date
fi


which also illustrates how to allow for the possibility that
the "default" server might not be responding at the time, so
it has 2 fallback servers.

Hoping this helps,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 11-Sep-06                                       Time: 13:42:22
------------------------------ XFMail ------------------------------


From mothsailor at googlemail.com  Mon Sep 11 14:48:49 2006
From: mothsailor at googlemail.com (David Barron)
Date: Mon, 11 Sep 2006 13:48:49 +0100
Subject: [R] Description of x-axis
In-Reply-To: <45055A28.30302@matsci.uni-sb.de>
References: <45055A28.30302@matsci.uni-sb.de>
Message-ID: <815b70590609110548g4d763904tf07e674b07f215cf@mail.gmail.com>

I think ?mtext should show you how to do what you want.

On 11/09/06, Christian Oswald <c.oswald at matsci.uni-sb.de> wrote:
> Hello,
>
> I have made a barplot with some data and need a description below the
> x-axis. For example there are 20 values and I need a description "2003"
> for the first four values, then "2005" for the next eleven and "2006"
> for the last five values.
>
> I want the description below the x-axis-labels and above the x-axis title.
>
> Thanks,
>
> Christian
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From ripley at stats.ox.ac.uk  Mon Sep 11 15:01:33 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 11 Sep 2006 14:01:33 +0100 (BST)
Subject: [R] exactly representable numbers
In-Reply-To: <45055AA1.6020101@stats.uwo.ca>
References: <71115B0F-CF1B-4A63-9A58-106947D19F69@soc.soton.ac.uk>
	<45053157.3060107@pdf.com>
	<5726D19C-9ACB-4299-812B-3DD41CAA6B8D@soc.soton.ac.uk>
	<45054483.4040200@stats.uwo.ca>
	<Pine.LNX.4.64.0609111303560.12949@gannet.stats.ox.ac.uk>
	<45055AA1.6020101@stats.uwo.ca>
Message-ID: <Pine.LNX.4.64.0609111351560.974@gannet.stats.ox.ac.uk>

On Mon, 11 Sep 2006, Duncan Murdoch wrote:

> On 9/11/2006 8:20 AM, Prof Brian Ripley wrote:
> > On Mon, 11 Sep 2006, Duncan Murdoch wrote:
> > 
> > > On 9/11/2006 6:15 AM, Robin Hankin wrote:
> > > > Hi Sundar
> > > > 
> > > > 
> > > > thanks for this.  But I didn't make it clear that I'm interested in
> > > > extreme numbers
> > > > such as 1e300 and 1e-300.
> > 
> > That's not relevant, unless you are interested in extremely small numbers.
> > 
> > > > Then
> > > > 
> > > > > f(1e300)
> > > > [1] 7.911257e+283
> > 
> > (That is inaccurate)
> > 
> > > > is different from
> > > > 
> > > > 1e300*.Machine$double.eps
> > 
> > Yes, since 1e300 is not a power of two.  However, Sundar is right in the
> > sense that this is an upper bound for normalized numbers.
> > 
> > f(1) is .Machine$double.neg.eps, but so it is for all 1 <= x < 2.
> > This gives you the answer:  .Machine$double.neg.eps * 2^floor(log2(x))
> 
> I'm not sure what is going wrong, but that is too small (on my machine, at
> least):
> 
> > f1 <- function(x) .Machine$double.neg.eps * 2^floor(log2(x))
> > f1(1e300)
> [1] 7.435085e+283
> > 1e300 + f1(1e300) == 1e300
> [1] TRUE
> 
> Notice the difference in the 3rd decimal place from the empirical answer from
> my bisection search below.

I wasn't going into that much detail: just what accuracy are we looking 
for here?  .Machine$double.neg.eps isn't that accurate (as its help page 
says).

> > Similarly for going below (but carefully as you get an extra halving on the
> > powers of two).
> > 
> > These results hold for all but denormalized numbers (those below 1e-308).
> > 
> > 
> > > > [I'm interested in the gap between successive different exactly
> > > > representable
> > > > numbers right across the IEEE range]
> > > 
> > > I'm not sure the result you're looking for is well defined, because on at
> > > least the Windows platform, R makes use of 80 bit temporaries as well as
> > > 64 bit double precision reals.  I don't know any, but would guess there
> > > exist examples of apparently equivalent formulations of your question that
> > > give different answers because one uses the temporaries and the other
> > > doesn't.
> > 
> > Not at R level.  For something to get stored in a real vector, it will be a
> > standard 64-bit double.
> 
> I don't think that's a proof, since R level code can call C functions, and
> there are an awful lot of callable functions in R, but I don't have a
> counter-example.

Oh, it is proof: the storage is declared as C double, and extended 
precision double only exists on the chip.  Since R has to look up 'x' 
whenever it sees the symbol, it looks at the stored value, not on a 
register.  A compiler could do better, but the current code cannot.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From petr.pikal at precheza.cz  Mon Sep 11 15:03:33 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 11 Sep 2006 15:03:33 +0200
Subject: [R] Description of x-axis
In-Reply-To: <45055A28.30302@matsci.uni-sb.de>
Message-ID: <45057AC5.12697.1898564@localhost>

Hi

see
?mtext and its parameters side, line and at.

HTH
Petr


On 11 Sep 2006 at 14:44, Christian Oswald wrote:

Date sent:      	Mon, 11 Sep 2006 14:44:24 +0200
From:           	Christian Oswald <c.oswald at matsci.uni-sb.de>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] Description of x-axis
Send reply to:  	c.oswald at matsci.uni-sb.de
	<mailto:r-help-request at stat.math.ethz.ch?subject=unsubscribe>
	<mailto:r-help-request at stat.math.ethz.ch?subject=subscribe>

> Hello,
> 
> I have made a barplot with some data and need a description below the
> x-axis. For example there are 20 values and I need a description
> "2003" for the first four values, then "2005" for the next eleven and
> "2006" for the last five values.
> 
> I want the description below the x-axis-labels and above the x-axis
> title.
> 
> Thanks,
> 
> Christian
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From afshart at exchange.sba.miami.edu  Mon Sep 11 15:09:36 2006
From: afshart at exchange.sba.miami.edu (Afshartous, David)
Date: Mon, 11 Sep 2006 09:09:36 -0400
Subject: [R] augPred plot in nlme library
Message-ID: <6BCB4D493A447546A8126F24332056E80415E90B@school1.business.edu>

Hi Peter,

Thanks for the email.  No, plot(augPred(fm2c)) still
graphs the original points but doesn't graph the 
prediction curve.  However, this is solved by hardcording
the dependent variable as below.  yes, I attached Orthodont.


cheers,
dave


-----Original Message-----
From: Petr Pikal [mailto:petr.pikal at precheza.cz] 
Sent: Monday, September 11, 2006 2:52 AM
To: Afshartous, David; r-help at stat.math.ethz.ch
Subject: Re: [R] augPred plot in nlme library

Hi

please try not to hide an information

> plot(augPred(fm2c))
Error in log(distance) : object "distance" not found

Is it what you have got and what you mean by "does not graph any 
prediction at all"?

If not did you attached Orthodont before?

I suppose plot.augPred probably expects the same name for original 
and and fitted data. You can go through source code to see what 
happens by

nlme:::plot.augPred

HTH
Petr



On 9 Sep 2006 at 13:39, Afshartous, David wrote:

Date sent:      	Sat, 9 Sep 2006 13:39:04 -0400
From:           	"Afshartous, David"
<afshart at exchange.sba.miami.edu>
To:             	"Deepayan Sarkar" <deepayan.sarkar at gmail.com>
Copies to:      	r-help at stat.math.ethz.ch
Subject:        	Re: [R] augPred plot in nlme library

> 
> Thanks Deepayan and Andrew.
> 
> msMaxIter solved the convergence problem and plot(augPred) works with
> my data when I employ I() in the function call.
> 
> One other strange thing I noticed is that when I take logs of
> dependent variable in the function call, the plot of augPred 
> doesn't graph any prediction line at all.  
> 
> contr=nlmeControl(msMaxIter = 500)
> fm2c <- lme(log(distance) ~ age + I(age^2), data = Orthodont,
> control=contr)
> plot(augPred(fm2c)) 
> 
> However, this is fixed by hard coding the dependent variable:
> 
> log.dist = log(distance)
> fm2c <- lme(log.dist ~ age + I(age^2), data = Orthodont,
> control=contr) plot(augPred(fm2c))
> 
> 
> 
> -----Original Message-----
> From: Deepayan Sarkar [mailto:deepayan.sarkar at gmail.com] 
> Sent: Saturday, September 09, 2006 11:46 AM
> To: Afshartous, David
> Cc: Andrew Robinson; r-help at stat.math.ethz.ch
> Subject: Re: [R] augPred plot in nlme library
> 
> On 9/9/06, Afshartous, David <afshart at exchange.sba.miami.edu> wrote: >
> Hi Andrew, > > Thanks for your email.  I assume you mean age^2 instead
> of age.2 for > fm2a, and for fm2b, I get the following error: > > >
> fm2b <- lme(distance ~ age + I(age^2), data = Orthodont) > Error in
> lme.formula(distance ~ age + I(age^2), data = Orthodont) : >        
> iteration limit reached without convergence (9) > > do you get his
> error as well?
> 
> For me, adding 'control = list(msMaxIter = 500)' worked. I'm writing
> from memory, so the name may not be exactly right, see ?nlmeControl.
> 
> > Finally, the Pixel example on p.42 of Pinheiro & Bates gets the
> > quadratic plot w/o using I() as you do below; is this due to a
> > difference between S and R?
> 
> Yes.
> 
> >
> > thanks!
> > dave
> >
> > ps - sorry for not making the data available; if anyone is
> > interested please let me know and I'll send it directly.
> >
> >
> >
> >
> > -----Original Message-----
> > From: Andrew Robinson [mailto:A.Robinson at ms.unimelb.edu.au]
> > Sent: Friday, September 08, 2006 5:46 PM
> > To: Afshartous, David
> > Cc: Deepayan Sarkar; r-help at stat.math.ethz.ch
> > Subject: Re: [R] augPred plot in nlme library
> >
> > Hi David,
> >
> > this is the sort of thing that Deepayan meant.  Make a dataset
> > available to us, or use one that will be installed by default on R.
> >
> > eg
> >
> > require(nlme)
> > fm1 <- lme(distance ~ age, data = Orthodont)
> > plot(augPred(fm1))
> >
> > #  All linear
> >
> > fm2a <- lme(distance ~ age + age.2, data = Orthodont)
> > plot(augPred(fm2a))
> >
> > # Still linear
> >
> > fm2b <- lme(distance ~ age + I(age^2), data = Orthodont)
> > plot(augPred(fm2b))
> >
> > # Quadratic!
> >
> > I hope that this helps you resolve the problem.
> >
> > Andrew
> >
> >
> >
> > On Fri, Sep 08, 2006 at 05:18:13PM -0400, Afshartous, David wrote:
> > >
> > > Deepayan,
> > >
> > > Thanks for your suggestion.  Here are more details:
> > >
> > > I have a grouped data object for repeated measures data just like
> > > the Pixel grouped data object on p.42 of Pinheiro and Bates
> > > (2000).
> > >
> > > comp.adj.UKV.3 <- groupedData(adj.UKV ~ Time |
> Patient_no/Lisinopril,
> > >       data = comp.adj.UKV.frm, order.groups = F
> > >       #labels = list(x = "Hour", y = "adj.UKV")
> > > )
> > >
> > > i.e., the response is continuous, Time is not treated as a factor,
> > > and
> >
> > > there exists two factors, one nested within the other (Lisinopril
> > > nested
> > >
> > > witin patient, similar to Side within Dog on p.42).
> > >
> > > I also fit a model very similar to their model:
> > >
> > > fm1comp = lme(adj.UKV ~ Time + Time.sq, data = comp.adj.UKV.3,
> > > random = list(Patient_no = ~ 1 , Lisinopril = ~ 1) )
> > >
> > >
> > > However, the command below does not produce the fitted curves from
> > > this model, but rather it seems to be the fitted curves from a
> > > linear model.
> > >
> > > plot(augPred(fm3comp))
> > >
> > > Possibly augPred behaves differently in R than in S, but reading
> > > the
> 
> > > R
> >
> > > help and trying various other approaches has not solved this.
> > >
> > > Thanks!
> > > Dave
> > >
> > >
> > >
> > >
> > > -----Original Message-----
> > > From: Deepayan Sarkar [mailto:deepayan.sarkar at gmail.com]
> > > Sent: Friday, September 08, 2006 4:37 PM
> > > To: Afshartous, David
> > > Cc: r-help at stat.math.ethz.ch
> > > Subject: Re: [R] augPred plot in nlme library
> > >
> > > On 9/8/06, Afshartous, David <afshart at exchange.sba.miami.edu>
> > > wrote:
> > > > All,
> > > >
> > > > I've solved part of the problem below by making sure that the
> > > > formula in the grouped data object is the same as the formula
> > > > specified within
> > >
> > > > lme (this isn't the case in the cited example from Pinheiro &
> > Bates).
> > > >
> > > > However, augPred seems to plot only a linear model instead of
> > > > the polynomial model.  Does anyone know how to make sure that
> > > > augPred plots the same model as that specified in the model (as
> > > > below)?
> > >
> > > You are unlikely to get any helpful answers unless you give us
> > > more information, as every r-help message asks you to do:
> > >
> > > > R-help at stat.math.ethz.ch mailing list 
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide
> > > > http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible
> > > > code.
> > >
> > > -Deepayan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From bolker at zoo.ufl.edu  Mon Sep 11 15:08:54 2006
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Mon, 11 Sep 2006 13:08:54 +0000 (UTC)
Subject: [R] maximizing a likelihood function containing an integral
References: <20060908233458.77728.qmail@web34711.mail.mud.yahoo.com>
Message-ID: <loom.20060911T150505-535@post.gmane.org>

Sam Wong <qxsr <at> yahoo.com> writes:

> 
> 
> Hi, R Users;
> 
> I am trying to maximize a likelihood function which
> contains an integral.  The integral contains the
> unknown parameter as well.  I am trying to use the
> following code to do the maximization:
> 

  [snip]

> The error message is: 
> Error in log(x) : Non-numeric argument to mathematical
> function
> 


   The only obvious (?) mistake in the code is that
you refer to integrate(...)$integral -- the help page
for ?integrate says that $value is the estimate of
the integral, so you might be getting a NULL here.

However, the code you gave us is not reproducible/self-contained -- since I don't
know what z1, z2, n1, rho are, I can't run it and see if that's the only 
problem.

  cheers
    Ben Bolker


From trajnp at gmail.com  Mon Sep 11 15:14:01 2006
From: trajnp at gmail.com (Raj, Towfique)
Date: Mon, 11 Sep 2006 14:14:01 +0100
Subject: [R] Wilcoxon Rank-Sum Test with Bonferroni's correction
Message-ID: <bcb1e1da0609110614m4598e2b5tb556cc552c76cd26@mail.gmail.com>

Dear all,

I am trying to run Wilcoxon Rank-Sum Test with Bonferroni's
correction. I have two lists: l0, l1:

mapply(function(x,y)wilcox.test(x,y)$p.value, l0, l1)

How do I run Bonferroni's correction on mapply? Any help is much apperciated.
Thanks,

-Raj


From ggrothendieck at gmail.com  Mon Sep 11 15:18:01 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 11 Sep 2006 09:18:01 -0400
Subject: [R] Weighted association map
In-Reply-To: <8688BD6B-8649-4CF2-A5C4-AAD86E0F380D@mus.utu.fi>
References: <38929107-8CB9-4C22-827F-952D371B094E@local>
	<971536df0609080237q12aa5bc7pb9069d0a2dca57a5@mail.gmail.com>
	<971536df0609080453p2565c7d5o69d07e895fde4ba4@mail.gmail.com>
	<8688BD6B-8649-4CF2-A5C4-AAD86E0F380D@mus.utu.fi>
Message-ID: <971536df0609110618m285f7b1o60e986c2172d6895@mail.gmail.com>

Great.  In looking at it once more I realize that even the sign(kor)
part could be shortened to just kor in this example.  Regards.

library(sna)
set.seed(123)
kor <- cor(iris[1:4])
gplot(kor, edge.lwd = 10*kor, displaylabels = TRUE, label = rownames(kor))



On 9/11/06, kone <attenka at utu.fi> wrote:
> Sorry to answer so late, but this is just what I want ;-)
>
> -Atte
>
> Gabor Grothendieck kirjoitti 8.9.2006 kello 14.53:
>
> > Actually the discretization does not appear to be needed.  This
> > works just as well:
> >
> > set.seed(123)
> > kor <- cor(iris[1:4])
> > gplot(sign(kor), edge.lwd = 10*kor, displaylabels = TRUE, label =
> > rownames(kor))
> >
> > On 9/8/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> >> Try the sna package.  Below we calculate the
> >> correlation matrix, kor, of the numeric cols of builtin iris
> >> dataset.  Zap negative ones and discretize rest to
> >> get lwd width matrix, lwd, used for edge widths.  From
> >> that create the adjacency matrix, sign(lwd), and plot it
> >> using indicated layout mode.  Seems like three of
> >> the variables are correlated and Sepal.Width is uncorrelated
> >> or negatively correlated to those. Try playing around with
> >> gplot args to create variations.
> >>
> >> library(sna)
> >> set.seed(123) # layout uses random numbers
> >> kor <- cor(iris[1:4])
> >> lwd <- replace(kor, TRUE, 10 * round(pmax(0, kor), 1))
> >> gplot(sign(lwd), edge.lwd = lwd, displaylabels = TRUE, label =
> >> rownames(kor))
> >>
> >> On 9/8/06, kone <attenka at utu.fi> wrote:
> >> > Could somebody program this kind of plot type to R, if none exists,
> >> > based on mds or correlation tables or some more suitable method?
> >> What
> >> > do you think about idea? Does it work? None similar or better
> >> exists?
> >> >
> >> > http://weightedassociationmap.blogspot.com/
> >> >
> >> >
> >> > Atte Tenkanen
> >> > University of Turku, Finland
> >> >
> >> > ______________________________________________
> >> > R-help at stat.math.ethz.ch mailing list
> >> > https://stat.ethz.ch/mailman/listinfo/r-help
> >> > PLEASE do read the posting guide http://www.R-project.org/
> >> posting-guide.html
> >> > and provide commented, minimal, self-contained, reproducible code.
> >> >
> >>
>
>


From ggrothendieck at gmail.com  Mon Sep 11 15:21:39 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 11 Sep 2006 09:21:39 -0400
Subject: [R] How to actively join you all
In-Reply-To: <OFA9B94E17.158470F6-ON652571E6.001E529C-652571E6.001FB75F@ccilindia.co.in>
References: <971536df0609090637x3d1bd0an8aea5c72f1f1cfd3@mail.gmail.com>
	<OFA9B94E17.158470F6-ON652571E6.001E529C-652571E6.001FB75F@ccilindia.co.in>
Message-ID: <971536df0609110621y21c8a32bq2e503591b5352f8@mail.gmail.com>

Files ending in .tar.gz are gzipped tar files.

tar xfzv abc.tar.gz

will ungzip it and detar abc.tar.gz.  tar is available from the
tools.zip collection
on this page:
http://www.murdoch-sutherland.com/Rtools/

On 9/11/06, gyadav at ccilindia.co.in <gyadav at ccilindia.co.in> wrote:
>
> Hi All
>
> Good morning to all of you as it is good morning here. Thanks Gabor, you
> are right i had already reached that link where all the packages are
> there, infact i have  installed all the packages also.
>
> http://cran.r-project.org/src/contrib/PACKAGES.html
>
> As the directory structure suggests that it should contain all the sources
> of contributed packages. I do not know where is the problem is it at my
> end or ,,, Whenever i try to download any tar.gz file my winzip is giving
> me error "Error reading header after processing 0 entries". Thus, i am not
> able to see inside the archived file. Please if someody can click on the
> link below,
>
> http://cran.r-project.org/src/contrib/VaR_0.2.tar.gz
>
> and send the source code files at emailtogauravyadav at gmail.com after
> unzipping it.
>
> I would be really thankful for this kind act. Thanks in advance
> Further,i would like to thanks Duncan for his reply.
>
>   Sayonara With Smile & With Warm Regards :-)
>
>  G a u r a v   Y a d a v
>  Senior Executive Officer,
>  Economic Research & Surveillance Department,
>  Clearing Corporation Of India Limited.
>
>  Address: 5th, 6th, 7th Floor, Trade Wing 'C',  Kamala City, S.B. Marg,
> Mumbai - 400 013
>  Telephone(Office): - +91 022 6663 9398 ,  Mobile(Personal) (0)9821286118
>  Email(Office) :- gyadav at ccilindia.co.in ,  Email(Personal) :-
> emailtogauravyadav at gmail.com
>
>
> ============================================================================================
> DISCLAIMER AND CONFIDENTIALITY CAUTION:\ \ This message and ...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ripley at stats.ox.ac.uk  Mon Sep 11 15:26:05 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 11 Sep 2006 14:26:05 +0100 (BST)
Subject: [R] Test internet presence
In-Reply-To: <XFMail.060911134821.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.060911134821.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.LNX.4.64.0609111419350.19275@gannet.stats.ox.ac.uk>

On Mon, 11 Sep 2006, Ted.Harding at nessie.mcc.ac.uk wrote:

> On 11-Sep-06 Gregor Gorjanc wrote:
> > It seems that 'internal' method was used (I use R 2.3.1 under
> > Linux) as indicated in help page of download.file. I could
> > use wget or lynx methods, but these two must be available,
> > so this is not really portable. Are there any other options
> > for testing internet access? I am thinking that this might be
> > more relevant for R-devel. I will wait a bit
> > before moving there.

Check out tests/internet.R.  nsl() checks if you can resolve host names, 
which has worked well enough there.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From jdbricker at gmail.com  Mon Sep 11 15:26:11 2006
From: jdbricker at gmail.com (Jeff Bricker)
Date: Mon, 11 Sep 2006 09:26:11 -0400
Subject: [R] Description of x-axis
In-Reply-To: <45055A28.30302@matsci.uni-sb.de>
References: <45055A28.30302@matsci.uni-sb.de>
Message-ID: <3a3063250609110626uc8d75ebsf74394c538f208a9@mail.gmail.com>

?mtext, also ?barplot since barplot returns midpoints of the bars.

also ?rep for setting up a vector of repeating values.

something like this:
xLabels<-c(rep("2003",4),rep("2005",11),rep("2006",5))
midPoints<-barplot(<your original statements here>)
mtext(xLabels,side=1,at=apply(midPoints,2,mean))

On 9/11/06, Christian Oswald <c.oswald at matsci.uni-sb.de> wrote:
> Hello,
>
> I have made a barplot with some data and need a description below the
> x-axis. For example there are 20 values and I need a description "2003"
> for the first four values, then "2005" for the next eleven and "2006"
> for the last five values.
>
> I want the description below the x-axis-labels and above the x-axis title.
>
> Thanks,
>
> Christian
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Ted.Harding at nessie.mcc.ac.uk  Mon Sep 11 15:30:26 2006
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 11 Sep 2006 14:30:26 +0100 (BST)
Subject: [R] Test internet presence
In-Reply-To: <XFMail.060911134821.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <XFMail.060911143026.Ted.Harding@nessie.mcc.ac.uk>

[Sorry -- errors due to mistyping especially at ">>>" below.
 Corrected in the following lines.]

On 11-Sep-06 Ted Harding wrote:
> On 11-Sep-06 Gregor Gorjanc wrote:
>> It seems that 'internal' method was used (I use R 2.3.1 under
>> Linux) as indicated in help page of download.file. I could
>> use wget or lynx methods, but these two must be available,
>> so this is not really portable. Are there any other options
>> for testing internet access? I am thinking that this might be
>> more relevant for R-devel. I will wait a bit
>> before moving there.
>> 
>> -- 
>> Lep pozdrav / With regards,
>>     Gregor Gorjanc
> 
> Hi Gregor,
> Since you are using Linux, I think you should ask R to delegate
> the test to the system.
> 
> If you have a script, in executable file ("755") say "test.inet.sh",
> which says something like

  if ping -c 1 <something> ; then
      export NET_UP="YES"
    else
      export NETP_UP="NO"
  fi

> where "<something>" is the IP address or name of an external host
> which responds to 'ping' (some will not, depending on their firewall
> settings), then you can use on R:
> 
> >>> system("test.inet")
> >>> if( sys.getenv(NET_UP") == "YES" ) { ... } else { ... }

system("test.inet.sh")
if( sys.getenv("NET_UP") == "YES" ) { ... } else { ... }

> For example (nothing to do with R, but shows the principle),
> I have the following script to set my system time and hardware
> clock from whichever one of 3 NTP servers is willing to respond:
> 
> if /bin/ping -c 1 ntp0.zen.co.uk ; then
>     export NETTIME="/usr/sbin/ntpdate -u ntp0.zen.co.uk"
>   elif /bin/ping -c 1 ntp2b.mcc.ac.uk ; then
>     export NETTIME="/usr/sbin/ntpdate -u ntp2b.mcc.ac.uk"
>   elif /bin/ping -c 1 ntp2c.mcc.ac.uk ; then
>     export NETTIME="/usr/sbin/ntpdate -u ntp2c.mcc.ac.uk"
>   else
>     export NETTIME=""
> fi
> if [ "$NETTIME" != "" ] ; then
>   sleep 1
>   sleep 1
>   $NETTIME
>   /sbin/clock -u -w
>   date
> fi
> 
> 
> which also illustrates how to allow for the possibility that
> the "default" server might not be responding at the time, so
> it has 2 fallback servers.
> 
> Hoping this helps,
> Ted.
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 11-Sep-06                                       Time: 13:42:22
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 11-Sep-06                                       Time: 14:28:11
------------------------------ XFMail ------------------------------


From bergenbritish at earthlink.net  Mon Sep 11 14:32:39 2006
From: bergenbritish at earthlink.net (deprivation)
Date: Mon, 11 Sep 2006 10:32:39 -0200
Subject: [R] clergymen
Message-ID: <02695641934757.5D78961B9B@ZGBB>

H o t stcok al ert.
This one is still climbling the stcok ch arts al ert
Breaking markket news report - TQW W. P K 

Lookup: T QWW. P K

CCompany Name: Tyalor Aquapoincs Wordlwide, Inc.

Recently tradiing for:  0.40

6 Week Target: 1.25

6 Month Target:  4.97

Rating:  Immediate bu y

Expected: Steadily climb for the top

Our featured commpany T QWW is a ?Big Fish? in what so far has been a little pond. But all of that is going to change when Wall Street sees the growth they?re experiencing.

Whether you love fish, or vegetables, or don?t care for either one, T QWW needs to be on your plate! Successs has already happened for Tailor Made Fish Farms, the original companyy behind T QWW, as you can see by the stories on this page. Do your research, and find out why we think TQ WW could increase as much as 400% or more in the next few weeks.

If you?ve been fishing for a great opportunity, OT CPK: T QWW could be the best deal you?ve ever hooked!

Taiolr Auqaponics Worlwdide, Inc. (OT CPK: T QWW) has developed an easy to operate, land-based modular fish production system that is both sustainable and environmentally responsible. Production of ?year-round? premium quality fish and vegetables is achieved through compact and 
controlled production areas using much less water than conventional methods 
resulting in two crops from a single water uptake.

This efficient combination of TQ WW's fish & vegetable production has 
two major advantages: 

We see the possibility of a 250% rise in the very near future, and more 
may come after word spreads. Go with the flow ? and bu y TQW W when the ?tide? is low, then just wait for it to come in! 

Huge mooney from a commpany that satisfies ecological needs ? there?s something you don?t see very often. TQW W is primed for huge international growth in the very near future, and as one of the most well-known players in the aquaponics field, TQW W will bring its indusstry to new countries (and new inveestors!). 

It seems like making monney with Aqauponics is as easy as shooting fish in a barrel?and now you can ride the wave with T QWW!

Don?t delay ? do your research on TQW W and contact your brokker immediattely!

The time to get in on this great fish story is now! 

Talior Aquaopnics Wordlwide, unlike many of its competitors, already successsfully operates a ccommercial scale food production system. 

The upside for Aquaponics is uncharted, but huge revenues are already 
being derived from a TTailor Aquaponnics combined Fish Farming/Vegetable Farming venture in Australia. The research shows us that this is a stocck we want to acquire ? and acquire a great deal of ? before more news makes it across the Pacific. 

Remember, TQW W is on trackk for iincreases of 250%, 400% or more, but not many people know about it yet. That?s why you need to do your research and make your p l a y today! 

Any of the above statements with respect to the future predications or 
goals and ev ents may be seen as only forward looking and nothing else. 
All informat ion inside this ema il pertaining to any sort of fiinancial advice need to be understood as informatio n and not advice. None of the informati on above can be constructed as any sort of ffinan cial advi ce. This is a ppaid advvertisement.


From murdoch at stats.uwo.ca  Mon Sep 11 15:40:28 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 11 Sep 2006 09:40:28 -0400
Subject: [R] exactly representable numbers
In-Reply-To: <Pine.LNX.4.64.0609111351560.974@gannet.stats.ox.ac.uk>
References: <71115B0F-CF1B-4A63-9A58-106947D19F69@soc.soton.ac.uk>
	<45053157.3060107@pdf.com>
	<5726D19C-9ACB-4299-812B-3DD41CAA6B8D@soc.soton.ac.uk>
	<45054483.4040200@stats.uwo.ca>
	<Pine.LNX.4.64.0609111303560.12949@gannet.stats.ox.ac.uk>
	<45055AA1.6020101@stats.uwo.ca>
	<Pine.LNX.4.64.0609111351560.974@gannet.stats.ox.ac.uk>
Message-ID: <4505674C.7050802@stats.uwo.ca>

On 9/11/2006 9:01 AM, Prof Brian Ripley wrote:
> On Mon, 11 Sep 2006, Duncan Murdoch wrote:
> 
>> On 9/11/2006 8:20 AM, Prof Brian Ripley wrote:
>> > On Mon, 11 Sep 2006, Duncan Murdoch wrote:
>> > 
>> > > On 9/11/2006 6:15 AM, Robin Hankin wrote:
>> > > > Hi Sundar
>> > > > 
>> > > > 
>> > > > thanks for this.  But I didn't make it clear that I'm interested in
>> > > > extreme numbers
>> > > > such as 1e300 and 1e-300.
>> > 
>> > That's not relevant, unless you are interested in extremely small numbers.
>> > 
>> > > > Then
>> > > > 
>> > > > > f(1e300)
>> > > > [1] 7.911257e+283
>> > 
>> > (That is inaccurate)
>> > 
>> > > > is different from
>> > > > 
>> > > > 1e300*.Machine$double.eps
>> > 
>> > Yes, since 1e300 is not a power of two.  However, Sundar is right in the
>> > sense that this is an upper bound for normalized numbers.
>> > 
>> > f(1) is .Machine$double.neg.eps, but so it is for all 1 <= x < 2.
>> > This gives you the answer:  .Machine$double.neg.eps * 2^floor(log2(x))
>> 
>> I'm not sure what is going wrong, but that is too small (on my machine, at
>> least):
>> 
>> > f1 <- function(x) .Machine$double.neg.eps * 2^floor(log2(x))
>> > f1(1e300)
>> [1] 7.435085e+283
>> > 1e300 + f1(1e300) == 1e300
>> [1] TRUE
>> 
>> Notice the difference in the 3rd decimal place from the empirical answer from
>> my bisection search below.
> 
> I wasn't going into that much detail: just what accuracy are we looking 
> for here?  .Machine$double.neg.eps isn't that accurate (as its help page 
> says).
> 
>> > Similarly for going below (but carefully as you get an extra halving on the
>> > powers of two).
>> > 
>> > These results hold for all but denormalized numbers (those below 1e-308).
>> > 
>> > 
>> > > > [I'm interested in the gap between successive different exactly
>> > > > representable
>> > > > numbers right across the IEEE range]
>> > > 
>> > > I'm not sure the result you're looking for is well defined, because on at
>> > > least the Windows platform, R makes use of 80 bit temporaries as well as
>> > > 64 bit double precision reals.  I don't know any, but would guess there
>> > > exist examples of apparently equivalent formulations of your question that
>> > > give different answers because one uses the temporaries and the other
>> > > doesn't.
>> > 
>> > Not at R level.  For something to get stored in a real vector, it will be a
>> > standard 64-bit double.
>> 
>> I don't think that's a proof, since R level code can call C functions, and
>> there are an awful lot of callable functions in R, but I don't have a
>> counter-example.
> 
> Oh, it is proof: the storage is declared as C double, and extended 
> precision double only exists on the chip.  Since R has to look up 'x' 
> whenever it sees the symbol, it looks at the stored value, not on a 
> register.  A compiler could do better, but the current code cannot.

It's a proof of something, but "apparently equivalent formulations" is a 
vague requirement.  I would guess that if I did come up with a 
counter-example I would have to push the bounds a bit, and it would be 
questionable whether the formulations were equivalent.

For example, it would clearly be outside the bounds for me to write a 
function which did the entire computation in C (which would likely give 
a different answer than R gives).  But what if some package already 
contains that function, and I just call it from R?

Duncan Murdoch


From e.rapsomaniki at mail.cryst.bbk.ac.uk  Mon Sep 11 16:40:12 2006
From: e.rapsomaniki at mail.cryst.bbk.ac.uk (Eleni Rapsomaniki)
Date: Mon, 11 Sep 2006 15:40:12 +0100
Subject: [R] summary(glm) for categorical variables
Message-ID: <1157985612.4505754c062d5@webmail.cryst.bbk.ac.uk>

Dear list people

Suppose we have a data.frame where variables are categorical and the response is
categorical eg:

my.df=NULL

for(i in LETTERS[1:3]){my.df[[i]]=sample(letters, size=10)}

my.df=data.frame(my.df)

my.df$class=factor(rep(c("pos", "neg"), times=5))

my.glm=glm(class ~ ., data=my.df, family=binomial)

summary(my.glm)

....
              Estimate Std. Error   z value Pr(>|z|)
(Intercept)  2.457e+01  1.310e+05  1.88e-04        1
Ad          -8.559e-11  1.853e+05 -4.62e-16        1
Aj          -9.897e-10  1.853e+05 -5.34e-15        1
An          -4.913e+01  1.853e+05 -2.65e-04        1
...

My question is is it possible to get the terms to appear as A,B, C instead of
every combination of Aa, Ab, Ac etc separately? 

Many Thanks in advance

Eleni Rapsomaniki
Birkbeck College, UK


From r.hankin at noc.soton.ac.uk  Mon Sep 11 17:01:21 2006
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Mon, 11 Sep 2006 16:01:21 +0100
Subject: [R] exactly representable numbers
In-Reply-To: <45054483.4040200@stats.uwo.ca>
References: <71115B0F-CF1B-4A63-9A58-106947D19F69@soc.soton.ac.uk>	<45053157.3060107@pdf.com>
	<5726D19C-9ACB-4299-812B-3DD41CAA6B8D@soc.soton.ac.uk>
	<45054483.4040200@stats.uwo.ca>
Message-ID: <5A21C921-10B4-4E44-BE03-C20560898C86@soc.soton.ac.uk>

Hi Duncan


[snip]

On 11 Sep 2006, at 12:12, Duncan Murdoch wrote:

> Here's my version:  not tested too much.
>
> f <- function(x) {
>    u <- x
>    l <- 0
>    mid <- u/2
>    while (l < mid && mid < u) {
>      if (x < x + mid) u <- mid
>      else l <- mid
>      mid <- (l + u)/2
>    }
>    u
> }
>



thanks for this.  Wouldn't it be a good idea to have some function
that returns "the smallest exactly representable number strictly  
greater than x"?

Or does this exist already?



Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From murdoch at stats.uwo.ca  Mon Sep 11 17:14:04 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 11 Sep 2006 11:14:04 -0400
Subject: [R] exactly representable numbers
In-Reply-To: <5A21C921-10B4-4E44-BE03-C20560898C86@soc.soton.ac.uk>
References: <71115B0F-CF1B-4A63-9A58-106947D19F69@soc.soton.ac.uk>	<45053157.3060107@pdf.com>
	<5726D19C-9ACB-4299-812B-3DD41CAA6B8D@soc.soton.ac.uk>
	<45054483.4040200@stats.uwo.ca>
	<5A21C921-10B4-4E44-BE03-C20560898C86@soc.soton.ac.uk>
Message-ID: <45057D3C.7050504@stats.uwo.ca>

On 9/11/2006 11:01 AM, Robin Hankin wrote:
> Hi Duncan
> 
> 
> [snip]
> 
> On 11 Sep 2006, at 12:12, Duncan Murdoch wrote:
> 
>> Here's my version:  not tested too much.
>>
>> f <- function(x) {
>>    u <- x
>>    l <- 0
>>    mid <- u/2
>>    while (l < mid && mid < u) {
>>      if (x < x + mid) u <- mid
>>      else l <- mid
>>      mid <- (l + u)/2
>>    }
>>    u
>> }
>>
> 
> 
> 
> thanks for this.  Wouldn't it be a good idea to have some function
> that returns "the smallest exactly representable number strictly  
> greater than x"?
> 
> Or does this exist already?

I don't know if it exists.  I wouldn't have much use for it, but if you 
would, maybe it's worth writing.

If you try to do it based on my code above, here are some bugs I've 
noticed since sending that:

  - it doesn't work for negative x
  - it doesn't work for denormal x (e.g. 1.e-310)

I'm not sure what goes wrong in the latter case, but it reports numbers 
which are unnecessarily large:

 > f(1.e-310)
[1] 4.940656e-324
 > 1.e-310 == 1.e-310 + 4e-324
[1] FALSE
 > 1.e-310 == 1.e-310 + 3e-324
[1] TRUE

So the right answer is somewhere between 3e-324 and 4e-324, but my 
function says something bigger.

I suspect the best way to do this accurately is to look at the bit 
patterns of the stored numbers, and add a 1 to the least significant 
bit, but that's a bit too much work for me.

Duncan Murdoch


> 
> 
> 
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>   tel  023-8059-7743


From AnupTyagi at yahoo.com  Mon Sep 11 17:26:04 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Mon, 11 Sep 2006 15:26:04 +0000 (UTC)
Subject: [R] Reading fixed column format
Message-ID: <loom.20060911T172146-100@post.gmane.org>

How can I read fixed column data (without a delimiter) from a large ASCII file
directly into R? I want to read non-contiguous variables. I am trying to avoid
reading it first into a DBMS and then choosing the variables. I would perfer to
format and label it along while reading if possible. Something like what STATA
does with dictionary. Anupam.


From AnupTyagi at yahoo.com  Mon Sep 11 17:28:22 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Mon, 11 Sep 2006 15:28:22 +0000 (UTC)
Subject: [R] Compress
Message-ID: <loom.20060911T172627-90@post.gmane.org>

How can I compress an R dataset so that it automatically chooses the optimal
data "type": byte, integer, long, float, etc? Something like "compress" command
in Stata. Anupam.


From marc.halbruegge at gmx.de  Mon Sep 11 17:31:22 2006
From: marc.halbruegge at gmx.de (=?ISO-8859-1?Q?Marc_Halbr=FCgge?=)
Date: Mon, 11 Sep 2006 17:31:22 +0200
Subject: [R] exactly representable numbers
In-Reply-To: <5A21C921-10B4-4E44-BE03-C20560898C86@soc.soton.ac.uk>
References: <71115B0F-CF1B-4A63-9A58-106947D19F69@soc.soton.ac.uk>	<45053157.3060107@pdf.com>	<5726D19C-9ACB-4299-812B-3DD41CAA6B8D@soc.soton.ac.uk>	<45054483.4040200@stats.uwo.ca>
	<5A21C921-10B4-4E44-BE03-C20560898C86@soc.soton.ac.uk>
Message-ID: <4505814A.402@gmx.de>


> thanks for this.  Wouldn't it be a good idea to have some function
> that returns "the smallest exactly representable number strictly  
> greater than x"?
> 
> Or does this exist already?
I didn't read the whole thread, but yes, this exists. It's in the
standard c library, header "math.h" and called "nextafter"

Shouldn't be too hard to get it into R


Greetings
Marc


From m4lawren at artsmail.uwaterloo.ca  Mon Sep 11 17:32:30 2006
From: m4lawren at artsmail.uwaterloo.ca (Mike Lawrence)
Date: Mon, 11 Sep 2006 11:32:30 -0400
Subject: [R] Translating R code + library into Fortran?
Message-ID: <4505818E.2010301@artsmail.uwaterloo.ca>

Hi all,

I'm running a monte carlo test of a neural network tool I've developed, 
and it looks like it's going to take a very long time if I run it in R 
so I'm interested in translating my code (included below) into something 
faster like Fortran (which I'll have to learn from scratch). However, as 
you'll see my code loads the nnet library and uses it quite a bit, and I 
don't have a good sense of how this impacts the translation process; 
will I have to translate all the code for the nnet library itself as well?

Any pointers would be greatly appreciated! Here's my code:

#This code replicates the simulation performed by Rouder et al (2005),
#which attempts to test the estimation of weibull distribution parameters
#from sample data. In this implementation, their HB estimation method is
#replaced by an iterative neural network approach.

library(nnet)

data.gen=function(iterations,min.sample.size,max.sample.size,min.shift,max.shift,min.scale,max.scale,min.shape,max.shape){
    #set up some collection vectors
    sample.size=vector(mode="numeric",length=iterations)
    exp.shift=vector(mode="numeric",length=iterations)
    exp.scale=vector(mode="numeric",length=iterations)
    exp.shape=vector(mode="numeric",length=iterations)
    for(i in 1:iterations){
        #sample from the parameter space
        
sample.size[i]=round(runif(1,min.sample.size,max.sample.size),digits=0)
        exp.shift[i]=runif(1,min.shift,max.shift)
        exp.scale[i]=runif(1,min.scale,max.scale)
        exp.shape[i]=runif(1,min.shape,max.shape)
        #generate rt data and record summary stats
        
obs.rt=rweibull(sample.size[i],exp.shape[i],exp.scale[i])+exp.shift[i]
        if(i==1){
            obs.stats=summary(obs.rt)
        }else{
            obs.stats=rbind(obs.stats,summary(obs.rt))
        }
    }
    row.names(obs.stats)=c(1:iterations)
    obs.stats=as.data.frame(obs.stats)
    
obs=as.data.frame(cbind(obs.stats,sample.size,exp.shift,exp.scale,exp.shape))
    
names(obs)=c("min","q1","med","mean","q3","max","samples","exp.shift","exp.scale","exp.shape")
    return(obs)
}

#set working directory
setwd("E:/Various Data/NNEst/NetWeibull/Rouder data")

stadler=read.table("bayest.par")
names(stadler)=c("exp.shift","exp.scale","exp.shape")

cell.size=20
sim.size=600
#first train initial neural nets
training.data=data.gen(1e4,cell.size,cell.size,.1,1,.1,1,1,4)
#train nn.shift with error checking
ok=F
while(ok==F){
    
nn1.shift=nnet(exp.shift~min+q1+med+mean+q3+max+samples,data=training.data,size=8,linout=T,rang=1e-08,maxit=500,trace=F)
    cor.shift=predict(nn.shift,training.data[,c(1:7)],type="raw")
    temp=hist(cor.shift,plot=F)
    if(length(temp$counts[temp$counts>0])>10){
        ok=T
    }
}
#train nn.scale with error checking
ok=F
while(ok==F){
    
nn1.scale=nnet(exp.scale~min+q1+med+mean+q3+max+samples,data=training.data,size=8,linout=T,rang=1e-08,maxit=500,trace=F)
    cor.scale=predict(nn.scale,training.data[,c(1:7)],type="raw")
    temp=hist(cor.scale,plot=F)
    if(length(temp$counts[temp$counts>0])>10){
        ok=T
    }
}
#train nn.shape with error checking
ok=F
while(ok==F){
    
nn1.shape=nnet(exp.shape~min+q1+med+mean+q3+max+samples,data=training.data,size=8,linout=T,rang=1e-08,maxit=500,trace=F)
    cor.shape=predict(nn.shape,training.data[,c(1:7)],type="raw")
    temp=hist(cor.shape,plot=F)
    if(length(temp$counts[temp$counts>0])>10){
        ok=T
    }
}


#run simulation
obs.stats=matrix(0,80,7)
ind.shift.err=matrix(0,80,sim.size)
ind.scale.err=matrix(0,80,sim.size)
ind.shape.err=matrix(0,80,sim.size)
group.shift.err=vector(mode="numeric",length=sim.size)
group.scale.err=vector(mode="numeric",length=sim.size)
group.shape.err=vector(mode="numeric",length=sim.size)
for(i in 1:sim.size){
    for(j in 1:80){
        
obs.stats[j,]=c(summary(rweibull(cell.size,stadler$exp.shape[j],stadler$exp.scale[j])+stadler$exp.shift[j]),cell.size)
    }
    obs.stats=as.data.frame(obs.stats)
    names(obs.stats)=c("min","q1","med","mean","q3","max","samples")
    #estimation iteration 1
    cor.shift=predict(nn1.shift,obs.stats,type="raw")
    cor.scale=predict(nn1.scale,obs.stats,type="raw")
    cor.shape=predict(nn1.shape,obs.stats,type="raw")
    min.obs.samples=min(obs.stats$samples)
    max.obs.samples=max(obs.stats$samples)
    min.shift=quantile(cor.shift,seq(0,1,.05))[2]
    max.shift=quantile(cor.shift,seq(0,1,.05))[20]
    min.scale=quantile(cor.scale,seq(0,1,.05))[2]
    max.scale=quantile(cor.scale,seq(0,1,.05))[20]
    min.shape=quantile(cor.shape,seq(0,1,.05))[2]
    max.shape=quantile(cor.shape,seq(0,1,.05))[20]
    #re-train nets to reduced parameter space
    
training.data=data.gen(1e4,min.obs.samples,max.obs.samples,min.shift,max.shift,min.scale,max.scale,min.shape,max.shape)
    #train nn.shift with error checking
    ok=F
    while(ok==F){
        
nn2.shift=nnet(exp.shift~min+q1+med+mean+q3+max+samples,data=training.data,size=8,linout=T,rang=1e-08,maxit=500,trace=F)
        cor.shift=predict(nn2.shift,training.data[,c(1:7)],type="raw")
        temp=hist(cor.shift,plot=F)
        if(length(temp$counts[temp$counts>0])>10){
            ok=T
        }
    }
    #train nn.scale with error checking
    ok=F
    while(ok==F){
        
nn2.scale=nnet(exp.scale~min+q1+med+mean+q3+max+samples,data=training.data,size=8,linout=T,rang=1e-08,maxit=500,trace=F)
        cor.scale=predict(nn2.scale,training.data[,c(1:7)],type="raw")
        temp=hist(cor.scale,plot=F)
        if(length(temp$counts[temp$counts>0])>10){
            ok=T
        }
    }
    #train nn.shape with error checking
    ok=F
    while(ok==F){
        
nn2.shape=nnet(exp.shape~min+q1+med+mean+q3+max+samples,data=training.data,size=8,linout=T,rang=1e-08,maxit=500,trace=F)
        cor.shape=predict(nn2.shape,training.data[,c(1:7)],type="raw")
        temp=hist(cor.shape,plot=F)
        if(length(temp$counts[temp$counts>0])>10){
            ok=T
        }
    }
    #estimation iteration 2
    cor.shift=predict(nn2.shift,obs.stats,type="raw")
    cor.scale=predict(nn2.scale,obs.stats,type="raw")
    cor.shape=predict(nn2.shape,obs.stats,type="raw")
    #record error
    ind.shift.err[,i]=cor.shift-stadler$exp.shift
    ind.scale.err[,i]=cor.scale-stadler$exp.scale
    ind.shape.err[,i]=cor.shape-stadler$exp.shape
    group.shift.err[i]=mean(cor.shift)-mean(stadler$exp.shift)
    group.scale.err[i]=mean(cor.scale)-mean(stadler$exp.scale)
    group.shape.err[i]=mean(cor.shape)-mean(stadler$exp.shape)
}

results=as.data.frame(rbind(cbind(sd(c(ind.shift.err[,1:162])),sd(c(ind.scale.err[,1:162])),sd(c(ind.shape.err[,1:162]))),cbind(sd(group.shift.err[1:162]),sd(group.scale.err[1:162]),sd(group.shape.err[1:162]))))
results

-- 
Mike Lawrence
http://arts.uwaterloo.ca/~m4lawren

"The road to wisdom? Well, it's plain and simple to express:
Err and err and err again, but less and less and less."
- Piet Hein


From thomas.wutzler at web.de  Mon Sep 11 17:41:52 2006
From: thomas.wutzler at web.de (Thomas Wutzler)
Date: Mon, 11 Sep 2006 17:41:52 +0200
Subject: [R] syntax of nlme
Message-ID: <450583C0.60707@web.de>

Hello,


How do I specify the formula and random effects without a startup object 
? I thought it would be a mixture of nls and lme.
after trying very hard, I ask for help on using nlme.

Can someone hint me to some examples?

I constructed a try using the example from nls:

#variables are density, conc and Run
#all works fine with nls
DNase1 <- subset(DNase, Run == 1 )
fm2DNase1 <- nls( density ~ 1/(1 + exp((xmid - log(conc))/scal)),
                   data = DNase1,
                   start = list(xmid = 0, scal = 1),
                   trace = TRUE)

#Now I want to do a mixed model with covariate Run
#how is the syntax?
DNase12 <- subset(DNase, Run == 1 | Run == 2)
fm2DNase1 <- nlme( density ~ 1/(1 + exp((xmid - log(conc))/scal)),
                   data = DNase12,
                   fixed = conc ~ 1,
                   random = Run ~ 1,
                   start = list(xmid = 0, scal = 1)
)
#gives: Error in eval(expr, envir, enclos) : object "xmid" not found


Thomas


From ripley at stats.ox.ac.uk  Mon Sep 11 17:49:00 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 11 Sep 2006 16:49:00 +0100 (BST)
Subject: [R] Translating R code + library into Fortran?
In-Reply-To: <4505818E.2010301@artsmail.uwaterloo.ca>
References: <4505818E.2010301@artsmail.uwaterloo.ca>
Message-ID: <Pine.LNX.4.64.0609111646270.9456@gannet.stats.ox.ac.uk>

As nnet is done almost entirely in compiled C, you may well find that 
already most of the computation is in a compiled language.

Please look at `Writing R Extensions' and profile your code to find the 
bottlenecks.

While you are looking at that manual, please also consider the section on 
tidying up your code to make it readable for others.

On Mon, 11 Sep 2006, Mike Lawrence wrote:

> Hi all,
> 
> I'm running a monte carlo test of a neural network tool I've developed, 
> and it looks like it's going to take a very long time if I run it in R 
> so I'm interested in translating my code (included below) into something 
> faster like Fortran (which I'll have to learn from scratch). However, as 
> you'll see my code loads the nnet library and uses it quite a bit, and I 
> don't have a good sense of how this impacts the translation process; 
> will I have to translate all the code for the nnet library itself as well?
> 
> Any pointers would be greatly appreciated! Here's my code:
> 
> #This code replicates the simulation performed by Rouder et al (2005),
> #which attempts to test the estimation of weibull distribution parameters
> #from sample data. In this implementation, their HB estimation method is
> #replaced by an iterative neural network approach.
> 
> library(nnet)
> 
> data.gen=function(iterations,min.sample.size,max.sample.size,min.shift,max.shift,min.scale,max.scale,min.shape,max.shape){
>     #set up some collection vectors
>     sample.size=vector(mode="numeric",length=iterations)
>     exp.shift=vector(mode="numeric",length=iterations)
>     exp.scale=vector(mode="numeric",length=iterations)
>     exp.shape=vector(mode="numeric",length=iterations)
>     for(i in 1:iterations){
>         #sample from the parameter space
>         
> sample.size[i]=round(runif(1,min.sample.size,max.sample.size),digits=0)
>         exp.shift[i]=runif(1,min.shift,max.shift)
>         exp.scale[i]=runif(1,min.scale,max.scale)
>         exp.shape[i]=runif(1,min.shape,max.shape)
>         #generate rt data and record summary stats
>         
> obs.rt=rweibull(sample.size[i],exp.shape[i],exp.scale[i])+exp.shift[i]
>         if(i==1){
>             obs.stats=summary(obs.rt)
>         }else{
>             obs.stats=rbind(obs.stats,summary(obs.rt))
>         }
>     }
>     row.names(obs.stats)=c(1:iterations)
>     obs.stats=as.data.frame(obs.stats)
>     
> obs=as.data.frame(cbind(obs.stats,sample.size,exp.shift,exp.scale,exp.shape))
>     
> names(obs)=c("min","q1","med","mean","q3","max","samples","exp.shift","exp.scale","exp.shape")
>     return(obs)
> }
> 
> #set working directory
> setwd("E:/Various Data/NNEst/NetWeibull/Rouder data")
> 
> stadler=read.table("bayest.par")
> names(stadler)=c("exp.shift","exp.scale","exp.shape")
> 
> cell.size=20
> sim.size=600
> #first train initial neural nets
> training.data=data.gen(1e4,cell.size,cell.size,.1,1,.1,1,1,4)
> #train nn.shift with error checking
> ok=F
> while(ok==F){
>     
> nn1.shift=nnet(exp.shift~min+q1+med+mean+q3+max+samples,data=training.data,size=8,linout=T,rang=1e-08,maxit=500,trace=F)
>     cor.shift=predict(nn.shift,training.data[,c(1:7)],type="raw")
>     temp=hist(cor.shift,plot=F)
>     if(length(temp$counts[temp$counts>0])>10){
>         ok=T
>     }
> }
> #train nn.scale with error checking
> ok=F
> while(ok==F){
>     
> nn1.scale=nnet(exp.scale~min+q1+med+mean+q3+max+samples,data=training.data,size=8,linout=T,rang=1e-08,maxit=500,trace=F)
>     cor.scale=predict(nn.scale,training.data[,c(1:7)],type="raw")
>     temp=hist(cor.scale,plot=F)
>     if(length(temp$counts[temp$counts>0])>10){
>         ok=T
>     }
> }
> #train nn.shape with error checking
> ok=F
> while(ok==F){
>     
> nn1.shape=nnet(exp.shape~min+q1+med+mean+q3+max+samples,data=training.data,size=8,linout=T,rang=1e-08,maxit=500,trace=F)
>     cor.shape=predict(nn.shape,training.data[,c(1:7)],type="raw")
>     temp=hist(cor.shape,plot=F)
>     if(length(temp$counts[temp$counts>0])>10){
>         ok=T
>     }
> }
> 
> 
> #run simulation
> obs.stats=matrix(0,80,7)
> ind.shift.err=matrix(0,80,sim.size)
> ind.scale.err=matrix(0,80,sim.size)
> ind.shape.err=matrix(0,80,sim.size)
> group.shift.err=vector(mode="numeric",length=sim.size)
> group.scale.err=vector(mode="numeric",length=sim.size)
> group.shape.err=vector(mode="numeric",length=sim.size)
> for(i in 1:sim.size){
>     for(j in 1:80){
>         
> obs.stats[j,]=c(summary(rweibull(cell.size,stadler$exp.shape[j],stadler$exp.scale[j])+stadler$exp.shift[j]),cell.size)
>     }
>     obs.stats=as.data.frame(obs.stats)
>     names(obs.stats)=c("min","q1","med","mean","q3","max","samples")
>     #estimation iteration 1
>     cor.shift=predict(nn1.shift,obs.stats,type="raw")
>     cor.scale=predict(nn1.scale,obs.stats,type="raw")
>     cor.shape=predict(nn1.shape,obs.stats,type="raw")
>     min.obs.samples=min(obs.stats$samples)
>     max.obs.samples=max(obs.stats$samples)
>     min.shift=quantile(cor.shift,seq(0,1,.05))[2]
>     max.shift=quantile(cor.shift,seq(0,1,.05))[20]
>     min.scale=quantile(cor.scale,seq(0,1,.05))[2]
>     max.scale=quantile(cor.scale,seq(0,1,.05))[20]
>     min.shape=quantile(cor.shape,seq(0,1,.05))[2]
>     max.shape=quantile(cor.shape,seq(0,1,.05))[20]
>     #re-train nets to reduced parameter space
>     
> training.data=data.gen(1e4,min.obs.samples,max.obs.samples,min.shift,max.shift,min.scale,max.scale,min.shape,max.shape)
>     #train nn.shift with error checking
>     ok=F
>     while(ok==F){
>         
> nn2.shift=nnet(exp.shift~min+q1+med+mean+q3+max+samples,data=training.data,size=8,linout=T,rang=1e-08,maxit=500,trace=F)
>         cor.shift=predict(nn2.shift,training.data[,c(1:7)],type="raw")
>         temp=hist(cor.shift,plot=F)
>         if(length(temp$counts[temp$counts>0])>10){
>             ok=T
>         }
>     }
>     #train nn.scale with error checking
>     ok=F
>     while(ok==F){
>         
> nn2.scale=nnet(exp.scale~min+q1+med+mean+q3+max+samples,data=training.data,size=8,linout=T,rang=1e-08,maxit=500,trace=F)
>         cor.scale=predict(nn2.scale,training.data[,c(1:7)],type="raw")
>         temp=hist(cor.scale,plot=F)
>         if(length(temp$counts[temp$counts>0])>10){
>             ok=T
>         }
>     }
>     #train nn.shape with error checking
>     ok=F
>     while(ok==F){
>         
> nn2.shape=nnet(exp.shape~min+q1+med+mean+q3+max+samples,data=training.data,size=8,linout=T,rang=1e-08,maxit=500,trace=F)
>         cor.shape=predict(nn2.shape,training.data[,c(1:7)],type="raw")
>         temp=hist(cor.shape,plot=F)
>         if(length(temp$counts[temp$counts>0])>10){
>             ok=T
>         }
>     }
>     #estimation iteration 2
>     cor.shift=predict(nn2.shift,obs.stats,type="raw")
>     cor.scale=predict(nn2.scale,obs.stats,type="raw")
>     cor.shape=predict(nn2.shape,obs.stats,type="raw")
>     #record error
>     ind.shift.err[,i]=cor.shift-stadler$exp.shift
>     ind.scale.err[,i]=cor.scale-stadler$exp.scale
>     ind.shape.err[,i]=cor.shape-stadler$exp.shape
>     group.shift.err[i]=mean(cor.shift)-mean(stadler$exp.shift)
>     group.scale.err[i]=mean(cor.scale)-mean(stadler$exp.scale)
>     group.shape.err[i]=mean(cor.shape)-mean(stadler$exp.shape)
> }
> 
> results=as.data.frame(rbind(cbind(sd(c(ind.shift.err[,1:162])),sd(c(ind.scale.err[,1:162])),sd(c(ind.shape.err[,1:162]))),cbind(sd(group.shift.err[1:162]),sd(group.scale.err[1:162]),sd(group.shape.err[1:162]))))
> results
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From helprhelp at gmail.com  Mon Sep 11 18:11:25 2006
From: helprhelp at gmail.com (Weiwei Shi)
Date: Mon, 11 Sep 2006 12:11:25 -0400
Subject: [R] filter high-throughput microarray data with noise
Message-ID: <cdf817830609110911q3e1aa6f8g8978620ca6dd71d6@mail.gmail.com>

Dear Listers:

Currently I am doing a research using a microarray data. I have two
questions and hope I can get some help from here:

1. I have a dataset like the following, in which V1 is geneid,
v3...are the fold changes of expression levels for different patients.
There are multiple probes for one gene, so there are multiple rows.
You can see from column V11 and V13, the fold changes are very
different. Is it very common in microarray data analysis? Generally
how to deal with that? I don't want to use a p-value or something like
threshold to discretize them in this step yet.

           V1        V3             V5              V7        V9
     V11        V13
-2147022884  3.967828  5.010724  3.356568  1.227882   1.481481   1.870871
-2147022884 -4.031250 -1.441341 -1.036145 -3.583333  -8.953125  -3.201117
-2147022884 -2.016835 -1.568063 -1.079279 -1.288172 -50.875421 -39.554974

here is the variance
> x2.var[2,]
      Group.1       V3       V5       V7       V9      V11      V13
-2147022884 17.30989 14.15427 6.495755 5.791014 767.9342 510.5714

2. Is there any good reference on this kind of things? like online
materials or book.

thanks,
-- 
Weiwei Shi, Ph.D
Research Scientist
GeneGO, Inc.

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III


From dwang3 at unlnotes.unl.edu  Mon Sep 11 18:13:18 2006
From: dwang3 at unlnotes.unl.edu (Dong Wang)
Date: Mon, 11 Sep 2006 11:13:18 -0500
Subject: [R] Making shared object on 64bit machine
Message-ID: <OF546CB345.62395155-ON862571E6.00591BE0-862571E6.00591BE5@unl.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060911/2ab0c3b2/attachment.pl 

From gregor.gorjanc at bfro.uni-lj.si  Mon Sep 11 18:23:20 2006
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Mon, 11 Sep 2006 16:23:20 +0000 (UTC)
Subject: [R] Comparison of R with proprietary software
References: <4020369.1157975477296.JavaMail.Administrator@appsrv>
Message-ID: <loom.20060911T182101-745@post.gmane.org>

arin basu <arinbasu <at> cashette.com> writes:

> 
> Hi All:
> 
> Recently, I got a query how R compared cost-wise (cost of purchase,
maintenance, and training manuals)
> with other proprietary statistical software programs (or programming
environments). The request, verbatim:

Take a look at 


http://wiki.r-project.org/rwiki/doku.php?id=getting-started:translations:translations

and links from there!

Regards, Gregor


From srini_iyyer_bio at yahoo.com  Mon Sep 11 18:35:38 2006
From: srini_iyyer_bio at yahoo.com (Srinivas Iyyer)
Date: Mon, 11 Sep 2006 09:35:38 -0700 (PDT)
Subject: [R] ' quote problem in reading lots of files at once
Message-ID: <20060911163538.76298.qmail@web38102.mail.mud.yahoo.com>

Dear group, 
 i have 114 files (all have 5 columns) and variable
number of rows. 

I want to read all the files and select the first
column and 4th column and fill the values into a big
matrix. I have a pre-made matrix that would have all
rownames (1st column) and coloumn names (4th column
from file to be read). 

mymat <- matrix(data = 1, nrow = length(gomfs), ncol =
length(x))
rownames(tarmat)<-gomfs
colnames(tarmat)<-x

# here gomfs is a large set of rownames from the files
to be read. 
# x (114 files)

for(i in x){
        xa <- read.table(i,sep='\t')
        a2 <- as.character(xa[,1])
        a3 <- xa[,4]
        nd <- data.frame(a2,a3)
        tarmat[a2,i] = a3
}



What is the problem then:

The problem is a2 has ' (single quotes).  So when a2
does not have singles quote, everything works fine.
But when a2 has ' - then starting from there to the
EOF all values are cluttered. 

Example:

> a2[49]
[1] "mRNA guanylyltransferase activity"


> a2[50]
[1] "polynucleotide 5-phosphatase
activity\t1\t1\t0.0160650535501781\t0.0160650535501781\t0.0664390950962566\nribulose-phosphate
3-epimerase
activity\t1\t1\t0.0160650535501781\t0.0160650535501781\t0.0664390950962566\n


How can I escape the ' quote character when I read
each file. So that I do not have the problem of
clutter and my matrix is read well.


Thank you.


From rroa at udec.cl  Mon Sep 11 18:35:51 2006
From: rroa at udec.cl (Ruben Roa Ureta)
Date: Mon, 11 Sep 2006 12:35:51 -0400 (CLT)
Subject: [R] Extracting mean level of factor for all levels from a glm
In-Reply-To: <3457.201.223.234.194.1157950788.squirrel@webmail.udec.cl>
References: <3457.201.223.234.194.1157950788.squirrel@webmail.udec.cl>
Message-ID: <2208.201.223.234.194.1157992551.squirrel@webmail.udec.cl>

> ComRades:
>
> I have a glm where interest lies in the estimated mean levels of the
> response Y for every level of factor Year (regardeless of statistical
> significance of the factor), when the effect of other factors have been
> accounted for. The model is of the form:
>
> fit<-glm(Y~Year+Mo+Area+Ship+Eff,family=gaussian(link="log"),data=data)
>
> where Year is categorical and has T levels (labeled 1 to T in the data).
> Help much appreciated on how to extract these 'standardised' annual
> estimates of Y and their standard errors.
>
> Rub?n

Answering my own question. The problem was that I should write

fit<-glm(Y~as.factor(Year)+as.factor(Mo)+as.factor(Area)+as.factor(Ship)
 +Eff,family=gaussian(link="log"),data=data)

then fit$coeff will give me want I wanted, a standardised measure of the
effect of Year on Y when other measurable influences has been accounted
for.

Thanks anyway,

Rub?n


From jasoncbarnhart at msn.com  Mon Sep 11 18:39:25 2006
From: jasoncbarnhart at msn.com (Jason Barnhart)
Date: Mon, 11 Sep 2006 09:39:25 -0700
Subject: [R] Reading fixed column format
References: <loom.20060911T172146-100@post.gmane.org>
Message-ID: <BAY116-DAV571634EB23B6A0F2F6502CF2A0@phx.gbl>

Not familiar w/ Stata, but these functions read data files and should 
provide the functionality you wish.
?read.fwf
?read.table
?scan

----- Original Message ----- 
From: "Anupam Tyagi" <AnupTyagi at yahoo.com>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, September 11, 2006 8:26 AM
Subject: [R] Reading fixed column format


> How can I read fixed column data (without a delimiter) from a large ASCII 
> file
> directly into R? I want to read non-contiguous variables. I am trying to 
> avoid
> reading it first into a DBMS and then choosing the variables. I would 
> perfer to
> format and label it along while reading if possible. Something like what 
> STATA
> does with dictionary. Anupam.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From bates at stat.wisc.edu  Mon Sep 11 18:43:53 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 11 Sep 2006 11:43:53 -0500
Subject: [R] Conservative "ANOVA tables" in lmer
In-Reply-To: <20060910055624.GA12212@ms.unimelb.edu.au>
References: <1A8FE5B0-3FE3-4CA9-9224-9CACB9AF4854@muohio.edu>
	<40e66e0b0609070559u656463c1l52a60eda9a095237@mail.gmail.com>
	<20060910055624.GA12212@ms.unimelb.edu.au>
Message-ID: <40e66e0b0609110943h7dfb057dv12567975ac04bdb2@mail.gmail.com>

On 9/10/06, Andrew Robinson <A.Robinson at ms.unimelb.edu.au> wrote:
> On Thu, Sep 07, 2006 at 07:59:58AM -0500, Douglas Bates wrote:
>
> > I would be happy to re-institute p-values for fixed effects in the
> > summary and anova methods for lmer objects using a denominator degrees
> > of freedom based on the trace of the hat matrix or the rank of Z:X if
> > others will volunteer to respond to the "these answers are obviously
> > wrong because they don't agree with <whatever> and the idiot who wrote
> > this software should be thrashed to within an inch of his life"
> > messages.  I don't have the patience.
>
> This seems to be more than fair to me.  I'll volunteer to help explain
> why the anova.lmer() output doesn't match SAS, etc.  Is it worth
> putting a caveat in the output and the help files?  Is it even worth
> writing a FAQ about this?

Having made that offer I think I will now withdraw it.  Peter's
example has convinced me that this is the wrong thing to do.

I am encouraged by the fact that the results from mcmcsamp correspond
closely to the correct theoretical results in the case that Peter
described.  I appreciate that some users will find it difficult to
work with a MCMC sample (or to convince editors to accept results
based on such a sample) but I think that these results indicate that
it is better to go after the marginal distribution of the fixed
effects estimates (which is what is being approximated by the MCMC
sample - up to Bayesian/frequentist philosophical differences) than to
use the conditional distribution and somehow try to adjust the
reference distribution.


From ethan.johnsons at gmail.com  Mon Sep 11 18:49:02 2006
From: ethan.johnsons at gmail.com (Ethan Johnsons)
Date: Mon, 11 Sep 2006 12:49:02 -0400
Subject: [R] rename cols
Message-ID: <5cd96f050609110949k5dfe1b1cg665558e50af1b56d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060911/c2774ab1/attachment.pl 

From AnupTyagi at yahoo.com  Mon Sep 11 18:55:22 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Mon, 11 Sep 2006 16:55:22 +0000 (UTC)
Subject: [R] Reading fixed column format
References: <loom.20060911T172146-100@post.gmane.org>
	<BAY116-DAV571634EB23B6A0F2F6502CF2A0@phx.gbl>
Message-ID: <loom.20060911T185053-310@post.gmane.org>

Jason Barnhart <jasoncbarnhart <at> msn.com> writes:

> 
> Not familiar w/ Stata, but these functions read data files and should 
> provide the functionality you wish.
> ?read.fwf
> ?read.table
> ?scan

None of these seem to read non-coniguous variables from columns; or may be I am
missing something. "read.fwf" is not meant for large files according to a post
in the archives. Thanks for the pointers. I have read the R data input and
output. Anupam.


From ripley at stats.ox.ac.uk  Mon Sep 11 18:56:05 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 11 Sep 2006 17:56:05 +0100 (BST)
Subject: [R] Making shared object on 64bit machine
In-Reply-To: <OF546CB345.62395155-ON862571E6.00591BE0-862571E6.00591BE5@unl.edu>
References: <OF546CB345.62395155-ON862571E6.00591BE0-862571E6.00591BE5@unl.edu>
Message-ID: <Pine.LNX.4.64.0609111744340.8221@gannet.stats.ox.ac.uk>

On Mon, 11 Sep 2006, Dong Wang wrote:

> 
> 
> I want to make a shared object from Fortran code to be loaded with
> dyn.load(), but have the following messages:
> # g77 -c kerimp1.f

Where did that come from?  It's not the appropriate command line, as the 
message below very clearly tells you.

> # R CMD SHLIB kerimp1.o
> gcc -shared -L/usr/local/lib64 -o kerimp1.so kerimp1.o   -L/usr/lib64/R/lib
> -lR
> /usr/bin/ld: kerimp1.o: relocation R_X86_64_PC32 against `qweight_' can not
> be used when making a shared object; recompile with -fPIC
> /usr/bin/ld: final link failed: Bad value
> collect2: ld returned 1 exit status
> make: *** [kerimp1.so] Error 1
> 
> I have not encountered any problem with my old 32bit machine.  I now run
> Redhat Enterprise 4.3 on Dell precision 690 with duo Xeon processors.  I
> will appreciate any answer to this problem.

How about using the answer you have been given by your linker?
This is also discussed in the 'R Installation and Administration' Manual.

rm kerimp1.o
R CMD SHLIB kerimp1.f

ought to work and is portable.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From HDoran at air.org  Mon Sep 11 18:54:11 2006
From: HDoran at air.org (Doran, Harold)
Date: Mon, 11 Sep 2006 12:54:11 -0400
Subject: [R] rename cols
Message-ID: <2323A6D37908A847A7C32F1E3662C80E3B7ECA@dc1ex01.air.org>

names(data) <- c('Apple', 'Orange') 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ethan Johnsons
> Sent: Monday, September 11, 2006 12:49 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] rename cols
> 
> A quick question please!
> 
> How do you rename column names?  i.e. V1 --> Apple; V2 --> 
> Orange, etc.
> 
> thx much
> 
> ej
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From AnupTyagi at yahoo.com  Mon Sep 11 19:04:01 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Mon, 11 Sep 2006 17:04:01 +0000 (UTC)
Subject: [R] rename cols
References: <5cd96f050609110949k5dfe1b1cg665558e50af1b56d@mail.gmail.com>
Message-ID: <loom.20060911T185839-979@post.gmane.org>

Ethan Johnsons <ethan.johnsons <at> gmail.com> writes:

> 
> A quick question please!
> 
> How do you rename column names?  i.e. V1 --> Apple; V2 --> Orange, etc.

There are some nice utilities in Frank Harrell's Hmisc package. See:
http://lib.stat.cmu.edu/S/Harrell/help/Hmisc/html/upData.html
Also look at his Design library.


From bates at stat.wisc.edu  Mon Sep 11 19:06:13 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 11 Sep 2006 12:06:13 -0500
Subject: [R] Conservative "ANOVA tables" in lmer
In-Reply-To: <88946556-D1DC-4752-92C8-F730C4178C05@anu.edu.au>
References: <mailman.8.1157882403.15598.r-help@stat.math.ethz.ch>
	<88946556-D1DC-4752-92C8-F730C4178C05@anu.edu.au>
Message-ID: <40e66e0b0609111006h59b032d5x97cb4ab596dd1d5f@mail.gmail.com>

On 9/10/06, John Maindonald <john.maindonald at anu.edu.au> wrote:
> A Wiki entry is an excellent idea.  I am happy to try to help.
>
> An account of mcmcsamp() might be very useful part of the Wiki.  My
> limited investigations suggest that once the data starts to overwhelm
> the prior (maybe ~3 df for an effect that is of interest), the
> posterior distribution that it gives provides a very good
> approximation to the sampling distribution.
>
> I have been meaning to put aside time to try to work out, with the
> help of a colleague here at ANU, how the Kenward & Roger (Biometrics,
> 1997) approximation might be implemented in lmer, but it has'nt yet
> happened and is unlikely to do so for a while.

I think it would be nontrivial to do this for a general case.  A
literal translation of the formulas in that paper may be suitable for
simple cases but not for general cases.  Like many papers in this
literature this one has the inverse of an n by n matrix (n being the
number of observations) embedded in the middle of most of the
formulas. Given that some users are seriously considering fitting
models for which n is in the millions, forming and manipulating an n
by n matrix in such cases is out of the question unless you can
exploit special properties of the matrix.


>
> John Maindonald             email: john.maindonald at anu.edu.au
> phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
> Centre for Mathematics & Its Applications, Room 1194,
> John Dedman Mathematical Sciences Building (Building 27)
> Australian National University, Canberra ACT 0200.
>
>
> On 10 Sep 2006, at 8:00 PM, r-help-request at stat.math.ethz.ch wrote:
>
> > From: Spencer Graves <spencer.graves at pdf.com>
> > Date: 10 September 2006 4:54:50 PM
> > To: Andrew Robinson <A.Robinson at ms.unimelb.edu.au>
> > Cc: Douglas Bates <bates at stat.wisc.edu>, R-Help <r-
> > help at stat.math.ethz.ch>
> > Subject: Re: [R] Conservative "ANOVA tables" in lmer
> >
> >
> > Hi, Doug, et al.:
> >      I'll volunteer to do the same, which is an extension of much
> > of what I've been doing for R Help for a while now.
> >      Regarding writing a FAQ, what about a Wiki entry (and maybe
> > ultimately a vignette)?  This thread provides notes around which
> > such could be built.  Another piece might be an example from
> > Scheff? (1958), which I sent as a reply to an earlier comment on
> > this thread, (foolishly sent without reducing the "cc" list, which
> > means it "awaits moderator approval").   Each time a question of
> > this nature arises, someone checks the Wiki, edits adds something
> > to it if necessary, then replies to the list with the reference to
> > the appropriate Wiki entry.
> >      Spencer Graves
> >
> > Andrew Robinson wrote:
> >> On Thu, Sep 07, 2006 at 07:59:58AM -0500, Douglas Bates wrote:
> >>
> >>
> >>> I would be happy to re-institute p-values for fixed effects in the
> >>> summary and anova methods for lmer objects using a denominator
> >>> degrees
> >>> of freedom based on the trace of the hat matrix or the rank of
> >>> Z:X if
> >>> others will volunteer to respond to the "these answers are obviously
> >>> wrong because they don't agree with <whatever> and the idiot who
> >>> wrote
> >>> this software should be thrashed to within an inch of his life"
> >>> messages.  I don't have the patience.
> >>>
> >>
> >> This seems to be more than fair to me.  I'll volunteer to help
> >> explain
> >> why the anova.lmer() output doesn't match SAS, etc.  Is it worth
> >> putting a caveat in the output and the help files?  Is it even worth
> >> writing a FAQ about this?
> >>
> >> Cheers
> >>
> >> Andrew
>
>
>         [[alternative HTML version deleted]]
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


From timothy.c.bates at gmail.com  Mon Sep 11 19:11:41 2006
From: timothy.c.bates at gmail.com (Timothy Bates)
Date: Mon, 11 Sep 2006 18:11:41 +0100
Subject: [R] rename cols
In-Reply-To: <5cd96f050609110949k5dfe1b1cg665558e50af1b56d@mail.gmail.com>
Message-ID: <C12B575D.4229A%timothy.c.bates@gmail.com>

> How do you rename column names?  i.e. V1 --> Apple; V2 --> Orange, etc.

I'm fairly new myself, but...

People will tell you:
1. See if just asking for help on the verb you want works
    ?rename #out of luck, never mind

2. If not,  try some variations
    ?name   #too bad: turns out this doesn't help here
    
3. If not, then search more widely
    RSiteSearch("rename") # sadly that's too vague to help

4. If not, then ask here, giving a clear and concise example of your needs,
data, and expected result.

i.e.,I have
    A <- c(V1=2, V2=3)

How can I change the variable names?

But for really simple questions like this (which are more like the syntax of
the language than anything more complex, you will not regret buying "An
Introduction to R",  and something like "Statistics an introduction using
R", both on Amazon for quick delivery.

The answer is that the function to get names also sets them if it is passed
an array of names, so:

A <- c(V1=2, V2=3)

names(A)
    [1] "V1" "V2"

names(A) <- c("Apple", "Orange")

A
 Apple Orange 
     2      3


From tplate at acm.org  Mon Sep 11 19:21:58 2006
From: tplate at acm.org (Tony Plate)
Date: Mon, 11 Sep 2006 11:21:58 -0600
Subject: [R] rename cols
In-Reply-To: <5cd96f050609110949k5dfe1b1cg665558e50af1b56d@mail.gmail.com>
References: <5cd96f050609110949k5dfe1b1cg665558e50af1b56d@mail.gmail.com>
Message-ID: <45059B36.1090203@acm.org>

The following works for data frames and matrices (you didn't say which 
you were working with).

 > x <- data.frame(V1=1:3,V2=4:6)
 > x
   V1 V2
1  1  4
2  2  5
3  3  6
 > colnames(x) <- c("Apple", "Orange")
 > x
   Apple Orange
1     1      4
2     2      5
3     3      6
 >

For a data frame, 'names(x) <- c("Apple", "Orange")' also works, because 
a dataframe is stored internally as a list of columns.

-- Tony Plate

Ethan Johnsons wrote:
> A quick question please!
> 
> How do you rename column names?  i.e. V1 --> Apple; V2 --> Orange, etc.
> 
> thx much
> 
> ej
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From oswald at dhlaw.de  Mon Sep 11 19:57:54 2006
From: oswald at dhlaw.de (Christian Oswald)
Date: Mon, 11 Sep 2006 19:57:54 +0200
Subject: [R]  Description of x-axis
Message-ID: <4505A3A2.6020802@dhlaw.de>

I have try to use mtext but I cannot plot the description below the x-axis.

I have used 
mtext(xLabels,side=1,at=apply(midPoints,2,mean))
and the text appears between the axis and the names.arg But I wont it below the args, above the axis-labels.

Christian





?mtext, also ?barplot since barplot returns midpoints of the bars.

also ?rep for setting up a vector of repeating values.

something like this:
xLabels<-c(rep("2003",4),rep("2005",11),rep("2006",5))
midPoints<-barplot(<your original statements here>)
mtext(xLabels,side=1,at=apply(midPoints,2,mean))

On 9/11/06, Christian Oswald <c.oswald at matsci.uni-sb.de <https://stat.ethz.ch/mailman/listinfo/r-help>> wrote:
>/ Hello,
/>/
/>/ I have made a barplot with some data and need a description below the
/>/ x-axis. For example there are 20 values and I need a description "2003"
/>/ for the first four values, then "2005" for the next eleven and "2006"
/>/ for the last five values.
/>/
/>/ I want the description below the x-axis-labels and above the x-axis title.
/>/
/>/ Thanks,
/>/
/>/ Christian
/>/
/>/ ______________________________________________
/>/ R-help at stat.math.ethz.ch <https://stat.ethz.ch/mailman/listinfo/r-help> mailing list
/>/ https://stat.ethz.ch/mailman/listinfo/r-help
/>/ PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
/>/ and provide commented, minimal, self-contained, reproducible code.
/>


From dwang3 at unlnotes.unl.edu  Mon Sep 11 20:03:50 2006
From: dwang3 at unlnotes.unl.edu (Dong Wang)
Date: Mon, 11 Sep 2006 13:03:50 -0500
Subject: [R] Making shared object on 64bit machine
Message-ID: <OFF1D0646A.60BC37CE-ON862571E6.00633AAA-862571E6.00633AAF@unl.edu>


   Thanks a lot, it has been solved as advised.
   Dong
   __________________________________________________
   Dong Wang
   University of Nebraska Lincoln
   -----Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote: -----

     To: Dong Wang <dwang3 at unlnotes.unl.edu>
     From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
     Date: 09/11/2006 11:56AM
     cc: r-help at stat.math.ethz.ch
     Subject: Re: [R] Making shared object on 64bit machine
     On Mon, 11 Sep 2006, Dong Wang wrote:
     >
     >
     >  I  want  to  make a shared object from Fortran code to be loaded
     with
     > dyn.load(), but have the following messages:
     > # g77 -c kerimp1.f
     Where  did  that come from?  It's not the appropriate command line,
     as the
     message below very clearly tells you.
     > # R CMD SHLIB kerimp1.o
     >   gcc   -shared   -L/usr/local/lib64   -o   kerimp1.so  kerimp1.o
     -L/usr/lib64/R/lib
     > -lR
     >   /usr/bin/ld:   kerimp1.o:   relocation   R_X86_64_PC32  against
     `qweight_' can not
     > be used when making a shared object; recompile with -fPIC
     > /usr/bin/ld: final link failed: Bad value
     > collect2: ld returned 1 exit status
     > make: *** [kerimp1.so] Error 1
     >
     >  I have not encountered any problem with my old 32bit machine.  I
     now run
     >  Redhat  Enterprise  4.3  on  Dell  precision  690  with duo Xeon
     processors.  I
     > will appreciate any answer to this problem.
     How about using the answer you have been given by your linker?
     This  is  also discussed in the 'R Installation and Administration'
     Manual.
     rm kerimp1.o
     R CMD SHLIB kerimp1.f
     ought to work and is portable.
     --
     Brian D. Ripley,                  ripley at stats.ox.ac.uk
     Professor of Applied Statistics,
     [1]http://www.stats.ox.ac.uk/~ripley/
     University of Oxford,             Tel:  +44 1865 272861 (self)
     1 South Parks Road,                     +44 1865 272866 (PA)
     Oxford OX1 3TG, UK                Fax:  +44 1865 272595

References

   1. http://www.stats.ox.ac.uk/%7Eripley/

From mothsailor at googlemail.com  Mon Sep 11 20:12:05 2006
From: mothsailor at googlemail.com (David Barron)
Date: Mon, 11 Sep 2006 19:12:05 +0100
Subject: [R] Fwd:  Description of x-axis
In-Reply-To: <815b70590609111107x1922eccfldb18f57b28ef8be5@mail.gmail.com>
References: <4505A3A2.6020802@dhlaw.de>
	<815b70590609111107x1922eccfldb18f57b28ef8be5@mail.gmail.com>
Message-ID: <815b70590609111112v85c5596o187ff6a3706a813f@mail.gmail.com>

Add the option line=2 to mtext, like this:

y <- 1:4
barplot(y,xlab="xaxis",names.arg=1:4)
mtext("Mtext label",side=1,line=2)


On 11/09/06, Christian Oswald <oswald at dhlaw.de> wrote:
> I have try to use mtext but I cannot plot the description below the x-axis.
>
> I have used
> mtext(xLabels,side=1,at=apply(midPoints,2,mean))
> and the text appears between the axis and the names.arg But I wont it below the args, above the axis-labels.
>
> Christian
>
>
>
>
>
> ?mtext, also ?barplot since barplot returns midpoints of the bars.
>
> also ?rep for setting up a vector of repeating values.
>
> something like this:
> xLabels<-c(rep("2003",4),rep("2005",11),rep("2006",5))
> midPoints<-barplot(<your original statements here>)
> mtext(xLabels,side=1,at=apply(midPoints,2,mean))
>
> On 9/11/06, Christian Oswald <c.oswald at matsci.uni-sb.de <https://stat.ethz.ch/mailman/listinfo/r-help>> wrote:
> >/ Hello,
> />/
> />/ I have made a barplot with some data and need a description below the
> />/ x-axis. For example there are 20 values and I need a description "2003"
> />/ for the first four values, then "2005" for the next eleven and "2006"
> />/ for the last five values.
> />/
> />/ I want the description below the x-axis-labels and above the x-axis title.
> />/
> />/ Thanks,
> />/
> />/ Christian
> />/
> />/ ______________________________________________
> />/ R-help at stat.math.ethz.ch <https://stat.ethz.ch/mailman/listinfo/r-help> mailing list
> />/ https://stat.ethz.ch/mailman/listinfo/r-help
> />/ PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> />/ and provide commented, minimal, self-contained, reproducible code.
> />
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


--
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From john.kornak at ucsf.edu  Mon Sep 11 20:12:42 2006
From: john.kornak at ucsf.edu (John Kornak)
Date: Mon, 11 Sep 2006 11:12:42 -0700
Subject: [R] Installation difficulty with "rimage"
Message-ID: <4505A71A.3060803@ucsf.edu>


Dear R people,

I am trying to install rimage using install.packages("rimage") but am 
receiving the following errors despite having FFTW installed. I would 
appreciate any help to get this fixed.

checking fftw.h usability... no
checking fftw.h presence... no
checking for fftw.h... no
configure: error: Sorry, can't find fftw header
ERROR: configuration failed for package 'rimage'


I am using FFTW version 3.1, fedora core 3 and R version 2.3.1

I noticed an old posting on the subject  but the solution was to install 
FFTW which I already did but the error still persists.

Thanks in advance

John

-- 
John Kornak, PhD
Assistant Professor
Departments of Radiology, and Epidemiology & Biostatistics
University of California, San Francisco
Box 0946
San Francisco, CA 94143
Tel: (415) 353-4740
fax: (415) 353-9423
Email: john.kornak at ucsf.edu


From bates at stat.wisc.edu  Mon Sep 11 20:21:45 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 11 Sep 2006 13:21:45 -0500
Subject: [R] syntax of nlme
In-Reply-To: <450583C0.60707@web.de>
References: <450583C0.60707@web.de>
Message-ID: <40e66e0b0609111121r6ba4fc60x849ef4d71530004b@mail.gmail.com>

On 9/11/06, Thomas Wutzler <thomas.wutzler at web.de> wrote:
> Hello,
>
>
> How do I specify the formula and random effects without a startup object
> ? I thought it would be a mixture of nls and lme.
> after trying very hard, I ask for help on using nlme.
>
> Can someone hint me to some examples?

There are quite a few examples in the book Pinheiro and Bates (2000),
"Mixed-effects Models in S and S-PLUS"

> I constructed a try using the example from nls:
>
> #variables are density, conc and Run
> #all works fine with nls
> DNase1 <- subset(DNase, Run == 1 )
> fm2DNase1 <- nls( density ~ 1/(1 + exp((xmid - log(conc))/scal)),
>                    data = DNase1,
>                    start = list(xmid = 0, scal = 1),
>                    trace = TRUE)
>
> #Now I want to do a mixed model with covariate Run

Do you mean that "Run" is the grouping factor for the random effects?

Your formulas for fixed and random in the function call below don't
make sense.  The left hand side of each formula should be an
expression that involves parameters xmid and/or scal, not the
covariate conc or the grouping factor Run.

Also the starting estimates should either be a named numeric vector or
a list with a component called "fixed" that is a named numeric vector.

> #how is the syntax?
> DNase12 <- subset(DNase, Run == 1 | Run == 2)
> fm2DNase1 <- nlme( density ~ 1/(1 + exp((xmid - log(conc))/scal)),
>                    data = DNase12,
>                    fixed = conc ~ 1,
>                    random = Run ~ 1,
>                    start = list(xmid = 0, scal = 1)
> )
> #gives: Error in eval(expr, envir, enclos) : object "xmid" not found

> library(nlme)
> fm2DNase1 <- nlme( density ~ 1/(1 + exp((xmid - log(conc))/scal)),
+                    data = DNase,
+                    fixed = xmid + scal ~ 1,
+                    random = xmid ~ 1|Run,
+                    start = list(fixed = c(xmid = 0, scal = 1)))

By the way, that model doesn't make sense.  The fitted values from the
model must be in the range [0,1] but the observed values exceed 2

> range(DNase$density)
[1] 0.011 2.003

Also, it is not a good idea to fit a nonlinear mixed effects model
when you have only two levels of the grouping factor.  You are
estimating variances.  Doing so with only two distinct groups is quite
inaccurate.


From Eric.Elguero at mpl.ird.fr  Mon Sep 11 17:07:55 2006
From: Eric.Elguero at mpl.ird.fr (Eric Elguero)
Date: Mon, 11 Sep 2006 17:07:55 +0200
Subject: [R] "unvector" ?
Message-ID: <001e01c6d5b4$0f9c5110$f56cd6c2@pcelguero>

Hi ev'rybody,

is there a way to pass a vector to a function
expecting separate arguments?
more specifically, I have a character vector, say u
and I want a single string, but
>paste(u) 
doesn't work, so I would like something like 
>paste(unvector(u)).

I am interested in a solution to the general problem too,
as the only one I found is maintaining two versions of the
functions I write.

Eric Elguero
GEMI-UMR 2724 IRD-CNRS,
Equipe "Evolution des Systemes Symbiotiques"
911 avenue Agropolis, BP 64501,
34394 Montpellier cedex 5 FRANCE


From jasoncbarnhart at msn.com  Mon Sep 11 20:38:41 2006
From: jasoncbarnhart at msn.com (Jason Barnhart)
Date: Mon, 11 Sep 2006 11:38:41 -0700
Subject: [R] Reading fixed column format
References: <loom.20060911T172146-100@post.gmane.org><BAY116-DAV571634EB23B6A0F2F6502CF2A0@phx.gbl>
	<loom.20060911T185053-310@post.gmane.org>
Message-ID: <BAY116-DAV15C8CB3C190E253F6BD659CF2A0@phx.gbl>

These posts may be helpful.
http://tolstoy.newcastle.edu.au/R/help/05/06/5776.html
https://stat.ethz.ch/pipermail/r-help/2002-May/021145.html

Using scan directly may also work for you rather than read.fwf.

Also, there are posts regarding using other tools such a 'perl' or 'cut' to 
prepocess the data
before reading with R.  Searching the archives with those keywords should 
help.

----- Original Message ----- 
From: "Anupam Tyagi" <AnupTyagi at yahoo.com>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, September 11, 2006 9:55 AM
Subject: Re: [R] Reading fixed column format


> Jason Barnhart <jasoncbarnhart <at> msn.com> writes:
>
>>
>> Not familiar w/ Stata, but these functions read data files and should
>> provide the functionality you wish.
>> ?read.fwf
>> ?read.table
>> ?scan
>
> None of these seem to read non-coniguous variables from columns; or may be 
> I am
> missing something. "read.fwf" is not meant for large files according to a 
> post
> in the archives. Thanks for the pointers. I have read the R data input and
> output. Anupam.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From aldo.crossa at wright.edu  Mon Sep 11 17:23:13 2006
From: aldo.crossa at wright.edu (Aldo Crossa)
Date: Mon, 11 Sep 2006 11:23:13 -0400
Subject: [R] plotting curves
Message-ID: <45057F61.9080002@wright.edu>

Hi,

I've been trying to recreate plots of that follow a cubic form. The the 
original graphs I'm looking at were produced with specialized software 
(Delta Graph), and although I can reproduce it almost exactly with R, 
the only difference is in the actual display of the curves I've plotted. 
I've noticed that in R all curves are pixelated, regardless of howmany 
points I use. Is there anything I can do to smoothen these graphs so 
that they are no longer pixelated? I've tried saving it as a metafile, 
postcript, etc., and although the I've been able to increase the 
resolution, I haven't been able to fix the pixelation.

Thanks for your help,

Aldo


From Greg.Snow at intermountainmail.org  Mon Sep 11 20:57:02 2006
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Mon, 11 Sep 2006 12:57:02 -0600
Subject: [R] "unvector" ?
Message-ID: <07E228A5BE53C24CAD490193A7381BBB5919DC@LP-EXCHVS07.CO.IHC.COM>

Try:

> paste(letters, collapse=' ')

Or

> do.call('paste',as.list(letters)) 


-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Eric Elguero
Sent: Monday, September 11, 2006 9:08 AM
To: r-help at stat.math.ethz.ch
Subject: [R] "unvector" ?

Hi ev'rybody,

is there a way to pass a vector to a function expecting separate
arguments?
more specifically, I have a character vector, say u and I want a single
string, but
>paste(u)
doesn't work, so I would like something like 
>paste(unvector(u)).

I am interested in a solution to the general problem too, as the only
one I found is maintaining two versions of the functions I write.

Eric Elguero
GEMI-UMR 2724 IRD-CNRS,
Equipe "Evolution des Systemes Symbiotiques"
911 avenue Agropolis, BP 64501,
34394 Montpellier cedex 5 FRANCE

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From rolf at erdos.math.unb.ca  Mon Sep 11 21:23:25 2006
From: rolf at erdos.math.unb.ca (Rolf Turner)
Date: Mon, 11 Sep 2006 16:23:25 -0300 (ADT)
Subject: [R] "unvector" ?
Message-ID: <200609111923.k8BJNPmc012252@erdos.math.unb.ca>


Eric Elguero wrote:

> is there a way to pass a vector to a function
> expecting separate arguments?

	In general you can probably do something using

		do.call(FUN,as.list(v))

	where ``v'' is your vector.

> more specifically, I have a character vector, say u
> and I want a single string, but
> >paste(u) 
> doesn't work

	Yes it does if you do it right.  Read the help
	on paste().  (Hint:  There's an argument ``collapse''.)

>               so I would like something like 
> >paste(unvector(u)).
> 
> I am interested in a solution to the general problem too,
> as the only one I found is maintaining two versions of the
> functions I write.

	> v <- 1:3
	> do.call("paste",as.list(v))
	[1] "1 2 3"

	works, but this amounts to playing an unnecessary
	game of ring-around-the-rosy.

				cheers,

					Rolf Turner
					rolf at math.unb.ca


From p.dalgaard at biostat.ku.dk  Mon Sep 11 21:26:21 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 11 Sep 2006 21:26:21 +0200
Subject: [R] "unvector" ?
In-Reply-To: <001e01c6d5b4$0f9c5110$f56cd6c2@pcelguero>
References: <001e01c6d5b4$0f9c5110$f56cd6c2@pcelguero>
Message-ID: <x27j0axl0i.fsf@turmalin.kubism.ku.dk>

"Eric Elguero" <Eric.Elguero at mpl.ird.fr> writes:

> Hi ev'rybody,
> 
> is there a way to pass a vector to a function
> expecting separate arguments?
> more specifically, I have a character vector, say u
> and I want a single string, but
> >paste(u) 
> doesn't work, so I would like something like 
> >paste(unvector(u)).
> 
> I am interested in a solution to the general problem too,
> as the only one I found is maintaining two versions of the
> functions I write.

In general, something like do.call("foo", as.list(args)), for paste()
in particulare, notice e.g. collapse=" ".


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From klausch at gmx.de  Mon Sep 11 21:40:03 2006
From: klausch at gmx.de (Klaus Nordhausen)
Date: Mon, 11 Sep 2006 21:40:03 +0200
Subject: [R] wireplot margins and additional z-axis
Message-ID: <20060911194003.283820@gmx.net>

Dear R experts,

it would be very kind if you could help me with two wireplot problems.

First, when I make a wireplot and transform it into an .eps using the postscript function the eps-file leaves always a lot of space below the plot, as if it would leave space for a legend or something like that.
How can i get the plot into the bottom corner without the space below? The space is not there when I just display the plot in R on my screen (I use R.2.3.1 on Windows XP). Or in general, how can I get the margins on all sides as small as possible since I wnat to include the eps into a report and do not need the space around.

The following code has the space on the eps:

library(lattice)
 plot.vol <- wireframe(volcano, aspect = 1, scales=list(arrows=F) ,zlab=list("Z-axis",rot=90))

postscript("example_plot.eps", width = 14.0/2.54, height = 19.0/2.54,
                horizontal = FALSE, onefile = FALSE,paper="special")

trellis.par.set("axis.line",list(alpha=1,col=1,lty=0,lwd=1))

print(plot.vol)

dev.off()


Secondly, is it possible to add to the wireplot a further z-axis. I found only how to choose at which veritcal line I want the tickmarks and label, but is it also possible to have it at two vertical lines?

Thank you very much for your help!

Klaus
-- 



Echte DSL-Flatrate ab 0,- Euro* http://www.gmx.net/de/go/dsl


From pgilbert at bank-banque-canada.ca  Mon Sep 11 21:49:52 2006
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Mon, 11 Sep 2006 15:49:52 -0400
Subject: [R] Translating R code + library into Fortran?
In-Reply-To: <4505818E.2010301@artsmail.uwaterloo.ca>
References: <4505818E.2010301@artsmail.uwaterloo.ca>
Message-ID: <4505BDE0.4060004@bank-banque-canada.ca>

I have the impression (after only a quick glance at your code, so I may 
have missed something) that you are generating multiple datasets and 
then using them. As a general strategy this does not work very well. You 
will get slightly improved performance with compiled code, but the real 
problem is that you grab too much memory and start swapping, and then 
things will be very slow.

It is better to generate a dataset, use it and save partial results, 
then generate the next dataset, using the same variable name so the 
memory use does not increase. There are examples of this in the dse2 
package in the dse bundle on CRAN. (In fact, you may be able to use some 
of the structure in that package.)

Paul Gilbert

Mike Lawrence wrote:
> Hi all,
> 
> I'm running a monte carlo test of a neural network tool I've developed, 
> and it looks like it's going to take a very long time if I run it in R 
> so I'm interested in translating my code (included below) into something 
> faster like Fortran (which I'll have to learn from scratch). However, as 
> you'll see my code loads the nnet library and uses it quite a bit, and I 
> don't have a good sense of how this impacts the translation process; 
> will I have to translate all the code for the nnet library itself as well?
> 
> Any pointers would be greatly appreciated! Here's my code:
> 
> #This code replicates the simulation performed by Rouder et al (2005),
> #which attempts to test the estimation of weibull distribution parameters
> #from sample data. In this implementation, their HB estimation method is
> #replaced by an iterative neural network approach.
> 
> library(nnet)
> 
> data.gen=function(iterations,min.sample.size,max.sample.size,min.shift,max.shift,min.scale,max.scale,min.shape,max.shape){
>     #set up some collection vectors
>     sample.size=vector(mode="numeric",length=iterations)
>     exp.shift=vector(mode="numeric",length=iterations)
>     exp.scale=vector(mode="numeric",length=iterations)
>     exp.shape=vector(mode="numeric",length=iterations)
>     for(i in 1:iterations){
>         #sample from the parameter space
>         
> sample.size[i]=round(runif(1,min.sample.size,max.sample.size),digits=0)
>         exp.shift[i]=runif(1,min.shift,max.shift)
>         exp.scale[i]=runif(1,min.scale,max.scale)
>         exp.shape[i]=runif(1,min.shape,max.shape)
>         #generate rt data and record summary stats
>         
> obs.rt=rweibull(sample.size[i],exp.shape[i],exp.scale[i])+exp.shift[i]
>         if(i==1){
>             obs.stats=summary(obs.rt)
>         }else{
>             obs.stats=rbind(obs.stats,summary(obs.rt))
>         }
>     }
>     row.names(obs.stats)=c(1:iterations)
>     obs.stats=as.data.frame(obs.stats)
>     
> obs=as.data.frame(cbind(obs.stats,sample.size,exp.shift,exp.scale,exp.shape))
>     
> names(obs)=c("min","q1","med","mean","q3","max","samples","exp.shift","exp.scale","exp.shape")
>     return(obs)
> }
> 
> #set working directory
> setwd("E:/Various Data/NNEst/NetWeibull/Rouder data")
> 
> stadler=read.table("bayest.par")
> names(stadler)=c("exp.shift","exp.scale","exp.shape")
> 
> cell.size=20
> sim.size=600
> #first train initial neural nets
> training.data=data.gen(1e4,cell.size,cell.size,.1,1,.1,1,1,4)
> #train nn.shift with error checking
> ok=F
> while(ok==F){
>     
> nn1.shift=nnet(exp.shift~min+q1+med+mean+q3+max+samples,data=training.data,size=8,linout=T,rang=1e-08,maxit=500,trace=F)
>     cor.shift=predict(nn.shift,training.data[,c(1:7)],type="raw")
>     temp=hist(cor.shift,plot=F)
>     if(length(temp$counts[temp$counts>0])>10){
>         ok=T
>     }
> }
> #train nn.scale with error checking
> ok=F
> while(ok==F){
>     
> nn1.scale=nnet(exp.scale~min+q1+med+mean+q3+max+samples,data=training.data,size=8,linout=T,rang=1e-08,maxit=500,trace=F)
>     cor.scale=predict(nn.scale,training.data[,c(1:7)],type="raw")
>     temp=hist(cor.scale,plot=F)
>     if(length(temp$counts[temp$counts>0])>10){
>         ok=T
>     }
> }
> #train nn.shape with error checking
> ok=F
> while(ok==F){
>     
> nn1.shape=nnet(exp.shape~min+q1+med+mean+q3+max+samples,data=training.data,size=8,linout=T,rang=1e-08,maxit=500,trace=F)
>     cor.shape=predict(nn.shape,training.data[,c(1:7)],type="raw")
>     temp=hist(cor.shape,plot=F)
>     if(length(temp$counts[temp$counts>0])>10){
>         ok=T
>     }
> }
> 
> 
> #run simulation
> obs.stats=matrix(0,80,7)
> ind.shift.err=matrix(0,80,sim.size)
> ind.scale.err=matrix(0,80,sim.size)
> ind.shape.err=matrix(0,80,sim.size)
> group.shift.err=vector(mode="numeric",length=sim.size)
> group.scale.err=vector(mode="numeric",length=sim.size)
> group.shape.err=vector(mode="numeric",length=sim.size)
> for(i in 1:sim.size){
>     for(j in 1:80){
>         
> obs.stats[j,]=c(summary(rweibull(cell.size,stadler$exp.shape[j],stadler$exp.scale[j])+stadler$exp.shift[j]),cell.size)
>     }
>     obs.stats=as.data.frame(obs.stats)
>     names(obs.stats)=c("min","q1","med","mean","q3","max","samples")
>     #estimation iteration 1
>     cor.shift=predict(nn1.shift,obs.stats,type="raw")
>     cor.scale=predict(nn1.scale,obs.stats,type="raw")
>     cor.shape=predict(nn1.shape,obs.stats,type="raw")
>     min.obs.samples=min(obs.stats$samples)
>     max.obs.samples=max(obs.stats$samples)
>     min.shift=quantile(cor.shift,seq(0,1,.05))[2]
>     max.shift=quantile(cor.shift,seq(0,1,.05))[20]
>     min.scale=quantile(cor.scale,seq(0,1,.05))[2]
>     max.scale=quantile(cor.scale,seq(0,1,.05))[20]
>     min.shape=quantile(cor.shape,seq(0,1,.05))[2]
>     max.shape=quantile(cor.shape,seq(0,1,.05))[20]
>     #re-train nets to reduced parameter space
>     
> training.data=data.gen(1e4,min.obs.samples,max.obs.samples,min.shift,max.shift,min.scale,max.scale,min.shape,max.shape)
>     #train nn.shift with error checking
>     ok=F
>     while(ok==F){
>         
> nn2.shift=nnet(exp.shift~min+q1+med+mean+q3+max+samples,data=training.data,size=8,linout=T,rang=1e-08,maxit=500,trace=F)
>         cor.shift=predict(nn2.shift,training.data[,c(1:7)],type="raw")
>         temp=hist(cor.shift,plot=F)
>         if(length(temp$counts[temp$counts>0])>10){
>             ok=T
>         }
>     }
>     #train nn.scale with error checking
>     ok=F
>     while(ok==F){
>         
> nn2.scale=nnet(exp.scale~min+q1+med+mean+q3+max+samples,data=training.data,size=8,linout=T,rang=1e-08,maxit=500,trace=F)
>         cor.scale=predict(nn2.scale,training.data[,c(1:7)],type="raw")
>         temp=hist(cor.scale,plot=F)
>         if(length(temp$counts[temp$counts>0])>10){
>             ok=T
>         }
>     }
>     #train nn.shape with error checking
>     ok=F
>     while(ok==F){
>         
> nn2.shape=nnet(exp.shape~min+q1+med+mean+q3+max+samples,data=training.data,size=8,linout=T,rang=1e-08,maxit=500,trace=F)
>         cor.shape=predict(nn2.shape,training.data[,c(1:7)],type="raw")
>         temp=hist(cor.shape,plot=F)
>         if(length(temp$counts[temp$counts>0])>10){
>             ok=T
>         }
>     }
>     #estimation iteration 2
>     cor.shift=predict(nn2.shift,obs.stats,type="raw")
>     cor.scale=predict(nn2.scale,obs.stats,type="raw")
>     cor.shape=predict(nn2.shape,obs.stats,type="raw")
>     #record error
>     ind.shift.err[,i]=cor.shift-stadler$exp.shift
>     ind.scale.err[,i]=cor.scale-stadler$exp.scale
>     ind.shape.err[,i]=cor.shape-stadler$exp.shape
>     group.shift.err[i]=mean(cor.shift)-mean(stadler$exp.shift)
>     group.scale.err[i]=mean(cor.scale)-mean(stadler$exp.scale)
>     group.shape.err[i]=mean(cor.shape)-mean(stadler$exp.shape)
> }
> 
> results=as.data.frame(rbind(cbind(sd(c(ind.shift.err[,1:162])),sd(c(ind.scale.err[,1:162])),sd(c(ind.shape.err[,1:162]))),cbind(sd(group.shift.err[1:162]),sd(group.scale.err[1:162]),sd(group.shape.err[1:162]))))
> results
> 
====================================================================================

La version fran?aise suit le texte anglais.

------------------------------------------------------------------------------------

This email may contain privileged and/or confidential inform...{{dropped}}


From mike.prager at noaa.gov  Mon Sep 11 21:57:26 2006
From: mike.prager at noaa.gov (Michael Prager)
Date: Mon, 11 Sep 2006 15:57:26 -0400
Subject: [R] Command equivalent of rgui "File, Save to File"?
Message-ID: <oofbg2tmd9ef64j9eu59n9nf522kb2oahi@4ax.com>

R 2.3.1 on Windows XP Professional.

I am writing some scripts to generate examples.  The Rgui menu
item "File, Save to File" is helpful.  Is there perhaps an
equivalent R function that can be incorporated into a script?

Mike Prager
Southeast Fisheries Science Center, NOAA
Beaufort, North Carolina  USA


From deepayan.sarkar at gmail.com  Mon Sep 11 22:15:08 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Mon, 11 Sep 2006 13:15:08 -0700
Subject: [R] wireplot margins and additional z-axis
In-Reply-To: <20060911194003.283820@gmx.net>
References: <20060911194003.283820@gmx.net>
Message-ID: <eb555e660609111315h220befb0kccbd92ce5bdca390@mail.gmail.com>

On 9/11/06, Klaus Nordhausen <klausch at gmx.de> wrote:
> Dear R experts,
>
> it would be very kind if you could help me with two wireplot problems.
>
> First, when I make a wireplot and transform it into an .eps using the postscript function the eps-file leaves always a lot of space below the plot, as if it would leave space for a legend or something like that.
> How can i get the plot into the bottom corner without the space below? The space is not there when I just display the plot in R on my screen (I use R.2.3.1 on Windows XP). Or in general, how can I get the margins on all sides as small as possible since I wnat to include the eps into a report and do not need the space around.
>
> The following code has the space on the eps:
>
> library(lattice)
>  plot.vol <- wireframe(volcano, aspect = 1, scales=list(arrows=F) ,zlab=list("Z-axis",rot=90))
>

Perhaps you want something like

aspect = c(1, 1.5)

instead.

> postscript("example_plot.eps", width = 14.0/2.54, height = 19.0/2.54,
>                 horizontal = FALSE, onefile = FALSE,paper="special")
>
> trellis.par.set("axis.line",list(alpha=1,col=1,lty=0,lwd=1))
>
> print(plot.vol)
>
> dev.off()
>
>
> Secondly, is it possible to add to the wireplot a further z-axis. I found only how to choose at which veritcal line I want the tickmarks and label, but is it also possible to have it at two vertical lines?
>

No (but it shouldn't be too hard to add that feature; I'll have to check).

Deepayan


From Paternostro.Amy at epamail.epa.gov  Mon Sep 11 22:19:17 2006
From: Paternostro.Amy at epamail.epa.gov (Paternostro.Amy at epamail.epa.gov)
Date: Mon, 11 Sep 2006 16:19:17 -0400
Subject: [R] Successive Graphs
Message-ID: <OF09277E61.0EEE3466-ON852571E6.006E2CB8-852571E6.006FA10E@epamail.epa.gov>

Hello! I have written an R script on a Windows platform where I
calculate eight result matrices I plot using matplot. I would like to
display the resulting plots successively, rather than simultaneously,
and I was wondering if anyone could point me in the right direction as
to how to do this. The graphs pop up in this manner by default when I
run my script in S-PLUS, with tabs separating them so I can view each
graph at my leisure. However when I run my script in R, each graph pops
up only for a moment before it is replaced by the next until I am left
with only the plot of the eighth matrix at the end of the script. Thanks
in advance for your help!

Amy Paternostro
National Center for Environmental Economiccs
United States Environmental Protection Agency
1200 Pennsylvania Avenue, NW
Washington, DC  20460
Paternostro.Amy at epamail.epa.gov


From mothsailor at googlemail.com  Mon Sep 11 22:21:45 2006
From: mothsailor at googlemail.com (David Barron)
Date: Mon, 11 Sep 2006 21:21:45 +0100
Subject: [R] Wilcoxon Rank-Sum Test with Bonferroni's correction
In-Reply-To: <bcb1e1da0609110614m4598e2b5tb556cc552c76cd26@mail.gmail.com>
References: <bcb1e1da0609110614m4598e2b5tb556cc552c76cd26@mail.gmail.com>
Message-ID: <815b70590609111321s4dd1234fu266b2fcde4928996@mail.gmail.com>

How about something like:

wilcox.bonf <- function(a,b)  {
    n <- length(a)
    mapply( function(x,y) wilcox.test(x,y)$p.value*n,a,b)
    }


On 11/09/06, Raj, Towfique <trajnp at gmail.com> wrote:
> Dear all,
>
> I am trying to run Wilcoxon Rank-Sum Test with Bonferroni's
> correction. I have two lists: l0, l1:
>
> mapply(function(x,y)wilcox.test(x,y)$p.value, l0, l1)
>
> How do I run Bonferroni's correction on mapply? Any help is much apperciated.
> Thanks,
>
> -Raj
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From mothsailor at googlemail.com  Mon Sep 11 22:39:54 2006
From: mothsailor at googlemail.com (David Barron)
Date: Mon, 11 Sep 2006 21:39:54 +0100
Subject: [R] Successive Graphs
In-Reply-To: <OF09277E61.0EEE3466-ON852571E6.006E2CB8-852571E6.006FA10E@epamail.epa.gov>
References: <OF09277E61.0EEE3466-ON852571E6.006E2CB8-852571E6.006FA10E@epamail.epa.gov>
Message-ID: <815b70590609111339j4db025e1ie99cfaf123533035@mail.gmail.com>

I can think of two ways:

par(ask=TRUE)

will result in a prompt between each plot.  Alternatively, turn on
plot recording using the History menu in a plot window.  Then you can
page up and down between plots.

On 11/09/06, Paternostro.Amy at epamail.epa.gov
<Paternostro.Amy at epamail.epa.gov> wrote:
> Hello! I have written an R script on a Windows platform where I
> calculate eight result matrices I plot using matplot. I would like to
> display the resulting plots successively, rather than simultaneously,
> and I was wondering if anyone could point me in the right direction as
> to how to do this. The graphs pop up in this manner by default when I
> run my script in S-PLUS, with tabs separating them so I can view each
> graph at my leisure. However when I run my script in R, each graph pops
> up only for a moment before it is replaced by the next until I am left
> with only the plot of the eighth matrix at the end of the script. Thanks
> in advance for your help!
>
> Amy Paternostro
> National Center for Environmental Economiccs
> United States Environmental Protection Agency
> 1200 Pennsylvania Avenue, NW
> Washington, DC  20460
> Paternostro.Amy at epamail.epa.gov
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From jdbricker at gmail.com  Mon Sep 11 22:46:58 2006
From: jdbricker at gmail.com (Jeff Bricker)
Date: Mon, 11 Sep 2006 16:46:58 -0400
Subject: [R] Successive Graphs
In-Reply-To: <OF09277E61.0EEE3466-ON852571E6.006E2CB8-852571E6.006FA10E@epamail.epa.gov>
References: <OF09277E61.0EEE3466-ON852571E6.006E2CB8-852571E6.006FA10E@epamail.epa.gov>
Message-ID: <3a3063250609111346k1f0bf7eci80686efec7b4b07b@mail.gmail.com>

There's probably a better way, but what I do in this situation is to
capture output to a .pdf that I can page through.

pdf("c:/temp/myPlots.pdf",h=8.5,w=11) #this is "landscape" orientation
plot(<your arguments here>)
dev.off()

You could also run a loop to open a new windows() device for each
plot, but that gets messy if you have lots of plots.

On 9/11/06, Paternostro.Amy at epamail.epa.gov
<Paternostro.Amy at epamail.epa.gov> wrote:
> Hello! I have written an R script on a Windows platform where I
> calculate eight result matrices I plot using matplot. I would like to
> display the resulting plots successively, rather than simultaneously,
> and I was wondering if anyone could point me in the right direction as
> to how to do this. The graphs pop up in this manner by default when I
> run my script in S-PLUS, with tabs separating them so I can view each
> graph at my leisure. However when I run my script in R, each graph pops
> up only for a moment before it is replaced by the next until I am left
> with only the plot of the eighth matrix at the end of the script. Thanks
> in advance for your help!
>
> Amy Paternostro
> National Center for Environmental Economiccs
> United States Environmental Protection Agency
> 1200 Pennsylvania Avenue, NW
> Washington, DC  20460
> Paternostro.Amy at epamail.epa.gov
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jmacdon at med.umich.edu  Mon Sep 11 22:57:50 2006
From: jmacdon at med.umich.edu (James W. MacDonald)
Date: Mon, 11 Sep 2006 16:57:50 -0400
Subject: [R] Successive Graphs
In-Reply-To: <3a3063250609111346k1f0bf7eci80686efec7b4b07b@mail.gmail.com>
References: <OF09277E61.0EEE3466-ON852571E6.006E2CB8-852571E6.006FA10E@epamail.epa.gov>
	<3a3063250609111346k1f0bf7eci80686efec7b4b07b@mail.gmail.com>
Message-ID: <4505CDCE.2020100@med.umich.edu>

Jeff Bricker wrote:
> There's probably a better way, but what I do in this situation is to
> capture output to a .pdf that I can page through.

There are two ways that might be considered better, if better implies 
doing things from within R.

1.) At an R plot, type x11() to start a window, then under the History 
menu item, click 'Recording'. Now do your plots, and you can go back and 
forth through them using 'Page up' and 'Page down'.

2.) Set par(ask = TRUE), then submit your code. You will then have to 
hit return to create each sucessive plot.

HTH,

Jim


> 
> pdf("c:/temp/myPlots.pdf",h=8.5,w=11) #this is "landscape" orientation
> plot(<your arguments here>)
> dev.off()
> 
> You could also run a loop to open a new windows() device for each
> plot, but that gets messy if you have lots of plots.
> 
> On 9/11/06, Paternostro.Amy at epamail.epa.gov
> <Paternostro.Amy at epamail.epa.gov> wrote:
> 
>>Hello! I have written an R script on a Windows platform where I
>>calculate eight result matrices I plot using matplot. I would like to
>>display the resulting plots successively, rather than simultaneously,
>>and I was wondering if anyone could point me in the right direction as
>>to how to do this. The graphs pop up in this manner by default when I
>>run my script in S-PLUS, with tabs separating them so I can view each
>>graph at my leisure. However when I run my script in R, each graph pops
>>up only for a moment before it is replaced by the next until I am left
>>with only the plot of the eighth matrix at the end of the script. Thanks
>>in advance for your help!
>>
>>Amy Paternostro
>>National Center for Environmental Economiccs
>>United States Environmental Protection Agency
>>1200 Pennsylvania Avenue, NW
>>Washington, DC  20460
>>Paternostro.Amy at epamail.epa.gov
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


-- 
James W. MacDonald, M.S.
Biostatistician
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623


**********************************************************
Electronic Mail is not secure, may not be read every day, and should not be used for urgent or sensitive issues.


From ripley at stats.ox.ac.uk  Mon Sep 11 23:30:17 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 11 Sep 2006 22:30:17 +0100 (BST)
Subject: [R] Installation difficulty with "rimage"
In-Reply-To: <4505A71A.3060803@ucsf.edu>
References: <4505A71A.3060803@ucsf.edu>
Message-ID: <Pine.LNX.4.64.0609112227540.16814@gannet.stats.ox.ac.uk>

On Mon, 11 Sep 2006, John Kornak wrote:

> Dear R people,
> 
> I am trying to install rimage using install.packages("rimage") but am 
> receiving the following errors despite having FFTW installed. I would 
> appreciate any help to get this fixed.
> 
> checking fftw.h usability... no
> checking fftw.h presence... no
> checking for fftw.h... no
> configure: error: Sorry, can't find fftw header
> ERROR: configuration failed for package 'rimage'
> 
> I am using FFTW version 3.1, fedora core 3 and R version 2.3.1
> 
> I noticed an old posting on the subject  but the solution was to install 
> FFTW which I already did but the error still persists.

You need fftw2, not fftw3 (as the DESCRIPTION file says).
That posting was spot on.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Manuel.A.Morales at williams.edu  Mon Sep 11 23:34:28 2006
From: Manuel.A.Morales at williams.edu (Manuel Morales)
Date: Mon, 11 Sep 2006 17:34:28 -0400
Subject: [R] Conservative "ANOVA tables" in lmer
In-Reply-To: <40e66e0b0609110943h7dfb057dv12567975ac04bdb2@mail.gmail.com>
References: <1A8FE5B0-3FE3-4CA9-9224-9CACB9AF4854@muohio.edu>
	<40e66e0b0609070559u656463c1l52a60eda9a095237@mail.gmail.com>
	<20060910055624.GA12212@ms.unimelb.edu.au>
	<40e66e0b0609110943h7dfb057dv12567975ac04bdb2@mail.gmail.com>
Message-ID: <1158010468.3278.24.camel@solidago.localdomain>

On Mon, 2006-09-11 at 11:43 -0500, Douglas Bates wrote:
> On 9/10/06, Andrew Robinson <A.Robinson at ms.unimelb.edu.au> wrote:
> > On Thu, Sep 07, 2006 at 07:59:58AM -0500, Douglas Bates wrote:
> >
> > > I would be happy to re-institute p-values for fixed effects in the
> > > summary and anova methods for lmer objects using a denominator degrees
> > > of freedom based on the trace of the hat matrix or the rank of Z:X if
> > > others will volunteer to respond to the "these answers are obviously
> > > wrong because they don't agree with <whatever> and the idiot who wrote
> > > this software should be thrashed to within an inch of his life"
> > > messages.  I don't have the patience.
> >
> > This seems to be more than fair to me.  I'll volunteer to help explain
> > why the anova.lmer() output doesn't match SAS, etc.  Is it worth
> > putting a caveat in the output and the help files?  Is it even worth
> > writing a FAQ about this?
> 
> Having made that offer I think I will now withdraw it.  Peter's
> example has convinced me that this is the wrong thing to do.
> 
> I am encouraged by the fact that the results from mcmcsamp correspond
> closely to the correct theoretical results in the case that Peter
> described.  I appreciate that some users will find it difficult to
> work with a MCMC sample (or to convince editors to accept results
> based on such a sample) but I think that these results indicate that
> it is better to go after the marginal distribution of the fixed
> effects estimates (which is what is being approximated by the MCMC
> sample - up to Bayesian/frequentist philosophical differences) than to
> use the conditional distribution and somehow try to adjust the
> reference distribution.

Am I right that the MCMC sample can not be used, however, to evaluate
the significance of parameter groups. For example, to assess the
significance of a three-level factor? Are there better alternatives than
simply adjusting the CI for the number of factor levels
(1-alpha/levels).

Thanks!

Manuel

=
Manuel A. Morales
Asst. Prof., Biology
Williams College
http://mutualism.williams.edu


From p.murrell at auckland.ac.nz  Mon Sep 11 23:36:05 2006
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Tue, 12 Sep 2006 09:36:05 +1200
Subject: [R] reading images in R
In-Reply-To: <07E228A5BE53C24CAD490193A7381BBB12A0BE@LP-EXCHVS07.CO.IHC.COM>
References: <A32055BDEA88C34BB3DBBCD2293807786E3339@iu-mssg-mbx109.ads.iu.edu>
	<07E228A5BE53C24CAD490193A7381BBB12A0BE@LP-EXCHVS07.CO.IHC.COM>
Message-ID: <4505D6C5.1030202@stat.auckland.ac.nz>

Hi


Greg Snow wrote:
> Look at the pixmap and rimage packages.


There's also grImport for importing PostScript (which is easy to get to
from PDF)

Paul


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch on behalf of Nair, Murlidharan T
> Sent: Thu 9/7/2006 3:38 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] reading images in R
>  
>  
> 
> Are there functions to read image files in jpg, gif or even a pdf file?
> 
> Thanks ../Murli
> 
>  
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/


From ripley at stats.ox.ac.uk  Mon Sep 11 23:37:02 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 11 Sep 2006 22:37:02 +0100 (BST)
Subject: [R] Successive Graphs
In-Reply-To: <4505CDCE.2020100@med.umich.edu>
References: <OF09277E61.0EEE3466-ON852571E6.006E2CB8-852571E6.006FA10E@epamail.epa.gov>
	<3a3063250609111346k1f0bf7eci80686efec7b4b07b@mail.gmail.com>
	<4505CDCE.2020100@med.umich.edu>
Message-ID: <Pine.LNX.4.64.0609112232130.16814@gannet.stats.ox.ac.uk>

On Mon, 11 Sep 2006, James W. MacDonald wrote:

> Jeff Bricker wrote:
> > There's probably a better way, but what I do in this situation is to
> > capture output to a .pdf that I can page through.
> 
> There are two ways that might be considered better, if better implies 
> doing things from within R.
> 
> 1.) At an R plot, type x11() to start a window, then under the History 
> menu item, click 'Recording'. Now do your plots, and you can go back and 
> forth through them using 'Page up' and 'Page down'.

Better, windows(record=TRUE) to open a graphics device with recording 
turned on.  [x11() is only there for Unix users of R who have trouble 
adjusting to other windowing systems.]

This is the approach the Windows porters of R thought most convenient
(even more convenient that tabbed windows).

> 2.) Set par(ask = TRUE), then submit your code. You will then have to 
> hit return to create each sucessive plot.
> 
> HTH,
> 
> Jim
> 
> 
> > 
> > pdf("c:/temp/myPlots.pdf",h=8.5,w=11) #this is "landscape" orientation
> > plot(<your arguments here>)
> > dev.off()
> > 
> > You could also run a loop to open a new windows() device for each
> > plot, but that gets messy if you have lots of plots.
> > 
> > On 9/11/06, Paternostro.Amy at epamail.epa.gov
> > <Paternostro.Amy at epamail.epa.gov> wrote:
> > 
> >>Hello! I have written an R script on a Windows platform where I
> >>calculate eight result matrices I plot using matplot. I would like to
> >>display the resulting plots successively, rather than simultaneously,
> >>and I was wondering if anyone could point me in the right direction as
> >>to how to do this. The graphs pop up in this manner by default when I
> >>run my script in S-PLUS, with tabs separating them so I can view each
> >>graph at my leisure. However when I run my script in R, each graph pops
> >>up only for a moment before it is replaced by the next until I am left
> >>with only the plot of the eighth matrix at the end of the script. Thanks
> >>in advance for your help!
> >>
> >>Amy Paternostro
> >>National Center for Environmental Economiccs
> >>United States Environmental Protection Agency
> >>1200 Pennsylvania Avenue, NW
> >>Washington, DC  20460
> >>Paternostro.Amy at epamail.epa.gov
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>and provide commented, minimal, self-contained, reproducible code.
> >>
> > 
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From mike.prager at noaa.gov  Mon Sep 11 23:37:44 2006
From: mike.prager at noaa.gov (Michael Prager)
Date: Mon, 11 Sep 2006 17:37:44 -0400
Subject: [R] Successive Graphs
References: <OF09277E61.0EEE3466-ON852571E6.006E2CB8-852571E6.006FA10E@epamail.epa.gov>
Message-ID: <bilbg2t3nmhb39utelck491pecfq98i8kf@4ax.com>

Paternostro.Amy at epamail.epa.gov wrote:

> Hello! I have written an R script on a Windows platform where I
> calculate eight result matrices I plot using matplot. I would like to
> display the resulting plots successively, rather than simultaneously,
> and I was wondering if anyone could point me in the right direction as
> to how to do this. The graphs pop up in this manner by default when I
> run my script in S-PLUS, with tabs separating them so I can view each
> graph at my leisure. However when I run my script in R, each graph pops
> up only for a moment before it is replaced by the next until I am left
> with only the plot of the eighth matrix at the end of the script. Thanks
> in advance for your help!

Others have pointed out the R plot history mechanism, which is
very nice.  A few additions.

If you are re-running the script often and want to get rid of
old windows, you can put near the top of your script

graphics.off()

You can open a graphics window -- with history enabled -- with

windows(record=TRUE)

Plot history is saved.  If desired, you can clear the old
history before making new plots with

.SavedPlots <- NULL


Mike Prager
Southeast Fisheries Science Center, NOAA
Beaufort, North Carolina  USA


From AnupTyagi at yahoo.com  Tue Sep 12 06:34:23 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Tue, 12 Sep 2006 04:34:23 +0000 (UTC)
Subject: [R] rename cols
References: <5cd96f050609110949k5dfe1b1cg665558e50af1b56d@mail.gmail.com>
	<45059B36.1090203@acm.org>
Message-ID: <loom.20060912T062854-847@post.gmane.org>

For a newcomer who wants to rename variable "fksm" and "klmk" in a dataframe of
with 439 variables there is not easy and intuitive solution. That person has to
spend a lot of time listing columns and counting columns or doing string
searches or using brackets within brackets within brackets to get a simple thing
done. Is there a simple function or solution to this in R without using an
add-on package?


From alexnerdy at hotmail.com  Tue Sep 12 02:12:54 2006
From: alexnerdy at hotmail.com (Alexander Nervedi)
Date: Tue, 12 Sep 2006 00:12:54 +0000
Subject: [R] dotchart() help
In-Reply-To: <bilbg2t3nmhb39utelck491pecfq98i8kf@4ax.com>
Message-ID: <BAY106-F13AA9469780CEEF4E25452BB2B0@phx.gbl>

Hi

I am having trouble with dotcharts, and I keep getting the error message:

Error in Summary.data.frame(..., na.rm = na.rm) :
        only defined on a data frame with all numeric or complex variables

I am sure there is a really simple fix, but I am missing it and I wondered 
if you may have some advice. Test code from R help works prefectly, but I 
cant seem to recreate it

# example
>dotchart(VADeaths)
>VADeaths
      Rural Male Rural Female Urban Male Urban Female
50-54       11.7          8.7       15.4          8.4
55-59       18.1         11.7       24.3         13.6
60-64       26.9         20.3       37.0         19.3
65-69       41.0         30.9       54.6         35.1
70-74       66.0         54.3       71.1         50.0

# it works with mssing data

>test <-VADeaths
>test[2,]<- NA
>test
      Rural Male Rural Female Urban Male Urban Female
50-54       11.7          8.7       15.4          8.4
55-59         NA           NA         NA           NA
60-64       26.9         20.3       37.0         19.3
65-69       41.0         30.9       54.6         35.1
70-74       66.0         54.3       71.1         50.0
>dotchart(test)

# So i created my own test data
test<- expand.grid( Educ = c("B", "I", "A"),
                    Prof = c("L", "M","C"),
                    Blacks = NA,
                    Asian = NA,
                    Hispanic = NA,
                    Native = NA,
                    Female = NA)
rownames(test) <- with(test, paste(Educ,Prof, sep = "-"))
test[2:9,3:7] <- 2
temp <- test[,3:7]
temp[1, 2:3] <-5

# I want to plot temp which looks like
>temp
    Blacks Asian Hispanic Native Female
B-L     NA     5        5     NA     NA
I-L        2     2        2      2      2
A-L       2     2        2      2      2
B-M      2     2        2      2      2
I-M       2     2        2      2      2
A-M      2     2        2      2      2
B-C      2     2        2      2      2
I-C      2     2        2      2      2
A-C      2     2        2      2      2
>dotchart(temp)
Error in Summary.data.frame(..., na.rm = na.rm) :
        only defined on a data frame with all numeric or complex variables

Everything in temp is numeric and hence I dont understand the error message. 
Any leads would be most helpful. thank you

Al Nerdy.

_________________________________________________________________
Get the new Windows Live Messenger!


From t.gardner at uea.ac.uk  Tue Sep 12 00:38:59 2006
From: t.gardner at uea.ac.uk (Toby Gardner)
Date: Mon, 11 Sep 2006 23:38:59 +0100
Subject: [R] Extracting overdispersion estimates from lmer amd glm objects
Message-ID: <002b01c6d5f3$13930810$5801a8c0@Toby>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060911/9bdac8ba/attachment.ksh 

From christos at nuverabio.com  Tue Sep 12 07:15:59 2006
From: christos at nuverabio.com (Christos Hatzis)
Date: Tue, 12 Sep 2006 01:15:59 -0400
Subject: [R] rename cols
In-Reply-To: <loom.20060912T062854-847@post.gmane.org>
Message-ID: <000001c6d62a$892f8080$0202a8c0@headquarters.silicoinsights>

Try this:

old.colnames <- colnames(my.439.vars.df)
old.colnames[old.colnames=="fksm"] <- "new.name.a" 
old.colnames[old.colnames=="klmk"] <- "new.name.b"

I don't think it would be too complicated to put this into a function.

-Christos

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Anupam Tyagi
Sent: Tuesday, September 12, 2006 12:34 AM
To: r-help at stat.math.ethz.ch
Subject: Re: [R] rename cols

For a newcomer who wants to rename variable "fksm" and "klmk" in a dataframe
of with 439 variables there is not easy and intuitive solution. That person
has to spend a lot of time listing columns and counting columns or doing
string searches or using brackets within brackets within brackets to get a
simple thing done. Is there a simple function or solution to this in R
without using an add-on package?

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From gregor.gorjanc at bfro.uni-lj.si  Tue Sep 12 00:59:31 2006
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Tue, 12 Sep 2006 00:59:31 +0200
Subject: [R] Test internet presence
In-Reply-To: <XFMail.060911143026.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.060911143026.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <4505EA53.5010502@bfro.uni-lj.si>

Thank you. Your solution is usable but unfortunatelly not portable to
Windows. I would like to use this test in package check, which can
include also windows OS.

> On 11-Sep-06 Ted Harding wrote:
>> On 11-Sep-06 Gregor Gorjanc wrote:
>>> It seems that 'internal' method was used (I use R 2.3.1 under
>>> Linux) as indicated in help page of download.file. I could
>>> use wget or lynx methods, but these two must be available,
>>> so this is not really portable. Are there any other options
>>> for testing internet access? I am thinking that this might be
>>> more relevant for R-devel. I will wait a bit
>>> before moving there.
>>>
>>> -- 
>>> Lep pozdrav / With regards,
>>>     Gregor Gorjanc
>> Hi Gregor,
>> Since you are using Linux, I think you should ask R to delegate
>> the test to the system.
>>
>> If you have a script, in executable file ("755") say "test.inet.sh",
>> which says something like
> 
>   if ping -c 1 <something> ; then
>       export NET_UP="YES"
>     else
>       export NETP_UP="NO"
>   fi
> 
>> where "<something>" is the IP address or name of an external host
>> which responds to 'ping' (some will not, depending on their firewall
>> settings), then you can use on R:
>>
>>>>> system("test.inet")
>>>>> if( sys.getenv(NET_UP") == "YES" ) { ... } else { ... }
> 
> system("test.inet.sh")
> if( sys.getenv("NET_UP") == "YES" ) { ... } else { ... }
> 
>> For example (nothing to do with R, but shows the principle),
>> I have the following script to set my system time and hardware
>> clock from whichever one of 3 NTP servers is willing to respond:
>>
>> if /bin/ping -c 1 ntp0.zen.co.uk ; then
>>     export NETTIME="/usr/sbin/ntpdate -u ntp0.zen.co.uk"
>>   elif /bin/ping -c 1 ntp2b.mcc.ac.uk ; then
>>     export NETTIME="/usr/sbin/ntpdate -u ntp2b.mcc.ac.uk"
>>   elif /bin/ping -c 1 ntp2c.mcc.ac.uk ; then
>>     export NETTIME="/usr/sbin/ntpdate -u ntp2c.mcc.ac.uk"
>>   else
>>     export NETTIME=""
>> fi
>> if [ "$NETTIME" != "" ] ; then
>>   sleep 1
>>   sleep 1
>>   $NETTIME
>>   /sbin/clock -u -w
>>   date
>> fi
>>
>>
>> which also illustrates how to allow for the possibility that
>> the "default" server might not be responding at the time, so
>> it has 2 fallback servers.

-- 
Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana     PhD student
Biotechnical Faculty
Zootechnical Department     URI: http://www.bfro.uni-lj.si/MR/ggorjan
Groblje 3                   mail: gregor.gorjanc <at> bfro.uni-lj.si

SI-1230 Domzale             tel: +386 (0)1 72 17 861
Slovenia, Europe            fax: +386 (0)1 72 17 888

----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.


From Ted.Harding at nessie.mcc.ac.uk  Tue Sep 12 02:28:21 2006
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 12 Sep 2006 01:28:21 +0100 (BST)
Subject: [R] Test internet presence
In-Reply-To: <4505EA53.5010502@bfro.uni-lj.si>
Message-ID: <XFMail.060912012821.Ted.Harding@nessie.mcc.ac.uk>

On 11-Sep-06 Gregor Gorjanc wrote:
> Thank you. Your solution is usable but unfortunatelly not portable to
> Windows. I would like to use this test in package check, which can
> include also windows OS.

Now that I think about it (should have done that earlier),
you can use the value of .Platform$OS.type (either "unix"
or "windows") to determine the OS type, and also the
value of the "system" command to see what is sent back
from the "ping" command.

Then the R command to ping a remote host once would be

  system("ping -c 1 <remote host>") for "unix"

  system("ping -n 1 <remote host>") for "windows"
    (at any rate for the DOS-based ping in Windows 98).

However, the returned value of "system" (which consists
of the output of the ping commmand) is a multi-line value,
and is different between Unix and Windows

So to distinguish between success and failure, you would
have to do some slightly complex parsing of the output
returned by "system", and evaluate it differently according
to operating system. And the latter may vary between different
versions of the OS ... Maybe it wasn't such a good suggestion
after all.

However, since separate versions of your package would be
compiled for Unix-like systems and for Windows, perhaps a
way round such differences is to write a different script
for each one (shell script as before for Unix, "batch file"
for Windows), crafted so as to return identical results in
the two cases for success, and for failure.

Best wishes,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 12-Sep-06                                       Time: 01:28:15
------------------------------ XFMail ------------------------------


From john.kornak at ucsf.edu  Tue Sep 12 04:01:33 2006
From: john.kornak at ucsf.edu (John Kornak)
Date: Mon, 11 Sep 2006 19:01:33 -0700
Subject: [R] Installation difficulty with "rimage"
In-Reply-To: <Pine.LNX.4.64.0609112227540.16814@gannet.stats.ox.ac.uk>
References: <4505A71A.3060803@ucsf.edu>
	<Pine.LNX.4.64.0609112227540.16814@gannet.stats.ox.ac.uk>
Message-ID: <450614FD.6070704@ucsf.edu>


Thanks to Professor Ripley and Sarah Goslee for their helpful responses 
which have progressed my installation of rimage a little further.
Installing fftw2 instead of fftw3 of course solves the problem and I 
apologize for initially missing that requirement in the rimage 
documentation.

My install has now unfortunately stalled at the next stage and once 
again I would appreciate any advice as to how to solve the problem:

checking jpeglib.h usability... no
checking jpeglib.h presence... no
checking for jpeglib.h... no
configure: error: Sorry, can't find jpeglib header
ERROR: configuration failed for package 'rimage'

I believe that I have successfully installed libjpeg from the web site 
http://www.ijg.org as specified in the documentation.

I have also checked for the existence of the jpeglib.h file which is 
sitting in /usr/local/bin/jpeg-6b

I noticed an old posting on this issue, but the solution was to make 
sure that the jpeglib.h existed, which it does.

I am now running FFTW version 2.1.5, fedora core 3 and R version 2.3.1

Thanks again

John

Prof Brian Ripley wrote:
> On Mon, 11 Sep 2006, John Kornak wrote:
> 
>> Dear R people,
>>
>> I am trying to install rimage using install.packages("rimage") but am 
>> receiving the following errors despite having FFTW installed. I would 
>> appreciate any help to get this fixed.
>>
>> checking fftw.h usability... no
>> checking fftw.h presence... no
>> checking for fftw.h... no
>> configure: error: Sorry, can't find fftw header
>> ERROR: configuration failed for package 'rimage'
>>
>> I am using FFTW version 3.1, fedora core 3 and R version 2.3.1
>>
>> I noticed an old posting on the subject  but the solution was to install 
>> FFTW which I already did but the error still persists.
> 
> You need fftw2, not fftw3 (as the DESCRIPTION file says).
> That posting was spot on.
> 

-- 
John Kornak,PhD
Assistant Professor
Departments of Radiology, and Epidemiology & Biostatistics
University of California, San Francisco
Box 0946
San Francisco, CA 94143
Tel: (415) 353-4740
fax: (415) 353-9423
Email: john.kornak at ucsf.edu


From ggrothendieck at gmail.com  Tue Sep 12 07:53:00 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 12 Sep 2006 01:53:00 -0400
Subject: [R] Test internet presence
In-Reply-To: <4505EA53.5010502@bfro.uni-lj.si>
References: <XFMail.060911143026.Ted.Harding@nessie.mcc.ac.uk>
	<4505EA53.5010502@bfro.uni-lj.si>
Message-ID: <971536df0609112253t413c526y2fcf8f668e51f77@mail.gmail.com>

Here is a variation for Windows.  The second line returns TRUE or FALSE
and may need to be varied if the output of ping is not the same on your
system as on mine:

ping <- system("ping www.google.com", intern = TRUE)
as.numeric(strsplit(grep("Received", ping, value = TRUE), "[ ,]")[[1]][8]) > 0


On 9/11/06, Gregor Gorjanc <gregor.gorjanc at bfro.uni-lj.si> wrote:
> Thank you. Your solution is usable but unfortunatelly not portable to
> Windows. I would like to use this test in package check, which can
> include also windows OS.
>
> > On 11-Sep-06 Ted Harding wrote:
> >> On 11-Sep-06 Gregor Gorjanc wrote:
> >>> It seems that 'internal' method was used (I use R 2.3.1 under
> >>> Linux) as indicated in help page of download.file. I could
> >>> use wget or lynx methods, but these two must be available,
> >>> so this is not really portable. Are there any other options
> >>> for testing internet access? I am thinking that this might be
> >>> more relevant for R-devel. I will wait a bit
> >>> before moving there.
> >>>
> >>> --
> >>> Lep pozdrav / With regards,
> >>>     Gregor Gorjanc
> >> Hi Gregor,
> >> Since you are using Linux, I think you should ask R to delegate
> >> the test to the system.
> >>
> >> If you have a script, in executable file ("755") say "test.inet.sh",
> >> which says something like
> >
> >   if ping -c 1 <something> ; then
> >       export NET_UP="YES"
> >     else
> >       export NETP_UP="NO"
> >   fi
> >
> >> where "<something>" is the IP address or name of an external host
> >> which responds to 'ping' (some will not, depending on their firewall
> >> settings), then you can use on R:
> >>
> >>>>> system("test.inet")
> >>>>> if( sys.getenv(NET_UP") == "YES" ) { ... } else { ... }
> >
> > system("test.inet.sh")
> > if( sys.getenv("NET_UP") == "YES" ) { ... } else { ... }
> >
> >> For example (nothing to do with R, but shows the principle),
> >> I have the following script to set my system time and hardware
> >> clock from whichever one of 3 NTP servers is willing to respond:
> >>
> >> if /bin/ping -c 1 ntp0.zen.co.uk ; then
> >>     export NETTIME="/usr/sbin/ntpdate -u ntp0.zen.co.uk"
> >>   elif /bin/ping -c 1 ntp2b.mcc.ac.uk ; then
> >>     export NETTIME="/usr/sbin/ntpdate -u ntp2b.mcc.ac.uk"
> >>   elif /bin/ping -c 1 ntp2c.mcc.ac.uk ; then
> >>     export NETTIME="/usr/sbin/ntpdate -u ntp2c.mcc.ac.uk"
> >>   else
> >>     export NETTIME=""
> >> fi
> >> if [ "$NETTIME" != "" ] ; then
> >>   sleep 1
> >>   sleep 1
> >>   $NETTIME
> >>   /sbin/clock -u -w
> >>   date
> >> fi
> >>
> >>
> >> which also illustrates how to allow for the possibility that
> >> the "default" server might not be responding at the time, so
> >> it has 2 fallback servers.
>
> --
> Lep pozdrav / With regards,
>    Gregor Gorjanc
>
> ----------------------------------------------------------------------
> University of Ljubljana     PhD student
> Biotechnical Faculty
> Zootechnical Department     URI: http://www.bfro.uni-lj.si/MR/ggorjan
> Groblje 3                   mail: gregor.gorjanc <at> bfro.uni-lj.si
>
> SI-1230 Domzale             tel: +386 (0)1 72 17 861
> Slovenia, Europe            fax: +386 (0)1 72 17 888
>
> ----------------------------------------------------------------------
> "One must learn by doing the thing; for though you think you know it,
>  you have no certainty until you try." Sophocles ~ 450 B.C.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From murdoch at stats.uwo.ca  Tue Sep 12 02:37:52 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 11 Sep 2006 20:37:52 -0400
Subject: [R] Command equivalent of rgui "File, Save to File"?
In-Reply-To: <oofbg2tmd9ef64j9eu59n9nf522kb2oahi@4ax.com>
References: <oofbg2tmd9ef64j9eu59n9nf522kb2oahi@4ax.com>
Message-ID: <45060160.6040906@stats.uwo.ca>

On 9/11/2006 3:57 PM, Michael Prager wrote:
> R 2.3.1 on Windows XP Professional.
> 
> I am writing some scripts to generate examples.  The Rgui menu
> item "File, Save to File" is helpful.  Is there perhaps an
> equivalent R function that can be incorporated into a script?

I think sink() is the closest you can get: set R to write to a file 
before generating whatever output you want to save.

The menu item writes out the GUI text buffer; the R core doesn't know 
what's in that buffer.  Other front ends don't have a buffer at all.

Of course, we could write an Rgui-specific C function that put that text 
into an R character vector so you could do what you liked with it, but 
since sink() exists and is portable, there isn't a lot of motivation to 
do that.  This will happen automatically if we ever get around to 
defining an abstract GUI interface all controllable and configurable 
from R, but I don't know anyone working on that right now.

Duncan Murdoch

> 
> Mike Prager
> Southeast Fisheries Science Center, NOAA
> Beaufort, North Carolina  USA
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From gregor.gorjanc at bfro.uni-lj.si  Tue Sep 12 00:49:10 2006
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Mon, 11 Sep 2006 22:49:10 +0000 (UTC)
Subject: [R] Test internet presence
References: <XFMail.060911134821.Ted.Harding@nessie.mcc.ac.uk>
	<Pine.LNX.4.64.0609111419350.19275@gannet.stats.ox.ac.uk>
Message-ID: <loom.20060912T003831-608@post.gmane.org>

Prof Brian Ripley <ripley <at> stats.ox.ac.uk> writes:
...
> Check out tests/internet.R.  nsl() checks if you can resolve host names, 
> which has worked well enough there.

Thank you prof. Ripley for this pointer. I am posting here the relevant part if
someone does not look at SVN. I would just like to ask why is .Platform$OS.type
== "unix" added to the test? Is nsl() available only on unix like platforms or
... I did not found any specifics in its help page.

if(!capabilities()["http/ftp"]) {
    warning("no internet capabilities")
    q()
}

if(.Platform$OS.type == "unix" &&
   is.null(nsl("cran.r-project.org"))) q()

Does it make any sense to write a function that would use these two tests.

isNetAvailable <- function()
{
  ifelse(!capabilities()["http/ftp"] && 
##         .Platform$OS.type == "unix" && ## ??? 
         is.null(nsl("cran.r-project.org")), 
         FALSE, 
         TRUE)
}

Regards, Gregor


From ripley at stats.ox.ac.uk  Tue Sep 12 08:08:57 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 12 Sep 2006 07:08:57 +0100 (BST)
Subject: [R] Installation difficulty with "rimage"
In-Reply-To: <450614FD.6070704@ucsf.edu>
References: <4505A71A.3060803@ucsf.edu>
	<Pine.LNX.4.64.0609112227540.16814@gannet.stats.ox.ac.uk>
	<450614FD.6070704@ucsf.edu>
Message-ID: <Pine.LNX.4.64.0609120705440.10962@gannet.stats.ox.ac.uk>

On Mon, 11 Sep 2006, John Kornak wrote:

> 
> Thanks to Professor Ripley and Sarah Goslee for their helpful responses 
> which have progressed my installation of rimage a little further.
> Installing fftw2 instead of fftw3 of course solves the problem and I 
> apologize for initially missing that requirement in the rimage 
> documentation.
> 
> My install has now unfortunately stalled at the next stage and once 
> again I would appreciate any advice as to how to solve the problem:
> 
> checking jpeglib.h usability... no
> checking jpeglib.h presence... no
> checking for jpeglib.h... no
> configure: error: Sorry, can't find jpeglib header
> ERROR: configuration failed for package 'rimage'
> 
> I believe that I have successfully installed libjpeg from the web site 
> http://www.ijg.org as specified in the documentation.
> 
> I have also checked for the existence of the jpeglib.h file which is 
> sitting in /usr/local/bin/jpeg-6b

Ah, but it needs to be in /usr/include or /usr/local/include for the 
package to find it.

> I noticed an old posting on this issue, but the solution was to make 
> sure that the jpeglib.h existed, which it does.
> 
> I am now running FFTW version 2.1.5, fedora core 3 and R version 2.3.1

My FC3 system has RPMs libjpeg-6b-33 and libjpeg-devel-6b-33.  If you 
install those all your paths should be as you need them.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From spencer.graves at pdf.com  Tue Sep 12 00:54:58 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 12 Sep 2006 00:54:58 +0200
Subject: [R] syntax of nlme
In-Reply-To: <40e66e0b0609111121r6ba4fc60x849ef4d71530004b@mail.gmail.com>
References: <450583C0.60707@web.de>
	<40e66e0b0609111121r6ba4fc60x849ef4d71530004b@mail.gmail.com>
Message-ID: <4505E942.2080106@pdf.com>

<in line>

Douglas Bates wrote:
> On 9/11/06, Thomas Wutzler <thomas.wutzler at web.de> wrote:
>   
>> Hello,
>>
>>
>> How do I specify the formula and random effects without a startup object
>> ? I thought it would be a mixture of nls and lme.
>> after trying very hard, I ask for help on using nlme.
>>
>> Can someone hint me to some examples?
>>     
>
> There are quite a few examples in the book Pinheiro and Bates (2000),
> "Mixed-effects Models in S and S-PLUS"
>   
      Permit me to offer a personal testimonial:  If you do more 
statistics than just this one data set, I believe you will be amply 
rewarded for investing money and time in that book.  The value of this 
investment will be further enhanced by working line by line (as you read 
the text) through the files "ch01.R", "ch02.R", ..., "ch06.R" and 
"ch08.R" in "~library\nlme\scripts" in your R installation directory.  
Spencer Graves
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From bolker at zoo.ufl.edu  Tue Sep 12 00:44:37 2006
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Mon, 11 Sep 2006 22:44:37 +0000 (UTC)
Subject: [R] plotting curves
References: <45057F61.9080002@wright.edu>
Message-ID: <loom.20060912T003951-61@post.gmane.org>

Aldo Crossa <aldo.crossa <at> wright.edu> writes:

> 
> Hi,
> 
> I've been trying to recreate plots of that follow a cubic form. 
[snip]
> I've noticed that in R all curves are pixelated, regardless of howmany 
> points I use. Is there anything I can do to smoothen these graphs so 
> that they are no longer pixelated? I've tried saving it as a metafile, 
> postcript, etc., and although the I've been able to increase the 
> resolution, I haven't been able to fix the pixelation.

  Weird.  I think you need to give us a reproducible example
so we can help.  The best thing, if possible, would be to 
give us an example of the code you used and simultaneously
to post examples of the output to a web page somewhere, if
that's possible.

  I would just say, e.g.,

curve(x^3-2*x^2+x-1,from=-10,to=10)

 there is an "n=" argument that specifies how many points
to use, but it doesn't seem necessary.

   Ben Bolker


From ripley at stats.ox.ac.uk  Tue Sep 12 08:18:36 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 12 Sep 2006 07:18:36 +0100 (BST)
Subject: [R] dotchart() help
In-Reply-To: <BAY106-F13AA9469780CEEF4E25452BB2B0@phx.gbl>
References: <BAY106-F13AA9469780CEEF4E25452BB2B0@phx.gbl>
Message-ID: <Pine.LNX.4.64.0609120711310.10962@gannet.stats.ox.ac.uk>

On Tue, 12 Sep 2006, Alexander Nervedi wrote:

> Hi
> 
> I am having trouble with dotcharts, and I keep getting the error message:
> 
> Error in Summary.data.frame(..., na.rm = na.rm) :
>         only defined on a data frame with all numeric or complex variables
> 
> I am sure there is a really simple fix, but I am missing it and I wondered 
> if you may have some advice. Test code from R help works prefectly, but I 
> cant seem to recreate it
> 
> # example
> >dotchart(VADeaths)
> >VADeaths
>       Rural Male Rural Female Urban Male Urban Female
> 50-54       11.7          8.7       15.4          8.4
> 55-59       18.1         11.7       24.3         13.6
> 60-64       26.9         20.3       37.0         19.3
> 65-69       41.0         30.9       54.6         35.1
> 70-74       66.0         54.3       71.1         50.0
> 
> # it works with mssing data
> 
> >test <-VADeaths
> >test[2,]<- NA
> >test
>       Rural Male Rural Female Urban Male Urban Female
> 50-54       11.7          8.7       15.4          8.4
> 55-59         NA           NA         NA           NA
> 60-64       26.9         20.3       37.0         19.3
> 65-69       41.0         30.9       54.6         35.1
> 70-74       66.0         54.3       71.1         50.0
> >dotchart(test)
> 
> # So i created my own test data
> test<- expand.grid( Educ = c("B", "I", "A"),
>                     Prof = c("L", "M","C"),
>                     Blacks = NA,
>                     Asian = NA,
>                     Hispanic = NA,
>                     Native = NA,
>                     Female = NA)
> rownames(test) <- with(test, paste(Educ,Prof, sep = "-"))
> test[2:9,3:7] <- 2
> temp <- test[,3:7]
> temp[1, 2:3] <-5
> 
> # I want to plot temp which looks like
> >temp
>     Blacks Asian Hispanic Native Female
> B-L     NA     5        5     NA     NA
> I-L        2     2        2      2      2
> A-L       2     2        2      2      2
> B-M      2     2        2      2      2
> I-M       2     2        2      2      2
> A-M      2     2        2      2      2
> B-C      2     2        2      2      2
> I-C      2     2        2      2      2
> A-C      2     2        2      2      2
> >dotchart(temp)
> Error in Summary.data.frame(..., na.rm = na.rm) :
>         only defined on a data frame with all numeric or complex variables
> 
> Everything in temp is numeric and hence I dont understand the error message. 

>From the help page of dotchart:

       x: either a vector or matrix of numeric values ('NA's are
          allowed).  If 'x' is a matrix the overall plot consists of
          juxtaposed dotplots for each row.

> class(temp)
[1] "data.frame"

which is neither.  Try as.matrix(temp).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From c.oswald at matsci.uni-sb.de  Tue Sep 12 08:42:17 2006
From: c.oswald at matsci.uni-sb.de (Christian Oswald)
Date: Tue, 12 Sep 2006 08:42:17 +0200
Subject: [R] Description of x-axis
In-Reply-To: <3a3063250609110626uc8d75ebsf74394c538f208a9@mail.gmail.com>
References: <45055A28.30302@matsci.uni-sb.de>
	<3a3063250609110626uc8d75ebsf74394c538f208a9@mail.gmail.com>
Message-ID: <450656C9.9000302@matsci.uni-sb.de>

Thanks,
that works but what are the coordinates of "at" in the y-direction? On
which y-position mtext plots?

Christian






Jeff Bricker schrieb:
> ?mtext, also ?barplot since barplot returns midpoints of the bars.
> 
> also ?rep for setting up a vector of repeating values.
> 
> something like this:
> xLabels<-c(rep("2003",4),rep("2005",11),rep("2006",5))
> midPoints<-barplot(<your original statements here>)
> mtext(xLabels,side=1,at=apply(midPoints,2,mean))
> 
> On 9/11/06, Christian Oswald <c.oswald at matsci.uni-sb.de> wrote:
>> Hello,
>>
>> I have made a barplot with some data and need a description below the
>> x-axis. For example there are 20 values and I need a description "2003"
>> for the first four values, then "2005" for the next eleven and "2006"
>> for the last five values.
>>
>> I want the description below the x-axis-labels and above the x-axis
>> title.
>>
>> Thanks,
>>
>> Christian
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> 
> 


-- 
Diplom-Ingenieur Christian Oswald
Universitaet des Saarlandes
Lehrstuhl fuer Pulvertechnologie von Glas und Keramik
Gebaude D2 2
D-66123 Saarbruecken

Tel.: (+49) 0681/302-5249
Fax.: (+49) 0681/302-5227


From AnupTyagi at yahoo.com  Tue Sep 12 08:44:03 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Tue, 12 Sep 2006 06:44:03 +0000 (UTC)
Subject: [R] rename cols
References: <loom.20060912T062854-847@post.gmane.org>
	<000001c6d62a$892f8080$0202a8c0@headquarters.silicoinsights>
Message-ID: <loom.20060912T083021-596@post.gmane.org>

Christos Hatzis <christos <at> nuverabio.com> writes:

> 
> Try this:
> 
> old.colnames <- colnames(my.439.vars.df)
> old.colnames[old.colnames=="fksm"] <- "new.name.a" 
> old.colnames[old.colnames=="klmk"] <- "new.name.b"

For a newcomer, it will be useful to have a function like this in the base R:
that can take a list of old.names and new.names, and do the assignment. It is
far more efficient to have functions that are shared via the R distribution,
than having to write own functions for carrying out basic data management tasks,
and simple routinely used statistical procedures. Most users would rather spend
time on thinking about the substantive work, instead of figuring out how to
program---this may be specially true for new users. This way the functions used
will also be more efficient and better designed than the typical new user.


From gunter.berton at gene.com  Tue Sep 12 01:16:14 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 11 Sep 2006 16:16:14 -0700
Subject: [R] Successive Graphs
In-Reply-To: <bilbg2t3nmhb39utelck491pecfq98i8kf@4ax.com>
Message-ID: <009201c6d5f8$470910e0$711f210a@gne.windows.gene.com>

... or use lattice and splom() instead. If the successive graphs bear some
relationship to each other, this might produce a more useful display, too.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Michael Prager
> Sent: Monday, September 11, 2006 2:38 PM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] Successive Graphs
> 
> Paternostro.Amy at epamail.epa.gov wrote:
> 
> > Hello! I have written an R script on a Windows platform where I
> > calculate eight result matrices I plot using matplot. I 
> would like to
> > display the resulting plots successively, rather than 
> simultaneously,
> > and I was wondering if anyone could point me in the right 
> direction as
> > to how to do this. The graphs pop up in this manner by 
> default when I
> > run my script in S-PLUS, with tabs separating them so I can 
> view each
> > graph at my leisure. However when I run my script in R, 
> each graph pops
> > up only for a moment before it is replaced by the next 
> until I am left
> > with only the plot of the eighth matrix at the end of the 
> script. Thanks
> > in advance for your help!
> 
> Others have pointed out the R plot history mechanism, which is
> very nice.  A few additions.
> 
> If you are re-running the script often and want to get rid of
> old windows, you can put near the top of your script
> 
> graphics.off()
> 
> You can open a graphics window -- with history enabled -- with
> 
> windows(record=TRUE)
> 
> Plot history is saved.  If desired, you can clear the old
> history before making new plots with
> 
> .SavedPlots <- NULL
> 
> 
> Mike Prager
> Southeast Fisheries Science Center, NOAA
> Beaufort, North Carolina  USA
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ligges at statistik.uni-dortmund.de  Tue Sep 12 08:48:55 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 12 Sep 2006 08:48:55 +0200
Subject: [R] Description of x-axis
In-Reply-To: <450656C9.9000302@matsci.uni-sb.de>
References: <45055A28.30302@matsci.uni-sb.de>	<3a3063250609110626uc8d75ebsf74394c538f208a9@mail.gmail.com>
	<450656C9.9000302@matsci.uni-sb.de>
Message-ID: <45065857.8000309@statistik.uni-dortmund.de>



Christian Oswald wrote:
> Thanks,
> that works but what are the coordinates of "at" in the y-direction? On
> which y-position mtext plots?

Please read ?mtext which tells you to use argument "line".

Uwe Ligges

> Christian
> 
> 
> 
> 
> 
> 
> Jeff Bricker schrieb:
>> ?mtext, also ?barplot since barplot returns midpoints of the bars.
>>
>> also ?rep for setting up a vector of repeating values.
>>
>> something like this:
>> xLabels<-c(rep("2003",4),rep("2005",11),rep("2006",5))
>> midPoints<-barplot(<your original statements here>)
>> mtext(xLabels,side=1,at=apply(midPoints,2,mean))
>>
>> On 9/11/06, Christian Oswald <c.oswald at matsci.uni-sb.de> wrote:
>>> Hello,
>>>
>>> I have made a barplot with some data and need a description below the
>>> x-axis. For example there are 20 values and I need a description "2003"
>>> for the first four values, then "2005" for the next eleven and "2006"
>>> for the last five values.
>>>
>>> I want the description below the x-axis-labels and above the x-axis
>>> title.
>>>
>>> Thanks,
>>>
>>> Christian
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>
> 
>


From pekka.kontiainen at helsinki.fi  Tue Sep 12 08:49:28 2006
From: pekka.kontiainen at helsinki.fi (Pekka Kontiainen)
Date: Tue, 12 Sep 2006 09:49:28 +0300
Subject: [R] Can't run nlme with nested structure
Message-ID: <1158043768.45065878ea72d@www2.helsinki.fi>

Hello!


So, my problem is following. I have bird offspring growth data and I'd 
like to model individual growth curves (aim is to study asymptotes and 
inflection points) with nlme according to Pinheiro & Bates 2000: first using 
nlsList to generate individual curves and then nlme to study the 
parameters and fixed effects. The data is structured to two levels. I 
have broods and individuals within the broods. Problems arise if I specify 
the groupedData object to have two levels. Running of the nlme gives me an 
error message "can't run the model with multiple levels", or something 
alike..

Is there a way around this or should I start looking fo another way of 
analysing the data?

Thank you for your time.

Pekka Kontiainen
Ph.D. Candidate
Bird Ecology Unit
Department of Environmental and Biosciences
P.O. Box 65 (00014) University of Helsinki 
FINLAND
Phone +35844-5496858

----- Edelleenl?hetetty viesti p??ttyy -----


Pekka Kontiainen
Ph.D. Candidate
Bird Ecology Unit
Department of Environmental and Biosciences
P.O. Box 65 (00014) University of Helsinki 
FINLAND
Phone +35844-5496858


From AnupTyagi at yahoo.com  Tue Sep 12 08:47:56 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Tue, 12 Sep 2006 06:47:56 +0000 (UTC)
Subject: [R] Reading fixed column format
References: <loom.20060911T172146-100@post.gmane.org><BAY116-DAV571634EB23B6A0F2F6502CF2A0@phx.gbl>
	<loom.20060911T185053-310@post.gmane.org>
	<BAY116-DAV15C8CB3C190E253F6BD659CF2A0@phx.gbl>
Message-ID: <loom.20060912T084704-214@post.gmane.org>

Jason Barnhart <jasoncbarnhart <at> msn.com> writes:

> 
> These posts may be helpful.
> http://tolstoy.newcastle.edu.au/R/help/05/06/5776.html
> https://stat.ethz.ch/pipermail/r-help/2002-May/021145.html
> 
> Using scan directly may also work for you rather than read.fwf.
> 
> Also, there are posts regarding using other tools such a 'perl' or 'cut' to 
> prepocess the data
> before reading with R.  Searching the archives with those keywords should 
> help.

I new user should not have to learn "perl","cut", "awk", etc simply to be able
to use R. Does not make sense to me.


From blomsp at ozemail.com.au  Tue Sep 12 08:53:02 2006
From: blomsp at ozemail.com.au (Simon Blomberg)
Date: Tue, 12 Sep 2006 16:53:02 +1000
Subject: [R] Extracting overdispersion estimates from lmer amd glm
	objects
In-Reply-To: <002b01c6d5f3$13930810$5801a8c0@Toby>
References: <002b01c6d5f3$13930810$5801a8c0@Toby>
Message-ID: <4506594E.3040309@ozemail.com.au>

summary(modeltest)@sigma

Toby Gardner wrote:
> Dear list, 
>
> I am needing to extract the estimate of overdispersion (deviance / residual degrees of freedom or c-hat) from multiple model objects - so they can then be used to compare the extent of overdispersion among alternative models as well as calculate qausi-AIC values.  I have been unable to do this, despite consulting a number of manuals and searching the R-help.  I am imaging that in theory it should be possible with some call to attr(), but i have so far had no success.  
>
> An example model output would be: 
>
>   
>> modeltest<-lmer(Coleodactylus_amazonicus_N~USD + (1|site),data=SFArray,family=poisson,method="Laplace",control=list(usePQL=FALSE, msVerbose=TRUE))
>> summary(modeltest)
>>     
>
> ------------
>
> Generalized linear mixed model fit using Laplace 
> Formula: Coleodactylus_amazonicus_N ~ USD + (1 | site) 
>    Data: SFArray 
>  Family: poisson(log link)
>       AIC      BIC    logLik deviance
>  75.94996 81.68603 -34.97498 69.94996
> Random effects:
>  Groups Name        Variance Std.Dev.
>  site   (Intercept) 2.6076   1.6148  
> number of obs: 50, groups: site, 5
>
> Estimated scale (compare to 1)  1.080798 
>
> ------------
>
> What I need is to extract this value (1.080798) from multiple lmer objects.  Has anyone any recommendations? I also need to do this for glm objects although I suspect if someone was able to kindly point me in the right direction then the solution is likely to be similar. 
>
> Very many thanks, 
>
> Toby Gardner
>
>   
>> sessionInfo()
>>     
> Version 2.3.1 (2006-06-01) 
> i386-pc-mingw32 
>
> attached base packages:
> [1] "datasets"  "graphics"  "grDevices" "methods"   "stats"     "utils"     "base"     
>
> other attached packages:
>        JGR     iplots     JavaGD       lme4     Matrix    lattice       MASS      rJava 
>    "1.4-7"    "1.0-3"    "0.3-4"  "0.995-2" "0.995-15"   "0.13-8" "7.2-27.1"    "0.4-6" 
>
>
> School of Environmental Sciences
> University of East Anglia
> Norwich, NR4 7TJ
> United Kingdom
> Email: t.gardner at uea.ac.uk
> Website: www.uea.ac.uk/~e387495
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>   


-- 
Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat. 
Centre for Resource and Environmental Studies
The Australian National University              
Canberra ACT 0200                               
Australia                                       
T: +61 2 6125 7800 email: Simon.Blomberg_at_anu.edu.au
F: +61 2 6125 0757
CRICOS Provider # 00120C

The combination of some data and an aching desire for 
an answer does not ensure that a reasonable answer 
can be extracted from a given body of data.
- John Tukey.


From mail at xesoftware.com.au  Tue Sep 12 09:22:26 2006
From: mail at xesoftware.com.au (stephenc)
Date: Tue, 12 Sep 2006 17:22:26 +1000
Subject: [R] stepAIC
Message-ID: <001c01c6d63c$331e8da0$7701a8c0@tablet>

Hi

 

I hope this isn't off topics, but I have always found when I stepAIC() some
glm I get an improvement in accuracy and kappa, but I have just done a case
where I got a marginal deterioration.  Is this possible, or should I be
going through my figures carefully to see if I have messed up?

 

Stephen Choularton

02 9999 2226

0413 545 182

 

 

 

-------------- next part --------------

Checked by AVG Free Edition.


From mothsailor at googlemail.com  Tue Sep 12 09:55:43 2006
From: mothsailor at googlemail.com (David Barron)
Date: Tue, 12 Sep 2006 08:55:43 +0100
Subject: [R] Extracting overdispersion estimates from lmer amd glm
	objects
In-Reply-To: <002b01c6d5f3$13930810$5801a8c0@Toby>
References: <002b01c6d5f3$13930810$5801a8c0@Toby>
Message-ID: <815b70590609120055j40f001c6redeb4be78d149180@mail.gmail.com>

You can extract it from a summary.lmer object, which has a slot names
"sigma" that contains this scale parameter.  For example,

> sum.modeltest <- summary(modeltest)
> sum.modeltest at sigma

On 11/09/06, Toby Gardner <t.gardner at uea.ac.uk> wrote:
> Dear list,
>
> I am needing to extract the estimate of overdispersion (deviance / residual degrees of freedom or c-hat) from multiple model objects - so they can then be used to compare the extent of overdispersion among alternative models as well as calculate qausi-AIC values.  I have been unable to do this, despite consulting a number of manuals and searching the R-help.  I am imaging that in theory it should be possible with some call to attr(), but i have so far had no success.
>
> An example model output would be:
>
> > modeltest<-lmer(Coleodactylus_amazonicus_N~USD + (1|site),data=SFArray,family=poisson,method="Laplace",control=list(usePQL=FALSE, msVerbose=TRUE))
> > summary(modeltest)
>
> ------------
>
> Generalized linear mixed model fit using Laplace
> Formula: Coleodactylus_amazonicus_N ~ USD + (1 | site)
>    Data: SFArray
>  Family: poisson(log link)
>       AIC      BIC    logLik deviance
>  75.94996 81.68603 -34.97498 69.94996
> Random effects:
>  Groups Name        Variance Std.Dev.
>  site   (Intercept) 2.6076   1.6148
> number of obs: 50, groups: site, 5
>
> Estimated scale (compare to 1)  1.080798
>
> ------------
>
> What I need is to extract this value (1.080798) from multiple lmer objects.  Has anyone any recommendations? I also need to do this for glm objects although I suspect if someone was able to kindly point me in the right direction then the solution is likely to be similar.
>
> Very many thanks,
>
> Toby Gardner
>
> > sessionInfo()
> Version 2.3.1 (2006-06-01)
> i386-pc-mingw32
>
> attached base packages:
> [1] "datasets"  "graphics"  "grDevices" "methods"   "stats"     "utils"     "base"
>
> other attached packages:
>        JGR     iplots     JavaGD       lme4     Matrix    lattice       MASS      rJava
>    "1.4-7"    "1.0-3"    "0.3-4"  "0.995-2" "0.995-15"   "0.13-8" "7.2-27.1"    "0.4-6"
>
>
> School of Environmental Sciences
> University of East Anglia
> Norwich, NR4 7TJ
> United Kingdom
> Email: t.gardner at uea.ac.uk
> Website: www.uea.ac.uk/~e387495
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From ripley at stats.ox.ac.uk  Tue Sep 12 10:01:14 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 12 Sep 2006 09:01:14 +0100 (BST)
Subject: [R] stepAIC
In-Reply-To: <001c01c6d63c$331e8da0$7701a8c0@tablet>
References: <001c01c6d63c$331e8da0$7701a8c0@tablet>
Message-ID: <Pine.LNX.4.64.0609120859350.27923@gannet.stats.ox.ac.uk>

On Tue, 12 Sep 2006, stephenc wrote:

> Hi
> 
>  
> 
> I hope this isn't off topics, but I have always found when I stepAIC() some
> glm I get an improvement in accuracy and kappa, but I have just done a case
> where I got a marginal deterioration.  Is this possible, or should I be
> going through my figures carefully to see if I have messed up?

It is certainly possible.

Optimizing AIC is only aimed at getting an improvement in one prediction 
measure, on average.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ralph.wirth at gfk.com  Tue Sep 12 10:20:13 2006
From: ralph.wirth at gfk.com (ralph.wirth at gfk.com)
Date: Tue, 12 Sep 2006 10:20:13 +0200
Subject: [R] Problem with geweke.diag
Message-ID: <OF22C32CB5.E15B7184-ONC12571E7.002D148D-C12571E7.002DCBFF@gfk.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060912/428e4952/attachment.ksh 

From A.Robinson at ms.unimelb.edu.au  Tue Sep 12 10:25:19 2006
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Tue, 12 Sep 2006 18:25:19 +1000
Subject: [R] Can't run nlme with nested structure
In-Reply-To: <1158043768.45065878ea72d@www2.helsinki.fi>
References: <1158043768.45065878ea72d@www2.helsinki.fi>
Message-ID: <20060912082519.GI12212@ms.unimelb.edu.au>

Hello Pekka,

there may be a way around it.  However, you should provide commented,
minimal, self-contained, reproducible code so we can see what you are
trying to do.  We need to see exactly what you are telling R, and what
nlme is telling you.

Cheers

Andrew

On Tue, Sep 12, 2006 at 09:49:28AM +0300, Pekka Kontiainen wrote:
> Hello!
> 
> 
> So, my problem is following. I have bird offspring growth data and I'd 
> like to model individual growth curves (aim is to study asymptotes and 
> inflection points) with nlme according to Pinheiro & Bates 2000: first using 
> nlsList to generate individual curves and then nlme to study the 
> parameters and fixed effects. The data is structured to two levels. I 
> have broods and individuals within the broods. Problems arise if I specify 
> the groupedData object to have two levels. Running of the nlme gives me an 
> error message "can't run the model with multiple levels", or something 
> alike..
> 
> Is there a way around this or should I start looking fo another way of 
> analysing the data?
> 
> Thank you for your time.
> 
> Pekka Kontiainen
> Ph.D. Candidate
> Bird Ecology Unit
> Department of Environmental and Biosciences
> P.O. Box 65 (00014) University of Helsinki 
> FINLAND
> Phone +35844-5496858
> 
> ----- Edelleenl?hetetty viesti p??ttyy -----
> 
> 
> Pekka Kontiainen
> Ph.D. Candidate
> Bird Ecology Unit
> Department of Environmental and Biosciences
> P.O. Box 65 (00014) University of Helsinki 
> FINLAND
> Phone +35844-5496858
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
Email: a.robinson at ms.unimelb.edu.au         http://www.ms.unimelb.edu.au


From mothsailor at googlemail.com  Tue Sep 12 10:26:54 2006
From: mothsailor at googlemail.com (David Barron)
Date: Tue, 12 Sep 2006 09:26:54 +0100
Subject: [R] Extracting overdispersion estimates from lmer amd glm
	objects
In-Reply-To: <002b01c6d5f3$13930810$5801a8c0@Toby>
References: <002b01c6d5f3$13930810$5801a8c0@Toby>
Message-ID: <815b70590609120126r7d55cf22racfc2ccc19ba77ef@mail.gmail.com>

PS, the equivalent for a glm is:

> sum.modeltest <- summary(modeltest)
> sum.modeltest$dispersion

Hope this helps.

On 11/09/06, Toby Gardner <t.gardner at uea.ac.uk> wrote:
> Dear list,
>
> I am needing to extract the estimate of overdispersion (deviance / residual degrees of freedom or c-hat) from multiple model objects - so they can then be used to compare the extent of overdispersion among alternative models as well as calculate qausi-AIC values.  I have been unable to do this, despite consulting a number of manuals and searching the R-help.  I am imaging that in theory it should be possible with some call to attr(), but i have so far had no success.
>
> An example model output would be:
>
> > modeltest<-lmer(Coleodactylus_amazonicus_N~USD + (1|site),data=SFArray,family=poisson,method="Laplace",control=list(usePQL=FALSE, msVerbose=TRUE))
> > summary(modeltest)
>
> ------------
>
> Generalized linear mixed model fit using Laplace
> Formula: Coleodactylus_amazonicus_N ~ USD + (1 | site)
>    Data: SFArray
>  Family: poisson(log link)
>       AIC      BIC    logLik deviance
>  75.94996 81.68603 -34.97498 69.94996
> Random effects:
>  Groups Name        Variance Std.Dev.
>  site   (Intercept) 2.6076   1.6148
> number of obs: 50, groups: site, 5
>
> Estimated scale (compare to 1)  1.080798
>
> ------------
>
> What I need is to extract this value (1.080798) from multiple lmer objects.  Has anyone any recommendations? I also need to do this for glm objects although I suspect if someone was able to kindly point me in the right direction then the solution is likely to be similar.
>
> Very many thanks,
>
> Toby Gardner
>
> > sessionInfo()
> Version 2.3.1 (2006-06-01)
> i386-pc-mingw32
>
> attached base packages:
> [1] "datasets"  "graphics"  "grDevices" "methods"   "stats"     "utils"     "base"
>
> other attached packages:
>        JGR     iplots     JavaGD       lme4     Matrix    lattice       MASS      rJava
>    "1.4-7"    "1.0-3"    "0.3-4"  "0.995-2" "0.995-15"   "0.13-8" "7.2-27.1"    "0.4-6"
>
>
> School of Environmental Sciences
> University of East Anglia
> Norwich, NR4 7TJ
> United Kingdom
> Email: t.gardner at uea.ac.uk
> Website: www.uea.ac.uk/~e387495
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From kubovy at virginia.edu  Tue Sep 12 11:25:09 2006
From: kubovy at virginia.edu (Michael Kubovy)
Date: Tue, 12 Sep 2006 05:25:09 -0400
Subject: [R] Reading fixed column format
In-Reply-To: <loom.20060912T084704-214@post.gmane.org>
References: <loom.20060911T172146-100@post.gmane.org><BAY116-DAV571634EB23B6A0F2F6502CF2A0@phx.gbl>
	<loom.20060911T185053-310@post.gmane.org>
	<BAY116-DAV15C8CB3C190E253F6BD659CF2A0@phx.gbl>
	<loom.20060912T084704-214@post.gmane.org>
Message-ID: <3EBEC9D3-559C-4607-838D-46042D36A3AA@virginia.edu>

On Sep 12, 2006, at 2:47 AM, Anupam Tyagi wrote:

> Jason Barnhart <jasoncbarnhart <at> msn.com> writes:
>
>>
>> These posts may be helpful.
>> http://tolstoy.newcastle.edu.au/R/help/05/06/5776.html
>> https://stat.ethz.ch/pipermail/r-help/2002-May/021145.html
>>
>> Using scan directly may also work for you rather than read.fwf.
>>
>> Also, there are posts regarding using other tools such a 'perl' or  
>> 'cut' to
>> prepocess the data
>> before reading with R.  Searching the archives with those keywords  
>> should
>> help.
>
> I new user should not have to learn "perl","cut", "awk", etc simply  
> to be able
> to use R. Does not make sense to me.

Hi Anupam,

You'll get much better help here if you're not ill-tempered. This is  
a group of extraordinarily helpful volunteers who owe you less than  
you paid for the product.

Please consider saving your data in a way that will make it easier to  
read into R. No program can read every dataset.
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From B.Rowlingson at lancaster.ac.uk  Tue Sep 12 11:34:02 2006
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 12 Sep 2006 10:34:02 +0100
Subject: [R] Reading fixed column format
In-Reply-To: <3EBEC9D3-559C-4607-838D-46042D36A3AA@virginia.edu>
References: <loom.20060911T172146-100@post.gmane.org><BAY116-DAV571634EB23B6A0F2F6502CF2A0@phx.gbl>	<loom.20060911T185053-310@post.gmane.org>	<BAY116-DAV15C8CB3C190E253F6BD659CF2A0@phx.gbl>	<loom.20060912T084704-214@post.gmane.org>
	<3EBEC9D3-559C-4607-838D-46042D36A3AA@virginia.edu>
Message-ID: <45067F0A.3060304@lancaster.ac.uk>

Michael Kubovy wrote:

> Please consider saving your data in a way that will make it easier to
>  read into R. No program can read every dataset.

going back to the original post, there seems to be a couple of hanging 
questions:

> None of these seem to read non-coniguous variables from columns; or 
> may be I am missing something. "read.fwf" is not meant for large
> files according to a post in the archives. Thanks for the pointers. I
> have read the R data input and output. Anupam.

  First up, how 'large' is your 'large ASCII file'? How many rows and 
columns?

  Secondly, what are 'non-contiguous' variables?

  Perhaps if you posted the first few lines and columns of the file then 
we might get an idea of how to read it in.

Barry


From phhs80 at gmail.com  Tue Sep 12 11:49:15 2006
From: phhs80 at gmail.com (Paul Smith)
Date: Tue, 12 Sep 2006 10:49:15 +0100
Subject: [R] Gnuplot epslatex format also in R?
Message-ID: <6ade6f6c0609120249k1ec5bdb2tec2f185f42ce1a2d@mail.gmail.com>

Dear All

Is there some way of exporting R plots to epslatex, i.e., to a file
with the eps file and another one with the LaTeX commands
(representing the text in the plots), likewise Gnuplot does? If so,
could you please indicate it to me?

Thanks in advance,

Paul


From singularitaet at gmx.net  Tue Sep 12 12:00:46 2006
From: singularitaet at gmx.net (Stefan Grosse)
Date: Tue, 12 Sep 2006 12:00:46 +0200
Subject: [R] Gnuplot epslatex format also in R?
In-Reply-To: <6ade6f6c0609120249k1ec5bdb2tec2f185f42ce1a2d@mail.gmail.com>
References: <6ade6f6c0609120249k1ec5bdb2tec2f185f42ce1a2d@mail.gmail.com>
Message-ID: <4506854E.6060800@gmx.net>

I did an eps/ps file with e.g.:

postscript("c:/Temp/test.eps", width = 8.0, height = 6.0, horizontal =
FALSE, onefile = FALSE, paper = "special")

curve(x^2)

dev.off()

but I am not sure what you mean with the second file with the latex
commands.

Paul Smith schrieb:
> Dear All
>
> Is there some way of exporting R plots to epslatex, i.e., to a file
> with the eps file and another one with the LaTeX commands
> (representing the text in the plots), likewise Gnuplot does? If so,
> could you please indicate it to me?
>
> Thanks in advance,
>
> Paul
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


From ripley at stats.ox.ac.uk  Tue Sep 12 12:27:29 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 12 Sep 2006 11:27:29 +0100 (BST)
Subject: [R] Gnuplot epslatex format also in R?
In-Reply-To: <6ade6f6c0609120249k1ec5bdb2tec2f185f42ce1a2d@mail.gmail.com>
References: <6ade6f6c0609120249k1ec5bdb2tec2f185f42ce1a2d@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0609121126280.5673@gannet.stats.ox.ac.uk>

On Tue, 12 Sep 2006, Paul Smith wrote:

> Is there some way of exporting R plots to epslatex, i.e., to a file
> with the eps file and another one with the LaTeX commands
> (representing the text in the plots), likewise Gnuplot does? If so,
> could you please indicate it to me?

R has an xfig driver, and AFAIK you can do this from xfig.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From joe-byers at utulsa.edu  Thu Sep  7 22:26:52 2006
From: joe-byers at utulsa.edu (Joe Byers)
Date: Thu, 07 Sep 2006 15:26:52 -0500
Subject: [R] Help understanding how nls parses the formula argument to
 estimate the model
Message-ID: <edpv9c$j4g$1@sea.gmane.org>

I could use some help understanding how nls parses the formula argument 
to a model.frame and estimates the model.  I am trying to utilize the 
functionality of the nls formula argument to modify garchFit() to handle 
other variables in the mean equation besides just an arma(u,v) 
specification.

My nonlinear model is
     y<-nls(t~a*sin(w*2*pi/365*id+p)+b*id+int,data=t1,
	start=list(w=.5,a=.1,p=.5,b=init.y$coef[2],int=init.y$coef[1] ),
	control=list(maxiter=1000000,minFactor=1e-18))
where t is change in daily temperatures, id is just a time trend and the 
a*sin is a one year fourier series.

I have tried to debug the nls code using the following code
t1<-data.frame(t=as.vector(x),id=index(x))
data=t1;
formula <- as.formula(t ~ a *sin(w *2* pi/365 * id + p) + b * id + int);
     varNames <- all.vars(formula)
     algorithm<-'default';
     mf <- match.call(definition=nls,expand.dots=FALSE,
     call('nls',formula, data=parent.frame(),start,control = nls.control(),
     algorithm = "default", trace = FALSE,
     subset, weights, na.action, model = FALSE, lower = -Inf,
     upper = Inf));
     mWeights<-F;#missing(weights);
	start=list(w=.5,a=.1,p=.5,b=init.y$coef[2],int=init.y$coef[1] );
     pnames <- names(start);
      varNames <- varNames[is.na(match(varNames, pnames, nomatch = NA))]

	varIndex <- sapply(varNames,
		function(varName, data, respLength) {
         	length(eval(as.name(varName), data))%%respLength == 0},
         	 data, length(eval(formula[[2]], data))
         );
	mf$formula <- as.formula(paste("~", paste(varNames[varIndex],
         collapse = "+")), env = environment(formula));
	mf$start <- NULL;mf$control <- NULL;mf$algorithm <- NULL;
	mf$trace <- NULL;mf$model <- NULL;
     mf$lower <- NULL;mf$upper <- NULL;
     mf[[1]] <- as.name("model.frame");
     mf<-evalq(mf,data);
     n<-nrow(mf)
     mf<-as.list(mf);
     wts <- if (!mWeights)
         model.weights(mf)
     else rep(1, n)
     if (any(wts < 0 | is.na(wts)))
         stop("missing or negative weights not allowed")

     m <- switch(algorithm,
     		plinear = nlsModel.plinear(formula, mf, start, wts),
     		port = nlsModel(formula, mf, start, wts, upper),
     		nlsModel(formula, mf, start, wts));

I am struggling with the environment issues associated with performing 
these operations.

thank you


From mngwenya at stats.uct.ac.za  Tue Sep 12 13:57:22 2006
From: mngwenya at stats.uct.ac.za (mzabalazo ngwenya)
Date: Tue, 12 Sep 2006 13:57:22 +0200
Subject: [R] nls
Message-ID: <4506A0A2.3AD3D236@stats.uct.ac.za>

Hello everyone !

I am trying to write a short program to estimate  semivariogram
parameters. But I keep running into a problem when using the nls
function.

Could you please shed some light. I have put a sample of one of the
codes and ran a short example so you see what I mean.

-----------------

fit.gaus<-function(coordinates,values,guess.c0,guess.c1,guess.a)
{
         long<-rep(coordinates[,1],each=length(coordinates[,1]))
        
lag.long<-t(matrix(long,nrow=length(coordinates[,1]),byrow=TRUE))
         dif.long <-(lag.long-t(lag.long))^2
         lat <-rep(coordinates[,2],each=length(coordinates[,2]))
         lag.lat<-t(matrix(lat,nrow=length(coordinates[,2]),byrow=TRUE))
         dif.lat <-(lag.lat-t(lag.lat))^2
         h <-sqrt(dif.long+dif.lat) 
         
                                print (h)  #distance matrix between data
points
         
         
        if( length(values[1,])>1)
               {
                     y.m <-apply(values,1,sum,na.rm=TRUE)
                     y.m <-as.matrix(y.m)
                     y.mod <-(1/length(values[1,]))*(y.m)
            }
         else
               {
                      y.mod <-as.matrix(values)
            }

        semi <-rep(y.mod,each=length(y.mod))
        mat1<-t(matrix(semi,nrow=length(y.mod),byrow=TRUE))
        mat2<-t(mat1)
        semivariance <-(1/2)*(mat1-mat2)^2

        model <-semivariance ~c0+c1*(1-exp(-(h^2)/a^2)) 
        parameters <-nls(model,start =
list(c0=guess.c0,c1=guess.c1,a=guess.a),trace=TRUE)
        results <-summary(parameters)
                      print(results)
} 

--------

>  don <-matrix(c(2,3,9,6,5,2,7,9,5,3),5,2)
> don
     [,1] [,2]
[1,]    2    2
[2,]    3    7
[3,]    9    9
[4,]    6    5
[5,]    5    3
>  data <-matrix(c(3,4,2,4,6))
> data
     [,1]
[1,]    3
[2,]    4
[3,]    2
[4,]    4
[5,]    6
> fit.gaus(don,data,2,3,5)
         [,1]     [,2]     [,3]     [,4]     [,5]
[1,] 0.000000 5.099020 9.899495 5.000000 3.162278
[2,] 5.099020 0.000000 6.324555 3.605551 4.472136
[3,] 9.899495 6.324555 0.000000 5.000000 7.211103
[4,] 5.000000 3.605551 5.000000 0.000000 2.236068
[5,] 3.162278 4.472136 7.211103 2.236068 0.000000
178.9113 :  2 3 5 
Error in qr.qty(QR, resid) : 'qr' and 'y' must have the same number of
rows
>


From petr.pikal at precheza.cz  Tue Sep 12 14:03:49 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 12 Sep 2006 14:03:49 +0200
Subject: [R] Reading fixed column format
In-Reply-To: <loom.20060912T084704-214@post.gmane.org>
Message-ID: <4506BE45.2482.151B195@localhost>

Hi

Well. I use R quite extensively for a quite a long time without 
knowing "perl, cut, awk etc". Do you think I shall learn it?

I agree with Barry Rowlingson that best way how to get a correct 
answer is to present all relevant information. Seems to me that 
read.table, read.fwf are obvious choce, but there are other read 
options as you can find out from help index, e.g. readLines, readBin.

Maybe you could try to fine tune readLines.

HTH
Petr

On 12 Sep 2006 at 6:47, Anupam Tyagi wrote:

To:             	r-help at stat.math.ethz.ch
From:           	Anupam Tyagi <AnupTyagi at yahoo.com>
Date sent:      	Tue, 12 Sep 2006 06:47:56 +0000 (UTC)
Subject:        	Re: [R] Reading fixed column format

> Jason Barnhart <jasoncbarnhart <at> msn.com> writes:
> 
> > 
> > These posts may be helpful.
> > http://tolstoy.newcastle.edu.au/R/help/05/06/5776.html
> > https://stat.ethz.ch/pipermail/r-help/2002-May/021145.html
> > 
> > Using scan directly may also work for you rather than read.fwf.
> > 
> > Also, there are posts regarding using other tools such a 'perl' or
> > 'cut' to prepocess the data before reading with R.  Searching the
> > archives with those keywords should help.
> 
> I new user should not have to learn "perl","cut", "awk", etc simply to
> be able to use R. Does not make sense to me.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From mngwenya at stats.uct.ac.za  Tue Sep 12 14:13:01 2006
From: mngwenya at stats.uct.ac.za (mzabalazo ngwenya)
Date: Tue, 12 Sep 2006 14:13:01 +0200
Subject: [R] nls
Message-ID: <4506A44D.294B67C8@stats.uct.ac.za>

Hello everyone !

I am trying to write a short program to estimate  semivariogram
parameters. But I keep running into a problem when using the nls
function.

Could you please shed some light. I have put a sample of one of the
codes and ran a short example so you see what I mean.

-----------------



fit.gaus<-function(coordinates,values,guess.c0,guess.c1,guess.a)
{
         long<-rep(coordinates[,1],each=length(coordinates[,1]))
        
lag.long<-t(matrix(long,nrow=length(coordinates[,1]),byrow=TRUE))
         dif.long <-(lag.long-t(lag.long))^2
         lat <-rep(coordinates[,2],each=length(coordinates[,2]))
         lag.lat<-t(matrix(lat,nrow=length(coordinates[,2]),byrow=TRUE))
         dif.lat <-(lag.lat-t(lag.lat))^2
         h <-sqrt(dif.long+dif.lat) 
         
                                        
         if( length(values[1,])>1)
               {
                     y.m <-apply(values,1,sum,na.rm=TRUE)
                     y.m <-as.matrix(y.m)
                     y.mod <-(1/length(values[1,]))*(y.m)
            }
         else
               {
                      y.mod <-as.matrix(values)
            }

        semi <-rep(y.mod,each=length(y.mod))
        mat1<-t(matrix(semi,nrow=length(y.mod),byrow=TRUE))
        mat2<-t(mat1)
        semivariance <-(1/2)*(mat1-mat2)^2

        model <-semivariance ~c0+c1*(1-exp(-(h^2)/a^2)) 
        parameters <-nls(model,start =
list(c0=guess.c0,c1=guess.c1,a=guess.a),trace=TRUE)
        results <-summary(parameters)
                      print(results)
} 
--------------------------

>  don <-matrix(c(2,3,9,6,5,2,7,9,5,3),5,2)
> don
     [,1] [,2]
[1,]    2    2
[2,]    3    7
[3,]    9    9
[4,]    6    5
[5,]    5    3
>  data <-matrix(c(3,4,2,4,6))
> data
     [,1]
[1,]    3
[2,]    4
[3,]    2
[4,]    4
[5,]    6
> fit.gaus(don,data,2,3,5)
         [,1]     [,2]     [,3]     [,4]     [,5]
[1,] 0.000000 5.099020 9.899495 5.000000 3.162278
[2,] 5.099020 0.000000 6.324555 3.605551 4.472136
[3,] 9.899495 6.324555 0.000000 5.000000 7.211103
[4,] 5.000000 3.605551 5.000000 0.000000 2.236068
[5,] 3.162278 4.472136 7.211103 2.236068 0.000000
178.9113 :  2 3 5 
Error in qr.qty(QR, resid) : 'qr' and 'y' must have the same number of
rows
>


From petr.pikal at precheza.cz  Tue Sep 12 14:20:05 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 12 Sep 2006 14:20:05 +0200
Subject: [R] rename cols
In-Reply-To: <loom.20060912T083021-596@post.gmane.org>
Message-ID: <4506C215.10046.1609615@localhost>

Hi

On 12 Sep 2006 at 6:44, Anupam Tyagi wrote:

To:             	r-help at stat.math.ethz.ch
From:           	Anupam Tyagi <AnupTyagi at yahoo.com>
Date sent:      	Tue, 12 Sep 2006 06:44:03 +0000 (UTC)
Subject:        	Re: [R] rename cols

> Christos Hatzis <christos <at> nuverabio.com> writes:
> 
> > 
> > Try this:
> > 
> > old.colnames <- colnames(my.439.vars.df)
> > old.colnames[old.colnames=="fksm"] <- "new.name.a" 
> > old.colnames[old.colnames=="klmk"] <- "new.name.b"
> 
> For a newcomer, it will be useful to have a function like this in the
> base R: that can take a list of old.names and new.names, and do the
> assignment. It is far more efficient to have functions that are shared
> via the R distribution, than having to write own functions for
> carrying out basic data management tasks, and simple routinely used
> statistical procedures. Most users would rather spend time on thinking
> about the substantive work, instead of figuring out how to
> program---this may be specially true for new users. This way the

R is programming environment and language so more or less even 
newcomers are expected to do some programming. And it is also 
volunteer project. I expect that renaming columns is task which is 
not done very often, so there did not come up anybody who was 
interested in programming such function. 

Also the pool of available functions seems to be quite extensive in 
base R and even bigger in all available packages. Sometimes is hard 
enough to remember correct function name.

There is also excellent help and manual pages with nice copy/paste 
feature examples. I wonder if you ever tried to get some help from 
e.g. Excel help. Quite often you are completely lost.

> functions used will also be more efficient and better designed than
> the typical new user.

If a new user does not wont to use command line syntax he/she can use 
some of available GUIS (see R GUI in home page) e.g. JGR.

Petr

> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From HDoran at air.org  Tue Sep 12 14:17:14 2006
From: HDoran at air.org (Doran, Harold)
Date: Tue, 12 Sep 2006 08:17:14 -0400
Subject: [R] rename cols
Message-ID: <2323A6D37908A847A7C32F1E3662C80E3B7EF9@dc1ex01.air.org>

I don't know, this seems pretty simple and intuitive (to me)

# Create a sample data set with 439 variables
tmp <- data.frame(matrix(c(rnorm(4390)), ncol=439))
colnames(tmp)<-paste("col", 1:439, sep = "")

# rename a certain variable in that dataset
names(tmp)[(which(names(tmp)=='col1'))]<-'NewName' 

Here you are using indexing and a simple evaluation to find a variable
by name in a large data set and rename it. R is so flexible that this
cat can be skinned in many (and maybe even easier) ways, though.

Harold


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Anupam Tyagi
> Sent: Tuesday, September 12, 2006 12:34 AM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] rename cols
> 
> For a newcomer who wants to rename variable "fksm" and "klmk" 
> in a dataframe of with 439 variables there is not easy and 
> intuitive solution. That person has to spend a lot of time 
> listing columns and counting columns or doing string searches 
> or using brackets within brackets within brackets to get a 
> simple thing done. Is there a simple function or solution to 
> this in R without using an add-on package?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From h.wickham at gmail.com  Tue Sep 12 14:31:43 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Tue, 12 Sep 2006 08:31:43 -0400
Subject: [R] rename cols
In-Reply-To: <loom.20060912T062854-847@post.gmane.org>
References: <5cd96f050609110949k5dfe1b1cg665558e50af1b56d@mail.gmail.com>
	<45059B36.1090203@acm.org> <loom.20060912T062854-847@post.gmane.org>
Message-ID: <f8e6ff050609120531k2c2012fege814daea5f1d633a@mail.gmail.com>

> For a newcomer who wants to rename variable "fksm" and "klmk" in a dataframe of
> with 439 variables there is not easy and intuitive solution. That person has to
> spend a lot of time listing columns and counting columns or doing string
> searches or using brackets within brackets within brackets to get a simple thing
> done. Is there a simple function or solution to this in R without using an
> add-on package?

I use:

rename <- function(x, replace) {
	replacement <-  replace[names(x)]
	names(x)[!is.na(replacement)] <- replacement[!is.na(replacement)]
	x
}

(which is available in the reshape package)

You use it like:

df <- data.frame(a=1:2, b=3:4)
df <- rename(df, c(a="variable 1"))

Hadley


From karloh at mi.uib.no  Tue Sep 12 14:30:45 2006
From: karloh at mi.uib.no (Karl Ove Hufthammer)
Date: Tue, 12 Sep 2006 14:30:45 +0200
Subject: [R] lattice cloud and conditional axis limits
Message-ID: <ee699l$h1b$1@sea.gmane.org>

I'm using the 'cloud' function in the 'lattice' package to produce
multi-panel 3D scatter plots. The range of the values used vary much
between each panel (especially on the z axis), so I wish the axis limits
to be calculated based on the (conditional) data.

Here's a minimal example:

library(lattice)
z=1:200
x=runif(200)
y=runif(200)
s=factor(rep(c(1,2),each=100))
cloud(z~x*y|s,scales=list(arrows=FALSE))

On the first panel, the z values are in the range [1,100], and on the second
panel, they are in the range [101,200]. I wish the z axis to reflect this,
i.e., to only span these values. (In my actual data sets, this does make
more sense than in this example.) If 'cloud' worked the same way the
'xypanel' function does, one of the following would work:

cloud(z~x*y|s,scales=list(arrows=FALSE,z=list(relation="free")))
cloud(z~x*y|s,scales=list(arrows=FALSE,relation="free"))

However, it does not. Any ideas how I can make it work?

-- 
Karl Ove Hufthammer
E-mail and Jabber: karl at huftis.org


From petr.pikal at precheza.cz  Tue Sep 12 14:43:26 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 12 Sep 2006 14:43:26 +0200
Subject: [R] rename cols
In-Reply-To: <f8e6ff050609120531k2c2012fege814daea5f1d633a@mail.gmail.com>
References: <loom.20060912T062854-847@post.gmane.org>
Message-ID: <4506C78E.7936.175F815@localhost>

Hi

There is even an Excel like possibility for renaming columns.

try

newDF<-edit(oldDF)

you can go through columns and after clicking on header you can 
change column name.

Petr

On 12 Sep 2006 at 8:31, hadley wickham wrote:

Date sent:      	Tue, 12 Sep 2006 08:31:43 -0400
From:           	"hadley wickham" <h.wickham at gmail.com>
To:             	"Anupam Tyagi" <AnupTyagi at yahoo.com>
Copies to:      	r-help at stat.math.ethz.ch
Subject:        	Re: [R] rename cols

> > For a newcomer who wants to rename variable "fksm" and "klmk" in a
> > dataframe of with 439 variables there is not easy and intuitive
> > solution. That person has to spend a lot of time listing columns and
> > counting columns or doing string searches or using brackets within
> > brackets within brackets to get a simple thing done. Is there a
> > simple function or solution to this in R without using an add-on
> > package?
> 
> I use:
> 
> rename <- function(x, replace) {
>  replacement <-  replace[names(x)]
>  names(x)[!is.na(replacement)] <- replacement[!is.na(replacement)]
>  x
> }
> 
> (which is available in the reshape package)
> 
> You use it like:
> 
> df <- data.frame(a=1:2, b=3:4)
> df <- rename(df, c(a="variable 1"))
> 
> Hadley
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From aeklund at kth.se  Tue Sep 12 14:49:18 2006
From: aeklund at kth.se (Anders Eklund)
Date: Tue, 12 Sep 2006 14:49:18 +0200 (CEST)
Subject: [R] (no subject)
Message-ID: <1077.213.89.84.14.1158065358.squirrel@webmail.sys.kth.se>

Hi,

I have a problem with aggregate.

x <- aggregate(t1,list(t2,t3,t4), mean)

z<-x[,3]

I want z to be a vector but it is a factor.

I've tried to use as.vector(z,mode="numeric") but then the numbers get
scrambeled.

Any help is appriciated

/anders


From phhs80 at gmail.com  Tue Sep 12 14:55:48 2006
From: phhs80 at gmail.com (Paul Smith)
Date: Tue, 12 Sep 2006 13:55:48 +0100
Subject: [R] Gnuplot epslatex format also in R?
In-Reply-To: <Pine.LNX.4.64.0609121126280.5673@gannet.stats.ox.ac.uk>
References: <6ade6f6c0609120249k1ec5bdb2tec2f185f42ce1a2d@mail.gmail.com>
	<Pine.LNX.4.64.0609121126280.5673@gannet.stats.ox.ac.uk>
Message-ID: <6ade6f6c0609120555pa300f9er9995892182741a8@mail.gmail.com>

On 9/12/06, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> > Is there some way of exporting R plots to epslatex, i.e., to a file
> > with the eps file and another one with the LaTeX commands
> > (representing the text in the plots), likewise Gnuplot does? If so,
> > could you please indicate it to me?
>
> R has an xfig driver, and AFAIK you can do this from xfig.

Yes, your suggestion works! Thanks.

Paul


From knut.wenzig at phil.uni-augsburg.de  Tue Sep 12 15:02:38 2006
From: knut.wenzig at phil.uni-augsburg.de (Knut Wenzig)
Date: Tue, 12 Sep 2006 15:02:38 +0200
Subject: [R] Kendall's tau-c
Message-ID: <4506AFEE.8040505@phil.uni-augsburg.de>

Hello,

I can't find a package which calculates Kendall's tau-c. There is the
package Kendall, but it only calcuates Kendall's tau-b.

Here is the example from
ttp://www2.chass.ncsu.edu/garson/pa765/assocordinal.htm.

cityriots <- data.frame(citysize=c(1,1,2,2,3,3),
riotsize=c(1,2,1,2,1,2), weight=c(4,2,2,3,0,4))
cityriots <- data.frame(lapply(cityriots,function(x)
rep(x,cityriots$weight)))
xtabs(~ riotsize+citysize,cityriots)

tau-c should be .57.

Do you have a hint?

Best regards

Knut Wenzig

-- 
Dipl.-Sozialw. Knut Wenzig
Lehrstuhl fuer Soziologie und empirische Sozialforschung
Universitaet Augsburg | Universitaetsstrasse 6     |  86159 Augsburg
Tel ++49 821 598-4101 | FAX -4222 | ICQ: 210200999 |  Germany
http://www.philso.uni-augsburg.de/lehrstuehle/soziologie/sozio2/


From petr.pikal at precheza.cz  Tue Sep 12 15:04:46 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 12 Sep 2006 15:04:46 +0200
Subject: [R] factor to numeric  (no subject)
In-Reply-To: <1077.213.89.84.14.1158065358.squirrel@webmail.sys.kth.se>
Message-ID: <4506CC8E.5613.1897DA4@localhost>

Hi

all culumns which are used for discrimination (in your case 3) are 
factor. If you want to change them to numeric you has to use

as.numeric(as.character(x[,3]))

I believe it is in FAQ.

HTH
Petr

Please use sensible subject.


On 12 Sep 2006 at 14:49, Anders Eklund wrote:

Date sent:      	Tue, 12 Sep 2006 14:49:18 +0200 (CEST)
From:           	"Anders Eklund" <aeklund at kth.se>
To:             	"r-help at lists.R-project.org" <r-help at stat.math.ethz.ch>
Subject:        	[R] (no subject)

> Hi,
> 
> I have a problem with aggregate.
> 
> x <- aggregate(t1,list(t2,t3,t4), mean)
> 
> z<-x[,3]
> 
> I want z to be a vector but it is a factor.
> 
> I've tried to use as.vector(z,mode="numeric") but then the numbers get
> scrambeled.
> 
> Any help is appriciated
> 
> /anders
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From mothsailor at googlemail.com  Tue Sep 12 15:09:08 2006
From: mothsailor at googlemail.com (David Barron)
Date: Tue, 12 Sep 2006 14:09:08 +0100
Subject: [R] Fwd:  (no subject)
In-Reply-To: <815b70590609120608t7e85f29cm31447a2959d015ca@mail.gmail.com>
References: <1077.213.89.84.14.1158065358.squirrel@webmail.sys.kth.se>
	<815b70590609120608t7e85f29cm31447a2959d015ca@mail.gmail.com>
Message-ID: <815b70590609120609o364b1fd4na36adc65dab5d475@mail.gmail.com>

>From the FAQ:

7.10 How do I convert factors to numeric?

It may happen that when reading numeric data into R (usually, when
reading in a file), they come in as factors. If f is such a factor
object, you can use

     as.numeric(as.character(f))

to get the numbers back. More efficient, but harder to remember, is

     as.numeric(levels(f))[as.integer(f)]

In any case, do not call as.numeric() or their likes directly for the
task at hand (as as.numeric() or unclass() give the internal codes).

On 12/09/06, Anders Eklund <aeklund at kth.se> wrote:
> Hi,
>
> I have a problem with aggregate.
>
> x <- aggregate(t1,list(t2,t3,t4), mean)
>
> z<-x[,3]
>
> I want z to be a vector but it is a factor.
>
> I've tried to use as.vector(z,mode="numeric") but then the numbers get
> scrambeled.
>
> Any help is appriciated
>
> /anders
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


--
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From mike.prager at noaa.gov  Tue Sep 12 15:27:04 2006
From: mike.prager at noaa.gov (Michael Prager)
Date: Tue, 12 Sep 2006 09:27:04 -0400
Subject: [R] Command equivalent of rgui "File, Save to File"?
References: <oofbg2tmd9ef64j9eu59n9nf522kb2oahi@4ax.com>
	<45060160.6040906@stats.uwo.ca>
Message-ID: <p3ddg2p4it8ljl8e50sr8rr4v8k60fsijn@4ax.com>

Duncan Murdoch wrote:

> On 9/11/2006 3:57 PM, Michael Prager wrote:
> > R 2.3.1 on Windows XP Professional.
> > 
> > I am writing some scripts to generate examples.  The Rgui menu
> > item "File, Save to File" is helpful.  Is there perhaps an
> > equivalent R function that can be incorporated into a script?
> 
> I think sink() is the closest you can get: set R to write to a file 
> before generating whatever output you want to save.
> 
> The menu item writes out the GUI text buffer; the R core doesn't know 
> what's in that buffer.  Other front ends don't have a buffer at all.
> 
[...]

Thanks, Duncan, for the helpful response.  Unless I have
overlooked something (quite possible), sink() saves only the
output, not the input as well.  I'll continue using the menu
system -- which I am delighted to have.

Mike

Mike Prager
Southeast Fisheries Science Center, NOAA
Beaufort, North Carolina  USA


From gregor.gorjanc at bfro.uni-lj.si  Tue Sep 12 15:34:39 2006
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Tue, 12 Sep 2006 15:34:39 +0200
Subject: [R] Test internet presence
In-Reply-To: <971536df0609112253t413c526y2fcf8f668e51f77@mail.gmail.com>
References: <XFMail.060911143026.Ted.Harding@nessie.mcc.ac.uk>	
	<4505EA53.5010502@bfro.uni-lj.si>
	<971536df0609112253t413c526y2fcf8f668e51f77@mail.gmail.com>
Message-ID: <4506B76F.9030402@bfro.uni-lj.si>

Gabor Grothendieck wrote:
> Here is a variation for Windows.  The second line returns TRUE or FALSE
> and may need to be varied if the output of ping is not the same on your
> system as on mine:
> 
> ping <- system("ping www.google.com", intern = TRUE)
> as.numeric(strsplit(grep("Received", ping, value = TRUE), "[
> ,]")[[1]][8]) > 0
> 

...

Thank you Gabor and Ted! These are all fine ways, but as both of you
have stated not really general as you never know what ping will produce
on different versions etc. I am really keen on test from
tests/internet.R. If that test is OK for base R, I do not see why it
should not be OK for R package. I just hope to get some more feedback on
my question[1] to prof. Ripley.

[1]https://stat.ethz.ch/pipermail/r-help/2006-September/112766.html

Thank you!

-- 
Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana     PhD student
Biotechnical Faculty
Zootechnical Department     URI: http://www.bfro.uni-lj.si/MR/ggorjan
Groblje 3                   mail: gregor.gorjanc <at> bfro.uni-lj.si

SI-1230 Domzale             tel: +386 (0)1 72 17 861
Slovenia, Europe            fax: +386 (0)1 72 17 888

----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.


From james.milks at wright.edu  Tue Sep 12 15:35:50 2006
From: james.milks at wright.edu (James R. Milks)
Date: Tue, 12 Sep 2006 09:35:50 -0400
Subject: [R] Using XY location data to calculate ecological parameters
Message-ID: <7927FFC1-1BE3-4190-8E70-D0EB93F0E4DF@wright.edu>

Dear R gurus,

I have XY data giving the locations of tree seedlings that were  
surveyed during a 210 meter belt transect.  This belt transect was  
taken by stretching a line across the field, then measuring all  
seedlings within 1 meter on either side of the line.  The end result  
was XY coordinates and height for ~1,300 seedlings.  I would like to  
use that data to calculate density of seedlings per 10 meters per  
species and relative species abundance in order to compare between  
this transect and other transects as well as to compare between  
segments of the same transect.  Are there any R packages out there  
that will allow me make those calculations?  To give an idea of my  
data, here's a fictional example:

Species				X (m)	Y(m)
Acer negundo		90.10	-.19
Acer negundo		90.14	-.90
Acer saccharinum	90.25	.54
Acer rubrum			90.89	.21
Acer negundo		91.25	.36
Acer negundo		91.46	-.65
etc.

Thanks.

Jim Milks


From MSchwartz at mn.rr.com  Tue Sep 12 15:46:44 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Tue, 12 Sep 2006 08:46:44 -0500
Subject: [R] Kendall's tau-c
In-Reply-To: <4506AFEE.8040505@phil.uni-augsburg.de>
References: <4506AFEE.8040505@phil.uni-augsburg.de>
Message-ID: <1158068804.6717.19.camel@localhost.localdomain>

On Tue, 2006-09-12 at 15:02 +0200, Knut Wenzig wrote:
> Hello,
> 
> I can't find a package which calculates Kendall's tau-c. There is the
> package Kendall, but it only calcuates Kendall's tau-b.
> 
> Here is the example from
> ttp://www2.chass.ncsu.edu/garson/pa765/assocordinal.htm.
> 
> cityriots <- data.frame(citysize=c(1,1,2,2,3,3),
> riotsize=c(1,2,1,2,1,2), weight=c(4,2,2,3,0,4))
> cityriots <- data.frame(lapply(cityriots,function(x)
> rep(x,cityriots$weight)))
> xtabs(~ riotsize+citysize,cityriots)
> 
> tau-c should be .57.
> 
> Do you have a hint?
> 
> Best regards
> 
> Knut Wenzig


Here is some code:

# Calculate CONcordant Pairs in a table
# cycle through x[r, c] and multiply by
# sum(x elements below and to the right of x[r, c])
# x = table
concordant <- function(x)
{
  x <- matrix(as.numeric(x), dim(x))
  
  # get sum(matrix values > r AND > c)
  # for each matrix[r, c]
  mat.lr <- function(r, c)
  { 
    lr <- x[(r.x > r) & (c.x > c)]
    sum(lr)
  }

  # get row and column index for each
  # matrix element
  r.x <- row(x)
  c.x <- col(x)

  # return the sum of each matrix[r, c] * sums
  # using mapply to sequence thru each matrix[r, c]
  sum(x * mapply(mat.lr, r = r.x, c = c.x))
}

# Calculate DIScordant Pairs in a table
# cycle through x[r, c] and multiply by
# sum(x elements below and to the left of x[r, c])
# x = table
discordant <- function(x)
{
  x <- matrix(as.numeric(x), dim(x))
  
  # get sum(matrix values > r AND < c)
  # for each matrix[r, c]
  mat.ll <- function(r, c)
  { 
    ll <- x[(r.x > r) & (c.x < c)]
    sum(ll)
  }

  # get row and column index for each
  # matrix element
  r.x <- row(x)
  c.x <- col(x)

  # return the sum of each matrix[r, c] * sums
  # using mapply to sequence thru each matrix[r, c]
  sum(x * mapply(mat.ll, r = r.x, c = c.x))
}


# Calculate Kendall-Stuart Tau-c
# x = table
calc.KSTc <- function(x)
{
  x <- matrix(as.numeric(x), dim(x))
  
  c <- concordant(x)
  d <- discordant(x)
  m <- min(dim(x))
  n <- sum(x)

  KSTc <- (m * 2 * (c - d)) / ((n ^ 2) * (m - 1))

  KSTc
}


> calc.KSTc(with(cityriots, table(riotsize, citysize)))
[1] 0.5688889


The above code, along with other such measures, will eventually find its
way into the CrossTable() function in the gmodels CRAN package when time
permits (which seems to be in short supply of late...)

HTH,

Marc Schwartz


From murdoch at stats.uwo.ca  Tue Sep 12 16:03:44 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 12 Sep 2006 10:03:44 -0400
Subject: [R] Command equivalent of rgui "File, Save to File"?
In-Reply-To: <p3ddg2p4it8ljl8e50sr8rr4v8k60fsijn@4ax.com>
References: <oofbg2tmd9ef64j9eu59n9nf522kb2oahi@4ax.com>	<45060160.6040906@stats.uwo.ca>
	<p3ddg2p4it8ljl8e50sr8rr4v8k60fsijn@4ax.com>
Message-ID: <4506BE40.5030901@stats.uwo.ca>

On 9/12/2006 9:27 AM, Michael Prager wrote:
> Duncan Murdoch wrote:
> 
>> On 9/11/2006 3:57 PM, Michael Prager wrote:
>> > R 2.3.1 on Windows XP Professional.
>> > 
>> > I am writing some scripts to generate examples.  The Rgui menu
>> > item "File, Save to File" is helpful.  Is there perhaps an
>> > equivalent R function that can be incorporated into a script?
>> 
>> I think sink() is the closest you can get: set R to write to a file 
>> before generating whatever output you want to save.
>> 
>> The menu item writes out the GUI text buffer; the R core doesn't know 
>> what's in that buffer.  Other front ends don't have a buffer at all.
>> 
> [...]
> 
> Thanks, Duncan, for the helpful response.  Unless I have
> overlooked something (quite possible), sink() saves only the
> output, not the input as well.  I'll continue using the menu
> system -- which I am delighted to have.
> 

Another possibility is to put your script into a file (e.g. script.R), 
and run

Rcmd BATCH script.R

This puts the whole transcript of the session into script.Rout.  It's 
not identical to running in the gui, because functions can tell whether 
they're running interactively or not, but it's close.

Duncan Murdoch


From Thierry.ONKELINX at inbo.be  Tue Sep 12 16:05:14 2006
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Tue, 12 Sep 2006 16:05:14 +0200
Subject: [R] Using XY location data to calculate ecological parameters
Message-ID: <2E9C414912813E4EB981326983E0A104020F3D16@inexch.instnat.be.grp>

Assuming you only use the X direction to split the data into 10 m cells.


table(list(Species, round(X, digits = -1)))

This will generate a table with the number of seedlings per species in
each 10 m cell. Divide this by the area of each cell and you get the
densities.

Cheers,

Thierry

------------------------------------------------------------------------
----

ir. Thierry Onkelinx

Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
and Forest

Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance

Gaverstraat 4

9500 Geraardsbergen

Belgium

tel. + 32 54/436 185

Thierry.Onkelinx op inbo.be

www.inbo.be 

-----Oorspronkelijk bericht-----
Van: r-help-bounces op stat.math.ethz.ch
[mailto:r-help-bounces op stat.math.ethz.ch] Namens James R. Milks
Verzonden: dinsdag 12 september 2006 15:36
Aan: r-help op stat.math.ethz.ch
Onderwerp: [R] Using XY location data to calculate ecological parameters

Dear R gurus,

I have XY data giving the locations of tree seedlings that were  
surveyed during a 210 meter belt transect.  This belt transect was  
taken by stretching a line across the field, then measuring all  
seedlings within 1 meter on either side of the line.  The end result  
was XY coordinates and height for ~1,300 seedlings.  I would like to  
use that data to calculate density of seedlings per 10 meters per  
species and relative species abundance in order to compare between  
this transect and other transects as well as to compare between  
segments of the same transect.  Are there any R packages out there  
that will allow me make those calculations?  To give an idea of my  
data, here's a fictional example:

Species				X (m)	Y(m)
Acer negundo		90.10	-.19
Acer negundo		90.14	-.90
Acer saccharinum	90.25	.54
Acer rubrum			90.89	.21
Acer negundo		91.25	.36
Acer negundo		91.46	-.65
etc.

Thanks.

Jim Milks

______________________________________________
R-help op stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From bates at stat.wisc.edu  Tue Sep 12 16:58:11 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 12 Sep 2006 09:58:11 -0500
Subject: [R] Conservative "ANOVA tables" in lmer
In-Reply-To: <1158010468.3278.24.camel@solidago.localdomain>
References: <1A8FE5B0-3FE3-4CA9-9224-9CACB9AF4854@muohio.edu>
	<40e66e0b0609070559u656463c1l52a60eda9a095237@mail.gmail.com>
	<20060910055624.GA12212@ms.unimelb.edu.au>
	<40e66e0b0609110943h7dfb057dv12567975ac04bdb2@mail.gmail.com>
	<1158010468.3278.24.camel@solidago.localdomain>
Message-ID: <40e66e0b0609120758g120f1e8ao746fb2671f1d05d6@mail.gmail.com>

On 9/11/06, Manuel Morales <Manuel.A.Morales at williams.edu> wrote:
> On Mon, 2006-09-11 at 11:43 -0500, Douglas Bates wrote:
> > On 9/10/06, Andrew Robinson <A.Robinson at ms.unimelb.edu.au> wrote:
> > > On Thu, Sep 07, 2006 at 07:59:58AM -0500, Douglas Bates wrote:
> > >
> > > > I would be happy to re-institute p-values for fixed effects in the
> > > > summary and anova methods for lmer objects using a denominator degrees
> > > > of freedom based on the trace of the hat matrix or the rank of Z:X if
> > > > others will volunteer to respond to the "these answers are obviously
> > > > wrong because they don't agree with <whatever> and the idiot who wrote
> > > > this software should be thrashed to within an inch of his life"
> > > > messages.  I don't have the patience.
> > >
> > > This seems to be more than fair to me.  I'll volunteer to help explain
> > > why the anova.lmer() output doesn't match SAS, etc.  Is it worth
> > > putting a caveat in the output and the help files?  Is it even worth
> > > writing a FAQ about this?
> >
> > Having made that offer I think I will now withdraw it.  Peter's
> > example has convinced me that this is the wrong thing to do.
> >
> > I am encouraged by the fact that the results from mcmcsamp correspond
> > closely to the correct theoretical results in the case that Peter
> > described.  I appreciate that some users will find it difficult to
> > work with a MCMC sample (or to convince editors to accept results
> > based on such a sample) but I think that these results indicate that
> > it is better to go after the marginal distribution of the fixed
> > effects estimates (which is what is being approximated by the MCMC
> > sample - up to Bayesian/frequentist philosophical differences) than to
> > use the conditional distribution and somehow try to adjust the
> > reference distribution.
>
> Am I right that the MCMC sample can not be used, however, to evaluate
> the significance of parameter groups. For example, to assess the
> significance of a three-level factor? Are there better alternatives than
> simply adjusting the CI for the number of factor levels
> (1-alpha/levels).

Hmm - I'm not sure what confidence interval and what number of levels
you mean there so I can't comment on that method.

Suppose we go back to Spencer's example and consider if there is a
signficant effect for the Nozzle factor.  That is equivalent to the
hypothesis H_0: beta_2 = beta_3 = 0 versus the general alternative.  A
"p-value" could be formulated from an MCMC sample if we assume that
the marginal distribution of the parameter estimates for beta_2 and
beta_3 has roughly elliptical contours and you can evaluate that by,
say, examining a hexbin plot of the values in the MCMC sample. One
could take the ellipses as defined by the standard errors and
estimated correlation or, probably better, by the observed standard
deviations and correlations in the MCMC sample.  Then determine the
proportion of (beta_2, beta_3) pairs in the sample that fall outside
the ellipse centered at the estimates and with that eccentricity and
scaling factors that passes through (0,0).  That would be an empirical
p-value for the test.

I would recommend calculating this for a couple of samples to check on
the reproducibility.


From john.kornak at ucsf.edu  Tue Sep 12 16:58:24 2006
From: john.kornak at ucsf.edu (John Kornak)
Date: Tue, 12 Sep 2006 07:58:24 -0700
Subject: [R] Installation difficulty with "rimage"
In-Reply-To: <efb536d50609120648w1b02ef2fj8d130be8f0f8da09@mail.gmail.com>
References: <4505A71A.3060803@ucsf.edu>	
	<Pine.LNX.4.64.0609112227540.16814@gannet.stats.ox.ac.uk>	
	<450614FD.6070704@ucsf.edu>
	<efb536d50609120648w1b02ef2fj8d130be8f0f8da09@mail.gmail.com>
Message-ID: <4506CB10.6090308@ucsf.edu>


Thanks again to both Sarah Goslee and Professor Ripley.

Installing the libjpeg-devel package was the key.

John

Sarah Goslee wrote:

> Well, there are two possible problems.
> One, you don't have the file.
> Two, it isn't where it's supposed to be.
>
> Did you install it from the rpm?
> If so, did you install both libjpeg and libjpeg-devel
> The header files (*.h) are usually in the devel packages,
> and are needed if you want to compile R code against
> them (which you do).
>
> The easiest way to do it is to install both packages from
> the rpms, using yum or however you prefer to do it,
> rather than to install the library as source a package from
> the developers. It sounds like you did the latter?
>
> You may be able to convince rimage that everything is
> okay by putting a symbolic link from the actual header
> location to where it is supposed to be:
> ln -s /usr/local/bin/jpeg-6b/jpeglib.h /usr/include/jpeglib.h
>
> You will be best off, though (probably) by just installing
> the libjpeg-devel package.
>
> Sarah
>
> P.S. Prof. Ripley's answer is both correct and sufficient, of
> course, but possibly a bit terse - I thought you might appreciate
> a longer explanation.
>
>

-- 
John Kornak,PhD
Assistant Professor
Departments of Radiology, and Epidemiology & Biostatistics
University of California, San Francisco
Box 0946
San Francisco, CA 94143
Tel: (415) 353-4740
fax: (415) 353-9423
Email: john.kornak at ucsf.edu


From afshart at exchange.sba.miami.edu  Tue Sep 12 17:22:58 2006
From: afshart at exchange.sba.miami.edu (Afshartous, David)
Date: Tue, 12 Sep 2006 11:22:58 -0400
Subject: [R] levels of factor when subsetting the factor
Message-ID: <6BCB4D493A447546A8126F24332056E8041C504E@school1.business.edu>

 
All,

When I take a subset of a factor the reduced factor still maintains all
the original levels of the factor when say forming the key in a plot.
The data is correct, but the variable still "remembers" the original
levels.  See below for reproducible code.  Does anyone know how to fix
this?
cheers,
dave

fact = as.factor(c(rep("A", 3),rep("B", 3), rep("C", 3)))
new.fact = fact[1:6]
> new.fact
[1] A A A B B B
Levels: A B C    ## should only show A B


From mothsailor at googlemail.com  Tue Sep 12 17:29:06 2006
From: mothsailor at googlemail.com (David Barron)
Date: Tue, 12 Sep 2006 16:29:06 +0100
Subject: [R] levels of factor when subsetting the factor
In-Reply-To: <6BCB4D493A447546A8126F24332056E8041C504E@school1.business.edu>
References: <6BCB4D493A447546A8126F24332056E8041C504E@school1.business.edu>
Message-ID: <815b70590609120829s9cb6755wf2a861caea359149@mail.gmail.com>

Try

> new.fact = fact[1:6, drop=TRUE]



On 12/09/06, Afshartous, David <afshart at exchange.sba.miami.edu> wrote:
>
> All,
>
> When I take a subset of a factor the reduced factor still maintains all
> the original levels of the factor when say forming the key in a plot.
> The data is correct, but the variable still "remembers" the original
> levels.  See below for reproducible code.  Does anyone know how to fix
> this?
> cheers,
> dave
>
> fact = as.factor(c(rep("A", 3),rep("B", 3), rep("C", 3)))
> new.fact = fact[1:6]
> > new.fact
> [1] A A A B B B
> Levels: A B C    ## should only show A B
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From bates at stat.wisc.edu  Tue Sep 12 17:29:56 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 12 Sep 2006 10:29:56 -0500
Subject: [R] levels of factor when subsetting the factor
In-Reply-To: <6BCB4D493A447546A8126F24332056E8041C504E@school1.business.edu>
References: <6BCB4D493A447546A8126F24332056E8041C504E@school1.business.edu>
Message-ID: <40e66e0b0609120829v50fa934cpbdff50e92a188f61@mail.gmail.com>

On 9/12/06, Afshartous, David <afshart at exchange.sba.miami.edu> wrote:
>
> All,
>
> When I take a subset of a factor the reduced factor still maintains all
> the original levels of the factor when say forming the key in a plot.
> The data is correct, but the variable still "remembers" the original
> levels.  See below for reproducible code.  Does anyone know how to fix
> this?

Use the optional argument "drop = TRUE"

> cheers,
> dave
>
> fact = as.factor(c(rep("A", 3),rep("B", 3), rep("C", 3)))
> new.fact = fact[1:6]
> > new.fact
> [1] A A A B B B
> Levels: A B C    ## should only show A B

> fact[1:6, drop = TRUE]
[1] A A A B B B
Levels: A B


From HDoran at air.org  Tue Sep 12 17:27:55 2006
From: HDoran at air.org (Doran, Harold)
Date: Tue, 12 Sep 2006 11:27:55 -0400
Subject: [R] levels of factor when subsetting the factor
Message-ID: <2323A6D37908A847A7C32F1E3662C80E3B7F2D@dc1ex01.air.org>

Just add the following to your code

new.fact = fact[1:6, drop=T]

> new.fact
[1] A A A B B B
Levels: A B 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Afshartous, David
> Sent: Tuesday, September 12, 2006 11:23 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] levels of factor when subsetting the factor
> 
>  
> All,
> 
> When I take a subset of a factor the reduced factor still 
> maintains all the original levels of the factor when say 
> forming the key in a plot.
> The data is correct, but the variable still "remembers" the 
> original levels.  See below for reproducible code.  Does 
> anyone know how to fix this?
> cheers,
> dave
> 
> fact = as.factor(c(rep("A", 3),rep("B", 3), rep("C", 3))) 
> new.fact = fact[1:6]
> > new.fact
> [1] A A A B B B
> Levels: A B C    ## should only show A B
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From andy_liaw at merck.com  Tue Sep 12 17:31:58 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 12 Sep 2006 11:31:58 -0400
Subject: [R] levels of factor when subsetting the factor
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02DD816C@usctmx1106.merck.com>

You have at least two choices:

R> factor(fact[1:6])
[1] A A A B B B
Levels: A B
R> fact[1:6, drop=TRUE]
[1] A A A B B B
Levels: A B

HTH,
Andy


From: Afshartous, David
>  
> All,
> 
> When I take a subset of a factor the reduced factor still 
> maintains all
> the original levels of the factor when say forming the key in a plot.
> The data is correct, but the variable still "remembers" the original
> levels.  See below for reproducible code.  Does anyone know how to fix
> this?
> cheers,
> dave
> 
> fact = as.factor(c(rep("A", 3),rep("B", 3), rep("C", 3)))
> new.fact = fact[1:6]
> > new.fact
> [1] A A A B B B
> Levels: A B C    ## should only show A B
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}


From Thierry.ONKELINX at inbo.be  Tue Sep 12 17:32:47 2006
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Tue, 12 Sep 2006 17:32:47 +0200
Subject: [R] levels of factor when subsetting the factor
Message-ID: <2E9C414912813E4EB981326983E0A104020F3D64@inexch.instnat.be.grp>

factor(new.fact) will do the trick. But that will recode the levels and
that might be something you don't want.

> fact = as.factor(c(rep("A", 3),rep("B", 3), rep("C", 3)))
> new.fact = fact[1:6]
> new.fact
[1] A A A B B B
Levels: A B C
> factor(new.fact)
[1] A A A B B B
Levels: A B

Cheers,

Thierry

------------------------------------------------------------------------
----

ir. Thierry Onkelinx

Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
and Forest

Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance

Gaverstraat 4

9500 Geraardsbergen

Belgium

tel. + 32 54/436 185

Thierry.Onkelinx op inbo.be

www.inbo.be 


-----Oorspronkelijk bericht-----
Van: r-help-bounces op stat.math.ethz.ch
[mailto:r-help-bounces op stat.math.ethz.ch] Namens Afshartous, David
Verzonden: dinsdag 12 september 2006 17:23
Aan: r-help op stat.math.ethz.ch
Onderwerp: [R] levels of factor when subsetting the factor

 
All,

When I take a subset of a factor the reduced factor still maintains all
the original levels of the factor when say forming the key in a plot.
The data is correct, but the variable still "remembers" the original
levels.  See below for reproducible code.  Does anyone know how to fix
this?
cheers,
dave

fact = as.factor(c(rep("A", 3),rep("B", 3), rep("C", 3)))
new.fact = fact[1:6]
> new.fact
[1] A A A B B B
Levels: A B C    ## should only show A B

______________________________________________
R-help op stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From dimitris.rizopoulos at med.kuleuven.be  Tue Sep 12 17:35:22 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 12 Sep 2006 17:35:22 +0200
Subject: [R] levels of factor when subsetting the factor
References: <6BCB4D493A447546A8126F24332056E8041C504E@school1.business.edu>
Message-ID: <020101c6d681$0f523990$0540210a@www.domain>

check ?"[.factor", you need:

fact[1:6, drop = TRUE]


Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Afshartous, David" <afshart at exchange.sba.miami.edu>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, September 12, 2006 5:22 PM
Subject: [R] levels of factor when subsetting the factor


>
> All,
>
> When I take a subset of a factor the reduced factor still maintains 
> all
> the original levels of the factor when say forming the key in a 
> plot.
> The data is correct, but the variable still "remembers" the original
> levels.  See below for reproducible code.  Does anyone know how to 
> fix
> this?
> cheers,
> dave
>
> fact = as.factor(c(rep("A", 3),rep("B", 3), rep("C", 3)))
> new.fact = fact[1:6]
>> new.fact
> [1] A A A B B B
> Levels: A B C    ## should only show A B
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From rdpeng at gmail.com  Tue Sep 12 17:36:47 2006
From: rdpeng at gmail.com (Roger D. Peng)
Date: Tue, 12 Sep 2006 11:36:47 -0400
Subject: [R] levels of factor when subsetting the factor
In-Reply-To: <6BCB4D493A447546A8126F24332056E8041C504E@school1.business.edu>
References: <6BCB4D493A447546A8126F24332056E8041C504E@school1.business.edu>
Message-ID: <4506D40F.1040508@gmail.com>

I think you want 'fact[1:6, drop = TRUE]'

-roger

Afshartous, David wrote:
>  
> All,
> 
> When I take a subset of a factor the reduced factor still maintains all
> the original levels of the factor when say forming the key in a plot.
> The data is correct, but the variable still "remembers" the original
> levels.  See below for reproducible code.  Does anyone know how to fix
> this?
> cheers,
> dave
> 
> fact = as.factor(c(rep("A", 3),rep("B", 3), rep("C", 3)))
> new.fact = fact[1:6]
>> new.fact
> [1] A A A B B B
> Levels: A B C    ## should only show A B
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Roger D. Peng  |  http://www.biostat.jhsph.edu/~rpeng/


From HDoran at air.org  Tue Sep 12 17:36:40 2006
From: HDoran at air.org (Doran, Harold)
Date: Tue, 12 Sep 2006 11:36:40 -0400
Subject: [R] levels of factor when subsetting the factor
Message-ID: <2323A6D37908A847A7C32F1E3662C80E3B7F31@dc1ex01.air.org>

Also, it is probably easier to use gl() than coerce your data into a
factor

fact <- gl(3, 3, label = c("A", "B", "C")) 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Liaw, Andy
> Sent: Tuesday, September 12, 2006 11:32 AM
> To: Afshartous, David; r-help at stat.math.ethz.ch
> Subject: Re: [R] levels of factor when subsetting the factor
> 
> You have at least two choices:
> 
> R> factor(fact[1:6])
> [1] A A A B B B
> Levels: A B
> R> fact[1:6, drop=TRUE]
> [1] A A A B B B
> Levels: A B
> 
> HTH,
> Andy
> 
> 
> From: Afshartous, David
> >  
> > All,
> > 
> > When I take a subset of a factor the reduced factor still maintains 
> > all the original levels of the factor when say forming the key in a 
> > plot.
> > The data is correct, but the variable still "remembers" the 
> original 
> > levels.  See below for reproducible code.  Does anyone know 
> how to fix 
> > this?
> > cheers,
> > dave
> > 
> > fact = as.factor(c(rep("A", 3),rep("B", 3), rep("C", 3))) 
> new.fact = 
> > fact[1:6]
> > > new.fact
> > [1] A A A B B B
> > Levels: A B C    ## should only show A B
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> > 
> > 
> 
> 
> --------------------------------------------------------------
> ----------------
> Notice:  This e-mail message, together with any 
> attachments,...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From p.dalgaard at biostat.ku.dk  Tue Sep 12 17:45:07 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 12 Sep 2006 17:45:07 +0200
Subject: [R] levels of factor when subsetting the factor
In-Reply-To: <6BCB4D493A447546A8126F24332056E8041C504E@school1.business.edu>
References: <6BCB4D493A447546A8126F24332056E8041C504E@school1.business.edu>
Message-ID: <x2ac556qd8.fsf@viggo.kubism.ku.dk>

"Afshartous, David" <afshart at exchange.sba.miami.edu> writes:

>  
> All,
> 
> When I take a subset of a factor the reduced factor still maintains all
> the original levels of the factor when say forming the key in a plot.
> The data is correct, but the variable still "remembers" the original
> levels.  See below for reproducible code.  Does anyone know how to fix
> this?
> cheers,
> dave
> 
> fact = as.factor(c(rep("A", 3),rep("B", 3), rep("C", 3)))
> new.fact = fact[1:6]
> > new.fact
> [1] A A A B B B
> Levels: A B C    ## should only show A B

Just use

> factor(new.fact)
[1] A A A B B B
Levels: A B

or

> fact[1:6, drop=T]
[1] A A A B B B
Levels: A B


And, no, it is not a bug. The fact that a subsample happens to consist
only of males does not turn gender into a one-level factor... (Apart
from the philosophy, it makes a real difference in tabulation.) 


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From ripley at stats.ox.ac.uk  Tue Sep 12 17:52:56 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 12 Sep 2006 16:52:56 +0100 (BST)
Subject: [R] Test internet presence
In-Reply-To: <loom.20060912T003831-608@post.gmane.org>
References: <XFMail.060911134821.Ted.Harding@nessie.mcc.ac.uk>
	<Pine.LNX.4.64.0609111419350.19275@gannet.stats.ox.ac.uk>
	<loom.20060912T003831-608@post.gmane.org>
Message-ID: <Pine.LNX.4.64.0609121648420.13250@gannet.stats.ox.ac.uk>

On Mon, 11 Sep 2006, Gregor Gorjanc wrote:

> Prof Brian Ripley <ripley <at> stats.ox.ac.uk> writes:
> ...
> > Check out tests/internet.R.  nsl() checks if you can resolve host names, 
> > which has worked well enough there.
> 
> Thank you prof. Ripley for this pointer. I am posting here the relevant part if
> someone does not look at SVN. I would just like to ask why is .Platform$OS.type
> == "unix" added to the test? Is nsl() available only on unix like platforms or
> ... I did not found any specifics in its help page.

Did you look at the help page on Windows?  Looking at the help page on 
Unix only tells you about Unix.

Hint: the help page is src/library/utils/man/unix/nsl.Rd

(In my country, PhD students are supposed to be able to find things 
like that out for themselves.)

> 
> if(!capabilities()["http/ftp"]) {
>     warning("no internet capabilities")
>     q()
> }
> 
> if(.Platform$OS.type == "unix" &&
>    is.null(nsl("cran.r-project.org"))) q()
> 
> Does it make any sense to write a function that would use these two tests.
> 
> isNetAvailable <- function()
> {
>   ifelse(!capabilities()["http/ftp"] && 
> ##         .Platform$OS.type == "unix" && ## ??? 
>          is.null(nsl("cran.r-project.org")), 
>          FALSE, 
>          TRUE)
> }
> 
> Regards, Gregor
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From afshart at exchange.sba.miami.edu  Tue Sep 12 18:00:20 2006
From: afshart at exchange.sba.miami.edu (Afshartous, David)
Date: Tue, 12 Sep 2006 12:00:20 -0400
Subject: [R] levels of factor when subsetting the factor
Message-ID: <6BCB4D493A447546A8126F24332056E8041C507B@school1.business.edu>


thanks to all for the quick replies!

if the factor is part of a dataframe, I can apply the subsetting
to the entire dataframe, and then use drop=True to the factor
separately and then put it back into the new dataframe (code below).  is there a way
to do this in a single step? 

dat <-data.frame(fact = as.factor(c(rep("A", 3),rep("B", 3), rep("C", 3))),Y = rnorm(9))
dat.new = dat[1:6, ]
dat.new$fact = dat$fact[1:6, drop = T]


 

-----Original Message-----
From: pd at pubhealth.ku.dk [mailto:pd at pubhealth.ku.dk] On Behalf Of Peter Dalgaard
Sent: Tuesday, September 12, 2006 11:45 AM
To: Afshartous, David
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] levels of factor when subsetting the factor

"Afshartous, David" <afshart at exchange.sba.miami.edu> writes:

>  
> All,
> 
> When I take a subset of a factor the reduced factor still maintains 
> all the original levels of the factor when say forming the key in a plot.
> The data is correct, but the variable still "remembers" the original 
> levels.  See below for reproducible code.  Does anyone know how to fix 
> this?
> cheers,
> dave
> 
> fact = as.factor(c(rep("A", 3),rep("B", 3), rep("C", 3))) new.fact = 
> fact[1:6]
> > new.fact
> [1] A A A B B B
> Levels: A B C    ## should only show A B

Just use

> factor(new.fact)
[1] A A A B B B
Levels: A B

or

> fact[1:6, drop=T]
[1] A A A B B B
Levels: A B


And, no, it is not a bug. The fact that a subsample happens to consist only of males does not turn gender into a one-level factor... (Apart from the philosophy, it makes a real difference in tabulation.) 


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From simone.gabbriellini at sp.unipi.it  Tue Sep 12 18:42:35 2006
From: simone.gabbriellini at sp.unipi.it (Simone Gabbriellini)
Date: Tue, 12 Sep 2006 18:42:35 +0200
Subject: [R] coerce matrix to number
Message-ID: <9C830578-548C-4528-9FBA-7D42042224AC@sp.unipi.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060912/1b30bdff/attachment.pl 

From martin at martinpercossi.com  Tue Sep 12 18:48:56 2006
From: martin at martinpercossi.com (Martin Percossi)
Date: Tue, 12 Sep 2006 17:48:56 +0100
Subject: [R] Plotting in R within NX client
Message-ID: <4506E4F8.4070305@martinpercossi.com>

Hello, has anyone tried doing this?

Local System:
NX client (windows version)

Remote System:
Linux Fedora Core 5, running on 64 bit Intel
R 2.3.1

The plots display weird in the sense that the y-labels don't show, i.e. 
they appear as solid black rectangles rather than numbers. I know the 
problem has something to do with NX because when I was about to 
screenshot the problem to send to this list, I noticed that the 
screenshot got the image "right". Searching for the problem didn't yield 
anything, however there seems to have been related problems with gnuplot 
+ nx. Oddly, gnuplot seems to work fine, although perhaps it's because 
by default the y-labels are not rotated there.

Lastly, tried installing the extra fonts for NX, but still no dice.

Does anyone have any suggestions?

TIA
Martin


From gael.even at unimib.it  Tue Sep 12 19:36:24 2006
From: gael.even at unimib.it (Gael Even)
Date: Tue, 12 Sep 2006 19:36:24 +0200
Subject: [R] Use of xvfb : X11 cannot allocate additional graphics colors.
	Consider using colortype="pseudo.cube" or "gray"
Message-ID: <200609121936.24319.gael.even@unimib.it>

Hi,

I use R in remote access on a Debian server.
I need X11() to create graphical things so I use xvfb-run (Virtual X server 
environnement) R to allowing X11() capabilities to R.
An other technique is to connect in shh -X to execute my R script.
With the connection ssh -X, my R script is well executed. BUT when I use Xvfb, 
some graphics are created but an error occured and  stop the script.
here the error :

" X11 cannot allocate additional graphics colors. Consider using 
colortype="pseudo.cube" or "gray" "

There is obviously a problem witth xvfb but I can't not resolve it
please help me!

Thank you

Gael Even


From exonintron at gmail.com  Tue Sep 12 19:18:42 2006
From: exonintron at gmail.com (Sender)
Date: Tue, 12 Sep 2006 10:18:42 -0700
Subject: [R] References about dot a files
Message-ID: <686bf0c50609121018r200bfec3md6e051b27c705bea@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060912/5b5a7f3a/attachment.pl 

From mschwartz at mn.rr.com  Tue Sep 12 19:21:37 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Tue, 12 Sep 2006 12:21:37 -0500
Subject: [R] coerce matrix to number
In-Reply-To: <9C830578-548C-4528-9FBA-7D42042224AC@sp.unipi.it>
References: <9C830578-548C-4528-9FBA-7D42042224AC@sp.unipi.it>
Message-ID: <1158081697.5092.14.camel@localhost.localdomain>

On Tue, 2006-09-12 at 18:42 +0200, Simone Gabbriellini wrote:
> Dear List,
> 
> how can I coerce a matrix like this
> 
>       [,1] [,2] [,3] [,4] [,5] [,6]
> [1,] "0"  "1"  "1"  "0"  "0"  "0"
> [2,] "1"  "0"  "1"  "0"  "0"  "0"
> [3,] "1"  "1"  "0"  "0"  "0"  "0"
> [4,] "0"  "0"  "0"  "0"  "1"  "0"
> [5,] "0"  "0"  "0"  "1"  "0"  "0"
> [6,] "0"  "0"  "0"  "0"  "0"  "0"
> 
> to be filled with numbers?
> 
> this is the result of replacing some character ("v", "d") with 0 and  
> 1, using the code I found with RSiteSearch()
> 
> z[] <- lapply(z, factor, levels = c("d", "v"), labels = c(0, 1));
> 
> thank you,
> Simone


I reverse engineered your (presumably) original data frame:

> z
  1 2 3 4 5 6
1 d v v d d d
2 v d v d d d
3 v v d d d d
4 d d d d v d
5 d d d v d d
6 d d d d d d


> str(z)
`data.frame':   6 obs. of  6 variables:
 $ 1: Factor w/ 2 levels "d","v": 1 2 2 1 1 1
 $ 2: Factor w/ 2 levels "d","v": 2 1 2 1 1 1
 $ 3: Factor w/ 2 levels "d","v": 2 2 1 1 1 1
 $ 4: Factor w/ 2 levels "d","v": 1 1 1 1 2 1
 $ 5: Factor w/ 2 levels "d","v": 1 1 1 2 1 1
 $ 6: Factor w/ 2 levels "d","v": 1 1 1 1 1 1



If that is correct, then the following should yield what you want in one
step:

> z.num <- sapply(z, function(x) as.numeric(x) - 1)

> z.num
     1 2 3 4 5 6
[1,] 0 1 1 0 0 0
[2,] 1 0 1 0 0 0
[3,] 1 1 0 0 0 0
[4,] 0 0 0 0 1 0
[5,] 0 0 0 1 0 0
[6,] 0 0 0 0 0 0

> str(z.num)
 num [1:6, 1:6] 0 1 1 0 0 0 1 0 1 0 ...
 - attr(*, "dimnames")=List of 2
  ..$ : NULL
  ..$ : chr [1:6] "1" "2" "3" "4" ...



Alternatively, if you were starting out with the character matrix:

> z.char
     [,1] [,2] [,3] [,4] [,5] [,6]
[1,] "0"  "1"  "1"  "0"  "0"  "0"
[2,] "1"  "0"  "1"  "0"  "0"  "0"
[3,] "1"  "1"  "0"  "0"  "0"  "0"
[4,] "0"  "0"  "0"  "0"  "1"  "0"
[5,] "0"  "0"  "0"  "1"  "0"  "0"
[6,] "0"  "0"  "0"  "0"  "0"  "0"


You could do:

> storage.mode(z.char) <- "numeric"

> z.char
     [,1] [,2] [,3] [,4] [,5] [,6]
[1,]    0    1    1    0    0    0
[2,]    1    0    1    0    0    0
[3,]    1    1    0    0    0    0
[4,]    0    0    0    0    1    0
[5,]    0    0    0    1    0    0
[6,]    0    0    0    0    0    0

> str(z.char)
 num [1:6, 1:6] 0 1 1 0 0 0 1 0 1 0 ...
 - attr(*, "dimnames")=List of 2
  ..$ : NULL
  ..$ : NULL



Yet another alternative:

> matrix(as.numeric(z.char), dim(z.char))
     [,1] [,2] [,3] [,4] [,5] [,6]
[1,]    0    1    1    0    0    0
[2,]    1    0    1    0    0    0
[3,]    1    1    0    0    0    0
[4,]    0    0    0    0    1    0
[5,]    0    0    0    1    0    0
[6,]    0    0    0    0    0    0



HTH,

Marc Schwartz


From sundar.dorai-raj at pdf.com  Tue Sep 12 19:23:46 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 12 Sep 2006 12:23:46 -0500
Subject: [R] levels of factor when subsetting the factor
In-Reply-To: <6BCB4D493A447546A8126F24332056E8041C507B@school1.business.edu>
References: <6BCB4D493A447546A8126F24332056E8041C507B@school1.business.edu>
Message-ID: <4506ED22.8000001@pdf.com>

Yes. I do this periodically:

dat.new <- dat[1:6, ]
dat.new[] <- lapply(dat.new, function(x)
                     if(is.factor(x)) factor(x) else x)

HTH,

--sundar

Afshartous, David said the following on 9/12/2006 11:00 AM:
> thanks to all for the quick replies!
> 
> if the factor is part of a dataframe, I can apply the subsetting
> to the entire dataframe, and then use drop=True to the factor
> separately and then put it back into the new dataframe (code below).  is there a way
> to do this in a single step? 
> 
> dat <-data.frame(fact = as.factor(c(rep("A", 3),rep("B", 3), rep("C", 3))),Y = rnorm(9))
> dat.new = dat[1:6, ]
> dat.new$fact = dat$fact[1:6, drop = T]
> 
> 
>  
> 
> -----Original Message-----
> From: pd at pubhealth.ku.dk [mailto:pd at pubhealth.ku.dk] On Behalf Of Peter Dalgaard
> Sent: Tuesday, September 12, 2006 11:45 AM
> To: Afshartous, David
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] levels of factor when subsetting the factor
> 
> "Afshartous, David" <afshart at exchange.sba.miami.edu> writes:
> 
>>  
>> All,
>>
>> When I take a subset of a factor the reduced factor still maintains 
>> all the original levels of the factor when say forming the key in a plot.
>> The data is correct, but the variable still "remembers" the original 
>> levels.  See below for reproducible code.  Does anyone know how to fix 
>> this?
>> cheers,
>> dave
>>
>> fact = as.factor(c(rep("A", 3),rep("B", 3), rep("C", 3))) new.fact = 
>> fact[1:6]
>>> new.fact
>> [1] A A A B B B
>> Levels: A B C    ## should only show A B
> 
> Just use
> 
>> factor(new.fact)
> [1] A A A B B B
> Levels: A B
> 
> or
> 
>> fact[1:6, drop=T]
> [1] A A A B B B
> Levels: A B
> 
> 
> And, no, it is not a bug. The fact that a subsample happens to consist only of males does not turn gender into a one-level factor... (Apart from the philosophy, it makes a real difference in tabulation.) 
> 
>


From jacques.veslot at good.ibl.fr  Tue Sep 12 19:26:23 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Tue, 12 Sep 2006 19:26:23 +0200
Subject: [R] coerce matrix to number
In-Reply-To: <9C830578-548C-4528-9FBA-7D42042224AC@sp.unipi.it>
References: <9C830578-548C-4528-9FBA-7D42042224AC@sp.unipi.it>
Message-ID: <4506EDBF.8070006@good.ibl.fr>

if only 2 letters:
(z=="v")*1
else:
lapply(z, function(x) as.numeric(as.character(factor(x,levels= c("d","v","w"),labels=c(0,1,2)))))
-------------------------------------------------------------------
Jacques VESLOT

CNRS UMR 8090
I.B.L (2?me ?tage)
1 rue du Professeur Calmette
B.P. 245
59019 Lille Cedex

Tel : 33 (0)3.20.87.10.44
Fax : 33 (0)3.20.87.10.31

http://www-good.ibl.fr
-------------------------------------------------------------------


Simone Gabbriellini a ?crit :
> Dear List,
> 
> how can I coerce a matrix like this
> 
>       [,1] [,2] [,3] [,4] [,5] [,6]
> [1,] "0"  "1"  "1"  "0"  "0"  "0"
> [2,] "1"  "0"  "1"  "0"  "0"  "0"
> [3,] "1"  "1"  "0"  "0"  "0"  "0"
> [4,] "0"  "0"  "0"  "0"  "1"  "0"
> [5,] "0"  "0"  "0"  "1"  "0"  "0"
> [6,] "0"  "0"  "0"  "0"  "0"  "0"
> 
> to be filled with numbers?
> 
> this is the result of replacing some character ("v", "d") with 0 and  
> 1, using the code I found with RSiteSearch()
> 
> z[] <- lapply(z, factor, levels = c("d", "v"), labels = c(0, 1));
> 
> thank you,
> Simone
> 
> |-------------------------------------------------|
> 
> dott. Simone Gabbriellini
> PhD Student
> Dipartimento di Scienze Sociali
> Universit? di Pisa
> via Colombo 35 - 56100 Pisa
> mail: simone.gabbriellini at sp.unipi.it
> mobile: +39 3475710037
> 
> |-------------------------------------------------|
> 
> Please avoid sending me Word or PowerPoint attachments.
> See http://www.gnu.org/philosophy/no-word-attachments.html
> 
> 
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From lei.wu at jax.org  Tue Sep 12 19:44:09 2006
From: lei.wu at jax.org (Lei Wu)
Date: Tue, 12 Sep 2006 13:44:09 -0400
Subject: [R] how to install R-2.3.1 on Mac OS X 10.3.9?
In-Reply-To: <8B135D6D-3973-4CA2-9C03-8983E843DB4A@research.att.com>
Message-ID: <20060912134409625.00000003904@phocid>

Hi, is there anyone knows how to install R-2.3.1 on Mac OS X 10.3.9(panther)?

Thanks a lot!

Lei


From simone.gabbriellini at sp.unipi.it  Tue Sep 12 19:46:39 2006
From: simone.gabbriellini at sp.unipi.it (Simone Gabbriellini)
Date: Tue, 12 Sep 2006 19:46:39 +0200
Subject: [R] coerce matrix to number
In-Reply-To: <1158081697.5092.14.camel@localhost.localdomain>
References: <9C830578-548C-4528-9FBA-7D42042224AC@sp.unipi.it>
	<1158081697.5092.14.camel@localhost.localdomain>
Message-ID: <7920DCB0-D13F-4431-AFD0-2D5D32FF17EB@sp.unipi.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060912/7ccfcec1/attachment.pl 

From ethan.johnsons at gmail.com  Tue Sep 12 19:52:21 2006
From: ethan.johnsons at gmail.com (Ethan Johnsons)
Date: Tue, 12 Sep 2006 13:52:21 -0400
Subject: [R] rename cols
In-Reply-To: <45059B36.1090203@acm.org>
References: <5cd96f050609110949k5dfe1b1cg665558e50af1b56d@mail.gmail.com>
	<45059B36.1090203@acm.org>
Message-ID: <5cd96f050609121052h22f56ccdr4818af07f937c15c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060912/51a4a13c/attachment.pl 

From ethan.johnsons at gmail.com  Tue Sep 12 19:55:31 2006
From: ethan.johnsons at gmail.com (Ethan Johnsons)
Date: Tue, 12 Sep 2006 13:55:31 -0400
Subject: [R] extract a value from vector
Message-ID: <5cd96f050609121055w5614c533w8ced8c440e630d1b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060912/7703361d/attachment.pl 

From ripley at stats.ox.ac.uk  Tue Sep 12 20:09:54 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 12 Sep 2006 19:09:54 +0100 (BST)
Subject: [R] Use of xvfb : X11 cannot allocate additional graphics
 colors. Consider using colortype="pseudo.cube" or "gray"
In-Reply-To: <200609121936.24319.gael.even@unimib.it>
References: <200609121936.24319.gael.even@unimib.it>
Message-ID: <Pine.LNX.4.64.0609121907560.18480@gannet.stats.ox.ac.uk>

It's a known problem.  Run the X11 device with the arguments stated,
as it says ....

On Tue, 12 Sep 2006, Gael Even wrote:

> Hi,
> 
> I use R in remote access on a Debian server.
> I need X11() to create graphical things so I use xvfb-run (Virtual X server 
> environnement) R to allowing X11() capabilities to R.
> An other technique is to connect in shh -X to execute my R script.
> With the connection ssh -X, my R script is well executed. BUT when I use Xvfb, 
> some graphics are created but an error occured and  stop the script.
> here the error :
> 
> " X11 cannot allocate additional graphics colors. Consider using 
> colortype="pseudo.cube" or "gray" "
> 
> There is obviously a problem witth xvfb but I can't not resolve it
> please help me!
> 
> Thank you
> 
> Gael Even
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From phhs80 at gmail.com  Tue Sep 12 20:11:01 2006
From: phhs80 at gmail.com (Paul Smith)
Date: Tue, 12 Sep 2006 19:11:01 +0100
Subject: [R] extract a value from vector
In-Reply-To: <5cd96f050609121055w5614c533w8ced8c440e630d1b@mail.gmail.com>
References: <5cd96f050609121055w5614c533w8ced8c440e630d1b@mail.gmail.com>
Message-ID: <6ade6f6c0609121111s49269b62h249530ffbd86b757@mail.gmail.com>

On 9/12/06, Ethan Johnsons <ethan.johnsons at gmail.com> wrote:
> A quick question, please!
>
> How do you extract a certain value of vector?
> i.e. x = c(2,5,3,6,21,3,6,24, ....)
>
> How do you get the 1st one (which is 2); the 5th one (which is 21); etc?

Simple, Ethan:

x[1], x[5], ...

Paul


From mothsailor at googlemail.com  Tue Sep 12 20:19:20 2006
From: mothsailor at googlemail.com (David Barron)
Date: Tue, 12 Sep 2006 19:19:20 +0100
Subject: [R] extract a value from vector
In-Reply-To: <5cd96f050609121055w5614c533w8ced8c440e630d1b@mail.gmail.com>
References: <5cd96f050609121055w5614c533w8ced8c440e630d1b@mail.gmail.com>
Message-ID: <815b70590609121119g5371d3afq6718c127a2146508@mail.gmail.com>

I'm not quite sure what you mean.  To get the first item in a vector you use

> x[1]
[1] 2

> x[5]
[1] 21

Is that what you want?

On 12/09/06, Ethan Johnsons <ethan.johnsons at gmail.com> wrote:
> A quick question, please!
>
> How do you extract a certain value of vector?
> i.e. x = c(2,5,3,6,21,3,6,24, ....)
>
> How do you get the 1st one (which is 2); the 5th one (which is 21); etc?
>
> thx much,
>
>  ej
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From p.dalgaard at biostat.ku.dk  Tue Sep 12 20:35:28 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 12 Sep 2006 20:35:28 +0200
Subject: [R] Use of xvfb : X11 cannot allocate additional graphics
	colors. Consider using colortype="pseudo.cube" or "gray"
In-Reply-To: <Pine.LNX.4.64.0609121907560.18480@gannet.stats.ox.ac.uk>
References: <200609121936.24319.gael.even@unimib.it>
	<Pine.LNX.4.64.0609121907560.18480@gannet.stats.ox.ac.uk>
Message-ID: <x2lkopndan.fsf@turmalin.kubism.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> It's a known problem.  Run the X11 device with the arguments stated,
> as it says ....

Alternatively, try playing with the pixel depth of xvfb. I think this
mainly happens with 8-bit displays. 
 
> On Tue, 12 Sep 2006, Gael Even wrote:
> 
> > Hi,
> > 
> > I use R in remote access on a Debian server.
> > I need X11() to create graphical things so I use xvfb-run (Virtual X server 
> > environnement) R to allowing X11() capabilities to R.
> > An other technique is to connect in shh -X to execute my R script.
> > With the connection ssh -X, my R script is well executed. BUT when I use Xvfb, 
> > some graphics are created but an error occured and  stop the script.
> > here the error :
> > 
> > " X11 cannot allocate additional graphics colors. Consider using 
> > colortype="pseudo.cube" or "gray" "
> > 
> > There is obviously a problem witth xvfb but I can't not resolve it
> > please help me!
> > 
> > Thank you
> > 
> > Gael Even


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From jennystadt at yahoo.ca  Tue Sep 12 20:44:11 2006
From: jennystadt at yahoo.ca (jennystadt)
Date: Tue, 12 Sep 2006 12:44:11 -0600
Subject: [R] About truncated distribution
References: <686bf0c50609121018r200bfec3md6e051b27c705bea@mail.gmail.com>
Message-ID: <200609121244095950784@yahoo.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060912/f2cf978f/attachment.pl 

From gregor.gorjanc at bfro.uni-lj.si  Tue Sep 12 21:25:50 2006
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Tue, 12 Sep 2006 21:25:50 +0200
Subject: [R] Test internet presence
In-Reply-To: <Pine.LNX.4.64.0609121648420.13250@gannet.stats.ox.ac.uk>
References: <XFMail.060911134821.Ted.Harding@nessie.mcc.ac.uk>
	<Pine.LNX.4.64.0609111419350.19275@gannet.stats.ox.ac.uk>
	<loom.20060912T003831-608@post.gmane.org>
	<Pine.LNX.4.64.0609121648420.13250@gannet.stats.ox.ac.uk>
Message-ID: <450709BE.7000803@bfro.uni-lj.si>

Prof Brian Ripley wrote:
> On Mon, 11 Sep 2006, Gregor Gorjanc wrote:
>> Prof Brian Ripley <ripley <at> stats.ox.ac.uk> writes:
>> ...
>>> Check out tests/internet.R.  nsl() checks if you can resolve host names, 
>>> which has worked well enough there.
>> Thank you prof. Ripley for this pointer. I am posting here the relevant part if
>> someone does not look at SVN. I would just like to ask why is .Platform$OS.type
>> == "unix" added to the test? Is nsl() available only on unix like platforms or
>> ... I did not found any specifics in its help page.
> 
> Did you look at the help page on Windows?  Looking at the help page on 
> Unix only tells you about Unix.
> 
> Hint: the help page is src/library/utils/man/unix/nsl.Rd
> 
> (In my country, PhD students are supposed to be able to find things 
> like that out for themselves.)
> 
> 
...

Thank you for additional pointer. I did not look windows help page as I
do not have windows at disposal all the time, but You are right - I
should have looked into the sources. I found out that there is no such
function i.e. nsl() under windows.

If I summarize the thread there is (currently) no way to test for
internet presence with a general approach.

-- 
Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana     PhD student
Biotechnical Faculty
Zootechnical Department     URI: http://www.bfro.uni-lj.si/MR/ggorjan
Groblje 3                   mail: gregor.gorjanc <at> bfro.uni-lj.si

SI-1230 Domzale             tel: +386 (0)1 72 17 861
Slovenia, Europe            fax: +386 (0)1 72 17 888

----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.


From franke.daniel at gmail.com  Tue Sep 12 21:38:51 2006
From: franke.daniel at gmail.com (Daniel Franke)
Date: Tue, 12 Sep 2006 21:38:51 +0200
Subject: [R] rgl: exporting to pdf or png does not work
Message-ID: <200609122138.51645.franke.daniel@gmail.com>


[This is a follow-up to the recent discussion on R-help]

Hi all,

I can reproduce both problems on gentoo (2006.0 profile), 
but not on OpenSuSE-10.1. 

bash> cat rgl.postscript.r
library(rgl)
example(spheres3d)
rgl.postscript("spheres.eps", fmt="eps")

R> source("rgl.postscript.r")

X Error of failed request:  GLXBadContextState
  Major opcode of failed request:  142 (GLX)
  Minor opcode of failed request:  5 (X_GLXMakeCurrent)
  Serial number of failed request:  102
  Current serial number in output stream:  102


bash> cat rgl.snapshot.r
library(rgl)
example(spheres3d)
rgl.postscript("spheres.png", fmt="png")

R> source("rgl.snapshot.r")
[1] "failed"


Installed libraries:
 * Gentoo: R-2.2.1, rgl-0.67.2, OpenSuSE: R-2.3.1, rgl-0.67.2
 * Gentoo: libpng-1.2.12, OpenSuSE: libpng-1.2.8
 * Gentoo: Xorg-6.8.2 (mga), OpenSuSE: 6.9.0 (nvidia)
 * Gentoo: freeglut-2.4.0, OpenSuSE:  freeglut-051110
 * OpenSuse: mesa-6.4.2

Please let me know if there is anything I can do to help to 
solve this issue.

Regards
	Daniel


P.S. Please CC me in your replies, I am not subscribed to the list.


From bflat24 at gmail.com  Tue Sep 12 21:51:35 2006
From: bflat24 at gmail.com (T C)
Date: Tue, 12 Sep 2006 15:51:35 -0400
Subject: [R] whole object approach for nested loops
Message-ID: <ca7bd80e0609121251s493e5131ge181927f54be8755@mail.gmail.com>

I have the following code that I am trying to execute using the whole
object approach and get rid of the for loop. I have looked at the
manual and seached the database for examples or similar questions with
no luck. The following example works without any problems.

for (j in 1:186)
{
entropy.cogp[1:30000, j]<-alpha3[1:30000]*c[j,2]
}

But when I try to remove the for loop and use

entropy.cogp[1:30000, 1:186]<-alpha3[1:30000]*c[1:186,2]

R tries to multiply the first member of alpha3 with the first member
of c[,2] and once c is exhausted, it multiplies the 187th member of
alpha3 with the first member of c[,2] and so on, resulting in an error
where it requires the size of alpha3 to be an exact multiple of the
size of c. This is clearly not what is intended by the for loop given
above. Is there a way to do this using a whole object approach to make
things run faster? Or is the for loop the only way of doing this?

Thanks...


From vokey at uleth.ca  Tue Sep 12 22:00:58 2006
From: vokey at uleth.ca (John Vokey)
Date: Tue, 12 Sep 2006 14:00:58 -0600
Subject: [R] summary(glm) for categorical variables
In-Reply-To: <mailman.11.1158055204.3008.r-help@stat.math.ethz.ch>
References: <mailman.11.1158055204.3008.r-help@stat.math.ethz.ch>
Message-ID: <B1A0AD7C-DA01-4A53-9782-55849A1A3985@uleth.ca>

You want:
anova(my.glm)

On 12-Sep-06, at 4:00 AM, r-help-request at stat.math.ethz.ch wrote:

> Suppose we have a data.frame where variables are categorical and  
> the response is
> categorical eg:
>
> my.df=NULL
>
> for(i in LETTERS[1:3]){my.df[[i]]=sample(letters, size=10)}
>
> my.df=data.frame(my.df)
>
> my.df$class=factor(rep(c("pos", "neg"), times=5))
>
> my.glm=glm(class ~ ., data=my.df, family=binomial)
>
> summary(my.glm)
>
> ....
>               Estimate Std. Error   z value Pr(>|z|)
> (Intercept)  2.457e+01  1.310e+05  1.88e-04        1
> Ad          -8.559e-11  1.853e+05 -4.62e-16        1
> Aj          -9.897e-10  1.853e+05 -5.34e-15        1
> An          -4.913e+01  1.853e+05 -2.65e-04        1
> ...
>
> My question is is it possible to get the terms to appear as A,B, C  
> instead of
> every combination of Aa, Ab, Ac etc separately?
>
> Many Thanks in advance


From ken.pierce at oregonstate.edu  Tue Sep 12 22:02:13 2006
From: ken.pierce at oregonstate.edu (Pierce, Ken)
Date: Tue, 12 Sep 2006 13:02:13 -0700
Subject: [R] variables in object names
Message-ID: <4D5DA98A54374044B7CC3F40A157B98B7557A7@thuja>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060912/194d8622/attachment.pl 

From erich.neuwirth at univie.ac.at  Tue Sep 12 22:11:12 2006
From: erich.neuwirth at univie.ac.at (Erich Neuwirth)
Date: Tue, 12 Sep 2006 22:11:12 +0200
Subject: [R] Bitmap file size problems
Message-ID: <45071460.9060506@univie.ac.at>

I am experiencing some problems with the windows graphics device
and bitmaps. (Everything is done in R 2.3.1 on Windows XP)
As an example, I will use

windows(3,2)
plot(1:10)

It is not a pretty or meaningful graph, but it demonstrates the problem.

savePlot(file="test",type="bmp")

creates a bitmap file of size 303x207 which is somewhat strange
because it seems to use slightly different values for
pixels per inch horizontally and vertically.
(By the way, in my version of R 2.4.0alpha, the bitmap has size
303x206)
The documentation warns that the graphics devices use
pixel per inch values reported by Windows, and that might be unreliable.
In my case,

windows(3,2,xpinch=96)
plot(1:10)
savePlot(file="test",type="bmp")

produces the same image (of the same size) as before, so
Windows seems to report 96 for the pixels per inch value.
The next result surprised me:

windows(3,2,xpinch=96,ypinch=96)
plot(1:10)
savePlot(file="test",type="bmp")

produces a square graphics window and a bitmap file of size
303x303. (In R 2.4.0 alpha the size is 303x302).
In fact, when ypinch is given, height seems to be ignored
and the graphics windows height:width ratio is the
ypinch:xpinch ratio from the call to windows.
windows(10,1,xpinch=96,ypinch=192)

will produce a window which is higher than wide,
which is not what I expected after reading the docs.
(This also is true in R 2.4.0 alpha)

The alternative solution is to use dev2bitmap.

windows(3,2)
plot(1:10)
dev2bitmap(file="test.bmp",type="bmp256",width=3,height=2,res=100)

produces a bitmap of size 300x200 which is exactly what is to
be expected. In this bitmap, however, the leftmost
part of the image displayed in R's graphics window is cut off.

What is the best way of creating bitmaps of a given size
containing the full contents of an R  graphics window?



-- 
Erich Neuwirth, Didactic Center for Computer Science
University of Vienna
Visit our SunSITE at http://sunsite.univie.ac.at
Phone: +43-1-4277-39464 Fax: +43-1-4277-9394


From sarah.goslee at gmail.com  Tue Sep 12 22:20:49 2006
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Tue, 12 Sep 2006 16:20:49 -0400
Subject: [R] variables in object names
In-Reply-To: <4D5DA98A54374044B7CC3F40A157B98B7557A7@thuja>
References: <4D5DA98A54374044B7CC3F40A157B98B7557A7@thuja>
Message-ID: <efb536d50609121320t5180c4f6n151dc2d1610122f2@mail.gmail.com>

Hi Ken,

Not quite the way you're thinking about it, but yes, there is,
and it is very useful. See ?get for more information, but here's
the basics:

for(i in 2:5) {
  thismodel <- get(paste("model", i, sep=""))
  rsq <- c(rsq, summary(thismodel)$r.squared)
}


Also see ?assign for the opposite effect.


On 9/12/06, Pierce, Ken <ken.pierce at oregonstate.edu> wrote:
> Is there any way to put an argument into an object name. For example,
> say I have 5 objects,  model1, model2, model3, model4 and model5.
>
> I would like to make a vector of the r.squares from each model by code
> such as this:
>
>
> rsq <- summary(model1)$r.squared
> for(i in 2:5){
> rsq <- c(rsq, summary(model%i%)$r.squared)
> }
>
>
-- 
Sarah Goslee
USDA-ARS PSWMRU
University Park, PA 16802
Sarah.Goslee at ars.usda.gov


From liuwensui at gmail.com  Tue Sep 12 22:23:36 2006
From: liuwensui at gmail.com (Wensui Liu)
Date: Tue, 12 Sep 2006 16:23:36 -0400
Subject: [R] variables in object names
In-Reply-To: <4D5DA98A54374044B7CC3F40A157B98B7557A7@thuja>
References: <4D5DA98A54374044B7CC3F40A157B98B7557A7@thuja>
Message-ID: <1115a2b00609121323q4622cd7bo9ec3d22685999cea@mail.gmail.com>

Ken,

I have a similar example in my blog:
http://statcompute.spaces.live.com/blog/cns!39C8032DBD1321B7!229.entry

On 9/12/06, Pierce, Ken <ken.pierce at oregonstate.edu> wrote:
> Is there any way to put an argument into an object name. For example,
> say I have 5 objects,  model1, model2, model3, model4 and model5.
>
> I would like to make a vector of the r.squares from each model by code
> such as this:
>
>
> rsq <- summary(model1)$r.squared
> for(i in 2:5){
> rsq <- c(rsq, summary(model%i%)$r.squared)
> }
>
>
> So I assign the first value to rsq then cycle through models 2 through 5
> gathering there values. The %i% in my third line indicates which object
> to draw from. The question is is there any way to pass a variable such
> as i as part of a name?
>
> Ken
>
>
>
> Kenneth B. Pierce Jr.
>
> Research Ecologist
>
> Landscape Ecology, Modeling, Mapping and Analysis Team
>
> PNW Research Station - USDA-FS
>
> 3200 SW Jefferson Way,  Corvallis,  OR 97331
>
> ken.pierce at oregonstate.edu
>
> 541 750-7393
>
> http://www.fsl.orst.edu/lemma/gnnfire
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
WenSui Liu
(http://spaces.msn.com/statcompute/blog)
Senior Decision Support Analyst
Health Policy and Clinical Effectiveness
Cincinnati Children Hospital Medical Center


From Dimitris.Rizopoulos at med.kuleuven.be  Tue Sep 12 22:23:45 2006
From: Dimitris.Rizopoulos at med.kuleuven.be (Dimitrios Rizopoulos)
Date: Tue, 12 Sep 2006 22:23:45 +0200
Subject: [R] variables in object names
In-Reply-To: <4D5DA98A54374044B7CC3F40A157B98B7557A7@thuja>
References: <4D5DA98A54374044B7CC3F40A157B98B7557A7@thuja>
Message-ID: <20060912222345.jv48gb3isb48c8cc@webmail3.kuleuven.be>

you need something like the following,

fit.lis <- list(model1, model2, model3, model4, model5)
# or if you have many models
fit.lis <- lapply(paste("model", 1:5, sep = ""), get)

sapply(fit.lis, function(x) summary(x)$r.squared)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
      http://www.student.kuleuven.be/~m0390867/dimitris.htm


Quoting "Pierce, Ken" <ken.pierce at oregonstate.edu>:

> Is there any way to put an argument into an object name. For example,
> say I have 5 objects,  model1, model2, model3, model4 and model5.
>
> I would like to make a vector of the r.squares from each model by code
> such as this:
>
>
> rsq <- summary(model1)$r.squared
> for(i in 2:5){
> rsq <- c(rsq, summary(model%i%)$r.squared)
> }
>
>
> So I assign the first value to rsq then cycle through models 2 through 5
> gathering there values. The %i% in my third line indicates which object
> to draw from. The question is is there any way to pass a variable such
> as i as part of a name?
>
> Ken
>
>
>
> Kenneth B. Pierce Jr.
>
> Research Ecologist
>
> Landscape Ecology, Modeling, Mapping and Analysis Team
>
> PNW Research Station - USDA-FS
>
> 3200 SW Jefferson Way,  Corvallis,  OR 97331
>
> ken.pierce at oregonstate.edu
>
> 541 750-7393
>
> http://www.fsl.orst.edu/lemma/gnnfire
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>



Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From mothsailor at googlemail.com  Tue Sep 12 22:24:05 2006
From: mothsailor at googlemail.com (David Barron)
Date: Tue, 12 Sep 2006 21:24:05 +0100
Subject: [R] variables in object names
In-Reply-To: <4D5DA98A54374044B7CC3F40A157B98B7557A7@thuja>
References: <4D5DA98A54374044B7CC3F40A157B98B7557A7@thuja>
Message-ID: <815b70590609121324y2c3f355bo762a06f2b14921b0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060912/edf38333/attachment.pl 

From ken.pierce at oregonstate.edu  Tue Sep 12 22:37:01 2006
From: ken.pierce at oregonstate.edu (Pierce, Ken)
Date: Tue, 12 Sep 2006 13:37:01 -0700
Subject: [R] variables in object names
In-Reply-To: <815b70590609121324y2c3f355bo762a06f2b14921b0@mail.gmail.com>
Message-ID: <4D5DA98A54374044B7CC3F40A157B98B7557A9@thuja>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060912/1b79e8ea/attachment.pl 

From oceanclear at gmail.com  Tue Sep 12 22:49:39 2006
From: oceanclear at gmail.com (Seaclear Theory)
Date: Tue, 12 Sep 2006 13:49:39 -0700
Subject: [R] error in make check
Message-ID: <172313400609121349w49f0c692kdf47e64d04f21af5@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060912/2d605a0c/attachment.pl 

From emmanuel.levy at gmail.com  Tue Sep 12 22:50:23 2006
From: emmanuel.levy at gmail.com (Emmanuel Levy)
Date: Tue, 12 Sep 2006 21:50:23 +0100
Subject: [R] Basic help needed: group bunch of lines in a list (matrix)
Message-ID: <e4654710609121350u5c273d70h77d8f0500af3dabc@mail.gmail.com>

Hello,

I'd like to group the lines of a matrix so that:
A 1.0 200
A 3.0 800
A 2.0 200
B 0.5 20
B 0.9 50
C 5.0 70

Would give:
A 2.0 400
B 0.7 35
C 5.0 70

So all lines corresponding to a letter (level), become a single line
where all the values of each column are averaged.

I've done that with a loop but it doesn't sound right (it is very
slow). I imagine there is a
sort of "apply" shortcut but I can't figure it out.

Please note that it is not exactly a matrix I'm using, the function
"typeof" tells me it's a list, however I access to it like it was a
matrix.

Could someone help me with the right function to use, a help topic or
a piece of code?

Thanks,

  Emmanuel


From sundar.dorai-raj at pdf.com  Tue Sep 12 23:03:01 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 12 Sep 2006 16:03:01 -0500
Subject: [R] Basic help needed: group bunch of lines in a list (matrix)
In-Reply-To: <e4654710609121350u5c273d70h77d8f0500af3dabc@mail.gmail.com>
References: <e4654710609121350u5c273d70h77d8f0500af3dabc@mail.gmail.com>
Message-ID: <45072085.2080200@pdf.com>



Emmanuel Levy said the following on 9/12/2006 3:50 PM:
> Hello,
> 
> I'd like to group the lines of a matrix so that:
> A 1.0 200
> A 3.0 800
> A 2.0 200
> B 0.5 20
> B 0.9 50
> C 5.0 70
> 
> Would give:
> A 2.0 400
> B 0.7 35
> C 5.0 70
> 
> So all lines corresponding to a letter (level), become a single line
> where all the values of each column are averaged.
> 
> I've done that with a loop but it doesn't sound right (it is very
> slow). I imagine there is a
> sort of "apply" shortcut but I can't figure it out.
> 
> Please note that it is not exactly a matrix I'm using, the function
> "typeof" tells me it's a list, however I access to it like it was a
> matrix.
> 
> Could someone help me with the right function to use, a help topic or
> a piece of code?
> 
> Thanks,
> 
>   Emmanuel
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


Try aggregate:

aggregate(x[1], x[2:3], mean)

where `x' is your data.frame.

--sundar


From rmh at temple.edu  Tue Sep 12 23:29:54 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Tue, 12 Sep 2006 17:29:54 -0400 (EDT)
Subject: [R] whole object approach for nested loops
Message-ID: <20060912172954.BHU66968@po-d.temple.edu>

you are looking for the outer product

?outer


> a <- 1:6
> cc <- matrix(1:6, 3, 2)


> e <- matrix(0, 6,3)
> for (j in 1:3) e[,j] <- a*cc[j,2]
> e
     [,1] [,2] [,3]
[1,]    4    5    6
[2,]    8   10   12
[3,]   12   15   18
[4,]   16   20   24
[5,]   20   25   30
[6,]   24   30   36


> a %o% cc[,2]
     [,1] [,2] [,3]
[1,]    4    5    6
[2,]    8   10   12
[3,]   12   15   18
[4,]   16   20   24
[5,]   20   25   30
[6,]   24   30   36
> >


From jennystadt at yahoo.ca  Tue Sep 12 23:42:00 2006
From: jennystadt at yahoo.ca (jennystadt)
Date: Tue, 12 Sep 2006 15:42:00 -0600
Subject: [R] About truncated distribution
References: <686bf0c50609121018r200bfec3md6e051b27c705bea@mail.gmail.com>
	<200609121244095950784@yahoo.ca>
Message-ID: <200609121541592080733@yahoo.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060912/14915803/attachment.pl 

From A.Robinson at ms.unimelb.edu.au  Wed Sep 13 00:04:23 2006
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Wed, 13 Sep 2006 08:04:23 +1000 (EST)
Subject: [R] Conservative "ANOVA tables" in lmer
In-Reply-To: <1158010468.3278.24.camel@solidago.localdomain>
References: <1A8FE5B0-3FE3-4CA9-9224-9CACB9AF4854@muohio.edu>
	<40e66e0b0609070559u656463c1l52a60eda9a095237@mail.gmail.com>
	<20060910055624.GA12212@ms.unimelb.edu.au>
	<40e66e0b0609110943h7dfb057dv12567975ac04bdb2@mail.gmail.com>
	<1158010468.3278.24.camel@solidago.localdomain>
Message-ID: <48130.220.237.183.166.1158098663.squirrel@webmail.ms.unimelb.edu.au>


On Tue, September 12, 2006 7:34 am, Manuel Morales wrote:
> On Mon, 2006-09-11 at 11:43 -0500, Douglas Bates wrote:
>> Having made that offer I think I will now withdraw it.  Peter's
>> example has convinced me that this is the wrong thing to do.
>>
>> I am encouraged by the fact that the results from mcmcsamp correspond
>> closely to the correct theoretical results in the case that Peter
>> described.  I appreciate that some users will find it difficult to
>> work with a MCMC sample (or to convince editors to accept results
>> based on such a sample) but I think that these results indicate that
>> it is better to go after the marginal distribution of the fixed
>> effects estimates (which is what is being approximated by the MCMC
>> sample - up to Bayesian/frequentist philosophical differences) than to
>> use the conditional distribution and somehow try to adjust the
>> reference distribution.
>
> Am I right that the MCMC sample can not be used, however, to evaluate
> the significance of parameter groups. For example, to assess the
> significance of a three-level factor? Are there better alternatives than
> simply adjusting the CI for the number of factor levels
> (1-alpha/levels).

I wonder whether the likelihood ratio test would be suitable here?  That
seems to be supported.  It just takes a little longer.

> require(lme4)
> data(sleepstudy)
> fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
> fm2 <- lmer(Reaction ~ Days + I(Days^2) + (Days|Subject), sleepstudy)
> anova(fm1, fm2)

So, a brief overview of the popular inferential needs and solutions would
then be:

1) Test the statistical significance of one or more fixed or random
effects - fit a model with and a model without the terms, and use the LRT.

2) Obtain confidence intervals for one or more fixed or random effects -
use mcmcsamp

Did I miss anything important? - What else would people like to do?

Cheers

Andrew

Andrew Robinson
Senior Lecturer in Statistics                       Tel: +61-3-8344-9763
Department of Mathematics and Statistics            Fax: +61-3-8344 4599
University of Melbourne, VIC 3010 Australia
Email: a.robinson at ms.unimelb.edu.au    Website: http://www.ms.unimelb.edu.au


From gunter.berton at gene.com  Wed Sep 13 00:53:10 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 12 Sep 2006 15:53:10 -0700
Subject: [R] About truncated distribution
In-Reply-To: <200609121541592080733@yahoo.ca>
Message-ID: <00c401c6d6be$38e48af0$711f210a@gne.windows.gene.com>

> 
> But my question is a bit different. What I know is the mean 
> and sd after truncation. If I assume the distribution is 
> normal, how I am gonna develope the original distribution 
> using this two parameters?

You can't, as they are plainly not sufficient (you need to know the amount
of truncation also). If you have only the mean and sd and neither the actual
data nor the truncation point you're through.

-- Bert Gunter
Genentech


 Could anybody give me some advice? 
> Thanks in advance!
> 
> Jen
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ritwik.sinha at gmail.com  Wed Sep 13 01:20:01 2006
From: ritwik.sinha at gmail.com (Ritwik Sinha)
Date: Tue, 12 Sep 2006 19:20:01 -0400
Subject: [R] About truncated distribution
In-Reply-To: <00c401c6d6be$38e48af0$711f210a@gne.windows.gene.com>
References: <200609121541592080733@yahoo.ca>
	<00c401c6d6be$38e48af0$711f210a@gne.windows.gene.com>
Message-ID: <42bc98300609121620i434c8e89ycb9f5a1a852adaae@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060912/0a90d1c6/attachment.pl 

From deepayan.sarkar at gmail.com  Wed Sep 13 01:34:10 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Tue, 12 Sep 2006 16:34:10 -0700
Subject: [R] lattice cloud and conditional axis limits
In-Reply-To: <ee699l$h1b$1@sea.gmane.org>
References: <ee699l$h1b$1@sea.gmane.org>
Message-ID: <eb555e660609121634n42a3f997h46a32b53f30af84c@mail.gmail.com>

On 9/12/06, Karl Ove Hufthammer <karloh at mi.uib.no> wrote:
> I'm using the 'cloud' function in the 'lattice' package to produce
> multi-panel 3D scatter plots. The range of the values used vary much
> between each panel (especially on the z axis), so I wish the axis limits
> to be calculated based on the (conditional) data.
>
> Here's a minimal example:
>
> library(lattice)
> z=1:200
> x=runif(200)
> y=runif(200)
> s=factor(rep(c(1,2),each=100))
> cloud(z~x*y|s,scales=list(arrows=FALSE))
>
> On the first panel, the z values are in the range [1,100], and on the second
> panel, they are in the range [101,200]. I wish the z axis to reflect this,
> i.e., to only span these values. (In my actual data sets, this does make
> more sense than in this example.) If 'cloud' worked the same way the
> 'xypanel' function does, one of the following would work:
>
> cloud(z~x*y|s,scales=list(arrows=FALSE,z=list(relation="free")))
> cloud(z~x*y|s,scales=list(arrows=FALSE,relation="free"))
>
> However, it does not. Any ideas how I can make it work?

There's no direct support, but you can write a small panel function
with more or less the desired effect:

cloud(z ~ x * y | s, scales = list(arrows=FALSE),

      panel =
      function(x, y, subscripts, z, ..., zlim) {
          zlim <- range(z[subscripts], na.rm = TRUE)
          panel.cloud(x, y, subscripts, z, ..., zlim = zlim)
      })

This is for z only, x and y should work similarly. A general solution
is possible, but I don't think the benefits justify the amount of work
required (patches are welcome, of course).

Deepayan


>
> --
> Karl Ove Hufthammer
> E-mail and Jabber: karl at huftis.org


From deepayan.sarkar at gmail.com  Wed Sep 13 01:41:04 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Tue, 12 Sep 2006 16:41:04 -0700
Subject: [R] wireplot margins and additional z-axis
In-Reply-To: <20060912072337.19620@gmx.net>
References: <20060911194003.283820@gmx.net>
	<eb555e660609111315h220befb0kccbd92ce5bdca390@mail.gmail.com>
	<20060912072337.19620@gmx.net>
Message-ID: <eb555e660609121641p54fb959bm56ecaa1851cd6ca9@mail.gmail.com>

On 9/12/06, Klaus Nordhausen <klausch at gmx.de> wrote:
> Dear Deepayan,
>
> thanks for your reply, the change of the aspect does however not solve my problem with the space below the graph on the .eps
> I attached the .eps (still with the old aspect) so that it is maybe clearer what my
> problem is.

No, it's not clearer; this is basically the same EPS that I got, so it
gives me no new information. What do you get with the new aspect? If
it's not what you want, you'll have to explain what you want more
clearly. Also, don't make the panel borders transparent, as it makes
it difficult to understand what's going on.

Deepayan

> Any other suggestions?
>
> Klaus
>
>
> > > Dear R experts,
> > >
> > > it would be very kind if you could help me with two wireplot problems.
> > >
> > > First, when I make a wireplot and transform it into an .eps using the
> > postscript function the eps-file leaves always a lot of space below the plot,
> > as if it would leave space for a legend or something like that.
> > > How can i get the plot into the bottom corner without the space below?
> > The space is not there when I just display the plot in R on my screen (I use
> > R.2.3.1 on Windows XP). Or in general, how can I get the margins on all
> > sides as small as possible since I wnat to include the eps into a report and
> > do not need the space around.
> > >
> > > The following code has the space on the eps:
> > >
> > > library(lattice)
> > >  plot.vol <- wireframe(volcano, aspect = 1, scales=list(arrows=F)
> > ,zlab=list("Z-axis",rot=90))
> > >
> >
> > Perhaps you want something like
> >
> > aspect = c(1, 1.5)
> >
> > instead.
> >
> > > postscript("example_plot.eps", width = 14.0/2.54, height = 19.0/2.54,
> > >                 horizontal = FALSE, onefile = FALSE,paper="special")
> > >
> > > trellis.par.set("axis.line",list(alpha=1,col=1,lty=0,lwd=1))
> > >
> > > print(plot.vol)
> > >
> > > dev.off()
> > >
> > >
> > > Secondly, is it possible to add to the wireplot a further z-axis. I
> > found only how to choose at which veritcal line I want the tickmarks and label,
> > but is it also possible to have it at two vertical lines?
> > >
> >
> > No (but it shouldn't be too hard to add that feature; I'll have to check).
> >
> > Deepayan
> >


From ssim at lic.co.nz  Wed Sep 13 11:52:54 2006
From: ssim at lic.co.nz (ssim at lic.co.nz)
Date: 13-Sep-2006 11:52:54 ZE12
Subject: [R] Bootstrapping for Firth bias reduction logistic regression
In-Reply-To: <200412211116.iBLB7p6W005858@hypatia.math.ethz.ch>
Message-ID: <200609122353.k8CNr2ok031695@hypatia.math.ethz.ch>

Dear list,

I want to validate the coefficients computed from Firth bias reduction
ligistic regression using bootstrap resampling method. However, I got an
error message which I coule not decipher. Help is needed....


 Error in if (mx > 1) delta <- delta/mx : missing value where TRUE/FALSE
needed


> fixed<-function(data,indices){
+      resampling<-data[indices,] # select obs. in bootstrap sample
+      mod<-logistf(abf.flag~TBF,data=resampling)
+      coefficients(mod)
+ }
> library(boot)
> set.seed(1231)
> fixed.boot<-boot(group3,fixed,R=1000)
Error in if (mx > 1) delta <- delta/mx : missing value where TRUE/FALSE
needed



Best regards,
stella


From jebyrnes at ucdavis.edu  Wed Sep 13 02:26:45 2006
From: jebyrnes at ucdavis.edu (Jarrett Byrnes)
Date: Tue, 12 Sep 2006 17:26:45 -0700
Subject: [R] Updating lmer - object is not subsettable?
Message-ID: <6ECEF463-5F56-461D-9A4C-E1D0D638A32E@ucdavis.edu>

I'm attempting to write a general function to implement Faraway's  
bootstrapping algorithm for mixed models with lmer, but have run into  
a curious problem.  I'm comparing two models

model.1<-lmer(Response ~ Treatment + (1|Trial), data=exp.data,  
method="ML")
model.2<-lmer(Response ~ 1 + (1|Trial), data=exp.data, method="ML")


When I attempt to update model.2 with simulated data, however, I get  
the following error:

sim.data<-unlist(simulate(model.1))
sim.model.2<-update(model.2, sim.data~.)

Error in x[[3]] : object is not subsettable


Now, the following
sim.model.1<-update(model.1, sim.data~.)

appears to work just fine.  Does anyone know why update won't work,  
and is there something I can do about this?

-Jarrett


From lewinger at usc.edu  Wed Sep 13 02:34:13 2006
From: lewinger at usc.edu (Juan Pablo Lewinger)
Date: Tue, 12 Sep 2006 17:34:13 -0700
Subject: [R] Retrieving value computed in inner function call
Message-ID: <7.0.1.0.0.20060912172620.01931950@usc.edu>

Dear R users,

Consider the following example function:

f = function(a,b) {
    g = function(x) a*x + b
    h = function(x) g(x)^2 + x^2
    opt = optimize(h,lower = -1, upper = 1)
    x.min = opt$minimum
    h.xmin = opt$objective
    g.xmin = g(x.min)
    return(c(x.min, h.xmin, g.xmin))
}

In my real problem the function that plays the role of "g" is costly 
to compute. Now, to minimize "h", "optimize" calls "h" with different 
values of x. In particular, at the end of the optimization, "h" would 
be called with argument x.min, the minimizer of h(x). Therefore, 
buried somewhere, there has to be a call to "g" with argument x=x.min 
which I would like to retrieve in order to avoid the extra call to 
"g" in the line before the return. Can this be done without too much pain?

I'd very much appreciate any help.



Juan Pablo Lewinger
Department of Preventive Medicine
University of Southern California


From ggrothendieck at gmail.com  Wed Sep 13 02:48:01 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 12 Sep 2006 20:48:01 -0400
Subject: [R] Retrieving value computed in inner function call
In-Reply-To: <7.0.1.0.0.20060912172620.01931950@usc.edu>
References: <7.0.1.0.0.20060912172620.01931950@usc.edu>
Message-ID: <971536df0609121748w3c0062d6r9e528e41eeaddaa2@mail.gmail.com>

Check out:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/83547.html

On 9/12/06, Juan Pablo Lewinger <lewinger at usc.edu> wrote:
> Dear R users,
>
> Consider the following example function:
>
> f = function(a,b) {
>    g = function(x) a*x + b
>    h = function(x) g(x)^2 + x^2
>    opt = optimize(h,lower = -1, upper = 1)
>    x.min = opt$minimum
>    h.xmin = opt$objective
>    g.xmin = g(x.min)
>    return(c(x.min, h.xmin, g.xmin))
> }
>
> In my real problem the function that plays the role of "g" is costly
> to compute. Now, to minimize "h", "optimize" calls "h" with different
> values of x. In particular, at the end of the optimization, "h" would
> be called with argument x.min, the minimizer of h(x). Therefore,
> buried somewhere, there has to be a call to "g" with argument x=x.min
> which I would like to retrieve in order to avoid the extra call to
> "g" in the line before the return. Can this be done without too much pain?
>
> I'd very much appreciate any help.
>
>
>
> Juan Pablo Lewinger
> Department of Preventive Medicine
> University of Southern California
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From bates at stat.wisc.edu  Wed Sep 13 05:16:31 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 12 Sep 2006 22:16:31 -0500
Subject: [R] Updating lmer - object is not subsettable?
In-Reply-To: <6ECEF463-5F56-461D-9A4C-E1D0D638A32E@ucdavis.edu>
References: <6ECEF463-5F56-461D-9A4C-E1D0D638A32E@ucdavis.edu>
Message-ID: <40e66e0b0609122016p71364b6fp6b42fe4af84bbee7@mail.gmail.com>

Can you show a traceback on this example?  It may be related to a
problem that I just fixed in the development version of the lme4
package.

Alternatively if you can make the data available I can generate a
traceback myself.


On 9/12/06, Jarrett Byrnes <jebyrnes at ucdavis.edu> wrote:
> I'm attempting to write a general function to implement Faraway's
> bootstrapping algorithm for mixed models with lmer, but have run into
> a curious problem.  I'm comparing two models
>
> model.1<-lmer(Response ~ Treatment + (1|Trial), data=exp.data,
> method="ML")
> model.2<-lmer(Response ~ 1 + (1|Trial), data=exp.data, method="ML")
>
>
> When I attempt to update model.2 with simulated data, however, I get
> the following error:
>
> sim.data<-unlist(simulate(model.1))
> sim.model.2<-update(model.2, sim.data~.)
>
> Error in x[[3]] : object is not subsettable
>
>
> Now, the following
> sim.model.1<-update(model.1, sim.data~.)
>
> appears to work just fine.  Does anyone know why update won't work,
> and is there something I can do about this?
>
> -Jarrett
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From chabotd at globetrotter.net  Wed Sep 13 05:44:56 2006
From: chabotd at globetrotter.net (Denis Chabot)
Date: Tue, 12 Sep 2006 23:44:56 -0400
Subject: [R] reshaping a dataset
Message-ID: <A451D802-DEE7-40E0-B918-9211000D6217@globetrotter.net>

Hi,

I'm trying to move to R the last few data handling routines I was  
performing in SAS.

I'm working on stomach content data. In the simplified example I  
provide below, there are variables describing the origin of each prey  
item (nbpc is a ship number, each ship may have been used on  
different trips, each trip has stations, and individual fish (tagno)  
can be caught at each station.

For each stomach the number of lines corresponds to the number of  
prey items. Thus a variable identifies prey type, and others (here  
only one, mass) provide information on prey abundance or size or  
digestion level.

Finally, there can be accompanying variables that are not used but  
that I need to keep for later analyses (e.g. depth in the example  
below).

At some point I need to transform such a dataset into another format  
where each stomach occupies a single line, and there are columns for  
each prey item.

The "reshape" function works really well, my program is in fact  
simpler than the SAS equivalent (not shown, don't want to bore you,  
but available on request), except that I need zeros when prey types  
are absent from a stomach instead of NAs, a problem for which I only  
have a shaky solution at the moment:

1) creation of a dummy dataset:
#######
nbpc <- rep(c(20,34), c(110,90))
trip <- c(rep(1:3, c(40, 40, 30)), rep(1:2, c(60,30)))
set <- c(rep(1:4, c(10, 8, 7, 15)), rep(c(10,12), c(25,15)), rep(1:3,  
rep(10,3)),
          rep(10:12, c(20, 10, 30)), rep(7:8, rep(15,2)))
depth <- c(rep(c(100, 150, 200, 250), c(10, 8, 7, 15)), rep(c 
(100,120), c(25,15)), rep(c(75, 50, 200), rep(10,3)),
          rep(c(200, 150, 50), c(20, 10, 30)), rep(c(100, 250), rep 
(15,2)))
tagno <- rep(round(runif(42,1,200)),
              c(7,3, 4,4, 2,2,3, 5,5,5,  4,6,4,3,5,3, 7,8, 4,6, 5,5,  
7,3,
                6,6,4,4, 4,6, 3,3,4,5,5,6,4, 5,5,5, 8,7))
prey.codes <-c(187, 438, 792, 811)
prey <- sample(prey.codes, 200, replace=T)
mass <- runif(200, 0, 10)

test <- data.frame(nbpc, trip, set, depth, tagno, prey, mass)
########

Because there are often multiple occurrences of the same prey in a  
single stomach, I need to sum them for each stomach before using  
"reshape". Here I use summarizeBy because my understanding of the  
many variants of "apply" is not very good:

########
test2 <- summaryBy(mass~nbpc+trip+set+tagno+prey, data=test, FUN=sum,  
keep.names=T, id=~depth)

#this messes up sorting order, I fix it
k <- order(test2$nbpc, test2$trip, test2$set, test2$tagno)
test3 <- test2[k,]
result <- reshape(test3, v.names="mass", idvar=c("nbpc", "trip",  
"set", "tagno"),
                 timevar="prey", direction="wide")
#########

I'm quite happy with this, although you may know of better ways of  
doing it.
But my problem is with preys that are absent from a stomach. In later  
analyses, I need them to have zero abundance instead of NA.
My shaky solution is:
#########
empties <- is.na(result)
result[empties] <- 0
#########

which did the job in this example, but it won't always. For instance  
there could have been NAs for "depth", which I do not want to become  
zero.

Is there a way to transform NAs into zeros for multiple columns of a  
dataframe in one step, while ignoring some columns?

Or maybe there is another way to achieve this that would have put  
zeros where I need them (i.e. something else than "reshape")?

Thanking you in advance,

Denis Chabot


From jebyrnes at ucdavis.edu  Wed Sep 13 06:10:49 2006
From: jebyrnes at ucdavis.edu (Jarrett Byrnes)
Date: Tue, 12 Sep 2006 21:10:49 -0700
Subject: [R] Updating lmer - object is not subsettable?
In-Reply-To: <40e66e0b0609122016p71364b6fp6b42fe4af84bbee7@mail.gmail.com>
References: <6ECEF463-5F56-461D-9A4C-E1D0D638A32E@ucdavis.edu>
	<40e66e0b0609122016p71364b6fp6b42fe4af84bbee7@mail.gmail.com>
Message-ID: <6FF04DAB-0CF7-4F2C-8744-D93E096EC204@ucdavis.edu>

Of course.  The traceback is as follows.  If you wish, I can  
privately email you the data, as well as the function I'm working on.

17: all.names(x)
16: inherits(x, "factor")
15: is.factor(table)
14: match(x, table, nomatch = 0)
13: "/" %in% all.names(x)
12: slashTerms(x[[3]])
11: FUN(X[[1]], ...)
10: lapply(bb, function(x) {
         if (is.list(trms <- slashTerms(x[[3]])))
             return(lapply(unlist(makeInteraction(trms)), function 
(trm) substitute(foo |
                 bar, list(foo = x[[2]], bar = trm))))
         x
     })
9: unlist(lapply(bb, function(x) {
        if (is.list(trms <- slashTerms(x[[3]])))
            return(lapply(unlist(makeInteraction(trms)), function 
(trm) substitute(foo |
                bar, list(foo = x[[2]], bar = trm))))
        x
    }))
8: expandSlash(findbars(formula[[3]]))
7: lmer(formula = sim.data ~ (1 | Trial), data = exp.data, method =  
"ML")
6: lmer(formula = sim.data ~ (1 | Trial), data = exp.data, method =  
"ML")
5: eval(expr, envir, enclos)
4: eval(call, parent.frame())
3: .local(object, ...)
2: update(model.2, sim.data ~ .)
1: update(model.2, sim.data ~ .)



On Sep 12, 2006, at 8:16 PM, Douglas Bates wrote:

> Can you show a traceback on this example?  It may be related to a
> problem that I just fixed in the development version of the lme4
> package.
>
> Alternatively if you can make the data available I can generate a
> traceback myself.
>
>
> On 9/12/06, Jarrett Byrnes <jebyrnes at ucdavis.edu> wrote:
>> I'm attempting to write a general function to implement Faraway's
>> bootstrapping algorithm for mixed models with lmer, but have run into
>> a curious problem.  I'm comparing two models
>>
>> model.1<-lmer(Response ~ Treatment + (1|Trial), data=exp.data,
>> method="ML")
>> model.2<-lmer(Response ~ 1 + (1|Trial), data=exp.data, method="ML")
>>
>>
>> When I attempt to update model.2 with simulated data, however, I get
>> the following error:
>>
>> sim.data<-unlist(simulate(model.1))
>> sim.model.2<-update(model.2, sim.data~.)
>>
>> Error in x[[3]] : object is not subsettable
>>
>>
>> Now, the following
>> sim.model.1<-update(model.1, sim.data~.)
>>
>> appears to work just fine.  Does anyone know why update won't work,
>> and is there something I can do about this?
>>
>> -Jarrett
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting- 
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>


From ggrothendieck at gmail.com  Wed Sep 13 06:32:28 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 13 Sep 2006 00:32:28 -0400
Subject: [R] reshaping a dataset
In-Reply-To: <A451D802-DEE7-40E0-B918-9211000D6217@globetrotter.net>
References: <A451D802-DEE7-40E0-B918-9211000D6217@globetrotter.net>
Message-ID: <971536df0609122132i73af6202y62f46a31a8ffd79@mail.gmail.com>

If I understand this correctly we want to sum the mass over each combination
of the first 6 variables and display the result with the 6th, prey,
along the top and the others along the side.

library(reshape)
testm <- melt(test, id = 1:6)
cast(testm, nbpc + trip + set + tagno + depth ~ prey)

Now fix up the NAs.

On 9/12/06, Denis Chabot <chabotd at globetrotter.net> wrote:
> Hi,
>
> I'm trying to move to R the last few data handling routines I was
> performing in SAS.
>
> I'm working on stomach content data. In the simplified example I
> provide below, there are variables describing the origin of each prey
> item (nbpc is a ship number, each ship may have been used on
> different trips, each trip has stations, and individual fish (tagno)
> can be caught at each station.
>
> For each stomach the number of lines corresponds to the number of
> prey items. Thus a variable identifies prey type, and others (here
> only one, mass) provide information on prey abundance or size or
> digestion level.
>
> Finally, there can be accompanying variables that are not used but
> that I need to keep for later analyses (e.g. depth in the example
> below).
>
> At some point I need to transform such a dataset into another format
> where each stomach occupies a single line, and there are columns for
> each prey item.
>
> The "reshape" function works really well, my program is in fact
> simpler than the SAS equivalent (not shown, don't want to bore you,
> but available on request), except that I need zeros when prey types
> are absent from a stomach instead of NAs, a problem for which I only
> have a shaky solution at the moment:
>
> 1) creation of a dummy dataset:
> #######
> nbpc <- rep(c(20,34), c(110,90))
> trip <- c(rep(1:3, c(40, 40, 30)), rep(1:2, c(60,30)))
> set <- c(rep(1:4, c(10, 8, 7, 15)), rep(c(10,12), c(25,15)), rep(1:3,
> rep(10,3)),
>          rep(10:12, c(20, 10, 30)), rep(7:8, rep(15,2)))
> depth <- c(rep(c(100, 150, 200, 250), c(10, 8, 7, 15)), rep(c
> (100,120), c(25,15)), rep(c(75, 50, 200), rep(10,3)),
>          rep(c(200, 150, 50), c(20, 10, 30)), rep(c(100, 250), rep
> (15,2)))
> tagno <- rep(round(runif(42,1,200)),
>              c(7,3, 4,4, 2,2,3, 5,5,5,  4,6,4,3,5,3, 7,8, 4,6, 5,5,
> 7,3,
>                6,6,4,4, 4,6, 3,3,4,5,5,6,4, 5,5,5, 8,7))
> prey.codes <-c(187, 438, 792, 811)
> prey <- sample(prey.codes, 200, replace=T)
> mass <- runif(200, 0, 10)
>
> test <- data.frame(nbpc, trip, set, depth, tagno, prey, mass)
> ########
>
> Because there are often multiple occurrences of the same prey in a
> single stomach, I need to sum them for each stomach before using
> "reshape". Here I use summarizeBy because my understanding of the
> many variants of "apply" is not very good:
>
> ########
> test2 <- summaryBy(mass~nbpc+trip+set+tagno+prey, data=test, FUN=sum,
> keep.names=T, id=~depth)
>
> #this messes up sorting order, I fix it
> k <- order(test2$nbpc, test2$trip, test2$set, test2$tagno)
> test3 <- test2[k,]
> result <- reshape(test3, v.names="mass", idvar=c("nbpc", "trip",
> "set", "tagno"),
>                 timevar="prey", direction="wide")
> #########
>
> I'm quite happy with this, although you may know of better ways of
> doing it.
> But my problem is with preys that are absent from a stomach. In later
> analyses, I need them to have zero abundance instead of NA.
> My shaky solution is:
> #########
> empties <- is.na(result)
> result[empties] <- 0
> #########
>
> which did the job in this example, but it won't always. For instance
> there could have been NAs for "depth", which I do not want to become
> zero.
>
> Is there a way to transform NAs into zeros for multiple columns of a
> dataframe in one step, while ignoring some columns?
>
> Or maybe there is another way to achieve this that would have put
> zeros where I need them (i.e. something else than "reshape")?
>
> Thanking you in advance,
>
> Denis Chabot
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Wed Sep 13 06:55:43 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 13 Sep 2006 00:55:43 -0400
Subject: [R] reshaping a dataset
In-Reply-To: <971536df0609122132i73af6202y62f46a31a8ffd79@mail.gmail.com>
References: <A451D802-DEE7-40E0-B918-9211000D6217@globetrotter.net>
	<971536df0609122132i73af6202y62f46a31a8ffd79@mail.gmail.com>
Message-ID: <971536df0609122155h694bd682odc5a837f7a7d3afb@mail.gmail.com>

I missed your second question which was how to set the NAs to zero
for some of the columns.  Suppose we want to replace the NAs
in columns ic and for sake of example suppose ic specifies
columns 1 to 8:

library(reshape)
testm <- melt(test, id = 1:6)
out <- cast(testm, nbpc + trip + set + tagno + depth ~ prey, sum)

# fix up NAs
ic <- 1:8
out2 <- out[,ic]
out2[is.na(out2)] <- 0
out[,ic] <- out2

On 9/13/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> If I understand this correctly we want to sum the mass over each combination
> of the first 6 variables and display the result with the 6th, prey,
> along the top and the others along the side.
>
> library(reshape)
> testm <- melt(test, id = 1:6)
> cast(testm, nbpc + trip + set + tagno + depth ~ prey)
>
> Now fix up the NAs.
>
> On 9/12/06, Denis Chabot <chabotd at globetrotter.net> wrote:
> > Hi,
> >
> > I'm trying to move to R the last few data handling routines I was
> > performing in SAS.
> >
> > I'm working on stomach content data. In the simplified example I
> > provide below, there are variables describing the origin of each prey
> > item (nbpc is a ship number, each ship may have been used on
> > different trips, each trip has stations, and individual fish (tagno)
> > can be caught at each station.
> >
> > For each stomach the number of lines corresponds to the number of
> > prey items. Thus a variable identifies prey type, and others (here
> > only one, mass) provide information on prey abundance or size or
> > digestion level.
> >
> > Finally, there can be accompanying variables that are not used but
> > that I need to keep for later analyses (e.g. depth in the example
> > below).
> >
> > At some point I need to transform such a dataset into another format
> > where each stomach occupies a single line, and there are columns for
> > each prey item.
> >
> > The "reshape" function works really well, my program is in fact
> > simpler than the SAS equivalent (not shown, don't want to bore you,
> > but available on request), except that I need zeros when prey types
> > are absent from a stomach instead of NAs, a problem for which I only
> > have a shaky solution at the moment:
> >
> > 1) creation of a dummy dataset:
> > #######
> > nbpc <- rep(c(20,34), c(110,90))
> > trip <- c(rep(1:3, c(40, 40, 30)), rep(1:2, c(60,30)))
> > set <- c(rep(1:4, c(10, 8, 7, 15)), rep(c(10,12), c(25,15)), rep(1:3,
> > rep(10,3)),
> >          rep(10:12, c(20, 10, 30)), rep(7:8, rep(15,2)))
> > depth <- c(rep(c(100, 150, 200, 250), c(10, 8, 7, 15)), rep(c
> > (100,120), c(25,15)), rep(c(75, 50, 200), rep(10,3)),
> >          rep(c(200, 150, 50), c(20, 10, 30)), rep(c(100, 250), rep
> > (15,2)))
> > tagno <- rep(round(runif(42,1,200)),
> >              c(7,3, 4,4, 2,2,3, 5,5,5,  4,6,4,3,5,3, 7,8, 4,6, 5,5,
> > 7,3,
> >                6,6,4,4, 4,6, 3,3,4,5,5,6,4, 5,5,5, 8,7))
> > prey.codes <-c(187, 438, 792, 811)
> > prey <- sample(prey.codes, 200, replace=T)
> > mass <- runif(200, 0, 10)
> >
> > test <- data.frame(nbpc, trip, set, depth, tagno, prey, mass)
> > ########
> >
> > Because there are often multiple occurrences of the same prey in a
> > single stomach, I need to sum them for each stomach before using
> > "reshape". Here I use summarizeBy because my understanding of the
> > many variants of "apply" is not very good:
> >
> > ########
> > test2 <- summaryBy(mass~nbpc+trip+set+tagno+prey, data=test, FUN=sum,
> > keep.names=T, id=~depth)
> >
> > #this messes up sorting order, I fix it
> > k <- order(test2$nbpc, test2$trip, test2$set, test2$tagno)
> > test3 <- test2[k,]
> > result <- reshape(test3, v.names="mass", idvar=c("nbpc", "trip",
> > "set", "tagno"),
> >                 timevar="prey", direction="wide")
> > #########
> >
> > I'm quite happy with this, although you may know of better ways of
> > doing it.
> > But my problem is with preys that are absent from a stomach. In later
> > analyses, I need them to have zero abundance instead of NA.
> > My shaky solution is:
> > #########
> > empties <- is.na(result)
> > result[empties] <- 0
> > #########
> >
> > which did the job in this example, but it won't always. For instance
> > there could have been NAs for "depth", which I do not want to become
> > zero.
> >
> > Is there a way to transform NAs into zeros for multiple columns of a
> > dataframe in one step, while ignoring some columns?
> >
> > Or maybe there is another way to achieve this that would have put
> > zeros where I need them (i.e. something else than "reshape")?
> >
> > Thanking you in advance,
> >
> > Denis Chabot
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From torsten.mathies at matec-gmbh.com  Wed Sep 13 07:18:13 2006
From: torsten.mathies at matec-gmbh.com (Torsten Mathies)
Date: Wed, 13 Sep 2006 07:18:13 +0200
Subject: [R] qcc - xbar.one how to change the background color
Message-ID: <000d01c6d6f4$03924640$5a03a8c0@msc.de>

Dear qcc-experts,
 
I would like to adjust the colors (i. e. background color) of my
xbar.one-charts.
 
par(bg="#FFFFFF") does not work.
 
What can I do?
 
Regards
 
Torsten


From AnupTyagi at yahoo.com  Wed Sep 13 07:48:47 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Wed, 13 Sep 2006 05:48:47 +0000 (UTC)
Subject: [R] Reading fixed column format
References: <loom.20060911T172146-100@post.gmane.org><BAY116-DAV571634EB23B6A0F2F6502CF2A0@phx.gbl>	<loom.20060911T185053-310@post.gmane.org>	<BAY116-DAV15C8CB3C190E253F6BD659CF2A0@phx.gbl>	<loom.20060912T084704-214@post.gmane.org>
	<3EBEC9D3-559C-4607-838D-46042D36A3AA@virginia.edu>
	<45067F0A.3060304@lancaster.ac.uk>
Message-ID: <loom.20060913T073708-542@post.gmane.org>

Barry Rowlingson <B.Rowlingson <at> lancaster.ac.uk> writes:


> > None of these seem to read non-coniguous variables from columns; or 
> > may be I am missing something. "read.fwf" is not meant for large
> > files according to a post in the archives. Thanks for the pointers. I
> > have read the R data input and output. Anupam.
> 
>   First up, how 'large' is your 'large ASCII file'? How many rows and 
> columns?

There are 356,112 records, 326 variables, fixed record length of 1283 positions.
Zipped file is 42MB. There are no field (variable) separaters (delimiters).

>   Secondly, what are 'non-contiguous' variables?

Variables that are not in adjoining positions in the file: reading them from the
file would require skipping columns while reading. For example, below are the
start positions of the first three variables I would like to read.

StartingColumn  VariableName  	FieldLength
1 	STATE 	2
24 	INTVID 	3
30 	PSU 	10


>   Perhaps if you posted the first few lines and columns of the file then 
> we might get an idea of how to read it in.

Because a record (row) of the file is 1283 columns, I would not like to post it
here.

Thank you for your response.

Anupam.


From ggrothendieck at gmail.com  Wed Sep 13 08:01:43 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 13 Sep 2006 02:01:43 -0400
Subject: [R] Reading fixed column format
In-Reply-To: <loom.20060913T073708-542@post.gmane.org>
References: <loom.20060911T172146-100@post.gmane.org>
	<BAY116-DAV571634EB23B6A0F2F6502CF2A0@phx.gbl>
	<loom.20060911T185053-310@post.gmane.org>
	<BAY116-DAV15C8CB3C190E253F6BD659CF2A0@phx.gbl>
	<loom.20060912T084704-214@post.gmane.org>
	<3EBEC9D3-559C-4607-838D-46042D36A3AA@virginia.edu>
	<45067F0A.3060304@lancaster.ac.uk>
	<loom.20060913T073708-542@post.gmane.org>
Message-ID: <971536df0609122301l502d10dch8af36d720ab801c9@mail.gmail.com>

I know you would prefer a 100% R solution but using the unix cut
command (a Windows version is available in tools.zip at:
http://www.murdoch-sutherland.com/Rtools/
) is really easy.  Maybe if you preprocessed it with that you
could then use read.fwf.

For example, look how easy it was to cut this file down to half
extracting columns 2-3 and 6-8:

C:\bin>type a.dat
123456789
123456789
123456789

C:\bin>cut -c2-3,6-8 a.dat
23678
23678
23678


On 9/13/06, Anupam Tyagi <AnupTyagi at yahoo.com> wrote:
> Barry Rowlingson <B.Rowlingson <at> lancaster.ac.uk> writes:
>
>
> > > None of these seem to read non-coniguous variables from columns; or
> > > may be I am missing something. "read.fwf" is not meant for large
> > > files according to a post in the archives. Thanks for the pointers. I
> > > have read the R data input and output. Anupam.
> >
> >   First up, how 'large' is your 'large ASCII file'? How many rows and
> > columns?
>
> There are 356,112 records, 326 variables, fixed record length of 1283 positions.
> Zipped file is 42MB. There are no field (variable) separaters (delimiters).
>
> >   Secondly, what are 'non-contiguous' variables?
>
> Variables that are not in adjoining positions in the file: reading them from the
> file would require skipping columns while reading. For example, below are the
> start positions of the first three variables I would like to read.
>
> StartingColumn  VariableName    FieldLength
> 1       STATE   2
> 24      INTVID  3
> 30      PSU     10
>
>
> >   Perhaps if you posted the first few lines and columns of the file then
> > we might get an idea of how to read it in.
>
> Because a record (row) of the file is 1283 columns, I would not like to post it
> here.
>
> Thank you for your response.
>
> Anupam.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From AnupTyagi at yahoo.com  Wed Sep 13 08:39:37 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Wed, 13 Sep 2006 06:39:37 +0000 (UTC)
Subject: [R] Reading fixed column format
References: <loom.20060911T172146-100@post.gmane.org>
	<BAY116-DAV571634EB23B6A0F2F6502CF2A0@phx.gbl>
	<loom.20060911T185053-310@post.gmane.org>
	<BAY116-DAV15C8CB3C190E253F6BD659CF2A0@phx.gbl>
	<loom.20060912T084704-214@post.gmane.org>
	<3EBEC9D3-559C-4607-838D-46042D36A3AA@virginia.edu>
	<45067F0A.3060304@lancaster.ac.uk>
	<loom.20060913T073708-542@post.gmane.org>
	<971536df0609122301l502d10dch8af36d720ab801c9@mail.gmail.com>
Message-ID: <loom.20060913T083003-590@post.gmane.org>

Gabor Grothendieck <ggrothendieck <at> gmail.com> writes:

> C:\bin>cut -c2-3,6-8 a.dat
> 23678
> 23678
> 23678

Thanks. I think this will work. How do I redirect the output to a file on
windows? Is there simple way to convert the cut command to a script on windows,
because the entire command may not fit on one line? Anupam.


From arinbasu at cashette.com  Wed Sep 13 04:37:25 2006
From: arinbasu at cashette.com (arin basu)
Date: Tue, 12 Sep 2006 19:37:25 -0700 (PDT)
Subject: [R] Of fixed column format (and more fixed mindsets)
Message-ID: <19581504.1158115045968.JavaMail.Administrator@appsrv>


> 
> Message: 107
> Date: Tue, 12 Sep 2006 05:25:09 -0400
> From: Michael Kubovy <kubovy at virginia.edu>
> Subject: Re: [R] Reading fixed column format
> To: Anupam Tyagi <AnupTyagi at yahoo.com>
> Cc: r-help at stat.math.ethz.ch
> Message-ID: <3EBEC9D3-559C-4607-838D-46042D36A3AA at virginia.edu>
> Content-Type: text/plain; charset=US-ASCII; delsp=yes; format=flowed
> 
> On Sep 12, 2006, at 2:47 AM, Anupam Tyagi wrote:
> 
> > Jason Barnhart <jasoncbarnhart <at> msn.com> writes:
> >
> >>
> >> These posts may be helpful.
> >> http://tolstoy.newcastle.edu.au/R/help/05/06/5776.html
> >> https://stat.ethz.ch/pipermail/r-help/2002-May/021145.html
> >>
> >> Using scan directly may also work for you rather than read.fwf.
> >>
> >> Also, there are posts regarding using other tools such a 'perl' or  
> >> 'cut' to
> >> prepocess the data
> >> before reading with R.  Searching the archives with those keywords  
> >> should
> >> help.
> >
> > I new user should not have to learn "perl","cut", "awk", etc simply  
> > to be able
> > to use R. Does not make sense to me.
> 
> Hi Anupam,
> 
> You'll get much better help here if you're not ill-tempered. This is  
> a group of extraordinarily helpful volunteers who owe you less than  
> you paid for the product.
> 
> Please consider saving your data in a way that will make it easier to  
> read into R. No program can read every dataset.
> _____________________________
> Professor Michael Kubovy
> University of Virginia
> Department of Psychology
> USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
> Parcels:    Room 102        Gilmer Hall
>          McCormick Road    Charlottesville, VA 22903
> Office:    B011    +1-434-982-4729
> Lab:        B019    +1-434-982-4751
> Fax:        +1-434-982-4766
> WWW:    http://www.people.virginia.edu/~mk9y/


Also, if a user is new to R, it always helps if he or she uses a spreadsheet or another program/ set of scripts to first preprocess the data before reading them in R. These preprocessing might include simple steps such as naming and renaming columns, selecting which columns one needs in the dataset, etc. Then, once one is more familiar working with R, these can be accomplished within R relatively easily.  

/Arin Basu


From AnupTyagi at yahoo.com  Tue Sep 12 13:24:54 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Tue, 12 Sep 2006 11:24:54 +0000 (UTC)
Subject: [R] Reading fixed column format
References: <loom.20060911T172146-100@post.gmane.org><BAY116-DAV571634EB23B6A0F2F6502CF2A0@phx.gbl>	<loom.20060911T185053-310@post.gmane.org>	<BAY116-DAV15C8CB3C190E253F6BD659CF2A0@phx.gbl>	<loom.20060912T084704-214@post.gmane.org>
	<3EBEC9D3-559C-4607-838D-46042D36A3AA@virginia.edu>
	<45067F0A.3060304@lancaster.ac.uk>
Message-ID: <loom.20060912T123009-247@post.gmane.org>

Barry Rowlingson <B.Rowlingson <at> lancaster.ac.uk> writes:

> > None of these seem to read non-coniguous variables from columns; or 
> > may be I am missing something. "read.fwf" is not meant for large
> > files according to a post in the archives. Thanks for the pointers. I
> > have read the R data input and output. Anupam.
> 
>   First up, how 'large' is your 'large ASCII file'? How many rows and 
> columns?

There are 356,112 records, and 326 variables. It has a fixed record length of
1283 positions, therefore "cut -b" can not be used.
 
>   Secondly, what are 'non-contiguous' variables?

When I do not want to read all columns. For example, I would like to read the
following:

StartingColumn  VariableName  	FieldLength
1 	STATE 	2
24 	INTVID 	3
27 	DISPCODE 3
30 	PSU 	10

Sometimes I would also like to format the data after it has been read. For
example, the ASCII file has price in columns 100 to 105 written as 005999. I
want to read this and format it as 59.99 (omitting leading zeros in the price).

>   Perhaps if you posted the first few lines and columns of the file then 
> we might get an idea of how to read it in.

I have not even downloaded the data onto my computer yet, because I am not sure
I can read it in. The zipped file is 67MB. Using similar data a few years ago, I
recall the unzipped file to be about 350--400 MB. I had used MySQL then, but it
took some doing to get it in, and there were things that did not seem to work as
I wanted them to---I could not figure out how to label the variables. I usually
do not have to work with a dataframe of more than 10-30 MB at a time.

It would be good to have a facility in R which defines the meta-data: labelling
and structure of the dataset: positions of variables, their names, their lables,
their levels (e.g. for ordered choice or group variables: yes, sometimes, no
type responses). This can be saved as a seperate object and passed to a function
that gets the named varibales from the ASCII file (names of variables to get can
be given as arguments or as, attaches the meta data and creates a dataframe with
all the meta-data attached. The meta-data of the dataframe could include notes
at dataframe and variable level, and other information. This information is
passed on to the plotting functions and used when formatting the output of
statistical procedures.

I agree with with Michael Kobovy that this is a very helpful list, and people do
not owe less than what one paid for the software :)

Anupam.


From AnupTyagi at yahoo.com  Tue Sep 12 13:34:01 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Tue, 12 Sep 2006 11:34:01 +0000 (UTC)
Subject: [R] Gnuplot epslatex format also in R?
References: <6ade6f6c0609120249k1ec5bdb2tec2f185f42ce1a2d@mail.gmail.com>
	<Pine.LNX.4.64.0609121126280.5673@gannet.stats.ox.ac.uk>
Message-ID: <loom.20060912T133212-909@post.gmane.org>

Prof Brian Ripley <ripley <at> stats.ox.ac.uk> writes:

> 
> R has an xfig driver, and AFAIK you can do this from xfig.
> 

Is there an xfig port for Windows, without cygwin? If so, I will be thankful for
a pointer to the where it can be downloaded from. I have been looking for it for
some time. Anupam.


From descall at blueyonder.co.uk  Tue Sep 12 08:43:17 2006
From: descall at blueyonder.co.uk (Des Callaghan)
Date: Tue, 12 Sep 2006 07:43:17 +0100
Subject: [R] Transformation of a data frame
Message-ID: <200609120643.k8C6hEIl016383@hypatia.math.ethz.ch>

Dear R-helpers,

Apologies in advance for this (probably) simple question.  I've searched the
R Archive and can't seem to find a solution to my problem.  

I have a data frame of vegetation quadrat data with the following format:

Q S C
1 A 5
1 B 10
1 C 50
1 D 10
2 A 20
2 E 10
2 C 40
3 D 5
3 F 1
3 G 5
3 B 75

Where Q is the sample (vegetation quadrats), S is the species and C is the
percentage cover of each species within the sample.  I wish to transform
this into a community data matrix for analysis within the vegan package,
which needs the following data frame format:

Q A  B  C  D  E  F  G
1 5  10 50 10 0  0  0
2 20 0  40 0  10 0  0
3 0  75 0  5  0  1  75

I can't manage to do this transformation so your help would be much
appreciated.  Thanks very much in advance.

All the best,
Des


From r.hankin at noc.soton.ac.uk  Wed Sep 13 09:10:00 2006
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Wed, 13 Sep 2006 08:10:00 +0100
Subject: [R] functions and strings
Message-ID: <AD43B569-8269-43DF-994C-875313DEC2A7@soc.soton.ac.uk>

Hi

If

string <- "xyz"
f <- function(x){1 + sin(cos(x)) + exp(x^2)}

How do I manipulate "string" and f() to give the string

"1 + sin(cos(xyz)) + exp(xyz^2)"

?


--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From ssu at george.org.au  Wed Sep 13 09:11:36 2006
From: ssu at george.org.au (Steve Su)
Date: Wed, 13 Sep 2006 17:11:36 +1000
Subject: [R] Power analysis for repeated measures ANCOVA
Message-ID: <3B862D6B11ECDE458F679DDA238A135A7A2F53@lisa>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060913/d945a379/attachment.pl 

From dimitris.rizopoulos at med.kuleuven.be  Wed Sep 13 09:31:40 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 13 Sep 2006 09:31:40 +0200
Subject: [R] Transformation of a data frame
References: <200609120643.k8C6hEIl016383@hypatia.math.ethz.ch>
Message-ID: <00a301c6d706$a7521d70$0540210a@www.domain>

one approach is to use reshape(), e.g.,

# suppose that 'dat' is your data.frame, then
res <- reshape(dat, direction = "wide", idvar = "Q", timevar = "S")
res[is.na(res)] <- 0
res


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Des Callaghan" <descall at blueyonder.co.uk>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, September 12, 2006 8:43 AM
Subject: [R] Transformation of a data frame


> Dear R-helpers,
>
> Apologies in advance for this (probably) simple question.  I've 
> searched the
> R Archive and can't seem to find a solution to my problem.
>
> I have a data frame of vegetation quadrat data with the following 
> format:
>
> Q S C
> 1 A 5
> 1 B 10
> 1 C 50
> 1 D 10
> 2 A 20
> 2 E 10
> 2 C 40
> 3 D 5
> 3 F 1
> 3 G 5
> 3 B 75
>
> Where Q is the sample (vegetation quadrats), S is the species and C 
> is the
> percentage cover of each species within the sample.  I wish to 
> transform
> this into a community data matrix for analysis within the vegan 
> package,
> which needs the following data frame format:
>
> Q A  B  C  D  E  F  G
> 1 5  10 50 10 0  0  0
> 2 20 0  40 0  10 0  0
> 3 0  75 0  5  0  1  75
>
> I can't manage to do this transformation so your help would be much
> appreciated.  Thanks very much in advance.
>
> All the best,
> Des
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From rfrancois at mango-solutions.com  Wed Sep 13 09:44:00 2006
From: rfrancois at mango-solutions.com (Romain Francois)
Date: Wed, 13 Sep 2006 08:44:00 +0100
Subject: [R] functions and strings
In-Reply-To: <AD43B569-8269-43DF-994C-875313DEC2A7@soc.soton.ac.uk>
References: <AD43B569-8269-43DF-994C-875313DEC2A7@soc.soton.ac.uk>
Message-ID: <4507B6C0.8070302@mango-solutions.com>

Robin Hankin wrote:
> Hi
>
> If
>
> string <- "xyz"
> f <- function(x){1 + sin(cos(x)) + exp(x^2)}
>
> How do I manipulate "string" and f() to give the string
>
> "1 + sin(cos(xyz)) + exp(xyz^2)"
>
> ?
>   
Hi,

Here what i'll do :

f <- function(x){
  sprintf("1 + sin(cos(%s)) + exp(%s^2)", x, x)
}

Cheers,

Romai


> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>   tel  023-8059-7743
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From dimitris.rizopoulos at med.kuleuven.be  Wed Sep 13 09:45:15 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 13 Sep 2006 09:45:15 +0200
Subject: [R] functions and strings
References: <AD43B569-8269-43DF-994C-875313DEC2A7@soc.soton.ac.uk>
Message-ID: <00b001c6d708$8d3838a0$0540210a@www.domain>

one approach could be the following

strng <- gsub("x", "xyz", deparse(body(f))[2])
sub('^[[:space:]]+', '', strng)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Robin Hankin" <r.hankin at noc.soton.ac.uk>
To: <R-help at r-project.org>
Sent: Wednesday, September 13, 2006 9:10 AM
Subject: [R] functions and strings


> Hi
>
> If
>
> string <- "xyz"
> f <- function(x){1 + sin(cos(x)) + exp(x^2)}
>
> How do I manipulate "string" and f() to give the string
>
> "1 + sin(cos(xyz)) + exp(xyz^2)"
>
> ?
>
>
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>  tel  023-8059-7743
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From r.hankin at noc.soton.ac.uk  Wed Sep 13 09:54:22 2006
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Wed, 13 Sep 2006 08:54:22 +0100
Subject: [R] functions and strings
In-Reply-To: <00b001c6d708$8d3838a0$0540210a@www.domain>
References: <AD43B569-8269-43DF-994C-875313DEC2A7@soc.soton.ac.uk>
	<00b001c6d708$8d3838a0$0540210a@www.domain>
Message-ID: <31027DE3-B4D1-41FA-9F8B-4B480DEB6EEC@soc.soton.ac.uk>

Hi Dmitris

thanks for this  but it's not quite right:


 > f <- function(x){sin(x)+exp(x)}
 > strng <- gsub("x", "xyz", deparse(body(f))[2])
 > sub('^[[:space:]]+', '', strng)
[1] "sin(xyz) + exyzp(xyz)"


and I would want "sin(xyz) + exp(xyz)"




On 13 Sep 2006, at 08:45, Dimitris Rizopoulos wrote:

> strng <- gsub("x", "xyz", deparse(body(f))[2])
> sub('^[[:space:]]+', '', strng)

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From klausch at gmx.de  Wed Sep 13 10:04:01 2006
From: klausch at gmx.de (Klaus Nordhausen)
Date: Wed, 13 Sep 2006 10:04:01 +0200
Subject: [R] wireplot margins and additional z-axis
In-Reply-To: <eb555e660609121641p54fb959bm56ecaa1851cd6ca9@mail.gmail.com>
References: <20060911194003.283820@gmx.net>	
	<eb555e660609111315h220befb0kccbd92ce5bdca390@mail.gmail.com>	
	<20060912072337.19620@gmx.net>
	<eb555e660609121641p54fb959bm56ecaa1851cd6ca9@mail.gmail.com>
Message-ID: <20060913080401.146390@gmx.net>

Dear Deepayan,

sorry for not being clear - but my problem has nothing to do with the aspect. If I create the eps the following way

library(lattice)
plot.vol<- wireframe(volcano,
               aspect = c(1,1.5), scales=list(arrows=F),zlab=list("Z-axis",rot=90))

postscript("example_plot_3.eps", width = 14.0/2.54, height = 19.0/2.54,
                horizontal = FALSE, onefile = FALSE,paper="special")
trellis.par.set("axis.line",list(alpha=1,col=1,lty=1,lwd=1))
print(plot.vol)
dev.off() 

The plot is still not in the left bottom corner of the file. There is a lot of space below the outer box line. If I include this eps in latex it will also include this space and if I put for example the figure caption below it I have this huge gap between actual graph and caption.

And for comparison, if I create with xyplot an eps like

postscript("example_plot_4.eps", width = 14.0/2.54, height = 19.0/2.54,
                horizontal = FALSE, onefile = FALSE,paper="special")

    Depth <- equal.count(quakes$depth, number=8, overlap=.1)
     plot.depth<-xyplot(lat ~ long | Depth, data = quakes)
     update(trellis.last.object(),
            strip = strip.custom(strip.names = TRUE, strip.levels = TRUE),
            par.strip.text = list(cex = 0.75),
            aspect = "iso")

print(plot.depth)
dev.off()

the figure is really in the left bottom corner and included in latex has not that gap between caption and actual figure.

I hope this describes my problem better.

Klaus



> On 9/12/06, Klaus Nordhausen <klausch at gmx.de> wrote:
> > Dear Deepayan,
> >
> > thanks for your reply, the change of the aspect does however not solve
> my problem with the space below the graph on the .eps
> > I attached the .eps (still with the old aspect) so that it is maybe
> clearer what my
> > problem is.
> 
> No, it's not clearer; this is basically the same EPS that I got, so it
> gives me no new information. What do you get with the new aspect? If
> it's not what you want, you'll have to explain what you want more
> clearly. Also, don't make the panel borders transparent, as it makes
> it difficult to understand what's going on.
> 
> Deepayan
> 
> > Any other suggestions?
> >
> > Klaus
> >
> >
> > > > Dear R experts,
> > > >
> > > > it would be very kind if you could help me with two wireplot
> problems.
> > > >
> > > > First, when I make a wireplot and transform it into an .eps using
> the
> > > postscript function the eps-file leaves always a lot of space below
> the plot,
> > > as if it would leave space for a legend or something like that.
> > > > How can i get the plot into the bottom corner without the space
> below?
> > > The space is not there when I just display the plot in R on my screen
> (I use
> > > R.2.3.1 on Windows XP). Or in general, how can I get the margins on
> all
> > > sides as small as possible since I wnat to include the eps into a
> report and
> > > do not need the space around.
> > > >
> > > > The following code has the space on the eps:
> > > >
> > > > library(lattice)
> > > >  plot.vol <- wireframe(volcano, aspect = 1, scales=list(arrows=F)
> > > ,zlab=list("Z-axis",rot=90))
> > > >
> > >
> > > Perhaps you want something like
> > >
> > > aspect = c(1, 1.5)
> > >
> > > instead.
> > >
> > > > postscript("example_plot.eps", width = 14.0/2.54, height =
> 19.0/2.54,
> > > >                 horizontal = FALSE, onefile = FALSE,paper="special")
> > > >
> > > > trellis.par.set("axis.line",list(alpha=1,col=1,lty=0,lwd=1))
> > > >
> > > > print(plot.vol)
> > > >
> > > > dev.off()
> > > >
> > > >
> > > > Secondly, is it possible to add to the wireplot a further z-axis. I
> > > found only how to choose at which veritcal line I want the tickmarks
> and label,
> > > but is it also possible to have it at two vertical lines?
> > > >
> > >
> > > No (but it shouldn't be too hard to add that feature; I'll have to
> check).
> > >
> > > Deepayan
> > >

-- 



Echte DSL-Flatrate ab 0,- Euro* http://www.gmx.net/de/go/dsl


From dimitris.rizopoulos at med.kuleuven.be  Wed Sep 13 10:08:49 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 13 Sep 2006 10:08:49 +0200
Subject: [R] functions and strings
References: <AD43B569-8269-43DF-994C-875313DEC2A7@soc.soton.ac.uk>
	<00b001c6d708$8d3838a0$0540210a@www.domain>
	<31027DE3-B4D1-41FA-9F8B-4B480DEB6EEC@soc.soton.ac.uk>
Message-ID: <00d901c6d70b$d7da2e10$0540210a@www.domain>

yes you're right, maybe this is better

> f <- function(x){sin(x)+exp(x)}
> strng <- gsub("(x)", "(xyz)", deparse(body(f))[2], fixed = TRUE)
> sub('^[[:space:]]+', '', strng)
[1] "sin(xyz) + exp(xyz)"


Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Robin Hankin" <r.hankin at noc.soton.ac.uk>
To: "Dimitris Rizopoulos" <dimitris.rizopoulos at med.kuleuven.be>
Cc: "Robin Hankin" <r.hankin at noc.soton.ac.uk>; <r-help at r-project.org>
Sent: Wednesday, September 13, 2006 9:54 AM
Subject: Re: [R] functions and strings


> Hi Dmitris
> 
> thanks for this  but it's not quite right:
> 
> 
> > f <- function(x){sin(x)+exp(x)}
> > strng <- gsub("x", "xyz", deparse(body(f))[2])
> > sub('^[[:space:]]+', '', strng)
> [1] "sin(xyz) + exyzp(xyz)"
> 
> 
> and I would want "sin(xyz) + exp(xyz)"
> 
> 
> 
> 
> On 13 Sep 2006, at 08:45, Dimitris Rizopoulos wrote:
> 
>> strng <- gsub("x", "xyz", deparse(body(f))[2])
>> sub('^[[:space:]]+', '', strng)
> 
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>  tel  023-8059-7743
>

Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From uctcmkt at ucl.ac.uk  Wed Sep 13 10:20:40 2006
From: uctcmkt at ucl.ac.uk (Mike Townsley)
Date: Wed, 13 Sep 2006 09:20:40 +0100
Subject: [R] forcing levelplot to use relative cuts (ie cuts for each panel)
Message-ID: <6.2.5.6.0.20060913090615.027f4338@ucl.ac.uk>

Dear guRus,

I'm having trouble producing a levelplot with relative cuts for each 
panel (my data has large differences in scales, so I want to use 
quantiles for each panel).

My attempts to change the 'at'  argument in panel.levelplot function 
have not met with success.

Below is a toy example.

xy <- expand.grid(x = 1:3, y = 1:3)

aaa <- rbind(cbind(xy, z = 1:9, site = rep('A', 9)),
              cbind(xy, z = (1:9)/10, site = rep('B', 9)),
              cbind(xy, z = (1:9)*10, site = rep('C', 9)))

aaa

library(lattice)
levelplot(z~x+y|site, data = aaa)         # using absolute cuts

# now, attempt relative cuts

levelplot(z~x+y|site, data = aaa, panel = function(...) {
           panel.levelplot(at = quantile(z),...) })

I get the following message:
Error in panel.levelplot(at = quantile(z), ...) :
         formal argument "at" matched by multiple actual arguments

My idea was to determine the cut points each time the panel function 
is called (ie each subset of the data), but I guess this was the 
wrong thing to do.  Can someone point out what I'm missing?

Thanks in advance,

MT


------------------------------------------------------------
Dr Michael Townsley
Senior Research Fellow
Jill Dando Institute of Crime Science
University College London
Second Floor, Brook House
London, WC1E 7HN

Phone: 020 7679 0820
Fax: 020 7679 0828
Email: m.townsley at ucl.ac.uk


From lewinger at usc.edu  Wed Sep 13 10:20:27 2006
From: lewinger at usc.edu (Pablo Lewinger)
Date: Wed, 13 Sep 2006 01:20:27 -0700
Subject: [R] Retrieving value computed in inner function call
In-Reply-To: <971536df0609121748w3c0062d6r9e528e41eeaddaa2@mail.gmail.co m>
References: <7.0.1.0.0.20060912172620.01931950@usc.edu>
	<7.0.1.0.0.20060912172620.01931950@usc.edu>
Message-ID: <5.0.2.1.0.20060913011604.00bbb040@email.usc.edu>

Though not obvious at first the posting you pointed me too is very helpful 
indeed. Thanks a lot Gabor.

Juan Pablo

At 08:48 PM 9/12/2006 -0400, Gabor Grothendieck wrote:
>Check out:
>
>http://finzi.psych.upenn.edu/R/Rhelp02a/archive/83547.html
>
>On 9/12/06, Juan Pablo Lewinger <lewinger at usc.edu> wrote:
>>Dear R users,
>>
>>Consider the following example function:
>>
>>f = function(a,b) {
>>    g = function(x) a*x + b
>>    h = function(x) g(x)^2 + x^2
>>    opt = optimize(h,lower = -1, upper = 1)
>>    x.min = opt$minimum
>>    h.xmin = opt$objective
>>    g.xmin = g(x.min)
>>    return(c(x.min, h.xmin, g.xmin))
>>}
>>
>>In my real problem the function that plays the role of "g" is costly
>>to compute. Now, to minimize "h", "optimize" calls "h" with different
>>values of x. In particular, at the end of the optimization, "h" would
>>be called with argument x.min, the minimizer of h(x). Therefore,
>>buried somewhere, there has to be a call to "g" with argument x=x.min
>>which I would like to retrieve in order to avoid the extra call to
>>"g" in the line before the return. Can this be done without too much pain?
>>
>>I'd very much appreciate any help.
>>
>>
>>
>>Juan Pablo Lewinger
>>Department of Preventive Medicine
>>University of Southern California
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.


From r.hankin at noc.soton.ac.uk  Wed Sep 13 10:22:58 2006
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Wed, 13 Sep 2006 09:22:58 +0100
Subject: [R] functions and strings
In-Reply-To: <00d901c6d70b$d7da2e10$0540210a@www.domain>
References: <AD43B569-8269-43DF-994C-875313DEC2A7@soc.soton.ac.uk>
	<00b001c6d708$8d3838a0$0540210a@www.domain>
	<31027DE3-B4D1-41FA-9F8B-4B480DEB6EEC@soc.soton.ac.uk>
	<00d901c6d70b$d7da2e10$0540210a@www.domain>
Message-ID: <0AF70300-F3B9-4C7A-A237-B9BDC5B7354C@soc.soton.ac.uk>

Hi Dmitris, Thierry,

I'm getting there but it's still not quite right if f() includes  
something like x^2:

f <- function(x){exp(x^2)}
>>

gsub("(x)", "(xyz)", deparse(body(f))[2], fixed = TRUE)


[1] "    x^2"

[I don't care about the spaces]



also,

  I can't quite see how to implement Thierry's suggestion about
changing the letter "x" into a letter that does not occur in f(),  
because of the
following example:

  f <- function(x){abcdefghijklmnopqrstuvwxyz(x^2)}




On 13 Sep 2006, at 09:08, Dimitris Rizopoulos wrote:

> yes you're right, maybe this is better
>
>> f <- function(x){sin(x)+exp(x)}
>> strng <- gsub("(x)", "(xyz)", deparse(body(f))[2], fixed = TRUE)
>> sub('^[[:space:]]+', '', strng)
> [1] "sin(xyz) + exp(xyz)"
>
>
> Best,
> Dimitris
>

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From lorenzo.isella at gmail.com  Wed Sep 13 10:38:01 2006
From: lorenzo.isella at gmail.com (Lorenzo Isella)
Date: Wed, 13 Sep 2006 10:38:01 +0200
Subject: [R] R & Matlab for Particle Tracking
Message-ID: <a2b3004b0609130138w12476e07ud0cce6351fc3431b@mail.gmail.com>

Dear All,
A question a bit outside statistics.
In my group, a lot of people use Matlab for simple simulations of
stochastic processes describing  convection/diffusion problems (as
long as the numerics does not get too expensive and one has to resort
to C or Fortran).
Leaving aside the theory, it all boils down to some kind of Monte
Carlo technique, picking random draws from a certain distribution at a
specified time step, tracking many particles and taking averages.
I cannot see why R is not suitable to do this (unless performance
becomes a problem).
Has anyone on the list already gained some experience in this kind of
simulations using R?
Is there an online collection of example codes?
It goes without saying that I am not going to start any flame war with
Matlab users, only I cannot see any specific reason why R could not be
deployed for this sort of tasks.
Kind Regards

Lorenzo


From as at hut.at  Wed Sep 13 10:38:16 2006
From: as at hut.at (sun)
Date: Wed, 13 Sep 2006 10:38:16 +0200
Subject: [R] formatting data to be analysed using multinomial logistic
	regression (nnet)
References: <5.1.0.14.0.20060910205402.00c10ba8@pop3.brisnet.org.au>
Message-ID: <004201c6d70f$f8a5e220$04879b83@campus.tue.nl>

bump.

would like to know the answer too. I am about using nnet--multinom to 
estimate a multinomial logit model, but are not sure if this function 
handles categorical data input.

Thanks for any help.

----- Original Message ----- 
From: "Bob Green" <bgreen at dyson.brisnet.org.au>
To: <r-help at stat.math.ethz.ch>
Sent: Sunday, September 10, 2006 10:24 PM
Subject: [R] formatting data to be analysed using multinomial logistic 
regression (nnet)


>
>
> I am looking into using the multinomial logistic regression option in the
> nnet library and have two questions about formatting the data.
>
> 1. Can data be analysed in the following format  or does it need to be
> transformed into count data, such as the housing data in MASS?
>
> Id Crime paranoia hallucinate toc disorg crimhist age
> 1 2 1 0 1 0 1 25
> 2 2 0 1 1 1 1 37
> 3 1 1 0 1 1 0 42
> 4 3 0 0 0 0 1 25
> 5 2 1 0 1 0 0 49
>
>
> 2.    Can a ratio variable such as $age be included into a model, such as
> the one below?
>
>
> crimepred <- glm  (crime ~ paranoia + hallucinate + toc  + crimhist, 
> family
> = poisson, data = mht )
>
>
> Any assistance with the above is appreciated,
>
>
> regards
>
> Bob Green
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From antoninferry at voila.fr  Wed Sep 13 11:00:15 2006
From: antoninferry at voila.fr (Antonin Ferry)
Date: Wed, 13 Sep 2006 11:00:15 +0200 (CEST)
Subject: [R] unexpected result in glm (family=poisson) for data with an only
 zero response in one factor
Message-ID: <1208320.1158138015580.JavaMail.www@wwinf4102>

Dear members,
here is my trouble: My data consists of counts of trapped insects in different attractive traps. I usually use GLMs with a poisson error distribution to find out the differences between my traitments (and to look at other factor effects). But for some dataset where one traitment contains only zeros, GLM with poisson family fail to find any difference between this particular traitment and anyother one (even with traitment that have trapped a lot of insects). GLMs with gaussian family does not seem to have the same problem but GLMs with binomial family does.
I'm not sure if it is a statistical problem or if it comes from R... in the latter case I think some solution exists (perhaps in the options of the glm() function ?).
Thank you for your help.


Here I figure out an exemple to past in the console:

## START ##############################################################################
# Take a data set of counts for two traitments, one containing only zeros
A=c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
B=c(1,0,0,0,2,1,0,0,1,2,0,0,0,1,2,2,0,1,1,0,1,0,2,1,1,0,1,2,0,1,0,1,1,1,0,1,1,1,0,1)
traitment=c(rep("A",40),rep("B",40))
response=c(A,B)
mydata=data.frame(traitment ,response)


# Make a GLM on this dataset , with "family=poisson"

 g=glm(response~traitment, data=mydata, family=poisson)
 anova.glm(g,test="Chisq")
# There is an effect of the traitment ...

 summary(g)
# But traitment A does not differ from traitment B ! ! ! (the pvalue is always close from 1 in such cases)

# Now if you replace only one zero of the A reponse to 1, the GLM works properly:
 mydata[1,2]=1
 g=glm(response~traitment, data=mydata, family=poisson)
 anova.glm(g,test="Chisq")
 summary(g)
#####################################################################################  END ##



Antonin Ferry (PhD)

"Laboratoire d'Ecobiologie des Insectes Parasitoides"
http://www.parasitoides.univ-rennes1.fr
Universit? de Renes1, FRANCE


From r.hankin at noc.soton.ac.uk  Wed Sep 13 11:09:27 2006
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Wed, 13 Sep 2006 10:09:27 +0100
Subject: [R] functions and strings
In-Reply-To: <2E9C414912813E4EB981326983E0A104020F3E9C@inexch.instnat.be.grp>
References: <2E9C414912813E4EB981326983E0A104020F3E9C@inexch.instnat.be.grp>
Message-ID: <04380A66-5F0E-4E5D-9392-E5EAB60758FB@soc.soton.ac.uk>

Hello everyone

I know it looks like I'm making heavy weather of this, but
I don't think I communicated my problem properly.  I really
appreciate you guys' help here.

I am writing a wrapper for a mathematical library to
which I want to send character strings that it can execute,
and then pass the answer back to R.

Now, I want a user to be able to type _any_ function
and _any_ string.  For example:

f <- function(i){sin(i) + cos(sin(i^2))}
string <- "tti"

and then I want a function do() such that do(f,string) will return

"sin(tti) + cos(sin(tti^2))"

without worrying about whether f()'s arguments include or
do not include a particular letter, and without insisting that "i"
always appears as "(i)" .

Although thinking about it, it's not
actually that bad to require the user to use some otherwise
rare sequence of letters, say "XxX" as
an argument, and then Dmitris's first method would work.

Having said that, this is not an ideal solution
and it would be nicer to have some method that could detect
what the argument to f() is, where it is in the body, and substitute
those occurences for "string".

I want a method  that is perfectly general; I posted my
example of abcd...z(), not to be annoying and pedantic
but to illustrate that a simple gsub approach wouldn't work:
one has to know in advance which letters can and cannot
be used, and this information isn't available.

I don't have a function so named (yet ;-).


best wishes

rksh

>
> Hi Dmitris, Thierry,
>
> I'm getting there but it's still not quite right if f() includes
> something like x^2:
>
> f <- function(x){exp(x^2)}
>>>
>
> gsub("(x)", "(xyz)", deparse(body(f))[2], fixed = TRUE)
>
>
> [1] "    x^2"
>
> [I don't care about the spaces]
>
>
>
> also,
>
>   I can't quite see how to implement Thierry's suggestion about
> changing the letter "x" into a letter that does not occur in f(),
> because of the
> following example:
>
>   f <- function(x){abcdefghijklmnopqrstuvwxyz(x^2)}
>
>
>
>
> On 13 Sep 2006, at 09:08, Dimitris Rizopoulos wrote:
>
>> yes you're right, maybe this is better
>>
>>> f <- function(x){sin(x)+exp(x)}
>>> strng <- gsub("(x)", "(xyz)", deparse(body(f))[2], fixed = TRUE)
>>> sub('^[[:space:]]+', '', strng)
>> [1] "sin(xyz) + exp(xyz)"
>>
>>
>> Best,
>> Dimitris
>>
>

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From mel at altk.com  Wed Sep 13 11:28:31 2006
From: mel at altk.com (mel)
Date: Wed, 13 Sep 2006 11:28:31 +0200
Subject: [R] Test internet presence
In-Reply-To: <450709BE.7000803@bfro.uni-lj.si>
References: <XFMail.060911134821.Ted.Harding@nessie.mcc.ac.uk>	<Pine.LNX.4.64.0609111419350.19275@gannet.stats.ox.ac.uk>	<loom.20060912T003831-608@post.gmane.org>	<Pine.LNX.4.64.0609121648420.13250@gannet.stats.ox.ac.uk>
	<450709BE.7000803@bfro.uni-lj.si>
Message-ID: <4507CF3F.6060002@altk.com>

Gregor Gorjanc a ?crit :

> If I summarize the thread there is (currently) no way to test for
> internet presence with a general approach.

what about try(readLines(...)) ?
(at least it works fine on Windows.)


From rich.fitzjohn at gmail.com  Wed Sep 13 11:54:51 2006
From: rich.fitzjohn at gmail.com (Rich FitzJohn)
Date: Wed, 13 Sep 2006 21:54:51 +1200
Subject: [R] functions and strings
In-Reply-To: <04380A66-5F0E-4E5D-9392-E5EAB60758FB@soc.soton.ac.uk>
References: <2E9C414912813E4EB981326983E0A104020F3E9C@inexch.instnat.be.grp>
	<04380A66-5F0E-4E5D-9392-E5EAB60758FB@soc.soton.ac.uk>
Message-ID: <5934ae570609130254k6c94cd81u702578b5593cf72f@mail.gmail.com>

Hi,

Perhaps try this (based on 'bquote'):

rewrite.expression <- function(expr, to, dep) {
  f <- function(expr) {
    if ( length(expr) == 1 )
      if ( expr == as.name(dep) )
        as.name(to)
      else
        expr
    else
      as.call(lapply(expr, f))
  }
  f(expr)
}

rewrite <- function(expr, to, dep='x') {
  rewrite.expression(substitute(expr), to, dep)
}

> rewrite(1 + sin(cos(x)) + exp(x^2), 'xyz')
1 + sin(cos(xyz)) + exp(xyz^2)
> rewrite(sin(x)+exp(x), 'xyz')
sin(xyz) + exp(xyz)
> rewrite(sin(i) + cos(sin(i^2)), 'tti', 'i')
sin(tti) + cos(sin(tti^2))
## Or, closer to your example, using the name of the argument and body
## of the function:
f <- function(r)
  2*r/sin(r) - b

> rewrite.expression(body(f), 'foo', names(formals(f)))
2 * foo/sin(foo) - b

Hope that helps,
Rich

On 9/13/06, Robin Hankin <r.hankin at noc.soton.ac.uk> wrote:
> Hello everyone
>
> I know it looks like I'm making heavy weather of this, but
> I don't think I communicated my problem properly.  I really
> appreciate you guys' help here.
>
> I am writing a wrapper for a mathematical library to
> which I want to send character strings that it can execute,
> and then pass the answer back to R.
>
> Now, I want a user to be able to type _any_ function
> and _any_ string.  For example:
>
> f <- function(i){sin(i) + cos(sin(i^2))}
> string <- "tti"
>
> and then I want a function do() such that do(f,string) will return
>
> "sin(tti) + cos(sin(tti^2))"
>
> without worrying about whether f()'s arguments include or
> do not include a particular letter, and without insisting that "i"
> always appears as "(i)" .
>
> Although thinking about it, it's not
> actually that bad to require the user to use some otherwise
> rare sequence of letters, say "XxX" as
> an argument, and then Dmitris's first method would work.
>
> Having said that, this is not an ideal solution
> and it would be nicer to have some method that could detect
> what the argument to f() is, where it is in the body, and substitute
> those occurences for "string".
>
> I want a method  that is perfectly general; I posted my
> example of abcd...z(), not to be annoying and pedantic
> but to illustrate that a simple gsub approach wouldn't work:
> one has to know in advance which letters can and cannot
> be used, and this information isn't available.
>
> I don't have a function so named (yet ;-).
>
>
> best wishes
>
> rksh
>
> >
> > Hi Dmitris, Thierry,
> >
> > I'm getting there but it's still not quite right if f() includes
> > something like x^2:
> >
> > f <- function(x){exp(x^2)}
> >>>
> >
> > gsub("(x)", "(xyz)", deparse(body(f))[2], fixed = TRUE)
> >
> >
> > [1] "    x^2"
> >
> > [I don't care about the spaces]
> >
> >
> >
> > also,
> >
> >   I can't quite see how to implement Thierry's suggestion about
> > changing the letter "x" into a letter that does not occur in f(),
> > because of the
> > following example:
> >
> >   f <- function(x){abcdefghijklmnopqrstuvwxyz(x^2)}
> >
> >
> >
> >
> > On 13 Sep 2006, at 09:08, Dimitris Rizopoulos wrote:
> >
> >> yes you're right, maybe this is better
> >>
> >>> f <- function(x){sin(x)+exp(x)}
> >>> strng <- gsub("(x)", "(xyz)", deparse(body(f))[2], fixed = TRUE)
> >>> sub('^[[:space:]]+', '', strng)
> >> [1] "sin(xyz) + exp(xyz)"
> >>
> >>
> >> Best,
> >> Dimitris
> >>
> >
>
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>   tel  023-8059-7743
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Rich FitzJohn
rich.fitzjohn <at> gmail.com


From phhs80 at gmail.com  Wed Sep 13 12:18:20 2006
From: phhs80 at gmail.com (Paul Smith)
Date: Wed, 13 Sep 2006 11:18:20 +0100
Subject: [R] Gnuplot epslatex format also in R?
In-Reply-To: <loom.20060912T133212-909@post.gmane.org>
References: <6ade6f6c0609120249k1ec5bdb2tec2f185f42ce1a2d@mail.gmail.com>
	<Pine.LNX.4.64.0609121126280.5673@gannet.stats.ox.ac.uk>
	<loom.20060912T133212-909@post.gmane.org>
Message-ID: <6ade6f6c0609130318l5ef402d3n1854c4c992c0beb0@mail.gmail.com>

On 9/12/06, Anupam Tyagi <AnupTyagi at yahoo.com> wrote:
> > R has an xfig driver, and AFAIK you can do this from xfig.
>
> Is there an xfig port for Windows, without cygwin? If so, I will be thankful for
> a pointer to the where it can be downloaded from. I have been looking for it for
> some time. Anupam.

WinFIG, Anupam? Its site at

http://www.schmidt-web-berlin.de/WinFIG.htm

Paul


From MUEHGE at de.ibm.com  Wed Sep 13 12:18:24 2006
From: MUEHGE at de.ibm.com (Thorsten Muehge)
Date: Wed, 13 Sep 2006 12:18:24 +0200
Subject: [R] R-question
Message-ID: <OF8109D7CC.D032CA84-ONC12571E8.0022D409-C12571E8.00389DF6@de.ibm.com>



Hello Colleagues,
I programmed in SAS for 3 years and would like to switch to a not so costly
software product.

Hence I started to evaluate R, and my first test look promising.

However I have some question:

1. Is it possible to query R files by SQL internally on data frames (not on
a database) and how is the syntax (I have the RODBC package installed).

I would like to extract year, Quarter, week, from a date column in a data
frame (see attachment). After this I want to attach the column to the
original data frame.

How do I do this in R?

Dr .Th.M?hge,

PMP?
Procurement Technology Center
IBM Deutschland GmbH, Hechtsheimer Str.2, D-55131 Mainz
Phone: xx49-(0)6131-84-2416
Mobile: xx49-(0)15117457978
e-mail: muehge at de.ibm.com
(See attached file: Debug1.csv)

From p_connolly at ihug.co.nz  Wed Sep 13 12:41:52 2006
From: p_connolly at ihug.co.nz (Patrick Connolly)
Date: Wed, 13 Sep 2006 22:41:52 +1200
Subject: [R] R CMD INSTALL with debugging
Message-ID: <20060913104152.GB4295@ihug.co.nz>

I've run into a problem with lazy loading on a Linux system when
trying to install the Matrix package (version 0.995-16) which didn't
happen with version 0.995-2.  The problem is not with a x86_64
system: it's a 32 bit machine, the exact description I don't have
right now (R-2.3.1).  The message looks like this:

Error in match.call(fmatch, fcall) : unused argument(s) (x ...)
In addition: There were 12 warnings (use warnings() to see them)
Error: unable to load R code in package 'Matrix'
Execution halted
ERROR: lazy loading failed for package 'Matrix'


I even tried with the --no-lazy switch but that only gave me a message
about being deprecated but otherwise the results were identical.

What do I need to do to get more information about what is making that
match.call message?  The warnings() message is of no use once R has
exited.  I suspect it has something to do with warnings about NULL
environments being deprecated, because I've not tracked down all
instances of them, and the newer Matrix package is more particular
about such things.  Finding what is producing the problem with
match.call might help me work out where the NULL environment messages
are coming from and that would be an added bonus.

TIA

-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}          		 Great minds discuss ideas    
 _( Y )_  	  	        Middle minds discuss events 
(:_~*~_:) 	       		 Small minds discuss people  
 (_)-(_)  	                           ..... Anon
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From jim at bitwrit.com.au  Thu Sep 14 02:47:13 2006
From: jim at bitwrit.com.au (Jim Lemon)
Date: Wed, 13 Sep 2006 20:47:13 -0400
Subject: [R] R-question
In-Reply-To: <OF8109D7CC.D032CA84-ONC12571E8.0022D409-C12571E8.00389DF6@de.ibm.com>
References: <OF8109D7CC.D032CA84-ONC12571E8.0022D409-C12571E8.00389DF6@de.ibm.com>
Message-ID: <4508A691.3080002@bitwrit.com.au>

Thorsten Muehge wrote:
> 
> Hello Colleagues,
> I programmed in SAS for 3 years and would like to switch to a not so costly
> software product.
> 
> Hence I started to evaluate R, and my first test look promising.
> 
> However I have some question:
> 
> 1. Is it possible to query R files by SQL internally on data frames (not on
> a database) and how is the syntax (I have the RODBC package installed).
> 
> I would like to extract year, Quarter, week, from a date column in a data
> frame (see attachment). After this I want to attach the column to the
> original data frame.
> 
> How do I do this in R?

Hi Thorsten,

You may just want to convert the dates as follows, adding them to the 
data frame on the fly:

test.df<-data.frame(dates=as.POSIXlt(c("2004-05-13","2005-07-23","2006-09-13")),
  nums=rnorm(3))
test.df$years<-format(test.df$dates,"%Y")
test.df$quarters<-floor((as.numeric(format(test.df$dates,"%m"))-1)/3)+1
test.df$week<-format(test.df$dates,"%U")
test.df

Jim


From r.hankin at noc.soton.ac.uk  Wed Sep 13 12:51:02 2006
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Wed, 13 Sep 2006 11:51:02 +0100
Subject: [R] functions and strings
In-Reply-To: <5934ae570609130254k6c94cd81u702578b5593cf72f@mail.gmail.com>
References: <2E9C414912813E4EB981326983E0A104020F3E9C@inexch.instnat.be.grp>
	<04380A66-5F0E-4E5D-9392-E5EAB60758FB@soc.soton.ac.uk>
	<5934ae570609130254k6c94cd81u702578b5593cf72f@mail.gmail.com>
Message-ID: <A8350457-631A-45D1-B19D-A9B30C76DFA2@soc.soton.ac.uk>

Rich

that is sweeeeeeeeeeeeet

and does exactly what I want.

Thank you very much.


best wishes

rksh

On 13 Sep 2006, at 10:54, Rich FitzJohn wrote:

> Hi,
>
> Perhaps try this (based on 'bquote'):
>
> rewrite.expression <- function(expr, to, dep) {
>   f <- function(expr) {
>     if ( length(expr) == 1 )
>       if ( expr == as.name(dep) )
>         as.name(to)
>       else
>         expr
>     else
>       as.call(lapply(expr, f))
>   }
>   f(expr)
> }
>
> rewrite <- function(expr, to, dep='x') {
>   rewrite.expression(substitute(expr), to, dep)
> }
>
>> rewrite(1 + sin(cos(x)) + exp(x^2), 'xyz')
> 1 + sin(cos(xyz)) + exp(xyz^2)
>> rewrite(sin(x)+exp(x), 'xyz')
> sin(xyz) + exp(xyz)
>> rewrite(sin(i) + cos(sin(i^2)), 'tti', 'i')
> sin(tti) + cos(sin(tti^2))
> ## Or, closer to your example, using the name of the argument and body
> ## of the function:
> f <- function(r)
>   2*r/sin(r) - b
>
>> rewrite.expression(body(f), 'foo', names(formals(f)))
> 2 * foo/sin(foo) - b
>
> Hope that helps,
> Rich
>

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From Manuel.A.Morales at williams.edu  Wed Sep 13 13:04:17 2006
From: Manuel.A.Morales at williams.edu (Manuel Morales)
Date: Wed, 13 Sep 2006 07:04:17 -0400
Subject: [R] Conservative "ANOVA tables" in lmer
In-Reply-To: <48130.220.237.183.166.1158098663.squirrel@webmail.ms.unimelb.edu.au>
References: <1A8FE5B0-3FE3-4CA9-9224-9CACB9AF4854@muohio.edu>
	<40e66e0b0609070559u656463c1l52a60eda9a095237@mail.gmail.com>
	<20060910055624.GA12212@ms.unimelb.edu.au>
	<40e66e0b0609110943h7dfb057dv12567975ac04bdb2@mail.gmail.com>
	<1158010468.3278.24.camel@solidago.localdomain>
	<48130.220.237.183.166.1158098663.squirrel@webmail.ms.unimelb.edu.au>
Message-ID: <1158145458.3699.3.camel@solidago.localdomain>

On Wed, 2006-09-13 at 08:04 +1000, Andrew Robinson wrote:
> On Tue, September 12, 2006 7:34 am, Manuel Morales wrote:
> > On Mon, 2006-09-11 at 11:43 -0500, Douglas Bates wrote:
> >> Having made that offer I think I will now withdraw it.  Peter's
> >> example has convinced me that this is the wrong thing to do.
> >>
> >> I am encouraged by the fact that the results from mcmcsamp correspond
> >> closely to the correct theoretical results in the case that Peter
> >> described.  I appreciate that some users will find it difficult to
> >> work with a MCMC sample (or to convince editors to accept results
> >> based on such a sample) but I think that these results indicate that
> >> it is better to go after the marginal distribution of the fixed
> >> effects estimates (which is what is being approximated by the MCMC
> >> sample - up to Bayesian/frequentist philosophical differences) than to
> >> use the conditional distribution and somehow try to adjust the
> >> reference distribution.
> >
> > Am I right that the MCMC sample can not be used, however, to evaluate
> > the significance of parameter groups. For example, to assess the
> > significance of a three-level factor? Are there better alternatives than
> > simply adjusting the CI for the number of factor levels
> > (1-alpha/levels).
> 
> I wonder whether the likelihood ratio test would be suitable here?  That
> seems to be supported.  It just takes a little longer.
> 
> > require(lme4)
> > data(sleepstudy)
> > fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
> > fm2 <- lmer(Reaction ~ Days + I(Days^2) + (Days|Subject), sleepstudy)
> > anova(fm1, fm2)
> 
> So, a brief overview of the popular inferential needs and solutions would
> then be:
> 
> 1) Test the statistical significance of one or more fixed or random
> effects - fit a model with and a model without the terms, and use the LRT.

I believe that the LRT is anti-conservative for fixed effects, as
described in Pinheiro and Bates companion book to NLME.

> 2) Obtain confidence intervals for one or more fixed or random effects -
> use mcmcsamp
> 
> Did I miss anything important? - What else would people like to do?
> 
> Cheers
> 
> Andrew
> 
> Andrew Robinson
> Senior Lecturer in Statistics                       Tel: +61-3-8344-9763
> Department of Mathematics and Statistics            Fax: +61-3-8344 4599
> University of Melbourne, VIC 3010 Australia
> Email: a.robinson at ms.unimelb.edu.au    Website: http://www.ms.unimelb.edu.au
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From murdoch at stats.uwo.ca  Wed Sep 13 13:06:36 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 13 Sep 2006 07:06:36 -0400
Subject: [R] Reading fixed column format
In-Reply-To: <loom.20060912T123009-247@post.gmane.org>
References: <loom.20060911T172146-100@post.gmane.org><BAY116-DAV571634EB23B6A0F2F6502CF2A0@phx.gbl>	<loom.20060911T185053-310@post.gmane.org>	<BAY116-DAV15C8CB3C190E253F6BD659CF2A0@phx.gbl>	<loom.20060912T084704-214@post.gmane.org>	<3EBEC9D3-559C-4607-838D-46042D36A3AA@virginia.edu>	<45067F0A.3060304@lancaster.ac.uk>
	<loom.20060912T123009-247@post.gmane.org>
Message-ID: <4507E63C.9050306@stats.uwo.ca>

Anupam Tyagi wrote:
> Barry Rowlingson <B.Rowlingson <at> lancaster.ac.uk> writes:
>
>   
>>> None of these seem to read non-coniguous variables from columns; or 
>>> may be I am missing something. "read.fwf" is not meant for large
>>> files according to a post in the archives. Thanks for the pointers. I
>>> have read the R data input and output. Anupam.
>>>       
>>   First up, how 'large' is your 'large ASCII file'? How many rows and 
>> columns?
>>     
>
> There are 356,112 records, and 326 variables. It has a fixed record length of
> 1283 positions, therefore "cut -b" can not be used.
>  
>   
>>   Secondly, what are 'non-contiguous' variables?
>>     
>
> When I do not want to read all columns. For example, I would like to read the
> following:
>
> StartingColumn  VariableName  	FieldLength
> 1 	STATE 	2
> 24 	INTVID 	3
> 27 	DISPCODE 3
> 30 	PSU 	10
>   

read.fwf() can handle the skipped columns (you use negative column 
values; see the man page).  It will break the read up into blocks, so 
the large size of the original file shouldn't be a problem.

Duncan Murdoch

> Sometimes I would also like to format the data after it has been read. For
> example, the ASCII file has price in columns 100 to 105 written as 005999. I
> want to read this and format it as 59.99 (omitting leading zeros in the price).
>
>   
>>   Perhaps if you posted the first few lines and columns of the file then 
>> we might get an idea of how to read it in.
>>     
>
> I have not even downloaded the data onto my computer yet, because I am not sure
> I can read it in. The zipped file is 67MB. Using similar data a few years ago, I
> recall the unzipped file to be about 350--400 MB. I had used MySQL then, but it
> took some doing to get it in, and there were things that did not seem to work as
> I wanted them to---I could not figure out how to label the variables. I usually
> do not have to work with a dataframe of more than 10-30 MB at a time.
>
> It would be good to have a facility in R which defines the meta-data: labelling
> and structure of the dataset: positions of variables, their names, their lables,
> their levels (e.g. for ordered choice or group variables: yes, sometimes, no
> type responses). This can be saved as a seperate object and passed to a function
> that gets the named varibales from the ASCII file (names of variables to get can
> be given as arguments or as, attaches the meta data and creates a dataframe with
> all the meta-data attached. The meta-data of the dataframe could include notes
> at dataframe and variable level, and other information. This information is
> passed on to the plotting functions and used when formatting the output of
> statistical procedures.
>
> I agree with with Michael Kobovy that this is a very helpful list, and people do
> not owe less than what one paid for the software :)
>
> Anupam.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From B.Rowlingson at lancaster.ac.uk  Wed Sep 13 13:15:53 2006
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Wed, 13 Sep 2006 12:15:53 +0100
Subject: [R] Reading fixed column format
In-Reply-To: <loom.20060912T123009-247@post.gmane.org>
References: <loom.20060911T172146-100@post.gmane.org><BAY116-DAV571634EB23B6A0F2F6502CF2A0@phx.gbl>	<loom.20060911T185053-310@post.gmane.org>	<BAY116-DAV15C8CB3C190E253F6BD659CF2A0@phx.gbl>	<loom.20060912T084704-214@post.gmane.org>	<3EBEC9D3-559C-4607-838D-46042D36A3AA@virginia.edu>	<45067F0A.3060304@lancaster.ac.uk>
	<loom.20060912T123009-247@post.gmane.org>
Message-ID: <4507E869.7030301@lancaster.ac.uk>

Anupam Tyagi wrote:

> There are 356,112 records, and 326 variables. It has a fixed record length of
> 1283 positions, therefore "cut -b" can not be used.

Okay, thats 'large' enough to be awkward...

> It would be good to have a facility in R which defines the meta-data: labelling
> and structure of the dataset: positions of variables, their names, their lables,
> their levels (e.g. for ordered choice or group variables: yes, sometimes, no
> type responses). This can be saved as a seperate object and passed to a function
> that gets the named varibales from the ASCII file (names of variables to get can
> be given as arguments or as, attaches the meta data and creates a dataframe with
> all the meta-data attached. The meta-data of the dataframe could include notes
> at dataframe and variable level, and other information. This information is
> passed on to the plotting functions and used when formatting the output of
> statistical procedures.

  I think you need the following functions to build that kind of thing in R:

  * z = unz("/tmp/file.zip","data.dat") - to create a connection to a 
file in a zip archive - this saves you having to explicitly unzip it...

  * open(z) - to open the connection to the file in the zip...

  * readLines(z,n) - to read 'n' lines from the current position in the 
file...

  * seek(z,m*lineLength-1) - to jump to line 'm' ready to read it.

  Then its just 'substr' and similar string-chopping functions to build 
up the data from each line you want.

  If I had a spare day...

Barry


From B.Rowlingson at lancaster.ac.uk  Wed Sep 13 13:33:37 2006
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Wed, 13 Sep 2006 12:33:37 +0100
Subject: [R] Reading fixed column format
In-Reply-To: <4507E869.7030301@lancaster.ac.uk>
References: <loom.20060911T172146-100@post.gmane.org><BAY116-DAV571634EB23B6A0F2F6502CF2A0@phx.gbl>	<loom.20060911T185053-310@post.gmane.org>	<BAY116-DAV15C8CB3C190E253F6BD659CF2A0@phx.gbl>	<loom.20060912T084704-214@post.gmane.org>	<3EBEC9D3-559C-4607-838D-46042D36A3AA@virginia.edu>	<45067F0A.3060304@lancaster.ac.uk>	<loom.20060912T123009-247@post.gmane.org>
	<4507E869.7030301@lancaster.ac.uk>
Message-ID: <4507EC91.9030804@lancaster.ac.uk>

Barry Rowlingson wrote:


>   If I had a spare day...

  Or if I'd just read Duncan's message about negative widths in read.fwf.

  Anyway, I've learnt about readLines() and seek() and reading zip files 
now, so I can read _anything_....

Barry


From p.dalgaard at biostat.ku.dk  Wed Sep 13 13:44:39 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 13 Sep 2006 13:44:39 +0200
Subject: [R] functions and strings
In-Reply-To: <A8350457-631A-45D1-B19D-A9B30C76DFA2@soc.soton.ac.uk>
References: <2E9C414912813E4EB981326983E0A104020F3E9C@inexch.instnat.be.grp>
	<04380A66-5F0E-4E5D-9392-E5EAB60758FB@soc.soton.ac.uk>
	<5934ae570609130254k6c94cd81u702578b5593cf72f@mail.gmail.com>
	<A8350457-631A-45D1-B19D-A9B30C76DFA2@soc.soton.ac.uk>
Message-ID: <x28xkoc7o8.fsf@viggo.kubism.ku.dk>

Robin Hankin <r.hankin at noc.soton.ac.uk> writes:

> Rich
> 
> that is sweeeeeeeeeeeeet
> 
> and does exactly what I want.
> 
> Thank you very much.


I just wonder whatever substitute() did to get ignored like that?

> substitute(1 + sin(cos(x)) + exp(x^2), list(x=quote(xyz)))
1 + sin(cos(xyz)) + exp(xyz^2)

 
> 
> best wishes
> 
> rksh
> 
> On 13 Sep 2006, at 10:54, Rich FitzJohn wrote:
> 
> > Hi,
> >
> > Perhaps try this (based on 'bquote'):
> >
> > rewrite.expression <- function(expr, to, dep) {
> >   f <- function(expr) {
> >     if ( length(expr) == 1 )
> >       if ( expr == as.name(dep) )
> >         as.name(to)
> >       else
> >         expr
> >     else
> >       as.call(lapply(expr, f))
> >   }
> >   f(expr)
> > }
> >
> > rewrite <- function(expr, to, dep='x') {
> >   rewrite.expression(substitute(expr), to, dep)
> > }
> >
> >> rewrite(1 + sin(cos(x)) + exp(x^2), 'xyz')
> > 1 + sin(cos(xyz)) + exp(xyz^2)
> >> rewrite(sin(x)+exp(x), 'xyz')
> > sin(xyz) + exp(xyz)
> >> rewrite(sin(i) + cos(sin(i^2)), 'tti', 'i')
> > sin(tti) + cos(sin(tti^2))
> > ## Or, closer to your example, using the name of the argument and body
> > ## of the function:
> > f <- function(r)
> >   2*r/sin(r) - b
> >
> >> rewrite.expression(body(f), 'foo', names(formals(f)))
> > 2 * foo/sin(foo) - b
> >
> > Hope that helps,
> > Rich
> >
> 
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>   tel  023-8059-7743
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From vmuggeo at dssm.unipa.it  Wed Sep 13 12:49:59 2006
From: vmuggeo at dssm.unipa.it (vito muggeo)
Date: Wed, 13 Sep 2006 12:49:59 +0200
Subject: [R] unexpected result in glm (family=poisson) for data with an
 only zero response in one factor
In-Reply-To: <1208320.1158138015580.JavaMail.www@wwinf4102>
References: <1208320.1158138015580.JavaMail.www@wwinf4102>
Message-ID: <4507E257.7070906@dssm.unipa.it>

Dear Antonin,
It is a statistical problem: the well-known monotone likelihood.

In this case ML estimate does not exist (or equals infinity) and Wald 
approximations (ob which SE are based) does not hold.

However LRT is valid and provides reliable results.

As far as I know, the only software dealing with monotone likelihood 
problems in loglinear models is LogXact by cytel corporation.

best,
vito


Antonin Ferry wrote:
> Dear members,
> here is my trouble: My data consists of counts of trapped insects in different attractive traps. I usually use GLMs with a poisson error distribution to find out the differences between my traitments (and to look at other factor effects). But for some dataset where one traitment contains only zeros, GLM with poisson family fail to find any difference between this particular traitment and anyother one (even with traitment that have trapped a lot of insects). GLMs with gaussian family does not seem to have the same problem but GLMs with binomial family does.
> I'm not sure if it is a statistical problem or if it comes from R... in the latter case I think some solution exists (perhaps in the options of the glm() function ?).
> Thank you for your help.
> 
> 
> Here I figure out an exemple to past in the console:
> 
> ## START ##############################################################################
> # Take a data set of counts for two traitments, one containing only zeros
> A=c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
> B=c(1,0,0,0,2,1,0,0,1,2,0,0,0,1,2,2,0,1,1,0,1,0,2,1,1,0,1,2,0,1,0,1,1,1,0,1,1,1,0,1)
> traitment=c(rep("A",40),rep("B",40))
> response=c(A,B)
> mydata=data.frame(traitment ,response)
> 
> 
> # Make a GLM on this dataset , with "family=poisson"
> 
>  g=glm(response~traitment, data=mydata, family=poisson)
>  anova.glm(g,test="Chisq")
> # There is an effect of the traitment ...
> 
>  summary(g)
> # But traitment A does not differ from traitment B ! ! ! (the pvalue is always close from 1 in such cases)
> 
> # Now if you replace only one zero of the A reponse to 1, the GLM works properly:
>  mydata[1,2]=1
>  g=glm(response~traitment, data=mydata, family=poisson)
>  anova.glm(g,test="Chisq")
>  summary(g)
> #####################################################################################  END ##
> 
> 
> 
> Antonin Ferry (PhD)
> 
> "Laboratoire d'Ecobiologie des Insectes Parasitoides"
> http://www.parasitoides.univ-rennes1.fr
> Universit? de Renes1, FRANCE
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
====================================
Vito M.R. Muggeo
Dip.to Sc Statist e Matem `Vianelli'
Universit? di Palermo
viale delle Scienze, edificio 13
90128 Palermo - ITALY
tel: 091 6626240
fax: 091 485726/485612


From h.wickham at gmail.com  Wed Sep 13 13:57:27 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Wed, 13 Sep 2006 07:57:27 -0400
Subject: [R] Transformation of a data frame
In-Reply-To: <00a301c6d706$a7521d70$0540210a@www.domain>
References: <200609120643.k8C6hEIl016383@hypatia.math.ethz.ch>
	<00a301c6d706$a7521d70$0540210a@www.domain>
Message-ID: <f8e6ff050609130457j1a5eb35cxbaf53afb266e7c4@mail.gmail.com>

> one approach is to use reshape(), e.g.,
>
> # suppose that 'dat' is your data.frame, then
> res <- reshape(dat, direction = "wide", idvar = "Q", timevar = "S")
> res[is.na(res)] <- 0
> res

You can also use the reshape package:

library(reshape)
datm <- melt(dat, id=1:2)
cast(datm, Q ~ S)

See the introduction to reshape
(http://www.had.co.nz/reshape/introduction.pdf) for more details and
examples.

Hadley


From john.maindonald at anu.edu.au  Wed Sep 13 14:02:04 2006
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Wed, 13 Sep 2006 22:02:04 +1000
Subject: [R] unexpected result in glm (family=poisson) for data with an only
	zero response in one factor
Message-ID: <96AF5A8D-AE19-4BE1-B017-4122B5EB133F@anu.edu.au>

The Wald statistics that are returned as "z value" can be a very  
rough approximation.  The standard error is radically different, on a  
logarithmic scale, between log(mu) = -20.30 [the best glm() managed  
in approximating -infinity] and log(mu) + log(a) = -0.29.  It is  
actually worse than might appear; the SE=2457.38 is an approximation  
to infinity!  The phenomenon is an extreme version, now with a  
poisson error model, of the Hauck-Donner effect (Modern Applied  
Statistics with S, 4th edn, pp.197-199) that occurs with binomial  
data.  Use the result from the anova likelihood ratio test, where the  
approximations that are involved are usually much better behaved (but  
it would not hurt to do a simulation as a check.)

There is an example of this phenomenon with a poisson error model in  
Subsection 8.4.2 (the same subsection number both for the 1st edn and  
the forthcoming 2nd edn) of Data Analysis & Graphics using R, CUP,  
2003 and 2006. Install and attach the DAAG package and try

example(moths)

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Mathematics & Its Applications, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.

> Dear members,
> here is my trouble: My data consists of counts of trapped insects  
> in different attractive traps. I usually use GLMs with a poisson  
> error distribution to find out the differences between my  
> traitments (and to look at other factor effects). But for some  
> dataset where one traitment contains only zeros, GLM with poisson  
> family fail to find any difference between this particular  
> traitment and anyother one (even with traitment that have trapped a  
> lot of insects). GLMs with gaussian family does not seem to have  
> the same problem but GLMs with binomial family does.
> I'm not sure if it is a statistical problem or if it comes from  
> R... in the latter case I think some solution exists (perhaps in  
> the options of the glm() function ?).
> Thank you for your help.
>
>
> Here I figure out an exemple to past in the console:
>
> ## START  
> ###################################################################### 
> ########
> # Take a data set of counts for two traitments, one containing only  
> zeros
> A=c 
> (0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0 
> ,0,0,0,0,0)
> B=c 
> (1,0,0,0,2,1,0,0,1,2,0,0,0,1,2,2,0,1,1,0,1,0,2,1,1,0,1,2,0,1,0,1,1,1,0 
> ,1,1,1,0,1)
> traitment=c(rep("A",40),rep("B",40))
> response=c(A,B)
> mydata=data.frame(traitment ,response)
>
>
> # Make a GLM on this dataset , with "family=poisson"
>
>  g=glm(response~traitment, data=mydata, family=poisson)
>  anova.glm(g,test="Chisq")
> # There is an effect of the traitment ...
>
>  summary(g)
> # But traitment A does not differ from traitment B ! ! ! (the  
> pvalue is always close from 1 in such cases)
>
> # Now if you replace only one zero of the A reponse to 1, the GLM  
> works properly:
>  mydata[1,2]=1
>  g=glm(response~traitment, data=mydata, family=poisson)
>  anova.glm(g,test="Chisq")
>  summary(g)
> ###################################################################### 
> ###############  END ##
>
>
>
> Antonin Ferry (PhD)
>
> "Laboratoire d'Ecobiologie des Insectes Parasitoides"
> http://www.parasitoides.univ-rennes1.fr
> Universit? de Renes1, FRANCE


From AnupTyagi at yahoo.com  Wed Sep 13 14:01:47 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Wed, 13 Sep 2006 12:01:47 +0000 (UTC)
Subject: [R] R-question
References: <OF8109D7CC.D032CA84-ONC12571E8.0022D409-C12571E8.00389DF6@de.ibm.com>
Message-ID: <loom.20060913T135343-0@post.gmane.org>

Thorsten Muehge <MUEHGE <at> de.ibm.com> writes:

> 1. Is it possible to query R files by SQL internally on data frames (not on
> a database) and how is the syntax (I have the RODBC package installed).

It is possible to do similar things "conceptually" in R as in SQL---at least the
basic SQL queries (I have not tried others). Unlike SQL, R retains the sort
order. So far as I know you can not use SQL code to query R data-frames. But you
can put SQL code in a .R file and use RODBC or ODBC (I have used this) to send
SQL queries to database; you can also get the results from SQL queries back to R
as R objects.


From john.maindonald at anu.edu.au  Wed Sep 13 14:14:30 2006
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Wed, 13 Sep 2006 22:14:30 +1000
Subject: [R] unexpected result in glm (family=poisson) for data with an only
	zero response in one factor
Message-ID: <8D10B5FC-3482-4C23-A8A1-4D9CAAEBE1C9@anu.edu.au>

PS.  In part, the problem is with the use of the log link, arising  
because the limit of log(mu), as mu goes to 0, is minus infinity.   
This is not an appropriate scale on which to represent a fitted value  
that is zero.  The estimated SE for a fitted value of zero should be  
0.  You will get a more sensible answer if you set family=poisson 
(link="sqrt")

g <- glm(response~traitment, data=mydata, family=poisson(link="sqrt"))
 > summary(g)

Call:
glm(formula = response ~ traitment, family = poisson(link = "sqrt"),
     data = mydata)

Deviance Residuals:
        Min          1Q      Median          3Q         Max
-1.225e+00  -2.730e-05  -2.730e-05   2.745e-01   1.193e+00

Coefficients:
              Estimate Std. Error  z value Pr(>|z|)
(Intercept) 0.0000193  0.0790569 0.000244        1
traitmentB  0.8660061  0.1118034    7.746  9.5e-15

(Dispersion parameter for poisson family taken to be 1)

     Null deviance: 75.485  on 79  degrees of freedom
Residual deviance: 33.896  on 78  degrees of freedom
AIC: 89.579

Number of Fisher Scoring iterations: 14


John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Mathematics & Its Applications, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.

> Dear members,
> here is my trouble: My data consists of counts of trapped insects  
> in different attractive traps. I usually use GLMs with a poisson  
> error distribution to find out the differences between my  
> traitments (and to look at other factor effects). But for some  
> dataset where one traitment contains only zeros, GLM with poisson  
> family fail to find any difference between this particular  
> traitment and anyother one (even with traitment that have trapped a  
> lot of insects). GLMs with gaussian family does not seem to have  
> the same problem but GLMs with binomial family does.
> I'm not sure if it is a statistical problem or if it comes from  
> R... in the latter case I think some solution exists (perhaps in  
> the options of the glm() function ?).
> Thank you for your help.
>
>
> Here I figure out an exemple to past in the console:
>
> ## START  
> ###################################################################### 
> ########
> # Take a data set of counts for two traitments, one containing only  
> zeros
> A=c 
> (0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0 
> ,0,0,0,0,0)
> B=c 
> (1,0,0,0,2,1,0,0,1,2,0,0,0,1,2,2,0,1,1,0,1,0,2,1,1,0,1,2,0,1,0,1,1,1,0 
> ,1,1,1,0,1)
> traitment=c(rep("A",40),rep("B",40))
> response=c(A,B)
> mydata=data.frame(traitment ,response)
>
>
> # Make a GLM on this dataset , with "family=poisson"
>
>  g=glm(response~traitment, data=mydata, family=poisson)
>  anova.glm(g,test="Chisq")
> # There is an effect of the traitment ...
>
>  summary(g)
> # But traitment A does not differ from traitment B ! ! ! (the  
> pvalue is always close from 1 in such cases)
>
> # Now if you replace only one zero of the A reponse to 1, the GLM  
> works properly:
>  mydata[1,2]=1
>  g=glm(response~traitment, data=mydata, family=poisson)
>  anova.glm(g,test="Chisq")
>  summary(g)
> ###################################################################### 
> ###############  END ##
>
>
>
> Antonin Ferry (PhD)
>
> "Laboratoire d'Ecobiologie des Insectes Parasitoides"
> http://www.parasitoides.univ-rennes1.fr
> Universit? de Renes1, FRANCE


From AnupTyagi at yahoo.com  Wed Sep 13 14:29:03 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Wed, 13 Sep 2006 12:29:03 +0000 (UTC)
Subject: [R] Reading fixed column format
References: <loom.20060911T172146-100@post.gmane.org><BAY116-DAV571634EB23B6A0F2F6502CF2A0@phx.gbl>	<loom.20060911T185053-310@post.gmane.org>	<BAY116-DAV15C8CB3C190E253F6BD659CF2A0@phx.gbl>	<loom.20060912T084704-214@post.gmane.org>	<3EBEC9D3-559C-4607-838D-46042D36A3AA@virginia.edu>	<45067F0A.3060304@lancaster.ac.uk>	<loom.20060912T123009-247@post.gmane.org>
	<4507E869.7030301@lancaster.ac.uk>
	<4507EC91.9030804@lancaster.ac.uk>
Message-ID: <loom.20060913T142747-931@post.gmane.org>

Barry Rowlingson <B.Rowlingson <at> lancaster.ac.uk> writes:

>   Or if I'd just read Duncan's message about negative widths in read.fwf.
> 
>   Anyway, I've learnt about readLines() and seek() and reading zip files 
> now, so I can read _anything_....

Thanks to everyone who answered my query. I have a lot to think about too.


From rmh at temple.edu  Wed Sep 13 14:36:43 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Wed, 13 Sep 2006 08:36:43 -0400 (EDT)
Subject: [R] wireplot margins and additional z-axis
Message-ID: <20060913083643.BHV57216@po-d.temple.edu>

Klaus Nordhausen:
> The plot is still not in the left bottom corner of the file. There is a lot of space below the
> outer box line. If I include this eps in latex it will also include this space and if I put for 
> example the figure caption below it I have this huge gap between actual graph and caption. 

This is not a problem in LaTeX.  You can put some negative vertical space between the
figure and the caption.  You can also put horizontal space in the figure to move the
useful part of the eps file where you want it.  Here is a trivial example.


tmp.R:
par(mfrow=c(2,2))
plot(1:10)
dev.copy2eps(file="tmp.eps")


tmp.tex:
\documentclass{article}
\usepackage{graphicx}
\begin{document}

\begin{figure}
\includegraphics{tmp.eps}
\caption{Default position.}
\end{figure}

\begin{figure}
\hspace*{.5in}
\includegraphics{tmp.eps}
\vspace*{-3.8in}
\caption{Controlled position.}
\end{figure}

\end{document}


From dhajage at gmail.com  Wed Sep 13 14:52:43 2006
From: dhajage at gmail.com (David Hajage)
Date: Wed, 13 Sep 2006 14:52:43 +0200
Subject: [R] install R on Sun Blade 2000
Message-ID: <a725cda30609130552g4f3afeb9ha2eea06ac45f3f24@mail.gmail.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20060913/9129132b/attachment.pl 

From murdoch at stats.uwo.ca  Wed Sep 13 15:02:03 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 13 Sep 2006 09:02:03 -0400
Subject: [R] wireplot margins and additional z-axis
In-Reply-To: <20060913080401.146390@gmx.net>
References: <20060911194003.283820@gmx.net>		<eb555e660609111315h220befb0kccbd92ce5bdca390@mail.gmail.com>		<20060912072337.19620@gmx.net>	<eb555e660609121641p54fb959bm56ecaa1851cd6ca9@mail.gmail.com>
	<20060913080401.146390@gmx.net>
Message-ID: <4508014B.7070501@stats.uwo.ca>

On 9/13/2006 4:04 AM, Klaus Nordhausen wrote:
> Dear Deepayan,
> 
> sorry for not being clear - but my problem has nothing to do with the aspect. If I create the eps the following way

There is some ambiguity here.  The "aspect" arg to wireframe controls 
the 3D aspect ratio.  You want to control the 2D aspect ratio in the 
displayed plot.  It is stored in the aspect.ratio member of the 
resulting object, and can be changed using

plot.vol$aspect.ratio <- 1.5

for example.  You shouldn't really be fiddling around inside the object 
like that, but I don't know how to avoid it in this case.

Duncan Murdoch

> 
> library(lattice)
> plot.vol<- wireframe(volcano,
>                aspect = c(1,1.5), scales=list(arrows=F),zlab=list("Z-axis",rot=90))
> 
> postscript("example_plot_3.eps", width = 14.0/2.54, height = 19.0/2.54,
>                 horizontal = FALSE, onefile = FALSE,paper="special")
> trellis.par.set("axis.line",list(alpha=1,col=1,lty=1,lwd=1))
> print(plot.vol)
> dev.off() 
> 
> The plot is still not in the left bottom corner of the file. There is a lot of space below the outer box line. If I include this eps in latex it will also include this space and if I put for example the figure caption below it I have this huge gap between actual graph and caption.
> 
> And for comparison, if I create with xyplot an eps like
> 
> postscript("example_plot_4.eps", width = 14.0/2.54, height = 19.0/2.54,
>                 horizontal = FALSE, onefile = FALSE,paper="special")
> 
>     Depth <- equal.count(quakes$depth, number=8, overlap=.1)
>      plot.depth<-xyplot(lat ~ long | Depth, data = quakes)
>      update(trellis.last.object(),
>             strip = strip.custom(strip.names = TRUE, strip.levels = TRUE),
>             par.strip.text = list(cex = 0.75),
>             aspect = "iso")
> 
> print(plot.depth)
> dev.off()
> 
> the figure is really in the left bottom corner and included in latex has not that gap between caption and actual figure.
> 
> I hope this describes my problem better.
> 
> Klaus
> 
> 
> 
>> On 9/12/06, Klaus Nordhausen <klausch at gmx.de> wrote:
>> > Dear Deepayan,
>> >
>> > thanks for your reply, the change of the aspect does however not solve
>> my problem with the space below the graph on the .eps
>> > I attached the .eps (still with the old aspect) so that it is maybe
>> clearer what my
>> > problem is.
>> 
>> No, it's not clearer; this is basically the same EPS that I got, so it
>> gives me no new information. What do you get with the new aspect? If
>> it's not what you want, you'll have to explain what you want more
>> clearly. Also, don't make the panel borders transparent, as it makes
>> it difficult to understand what's going on.
>> 
>> Deepayan
>> 
>> > Any other suggestions?
>> >
>> > Klaus
>> >
>> >
>> > > > Dear R experts,
>> > > >
>> > > > it would be very kind if you could help me with two wireplot
>> problems.
>> > > >
>> > > > First, when I make a wireplot and transform it into an .eps using
>> the
>> > > postscript function the eps-file leaves always a lot of space below
>> the plot,
>> > > as if it would leave space for a legend or something like that.
>> > > > How can i get the plot into the bottom corner without the space
>> below?
>> > > The space is not there when I just display the plot in R on my screen
>> (I use
>> > > R.2.3.1 on Windows XP). Or in general, how can I get the margins on
>> all
>> > > sides as small as possible since I wnat to include the eps into a
>> report and
>> > > do not need the space around.
>> > > >
>> > > > The following code has the space on the eps:
>> > > >
>> > > > library(lattice)
>> > > >  plot.vol <- wireframe(volcano, aspect = 1, scales=list(arrows=F)
>> > > ,zlab=list("Z-axis",rot=90))
>> > > >
>> > >
>> > > Perhaps you want something like
>> > >
>> > > aspect = c(1, 1.5)
>> > >
>> > > instead.
>> > >
>> > > > postscript("example_plot.eps", width = 14.0/2.54, height =
>> 19.0/2.54,
>> > > >                 horizontal = FALSE, onefile = FALSE,paper="special")
>> > > >
>> > > > trellis.par.set("axis.line",list(alpha=1,col=1,lty=0,lwd=1))
>> > > >
>> > > > print(plot.vol)
>> > > >
>> > > > dev.off()
>> > > >
>> > > >
>> > > > Secondly, is it possible to add to the wireplot a further z-axis. I
>> > > found only how to choose at which veritcal line I want the tickmarks
>> and label,
>> > > but is it also possible to have it at two vertical lines?
>> > > >
>> > >
>> > > No (but it shouldn't be too hard to add that feature; I'll have to
>> check).
>> > >
>> > > Deepayan
>> > >
>


From as at hut.at  Wed Sep 13 15:24:47 2006
From: as at hut.at (sun)
Date: Wed, 13 Sep 2006 15:24:47 +0200
Subject: [R] input data format of multinom in nnet
References: <5.1.0.14.0.20060910205402.00c10ba8@pop3.brisnet.org.au>
Message-ID: <006401c6d737$fc434d00$04879b83@campus.tue.nl>

Hi,

  I 'd like to use multinom to estimate  a multinomial logit model of a 
conjoint survey with a format like:

individual 1 ------choice ------ X1-----X2-----X3
1-----------------A------------1.2----blue----12
1-----------------0-------------1.4----red-----13
1-----------------B------------3.1----green---17
1-----------------0-------------4.2----red-----14
......
2----------------C------------2.1-----blue----11

.......
0 means this alternative did not be chosen. choice set size is 2 for each 
choice.

As I know at this moment, multinom deal with count data format (iris data). 
Anybody knows if multinom can take this format as input?

Thanks.


From karloh at mi.uib.no  Wed Sep 13 15:34:44 2006
From: karloh at mi.uib.no (Karl Ove Hufthammer)
Date: Wed, 13 Sep 2006 15:34:44 +0200
Subject: [R] lattice cloud and conditional axis limits
References: <ee699l$h1b$1@sea.gmane.org>
	<eb555e660609121634n42a3f997h46a32b53f30af84c@mail.gmail.com>
Message-ID: <ee91dk$8dg$1@sea.gmane.org>

Deepayan Sarkar skreiv:

>> cloud(z~x*y|s,scales=list(arrows=FALSE,relation="free"))
>>
>> However, it does not. Any ideas how I can make it work?
> 
> There's no direct support, but you can write a small panel function
> with more or less the desired effect:

Thank you so much. It works perfectly.

-- 
Karl Ove Hufthammer
E-mail and Jabber: karl at huftis.org


From ripley at stats.ox.ac.uk  Wed Sep 13 15:57:25 2006
From: ripley at stats.ox.ac.uk (Brian Ripley)
Date: Wed, 13 Sep 2006 14:57:25 +0100 (BST)
Subject: [R] install R on Sun Blade 2000
In-Reply-To: <a725cda30609130552g4f3afeb9ha2eea06ac45f3f24@mail.gmail.com>
References: <a725cda30609130552g4f3afeb9ha2eea06ac45f3f24@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0609131456330.3195@auk.stats>

Without knowing your OS, it is hard to say.  But if this is Solaris (the 
most likely OS), not to my knowledge.

On Wed, 13 Sep 2006, David Hajage wrote:

> Hello useRs,
>
> I would like to install R 2.3.1 on a computer : Sun Blade 2000.
>
> Is there a precompiled version of R for this computer ?
>
> Thank you very much.
>


From bolker at zoo.ufl.edu  Wed Sep 13 16:04:21 2006
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Wed, 13 Sep 2006 14:04:21 +0000 (UTC)
Subject: [R] rgl: exporting to pdf or png does not work
References: <200609122138.51645.franke.daniel@gmail.com>
Message-ID: <loom.20060913T160119-951@post.gmane.org>

Daniel Franke <franke.daniel <at> gmail.com> writes:

> 
> 
> [This is a follow-up to the recent discussion on R-help]
> 
> Hi all,
> 
> I can reproduce both problems on gentoo (2006.0 profile), 
> but not on OpenSuSE-10.1. 
> 
> Installed libraries:
>  * Gentoo: R-2.2.1, rgl-0.67.2, OpenSuSE: R-2.3.1, rgl-0.67.2
>  * Gentoo: libpng-1.2.12, OpenSuSE: libpng-1.2.8
>  * Gentoo: Xorg-6.8.2 (mga), OpenSuSE: 6.9.0 (nvidia)
>  * Gentoo: freeglut-2.4.0, OpenSuSE:  freeglut-051110
>  * OpenSuse: mesa-6.4.2
> 

  Just a quick question: if you use an example with
text in it, does the text show up in the output on
the version that works (OpenSuSE)?  Someone else reported
no crash, but no text output either.

   Otherwise I can't help.  For what it's worth,
I discovered that in the Ubuntu (and I assume any
Gnome) desktop, Alt-PrintScreen does screen capture
on a single window, which is a good workaround 
for rgl.snapshot() [although it would still be nice
to have rgl.postscript() working ...]

  Ben Bolker


From elagin at wias-berlin.de  Wed Sep 13 16:15:57 2006
From: elagin at wias-berlin.de (Mstislav Elagin)
Date: Wed, 13 Sep 2006 16:15:57 +0200
Subject: [R] printing a generated function crashes R
Message-ID: <4508129D.4040303@wias-berlin.de>

Dear All,

the last expression in the following code snippet crashes R (version 
2.3.1 on Windows XP) when run interactively:

make.bad.function <- function(kind)
{
   zz <- switch(kind,
                "1" = 1,
                "2" = 2)

   stopifnot( !is.null(zz) )

   eval( bquote( function(x)
                {
                  x + .(zz)
                }))
}

# bad.function <- make.bad.function("5") ## error as expected

bad.function <- make.bad.function("1")
print(bad.function(10)) ## -> 11

bad.function <- make.bad.function("2")
print(bad.function(10)) ## -> 12

bad.function            ## this works if the code is source()'d
print(bad.function)     ## oops!

However, it does work (i.e. prints the body of bad.function) if run 
non-interactively
(R --vanilla < bad-function.R).

Any ideas why this happens?

Thanks in advance

Mstislav Elagin


From ggrothendieck at gmail.com  Wed Sep 13 16:21:16 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 13 Sep 2006 10:21:16 -0400
Subject: [R] Reading fixed column format
In-Reply-To: <loom.20060913T083003-590@post.gmane.org>
References: <loom.20060911T172146-100@post.gmane.org>
	<BAY116-DAV571634EB23B6A0F2F6502CF2A0@phx.gbl>
	<loom.20060911T185053-310@post.gmane.org>
	<BAY116-DAV15C8CB3C190E253F6BD659CF2A0@phx.gbl>
	<loom.20060912T084704-214@post.gmane.org>
	<3EBEC9D3-559C-4607-838D-46042D36A3AA@virginia.edu>
	<45067F0A.3060304@lancaster.ac.uk>
	<loom.20060913T073708-542@post.gmane.org>
	<971536df0609122301l502d10dch8af36d720ab801c9@mail.gmail.com>
	<loom.20060913T083003-590@post.gmane.org>
Message-ID: <971536df0609130721i27475468icd5a21fa1f6d77d4@mail.gmail.com>

On 9/13/06, Anupam Tyagi <AnupTyagi at yahoo.com> wrote:
> Gabor Grothendieck <ggrothendieck <at> gmail.com> writes:
>
> > C:\bin>cut -c2-3,6-8 a.dat
> > 23678
> > 23678
> > 23678
>
> Thanks. I think this will work. How do I redirect the output to a file on
> windows?

Same as on UNIX

cut -c2-3,6-8 a.dat > a2.dat

> Is there simple way to convert the cut command to a script on windows,

Using notepad or other text editor put it in file a.bat and then
issue this command from the console

a.bat

Note that you could process it multiple time if you like:

cut -c6-8 a.dat > a2.dat
cut -c2-3 a2.dat > a3.dat

produces the same thing but uses 2 passes and so keeps each line shorter.
Be sure you do it from the tail end forward as shown above to avoid having
to recalculate the positions.

> because the entire command may not fit on one line? Anupam.
>


From Dietrich.Trenkler at uni-osnabrueck.de  Wed Sep 13 16:44:50 2006
From: Dietrich.Trenkler at uni-osnabrueck.de (Dietrich Trenkler)
Date: Wed, 13 Sep 2006 16:44:50 +0200
Subject: [R] S in cor.test(..., method="spearman")
Message-ID: <45081962.5070305@uni-osnabrueck.de>

Dear HelpeRs,

I have some data:

"ice" <- structure(c(0.386, 0.374, 0.393, 0.425, 0.406, 0.344,
    0.327, 0.288, 0.269, 0.256, 0.286, 0.298, 0.329, 0.318, 0.381,
    0.381, 0.47, 0.443, 0.386, 0.342, 0.319, 0.307, 0.284, 0.326,
    0.309, 0.359, 0.376, 0.416, 0.437, 0.548, 41, 56, 63, 68,
    69, 65, 61, 47, 32, 24, 28, 26, 32, 40, 55, 63, 72, 72, 67,
    60, 44, 40, 32, 27, 28, 33, 41, 52, 64, 71), .Dim = as.integer(c(30,
    2)))

Using

    cor.test(ice[,1],ice[,2],method="spearman")

I get (apart from a warning message due to ties)

        Spearman's rank correlation rho

data:  ice[, 1] and ice[, 2]
S = 769.4403, p-value = 1.543e-08
alternative hypothesis: true rho is not equal to 0
sample estimates:
     rho
0.828823

I wonder what S is. I presume it is

sum((rank(ice[,1])-rank(ice[,2]))^2),

but this delivers  768.5. Is it the way ranks are computed in cor.test?


Thank you in advance.

D. Trenkler                              

-- 
Dietrich Trenkler c/o Universitaet Osnabrueck 
Rolandstr. 8; D-49069 Osnabrueck, Germany    
email: Dietrich.Trenkler at Uni-Osnabrueck.de


From liuwensui at gmail.com  Wed Sep 13 17:22:49 2006
From: liuwensui at gmail.com (Wensui Liu)
Date: Wed, 13 Sep 2006 11:22:49 -0400
Subject: [R] R-question
In-Reply-To: <OF8109D7CC.D032CA84-ONC12571E8.0022D409-C12571E8.00389DF6@de.ibm.com>
References: <OF8109D7CC.D032CA84-ONC12571E8.0022D409-C12571E8.00389DF6@de.ibm.com>
Message-ID: <1115a2b00609130822i2193057g943f3c4f4e180294@mail.gmail.com>

For your 1st question, you can write query against the tables in DB using RODBC.

Being a SAS programmer, I have to say that reporting function of R is
not as good as that of SAS.



On 9/13/06, Thorsten Muehge <MUEHGE at de.ibm.com> wrote:
>
>
> Hello Colleagues,
> I programmed in SAS for 3 years and would like to switch to a not so costly
> software product.
>
> Hence I started to evaluate R, and my first test look promising.
>
> However I have some question:
>
> 1. Is it possible to query R files by SQL internally on data frames (not on
> a database) and how is the syntax (I have the RODBC package installed).
>
> I would like to extract year, Quarter, week, from a date column in a data
> frame (see attachment). After this I want to attach the column to the
> original data frame.
>
> How do I do this in R?
>
> Dr .Th.M?hge,
>
> PMP(r)
> Procurement Technology Center
> IBM Deutschland GmbH, Hechtsheimer Str.2, D-55131 Mainz
> Phone: xx49-(0)6131-84-2416
> Mobile: xx49-(0)15117457978
> e-mail: muehge at de.ibm.com
> (See attached file: Debug1.csv)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


-- 
WenSui Liu
(http://spaces.msn.com/statcompute/blog)
Senior Decision Support Analyst
Health Policy and Clinical Effectiveness
Cincinnati Children Hospital Medical Center


From sundar.dorai-raj at pdf.com  Wed Sep 13 17:31:12 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 13 Sep 2006 10:31:12 -0500
Subject: [R] S in cor.test(..., method="spearman")
In-Reply-To: <45081962.5070305@uni-osnabrueck.de>
References: <45081962.5070305@uni-osnabrueck.de>
Message-ID: <45082440.4040708@pdf.com>



Dietrich Trenkler said the following on 9/13/2006 9:44 AM:
> Dear HelpeRs,
> 
> I have some data:
> 
> "ice" <- structure(c(0.386, 0.374, 0.393, 0.425, 0.406, 0.344,
>     0.327, 0.288, 0.269, 0.256, 0.286, 0.298, 0.329, 0.318, 0.381,
>     0.381, 0.47, 0.443, 0.386, 0.342, 0.319, 0.307, 0.284, 0.326,
>     0.309, 0.359, 0.376, 0.416, 0.437, 0.548, 41, 56, 63, 68,
>     69, 65, 61, 47, 32, 24, 28, 26, 32, 40, 55, 63, 72, 72, 67,
>     60, 44, 40, 32, 27, 28, 33, 41, 52, 64, 71), .Dim = as.integer(c(30,
>     2)))
> 
> Using
> 
>     cor.test(ice[,1],ice[,2],method="spearman")
> 
> I get (apart from a warning message due to ties)
> 
>         Spearman's rank correlation rho
> 
> data:  ice[, 1] and ice[, 2]
> S = 769.4403, p-value = 1.543e-08
> alternative hypothesis: true rho is not equal to 0
> sample estimates:
>      rho
> 0.828823
> 
> I wonder what S is. I presume it is
> 
> sum((rank(ice[,1])-rank(ice[,2]))^2),
> 
> but this delivers  768.5. Is it the way ranks are computed in cor.test?
> 
> 
> Thank you in advance.
> 
> D. Trenkler                              
> 

Looking at the code will help. Try

stats:::cor.test.default

This reveals that S is determined by:

x <- ice[, 1]
y <- ice[, 2]
n <- nrow(ice)
r <- cor(rank(x), rank(y))
S <- (n^3 - n) * (1 - r)/6
S
## [1] 769.4403

See ?cor.test as to definition of "S".

HTH,

--sundar


From Greg.Snow at intermountainmail.org  Wed Sep 13 17:32:48 2006
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Wed, 13 Sep 2006 09:32:48 -0600
Subject: [R] Gnuplot epslatex format also in R?
Message-ID: <07E228A5BE53C24CAD490193A7381BBB591C83@LP-EXCHVS07.CO.IHC.COM>

There is a Java based implementation called jfig at:
http://tams-www.informatik.uni-hamburg.de/applets/jfig/ that works on
windows.

Hope this helps,


-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Anupam Tyagi
Sent: Tuesday, September 12, 2006 5:34 AM
To: r-help at stat.math.ethz.ch
Subject: Re: [R] Gnuplot epslatex format also in R?

Prof Brian Ripley <ripley <at> stats.ox.ac.uk> writes:

> 
> R has an xfig driver, and AFAIK you can do this from xfig.
> 

Is there an xfig port for Windows, without cygwin? If so, I will be
thankful for a pointer to the where it can be downloaded from. I have
been looking for it for some time. Anupam.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From f.harrell at vanderbilt.edu  Wed Sep 13 17:49:01 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 13 Sep 2006 10:49:01 -0500
Subject: [R] R-question
In-Reply-To: <1115a2b00609130822i2193057g943f3c4f4e180294@mail.gmail.com>
References: <OF8109D7CC.D032CA84-ONC12571E8.0022D409-C12571E8.00389DF6@de.ibm.com>
	<1115a2b00609130822i2193057g943f3c4f4e180294@mail.gmail.com>
Message-ID: <4508286D.1090305@vanderbilt.edu>

Wensui Liu wrote:
> For your 1st question, you can write query against the tables in DB using RODBC.
> 
> Being a SAS programmer, I have to say that reporting function of R is
> not as good as that of SAS.

I beg to differ.  See for example 
http://biostat.mc.vanderbilt.edu/StatReport

Frank Harrell
> 
> 
> 
> On 9/13/06, Thorsten Muehge <MUEHGE at de.ibm.com> wrote:
>>
>> Hello Colleagues,
>> I programmed in SAS for 3 years and would like to switch to a not so costly
>> software product.
>>
>> Hence I started to evaluate R, and my first test look promising.
>>
>> However I have some question:
>>
>> 1. Is it possible to query R files by SQL internally on data frames (not on
>> a database) and how is the syntax (I have the RODBC package installed).
>>
>> I would like to extract year, Quarter, week, from a date column in a data
>> frame (see attachment). After this I want to attach the column to the
>> original data frame.
>>
>> How do I do this in R?
>>
>> Dr .Th.M?hge,
>>
>> PMP(r)
>> Procurement Technology Center
>> IBM Deutschland GmbH, Hechtsheimer Str.2, D-55131 Mainz
>> Phone: xx49-(0)6131-84-2416
>> Mobile: xx49-(0)15117457978
>> e-mail: muehge at de.ibm.com
>> (See attached file: Debug1.csv)
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>
> 
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From liuwensui at gmail.com  Wed Sep 13 17:56:18 2006
From: liuwensui at gmail.com (Wensui Liu)
Date: Wed, 13 Sep 2006 11:56:18 -0400
Subject: [R] R-question
In-Reply-To: <4508286D.1090305@vanderbilt.edu>
References: <OF8109D7CC.D032CA84-ONC12571E8.0022D409-C12571E8.00389DF6@de.ibm.com>
	<1115a2b00609130822i2193057g943f3c4f4e180294@mail.gmail.com>
	<4508286D.1090305@vanderbilt.edu>
Message-ID: <1115a2b00609130856g4e5899b6q3f18916d83732d2a@mail.gmail.com>

well, Harrell,

I understand sweave or R2html could be a solution.

but please  show me their applications in a large business setting. On
the contrary, I can give you many such cases using SAS.

On 9/13/06, Frank E Harrell Jr <f.harrell at vanderbilt.edu> wrote:
> Wensui Liu wrote:
> > For your 1st question, you can write query against the tables in DB using RODBC.
> >
> > Being a SAS programmer, I have to say that reporting function of R is
> > not as good as that of SAS.
>
> I beg to differ.  See for example
> http://biostat.mc.vanderbilt.edu/StatReport
>
> Frank Harrell
> >
> >
> >
> > On 9/13/06, Thorsten Muehge <MUEHGE at de.ibm.com> wrote:
> >>
> >> Hello Colleagues,
> >> I programmed in SAS for 3 years and would like to switch to a not so costly
> >> software product.
> >>
> >> Hence I started to evaluate R, and my first test look promising.
> >>
> >> However I have some question:
> >>
> >> 1. Is it possible to query R files by SQL internally on data frames (not on
> >> a database) and how is the syntax (I have the RODBC package installed).
> >>
> >> I would like to extract year, Quarter, week, from a date column in a data
> >> frame (see attachment). After this I want to attach the column to the
> >> original data frame.
> >>
> >> How do I do this in R?
> >>
> >> Dr .Th.M?hge,
> >>
> >> PMP(r)
> >> Procurement Technology Center
> >> IBM Deutschland GmbH, Hechtsheimer Str.2, D-55131 Mainz
> >> Phone: xx49-(0)6131-84-2416
> >> Mobile: xx49-(0)15117457978
> >> e-mail: muehge at de.ibm.com
> >> (See attached file: Debug1.csv)
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >>
> >>
> >
> >
>
>
> --
> Frank E Harrell Jr   Professor and Chair           School of Medicine
>                       Department of Biostatistics   Vanderbilt University
>


-- 
WenSui Liu
(http://spaces.msn.com/statcompute/blog)
Senior Decision Support Analyst
Health Policy and Clinical Effectiveness
Cincinnati Children Hospital Medical Center


From p.dalgaard at biostat.ku.dk  Wed Sep 13 18:03:45 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 13 Sep 2006 18:03:45 +0200
Subject: [R] printing a generated function crashes R
In-Reply-To: <4508129D.4040303@wias-berlin.de>
References: <4508129D.4040303@wias-berlin.de>
Message-ID: <x2u03brbxa.fsf@viggo.kubism.ku.dk>

Mstislav Elagin <elagin at wias-berlin.de> writes:

> Dear All,
> 
> the last expression in the following code snippet crashes R (version 
> 2.3.1 on Windows XP) when run interactively:
> 
> make.bad.function <- function(kind)
> {
>    zz <- switch(kind,
>                 "1" = 1,
>                 "2" = 2)
> 
>    stopifnot( !is.null(zz) )
> 
>    eval( bquote( function(x)
>                 {
>                   x + .(zz)
>                 }))
> }
> 
> # bad.function <- make.bad.function("5") ## error as expected
> 
> bad.function <- make.bad.function("1")
> print(bad.function(10)) ## -> 11
> 
> bad.function <- make.bad.function("2")
> print(bad.function(10)) ## -> 12
> 
> bad.function            ## this works if the code is source()'d
> print(bad.function)     ## oops!
> 
> However, it does work (i.e. prints the body of bad.function) if run 
> non-interactively
> (R --vanilla < bad-function.R).
> 
> Any ideas why this happens?

Well, bquote seems to be doing nasty things if passed an expression with a
function inside:

> f <- bquote(function(x) {
+     x + 1
+ }
+ )
> f
function(x) {
    x + 1
}
> eval(f)
?
?H~

?H~

Program received signal SIGSEGV, Segmentation fault.


I think the story is that the source attribute is getting messed up.

> z <- eval(f)
> attr(z,"source")
"function(x) {"("x+1}")
> z
?X~
?X~
..poof..

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From f.harrell at vanderbilt.edu  Wed Sep 13 18:06:52 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 13 Sep 2006 11:06:52 -0500
Subject: [R] R-question
In-Reply-To: <1115a2b00609130856g4e5899b6q3f18916d83732d2a@mail.gmail.com>
References: <OF8109D7CC.D032CA84-ONC12571E8.0022D409-C12571E8.00389DF6@de.ibm.com>	
	<1115a2b00609130822i2193057g943f3c4f4e180294@mail.gmail.com>	
	<4508286D.1090305@vanderbilt.edu>
	<1115a2b00609130856g4e5899b6q3f18916d83732d2a@mail.gmail.com>
Message-ID: <45082C9C.1050801@vanderbilt.edu>

Wensui Liu wrote:
> well, Harrell,
> 
> I understand sweave or R2html could be a solution.
> 
> but please  show me their applications in a large business setting. On
> the contrary, I can give you many such cases using SAS.

SAS requires much more coding than R/S-Plus to produce reports that are 
not nearly as beautiful or informative.

Please define 'large business setting'.  R is being used routinely in 
many large businesses.

Frank

> 
> On 9/13/06, Frank E Harrell Jr <f.harrell at vanderbilt.edu> wrote:
>> Wensui Liu wrote:
>> > For your 1st question, you can write query against the tables in DB 
>> using RODBC.
>> >
>> > Being a SAS programmer, I have to say that reporting function of R is
>> > not as good as that of SAS.
>>
>> I beg to differ.  See for example
>> http://biostat.mc.vanderbilt.edu/StatReport
>>
>> Frank Harrell
>> >
>> >
>> >
>> > On 9/13/06, Thorsten Muehge <MUEHGE at de.ibm.com> wrote:
>> >>
>> >> Hello Colleagues,
>> >> I programmed in SAS for 3 years and would like to switch to a not 
>> so costly
>> >> software product.
>> >>
>> >> Hence I started to evaluate R, and my first test look promising.
>> >>
>> >> However I have some question:
>> >>
>> >> 1. Is it possible to query R files by SQL internally on data frames 
>> (not on
>> >> a database) and how is the syntax (I have the RODBC package 
>> installed).
>> >>
>> >> I would like to extract year, Quarter, week, from a date column in 
>> a data
>> >> frame (see attachment). After this I want to attach the column to the
>> >> original data frame.
>> >>
>> >> How do I do this in R?
>> >>
>> >> Dr .Th.M?hge,
>> >>
>> >> PMP(r)
>> >> Procurement Technology Center
>> >> IBM Deutschland GmbH, Hechtsheimer Str.2, D-55131 Mainz
>> >> Phone: xx49-(0)6131-84-2416
>> >> Mobile: xx49-(0)15117457978
>> >> e-mail: muehge at de.ibm.com
>> >> (See attached file: Debug1.csv)

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From Greg.Snow at intermountainmail.org  Wed Sep 13 18:11:08 2006
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Wed, 13 Sep 2006 10:11:08 -0600
Subject: [R] Conservative "ANOVA tables" in lmer
Message-ID: <07E228A5BE53C24CAD490193A7381BBB591C97@LP-EXCHVS07.CO.IHC.COM>

[snip]

 Douglas Bates wrote:
> Hmm - I'm not sure what confidence interval and what number of levels
> you mean there so I can't comment on that method.
> 
> Suppose we go back to Spencer's example and consider if there is a
> signficant effect for the Nozzle factor.  That is equivalent to the
> hypothesis H_0: beta_2 = beta_3 = 0 versus the general alternative.  A
> "p-value" could be formulated from an MCMC sample if we assume that
> the marginal distribution of the parameter estimates for beta_2 and
> beta_3 has roughly elliptical contours and you can evaluate that by,
> say, examining a hexbin plot of the values in the MCMC sample. One
> could take the ellipses as defined by the standard errors and
> estimated correlation or, probably better, by the observed standard
> deviations and correlations in the MCMC sample.  Then determine the
> proportion of (beta_2, beta_3) pairs in the sample that fall outside
> the ellipse centered at the estimates and with that eccentricity and
> scaling factors that passes through (0,0).  That would be an empirical
> p-value for the test.
> 
> I would recommend calculating this for a couple of samples to check on
> the reproducibility.

Here is another thought for an empirical p-value that may be easier to
compute and would require fewer assumptions:

Take the proportion of MCMC samples that fall into each quadrant (++,
+-, -+, --) and use the smallest of these proportions as the p-value
(or the smallest out of a subset of the quadrants for a one-sided
style test).

Think of it this way, if the smallest proportion is greater than
alpha, then any closed curve (ellipse, polygon, even concave polygons)
that includes 1-alpha proportion of the points would need to include
points from all 4 quadrants and therefore any convex curve would have
to include (0,0) which is consistent with the null hypothesis.

On the other hand if there is a quadrant that contains fewer than
alpha percent of the points then there exists at least one confidence
region (possibly concave) that contains 1-alpha proportion of the
points and excludes (0,0) and that entire quadrant, which is
consistent with the alternative that at least one of the betas differs
from 0.

A more conservative p-value would be to take the minimum proportion
and muliply it by 4 (or 2^p for p simultaneous tests) which is the
same idea as multipying by 2 for a 2 sided univariate test and assumes
that the confidence regions would exclude similar proportions of
points in each  direction (central confidence regions rather than
minimum length or other confidence regions).  This seems to me that it
would be over conservative in some cases (since all the proportions
must sum to 1, we don't really have 4 degrees of freedom and a smaller
adjustment factor may still be correct and less conservative).

Some simulations would be a good idea to see if the plain minimum is
to liberal and how conservative the other approach is for common
situations.

This is just my first thoughts on the matter, I have not tested
anything, so any comments or other discussion of this idea is welcome.


-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111


From AnupTyagi at yahoo.com  Wed Sep 13 18:21:23 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Wed, 13 Sep 2006 16:21:23 +0000 (UTC)
Subject: [R] Gnuplot epslatex format also in R?
References: <07E228A5BE53C24CAD490193A7381BBB591C83@LP-EXCHVS07.CO.IHC.COM>
Message-ID: <loom.20060913T181922-998@post.gmane.org>

Greg Snow <Greg.Snow <at> intermountainmail.org> writes:

> 
> There is a Java based implementation called jfig at:
> http://tams-www.informatik.uni-hamburg.de/applets/jfig/ that works on
> windows.
> 
> Hope this helps,
> 

Thanks. Is there also a port of "xv"? It can be useful for some graphical
output. I tried Gimp on windows it did not do what I thought "xv" would have
done fine. Anupam.


From emmanuel.levy at gmail.com  Wed Sep 13 18:38:17 2006
From: emmanuel.levy at gmail.com (Emmanuel Levy)
Date: Wed, 13 Sep 2006 17:38:17 +0100
Subject: [R] group bunch of lines in a data.frame, an additional requirement
Message-ID: <e4654710609130938p313dbe57g19c7e36c4ae51488@mail.gmail.com>

Thanks for pointing me out "aggregate", that works fine!

There is one complication though: I have mixed types (numerical and character),

So the matrix is of the form:

A 1.0 200 ID1
A 3.0 800 ID1
A 2.0 200 ID1
B 0.5 20   ID2
B 0.9 50   ID2
C 5.0 70   ID1

One letter always has the same ID but one ID can be shared by many
letters (like ID1)

I just want to keep track of the ID, and get a matrix like:

A 2.0 400 ID1
B 0.7 35 ID2
C 5.0 70 ID1

Any idea on how to do that without a loop?

  Many thanks,

     Emmanuel

On 9/12/06, Emmanuel Levy <emmanuel.levy at gmail.com> wrote:
> Hello,
>
> I'd like to group the lines of a matrix so that:
> A 1.0 200
> A 3.0 800
> A 2.0 200
> B 0.5 20
> B 0.9 50
> C 5.0 70
>
> Would give:
> A 2.0 400
> B 0.7 35
> C 5.0 70
>
> So all lines corresponding to a letter (level), become a single line
> where all the values of each column are averaged.
>
> I've done that with a loop but it doesn't sound right (it is very
> slow). I imagine there is a
> sort of "apply" shortcut but I can't figure it out.
>
> Please note that it is not exactly a matrix I'm using, the function
> "typeof" tells me it's a list, however I access to it like it was a
> matrix.
>
> Could someone help me with the right function to use, a help topic or
> a piece of code?
>
> Thanks,
>
>   Emmanuel
>


From Greg.Snow at intermountainmail.org  Wed Sep 13 18:52:00 2006
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Wed, 13 Sep 2006 10:52:00 -0600
Subject: [R] R-question
Message-ID: <07E228A5BE53C24CAD490193A7381BBB591CAD@LP-EXCHVS07.CO.IHC.COM>

I don't believe that doing a direct SQL query on a native R object is currently possible, others have pointed out ways to do some of the things you would want SQL for using built-in R commands.

If you really want to use SQL you could transfer the data frames you want to use to database tables, then query those and return the result.  You may want to look at the RSQLite and SQLiteDF packages that would help with these steps without requiring any database setup outside of R.

It probably not be too much work to write a function that would take an SQL query as a string and a list of data frames as arguments, copy the data frames to SQLite tables (SQLiteDF function sql.data.frame does this), then submit the query on those data frames (using RSQLite package) and return the result.

Hope this helps, 


-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Thorsten Muehge
Sent: Wednesday, September 13, 2006 4:18 AM
To: r-help at stat.math.ethz.ch
Subject: [R] R-question



Hello Colleagues,
I programmed in SAS for 3 years and would like to switch to a not so costly software product.

Hence I started to evaluate R, and my first test look promising.

However I have some question:

1. Is it possible to query R files by SQL internally on data frames (not on a database) and how is the syntax (I have the RODBC package installed).

I would like to extract year, Quarter, week, from a date column in a data frame (see attachment). After this I want to attach the column to the original data frame.

How do I do this in R?

Dr .Th.M?hge,

PMP?
Procurement Technology Center
IBM Deutschland GmbH, Hechtsheimer Str.2, D-55131 Mainz
Phone: xx49-(0)6131-84-2416
Mobile: xx49-(0)15117457978
e-mail: muehge at de.ibm.com
(See attached file: Debug1.csv)


From jasoncbarnhart at msn.com  Wed Sep 13 18:52:18 2006
From: jasoncbarnhart at msn.com (Jason Barnhart)
Date: Wed, 13 Sep 2006 09:52:18 -0700
Subject: [R] Reading fixed column format
References: <loom.20060911T172146-100@post.gmane.org><BAY116-DAV571634EB23B6A0F2F6502CF2A0@phx.gbl><loom.20060911T185053-310@post.gmane.org><BAY116-DAV15C8CB3C190E253F6BD659CF2A0@phx.gbl><loom.20060912T084704-214@post.gmane.org><3EBEC9D3-559C-4607-838D-46042D36A3AA@virginia.edu><45067F0A.3060304@lancaster.ac.uk><loom.20060913T073708-542@post.gmane.org><971536df0609122301l502d10dch8af36d720ab801c9@mail.gmail.com><loom.20060913T083003-590@post.gmane.org>
	<971536df0609130721i27475468icd5a21fa1f6d77d4@mail.gmail.com>
Message-ID: <BAY116-DAV871B16853B20972A68EDBCF280@phx.gbl>

Another possibility:

    1) Split the original file into smaller chunks of xx,xxx of rows.
    2) Process each file using read.fwf saving the requisite variables.
       (If necessary, save each intermediate matrix/data.frame to disk
       to conserve space)
    3) 'rbind' the results.

Not exactly elegant but it works.

----- Original Message ----- 
From: "Gabor Grothendieck" <ggrothendieck at gmail.com>
To: "Anupam Tyagi" <AnupTyagi at yahoo.com>
Cc: <r-help at stat.math.ethz.ch>
Sent: Wednesday, September 13, 2006 7:21 AM
Subject: Re: [R] Reading fixed column format


> On 9/13/06, Anupam Tyagi <AnupTyagi at yahoo.com> wrote:
>> Gabor Grothendieck <ggrothendieck <at> gmail.com> writes:
>>
>> > C:\bin>cut -c2-3,6-8 a.dat
>> > 23678
>> > 23678
>> > 23678
>>
>> Thanks. I think this will work. How do I redirect the output to a file on
>> windows?
>
> Same as on UNIX
>
> cut -c2-3,6-8 a.dat > a2.dat
>
>> Is there simple way to convert the cut command to a script on windows,
>
> Using notepad or other text editor put it in file a.bat and then
> issue this command from the console
>
> a.bat
>
> Note that you could process it multiple time if you like:
>
> cut -c6-8 a.dat > a2.dat
> cut -c2-3 a2.dat > a3.dat
>
> produces the same thing but uses 2 passes and so keeps each line shorter.
> Be sure you do it from the tail end forward as shown above to avoid having
> to recalculate the positions.
>
>> because the entire command may not fit on one line? Anupam.
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From bianca.vieru at free.fr  Wed Sep 13 18:57:36 2006
From: bianca.vieru at free.fr (Bianca Vieru)
Date: Wed, 13 Sep 2006 18:57:36 +0200
Subject: [R] kendall's w
Message-ID: <45083880.9060802@free.fr>

Hi,

I try to calculate Kendall's W coefficient and I have a bizarre error.


little.app.mat<-matrix(c(1,3,4,2,6,5,2,4,3,1,5,6,3,2,5,1,5,4),nrow=3,byrow=TRUE)
print(kendall.w(little.app.mat[-1,]))
 >>> Kendall's W for ordinal data
 >>> W = 0.7753623Error in if (is.na(x$p.table)) { : argument is of 
length zero

big.app.mat<-matrix(c(1,3,4,2,6,5,2,4,3,1,5,6,3,2,5,1,5,42,3,5,3,6,7,9,9,8,7),nrow=3,byrow=TRUE)
print(kendall.w(big.app.mat[-1,]))
 >>>Kendall's W for ordinal data
 >>>W = 0.4568966  p(X2[8]) = 0.5035488

Why is that working for the big matrix and not for the little one?

Thanks,
Bianca


From ggrothendieck at gmail.com  Wed Sep 13 19:02:57 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 13 Sep 2006 13:02:57 -0400
Subject: [R] Reading fixed column format
In-Reply-To: <971536df0609130721i27475468icd5a21fa1f6d77d4@mail.gmail.com>
References: <loom.20060911T172146-100@post.gmane.org>
	<loom.20060911T185053-310@post.gmane.org>
	<BAY116-DAV15C8CB3C190E253F6BD659CF2A0@phx.gbl>
	<loom.20060912T084704-214@post.gmane.org>
	<3EBEC9D3-559C-4607-838D-46042D36A3AA@virginia.edu>
	<45067F0A.3060304@lancaster.ac.uk>
	<loom.20060913T073708-542@post.gmane.org>
	<971536df0609122301l502d10dch8af36d720ab801c9@mail.gmail.com>
	<loom.20060913T083003-590@post.gmane.org>
	<971536df0609130721i27475468icd5a21fa1f6d77d4@mail.gmail.com>
Message-ID: <971536df0609131002l700cf9d1i1fcb93791936d952@mail.gmail.com>

On 9/13/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> On 9/13/06, Anupam Tyagi <AnupTyagi at yahoo.com> wrote:
> > Gabor Grothendieck <ggrothendieck <at> gmail.com> writes:
> >
> > > C:\bin>cut -c2-3,6-8 a.dat
> > > 23678
> > > 23678
> > > 23678
> >
> > Thanks. I think this will work. How do I redirect the output to a file on
> > windows?
>
> Same as on UNIX
>
> cut -c2-3,6-8 a.dat > a2.dat
>
> > Is there simple way to convert the cut command to a script on windows,
>
> Using notepad or other text editor put it in file a.bat and then
> issue this command from the console
>
> a.bat
>
> Note that you could process it multiple time if you like:
>
> cut -c6-8 a.dat > a2.dat
> cut -c2-3 a2.dat > a3.dat

Sorry that's wrong.  It should be:

cut -c2-3 a.dat > a1.dat
cut -c6-8 a.dat > a2.dat

Now read in each of the files, a1.dat, a2.dat into R.


>
> produces the same thing but uses 2 passes and so keeps each line shorter.
> Be sure you do it from the tail end forward as shown above to avoid having
> to recalculate the positions.
>
> > because the entire command may not fit on one line? Anupam.
> >
>


From chabotd at globetrotter.net  Wed Sep 13 19:06:38 2006
From: chabotd at globetrotter.net (Denis Chabot)
Date: Wed, 13 Sep 2006 13:06:38 -0400
Subject: [R] reshaping a dataset
In-Reply-To: <971536df0609122155h694bd682odc5a837f7a7d3afb@mail.gmail.com>
References: <A451D802-DEE7-40E0-B918-9211000D6217@globetrotter.net>
	<971536df0609122132i73af6202y62f46a31a8ffd79@mail.gmail.com>
	<971536df0609122155h694bd682odc5a837f7a7d3afb@mail.gmail.com>
Message-ID: <48C4D4D8-3A9C-4318-ABB4-7878821AB2EA@globetrotter.net>

Thank you Gabor,

I'll need to explore a bit the reshape package to see what benefits I  
get compared with the basic "reshape" function, but I'm glad you made  
me aware of it.

And your solution for fixing NAs just for the columns I want is just  
what I wanted.

Many thanks,

Denis
Le 06-09-13 ? 00:55, Gabor Grothendieck a ?crit :

> I missed your second question which was how to set the NAs to zero
> for some of the columns.  Suppose we want to replace the NAs
> in columns ic and for sake of example suppose ic specifies
> columns 1 to 8:
>
> library(reshape)
> testm <- melt(test, id = 1:6)
> out <- cast(testm, nbpc + trip + set + tagno + depth ~ prey, sum)
>
> # fix up NAs
> ic <- 1:8
> out2 <- out[,ic]
> out2[is.na(out2)] <- 0
> out[,ic] <- out2
>
> On 9/13/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
>> If I understand this correctly we want to sum the mass over each  
>> combination
>> of the first 6 variables and display the result with the 6th, prey,
>> along the top and the others along the side.
>>
>> library(reshape)
>> testm <- melt(test, id = 1:6)
>> cast(testm, nbpc + trip + set + tagno + depth ~ prey)
>>
>> Now fix up the NAs.
>>
>> On 9/12/06, Denis Chabot <chabotd at globetrotter.net> wrote:
>> > Hi,
>> >
>> > I'm trying to move to R the last few data handling routines I was
>> > performing in SAS.
>> >
>> > I'm working on stomach content data. In the simplified example I
>> > provide below, there are variables describing the origin of each  
>> prey
>> > item (nbpc is a ship number, each ship may have been used on
>> > different trips, each trip has stations, and individual fish  
>> (tagno)
>> > can be caught at each station.
>> >
>> > For each stomach the number of lines corresponds to the number of
>> > prey items. Thus a variable identifies prey type, and others (here
>> > only one, mass) provide information on prey abundance or size or
>> > digestion level.
>> >
>> > Finally, there can be accompanying variables that are not used but
>> > that I need to keep for later analyses (e.g. depth in the example
>> > below).
>> >
>> > At some point I need to transform such a dataset into another  
>> format
>> > where each stomach occupies a single line, and there are columns  
>> for
>> > each prey item.
>> >
>> > The "reshape" function works really well, my program is in fact
>> > simpler than the SAS equivalent (not shown, don't want to bore you,
>> > but available on request), except that I need zeros when prey types
>> > are absent from a stomach instead of NAs, a problem for which I  
>> only
>> > have a shaky solution at the moment:
>> >
>> > 1) creation of a dummy dataset:
>> > #######
>> > nbpc <- rep(c(20,34), c(110,90))
>> > trip <- c(rep(1:3, c(40, 40, 30)), rep(1:2, c(60,30)))
>> > set <- c(rep(1:4, c(10, 8, 7, 15)), rep(c(10,12), c(25,15)), rep 
>> (1:3,
>> > rep(10,3)),
>> >          rep(10:12, c(20, 10, 30)), rep(7:8, rep(15,2)))
>> > depth <- c(rep(c(100, 150, 200, 250), c(10, 8, 7, 15)), rep(c
>> > (100,120), c(25,15)), rep(c(75, 50, 200), rep(10,3)),
>> >          rep(c(200, 150, 50), c(20, 10, 30)), rep(c(100, 250), rep
>> > (15,2)))
>> > tagno <- rep(round(runif(42,1,200)),
>> >              c(7,3, 4,4, 2,2,3, 5,5,5,  4,6,4,3,5,3, 7,8, 4,6, 5,5,
>> > 7,3,
>> >                6,6,4,4, 4,6, 3,3,4,5,5,6,4, 5,5,5, 8,7))
>> > prey.codes <-c(187, 438, 792, 811)
>> > prey <- sample(prey.codes, 200, replace=T)
>> > mass <- runif(200, 0, 10)
>> >
>> > test <- data.frame(nbpc, trip, set, depth, tagno, prey, mass)
>> > ########
>> >
>> > Because there are often multiple occurrences of the same prey in a
>> > single stomach, I need to sum them for each stomach before using
>> > "reshape". Here I use summarizeBy because my understanding of the
>> > many variants of "apply" is not very good:
>> >
>> > ########
>> > test2 <- summaryBy(mass~nbpc+trip+set+tagno+prey, data=test,  
>> FUN=sum,
>> > keep.names=T, id=~depth)
>> >
>> > #this messes up sorting order, I fix it
>> > k <- order(test2$nbpc, test2$trip, test2$set, test2$tagno)
>> > test3 <- test2[k,]
>> > result <- reshape(test3, v.names="mass", idvar=c("nbpc", "trip",
>> > "set", "tagno"),
>> >                 timevar="prey", direction="wide")
>> > #########
>> >
>> > I'm quite happy with this, although you may know of better ways of
>> > doing it.
>> > But my problem is with preys that are absent from a stomach. In  
>> later
>> > analyses, I need them to have zero abundance instead of NA.
>> > My shaky solution is:
>> > #########
>> > empties <- is.na(result)
>> > result[empties] <- 0
>> > #########
>> >
>> > which did the job in this example, but it won't always. For  
>> instance
>> > there could have been NAs for "depth", which I do not want to  
>> become
>> > zero.
>> >
>> > Is there a way to transform NAs into zeros for multiple columns  
>> of a
>> > dataframe in one step, while ignoring some columns?
>> >
>> > Or maybe there is another way to achieve this that would have put
>> > zeros where I need them (i.e. something else than "reshape")?
>> >
>> > Thanking you in advance,
>> >
>> > Denis Chabot
>> >
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide http://www.R-project.org/ 
>> posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>> >
>>


From mpg33 at drexel.edu  Wed Sep 13 19:09:31 2006
From: mpg33 at drexel.edu (Michael Gormley)
Date: Wed, 13 Sep 2006 13:09:31 -0400
Subject: [R] Access Rows in a Data Frame by Row Name
Message-ID: <042a01c6d757$60f567a0$6b1f1981@9Mike9>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060913/7c4d7b47/attachment.pl 

From rmh at temple.edu  Wed Sep 13 19:13:07 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Wed, 13 Sep 2006 13:13:07 -0400 (EDT)
Subject: [R] Gnuplot epslatex format also in R?
Message-ID: <20060913131307.BHW07053@po-d.temple.edu>

xv has been available for windows for at least 10 years.

http://download.mirror.ac.uk/sites/ftp.cis.upenn.edu/pub/xv/


From kingroi at hotmail.com  Wed Sep 13 19:28:36 2006
From: kingroi at hotmail.com (paul king)
Date: Wed, 13 Sep 2006 17:28:36 +0000
Subject: [R] Course***Dr Frank Harrell's Regression Modeling Strategies
 in R/Splus course *** September 2006 near you (San Francisco, Washington DC,
 Atlanta)
Message-ID: <BAY107-W85AA48A8C86A71F1EE663B0280@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060913/7c30ca28/attachment.pl 

From mschwartz at mn.rr.com  Wed Sep 13 19:33:12 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Wed, 13 Sep 2006 12:33:12 -0500
Subject: [R] group bunch of lines in a data.frame,
	an additional	requirement
In-Reply-To: <e4654710609130938p313dbe57g19c7e36c4ae51488@mail.gmail.com>
References: <e4654710609130938p313dbe57g19c7e36c4ae51488@mail.gmail.com>
Message-ID: <1158168792.3954.26.camel@localhost.localdomain>

Try something like this:

# Initial data frame
> DF
  V1  V2  V3  V4
1  A 1.0 200 ID1
2  A 3.0 800 ID1
3  A 2.0 200 ID1
4  B 0.5  20 ID2
5  B 0.9  50 ID2
6  C 5.0  70 ID1


# Now do the aggregation to get the means
DF.1 <- aggregate(DF[, 2:3], list(V1 = DF$V1), mean)


> DF.1
  V1  V2  V3
1  A 2.0 400
2  B 0.7  35
3  C 5.0  70


# Now get the unique combinations of letters and IDs in DF
DF.U <- unique(DF[, c("V1", "V4")])

> DF.U
  V1  V4
1  A ID1
4  B ID2
6  C ID1


# Now merge the two data frames together, matching the letters
DF.NEW <- merge(DF.1, DF.U, by = "V1")

> DF.NEW
  V1  V2  V3  V4
1  A 2.0 400 ID1
2  B 0.7  35 ID2
3  C 5.0  70 ID1


See ?unique and ?merge for more information.

Also, for the sake of clarification, these are not matrices, but data
frames. A matrix may contain only one data type, whereas data frames are
specifically designed to contain multiple data types as you have here.

HTH,

Marc Schwartz

On Wed, 2006-09-13 at 17:38 +0100, Emmanuel Levy wrote:
> Thanks for pointing me out "aggregate", that works fine!
> 
> There is one complication though: I have mixed types (numerical and character),
> 
> So the matrix is of the form:
> 
> A 1.0 200 ID1
> A 3.0 800 ID1
> A 2.0 200 ID1
> B 0.5 20   ID2
> B 0.9 50   ID2
> C 5.0 70   ID1
> 
> One letter always has the same ID but one ID can be shared by many
> letters (like ID1)
> 
> I just want to keep track of the ID, and get a matrix like:
> 
> A 2.0 400 ID1
> B 0.7 35 ID2
> C 5.0 70 ID1
> 
> Any idea on how to do that without a loop?
> 
>   Many thanks,
> 
>      Emmanuel
> 
> On 9/12/06, Emmanuel Levy <emmanuel.levy at gmail.com> wrote:
> > Hello,
> >
> > I'd like to group the lines of a matrix so that:
> > A 1.0 200
> > A 3.0 800
> > A 2.0 200
> > B 0.5 20
> > B 0.9 50
> > C 5.0 70
> >
> > Would give:
> > A 2.0 400
> > B 0.7 35
> > C 5.0 70
> >
> > So all lines corresponding to a letter (level), become a single line
> > where all the values of each column are averaged.
> >
> > I've done that with a loop but it doesn't sound right (it is very
> > slow). I imagine there is a
> > sort of "apply" shortcut but I can't figure it out.
> >
> > Please note that it is not exactly a matrix I'm using, the function
> > "typeof" tells me it's a list, however I access to it like it was a
> > matrix.
> >
> > Could someone help me with the right function to use, a help topic or
> > a piece of code?
> >
> > Thanks,
> >
> >   Emmanuel
> >


From ggrothendieck at gmail.com  Wed Sep 13 19:44:55 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 13 Sep 2006 13:44:55 -0400
Subject: [R] group bunch of lines in a data.frame,
	an additional requirement
In-Reply-To: <e4654710609130938p313dbe57g19c7e36c4ae51488@mail.gmail.com>
References: <e4654710609130938p313dbe57g19c7e36c4ae51488@mail.gmail.com>
Message-ID: <971536df0609131044y1a628331k6ea5846adaa6c3f5@mail.gmail.com>

See below.

On 9/13/06, Emmanuel Levy <emmanuel.levy at gmail.com> wrote:
> Thanks for pointing me out "aggregate", that works fine!
>
> There is one complication though: I have mixed types (numerical and character),
>
> So the matrix is of the form:
>
> A 1.0 200 ID1
> A 3.0 800 ID1
> A 2.0 200 ID1
> B 0.5 20   ID2
> B 0.9 50   ID2
> C 5.0 70   ID1
>
> One letter always has the same ID but one ID can be shared by many
> letters (like ID1)
>
> I just want to keep track of the ID, and get a matrix like:
>
> A 2.0 400 ID1
> B 0.7 35 ID2
> C 5.0 70 ID1
>
> Any idea on how to do that without a loop?

If V4 is a function of V1 then you can aggregate by it too and it will
appear but have no effect on the classification:

> aggregate(DF[2:3], DF[c(1,4)], mean)
  V1  V4  V2  V3
1  A ID1 2.0 400
2  C ID1 5.0  70
3  B ID2 0.7  35


>
>  Many thanks,
>
>     Emmanuel
>
> On 9/12/06, Emmanuel Levy <emmanuel.levy at gmail.com> wrote:
> > Hello,
> >
> > I'd like to group the lines of a matrix so that:
> > A 1.0 200
> > A 3.0 800
> > A 2.0 200
> > B 0.5 20
> > B 0.9 50
> > C 5.0 70
> >
> > Would give:
> > A 2.0 400
> > B 0.7 35
> > C 5.0 70
> >
> > So all lines corresponding to a letter (level), become a single line
> > where all the values of each column are averaged.
> >
> > I've done that with a loop but it doesn't sound right (it is very
> > slow). I imagine there is a
> > sort of "apply" shortcut but I can't figure it out.
> >
> > Please note that it is not exactly a matrix I'm using, the function
> > "typeof" tells me it's a list, however I access to it like it was a
> > matrix.
> >
> > Could someone help me with the right function to use, a help topic or
> > a piece of code?
> >
> > Thanks,
> >
> >   Emmanuel
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mothsailor at googlemail.com  Wed Sep 13 19:47:36 2006
From: mothsailor at googlemail.com (David Barron)
Date: Wed, 13 Sep 2006 18:47:36 +0100
Subject: [R] kendall's w
In-Reply-To: <815b70590609131046l45215c7dped60c8f718516f7a@mail.gmail.com>
References: <45083880.9060802@free.fr>
	<815b70590609131046l45215c7dped60c8f718516f7a@mail.gmail.com>
Message-ID: <815b70590609131047h22c9714cpb7d35e422e6cf0ad@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060913/7c22473d/attachment.pl 

From tplate at acm.org  Wed Sep 13 20:01:31 2006
From: tplate at acm.org (Tony Plate)
Date: Wed, 13 Sep 2006 12:01:31 -0600
Subject: [R] Access Rows in a Data Frame by Row Name
In-Reply-To: <042a01c6d757$60f567a0$6b1f1981@9Mike9>
References: <042a01c6d757$60f567a0$6b1f1981@9Mike9>
Message-ID: <4508477B.8000203@acm.org>

Matrix-style indexing works for both columns and rows of data frames.

E.g.:
 > x <- data.frame(a=1:5, b=6:10, d=11:15)
 > x
   a  b  d
1 1  6 11
2 2  7 12
3 3  8 13
4 4  9 14
5 5 10 15
 > x[2:4,c(1,3)]
   a  d
2 2 12
3 3 13
4 4 14
 >

Time spend reading the help document "An Introduction to R" will 
probably be well worth it.  The relevant sections are "5 Arrays and 
matrices", and "6.3 Data frames".

-- Tony Plate

Michael Gormley wrote:
> I have created a data frame using the read.table command.  I want to be able to access the rows by the row name, or a vector of row names. I know that you can access columns by using the data.frame.name$col.name.  Is there a way to access row names in a similar manner?
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From slv511 at mail.usask.ca  Wed Sep 13 20:06:14 2006
From: slv511 at mail.usask.ca (Steven Van Wilgenburg)
Date: Wed, 13 Sep 2006 12:06:14 -0600 (CST)
Subject: [R] reformat records one to several
Message-ID: <3111851.1158170774545.JavaMail.slv511@mail.usask.ca>

Hi,

I am a new user of R and am still trying to figure out which statements 
do which functions and am looking for a jump start. 

I have a dataset where the data were collected as ten minute counts 
where the number of new individuals within a species was recorded as 
cohorts within 3 separate time intervals within the ten minute count 
persiod. Each row of data therefore follows a format like this:

       Date    Time       Sample	Species	t1 t2 t3
June 5,2006 5:20 AM AUSFAKE01	OVEN	3  0  1
etc.....

I would like to reformat these data as if the counts recorded only 
individuals and not cohorts, so that the above would look as follows

       Date    Time       Sample	Species	t1 t2 t3
June 5,2006 5:20 AM AUSFAKE01	OVEN	1  0  0
June 5,2006 5:20 AM AUSFAKE01	OVEN	1  0  0
June 5,2006 5:20 AM AUSFAKE01	OVEN	1  0  0
June 5,2006 5:20 AM AUSFAKE01	OVEN	0  0  1
etc.....
 
I believe I could do this in SAS with IF, THEN and DO statements,

e.g. 
if t1>0 then
	do i=1 to t1
		output Date,Time,Sample,Species,"1","0","0";
if t2>0 then
	do i=1 to t2
		output Date,Time,Sample,Species,"0","1","0";
if t3>0 then
	do i=1 to t3
		output Date,Time,Sample,Species,"0","0","1";
end;


Can anyone point me in the right direction? What is the similar 
statement to DO in R?

Steve VW


From gunter.berton at gene.com  Wed Sep 13 21:11:39 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 13 Sep 2006 12:11:39 -0700
Subject: [R] Access Rows in a Data Frame by Row Name
In-Reply-To: <4508477B.8000203@acm.org>
Message-ID: <004801c6d768$712d1b70$711f210a@gne.windows.gene.com>

The answer is yes, you can access rows of a data.frame by rowname in the
same way as columns, which you could have found by merely trying it. Don't
overlook the value of a little experimentation as the fastest way to an
answer.

-- Bert Gunter
Genentech
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Tony Plate
> Sent: Wednesday, September 13, 2006 11:02 AM
> To: Michael Gormley
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Access Rows in a Data Frame by Row Name
> 
> Matrix-style indexing works for both columns and rows of data frames.
> 
> E.g.:
>  > x <- data.frame(a=1:5, b=6:10, d=11:15)
>  > x
>    a  b  d
> 1 1  6 11
> 2 2  7 12
> 3 3  8 13
> 4 4  9 14
> 5 5 10 15
>  > x[2:4,c(1,3)]
>    a  d
> 2 2 12
> 3 3 13
> 4 4 14
>  >
> 
> Time spend reading the help document "An Introduction to R" will 
> probably be well worth it.  The relevant sections are "5 Arrays and 
> matrices", and "6.3 Data frames".
> 
> -- Tony Plate
> 
> Michael Gormley wrote:
> > I have created a data frame using the read.table command.  
> I want to be able to access the rows by the row name, or a 
> vector of row names. I know that you can access columns by 
> using the data.frame.name$col.name.  Is there a way to access 
> row names in a similar manner?
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From justin_bem at yahoo.fr  Wed Sep 13 21:50:33 2006
From: justin_bem at yahoo.fr (justin bem)
Date: Wed, 13 Sep 2006 19:50:33 +0000 (GMT)
Subject: [R] Dear FE  Harrell How can I get rreport ?
In-Reply-To: <4508286D.1090305@vanderbilt.edu>
Message-ID: <20060913195033.51077.qmail@web25708.mail.ukl.yahoo.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20060913/6227aa1b/attachment.pl 

From etach5056 at gmail.com  Wed Sep 13 22:00:26 2006
From: etach5056 at gmail.com (Superdealhouse)
Date: Wed, 13 Sep 2006 15:00:26 -0500
Subject: [R] Question about optim on survival data with censored data
Message-ID: <b7f3b2a40609131300o1e39c7c5qeb172bd904f461b9@mail.gmail.com>

I have a model which is mixed exponential model, with about 40
observations. I am wandering whether i can use the optim to optimized
the parameters? Thank you very much.


From A.Robinson at ms.unimelb.edu.au  Wed Sep 13 22:39:38 2006
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Thu, 14 Sep 2006 06:39:38 +1000
Subject: [R] Conservative "ANOVA tables" in lmer
In-Reply-To: <1158145458.3699.3.camel@solidago.localdomain>
References: <1A8FE5B0-3FE3-4CA9-9224-9CACB9AF4854@muohio.edu>
	<40e66e0b0609070559u656463c1l52a60eda9a095237@mail.gmail.com>
	<20060910055624.GA12212@ms.unimelb.edu.au>
	<40e66e0b0609110943h7dfb057dv12567975ac04bdb2@mail.gmail.com>
	<1158010468.3278.24.camel@solidago.localdomain>
	<48130.220.237.183.166.1158098663.squirrel@webmail.ms.unimelb.edu.au>
	<1158145458.3699.3.camel@solidago.localdomain>
Message-ID: <20060913203938.GH12212@ms.unimelb.edu.au>

On Wed, Sep 13, 2006 at 07:04:17AM -0400, Manuel Morales wrote:
> On Wed, 2006-09-13 at 08:04 +1000, Andrew Robinson wrote:
> > On Tue, September 12, 2006 7:34 am, Manuel Morales wrote:
> > > On Mon, 2006-09-11 at 11:43 -0500, Douglas Bates wrote:
> > >> Having made that offer I think I will now withdraw it.  Peter's
> > >> example has convinced me that this is the wrong thing to do.
> > >>
> > >> I am encouraged by the fact that the results from mcmcsamp correspond
> > >> closely to the correct theoretical results in the case that Peter
> > >> described.  I appreciate that some users will find it difficult to
> > >> work with a MCMC sample (or to convince editors to accept results
> > >> based on such a sample) but I think that these results indicate that
> > >> it is better to go after the marginal distribution of the fixed
> > >> effects estimates (which is what is being approximated by the MCMC
> > >> sample - up to Bayesian/frequentist philosophical differences) than to
> > >> use the conditional distribution and somehow try to adjust the
> > >> reference distribution.
> > >
> > > Am I right that the MCMC sample can not be used, however, to evaluate
> > > the significance of parameter groups. For example, to assess the
> > > significance of a three-level factor? Are there better alternatives than
> > > simply adjusting the CI for the number of factor levels
> > > (1-alpha/levels).
> > 
> > I wonder whether the likelihood ratio test would be suitable here?  That
> > seems to be supported.  It just takes a little longer.
> > 
> > > require(lme4)
> > > data(sleepstudy)
> > > fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
> > > fm2 <- lmer(Reaction ~ Days + I(Days^2) + (Days|Subject), sleepstudy)
> > > anova(fm1, fm2)
> > 
> > So, a brief overview of the popular inferential needs and solutions would
> > then be:
> > 
> > 1) Test the statistical significance of one or more fixed or random
> > effects - fit a model with and a model without the terms, and use the LRT.
> 
> I believe that the LRT is anti-conservative for fixed effects, as
> described in Pinheiro and Bates companion book to NLME.

Yes, you are right.  I had forgotten that.  Back to square one :).
Bert Gunter also kindly pointed this out to me.

Cherse

Andrew


 
> > 2) Obtain confidence intervals for one or more fixed or random effects -
> > use mcmcsamp
> > 
> > Did I miss anything important? - What else would people like to do?
> > 
> > Cheers
> > 
> > Andrew
> > 
> > Andrew Robinson
> > Senior Lecturer in Statistics                       Tel: +61-3-8344-9763
> > Department of Mathematics and Statistics            Fax: +61-3-8344 4599
> > University of Melbourne, VIC 3010 Australia
> > Email: a.robinson at ms.unimelb.edu.au    Website: http://www.ms.unimelb.edu.au
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
Email: a.robinson at ms.unimelb.edu.au         http://www.ms.unimelb.edu.au


From jennystadt at yahoo.ca  Wed Sep 13 22:47:52 2006
From: jennystadt at yahoo.ca (Jenny Stadt)
Date: Wed, 13 Sep 2006 14:47:52 -0600
Subject: [R] About truncated distribution
References: <200609121541592080733@yahoo.ca>
	<00c401c6d6be$38e48af0$711f210a@gne.windows.gene.com>
	<42bc98300609121620i434c8e89ycb9f5a1a852adaae@mail.gmail.com>
Message-ID: <200609131447509198132@yahoo.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060913/c821747b/attachment.pl 

From geelewis at gmail.com  Wed Sep 13 22:53:45 2006
From: geelewis at gmail.com (Warren)
Date: Wed, 13 Sep 2006 13:53:45 -0700
Subject: [R] recursive methods for concatenating sets of files
Message-ID: <900c4d920609131353o5e4a9d3v8eeb4ca8f1f43d98@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060913/e19cdb7c/attachment.pl 

From shitao at hotmail.com  Wed Sep 13 23:07:30 2006
From: shitao at hotmail.com (Tao Shi)
Date: Wed, 13 Sep 2006 21:07:30 +0000
Subject: [R] an error message with 't.test' with R under Unix
Message-ID: <BAY109-W41997B99A13F53F814C06C7280@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060913/180409e4/attachment.pl 

From shitao at hotmail.com  Wed Sep 13 23:07:30 2006
From: shitao at hotmail.com (Tao Shi)
Date: Wed, 13 Sep 2006 21:07:30 +0000
Subject: [R] an error message with 't.test' with R under Unix
Message-ID: <BAY109-W41997B99A13F53F814C06C7280@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060913/180409e4/attachment-0001.pl 

From jm540 at york.ac.uk  Wed Sep 13 23:13:47 2006
From: jm540 at york.ac.uk (Jon Minton)
Date: Wed, 13 Sep 2006 22:13:47 +0100
Subject: [R] inserting columns in the middle of a dataframe
Message-ID: <000901c6d779$809682c0$81c38840$@ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060913/32974346/attachment.pl 

From jholtman at gmail.com  Wed Sep 13 23:31:07 2006
From: jholtman at gmail.com (jim holtman)
Date: Wed, 13 Sep 2006 17:31:07 -0400
Subject: [R] recursive methods for concatenating sets of files
In-Reply-To: <900c4d920609131353o5e4a9d3v8eeb4ca8f1f43d98@mail.gmail.com>
References: <900c4d920609131353o5e4a9d3v8eeb4ca8f1f43d98@mail.gmail.com>
Message-ID: <644e1f320609131431w17da6a06g7c196215feeb7d83@mail.gmail.com>

Try this:

setwd("d:/perf/windows")  # wherever your data is
results <- list()
for (i in list.files(pattern="t.*txt$")){   # need the 'pattern' of the names
    results[[i]] <- read.delim(i, quote='', as.is=TRUE)
}
dataALL <- do.call('cbind', results)
write.table(dataALL,"0905p528.txt", quote=FALSE)


On 9/13/06, Warren <geelewis at gmail.com> wrote:
> Hello,
>
> I would like to read sets of files within a folder, perhaps using recursive
> methods.
>
> Right now, I rename the files before import.
> It would be even better to do this without renaming files, without providing
> explicit filenames, perhaps by importing files based on chronology,
> and translating each filename into a header?
>
> Please excuse my ignorance, and help cure my clunky programming
> (below) with more elegant code.
>
> Thanks,
> Warren
>
>
>
>
>
> data0 <-read.delim("t.txt", quote="", as.is=TRUE)
>
> data1 <-read.delim("t (1).txt", quote="", as.is=TRUE)
>
> data2 <-read.delim("t (2).txt", quote="", as.is=TRUE)
>
> data3 <-read.delim("t (3).txt", quote="", as.is=TRUE)
>
> data4 <-read.delim("t (4).txt", quote="", as.is=TRUE)
>
> data5 <-read.delim("t (5).txt", quote="", as.is=TRUE)
>
> data6 <-read.delim("t (6).txt", quote="", as.is=TRUE)
>
> data7 <-read.delim("t (7).txt", quote="", as.is=TRUE)
>
> data8 <-read.delim("t (8).txt", quote="", as.is=TRUE)
>
> data9 <-read.delim("t (9).txt", quote="", as.is=TRUE)
>
> data10 <-read.delim("t (10).txt", quote="", as.is=TRUE)
>
> data11 <-read.delim("t (11).txt", quote="", as.is=TRUE)
>
> data12 <-read.delim("t (12).txt", quote="", as.is=TRUE)
>
> data13 <-read.delim("t (13).txt", quote="", as.is=TRUE)
>
> data14 <-read.delim("t (14).txt", quote="", as.is=TRUE)
>
> data15 <-read.delim("t (15).txt", quote="", as.is=TRUE)
>
> data16 <-read.delim("t (16).txt", quote="", as.is=TRUE)
>
> data17 <-read.delim("t (17).txt", quote="", as.is=TRUE)
>
> data18 <-read.delim("t (18).txt", quote="", as.is=TRUE)
>
> data19 <-read.delim("t (19).txt", quote="", as.is=TRUE)
>
> data20 <-read.delim("t (20).txt", quote="", as.is=TRUE)
>
> data21 <-read.delim("t (21).txt", quote="", as.is=TRUE)
>
> data22 <-read.delim("t (22).txt", quote="", as.is=TRUE)
>
> data23 <-read.delim("t (23).txt", quote="", as.is=TRUE)
>
> data24 <-read.delim("t (24).txt", quote="", as.is=TRUE)
>
> data25 <-read.delim("t (25).txt", quote="", as.is=TRUE)
>
> data26 <-read.delim("t (26).txt", quote="", as.is=TRUE)
>
> data27 <-read.delim("t (27).txt", quote="", as.is=TRUE)
>
> data28 <-read.delim("t (28).txt", quote="", as.is=TRUE)
>
> data29 <-read.delim("t (29).txt", quote="", as.is=TRUE)
>
> data30 <-read.delim("t (30).txt", quote="", as.is=TRUE)
>
> data31 <-read.delim("t (31).txt", quote="", as.is=TRUE)
>
> data32 <-read.delim("t (32).txt", quote="", as.is=TRUE)
>
> data33 <-read.delim("t (33).txt", quote="", as.is=TRUE)
>
> data34 <-read.delim("t (34).txt", quote="", as.is=TRUE)
>
> data35 <-read.delim("t (35).txt", quote="", as.is=TRUE)
>
> data36 <-read.delim("t (36).txt", quote="", as.is=TRUE)
>
> data37 <-read.delim("t (37).txt", quote="", as.is=TRUE)
>
> data38 <-read.delim("t (38).txt", quote="", as.is=TRUE)
>
> data39 <-read.delim("t (39).txt", quote="", as.is=TRUE)
>
> data40 <-read.delim("t (40).txt", quote="", as.is=TRUE)
>
> data41 <-read.delim("t (41).txt", quote="", as.is=TRUE)
>
> data42 <-read.delim("t (42).txt", quote="", as.is=TRUE)
>
> data43 <-read.delim("t (43).txt", quote="", as.is=TRUE)
>
> data44 <-read.delim("t (44).txt", quote="", as.is=TRUE)
>
> data45 <-read.delim("t (45).txt", quote="", as.is=TRUE)
>
> data46 <-read.delim("t (46).txt", quote="", as.is=TRUE)
>
> data47 <-read.delim("t (47).txt", quote="", as.is=TRUE)
>
> data48 <-read.delim("t (48).txt", quote="", as.is=TRUE)
>
> data49 <-read.delim("t (49).txt", quote="", as.is=TRUE)
>
> data50 <-read.delim("t (50).txt", quote="", as.is=TRUE)
>
> data51 <-read.delim("t (51).txt", quote="", as.is=TRUE)
>
> data52 <-read.delim("t (52).txt", quote="", as.is=TRUE)
>
> data53 <-read.delim("t (53).txt", quote="", as.is=TRUE)
>
> data54 <-read.delim("t (54).txt", quote="", as.is=TRUE)
>
> data55 <-read.delim("t (55).txt", quote="", as.is=TRUE)
>
> data56 <-read.delim("t (56).txt", quote="", as.is=TRUE)
>
> data57 <-read.delim("t (57).txt", quote="", as.is=TRUE)
>
> data58 <-read.delim("t (58).txt", quote="", as.is=TRUE)
>
> data59 <-read.delim("t (59).txt", quote="", as.is=TRUE)
>
> data60 <-read.delim("t (60).txt", quote="", as.is=TRUE)
>
> data61 <-read.delim("t (61).txt", quote="", as.is=TRUE)
>
> data62 <-read.delim("t (62).txt", quote="", as.is=TRUE)
>
> data63 <-read.delim("t (63).txt", quote="", as.is=TRUE)
>
> data64 <-read.delim("t (64).txt", quote="", as.is=TRUE)
>
> data65 <-read.delim("t (65).txt", quote="", as.is=TRUE)
>
> data66 <-read.delim("t (66).txt", quote="", as.is=TRUE)
>
> data67 <-read.delim("t (67).txt", quote="", as.is=TRUE)
>
> data68 <-read.delim("t (68).txt", quote="", as.is=TRUE)
>
> data69 <-read.delim("t (69).txt", quote="", as.is=TRUE)
>
> data70 <-read.delim("t (70).txt", quote="", as.is=TRUE)
>
> data71 <-read.delim("t (71).txt", quote="", as.is=TRUE)
>
> data72 <-read.delim("t (72).txt", quote="", as.is=TRUE)
>
> data73 <-read.delim("t (73).txt", quote="", as.is=TRUE)
>
> data74 <-read.delim("t (74).txt", quote="", as.is=TRUE)
>
> data75 <-read.delim("t (75).txt", quote="", as.is=TRUE)
>
>
>
> dataALL
> <-cbind(data0,data1,data2,data3,data4,data5,data6,data7,data8,data9,data10,data11,data12,data13,data14,data15,data16,data17,data18,data19,data20,data21,data22,data23,data24,data25,data26,data27,data28,data29,data20,data31,data32,data33,data34,data35,data36,data37,data38,data39,data40,data41,data42,data43,data44,data45,data46,data47,data48,data49,data50,data51,data52,data53,data54,data55,data56,data57,data58,data59,data60,data61,data62,data63,data64,data65,data66,data67,data68,data69,data70,data71,data72,data73,data74,data75)
>
>
>
> write.table(dataALL,"0905p528.txt", quote=FALSE)
> --
> geelewis at gmail.com
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From spluque at gmail.com  Wed Sep 13 23:31:09 2006
From: spluque at gmail.com (Sebastian P. Luque)
Date: Wed, 13 Sep 2006 16:31:09 -0500
Subject: [R] recursive methods for concatenating sets of files
References: <900c4d920609131353o5e4a9d3v8eeb4ca8f1f43d98@mail.gmail.com>
Message-ID: <873bavh2si.fsf@patagonia.sebmags.homelinux.org>

On Wed, 13 Sep 2006 13:53:45 -0700,
Warren <geelewis at gmail.com> wrote:

> Hello, I would like to read sets of files within a folder, perhaps using
> recursive methods.

Maybe this:


fv <- list.files()
lf <- sapply(fv, read.delim, quote="", as.is=TRUE)
xx <- do.call(cbind, lf)


You can find more info in the respective help pages.


-- 
Seb


From p.dalgaard at biostat.ku.dk  Wed Sep 13 23:52:04 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 13 Sep 2006 23:52:04 +0200
Subject: [R] an error message with 't.test' with R under Unix
In-Reply-To: <BAY109-W41997B99A13F53F814C06C7280@phx.gbl>
References: <BAY109-W41997B99A13F53F814C06C7280@phx.gbl>
Message-ID: <x2r6yf8mez.fsf@turmalin.kubism.ku.dk>

"Tao Shi" <shitao at hotmail.com> writes:

> Hi list,Could you please help me to explain the following error
 messages with 't.test' in R Unix 2.1.1? 

This is completely unreadable! However, yes, there was at some point a
bug where the LHS of model formulas was checked more rigorously than
need be, using the same rules that apply to the RHS. This fact can be
found in a recent NEWS file.



> I don't see it in R under Windows (R 2.3.0) or Unix (R2.3.1).  Is it really due to the different R versions?Thanks,...TaoUnix session: (R.2.1.1)========================> R.version         _                       platform x86_64-unknown-linux-gnuarch     x86_64                  os       linux-gnu               system   x86_64, linux-gnu       status                           major    2                       minor    1.1                     year     2005                    month    06                      day      20                      language R                       > t.test(extra ~ group, data = sleep)        Welch Two Sample t-testdata:  extra by group t = -1.8608, df = 17.776, p-value = 0.0794alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: -3.3654832  0.2054832 sample estimates:mean in group 1 mean in group 2        !
>      0.75            2.33 > t.test((1:6)~rep(1:2,each=3))Error in terms.formula(formula[-3]) : invalid model formula in ExtractVars> t.test(2^(1:6)~rep(1:2,each=3))Error in terms.formula(formula[-3]) : invalid power in formula> t.test(2^extra ~ group, data = sleep)Error in terms.formula(formula[-3]) : invalid power in formulaUnix session: (R 2.3.1)============================> R.version               _                         platform       x86_64-unknown-linux-gnu  arch           x86_64                    os             linux-gnu                 system         x86_64, linux-gnu         status                                   major          2                         minor          3.1                       year           2006                      month          06                        day            01                        svn rev        38247                     language       R                         version.string Version 2.3.1 (2006-06-01)> t.test(1:6~rep(1:2,each
 =!
>  3))        Welch Two Sample t-testdata:  1:6 by rep(1:2, each = 3) t =
>  -3.6742, df = 4, p-value = 0.02131alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: -5.266958 -0.733042 sample estimates:mean in group 1 mean in group 2               2               5 Windows session:===========================> R.version               _                         platform       i386-pc-mingw32           arch           i386                      os             mingw32                   system         i386, mingw32             status                                   major          2                         minor          3.0                       year           2006                      month          04                        day            24                        svn rev        37909                     language       R                         version.string Version 2.3.0 (2006-04-24)> help(t.test)> t.test(extra ~ group, data = sleep)        Welch Two Sample t-testdata:  extra by group t = -1.8608, df = 17.
 7!
>  76, p-value = 0.0794alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: -3.3654832  0.2054832 sample estimates:mean in group 1 mean in group 2            0.75            2.33 > t.test(2^extra ~ group, data = sleep)        Welch Two Sample t-testdata:  2^extra by group t = -1.6362, df = 10.718, p-value = 0.1308alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: -18.641557   2.773344 sample estimates:mean in group 1 mean in group 2        3.448644       11.382751 > t.test(1:6~rep(1:2,each=3))        Welch Two Sample t-testdata:  1:6 by rep(1:2, each = 3) t = -3.6742, df = 4, p-value = 0.02131alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: -5.266958 -0.733042 sample estimates:mean in group 1 mean in group 2               2               5 
> _________________________________________________________________
> Use Messenger to talk to your IM friends, even those on Yahoo!
> 
> 685ee3e858fe
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From timothy.c.bates at gmail.com  Thu Sep 14 00:04:42 2006
From: timothy.c.bates at gmail.com (Timothy Bates)
Date: Wed, 13 Sep 2006 23:04:42 +0100
Subject: [R] inserting columns in the middle of a dataframe
In-Reply-To: <000901c6d779$809682c0$81c38840$@ac.uk>
Message-ID: <C12E3F0A.425E5%timothy.c.bates@gmail.com>


> Is there a built-in and simple way to insert new columns in a dataframe?

You do this by collecting the columns in the new order you desire, and
making a new frame.

oldframe           <- data.frame(matrix(0:14,ncol=3))
newcol              <- data.frame(20:24)
names(newcol) <- "newcol"
newframe         <- data.frame(c(oldframe[1],newcol, oldframe[2:3]))


From jholtman at gmail.com  Thu Sep 14 00:13:28 2006
From: jholtman at gmail.com (jim holtman)
Date: Wed, 13 Sep 2006 18:13:28 -0400
Subject: [R] reformat records one to several
In-Reply-To: <3111851.1158170774545.JavaMail.slv511@mail.usask.ca>
References: <3111851.1158170774545.JavaMail.slv511@mail.usask.ca>
Message-ID: <644e1f320609131513s7637d551v7712c5bd98249b17@mail.gmail.com>

Try this:

n <- 10  # create some sample data
x <- data.frame(date=1:n, t1=sample(0:3, n, TRUE), t2=sample(0:3, n, TRUE),
    t3=sample(0:3, n, TRUE))
x  # print data
result <- lapply(c('t1','t2','t3'), function(i){
    xsub <- x[rep(1:nrow(x), x[[i]]),]
    xsub$t1 <- xsub$t2 <- xsub$t3 <- 0
    xsub[[i]] <- 1
    xsub
})
do.call('rbind', result)  # result


On 9/13/06, Steven Van Wilgenburg <slv511 at mail.usask.ca> wrote:
> Hi,
>
> I am a new user of R and am still trying to figure out which statements
> do which functions and am looking for a jump start.
>
> I have a dataset where the data were collected as ten minute counts
> where the number of new individuals within a species was recorded as
> cohorts within 3 separate time intervals within the ten minute count
> persiod. Each row of data therefore follows a format like this:
>
>       Date    Time       Sample        Species t1 t2 t3
> June 5,2006 5:20 AM AUSFAKE01   OVEN    3  0  1
> etc.....
>
> I would like to reformat these data as if the counts recorded only
> individuals and not cohorts, so that the above would look as follows
>
>       Date    Time       Sample        Species t1 t2 t3
> June 5,2006 5:20 AM AUSFAKE01   OVEN    1  0  0
> June 5,2006 5:20 AM AUSFAKE01   OVEN    1  0  0
> June 5,2006 5:20 AM AUSFAKE01   OVEN    1  0  0
> June 5,2006 5:20 AM AUSFAKE01   OVEN    0  0  1
> etc.....
>
> I believe I could do this in SAS with IF, THEN and DO statements,
>
> e.g.
> if t1>0 then
>        do i=1 to t1
>                output Date,Time,Sample,Species,"1","0","0";
> if t2>0 then
>        do i=1 to t2
>                output Date,Time,Sample,Species,"0","1","0";
> if t3>0 then
>        do i=1 to t3
>                output Date,Time,Sample,Species,"0","0","1";
> end;
>
>
> Can anyone point me in the right direction? What is the similar
> statement to DO in R?
>
> Steve VW
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From mothsailor at googlemail.com  Thu Sep 14 00:20:05 2006
From: mothsailor at googlemail.com (David Barron)
Date: Wed, 13 Sep 2006 23:20:05 +0100
Subject: [R] inserting columns in the middle of a dataframe
In-Reply-To: <815b70590609131519w69120f5br62611827807176e7@mail.gmail.com>
References: <000901c6d779$809682c0$81c38840$@ac.uk>
	<C12E3F0A.425E5%timothy.c.bates@gmail.com>
	<815b70590609131519w69120f5br62611827807176e7@mail.gmail.com>
Message-ID: <815b70590609131520i7e549925ga274d40d986fdd81@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060913/ef37255e/attachment.pl 

From deepayan.sarkar at gmail.com  Thu Sep 14 00:20:55 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Wed, 13 Sep 2006 15:20:55 -0700
Subject: [R] wireplot margins and additional z-axis
In-Reply-To: <4508014B.7070501@stats.uwo.ca>
References: <20060911194003.283820@gmx.net>
	<eb555e660609111315h220befb0kccbd92ce5bdca390@mail.gmail.com>
	<20060912072337.19620@gmx.net>
	<eb555e660609121641p54fb959bm56ecaa1851cd6ca9@mail.gmail.com>
	<20060913080401.146390@gmx.net> <4508014B.7070501@stats.uwo.ca>
Message-ID: <eb555e660609131520s719b7cah34a6c47fb18e6fe5@mail.gmail.com>

On 9/13/06, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> On 9/13/2006 4:04 AM, Klaus Nordhausen wrote:
> > Dear Deepayan,
> >
> > sorry for not being clear - but my problem has nothing to do with the aspect. If I create the eps the following way
>
> There is some ambiguity here.  The "aspect" arg to wireframe controls
> the 3D aspect ratio.  You want to control the 2D aspect ratio in the
> displayed plot.  It is stored in the aspect.ratio member of the
> resulting object, and can be changed using
>
> plot.vol$aspect.ratio <- 1.5
>
> for example.  You shouldn't really be fiddling around inside the object
> like that, but I don't know how to avoid it in this case.

update(plot.vol, aspect = "fill")

seems to work, but I don't know if that should be considered a bug or
a feature.

I wonder what S-PLUS does (I no longer have access to a copy).

Deepayan


> Duncan Murdoch
>
> >
> > library(lattice)
> > plot.vol<- wireframe(volcano,
> >                aspect = c(1,1.5), scales=list(arrows=F),zlab=list("Z-axis",rot=90))
> >
> > postscript("example_plot_3.eps", width = 14.0/2.54, height = 19.0/2.54,
> >                 horizontal = FALSE, onefile = FALSE,paper="special")
> > trellis.par.set("axis.line",list(alpha=1,col=1,lty=1,lwd=1))
> > print(plot.vol)
> > dev.off()
> >
> > The plot is still not in the left bottom corner of the file. There is a lot of space below the outer box line. If I include this eps in latex it will also include this space and if I put for example the figure caption below it I have this huge gap between actual graph and caption.
> >
> > And for comparison, if I create with xyplot an eps like
> >
> > postscript("example_plot_4.eps", width = 14.0/2.54, height = 19.0/2.54,
> >                 horizontal = FALSE, onefile = FALSE,paper="special")
> >
> >     Depth <- equal.count(quakes$depth, number=8, overlap=.1)
> >      plot.depth<-xyplot(lat ~ long | Depth, data = quakes)
> >      update(trellis.last.object(),
> >             strip = strip.custom(strip.names = TRUE, strip.levels = TRUE),
> >             par.strip.text = list(cex = 0.75),
> >             aspect = "iso")
> >
> > print(plot.depth)
> > dev.off()
> >
> > the figure is really in the left bottom corner and included in latex has not that gap between caption and actual figure.
> >
> > I hope this describes my problem better.
> >
> > Klaus
> >
> >
> >
> >> On 9/12/06, Klaus Nordhausen <klausch at gmx.de> wrote:
> >> > Dear Deepayan,
> >> >
> >> > thanks for your reply, the change of the aspect does however not solve
> >> my problem with the space below the graph on the .eps
> >> > I attached the .eps (still with the old aspect) so that it is maybe
> >> clearer what my
> >> > problem is.
> >>
> >> No, it's not clearer; this is basically the same EPS that I got, so it
> >> gives me no new information. What do you get with the new aspect? If
> >> it's not what you want, you'll have to explain what you want more
> >> clearly. Also, don't make the panel borders transparent, as it makes
> >> it difficult to understand what's going on.
> >>
> >> Deepayan
> >>
> >> > Any other suggestions?
> >> >
> >> > Klaus
> >> >
> >> >
> >> > > > Dear R experts,
> >> > > >
> >> > > > it would be very kind if you could help me with two wireplot
> >> problems.
> >> > > >
> >> > > > First, when I make a wireplot and transform it into an .eps using
> >> the
> >> > > postscript function the eps-file leaves always a lot of space below
> >> the plot,
> >> > > as if it would leave space for a legend or something like that.
> >> > > > How can i get the plot into the bottom corner without the space
> >> below?
> >> > > The space is not there when I just display the plot in R on my screen
> >> (I use
> >> > > R.2.3.1 on Windows XP). Or in general, how can I get the margins on
> >> all
> >> > > sides as small as possible since I wnat to include the eps into a
> >> report and
> >> > > do not need the space around.
> >> > > >
> >> > > > The following code has the space on the eps:
> >> > > >
> >> > > > library(lattice)
> >> > > >  plot.vol <- wireframe(volcano, aspect = 1, scales=list(arrows=F)
> >> > > ,zlab=list("Z-axis",rot=90))
> >> > > >
> >> > >
> >> > > Perhaps you want something like
> >> > >
> >> > > aspect = c(1, 1.5)
> >> > >
> >> > > instead.
> >> > >
> >> > > > postscript("example_plot.eps", width = 14.0/2.54, height =
> >> 19.0/2.54,
> >> > > >                 horizontal = FALSE, onefile = FALSE,paper="special")
> >> > > >
> >> > > > trellis.par.set("axis.line",list(alpha=1,col=1,lty=0,lwd=1))
> >> > > >
> >> > > > print(plot.vol)
> >> > > >
> >> > > > dev.off()
> >> > > >
> >> > > >
> >> > > > Secondly, is it possible to add to the wireplot a further z-axis. I
> >> > > found only how to choose at which veritcal line I want the tickmarks
> >> and label,
> >> > > but is it also possible to have it at two vertical lines?
> >> > > >
> >> > >
> >> > > No (but it shouldn't be too hard to add that feature; I'll have to
> >> check).
> >> > >
> >> > > Deepayan
> >> > >
> >
>
>


-- 
http://www.stat.wisc.edu/~deepayan/


From christos at nuverabio.com  Thu Sep 14 00:22:29 2006
From: christos at nuverabio.com (Christos Hatzis)
Date: Wed, 13 Sep 2006 18:22:29 -0400
Subject: [R] inserting columns in the middle of a dataframe
In-Reply-To: <000901c6d779$809682c0$81c38840$@ac.uk>
Message-ID: <000901c6d783$19e0ac40$0202a8c0@headquarters.silicoinsights>

See ?append

-Christos 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jon Minton
Sent: Wednesday, September 13, 2006 5:14 PM
To: r-help at stat.math.ethz.ch
Cc: 'Jon Minton'
Subject: Re: [R] inserting columns in the middle of a dataframe

Dear R users:

 

Is there a built-in and simple way to insert new columns after other columns
in a dataframe?

 

I.e. currently I have:

 

V1 V2 V3 V4

[1,]

[2,]

Etc.

 

But I want 

                V1 V5 V2 V3 V4

[1,] 

[2,]

Etc.

 

Can this be done in one line?

 

Jon Minton

 

 


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From gunter.berton at gene.com  Thu Sep 14 00:32:37 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 13 Sep 2006 15:32:37 -0700
Subject: [R] inserting columns in the middle of a dataframe
In-Reply-To: <C12E3F0A.425E5%timothy.c.bates@gmail.com>
Message-ID: <007501c6d784$841fcf90$711f210a@gne.windows.gene.com>

Please folks -- use indexing.

myframe<-myframe[,c(1,5,2,3,4)]

Which begs the question: why bother rearranging the columns anyway, since
one can get them used, printed, etc. in any order you wish anytime you want
just by specifying the indices in the order you want them. I suspect the
question was motivated by too much Sas- or Excel -ism.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Timothy Bates
> Sent: Wednesday, September 13, 2006 3:05 PM
> To: Jon Minton; r-help at stat.math.ethz.ch
> Subject: Re: [R] inserting columns in the middle of a dataframe
> 
> 
> > Is there a built-in and simple way to insert new columns in 
> a dataframe?
> 
> You do this by collecting the columns in the new order you desire, and
> making a new frame.
> 
> oldframe           <- data.frame(matrix(0:14,ncol=3))
> newcol              <- data.frame(20:24)
> names(newcol) <- "newcol"
> newframe         <- data.frame(c(oldframe[1],newcol, oldframe[2:3]))
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From shitao at hotmail.com  Thu Sep 14 00:34:35 2006
From: shitao at hotmail.com (Tao Shi)
Date: Wed, 13 Sep 2006 22:34:35 +0000
Subject: [R] an error message with 't.test' with R under Unix
Message-ID: <BAY109-W75AF7A7DBC03D487A14BFC7280@phx.gbl>

Hi Peter,

Thank you for the reply!

Really sorry for the formating (forgot to change to plain text).  Here is basically what I'm talking about:

> R.version
         _                       
platform x86_64-unknown-linux-gnu
arch     x86_64                  
os       linux-gnu               
system   x86_64, linux-gnu       
status                           
major    2                       
minor    1.1                     
year     2005                    
month    06                      
day      20                      
language R                       
> t.test(1:6~rep(1:2,each=3))
Error in terms.formula(formula[-3]) : invalid model formula in ExtractVars
> 

Again, I don't see the error in R under Windows (R 2.3.0) or Unix (R2.3.1).  Is this the bug you were talking about?

...Tao




----------------------------------------
> To: shitao at hotmail.com
> CC: r-help at stat.math.ethz.ch
> Subject: Re: [R] an error message with 't.test' with R under Unix
> From: p.dalgaard at biostat.ku.dk
> Date: Wed, 13 Sep 2006 23:52:04 +0200
> 
> "Tao Shi" <shitao at hotmail.com> writes:
> 
> > Hi list,Could you please help me to explain the following error
>  messages with 't.test' in R Unix 2.1.1? 
> 
> This is completely unreadable! However, yes, there was at some point a
> bug where the LHS of model formulas was checked more rigorously than
> need be, using the same rules that apply to the RHS. This fact can be
> found in a recent NEWS file.
> 
> 
> 
> > I don't see it in R under Windows (R 2.3.0) or Unix (R2.3.1).  Is it really due to the different R versions?Thanks,...TaoUnix session: (R.2.1.1)========================> R.version         _                       platform x86_64-unknown-linux-gnuarch     x86_64                  os       linux-gnu               system   x86_64, linux-gnu       status                           major    2                       minor    1.1                     year     2005                    month    06                      day      20                      language R                       > t.test(extra ~ group, data = sleep)        Welch Two Sample t-testdata:  extra by group t = -1.8608, df = 17.776, p-value = 0.0794alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: -3.3654832  0.2054832 sample estimates:mean in group 1 mean in group 2        !
> >      0.75            2.33 > t.test((1:6)~rep(1:2,each=3))Error in terms.formula(formula[-3]) : invalid model formula in ExtractVars> t.test(2^(1:6)~rep(1:2,each=3))Error in terms.formula(formula[-3]) : invalid power in formula> t.test(2^extra ~ group, data = sleep)Error in terms.formula(formula[-3]) : invalid power in formulaUnix session: (R 2.3.1)============================> R.version               _                         platform       x86_64-unknown-linux-gnu  arch           x86_64                    os             linux-gnu                 system         x86_64, linux-gnu         status                                   major          2                         minor          3.1                       year           2006                      month          06                        day            01                        svn rev        38247                     language       R                         version.string Version 2.3.1 (2006-06-01)> t.test(1:6~rep(1:2,each
>  =!
> >  3))        Welch Two Sample t-testdata:  1:6 by rep(1:2, each = 3) t =
> >  -3.6742, df = 4, p-value = 0.02131alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: -5.266958 -0.733042 sample estimates:mean in group 1 mean in group 2               2               5 Windows session:===========================> R.version               _                         platform       i386-pc-mingw32           arch           i386                      os             mingw32                   system         i386, mingw32             status                                   major          2                         minor          3.0                       year           2006                      month          04                        day            24                        svn rev        37909                     language       R                         version.string Version 2.3.0 (2006-04-24)> help(t.test)> t.test(extra ~ group, data = sleep)        Welch Two Sample t-testdata:  extra by group t = -1.8608, df = 17.
>  7!
> >  76, p-value = 0.0794alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: -3.3654832  0.2054832 sample estimates:mean in group 1 mean in group 2            0.75            2.33 > t.test(2^extra ~ group, data = sleep)        Welch Two Sample t-testdata:  2^extra by group t = -1.6362, df = 10.718, p-value = 0.1308alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: -18.641557   2.773344 sample estimates:mean in group 1 mean in group 2        3.448644       11.382751 > t.test(1:6~rep(1:2,each=3))        Welch Two Sample t-testdata:  1:6 by rep(1:2, each = 3) t = -3.6742, df = 4, p-value = 0.02131alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: -5.266958 -0.733042 sample estimates:mean in group 1 mean in group 2               2               5 
> > _________________________________________________________________
> > Use Messenger to talk to your IM friends, even those on Yahoo!
> > 
> > 685ee3e858fe
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> > 
> 
> -- 
>    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907

_________________________________________________________________
Use Messenger to talk to your IM friends, even those on Yahoo!


From jm540 at york.ac.uk  Thu Sep 14 00:42:43 2006
From: jm540 at york.ac.uk (Jon Minton)
Date: Wed, 13 Sep 2006 23:42:43 +0100
Subject: [R] inserting columns in the middle of a dataframe
In-Reply-To: AAAAAF74qASRGpBDj5nAyzDyQuIEeSIA
References: <000901c6d779$809682c0$81c38840$@ac.uk>
	AAAAAF74qASRGpBDj5nAyzDyQuIEeSIA
Message-ID: <000001c6d785$ecec7720$c6c56560$@ac.uk>

Thanks, but isn't that only for elements in vectors?

I think I've found the following method to work:

e.g. for 
df <- data.frame(v1,v2,v3,v4)

use:

df <- data.frame(df[1:2],v5,df[-c(1:2)])

I *believe* this is the one-line solution I was looking for. Can anyone see
why this wouldn't work?

Jon 



-----Original Message-----
From: Christos Hatzis [mailto:christos at nuverabio.com] 
Sent: 13 September 2006 23:22
To: 'Jon Minton'; r-help at stat.math.ethz.ch
Subject: RE: [R] inserting columns in the middle of a dataframe

See ?append

-Christos 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jon Minton
Sent: Wednesday, September 13, 2006 5:14 PM
To: r-help at stat.math.ethz.ch
Cc: 'Jon Minton'
Subject: Re: [R] inserting columns in the middle of a dataframe

Dear R users:

 

Is there a built-in and simple way to insert new columns after other columns
in a dataframe?

 

I.e. currently I have:

 

V1 V2 V3 V4

[1,]

[2,]

Etc.

 

But I want 

                V1 V5 V2 V3 V4

[1,] 

[2,]

Etc.

 

Can this be done in one line?

 

Jon Minton

 

 


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From smiller73 at jhu.edu  Thu Sep 14 00:54:32 2006
From: smiller73 at jhu.edu (Steve Miller)
Date: Wed, 13 Sep 2006 17:54:32 -0500
Subject: [R] Reading fixed column format
In-Reply-To: <BAY116-DAV871B16853B20972A68EDBCF280@phx.gbl>
Message-ID: <200609132255.k8DMtCvR027215@hypatia.math.ethz.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060913/4010430b/attachment.pl 

From deepayan.sarkar at gmail.com  Thu Sep 14 00:59:06 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Wed, 13 Sep 2006 15:59:06 -0700
Subject: [R] forcing levelplot to use relative cuts (ie cuts for each
	panel)
In-Reply-To: <6.2.5.6.0.20060913090615.027f4338@ucl.ac.uk>
References: <6.2.5.6.0.20060913090615.027f4338@ucl.ac.uk>
Message-ID: <eb555e660609131559p27b67324m4ab188d0d6584025@mail.gmail.com>

On 9/13/06, Mike Townsley <uctcmkt at ucl.ac.uk> wrote:
> Dear guRus,
>
> I'm having trouble producing a levelplot with relative cuts for each
> panel (my data has large differences in scales, so I want to use
> quantiles for each panel).
>
> My attempts to change the 'at'  argument in panel.levelplot function
> have not met with success.
>
> Below is a toy example.
>
> xy <- expand.grid(x = 1:3, y = 1:3)
>
> aaa <- rbind(cbind(xy, z = 1:9, site = rep('A', 9)),
>               cbind(xy, z = (1:9)/10, site = rep('B', 9)),
>               cbind(xy, z = (1:9)*10, site = rep('C', 9)))
>
> aaa
>
> library(lattice)
> levelplot(z~x+y|site, data = aaa)         # using absolute cuts
>
> # now, attempt relative cuts
>
> levelplot(z~x+y|site, data = aaa, panel = function(...) {
>            panel.levelplot(at = quantile(z),...) })
>
> I get the following message:
> Error in panel.levelplot(at = quantile(z), ...) :
>          formal argument "at" matched by multiple actual arguments
>
> My idea was to determine the cut points each time the panel function
> is called (ie each subset of the data), but I guess this was the
> wrong thing to do.  Can someone point out what I'm missing?

Mostly that you have to catch the arguments you want to use/replace, e.g.

levelplot(z~x+y|site, data = aaa, panel = function(..., z, at) {
          panel.levelplot(..., z = z, at = quantile(z)) })

This won't actually give you want, you will need:

levelplot(z~x+y|site, data = aaa, panel = function(..., z, subscripts, at) {
          panel.levelplot(..., z = z, subscripts = subscripts, at =
quantile(z[subscripts])) })

?panel.levelplot should explain why.

Deepayan


From christos at nuverabio.com  Thu Sep 14 01:20:19 2006
From: christos at nuverabio.com (Christos Hatzis)
Date: Wed, 13 Sep 2006 19:20:19 -0400
Subject: [R] inserting columns in the middle of a dataframe
In-Reply-To: <000001c6d785$ecec7720$c6c56560$@ac.uk>
Message-ID: <001001c6d78b$2e62a300$0202a8c0@headquarters.silicoinsights>

Sorry, I guess I did not explain at all how append 
could work in a one-liner:

data.frame(df, v5)[append(1:4,5,2)]

Your method is fine as well.  The above might be more
flexible if you need a more general solution, e.g. if you wanted
to make it a function.

-Christos

-----Original Message-----
From: Jon Minton [mailto:jm540 at york.ac.uk] 
Sent: Wednesday, September 13, 2006 6:43 PM
To: christos at nuverabio.com; r-help at stat.math.ethz.ch
Cc: 'Jon Minton'
Subject: RE: [R] inserting columns in the middle of a dataframe

Thanks, but isn't that only for elements in vectors?

I think I've found the following method to work:

e.g. for
df <- data.frame(v1,v2,v3,v4)

use:

df <- data.frame(df[1:2],v5,df[-c(1:2)])

I *believe* this is the one-line solution I was looking for. Can anyone see
why this wouldn't work?

Jon 



-----Original Message-----
From: Christos Hatzis [mailto:christos at nuverabio.com]
Sent: 13 September 2006 23:22
To: 'Jon Minton'; r-help at stat.math.ethz.ch
Subject: RE: [R] inserting columns in the middle of a dataframe

See ?append

-Christos 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jon Minton
Sent: Wednesday, September 13, 2006 5:14 PM
To: r-help at stat.math.ethz.ch
Cc: 'Jon Minton'
Subject: Re: [R] inserting columns in the middle of a dataframe

Dear R users:

 

Is there a built-in and simple way to insert new columns after other columns
in a dataframe?

 

I.e. currently I have:

 

V1 V2 V3 V4

[1,]

[2,]

Etc.

 

But I want 

                V1 V5 V2 V3 V4

[1,] 

[2,]

Etc.

 

Can this be done in one line?

 

Jon Minton

 

 


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From loecher at eden.rutgers.edu  Thu Sep 14 04:32:11 2006
From: loecher at eden.rutgers.edu (Markus Loecher)
Date: Wed, 13 Sep 2006 22:32:11 -0400
Subject: [R] lme for time series prediction
Message-ID: <7.0.1.0.0.20060913222751.01e94b20@insightfromdata.com>

Could anyone give me a simple example how to use lme() for t time 
series prediction/modeling ? I understand the concept of longitudinal 
data and have read the book by Pinheiro but still have a difficult 
time for the (simpler) case of no grouped data. I am dealing with the 
case of predicting a scalar from another multivariate time series.

Thanks !

Mark


From A.Robinson at ms.unimelb.edu.au  Thu Sep 14 04:44:19 2006
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Thu, 14 Sep 2006 12:44:19 +1000
Subject: [R] lme for time series prediction
In-Reply-To: <7.0.1.0.0.20060913222751.01e94b20@insightfromdata.com>
References: <7.0.1.0.0.20060913222751.01e94b20@insightfromdata.com>
Message-ID: <20060914024419.GU12212@ms.unimelb.edu.au>

Hello Mark,

it's quite possible that someone can do this.  However, you should try
to help us as much as possible by providing commented, minimal,
self-contained, reproducible code.  What are you trying to do? In what
way is it not working?  Etc.

Cheers

Andrew

On Wed, Sep 13, 2006 at 10:32:11PM -0400, Markus Loecher wrote:
> Could anyone give me a simple example how to use lme() for t time 
> series prediction/modeling ? I understand the concept of longitudinal 
> data and have read the book by Pinheiro but still have a difficult 
> time for the (simpler) case of no grouped data. I am dealing with the 
> case of predicting a scalar from another multivariate time series.
> 
> Thanks !
> 
> Mark
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
Email: a.robinson at ms.unimelb.edu.au         http://www.ms.unimelb.edu.au


From gdunkel at mindspring.com  Thu Sep 14 05:01:19 2006
From: gdunkel at mindspring.com (Greg Dunkel)
Date: Thu, 14 Sep 2006 03:01:19 -0000
Subject: [R] building R-2.3.1 on Solaris 10
Message-ID: <44DFE77B.8090009@mindspring.com>

Sun has modified the standard gcc to provide better support for Sun's 
thread.  They used it to build OpenSolaris and it is available under the 
standard GPL license, ie, it is free.

I could build R-2.2.1 with the standard gcc, but I needed to use the gcc 
from Sun to build 2.3.x.

I am not entirely happy with R-2.3.0 because some of the demo(graphics) 
had their titles clipped, but I want to see how it works in practice. It 
did pass make check, but I didn't invoke the more strenuous  dev checks.

/greg


From AnupTyagi at yahoo.com  Thu Sep 14 07:08:34 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Thu, 14 Sep 2006 05:08:34 +0000 (UTC)
Subject: [R] Access Rows in a Data Frame by Row Name
References: <4508477B.8000203@acm.org>
	<004801c6d768$712d1b70$711f210a@gne.windows.gene.com>
Message-ID: <loom.20060914T064915-927@post.gmane.org>

I hope this helps.

> x <- data.frame(a=1:5, b=6:10, d=11:15)
> x
  a  b  d
1 1  6 11
2 2  7 12
3 3  8 13
4 4  9 14
5 5 10 15
> # access row with name "a". This does not work.
> x$a
[1] 1 2 3 4 5
> # access column with name "d"
> x$d
[1] 11 12 13 14 15
> x$row.names
NULL
> attributes(x)
$names
[1] "a" "b" "d"

$row.names
[1] "1" "2" "3" "4" "5"

$class
[1] "data.frame"

> x$row.names()
Error: attempt to apply non-function
> row.names(x)
[1] "1" "2" "3" "4" "5"
> # access first row by index
> x[1,]
  a b  d
1 1 6 11
> # access first row by "name"
> x["1",]
  a b  d
1 1 6 11
> # access second row by "name"
> x["2",]
  a b  d
2 2 7 12
> # change row names to alphabets.
> row.names(x) <- c("a","b","c","d","e")
> row.names(x)
[1] "a" "b" "c" "d" "e"
> # access second row by old name. Does not work because of name change. 
> Why this does not give error: "2" row name does not exist?
> x["2",]
    a  b  d
NA NA NA NA
> # access third row by "name".
> x["c",]
  a b  d
c 3 8 13


From AnupTyagi at yahoo.com  Thu Sep 14 07:14:26 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Thu, 14 Sep 2006 05:14:26 +0000 (UTC)
Subject: [R] inserting columns in the middle of a dataframe
References: <000001c6d785$ecec7720$c6c56560$@ac.uk>
	<001001c6d78b$2e62a300$0202a8c0@headquarters.silicoinsights>
Message-ID: <loom.20060914T071059-561@post.gmane.org>

I think it should be possible to create the column at the end and then use
"order" on the columns names and indexes to only change the order of column
indexes, rather than having to do operations on the data itself (which will be
very time consuming if the dataset is large). Perhaps people with better R
skills can suggest how to code this. Anupam.


From AnupTyagi at yahoo.com  Thu Sep 14 07:30:55 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Thu, 14 Sep 2006 05:30:55 +0000 (UTC)
Subject: [R] Dear FE  Harrell How can I get rreport ?
References: <4508286D.1090305@vanderbilt.edu>
	<20060913195033.51077.qmail@web25708.mail.ukl.yahoo.com>
Message-ID: <loom.20060914T073026-853@post.gmane.org>

justin bem <justin_bem <at> yahoo.fr> writes:

> 
> Mr Harrell,
> 
>  After reading discussion about R output and SAS output , I will like to use
rreport package. I a windows XP
> user 
> 
>  Sincerly

See:

http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/Rreport

Anupam.


From robi.ragan at gmail.com  Thu Sep 14 07:33:19 2006
From: robi.ragan at gmail.com (Robi Ragan)
Date: Thu, 14 Sep 2006 01:33:19 -0400
Subject: [R] Adding predicted values as a new variable in a data frame
Message-ID: <5dedcf4c0609132233s52c8e13y1329ecc2dea400cb@mail.gmail.com>

I am running a regression:

ols.reg1 <- lm(y ~ x1 + x2 + x3 + x4)

on a data.frame

and then generating fitted values:

y.hat <- ols.reg1$fitted.values

Then I would like to add these fitted values to the data.frame as a
new variable. The problem is that when the values are predicted the
resulting output has too few rows. for some reason certian
observations do not get predicted values. So this shrinks the column
down and I then cannot combine the output into the original
data.frame.

If someone could please help I would apreciate it. Stata automatically
adds a new column to the data set when you find the fitted values. So
having to fight with R just to do something I used to routimely do has
made me think of turning back to the dark side. I hope I have just
missed something trival in all the help files I have been
looking through.

Thanks,


--

Robi Ragan
Graduate Student
Department of Economics
Department of Political Science
The University of Georgia


From ggrothendieck at gmail.com  Thu Sep 14 08:04:38 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 14 Sep 2006 02:04:38 -0400
Subject: [R] Adding predicted values as a new variable in a data frame
In-Reply-To: <5dedcf4c0609132233s52c8e13y1329ecc2dea400cb@mail.gmail.com>
References: <5dedcf4c0609132233s52c8e13y1329ecc2dea400cb@mail.gmail.com>
Message-ID: <971536df0609132304y49e17232t588edcfd86f31620@mail.gmail.com>

Specify na.action = na.exlude, e.g.

> x <- y <- 1:10; x[5] <- NA
> fitted(lm(y ~ x, na.action = na.exclude))
 1  2  3  4  5  6  7  8  9 10
 1  2  3  4 NA  6  7  8  9 10

On 9/14/06, Robi Ragan <robi.ragan at gmail.com> wrote:
> I am running a regression:
>
> ols.reg1 <- lm(y ~ x1 + x2 + x3 + x4)
>
> on a data.frame
>
> and then generating fitted values:
>
> y.hat <- ols.reg1$fitted.values
>
> Then I would like to add these fitted values to the data.frame as a
> new variable. The problem is that when the values are predicted the
> resulting output has too few rows. for some reason certian
> observations do not get predicted values. So this shrinks the column
> down and I then cannot combine the output into the original
> data.frame.
>
> If someone could please help I would apreciate it. Stata automatically
> adds a new column to the data set when you find the fitted values. So
> having to fight with R just to do something I used to routimely do has
> made me think of turning back to the dark side. I hope I have just
> missed something trival in all the help files I have been
> looking through.
>
> Thanks,
>
>
> --
>
> Robi Ragan
> Graduate Student
> Department of Economics
> Department of Political Science
> The University of Georgia
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From nmi13 at ext.canterbury.ac.nz  Thu Sep 14 08:16:31 2006
From: nmi13 at ext.canterbury.ac.nz (nmi13)
Date: Thu, 14 Sep 2006 18:16:31 +1200
Subject: [R] Rv generation
Message-ID: <4509513B@webmail>

Hi,
 Can Someone inform me how to generate RV's using the below CDF, by inverse 
technique.
Thanks for your help and time.

My CDF is as follows
\[
F(x)=0 \ \text{if} \  x < 0\]\[
F(x)=\{\frac{x-x_i}{x_{i+1}-x_{i}}*(p_{i+1}-p_{i})\}+p_{i}\
\forall \ x_{i}\leq x < x_{i+1} \]
\[ F(x)=1 \ \text{if} \  x > x_{i+1}
\]
Regards
Murthy


From ripley at stats.ox.ac.uk  Thu Sep 14 08:42:15 2006
From: ripley at stats.ox.ac.uk (Prof Brian D Ripley)
Date: Thu, 14 Sep 2006 07:42:15 +0100 (GMT Daylight Time)
Subject: [R] Adding predicted values as a new variable in a data frame
In-Reply-To: <5dedcf4c0609132233s52c8e13y1329ecc2dea400cb@mail.gmail.com>
References: <5dedcf4c0609132233s52c8e13y1329ecc2dea400cb@mail.gmail.com>
Message-ID: <Pine.WNT.4.64.0609140729150.3500@Petrel>

?na.exclude should help you: my guess is that you asked (by using the 
default) na.action = na.omit) for rows with missing values to be excluded 
from the residuals. But since you have not mentioned missing values, we 
have to guess what 'for some reason' was: please note the footer of this 
messag.

On Thu, 14 Sep 2006, Robi Ragan wrote:

> I am running a regression:
>
> ols.reg1 <- lm(y ~ x1 + x2 + x3 + x4)
>
> on a data.frame

Hmm, no data frame is mentioned: you want a data= argument.

> and then generating fitted values:
>
> y.hat <- ols.reg1$fitted.values

Please use the accessor functions and not dive into the internal details, 
e.g.

y.hat <- fitted(ols.reg1)

BTW: where did you get the use of ols.reg1$fitted.values from?

> Then I would like to add these fitted values to the data.frame as a
> new variable. The problem is that when the values are predicted the
> resulting output has too few rows. for some reason certian
> observations do not get predicted values. So this shrinks the column
> down and I then cannot combine the output into the original
> data.frame.

fit <- lm(formula, data=data_frame, na.action=na.exclude)
data_frame$fitted <- fitted(fit)

> If someone could please help I would apreciate it. Stata automatically
> adds a new column to the data set when you find the fitted values. So
> having to fight with R just to do something I used to routimely do has
> made me think of turning back to the dark side. I hope I have just
> missed something trival in all the help files I have been
> looking through.

The above looks trivial to me.  It was not in R or S when lm was first 
introduced (1991 White Book), but was added last century (thanks to the 
ideas and persistent advocacy of Terry Therneau).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From p.dalgaard at biostat.ku.dk  Thu Sep 14 09:19:23 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 14 Sep 2006 09:19:23 +0200
Subject: [R] an error message with 't.test' with R under Unix
In-Reply-To: <BAY109-W75AF7A7DBC03D487A14BFC7280@phx.gbl>
References: <BAY109-W75AF7A7DBC03D487A14BFC7280@phx.gbl>
Message-ID: <x2hczazzic.fsf@turmalin.kubism.ku.dk>

"Tao Shi" <shitao at hotmail.com> writes:

> Again, I don't see the error in R under Windows (R 2.3.0) or Unix (R2.3.1).  Is this the bug you were talking about?


                CHANGES IN R VERSION 2.3.0
....
    o   Some of the classical tests put unnecessary restrictions on the
        LHS in the formula interface (e.g., t.test(x+y ~ g) was not
        allowed).



-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From dimitris.rizopoulos at med.kuleuven.be  Wed Sep 13 14:24:12 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 13 Sep 2006 14:24:12 +0200
Subject: [R] Conservative "ANOVA tables" in lmer
References: <1A8FE5B0-3FE3-4CA9-9224-9CACB9AF4854@muohio.edu><40e66e0b0609070559u656463c1l52a60eda9a095237@mail.gmail.com><20060910055624.GA12212@ms.unimelb.edu.au><40e66e0b0609110943h7dfb057dv12567975ac04bdb2@mail.gmail.com><1158010468.3278.24.camel@solidago.localdomain><48130.220.237.183.166.1158098663.squirrel@webmail.ms.unimelb.edu.au>
	<1158145458.3699.3.camel@solidago.localdomain>
Message-ID: <01a701c6d72f$855eb8d0$0540210a@www.domain>


----- Original Message ----- 
From: "Manuel Morales" <Manuel.A.Morales at williams.edu>
To: <A.Robinson at ms.unimelb.edu.au>
Cc: "Douglas Bates" <bates at stat.wisc.edu>; "Manuel Morales" 
<Manuel.A.Morales at williams.edu>; <r-help at stat.math.ethz.ch>
Sent: Wednesday, September 13, 2006 1:04 PM
Subject: Re: [R] Conservative "ANOVA tables" in lmer


> On Wed, 2006-09-13 at 08:04 +1000, Andrew Robinson wrote:
>> On Tue, September 12, 2006 7:34 am, Manuel Morales wrote:
>> > On Mon, 2006-09-11 at 11:43 -0500, Douglas Bates wrote:
>> >> Having made that offer I think I will now withdraw it.  Peter's
>> >> example has convinced me that this is the wrong thing to do.
>> >>
>> >> I am encouraged by the fact that the results from mcmcsamp 
>> >> correspond
>> >> closely to the correct theoretical results in the case that 
>> >> Peter
>> >> described.  I appreciate that some users will find it difficult 
>> >> to
>> >> work with a MCMC sample (or to convince editors to accept 
>> >> results
>> >> based on such a sample) but I think that these results indicate 
>> >> that
>> >> it is better to go after the marginal distribution of the fixed
>> >> effects estimates (which is what is being approximated by the 
>> >> MCMC
>> >> sample - up to Bayesian/frequentist philosophical differences) 
>> >> than to
>> >> use the conditional distribution and somehow try to adjust the
>> >> reference distribution.
>> >
>> > Am I right that the MCMC sample can not be used, however, to 
>> > evaluate
>> > the significance of parameter groups. For example, to assess the
>> > significance of a three-level factor? Are there better 
>> > alternatives than
>> > simply adjusting the CI for the number of factor levels
>> > (1-alpha/levels).
>>
>> I wonder whether the likelihood ratio test would be suitable here? 
>> That
>> seems to be supported.  It just takes a little longer.
>>
>> > require(lme4)
>> > data(sleepstudy)
>> > fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
>> > fm2 <- lmer(Reaction ~ Days + I(Days^2) + (Days|Subject), 
>> > sleepstudy)
>> > anova(fm1, fm2)
>>
>> So, a brief overview of the popular inferential needs and solutions 
>> would
>> then be:
>>
>> 1) Test the statistical significance of one or more fixed or random
>> effects - fit a model with and a model without the terms, and use 
>> the LRT.
>
> I believe that the LRT is anti-conservative for fixed effects, as
> described in Pinheiro and Bates companion book to NLME.
>

You have this effect if you're using REML, for ML I don't think there 
is any problem to use LRT between nested models with different 
fixed-effects structure.

Best,
Dimitris


>> 2) Obtain confidence intervals for one or more fixed or random 
>> effects -
>> use mcmcsamp
>>
>> Did I miss anything important? - What else would people like to do?
>>
>> Cheers
>>
>> Andrew
>>
>> Andrew Robinson
>> Senior Lecturer in Statistics                       Tel: 
>> +61-3-8344-9763
>> Department of Mathematics and Statistics            Fax: +61-3-8344 
>> 4599
>> University of Melbourne, VIC 3010 Australia
>> Email: a.robinson at ms.unimelb.edu.au    Website: 
>> http://www.ms.unimelb.edu.au
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From bates at stat.wisc.edu  Wed Sep 13 17:02:43 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 13 Sep 2006 10:02:43 -0500
Subject: [R] Conservative "ANOVA tables" in lmer
In-Reply-To: <01a701c6d72f$855eb8d0$0540210a@www.domain>
References: <1A8FE5B0-3FE3-4CA9-9224-9CACB9AF4854@muohio.edu>
	<40e66e0b0609070559u656463c1l52a60eda9a095237@mail.gmail.com>
	<20060910055624.GA12212@ms.unimelb.edu.au>
	<40e66e0b0609110943h7dfb057dv12567975ac04bdb2@mail.gmail.com>
	<1158010468.3278.24.camel@solidago.localdomain>
	<48130.220.237.183.166.1158098663.squirrel@webmail.ms.unimelb.edu.au>
	<1158145458.3699.3.camel@solidago.localdomain>
	<01a701c6d72f$855eb8d0$0540210a@www.domain>
Message-ID: <40e66e0b0609130802u34f4f942x4a20ed6d8bbd8298@mail.gmail.com>

On 9/13/06, Dimitris Rizopoulos <dimitris.rizopoulos at med.kuleuven.be> wrote:
>
> ----- Original Message -----
> From: "Manuel Morales" <Manuel.A.Morales at williams.edu>
> To: <A.Robinson at ms.unimelb.edu.au>
> Cc: "Douglas Bates" <bates at stat.wisc.edu>; "Manuel Morales"
> <Manuel.A.Morales at williams.edu>; <r-help at stat.math.ethz.ch>
> Sent: Wednesday, September 13, 2006 1:04 PM
> Subject: Re: [R] Conservative "ANOVA tables" in lmer
>
>
> > On Wed, 2006-09-13 at 08:04 +1000, Andrew Robinson wrote:
> >> On Tue, September 12, 2006 7:34 am, Manuel Morales wrote:
> >> > On Mon, 2006-09-11 at 11:43 -0500, Douglas Bates wrote:
> >> >> Having made that offer I think I will now withdraw it.  Peter's
> >> >> example has convinced me that this is the wrong thing to do.
> >> >>
> >> >> I am encouraged by the fact that the results from mcmcsamp
> >> >> correspond
> >> >> closely to the correct theoretical results in the case that
> >> >> Peter
> >> >> described.  I appreciate that some users will find it difficult
> >> >> to
> >> >> work with a MCMC sample (or to convince editors to accept
> >> >> results
> >> >> based on such a sample) but I think that these results indicate
> >> >> that
> >> >> it is better to go after the marginal distribution of the fixed
> >> >> effects estimates (which is what is being approximated by the
> >> >> MCMC
> >> >> sample - up to Bayesian/frequentist philosophical differences)
> >> >> than to
> >> >> use the conditional distribution and somehow try to adjust the
> >> >> reference distribution.
> >> >
> >> > Am I right that the MCMC sample can not be used, however, to
> >> > evaluate
> >> > the significance of parameter groups. For example, to assess the
> >> > significance of a three-level factor? Are there better
> >> > alternatives than
> >> > simply adjusting the CI for the number of factor levels
> >> > (1-alpha/levels).
> >>
> >> I wonder whether the likelihood ratio test would be suitable here?
> >> That
> >> seems to be supported.  It just takes a little longer.
> >>
> >> > require(lme4)
> >> > data(sleepstudy)
> >> > fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
> >> > fm2 <- lmer(Reaction ~ Days + I(Days^2) + (Days|Subject),
> >> > sleepstudy)
> >> > anova(fm1, fm2)
> >>
> >> So, a brief overview of the popular inferential needs and solutions
> >> would
> >> then be:
> >>
> >> 1) Test the statistical significance of one or more fixed or random
> >> effects - fit a model with and a model without the terms, and use
> >> the LRT.
> >
> > I believe that the LRT is anti-conservative for fixed effects, as
> > described in Pinheiro and Bates companion book to NLME.
> >
>
> You have this effect if you're using REML, for ML I don't think there
> is any problem to use LRT between nested models with different
> fixed-effects structure.

There are two issues here: how the test statistic is evaluated and
what reference distribution is used for the test statistic (i.e. how
do you convert a value of the test statistic to a p-value).  Manuel is
addressing the latter issue.  If you compare the difference of
-2*logLik for the models to a chi-square with degrees of freedom
determined by the difference in the number of parameters the test will
be anti-conservative when the number of observations is small.  The
use of the chi-square as a reference distribution is based on
asymptotic properties.

The other question is how does one evaluate the likelihood-ratio test
statistic and that is the issue that Dimitris is addressing.  The REML
criterion is a modified likelihood and it is inappropriate to look at
differences in the REML criterion when the models being compared have
different fixed-effects specifications, or even a different
parameterization of the fixed effects.  However, the anova method for
an lmer object does not use the REML criterion even when the model has
been estimated by REML.  It uses the profiled log-likelihood evaluated
at the REML estimates of the relative variances of the random effects.
 That's a complicated statement so let me break it down.

The optimization in lmer is done with respect to the elements of the
variance-covariance matrix of the random effects relative to
$\sigma^2$.  Given these values the conditional estimates of the
fixed-effects parameters and of $\sigma^2$ can be evaluated directly
with some linear algebra.  In the summary or show output of an lmer
model there are two quantities called the MLdeviance and the
REMLdeviance.  Those are based on the same relative variances but
different conditional estimates of $\sigma^2$ (and hence different
estimates of the elements of the variance-covariance of the random
effects).  It turns out that there is very little difference in the
value of the profiled log-likelihood at the ML estimates and at the
REML estimates.  This is not to say that the log-likelihood is similar
at the two (complete) sets of estimates - it is the profiled
log-likelihoods that are similar and these are what are used to create
the likelihood ratio test statistic, even when the models have been
fit by REML.

As I said, this is complicated and until I went to reply to this
message I hadn't really sorted it out for myself.  I knew the
empirical result, which I had checked before releasing the code, but I
hadn't worked out the details of why it occurred.  This discussion has
been very helpful to me at least.

If the explanation is too complicated, I suggest trying an example.
Run the code that Andrew sent and look at the logLik that is quoted in
the anova test.  It is a log-likelihood not a REML criterion.

Now try

> logLik(fm1ML <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy, method = "ML"))
'log Lik.' -875.9697 (df=5)
> logLik(fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy), REML = FALSE)
'log Lik.' -875.993 (df=5)

and

> logLik(fm2ML <- lmer(Reaction ~ Days + I(Days^2) + (Days|Subject), sleepstudy, method = "ML"))
'log Lik.' -875.1408 (df=6)
> logLik(fm2 <- lmer(Reaction ~ Days + I(Days^2) + (Days|Subject), sleepstudy), REML = FALSE)
'log Lik.' -875.1588 (df=6)

Thanks to everyone for there comments on this issue.

> >> 2) Obtain confidence intervals for one or more fixed or random
> >> effects -
> >> use mcmcsamp
> >>
> >> Did I miss anything important? - What else would people like to do?
> >>
> >> Cheers
> >>
> >> Andrew
> >>
> >> Andrew Robinson
> >> Senior Lecturer in Statistics                       Tel:
> >> +61-3-8344-9763
> >> Department of Mathematics and Statistics            Fax: +61-3-8344
> >> 4599
> >> University of Melbourne, VIC 3010 Australia
> >> Email: a.robinson at ms.unimelb.edu.au    Website:
> >> http://www.ms.unimelb.edu.au
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
>
>


From bates at stat.wisc.edu  Wed Sep 13 17:06:49 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 13 Sep 2006 10:06:49 -0500
Subject: [R] Conservative "ANOVA tables" in lmer
In-Reply-To: <01a701c6d72f$855eb8d0$0540210a@www.domain>
References: <1A8FE5B0-3FE3-4CA9-9224-9CACB9AF4854@muohio.edu>
	<40e66e0b0609070559u656463c1l52a60eda9a095237@mail.gmail.com>
	<20060910055624.GA12212@ms.unimelb.edu.au>
	<40e66e0b0609110943h7dfb057dv12567975ac04bdb2@mail.gmail.com>
	<1158010468.3278.24.camel@solidago.localdomain>
	<48130.220.237.183.166.1158098663.squirrel@webmail.ms.unimelb.edu.au>
	<1158145458.3699.3.camel@solidago.localdomain>
	<01a701c6d72f$855eb8d0$0540210a@www.domain>
Message-ID: <40e66e0b0609130806i43bce07dr7afb7ab215a2aaa7@mail.gmail.com>

On 9/13/06, Dimitris Rizopoulos <dimitris.rizopoulos at med.kuleuven.be> wrote:
>
> ----- Original Message -----
> From: "Manuel Morales" <Manuel.A.Morales at williams.edu>
> To: <A.Robinson at ms.unimelb.edu.au>
> Cc: "Douglas Bates" <bates at stat.wisc.edu>; "Manuel Morales"
> <Manuel.A.Morales at williams.edu>; <r-help at stat.math.ethz.ch>
> Sent: Wednesday, September 13, 2006 1:04 PM
> Subject: Re: [R] Conservative "ANOVA tables" in lmer
>
>
> > On Wed, 2006-09-13 at 08:04 +1000, Andrew Robinson wrote:
> >> On Tue, September 12, 2006 7:34 am, Manuel Morales wrote:
> >> > On Mon, 2006-09-11 at 11:43 -0500, Douglas Bates wrote:
> >> >> Having made that offer I think I will now withdraw it.  Peter's
> >> >> example has convinced me that this is the wrong thing to do.
> >> >>
> >> >> I am encouraged by the fact that the results from mcmcsamp
> >> >> correspond
> >> >> closely to the correct theoretical results in the case that
> >> >> Peter
> >> >> described.  I appreciate that some users will find it difficult
> >> >> to
> >> >> work with a MCMC sample (or to convince editors to accept
> >> >> results
> >> >> based on such a sample) but I think that these results indicate
> >> >> that
> >> >> it is better to go after the marginal distribution of the fixed
> >> >> effects estimates (which is what is being approximated by the
> >> >> MCMC
> >> >> sample - up to Bayesian/frequentist philosophical differences)
> >> >> than to
> >> >> use the conditional distribution and somehow try to adjust the
> >> >> reference distribution.
> >> >
> >> > Am I right that the MCMC sample can not be used, however, to
> >> > evaluate
> >> > the significance of parameter groups. For example, to assess the
> >> > significance of a three-level factor? Are there better
> >> > alternatives than
> >> > simply adjusting the CI for the number of factor levels
> >> > (1-alpha/levels).
> >>
> >> I wonder whether the likelihood ratio test would be suitable here?
> >> That
> >> seems to be supported.  It just takes a little longer.
> >>
> >> > require(lme4)
> >> > data(sleepstudy)
> >> > fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
> >> > fm2 <- lmer(Reaction ~ Days + I(Days^2) + (Days|Subject),
> >> > sleepstudy)
> >> > anova(fm1, fm2)
> >>
> >> So, a brief overview of the popular inferential needs and solutions
> >> would
> >> then be:
> >>
> >> 1) Test the statistical significance of one or more fixed or random
> >> effects - fit a model with and a model without the terms, and use
> >> the LRT.
> >
> > I believe that the LRT is anti-conservative for fixed effects, as
> > described in Pinheiro and Bates companion book to NLME.
> >
>
> You have this effect if you're using REML, for ML I don't think there
> is any problem to use LRT between nested models with different
> fixed-effects structure.

There are two issues here: how the test statistic is evaluated and
what reference distribution is used for the test statistic (i.e. how
do you convert a value of the test statistic to a p-value).  Manuel is
addressing the latter issue.  If you compare the difference of
-2*logLik for the models to a chi-square with degrees of freedom
determined by the difference in the number of parameters the test will
be anti-conservative when the number of observations is small.  The
use of the chi-square as a reference distribution is based on
asymptotic properties.

The other question is how does one evaluate the likelihood-ratio test
statistic and that is the issue that Dimitris is addressing.  The REML
criterion is a modified likelihood and it is inappropriate to look at
differences in the REML criterion when the models being compared have
different fixed-effects specifications, or even a different
parameterization of the fixed effects.  However, the anova method for
an lmer object does not use the REML criterion even when the model has
been estimated by REML.  It uses the profiled log-likelihood evaluated
at the REML estimates of the relative variances of the random effects.
 That's a complicated statement so let me break it down.

The optimization in lmer is done with respect to the elements of the
variance-covariance matrix of the random effects relative to
$\sigma^2$.  Given these values the conditional estimates of the
fixed-effects parameters and of $\sigma^2$ can be evaluated directly
with some linear algebra.  In the summary or show output of an lmer
model there are two quantities called the MLdeviance and the
REMLdeviance.  Those are based on the same relative variances but
different conditional estimates of $\sigma^2$ (and hence different
estimates of the elements of the variance-covariance of the random
effects).  It turns out that there is very little difference in the
value of the profiled log-likelihood at the ML estimates and at the
REML estimates.  This is not to say that the log-likelihood is similar
at the two (complete) sets of estimates - it is the profiled
log-likelihoods that are similar and these are what are used to create
the likelihood ratio test statistic, even when the models have been
fit by REML.

As I said, this is complicated and until I went to reply to this
message I hadn't really sorted it out for myself.  I knew the
empirical result, which I had checked before releasing the code, but I
hadn't worked out the details of why it occurred.  This discussion has
been very helpful to me at least.

If the explanation is too complicated, I suggest trying an example.
Run the code that Andrew sent and look at the logLik that is quoted in
the anova test.  It is a log-likelihood not a REML criterion.

Now try

> logLik(fm1ML <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy, method = "ML"))
'log Lik.' -875.9697 (df=5)
> logLik(fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy), REML = FALSE)
'log Lik.' -875.993 (df=5)

and

> logLik(fm2ML <- lmer(Reaction ~ Days + I(Days^2) + (Days|Subject), sleepstudy, method = "ML"))
'log Lik.' -875.1408 (df=6)
> logLik(fm2 <- lmer(Reaction ~ Days + I(Days^2) + (Days|Subject), sleepstudy), REML = FALSE)
'log Lik.' -875.1588 (df=6)

Thanks to everyone for there comments on this issue.

> >> 2) Obtain confidence intervals for one or more fixed or random
> >> effects -
> >> use mcmcsamp
> >>
> >> Did I miss anything important? - What else would people like to do?
> >>
> >> Cheers
> >>
> >> Andrew
> >>
> >> Andrew Robinson
> >> Senior Lecturer in Statistics                       Tel:
> >> +61-3-8344-9763
> >> Department of Mathematics and Statistics            Fax: +61-3-8344
> >> 4599
> >> University of Melbourne, VIC 3010 Australia
> >> Email: a.robinson at ms.unimelb.edu.au    Website:
> >> http://www.ms.unimelb.edu.au
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
>
>


From rmc095 at bham.ac.uk  Thu Sep 14 10:23:13 2006
From: rmc095 at bham.ac.uk (Russell Compton)
Date: Thu, 14 Sep 2006 09:23:13 +0100
Subject: [R] ANOVA in R
Message-ID: <000d01c6d7d7$0538b220$6600000a@D6MKM61J>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060914/a1c26f14/attachment.pl 

From rmc095 at bham.ac.uk  Thu Sep 14 10:25:42 2006
From: rmc095 at bham.ac.uk (Russell Compton)
Date: Thu, 14 Sep 2006 09:25:42 +0100
Subject: [R] ANOVA in R
Message-ID: <001201c6d7d7$5e359c30$6600000a@D6MKM61J>

Despite having used R on a daily basis for the past two years, I?m
encountering some difficulty performing an ANOVA on my data. What I?m trying
to do is the following:
?
Given data such as:
?
Day 1??? Day 4??? Day 8
2????????? 7????????? 2
3????????? 2????????? 8????????? 
3????????? 4????????? 7
6????????? 6????????? 8
1????????? 3????????? 4
?
I want to use ANOVA to determine if there is a significant change over the
three days. In other stats packages I have used, I can just select this data
and run the ANOVA function and get the F and p values. However in R, the
anova function seems to only work with a fitted model, eg. Linear
regression. This function seems to assume there is a relationship such as
day1~ day 4 + day 8, but in my case there isn?t ? I just want to perform an
ANOVA without regression. If anyone could point me in the right direction
I?d greatly appreciate it,
?
Thanks


From A.Robinson at ms.unimelb.edu.au  Thu Sep 14 11:05:15 2006
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Thu, 14 Sep 2006 19:05:15 +1000
Subject: [R] ANOVA in R
In-Reply-To: <000d01c6d7d7$0538b220$6600000a@D6MKM61J>
References: <000d01c6d7d7$0538b220$6600000a@D6MKM61J>
Message-ID: <20060914090515.GC12212@ms.unimelb.edu.au>



Try

test <- data.frame(day.1=c(2,3,3,6,1), 
     day.4=c(7,2,4,6,3),
     day.8=c(2,8,7,8,4))

test

test.long <- reshape(test, direction="long", 
	  varying=c("day.1","day.4","day.8"),
	  v.names="response",
	  timevar="day",
	  times=names(test))

test.long$day <- factor(test.long$day)

test.long

aov(response ~ day, data=test.long)


I hope that this helps,

Andrew


On Thu, Sep 14, 2006 at 09:23:13AM +0100, Russell Compton wrote:
> Despite having used R on a daily basis for the past two years, I'm
> encountering some difficulty performing an ANOVA on my data. What I'm trying
> to do is the following:
> 
>  
> 
> Given data such as:
> 
>  
> 
> Day 1    Day 4    Day 8
> 
> 2          7          2
> 
> 3          2          8          
> 
> 3          4          7
> 
> 6          6          8
> 
> 1          3          4
> 
>  
> 
> I want to use ANOVA to determine if there is a significant change over the
> three days. In other stats packages I have used, I can just select this data
> and run the ANOVA function and get the F and p values. However in R, the
> anova function seems to only work with a fitted model, eg. Linear
> regression. This function seems to assume there is a relationship such as
> day1~ day 4 + day 8, but in my case there isn't - I just want to perform an
> ANOVA without regression. If anyone could point me in the right direction
> I'd greatly appreciate it,
> 
>  
> 
> Thanks
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
Email: a.robinson at ms.unimelb.edu.au         http://www.ms.unimelb.edu.au


From johannes.jenkner at env.ethz.ch  Thu Sep 14 11:18:30 2006
From: johannes.jenkner at env.ethz.ch (Johannes Jenkner)
Date: Thu, 14 Sep 2006 11:18:30 +0200
Subject: [R] R input output error
Message-ID: <45091E66.3070109@env.ethz.ch>

Dear R users!

I have some problems with some cronjobs containing R programs in batch 
mode. The CPU  load always is quite high, as I plot some weather charts 
which require extensive interpolation procedures. The crucial point is 
that I frequently get "R input/output" errors, if my linux PC operates 
at full capacity. I am curious, if anyone of you has similar problems. 
Is there a possibility to prevent those runtime errors?

Thanks in advance!

Johannes

-- 
Johannes Jenkner
Institute for Atmospheric and Climate Science
ETH Zuerich
Universit?tsstrasse 16
ETH Zentrum, CHN M18
CH-8092 Zuerich
phone: +41/44/6332773
www: http://www.iac.ethz.ch


From p.dalgaard at biostat.ku.dk  Thu Sep 14 11:25:32 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 14 Sep 2006 11:25:32 +0200
Subject: [R] ANOVA in R
In-Reply-To: <20060914090515.GC12212@ms.unimelb.edu.au>
References: <000d01c6d7d7$0538b220$6600000a@D6MKM61J>
	<20060914090515.GC12212@ms.unimelb.edu.au>
Message-ID: <x27j06sstv.fsf@viggo.kubism.ku.dk>

Andrew Robinson <A.Robinson at ms.unimelb.edu.au> writes:

> Try
> 
> test <- data.frame(day.1=c(2,3,3,6,1), 
>      day.4=c(7,2,4,6,3),
>      day.8=c(2,8,7,8,4))
> 
> test
> 
> test.long <- reshape(test, direction="long", 
> 	  varying=c("day.1","day.4","day.8"),
> 	  v.names="response",
> 	  timevar="day",
> 	  times=names(test))
> 
> test.long$day <- factor(test.long$day)
> 
> test.long
> 
> aov(response ~ day, data=test.long)


Was a one-way ANOVA intended? He never said.

On a more elementary level,

y <- with(test, c(day.1,day.4,day.8))
day <- factor(rep(c(1,4,8),each=5)) # or gl(3,5,labels=c(1,4,8))
sub <- factor(rep(1:5,3))           # or gl(5,1,15)

print(data.frame(y,day,sub)) # just to show the point

anova(lm(y~day))        # 1-way
anova(lm(y~day+sub))    # 2-way

# This could be better for unbalanced designs:

drop1(lm(y~day+sub),test="F")

 

 
> 
> I hope that this helps,
> 
> Andrew
> 
> 
> On Thu, Sep 14, 2006 at 09:23:13AM +0100, Russell Compton wrote:
> > Despite having used R on a daily basis for the past two years, I'm
> > encountering some difficulty performing an ANOVA on my data. What I'm trying
> > to do is the following:
> > 
> >  
> > 
> > Given data such as:
> > 
> >  
> > 
> > Day 1    Day 4    Day 8
> > 
> > 2          7          2
> > 
> > 3          2          8          
> > 
> > 3          4          7
> > 
> > 6          6          8
> > 
> > 1          3          4
> > 
> >  
> > 
> > I want to use ANOVA to determine if there is a significant change over the
> > three days. In other stats packages I have used, I can just select this data
> > and run the ANOVA function and get the F and p values. However in R, the
> > anova function seems to only work with a fitted model, eg. Linear
> > regression. This function seems to assume there is a relationship such as
> > day1~ day 4 + day 8, but in my case there isn't - I just want to perform an
> > ANOVA without regression. If anyone could point me in the right direction
> > I'd greatly appreciate it,
> > 
> >  
> > 
> > Thanks
> > 
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> -- 
> Andrew Robinson  
> Department of Mathematics and Statistics            Tel: +61-3-8344-9763
> University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
> Email: a.robinson at ms.unimelb.edu.au         http://www.ms.unimelb.edu.au
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From rmc095 at bham.ac.uk  Thu Sep 14 11:43:32 2006
From: rmc095 at bham.ac.uk (Russell Compton)
Date: Thu, 14 Sep 2006 10:43:32 +0100
Subject: [R] ANOVA in R
In-Reply-To: <x27j06sstv.fsf@viggo.kubism.ku.dk>
Message-ID: <001a01c6d7e2$3dae6d60$6600000a@D6MKM61J>

Andrew, Peter,

Thanks both for the help, that's exactly what I was after. 

It is for a one-way ANOVA, looking at identifying differentially expressed
genes across time in a microarray dataset. Also, one of the datasets I'm
working with is unbalanced, so that additional code will be most useful.

Thanks again,

Russell Compton

-----Original Message-----
From: pd at pubhealth.ku.dk [mailto:pd at pubhealth.ku.dk] On Behalf Of Peter
Dalgaard
Sent: 14 September 2006 10:26
To: Andrew Robinson
Cc: Russell Compton; r-help at stat.math.ethz.ch
Subject: Re: [R] ANOVA in R

Andrew Robinson <A.Robinson at ms.unimelb.edu.au> writes:

> Try
> 
> test <- data.frame(day.1=c(2,3,3,6,1), 
>      day.4=c(7,2,4,6,3),
>      day.8=c(2,8,7,8,4))
> 
> test
> 
> test.long <- reshape(test, direction="long", 
> 	  varying=c("day.1","day.4","day.8"),
> 	  v.names="response",
> 	  timevar="day",
> 	  times=names(test))
> 
> test.long$day <- factor(test.long$day)
> 
> test.long
> 
> aov(response ~ day, data=test.long)


Was a one-way ANOVA intended? He never said.

On a more elementary level,

y <- with(test, c(day.1,day.4,day.8))
day <- factor(rep(c(1,4,8),each=5)) # or gl(3,5,labels=c(1,4,8))
sub <- factor(rep(1:5,3))           # or gl(5,1,15)

print(data.frame(y,day,sub)) # just to show the point

anova(lm(y~day))        # 1-way
anova(lm(y~day+sub))    # 2-way

# This could be better for unbalanced designs:

drop1(lm(y~day+sub),test="F")

 

 
> 
> I hope that this helps,
> 
> Andrew
> 
> 
> On Thu, Sep 14, 2006 at 09:23:13AM +0100, Russell Compton wrote:
> > Despite having used R on a daily basis for the past two years, I'm
> > encountering some difficulty performing an ANOVA on my data. What I'm
trying
> > to do is the following:
> > 
> >  
> > 
> > Given data such as:
> > 
> >  
> > 
> > Day 1    Day 4    Day 8
> > 
> > 2          7          2
> > 
> > 3          2          8          
> > 
> > 3          4          7
> > 
> > 6          6          8
> > 
> > 1          3          4
> > 
> >  
> > 
> > I want to use ANOVA to determine if there is a significant change over
the
> > three days. In other stats packages I have used, I can just select this
data
> > and run the ANOVA function and get the F and p values. However in R, the
> > anova function seems to only work with a fitted model, eg. Linear
> > regression. This function seems to assume there is a relationship such
as
> > day1~ day 4 + day 8, but in my case there isn't - I just want to perform
an
> > ANOVA without regression. If anyone could point me in the right
direction
> > I'd greatly appreciate it,
> > 
> >  
> > 
> > Thanks
> > 
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> -- 
> Andrew Robinson  
> Department of Mathematics and Statistics            Tel: +61-3-8344-9763
> University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
> Email: a.robinson at ms.unimelb.edu.au         http://www.ms.unimelb.edu.au
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From gregor.gorjanc at bfro.uni-lj.si  Thu Sep 14 11:54:01 2006
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Thu, 14 Sep 2006 09:54:01 +0000 (UTC)
Subject: [R] Conservative
References: <1A8FE5B0-3FE3-4CA9-9224-9CACB9AF4854@muohio.edu>
	<40e66e0b0609070559u656463c1l52a60eda9a095237@mail.gmail.com>
	<20060910055624.GA12212@ms.unimelb.edu.au>
	<40e66e0b0609110943h7dfb057dv12567975ac04bdb2@mail.gmail.com>
	<1158010468.3278.24.camel@solidago.localdomain>
	<48130.220.237.183.166.1158098663.squirrel@webmail.ms.unimelb.edu.au>
	<1158145458.3699.3.camel@solidago.localdomain>
	<01a701c6d72f$855eb8d0$0540210a@www.domain>
	<40e66e0b0609130806i43bce07dr7afb7ab215a2aaa7@mail.gmail.com>
Message-ID: <loom.20060914T114209-484@post.gmane.org>

Douglas Bates <bates <at> stat.wisc.edu> writes:

> On 9/13/06, Dimitris Rizopoulos <dimitris.rizopoulos <at> med.kuleuven.be>
> > > I believe that the LRT is anti-conservative for fixed effects, as
> > > described in Pinheiro and Bates companion book to NLME.
> > >
> > You have this effect if you're using REML, for ML I don't think there
> > is any problem to use LRT between nested models with different
> > fixed-effects structure.
...
> The other question is how does one evaluate the likelihood-ratio test
> statistic and that is the issue that Dimitris is addressing.  The REML
> criterion is a modified likelihood and it is inappropriate to look at
> differences in the REML criterion when the models being compared have
> different fixed-effects specifications, or even a different
> parameterization of the fixed effects.  However, the anova method for
> an lmer object does not use the REML criterion even when the model has
> been estimated by REML.  It uses the profiled log-likelihood evaluated
> at the REML estimates of the relative variances of the random effects.
>  That's a complicated statement so let me break it down.
...

Is this then the same answer as given by Robinson:1991 (ref at the end) to
question by Robin Thompson on which likelihood (ML or REML) should be used
in testing the "fixed" effects. Robinson answered (page 49 near bottom 
right) that both likelihoods give the same conclusion about fixed effects. 
Can anyone comment on this issues? 

Thanks, Gregor

@Article{Robinson:1991,
  author =       {Robinson, G. K.},
  title =        {That {BLUP} is a good thing: the estimation of random
                  effects},
  journal =      ss,
  year =         {1991},
  volume =       {6},
  number =       {1},
  pages =        {15--51},
  keywords =     {BLUP, example, derivations, links, applications},
  vnos =         {GG}
}


From jim at bitwrit.com.au  Fri Sep 15 01:56:36 2006
From: jim at bitwrit.com.au (Jim Lemon)
Date: Thu, 14 Sep 2006 19:56:36 -0400
Subject: [R] kendall's w
In-Reply-To: <45083880.9060802@free.fr>
References: <45083880.9060802@free.fr>
Message-ID: <4509EC34.1070804@bitwrit.com.au>

Bianca Vieru wrote:
> Hi,
> 
> I try to calculate Kendall's W coefficient and I have a bizarre error.
> 
> 
> little.app.mat<-matrix(c(1,3,4,2,6,5,2,4,3,1,5,6,3,2,5,1,5,4),nrow=3,byrow=TRUE)
> print(kendall.w(little.app.mat[-1,]))
>  >>> Kendall's W for ordinal data
>  >>> W = 0.7753623Error in if (is.na(x$p.table)) { : argument is of 
> length zero
> 
> big.app.mat<-matrix(c(1,3,4,2,6,5,2,4,3,1,5,6,3,2,5,1,5,42,3,5,3,6,7,9,9,8,7),nrow=3,byrow=TRUE)
> print(kendall.w(big.app.mat[-1,]))
>  >>>Kendall's W for ordinal data
>  >>>W = 0.4568966  p(X2[8]) = 0.5035488
> 
> Why is that working for the big matrix and not for the little one?
> 
Thanks for finding this and thanks to David Barron for the correct 
answer - I'll insert a check for out of range matrices like this and 
submit a new version.

Jim


From dan.bebber at plants.ox.ac.uk  Thu Sep 14 11:55:31 2006
From: dan.bebber at plants.ox.ac.uk (Dan Bebber)
Date: Thu, 14 Sep 2006 10:55:31 +0100
Subject: [R] Greedy triangulation
Message-ID: <000f01c6d7e3$ea17bc90$d22401a3@plants.ox.ac.uk>

Hello,

does anyone have code that will generate a greedy triangulation 
(triangulation that uses shortest non-overlapping edges) for a set of points 
in Euclidean space?

Thanks,
Dan Bebber
_______________________
Dr. Daniel P. Bebber
Department of Plant Sciences
University of Oxford
South Parks Road
Oxford OX1 3RB
UK
Tel. 01865 275060


From p.dalgaard at biostat.ku.dk  Thu Sep 14 12:21:28 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 14 Sep 2006 12:21:28 +0200
Subject: [R] Conservative
In-Reply-To: <loom.20060914T114209-484@post.gmane.org>
References: <1A8FE5B0-3FE3-4CA9-9224-9CACB9AF4854@muohio.edu>
	<40e66e0b0609070559u656463c1l52a60eda9a095237@mail.gmail.com>
	<20060910055624.GA12212@ms.unimelb.edu.au>
	<40e66e0b0609110943h7dfb057dv12567975ac04bdb2@mail.gmail.com>
	<1158010468.3278.24.camel@solidago.localdomain>
	<48130.220.237.183.166.1158098663.squirrel@webmail.ms.unimelb.edu.au>
	<1158145458.3699.3.camel@solidago.localdomain>
	<01a701c6d72f$855eb8d0$0540210a@www.domain>
	<40e66e0b0609130806i43bce07dr7afb7ab215a2aaa7@mail.gmail.com>
	<loom.20060914T114209-484@post.gmane.org>
Message-ID: <x2y7smrbo7.fsf@viggo.kubism.ku.dk>

Gregor Gorjanc <gregor.gorjanc at bfro.uni-lj.si> writes:

> Douglas Bates <bates <at> stat.wisc.edu> writes:
> 
> > On 9/13/06, Dimitris Rizopoulos <dimitris.rizopoulos <at> med.kuleuven.be>
> > > > I believe that the LRT is anti-conservative for fixed effects, as
> > > > described in Pinheiro and Bates companion book to NLME.
> > > >
> > > You have this effect if you're using REML, for ML I don't think there
> > > is any problem to use LRT between nested models with different
> > > fixed-effects structure.
> ...
> > The other question is how does one evaluate the likelihood-ratio test
> > statistic and that is the issue that Dimitris is addressing.  The REML
> > criterion is a modified likelihood and it is inappropriate to look at
> > differences in the REML criterion when the models being compared have
> > different fixed-effects specifications, or even a different
> > parameterization of the fixed effects.  However, the anova method for
> > an lmer object does not use the REML criterion even when the model has
> > been estimated by REML.  It uses the profiled log-likelihood evaluated
> > at the REML estimates of the relative variances of the random effects.
> >  That's a complicated statement so let me break it down.
> ...
> 
> Is this then the same answer as given by Robinson:1991 (ref at the end) to
> question by Robin Thompson on which likelihood (ML or REML) should be used
> in testing the "fixed" effects. Robinson answered (page 49 near bottom 
> right) that both likelihoods give the same conclusion about fixed effects. 
> Can anyone comment on this issues? 

At the risk of sticking my foot in it due to not reading the paper
carefully enough: There appears to be two other likelihoods in play,
one traditional one depending on fixed effects and variances and
another depending on fixed effects and BLUPs ("most likely
unobservables"). I think Robinson is talking about the equivalence of
those two.

(and BTW ss=Statistical Science in the ref.)

 
> Thanks, Gregor
> 
> @Article{Robinson:1991,
>   author =       {Robinson, G. K.},
>   title =        {That {BLUP} is a good thing: the estimation of random
>                   effects},
>   journal =      ss,
>   year =         {1991},
>   volume =       {6},
>   number =       {1},
>   pages =        {15--51},
>   keywords =     {BLUP, example, derivations, links, applications},
>   vnos =         {GG}
> }


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From murdoch at stats.uwo.ca  Thu Sep 14 12:33:42 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 14 Sep 2006 06:33:42 -0400
Subject: [R] Rv generation
In-Reply-To: <4509513B@webmail>
References: <4509513B@webmail>
Message-ID: <45093006.1010602@stats.uwo.ca>

On 9/14/2006 2:16 AM, nmi13 wrote:
> Hi,
>  Can Someone inform me how to generate RV's using the below CDF, by inverse 
> technique.
> Thanks for your help and time.
> 
> My CDF is as follows
> \[
> F(x)=0 \ \text{if} \  x < 0\]\[
> F(x)=\{\frac{x-x_i}{x_{i+1}-x_{i}}*(p_{i+1}-p_{i})\}+p_{i}\
> \forall \ x_{i}\leq x < x_{i+1} \]
> \[ F(x)=1 \ \text{if} \  x > x_{i+1}
> \]

This sounds an awful lot like a class assignment.  You should ask your 
instructor for help with it.

Duncan Murdoch


From ramuigib at gmail.com  Thu Sep 14 14:19:43 2006
From: ramuigib at gmail.com (Srinivasan Ramachandran)
Date: Thu, 14 Sep 2006 17:49:43 +0530
Subject: [R] Binomial test using R
Message-ID: <b10617bf0609140519i19dd0002ob7b9917ea0059e07@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060914/07f9a91e/attachment.pl 

From p.dalgaard at biostat.ku.dk  Thu Sep 14 14:41:29 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 14 Sep 2006 14:41:29 +0200
Subject: [R] Binomial test using R
In-Reply-To: <b10617bf0609140519i19dd0002ob7b9917ea0059e07@mail.gmail.com>
References: <b10617bf0609140519i19dd0002ob7b9917ea0059e07@mail.gmail.com>
Message-ID: <x2odtir56u.fsf@viggo.kubism.ku.dk>

"Srinivasan Ramachandran" <ramuigib at gmail.com> writes:

> Hullo,
> 
> Can someone suggest whether the binomial test as described in the link
> http://home.clara.net/sisa/binomial.htm is available in an equivalent form
> in R? I have downloaded the R package from the CRAN site.
> 
> Using R will help me do this test rapidly

I think you could be expected to do a bit more of your homework before
querying the list! Anyways, have a look at binom.test().

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From wuming.gong at gmail.com  Thu Sep 14 14:43:28 2006
From: wuming.gong at gmail.com (Wuming Gong)
Date: Thu, 14 Sep 2006 20:43:28 +0800
Subject: [R] Run-time error "521" in SciViews
Message-ID: <b428d06d0609140543u4b8b2c50xbfbae1a4bc7517aa@mail.gmail.com>

Dear list,

I use SciViews R Console 0.8.9 with R-2.2.1 under Windows XP SP2, and
it works very well for most of time.  However, sometimes, when
commands were executed by clicking F5, the error "Run-time error 521,
Can't open clipboard" pop out.  After choosing "Yes", the both R
console and SciViews exits, and all things in the Command window
losts, which is very annoying.  Could anyone tell me how to avoid this
problem?

Thanks.

Wuming


From afshart at exchange.sba.miami.edu  Thu Sep 14 16:08:02 2006
From: afshart at exchange.sba.miami.edu (Afshartous, David)
Date: Thu, 14 Sep 2006 10:08:02 -0400
Subject: [R] plotting all subgroups with augPred
Message-ID: <6BCB4D493A447546A8126F24332056E8041C5599@school1.business.edu>

All,

I have a question RE plotting the prediction lines of a random effects
model via augPred.  I'll illustrate via the Pixel dataset within the
nlme package: 

library(nlme)
attach(Pixel)
fm1Pixel = lme(pixel ~ day + I(day^2), data = Pixel, random = list(Dog =
~ 1))
plot(augPred(fm1Pixel))   ### 10 fitted lines since there are 10 dogs

fm2Pixel = update(fm1Pixel, . ~ . + Side)
plot(augPred(fm2Pixel))    ## also produces 10 fitted lines

For the second plot, shouldn't we have 2 fitted lines per dog, one for
each level
of the fixed effect Side?  

1) How does one plot insure that this is plotted accordingly?

2) How does one plot say only the first 5 dogs?


Thanks!
Dave


From revans at jlab.org  Thu Sep 14 16:22:48 2006
From: revans at jlab.org (Richard Evans)
Date: Thu, 14 Sep 2006 10:22:48 -0400
Subject: [R] working with strptime data
Message-ID: <000001c6d809$44388d60$eb173981@revansx>

Dear R-forum,

I am looking for a good resource/help on working with POSIXct values and
controlling the pretty x-axis labels and tick marks for a data VS time
plots. Specifically, I wish to do programming logic which creates
different vertical ablines calculations based on the range of times
which i am working on. The default plot results are adequate, but I
would love to make explicit calls on how the x-axis annotates the
timestamps.

Does anyone have example code or know of a good reference for these
kinds of R-programming tasks?


Here's a simplified example:
----------------------------------------------------------
I have a large data set that consists of N pairs of values and
timestamps strings.
Like this:

TheData <- c(1.2,             0.9,             etc...[to the Nth
datapoint])
TheTime <- c("9/1/2006 8:00", "9/1/2006 8:13", etc...[to the Nth
timestamp])

I convert the timestamp strings into POSIXct types as:

TheTime <- as.POSIXct(strptime(TheTime, format="%m/%d/%Y %H:%M"))

And create a plot as such:

plot(MyTime,MyData)

----------------------------------------------------------

And here is a specific question:

How do I calculate the number of months than are spanned between two
POSIXct values?
(i.e. NumOfMonths <- MonthFunction(range(MyTimeStampData))

Thanks-in-advance,
- rich


From bates at stat.wisc.edu  Thu Sep 14 16:32:46 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 14 Sep 2006 09:32:46 -0500
Subject: [R] Conservative
In-Reply-To: <x2y7smrbo7.fsf@viggo.kubism.ku.dk>
References: <1A8FE5B0-3FE3-4CA9-9224-9CACB9AF4854@muohio.edu>
	<20060910055624.GA12212@ms.unimelb.edu.au>
	<40e66e0b0609110943h7dfb057dv12567975ac04bdb2@mail.gmail.com>
	<1158010468.3278.24.camel@solidago.localdomain>
	<48130.220.237.183.166.1158098663.squirrel@webmail.ms.unimelb.edu.au>
	<1158145458.3699.3.camel@solidago.localdomain>
	<01a701c6d72f$855eb8d0$0540210a@www.domain>
	<40e66e0b0609130806i43bce07dr7afb7ab215a2aaa7@mail.gmail.com>
	<loom.20060914T114209-484@post.gmane.org>
	<x2y7smrbo7.fsf@viggo.kubism.ku.dk>
Message-ID: <40e66e0b0609140732u77509a5di181082736af3e588@mail.gmail.com>

On 14 Sep 2006 12:21:28 +0200, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> Gregor Gorjanc <gregor.gorjanc at bfro.uni-lj.si> writes:
>
> > Douglas Bates <bates <at> stat.wisc.edu> writes:
> >
> > > On 9/13/06, Dimitris Rizopoulos <dimitris.rizopoulos <at> med.kuleuven.be>
> > > > > I believe that the LRT is anti-conservative for fixed effects, as
> > > > > described in Pinheiro and Bates companion book to NLME.
> > > > >
> > > > You have this effect if you're using REML, for ML I don't think there
> > > > is any problem to use LRT between nested models with different
> > > > fixed-effects structure.
> > ...
> > > The other question is how does one evaluate the likelihood-ratio test
> > > statistic and that is the issue that Dimitris is addressing.  The REML
> > > criterion is a modified likelihood and it is inappropriate to look at
> > > differences in the REML criterion when the models being compared have
> > > different fixed-effects specifications, or even a different
> > > parameterization of the fixed effects.  However, the anova method for
> > > an lmer object does not use the REML criterion even when the model has
> > > been estimated by REML.  It uses the profiled log-likelihood evaluated
> > > at the REML estimates of the relative variances of the random effects.
> > >  That's a complicated statement so let me break it down.
> > ...
> >
> > Is this then the same answer as given by Robinson:1991 (ref at the end) to
> > question by Robin Thompson on which likelihood (ML or REML) should be used
> > in testing the "fixed" effects. Robinson answered (page 49 near bottom
> > right) that both likelihoods give the same conclusion about fixed effects.
> > Can anyone comment on this issues?
>
> At the risk of sticking my foot in it due to not reading the paper
> carefully enough: There appears to be two other likelihoods in play,
> one traditional one depending on fixed effects and variances and
> another depending on fixed effects and BLUPs ("most likely
> unobservables"). I think Robinson is talking about the equivalence of
> those two.
>
> (and BTW ss=Statistical Science in the ref.)
> >
> > @Article{Robinson:1991,
> >   author =       {Robinson, G. K.},
> >   title =        {That {BLUP} is a good thing: the estimation of random
> >                   effects},
> >   journal =      ss,
> >   year =         {1991},
> >   volume =       {6},
> >   number =       {1},
> >   pages =        {15--51},
> >   keywords =     {BLUP, example, derivations, links, applications},
> >   vnos =         {GG}
> > }

The issue that I was attempting to describe is somewhat different.
I'm only considering using the log-likelihood values for the
likelihood ratio test (which, BTW, I would not recommend for testing
terms in the fixed-effects specification but that's another
discussion).  To perform such a test one should evaluate the
log-likelihood for the models being compared, which is to say you need
the minimum of the log-likelihood for each model over that model's
parameter space.  If you estimated parameters in a model by REML (the
default criterion) then all you know is that the log-likelihood for
the model is bounded above by the log-likelihood for the model
evaluated at those parameter estimates.  To be able to perform the
test properly you should evaluate the ML estimates and take the
log-likelihood at those estimates.  For small data sets and simple
models this wouldn't be a problem.  For large data sets with complex
model structures this could take some time.

However, the profiled log-likelihood evaluated at the REML estimates
of the relative variances of the random effects (and the corresponding
conditional ML estimates of $\sigma^2$ and the fixed-effects) is very
close to the log-likelihood for the model so I take a short cut and
use that as the log-likelihood for the model.

So the quantities labelled "MLdeviance" and "REMLdeviance" in the
show() or summary() output of an lmer model can be interpreted as
-2*log-likelihood and -2*log-restricted-likelihood for the model not
for the particular parameters being reported.  If you estimated the
parameters by REML then the REMLdeviance is accurate and the
MLdeviance will be a slight over-estimate.  If you estimated the
parameters by ML you get the opposite situation (accurate MLdeviance,
slight over-estimate of REMLdeviance).  The rest of the output shows
the parameter estimates for the criterion that you used to estimate
the model.  The parameter estimates for the other criterion could be
quite different.

Here's a rather extreme example using the PBIB data from the SASmixed package.


> (fm1 <- lmer(response ~ Treatment + (1|Block), PBIB, method = "REML"))
Linear mixed-effects model fit by REML
Formula: response ~ Treatment + (1 | Block)
   Data: PBIB
     AIC      BIC    logLik MLdeviance REMLdeviance
 83.9849 117.4944 -25.99245   22.82831     51.98489
Random effects:
 Groups   Name        Variance Std.Dev.
 Block    (Intercept) 0.046522 0.21569
 Residual             0.085559 0.29250
number of obs: 60, groups: Block, 15

Fixed effects:
             Estimate Std. Error t value
(Intercept)  2.817523   0.166413 16.9309
Treatment10 -0.326461   0.222061 -1.4701
Treatment11  0.081177   0.227200  0.3573
Treatment12  0.235299   0.222061  1.0596
Treatment13 -0.199753   0.222061 -0.8995
Treatment14 -0.326211   0.222061 -1.4690
Treatment15  0.041711   0.222061  0.1878
Treatment2  -0.412208   0.222061 -1.8563
Treatment3  -0.362579   0.222061 -1.6328
Treatment4  -0.033692   0.222061 -0.1517
Treatment5  -0.012625   0.222061 -0.0569
Treatment6   0.093171   0.227200  0.4101
Treatment7  -0.028537   0.222061 -0.1285
Treatment8  -0.035917   0.222061 -0.1617
Treatment9   0.073789   0.222061  0.3323

> (fm1ML <- lmer(response ~ Treatment + (1|Block), PBIB, method = "ML"))
Linear mixed-effects model fit by maximum likelihood
Formula: response ~ Treatment + (1 | Block)
   Data: PBIB
      AIC      BIC    logLik MLdeviance REMLdeviance
 54.57058 88.08009 -11.28529   22.57058     52.20403
Random effects:
 Groups   Name        Variance Std.Dev.
 Block    (Intercept) 0.045030 0.21220
 Residual             0.060371 0.24571
number of obs: 60, groups: Block, 15

Fixed effects:
             Estimate Std. Error t value
(Intercept)  2.822676   0.143563 19.6615
Treatment10 -0.327070   0.187925 -1.7404
Treatment11  0.067533   0.192717  0.3504
Treatment12  0.222437   0.187925  1.1836
Treatment13 -0.195125   0.187925 -1.0383
Treatment14 -0.323562   0.187925 -1.7218
Treatment15  0.034576   0.187925  0.1840
Treatment2  -0.416171   0.187925 -2.2146
Treatment3  -0.368036   0.187925 -1.9584
Treatment4  -0.057737   0.187925 -0.3072
Treatment5  -0.017386   0.187925 -0.0925
Treatment6   0.086646   0.192717  0.4496
Treatment7  -0.037247   0.187925 -0.1982
Treatment8  -0.035441   0.187925 -0.1886
Treatment9   0.076438   0.187925  0.4067

You can see that the MLdeviance at the ML estimates is lower than at
the REML estimates but not a lot lower.  Other quantities like the
estimate of $\sigma^2$ and the standard errors of the fixed effects do
change considerably between the two sets of estimates.

This example also shows why using likelihood ratio tests for terms in
the fixed-effects specification is not a good idea.  A likelihood
ratio test for the Treatment effect would be either
> (fm2ML <- lmer(response ~ 1 + (1|Block), PBIB, method = "ML"))
Linear mixed-effects model fit by maximum likelihood
Formula: response ~ 1 + (1 | Block)
   Data: PBIB
      AIC      BIC    logLik MLdeviance REMLdeviance
 50.15189 54.34058 -23.07595   46.15189     49.53125
Random effects:
 Groups   Name        Variance Std.Dev.
 Block    (Intercept) 0.057544 0.23988
 Residual             0.092444 0.30405
number of obs: 60, groups: Block, 15

Fixed effects:
            Estimate Std. Error t value
(Intercept) 2.736667   0.073328  37.321
> anova(fm2ML, fm1ML)
Data: PBIB
Models:
fm2ML: response ~ 1 + (1 | Block)
fm1ML: response ~ Treatment + (1 | Block)
      Df     AIC     BIC  logLik  Chisq Chi Df Pr(>Chisq)
fm2ML  2  50.152  54.341 -23.076
fm1ML 16  54.571  88.080 -11.285 23.581     14    0.05144 .

or

> (fm2 <- lmer(response ~ 1 + (1|Block), PBIB))
Linear mixed-effects model fit by REML
Formula: response ~ 1 + (1 | Block)
   Data: PBIB
      AIC      BIC    logLik MLdeviance REMLdeviance
 53.50553 57.69422 -24.75277   46.17836     49.50553
Random effects:
 Groups   Name        Variance Std.Dev.
 Block    (Intercept) 0.063306 0.25161
 Residual             0.092444 0.30405
number of obs: 60, groups: Block, 15

Fixed effects:
            Estimate Std. Error t value
(Intercept) 2.736667   0.075902  36.055
> anova(fm2, fm1)
Data: PBIB
Models:
fm2: response ~ 1 + (1 | Block)
fm1: response ~ Treatment + (1 | Block)
    Df     AIC     BIC  logLik Chisq Chi Df Pr(>Chisq)
fm2  2  50.178  54.367 -23.089
fm1 16  54.828  88.338 -11.414 23.35     14    0.05481 .

(The first is more accurate than the second.)  However, both p-values
are anti-conservative relative to an empirical p-value obtained from
simulation of data sets under the null hypothesis.


From martin.wagner at ilr.tu-berlin.de  Thu Sep 14 16:36:53 2006
From: martin.wagner at ilr.tu-berlin.de (Martin Wagner)
Date: Thu, 14 Sep 2006 16:36:53 +0200
Subject: [R] time varying covariates
Message-ID: <45096905.1050300@ilr.tu-berlin.de>

Hello,

I am trying to model an intensity function with time-varying covariates.
Before, I have successfully defined a log likelihood function for a 
Power-Law Process (lambda(t)=alpha*beta*t^(beta-1))  with two paramters 
and no covariates for a repairable systems with failure times (t).
This function was maximized with R optim. No problem!

But now I want to include a covariate indicating a time-varying value at 
each failure time t. For constant covariates, the procedure is feasible :


leads to following log likelihood funciton:


here zi are the covariates which are constant for each unit i under 
observation. tij are the failure time for failure j of unit i.

Do you know how to formulate a log likelihood function for covariates 
which vary for each tj of each unit i ?

Thank you very much
Best regards

Martin Wagner
Berlin University of Technology

From Jeffrey.Marcus at nuance.com  Thu Sep 14 16:48:13 2006
From: Jeffrey.Marcus at nuance.com (Marcus, Jeffrey)
Date: Thu, 14 Sep 2006 10:48:13 -0400
Subject: [R] Problem setting options(error=recover) in my Rprofile.site
Message-ID: <F8940C21CD563F49BC884A274C4653DF05BE5044@bn-exch1.speechworks.com>

I'd like to be able to set options(error=recover) in my Rprofile.site file.
When I do this I get the message
 "Error in options(error = recover) : object "recover" not found

I assume this is because the utils package (where recover and dump.frames
are defined) has not been loaded at the time I make this call. 

I suppose I could explicitly do library("utils") before setting the
"options" even though it will be loaded again when the default packages are
loaded. 

Any simpler suggestions of how to get options(error=recover) set
automatically every time I start R? 

More generally, is there a way to have code executed on startup but *after*
the default packages are loaded? 

Thanks.

  Jeff


From Greg.Snow at intermountainmail.org  Thu Sep 14 17:32:12 2006
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Thu, 14 Sep 2006 09:32:12 -0600
Subject: [R] Greedy triangulation
Message-ID: <07E228A5BE53C24CAD490193A7381BBB591DE7@LP-EXCHVS07.CO.IHC.COM>

Does the deldir package do what you want? 


-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dan Bebber
Sent: Thursday, September 14, 2006 3:56 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Greedy triangulation

Hello,

does anyone have code that will generate a greedy triangulation
(triangulation that uses shortest non-overlapping edges) for a set of
points in Euclidean space?

Thanks,
Dan Bebber
_______________________
Dr. Daniel P. Bebber
Department of Plant Sciences
University of Oxford
South Parks Road
Oxford OX1 3RB
UK
Tel. 01865 275060

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From rkoenker at uiuc.edu  Thu Sep 14 17:40:27 2006
From: rkoenker at uiuc.edu (roger koenker)
Date: Thu, 14 Sep 2006 10:40:27 -0500
Subject: [R] Greedy triangulation
In-Reply-To: <07E228A5BE53C24CAD490193A7381BBB591DE7@LP-EXCHVS07.CO.IHC.COM>
References: <07E228A5BE53C24CAD490193A7381BBB591DE7@LP-EXCHVS07.CO.IHC.COM>
Message-ID: <71DF54E6-B362-470D-B80C-2F33C0D38281@uiuc.edu>

Or, perhaps, tripack?

url:    www.econ.uiuc.edu/~roger            Roger Koenker
email    rkoenker at uiuc.edu            Department of Economics
vox:     217-333-4558                University of Illinois
fax:       217-244-6678                Champaign, IL 61820


On Sep 14, 2006, at 10:32 AM, Greg Snow wrote:

> Does the deldir package do what you want?
>
>
> --  
> Gregory (Greg) L. Snow Ph.D.
> Statistical Data Center
> Intermountain Healthcare
> greg.snow at intermountainmail.org
> (801) 408-8111
>
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dan Bebber
> Sent: Thursday, September 14, 2006 3:56 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Greedy triangulation
>
> Hello,
>
> does anyone have code that will generate a greedy triangulation
> (triangulation that uses shortest non-overlapping edges) for a set of
> points in Euclidean space?
>
> Thanks,
> Dan Bebber
> _______________________
> Dr. Daniel P. Bebber
> Department of Plant Sciences
> University of Oxford
> South Parks Road
> Oxford OX1 3RB
> UK
> Tel. 01865 275060
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dan.bebber at plants.ox.ac.uk  Thu Sep 14 17:40:23 2006
From: dan.bebber at plants.ox.ac.uk (Dan Bebber)
Date: Thu, 14 Sep 2006 16:40:23 +0100
Subject: [R] Greedy triangulation
References: <07E228A5BE53C24CAD490193A7381BBB591DE7@LP-EXCHVS07.CO.IHC.COM>
Message-ID: <003301c6d814$17c6ae00$d22401a3@plants.ox.ac.uk>

Thanks, but no, the Delaunay is different.

I have written the following, which interested persons might want to check 
for accuracy and streamlining:

#GREEDY TRIANGULATION
#
#Pick all lines that are shortest and don't overlap, starting with shortest.
#
greedy<-function(xy){   #input is a matrix with 2 columns (x,y)
require(spatstat)   #need the crossing.psp function
w<-owin(range(xy[,1]),range(xy[,2]))    #set the window for crossing.psp
dists<-dist(xy) #calculate Euclidean distances for each line
ind<-which(lower.tri(dists),arr.ind=T)  #matrix with 2 columns (start point, 
end point)
x1<-xy[ind[,1],1]   #x of start point
y1<-xy[ind[,1],2]   #y of start point
x2<-xy[ind[,2],1]   #x of end point
y2<-xy[ind[,2],2]   #y of end point
dat<-data.frame(strt=ind[,1],fin=ind[,2],
x1=x1,y1=y1,x2=x2,y2=y2,len=as.vector(dists))#put all data for lines 
together
dat<-dat[order(dists),] #order data by line length, shortest first
inc<-dat[1,]    #include the shortest line in the triangulation
dat<-dat[-1,]   #keep remaining lines
while(nrow(dat)>0){ #while lines remain to be tested, do the following...
A<-psp(inc$x1,inc$y1,inc$x2,inc$y2,w) #create the psp object for the lines 
already included
B<-psp(dat$x1[1],dat$y1[1],dat$x2[1],dat$y2[1],w) #create the psp object for 
the next line to be tested
cross<-crossing.psp(A,B) #check for crossing of the test line over the lines 
already included
p.match<-sum(cross$x%in%c(inc$x1,inc$x2)) #check if the crossing occurs 
because same points are included
if(cross$n-p.match==0){inc[nrow(inc)+1,]<-dat[1,]} #if the only crossing are 
due to shared points, include the line
dat<-dat[-1,]}  #remove the line from the untested lines
return(inc)} #when all lines have been tested, return all included lines
#
#Plot the greedy triangulation
#
plot.greedy<-function(xy,gr,...){
plot(xy,asp=1,xlab="x",ylab="y",...)
segments(gr$x1,gr$y1,gr$x2,gr$y2)}
#
#Test it
#
xy<-matrix(runif(40,0,1),nc=2)
gr<-greedy(xy)
plot.greedy(xy,gr,main="Greedy Triangulation")
#END

----- Original Message ----- 
From: "Greg Snow" <Greg.Snow at intermountainmail.org>
To: "Dan Bebber" <dan.bebber at plants.ox.ac.uk>; <r-help at stat.math.ethz.ch>
Sent: Thursday, September 14, 2006 4:32 PM
Subject: RE: [R] Greedy triangulation


Does the deldir package do what you want?


-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111


From mothsailor at googlemail.com  Thu Sep 14 18:08:51 2006
From: mothsailor at googlemail.com (David Barron)
Date: Thu, 14 Sep 2006 17:08:51 +0100
Subject: [R] time varying covariates
In-Reply-To: <815b70590609140908l149dcc25i744734dae660c21e@mail.gmail.com>
References: <45096905.1050300@ilr.tu-berlin.de>
	<815b70590609140908l149dcc25i744734dae660c21e@mail.gmail.com>
Message-ID: <815b70590609140908r38601f07i711a438c639d0b7@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060914/ce6b5e38/attachment.pl 

From ripley at stats.ox.ac.uk  Thu Sep 14 18:28:34 2006
From: ripley at stats.ox.ac.uk (Prof Brian D Ripley)
Date: Thu, 14 Sep 2006 17:28:34 +0100 (GMT Daylight Time)
Subject: [R] Problem setting options(error=recover) in my Rprofile.site
In-Reply-To: <F8940C21CD563F49BC884A274C4653DF05BE5044@bn-exch1.speechworks.com>
References: <F8940C21CD563F49BC884A274C4653DF05BE5044@bn-exch1.speechworks.com>
Message-ID: <Pine.WNT.4.64.0609141723510.684@Petrel>

On Thu, 14 Sep 2006, Marcus, Jeffrey wrote:

> I'd like to be able to set options(error=recover) in my Rprofile.site file.
> When I do this I get the message
> "Error in options(error = recover) : object "recover" not found
>
> I assume this is because the utils package (where recover and dump.frames
> are defined) has not been loaded at the time I make this call.
>
> I suppose I could explicitly do library("utils") before setting the
> "options" even though it will be loaded again when the default packages are
> loaded.

It will not be loaded again.

> Any simpler suggestions of how to get options(error=recover) set
> automatically every time I start R?

options(error=utils::recover)

> More generally, is there a way to have code executed on startup but *after*
> the default packages are loaded?

Load the default packages yourself in your startup code, by calling 
.First.sys() (and resetting it to avoid the slight overhead of subsequent 
require() calls if you want).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Mark.Pinkerton at rms.com  Thu Sep 14 19:42:13 2006
From: Mark.Pinkerton at rms.com (Mark Pinkerton)
Date: Thu, 14 Sep 2006 18:42:13 +0100
Subject: [R] Beta stochastic simulation
Message-ID: <D71DD12EF4EDE740A09A929F6D2C5099750EA9@mailuk1.rms.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060914/4d6cb109/attachment.pl 

From grazia at stat.columbia.edu  Thu Sep 14 19:18:50 2006
From: grazia at stat.columbia.edu (grazia at stat.columbia.edu)
Date: Thu, 14 Sep 2006 13:18:50 -0400 (EDT)
Subject: [R] polr
Message-ID: <Pine.LNX.4.64.0609141317330.4660@gauss.stat.columbia.edu>

Dear R heplper,
I'm using polr:

fm <- polr(factor(y) ~ x, ,method='logistic', data = dat, Hess=T)

ans I'm getting a very strange message:
Error in optim(start, fmin, gmin, method = "BFGS", hessian = Hess, ...) :
initial value in 'vmmin' is not finite
Can you help me?
Thank you in advance,
Grazia


----
M. Grazia Pittau, Ph.D.
Post-Doctoral Research Fellow
Department of Statistics
Columbia University
1255 Amsterdam Avenue
New York, NY  10027

grazia at stat.columbia.edu
Phone: 212.851.2160
Fax: 212.851.2164


From murdoch at stats.uwo.ca  Thu Sep 14 20:31:28 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 14 Sep 2006 14:31:28 -0400
Subject: [R] Beta stochastic simulation
In-Reply-To: <D71DD12EF4EDE740A09A929F6D2C5099750EA9@mailuk1.rms.com>
References: <D71DD12EF4EDE740A09A929F6D2C5099750EA9@mailuk1.rms.com>
Message-ID: <4509A000.1080204@stats.uwo.ca>

On 9/14/2006 1:42 PM, Mark Pinkerton wrote:
> Hi,
> I am finding that I get quite different results when I interchange the
> following "equivalent" lines for sampling from a beta distribution in my
> r script. The rbeta line is correct judging by the summary statistics of
> the simulated values, while the qbeta line consistently leads to a
> higher mean simulated value.
>  
> simulation <- rbeta(1, alpha, beta)
> simulation <- qbeta(runif(1), alpha, beta)
>  
> Are there any implementation reasons for this?

You need to be more specific about the R version, the parameter values 
you're using, and the size of the differences you're seeing.  I just tried

 > x <- rbeta(100000, 1, 6)
 > y <- qbeta(runif(100000), 1, 6)
 > summary(x); summary(y)
      Min.   1st Qu.    Median      Mean   3rd Qu.      Max.
4.076e-07 4.681e-02 1.085e-01 1.423e-01 2.055e-01 8.773e-01
      Min.   1st Qu.    Median      Mean   3rd Qu.      Max.
2.803e-06 4.583e-02 1.078e-01 1.419e-01 2.047e-01 8.861e-01

and I think those results look reasonable (and the qbeta method has a 
smaller mean, this run).

Duncan Murdoch

>  
> Thanks
>  
> Mark Pinkerton
> Risk Management Solutions 
> Peninsular House
> 30 Monument Street
> London EC3R 8HB 
> UK 
>  
> www.RMS.com <http://www.rms.com/>  
> Tel:  +44 20 7444 7783 
> Fax: +44 20 7444 7601
>  
> 
> 
> This message and any attachments contain information that may be RMS Inc. 
> confidential and/or privileged.  If you are not the intended recipient 
> (or authorized to receive for the intended recipient), and have received 
> this message in error, any use, disclosure or distribution is strictly 
> prohibited.   If you have received this message in error, please notify 
> the sender immediately by replying to the e-mail and permanently deleting 
> the message from your computer and/or storage system.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From learningr at gmail.com  Thu Sep 14 21:04:11 2006
From: learningr at gmail.com (Learning R)
Date: Thu, 14 Sep 2006 15:04:11 -0400
Subject: [R] Help On EBAM
Message-ID: <30d556050609141204s226d269cyc26446517ee1f3b5@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060914/a00c7b2c/attachment.pl 

From deepayan.sarkar at gmail.com  Thu Sep 14 21:31:51 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 14 Sep 2006 12:31:51 -0700
Subject: [R] converting strings to expressions
Message-ID: <eb555e660609141231t167c3966n3781d4c2d6511b29@mail.gmail.com>

Hi,

consider this:

--------------

estr <- c("2^4", "alpha[1]")
eexp <- expression(2^4, alpha[1])


## Is it possible to get 'eexp' starting from 'estr'? The closest I could
## get was:

do.call(expression, lapply(estr, as.name))

## but it is not quite the same; e.g. the following behave differently:

library(lattice)

xyplot(1:10 ~ 1:10,
       scales = list(x = list(at = c(3, 6), labels = eexp)))

xyplot(1:10 ~ 1:10,
       scales = list(x = list(at = c(3, 6),
                     labels = do.call(expression,
                     lapply(estr, as.name)))))

---------------

This happens in both 2.3.1 and pre-2.4.0.

Deepayan


> sessionInfo()
Version 2.3.1 Patched (2006-08-27 r39012)
x86_64-unknown-linux-gnu

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
[7] "base"

other attached packages:
  lattice
"0.13-10"

> sessionInfo()
R version 2.4.0 Under development (unstable) (2006-08-30 r39022)
x86_64-unknown-linux-gnu

locale:
C

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
[7] "base"

other attached packages:
 lattice
"0.14-3"


From p.dalgaard at biostat.ku.dk  Thu Sep 14 21:44:01 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 14 Sep 2006 21:44:01 +0200
Subject: [R] converting strings to expressions
In-Reply-To: <eb555e660609141231t167c3966n3781d4c2d6511b29@mail.gmail.com>
References: <eb555e660609141231t167c3966n3781d4c2d6511b29@mail.gmail.com>
Message-ID: <x2y7smdyim.fsf@turmalin.kubism.ku.dk>

"Deepayan Sarkar" <deepayan.sarkar at gmail.com> writes:

> Hi,
> 
> consider this:
> 
> --------------
> 
> estr <- c("2^4", "alpha[1]")
> eexp <- expression(2^4, alpha[1])
> 
> 
> ## Is it possible to get 'eexp' starting from 'estr'? The closest I could
> ## get was:
> 
> do.call(expression, lapply(estr, as.name))
> 
> ## but it is not quite the same; e.g. the following behave differently:

Er, how about

> estr <- c("2^4", "alpha[1]")
> parse(text=estr)
expression(2^4, alpha[1])

or (brain teaser alert!)

> parse(text=deparse(parse(text=estr)))[[1]]
expression(2^4, alpha[1])

which is _not_ quite the same thing.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From sundar.dorai-raj at pdf.com  Thu Sep 14 21:44:04 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 14 Sep 2006 14:44:04 -0500
Subject: [R] converting strings to expressions
In-Reply-To: <eb555e660609141231t167c3966n3781d4c2d6511b29@mail.gmail.com>
References: <eb555e660609141231t167c3966n3781d4c2d6511b29@mail.gmail.com>
Message-ID: <4509B104.1090700@pdf.com>



Deepayan Sarkar said the following on 9/14/2006 2:31 PM:
> Hi,
> 
> consider this:
> 
> --------------
> 
> estr <- c("2^4", "alpha[1]")
> eexp <- expression(2^4, alpha[1])
> 
> 
> ## Is it possible to get 'eexp' starting from 'estr'? The closest I could
> ## get was:
> 
> do.call(expression, lapply(estr, as.name))
> 
> ## but it is not quite the same; e.g. the following behave differently:
> 
> library(lattice)
> 
> xyplot(1:10 ~ 1:10,
>        scales = list(x = list(at = c(3, 6), labels = eexp)))
> 
> xyplot(1:10 ~ 1:10,
>        scales = list(x = list(at = c(3, 6),
>                      labels = do.call(expression,
>                      lapply(estr, as.name)))))
> 
> ---------------
> 
> This happens in both 2.3.1 and pre-2.4.0.
> 
> Deepayan
> 
> 
>> sessionInfo()
> Version 2.3.1 Patched (2006-08-27 r39012)
> x86_64-unknown-linux-gnu
> 
> attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
> [7] "base"
> 
> other attached packages:
>   lattice
> "0.13-10"
> 
>> sessionInfo()
> R version 2.4.0 Under development (unstable) (2006-08-30 r39022)
> x86_64-unknown-linux-gnu
> 
> locale:
> C
> 
> attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
> [7] "base"
> 
> other attached packages:
>  lattice
> "0.14-3"
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

Hi, Deepayan,

Will this work for you?

estr <- c("2^4", "alpha[1]")
eexp <- expression(2^4, alpha[1])

library(lattice)

xyplot(1:10 ~ 1:10,
        scales = list(x = list(at = c(3, 6), labels = eexp)))

estr.2 <- sprintf("expression(%s)", paste(estr, collapse = ","))
eexp.2 <- eval(parse(text = estr.2))
xyplot(1:10 ~ 1:10,
        scales = list(x = list(at = c(3, 6), labels = eexp.2)))

Works in both R-2.3.1 and R-2.4.0dev.

Typically, I don't like using eval(parse(text=...)), but I've run into 
this problem before I could not see another way. Perhaps Gabor will 
enlighten us with something slicker.

HTH,

--sundar


From deepayan.sarkar at gmail.com  Thu Sep 14 21:48:13 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 14 Sep 2006 12:48:13 -0700
Subject: [R] converting strings to expressions
In-Reply-To: <x2y7smdyim.fsf@turmalin.kubism.ku.dk>
References: <eb555e660609141231t167c3966n3781d4c2d6511b29@mail.gmail.com>
	<x2y7smdyim.fsf@turmalin.kubism.ku.dk>
Message-ID: <eb555e660609141248r55bee176uff9eb68307cf05ba@mail.gmail.com>

On 14 Sep 2006 21:44:01 +0200, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> "Deepayan Sarkar" <deepayan.sarkar at gmail.com> writes:
>
> > Hi,
> >
> > consider this:
> >
> > --------------
> >
> > estr <- c("2^4", "alpha[1]")
> > eexp <- expression(2^4, alpha[1])
> >
> >
> > ## Is it possible to get 'eexp' starting from 'estr'? The closest I could
> > ## get was:
> >
> > do.call(expression, lapply(estr, as.name))
> >
> > ## but it is not quite the same; e.g. the following behave differently:
>
> Er, how about
>
> > estr <- c("2^4", "alpha[1]")
> > parse(text=estr)
> expression(2^4, alpha[1])
>
> or (brain teaser alert!)
>
> > parse(text=deparse(parse(text=estr)))[[1]]
> expression(2^4, alpha[1])
>
> which is _not_ quite the same thing.

Ah, I'd forgotten about parse. A link from ?expression might be reasonable.

Thanks,
Deepayan


From rmh at temple.edu  Thu Sep 14 21:54:47 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Thu, 14 Sep 2006 15:54:47 -0400 (EDT)
Subject: [R] converting strings to expressions
Message-ID: <20060914155447.BHY19169@po-d.temple.edu>

sapply(estr, FUN=function(x) parse(text=x))

and it does print the greek letter in the xlab.

xyplot(1:10 ~ 1:10,
       scales = list(x = list(at = c(3, 6),
                     labels = sapply(estr, FUN=function(x) parse(text=x)))))

Rich


From ripley at stats.ox.ac.uk  Thu Sep 14 22:20:40 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 14 Sep 2006 21:20:40 +0100 (BST)
Subject: [R] converting strings to expressions
In-Reply-To: <eb555e660609141248r55bee176uff9eb68307cf05ba@mail.gmail.com>
References: <eb555e660609141231t167c3966n3781d4c2d6511b29@mail.gmail.com>
	<x2y7smdyim.fsf@turmalin.kubism.ku.dk>
	<eb555e660609141248r55bee176uff9eb68307cf05ba@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0609142117460.17107@gannet.stats.ox.ac.uk>

On Thu, 14 Sep 2006, Deepayan Sarkar wrote:

[...]

> Ah, I'd forgotten about parse. A link from ?expression might be reasonable.

But it says

Details:

      'Expression' here is not being used in its colloquial sense, that
      of mathematical expressions.  Those are calls (see 'call') in R,
      and an R expression vector is a list of calls, typically as
      returned by 'parse'.

so it already has a link.

'Expression' was often misused in the R documentation, and a couple of 
months ago I tried to move to a more careful usage.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From p.dalgaard at biostat.ku.dk  Thu Sep 14 22:35:27 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 14 Sep 2006 22:35:27 +0200
Subject: [R] converting strings to expressions
In-Reply-To: <Pine.LNX.4.64.0609142117460.17107@gannet.stats.ox.ac.uk>
References: <eb555e660609141231t167c3966n3781d4c2d6511b29@mail.gmail.com>
	<x2y7smdyim.fsf@turmalin.kubism.ku.dk>
	<eb555e660609141248r55bee176uff9eb68307cf05ba@mail.gmail.com>
	<Pine.LNX.4.64.0609142117460.17107@gannet.stats.ox.ac.uk>
Message-ID: <x2sliudw4w.fsf@turmalin.kubism.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> On Thu, 14 Sep 2006, Deepayan Sarkar wrote:
> 
> [...]
> 
> > Ah, I'd forgotten about parse. A link from ?expression might be reasonable.
> 
> But it says
> 
> Details:
> 
>       'Expression' here is not being used in its colloquial sense, that
>       of mathematical expressions.  Those are calls (see 'call') in R,
>       and an R expression vector is a list of calls, typically as
>       returned by 'parse'.
> 
> so it already has a link.
> 
> 'Expression' was often misused in the R documentation, and a couple of
> months ago I tried to move to a more careful usage.

Actually, you need to be even more careful because has commonly been
called "unevaluated expressions" cover symbols and constants too.
I.e., quote(1) and quote(x) are not calls. And such non-call objects
can be elements of an expression vector.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From emmanuel.levy at gmail.com  Thu Sep 14 22:35:33 2006
From: emmanuel.levy at gmail.com (Emmanuel Levy)
Date: Thu, 14 Sep 2006 21:35:33 +0100
Subject: [R] group bunch of lines in a data.frame,
	an additional requirement
In-Reply-To: <971536df0609131044y1a628331k6ea5846adaa6c3f5@mail.gmail.com>
References: <e4654710609130938p313dbe57g19c7e36c4ae51488@mail.gmail.com>
	<971536df0609131044y1a628331k6ea5846adaa6c3f5@mail.gmail.com>
Message-ID: <e4654710609141335h4ef212c4m2912d0d90cd32acb@mail.gmail.com>

Thanks Gabor, that is much faster than using a loop!

I've got a last question:

Can you think of a fast way of keeping track of the number of
observations collapsed for each entry?

i.e. I'd like to end up with:

A 2.0 400 ID1 3 (3obs in the first matrix)
B 0.7 35 ID2 2 (2obs in the first matrix)
C 5.0 70 ID1 1 (1obs in the first matrix)

Or is it required to use an temporary matrix that is merged later? (As
examplified by Mark in a previous email?)

Thanks a lot for your help,

  Emmanuel

On 9/13/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> See below.
>
> On 9/13/06, Emmanuel Levy <emmanuel.levy at gmail.com> wrote:
> > Thanks for pointing me out "aggregate", that works fine!
> >
> > There is one complication though: I have mixed types (numerical and character),
> >
> > So the matrix is of the form:
> >
> > A 1.0 200 ID1
> > A 3.0 800 ID1
> > A 2.0 200 ID1
> > B 0.5 20   ID2
> > B 0.9 50   ID2
> > C 5.0 70   ID1
> >
> > One letter always has the same ID but one ID can be shared by many
> > letters (like ID1)
> >
> > I just want to keep track of the ID, and get a matrix like:
> >
> > A 2.0 400 ID1
> > B 0.7 35 ID2
> > C 5.0 70 ID1
> >
> > Any idea on how to do that without a loop?
>
> If V4 is a function of V1 then you can aggregate by it too and it will
> appear but have no effect on the classification:
>
> > aggregate(DF[2:3], DF[c(1,4)], mean)
>   V1  V4  V2  V3
> 1  A ID1 2.0 400
> 2  C ID1 5.0  70
> 3  B ID2 0.7  35
>
>
> >
> >  Many thanks,
> >
> >     Emmanuel
> >
> > On 9/12/06, Emmanuel Levy <emmanuel.levy at gmail.com> wrote:
> > > Hello,
> > >
> > > I'd like to group the lines of a matrix so that:
> > > A 1.0 200
> > > A 3.0 800
> > > A 2.0 200
> > > B 0.5 20
> > > B 0.9 50
> > > C 5.0 70
> > >
> > > Would give:
> > > A 2.0 400
> > > B 0.7 35
> > > C 5.0 70
> > >
> > > So all lines corresponding to a letter (level), become a single line
> > > where all the values of each column are averaged.
> > >
> > > I've done that with a loop but it doesn't sound right (it is very
> > > slow). I imagine there is a
> > > sort of "apply" shortcut but I can't figure it out.
> > >
> > > Please note that it is not exactly a matrix I'm using, the function
> > > "typeof" tells me it's a list, however I access to it like it was a
> > > matrix.
> > >
> > > Could someone help me with the right function to use, a help topic or
> > > a piece of code?
> > >
> > > Thanks,
> > >
> > >   Emmanuel
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From p.dalgaard at biostat.ku.dk  Thu Sep 14 22:40:09 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 14 Sep 2006 22:40:09 +0200
Subject: [R] converting strings to expressions
In-Reply-To: <x2sliudw4w.fsf@turmalin.kubism.ku.dk>
References: <eb555e660609141231t167c3966n3781d4c2d6511b29@mail.gmail.com>
	<x2y7smdyim.fsf@turmalin.kubism.ku.dk>
	<eb555e660609141248r55bee176uff9eb68307cf05ba@mail.gmail.com>
	<Pine.LNX.4.64.0609142117460.17107@gannet.stats.ox.ac.uk>
	<x2sliudw4w.fsf@turmalin.kubism.ku.dk>
Message-ID: <x2odtidvx2.fsf@turmalin.kubism.ku.dk>

Peter Dalgaard <p.dalgaard at biostat.ku.dk> writes:

> Actually, you need to be even more careful because has commonly been

Drats! ...because _what_ has..., of course. Talk about being careful. 

> called "unevaluated expressions" cover symbols and constants too.
> I.e., quote(1) and quote(x) are not calls. And such non-call objects
> can be elements of an expression vector.
> 
> -- 
>    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From deepayan.sarkar at gmail.com  Thu Sep 14 22:40:44 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 14 Sep 2006 13:40:44 -0700
Subject: [R] converting strings to expressions
In-Reply-To: <Pine.LNX.4.64.0609142117460.17107@gannet.stats.ox.ac.uk>
References: <eb555e660609141231t167c3966n3781d4c2d6511b29@mail.gmail.com>
	<x2y7smdyim.fsf@turmalin.kubism.ku.dk>
	<eb555e660609141248r55bee176uff9eb68307cf05ba@mail.gmail.com>
	<Pine.LNX.4.64.0609142117460.17107@gannet.stats.ox.ac.uk>
Message-ID: <eb555e660609141340h65971506o348b9e64820ddc3c@mail.gmail.com>

On 9/14/06, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> On Thu, 14 Sep 2006, Deepayan Sarkar wrote:
>
> [...]
>
> > Ah, I'd forgotten about parse. A link from ?expression might be reasonable.
>
> But it says
>
> Details:
>
>       'Expression' here is not being used in its colloquial sense, that
>       of mathematical expressions.  Those are calls (see 'call') in R,
>       and an R expression vector is a list of calls, typically as
>       returned by 'parse'.
>
> so it already has a link.

Right, I missed that (although I remember reading the first part of
that paragraph).

Deepayan

>
> 'Expression' was often misused in the R documentation, and a couple of
> months ago I tried to move to a more careful usage.
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>


From ethan.johnsons at gmail.com  Fri Sep 15 01:37:44 2006
From: ethan.johnsons at gmail.com (Ethan Johnsons)
Date: Thu, 14 Sep 2006 19:37:44 -0400
Subject: [R] hist for two sets
Message-ID: <5cd96f050609141637p61def982scfdc112d1cb772ac@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060914/6c613165/attachment.pl 

From murdoch at stats.uwo.ca  Fri Sep 15 01:44:55 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 14 Sep 2006 19:44:55 -0400
Subject: [R] Beta stochastic simulation
In-Reply-To: <D71DD12EF4EDE740A09A929F6D2C50994404FD@mailuk1.rms.com>
References: <D71DD12EF4EDE740A09A929F6D2C50994404FD@mailuk1.rms.com>
Message-ID: <4509E977.5020409@stats.uwo.ca>

On 9/14/2006 5:26 PM, Mark Pinkerton wrote:
> Hi Duncan,
> I had also validated the logic with a simple test which is why I was surprised by the differences I was seeing from tthe more complex simulation. I am running R on a Windows 2000 - I'll have to check which version at my desk tomorrow but it's pretty up to date, maybe 6 monthes old. Attached is a code snippet  from my simulation program which is used to estimate multi-event annual losses for US hurricanes. The event set being sampled from is quite large (~14000) with each event and account combination having a unique beta loss distribution. Simply swapping lines 23 and 24 has the effect on results that I mentioned in the previous email. The simulation is large enough that the MC error in the estimated means are negligible.

The code you sent isn't usable, because it's missing your data.  Could 
you please do the following?

  - verify that the behaviour still happens in the current alpha test 
version

  - try to simplify the example code so someone else can run it?  It 
could be that certain values of alpha and beta trigger a bug but the 
ones I tried were fine.

Duncan Murdoch


From learningr at gmail.com  Fri Sep 15 01:47:32 2006
From: learningr at gmail.com (Learning R)
Date: Thu, 14 Sep 2006 19:47:32 -0400
Subject: [R] EBAM ERROR
Message-ID: <30d556050609141647g385ad1a2qa722fc8c8cbacf1b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060914/a392af38/attachment.pl 

From MSchwartz at mn.rr.com  Fri Sep 15 01:55:34 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Thu, 14 Sep 2006 18:55:34 -0500
Subject: [R] hist for two sets
In-Reply-To: <5cd96f050609141637p61def982scfdc112d1cb772ac@mail.gmail.com>
References: <5cd96f050609141637p61def982scfdc112d1cb772ac@mail.gmail.com>
Message-ID: <1158278134.7482.4.camel@localhost.localdomain>

On Thu, 2006-09-14 at 19:37 -0400, Ethan Johnsons wrote:
> A quick question, please.
> 
> x = c(0.0001, 0.0059, 0.0855, 0.9082)
> y = c(0.54, 0.813, 0.379, 0.35)
> 
> where A = 1st set, B = 2nd set, C = 3rd set, D = 4th set respectivley.
> 
> How do you make hist plot side by side for x & y?
> 
> i.e. 0.0001 and then to the right 0.54, 0.0059 and then to the right 0.813,
> etc.
> 
> thx much

You don't want a histogram, but a barplot:

 x <- c(0.0001, 0.0059, 0.0855, 0.9082)
 y <- c(0.54, 0.813, 0.379, 0.35)

 # create a two row matrix with x and y
 height <- rbind(x, y)

 # Use height and set 'beside = TRUE' to get pairs
 # save the bar midpoints in 'mp'
 # Set the bar pair labels to A:D
 mp <- barplot(height, beside = TRUE, ylim = c(0, 1), 
               names.arg = LETTERS[1:4])

 # Draw the bar values above the bars
 text(mp, height, labels = format(height, 4), pos = 3, cex = .75)


See ?barplot, ?text and ?format (or ?formatC or ?sprintf).

HTH,

Marc Schwartz


From zhaoshi at u.washington.edu  Fri Sep 15 02:08:28 2006
From: zhaoshi at u.washington.edu (zhaoshi)
Date: Thu, 14 Sep 2006 17:08:28 -0700
Subject: [R] what does Height represent?
Message-ID: <4509EEFC.4040500@u.washington.edu>

hi--

I am new to R and try to use R cluster my binary data. I use 
hierarchical clustering
plot (hclust (dist(x,method="binary"),method="average"),cex=0.1)
I end up with a cluster Dendrogram. On the left of my dendrogram, there 
is scale called Height from 0.0 to 1.0.
I don't understand what does Height represent. If the Height represents 
the distance scale between two different data point,
it looks like if I add up the length of each branch, I end up with 
distance of some pairs > 1. It is not possible the distance
between any data point will greater than 1. Could some help me out?
 
Thanks


Zhaoshi


From MSchwartz at mn.rr.com  Fri Sep 15 02:11:38 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Thu, 14 Sep 2006 19:11:38 -0500
Subject: [R] group bunch of lines in a data.frame,
	an additional	requirement
In-Reply-To: <e4654710609141335h4ef212c4m2912d0d90cd32acb@mail.gmail.com>
References: <e4654710609130938p313dbe57g19c7e36c4ae51488@mail.gmail.com>
	<971536df0609131044y1a628331k6ea5846adaa6c3f5@mail.gmail.com>
	<e4654710609141335h4ef212c4m2912d0d90cd32acb@mail.gmail.com>
Message-ID: <1158279098.7482.19.camel@localhost.localdomain>

Emmanuel,

I wouldn't be surprised if Gabor comes up with something, but since
aggregate() can only return scalars, you can't do it in one step here.
There are possibilities using other functions such as split(), tapply()
or by(), but each has it own respective limitations requiring more than
one step or post consolidation reformatting. 

You could certainly write a unified wrapper function that would do this
in a single call, but unless you plan on doing this sort of operation a
lot, it is probably easier with multiple calls.

I suspect using Gabor's approach as he had below, combined with my own
on using aggregate() (now twice) then using merge() may be the easiest.

HTH,

Marc

On Thu, 2006-09-14 at 21:35 +0100, Emmanuel Levy wrote:
> Thanks Gabor, that is much faster than using a loop!
> 
> I've got a last question:
> 
> Can you think of a fast way of keeping track of the number of
> observations collapsed for each entry?
> 
> i.e. I'd like to end up with:
> 
> A 2.0 400 ID1 3 (3obs in the first matrix)
> B 0.7 35 ID2 2 (2obs in the first matrix)
> C 5.0 70 ID1 1 (1obs in the first matrix)
> 
> Or is it required to use an temporary matrix that is merged later? (As
> examplified by Mark in a previous email?)
> 
> Thanks a lot for your help,
> 
>   Emmanuel
> 
> On 9/13/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > See below.
> >
> > On 9/13/06, Emmanuel Levy <emmanuel.levy at gmail.com> wrote:
> > > Thanks for pointing me out "aggregate", that works fine!
> > >
> > > There is one complication though: I have mixed types (numerical and character),
> > >
> > > So the matrix is of the form:
> > >
> > > A 1.0 200 ID1
> > > A 3.0 800 ID1
> > > A 2.0 200 ID1
> > > B 0.5 20   ID2
> > > B 0.9 50   ID2
> > > C 5.0 70   ID1
> > >
> > > One letter always has the same ID but one ID can be shared by many
> > > letters (like ID1)
> > >
> > > I just want to keep track of the ID, and get a matrix like:
> > >
> > > A 2.0 400 ID1
> > > B 0.7 35 ID2
> > > C 5.0 70 ID1
> > >
> > > Any idea on how to do that without a loop?
> >
> > If V4 is a function of V1 then you can aggregate by it too and it will
> > appear but have no effect on the classification:
> >
> > > aggregate(DF[2:3], DF[c(1,4)], mean)
> >   V1  V4  V2  V3
> > 1  A ID1 2.0 400
> > 2  C ID1 5.0  70
> > 3  B ID2 0.7  35
> >
> >
> > >
> > >  Many thanks,
> > >
> > >     Emmanuel
> > >
> > > On 9/12/06, Emmanuel Levy <emmanuel.levy at gmail.com> wrote:
> > > > Hello,
> > > >
> > > > I'd like to group the lines of a matrix so that:
> > > > A 1.0 200
> > > > A 3.0 800
> > > > A 2.0 200
> > > > B 0.5 20
> > > > B 0.9 50
> > > > C 5.0 70
> > > >
> > > > Would give:
> > > > A 2.0 400
> > > > B 0.7 35
> > > > C 5.0 70
> > > >
> > > > So all lines corresponding to a letter (level), become a single line
> > > > where all the values of each column are averaged.
> > > >
> > > > I've done that with a loop but it doesn't sound right (it is very
> > > > slow). I imagine there is a
> > > > sort of "apply" shortcut but I can't figure it out.
> > > >
> > > > Please note that it is not exactly a matrix I'm using, the function
> > > > "typeof" tells me it's a list, however I access to it like it was a
> > > > matrix.
> > > >
> > > > Could someone help me with the right function to use, a help topic or
> > > > a piece of code?
> > > >
> > > > Thanks,
> > > >
> > > >   Emmanuel
> > > >
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ggrothendieck at gmail.com  Fri Sep 15 02:15:42 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 14 Sep 2006 20:15:42 -0400
Subject: [R] working with strptime data
In-Reply-To: <000001c6d809$44388d60$eb173981@revansx>
References: <000001c6d809$44388d60$eb173981@revansx>
Message-ID: <971536df0609141715s2b7a4d9dg6555a8056b08c000@mail.gmail.com>

Read the help desk article in R News 4/1.

On 9/14/06, Richard Evans <revans at jlab.org> wrote:
> Dear R-forum,
>
> I am looking for a good resource/help on working with POSIXct values and
> controlling the pretty x-axis labels and tick marks for a data VS time
> plots. Specifically, I wish to do programming logic which creates
> different vertical ablines calculations based on the range of times
> which i am working on. The default plot results are adequate, but I
> would love to make explicit calls on how the x-axis annotates the
> timestamps.
>
> Does anyone have example code or know of a good reference for these
> kinds of R-programming tasks?
>
>
> Here's a simplified example:
> ----------------------------------------------------------
> I have a large data set that consists of N pairs of values and
> timestamps strings.
> Like this:
>
> TheData <- c(1.2,             0.9,             etc...[to the Nth
> datapoint])
> TheTime <- c("9/1/2006 8:00", "9/1/2006 8:13", etc...[to the Nth
> timestamp])
>
> I convert the timestamp strings into POSIXct types as:
>
> TheTime <- as.POSIXct(strptime(TheTime, format="%m/%d/%Y %H:%M"))
>
> And create a plot as such:
>
> plot(MyTime,MyData)
>
> ----------------------------------------------------------
>
> And here is a specific question:
>
> How do I calculate the number of months than are spanned between two
> POSIXct values?
> (i.e. NumOfMonths <- MonthFunction(range(MyTimeStampData))
>
> Thanks-in-advance,
> - rich
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From geoffrey.russell at gmail.com  Fri Sep 15 02:51:34 2006
From: geoffrey.russell at gmail.com (Geoff Russell)
Date: Fri, 15 Sep 2006 10:21:34 +0930
Subject: [R] Table manipulation question
Message-ID: <93c3eada0609141751u4bd95445ibf4ba8a191cc9957@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060915/4702da36/attachment.pl 

From ggrothendieck at gmail.com  Fri Sep 15 02:55:29 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 14 Sep 2006 20:55:29 -0400
Subject: [R] group bunch of lines in a data.frame,
	an additional requirement
In-Reply-To: <e4654710609141335h4ef212c4m2912d0d90cd32acb@mail.gmail.com>
References: <e4654710609130938p313dbe57g19c7e36c4ae51488@mail.gmail.com>
	<971536df0609131044y1a628331k6ea5846adaa6c3f5@mail.gmail.com>
	<e4654710609141335h4ef212c4m2912d0d90cd32acb@mail.gmail.com>
Message-ID: <971536df0609141755l1715b56dw38902fb4397fad94@mail.gmail.com>

Here are three different ways to do it:

# base R
fb <- function(x)
   c(V1 = x$V1[1], V4 = x$V4[1], V2.mean = mean(x$V2),
     V3.mean = mean(x$V3), n = length(x$V1))
do.call(rbind, by(DF, DF[c(1,4)], fb))

# package doBy
library(doBy)
summaryBy(V2 + V3 ~ V1 + V4, DF, FUN = c(mean, length))[,-5]

# package reshape
library(reshape)
f <- function(x) c(mean = mean(x), n = length(x))
cast(melt(DF, id = c(1,4)), V1 + V4 ~ variable, fun.aggregate = f)[,-6]

----

> # base R
> fb <- function(x)
+    c(V1 = x$V1[1], V4 = x$V4[1], V2.mean = mean(x$V2),
+      V3.mean = mean(x$V3), n = length(x$V1))
> do.call(rbind, by(DF, DF[c(1,4)], fb))
     V1 V4 V2.mean V3.mean n
[1,]  1  1     2.0     400 3
[2,]  3  1     5.0      70 1
[3,]  2  2     0.7      35 2
>
> # package doBy
> library(doBy)
> summaryBy(V2 + V3 ~ V1 + V4, DF, FUN = c(mean, length))[,-5]
  V1  V4 mean.V2 mean.V3 length.V3
1  A ID1     2.0     400         3
2  C ID1     5.0      70         1
3  B ID2     0.7      35         2
>
> # package reshape
> library(reshape)
> f <- function(x) c(mean = mean(x), n = length(x))
> cast(melt(DF, id = c(1,4)), V1 + V4 ~ variable, fun.aggregate = f)[,-6]
  V1  V4 V2_mean V2_n V3_mean
1  A ID1     2.0    3     400
2  B ID2     0.7    2      35
3  C ID1     5.0    1      70






---

> library(doBy)
> summaryBy(V2 + V3 ~ V1 + V4, DF, FUN = c(mean, length))[,-5]
  V1  V4 mean.V2 mean.V3 length.V3
1  A ID1     2.0     400         3
2  C ID1     5.0      70         1
3  B ID2     0.7      35         2
>
> library(reshape)
> f <- function(x) c(mean = mean(x), n = length(x))
> cast(melt(DF, id = c(1,4)), V1 + V4 ~ variable, fun.aggregate = f)[,-6]
  V1  V4 V2_mean V2_n V3_mean
1  A ID1     2.0    3     400
2  B ID2     0.7    2      35
3  C ID1     5.0    1      70



On 9/14/06, Emmanuel Levy <emmanuel.levy at gmail.com> wrote:
> Thanks Gabor, that is much faster than using a loop!
>
> I've got a last question:
>
> Can you think of a fast way of keeping track of the number of
> observations collapsed for each entry?
>
> i.e. I'd like to end up with:
>
> A 2.0 400 ID1 3 (3obs in the first matrix)
> B 0.7 35 ID2 2 (2obs in the first matrix)
> C 5.0 70 ID1 1 (1obs in the first matrix)
>
> Or is it required to use an temporary matrix that is merged later? (As
> examplified by Mark in a previous email?)
>
> Thanks a lot for your help,
>
>  Emmanuel
>
> On 9/13/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > See below.
> >
> > On 9/13/06, Emmanuel Levy <emmanuel.levy at gmail.com> wrote:
> > > Thanks for pointing me out "aggregate", that works fine!
> > >
> > > There is one complication though: I have mixed types (numerical and character),
> > >
> > > So the matrix is of the form:
> > >
> > > A 1.0 200 ID1
> > > A 3.0 800 ID1
> > > A 2.0 200 ID1
> > > B 0.5 20   ID2
> > > B 0.9 50   ID2
> > > C 5.0 70   ID1
> > >
> > > One letter always has the same ID but one ID can be shared by many
> > > letters (like ID1)
> > >
> > > I just want to keep track of the ID, and get a matrix like:
> > >
> > > A 2.0 400 ID1
> > > B 0.7 35 ID2
> > > C 5.0 70 ID1
> > >
> > > Any idea on how to do that without a loop?
> >
> > If V4 is a function of V1 then you can aggregate by it too and it will
> > appear but have no effect on the classification:
> >
> > > aggregate(DF[2:3], DF[c(1,4)], mean)
> >   V1  V4  V2  V3
> > 1  A ID1 2.0 400
> > 2  C ID1 5.0  70
> > 3  B ID2 0.7  35
> >
> >
> > >
> > >  Many thanks,
> > >
> > >     Emmanuel
> > >
> > > On 9/12/06, Emmanuel Levy <emmanuel.levy at gmail.com> wrote:
> > > > Hello,
> > > >
> > > > I'd like to group the lines of a matrix so that:
> > > > A 1.0 200
> > > > A 3.0 800
> > > > A 2.0 200
> > > > B 0.5 20
> > > > B 0.9 50
> > > > C 5.0 70
> > > >
> > > > Would give:
> > > > A 2.0 400
> > > > B 0.7 35
> > > > C 5.0 70
> > > >
> > > > So all lines corresponding to a letter (level), become a single line
> > > > where all the values of each column are averaged.
> > > >
> > > > I've done that with a loop but it doesn't sound right (it is very
> > > > slow). I imagine there is a
> > > > sort of "apply" shortcut but I can't figure it out.
> > > >
> > > > Please note that it is not exactly a matrix I'm using, the function
> > > > "typeof" tells me it's a list, however I access to it like it was a
> > > > matrix.
> > > >
> > > > Could someone help me with the right function to use, a help topic or
> > > > a piece of code?
> > > >
> > > > Thanks,
> > > >
> > > >   Emmanuel
> > > >
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
>


From liuwensui at gmail.com  Fri Sep 15 02:57:11 2006
From: liuwensui at gmail.com (Wensui Liu)
Date: Thu, 14 Sep 2006 20:57:11 -0400
Subject: [R] Table manipulation question
In-Reply-To: <93c3eada0609141751u4bd95445ibf4ba8a191cc9957@mail.gmail.com>
References: <93c3eada0609141751u4bd95445ibf4ba8a191cc9957@mail.gmail.com>
Message-ID: <1115a2b00609141757x51884a5fm49d0a1f959ad6e51@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060914/453da90c/attachment.pl 

From ethan.johnsons at gmail.com  Fri Sep 15 04:02:23 2006
From: ethan.johnsons at gmail.com (Ethan Johnsons)
Date: Thu, 14 Sep 2006 22:02:23 -0400
Subject: [R] hist for two sets
In-Reply-To: <1158278134.7482.4.camel@localhost.localdomain>
References: <5cd96f050609141637p61def982scfdc112d1cb772ac@mail.gmail.com>
	<1158278134.7482.4.camel@localhost.localdomain>
Message-ID: <5cd96f050609141902j3f05e47fua0a22ce0524763cc@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060914/4aab9c9f/attachment.pl 

From knaxerov at ix.urz.uni-heidelberg.de  Fri Sep 15 05:55:33 2006
From: knaxerov at ix.urz.uni-heidelberg.de (Kamila Naxerova)
Date: Thu, 14 Sep 2006 23:55:33 -0400
Subject: [R] plot region too large
Message-ID: <450A2435.9020105@ix.urz.uni-heidelberg.de>

Hi!

I don't understand this:

layout(matrix(c(1:10),5,2),heights=c(1,rep(2,4)))
plot(1,1)
error in plot.new() :  plot region too large

Why??????

Thanks!
Kamila


From Kiermeier.Andreas at saugov.sa.gov.au  Fri Sep 15 06:35:03 2006
From: Kiermeier.Andreas at saugov.sa.gov.au (Kiermeier,
	Andreas (PIRSA - SARDI))
Date: Fri, 15 Sep 2006 14:05:03 +0930
Subject: [R] plot region too large
In-Reply-To: <450A2435.9020105@ix.urz.uni-heidelberg.de>
Message-ID: <2CD079F5961A2E4199FD0A177852A51402E67E60@sagemsg0024.sagemsmrd01.sa.gov.au>

The figure margins come from what is set in par("mar"), eg

> layout(matrix(c(1:10),5,2),heights=c(1,rep(2,4)))
> par("mar")
[1] 5.1 4.1 4.1 2.1
> 

There is not enough space left to plot anything with those margins.  You
will need to make them smaller first, e.g.

> par(mar=c(1,1,1,1,))
> plot(1,1)

In which case things work.

Cheers,

Andreas

_____________________________
Dr Andreas Kiermeier
Senior Statistician
SARDI FOOD SAFETY PROGRAM

33 Flemington Street
Glenside   SA   5065
Ph:	+61 8 8207 7884
Fax:	+61 8 8207 7854
Mob:	0423 028 565

Email: Kiermeier.Andreas at saugov.sa.gov.au
Web: http://www.sardi.sa.gov.au/
_____________________________
If you would like to correspond with me in a secure way, using public
key encryption, please contact me directly for details of my GPG public
key or visit the SARDI website, where you can download my public key
from my personal staff page.

The information in this e-mail and attachments (if any) may be
confidential and/or legally privileged. If you are not the intended
recipient, any disclosure, copying, distribution or action taken is
prohibited. SARDI, The South Australian Research and Development
Institute, is the research division of Primary Industries and Resources
(SA) 


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Kamila Naxerova
Sent: Friday, 15 September 2006 13:26
To: r-help at stat.math.ethz.ch
Subject: [R] plot region too large

Hi!

I don't understand this:

layout(matrix(c(1:10),5,2),heights=c(1,rep(2,4)))
plot(1,1)
error in plot.new() :  plot region too large

Why??????

Thanks!
Kamila

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ripley at stats.ox.ac.uk  Fri Sep 15 08:34:11 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 15 Sep 2006 07:34:11 +0100 (BST)
Subject: [R] plot region too large
In-Reply-To: <2CD079F5961A2E4199FD0A177852A51402E67E60@sagemsg0024.sagemsmrd01.sa.gov.au>
References: <2CD079F5961A2E4199FD0A177852A51402E67E60@sagemsg0024.sagemsmrd01.sa.gov.au>
Message-ID: <Pine.LNX.4.64.0609150731150.21383@gannet.stats.ox.ac.uk>

On Fri, 15 Sep 2006, Kiermeier, Andreas (PIRSA - SARDI) wrote:

> The figure margins come from what is set in par("mar"), eg
>
>> layout(matrix(c(1:10),5,2),heights=c(1,rep(2,4)))
>> par("mar")
> [1] 5.1 4.1 4.1 2.1
>>
>
> There is not enough space left to plot anything with those margins.  You
> will need to make them smaller first, e.g.
>
>> par(mar=c(1,1,1,1,))
>> plot(1,1)
>
> In which case things work.

Or as the underlying cause is that the text is too large for a 5x2 layout, 
reduce the pointsize or increase the device region of the device in use.

Using par(mfrow/mfcol) reduces the text size for large layouts: layout() 
does not.


> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Kamila Naxerova
> Sent: Friday, 15 September 2006 13:26
> To: r-help at stat.math.ethz.ch
> Subject: [R] plot region too large
>
> Hi!
>
> I don't understand this:
>
> layout(matrix(c(1:10),5,2),heights=c(1,rep(2,4)))
> plot(1,1)
> error in plot.new() :  plot region too large
>
> Why??????


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Arelia.Werner at ec.gc.ca  Fri Sep 15 02:10:26 2006
From: Arelia.Werner at ec.gc.ca (Werner,Arelia [PYR])
Date: Thu, 14 Sep 2006 17:10:26 -0700
Subject: [R] "ccf versus acf"
Message-ID: <660EA801235B224390215B7D2949E6C36CD2A3@PYRVEX2.pacific.int.ec.gc.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060914/5e0efeae/attachment.pl 

From ploua at allstate.com  Fri Sep 15 03:37:57 2006
From: ploua at allstate.com (Louisell, Paul)
Date: Thu, 14 Sep 2006 18:37:57 -0700
Subject: [R] Kernel Smoothing with more than 2 predictors
Message-ID: <633AD4C78F3E8F489614A06990F6077B01C8AFAB@a0203-xpo0111-s.hodc.ad.allstate.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060914/5d1cd640/attachment.pl 

From bsmstn at yahoo.fr  Thu Sep 14 11:34:46 2006
From: bsmstn at yahoo.fr (Ben Salah Mohamed Selim)
Date: Thu, 14 Sep 2006 11:34:46 +0200 (CEST)
Subject: [R] Help
Message-ID: <20060914093446.23409.qmail@web25413.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060914/74115d8e/attachment.pl 

From rkrug at sun.ac.za  Fri Sep 15 09:32:14 2006
From: rkrug at sun.ac.za (Rainer M Krug)
Date: Fri, 15 Sep 2006 09:32:14 +0200
Subject: [R] execution of source() command
Message-ID: <450A56FE.6020003@sun.ac.za>

Hi

Linux SuSE 10

platform       i686-pc-linux-gnu
arch           i686
os             linux-gnu
system         i686, linux-gnu
status
major          2
minor          3.1
year           2006
month          06
day            01
svn rev        38247
language       R
version.string Version 2.3.1 (2006-06-01)

I have two questions concerning the source("test.R") command.
1) Is there any command which I can put into the test.R script file
which aborts the execution of the script? At the moment I use

CodeToBeExecutedInScript
if (FALSE)
{
	CodeNotToBeExecutedInScript
}

which is not elegant, but it works. I would prefer something like:

CodeToBeExecutedInScript
CommandToAbotrExecutionOfScriptFile
CodeNotToBeExecutedInScript

2) When I call source("test.R") and it is running for some time. Does
changing the file test.R while it is executed change the execution, i.e.
 does the call of source() load the script file into memory and parses
and executes it from there or does it parse and execute the file on the
disk?

Thanks in advance,

Rainer

-- 
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
Biology (UCT)

Department of Conservation Ecology and Entomology
University of Stellenbosch
Matieland 7602
South Africa

Tel:		+27 - (0)72 808 2975 (w)
Fax:		+27 - (0)21 808 3304
Cell:		+27 - (0)83 9479 042

email:	RKrug at sun.ac.za
      	Rainer at krugs.de


From mothsailor at googlemail.com  Fri Sep 15 09:54:48 2006
From: mothsailor at googlemail.com (David Barron)
Date: Fri, 15 Sep 2006 08:54:48 +0100
Subject: [R] Help
In-Reply-To: <20060914093446.23409.qmail@web25413.mail.ukl.yahoo.com>
References: <20060914093446.23409.qmail@web25413.mail.ukl.yahoo.com>
Message-ID: <815b70590609150054j44fc4b12y9fdc75873886eb2@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060915/0b46c5d2/attachment.pl 

From jan.sabee at gmail.com  Fri Sep 15 10:22:04 2006
From: jan.sabee at gmail.com (Jan Sabee)
Date: Fri, 15 Sep 2006 10:22:04 +0200
Subject: [R] predict with logistic regression
Message-ID: <96507a8e0609150122g1487442clfe98df164169063@mail.gmail.com>

I am learning about using logistic regression with glm.
Suppose I have dataset:
duration <- c(45,15,40,83,90,25,35,65,95,35,75,45,50,75,30,25,20,60,70,30,60,61,65,15,20,45,15,25,15,30,40,15,135,20,40)
type <- c(0,0,0,1,1,1,rep(0,5),1,1,1,0,0,1,1,1,rep(0,4),1,1,0,1,0,1,0,0,rep(1,4))
sore <- factor(rep(c("M", "F"), c(16, 19)))
sore.fr <- data.frame(duration, type, sore)
str(sore.fr)

then with glm I have the result.
sorethroat.lg <- glm(sore ~ type+duration, family=binomial, data=sore.fr)
summary(sorethroat.lg, cor=TRUE)

If I have a new dataset then predict it, the result:
new.sore <- data.frame(duration=c(35,25,41,33,30,55,35,62,93,34),
                       type=c(0,1,0,1,0,1,1,1,0,1))
predict(sorethroat.lg, new.sore, type="response")
> predict(sorethroat.lg, new.sore, type="response")
           1            2            3            4            5            6
0.5176877150 0.2750893421 0.5407418590 0.3003664211 0.4984140283 0.3760831903
           7            8            9           10
0.3068890332 0.4017370393 0.7242061280 0.3036178483
>

I know that is probability of predict for new dataset.
My question is how can I know each probability according to class (sore).
I mean that I need the result of predit something like (M=1, F=0):
 1  2  3  4  5  6  7  8  9  10
 1  0  0  0  1  0  1  1  0   1

Sincerelly,
JS


From ripley at stats.ox.ac.uk  Fri Sep 15 10:54:08 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 15 Sep 2006 09:54:08 +0100 (BST)
Subject: [R] "ccf versus acf"
In-Reply-To: <660EA801235B224390215B7D2949E6C36CD2A3@PYRVEX2.pacific.int.ec.gc.ca>
References: <660EA801235B224390215B7D2949E6C36CD2A3@PYRVEX2.pacific.int.ec.gc.ca>
Message-ID: <Pine.LNX.4.64.0609150948210.3687@gannet.stats.ox.ac.uk>

On Thu, 14 Sep 2006, Werner,Arelia [PYR] wrote:

> I am trying to run a cross-correlation using the "ccf()" function. When
> I select plot = TRUE in the ccf() I get a graph which has ACF on the
> y-axis, which would suggest that these y-values are the auto-correlation
> values.

But cross-correlations are part of the ACF for a bivariate time series, so 
why do you think that is a problem?

Your example is not reproducible.  However, there is a reproducible 
example on the help page that looks like cross-correlations (look at the 
value at lag 0). And

> ccf(mdeaths, fdeaths, ylab="ccf")

might be what you are looking for.


> How should I adjust the code to produce a plot that provides the
> cross-correlation values?
>
> Here is my code:
>
> w002dat <- read.csv("w054_1128958_08NM174.csv", header=TRUE)
> w002dat$date <- as.Date(w002dat$date,"%m/%d/%Y")
> attach(w002dat)
> w002ccfhdgw <- ccf((gwneg), (hydro), lag.max = 400, type =
> c("correlation"),
>         plot = TRUE, na.action = na.exclude)
>
> Thank you
>
> Arelia
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

PLEASE do!

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Fri Sep 15 11:03:29 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 15 Sep 2006 10:03:29 +0100 (BST)
Subject: [R] execution of source() command
In-Reply-To: <450A56FE.6020003@sun.ac.za>
References: <450A56FE.6020003@sun.ac.za>
Message-ID: <Pine.LNX.4.64.0609150955240.3687@gannet.stats.ox.ac.uk>

On Fri, 15 Sep 2006, Rainer M Krug wrote:

> Hi
>
> Linux SuSE 10
>
> platform       i686-pc-linux-gnu
> arch           i686
> os             linux-gnu
> system         i686, linux-gnu
> status
> major          2
> minor          3.1
> year           2006
> month          06
> day            01
> svn rev        38247
> language       R
> version.string Version 2.3.1 (2006-06-01)
>
> I have two questions concerning the source("test.R") command.
> 1) Is there any command which I can put into the test.R script file
> which aborts the execution of the script? At the moment I use
>
> CodeToBeExecutedInScript
> if (FALSE)
> {
> 	CodeNotToBeExecutedInScript
> }
>
> which is not elegant, but it works. I would prefer something like:
>
> CodeToBeExecutedInScript
> CommandToAbotrExecutionOfScriptFile
> CodeNotToBeExecutedInScript

?stop or ?q, depending on what you mean by 'aborts the execution'.

> 2) When I call source("test.R") and it is running for some time. Does
> changing the file test.R while it is executed change the execution, i.e.
> does the call of source() load the script file into memory and parses
> and executes it from there or does it parse and execute the file on the 
> disk.

The help file says:

      'source' causes R to accept its input from the named file or URL
      (the name must be quoted) or connection.  Input is read and
      'parse'd by from that file until the end of the file is reached,
      then the parsed expressions are evaluated sequentially in the
      chosen environment.

library(fortunes); fortune("WTFM")


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From rkrug at sun.ac.za  Fri Sep 15 12:10:50 2006
From: rkrug at sun.ac.za (Rainer M Krug)
Date: Fri, 15 Sep 2006 12:10:50 +0200
Subject: [R] execution of source() command
In-Reply-To: <Pine.LNX.4.64.0609150955240.3687@gannet.stats.ox.ac.uk>
References: <450A56FE.6020003@sun.ac.za>
	<Pine.LNX.4.64.0609150955240.3687@gannet.stats.ox.ac.uk>
Message-ID: <450A7C2A.6030003@sun.ac.za>

Prof Brian Ripley wrote:
> On Fri, 15 Sep 2006, Rainer M Krug wrote:
> 
>> Hi
>> I have two questions concerning the source("test.R") command.
>> 1) Is there any command which I can put into the test.R script file
>> which aborts the execution of the script? At the moment I use
>>
>> CodeToBeExecutedInScript
>> if (FALSE)
>> {
>>     CodeNotToBeExecutedInScript
>> }
>>
>> which is not elegant, but it works. I would prefer something like:
>>
>> CodeToBeExecutedInScript
>> CommandToAbotrExecutionOfScriptFile
>> CodeNotToBeExecutedInScript
> 
> ?stop or ?q, depending on what you mean by 'aborts the execution'.

Thanks - I would have expected to find it together with the other
flowcontrol commands like if, while, ...

> 
>> 2) When I call source("test.R") and it is running for some time. Does
>> changing the file test.R while it is executed change the execution, i.e.
>> does the call of source() load the script file into memory and parses
>> and executes it from there or does it parse and execute the file on
>> the disk.
> 
> The help file says:
> 
>      'source' causes R to accept its input from the named file or URL
>      (the name must be quoted) or connection.  Input is read and
>      'parse'd by from that file until the end of the file is reached,
>      then the parsed expressions are evaluated sequentially in the
>      chosen environment.
> 
> library(fortunes); fortune("WTFM")
> 
> 

Again - thanks a lot for the clarification

Rainer
-- 
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
Biology (UCT)

Department of Conservation Ecology and Entomology
University of Stellenbosch
Matieland 7602
South Africa

Tel:		+27 - (0)72 808 2975 (w)
Fax:		+27 - (0)21 808 3304
Cell:		+27 - (0)83 9479 042

email:	RKrug at sun.ac.za
      	Rainer at krugs.de


From murdoch at stats.uwo.ca  Fri Sep 15 13:15:11 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 15 Sep 2006 07:15:11 -0400
Subject: [R] Beta stochastic simulation
In-Reply-To: <D71DD12EF4EDE740A09A929F6D2C5099750EAF@mailuk1.rms.com>
References: <D71DD12EF4EDE740A09A929F6D2C5099750EAF@mailuk1.rms.com>
Message-ID: <450A8B3F.1040506@stats.uwo.ca>

On 9/15/2006 6:43 AM, Mark Pinkerton wrote:
> Hi Duncan,
> Thanks for having a look at this. Find attached a zip with all the
> relevant files to run the simulation. I am running this on Windows XP, R
> version 2.3.1. 

Does the error still occur in a recent alpha build?  It's downloadable 
from CRAN, in cran.r-project.org/bin/windows/base/rtest.html  (though I 
notice the version there is a week old; I'd better kick the build script).

Duncan Murdoch
'
> 
> The correct result for the average annual loss, calculated using a
> battle tested FFT engine, is 1,609,361 The summary stats from my last
> run are below:
> 
>> # Summary stats
>> summary(totals.losses1)
>      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
>         0         0      1142   1620000    698000 132500000 
>> mean(totals.losses1)
> [1] 1619891
>> sd(totals.losses1)/sqrt(length(totals.losses1))
> [1] 77949.25
>> summary(totals.losses2)
>      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
>         0         0      2352   2341000    749700 141700000 
>> mean(totals.losses2)
> [1] 2341237
>> sd(totals.losses2)/sqrt(length(totals.losses2))
> [1] 129695.9
> 
> Thanks,
> Mark
> 
> Mark Pinkerton
> Risk Management Solutions 
> Peninsular House
> 30 Monument Street
> London EC3R 8HB 
> UK 
>  
> www.RMS.com 
> Tel:  +44 20 7444 7783 
> Fax: +44 20 7444 7601
> 
> -----Original Message-----
> From: Duncan Murdoch [mailto:murdoch at stats.uwo.ca] 
> Sent: 15 September 2006 00:45
> To: Mark Pinkerton
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Beta stochastic simulation
> 
> On 9/14/2006 5:26 PM, Mark Pinkerton wrote:
>> Hi Duncan,
>> I had also validated the logic with a simple test which is why I was
> surprised by the differences I was seeing from tthe more complex
> simulation. I am running R on a Windows 2000 - I'll have to check which
> version at my desk tomorrow but it's pretty up to date, maybe 6 monthes
> old. Attached is a code snippet  from my simulation program which is
> used to estimate multi-event annual losses for US hurricanes. The event
> set being sampled from is quite large (~14000) with each event and
> account combination having a unique beta loss distribution. Simply
> swapping lines 23 and 24 has the effect on results that I mentioned in
> the previous email. The simulation is large enough that the MC error in
> the estimated means are negligible.
> 
> The code you sent isn't usable, because it's missing your data.  Could
> you please do the following?
> 
>   - verify that the behaviour still happens in the current alpha test
> version
> 
>   - try to simplify the example code so someone else can run it?  It
> could be that certain values of alpha and beta trigger a bug but the
> ones I tried were fine.
> 
> Duncan Murdoch
> 
> 
> This message and any attachments contain information that may be RMS Inc. 
> confidential and/or privileged.  If you are not the intended recipient 
> (or authorized to receive for the intended recipient), and have received 
> this message in error, any use, disclosure or distribution is strictly 
> prohibited.   If you have received this message in error, please notify 
> the sender immediately by replying to the e-mail and permanently deleting 
> the message from your computer and/or storage system.


From Mark.Pinkerton at rms.com  Fri Sep 15 13:51:07 2006
From: Mark.Pinkerton at rms.com (Mark Pinkerton)
Date: Fri, 15 Sep 2006 12:51:07 +0100
Subject: [R] Beta stochastic simulation
Message-ID: <D71DD12EF4EDE740A09A929F6D2C5099750EB1@mailuk1.rms.com>

I have just installed 2.4.0 alpha and the problem persists. Here is the
output of the run:

> # Summary stats
> summary(totals.losses1)
     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
        0         0      1284   1617000    685100 219200000 
> mean(totals.losses1)
[1] 1617219
> sd(totals.losses1)/sqrt(length(totals.losses1))
[1] 78863.17
> 
> summary(totals.losses2)
     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
        0         0      1422   2417000    819200 118200000 
> mean(totals.losses2)
[1] 2417471
> sd(totals.losses2)/sqrt(length(totals.losses2))
[1] 134866.0 

Thanks,
Mark

Mark Pinkerton
Risk Management Solutions 
Peninsular House
30 Monument Street
London EC3R 8HB 
UK 
 
www.RMS.com 
Tel:  +44 20 7444 7783 
Fax: +44 20 7444 7601

-----Original Message-----
From: Duncan Murdoch [mailto:murdoch at stats.uwo.ca] 
Sent: 15 September 2006 12:15
To: Mark Pinkerton
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Beta stochastic simulation

On 9/15/2006 6:43 AM, Mark Pinkerton wrote:
> Hi Duncan,
> Thanks for having a look at this. Find attached a zip with all the 
> relevant files to run the simulation. I am running this on Windows XP,

> R version 2.3.1.

Does the error still occur in a recent alpha build?  It's downloadable
from CRAN, in cran.r-project.org/bin/windows/base/rtest.html  (though I
notice the version there is a week old; I'd better kick the build
script).

Duncan Murdoch
'
> 
> The correct result for the average annual loss, calculated using a 
> battle tested FFT engine, is 1,609,361 The summary stats from my last 
> run are below:
> 
>> # Summary stats
>> summary(totals.losses1)
>      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
>         0         0      1142   1620000    698000 132500000 
>> mean(totals.losses1)
> [1] 1619891
>> sd(totals.losses1)/sqrt(length(totals.losses1))
> [1] 77949.25
>> summary(totals.losses2)
>      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
>         0         0      2352   2341000    749700 141700000 
>> mean(totals.losses2)
> [1] 2341237
>> sd(totals.losses2)/sqrt(length(totals.losses2))
> [1] 129695.9
> 
> Thanks,
> Mark
> 
> Mark Pinkerton
> Risk Management Solutions
> Peninsular House
> 30 Monument Street
> London EC3R 8HB
> UK
>  
> www.RMS.com
> Tel:  +44 20 7444 7783
> Fax: +44 20 7444 7601
> 
> -----Original Message-----
> From: Duncan Murdoch [mailto:murdoch at stats.uwo.ca]
> Sent: 15 September 2006 00:45
> To: Mark Pinkerton
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Beta stochastic simulation
> 
> On 9/14/2006 5:26 PM, Mark Pinkerton wrote:
>> Hi Duncan,
>> I had also validated the logic with a simple test which is why I was
> surprised by the differences I was seeing from tthe more complex 
> simulation. I am running R on a Windows 2000 - I'll have to check 
> which version at my desk tomorrow but it's pretty up to date, maybe 6 
> monthes old. Attached is a code snippet  from my simulation program 
> which is used to estimate multi-event annual losses for US hurricanes.

> The event set being sampled from is quite large (~14000) with each 
> event and account combination having a unique beta loss distribution. 
> Simply swapping lines 23 and 24 has the effect on results that I 
> mentioned in the previous email. The simulation is large enough that 
> the MC error in the estimated means are negligible.
> 
> The code you sent isn't usable, because it's missing your data.  Could

> you please do the following?
> 
>   - verify that the behaviour still happens in the current alpha test 
> version
> 
>   - try to simplify the example code so someone else can run it?  It 
> could be that certain values of alpha and beta trigger a bug but the 
> ones I tried were fine.
> 
> Duncan Murdoch
> 
> 
> This message and any attachments contain information that may be RMS
Inc. 
> confidential and/or privileged.  If you are not the intended recipient

> (or authorized to receive for the intended recipient), and have 
> received this message in error, any use, disclosure or distribution is
strictly
> prohibited.   If you have received this message in error, please
notify 
> the sender immediately by replying to the e-mail and permanently 
> deleting the message from your computer and/or storage system.


This message and any attachments contain information that may be RMS Inc. 
confidential and/or privileged.  If you are not the intended recipient 
(or authorized to receive for the intended recipient), and have received 
this message in error, any use, disclosure or distribution is strictly 
prohibited.   If you have received this message in error, please notify 
the sender immediately by replying to the e-mail and permanently deleting 
the message from your computer and/or storage system.


From murdoch at stats.uwo.ca  Fri Sep 15 14:06:34 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 15 Sep 2006 08:06:34 -0400
Subject: [R] Beta stochastic simulation
In-Reply-To: <D71DD12EF4EDE740A09A929F6D2C5099750EB1@mailuk1.rms.com>
References: <D71DD12EF4EDE740A09A929F6D2C5099750EB1@mailuk1.rms.com>
Message-ID: <450A974A.6040004@stats.uwo.ca>

On 9/15/2006 7:51 AM, Mark Pinkerton wrote:
> I have just installed 2.4.0 alpha and the problem persists. Here is the
> output of the run:

Thanks.  I'll try your script and see if I can track down what's going on.

Duncan Murdoch

> 
>> # Summary stats
>> summary(totals.losses1)
>      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
>         0         0      1284   1617000    685100 219200000 
>> mean(totals.losses1)
> [1] 1617219
>> sd(totals.losses1)/sqrt(length(totals.losses1))
> [1] 78863.17
>> 
>> summary(totals.losses2)
>      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
>         0         0      1422   2417000    819200 118200000 
>> mean(totals.losses2)
> [1] 2417471
>> sd(totals.losses2)/sqrt(length(totals.losses2))
> [1] 134866.0 
> 
> Thanks,
> Mark
> 
> Mark Pinkerton
> Risk Management Solutions 
> Peninsular House
> 30 Monument Street
> London EC3R 8HB 
> UK 
>  
> www.RMS.com 
> Tel:  +44 20 7444 7783 
> Fax: +44 20 7444 7601
> 
> -----Original Message-----
> From: Duncan Murdoch [mailto:murdoch at stats.uwo.ca] 
> Sent: 15 September 2006 12:15
> To: Mark Pinkerton
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Beta stochastic simulation
> 
> On 9/15/2006 6:43 AM, Mark Pinkerton wrote:
>> Hi Duncan,
>> Thanks for having a look at this. Find attached a zip with all the 
>> relevant files to run the simulation. I am running this on Windows XP,
> 
>> R version 2.3.1.
> 
> Does the error still occur in a recent alpha build?  It's downloadable
> from CRAN, in cran.r-project.org/bin/windows/base/rtest.html  (though I
> notice the version there is a week old; I'd better kick the build
> script).
> 
> Duncan Murdoch
> '
>> 
>> The correct result for the average annual loss, calculated using a 
>> battle tested FFT engine, is 1,609,361 The summary stats from my last 
>> run are below:
>> 
>>> # Summary stats
>>> summary(totals.losses1)
>>      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
>>         0         0      1142   1620000    698000 132500000 
>>> mean(totals.losses1)
>> [1] 1619891
>>> sd(totals.losses1)/sqrt(length(totals.losses1))
>> [1] 77949.25
>>> summary(totals.losses2)
>>      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
>>         0         0      2352   2341000    749700 141700000 
>>> mean(totals.losses2)
>> [1] 2341237
>>> sd(totals.losses2)/sqrt(length(totals.losses2))
>> [1] 129695.9
>> 
>> Thanks,
>> Mark
>> 
>> Mark Pinkerton
>> Risk Management Solutions
>> Peninsular House
>> 30 Monument Street
>> London EC3R 8HB
>> UK
>>  
>> www.RMS.com
>> Tel:  +44 20 7444 7783
>> Fax: +44 20 7444 7601
>> 
>> -----Original Message-----
>> From: Duncan Murdoch [mailto:murdoch at stats.uwo.ca]
>> Sent: 15 September 2006 00:45
>> To: Mark Pinkerton
>> Cc: r-help at stat.math.ethz.ch
>> Subject: Re: [R] Beta stochastic simulation
>> 
>> On 9/14/2006 5:26 PM, Mark Pinkerton wrote:
>>> Hi Duncan,
>>> I had also validated the logic with a simple test which is why I was
>> surprised by the differences I was seeing from tthe more complex 
>> simulation. I am running R on a Windows 2000 - I'll have to check 
>> which version at my desk tomorrow but it's pretty up to date, maybe 6 
>> monthes old. Attached is a code snippet  from my simulation program 
>> which is used to estimate multi-event annual losses for US hurricanes.
> 
>> The event set being sampled from is quite large (~14000) with each 
>> event and account combination having a unique beta loss distribution. 
>> Simply swapping lines 23 and 24 has the effect on results that I 
>> mentioned in the previous email. The simulation is large enough that 
>> the MC error in the estimated means are negligible.
>> 
>> The code you sent isn't usable, because it's missing your data.  Could
> 
>> you please do the following?
>> 
>>   - verify that the behaviour still happens in the current alpha test 
>> version
>> 
>>   - try to simplify the example code so someone else can run it?  It 
>> could be that certain values of alpha and beta trigger a bug but the 
>> ones I tried were fine.
>> 
>> Duncan Murdoch
>> 
>> 
>> This message and any attachments contain information that may be RMS
> Inc. 
>> confidential and/or privileged.  If you are not the intended recipient
> 
>> (or authorized to receive for the intended recipient), and have 
>> received this message in error, any use, disclosure or distribution is
> strictly
>> prohibited.   If you have received this message in error, please
> notify 
>> the sender immediately by replying to the e-mail and permanently 
>> deleting the message from your computer and/or storage system.
> 
> 
> This message and any attachments contain information that may be RMS Inc. 
> confidential and/or privileged.  If you are not the intended recipient 
> (or authorized to receive for the intended recipient), and have received 
> this message in error, any use, disclosure or distribution is strictly 
> prohibited.   If you have received this message in error, please notify 
> the sender immediately by replying to the e-mail and permanently deleting 
> the message from your computer and/or storage system.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From murdoch at stats.uwo.ca  Fri Sep 15 14:22:47 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 15 Sep 2006 08:22:47 -0400
Subject: [R] Beta stochastic simulation
In-Reply-To: <D71DD12EF4EDE740A09A929F6D2C5099750EB1@mailuk1.rms.com>
References: <D71DD12EF4EDE740A09A929F6D2C5099750EB1@mailuk1.rms.com>
Message-ID: <450A9B17.7020001@stats.uwo.ca>

On 9/15/2006 7:51 AM, Mark Pinkerton wrote:
> I have just installed 2.4.0 alpha and the problem persists. Here is the
> output of the run:

When I run it, I get a series of warning messages from qbeta.  Do you 
get those?

Duncan Murdoch

> 
>> # Summary stats
>> summary(totals.losses1)
>      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
>         0         0      1284   1617000    685100 219200000 
>> mean(totals.losses1)
> [1] 1617219
>> sd(totals.losses1)/sqrt(length(totals.losses1))
> [1] 78863.17
>> 
>> summary(totals.losses2)
>      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
>         0         0      1422   2417000    819200 118200000 
>> mean(totals.losses2)
> [1] 2417471
>> sd(totals.losses2)/sqrt(length(totals.losses2))
> [1] 134866.0 
> 
> Thanks,
> Mark
> 
> Mark Pinkerton
> Risk Management Solutions 
> Peninsular House
> 30 Monument Street
> London EC3R 8HB 
> UK 
>  
> www.RMS.com 
> Tel:  +44 20 7444 7783 
> Fax: +44 20 7444 7601
> 
> -----Original Message-----
> From: Duncan Murdoch [mailto:murdoch at stats.uwo.ca] 
> Sent: 15 September 2006 12:15
> To: Mark Pinkerton
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Beta stochastic simulation
> 
> On 9/15/2006 6:43 AM, Mark Pinkerton wrote:
>> Hi Duncan,
>> Thanks for having a look at this. Find attached a zip with all the 
>> relevant files to run the simulation. I am running this on Windows XP,
> 
>> R version 2.3.1.
> 
> Does the error still occur in a recent alpha build?  It's downloadable
> from CRAN, in cran.r-project.org/bin/windows/base/rtest.html  (though I
> notice the version there is a week old; I'd better kick the build
> script).
> 
> Duncan Murdoch
> '
>> 
>> The correct result for the average annual loss, calculated using a 
>> battle tested FFT engine, is 1,609,361 The summary stats from my last 
>> run are below:
>> 
>>> # Summary stats
>>> summary(totals.losses1)
>>      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
>>         0         0      1142   1620000    698000 132500000 
>>> mean(totals.losses1)
>> [1] 1619891
>>> sd(totals.losses1)/sqrt(length(totals.losses1))
>> [1] 77949.25
>>> summary(totals.losses2)
>>      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
>>         0         0      2352   2341000    749700 141700000 
>>> mean(totals.losses2)
>> [1] 2341237
>>> sd(totals.losses2)/sqrt(length(totals.losses2))
>> [1] 129695.9
>> 
>> Thanks,
>> Mark
>> 
>> Mark Pinkerton
>> Risk Management Solutions
>> Peninsular House
>> 30 Monument Street
>> London EC3R 8HB
>> UK
>>  
>> www.RMS.com
>> Tel:  +44 20 7444 7783
>> Fax: +44 20 7444 7601
>> 
>> -----Original Message-----
>> From: Duncan Murdoch [mailto:murdoch at stats.uwo.ca]
>> Sent: 15 September 2006 00:45
>> To: Mark Pinkerton
>> Cc: r-help at stat.math.ethz.ch
>> Subject: Re: [R] Beta stochastic simulation
>> 
>> On 9/14/2006 5:26 PM, Mark Pinkerton wrote:
>>> Hi Duncan,
>>> I had also validated the logic with a simple test which is why I was
>> surprised by the differences I was seeing from tthe more complex 
>> simulation. I am running R on a Windows 2000 - I'll have to check 
>> which version at my desk tomorrow but it's pretty up to date, maybe 6 
>> monthes old. Attached is a code snippet  from my simulation program 
>> which is used to estimate multi-event annual losses for US hurricanes.
> 
>> The event set being sampled from is quite large (~14000) with each 
>> event and account combination having a unique beta loss distribution. 
>> Simply swapping lines 23 and 24 has the effect on results that I 
>> mentioned in the previous email. The simulation is large enough that 
>> the MC error in the estimated means are negligible.
>> 
>> The code you sent isn't usable, because it's missing your data.  Could
> 
>> you please do the following?
>> 
>>   - verify that the behaviour still happens in the current alpha test 
>> version
>> 
>>   - try to simplify the example code so someone else can run it?  It 
>> could be that certain values of alpha and beta trigger a bug but the 
>> ones I tried were fine.
>> 
>> Duncan Murdoch
>> 
>> 
>> This message and any attachments contain information that may be RMS
> Inc. 
>> confidential and/or privileged.  If you are not the intended recipient
> 
>> (or authorized to receive for the intended recipient), and have 
>> received this message in error, any use, disclosure or distribution is
> strictly
>> prohibited.   If you have received this message in error, please
> notify 
>> the sender immediately by replying to the e-mail and permanently 
>> deleting the message from your computer and/or storage system.
> 
> 
> This message and any attachments contain information that may be RMS Inc. 
> confidential and/or privileged.  If you are not the intended recipient 
> (or authorized to receive for the intended recipient), and have received 
> this message in error, any use, disclosure or distribution is strictly 
> prohibited.   If you have received this message in error, please notify 
> the sender immediately by replying to the e-mail and permanently deleting 
> the message from your computer and/or storage system.


From qiongwangufl at gmail.com  Fri Sep 15 14:35:13 2006
From: qiongwangufl at gmail.com (Qiong Wang)
Date: Fri, 15 Sep 2006 08:35:13 -0400
Subject: [R] missing data codes
Message-ID: <80f2ffb90609150535m45ad12f0wb04402f5de71a266@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060915/e04fc15f/attachment.pl 

From Mark.Pinkerton at rms.com  Fri Sep 15 15:06:49 2006
From: Mark.Pinkerton at rms.com (Mark Pinkerton)
Date: Fri, 15 Sep 2006 14:06:49 +0100
Subject: [R] Beta stochastic simulation
Message-ID: <D71DD12EF4EDE740A09A929F6D2C5099750EB2@mailuk1.rms.com>

Yes, indeed I do. Is there any way I can dig into these a bit more? I
have also just tried using the OO inverse beta from the distr package
and this seems to work.

Mark Pinkerton
Risk Management Solutions 
Peninsular House
30 Monument Street
London EC3R 8HB 
UK 
 
www.RMS.com 
Tel:  +44 20 7444 7783 
Fax: +44 20 7444 7601

-----Original Message-----
From: Duncan Murdoch [mailto:murdoch at stats.uwo.ca] 
Sent: 15 September 2006 12:15
To: Mark Pinkerton
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Beta stochastic simulation

On 9/15/2006 6:43 AM, Mark Pinkerton wrote:
> Hi Duncan,
> Thanks for having a look at this. Find attached a zip with all the 
> relevant files to run the simulation. I am running this on Windows XP,

> R version 2.3.1.

Does the error still occur in a recent alpha build?  It's downloadable
from CRAN, in cran.r-project.org/bin/windows/base/rtest.html  (though I
notice the version there is a week old; I'd better kick the build
script).

Duncan Murdoch
'
> 
> The correct result for the average annual loss, calculated using a 
> battle tested FFT engine, is 1,609,361 The summary stats from my last 
> run are below:
> 
>> # Summary stats
>> summary(totals.losses1)
>      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
>         0         0      1142   1620000    698000 132500000 
>> mean(totals.losses1)
> [1] 1619891
>> sd(totals.losses1)/sqrt(length(totals.losses1))
> [1] 77949.25
>> summary(totals.losses2)
>      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
>         0         0      2352   2341000    749700 141700000 
>> mean(totals.losses2)
> [1] 2341237
>> sd(totals.losses2)/sqrt(length(totals.losses2))
> [1] 129695.9
> 
> Thanks,
> Mark
> 
> Mark Pinkerton
> Risk Management Solutions
> Peninsular House
> 30 Monument Street
> London EC3R 8HB
> UK
>  
> www.RMS.com
> Tel:  +44 20 7444 7783
> Fax: +44 20 7444 7601
> 
> -----Original Message-----
> From: Duncan Murdoch [mailto:murdoch at stats.uwo.ca]
> Sent: 15 September 2006 00:45
> To: Mark Pinkerton
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Beta stochastic simulation
> 
> On 9/14/2006 5:26 PM, Mark Pinkerton wrote:
>> Hi Duncan,
>> I had also validated the logic with a simple test which is why I was
> surprised by the differences I was seeing from tthe more complex 
> simulation. I am running R on a Windows 2000 - I'll have to check 
> which version at my desk tomorrow but it's pretty up to date, maybe 6 
> monthes old. Attached is a code snippet  from my simulation program 
> which is used to estimate multi-event annual losses for US hurricanes.

> The event set being sampled from is quite large (~14000) with each 
> event and account combination having a unique beta loss distribution. 
> Simply swapping lines 23 and 24 has the effect on results that I 
> mentioned in the previous email. The simulation is large enough that 
> the MC error in the estimated means are negligible.
> 
> The code you sent isn't usable, because it's missing your data.  Could

> you please do the following?
> 
>   - verify that the behaviour still happens in the current alpha test 
> version
> 
>   - try to simplify the example code so someone else can run it?  It 
> could be that certain values of alpha and beta trigger a bug but the 
> ones I tried were fine.
> 
> Duncan Murdoch
> 
> 
> This message and any attachments contain information that may be RMS
Inc. 
> confidential and/or privileged.  If you are not the intended recipient

> (or authorized to receive for the intended recipient), and have 
> received this message in error, any use, disclosure or distribution is
strictly
> prohibited.   If you have received this message in error, please
notify 
> the sender immediately by replying to the e-mail and permanently 
> deleting the message from your computer and/or storage system.


This message and any attachments contain information that may be RMS Inc. 
confidential and/or privileged.  If you are not the intended recipient 
(or authorized to receive for the intended recipient), and have received 
this message in error, any use, disclosure or distribution is strictly 
prohibited.   If you have received this message in error, please notify 
the sender immediately by replying to the e-mail and permanently deleting 
the message from your computer and/or storage system.


From murdoch at stats.uwo.ca  Fri Sep 15 15:19:28 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 15 Sep 2006 09:19:28 -0400
Subject: [R] Beta stochastic simulation
In-Reply-To: <D71DD12EF4EDE740A09A929F6D2C5099750EB2@mailuk1.rms.com>
References: <D71DD12EF4EDE740A09A929F6D2C5099750EB2@mailuk1.rms.com>
Message-ID: <450AA85F.6040509@stats.uwo.ca>

On 9/15/2006 9:06 AM, Mark Pinkerton wrote:
> Yes, indeed I do. Is there any way I can dig into these a bit more? I
> have also just tried using the OO inverse beta from the distr package
> and this seems to work.

This is pretty irritating.  You were getting warnings from R that the 
calculations were inaccurate, and you didn't think that was worth 
mentioning?

I'll continue working on this after "Risk Management Solutions" 
compensates me for my wasted time, and pays me in advance for additional 
work.

Duncan Murdoch

> 
> Mark Pinkerton
> Risk Management Solutions 
> Peninsular House
> 30 Monument Street
> London EC3R 8HB 
> UK 
>  
> www.RMS.com 
> Tel:  +44 20 7444 7783 
> Fax: +44 20 7444 7601
> 
> -----Original Message-----
> From: Duncan Murdoch [mailto:murdoch at stats.uwo.ca] 
> Sent: 15 September 2006 12:15
> To: Mark Pinkerton
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Beta stochastic simulation
> 
> On 9/15/2006 6:43 AM, Mark Pinkerton wrote:
>> Hi Duncan,
>> Thanks for having a look at this. Find attached a zip with all the 
>> relevant files to run the simulation. I am running this on Windows XP,
> 
>> R version 2.3.1.
> 
> Does the error still occur in a recent alpha build?  It's downloadable
> from CRAN, in cran.r-project.org/bin/windows/base/rtest.html  (though I
> notice the version there is a week old; I'd better kick the build
> script).
> 
> Duncan Murdoch
> '
>> 
>> The correct result for the average annual loss, calculated using a 
>> battle tested FFT engine, is 1,609,361 The summary stats from my last 
>> run are below:
>> 
>>> # Summary stats
>>> summary(totals.losses1)
>>      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
>>         0         0      1142   1620000    698000 132500000 
>>> mean(totals.losses1)
>> [1] 1619891
>>> sd(totals.losses1)/sqrt(length(totals.losses1))
>> [1] 77949.25
>>> summary(totals.losses2)
>>      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
>>         0         0      2352   2341000    749700 141700000 
>>> mean(totals.losses2)
>> [1] 2341237
>>> sd(totals.losses2)/sqrt(length(totals.losses2))
>> [1] 129695.9
>> 
>> Thanks,
>> Mark
>> 
>> Mark Pinkerton
>> Risk Management Solutions
>> Peninsular House
>> 30 Monument Street
>> London EC3R 8HB
>> UK
>>  
>> www.RMS.com
>> Tel:  +44 20 7444 7783
>> Fax: +44 20 7444 7601
>> 
>> -----Original Message-----
>> From: Duncan Murdoch [mailto:murdoch at stats.uwo.ca]
>> Sent: 15 September 2006 00:45
>> To: Mark Pinkerton
>> Cc: r-help at stat.math.ethz.ch
>> Subject: Re: [R] Beta stochastic simulation
>> 
>> On 9/14/2006 5:26 PM, Mark Pinkerton wrote:
>>> Hi Duncan,
>>> I had also validated the logic with a simple test which is why I was
>> surprised by the differences I was seeing from tthe more complex 
>> simulation. I am running R on a Windows 2000 - I'll have to check 
>> which version at my desk tomorrow but it's pretty up to date, maybe 6 
>> monthes old. Attached is a code snippet  from my simulation program 
>> which is used to estimate multi-event annual losses for US hurricanes.
> 
>> The event set being sampled from is quite large (~14000) with each 
>> event and account combination having a unique beta loss distribution. 
>> Simply swapping lines 23 and 24 has the effect on results that I 
>> mentioned in the previous email. The simulation is large enough that 
>> the MC error in the estimated means are negligible.
>> 
>> The code you sent isn't usable, because it's missing your data.  Could
> 
>> you please do the following?
>> 
>>   - verify that the behaviour still happens in the current alpha test 
>> version
>> 
>>   - try to simplify the example code so someone else can run it?  It 
>> could be that certain values of alpha and beta trigger a bug but the 
>> ones I tried were fine.
>> 
>> Duncan Murdoch
>> 
>> 
>> This message and any attachments contain information that may be RMS
> Inc. 
>> confidential and/or privileged.  If you are not the intended recipient
> 
>> (or authorized to receive for the intended recipient), and have 
>> received this message in error, any use, disclosure or distribution is
> strictly
>> prohibited.   If you have received this message in error, please
> notify 
>> the sender immediately by replying to the e-mail and permanently 
>> deleting the message from your computer and/or storage system.
> 
> 
> This message and any attachments contain information that may be RMS Inc. 
> confidential and/or privileged.  If you are not the intended recipient 
> (or authorized to receive for the intended recipient), and have received 
> this message in error, any use, disclosure or distribution is strictly 
> prohibited.   If you have received this message in error, please notify 
> the sender immediately by replying to the e-mail and permanently deleting 
> the message from your computer and/or storage system.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From Arne.Muller at sanofi-aventis.com  Fri Sep 15 15:32:00 2006
From: Arne.Muller at sanofi-aventis.com (Arne.Muller at sanofi-aventis.com)
Date: Fri, 15 Sep 2006 15:32:00 +0200
Subject: [R] graphics and 'layout' question
Message-ID: <C80ECAFA2ACC1B45BE45D133ED660ADE02C8BE7D@CRBSMXSUSR04>

Hello,

I got stuck with a graphics question: I've 3 figures that I present on a single page (window) via 'layout'. The layout is 

layout(matrix(c(1,1,2,3), 2, 2, byrow=TRUE));

so that the frst plot spans the both columns in row one. Now I'd like to magnify the fist figure so that it takes 20% more vertical space (i.e. more space for the y-axis). How would I do this in R?

    thanks a lot for your help,

    Arne


From mothsailor at googlemail.com  Fri Sep 15 15:38:24 2006
From: mothsailor at googlemail.com (David Barron)
Date: Fri, 15 Sep 2006 14:38:24 +0100
Subject: [R] missing data codes
In-Reply-To: <80f2ffb90609150535m45ad12f0wb04402f5de71a266@mail.gmail.com>
References: <80f2ffb90609150535m45ad12f0wb04402f5de71a266@mail.gmail.com>
Message-ID: <815b70590609150638v39d3b64dw748295142941a3ab@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060915/3223db22/attachment.pl 

From ccleland at optonline.net  Fri Sep 15 15:42:37 2006
From: ccleland at optonline.net (Chuck Cleland)
Date: Fri, 15 Sep 2006 09:42:37 -0400
Subject: [R] graphics and 'layout' question
In-Reply-To: <C80ECAFA2ACC1B45BE45D133ED660ADE02C8BE7D@CRBSMXSUSR04>
References: <C80ECAFA2ACC1B45BE45D133ED660ADE02C8BE7D@CRBSMXSUSR04>
Message-ID: <450AADCD.6020508@optonline.net>

Arne.Muller at sanofi-aventis.com wrote:
> Hello,
> 
> I got stuck with a graphics question: I've 3 figures that I present on a single page (window) via 'layout'. The layout is 
> 
> layout(matrix(c(1,1,2,3), 2, 2, byrow=TRUE));
> 
> so that the frst plot spans the both columns in row one. Now I'd like to magnify the fist figure so that it takes 20% more vertical space (i.e. more space for the y-axis). How would I do this in R?

  Are you looking for the heights argument?  For example:

nf <- layout(matrix(c(1,1,2,3), 2, 2, byrow=TRUE), heights=c(70,30))
layout.show(nf)

>     thanks a lot for your help,
> 
>     Arne
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From mothsailor at googlemail.com  Fri Sep 15 15:46:24 2006
From: mothsailor at googlemail.com (David Barron)
Date: Fri, 15 Sep 2006 14:46:24 +0100
Subject: [R] graphics and 'layout' question
In-Reply-To: <C80ECAFA2ACC1B45BE45D133ED660ADE02C8BE7D@CRBSMXSUSR04>
References: <C80ECAFA2ACC1B45BE45D133ED660ADE02C8BE7D@CRBSMXSUSR04>
Message-ID: <815b70590609150646h7ad38bb2xe09ccc248f4545ca@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060915/699fc76c/attachment.pl 

From jmacdon at med.umich.edu  Fri Sep 15 15:45:24 2006
From: jmacdon at med.umich.edu (James W. MacDonald)
Date: Fri, 15 Sep 2006 09:45:24 -0400
Subject: [R] graphics and 'layout' question
In-Reply-To: <C80ECAFA2ACC1B45BE45D133ED660ADE02C8BE7D@CRBSMXSUSR04>
References: <C80ECAFA2ACC1B45BE45D133ED660ADE02C8BE7D@CRBSMXSUSR04>
Message-ID: <450AAE74.7020900@med.umich.edu>

Arne.Muller at sanofi-aventis.com wrote:
> Hello,
> 
> I got stuck with a graphics question: I've 3 figures that I present
> on a single page (window) via 'layout'. The layout is
> 
> layout(matrix(c(1,1,2,3), 2, 2, byrow=TRUE));
> 
> so that the frst plot spans the both columns in row one. Now I'd like
> to magnify the fist figure so that it takes 20% more vertical space
> (i.e. more space for the y-axis). How would I do this in R?

 From ?layout

heights: a vector of values for the heights of rows on the device.
           Relative and absolute heights can be specified, see 'widths'
           above.

So something like
layout(matrix(c(1,1,2,3),2,2,byrow = TRUE), heights = c(0.6, 0.4))

should do the trick.

Best,

Jim


> 
> thanks a lot for your help,
> 
> Arne
> 
> ______________________________________________ 
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the
> posting guide http://www.R-project.org/posting-guide.html and provide
> commented, minimal, self-contained, reproducible code.


-- 
James W. MacDonald, M.S.
Biostatistician
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623


**********************************************************
Electronic Mail is not secure, may not be read every day, and should not be used for urgent or sensitive issues.


From ana.pmartins at ine.pt  Fri Sep 15 15:52:44 2006
From: ana.pmartins at ine.pt (Ana Patricia Martins)
Date: Fri, 15 Sep 2006 14:52:44 +0100
Subject: [R] missing data codes
Message-ID: <E97312684A84D511BDD40002A50968D6086DB5F8@lxpobw01.ine.pt>

See MICE
 
http://www.multiple-imputation.com/


-----Original Message-----
From: David Barron [mailto:mothsailor at googlemail.com] 
Sent: sexta-feira, 15 de Setembro de 2006 14:38
To: Qiong Wang; r-help
Subject: Re: [R] missing data codes

You've not given us much information to go on!  Have you tried

> help(aregImpute,package="Hmisc")


On 15/09/06, Qiong Wang <qiongwangufl at gmail.com> wrote:
>
> Dear all,
>
> I am new to R. I wish to use R's multiple imputation to deal with missing
> data. I have a data set with the size around 300 observations and 150
> variables. I checked the help function in R and could not locate how to
> write the codes for this. can anyone give a hand?
>
> Do appreciate your time and kindness!
> Q.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From emmanuel.levy at gmail.com  Fri Sep 15 16:32:49 2006
From: emmanuel.levy at gmail.com (Emmanuel Levy)
Date: Fri, 15 Sep 2006 15:32:49 +0100
Subject: [R] group bunch of lines in a data.frame,
	an additional requirement
In-Reply-To: <e4654710609150659v584bfe3bw91eea68bf60f76f9@mail.gmail.com>
References: <e4654710609130938p313dbe57g19c7e36c4ae51488@mail.gmail.com>
	<971536df0609131044y1a628331k6ea5846adaa6c3f5@mail.gmail.com>
	<e4654710609141335h4ef212c4m2912d0d90cd32acb@mail.gmail.com>
	<971536df0609141755l1715b56dw38902fb4397fad94@mail.gmail.com>
	<e4654710609150659v584bfe3bw91eea68bf60f76f9@mail.gmail.com>
Message-ID: <e4654710609150732h651e298bga6f8d632ca63a742@mail.gmail.com>

(re)-Hello
I actually thought about another possibility with a "1" column, a sum
(instead of a mean), and a division of the columns for which I want
the mean:

> DF = data.frame( V1=c("A","A","A","B","B","C") , V2=c(1,3,2,0.5,0.9,5.0), V3=c(200,800,200,20,50,70), V4=c("ID1","ID1","ID1","ID2","ID2","ID3"))
> DF2 = cbind(DF,o=rep(1,length(DF[,1])))
> DF3 = aggregate(DF2[,c(2,3,5)], data.frame(code=DF2[,1],id=DF2[,4]), sum, na.rm=T)
> DF3[,3:4]=DF3[,3:4]/DF3[,5]
> DF3
  code  id  V2  V3 o
1    A ID1 2.0 400 3
2    B ID2 0.7  35 2
3    C ID3 5.0  70 1

Thanks again,

  Best,

   Emmanuel

On 9/15/06, Emmanuel Levy <emmanuel.levy at gmail.com> wrote:
> Dear Mark, dear Gabor,
>
> Thanks again for your help! It is great to be able to get answers when
> no-one (can you believe this?) uses R in your lab.
>
> The package doBy looks really convenient for many puposes.
>
> Best,
>
>    Emmanuel
>
> On 9/15/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > Here are three different ways to do it:
> >
> > # base R
> > fb <- function(x)
> >    c(V1 = x$V1[1], V4 = x$V4[1], V2.mean = mean(x$V2),
> >      V3.mean = mean(x$V3), n = length(x$V1))
> > do.call(rbind, by(DF, DF[c(1,4)], fb))
> >
> > # package doBy
> > library(doBy)
> > summaryBy(V2 + V3 ~ V1 + V4, DF, FUN = c(mean, length))[,-5]
> >
> > # package reshape
> > library(reshape)
> > f <- function(x) c(mean = mean(x), n = length(x))
> > cast(melt(DF, id = c(1,4)), V1 + V4 ~ variable, fun.aggregate = f)[,-6]
> >
> > ----
> >
> > > # base R
> > > fb <- function(x)
> > +    c(V1 = x$V1[1], V4 = x$V4[1], V2.mean = mean(x$V2),
> > +      V3.mean = mean(x$V3), n = length(x$V1))
> > > do.call(rbind, by(DF, DF[c(1,4)], fb))
> >      V1 V4 V2.mean V3.mean n
> > [1,]  1  1     2.0     400 3
> > [2,]  3  1     5.0      70 1
> > [3,]  2  2     0.7      35 2
> > >
> > > # package doBy
> > > library(doBy)
> > > summaryBy(V2 + V3 ~ V1 + V4, DF, FUN = c(mean, length))[,-5]
> >   V1  V4 mean.V2 mean.V3 length.V3
> > 1  A ID1     2.0     400         3
> > 2  C ID1     5.0      70         1
> > 3  B ID2     0.7      35         2
> > >
> > > # package reshape
> > > library(reshape)
> > > f <- function(x) c(mean = mean(x), n = length(x))
> > > cast(melt(DF, id = c(1,4)), V1 + V4 ~ variable, fun.aggregate = f)[,-6]
> >   V1  V4 V2_mean V2_n V3_mean
> > 1  A ID1     2.0    3     400
> > 2  B ID2     0.7    2      35
> > 3  C ID1     5.0    1      70
> >
> >
> >
> >
> >
> >
> > ---
> >
> > > library(doBy)
> > > summaryBy(V2 + V3 ~ V1 + V4, DF, FUN = c(mean, length))[,-5]
> >   V1  V4 mean.V2 mean.V3 length.V3
> > 1  A ID1     2.0     400         3
> > 2  C ID1     5.0      70         1
> > 3  B ID2     0.7      35         2
> > >
> > > library(reshape)
> > > f <- function(x) c(mean = mean(x), n = length(x))
> > > cast(melt(DF, id = c(1,4)), V1 + V4 ~ variable, fun.aggregate = f)[,-6]
> >   V1  V4 V2_mean V2_n V3_mean
> > 1  A ID1     2.0    3     400
> > 2  B ID2     0.7    2      35
> > 3  C ID1     5.0    1      70
> >
> >
> >
> > On 9/14/06, Emmanuel Levy <emmanuel.levy at gmail.com> wrote:
> > > Thanks Gabor, that is much faster than using a loop!
> > >
> > > I've got a last question:
> > >
> > > Can you think of a fast way of keeping track of the number of
> > > observations collapsed for each entry?
> > >
> > > i.e. I'd like to end up with:
> > >
> > > A 2.0 400 ID1 3 (3obs in the first matrix)
> > > B 0.7 35 ID2 2 (2obs in the first matrix)
> > > C 5.0 70 ID1 1 (1obs in the first matrix)
> > >
> > > Or is it required to use an temporary matrix that is merged later? (As
> > > examplified by Mark in a previous email?)
> > >
> > > Thanks a lot for your help,
> > >
> > >  Emmanuel
> > >
> > > On 9/13/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > > > See below.
> > > >
> > > > On 9/13/06, Emmanuel Levy <emmanuel.levy at gmail.com> wrote:
> > > > > Thanks for pointing me out "aggregate", that works fine!
> > > > >
> > > > > There is one complication though: I have mixed types (numerical and character),
> > > > >
> > > > > So the matrix is of the form:
> > > > >
> > > > > A 1.0 200 ID1
> > > > > A 3.0 800 ID1
> > > > > A 2.0 200 ID1
> > > > > B 0.5 20   ID2
> > > > > B 0.9 50   ID2
> > > > > C 5.0 70   ID1
> > > > >
> > > > > One letter always has the same ID but one ID can be shared by many
> > > > > letters (like ID1)
> > > > >
> > > > > I just want to keep track of the ID, and get a matrix like:
> > > > >
> > > > > A 2.0 400 ID1
> > > > > B 0.7 35 ID2
> > > > > C 5.0 70 ID1
> > > > >
> > > > > Any idea on how to do that without a loop?
> > > >
> > > > If V4 is a function of V1 then you can aggregate by it too and it will
> > > > appear but have no effect on the classification:
> > > >
> > > > > aggregate(DF[2:3], DF[c(1,4)], mean)
> > > >   V1  V4  V2  V3
> > > > 1  A ID1 2.0 400
> > > > 2  C ID1 5.0  70
> > > > 3  B ID2 0.7  35
> > > >
> > > >
> > > > >
> > > > >  Many thanks,
> > > > >
> > > > >     Emmanuel
> > > > >
> > > > > On 9/12/06, Emmanuel Levy <emmanuel.levy at gmail.com> wrote:
> > > > > > Hello,
> > > > > >
> > > > > > I'd like to group the lines of a matrix so that:
> > > > > > A 1.0 200
> > > > > > A 3.0 800
> > > > > > A 2.0 200
> > > > > > B 0.5 20
> > > > > > B 0.9 50
> > > > > > C 5.0 70
> > > > > >
> > > > > > Would give:
> > > > > > A 2.0 400
> > > > > > B 0.7 35
> > > > > > C 5.0 70
> > > > > >
> > > > > > So all lines corresponding to a letter (level), become a single line
> > > > > > where all the values of each column are averaged.
> > > > > >
> > > > > > I've done that with a loop but it doesn't sound right (it is very
> > > > > > slow). I imagine there is a
> > > > > > sort of "apply" shortcut but I can't figure it out.
> > > > > >
> > > > > > Please note that it is not exactly a matrix I'm using, the function
> > > > > > "typeof" tells me it's a list, however I access to it like it was a
> > > > > > matrix.
> > > > > >
> > > > > > Could someone help me with the right function to use, a help topic or
> > > > > > a piece of code?
> > > > > >
> > > > > > Thanks,
> > > > > >
> > > > > >   Emmanuel
> > > > > >
> > > > >
> > > > > ______________________________________________
> > > > > R-help at stat.math.ethz.ch mailing list
> > > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > > > and provide commented, minimal, self-contained, reproducible code.
> > > > >
> > > >
> > >
> >
>


From depire at gmail.com  Fri Sep 15 16:45:31 2006
From: depire at gmail.com (Alexandre Depire)
Date: Fri, 15 Sep 2006 16:45:31 +0200
Subject: [R] Histogram of data with categorical varialbe
Message-ID: <19de4f9e0609150745h23b357eek9978f63ab3ce7b75@mail.gmail.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20060915/8b4027b0/attachment.pl 

From ggrothendieck at gmail.com  Fri Sep 15 16:47:25 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 15 Sep 2006 10:47:25 -0400
Subject: [R] group bunch of lines in a data.frame,
	an additional requirement
In-Reply-To: <e4654710609150732h651e298bga6f8d632ca63a742@mail.gmail.com>
References: <e4654710609130938p313dbe57g19c7e36c4ae51488@mail.gmail.com>
	<971536df0609131044y1a628331k6ea5846adaa6c3f5@mail.gmail.com>
	<e4654710609141335h4ef212c4m2912d0d90cd32acb@mail.gmail.com>
	<971536df0609141755l1715b56dw38902fb4397fad94@mail.gmail.com>
	<e4654710609150659v584bfe3bw91eea68bf60f76f9@mail.gmail.com>
	<e4654710609150732h651e298bga6f8d632ca63a742@mail.gmail.com>
Message-ID: <971536df0609150747v7f021c35mea9cdb72f43d31c@mail.gmail.com>

Good idea.  You could write it compactly like this:

> transform(aggregate(cbind(DF[2:3], o = 1), DF[c(1,4)], sum, na.rm = TRUE),
+  V2 = V2/o, V3 = V3/o)

  V1  V4  V2  V3 o
1  A ID1 2.0 400 3
2  B ID2 0.7  35 2
3  C ID3 5.0  70 1

On 9/15/06, Emmanuel Levy <emmanuel.levy at gmail.com> wrote:
> (re)-Hello
> I actually thought about another possibility with a "1" column, a sum
> (instead of a mean), and a division of the columns for which I want
> the mean:
>
> > DF = data.frame( V1=c("A","A","A","B","B","C") , V2=c(1,3,2,0.5,0.9,5.0), V3=c(200,800,200,20,50,70), V4=c("ID1","ID1","ID1","ID2","ID2","ID3"))
> > DF2 = cbind(DF,o=rep(1,length(DF[,1])))
> > DF3 = aggregate(DF2[,c(2,3,5)], data.frame(code=DF2[,1],id=DF2[,4]), sum, na.rm=T)
> > DF3[,3:4]=DF3[,3:4]/DF3[,5]
> > DF3
>  code  id  V2  V3 o
> 1    A ID1 2.0 400 3
> 2    B ID2 0.7  35 2
> 3    C ID3 5.0  70 1
>
> Thanks again,
>
>  Best,
>
>   Emmanuel
>
> On 9/15/06, Emmanuel Levy <emmanuel.levy at gmail.com> wrote:
> > Dear Mark, dear Gabor,
> >
> > Thanks again for your help! It is great to be able to get answers when
> > no-one (can you believe this?) uses R in your lab.
> >
> > The package doBy looks really convenient for many puposes.
> >
> > Best,
> >
> >    Emmanuel
> >
> > On 9/15/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > > Here are three different ways to do it:
> > >
> > > # base R
> > > fb <- function(x)
> > >    c(V1 = x$V1[1], V4 = x$V4[1], V2.mean = mean(x$V2),
> > >      V3.mean = mean(x$V3), n = length(x$V1))
> > > do.call(rbind, by(DF, DF[c(1,4)], fb))
> > >
> > > # package doBy
> > > library(doBy)
> > > summaryBy(V2 + V3 ~ V1 + V4, DF, FUN = c(mean, length))[,-5]
> > >
> > > # package reshape
> > > library(reshape)
> > > f <- function(x) c(mean = mean(x), n = length(x))
> > > cast(melt(DF, id = c(1,4)), V1 + V4 ~ variable, fun.aggregate = f)[,-6]
> > >
> > > ----
> > >
> > > > # base R
> > > > fb <- function(x)
> > > +    c(V1 = x$V1[1], V4 = x$V4[1], V2.mean = mean(x$V2),
> > > +      V3.mean = mean(x$V3), n = length(x$V1))
> > > > do.call(rbind, by(DF, DF[c(1,4)], fb))
> > >      V1 V4 V2.mean V3.mean n
> > > [1,]  1  1     2.0     400 3
> > > [2,]  3  1     5.0      70 1
> > > [3,]  2  2     0.7      35 2
> > > >
> > > > # package doBy
> > > > library(doBy)
> > > > summaryBy(V2 + V3 ~ V1 + V4, DF, FUN = c(mean, length))[,-5]
> > >   V1  V4 mean.V2 mean.V3 length.V3
> > > 1  A ID1     2.0     400         3
> > > 2  C ID1     5.0      70         1
> > > 3  B ID2     0.7      35         2
> > > >
> > > > # package reshape
> > > > library(reshape)
> > > > f <- function(x) c(mean = mean(x), n = length(x))
> > > > cast(melt(DF, id = c(1,4)), V1 + V4 ~ variable, fun.aggregate = f)[,-6]
> > >   V1  V4 V2_mean V2_n V3_mean
> > > 1  A ID1     2.0    3     400
> > > 2  B ID2     0.7    2      35
> > > 3  C ID1     5.0    1      70
> > >
> > >
> > >
> > >
> > >
> > >
> > > ---
> > >
> > > > library(doBy)
> > > > summaryBy(V2 + V3 ~ V1 + V4, DF, FUN = c(mean, length))[,-5]
> > >   V1  V4 mean.V2 mean.V3 length.V3
> > > 1  A ID1     2.0     400         3
> > > 2  C ID1     5.0      70         1
> > > 3  B ID2     0.7      35         2
> > > >
> > > > library(reshape)
> > > > f <- function(x) c(mean = mean(x), n = length(x))
> > > > cast(melt(DF, id = c(1,4)), V1 + V4 ~ variable, fun.aggregate = f)[,-6]
> > >   V1  V4 V2_mean V2_n V3_mean
> > > 1  A ID1     2.0    3     400
> > > 2  B ID2     0.7    2      35
> > > 3  C ID1     5.0    1      70
> > >
> > >
> > >
> > > On 9/14/06, Emmanuel Levy <emmanuel.levy at gmail.com> wrote:
> > > > Thanks Gabor, that is much faster than using a loop!
> > > >
> > > > I've got a last question:
> > > >
> > > > Can you think of a fast way of keeping track of the number of
> > > > observations collapsed for each entry?
> > > >
> > > > i.e. I'd like to end up with:
> > > >
> > > > A 2.0 400 ID1 3 (3obs in the first matrix)
> > > > B 0.7 35 ID2 2 (2obs in the first matrix)
> > > > C 5.0 70 ID1 1 (1obs in the first matrix)
> > > >
> > > > Or is it required to use an temporary matrix that is merged later? (As
> > > > examplified by Mark in a previous email?)
> > > >
> > > > Thanks a lot for your help,
> > > >
> > > >  Emmanuel
> > > >
> > > > On 9/13/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > > > > See below.
> > > > >
> > > > > On 9/13/06, Emmanuel Levy <emmanuel.levy at gmail.com> wrote:
> > > > > > Thanks for pointing me out "aggregate", that works fine!
> > > > > >
> > > > > > There is one complication though: I have mixed types (numerical and character),
> > > > > >
> > > > > > So the matrix is of the form:
> > > > > >
> > > > > > A 1.0 200 ID1
> > > > > > A 3.0 800 ID1
> > > > > > A 2.0 200 ID1
> > > > > > B 0.5 20   ID2
> > > > > > B 0.9 50   ID2
> > > > > > C 5.0 70   ID1
> > > > > >
> > > > > > One letter always has the same ID but one ID can be shared by many
> > > > > > letters (like ID1)
> > > > > >
> > > > > > I just want to keep track of the ID, and get a matrix like:
> > > > > >
> > > > > > A 2.0 400 ID1
> > > > > > B 0.7 35 ID2
> > > > > > C 5.0 70 ID1
> > > > > >
> > > > > > Any idea on how to do that without a loop?
> > > > >
> > > > > If V4 is a function of V1 then you can aggregate by it too and it will
> > > > > appear but have no effect on the classification:
> > > > >
> > > > > > aggregate(DF[2:3], DF[c(1,4)], mean)
> > > > >   V1  V4  V2  V3
> > > > > 1  A ID1 2.0 400
> > > > > 2  C ID1 5.0  70
> > > > > 3  B ID2 0.7  35
> > > > >
> > > > >
> > > > > >
> > > > > >  Many thanks,
> > > > > >
> > > > > >     Emmanuel
> > > > > >
> > > > > > On 9/12/06, Emmanuel Levy <emmanuel.levy at gmail.com> wrote:
> > > > > > > Hello,
> > > > > > >
> > > > > > > I'd like to group the lines of a matrix so that:
> > > > > > > A 1.0 200
> > > > > > > A 3.0 800
> > > > > > > A 2.0 200
> > > > > > > B 0.5 20
> > > > > > > B 0.9 50
> > > > > > > C 5.0 70
> > > > > > >
> > > > > > > Would give:
> > > > > > > A 2.0 400
> > > > > > > B 0.7 35
> > > > > > > C 5.0 70
> > > > > > >
> > > > > > > So all lines corresponding to a letter (level), become a single line
> > > > > > > where all the values of each column are averaged.
> > > > > > >
> > > > > > > I've done that with a loop but it doesn't sound right (it is very
> > > > > > > slow). I imagine there is a
> > > > > > > sort of "apply" shortcut but I can't figure it out.
> > > > > > >
> > > > > > > Please note that it is not exactly a matrix I'm using, the function
> > > > > > > "typeof" tells me it's a list, however I access to it like it was a
> > > > > > > matrix.
> > > > > > >
> > > > > > > Could someone help me with the right function to use, a help topic or
> > > > > > > a piece of code?
> > > > > > >
> > > > > > > Thanks,
> > > > > > >
> > > > > > >   Emmanuel
> > > > > > >
> > > > > >
> > > > > > ______________________________________________
> > > > > > R-help at stat.math.ethz.ch mailing list
> > > > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > > > > and provide commented, minimal, self-contained, reproducible code.
> > > > > >
> > > > >
> > > >
> > >
> >
>


From f.harrell at vanderbilt.edu  Fri Sep 15 17:16:21 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 15 Sep 2006 10:16:21 -0500
Subject: [R] missing data codes
In-Reply-To: <815b70590609150638v39d3b64dw748295142941a3ab@mail.gmail.com>
References: <80f2ffb90609150535m45ad12f0wb04402f5de71a266@mail.gmail.com>
	<815b70590609150638v39d3b64dw748295142941a3ab@mail.gmail.com>
Message-ID: <450AC3C5.7040905@vanderbilt.edu>

David Barron wrote:
> You've not given us much information to go on!  Have you tried
> 
>> help(aregImpute,package="Hmisc")

And for that sample size you'll have to tell aregImpute to force all 
continuous variables to act linearly

Frank

> 
> 
> On 15/09/06, Qiong Wang <qiongwangufl at gmail.com> wrote:
>> Dear all,
>>
>> I am new to R. I wish to use R's multiple imputation to deal with missing
>> data. I have a data set with the size around 300 observations and 150
>> variables. I checked the help function in R and could not locate how to
>> write the codes for this. can anyone give a hand?
>>
>> Do appreciate your time and kindness!
>> Q.
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> 
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From f.harrell at vanderbilt.edu  Fri Sep 15 17:17:54 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 15 Sep 2006 10:17:54 -0500
Subject: [R] Dear FE  Harrell How can I get rreport ?
In-Reply-To: <loom.20060914T073026-853@post.gmane.org>
References: <4508286D.1090305@vanderbilt.edu>	<20060913195033.51077.qmail@web25708.mail.ukl.yahoo.com>
	<loom.20060914T073026-853@post.gmane.org>
Message-ID: <450AC422.5070303@vanderbilt.edu>

Anupam Tyagi wrote:
> justin bem <justin_bem <at> yahoo.fr> writes:
> 
>> Mr Harrell,
>>
>>  After reading discussion about R output and SAS output , I will like to use
> rreport package. I a windows XP
>> user 
>>
>>  Sincerly
> 
> See:
> 
> http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/Rreport
> 
> Anupam.
> 

rreport has not been put in a package but all the source code is 
available on a per-function basis in our online cvs repository.  What is 
really lacking is documentation but there is a detailed example on the 
above web site.
-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From rfrancois at mango-solutions.com  Fri Sep 15 17:19:42 2006
From: rfrancois at mango-solutions.com (Romain Francois)
Date: Fri, 15 Sep 2006 16:19:42 +0100
Subject: [R] Histogram of data with categorical varialbe
In-Reply-To: <19de4f9e0609150745h23b357eek9978f63ab3ce7b75@mail.gmail.com>
References: <19de4f9e0609150745h23b357eek9978f63ab3ce7b75@mail.gmail.com>
Message-ID: <450AC48E.6090107@mango-solutions.com>

Alexandre Depire wrote:
> Hello,
> I have the following data:
> Km Sex
> 250 1
> 300 2
> 290 2
> 600 1
> 450 2
> 650 1
> .........
>
> I would like to obtain one histogram where the data (or the part) of each
> sex is visible, it is like cumulative histogram or spinogram.
> To be more comprehensible, i would like to know if the following graph is
> obtainable easily in R. It is the first graph on page 5 in the following
> document http://jasp.ism.ac.jp/~nakanoj/workshop04/TalkII.pdf
>   
Something like :

d <- data.frame( x = rnorm(100), sex = sample(c(1,2), replace=TRUE, 
size=100))
out <- hist(d$x, col="gray")
hist(d$x[d$sex==2], col="red", add=T, breaks=out$breaks)
legend("topleft", c("male","female") , fill=c("gray","red"))
box()


Cheers,

Romain

-- 
*mangosolutions*
/data analysis that delivers/

Tel   +44 1249 467 467
Fax   +44 1249 467 468


From gunter.berton at gene.com  Fri Sep 15 17:24:09 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 15 Sep 2006 08:24:09 -0700
Subject: [R] FW: R Reference Card and other help (especially useful for
	Newbies)
Message-ID: <000901c6d8da$fdab2600$711f210a@gne.windows.gene.com>

 
Hi all: 

  
Newbies (and others!) may find useful the R Reference Card made available by

Tom Short of EPRI Solutions at http://www.rpad.org/Rpad/Rpad-refcard.pdf  or
through 
the "Contributed" link on CRAN (where some other reference cards are also 
linked). It categorizes and organizes a bunch of R's basic, most used 
functions so that they can be easily found. For example, paste() is under 
the "Strings" heading and expand.grid() is under "Data Creation." For 
newbies struggling to find the right R function as well as veterans who 
can't quite remember the function name, it's very handy. 

Also don't forget R's other Help facilties: 

help.search("keyword or phrase") to search the **installed** man pages 

RSiteSearch("keyword or phrase") to search the CRAN website via Jonathan
Baron's search engine. This can also be done directly from CRAN by following
the "search" link there.

And, occasionally, find()/apropos() to search the ** attached** packages for
functions using regexp's. 

Though R certainly can be intimidating, please **do** try these measures
first before posting questions to the list. And please **do** read the other
basic R reference materials. Better and faster answers can often be found
this way.

  
-- Bert Gunter 
Genentech Non-Clinical Statistics 
South San Francisco, CA 
  
"The business of the statistician is to catalyze the scientific learning 
process."  - George E. P. Box


From Achim.Zeileis at wu-wien.ac.at  Fri Sep 15 17:36:57 2006
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Fri, 15 Sep 2006 17:36:57 +0200
Subject: [R] Histogram of data with categorical varialbe
In-Reply-To: <19de4f9e0609150745h23b357eek9978f63ab3ce7b75@mail.gmail.com>
References: <19de4f9e0609150745h23b357eek9978f63ab3ce7b75@mail.gmail.com>
Message-ID: <20060915173657.d4e5106c.Achim.Zeileis@wu-wien.ac.at>

On Fri, 15 Sep 2006 16:45:31 +0200 Alexandre Depire wrote:

> Hello,
> I have the following data:
> Km Sex
> 250 1
> 300 2
> 290 2
> 600 1
> 450 2
> 650 1
> .........
> 
> I would like to obtain one histogram where the data (or the part) of
> each sex is visible, it is like cumulative histogram or spinogram.
> To be more comprehensible, i would like to know if the following
> graph is obtainable easily in R. It is the first graph on page 5 in
> the following document
> http://jasp.ism.ac.jp/~nakanoj/workshop04/TalkII.pdf

I think it's not available out of the box, but you can easily generate
this yourself. Using the space shuttle o-ring data as an example (see
also ?spineplot):

  fail <- factor(c(2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1,
    1, 2, 1, 1, 1, 1, 1), levels = c(1, 2), labels = c("no", "yes"))
  temperature <- c(53, 57, 58, 63, 66, 67, 67, 67, 68, 69, 70, 70,
    70, 70, 72, 73, 75, 75, 76, 76, 78, 79, 81)

The you can do the following:

  ## generate unconditional histogram of numeric variable P(x)
  col <- gray.colors(2)
  ht <- hist(temperature, freq = FALSE, col = col[2])
  ## using the same breaks, compute the conditional histogram P(x|group)
  br <- ht$breaks
  ht2 <- hist(temperature[fail == "yes"], plot = FALSE, freq = FALSE,
    breaks = br)
  ## and then add the rectangles using the joint densities
  ## P(x & group) = P(x|group) * P(group)
  rect(br[-length(br)], 0, br[-1], ht2$density * mean(fail == "yes"),
    col = col[1])

Furthermore, you can take a look at the "iplots" package which should
provide an interactive approach to this.
Z
 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.
>


From Joe-Byers at utulsa.edu  Fri Sep 15 17:43:47 2006
From: Joe-Byers at utulsa.edu (Joe W. Byers)
Date: Fri, 15 Sep 2006 10:43:47 -0500
Subject: [R] Formula aruguments with NLS and model.frame()
Message-ID: <eeehnh$8hk$1@sea.gmane.org>

I could use some help understanding how nls parses the formula argument
to a model.frame and estimates the model.  I am trying to utilize the
functionality of the nls formula argument to modify garchFit() to handle
other variables in the mean equation besides just an arma(u,v)
specification.

My nonlinear model is
      y<-nls(t~a*sin(w*2*pi/365*id+p)+b*id+int,data=t1,
	start=list(w=.5,a=.1,p=.5,b=init.y$coef[2],int=init.y$coef[1] ),
	control=list(maxiter=1000000,minFactor=1e-18))
where t is change in daily temperatures, id is just a time trend and the
a*sin is a one year fourier series.

I have tried to debug the nls code using the following code
t1<-data.frame(t=as.vector(x),id=index(x))
data=t1;
formula <- as.formula(t ~ a *sin(w *2* pi/365 * id + p) + b * id + int);
      varNames <- all.vars(formula)
      algorithm<-'default';
      mf <- match.call(definition=nls,expand.dots=FALSE,
      call('nls',formula, data=parent.frame(),start,control = nls.control(),
      algorithm = "default", trace = FALSE,
      subset, weights, na.action, model = FALSE, lower = -Inf,
      upper = Inf));
      mWeights<-F;#missing(weights);
	start=list(w=.5,a=.1,p=.5,b=init.y$coef[2],int=init.y$coef[1] );
      pnames <- names(start);
       varNames <- varNames[is.na(match(varNames, pnames, nomatch = NA))]

	varIndex <- sapply(varNames,
		function(varName, data, respLength) {
          	length(eval(as.name(varName), data))%%respLength == 0},
          	 data, length(eval(formula[[2]], data))
          );
	mf$formula <- as.formula(paste("~", paste(varNames[varIndex],
          collapse = "+")), env = environment(formula));
	mf$start <- NULL;mf$control <- NULL;mf$algorithm <- NULL;
	mf$trace <- NULL;mf$model <- NULL;
      mf$lower <- NULL;mf$upper <- NULL;
      mf[[1]] <- as.name("model.frame");
      mf<-evalq(mf,data);
      n<-nrow(mf)
      mf<-as.list(mf);
      wts <- if (!mWeights)
          model.weights(mf)
      else rep(1, n)
      if (any(wts < 0 | is.na(wts)))
          stop("missing or negative weights not allowed")

      m <- switch(algorithm,
      		plinear = nlsModel.plinear(formula, mf, start, wts),
      		port = nlsModel(formula, mf, start, wts, upper),
      		nlsModel(formula, mf, start, wts));

I am struggling with the environment issues associated with performing
these operations.  I did not include the data because it is 9000 
observations of temperature data.  If anyone would like the data, I can 
provide it or a subset in a csv file.


thank you
Joe


From e.rapsomaniki at mail.cryst.bbk.ac.uk  Fri Sep 15 17:44:42 2006
From: e.rapsomaniki at mail.cryst.bbk.ac.uk (e.rapsomaniki at mail.cryst.bbk.ac.uk)
Date: Fri, 15 Sep 2006 16:44:42 +0100
Subject: [R] Grouping columns in a data frame based on the values of a column
Message-ID: <1158335082.450aca6a7dce2@webmail.cryst.bbk.ac.uk>

Dear R users,

This is a trivial question, there might even be an R function for it, but I have
to do it many times and wonder if there is an efficient for it.


Suppose we have a data frame like this:
d <- data.frame(x=sample(seq(0.1:1, by=0.01), size=100, replace=TRUE),
y=rnorm(100, 0.2, 0.6))

and want to have the average of y for a given interval of x, for example
mean(y)[0>x>0.1]. Is there a simple way of doing this or I need to improvise?

Thank you so much for any help
Eleni Rapsomaniki


From Stefano.Guazzetti at ausl.re.it  Fri Sep 15 17:58:02 2006
From: Stefano.Guazzetti at ausl.re.it (Guazzetti Stefano)
Date: Fri, 15 Sep 2006 17:58:02 +0200
Subject: [R] R: Grouping columns in a data frame based on the values of a
	column
Message-ID: <B8A1EED732379B44A7E59D22E82E4442020D6C31@IMHOTEP.ausl.org>

Perhaps using 'ave' and 'cut':

 df <- data.frame(x=runif(100, 0.1, 1),  y=rnorm(100, 0.2, 0.6))
 df$xcut<-cut(df$x, seq(0, 1, 0.1))
 df$z<-ave(df$y, df$xcut)
 df[order(df$x),]


Stefano

-----Messaggio originale-----
Da: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]Per conto di
e.rapsomaniki at mail.cryst.bbk.ac.uk
Inviato: venerd? 15 settembre 2006 17.45
A: r-help at stat.math.ethz.ch
Oggetto: [R] Grouping columns in a data frame based on the values of a
column


Dear R users,

This is a trivial question, there might even be an R function for it, but I have
to do it many times and wonder if there is an efficient for it.


Suppose we have a data frame like this:
d <- data.frame(x=sample(seq(0.1:1, by=0.01), size=100, replace=TRUE),
y=rnorm(100, 0.2, 0.6))

and want to have the average of y for a given interval of x, for example
mean(y)[0>x>0.1]. Is there a simple way of doing this or I need to improvise?

Thank you so much for any help
Eleni Rapsomaniki

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From bruno at est.ufpr.br  Fri Sep 15 20:18:07 2006
From: bruno at est.ufpr.br (Bruno Grimaldo Martinho Churatae)
Date: Fri, 15 Sep 2006 15:18:07 -0300 (BRT)
Subject: [R] question about pairs observations
Message-ID: <Pine.LNX.4.63.0609151449300.4024@est.ufpr.br>

Hi,

I would like to paired ID_ with Cod for analysis in spdep.
Any ideas?

> head(bai$att.data)
            ID_       NAME1_ NAME2_ PARTS_ POINTS_  LENGTH_     AREA_
1 410690205001 410690205001   <NA>      1     158 5.243338 1.2668820
2 410690205009 410690205009   <NA>      1     159 6.071286 1.8409600
3 410690205026 410690205026   <NA>      1     108 3.955380 0.8876151
4 410690205027 410690205027   <NA>      1     251 6.747801 2.2430790
5 410690205041 410690205041   <NA>      1     243 7.314878 2.3123150
6 410690205042 410690205042   <NA>      1     269 5.405646 1.4390610

> head(att.data)
            Cod            bairro alunos resid
1 410690205050         Abranches     33 11165
2 410690205014               Ahu     58 11148
3 410690205064    Alto Boqueirao     36 51155
4 410690205004   Alto da Glo>ria     23  5588
5 410690205005    Alto da Rua XV     48  8683
6 410690205055             Atuba     11 12632

Thanks!

------------------------------------
Bruno G. M. Churata


From sachinj.2006 at yahoo.com  Fri Sep 15 21:35:36 2006
From: sachinj.2006 at yahoo.com (Sachin J)
Date: Fri, 15 Sep 2006 12:35:36 -0700 (PDT)
Subject: [R] prediction interval for new value
Message-ID: <20060915193536.70071.qmail@web37613.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060915/5d8c600b/attachment.pl 

From btyner at gmail.com  Fri Sep 15 21:36:35 2006
From: btyner at gmail.com (Benjamin Tyner)
Date: Fri, 15 Sep 2006 15:36:35 -0400
Subject: [R] dotplot, dropping unused levels of 'y'
Message-ID: <450B00C3.3070005@stat.purdue.edu>

In dotplot, what's the best way to suppress the unused levels of 'y' on 
a per-panel basis? This is useful for the case that 'y' is a factor 
taking perhaps thousands of levels, but for a given panel, only a 
handfull of these levels ever present.

Thanks,
Ben


From mothsailor at googlemail.com  Fri Sep 15 21:45:26 2006
From: mothsailor at googlemail.com (David Barron)
Date: Fri, 15 Sep 2006 20:45:26 +0100
Subject: [R] prediction interval for new value
In-Reply-To: <20060915193536.70071.qmail@web37613.mail.mud.yahoo.com>
References: <20060915193536.70071.qmail@web37613.mail.mud.yahoo.com>
Message-ID: <815b70590609151245u166624d4kbb797c6dfa4b9baa@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060915/08f3a60d/attachment.pl 

From sundar.dorai-raj at pdf.com  Fri Sep 15 21:45:42 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 15 Sep 2006 14:45:42 -0500
Subject: [R] dotplot, dropping unused levels of 'y'
In-Reply-To: <450B00C3.3070005@stat.purdue.edu>
References: <450B00C3.3070005@stat.purdue.edu>
Message-ID: <450B02E6.6020509@pdf.com>



Benjamin Tyner said the following on 9/15/2006 2:36 PM:
> In dotplot, what's the best way to suppress the unused levels of 'y' on 
> a per-panel basis? This is useful for the case that 'y' is a factor 
> taking perhaps thousands of levels, but for a given panel, only a 
> handfull of these levels ever present.
> 
> Thanks,
> Ben
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

Hi, Ben,

Use scales(x = list(relation = "free")) in your call to dotplot:

library(lattice)
z <- data.frame(x = rep(LETTERS[1:20], each = 4),
                 y = 1:80, g = gl(4, 20))
dotplot(y ~ x | g, z,
         scales = list(x = list(relation = "free")))

HTH,

--sundar


From sachinj.2006 at yahoo.com  Fri Sep 15 21:51:46 2006
From: sachinj.2006 at yahoo.com (Sachin J)
Date: Fri, 15 Sep 2006 12:51:46 -0700 (PDT)
Subject: [R] prediction interval for new value
In-Reply-To: <815b70590609151245u166624d4kbb797c6dfa4b9baa@mail.gmail.com>
Message-ID: <20060915195146.15606.qmail@web37601.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060915/20320233/attachment.pl 

From yijzhou at jhsph.edu  Fri Sep 15 22:00:24 2006
From: yijzhou at jhsph.edu (Zhou, Yijie)
Date: Fri, 15 Sep 2006 16:00:24 -0400
Subject: [R] Question about span in Loess function
Message-ID: <E619BDBD99B4F74D9DCA32F43BE92671023B7D14@XCH-VN02.sph.ad.jhsph.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060915/d343bbbb/attachment.pl 

From deepayan.sarkar at gmail.com  Fri Sep 15 22:01:52 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Fri, 15 Sep 2006 13:01:52 -0700
Subject: [R] dotplot, dropping unused levels of 'y'
In-Reply-To: <450B00C3.3070005@stat.purdue.edu>
References: <450B00C3.3070005@stat.purdue.edu>
Message-ID: <eb555e660609151301q3c0a4e30h8a934811d78eb81b@mail.gmail.com>

On 9/15/06, Benjamin Tyner <btyner at gmail.com> wrote:
> In dotplot, what's the best way to suppress the unused levels of 'y' on
> a per-panel basis? This is useful for the case that 'y' is a factor
> taking perhaps thousands of levels, but for a given panel, only a
> handfull of these levels ever present.

It's a bit problematic. Basically, you can use
relation="free"/"sliced", but y behaves as as.numeric(y) would. So, if
the small subset in each panel are always more or less contiguous (in
terms of the levels being close to each other) then you would be fine.
Otherwise you would not. In that case, you can still write your own
prepanel and panel functions, e.g.:
-------------

library(lattice)

y <- factor(sample(1:100), levels = 1:100)
x <- 1:100
a <- gl(9, 1, 100)

dotplot(y ~ x | a)

p <-
    dotplot(y ~ x | a,
            scales = list(y = list(relation = "free", rot = 0)),

            prepanel = function(x, y, ...) {
                yy <- y[, drop = TRUE]
                list(ylim = levels(yy),
                     yat = sort(unique(as.numeric(yy))))
            },

            panel = function(x, y, ...) {
                yy <- y[, drop = TRUE]
                panel.dotplot(x, yy, ...)
            })

----------

Hope that gives you what you want.

Deepayan


From jennystadt at yahoo.ca  Fri Sep 15 22:09:07 2006
From: jennystadt at yahoo.ca (Jenny Stadt)
Date: Fri, 15 Sep 2006 14:09:07 -0600
Subject: [R] About truncated distribution
References: <200609121541592080733@yahoo.ca>
	<00c401c6d6be$38e48af0$711f210a@gne.windows.gene.com>
	<42bc98300609121620i434c8e89ycb9f5a1a852adaae@mail.gmail.com>
	<200609131447509198132@yahoo.ca>
Message-ID: <200609151409054957531@yahoo.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060915/7333d67f/attachment.pl 

From tilling at gmail.com  Fri Sep 15 22:55:10 2006
From: tilling at gmail.com (John Tillinghast)
Date: Fri, 15 Sep 2006 13:55:10 -0700
Subject: [R] Trouble installing modified package
Message-ID: <73bcebe0609151355n38eedbe6i93a9a49b4d836de8@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060915/9604dc91/attachment.pl 

From m.blizinski at wit.edu.pl  Fri Sep 15 23:26:11 2006
From: m.blizinski at wit.edu.pl (Maciej =?UTF-8?Q?Blizi=C5=84ski?=)
Date: Fri, 15 Sep 2006 23:26:11 +0200
Subject: [R] Rpart, custom penalty for an error
In-Reply-To: <Pine.LNX.4.64.0609102029370.17500@gannet.stats.ox.ac.uk>
References: <1157908182.21564.13.camel@localhost.localnet>
	<Pine.LNX.4.64.0609102029370.17500@gannet.stats.ox.ac.uk>
Message-ID: <1158355571.10042.22.camel@localhost.localnet>

On Sun, 2006-09-10 at 20:36 +0100, Prof Brian Ripley wrote: 
> > I am however interested in areas where the probability of success is 
> > noticeably higher than 5%, for example 20%. I've tried rpart and the 
> > weights option, increasing the weights of the success-observations.
> 
> You are 'misleading' rpart by using 'weights', claiming to have case
> weights for cases you do not have.  You need to use 'cost' instead.

As for the rpart() function, the `cost' parameter is for scaling the
variables, not for the cost of misclassifications. To specify it, the
parameter `parms' needs to be used, as a list with a `loss' element, in
form of a matrix. In other words, cost parm is not for cost, use loss
parm of the parms parm. Example usage:

tr <- rpart(y ~ x, data = some.data, method = 'class',
	parms = list(loss = matrix(c(0, 1, 20, 0), nrow = 2)))

> This is a standard issue, discussed in all good books on classification
> (including mine).

Yes, in MASS, section 12.2, Classification Theory, page 338 (fourth edition).
I was looking for it in section 9.2, where rpart() is discussed.

Thanks!

Regards,
Maciej

-- 
http://automatthias.wordpress.com


From guillaume.blanchet.1 at umontreal.ca  Sat Sep 16 00:05:50 2006
From: guillaume.blanchet.1 at umontreal.ca (Guillaume Blanchet)
Date: Fri, 15 Sep 2006 18:05:50 -0400
Subject: [R] Periodogram of Schuster
Message-ID: <a0623090bc130d3547a25@[192.168.0.12]>

Dear All,

Is there a function in R which can do a periodogram of Schuster ?

Thanks in advance !

Guillaume Blanchet


From rvaradhan at jhmi.edu  Sat Sep 16 00:49:17 2006
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Fri, 15 Sep 2006 18:49:17 -0400
Subject: [R] LARS for generalized linear models
Message-ID: <000001c6d919$2c8b1500$7c94100a@win.ad.jhu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060915/08355836/attachment.pl 

From MSchwartz at mn.rr.com  Sat Sep 16 01:00:30 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Fri, 15 Sep 2006 18:00:30 -0500
Subject: [R] LARS for generalized linear models
In-Reply-To: <000001c6d919$2c8b1500$7c94100a@win.ad.jhu.edu>
References: <000001c6d919$2c8b1500$7c94100a@win.ad.jhu.edu>
Message-ID: <1158361230.4239.1.camel@localhost.localdomain>

On Fri, 2006-09-15 at 18:49 -0400, Ravi Varadhan wrote:
> Hi,

> Is there an R implementation of least angle regression for binary response
> modeling?  I know that this question has been asked before, and I am also
> aware of the "lasso2" package, but that only implements an L1 penalty, i.e.
> the Lasso approach.  

> Madigan and Ridgeway in their discussion of Efron et al (2004) describe a
> LARS-type algorithm for generalized linear models.  Has anyone implemented
> this in R?

> Thanks for any help.

> Best,
> 
> Ravi

This just came up last month.  See this post:

  https://stat.ethz.ch/pipermail/r-help/2006-August/111352.html

HTH,

Marc Schwartz


From gyadav at ccilindia.co.in  Sat Sep 16 06:00:50 2006
From: gyadav at ccilindia.co.in (gyadav at ccilindia.co.in)
Date: Sat, 16 Sep 2006 09:30:50 +0530
Subject: [R] regarding chaos
In-Reply-To: <33505FF1-C142-4A0A-98FE-B71376CE8307@unimi.it>
Message-ID: <OF94065FF0.23F0F354-ON652571EB.0015CCA9-652571B3.001629B3@ccilindia.co.in>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060916/dcb57435/attachment.pl 

From pfh at synapse9.com  Sat Sep 16 16:27:55 2006
From: pfh at synapse9.com (phil henshaw)
Date: Sat, 16 Sep 2006 10:27:55 -0400
Subject: [R] tutors for time series....NYC
Message-ID: <002001c6d99c$4d5c2d90$2f01a8c0@SavyII>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060916/7c0ceaef/attachment.pl 

From jrkrideau at yahoo.ca  Sat Sep 16 18:24:20 2006
From: jrkrideau at yahoo.ca (John Kane)
Date: Sat, 16 Sep 2006 12:24:20 -0400 (EDT)
Subject: [R] Grouping columns in a data frame based on the values of a
	column
In-Reply-To: <1158335082.450aca6a7dce2@webmail.cryst.bbk.ac.uk>
Message-ID: <20060916162420.6610.qmail@web32813.mail.mud.yahoo.com>


--- e.rapsomaniki at mail.cryst.bbk.ac.uk wrote:

> Dear R users,
> 
> This is a trivial question, there might even be an R
> function for it, but I have
> to do it many times and wonder if there is an
> efficient for it.
> 
> 
> Suppose we have a data frame like this:
> d <- data.frame(x=sample(seq(0.1:1, by=0.01),
> size=100, replace=TRUE),
> y=rnorm(100, 0.2, 0.6))
> 
> and want to have the average of y for a given
> interval of x, for example
> mean(y)[0>x>0.1]. Is there a simple way of doing
> this or I need to improvise?

I don't think so.  I don't think there is any value of
x < 0.1 in the dataframe.

However if we change the data.frame to read
d <- data.frame(x=sample(seq(0.01:1, by=0.01),
size=100, replace=TRUE),
y=rnorm(100, 0.2, 0.6))

dd <- subset(d,  x> 0 & x < 0.1)
mean(dd[,2])

seems to work.
or if you do this a lot you might want to write it as
a funtion.

sub.mean <- function (frame, first.col, second.col,
upper, lower) {
dd <- subset(frame,  first.col > lower & first.col <
upper)
mean(frame[,2])
}

sub.mean(d, 1,2,0.1,0)


From btyner at gmail.com  Sat Sep 16 22:28:53 2006
From: btyner at gmail.com (Benjamin Tyner)
Date: Sat, 16 Sep 2006 16:28:53 -0400
Subject: [R] dotplot/Dotplot: connecting points within factor level across
	time
Message-ID: <450C5E85.50803@stat.purdue.edu>

For each level of the factor in dotplot, I have time points I'd like to 
connect with a line. In the example below, 'x' represents a starting 
time and 'd' a duration, and I wish to connect 'x' to 'x+d'. Ordinarily 
I would use Dotplot from hmisc for this, but I have not been able to 
find a time class that Dotplot will allow. I can get lattice dotplot to 
put the points up, but I've not figured out how to connect them. Any 
suggestions?

require(lattice)
z<-data.frame(y=factor(letters),
              x=structure(1:26,class=c("POSIXt","POSIXct"),tzone=""),
              d=runif(26))

# this puts the points, but does not connect them
p<-dotplot(y~x+I(x+d),
           data=z,
           scales=list(x=list(format="%M:%S"))
           )

zh<-z
zh$x<-as.numeric(zh$x)
require(hmisc)
# this connects them, but at the expense of the time info
ph<-Dotplot(y~Cbind(x,x,x+d),
            data=zh,
            pch=" ")



Thanks,
Ben


From liuwensui at gmail.com  Sat Sep 16 22:59:23 2006
From: liuwensui at gmail.com (Wensui Liu)
Date: Sat, 16 Sep 2006 16:59:23 -0400
Subject: [R] how to rescale the limits of yaxis rather than using the data
	range by default?
Message-ID: <1115a2b00609161359s59515557m6ae36d29cb31c88a@mail.gmail.com>

Dear Lister,

plot() is using the data range as the default limits of yaxis. Is
there any way I can change the limits? I just look at the help of
plot() and par() and couldn't find answers.

Thanks.


-- 
WenSui Liu
(http://spaces.msn.com/statcompute/blog)
Senior Decision Support Analyst
Health Policy and Clinical Effectiveness
Cincinnati Children Hospital Medical Center


From liuwensui at gmail.com  Sat Sep 16 23:33:20 2006
From: liuwensui at gmail.com (Wensui Liu)
Date: Sat, 16 Sep 2006 17:33:20 -0400
Subject: [R] how to rescale the limits of yaxis rather than using the
	data range by default?
In-Reply-To: <b3161b5f0609161416x6f93c4b3xb80d5811a5d591c3@mail.gmail.com>
References: <1115a2b00609161359s59515557m6ae36d29cb31c88a@mail.gmail.com>
	<b3161b5f0609161416x6f93c4b3xb80d5811a5d591c3@mail.gmail.com>
Message-ID: <1115a2b00609161433y372fabbao48c4e70327328f08@mail.gmail.com>

HI, Peter,

It is exactly what I want. Thank you so much!

wensui

On 9/16/06, Peter Konings <peter.l.e.konings at gmail.com> wrote:
> Hi,
>
> I'm not sure I understand your question correctly, but does the ylim option
> of plot do what you want?
>
> HTH
> Peter.
>
>
> On 9/16/06, Wensui Liu <liuwensui at gmail.com> wrote:
> >
> Dear Lister,
>
> plot() is using the data range as the default limits of yaxis. Is
> there any way I can change the limits? I just look at the help of
> plot() and par() and couldn't find answers.
>
> Thanks.
>
>
> --
>  WenSui Liu
> (http://spaces.msn.com/statcompute/blog)
> Senior Decision Support Analyst
> Health Policy and Clinical Effectiveness
> Cincinnati Children Hospital Medical Center
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


-- 
WenSui Liu
(http://spaces.msn.com/statcompute/blog)
Senior Decision Support Analyst
Health Policy and Clinical Effectiveness
Cincinnati Children Hospital Medical Center


From mothsailor at googlemail.com  Sat Sep 16 23:35:03 2006
From: mothsailor at googlemail.com (David Barron)
Date: Sat, 16 Sep 2006 22:35:03 +0100
Subject: [R] how to rescale the limits of yaxis rather than using the
	data range by default?
In-Reply-To: <1115a2b00609161359s59515557m6ae36d29cb31c88a@mail.gmail.com>
References: <1115a2b00609161359s59515557m6ae36d29cb31c88a@mail.gmail.com>
Message-ID: <815b70590609161435q16e98dc1o64fcf222ae7f43bc@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060916/2568e41e/attachment.pl 

From cjkogan111 at yahoo.com  Sun Sep 17 00:29:24 2006
From: cjkogan111 at yahoo.com (cjkogan111)
Date: Sat, 16 Sep 2006 15:29:24 -0700 (PDT)
Subject: [R] Error Message Documentation
Message-ID: <6344607.post@talk.nabble.com>


Hello,
I am new to R, and trying to work with it. I have a couple of quick
questions. First, I made a program and got the following error message.
------------------------------------------------------------------------------------------------------------------
Error in if (DatMdFile$Time.Value[NmRecord] < VBinTimesMinTop[NmCounter]) {
: 
        missing value where TRUE/FALSE needed
In addition: Warning message:
< not meaningful for factors in: Ops.factor(DatMdFile$Time.Value[NmRecord],
VBinTimesMinTop[NmCounter]) 
-----------------------------------------------------------------------------------------------------------------
I noticed that one of the values had decimal places, while the other didn't,
so the comparison would be 11.00 > 10
I don't know if that might have anything to do with my problem.
Anyway, I don't have much idea what the error means, and I don't know how to
check what data type the different vectors are.

I was wondering if anyone could help me out, and also, I was wondering if
there is any error documentation (stuff that tells what the error means.)

Thanks!
- cjkogan111
-- 
View this message in context: http://www.nabble.com/Error-Message-Documentation-tf2283899.html#a6344607
Sent from the R help forum at Nabble.com.


From ggrothendieck at gmail.com  Sun Sep 17 02:55:45 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 16 Sep 2006 20:55:45 -0400
Subject: [R] dotplot/Dotplot: connecting points within factor level
	across time
In-Reply-To: <450C5E85.50803@stat.purdue.edu>
References: <450C5E85.50803@stat.purdue.edu>
Message-ID: <971536df0609161755q6b1db6b4gd43e7f5af7ab7db7@mail.gmail.com>

Try this:

print(p)
trellis.focus("panel", 1, 1)
with(z, panel.segments(x, as.numeric(y), x+d, as.numeric(y)))
trellis.unfocus()

On 9/16/06, Benjamin Tyner <btyner at gmail.com> wrote:
> For each level of the factor in dotplot, I have time points I'd like to
> connect with a line. In the example below, 'x' represents a starting
> time and 'd' a duration, and I wish to connect 'x' to 'x+d'. Ordinarily
> I would use Dotplot from hmisc for this, but I have not been able to
> find a time class that Dotplot will allow. I can get lattice dotplot to
> put the points up, but I've not figured out how to connect them. Any
> suggestions?
>
> require(lattice)
> z<-data.frame(y=factor(letters),
>              x=structure(1:26,class=c("POSIXt","POSIXct"),tzone=""),
>              d=runif(26))
>
> # this puts the points, but does not connect them
> p<-dotplot(y~x+I(x+d),
>           data=z,
>           scales=list(x=list(format="%M:%S"))
>           )
>
> zh<-z
> zh$x<-as.numeric(zh$x)
> require(hmisc)
> # this connects them, but at the expense of the time info
> ph<-Dotplot(y~Cbind(x,x,x+d),
>            data=zh,
>            pch=" ")
>
>
>
> Thanks,
> Ben
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From bingshanli at yahoo.com  Sun Sep 17 04:19:48 2006
From: bingshanli at yahoo.com (Bingshan Li)
Date: Sat, 16 Sep 2006 19:19:48 -0700 (PDT)
Subject: [R] using "table" in R
Message-ID: <20060917021948.86887.qmail@web56005.mail.re3.yahoo.com>

Hi there,

I have a dataframe whose elements are numbers or
characters. I want to extract the frequencies of each
elements in the dataframe. For example,

d = as.data.frame(matrix(c(1, 2, 3, 3), 2,2))

What I want is first what are the elements in the data
(1,2,3 here) and second what are their frequencies
(1,1,2 respectively). How to use "table" to extract
these two pieces of information? I played with "table"
but couldn't extract the information. Please assume
that we do not know how many elements in the dataframe
a priori.

Thanks a lot!


From jholtman at gmail.com  Sun Sep 17 05:09:03 2006
From: jholtman at gmail.com (jim holtman)
Date: Sat, 16 Sep 2006 23:09:03 -0400
Subject: [R] using "table" in R
In-Reply-To: <20060917021948.86887.qmail@web56005.mail.re3.yahoo.com>
References: <20060917021948.86887.qmail@web56005.mail.re3.yahoo.com>
Message-ID: <644e1f320609162009k6634507di6ac8c398b6f35e36@mail.gmail.com>

Here is one way; you create a vector of the data in the dataframe with
'unlist' and then use table:

> d = as.data.frame(matrix(c(1, 2, 3, 3), 2,2))
> d
  V1 V2
1  1  3
2  2  3
> table(unlist(d))

1 2 3
1 1 2
>


On 9/16/06, Bingshan Li <bingshanli at yahoo.com> wrote:
> Hi there,
>
> I have a dataframe whose elements are numbers or
> characters. I want to extract the frequencies of each
> elements in the dataframe. For example,
>
> d = as.data.frame(matrix(c(1, 2, 3, 3), 2,2))
>
> What I want is first what are the elements in the data
> (1,2,3 here) and second what are their frequencies
> (1,1,2 respectively). How to use "table" to extract
> these two pieces of information? I played with "table"
> but couldn't extract the information. Please assume
> that we do not know how many elements in the dataframe
> a priori.
>
> Thanks a lot!
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From mmalten at gmail.com  Sun Sep 17 05:43:20 2006
From: mmalten at gmail.com (Mitchell Maltenfort)
Date: Sat, 16 Sep 2006 23:43:20 -0400
Subject: [R] Any other R users in Philadelphia?
Message-ID: <8913fde30609162043t753c5754m503c5f8dc6bd00fe@mail.gmail.com>

I'm still new to R and wouldn't mind meeting other R users, at any
level of experience.

Regards,

Mitch Maltenfort
Thomas Jefferson University

-- 
I can answer any question.
"I don't know" is an answer.
"I don't know yet" is a better answer.


From bingshanli at yahoo.com  Sun Sep 17 06:14:28 2006
From: bingshanli at yahoo.com (Bingshan Li)
Date: Sat, 16 Sep 2006 21:14:28 -0700 (PDT)
Subject: [R] using "table" in R
In-Reply-To: <644e1f320609162009k6634507di6ac8c398b6f35e36@mail.gmail.com>
Message-ID: <20060917041428.77272.qmail@web56003.mail.re3.yahoo.com>

Hi Jim,

This is the way to get the frequencies. But what I
want is to store the elements in one vector and their
frequencies in another vector. My problem is that when
I call "table" to return the frequency table, I do not
know how to extract these two vectors. I tried
table(...)$dinnames and it did not work. It returned
NULL.

Thanks!


--- jim holtman <jholtman at gmail.com> wrote:

> Here is one way; you create a vector of the data in
> the dataframe with
> 'unlist' and then use table:
> 
> > d = as.data.frame(matrix(c(1, 2, 3, 3), 2,2))
> > d
>   V1 V2
> 1  1  3
> 2  2  3
> > table(unlist(d))
> 
> 1 2 3
> 1 1 2
> >
> 
> 
> On 9/16/06, Bingshan Li <bingshanli at yahoo.com>
> wrote:
> > Hi there,
> >
> > I have a dataframe whose elements are numbers or
> > characters. I want to extract the frequencies of
> each
> > elements in the dataframe. For example,
> >
> > d = as.data.frame(matrix(c(1, 2, 3, 3), 2,2))
> >
> > What I want is first what are the elements in the
> data
> > (1,2,3 here) and second what are their frequencies
> > (1,1,2 respectively). How to use "table" to
> extract
> > these two pieces of information? I played with
> "table"
> > but couldn't extract the information. Please
> assume
> > that we do not know how many elements in the
> dataframe
> > a priori.
> >
> > Thanks a lot!
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained,
> reproducible code.
> >
> 
> 
> -- 
> Jim Holtman
> Cincinnati, OH
> +1 513 646 9390
> 
> What is the problem you are trying to solve?
>


From ggrothendieck at gmail.com  Sun Sep 17 07:28:16 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 17 Sep 2006 01:28:16 -0400
Subject: [R] Periodogram of Schuster
In-Reply-To: <a0623090bc130d3547a25@192.168.0.12>
References: <a0623090bc130d3547a25@192.168.0.12>
Message-ID: <971536df0609162228t48aa222cy11004c37dc21bac9@mail.gmail.com>

Check out:

http://cran.r-project.org/doc/contrib/Ricci-refcard-ts.pdf

On 9/15/06, Guillaume Blanchet <guillaume.blanchet.1 at umontreal.ca> wrote:
> Dear All,
>
> Is there a function in R which can do a periodogram of Schuster ?
>
> Thanks in advance !
>
> Guillaume Blanchet
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ligges at statistik.uni-dortmund.de  Sun Sep 17 10:25:32 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 17 Sep 2006 10:25:32 +0200
Subject: [R] Error Message Documentation
In-Reply-To: <6344607.post@talk.nabble.com>
References: <6344607.post@talk.nabble.com>
Message-ID: <450D067C.9060109@statistik.uni-dortmund.de>

cjkogan111 wrote:
> Hello,
> I am new to R, and trying to work with it. I have a couple of quick
> questions. First, I made a program and got the following error message.
> ------------------------------------------------------------------------------------------------------------------
> Error in if (DatMdFile$Time.Value[NmRecord] < VBinTimesMinTop[NmCounter]) {
> : 
>         missing value where TRUE/FALSE needed
> In addition: Warning message:
> < not meaningful for factors in: Ops.factor(DatMdFile$Time.Value[NmRecord],
> VBinTimesMinTop[NmCounter]) 
> -----------------------------------------------------------------------------------------------------------------
> I noticed that one of the values had decimal places, while the other didn't,
> so the comparison would be 11.00 > 10
> I don't know if that might have anything to do with my problem.

Well, not really the decimal places, but the data type.

> Anyway, I don't have much idea what the error means, and I don't know how to
> check what data type the different vectors are.

So time to start looking at the documentation?
class() is your friend, you might also want to try str().

> I was wondering if anyone could help me out, and also, I was wondering if
> there is any error documentation (stuff that tells what the error means.)

Probably one of your objects is a factor rather than a numeric value.


> Thanks!
> - cjkogan111

ujligges1234


From wuertz at itp.phys.ethz.ch  Sun Sep 17 10:27:13 2006
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Sun, 17 Sep 2006 10:27:13 +0200
Subject: [R] regarding chaos
In-Reply-To: <OF94065FF0.23F0F354-ON652571EB.0015CCA9-652571B3.001629B3@ccilindia.co.in>
References: <OF94065FF0.23F0F354-ON652571EB.0015CCA9-652571B3.001629B3@ccilindia.co.in>
Message-ID: <450D06E1.80408@itp.phys.ethz.ch>

gyadav at ccilindia.co.in wrote:

>hi all,
>
>I have a simple question that does power spectral analysis related to 
>capacity dimension, information dimension, lyapunov exponent, hurst 
>exponent.
>  
>
Hurst Aanalysis is implemented in fSeries from Rmetrics.
The following functions are available:

###############################################################################
# PART I: Simulation
# FUNCTIONS:            FRACTIONAL BROWNIAN MOTION:
#  fbmSim                Generates fractional Brownian motion
#                         Available Methods:
#                          Numerical approximation of the stochastic 
integral
#                          Choleki's decomposition of the covariance matrix
#                          Method of Levinson
#                          Method of Wood and Chan
#                          Wavelet synthesis
# FUNCTIONS:            FRACTIONAL GAUSSIAN NOISE:
#  fgnSim                Generates fractional Gaussian noise
#                         Available Methods:
#                          Durbin's Method
#                          Paxson's Method
#                          Beran's Method
# FUNCTIONS:            FARIMA PROCESS:
#  farimaSim             Generates FARIMA time series process
################################################################################


################################################################################
# PART II: Reimplemented functions from Beran's SPlus Scripts
# FUNCTIONS:            DESCRIPTION:
#  farimaTrueacf         Returns FARMA true autocorrelation function
#  farimaTruefft         Returns FARMA true fast Fourier transform
#  fgnTrueacf            Returns FGN true autocorrelation function
#  fgnTruefft            Returns FGN true fast Fourier transform
# FUNCTIONS:            WHITTLE ESTIMATOR:
#  whittleFit            Whittle Estimator
################################################################################


################################################################################
# PART III: Reimplemented SPlus/C functions from
#   Taqqu M.S, Teverovsky V, Willinger W.
#   Estimators for Long-Range Dependence: An Empirical Study
#   Fractals, Vol 3, No. 4, 785-788, 1995
# FUNCTIONS:          HURST EXPONENT:
#  'fHURST'            S4 Class Representation
#   print.fHURST        S3 Print Method
#   plot.fHURST         S3 Plot Method
#  aggvarFit               Aggregated variance method
#  diffvarFit              Differenced aggregated variance method
#  absvalFit               Absolute values (moments) method
#  higuchiFit              Higuchi's method
#  pengFit                 Peng's or Residuals of Regression method
#  rsFit                   R/S method
#  perFit                  Periodogram and cumulated periodogram method
#  boxperFit               Boxed (modified) peridogram method
#  whittleFit              Whittle estimator -> PART II
#  hurstSlider         Hurst Slider
################################################################################


Diethelm Wuertz

>If yes then please show me the way. I am newbie in the world of chaos.
>
>   Sayonara With Smile & With Warm Regards :-)
>
>  G a u r a v   Y a d a v
>  Senior Executive Officer,
>  Economic Research & Surveillance Department,
>  Clearing Corporation Of India Limited.
>
>  Address: 5th, 6th, 7th Floor, Trade Wing 'C',  Kamala City, S.B. Marg, 
>Mumbai - 400 013
>  Telephone(Office): - +91 022 6663 9398 ,  Mobile(Personal) (0)9821286118
>  Email(Office) :- gyadav at ccilindia.co.in ,  Email(Personal) :- 
>emailtogauravyadav at gmail.com
>
>
>============================================================================================
>DISCLAIMER AND CONFIDENTIALITY CAUTION:\ \ This message and ...{{dropped}}
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>  
>


From merser at image.dk  Sun Sep 17 11:15:32 2006
From: merser at image.dk (=?iso-8859-1?Q?S=F8ren_Merser?=)
Date: Sun, 17 Sep 2006 11:15:32 +0200
Subject: [R] using "table" in R
References: <20060917041428.77272.qmail@web56003.mail.re3.yahoo.com>
Message-ID: <001101c6da39$d71e0e20$6400a8c0@IBM>

hi, here's a way:
> attr(table(unlist(d)),'dimnames')[[1]]
[1] "1" "2" "3"
> as.numeric(table(unlist(d)))
[1] 1 1 2
soren

----- Original Message ----- 
From: "Bingshan Li" <bingshanli at yahoo.com>
To: "jim holtman" <jholtman at gmail.com>
Cc: <r-help at stat.math.ethz.ch>
Sent: Sunday, September 17, 2006 6:14 AM
Subject: Re: [R] using "table" in R


> Hi Jim,
>
> This is the way to get the frequencies. But what I
> want is to store the elements in one vector and their
> frequencies in another vector. My problem is that when
> I call "table" to return the frequency table, I do not
> know how to extract these two vectors. I tried
> table(...)$dinnames and it did not work. It returned
> NULL.
>
> Thanks!
>
>
> --- jim holtman <jholtman at gmail.com> wrote:
>
>> Here is one way; you create a vector of the data in
>> the dataframe with
>> 'unlist' and then use table:
>>
>> > d = as.data.frame(matrix(c(1, 2, 3, 3), 2,2))
>> > d
>>   V1 V2
>> 1  1  3
>> 2  2  3
>> > table(unlist(d))
>>
>> 1 2 3
>> 1 1 2
>> >
>>
>>
>> On 9/16/06, Bingshan Li <bingshanli at yahoo.com>
>> wrote:
>> > Hi there,
>> >
>> > I have a dataframe whose elements are numbers or
>> > characters. I want to extract the frequencies of
>> each
>> > elements in the dataframe. For example,
>> >
>> > d = as.data.frame(matrix(c(1, 2, 3, 3), 2,2))
>> >
>> > What I want is first what are the elements in the
>> data
>> > (1,2,3 here) and second what are their frequencies
>> > (1,1,2 respectively). How to use "table" to
>> extract
>> > these two pieces of information? I played with
>> "table"
>> > but couldn't extract the information. Please
>> assume
>> > that we do not know how many elements in the
>> dataframe
>> > a priori.
>> >
>> > Thanks a lot!
>> >
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained,
>> reproducible code.
>> >
>>
>>
>> -- 
>> Jim Holtman
>> Cincinnati, OH
>> +1 513 646 9390
>>
>> What is the problem you are trying to solve?
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From mouandny at gmail.com  Sun Sep 17 11:27:27 2006
From: mouandny at gmail.com (Finosaur)
Date: Sun, 17 Sep 2006 04:27:27 -0500
Subject: [R] help on data frame and matrix
Message-ID: <003301c6da3b$7fdb9360$dfe38780@finosaur>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060917/ab35f652/attachment.pl 

From Dimitris.Rizopoulos at med.kuleuven.be  Sun Sep 17 11:32:59 2006
From: Dimitris.Rizopoulos at med.kuleuven.be (Dimitrios Rizopoulos)
Date: Sun, 17 Sep 2006 11:32:59 +0200
Subject: [R] help on data frame and matrix
In-Reply-To: <003301c6da3b$7fdb9360$dfe38780@finosaur>
References: <003301c6da3b$7fdb9360$dfe38780@finosaur>
Message-ID: <20060917113259.ppur7qes8log4gws@webmail4.kuleuven.be>

look at ?data.matrix()

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
      http://www.student.kuleuven.be/~m0390867/dimitris.htm


Quoting Finosaur <mouandny at gmail.com>:

>
> Hey,
>
>
>
> I am given a data frame, which actually contains a matrix. But I   
> need to convert the data frame into a matrix so that I can use the   
> matrix operators. Can anybody help me out? Thanks a lot.
>
>
>
> Thaleia
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>



Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From mouandny at gmail.com  Sun Sep 17 12:12:20 2006
From: mouandny at gmail.com (Finosaur)
Date: Sun, 17 Sep 2006 05:12:20 -0500
Subject: [R] help on sampling from Dirichlet process
Message-ID: <005201c6da41$c5111bc0$dfe38780@finosaur>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060917/04b9ac9d/attachment.pl 

From jholtman at gmail.com  Sun Sep 17 14:53:30 2006
From: jholtman at gmail.com (jim holtman)
Date: Sun, 17 Sep 2006 08:53:30 -0400
Subject: [R] using "table" in R
In-Reply-To: <001101c6da39$d71e0e20$6400a8c0@IBM>
References: <20060917041428.77272.qmail@web56003.mail.re3.yahoo.com>
	<001101c6da39$d71e0e20$6400a8c0@IBM>
Message-ID: <644e1f320609170553q3553dce1wb3735b91b629d335@mail.gmail.com>

You can use 'names'

> d = as.data.frame(matrix(c(1, 2, 3, 3), 2,2))
>
> x <- table(unlist(d))
> x

1 2 3
1 1 2
> names(x)
[1] "1" "2" "3"
>


On 9/17/06, S?ren Merser <merser at image.dk> wrote:
> hi, here's a way:
> > attr(table(unlist(d)),'dimnames')[[1]]
> [1] "1" "2" "3"
> > as.numeric(table(unlist(d)))
> [1] 1 1 2
> soren
>
> ----- Original Message -----
> From: "Bingshan Li" <bingshanli at yahoo.com>
> To: "jim holtman" <jholtman at gmail.com>
> Cc: <r-help at stat.math.ethz.ch>
> Sent: Sunday, September 17, 2006 6:14 AM
> Subject: Re: [R] using "table" in R
>
>
> > Hi Jim,
> >
> > This is the way to get the frequencies. But what I
> > want is to store the elements in one vector and their
> > frequencies in another vector. My problem is that when
> > I call "table" to return the frequency table, I do not
> > know how to extract these two vectors. I tried
> > table(...)$dinnames and it did not work. It returned
> > NULL.
> >
> > Thanks!
> >
> >
> > --- jim holtman <jholtman at gmail.com> wrote:
> >
> >> Here is one way; you create a vector of the data in
> >> the dataframe with
> >> 'unlist' and then use table:
> >>
> >> > d = as.data.frame(matrix(c(1, 2, 3, 3), 2,2))
> >> > d
> >>   V1 V2
> >> 1  1  3
> >> 2  2  3
> >> > table(unlist(d))
> >>
> >> 1 2 3
> >> 1 1 2
> >> >
> >>
> >>
> >> On 9/16/06, Bingshan Li <bingshanli at yahoo.com>
> >> wrote:
> >> > Hi there,
> >> >
> >> > I have a dataframe whose elements are numbers or
> >> > characters. I want to extract the frequencies of
> >> each
> >> > elements in the dataframe. For example,
> >> >
> >> > d = as.data.frame(matrix(c(1, 2, 3, 3), 2,2))
> >> >
> >> > What I want is first what are the elements in the
> >> data
> >> > (1,2,3 here) and second what are their frequencies
> >> > (1,1,2 respectively). How to use "table" to
> >> extract
> >> > these two pieces of information? I played with
> >> "table"
> >> > but couldn't extract the information. Please
> >> assume
> >> > that we do not know how many elements in the
> >> dataframe
> >> > a priori.
> >> >
> >> > Thanks a lot!
> >> >
> >> > ______________________________________________
> >> > R-help at stat.math.ethz.ch mailing list
> >> > https://stat.ethz.ch/mailman/listinfo/r-help
> >> > PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> > and provide commented, minimal, self-contained,
> >> reproducible code.
> >> >
> >>
> >>
> >> --
> >> Jim Holtman
> >> Cincinnati, OH
> >> +1 513 646 9390
> >>
> >> What is the problem you are trying to solve?
> >>
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From h.wickham at gmail.com  Sun Sep 17 15:10:32 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Sun, 17 Sep 2006 08:10:32 -0500
Subject: [R] using "table" in R
In-Reply-To: <20060917041428.77272.qmail@web56003.mail.re3.yahoo.com>
References: <644e1f320609162009k6634507di6ac8c398b6f35e36@mail.gmail.com>
	<20060917041428.77272.qmail@web56003.mail.re3.yahoo.com>
Message-ID: <f8e6ff050609170610v581be77aub75241b95131eb16@mail.gmail.com>

> This is the way to get the frequencies. But what I
> want is to store the elements in one vector and their
> frequencies in another vector. My problem is that when
> I call "table" to return the frequency table, I do not
> know how to extract these two vectors. I tried
> table(...)$dinnames and it did not work. It returned
> NULL.

Try:
> as.data.frame(table(1:10))

Hadley


From ripley at stats.ox.ac.uk  Sun Sep 17 15:53:49 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 17 Sep 2006 14:53:49 +0100 (BST)
Subject: [R] using "table" in R
In-Reply-To: <644e1f320609170553q3553dce1wb3735b91b629d335@mail.gmail.com>
References: <20060917041428.77272.qmail@web56003.mail.re3.yahoo.com>
	<001101c6da39$d71e0e20$6400a8c0@IBM>
	<644e1f320609170553q3553dce1wb3735b91b629d335@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0609171447460.20907@gannet.stats.ox.ac.uk>

On Sun, 17 Sep 2006, jim holtman wrote:

> You can use 'names'
>
>> d = as.data.frame(matrix(c(1, 2, 3, 3), 2,2))
>>
>> x <- table(unlist(d))
>> x
>
> 1 2 3
> 1 1 2
>> names(x)
> [1] "1" "2" "3"

You can, but S?ren was equally correct: names() on a 1D array actually 
looks up the dimnames.  OTOH, we do really want to encourage people to use 
the accessor functions, so

> dimnames(x)[[1]]
[1] "1" "2" "3"

is better supported.


> On 9/17/06, S?ren Merser <merser at image.dk> wrote:
>> hi, here's a way:
>>> attr(table(unlist(d)),'dimnames')[[1]]
>> [1] "1" "2" "3"
>>> as.numeric(table(unlist(d)))
>> [1] 1 1 2

You don't need as.numeric here: as.vector will remove the attributes if 
that is the intention.


>> soren
>>
>> ----- Original Message -----
>> From: "Bingshan Li" <bingshanli at yahoo.com>
>> To: "jim holtman" <jholtman at gmail.com>
>> Cc: <r-help at stat.math.ethz.ch>
>> Sent: Sunday, September 17, 2006 6:14 AM
>> Subject: Re: [R] using "table" in R
>>
>>
>>> Hi Jim,
>>>
>>> This is the way to get the frequencies. But what I
>>> want is to store the elements in one vector and their
>>> frequencies in another vector. My problem is that when
>>> I call "table" to return the frequency table, I do not
>>> know how to extract these two vectors. I tried
>>> table(...)$dinnames and it did not work. It returned
>>> NULL.
>>>
>>> Thanks!
>>>
>>>
>>> --- jim holtman <jholtman at gmail.com> wrote:
>>>
>>>> Here is one way; you create a vector of the data in
>>>> the dataframe with
>>>> 'unlist' and then use table:
>>>>
>>>>> d = as.data.frame(matrix(c(1, 2, 3, 3), 2,2))
>>>>> d
>>>>   V1 V2
>>>> 1  1  3
>>>> 2  2  3
>>>>> table(unlist(d))
>>>>
>>>> 1 2 3
>>>> 1 1 2
>>>>>
>>>>
>>>>
>>>> On 9/16/06, Bingshan Li <bingshanli at yahoo.com>
>>>> wrote:
>>>>> Hi there,
>>>>>
>>>>> I have a dataframe whose elements are numbers or
>>>>> characters. I want to extract the frequencies of
>>>> each
>>>>> elements in the dataframe. For example,
>>>>>
>>>>> d = as.data.frame(matrix(c(1, 2, 3, 3), 2,2))
>>>>>
>>>>> What I want is first what are the elements in the
>>>> data
>>>>> (1,2,3 here) and second what are their frequencies
>>>>> (1,1,2 respectively). How to use "table" to
>>>> extract
>>>>> these two pieces of information? I played with
>>>> "table"
>>>>> but couldn't extract the information. Please
>>>> assume
>>>>> that we do not know how many elements in the
>>>> dataframe
>>>>> a priori.
>>>>>
>>>>> Thanks a lot!
>>>>>
>>>>> ______________________________________________
>>>>> R-help at stat.math.ethz.ch mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained,
>>>> reproducible code.
>>>>>
>>>>
>>>>
>>>> --
>>>> Jim Holtman
>>>> Cincinnati, OH
>>>> +1 513 646 9390
>>>>
>>>> What is the problem you are trying to solve?
>>>>
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From depire at gmail.com  Sun Sep 17 16:45:11 2006
From: depire at gmail.com (Alexandre Depire)
Date: Sun, 17 Sep 2006 16:45:11 +0200
Subject: [R] Insert R code in LaTeX document
Message-ID: <19de4f9e0609170745j43d703e3y105e642663b50a2c@mail.gmail.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20060917/27772bd9/attachment.pl 

From MSchwartz at mn.rr.com  Sun Sep 17 17:30:47 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Sun, 17 Sep 2006 10:30:47 -0500
Subject: [R] Insert R code in LaTeX document
In-Reply-To: <19de4f9e0609170745j43d703e3y105e642663b50a2c@mail.gmail.com>
References: <19de4f9e0609170745j43d703e3y105e642663b50a2c@mail.gmail.com>
Message-ID: <1158507047.12595.17.camel@localhost.localdomain>

On Sun, 2006-09-17 at 16:45 +0200, Alexandre Depire wrote:
> Hello,
> i would like to insert R code in LaTeX document.
> I see something about the 'listings' package, but i would like if it is the
> best way and if it is possible to use the command \include{programme.R}.
> I have the following solution but it doesn't work with \include and \input
> 
> ======
> LaTex
> ======
> 
> main.tex
> ------------------
> \documentclass{report}
> \usepackage{listings}
> \lstloadlanguages{R}
> 
> \begin{document}
> \include{annexe}
> \end{document}
> 
> annexe.tex
> ---------------
> The code is:
> \lstset{language=R}
> 
> #\include{} or  \input or direct code ???
> 
> R code
> ========
> # test
> rm(list=ls())
> x<-1:10
> mean(x)

Depending upon how much functionality you actually need, using the
'verbatim' environment may be easier:

\begin{verbatim}
  R Code Here
\end{verbatim}

Then you don't need the rest of the LaTeX packages and associated code.
The verbatim environment will use a monospace font by default. It is
available by default in LaTeX, but you can also load the verbatim
package (\usepackage{verbatim}) for a better implementation. The
verbatim environment also provides a \verbatiminput{FileName} command
that will enable you to insert a text file that will automatically be
put in a verbatim environment:

  \verbatiminput{FileName.R}

The listings package provides a lot of other functionality such as
syntax highlighting, line numbers and so forth. I presume, given what
you have above, that you have already looked at the manual for the
package. If not and you are mimicking some online examples, then you
might want to review it:

ftp://indian.cse.msu.edu/pub/mirrors/CTAN/macros/latex/contrib/listings/listings-1.3.pdf

In either case, if you have an external file, you don't want to use
\include, as that is for .tex files specifically. The file argument is
the base name of the file only, without the extension and .tex is
presumed.

Thus, you want to use \input, where you can specify the full FileName.R
as the argument:
  
  \input{FileName.R}

If you just have a small amount of R code to include, then just put it
in the body of your main .tex file and you don't have to worry about
multiple files or using \input, etc.

HTH,

Marc Schwartz


From jrkrideau at yahoo.ca  Sun Sep 17 18:10:19 2006
From: jrkrideau at yahoo.ca (John Kane)
Date: Sun, 17 Sep 2006 12:10:19 -0400 (EDT)
Subject: [R] help on data frame and matrix
In-Reply-To: <003301c6da3b$7fdb9360$dfe38780@finosaur>
Message-ID: <20060917161019.74792.qmail@web32808.mail.mud.yahoo.com>


--- Finosaur <mouandny at gmail.com> wrote:


> I am given a data frame, which actually contains a
> matrix. But I need to convert the data frame into a
> matrix so that I can use the matrix operators. Can
> anybody help me out? Thanks a lot.
> 
>  
> 
> Thaleia
 d <- as.matrix(data.frame)


From AnupTyagi at yahoo.com  Sun Sep 17 18:29:04 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Sun, 17 Sep 2006 16:29:04 +0000 (UTC)
Subject: [R] Insert R code in LaTeX document
References: <19de4f9e0609170745j43d703e3y105e642663b50a2c@mail.gmail.com>
Message-ID: <loom.20060917T182804-875@post.gmane.org>

Alexandre Depire <depire <at> gmail.com> writes:

> 
> Hello,
> i would like to insert R code in LaTeX document.
> I see something about the 'listings' package, but i would like if it is the
> best way and if it is possible to use the command \include{programme.R}.
> I have the following solution but it doesn't work with \include and \input

Following latex code worked for me. Anupam

\documentclass{report}
\usepackage{listings}
\begin{document}

Somethings .....

\lstset{% general command to set parameter(s)
basicstyle=\small, % print whole in small
stringstyle=\ttfamily, % typewriter type for strings
numbers=left, % numbers on the left
numberstyle=\tiny, % Tiny numbers
stepnumber=2, % number every second line of code
numbersep=5pt, % 5pt seperation between numbering and code listing
language=R }

\lstinputlisting{text1.R}

\end{document}


From vincent.goulet at act.ulaval.ca  Sun Sep 17 18:36:12 2006
From: vincent.goulet at act.ulaval.ca (Vincent Goulet)
Date: Sun, 17 Sep 2006 12:36:12 -0400
Subject: [R] Building the call of an arbitrary function
Message-ID: <200609171236.12215.vincent.goulet@act.ulaval.ca>

Hy all,

Is there a direct way to build the complete function call of an arbitrary 
function?

Here's what I want to do. A function will build a function which will itself 
call a probability density function for some law given in argument to the 
first function:

> f("gamma", 1000)

will return, say,

function(x, shape, rate, scale = 1/rate) 
    dgamma(x + 1000, shape, rate, scale = 1/rate)

(Notice that the arguments of the output function are those of dgamma().)

I tried all sorts of combinations of call(), formals(), args() et al. to no 
avail. But then, I avoided, so far, to build the whole thing as a character 
string. Would it be the only option?

Thanks for any help.

-- 
  Vincent Goulet, Professeur agr?g?
  ?cole d'actuariat
  Universit? Laval, Qu?bec 
  Vincent.Goulet at act.ulaval.ca   http://vgoulet.act.ulaval.ca


From AnupTyagi at yahoo.com  Sun Sep 17 18:48:51 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Sun, 17 Sep 2006 16:48:51 +0000 (UTC)
Subject: [R] Any other R users in Philadelphia?
References: <8913fde30609162043t753c5754m503c5f8dc6bd00fe@mail.gmail.com>
Message-ID: <loom.20060917T184558-645@post.gmane.org>

Mitchell Maltenfort <mmalten <at> gmail.com> writes:

> I'm still new to R and wouldn't mind meeting other R users, at any
> level of experience.

This list is as good a place as any. Other is an R conference. There may also be
some undergrads at economics dept at UPenn. Anupam.


From AnupTyagi at yahoo.com  Sun Sep 17 19:27:30 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Sun, 17 Sep 2006 17:27:30 +0000 (UTC)
Subject: [R]
	=?utf-8?q?FW=3A_R_Reference_Card_and_other_help_=28especially?=
	=?utf-8?q?_useful_for=09Newbies=29?=
References: <000901c6d8da$fdab2600$711f210a@gne.windows.gene.com>
Message-ID: <loom.20060917T191832-847@post.gmane.org>

New users may also want to look at SciViews R Graphical User Interface(GUI). It
can be a good learning tool. Its text based editor is basic compared to WinEdt
with the R editing plug-in, or ESS and (X)Emacs combination. But it has
point-and-click menus that help in writing code, and easy view of objects, etc
can be very helpful for new users. Using this GUI may require you to install
some R packages listed on the SciViews page.

http://www.sciviews.org/SciViews-R/

Anupam.


From spencer.graves at pdf.com  Sun Sep 17 19:30:44 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 17 Sep 2006 10:30:44 -0700
Subject: [R] Problem with Variance Components (and general glmmconfusion)
In-Reply-To: <00da01c6d289$08dafe40$8262de8b@Toby>
References: <01b001c6d075$86fb3940$5401a8c0@Toby>	<40e66e0b0609061723l494b9990n602939aa2f6ac152@mail.gmail.com>
	<00da01c6d289$08dafe40$8262de8b@Toby>
Message-ID: <450D8644.4050809@pdf.com>

      With minor modifications, I got your example to work.  After 
reading in your 'testdata', I then made 'site' and 'Array' factors: 

testdata$site <- factor(testdata$site)
testdata$Array <- factor(testdata$Array)

# Note:  S-Plus and R are case sensitive, so 'array' and 'Array' are two 
different things. 

      Then rather than using 'attach', I used the 'data' argument.  It's 
extra book keeping, but to me seems cleaner. 

library(lme4)
modelusd<-lmer(USD~1 + (1|forest/site), testdata)
MC.modelusd<-mcmcsamp(modelusd, 50000)
modelusd2<-lmer(USD~1 + (1|forest/site/Array), data=testdata)
MC.modelusd2<-mcmcsamp(modelusd2, 50000)

      These all worked fine for me;  see 'sessionInfo()' below. 

      However, the second model is nonsense, because you have only one 
level of Array within each forest/site combination.  Thus, Array is 
completely confounded with the residual.  The variance decomposition for 
modelusd and modelusd2 are identical, except that the Residual Variance 
in the first is the sum of the Variances for Array:(site:forest) in the 
second.  From HPDinterval, I got the following for the two: 

 > HPDinterval(MC.modelusd)
                     lower      upper
(Intercept)     -4.6265645  24.168019
log(sigma^2)     2.1184607   2.595467
log(st:f.(In))   0.8987962   2.901460
log(frst.(In))   1.7248681   6.889331
deviance       814.9391134 827.032135
attr(,"Probability")
[1] 0.95
 > HPDinterval(MC.modelusd2)
                     lower      upper
(Intercept)     -4.6414218  23.653071
log(sigma^2)   -16.4372306   2.666573
log(A:(:.(In))  -4.6028284   2.744530
log(st:f.(In))   0.8660924   2.987095
log(frst.(In))   1.4793119   6.981519
deviance       814.8988318 827.634986
attr(,"Probability")
[1] 0.95
 >
      The intervals are similar for site and forest between the two 
models.  However, the interval for log(sigma^2) is not terribly wide for 
the first but is essentially unbounded below in the second for both 
log(sigma^2) and log(Variance(Array)):  The lower limit for sigma^2 = 
exp(-16.44) = 7e-8 and for var(Array) = exp(-4.6) = 0.01. 

      Regarding intraclass correlation, it's not clear to me what 
definition you are using.  (Dave Howell notes that there are many 
different definitions;  
"www.uvm.edu/~dhowell/StatPages/More_Stuff/icc/icc.html".)   Whatever 
definition you are using, however, there is no easy way to combine 
multiple intervals into one with a transformation.  Before MCMC, it was 
common to use a first order Taylor approximation.  That procedure was 
great when there was nothing else sensible available, but could be 
highly misleading if the original intervals were not symmetric or the 
linear approximation not very good over  most of the intervals of 
interest.  Fortunately, with MCMC, it's fairly easy.  The documentation 
says that the output of  'mcmcsamp' is an object of class 'mcmc'.  Using 
'str', I find that it looks like it might be a numeric array, which is 
confirmed as follows: 

 > is.array(MC.modelusd)
[1] TRUE

      Therefore, I will try to just add columns to that array as follows: 

MC.modelusd. <- cbind(MC.modelusd,
         sigma2=exp(MC.modelusd[, "log(sigma^2)"]),
         site=exp(MC.modelusd[, "log(st:f.(In))"]),
         forest=exp(MC.modelusd[, "log(frst.(In))"] ) )
MC.modelusd.a <- cbind(MC.modelusd.,
     intraclass.site=MC.modelusd.[, "site"] / (
       MC.modelusd.[, "site"]+MC.modelusd.[, "sigma2"]) )

      When I tried 'HPDinterval(MC.modelusd.a) at this point, I got an 
error message.  Therefore, I changed the class and tried again: 

class(MC.modelusd.a) <- class(MC.modelusd)
HPDinterval(MC.modelusd.a)
                        lower       upper
(Intercept)      -4.626564547  24.1680185
log(sigma^2)      2.118460743   2.5954665
log(st:f.(In))    0.898796162   2.9014600
log(frst.(In))    1.724868091   6.8893307
deviance        814.939113359 827.0321354
sigma2            8.198757903  13.2489330
site              1.627992273  15.8475869
forest            0.003788248 650.2006949
intraclass.site   0.173906803   0.6321633
attr(,"Probability")
[1] 0.95
 
      If you follow this example, I believe you should be able to get 
other desired "highest posterior density (HPD)" intervals. 

      Hope this helps. 
      Spencer Graves
 > sessionInfo()
Version 2.3.1 Patched (2006-08-13 r38872)
i386-pc-mingw32

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
[7] "base"    

other attached packages:
      coda       lme4     Matrix    lattice
  "0.10-6"  "0.995-2" "0.995-16"  "0.13-10"

Toby Gardner wrote:

<snip>
> What I am really after are the intra-class correlation coefficients so I can 
> demonstrate the variability in a given environmental variable at different 
> spatial scales.  I can of course calculate the % variance explained for each 
> random effect from the summary(lmer).  However - and this may be a stupid 
> question! - but can the intervals for the StDev of the random effects also 
> just be transformed to intervals of the variance (and then converted to % 
> values for the intra-class correlation coefficients) by squaring?
>   
SG:  NO, there is no easy transformation of the intervals. 
> Ideally I would like to partition the variance explained by all (three) 
> spatially nested scales - forest / site / array - where array is the sample 
> unit.  Using lmer produces the model summary I want:
>
>   
>> modelusd2<-lmer(USD~1 + (1|forest/site/array))
>>     
>
>   
>> summary(modelusd2)
>>     
>
>
>
> Linear mixed-effects model fit by REML
>
> Formula: USD ~ 1 + (1 | forest/site/array)
>
>       AIC      BIC    logLik MLdeviance REMLdeviance
>
>  818.7469 830.7894 -405.3734   815.0236     810.7469
>
> Random effects:
>
>  Groups              Name        Variance Std.Dev.
>
>  array:(site:forest) (Intercept)  7.5559  2.7488
>
>  site:forest         (Intercept)  6.2099  2.4920
>
>  forest              (Intercept) 33.0435  5.7484
>
>  Residual                         2.8776  1.6963
>
> number of obs: 150, groups: array:(site:forest), 150; site:forest, 15; 
> forest, 3
>
>
>
> Fixed effects:
>
>             Estimate Std. Error t value
>
> (Intercept)   9.8033     3.3909  2.8911
>
>
>
> However - the mcmcsamp process fails
>
>
>
>   
>> MC.modelusd2<-mcmcsamp(modelusd2, 50000)
>>     
>
>
>
> with this error message:
>
>
>
> Error: Leading minor of order 1 in downdated X'X is not positive definite
>
> Error in t(.Call(mer_MCMCsamp, object, saveb, n, trans, verbose)) :
>
>             unable to find the argument 'x' in selecting a method for 
> function 't'
>
>   
>
>
>
> Am I trying something impossible here?
>
>
>
> Regarding GLMMs..(now with species count data, blocking random factors and 
> multiple fixed factors)
>
>
>
> When using lmer I would suggest using method = "Laplace" and perhaps
> control = list(usePQL = FALSE, msVerbose = 1) as I mentioned in
> another reply to the list a few minutes ago.
>
>
>
> This seems to work well, thanks.
>
>
>
> With the greatest respect to all concerned, if I could I would like to echo 
> the request by Martin Maechler on the list a few weeks ago that it would be 
> extremely useful (especially for newcomers like me - and likely would 
> greatly reduce the traffic on this list looking at many of the past threads) 
> if authors of packages were able to be explicit in the help files about how 
> functions differ (key advantages and disadvantages) from packages offering 
> otherwise very similar functions (e.g. lmer/glmmML - although the subsequent 
> comment by Dr Bates on this helped a lot).
>
>
>
> Many thanks!
>
>
>
> Toby Gardner
>
>
>
> platform       i386-pc-mingw32
> arch           i386
> os             mingw32
> system         i386, mingw32
> status
> major          2
> minor          3.1
> year           2006
> month          06
> day            01
> svn rev        38247
> language       R
> version.string Version 2.3.1 (2006-06-01)
>
>
>
>   
>> dump("testdata", file=stdout())
>>     
> testdata <-
> structure(list(forest = structure(as.integer(c(2, 2, 2, 2, 2,
> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
> 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
> 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
> 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)), .Label = c("EUC",
> "PF", "SF"), class = "factor"), site = as.integer(c(1, 1, 1,
> 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3,
> 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5,
> 5, 5, 5, 5, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,
> 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4,
> 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 1, 1, 1, 1, 1, 1, 1,
> 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3,
> 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5
> )), Array = as.integer(c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2,
> 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2,
> 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2,
> 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2,
> 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2,
> 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2,
> 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2,
> 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)), USD = c(13.67,
> 13, 9.33, 10.67, 11, 10, 11.67, 10.33, 7, 8, 5.67, 7.67, 13.67,
> 10.33, 10.67, 7, 10.33, 12, 14, 7.67, 6.67, 4.33, 9, 13.67, 11,
> 19, 14, 19.67, 18.67, 8.33, 4.67, 10, 5, 11, 8, 7.67, 11, 12,
> 11, 7.67, 13.67, 15.33, 12, 11.33, 14.67, 13.33, 7, 12, 11.33,
> 11.33, 0.67, 3.33, 2, 2.67, 0.33, 1.33, 1.33, 1, 0.67, 0, 3.33,
> 3.33, 5.67, 4.67, 1.33, 3.67, 1, 6.33, 3, 1.67, 3.67, 5.67, 5.33,
> 2.67, 1.67, 2.33, 3.67, 6.67, 5.33, 9, 8, 5.67, 2.67, 0, 4.33,
> 8, 6, 3.67, 10.67, 9, 0.67, 1, 1.33, 0.67, 3, 1.5, 0.67, 0.33,
> 7.67, 7.33, 22, 18.67, 16.67, 21.33, 22.33, 22.33, 17.67, 16.33,
> 19.67, 20.67, 21.33, 17.33, 19, 19.33, 18.33, 18.67, 18.33, 19,
> 18.33, 19.33, 17.33, 12.33, 9.33, 8.33, 6, 17, 18, 8, 12, 15.67,
> 6, 1.33, 19, 11.67, 7, 16.33, 16, 14, 10.33, 4, 19, 19.67, 14,
> 15.33, 14, 6.33, 11.33, 11.67, 14, 15.33)), .Names = c("forest",
> "site", "Array", "USD"), class = "data.frame", row.names = c("1",
> "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13",
> "14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24",
> "25", "26", "27", "28", "29", "30", "31", "32", "33", "34", "35",
> "36", "37", "38", "39", "40", "41", "42", "43", "44", "45", "46",
> "47", "48", "49", "50", "51", "52", "53", "54", "55", "56", "57",
> "58", "59", "60", "61", "62", "63", "64", "65", "66", "67", "68",
> "69", "70", "71", "72", "73", "74", "75", "76", "77", "78", "79",
> "80", "81", "82", "83", "84", "85", "86", "87", "88", "89", "90",
> "91", "92", "93", "94", "95", "96", "97", "98", "99", "100",
> "101", "102", "103", "104", "105", "106", "107", "108", "109",
> "110", "111", "112", "113", "114", "115", "116", "117", "118",
> "119", "120", "121", "122", "123", "124", "125", "126", "127",
> "128", "129", "130", "131", "132", "133", "134", "135", "136",
> "137", "138", "139", "140", "141", "142", "143", "144", "145",
> "146", "147", "148", "149", "150"))
>
>
>
>
>
>
>
> ----- Original Message ----- 
> From: "Douglas Bates" <bates at stat.wisc.edu>
> To: "Toby Gardner" <t.gardner at uea.ac.uk>
> Cc: "R-help" <r-help at stat.math.ethz.ch>
> Sent: Thursday, September 07, 2006 1:23 AM
> Subject: Re: [R] Problem with Variance Components (and general 
> glmmconfusion)
>
>
>   
>> On 9/4/06, Toby Gardner <t.gardner at uea.ac.uk> wrote:
>>     
>>> Dear list,
>>>
>>> I am having some problems with extracting Variance Components from a 
>>> random-effects model:
>>>
>>> I am running a simple random-effects model using lme:
>>>
>>> model<-lme(y~1,random=~1|groupA/groupB)
>>>
>>> which returns the output for the StdDev of the Random effects, and model 
>>> AIC etc as expected.
>>>
>>> Until yesterday I was using R v. 2.0, and had no problem in calling the 
>>> variance components of the above model using VarCorr(model), together 
>>> with their 95% confidence intervals using intervals() - although for some 
>>> response variables a call to intervals() returns the error: Cannot get 
>>> confidence intervals on var-cov components: Non-positive definite 
>>> approximate variance-covariance.
>>>
>>> I have now installed R v. 2.3.1 and am now experiencing odd behaviour 
>>> with VarCorr(lme.object), with an error message typically being returned:
>>>
>>> Error in VarCorr(model) : no direct or inherited method for function 
>>> 'VarCorr' for this call
>>>
>>> Is this known to happen? For instance could it be due to the subsequent 
>>> loading of new packages? (lme4 for instance?).
>>>       
>> Yes.  Avoid loading lme4 and nlme simultaneously.
>>
>>     
>>> To get around this problem I have tried running the same model using 
>>> lmer:
>>>
>>> model2<-lmer(y~1 + (1|groupA) + (1|groupB))
>>>       
>> In recent versions of lme4 you can use the specification
>>
>> model2 <- lmer(y ~ 1 + (1|groupA/groupB))
>>
>> Your version may be correct or not.  It depends on what the distinct
>> levels of groupB correspond to.  The version with the / is more
>> reliable.
>>
>>     
>>> Should this not produce the same model? The variance components are very 
>>> similar but not identical, making me think that I am doing something 
>>> wrong. I am also correct in thinking that intervals() does not work with 
>>> lmer? I get: Error in intervals(model2) : no applicable method for 
>>> "intervals"
>>>       
>> That is correct.  Currently there is no intervals method for an lmer
>> model.  You can use mcmcsamp to get a Markov chain Monte Carlo sample
>> to which you can apply HPDinterval from the "coda" package.  However,
>> these are stochastic intervals so it is best to try on a couple of
>> chains to check on the reproducibility or the intervals.
>>
>>     
>>> GLMM
>>>
>>> I have a general application question - please excuse my ignorance, I am 
>>> relatively new to this and trying to find a way through the maze.  In 
>>> short I need to compile generalized linear mixed models both for (a) 
>>> Poisson data and (b) binonial data incorporating a two nested random 
>>> factors, and I need to be able to extract AIC values as I am taking an 
>>> information-theoretic approach to model selection.  Prior to sending an 
>>> email to the list I have spent quite a few days reading the background on 
>>> a number of functions, all of which offer potential for this; glmmML, 
>>> glmmPQL, lmer, and glmmADMB.  I can understand that glmmPQL is unsuitable 
>>> because there is no way of knowing the maximised likelihood, but is there 
>>> much difference between the remaining three options? I have seen 
>>> simulation comparisons published on this list between glmmADMB and 
>>> glmmPQL and lmer, but it seems these are before the latest release of 
>>> lmer, and also they do not evaluate glmmML.  To a newcomer this myria!
>>>       
>> d !
>>     
>>>  of options is bewildering, can anyone offer advice as to the most robust 
>>> approach?
>>>       
>> Goran can correct me if I am wrong but I don't believe that glmmML can
>> be used with multiple levels of random effects.
>>
>> I'm not sure what the status of glmmADMB is these days.  There was
>> some controversy regarding the license applied to some of that code a
>> while back.  I don't know if it has been resolved to everyone's
>> satisfaction.
>>
>> When using lmer I would suggest using method = "Laplace" and perhaps
>> control = list(usePQL = FALSE, msVerbose = 1) as I mentioned in
>> another reply to the list a few minutes ago.
>>
>> Let us know how it works out.
>>
>>     
>>> Many thanks for your time and patience,
>>>
>>> Toby Gardner
>>>
>>> School of Environmental Sciences
>>> University of East Anglia
>>> Norwich, NR4 7TJ
>>> United Kingdom
>>> Email: t.gardner at uea.ac.uk
>>> Website: www.uea.ac.uk/~e387495
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide 
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>       
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>     
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From marcella.marinelli at uniroma1.it  Sun Sep 17 20:02:08 2006
From: marcella.marinelli at uniroma1.it (march)
Date: Sun, 17 Sep 2006 11:02:08 -0700 (PDT)
Subject: [R] time series simulation
Message-ID: <6352577.post@talk.nabble.com>


Hi everybody
I'm trying to simulate a stochastic process in R. I would like consider n
log normal time series. The first time serie has a growth rate lower than
the second and so on. the initial time of the first serie is lower than the
initial time of the second and so on. In the long run the series have the
same value. Do you have any idea at running such a process?
Other question: How can I reduce the domain of a random variable?
Thanks
March
                                                                  
-- 
View this message in context: http://www.nabble.com/time-series-simulation-tf2287059.html#a6352577
Sent from the R help forum at Nabble.com.


From ggrothendieck at gmail.com  Sun Sep 17 20:12:39 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 17 Sep 2006 14:12:39 -0400
Subject: [R] Building the call of an arbitrary function
In-Reply-To: <200609171236.12215.vincent.goulet@act.ulaval.ca>
References: <200609171236.12215.vincent.goulet@act.ulaval.ca>
Message-ID: <971536df0609171112s689f5681q8100a931ba197fe0@mail.gmail.com>

Try this.   We use do.call to call f with the args defined by ...
except that we replace the first arg with ..1+a (where ..1 means
first arg in R):

F <- function(f, a)
          function(...) do.call(match.fun(f), replace(list(...), 1, ..1 + a))
g <- F("+", 1000)
g(1,2) # 1003


On 9/17/06, Vincent Goulet <vincent.goulet at act.ulaval.ca> wrote:
> Hy all,
>
> Is there a direct way to build the complete function call of an arbitrary
> function?
>
> Here's what I want to do. A function will build a function which will itself
> call a probability density function for some law given in argument to the
> first function:
>
> > f("gamma", 1000)
>
> will return, say,
>
> function(x, shape, rate, scale = 1/rate)
>    dgamma(x + 1000, shape, rate, scale = 1/rate)
>
> (Notice that the arguments of the output function are those of dgamma().)
>
> I tried all sorts of combinations of call(), formals(), args() et al. to no
> avail. But then, I avoided, so far, to build the whole thing as a character
> string. Would it be the only option?
>
> Thanks for any help.
>
> --
>  Vincent Goulet, Professeur agr?g?
>  ?cole d'actuariat
>  Universit? Laval, Qu?bec
>  Vincent.Goulet at act.ulaval.ca   http://vgoulet.act.ulaval.ca
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From murdoch at stats.uwo.ca  Sun Sep 17 20:12:29 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 17 Sep 2006 14:12:29 -0400
Subject: [R] Building the call of an arbitrary function
In-Reply-To: <200609171236.12215.vincent.goulet@act.ulaval.ca>
References: <200609171236.12215.vincent.goulet@act.ulaval.ca>
Message-ID: <450D900D.6010702@stats.uwo.ca>

On 9/17/2006 12:36 PM, Vincent Goulet wrote:
> Hy all,
> 
> Is there a direct way to build the complete function call of an arbitrary 
> function?
> 
> Here's what I want to do. A function will build a function which will itself 
> call a probability density function for some law given in argument to the 
> first function:
> 
>> f("gamma", 1000)
> 
> will return, say,
> 
> function(x, shape, rate, scale = 1/rate) 
>     dgamma(x + 1000, shape, rate, scale = 1/rate)
> 
> (Notice that the arguments of the output function are those of dgamma().)
> 
> I tried all sorts of combinations of call(), formals(), args() et al. to no 
> avail. But then, I avoided, so far, to build the whole thing as a character 
> string. Would it be the only option?

No, do.call is what you want.

dgamma(x + 1000, shape, rate, scale = 1/rate)

is the same as

do.call("dgamma", list(x+1000, shape, rate, scale=1/rate))

But since you're going to have to look up the parameters that are 
appropriate to your target density (i.e. shape, rate, scale), I'm not 
sure how useful this will be.  It might be easier just to code the call 
to dgamma directly.

Duncan Murdoch


From m.blizinski at wit.edu.pl  Sun Sep 17 20:22:28 2006
From: m.blizinski at wit.edu.pl (Maciej =?UTF-8?Q?Blizi=C5=84ski?=)
Date: Sun, 17 Sep 2006 20:22:28 +0200
Subject: [R] Standard error of coefficient in linear regression
Message-ID: <1158517348.1600.25.camel@localhost.localnet>

Hello R users,

I have a substantial question about statistics, not about R itself, but
I would love to have an answer from an R user, in form of an example in
R syntax. I have spent whole Sunday searching in Google and browsing the
books. I've been really close to the answer but there are at least three
standard errors you can talk about in the linear regression and I'm
really confused. The question is:

How exactly are standard errors of coefficients calculated in the linear
regression?

Here's an example from a website I've read [1]. A company wants to know
if there is a relationship between its advertising expenditures and its
sales volume. 

========================================================
> exped <- c(4.2, 6.1, 3.9, 5.7, 7.3, 5.9)
> sales <- c(27.1, 30.4, 25.0, 29.7, 40.1, 28.8)
> S <- data.frame(exped, sales)
> summary(lm(sales ~ exped, data = S))

Call:
lm(formula = sales ~ exped, data = S)

Residuals:
      1       2       3       4       5       6
 1.7643 -1.9310  0.7688 -1.1583  3.3509 -2.7947

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept)   9.8725     5.2394   1.884   0.1326
exped         3.6817     0.9295   3.961   0.0167 *
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Residual standard error: 2.637 on 4 degrees of freedom
Multiple R-Squared: 0.7968,     Adjusted R-squared: 0.7461
F-statistic: 15.69 on 1 and 4 DF,  p-value: 0.01666
========================================================

I can calculate the standard error of the estimate, according to the
equation [2]...

> S.m <- lm(sales ~ exped, data = S)
> S$pred <- predict(S.m)
> S$ye <- S$sales - S$pred
> S$ye2 <- S$ye ^ 2
> Se <- sqrt(sum(S$ye2)/(length(S$sales) - 1 - 1))
> Se
[1] 2.636901

...which matches the "Residual standard error" and I'm on the right
track. Next step would be to use the equation [3] to calculate the
standard error of the regression coefficient (here: exped). The equation
[3] uses two variables, meaning of which I can't really figure out. As
the calculated value Sb is scalar, all the parameters need also to be
scalars. I've already calculated Se, so I'm missing x and \bar{x}. The
latter could be the estimated coefficient. What is x then?

Regards,
Maciej

[1] http://www.statpac.com/statistics-calculator/correlation-regression.htm
[2] http://www.answers.com/topic/standard-error-of-the-estimate
[3] http://www.answers.com/topic/standard-error-of-the-regression-coefficient

-- 
Maciej Blizi?ski <m.blizinski at wit.edu.pl>
http://automatthias.wordpress.com


From Dimitris.Rizopoulos at med.kuleuven.be  Sun Sep 17 21:13:02 2006
From: Dimitris.Rizopoulos at med.kuleuven.be (Dimitrios Rizopoulos)
Date: Sun, 17 Sep 2006 21:13:02 +0200
Subject: [R] Standard error of coefficient in linear regression
In-Reply-To: <1158517348.1600.25.camel@localhost.localnet>
References: <1158517348.1600.25.camel@localhost.localnet>
Message-ID: <20060917211302.oephpbipfls00cgc@webmail4.kuleuven.be>

these standard errors and other quantities are calculated as by  
products of the QR decomposition used in lm.fit(). A simple way (but  
not efficient) to obtain them is:

exped <- c(4.2, 6.1, 3.9, 5.7, 7.3, 5.9)
sales <- c(27.1, 30.4, 25.0, 29.7, 40.1, 28.8)
S <- data.frame(exped, sales)
lmfit <- lm(sales ~ exped, data = S)

X <- model.matrix(lmfit)
sigma2 <- sum((sales - fitted(lmfit))^2) / (nrow(X) - ncol(X))

sqrt(sigma2)
sqrt(diag(solve(crossprod(X))) * sigma2)

I hope it helps.

Best,
Dimitris


----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
      http://www.student.kuleuven.be/~m0390867/dimitris.htm


Quoting Maciej Blizi?ski <m.blizinski at wit.edu.pl>:

> Hello R users,
>
> I have a substantial question about statistics, not about R itself, but
> I would love to have an answer from an R user, in form of an example in
> R syntax. I have spent whole Sunday searching in Google and browsing the
> books. I've been really close to the answer but there are at least three
> standard errors you can talk about in the linear regression and I'm
> really confused. The question is:
>
> How exactly are standard errors of coefficients calculated in the linear
> regression?
>
> Here's an example from a website I've read [1]. A company wants to know
> if there is a relationship between its advertising expenditures and its
> sales volume.
>
> ========================================================
>> exped <- c(4.2, 6.1, 3.9, 5.7, 7.3, 5.9)
>> sales <- c(27.1, 30.4, 25.0, 29.7, 40.1, 28.8)
>> S <- data.frame(exped, sales)
>> summary(lm(sales ~ exped, data = S))
>
> Call:
> lm(formula = sales ~ exped, data = S)
>
> Residuals:
>       1       2       3       4       5       6
>  1.7643 -1.9310  0.7688 -1.1583  3.3509 -2.7947
>
> Coefficients:
>             Estimate Std. Error t value Pr(>|t|)
> (Intercept)   9.8725     5.2394   1.884   0.1326
> exped         3.6817     0.9295   3.961   0.0167 *
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
> Residual standard error: 2.637 on 4 degrees of freedom
> Multiple R-Squared: 0.7968,     Adjusted R-squared: 0.7461
> F-statistic: 15.69 on 1 and 4 DF,  p-value: 0.01666
> ========================================================
>
> I can calculate the standard error of the estimate, according to the
> equation [2]...
>
>> S.m <- lm(sales ~ exped, data = S)
>> S$pred <- predict(S.m)
>> S$ye <- S$sales - S$pred
>> S$ye2 <- S$ye ^ 2
>> Se <- sqrt(sum(S$ye2)/(length(S$sales) - 1 - 1))
>> Se
> [1] 2.636901
>
> ...which matches the "Residual standard error" and I'm on the right
> track. Next step would be to use the equation [3] to calculate the
> standard error of the regression coefficient (here: exped). The equation
> [3] uses two variables, meaning of which I can't really figure out. As
> the calculated value Sb is scalar, all the parameters need also to be
> scalars. I've already calculated Se, so I'm missing x and \bar{x}. The
> latter could be the estimated coefficient. What is x then?
>
> Regards,
> Maciej
>
> [1] http://www.statpac.com/statistics-calculator/correlation-regression.htm
> [2] http://www.answers.com/topic/standard-error-of-the-estimate
> [3] http://www.answers.com/topic/standard-error-of-the-regression-coefficient
>
> --
> Maciej Blizi?ski <m.blizinski at wit.edu.pl>
> http://automatthias.wordpress.com
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From Charles.Annis at StatisticalEngineering.com  Sun Sep 17 21:23:52 2006
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Sun, 17 Sep 2006 15:23:52 -0400
Subject: [R] Standard error of coefficient in linear regression
In-Reply-To: <20060917211302.oephpbipfls00cgc@webmail4.kuleuven.be>
Message-ID: <015401c6da8e$cfee25e0$6400a8c0@DD4XFW31>

An easier way is to use summary()

summary(lmfit)

or 

> summary(lmfit)$coefficients
            Estimate Std. Error  t value   Pr(>|t|)
(Intercept) 9.872541  5.2394254 1.884279 0.13262386
exped       3.681715  0.9294818 3.961040 0.01666313

or 

> summary(lmfit)$coefficients[,2]
(Intercept)       exped 
  5.2394254   0.9294818 



Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 
-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dimitrios Rizopoulos
Sent: Sunday, September 17, 2006 3:13 PM
To: Maciej Blizi?ski
Cc: R - help
Subject: Re: [R] Standard error of coefficient in linear regression

these standard errors and other quantities are calculated as by  
products of the QR decomposition used in lm.fit(). A simple way (but  
not efficient) to obtain them is:

exped <- c(4.2, 6.1, 3.9, 5.7, 7.3, 5.9)
sales <- c(27.1, 30.4, 25.0, 29.7, 40.1, 28.8)
S <- data.frame(exped, sales)
lmfit <- lm(sales ~ exped, data = S)

X <- model.matrix(lmfit)
sigma2 <- sum((sales - fitted(lmfit))^2) / (nrow(X) - ncol(X))

sqrt(sigma2)
sqrt(diag(solve(crossprod(X))) * sigma2)

I hope it helps.

Best,
Dimitris


----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
      http://www.student.kuleuven.be/~m0390867/dimitris.htm


Quoting Maciej Blizi?ski <m.blizinski at wit.edu.pl>:

> Hello R users,
>
> I have a substantial question about statistics, not about R itself, but
> I would love to have an answer from an R user, in form of an example in
> R syntax. I have spent whole Sunday searching in Google and browsing the
> books. I've been really close to the answer but there are at least three
> standard errors you can talk about in the linear regression and I'm
> really confused. The question is:
>
> How exactly are standard errors of coefficients calculated in the linear
> regression?
>
> Here's an example from a website I've read [1]. A company wants to know
> if there is a relationship between its advertising expenditures and its
> sales volume.
>
> ========================================================
>> exped <- c(4.2, 6.1, 3.9, 5.7, 7.3, 5.9)
>> sales <- c(27.1, 30.4, 25.0, 29.7, 40.1, 28.8)
>> S <- data.frame(exped, sales)
>> summary(lm(sales ~ exped, data = S))
>
> Call:
> lm(formula = sales ~ exped, data = S)
>
> Residuals:
>       1       2       3       4       5       6
>  1.7643 -1.9310  0.7688 -1.1583  3.3509 -2.7947
>
> Coefficients:
>             Estimate Std. Error t value Pr(>|t|)
> (Intercept)   9.8725     5.2394   1.884   0.1326
> exped         3.6817     0.9295   3.961   0.0167 *
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>
> Residual standard error: 2.637 on 4 degrees of freedom
> Multiple R-Squared: 0.7968,     Adjusted R-squared: 0.7461
> F-statistic: 15.69 on 1 and 4 DF,  p-value: 0.01666
> ========================================================
>
> I can calculate the standard error of the estimate, according to the
> equation [2]...
>
>> S.m <- lm(sales ~ exped, data = S)
>> S$pred <- predict(S.m)
>> S$ye <- S$sales - S$pred
>> S$ye2 <- S$ye ^ 2
>> Se <- sqrt(sum(S$ye2)/(length(S$sales) - 1 - 1))
>> Se
> [1] 2.636901
>
> ...which matches the "Residual standard error" and I'm on the right
> track. Next step would be to use the equation [3] to calculate the
> standard error of the regression coefficient (here: exped). The equation
> [3] uses two variables, meaning of which I can't really figure out. As
> the calculated value Sb is scalar, all the parameters need also to be
> scalars. I've already calculated Se, so I'm missing x and \bar{x}. The
> latter could be the estimated coefficient. What is x then?
>
> Regards,
> Maciej
>
> [1]
http://www.statpac.com/statistics-calculator/correlation-regression.htm
> [2] http://www.answers.com/topic/standard-error-of-the-estimate
> [3]
http://www.answers.com/topic/standard-error-of-the-regression-coefficient
>
> --
> Maciej Blizi?ski <m.blizinski at wit.edu.pl>
> http://automatthias.wordpress.com
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From miltinho_astronauta at yahoo.com.br  Sun Sep 17 22:08:29 2006
From: miltinho_astronauta at yahoo.com.br (Milton Cezar)
Date: Sun, 17 Sep 2006 17:08:29 -0300 (ART)
Subject: [R] Excluding columns from dataframe and selecting row records
Message-ID: <20060917200829.74948.qmail@web53408.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060917/770387eb/attachment.pl 

From spluque at gmail.com  Sun Sep 17 22:12:30 2006
From: spluque at gmail.com (Sebastian P. Luque)
Date: Sun, 17 Sep 2006 15:12:30 -0500
Subject: [R] histogram frequency weighing
Message-ID: <87ejuai769.fsf@patagonia.sebmags.homelinux.org>

Fellow R-helpers,

Suppose we create a histogram as follows (although it could be any vector
with zeroes in it):


R> lenh <- hist(iris$Sepal.Length, br=seq(4, 8, 0.05))
R> lenh$counts
 [1]  0  0  0  0  0  1  0  3  0  1  0  4  0  2  0  5  0  6  0 10  0  9  0  4  0
[26]  1  0  6  0  7  0  6  0  8  0  7  0  3  0  6  0  6  0  4  0  9  0  7  0  5
[51]  0  2  0  8  0  3  0  4  0  1  0  1  0  3  0  1  0  1  0  0  0  1  0  4  0
[76]  0  0  1  0  0


and we wanted to apply a weighing scheme where frequencies immediately
following (and only those) empty class intervals (0) should be adjusted by
averaging them over the number of preceding empty intervals + 1.  For
example, the first frequency that would need to be adjusted in 'lenh' is
element 6 (1), which has 5 preceding empty intervals, so its adjusted
count would be 1/6.  Similarly, the second one would be element 8 (3),
which has 1 preceding empty interval, so its adjusted count would be 3/2.
Can somebody please provide a hint to implement such a weighing scheme?

I thought about some very contrived ways to accomplish this, involving
'which' and 'diff', but I sense a function might already be available to
do this efficiently.  I couldn't find relevant info in the usual channels.
Thanks in advance for any pointers.


Cheers,

-- 
Seb


From lr at proxc.net  Sun Sep 17 22:27:02 2006
From: lr at proxc.net (Logan Lewis)
Date: Sun, 17 Sep 2006 16:27:02 -0400
Subject: [R] Excluding columns from dataframe and selecting row records
In-Reply-To: <20060917200829.74948.qmail@web53408.mail.yahoo.com>
References: <20060917200829.74948.qmail@web53408.mail.yahoo.com>
Message-ID: <200609171627.03031.lr@proxc.net>

Miltinho,

On Sun, Sep 17, 2006 at 05:08:29PM -0300, Milton Cezar wrote:
> Dear R-friends,
>
>   I have to simple questions. First I would like to exclude some 
columns from a dataframe and after I need select rows that satisfy some 
conditions.
>   My data frame looks like
>
>   Region Species Bodysize Weigth Age
>   Africa  Sp1 10.2 20 2
>   Africa  Sp2 12.2 12 2
>   Africa  Sp3 15.3 18 3
>   Africa  Sp4 11.5 40 4
>   Brazil  Sp1 10.2 40 3
>   Brazil  Sp2 22.2 32 2
>   Brazil  Sp3 12.3 28 3
>   Brazil  Sp4 21.5 30 5
>
>   And I need for example only "columns" Bodysize Weigth and Age when 
Region=="Brazil" AND Species=="Sp2" AND Age>=3
>
The "subset" command should suit your needs: subset(dataframe,
Region=="Brazil" && Species=="Sp2" && Age>=3,select =
c("Bodysize","Weigth","Age"))

See help(subset) for more info.

Regards,
Logan


From darrenleeweber at gmail.com  Sun Sep 17 23:37:15 2006
From: darrenleeweber at gmail.com (Darren Weber)
Date: Sun, 17 Sep 2006 14:37:15 -0700
Subject: [R] currency or stock trading strategy
Message-ID: <d2095b8c0609171437o496256aam53bbbcc25d1acf43@mail.gmail.com>

Hi,

are there any good charting and analysis tools for use with
currencies, stocks, etc. in R?  I have some tools to download currency
data from the NYFRB using python and XML.  Can we get and parse an XML
download using R?  Can we have interaction in R plots?  Does anyone
use R for back-testing trading strategies?  Are there any forums for
discussion of using R for this specific purpose (apart from this
general list)?  Is anyone aware of any general open-source
developments for these purposes (I don't see any from GNU or google
searches)?

Take care, Darren


From rroa at udec.cl  Sun Sep 17 23:47:25 2006
From: rroa at udec.cl (Ruben Roa Ureta)
Date: Sun, 17 Sep 2006 17:47:25 -0400 (CLT)
Subject: [R] Thousand separator in format of axis
Message-ID: <3125.201.223.234.96.1158529645.squirrel@webmail.udec.cl>

Hi:
How can i make that the numbers in the tick annotations of an axis be
written as, say 20 000 and not 20000 (i.e. with the little gap seprating
thousands)?
Thanks
Ruben


From jholtman at gmail.com  Mon Sep 18 00:05:15 2006
From: jholtman at gmail.com (jim holtman)
Date: Sun, 17 Sep 2006 18:05:15 -0400
Subject: [R] histogram frequency weighing
In-Reply-To: <87ejuai769.fsf@patagonia.sebmags.homelinux.org>
References: <87ejuai769.fsf@patagonia.sebmags.homelinux.org>
Message-ID: <644e1f320609171505w27e01070h2fe81f5077ed3e2@mail.gmail.com>

I think this should do it:

> lenh <- hist(iris$Sepal.Length, br=seq(4, 8, 0.05))$counts
> lenh  # original data
 [1]  0  0  0  0  0  1  0  3  0  1  0  4  0  2  0  5  0  6  0 10  0  9
 0  4  0  1  0  6  0  7  0  6  0
[34]  8  0  7  0  3  0  6  0  6  0  4  0  9  0  7  0  5  0  2  0  8  0
 3  0  4  0  1  0  1  0  3  0  1
[67]  0  1  0  0  0  1  0  4  0  0  0  1  0  0
> l.rle <- rle(lenh)
> # determine where '0's are
> Zero <- which(l.rle$values == 0)
> # if last entry in rle was 0, delete from offsets since we are changing +1
> if (tail(l.rle$values,1) == 0) Zero <- Zero[-length(Zero)]
> l.offsets <- cumsum(l.rle$lengths)  # offsets into original vector# modify original input
> lenh[l.offsets[Zero+1]] <- lenh[l.offsets[Zero  + 1]] / (l.rle$lengths[Zero]+1)
> lenh  # modified data
 [1] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.1666667
0.0000000 1.5000000 0.0000000 0.5000000
[11] 0.0000000 2.0000000 0.0000000 1.0000000 0.0000000 2.5000000
0.0000000 3.0000000 0.0000000 5.0000000
[21] 0.0000000 4.5000000 0.0000000 2.0000000 0.0000000 0.5000000
0.0000000 3.0000000 0.0000000 3.5000000
[31] 0.0000000 3.0000000 0.0000000 4.0000000 0.0000000 3.5000000
0.0000000 1.5000000 0.0000000 3.0000000
[41] 0.0000000 3.0000000 0.0000000 2.0000000 0.0000000 4.5000000
0.0000000 3.5000000 0.0000000 2.5000000
[51] 0.0000000 1.0000000 0.0000000 4.0000000 0.0000000 1.5000000
0.0000000 2.0000000 0.0000000 0.5000000
[61] 0.0000000 0.5000000 0.0000000 1.5000000 0.0000000 0.5000000
0.0000000 0.5000000 0.0000000 0.0000000
[71] 0.0000000 0.2500000 0.0000000 2.0000000 0.0000000 0.0000000
0.0000000 0.2500000 0.0000000 0.0000000
>
>

On 9/17/06, Sebastian P. Luque <spluque at gmail.com> wrote:
> Fellow R-helpers,
>
> Suppose we create a histogram as follows (although it could be any vector
> with zeroes in it):
>
>
> R> lenh <- hist(iris$Sepal.Length, br=seq(4, 8, 0.05))
> R> lenh$counts
>  [1]  0  0  0  0  0  1  0  3  0  1  0  4  0  2  0  5  0  6  0 10  0  9  0  4  0
> [26]  1  0  6  0  7  0  6  0  8  0  7  0  3  0  6  0  6  0  4  0  9  0  7  0  5
> [51]  0  2  0  8  0  3  0  4  0  1  0  1  0  3  0  1  0  1  0  0  0  1  0  4  0
> [76]  0  0  1  0  0
>
>
> and we wanted to apply a weighing scheme where frequencies immediately
> following (and only those) empty class intervals (0) should be adjusted by
> averaging them over the number of preceding empty intervals + 1.  For
> example, the first frequency that would need to be adjusted in 'lenh' is
> element 6 (1), which has 5 preceding empty intervals, so its adjusted
> count would be 1/6.  Similarly, the second one would be element 8 (3),
> which has 1 preceding empty interval, so its adjusted count would be 3/2.
> Can somebody please provide a hint to implement such a weighing scheme?
>
> I thought about some very contrived ways to accomplish this, involving
> 'which' and 'diff', but I sense a function might already be available to
> do this efficiently.  I couldn't find relevant info in the usual channels.
> Thanks in advance for any pointers.
>
>
> Cheers,
>
> --
> Seb
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From rmh at temple.edu  Mon Sep 18 00:12:34 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Sun, 17 Sep 2006 18:12:34 -0400 (EDT)
Subject: [R] Thousand separator in format of axis
Message-ID: <20060917181234.BID12797@po-d.temple.edu>

> format(10000, big.mark=" ")
[1] "10 000"
> format(seq(10000,100000,10000), big.mark=" ", scientific=FALSE)
 [1] " 10 000" " 20 000" " 30 000" " 40 000" " 50 000" " 60 000" " 70 000"
 [8] " 80 000" " 90 000" "100 000"

> plot(seq(10000, 100000, 10000), yaxt="n")
> axis(2, at=seq(10000, 100000, 10000), labels=format(seq(10000,100000,10000),
+       big.mark=" ", scientific=FALSE))


From MSchwartz at mn.rr.com  Mon Sep 18 00:24:33 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Sun, 17 Sep 2006 17:24:33 -0500
Subject: [R] Thousand separator in format of axis
In-Reply-To: <3125.201.223.234.96.1158529645.squirrel@webmail.udec.cl>
References: <3125.201.223.234.96.1158529645.squirrel@webmail.udec.cl>
Message-ID: <1158531873.4382.13.camel@localhost.localdomain>

On Sun, 2006-09-17 at 17:47 -0400, Ruben Roa Ureta wrote:
> Hi:
> How can i make that the numbers in the tick annotations of an axis be
> written as, say 20 000 and not 20000 (i.e. with the little gap seprating
> thousands)?
> Thanks
> Ruben


There are several ways to format numbers:
  ?format
  ?sprintf  
  ?formatC

A quick example:

 y <- seq(0, 100000, 10000)
 
 # Set the y axis so that it does not get drawn
 # See ?par
 plot(1:11, y, yaxt = "n", ylab = "")

 # Use formatC() and set 'big.mark' to " "
 # See ?axis also
 axis(2, at = y, labels = formatC(y, big.mark = " ", format = "d"), 
      las = 2)

 
HTH,

Marc Schwartz


From spluque at gmail.com  Mon Sep 18 00:27:57 2006
From: spluque at gmail.com (Sebastian P. Luque)
Date: Sun, 17 Sep 2006 17:27:57 -0500
Subject: [R] histogram frequency weighing
References: <87ejuai769.fsf@patagonia.sebmags.homelinux.org>
Message-ID: <8764fmi0wi.fsf@patagonia.sebmags.homelinux.org>

On Sun, 17 Sep 2006 15:12:30 -0500,
"Sebastian P. Luque" <spluque at gmail.com> wrote:

[...]

> I thought about some very contrived ways to accomplish this, involving
> 'which' and 'diff', but I sense a function might already be available to
> do this efficiently.

I think I found a better combination of those two functions than I what I
was initially playing with:


R> lenh <- hist(iris$Sepal.Length, br=seq(4, 8, 0.05))
R> ok <- which(lenh$counts > 0)
R> lenh$counts[ok] <- lenh$counts[ok] / diff(c(0, ok))
R> lenh$counts
 [1] 0.0000 0.0000 0.0000 0.0000 0.0000 0.1667 0.0000 1.5000 0.0000 0.5000
[11] 0.0000 2.0000 0.0000 1.0000 0.0000 2.5000 0.0000 3.0000 0.0000 5.0000
[21] 0.0000 4.5000 0.0000 2.0000 0.0000 0.5000 0.0000 3.0000 0.0000 3.5000
[31] 0.0000 3.0000 0.0000 4.0000 0.0000 3.5000 0.0000 1.5000 0.0000 3.0000
[41] 0.0000 3.0000 0.0000 2.0000 0.0000 4.5000 0.0000 3.5000 0.0000 2.5000
[51] 0.0000 1.0000 0.0000 4.0000 0.0000 1.5000 0.0000 2.0000 0.0000 0.5000
[61] 0.0000 0.5000 0.0000 1.5000 0.0000 0.5000 0.0000 0.5000 0.0000 0.0000
[71] 0.0000 0.2500 0.0000 2.0000 0.0000 0.0000 0.0000 0.2500 0.0000 0.0000


It makes sense, although I'm a bit nervous about floating-point issues
that might make this fail in some cases.  Any suggestions/comments
welcome.


-- 
Seb


From spluque at gmail.com  Mon Sep 18 00:33:57 2006
From: spluque at gmail.com (Sebastian P. Luque)
Date: Sun, 17 Sep 2006 17:33:57 -0500
Subject: [R] histogram frequency weighing
References: <87ejuai769.fsf@patagonia.sebmags.homelinux.org>
	<644e1f320609171505w27e01070h2fe81f5077ed3e2@mail.gmail.com>
Message-ID: <87y7sigm22.fsf@patagonia.sebmags.homelinux.org>

On Sun, 17 Sep 2006 18:05:15 -0400,
"jim holtman" <jholtman at gmail.com> wrote:

> I think this should do it:

[...]

Thank you Jim, the idea with 'rle' is great.  I missed your follow-up
before mine a minute ago with another solution.  I'll do some testing with
both.


Cheers,

-- 
Seb


From chabotd at globetrotter.net  Mon Sep 18 03:11:22 2006
From: chabotd at globetrotter.net (Denis Chabot)
Date: Sun, 17 Sep 2006 21:11:22 -0400
Subject: [R] merge gives me too many rows
Message-ID: <1506660C-D5E4-498C-B2D2-E0AB702FF6F1@globetrotter.net>

Hi,

I am using merge to add some variables to an existing dataframe. I  
use the option "all.x=F" so that my final dataframe will only have as  
many rows as the first file I name in the call to merge.

With a large dataframe using a lot of "by" variables, the number of  
rows of the merged dataframe increases from 177325 to 179690:

 >dim(test)
[1] 177325      9
 > test2 <- merge(test, fish, by=c("predateu", "origin", "navire",  
"nbpc", "no_rel", "trait", "tagno"), all.x=F)
 > dim(test2)
[1] 179690     11

I tried to make a smaller dataset with R commands that I could post  
here so that other people could reproduce, but merge behaved as  
expected: final number of rows was the same as the number of rows in  
the first file named in the call to merge.

I took a subset of my large dataframe and could mail this to anyone  
interested in verifying the problem.

 > test3 <- test[100001:160000,]
 >
 > dim(test3)
[1] 60000     9
 > test4 <- merge(test3, fish, by=c("predateu", "origin", "navire",  
"nbpc", "no_rel", "trait", "tagno"), all.x=F)
 >
 > dim(test4)
[1] 60043    11

I compared test3 and test4 line by line. The first 11419 lines were  
the same (except for added variables, obviously) in both dataframes,  
but then lines 11420 to 11423 were repeated in test4. Then no problem  
for a lot of rows, until rows 45756-45760 in test3. These are offset  
by 4 in test4 because of the first group of extraneous lines just  
reported, and are found on lines 45760 to 45765. But they are also  
repeated on lines 45765 to 45769. And so on a few more times.

Thus merge added lines (repeated a small number of lines) to the  
final dataframe despite my use of all.x=F.

Am I doing something wrong? If not, is there a solution? Not being  
able to merge is a setback! I was attempting to move the last few  
things I was doing with SAS to R...

Please let me know if you want the file test3 (2.3 MB as a csv file,  
but only 352 KB in R (.rda) format).

Sincerely,

Denis Chabot

 > R.Version()
$platform
[1] "powerpc-apple-darwin8.6.0"

$arch
[1] "powerpc"

$os
[1] "darwin8.6.0"

$system
[1] "powerpc, darwin8.6.0"

$status
[1] ""

$major
[1] "2"

$minor
[1] "3.1"

$year
[1] "2006"

$month
[1] "06"

$day
[1] "01"

$`svn rev`
[1] "38247"

$language
[1] "R"

$version.string
[1] "Version 2.3.1 (2006-06-01)"


From bingshanli at yahoo.com  Mon Sep 18 03:44:11 2006
From: bingshanli at yahoo.com (Bingshan Li)
Date: Sun, 17 Sep 2006 18:44:11 -0700 (PDT)
Subject: [R] merge strings
Message-ID: <20060918014411.12536.qmail@web56012.mail.re3.yahoo.com>

Hi all,

I have a vector and want to merge its elements one by
one into a single string or number. For example,
x=c(1,2,3), what I want is a new number 123. I used
"paste" but it just output "1" "2" "3" which is not
what I want. Is there any way to do this?

Thanks!


From murdoch at stats.uwo.ca  Mon Sep 18 03:47:59 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 17 Sep 2006 21:47:59 -0400
Subject: [R] merge strings
In-Reply-To: <20060918014411.12536.qmail@web56012.mail.re3.yahoo.com>
References: <20060918014411.12536.qmail@web56012.mail.re3.yahoo.com>
Message-ID: <450DFACF.8080200@stats.uwo.ca>

Bingshan Li wrote:
> Hi all,
>
> I have a vector and want to merge its elements one by
> one into a single string or number. For example,
> x=c(1,2,3), what I want is a new number 123. I used
> "paste" but it just output "1" "2" "3" which is not
> what I want. Is there any way to do this?
>
> Thanks!

Use the collapse argument to paste:

paste(x, collapse="")

Or, if you want a number,

sum(x * 10^rev(1:length(x) - 1))

Duncan Murdoch


From bingshanli at yahoo.com  Mon Sep 18 04:16:51 2006
From: bingshanli at yahoo.com (Bingshan Li)
Date: Sun, 17 Sep 2006 19:16:51 -0700 (PDT)
Subject: [R] merge strings
In-Reply-To: <450DFACF.8080200@stats.uwo.ca>
Message-ID: <20060918021651.62571.qmail@web56002.mail.re3.yahoo.com>

Hi Duncan Murdoch, 

Thanks for the trick and it works!


--- Duncan Murdoch <murdoch at stats.uwo.ca> wrote:

> Bingshan Li wrote:
> > Hi all,
> >
> > I have a vector and want to merge its elements one
> by
> > one into a single string or number. For example,
> > x=c(1,2,3), what I want is a new number 123. I
> used
> > "paste" but it just output "1" "2" "3" which is
> not
> > what I want. Is there any way to do this?
> >
> > Thanks!
> 
> Use the collapse argument to paste:
> 
> paste(x, collapse="")
> 
> Or, if you want a number,
> 
> sum(x * 10^rev(1:length(x) - 1))
> 
> Duncan Murdoch
>


From stgries_lists at arcor.de  Mon Sep 18 06:15:31 2006
From: stgries_lists at arcor.de (Stefan Th. Gries)
Date: Mon, 18 Sep 2006 06:15:31 +0200 (CEST)
Subject: [R] 404 HTTP not found
Message-ID: <15134490.1158552931340.JavaMail.ngmail@webmail15>

Hi

I wrote a script which retrieves links from websites and loads them with scan:

...
website<-tolower(scan(current.pages[i], what="character", sep="\n", quiet=TRUE))
...

However occasionally, the script finds broken links, such as <http://www.google.com/test>. when the script tries to access such websites, the repeat loop breaks and I get the error message

Error in file(file, "r") : unable to open connection
In addition: Warning message:
cannot open: HTTP status was '404 Not Found' 

Now my question: is there a way to test whether the target of a link exists that does not result in an error and, thus, discontinues my loop? I looked at the help files for files, scans, connections, and did a search for "404?' in th archives but couldn't find anything. I work with R 2.3.1 patched on Windows XP (both Home and Prof) and would appreciate any pointers ...
Thanks a lot,
STG


From ggrothendieck at gmail.com  Mon Sep 18 06:46:23 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 18 Sep 2006 00:46:23 -0400
Subject: [R] 404 HTTP not found
In-Reply-To: <15134490.1158552931340.JavaMail.ngmail@webmail15>
References: <15134490.1158552931340.JavaMail.ngmail@webmail15>
Message-ID: <971536df0609172146o379b4db7tdbc1bea641c33578@mail.gmail.com>

See ?try as in this example:

current.pages <- c("http://www.google.com", "http://www.google.com/test",
  "http://www.yahoo.com")

for(i in seq(along = current.pages)) {
	website <- try(tolower(scan(current.pages[i],
		what="character", sep="\n", quiet=TRUE)))
	if (inherits(website, "try-error")) cat(current.pages[i], "bad\n")
	else cat(current.pages[i], "ok\n")
}



On 9/18/06, Stefan Th. Gries <stgries_lists at arcor.de> wrote:
> Hi
>
> I wrote a script which retrieves links from websites and loads them with scan:
>
> ...
> website<-tolower(scan(current.pages[i], what="character", sep="\n", quiet=TRUE))
> ...
>
> However occasionally, the script finds broken links, such as <http://www.google.com/test>. when the script tries to access such websites, the repeat loop breaks and I get the error message
>
> Error in file(file, "r") : unable to open connection
> In addition: Warning message:
> cannot open: HTTP status was '404 Not Found'
>
> Now my question: is there a way to test whether the target of a link exists that does not result in an error and, thus, discontinues my loop? I looked at the help files for files, scans, connections, and did a search for "404?' in th archives but couldn't find anything. I work with R 2.3.1 patched on Windows XP (both Home and Prof) and would appreciate any pointers ...
> Thanks a lot,
> STG
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From macq at llnl.gov  Mon Sep 18 07:38:43 2006
From: macq at llnl.gov (Don MacQueen)
Date: Sun, 17 Sep 2006 22:38:43 -0700
Subject: [R] merge gives me too many rows
In-Reply-To: <1506660C-D5E4-498C-B2D2-E0AB702FF6F1@globetrotter.net>
References: <1506660C-D5E4-498C-B2D2-E0AB702FF6F1@globetrotter.net>
Message-ID: <p06230900c133dfaf2fba@[192.168.2.7]>

I think you may misunderstand the meaning of all.x = FALSE.

Setting all.x to false ensures that only rows of x that have matches 
in y will be included. Equivalently, if a row of x is not matched in 
y, it will not be in the output. However, if a row in x is matched by 
more than one row in y, then that row will be repeated as many times 
as there are matching rows in y. That is, you have a 1 to many match 
(1 in x to many in y). SAS behaves the same way.

Are you sure this is not what is happening?

Also, all.x = FALSE is the default; it is not necessary to specify 
it. In fact, the default is to output only rows that are found in 
both x and y (matching on the specified variables, of course).

-Don

At 9:11 PM -0400 9/17/06, Denis Chabot wrote:
>Hi,
>
>I am using merge to add some variables to an existing dataframe. I 
>use the option "all.x=F" so that my final dataframe will only have as 
>many rows as the first file I name in the call to merge.
>
>With a large dataframe using a lot of "by" variables, the number of 
>rows of the merged dataframe increases from 177325 to 179690:
>
>  >dim(test)
>[1] 177325      9
>  > test2 <- merge(test, fish, by=c("predateu", "origin", "navire", 
>"nbpc", "no_rel", "trait", "tagno"), all.x=F)
>  > dim(test2)
>[1] 179690     11
>
>I tried to make a smaller dataset with R commands that I could post 
>here so that other people could reproduce, but merge behaved as 
>expected: final number of rows was the same as the number of rows in 
>the first file named in the call to merge.
>
>I took a subset of my large dataframe and could mail this to anyone 
>interested in verifying the problem.
>
>  > test3 <- test[100001:160000,]
>  >
>  > dim(test3)
>[1] 60000     9
>  > test4 <- merge(test3, fish, by=c("predateu", "origin", "navire", 
>"nbpc", "no_rel", "trait", "tagno"), all.x=F)
>  >
>  > dim(test4)
>[1] 60043    11
>
>I compared test3 and test4 line by line. The first 11419 lines were 
>the same (except for added variables, obviously) in both dataframes, 
>but then lines 11420 to 11423 were repeated in test4. Then no problem 
>for a lot of rows, until rows 45756-45760 in test3. These are offset 
>by 4 in test4 because of the first group of extraneous lines just 
>reported, and are found on lines 45760 to 45765. But they are also 
>repeated on lines 45765 to 45769. And so on a few more times.
>
>Thus merge added lines (repeated a small number of lines) to the 
>final dataframe despite my use of all.x=F.
>
>Am I doing something wrong? If not, is there a solution? Not being 
>able to merge is a setback! I was attempting to move the last few 
>things I was doing with SAS to R...
>
>Please let me know if you want the file test3 (2.3 MB as a csv file, 
>but only 352 KB in R (.rda) format).
>
>Sincerely,
>
>Denis Chabot
>
>  > R.Version()
>$platform
>[1] "powerpc-apple-darwin8.6.0"
>
>$arch
>[1] "powerpc"
>
>$os
>[1] "darwin8.6.0"
>
>$system
>[1] "powerpc, darwin8.6.0"
>
>$status
>[1] ""
>
>$major
>[1] "2"
>
>$minor
>[1] "3.1"
>
>$year
>[1] "2006"
>
>$month
>[1] "06"
>
>$day
>[1] "01"
>
>$`svn rev`
>[1] "38247"
>
>$language
>[1] "R"
>
>$version.string
>[1] "Version 2.3.1 (2006-06-01)"
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


-- 
---------------------------------
Don MacQueen
Lawrence Livermore National Laboratory
Livermore, CA, USA


From gunther.hoening at ukmainz.de  Mon Sep 18 08:05:28 2006
From: gunther.hoening at ukmainz.de (=?iso-8859-1?Q?Gunther_H=F6ning?=)
Date: Mon, 18 Sep 2006 08:05:28 +0200
Subject: [R] Question on apply()
In-Reply-To: <87y7sigm22.fsf@patagonia.sebmags.homelinux.org>
Message-ID: <005601c6dae8$70a865f0$0f1e0b0a@3med.klinik.unimainz.de>

 Dear list,

I try to do the following:
I have an list of length n, with elements done by smooth.spline
(SmoothList).
Now I have a matrix with n rows and m columns with x-values(Xarray) 
Now I want ot predict the y-values.
Therefor I want to take the first element of SmoothList and the first row of
Xarray and predict for each element in Xarray the y value.
And then take the second element of SmoothList and second row of Xarray,
third row of SmoothList and third row of Xarray and so on....

I tried following:

NewValues <- function(x,LIST){predict(LIST,x)}
apply(Xarray, 2, NewValues,SmoothList)

But it don't work.

Could anybody help please ?

Gunther


From ripley at stats.ox.ac.uk  Mon Sep 18 08:14:08 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 18 Sep 2006 07:14:08 +0100 (BST)
Subject: [R] 404 HTTP not found
In-Reply-To: <15134490.1158552931340.JavaMail.ngmail@webmail15>
References: <15134490.1158552931340.JavaMail.ngmail@webmail15>
Message-ID: <Pine.LNX.4.64.0609180712030.18162@gannet.stats.ox.ac.uk>

See

?try
?tryCatch

On Mon, 18 Sep 2006, Stefan Th. Gries wrote:

> Hi
>
> I wrote a script which retrieves links from websites and loads them with scan:
>
> ...
> website<-tolower(scan(current.pages[i], what="character", sep="\n", quiet=TRUE))
> ...
>
> However occasionally, the script finds broken links, such as <http://www.google.com/test>. when the script tries to access such websites, the repeat loop breaks and I get the error message
>
> Error in file(file, "r") : unable to open connection
> In addition: Warning message:
> cannot open: HTTP status was '404 Not Found'
>
> Now my question: is there a way to test whether the target of a link exists that does not result in an error and, thus, discontinues my loop? I looked at the help files for files, scans, connections, and did a search for "404?' in th archives but couldn't find anything. I work with R 2.3.1 patched on Windows XP (both Home and Prof) and would appreciate any pointers ...
> Thanks a lot,
> STG

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From petr.pikal at precheza.cz  Mon Sep 18 08:43:19 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 18 Sep 2006 08:43:19 +0200
Subject: [R] Question on apply()
In-Reply-To: <005601c6dae8$70a865f0$0f1e0b0a@3med.klinik.unimainz.de>
References: <87y7sigm22.fsf@patagonia.sebmags.homelinux.org>
Message-ID: <450E5C27.15088.31C574@localhost>

Hi

not much information about what can be wrong. As nobody knows your 
Xarray and SmoothList it is hard to guess. You even omitted to show 
what "does not work"
So here are few guesses.

predict usually expects comparable data
apply(Xarray, 2, NewValues,LIST=SmoothList)

HTH
Petr




On 18 Sep 2006 at 8:05, Gunther H?ning wrote:

From:           	Gunther H?ning <gunther.hoening at ukmainz.de>
To:             	<r-help at stat.math.ethz.ch>
Date sent:      	Mon, 18 Sep 2006 08:05:28 +0200
Subject:        	[R] Question on apply()

>  Dear list,
> 
> I try to do the following:
> I have an list of length n, with elements done by smooth.spline
> (SmoothList).
> Now I have a matrix with n rows and m columns with x-values(Xarray)
> Now I want ot predict the y-values. Therefor I want to take the first
> element of SmoothList and the first row of Xarray and predict for each
> element in Xarray the y value. And then take the second element of
> SmoothList and second row of Xarray, third row of SmoothList and third
> row of Xarray and so on....
> 
> I tried following:
> 
> NewValues <- function(x,LIST){predict(LIST,x)}
> apply(Xarray, 2, NewValues,SmoothList)
> 
> But it don't work.
> 
> Could anybody help please ?
> 
> Gunther
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From marcella.marinelli at uniroma1.it  Mon Sep 18 08:44:55 2006
From: marcella.marinelli at uniroma1.it (march)
Date: Sun, 17 Sep 2006 23:44:55 -0700 (PDT)
Subject: [R] multiple density function
Message-ID: <6358504.post@talk.nabble.com>


Hi everybody
I'm new in R so the question will be easy for you
I'm running multiple density functions taking account of the following
conditions:
mean=seq(10,1,length=10)        
var=seq(3,1,length=10)


How can I describe the density functions on the same chart?
thanks
Marcella

-- 
View this message in context: http://www.nabble.com/multiple-density-function-tf2289386.html#a6358504
Sent from the R help forum at Nabble.com.


From maechler at stat.math.ethz.ch  Mon Sep 18 08:52:55 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 18 Sep 2006 08:52:55 +0200
Subject: [R] help on data frame and matrix
In-Reply-To: <20060917161019.74792.qmail@web32808.mail.mud.yahoo.com>
References: <003301c6da3b$7fdb9360$dfe38780@finosaur>
	<20060917161019.74792.qmail@web32808.mail.mud.yahoo.com>
Message-ID: <17678.16967.468474.258912@stat.math.ethz.ch>

>>>>> "John" == John Kane <jrkrideau at yahoo.ca>
>>>>>     on Sun, 17 Sep 2006 12:10:19 -0400 (EDT) writes:

    John> --- Finosaur <mouandny at gmail.com> wrote:

    >> I am given a data frame, which actually contains a
    >> matrix. But I need to convert the data frame into a
    >> matrix so that I can use the matrix operators. Can
    >> anybody help me out? Thanks a lot.
    >> 
    >> 
    >> Thaleia

    John>  d <- as.matrix(data.frame)

It's (almost always) better to use Dimitri's suggestion,
data.matrix(), since that also produces a numeric matrix when
the data frame contains factors.

Martin


From AnupTyagi at yahoo.com  Sun Sep 17 19:54:08 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Sun, 17 Sep 2006 17:54:08 +0000 (UTC)
Subject: [R]
	=?utf-8?q?FW=3A_R_Reference_Card_and_other_help_=28especially?=
	=?utf-8?q?_useful_for=09Newbies=29?=
References: <000901c6d8da$fdab2600$711f210a@gne.windows.gene.com>
	<loom.20060917T191832-847@post.gmane.org>
Message-ID: <loom.20060917T194039-387@post.gmane.org>

Anupam Tyagi <AnupTyagi <at> yahoo.com> writes:

> 
> New users may also want to look at SciViews R Graphical User Interface(GUI). It
> can be a good learning tool. Its text based editor is basic compared to WinEdt
> with the R editing plug-in, or ESS and (X)Emacs combination. But it has
> point-and-click menus that help in writing code, and easy view of objects, etc
> can be very helpful for new users. Using this GUI may require you to install
> some R packages listed on the SciViews page.
> 
> http://www.sciviews.org/SciViews-R/

A user may need to change the file target SciViews shortcuts points to. These
are added to Windows Start menu. This is needed for SciViews to work with the
intalled R version.

Use start menu to go to menu item SciViews-R. Go to "R Console". Right click. Go
to last item in the list called "properties". You will find something like the
following in the "Target field"

"C:\Program Files\R\R-2.2.0\bin\Rgui.exe" --sdi LANGUAGE=en RSciViews.RData

note the C:....\R-2.2.0\...

It assumes that you have R-2.2.0 installaed in:

C:\Program Files\R\R-2.2.0

If your R installation is R-2.2.1, and is installed in

C:\Program Files\R\R-2.2.1

then you need to change the "target" of SciViews-R "R console" shortcut to:

"C:\Program Files\R\R-2.2.1\bin\Rgui.exe" --sdi LANGUAGE=en RSciViews.RData

note the change to: C:....\R-2-2.1\...

note the "LANGUAGE=en" for English language GUI. You may be able to change this
to a language you want to use with R. See documentation for SciViews


From peter.gremse at numbers.net.au  Mon Sep 18 06:01:03 2006
From: peter.gremse at numbers.net.au (peter.gremse at numbers.net.au)
Date: Mon, 18 Sep 2006 14:01:03 +1000
Subject: [R] Cochrans Q Test
Message-ID: <20060918140103.s05s5vltv1xc88oc@webmail.numbers.net.au>

Hi!

I would like to conduct a Cochran`s Q Test in R, but have not found any 
suitable function.

My first idea was: J <- as.table(matrix(c(6,16,2,4),ncol=2, dimnames = 
list("C" = c("Favorable","Unfavorable"),"Drug A Favorable"=c("B 
Favorable","B Unfavorable"))))
L <- as.table(matrix(c(2,4,6,6),ncol=2, dimnames = list("C" = 
c("Favorable","Unfavorable"),"Drug A Unfavorable"=c("B Favorable","B 
Unfavorable"))))
mantelhaen.test(J,L, alternative="t")

But this is obviously the wrong function.

Then I googled and found (different data):

K <- as.table(matrix(c(1,1,0,0, 1,1,0,1, 1,1,1,1, 1,1,1,1, 1,0,0,0, 
1,1,1,1, 1,1,1,1, 0,0,0,0, 0,1,0,1, 1,1,1,1, 0,1,0,1, 0,1,0,1),ncol=12, 
dimnames = list("Seating type" = c("I","II","III","IV"),"Test 
subject"=c("A","B","C","D","E","F","G","H","I","J","K","L"))))
K
pcochran(K,4,12)

But R said that this function does not exist.
Can anyone help?


From gunther.hoening at ukmainz.de  Mon Sep 18 09:26:01 2006
From: gunther.hoening at ukmainz.de (=?iso-8859-1?Q?Gunther_H=F6ning?=)
Date: Mon, 18 Sep 2006 09:26:01 +0200
Subject: [R] Question on apply() with more information...
In-Reply-To: <450E5C27.15088.31C574@localhost>
Message-ID: <000001c6daf3$b1b33e20$0f1e0b0a@3med.klinik.unimainz.de>

Ok.
I tried this too, but it still doesn't work.
Here some more information to try out, but just an excerpt of Xarray

x <- c(0.11,0.25,0.45,0.65,0.80,0.95,1)
Y <- matrix(c(15,83,57,111,150,168,175,37,207,142,277,375,420,437),nrow=2)

sm <- function(y,x){smooth.spline(x,y)} 
SmoothList <- apply(Y,1,sm,x)
NewValues <- function(x,LIST){predict(LIST,x)} 
Xarray <-
matrix(c(0.15,0.56,0.66,0.45,0.19,0.17,0.99,0.56,0.77,0.41,0.11,0.63,0.42,0.
43),nrow=2)


apply(Xarray, 2, NewValues,SmoothList)
apply(Xarray, 2, NewValues,LIST=SmoothList)



-----Urspr?ngliche Nachricht-----
Von: Petr Pikal [mailto:petr.pikal at precheza.cz] 
Gesendet: Montag, 18. September 2006 08:43
An: Gunther H?ning; r-help at stat.math.ethz.ch
Betreff: Re: [R] Question on apply()

Hi

not much information about what can be wrong. As nobody knows your Xarray
and SmoothList it is hard to guess. You even omitted to show what "does not
work"
So here are few guesses.

predict usually expects comparable data
apply(Xarray, 2, NewValues,LIST=SmoothList)


HTH
Petr




On 18 Sep 2006 at 8:05, Gunther H?ning wrote:

From:           	Gunther H?ning <gunther.hoening at ukmainz.de>
To:             	<r-help at stat.math.ethz.ch>
Date sent:      	Mon, 18 Sep 2006 08:05:28 +0200
Subject:        	[R] Question on apply()

>  Dear list,
> 
> I try to do the following:
> I have an list of length n, with elements done by smooth.spline 
> (SmoothList).
> Now I have a matrix with n rows and m columns with x-values(Xarray) 
> Now I want ot predict the y-values. Therefor I want to take the first 
> element of SmoothList and the first row of Xarray and predict for each 
> element in Xarray the y value. And then take the second element of 
> SmoothList and second row of Xarray, third row of SmoothList and third 
> row of Xarray and so on....
> 
> I tried following:
> 
> NewValues <- function(x,LIST){predict(LIST,x)} apply(Xarray, 2, 
> NewValues,SmoothList)
> 
> But it don't work.
> 
> Could anybody help please ?
> 
> Gunther
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented, 
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From seanpor at acm.org  Mon Sep 18 09:37:51 2006
From: seanpor at acm.org (Sean O'Riordain)
Date: Mon, 18 Sep 2006 07:37:51 +0000
Subject: [R] uniform integer RNG 0 to t inclusive
Message-ID: <8ed68eed0609180037y175ea091p4927ab87f7f8ccfb@mail.gmail.com>

Good morning,

I'm trying to concisely generate a single integer from 0 to n
inclusive, where n might be of the order of hundreds of millions.
This will however be used many times during the general procedure, so
it must be "reasonably efficient" in both memory and time... (at some
later stage in the development I hope to go vectorized)

The examples I've found through searching RSiteSearch() relating to
generating random integers say to use : sample(0:n, 1)
However, when n is "large" this first generates a large sequence 0:n
before taking a sample of one... this computer doesn't have the memory
for that!

When I look at the documentation for runif(n, min, max) it states that
the generated numbers will be min <= x <= max.  Note the "<= max"...

How do I generate an x such that the probability of being (the
integer) max is the same as any other integer from min (an integer) to
max-1 (an integer) inclusive... My attempt is:

urand.int <- function(n,t) {
  as.integer(runif(n,min=0, max=t+1-.Machine$double.eps))
}
# where I've included the parameter n to help testing...

is floor() "better" than as.integer?

Is this correct?  Is the probability of the integer t the same as the
integer 1 or 0 etc... I have done some rudimentary testing and this
appears to work, but power being what it is, I can't see how to
realistically test this hypothesis.

Or is there a a better way of doing this?

I'm trying to implement an algorithm which samples into an array,
hence the need for an integer - and yes I know about sample() thanks!
:-)

{ incidentally, I was surprised to note that the maximum value
returned by summary(integer_vector) is "pretty" and appears to be
rounded up to a "nice round number", and is not necessarily the same
as max(integer_vector) where the value is large, i.e. of the order of
say 50 million }

Is version etc relevant? (I'll want to be portable)
> version               _
platform       i386-pc-mingw32
arch           i386
os             mingw32
system         i386, mingw32
status
major          2
minor          3.1
year           2006
month          06
day            01
svn rev        38247
language       R
version.string Version 2.3.1 (2006-06-01)
>

Many thanks in advance for your help.
Sean O'Riordain
affiliation <- NULL


From isidora10 at yahoo.com  Mon Sep 18 10:46:05 2006
From: isidora10 at yahoo.com (isidora k)
Date: Mon, 18 Sep 2006 01:46:05 -0700 (PDT)
Subject: [R] Excluding columns from dataframe and selecting row records
In-Reply-To: <20060917200829.74948.qmail@web53408.mail.yahoo.com>
Message-ID: <20060918084605.97588.qmail@web52106.mail.yahoo.com>

check out 'cbind' to select columns.
regards
Isidora


From uleopold at science.uva.nl  Mon Sep 18 11:35:26 2006
From: uleopold at science.uva.nl (Ulrich Leopold)
Date: Mon, 18 Sep 2006 11:35:26 +0200 (CEST)
Subject: [R] problems in sourcing R script
Message-ID: <29818.158.64.4.14.1158572126.squirrel@webmail.science.uva.nl>

Dear list,

First my information:
platform       i386-pc-linux-gnu
arch           i386
os             linux-gnu
system         i386, linux-gnu
status
major          2
minor          3.1
year           2006
month          06
day            01
svn rev        38247
language       R
version.string Version 2.3.1 (2006-06-01)

Now my question:

How is it possible that a command in an R script is not executed completely
whereas the same command is fully executed when I copy paste it to the R
terminal?

It is the last command "summary(....)" which shows such behaviour.


The result of the execution is:


         Df Sum of Sq     RSS     AIC
<none>                 18.666 -49.277
- Flower  1     2.113  20.779 -45.700


It does not compute the summary statistics any more.

Whereas this is the result when I copy paste the comamnd into the R terminal:


         Df Sum of Sq     RSS     AIC
<none>                 18.666 -49.277
- Flower  1     2.113  20.779 -45.700

Call:
lm(formula = NH4Mar04 ~ Flower, data = N)

Residuals:
    Min      1Q  Median      3Q     Max
-1.0750 -0.1968 -0.1518 -0.0968  2.9632



Any ideas what the problem could be?

Is it possible that there is some kind of character problem or a hidden
control character in the text file or an encoding problem (I use UTF)? I
checked already with cat -A whether it shows unwanted control characters but
nothing showed up.
I also used 2 different text editors to see whetehr it is related to a text
editor.


Thanks, Ulrich


R.script sourced with
> source("script.R")

------------------------------------------------------------

# Load MASS library for stepAIC, truehist, etc.
library(MASS)
# Read in the data as comma separated file
N <- read.csv2("MatrixNO3_08Aug06.csv", header=TRUE, sep=";", dec=".")

# Compute stepwise generalised linear regression
summary(stepAIC(lm(NH4Mar04~Elevation+Soil1+Soil2+Text1+Text2+Text3+Rice+Cabbage+Squash+Chilli+Flower,
N)))

--------------------------------------------------------

And here is the example for reproduction:

N <-
structure(list(samplePCR... = as.integer(c(1, 2, 3, 4, 5, 6,
7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22,
23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,
39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52)), Elevation = c(12.9,
12.9, 12.9, 12.9, 12.9, 12.9, 13.1, 13.1, 13.2, 13.3, 13.1, 13,
13.2, 13.3, 13.4, 13.6, 13.5, 13.5, 13.3, 13, 12.8, 13.8, 14.1,
13.2, 13.1, 12.5, 12, 11.8, 12.4, 13.8, 13.7, 12.8, 12.9, 12.8,
12.8, 12.7, 12.5, 12.4, 12.4, 12.5, 12.2, 12.3, 11.8, 13.7, 13.8,
13.8, 13.9, 13, 12.9, 13.9, 13.6, 13.8), Soil1 = as.integer(c(1,
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 1, 0, 0, 0, 0, 0, 0)), Soil2 = as.integer(c(0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
1, 1, 1, 1, 1, 1)), Text1 = as.integer(c(1, 1, 1, 1, 1, 1, 1,
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
1, 0, 0)), Text2 = as.integer(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1
)), Text3 = as.integer(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)), Rice =
as.integer(c(0 ,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
1, 1, 0, 1, 0, 0, 0, 0, 0)), Cabbage = as.integer(c(0, 0, 0,
0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
1, 0, 0, 0, 0, 0, 0)), Squash = as.integer(c(1, 1, 1, 1, 1, 1,
1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0)), Chilli = as.integer(c(0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0)), Flower = as.integer(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)), NO3Mar04 = c(16.04,
7.24, 19.65, 19.87, 20.64, 18.72, 17.24, 8.84, 26.2, 6.04, 4.35,
20.6, 14.44, 10.24, 19.48, 19.52, 10.8, 10.8, 2.92, 38.8, 5.38,
41.04, 37.18, 20.6, 18.12, 6.04, 22.85, 29.56, 28.35, 28.24,
20.35, 20.65, 22.85, 7.49, 10.8, 6.92, 31.8, 28.7, 24.2, 26.76,
18.16, 20.3, 35.16, 33.73, 21.73, 22.42, 27.88, 6.84, 10.52,
10.24, 20.04, 16.92), NO3Aug04 = c(13.84, 11.36, 4.64, 13.64,
12.64, 11.42, 19.87, 2.04, 11.76, 1.28, 13.06, 11.87, 2.68, 9.12,
2.68, 4.92, 14.85, 4.08, 10.75, 12.48, 16.68, 27.16, 22.28, 2.96,
12.88, 22.84, 15.35, 8.04, 8.66, 1.28, 23.84, 25.77, 28.6, 1.28,
23.18, 8.84, 20, 21.84, 9.98, 37.9, 17.8, 21.27, 10.32, 1.98,
0.83, 1.18, 2.68, 11.08, 14.44, 7.84, 13.88, 5.48), NH4Mar04 = c(0.76,
0.68, 0.79, 0.79, 0.79, 0.78, 0.77, 3.92, 1.12, 1.12, 0.66, 0.84,
0.84, 0.56, 1.12, 0.78, 0.56, 1.12, 0.65, 0.84, 0.67, 3.08, 0.93,
0.56, 0.77, 0.84, 0.81, 1.4, 0.86, 0.86, 0.79, 0.79, 0.81, 0.68,
0.84, 0.68, 1.4, 0.86, 0.82, 1.68, 0.77, 0.79, 0.84, 0.9, 0.8,
0.81, 0.84, 0.68, 0.56, 1.4, 3.08, 0.76), NH4Aug04 = c(0.78,
0.69, 2.24, 0.77, 1.12, 0.7, 0.98, 0.84, 0.71, 0.84, 0.75, 0.71,
1.12, 1.68, 0.84, 0.56, 0.81, 0.84, 0.67, 1.96, 0.87, 1.12, 1.4,
0.84, 0.75, 1.12, 0.83, 1.12, 0.6, 0.56, 1.11, 1.18, 1.96, 0.84,
1.09, 2.24, 1.4, 1.05, 0.65, 1.68, 0.84, 1.03, 1.12, 0.38, 0.34,
0.35, 1.4, 0.5, 1.68, 1.4, 1.96, 3.08), NO3Mar05 = c(5.5, 19.6,
6, 6.2, 5.6, 5.2, 5.1, 10.4, 12.2, 13.44, 9.7, 8.12, 5.88, 8.9,
7.8, 13.44, 12.6, 14.56, 14.4, 10.64, 11.5, 29.04, 22.08, 2.8,
5.5, 7, 6.1, 5.88, 12.6, 9.53, 4.9, 8.4, 5.9, 6.44, 12.32, 22.96,
7.28, 5.88, 6.12, 6.16, 1.96, 6.72, 6.46, 5.64, 5.38, 12.24,
5.04, 7.56, 2.96, 8.4, 1.68, 1.76), NO3Aug05 = c(6.74, 3.36,
2.8, 6.4, 8.2, 4.2, 8.2, 9.1, 9.46, 8.64, 8.57, 3.64, 5.87, 6.98,
7.48, 11.64, 4.48, 3.36, 7.86, 8.96, 7.68, 10.16, 12.8, 8.24,
5.25, 5.44, 6.24, 9.72, 8.67, 8.54, 12.4, 11.56, 11.04, 10.48,
9.64, 9.89, 8.08, 7.68, 6.84, 7.02, 6.89, 8.04, 5.76, 5.86, 6.26,
8.96, 13.07, 7.28, 11.28, 6.72, 7.46, 6.52), NH4Mar05 = c(0.84,
0.28, 0.84, 0.68, 0.84, 0.92, 0.96, 0.56, 0.62, 0.56, 0.48, 0.56,
0.84, 0.48, 0.62, 0.84, 0.95, 2.24, 0.58, 0.56, 0.52, 2.24, 0.28,
0.28, 0.72, 0.84, 0.72, 0.84, 0.96, 0.56, 0.68, 1.12, 0.7, 0.7,
0.56, 0.56, 0.56, 0.52, 0.54, 0.28, 0.84, 0.56, 0.56, 0.49, 0.5,
0.52, 0.56, 1.68, 0.22, 0.28, 0.28, 0.26), NH4Aug05 = c(0.64,
1.96, 0.56, 0.68, 0.64, 1.12, 0.76, 0.56, 0.6, 0.56, 0.62, 0.56,
0.64, 0.52, 0.62, 0.64, 0.56, 0.84, 0.78, 1.12, 0.68, 1.12, 1.24,
0.48, 0.58, 0.64, 0.63, 0.64, 0.76, 0.78, 1.68, 0.86, 0.74, 0.68,
0.56, 0.64, 0.62, 0.54, 0.52, 0.32, 0.64, 0.56, 0.58, 0.58, 0.48,
0.54, 0.72, 2.24, 0.84, 0.56, 0.38, 0.36)), .Names = c("samplePCR...",
"Elevation", "Soil1", "Soil2", "Text1", "Text2", "Text3", "Rice",
"Cabbage", "Squash", "Chilli", "Flower", "NO3Mar04", "NO3Aug04",
"NH4Mar04", "NH4Aug04", "NO3Mar05", "NO3Aug05", "NH4Mar05", "NH4Aug05"
), class = "data.frame", row.names = c("1", "2", "3", "4", "5",
"6", "7", "8", "9", "10", "11", "12", "13", "14", "15", "16",
"17", "18", "19", "20", "21", "22", "23", "24", "25", "26", "27",
"28", "29", "30", "31", "32", "33", "34", "35", "36", "37", "38",
"39", "40", "41", "42", "43", "44", "45", "46", "47", "48", "49",
"50", "51", "52"))



_______________________________________________

Ulrich Leopold MSc.

Dep. Phys. Geography and Soil Science
Inst. for Biodiversity and Ecosystem Dynamics
Faculty of Science
University of Amsterdam
Nieuwe Achtergracht 166
NL-1018 WV Amsterdam

Phone: +31-(0)20-525-7456 (7451 Sectretary)
Fax:   +31-(0)20-525-7431
Email: uleopold at science.uva.nl
http://www.science.uva.nl/ibed/cbpg/index.html


From petr.pikal at precheza.cz  Mon Sep 18 11:44:11 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 18 Sep 2006 11:44:11 +0200
Subject: [R] Question on apply() with more information...
In-Reply-To: <000001c6daf3$b1b33e20$0f1e0b0a@3med.klinik.unimainz.de>
References: <450E5C27.15088.31C574@localhost>
Message-ID: <450E868B.6767.D767EB@localhost>

Hi

If I am correct apply do not choose from SmoothList as you expected. 
Instead probably

lapply(SmoothList, predict,Xarray)
or
mapply(predict,SmoothList, Xarray)

can give you probably what you want.

HTH
Petr


On 18 Sep 2006 at 9:26, Gunther H?ning wrote:

From:           	Gunther H?ning <gunther.hoening at ukmainz.de>
To:             	"'Petr Pikal'" <petr.pikal at precheza.cz>,
	<r-help at stat.math.ethz.ch>
Subject:        	AW: [R] Question on apply() with more information...
Date sent:      	Mon, 18 Sep 2006 09:26:01 +0200

> Ok.
> I tried this too, but it still doesn't work.
> Here some more information to try out, but just an excerpt of Xarray
> 
> x <- c(0.11,0.25,0.45,0.65,0.80,0.95,1)
> Y <-
> matrix(c(15,83,57,111,150,168,175,37,207,142,277,375,420,437),nrow=2)
> 
> sm <- function(y,x){smooth.spline(x,y)} 
> SmoothList <- apply(Y,1,sm,x)
> NewValues <- function(x,LIST){predict(LIST,x)} 
> Xarray <-
> matrix(c(0.15,0.56,0.66,0.45,0.19,0.17,0.99,0.56,0.77,0.41,0.11,0.63,0
> .42,0. 43),nrow=2)
> 
> 
> apply(Xarray, 2, NewValues,SmoothList)
> apply(Xarray, 2, NewValues,LIST=SmoothList)
> 
> 
> 
> -----Urspr?ngliche Nachricht-----
> Von: Petr Pikal [mailto:petr.pikal at precheza.cz] 
> Gesendet: Montag, 18. September 2006 08:43
> An: Gunther H?ning; r-help at stat.math.ethz.ch
> Betreff: Re: [R] Question on apply()
> 
> Hi
> 
> not much information about what can be wrong. As nobody knows your
> Xarray and SmoothList it is hard to guess. You even omitted to show
> what "does not work" So here are few guesses.
> 
> predict usually expects comparable data
> apply(Xarray, 2, NewValues,LIST=SmoothList)
> 
> 
> HTH
> Petr
> 
> 
> 
> 
> On 18 Sep 2006 at 8:05, Gunther H?ning wrote:
> 
> From:           	Gunther H?ning <gunther.hoening at ukmainz.de>
> To:             	<r-help at stat.math.ethz.ch>
> Date sent:      	Mon, 18 Sep 2006 08:05:28 +0200
> Subject:        	[R] Question on apply()
> 
> >  Dear list,
> > 
> > I try to do the following:
> > I have an list of length n, with elements done by smooth.spline
> > (SmoothList). Now I have a matrix with n rows and m columns with
> > x-values(Xarray) Now I want ot predict the y-values. Therefor I want
> > to take the first element of SmoothList and the first row of Xarray
> > and predict for each element in Xarray the y value. And then take
> > the second element of SmoothList and second row of Xarray, third row
> > of SmoothList and third row of Xarray and so on....
> > 
> > I tried following:
> > 
> > NewValues <- function(x,LIST){predict(LIST,x)} apply(Xarray, 2,
> > NewValues,SmoothList)
> > 
> > But it don't work.
> > 
> > Could anybody help please ?
> > 
> > Gunther
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented,
> > minimal, self-contained, reproducible code.
> 
> Petr Pikal
> petr.pikal at precheza.cz
> 

Petr Pikal
petr.pikal at precheza.cz


From p.dalgaard at biostat.ku.dk  Mon Sep 18 11:48:05 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 Sep 2006 11:48:05 +0200
Subject: [R] problems in sourcing R script
In-Reply-To: <29818.158.64.4.14.1158572126.squirrel@webmail.science.uva.nl>
References: <29818.158.64.4.14.1158572126.squirrel@webmail.science.uva.nl>
Message-ID: <x21wq9jyju.fsf@viggo.kubism.ku.dk>

"Ulrich Leopold" <uleopold at science.uva.nl> writes:

> Now my question:
> 
> How is it possible that a command in an R script is not executed completely
> whereas the same command is fully executed when I copy paste it to the R
> terminal?

It is fully executed, but you're not printing the results. Read
help(source) and use echo=TRUE.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From ripley at stats.ox.ac.uk  Mon Sep 18 11:51:40 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 18 Sep 2006 10:51:40 +0100 (BST)
Subject: [R] problems in sourcing R script
In-Reply-To: <29818.158.64.4.14.1158572126.squirrel@webmail.science.uva.nl>
References: <29818.158.64.4.14.1158572126.squirrel@webmail.science.uva.nl>
Message-ID: <Pine.LNX.4.64.0609181051050.10660@gannet.stats.ox.ac.uk>

You need to print() explicitly inside sourced-ed script: auto-printing is 
not active.

On Mon, 18 Sep 2006, Ulrich Leopold wrote:

> Dear list,
>
> First my information:
> platform       i386-pc-linux-gnu
> arch           i386
> os             linux-gnu
> system         i386, linux-gnu
> status
> major          2
> minor          3.1
> year           2006
> month          06
> day            01
> svn rev        38247
> language       R
> version.string Version 2.3.1 (2006-06-01)
>
> Now my question:
>
> How is it possible that a command in an R script is not executed completely
> whereas the same command is fully executed when I copy paste it to the R
> terminal?
>
> It is the last command "summary(....)" which shows such behaviour.
>
>
> The result of the execution is:
>
>
>         Df Sum of Sq     RSS     AIC
> <none>                 18.666 -49.277
> - Flower  1     2.113  20.779 -45.700
>
>
> It does not compute the summary statistics any more.
>
> Whereas this is the result when I copy paste the comamnd into the R terminal:
>
>
>         Df Sum of Sq     RSS     AIC
> <none>                 18.666 -49.277
> - Flower  1     2.113  20.779 -45.700
>
> Call:
> lm(formula = NH4Mar04 ~ Flower, data = N)
>
> Residuals:
>    Min      1Q  Median      3Q     Max
> -1.0750 -0.1968 -0.1518 -0.0968  2.9632
>
>
>
> Any ideas what the problem could be?
>
> Is it possible that there is some kind of character problem or a hidden
> control character in the text file or an encoding problem (I use UTF)? I
> checked already with cat -A whether it shows unwanted control characters but
> nothing showed up.
> I also used 2 different text editors to see whetehr it is related to a text
> editor.
>
>
> Thanks, Ulrich
>
>
> R.script sourced with
>> source("script.R")
>
> ------------------------------------------------------------
>
> # Load MASS library for stepAIC, truehist, etc.
> library(MASS)
> # Read in the data as comma separated file
> N <- read.csv2("MatrixNO3_08Aug06.csv", header=TRUE, sep=";", dec=".")
>
> # Compute stepwise generalised linear regression
> summary(stepAIC(lm(NH4Mar04~Elevation+Soil1+Soil2+Text1+Text2+Text3+Rice+Cabbage+Squash+Chilli+Flower,
> N)))
>
> --------------------------------------------------------
>
> And here is the example for reproduction:
>
> N <-
> structure(list(samplePCR... = as.integer(c(1, 2, 3, 4, 5, 6,
> 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22,
> 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,
> 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52)), Elevation = c(12.9,
> 12.9, 12.9, 12.9, 12.9, 12.9, 13.1, 13.1, 13.2, 13.3, 13.1, 13,
> 13.2, 13.3, 13.4, 13.6, 13.5, 13.5, 13.3, 13, 12.8, 13.8, 14.1,
> 13.2, 13.1, 12.5, 12, 11.8, 12.4, 13.8, 13.7, 12.8, 12.9, 12.8,
> 12.8, 12.7, 12.5, 12.4, 12.4, 12.5, 12.2, 12.3, 11.8, 13.7, 13.8,
> 13.8, 13.9, 13, 12.9, 13.9, 13.6, 13.8), Soil1 = as.integer(c(1,
> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
> 0, 0, 1, 0, 0, 0, 0, 0, 0)), Soil2 = as.integer(c(0, 0, 0, 0,
> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
> 1, 1, 1, 1, 1, 1)), Text1 = as.integer(c(1, 1, 1, 1, 1, 1, 1,
> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
> 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
> 1, 0, 0)), Text2 = as.integer(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
> 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1
> )), Text3 = as.integer(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)), Rice =
> as.integer(c(0 ,
> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
> 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
> 1, 1, 0, 1, 0, 0, 0, 0, 0)), Cabbage = as.integer(c(0, 0, 0,
> 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
> 1, 0, 0, 0, 0, 0, 0)), Squash = as.integer(c(1, 1, 1, 1, 1, 1,
> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
> 0, 0, 0, 0)), Chilli = as.integer(c(0, 0, 0, 0, 0, 0, 0, 0, 0,
> 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
> 0)), Flower = as.integer(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
> 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)), NO3Mar04 = c(16.04,
> 7.24, 19.65, 19.87, 20.64, 18.72, 17.24, 8.84, 26.2, 6.04, 4.35,
> 20.6, 14.44, 10.24, 19.48, 19.52, 10.8, 10.8, 2.92, 38.8, 5.38,
> 41.04, 37.18, 20.6, 18.12, 6.04, 22.85, 29.56, 28.35, 28.24,
> 20.35, 20.65, 22.85, 7.49, 10.8, 6.92, 31.8, 28.7, 24.2, 26.76,
> 18.16, 20.3, 35.16, 33.73, 21.73, 22.42, 27.88, 6.84, 10.52,
> 10.24, 20.04, 16.92), NO3Aug04 = c(13.84, 11.36, 4.64, 13.64,
> 12.64, 11.42, 19.87, 2.04, 11.76, 1.28, 13.06, 11.87, 2.68, 9.12,
> 2.68, 4.92, 14.85, 4.08, 10.75, 12.48, 16.68, 27.16, 22.28, 2.96,
> 12.88, 22.84, 15.35, 8.04, 8.66, 1.28, 23.84, 25.77, 28.6, 1.28,
> 23.18, 8.84, 20, 21.84, 9.98, 37.9, 17.8, 21.27, 10.32, 1.98,
> 0.83, 1.18, 2.68, 11.08, 14.44, 7.84, 13.88, 5.48), NH4Mar04 = c(0.76,
> 0.68, 0.79, 0.79, 0.79, 0.78, 0.77, 3.92, 1.12, 1.12, 0.66, 0.84,
> 0.84, 0.56, 1.12, 0.78, 0.56, 1.12, 0.65, 0.84, 0.67, 3.08, 0.93,
> 0.56, 0.77, 0.84, 0.81, 1.4, 0.86, 0.86, 0.79, 0.79, 0.81, 0.68,
> 0.84, 0.68, 1.4, 0.86, 0.82, 1.68, 0.77, 0.79, 0.84, 0.9, 0.8,
> 0.81, 0.84, 0.68, 0.56, 1.4, 3.08, 0.76), NH4Aug04 = c(0.78,
> 0.69, 2.24, 0.77, 1.12, 0.7, 0.98, 0.84, 0.71, 0.84, 0.75, 0.71,
> 1.12, 1.68, 0.84, 0.56, 0.81, 0.84, 0.67, 1.96, 0.87, 1.12, 1.4,
> 0.84, 0.75, 1.12, 0.83, 1.12, 0.6, 0.56, 1.11, 1.18, 1.96, 0.84,
> 1.09, 2.24, 1.4, 1.05, 0.65, 1.68, 0.84, 1.03, 1.12, 0.38, 0.34,
> 0.35, 1.4, 0.5, 1.68, 1.4, 1.96, 3.08), NO3Mar05 = c(5.5, 19.6,
> 6, 6.2, 5.6, 5.2, 5.1, 10.4, 12.2, 13.44, 9.7, 8.12, 5.88, 8.9,
> 7.8, 13.44, 12.6, 14.56, 14.4, 10.64, 11.5, 29.04, 22.08, 2.8,
> 5.5, 7, 6.1, 5.88, 12.6, 9.53, 4.9, 8.4, 5.9, 6.44, 12.32, 22.96,
> 7.28, 5.88, 6.12, 6.16, 1.96, 6.72, 6.46, 5.64, 5.38, 12.24,
> 5.04, 7.56, 2.96, 8.4, 1.68, 1.76), NO3Aug05 = c(6.74, 3.36,
> 2.8, 6.4, 8.2, 4.2, 8.2, 9.1, 9.46, 8.64, 8.57, 3.64, 5.87, 6.98,
> 7.48, 11.64, 4.48, 3.36, 7.86, 8.96, 7.68, 10.16, 12.8, 8.24,
> 5.25, 5.44, 6.24, 9.72, 8.67, 8.54, 12.4, 11.56, 11.04, 10.48,
> 9.64, 9.89, 8.08, 7.68, 6.84, 7.02, 6.89, 8.04, 5.76, 5.86, 6.26,
> 8.96, 13.07, 7.28, 11.28, 6.72, 7.46, 6.52), NH4Mar05 = c(0.84,
> 0.28, 0.84, 0.68, 0.84, 0.92, 0.96, 0.56, 0.62, 0.56, 0.48, 0.56,
> 0.84, 0.48, 0.62, 0.84, 0.95, 2.24, 0.58, 0.56, 0.52, 2.24, 0.28,
> 0.28, 0.72, 0.84, 0.72, 0.84, 0.96, 0.56, 0.68, 1.12, 0.7, 0.7,
> 0.56, 0.56, 0.56, 0.52, 0.54, 0.28, 0.84, 0.56, 0.56, 0.49, 0.5,
> 0.52, 0.56, 1.68, 0.22, 0.28, 0.28, 0.26), NH4Aug05 = c(0.64,
> 1.96, 0.56, 0.68, 0.64, 1.12, 0.76, 0.56, 0.6, 0.56, 0.62, 0.56,
> 0.64, 0.52, 0.62, 0.64, 0.56, 0.84, 0.78, 1.12, 0.68, 1.12, 1.24,
> 0.48, 0.58, 0.64, 0.63, 0.64, 0.76, 0.78, 1.68, 0.86, 0.74, 0.68,
> 0.56, 0.64, 0.62, 0.54, 0.52, 0.32, 0.64, 0.56, 0.58, 0.58, 0.48,
> 0.54, 0.72, 2.24, 0.84, 0.56, 0.38, 0.36)), .Names = c("samplePCR...",
> "Elevation", "Soil1", "Soil2", "Text1", "Text2", "Text3", "Rice",
> "Cabbage", "Squash", "Chilli", "Flower", "NO3Mar04", "NO3Aug04",
> "NH4Mar04", "NH4Aug04", "NO3Mar05", "NO3Aug05", "NH4Mar05", "NH4Aug05"
> ), class = "data.frame", row.names = c("1", "2", "3", "4", "5",
> "6", "7", "8", "9", "10", "11", "12", "13", "14", "15", "16",
> "17", "18", "19", "20", "21", "22", "23", "24", "25", "26", "27",
> "28", "29", "30", "31", "32", "33", "34", "35", "36", "37", "38",
> "39", "40", "41", "42", "43", "44", "45", "46", "47", "48", "49",
> "50", "51", "52"))
>
>
>
> _______________________________________________
>
> Ulrich Leopold MSc.
>
> Dep. Phys. Geography and Soil Science
> Inst. for Biodiversity and Ecosystem Dynamics
> Faculty of Science
> University of Amsterdam
> Nieuwe Achtergracht 166
> NL-1018 WV Amsterdam
>
> Phone: +31-(0)20-525-7456 (7451 Sectretary)
> Fax:   +31-(0)20-525-7431
> Email: uleopold at science.uva.nl
> http://www.science.uva.nl/ibed/cbpg/index.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From chrysopa at gmail.com  Mon Sep 18 12:44:48 2006
From: chrysopa at gmail.com (Ronaldo Reis-Jr.)
Date: Mon, 18 Sep 2006 07:44:48 -0300
Subject: [R] problem in font
Message-ID: <200609180744.48832.chrysopa@gmail.com>

Hi,

after a debian upgrade my dev.copy2eps dont work anymore. I have this message:

Error in matchFont(postscriptFonts(family)[[1]], old$encoding) : 
	unknown font

I try to change the font family in my .Rprofile:

setHook(packageEvent("graphics", "onLoad"),
        function(...) {
          grDevices::ps.options(family="ComputerModern")
        } )

But it no fix the error.

How to fix it?

Thanks
ROnaldo
-- 
I've been on this lonely road so long,
Does anybody know where it goes,
I remember last time the signs pointed home,
A month ago.
		-- Carpenters, "Road Ode"
--
> Prof. Ronaldo Reis J?nior
|  .''`. UNIMONTES/Depto. Biologia Geral/Lab. Ecologia Evolutiva
| : :'  : Campus Universit?rio Prof. Darcy Ribeiro, Vila Mauric?ia
| `. `'` CP: 126, CEP: 39401-089, Montes Claros - MG - Brasil
|   `- Fone: (38) 3229-8190 | chrysopa em gmail.com
| ICQ#: 5692561 | LinuxUser#: 205366


From chabotd at globetrotter.net  Mon Sep 18 13:02:53 2006
From: chabotd at globetrotter.net (Denis Chabot)
Date: Mon, 18 Sep 2006 07:02:53 -0400
Subject: [R] merge gives me too many rows
In-Reply-To: <Pine.LNX.4.64.0609180150520.29606@springer.berkeley.edu>
References: <1506660C-D5E4-498C-B2D2-E0AB702FF6F1@globetrotter.net>
	<Pine.LNX.4.64.0609180150520.29606@springer.berkeley.edu>
Message-ID: <BADB9751-15A7-445B-A68B-5EA6C1D16BB7@globetrotter.net>

Hi Phil and Don,
Le 06-09-18 ? 05:02, Phil Spector a ?crit :

> Denis -
>    As long as there is one data frame that has exactly 0 or 1  
> observations
> for each level of the by variables, merge in R will operate  
> identically
> to SAS without the need for any special options.
>    The problem arises when there are multiple observations for some of
> the levels of the by variables in both of the data frames.  In
> relational databases and R, new observations are created to  
> represent every combination of observations in the original data  
> set that share
> the same level of the by variables.  In sas, the last observation  
> in the data set with fewer observations having a particular level  
> of the by variable is repeated enough times to provide a match for  
> each
> observation in the other data set.
>     You can see this with a tiny example -- look at the observations
> in the output data set corresponding to id==3:
>
>> a = data.frame(id=c(1,2,3,3,4),x=rnorm(5))
>> b = data.frame(id=c(1,2,3,3,3,4),y=rnorm(6))
>> c = merge(a,b)
>> dim(a)
> [1] 5 2
>> dim(b)
> [1] 6 2
>> dim(c)
> [1] 9 3
>> c
>   id           x            y
> 1  1 -2.15987259 -1.555878070
> 2  2  0.96856842 -0.666941824
> 3  3  1.68080409 -0.009239307
> 4  3  0.06044914 -0.009239307
> 5  3  1.68080409 -0.242538473
> 6  3  0.06044914 -0.242538473
> 7  3  1.68080409 -1.298344557
> 8  3  0.06044914 -1.298344557
> 9  4  1.76424928 -0.420144744
>
> The important thing to remember is that there is no unambiguous way  
> to merge two data sets when there are differing numbers of  
> observations for a given level of
> the by variable in the two data sets -- sas and R simply choose  
> different strategies
> in this case.
>
>                                        - Phil Spector
> 					 Statistical Computing Facility
> 					 Department of Statistics
> 					 UC Berkeley
> 					 spector at stat.berkeley.edu
>
>

I knew this, but it is good to be remembered and be forced to verify  
one's assumptions.

Originally both informations about stomach contents and fish size  
came from the same, larger, database.
I made the 2 smaller databases to ease the memory footprint in R when  
not all variables were needed. The method I used, actually suggested  
to me on this very helpful list a long time back, insured that all  
rows were unique:

fish <- subset(esto, select=c(predateu, origin, navire, nbpc, no_rel,  
trait, tagno, longueur, masse))
fish <- unique(fish)       # NO NEED TO SORT FIRST

Or at least it would if there were NO MISTAKES in my database to  
start with. It is this false feeling that all rows had to be unique  
that made me pull my hair last night and assume the problem was with  
merge.

I just verified and found that some fish have 2 lengths or masses,  
either mistakes when entering them into the databases, or mistakes  
when numbering fishes (2 fish with the same number). I'll go back to  
the raw data to fix this! So thanks for making me verify this  
assumption.

Le 06-09-18 ? 01:38, Don MacQueen a ?crit :
> I think you may misunderstand the meaning of all.x = FALSE.
>
> Setting all.x to false ensures that only rows of x that have  
> matches in y will be included. Equivalently, if a row of x is not  
> matched in y, it will not be in the output. However, if a row in x  
> is matched by more than one row in y, then that row will be  
> repeated as many times as there are matching rows in y. That is,  
> you have a 1 to many match (1 in x to many in y). SAS behaves the  
> same way.
>
> Are you sure this is not what is happening?
>
> Also, all.x = FALSE is the default; it is not necessary to specify  
> it. In fact, the default is to output only rows that are found in  
> both x and y (matching on the specified variables, of course).
>
> -Don

Thank you Don,

Looking at my old programs using merge, I always used "all.x=T".  
Yesterday, because I was getting more rows than expected, I assumed  
that switch caused it and started setting it to F (i.e. early on when  
I got extra lines, I assumed that fish measured in the second  
dataframe but for which I did not have stomach contents were merged  
anyway). I never went back to all.X=T after verifying it was not the  
cause of the problem, instead some lines in the first database were  
duplicated (multiple matches, it turns out). But you are right. In  
fact I made some tests and the behavior I want in this and most cases  
is "all.x=T" but "all.y=F". That is if I ever found a case where I  
had a line in the first dataframe with no match in the second, I'd  
want to keep that line in the final dataframe.

Again, many thanks,

Denis

> At 9:11 PM -0400 9/17/06, Denis Chabot wrote:
>
>> Hi,
>>
>> I am using merge to add some variables to an existing dataframe. I  
>> use the option "all.x=F" so that my final dataframe will only have  
>> as many rows as the first file I name in the call to merge.
>>
>> With a large dataframe using a lot of "by" variables, the number  
>> of rows of the merged dataframe increases from 177325 to 179690:
>>
>>  >dim(test)
>> [1] 177325      9
>>  > test2 <- merge(test, fish, by=c("predateu", "origin", "navire",  
>> "nbpc", "no_rel", "trait", "tagno"), all.x=F)
>>  > dim(test2)
>> [1] 179690     11
>>
>> I tried to make a smaller dataset with R commands that I could  
>> post here so that other people could reproduce, but merge behaved  
>> as expected: final number of rows was the same as the number of  
>> rows in the first file named in the call to merge.
>>
>> I took a subset of my large dataframe and could mail this to  
>> anyone interested in verifying the problem.
>>
>>  > test3 <- test[100001:160000,]
>>  >
>>  > dim(test3)
>> [1] 60000     9
>>  > test4 <- merge(test3, fish, by=c("predateu", "origin",  
>> "navire", "nbpc", "no_rel", "trait", "tagno"), all.x=F)
>>  >
>>  > dim(test4)
>> [1] 60043    11
>>
>> I compared test3 and test4 line by line. The first 11419 lines  
>> were the same (except for added variables, obviously) in both  
>> dataframes, but then lines 11420 to 11423 were repeated in test4.  
>> Then no problem for a lot of rows, until rows 45756-45760 in  
>> test3. These are offset by 4 in test4 because of the first group  
>> of extraneous lines just reported, and are found on lines 45760 to  
>> 45765. But they are also repeated on lines 45765 to 45769. And so  
>> on a few more times.
>>
>> Thus merge added lines (repeated a small number of lines) to the  
>> final dataframe despite my use of all.x=F.
>>
>> Am I doing something wrong? If not, is there a solution? Not being  
>> able to merge is a setback! I was attempting to move the last few  
>> things I was doing with SAS to R...
>>
>> Please let me know if you want the file test3 (2.3 MB as a csv  
>> file, but only 352 KB in R (.rda) format).
>>
>> Sincerely,
>>
>> Denis Chabot
>>
>>  > R.Version()
>> $platform
>> [1] "powerpc-apple-darwin8.6.0"
>>
>> $arch
>> [1] "powerpc"
>>
>> $os
>> [1] "darwin8.6.0"
>>
>> $system
>> [1] "powerpc, darwin8.6.0"
>>
>> $status
>> [1] ""
>>
>> $major
>> [1] "2"
>>
>> $minor
>> [1] "3.1"
>>
>> $year
>> [1] "2006"
>>
>> $month
>> [1] "06"
>>
>> $day
>> [1] "01"
>>
>> $`svn rev`
>> [1] "38247"
>>
>> $language
>> [1] "R"
>>
>> $version.string
>> [1] "Version 2.3.1 (2006-06-01)"


From pngom at ucad.sn  Mon Sep 18 09:13:12 2006
From: pngom at ucad.sn (pngom at ucad.sn)
Date: Mon, 18 Sep 2006 07:13:12 +0000
Subject: [R] Simulation of a Dirichlet Process.
Message-ID: <20060918071312.wrws4sdynls08wo8@mail.ucad.sn>

I'm just getting started with R, having a lot of original work on
modeling and exploring the simulation by MCMC. I want to simulate the 
prior and the posterior distribution of Dirichlet Process by MCMC.
Is there anyone in NYC that might be a good
tutor for me?

-- 
Dr. P. NGOM,
Facult? des Sciences et Techniques
D?partement de Math?matiques et Informatique
Universit? Cheikh Anta Diop Dakar - S?n?gal

----------------------------------------------------------------
Universite Cheikh Anta DIOP - DAKAR


From chrysopa at gmail.com  Mon Sep 18 13:28:15 2006
From: chrysopa at gmail.com (Ronaldo Reis-Jr.)
Date: Mon, 18 Sep 2006 08:28:15 -0300
Subject: [R] problem in font
Message-ID: <200609180828.15686.chrysopa@gmail.com>

Hi,

This is a false problem.

This problem is only in a .RData file, not a global R problem.

Thanks
ROnaldo
-- 
I've been on this lonely road so long,
Does anybody know where it goes,
I remember last time the signs pointed home,
A month ago.
		-- Carpenters, "Road Ode"
--
> Prof. Ronaldo Reis J?nior
|  .''`. UNIMONTES/Depto. Biologia Geral/Lab. Ecologia Evolutiva
| : :'  : Campus Universit?rio Prof. Darcy Ribeiro, Vila Mauric?ia
| `. `'` CP: 126, CEP: 39401-089, Montes Claros - MG - Brasil
|   `- Fone: (38) 3229-8190 | chrysopa em gmail.com
| ICQ#: 5692561 | LinuxUser#: 205366


From gunther.hoening at ukmainz.de  Mon Sep 18 13:26:25 2006
From: gunther.hoening at ukmainz.de (=?iso-8859-1?Q?Gunther_H=F6ning?=)
Date: Mon, 18 Sep 2006 13:26:25 +0200
Subject: [R] Question on apply() with more information...
In-Reply-To: <450E868B.6767.D767EB@localhost>
Message-ID: <000e01c6db15$46bdae30$0f1e0b0a@3med.klinik.unimainz.de>

Hi,

I tried both ideas, but it isn't that what I'm looking for.
I want to avoid for loop, because the matrix is of big size(1200*1200
entries)

With a loop I would do:

for ( i in seq(along = SmoothList))
{
	Xarry[i,] <- predict(SmoothList[[i]],Xarry[i,])$y
} 

Actually I want to do more than just to predict a value, but it isn't
important for the initial question...

Gunther

-----Urspr?ngliche Nachricht-----
Von: Petr Pikal [mailto:petr.pikal at precheza.cz] 
Gesendet: Montag, 18. September 2006 11:44
An: Gunther H?ning
Cc: r-help at stat.math.ethz.ch
Betreff: Re: AW: [R] Question on apply() with more information...

Hi

If I am correct apply do not choose from SmoothList as you expected. 
Instead probably

lapply(SmoothList, predict,Xarray)
or
mapply(predict,SmoothList, Xarray)

can give you probably what you want.

HTH
Petr


On 18 Sep 2006 at 9:26, Gunther H?ning wrote:

From:           	Gunther H?ning <gunther.hoening at ukmainz.de>
To:             	"'Petr Pikal'" <petr.pikal at precheza.cz>,
	<r-help at stat.math.ethz.ch>
Subject:        	AW: [R] Question on apply() with more information...
Date sent:      	Mon, 18 Sep 2006 09:26:01 +0200

> Ok.
> I tried this too, but it still doesn't work.
> Here some more information to try out, but just an excerpt of Xarray
> 
> x <- c(0.11,0.25,0.45,0.65,0.80,0.95,1)
> Y <-
> matrix(c(15,83,57,111,150,168,175,37,207,142,277,375,420,437),nrow=2)
> 
> sm <- function(y,x){smooth.spline(x,y)} SmoothList <- apply(Y,1,sm,x) 
> NewValues <- function(x,LIST){predict(LIST,x)} Xarray <- 
> matrix(c(0.15,0.56,0.66,0.45,0.19,0.17,0.99,0.56,0.77,0.41,0.11,0.63,0
> .42,0. 43),nrow=2)
> 
> 
> apply(Xarray, 2, NewValues,SmoothList) apply(Xarray, 2, 
> NewValues,LIST=SmoothList)
> 
> 
> 
> -----Urspr?ngliche Nachricht-----
> Von: Petr Pikal [mailto:petr.pikal at precheza.cz]
> Gesendet: Montag, 18. September 2006 08:43
> An: Gunther H?ning; r-help at stat.math.ethz.ch
> Betreff: Re: [R] Question on apply()
> 
> Hi
> 
> not much information about what can be wrong. As nobody knows your 
> Xarray and SmoothList it is hard to guess. You even omitted to show 
> what "does not work" So here are few guesses.
> 
> predict usually expects comparable data apply(Xarray, 2, 
> NewValues,LIST=SmoothList)
> 
> 
> HTH
> Petr
> 
> 
> 
> 
> On 18 Sep 2006 at 8:05, Gunther H?ning wrote:
> 
> From:           	Gunther H?ning <gunther.hoening at ukmainz.de>
> To:             	<r-help at stat.math.ethz.ch>
> Date sent:      	Mon, 18 Sep 2006 08:05:28 +0200
> Subject:        	[R] Question on apply()
> 
> >  Dear list,
> > 
> > I try to do the following:
> > I have an list of length n, with elements done by smooth.spline 
> > (SmoothList). Now I have a matrix with n rows and m columns with
> > x-values(Xarray) Now I want ot predict the y-values. Therefor I want 
> > to take the first element of SmoothList and the first row of Xarray 
> > and predict for each element in Xarray the y value. And then take 
> > the second element of SmoothList and second row of Xarray, third row 
> > of SmoothList and third row of Xarray and so on....
> > 
> > I tried following:
> > 
> > NewValues <- function(x,LIST){predict(LIST,x)} apply(Xarray, 2,
> > NewValues,SmoothList)
> > 
> > But it don't work.
> > 
> > Could anybody help please ?
> > 
> > Gunther
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented, 
> > minimal, self-contained, reproducible code.
> 
> Petr Pikal
> petr.pikal at precheza.cz
> 

Petr Pikal
petr.pikal at precheza.cz


From chrysopa at gmail.com  Mon Sep 18 13:32:40 2006
From: chrysopa at gmail.com (Ronaldo Reis-Jr.)
Date: Mon, 18 Sep 2006 08:32:40 -0300
Subject: [R] problem in font
Message-ID: <200609180832.40581.chrysopa@gmail.com>

Hi,

But, I have this warning in the enviromnent that have this problem:

During startup - Warning message:
using .GlobalEnv instead of 'package:lattice'

maybe this is the problem?

How to fix it?

Thanks
ROnaldo
-- 
I've been on this lonely road so long,
Does anybody know where it goes,
I remember last time the signs pointed home,
A month ago.
		-- Carpenters, "Road Ode"
--
> Prof. Ronaldo Reis J?nior
|  .''`. UNIMONTES/Depto. Biologia Geral/Lab. Ecologia Evolutiva
| : :'  : Campus Universit?rio Prof. Darcy Ribeiro, Vila Mauric?ia
| `. `'` CP: 126, CEP: 39401-089, Montes Claros - MG - Brasil
|   `- Fone: (38) 3229-8190 | chrysopa em gmail.com
| ICQ#: 5692561 | LinuxUser#: 205366


From uleopold at science.uva.nl  Mon Sep 18 14:03:49 2006
From: uleopold at science.uva.nl (Ulrich Leopold)
Date: Mon, 18 Sep 2006 14:03:49 +0200 (CEST)
Subject: [R] write full information of class summary.lm to file
Message-ID: <47239.158.64.4.14.1158581029.squirrel@webmail.science.uva.nl>

Dear list,

how can I write the full information of class summary.lm to a text, latex or
html file? xtable only extracts the table but not the rest of the information.

Best regards, Ulrich


From sachinj.2006 at yahoo.com  Mon Sep 18 14:48:20 2006
From: sachinj.2006 at yahoo.com (Sachin J)
Date: Mon, 18 Sep 2006 05:48:20 -0700 (PDT)
Subject: [R] prediction interval for new value
In-Reply-To: <815b70590609180139j3ffa4839kf9f6071363f01f3b@mail.gmail.com>
Message-ID: <20060918124820.28094.qmail@web37612.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060918/b556b95d/attachment.pl 

From ligges at statistik.uni-dortmund.de  Mon Sep 18 15:02:16 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 18 Sep 2006 15:02:16 +0200
Subject: [R] problem in font
In-Reply-To: <200609180832.40581.chrysopa@gmail.com>
References: <200609180832.40581.chrysopa@gmail.com>
Message-ID: <450E98D8.5070400@statistik.uni-dortmund.de>

Ronaldo Reis-Jr. wrote:
> Hi,
> 
> But, I have this warning in the enviromnent that have this problem:
> 
> During startup - Warning message:
> using .GlobalEnv instead of 'package:lattice'


So, can you please be a bit more specific?
R version, your startup files such as Rprofile files etc?

Uwe Ligges


> maybe this is the problem?
> 
> How to fix it?
> 
> Thanks
> ROnaldo


From MSchwartz at mn.rr.com  Mon Sep 18 15:04:05 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Mon, 18 Sep 2006 08:04:05 -0500
Subject: [R] Cochrans Q Test
In-Reply-To: <20060918140103.s05s5vltv1xc88oc@webmail.numbers.net.au>
References: <20060918140103.s05s5vltv1xc88oc@webmail.numbers.net.au>
Message-ID: <1158584645.4382.36.camel@localhost.localdomain>

On Mon, 2006-09-18 at 14:01 +1000, peter.gremse at numbers.net.au wrote:
> Hi!
> 
> I would like to conduct a Cochran`s Q Test in R, but have not found any 
> suitable function.
> 
> My first idea was: J <- as.table(matrix(c(6,16,2,4),ncol=2, dimnames = 
> list("C" = c("Favorable","Unfavorable"),"Drug A Favorable"=c("B 
> Favorable","B Unfavorable"))))
> L <- as.table(matrix(c(2,4,6,6),ncol=2, dimnames = list("C" = 
> c("Favorable","Unfavorable"),"Drug A Unfavorable"=c("B Favorable","B 
> Unfavorable"))))
> mantelhaen.test(J,L, alternative="t")
> 
> But this is obviously the wrong function.

The CMH test does not consider the dependent nature of the measurements,
so it is indeed the wrong test, if you have dependent measures as your
data 'K' below would suggest.

Cochran's Q is a generalization of the McNemar paired chi square.

> Then I googled and found (different data):
> 
> K <- as.table(matrix(c(1,1,0,0, 1,1,0,1, 1,1,1,1, 1,1,1,1, 1,0,0,0, 
> 1,1,1,1, 1,1,1,1, 0,0,0,0, 0,1,0,1, 1,1,1,1, 0,1,0,1, 0,1,0,1),ncol=12, 
> dimnames = list("Seating type" = c("I","II","III","IV"),"Test 
> subject"=c("A","B","C","D","E","F","G","H","I","J","K","L"))))
> K
> pcochran(K,4,12)
> 
> But R said that this function does not exist.
> Can anyone help?

Here is a version of the Cochran's Q that I wrote and posted to
sci.stat.consult back in June in response to a thread there. This is
based upon Sheskin's Handbook of Parametric and Nonparametric
Statistical Procedures (2004) page 867. You might want to secure a copy
of the book and review the comments regarding the assumptions underlying
this test and the considerations for it use.

cochranq.test <- function(mat)
{
  k <- ncol(mat)

  C <- sum(colSums(mat) ^ 2)
  R <- sum(rowSums(mat) ^ 2)
  T <- sum(rowSums(mat))

  num <- (k - 1) * ((k * C) - (T ^ 2))
  den <- (k * T) - R

  Q <- num / den

  df <- k - 1
  names(df) <- "df"
  names(Q) <- "Cochran's Q"

  p.val <- pchisq(Q, df, lower = FALSE)

  QVAL <- list(statistic = Q, parameter = df, p.value = p.val,
               method = "Cochran's Q Test for Dependent Samples",
               data.name = deparse(substitute(mat)))
  class(QVAL) <- "htest"
  return(QVAL)
}


Using your matrix 'K':

> K
            Test
subject
Seating type A B C D E F G H I J K L
         I   1 1 1 1 1 1 1 0 0 1 0 0
         II  1 1 1 1 0 1 1 0 1 1 1 1
         III 0 0 1 1 0 1 1 0 0 1 0 0
         IV  0 1 1 1 0 1 1 0 1 1 1 1


> cochranq.test(K)

        Cochran's Q Test for Dependent Samples

data:  K
Cochran's Q = 23.9298, df = 11, p-value = 0.01303


BTW, a quick Google search shows that the pcochran() function is in the
'outliers' package on CRAN, which also has a cochran.test() function

HTH,

Marc Schwartz


From kyegon at hotmail.com  Mon Sep 18 15:22:28 2006
From: kyegon at hotmail.com (ERICK YEGON)
Date: Mon, 18 Sep 2006 16:22:28 +0300
Subject: [R] (no subject)
Message-ID: <BAY22-F165075928B1F1795BCDD0BDE2D0@phx.gbl>

Hi Gurus, i have a small problem with working with graphs on R.
Say i have  data say bull-c(34,23,7,4) and i assign names to the elements in 
the brackets
if i do
Pie(bull) i get a pie chart of bull  togtjer with the names.
Question. How can i add values (percentages) in the graph

Thanks


From Torsten.Hothorn at rzmail.uni-erlangen.de  Mon Sep 18 15:24:05 2006
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Mon, 18 Sep 2006 15:24:05 +0200 (CEST)
Subject: [R] Cochrans Q Test
In-Reply-To: <1158584645.4382.36.camel@localhost.localdomain>
References: <20060918140103.s05s5vltv1xc88oc@webmail.numbers.net.au>
	<1158584645.4382.36.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.64.0609181520530.24641@imbe153.imbe.med.uni-erlangen.de>


[...]

>
>> cochranq.test(K)
>
>        Cochran's Q Test for Dependent Samples
>
> data:  K
> Cochran's Q = 23.9298, df = 11, p-value = 0.01303
>

Cochran's Q fits into the `coin' framework and thus:

>  K <- as.table(matrix(c(1,1,0,0, 1,1,0,1, 1,1,1,1, 1,1,1,1, 1,0,0,0,
+  1,1,1,1, 1,1,1,1, 0,0,0,0, 0,1,0,1, 1,1,1,1, 0,1,0,1, 0,1,0,1),ncol=12,
+  dimnames = list("Seating type" = c("I","II","III","IV"),"Test
+  subject"=c("A","B","C","D","E","F","G","H","I","J","K","L"))))
>
> df <- data.frame(success = as.vector(K),
+                  test = factor(rep(colnames(K), rep(4, 12))),
+                  subject = factor(rep(rownames(K), 12)))
>
> library("coin")
> symmetry_test(success ~ test | subject, data = df, teststat = "quad")

         Asymptotic General Independence Test

data:  success by
          groups A, B, C, D, E, F, G, H, I, J, K, L
          stratified by subject
chi-squared = 23.9298, df = 11, p-value = 0.01303

>

can be used to compute the test without additional coding and

> symmetry_test(success ~ test | subject, data = df, teststat = "quad",
                 distribution = approximate(10000))

         Approximative General Independence Test

data:  success by
          groups A, B, C, D, E, F, G, H, I, J, K, L
          stratified by subject
chi-squared = 23.9298, p-value = 0.006

>

approximates the p value by Monte-Carlo procedures.

Best wishes,

Torsten


>
> BTW, a quick Google search shows that the pcochran() function is in the
> 'outliers' package on CRAN, which also has a cochran.test() function
>
> HTH,
>
> Marc Schwartz
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From p.dalgaard at biostat.ku.dk  Mon Sep 18 16:16:34 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 Sep 2006 16:16:34 +0200
Subject: [R] prediction interval for new value
In-Reply-To: <20060918124820.28094.qmail@web37612.mail.mud.yahoo.com>
References: <20060918124820.28094.qmail@web37612.mail.mud.yahoo.com>
Message-ID: <x2wt81i7jx.fsf@viggo.kubism.ku.dk>

Sachin J <sachinj.2006 at yahoo.com> writes:

> RUsers: 
>    
>   Just confirming, does predict function with interval="prediction"
>   option gives prediction interval or tolerance interval?. Sorry for
>   reposting this question.


Is there any definition of tolerance interval that is different from
prediction interval?

(Tolerance intervals in the medical sense means intervals that are
designed to detect patients with abnormal levels of serum cholesterol
(say).)

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From sachinj.2006 at yahoo.com  Mon Sep 18 16:23:58 2006
From: sachinj.2006 at yahoo.com (Sachin J)
Date: Mon, 18 Sep 2006 07:23:58 -0700 (PDT)
Subject: [R] prediction interval for new value
In-Reply-To: <x2wt81i7jx.fsf@viggo.kubism.ku.dk>
Message-ID: <20060918142358.63531.qmail@web37612.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060918/5093942a/attachment.pl 

From p.dalgaard at biostat.ku.dk  Mon Sep 18 16:48:12 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 Sep 2006 16:48:12 +0200
Subject: [R] prediction interval for new value
In-Reply-To: <20060918142358.63531.qmail@web37612.mail.mud.yahoo.com>
References: <20060918142358.63531.qmail@web37612.mail.mud.yahoo.com>
Message-ID: <x2slipi637.fsf@viggo.kubism.ku.dk>

Sachin J <sachinj.2006 at yahoo.com> writes:

> Google search gave me this: 
>    
>   http://ewr.cee.vt.edu/environmental/teach/smprimer/intervals/interval.html
>    
>   TIA
>   Sachin

With those definitions (which are hardly universal), tolerance
intervals are the same as prediction intervals with k == m == 1, which
is what R provides.

   
> 
> Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
>   Sachin J writes:
> 
> > RUsers: 
> > 
> > Just confirming, does predict function with interval="prediction"
> > option gives prediction interval or tolerance interval?. Sorry for
> > reposting this question.
> 
> 
> Is there any definition of tolerance interval that is different from
> prediction interval?
> 
> (Tolerance intervals in the medical sense means intervals that are
> designed to detect patients with abnormal levels of serum cholesterol
> (say).)
> 
> -- 
> O__ ---- Peter Dalgaard ??ster Farimagsgade 5, Entr.B
> c/ /'_ --- Dept. of Biostatistics PO Box 2099, 1014 Cph. K
> (*) \(*) -- University of Copenhagen Denmark Ph: (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk) FAX: (+45) 35327907
> 
> 
>  		
> ---------------------------------
> Do you Yahoo!?
>  Everyone is raving about the  all-new Yahoo! Mail.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From spluque at gmail.com  Mon Sep 18 16:59:50 2006
From: spluque at gmail.com (Sebastian P. Luque)
Date: Mon, 18 Sep 2006 09:59:50 -0500
Subject: [R] non linear modelling with nls: starting values
Message-ID: <878xkhgqzd.fsf@patagonia.sebmags.homelinux.org>

Hi,

I'm trying to fit the following model to data using 'nls':


y = alpha_1 * beta_1 * exp(-beta_1 * x) +
    alpha_2 * beta_2 * exp(-beta_2 * x)


and the call I've been using is:


nls(y ~ alpha_1 * beta_1 * exp(-beta_1 * x) +
        alpha_2 * beta_2 * exp(-beta_2 * x),
    start=list(alpha_1=4, alpha_2=2, beta_1=3.5, beta_2=2.5),
    trace=TRUE, control=nls.control(maxiter = 200))


So the model has 4 parameters (alpha_1, alpha_2, beta_1, beta_2), but
providing appropriate starting values is proving difficult.  Although the
data could reasonably be fit with this model, the procedure is exiting
with "singular gradient matrix at initial parameter estimates".  How can
one obtain appropriate starting values, assuming that is really the
problem?  The archives show some suggestions to use 'optim', but that
requires starting values too, so I'm not sure how to proceed.

Searching for self-starting functions, I found that there's one for a
bi-exponential model, which is very similar to the one I'm trying to fit.
Would it be reasonable to create a modified version of this function, so
that it returns a value that can be used for the model above?  I would
greatly appreciate any comments and suggestions.


-- 
Seb


From sundar.dorai-raj at pdf.com  Mon Sep 18 17:15:18 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Mon, 18 Sep 2006 10:15:18 -0500
Subject: [R] Add percentage to pie (was "(no subject)")
In-Reply-To: <BAY22-F165075928B1F1795BCDD0BDE2D0@phx.gbl>
References: <BAY22-F165075928B1F1795BCDD0BDE2D0@phx.gbl>
Message-ID: <450EB806.8050209@pdf.com>



ERICK YEGON said the following on 9/18/2006 8:22 AM:
> Hi Gurus, i have a small problem with working with graphs on R.
> Say i have  data say bull-c(34,23,7,4) and i assign names to the elements in 
> the brackets
> if i do
> Pie(bull) i get a pie chart of bull  togtjer with the names.
> Question. How can i add values (percentages) in the graph
> 
> Thanks
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


(Please use a sensible subject line!)

Where do you want to add percentages? If next to the labels, then just use:

bull <- c(34, 23, 7, 4)
names(bull) <- LETTERS[seq(along = bull)]
lbl <- sprintf("%s = %3.1f%s", names(bull), bull/sum(bull)*100, "%")
pie(bull, lbl)

--sundar


From mothsailor at googlemail.com  Mon Sep 18 17:18:07 2006
From: mothsailor at googlemail.com (David Barron)
Date: Mon, 18 Sep 2006 16:18:07 +0100
Subject: [R] (no subject)
In-Reply-To: <BAY22-F165075928B1F1795BCDD0BDE2D0@phx.gbl>
References: <BAY22-F165075928B1F1795BCDD0BDE2D0@phx.gbl>
Message-ID: <815b70590609180818k2c018deate236007299c573a9@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060918/b8ce13fd/attachment.pl 

From ggrothendieck at gmail.com  Mon Sep 18 17:22:48 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 18 Sep 2006 11:22:48 -0400
Subject: [R] non linear modelling with nls: starting values
In-Reply-To: <878xkhgqzd.fsf@patagonia.sebmags.homelinux.org>
References: <878xkhgqzd.fsf@patagonia.sebmags.homelinux.org>
Message-ID: <971536df0609180822w369f8b83jd16300522e687f0d@mail.gmail.com>

Here are some approaches:

- we only have 4 parameters so just use grid search to get
starting values as in:

https://stat.ethz.ch/pipermail/r-help/2005-September/079617.html

- there are singularities near beta_1 = beta_2 and near alpha_1 = 0
and near alpha_2 = 0 so reparameterize and use the upper and
lower bounds to avoid those regions.  You could try a separate
reduced model for those.


On 9/18/06, Sebastian P. Luque <spluque at gmail.com> wrote:
> Hi,
>
> I'm trying to fit the following model to data using 'nls':
>
>
> y = alpha_1 * beta_1 * exp(-beta_1 * x) +
>    alpha_2 * beta_2 * exp(-beta_2 * x)
>
>
> and the call I've been using is:
>
>
> nls(y ~ alpha_1 * beta_1 * exp(-beta_1 * x) +
>        alpha_2 * beta_2 * exp(-beta_2 * x),
>    start=list(alpha_1=4, alpha_2=2, beta_1=3.5, beta_2=2.5),
>    trace=TRUE, control=nls.control(maxiter = 200))
>
>
> So the model has 4 parameters (alpha_1, alpha_2, beta_1, beta_2), but
> providing appropriate starting values is proving difficult.  Although the
> data could reasonably be fit with this model, the procedure is exiting
> with "singular gradient matrix at initial parameter estimates".  How can
> one obtain appropriate starting values, assuming that is really the
> problem?  The archives show some suggestions to use 'optim', but that
> requires starting values too, so I'm not sure how to proceed.
>
> Searching for self-starting functions, I found that there's one for a
> bi-exponential model, which is very similar to the one I'm trying to fit.
> Would it be reasonable to create a modified version of this function, so
> that it returns a value that can be used for the model above?  I would
> greatly appreciate any comments and suggestions.
>
>
> --
> Seb
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From bli1 at bcm.tmc.edu  Sun Sep 17 04:17:21 2006
From: bli1 at bcm.tmc.edu (Bingshan Li)
Date: Sat, 16 Sep 2006 21:17:21 -0500
Subject: [R] using table in R
Message-ID: <0B4594FE-FA27-4272-B755-AE5184C1F53A@bcm.tmc.edu>

Hi there,

I have a dataframe whose elements are numbers or characters. I want  
to extract the frequencies of each elements in the dataframe. For  
example,

d = as.data.frame(matrix(c(1, 2, 3, 3), 2,2))

What I want is first what are the elements in the data (1,2,3 here)  
and second what are their frequencies (1,1,2 respectively). How to  
use "table" to extract these two pieces of information? I played with  
"table" but couldn't extract the information. Please assume that we  
do not know how many elements in the dataframe a priori.

Thanks a lot!


From Greg.Snow at intermountainmail.org  Mon Sep 18 17:41:53 2006
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Mon, 18 Sep 2006 09:41:53 -0600
Subject: [R] Standard error of coefficient in linear regression
Message-ID: <07E228A5BE53C24CAD490193A7381BBB5C48C0@LP-EXCHVS07.CO.IHC.COM>

 I believe that your confusion is due to a typo in the formula in [3], it is missing a sumation sign (and a subscript on x if you want to be picky).  To get the denominator you subtract the mean of your x variable from all the x-values, square the differences, then sum them up (the missing sumation sign) and take the square root.  This is essentially the standard deviation of your x variable but without dividing by (n-1).

If you want to do this in R (a good thing while learning, there are better ways for actual analysis) you could use code like:

>  x.e <- exped - mean(exped)
>  x.e2 <- x.e^2
>  sx2 <- sqrt(sum(x.e2))
>  sb <- Se/sx2 # where Se is your residual standard error from below

Hope this helps, 




-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Maciej Blizinski
Sent: Sunday, September 17, 2006 12:22 PM
To: R - help
Subject: [R] Standard error of coefficient in linear regression

Hello R users,

I have a substantial question about statistics, not about R itself, but I would love to have an answer from an R user, in form of an example in R syntax. I have spent whole Sunday searching in Google and browsing the books. I've been really close to the answer but there are at least three standard errors you can talk about in the linear regression and I'm really confused. The question is:

How exactly are standard errors of coefficients calculated in the linear regression?

Here's an example from a website I've read [1]. A company wants to know if there is a relationship between its advertising expenditures and its sales volume. 

========================================================
> exped <- c(4.2, 6.1, 3.9, 5.7, 7.3, 5.9) sales <- c(27.1, 30.4, 25.0, 
> 29.7, 40.1, 28.8) S <- data.frame(exped, sales) summary(lm(sales ~ 
> exped, data = S))

Call:
lm(formula = sales ~ exped, data = S)

Residuals:
      1       2       3       4       5       6
 1.7643 -1.9310  0.7688 -1.1583  3.3509 -2.7947

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept)   9.8725     5.2394   1.884   0.1326
exped         3.6817     0.9295   3.961   0.0167 *
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 2.637 on 4 degrees of freedom
Multiple R-Squared: 0.7968,     Adjusted R-squared: 0.7461
F-statistic: 15.69 on 1 and 4 DF,  p-value: 0.01666 ========================================================

I can calculate the standard error of the estimate, according to the equation [2]...

> S.m <- lm(sales ~ exped, data = S)
> S$pred <- predict(S.m)
> S$ye <- S$sales - S$pred
> S$ye2 <- S$ye ^ 2
> Se <- sqrt(sum(S$ye2)/(length(S$sales) - 1 - 1)) Se
[1] 2.636901

...which matches the "Residual standard error" and I'm on the right track. Next step would be to use the equation [3] to calculate the standard error of the regression coefficient (here: exped). The equation [3] uses two variables, meaning of which I can't really figure out. As the calculated value Sb is scalar, all the parameters need also to be scalars. I've already calculated Se, so I'm missing x and \bar{x}. The latter could be the estimated coefficient. What is x then?

Regards,
Maciej

[1] http://www.statpac.com/statistics-calculator/correlation-regression.htm
[2] http://www.answers.com/topic/standard-error-of-the-estimate
[3] http://www.answers.com/topic/standard-error-of-the-regression-coefficient

--
Maciej Blizi?ski <m.blizinski at wit.edu.pl> http://automatthias.wordpress.com

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From spluque at gmail.com  Mon Sep 18 17:47:13 2006
From: spluque at gmail.com (Sebastian P. Luque)
Date: Mon, 18 Sep 2006 10:47:13 -0500
Subject: [R] using table in R
References: <0B4594FE-FA27-4272-B755-AE5184C1F53A@bcm.tmc.edu>
Message-ID: <87zmcxfa7y.fsf@patagonia.sebmags.homelinux.org>

On Sat, 16 Sep 2006 21:17:21 -0500,
Bingshan Li <bli1 at bcm.tmc.edu> wrote:

> Hi there, I have a dataframe whose elements are numbers or characters. I
> want to extract the frequencies of each elements in the dataframe. For
> example,

> d = as.data.frame(matrix(c(1, 2, 3, 3), 2,2))

Don't use a data frame; e.g.:

table(as.matrix(d))


-- 
Seb


From p.dalgaard at biostat.ku.dk  Mon Sep 18 17:55:24 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 Sep 2006 17:55:24 +0200
Subject: [R] non linear modelling with nls: starting values
In-Reply-To: <971536df0609180822w369f8b83jd16300522e687f0d@mail.gmail.com>
References: <878xkhgqzd.fsf@patagonia.sebmags.homelinux.org>
	<971536df0609180822w369f8b83jd16300522e687f0d@mail.gmail.com>
Message-ID: <x2d59ti2z7.fsf@viggo.kubism.ku.dk>

"Gabor Grothendieck" <ggrothendieck at gmail.com> writes:

> Here are some approaches:
> 
> - we only have 4 parameters so just use grid search to get
> starting values as in:
> 
> https://stat.ethz.ch/pipermail/r-help/2005-September/079617.html
> 
> - there are singularities near beta_1 = beta_2 and near alpha_1 = 0
> and near alpha_2 = 0 so reparameterize and use the upper and
> lower bounds to avoid those regions.  You could try a separate
> reduced model for those.


Or just use SSbiexp and reparametrize (it is exactly the same model)


 
> 
> On 9/18/06, Sebastian P. Luque <spluque at gmail.com> wrote:
> > Hi,
> >
> > I'm trying to fit the following model to data using 'nls':
> >
> >
> > y = alpha_1 * beta_1 * exp(-beta_1 * x) +
> >    alpha_2 * beta_2 * exp(-beta_2 * x)
> >
> >
> > and the call I've been using is:
> >
> >
> > nls(y ~ alpha_1 * beta_1 * exp(-beta_1 * x) +
> >        alpha_2 * beta_2 * exp(-beta_2 * x),
> >    start=list(alpha_1=4, alpha_2=2, beta_1=3.5, beta_2=2.5),
> >    trace=TRUE, control=nls.control(maxiter = 200))
> >
> >
> > So the model has 4 parameters (alpha_1, alpha_2, beta_1, beta_2), but
> > providing appropriate starting values is proving difficult.  Although the
> > data could reasonably be fit with this model, the procedure is exiting
> > with "singular gradient matrix at initial parameter estimates".  How can
> > one obtain appropriate starting values, assuming that is really the
> > problem?  The archives show some suggestions to use 'optim', but that
> > requires starting values too, so I'm not sure how to proceed.
> >
> > Searching for self-starting functions, I found that there's one for a
> > bi-exponential model, which is very similar to the one I'm trying to fit.
> > Would it be reasonable to create a modified version of this function, so
> > that it returns a value that can be used for the model above?  I would
> > greatly appreciate any comments and suggestions.
> >
> >
> > --
> > Seb
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From mwgrant2001 at yahoo.com  Mon Sep 18 18:04:39 2006
From: mwgrant2001 at yahoo.com (Michael Grant)
Date: Mon, 18 Sep 2006 09:04:39 -0700 (PDT)
Subject: [R] prediction interval for new value
In-Reply-To: <x2slipi637.fsf@viggo.kubism.ku.dk>
Message-ID: <20060918160439.80860.qmail@web52002.mail.yahoo.com>

Sachin,

One of the references in the link is a EPA groundwater statistics training
manual from the early nineties. For a more complete related discussion see pp.
50-62 of the July 1992 EPA draft addendum on groundwater monitoring statistics
at

http://www.epa.gov/correctiveaction/resource/guidance/sitechar/gwstats/gwstats.htm
 

Regarding discussion the prediction interval for linear regression you might
want to read sections of chapter 9 in Helsel and Hirsch:

http://pubs.usgs.gov/twri/twri4a3/

I would also recommend working of the examples in these or other references
with R in order to 1.) reassure yourself that the code is doing what you want
and 2.) you are comfortable with the concepts. I seem to recall seeing the
terms 'tolerance interval' and 'prediction interval' mingled in some
discussions--shades of Peter's 'hardly universal' comment. This means that
there is even more incentive to develop some facility with the concept and code
by working a few out first.

HTH


Regards,
Michael Grant

--- Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:

> Sachin J <sachinj.2006 at yahoo.com> writes:
> 
> > Google search gave me this: 
> >    
> >  
> http://ewr.cee.vt.edu/environmental/teach/smprimer/intervals/interval.html
> >    
> >   TIA
> >   Sachin
> 
> With those definitions (which are hardly universal), tolerance
> intervals are the same as prediction intervals with k == m == 1, which
> is what R provides.
> 
>    
> > 
> > Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> >   Sachin J writes:
> > 
> > > RUsers: 
> > > 
> > > Just confirming, does predict function with interval="prediction"
> > > option gives prediction interval or tolerance interval?. Sorry for
> > > reposting this question.
> > 
> > 
> > Is there any definition of tolerance interval that is different from
> > prediction interval?
> > 
> > (Tolerance intervals in the medical sense means intervals that are
> > designed to detect patients with abnormal levels of serum cholesterol
> > (say).)
> > 
> > -- 
> > O__ ---- Peter Dalgaard ??ster Farimagsgade 5, Entr.B
> > c/ /'_ --- Dept. of Biostatistics PO Box 2099, 1014 Cph. K
> > (*) \(*) -- University of Copenhagen Denmark Ph: (+45) 35327918
> > ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk) FAX: (+45) 35327907
> > 
> > 
> >  		
> > ---------------------------------
> > Do you Yahoo!?

> 
> -- 
>    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From gunter.berton at gene.com  Mon Sep 18 18:28:24 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 18 Sep 2006 09:28:24 -0700
Subject: [R] prediction interval for new value
In-Reply-To: <x2slipi637.fsf@viggo.kubism.ku.dk>
Message-ID: <006001c6db3f$77111a70$711f210a@gne.windows.gene.com>

Peter et. al.:
> 
> With those definitions (which are hardly universal), tolerance
> intervals are the same as prediction intervals with k == m == 1, which
> is what R provides.
> 
>  
  
I don't believe this is the case. See also:

http://www.itl.nist.gov/div898/handbook/prc/section2/prc263.htm

This **is** fairly standard, I believe. For example, see the venerable
classic text (INTRO TO MATH STAT) by Hogg and Craig.

To be clear, since I may also be misinterpreting, what I understand/mean is:

Peter's definition of a "tolerance/prediction interval" is a random interval
that with a prespecified confidence contain a future predicted value;

The definition I understand to be a random interval that with a prespecified
confidence will contain a prespecfied proportion of the distribution of
future values. ..e.g. a "95%/90%" tolerance interval will with 95%
confidence contain 90% of future values (and one may well ask, "which
90%"?).

Whether this is a useful idea is another issue: the parametric version is
extremely sensitive (as one might imagine) to the assumption of exact
normality; the nonparametric version relies on order statistics and is more
robust. I believe it is nontrivial and perhaps ambiguous to extend the
concept from the usual fixed distribution to the linear regression case. I
seem to recall some papers on this, perhaps in JASA, in the past few years.

As always, I welcome correction of any errors or misunderstandings herein.

Cheers to all,

Bert Gunter


From evan.cooch at cornell.edu  Mon Sep 18 18:30:57 2006
From: evan.cooch at cornell.edu (Evan Cooch)
Date: Mon, 18 Sep 2006 12:30:57 -0400
Subject: [R] symbolic matrix elements...
Message-ID: <450EC9C1.8040402@cornell.edu>

Normally, I do symbolics in Maple, or Mathematica, but I'm trying to 
write a simple script for students to handle some *very* simple 
calculations (for other purposes) with matrix or vector elements, where 
the elements are coded symbolically. What I've tried with *partial" 
success is use of the tilde (~) operator. So, for example, consider a 
simple vector:

test=matrix(c(~ x^3-5*x+4, ~log(x^2-4*x)))

Now, when I look at test, I see

 > test
     [,1]     
[1,] Expression
[2,] Expression

Fine. When I try to extract one of the vector elements, I see (for example)

 > test[1]
[[1]]
~x^3 - 5 * x + 4


Fine - but now I'm trying to figure out how to use the extracted matrix 
element for anything else. For example, using D for simple symbolic 
derivatives

f <- test[1];
D(f,"x")

should *in theory* work, but I get the following:

 > D(f,"x");
[1] NA

But, if I try

f <- expression(x^3-5*x+4);
D(f,"x");

works fine.

So, even though it looks as if each element of test is coded as an 
expression, it seems as though it is somehow a different type of 
expression than if I code it explicitly as an expression. I'm *guessing* 
it has to do with the tilde operator not assigning the formula to 
anything, but I'm not sure.

Suggestions? Pointers to the obvious?

Thanks!

-- 
----------------------------------------------------------------------
 Evan Cooch                          e.mail: evan.cooch at cornell.edu
 Department of Natural Resources     voice: 607-255-1368
 Fernow Hall - Cornell University    FAX: 607-255-0349
 Ithaca, NY    14853                 http://canuck.dnr.cornell.edu
----------------------------------------------------------------------
Nihilism is best done by professionals. - Iggy Pop


From Greg.Snow at intermountainmail.org  Mon Sep 18 18:55:36 2006
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Mon, 18 Sep 2006 10:55:36 -0600
Subject: [R] (no subject)
Message-ID: <07E228A5BE53C24CAD490193A7381BBB5C48ED@LP-EXCHVS07.CO.IHC.COM>

You may want to rethink your whole approach here:

1. Pie charts are usually a poor choice of graph, there are better
choices.
2. Adding percentages to a pie chart is a way of admitting that the pie
chart is not doing the job.
3. If you want people to compare percentages, then a table is what is
needed.
4. A pie chart with percentages added is essentially a colorful but
poorly layed out table.

Consider using a dotplot instead of a pie chart, it changes the job of
the viewer from comparing areas/angles (done poorly by humans) to
comparing positions along a common scale (done well by humans).

If you still feel the need to combine the table and graphic into 1
(usually they serve different purposes and are best kept separate) then
you can do something like this (at least the percentages are all aligned
now for easy comparison):

> library(lattice)
> bull<-c(34,23,7,4)
>
> bull.df <- data.frame(bull=bull, name=LETTERS[1:4],
pb=round(bull/sum(bull)*100,2))
> dotplot(name~pb, data=bull.df, 
+	scales=list( 
+		x=list(limits=c(0,100)),
+	),
+	panel=function(x,y,...){
+		panel.dotplot(x,y,...)
+		ltext(100,y,paste(format(x),"%",sep=''),adj=1,xpd=NA)
+	}
+ )

It would look even better if the percentages were outside the box, but I
did not have the time to figure this part out.

Hope this helps,


-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of ERICK YEGON
Sent: Monday, September 18, 2006 7:22 AM
To: r-help at stat.math.ethz.ch
Subject: [R] (no subject)

Hi Gurus, i have a small problem with working with graphs on R.
Say i have  data say bull-c(34,23,7,4) and i assign names to the
elements in the brackets if i do
Pie(bull) i get a pie chart of bull  togtjer with the names.
Question. How can i add values (percentages) in the graph

Thanks

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From vettorazzi at econ.uni-hamburg.de  Mon Sep 18 18:57:02 2006
From: vettorazzi at econ.uni-hamburg.de (Eik Vettorazzi)
Date: Mon, 18 Sep 2006 18:57:02 +0200
Subject: [R] symbolic matrix elements...
In-Reply-To: <450EC9C1.8040402@cornell.edu>
References: <450EC9C1.8040402@cornell.edu>
Message-ID: <op.tf22pclkj3tevv@econ.uni-hamburg.de>

test=matrix(c( expression(x^3-5*x+4), expression(log(x^2-4*x))))
works.

btw. you recieved an error because D expects an expression and you offered  
a list
> class(test[1])
[1] "list"
to get the error relating to the misuse of the tilde operator you have to  
prompt the "correct" extractor "[["
f<-test[[1]]
D(f,"x")


Am Mon, 18 Sep 2006 18:30:57 +0200 schrieb Evan Cooch  
<evan.cooch at cornell.edu>:

> Normally, I do symbolics in Maple, or Mathematica, but I'm trying to
> write a simple script for students to handle some *very* simple
> calculations (for other purposes) with matrix or vector elements, where
> the elements are coded symbolically. What I've tried with *partial"
> success is use of the tilde (~) operator. So, for example, consider a
> simple vector:
>
> test=matrix(c(~ x^3-5*x+4, ~log(x^2-4*x)))
>
> Now, when I look at test, I see
>
>  > test
>      [,1]
> [1,] Expression
> [2,] Expression
>
> Fine. When I try to extract one of the vector elements, I see (for  
> example)
>
>  > test[1]
> [[1]]
> ~x^3 - 5 * x + 4
>
>
> Fine - but now I'm trying to figure out how to use the extracted matrix
> element for anything else. For example, using D for simple symbolic
> derivatives
>
> f <- test[1];
> D(f,"x")
>
> should *in theory* work, but I get the following:
>
>  > D(f,"x");
> [1] NA
>
> But, if I try
>
> f <- expression(x^3-5*x+4);
> D(f,"x");
>
> works fine.
>
> So, even though it looks as if each element of test is coded as an
> expression, it seems as though it is somehow a different type of
> expression than if I code it explicitly as an expression. I'm *guessing*
> it has to do with the tilde operator not assigning the formula to
> anything, but I'm not sure.
>
> Suggestions? Pointers to the obvious?
>
> Thanks!
>



-- 
--------------------

Universit?t Hamburg
Institut f?r Statistik und ?konometrie
Dipl.-Wi.-Math. Eik Vettorazzi
Von-Melle-Park 5
20146 Hamburg

Tel.: +49 40-42838-3540


From marcodoc75 at yahoo.com  Mon Sep 18 19:07:10 2006
From: marcodoc75 at yahoo.com (Marco Geraci)
Date: Mon, 18 Sep 2006 10:07:10 -0700 (PDT)
Subject: [R] R enquiry: Replace blank spaces with a character value
In-Reply-To: <5.1.0.14.0.20060917084104.00c0a368@pop3.brisnet.org.au>
Message-ID: <20060918170710.37061.qmail@web39703.mail.mud.yahoo.com>

Dear Bob,

It's not clear to me what you want to do. You provided
my example, which I already know, and you didn't
provide yours.

Marco


--- Bob Green <bgreen at dyson.brisnet.org.au> wrote:

> Marco,
> 
> I saw your January e-mail to an enquiry about
> filling in missing values 
> with a zero I tried adapting the various syntax
> suggestions without much luck.
> 
> If I have a dataframe called test, which contains 2
> variables (var1 & var2) 
> which contain blank spaces.  I want replace blank
> spaces with the value 
> 'n'. How can I do this using the syntax you provided
> (see below):
> 
> 
>   Any assistance you can offer is appreciated,
> 
> Bob
> 
> 
>    foo <- data.frame(x=1:3,y=letters[1:3],z=4:6,
> w=as.Date(c("02/27/92", 
> "02/27/92", "01/14/92"), "%m/%d/%y")) foo[2,] <- NA
> 
>    foo
>     x y z w
> 1 1 a 4 1992-02-27
> 2 NA <NA> NA <NA>
> 3 3 c 6 1992-01-14
> 
>   class(foo$w)
> [1] "Date"
> 
>    mode(foo$w)
> [1] "numeric"
> 
> a <- sapply(foo, is.numeric)
> b <- !sapply(foo, class)=="Date"
> 
> foo[!complete.cases(foo),a & b] <- 0
> 
>    foo
>    x y z w
> 1 1 a 4 1992-02-27
> 2 0 <NA> 0 <NA>
> 3 3 c 6 1992-01-14 
> 
>


From markleeds at verizon.net  Mon Sep 18 19:12:13 2006
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Mon, 18 Sep 2006 12:12:13 -0500 (CDT)
Subject: [R] Question on apply() with more information...
Message-ID: <10806944.2134671158599533150.JavaMail.root@vms070.mailsrvcs.net>

>From: =?ISO646-US?Q?Gunther_H=3Fning?= <gunther.hoening at ukmainz.de>
>Date: 2006/09/18 Mon AM 06:26:25 CDT
>To: 'Petr Pikal' <petr.pikal at precheza.cz>
>Cc: r-help at stat.math.ethz.ch
>Subject: Re: [R] Question on apply() with more information...

I think you want something like below but it
probably needs some fixing up because i don't recall
the syntax exactly.

predictdata<-lapply(i=1:length(Smoothlist),function predict (smthlist[[i]],xarry[i,])smthlist= SmoothList,xarry=Xarry)




>Hi,
>
>I tried both ideas, but it isn't that what I'm looking for.
>I want to avoid for loop, because the matrix is of big size(1200*1200
>entries)
>
>With a loop I would do:
>
>for ( i in seq(along = SmoothList))
>{
>	Xarry[i,] <- predict(SmoothList[[i]],Xarry[i,])$y
>} 
>
>Actually I want to do more than just to predict a value, but it isn't
>important for the initial question...
>
>Gunther
>
>-----Urspr?ngliche Nachricht-----
>Von: Petr Pikal [mailto:petr.pikal at precheza.cz] 
>Gesendet: Montag, 18. September 2006 11:44
>An: Gunther H?ning
>Cc: r-help at stat.math.ethz.ch
>Betreff: Re: AW: [R] Question on apply() with more information...
>
>Hi
>
>If I am correct apply do not choose from SmoothList as you expected. 
>Instead probably
>
>lapply(SmoothList, predict,Xarray)
>or
>mapply(predict,SmoothList, Xarray)
>
>can give you probably what you want.
>
>HTH
>Petr
>
>
>On 18 Sep 2006 at 9:26, Gunther H?ning wrote:
>
>From:           	Gunther H?ning <gunther.hoening at ukmainz.de>
>To:             	"'Petr Pikal'" <petr.pikal at precheza.cz>,
>	<r-help at stat.math.ethz.ch>
>Subject:        	AW: [R] Question on apply() with more information...
>Date sent:      	Mon, 18 Sep 2006 09:26:01 +0200
>
>> Ok.
>> I tried this too, but it still doesn't work.
>> Here some more information to try out, but just an excerpt of Xarray
>> 
>> x <- c(0.11,0.25,0.45,0.65,0.80,0.95,1)
>> Y <-
>> matrix(c(15,83,57,111,150,168,175,37,207,142,277,375,420,437),nrow=2)
>> 
>> sm <- function(y,x){smooth.spline(x,y)} SmoothList <- apply(Y,1,sm,x) 
>> NewValues <- function(x,LIST){predict(LIST,x)} Xarray <- 
>> matrix(c(0.15,0.56,0.66,0.45,0.19,0.17,0.99,0.56,0.77,0.41,0.11,0.63,0
>> .42,0. 43),nrow=2)
>> 
>> 
>> apply(Xarray, 2, NewValues,SmoothList) apply(Xarray, 2, 
>> NewValues,LIST=SmoothList)
>> 
>> 
>> 
>> -----Urspr?ngliche Nachricht-----
>> Von: Petr Pikal [mailto:petr.pikal at precheza.cz]
>> Gesendet: Montag, 18. September 2006 08:43
>> An: Gunther H?ning; r-help at stat.math.ethz.ch
>> Betreff: Re: [R] Question on apply()
>> 
>> Hi
>> 
>> not much information about what can be wrong. As nobody knows your 
>> Xarray and SmoothList it is hard to guess. You even omitted to show 
>> what "does not work" So here are few guesses.
>> 
>> predict usually expects comparable data apply(Xarray, 2, 
>> NewValues,LIST=SmoothList)
>> 
>> 
>> HTH
>> Petr
>> 
>> 
>> 
>> 
>> On 18 Sep 2006 at 8:05, Gunther H?ning wrote:
>> 
>> From:           	Gunther H?ning <gunther.hoening at ukmainz.de>
>> To:             	<r-help at stat.math.ethz.ch>
>> Date sent:      	Mon, 18 Sep 2006 08:05:28 +0200
>> Subject:        	[R] Question on apply()
>> 
>> >  Dear list,
>> > 
>> > I try to do the following:
>> > I have an list of length n, with elements done by smooth.spline 
>> > (SmoothList). Now I have a matrix with n rows and m columns with
>> > x-values(Xarray) Now I want ot predict the y-values. Therefor I want 
>> > to take the first element of SmoothList and the first row of Xarray 
>> > and predict for each element in Xarray the y value. And then take 
>> > the second element of SmoothList and second row of Xarray, third row 
>> > of SmoothList and third row of Xarray and so on....
>> > 
>> > I tried following:
>> > 
>> > NewValues <- function(x,LIST){predict(LIST,x)} apply(Xarray, 2,
>> > NewValues,SmoothList)
>> > 
>> > But it don't work.
>> > 
>> > Could anybody help please ?
>> > 
>> > Gunther
>> > 
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list 
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> > http://www.R-project.org/posting-guide.html and provide commented, 
>> > minimal, self-contained, reproducible code.
>> 
>> Petr Pikal
>> petr.pikal at precheza.cz
>> 
>
>Petr Pikal
>petr.pikal at precheza.cz
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From evan.cooch at cornell.edu  Mon Sep 18 19:16:34 2006
From: evan.cooch at cornell.edu (Evan Cooch)
Date: Mon, 18 Sep 2006 13:16:34 -0400
Subject: [R] symbolic matrix elements...
In-Reply-To: <op.tf22pclkj3tevv@econ.uni-hamburg.de>
References: <450EC9C1.8040402@cornell.edu>
	<op.tf22pclkj3tevv@econ.uni-hamburg.de>
Message-ID: <450ED472.7090605@cornell.edu>



Eik Vettorazzi wrote:
> test=matrix(c( expression(x^3-5*x+4), expression(log(x^2-4*x))))
> works.
Well, not really (or I'm misunderstanding). Your code enters fine (no 
errors), but I can't access individual elements - e.g., test[1,1] gives 
me an error:

 > test=matrix(c( expression(x^3-5*x+4), expression(log(x^2-4*x))))
 > test[1,1]
Error: matrix subscripting not handled for this type

Meaning...what?

>
> btw. you recieved an error because D expects an expression and you 
> offered a list

OK - so why then are each of the elements identified as an expression 
which I print out the vector? Each element is reported to be an 
expression. OK, if so, then I remain puzzled as to how this is a 'list'.


From sarosh.jamal at utoronto.ca  Mon Sep 18 19:37:12 2006
From: sarosh.jamal at utoronto.ca (Sarosh Jamal)
Date: Mon, 18 Sep 2006 13:37:12 -0400
Subject: [R] (no subject)
Message-ID: <20060918173715Z890049-3883+17@bureau14.utcc.utoronto.ca>


Hi there,

I was updating the R-cmdr add-on (v.1.1-6 to the latest v.1.2) for R
(v.2.2.0) in a SunOS9 environment and came across some warnings during my
installation - it seems to download the dependencies but runs into the
following during install:

* Installing *source* package 'acepack' ...
** libs
/opt/sfw/R/R-2.2.0/bin/SHLIB: make: not found
ERROR: compilation failed for package 'acepack'
/opt/sfw/R/R-2.2.0/bin/INSTALL: test: argument expected
ERROR: failed to lock directory '/opt/sfw/R/R-2.2.0/library' for modifying
Try removing '/opt/sfw/R/R-2.2.0/library/00LOCK'

I don't see why I would have to remove the 00LOCK file since it seems to
have been created by the very session of R I use to run install.packages().

I'm attaching the complete log.

Any insight or feedback will be much appreciated.

Thank you,


Sarosh Jamal 
Geo Computing & IT Specialist, Department of Geography 
University of Toronto at Mississauga 
e: sarosh.jamal at utoronto.ca 

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: Rcmdr12log.txt
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060918/7d70acd8/attachment.txt 

From bruno.giordano at music.mcgill.ca  Mon Sep 18 19:50:27 2006
From: bruno.giordano at music.mcgill.ca (Bruno L. Giordano)
Date: Mon, 18 Sep 2006 13:50:27 -0400
Subject: [R]  mixed models with ordinal independents
Message-ID: <005701c6db4a$fa50f3f0$e00ece84@brungio>

Hello,
are there potential drawbacks in the use of rank (ordinal) variables as 
predictors in a "linear" mixed model for continuous dependents?

I am trying this approach to create mixed effects models independent of the 
(monotonic) transform of the independent variables. The use of rank 
transformed independents seems the logical choice to me.

Thank you for any feedback,
    Bruno


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Bruno L. Giordano, Ph.D.
CIRMMT
Schulich School of Music, McGill University
555 Sherbrooke Street West
Montr?al, QC H3A 1E3
Canada
http://www.music.mcgill.ca/~bruno/


From murdoch at stats.uwo.ca  Mon Sep 18 19:52:47 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 18 Sep 2006 13:52:47 -0400
Subject: [R] symbolic matrix elements...
In-Reply-To: <450ED472.7090605@cornell.edu>
References: <450EC9C1.8040402@cornell.edu>	<op.tf22pclkj3tevv@econ.uni-hamburg.de>
	<450ED472.7090605@cornell.edu>
Message-ID: <450EDCEF.2050304@stats.uwo.ca>

On 9/18/2006 1:16 PM, Evan Cooch wrote:
> 
> Eik Vettorazzi wrote:
>> test=matrix(c( expression(x^3-5*x+4), expression(log(x^2-4*x))))
>> works.
> Well, not really (or I'm misunderstanding). Your code enters fine (no 
> errors), but I can't access individual elements - e.g., test[1,1] gives 
> me an error:
> 
>  > test=matrix(c( expression(x^3-5*x+4), expression(log(x^2-4*x))))
>  > test[1,1]
> Error: matrix subscripting not handled for this type
> 
> Meaning...what?

Matrices in R are just vectors with a dim attribute.  The matrix 
function let you create the matrix, but the [ function doesn't know what 
to do when the vector is of mode expression.

This is an unimplemented case, a limitation (or perhaps bug if it wasn't 
intentional) in R.

>> btw. you recieved an error because D expects an expression and you 
>> offered a list
> 
> OK - so why then are each of the elements identified as an expression 
> which I print out the vector? Each element is reported to be an 
> expression. OK, if so, then I remain puzzled as to how this is a 'list'.

The problem is that if m is a list, then m[1] is a list with one 
element.  You wanted m[[1]] to extract the first element and get 
something that's not a list.

Duncan Murdoch


From p.dalgaard at biostat.ku.dk  Mon Sep 18 19:53:59 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 Sep 2006 19:53:59 +0200
Subject: [R] (no subject)
In-Reply-To: <20060918173715Z890049-3883+17@bureau14.utcc.utoronto.ca>
References: <20060918173715Z890049-3883+17@bureau14.utcc.utoronto.ca>
Message-ID: <x264fljc20.fsf@turmalin.kubism.ku.dk>

"Sarosh Jamal" <sarosh.jamal at utoronto.ca> writes:

> Hi there,
> 
> I was updating the R-cmdr add-on (v.1.1-6 to the latest v.1.2) for R
> (v.2.2.0) in a SunOS9 environment and came across some warnings during my
> installation - it seems to download the dependencies but runs into the
> following during install:
> 
> * Installing *source* package 'acepack' ...
> ** libs
> /opt/sfw/R/R-2.2.0/bin/SHLIB: make: not found
> ERROR: compilation failed for package 'acepack'
> /opt/sfw/R/R-2.2.0/bin/INSTALL: test: argument expected
> ERROR: failed to lock directory '/opt/sfw/R/R-2.2.0/library' for modifying
> Try removing '/opt/sfw/R/R-2.2.0/library/00LOCK'
> 
> I don't see why I would have to remove the 00LOCK file since it seems to
> have been created by the very session of R I use to run install.packages().
> 
> I'm attaching the complete log.
> 
> Any insight or feedback will be much appreciated.

Notice the _first_ issue reported: 

make: not found

without a functioning "make" command, you're not likely to get
anything to work. Presumably, since you have a functioning R, "make"
is there somewhere, but you need to adjust your PATH. The rest could
well just be consequences. 
 
> Thank you,
> 
> 
> Sarosh Jamal 
> Geo Computing & IT Specialist, Department of Geography 
> University of Toronto at Mississauga 
> e: sarosh.jamal at utoronto.ca 
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From tplate at acm.org  Mon Sep 18 19:54:10 2006
From: tplate at acm.org (Tony Plate)
Date: Mon, 18 Sep 2006 11:54:10 -0600
Subject: [R] symbolic matrix elements...
In-Reply-To: <450ED472.7090605@cornell.edu>
References: <450EC9C1.8040402@cornell.edu>	<op.tf22pclkj3tevv@econ.uni-hamburg.de>
	<450ED472.7090605@cornell.edu>
Message-ID: <450EDD42.2030202@acm.org>

If I construct the matrix by list()ing together the expressions rather 
than c()ing, then it works OK:

 > x <- matrix(list( expression(x3-5*x+4), expression(log(x2-4*x))))
 > x[1,1]
[[1]]
expression(x3 - 5 * x + 4)

 > x[[1,1]]
expression(x3 - 5 * x + 4)
 > D(x[[1,1]], "x")
-5
 >

The reason c() doesn't work properly here might have something to do 
with it creating a language object of an unconventional type:

 > c( expression(x3-5*x+4), expression(log(x2-4*x)))
expression(x3 - 5 * x + 4, log(x2 - 4 * x))
 > expression(x3-5*x+4)
expression(x3 - 5 * x + 4)
 >

Using list() with language objects is much safer if you just want to 
make lists of them.

-- Tony Plate

Evan Cooch wrote:
> 
> Eik Vettorazzi wrote:
> 
>>test=matrix(c( expression(x^3-5*x+4), expression(log(x^2-4*x))))
>>works.
> 
> Well, not really (or I'm misunderstanding). Your code enters fine (no 
> errors), but I can't access individual elements - e.g., test[1,1] gives 
> me an error:
> 
>  > test=matrix(c( expression(x^3-5*x+4), expression(log(x^2-4*x))))
>  > test[1,1]
> Error: matrix subscripting not handled for this type
> 
> Meaning...what?
> 
> 
>>btw. you recieved an error because D expects an expression and you 
>>offered a list
> 
> 
> OK - so why then are each of the elements identified as an expression 
> which I print out the vector? Each element is reported to be an 
> expression. OK, if so, then I remain puzzled as to how this is a 'list'.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mr.blacksheep at gmail.com  Mon Sep 18 20:08:37 2006
From: mr.blacksheep at gmail.com (Mike Nielsen)
Date: Mon, 18 Sep 2006 13:08:37 -0500
Subject: [R] (no subject)
In-Reply-To: <x264fljc20.fsf@turmalin.kubism.ku.dk>
References: <20060918173715Z890049-3883+17@bureau14.utcc.utoronto.ca>
	<x264fljc20.fsf@turmalin.kubism.ku.dk>
Message-ID: <46a360560609181108p1632f52ey8d7bbde113024fa5@mail.gmail.com>

On 18 Sep 2006 19:53:59 +0200, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> "Sarosh Jamal" <sarosh.jamal at utoronto.ca> writes:
>
> > Hi there,
> >
> > I was updating the R-cmdr add-on (v.1.1-6 to the latest v.1.2) for R
> > (v.2.2.0) in a SunOS9 environment and came across some warnings during my
> > installation - it seems to download the dependencies but runs into the
> > following during install:
> >
> > * Installing *source* package 'acepack' ...
> > ** libs
> > /opt/sfw/R/R-2.2.0/bin/SHLIB: make: not found
> > ERROR: compilation failed for package 'acepack'
> > /opt/sfw/R/R-2.2.0/bin/INSTALL: test: argument expected
> > ERROR: failed to lock directory '/opt/sfw/R/R-2.2.0/library' for modifying
> > Try removing '/opt/sfw/R/R-2.2.0/library/00LOCK'
> >
> > I don't see why I would have to remove the 00LOCK file since it seems to
> > have been created by the very session of R I use to run install.packages().
> >
> > I'm attaching the complete log.
> >
> > Any insight or feedback will be much appreciated.
>
> Notice the _first_ issue reported:
>
> make: not found
>
> without a functioning "make" command, you're not likely to get
> anything to work. Presumably, since you have a functioning R, "make"
> is there somewhere, but you need to adjust your PATH. The rest could
> well just be consequences.

And please do use a meaningful subject line.  If you're the type who
likes to look through the list archives to try to solve problems,
you'll find that good subject lines are most helpful.

>
> > Thank you,
> >
> >
> > Sarosh Jamal
> > Geo Computing & IT Specialist, Department of Geography
> > University of Toronto at Mississauga
> > e: sarosh.jamal at utoronto.ca
> >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
>    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Regards,

Mike Nielsen


From bates at stat.wisc.edu  Mon Sep 18 20:09:29 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 18 Sep 2006 13:09:29 -0500
Subject: [R] Conservative "ANOVA tables" in lmer
In-Reply-To: <40e66e0b0609120758g120f1e8ao746fb2671f1d05d6@mail.gmail.com>
References: <1A8FE5B0-3FE3-4CA9-9224-9CACB9AF4854@muohio.edu>
	<40e66e0b0609070559u656463c1l52a60eda9a095237@mail.gmail.com>
	<20060910055624.GA12212@ms.unimelb.edu.au>
	<40e66e0b0609110943h7dfb057dv12567975ac04bdb2@mail.gmail.com>
	<1158010468.3278.24.camel@solidago.localdomain>
	<40e66e0b0609120758g120f1e8ao746fb2671f1d05d6@mail.gmail.com>
Message-ID: <40e66e0b0609181109n66013995y43c003b31c29adff@mail.gmail.com>

On 9/12/06, Douglas Bates <bates at stat.wisc.edu> wrote:
> On 9/11/06, Manuel Morales <Manuel.A.Morales at williams.edu> wrote:
[snip]
> > Am I right that the MCMC sample can not be used, however, to evaluate
> > the significance of parameter groups. For example, to assess the
> > significance of a three-level factor? Are there better alternatives than
> > simply adjusting the CI for the number of factor levels
> > (1-alpha/levels).
>
> Hmm - I'm not sure what confidence interval and what number of levels
> you mean there so I can't comment on that method.
>
> Suppose we go back to Spencer's example and consider if there is a
> signficant effect for the Nozzle factor.  That is equivalent to the
> hypothesis H_0: beta_2 = beta_3 = 0 versus the general alternative.  A
> "p-value" could be formulated from an MCMC sample if we assume that
> the marginal distribution of the parameter estimates for beta_2 and
> beta_3 has roughly elliptical contours and you can evaluate that by,
> say, examining a hexbin plot of the values in the MCMC sample. One
> could take the ellipses as defined by the standard errors and
> estimated correlation or, probably better, by the observed standard
> deviations and correlations in the MCMC sample.  Then determine the
> proportion of (beta_2, beta_3) pairs in the sample that fall outside
> the ellipse centered at the estimates and with that eccentricity and
> scaling factors that passes through (0,0).  That would be an empirical
> p-value for the test.
>
> I would recommend calculating this for a couple of samples to check on
> the reproducibility.

There was some follow-up on this thread, including some code and
results that I find encouraging.  I didn't notice that the R-help list
had been dropped off the cc: in later exchanges. I enclose my
contribution to the conversation so that others on the list will get
to see it.

--- exerpt from previous private message ----

As soon as I described the idea I knew that someone would ask for a
function to perform it.  Here's one

mcmcpvalue <- function(samp)
{
   ## elementary version that creates an empirical p-value for the
   ## hypothesis that the columns of samp have mean zero versus a
   ## general multivariate distribution with elliptical contours.

   ## differences from the mean standardized by the observed
   ## variance-covariance factor
   std <- backsolve(chol(var(samp)),
                    cbind(0, t(samp)) - colMeans(samp),
                    transpose = TRUE)
   sqdist <- colSums(std * std)
   sum(sqdist[-1] > sqdist[1])/nrow(samp)
}

At least I think I have the standardization by the Cholesky factor of
the observed variance-covariance matrix correct.  However I always
manage to confuse myself on that calculation so please let me know if
I have it wrong.

As an example, consider a model fit to the AvgDailyGain data from the
SASmixed package.
> library(nlme)
> data(AvgDailyGain, package = "SASmixed")
> summary(fm1Adg <- lme(adg ~ InitWt*Treatment - 1, AvgDailyGain, random = ~1|Block))
Linear mixed-effects model fit by REML
 Data: AvgDailyGain
      AIC      BIC    logLik
 85.32685 97.10739 -32.66342

Random effects:
 Formula: ~1 | Block
       (Intercept)  Residual
StdDev:   0.5092266 0.2223268

Fixed effects: adg ~ InitWt * Treatment - 1
                       Value Std.Error DF    t-value p-value
InitWt              0.0022937 0.0017473 17  1.3126947  0.2067
Treatment0          0.4391370 0.7110881 17  0.6175564  0.5451
Treatment10         1.4261187 0.6375458 17  2.2368880  0.0390
Treatment20         0.4796285 0.5488867 17  0.8738206  0.3944
Treatment30         0.2001071 0.7751989 17  0.2581365  0.7994
InitWt:Treatment10 -0.0012108 0.0023326 17 -0.5190774  0.6104
InitWt:Treatment20  0.0010720 0.0021737 17  0.4931507  0.6282
InitWt:Treatment30  0.0021543 0.0027863 17  0.7731996  0.4500
 Correlation:
                  InitWt Trtmn0 Trtm10 Trtm20 Trtm30 IW:T10 IW:T20
Treatment0         -0.961
Treatment10         0.034  0.039
Treatment20         0.003  0.080  0.334
Treatment30         0.050  0.011  0.097  0.043
InitWt:Treatment10 -0.772  0.742 -0.631 -0.164 -0.059
InitWt:Treatment20 -0.806  0.775 -0.180 -0.555 -0.019  0.724
InitWt:Treatment30 -0.666  0.640 -0.046  0.024 -0.754  0.529  0.520

Standardized Within-Group Residuals:
       Min          Q1         Med          Q3         Max
-1.82903364 -0.44913967 -0.03023488  0.44738506  1.59877700

Number of Observations: 32
Number of Groups: 8
> anova(fm1Adg)
                numDF denDF  F-value p-value
InitWt               1    17 91.68230  <.0001
Treatment            4    17  8.81312  0.0005
InitWt:Treatment     3    17  0.93118  0.4471

Fitting the same model in lmer then generating an MCMC sample and
testing for the three interaction coefficients being zero would look
like

> data(AvgDailyGain, package = "SASmixed")
> (fm1Adg <- lmer(adg ~ InitWt*Treatment - 1 + (1|Block), AvgDailyGain))
Linear mixed-effects model fit by REML
Formula: adg ~ InitWt * Treatment - 1 + (1 | Block)
  Data: AvgDailyGain
  AIC   BIC logLik MLdeviance REMLdeviance
 83.33 96.52 -32.66      10.10        65.33
Random effects:
 Groups   Name        Variance Std.Dev.
 Block    (Intercept) 0.25930  0.50922
 Residual             0.04943  0.22233
number of obs: 32, groups: Block, 8

Fixed effects:
                   Estimate Std. Error t value
InitWt              0.002294   0.001747  1.3127
Treatment0          0.439128   0.711092  0.6175
Treatment10         1.426113   0.637549  2.2369
Treatment20         0.479621   0.548889  0.8738
Treatment30         0.200115   0.775204  0.2581
InitWt:Treatment10 -0.001211   0.002333 -0.5191
InitWt:Treatment20  0.001072   0.002174  0.4931
InitWt:Treatment30  0.002154   0.002786  0.7732

Correlation of Fixed Effects:
           InitWt Trtmn0 Trtm10 Trtm20 Trtm30 IW:T10 IW:T20
Treatment0  -0.961
Treatment10  0.034  0.039
Treatment20  0.003  0.080  0.334
Treatment30  0.050  0.011  0.097  0.043
IntWt:Trt10 -0.772  0.742 -0.631 -0.164 -0.059
IntWt:Trt20 -0.806  0.775 -0.180 -0.555 -0.019  0.724
IntWt:Trt30 -0.666  0.640 -0.046  0.024 -0.754  0.529  0.520
> AdgS1 <- mcmcsamp(fm1Adg, 50000)
> library(coda)
> HPDinterval(AdgS1)
                         lower        upper
InitWt             -0.001402986  0.006164517
Treatment0         -1.144313821  1.925351624
Treatment10         0.047128188  2.776309715
Treatment20        -0.761106539  1.602588258
Treatment30        -1.440932884  1.887536852
InitWt:Treatment10 -0.006309270  0.003569772
InitWt:Treatment20 -0.003578127  0.005614077
InitWt:Treatment30 -0.004003873  0.008055591
log(sigma^2)       -3.617259562 -2.198161379
log(Blck.(In))     -2.438121552  0.002976392
deviance           12.769059400 32.841699073
attr(,"Probability")
[1] 0.95
> mcmcpvalue(as.matrix(AdgS1)[, 6:8])
[1] 0.46876

If we drop the interaction term and consider the treatment term the test becomes

> (fm2Adg <- lmer(adg ~ InitWt + Treatment + (1|Block), AvgDailyGain))
Linear mixed-effects model fit by REML
Formula: adg ~ InitWt + Treatment + (1 | Block)
  Data: AvgDailyGain
  AIC   BIC logLik MLdeviance REMLdeviance
 48.34 57.13 -18.17      13.62        36.34
Random effects:
 Groups   Name        Variance Std.Dev.
 Block    (Intercept) 0.24084  0.49076
 Residual             0.05008  0.22379
number of obs: 32, groups: Block, 8

Fixed effects:
            Estimate Std. Error t value
(Intercept) 0.2490338  0.3776318   0.659
InitWt      0.0027797  0.0008334   3.336
Treatment10 0.4835075  0.1128530   4.284
Treatment20 0.4639445  0.1120505   4.140
Treatment30 0.5520737  0.1148132   4.808

Correlation of Fixed Effects:
           (Intr) InitWt Trtm10 Trtm20
InitWt      -0.863
Treatment10 -0.035 -0.130
Treatment20 -0.102 -0.053  0.502
Treatment30 -0.338  0.224  0.454  0.475
> AdgS2 <- mcmcsamp(fm2Adg, 50000)
> HPDinterval(AdgS2)
                     lower        upper
(Intercept)    -0.579312545  1.044946476
InitWt          0.001114375  0.004616652
Treatment10     0.246254015  0.714185069
Treatment20     0.220236717  0.686356392
Treatment30     0.316078702  0.797956531
log(sigma^2)   -3.553700311 -2.280238850
log(Blck.(In)) -2.448486301 -0.072657242
deviance       14.687593543 29.699825900
attr(,"Probability")
[1] 0.95
> mcmcpvalue(as.matrix(AdgS2[,3:5]))
[1] 0.00026

so these p-values seem to be in line with the results from the
analysis of variance p-values using one of the formula for calculation
of a residual degrees of freedom.

Related to the other question of the use of a likelihood ratio test
for the fixed effects, the p-values for the likelihood ratio tests
seem quite different

> anova(fm2Adg, fm1Adg)
Data: AvgDailyGain
Models:
fm2Adg: adg ~ InitWt + Treatment + (1 | Block)
fm1Adg: adg ~ InitWt * Treatment - 1 + (1 | Block)
      Df    AIC    BIC logLik  Chisq Chi Df Pr(>Chisq)
fm2Adg  6 25.623 34.417 -6.812
fm1Adg  9 28.098 41.290 -5.049 3.5249      3     0.3176

> fm3Adg <- lmer(adg ~ InitWt + (1|Block), AvgDailyGain)
> anova(fm2Adg, fm3Adg)
Data: AvgDailyGain
Models:
fm3Adg: adg ~ InitWt + (1 | Block)
fm2Adg: adg ~ InitWt + Treatment + (1 | Block)
      Df     AIC     BIC  logLik Chisq Chi Df Pr(>Chisq)
fm3Adg  3  41.913  46.310 -17.957
fm2Adg  6  25.623  34.417  -6.812 22.29      3  5.677e-05 ***

and it is not just a matter of the evaluation of the log-likelihood

> fm2AdgML <- update(fm2Adg, method = "ML")
> fm3AdgML <- update(fm3Adg, method = "ML")
> anova(fm3AdgML, fm2AdgML)
Data: AvgDailyGain
Models:
fm3AdgML: adg ~ InitWt + (1 | Block)
fm2AdgML: adg ~ InitWt + Treatment + (1 | Block)
        Df     AIC     BIC  logLik  Chisq Chi Df Pr(>Chisq)
fm3AdgML  3  41.882  46.279 -17.941
fm2AdgML  6  25.617  34.412  -6.809 22.265      3  5.746e-05 ***


From evan.cooch at cornell.edu  Mon Sep 18 20:13:58 2006
From: evan.cooch at cornell.edu (Evan Cooch)
Date: Mon, 18 Sep 2006 14:13:58 -0400
Subject: [R] symbolic matrix elements...
In-Reply-To: <450EDD42.2030202@acm.org>
References: <450EC9C1.8040402@cornell.edu>	<op.tf22pclkj3tevv@econ.uni-hamburg.de>
	<450ED472.7090605@cornell.edu> <450EDD42.2030202@acm.org>
Message-ID: <450EE1E6.80604@cornell.edu>

Seems to work also - thanks!

Tony Plate wrote:
> If I construct the matrix by list()ing together the expressions rather 
> than c()ing, then it works OK:
>
> > x <- matrix(list( expression(x3-5*x+4), expression(log(x2-4*x))))
> > x[1,1]
> [[1]]
> expression(x3 - 5 * x + 4)
>
> > x[[1,1]]
> expression(x3 - 5 * x + 4)
> > D(x[[1,1]], "x")
> -5
> >
>
> The reason c() doesn't work properly here might have something to do 
> with it creating a language object of an unconventional type:
>
> > c( expression(x3-5*x+4), expression(log(x2-4*x)))
> expression(x3 - 5 * x + 4, log(x2 - 4 * x))
> > expression(x3-5*x+4)
> expression(x3 - 5 * x + 4)
> >
>
> Using list() with language objects is much safer if you just want to 
> make lists of them.
>
> -- Tony Plate
>
> Evan Cooch wrote:
>>
>> Eik Vettorazzi wrote:
>>
>>> test=matrix(c( expression(x^3-5*x+4), expression(log(x^2-4*x))))
>>> works.
>>
>> Well, not really (or I'm misunderstanding). Your code enters fine (no 
>> errors), but I can't access individual elements - e.g., test[1,1] 
>> gives me an error:
>>
>>  > test=matrix(c( expression(x^3-5*x+4), expression(log(x^2-4*x))))
>>  > test[1,1]
>> Error: matrix subscripting not handled for this type
>>
>> Meaning...what?
>>
>>
>>> btw. you recieved an error because D expects an expression and you 
>>> offered a list
>>
>>
>> OK - so why then are each of the elements identified as an expression 
>> which I print out the vector? Each element is reported to be an 
>> expression. OK, if so, then I remain puzzled as to how this is a 'list'.
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

-- 
----------------------------------------------------------------------
 Evan Cooch                          e.mail: evan.cooch at cornell.edu
 Department of Natural Resources     voice: 607-255-1368
 Fernow Hall - Cornell University    FAX: 607-255-0349
 Ithaca, NY    14853                 http://canuck.dnr.cornell.edu
----------------------------------------------------------------------
A small error in the beginning is a great one in the end.- St. Thomas Aquinas


From gregr at rand.org  Mon Sep 18 20:17:02 2006
From: gregr at rand.org (Ridgeway, Greg)
Date: Mon, 18 Sep 2006 11:17:02 -0700
Subject: [R] LARS for generalized linear models
Message-ID: <4B995DE98BE2724699E6D68F41B6737577EC2E@smmail9.rand.org>


Check out Park & Hastie's glmpath package. They have a really clever
analysis and implementation of a generalized least angle regression.
Greg

>On Fri, 2006-09-15 at 18:49 -0400, Ravi Varadhan wrote:
> > Is there an R implementation of least angle regression for binary
response
> > modeling?  I know that this question has been asked before, and I am
also
> > aware of the "lasso2" package, but that only implements an L1
penalty, i.e.
> > the Lasso approach.
>
> > Madigan and Ridgeway in their discussion of Efron et al (2004)
describe a
> > LARS-type algorithm for generalized linear models.  Has anyone
implemented
> > this in R?



--------------------

This email message is for the sole use of the intended recip...{{dropped}}


From ripley at stats.ox.ac.uk  Mon Sep 18 20:35:58 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 18 Sep 2006 19:35:58 +0100 (BST)
Subject: [R] make missing on Solaris (was  no subject)
In-Reply-To: <x264fljc20.fsf@turmalin.kubism.ku.dk>
References: <20060918173715Z890049-3883+17@bureau14.utcc.utoronto.ca>
	<x264fljc20.fsf@turmalin.kubism.ku.dk>
Message-ID: <Pine.LNX.4.64.0609181932150.5952@gannet.stats.ox.ac.uk>

On Mon, 18 Sep 2006, Peter Dalgaard wrote:

> "Sarosh Jamal" <sarosh.jamal at utoronto.ca> writes:
>
>> Hi there,
>>
>> I was updating the R-cmdr add-on (v.1.1-6 to the latest v.1.2) for R
>> (v.2.2.0) in a SunOS9 environment and came across some warnings during my
>> installation - it seems to download the dependencies but runs into the
>> following during install:
>>
>> * Installing *source* package 'acepack' ...
>> ** libs
>> /opt/sfw/R/R-2.2.0/bin/SHLIB: make: not found
>> ERROR: compilation failed for package 'acepack'
>> /opt/sfw/R/R-2.2.0/bin/INSTALL: test: argument expected
>> ERROR: failed to lock directory '/opt/sfw/R/R-2.2.0/library' for modifying
>> Try removing '/opt/sfw/R/R-2.2.0/library/00LOCK'
>>
>> I don't see why I would have to remove the 00LOCK file since it seems to
>> have been created by the very session of R I use to run install.packages().
>>
>> I'm attaching the complete log.
>>
>> Any insight or feedback will be much appreciated.
>
> Notice the _first_ issue reported:
>
> make: not found

It is in /usr/ccs/bin, and not usually in the Parh on Solaris.

BUT, your R is very old (2.2.0, and 2.4.0 is about to go into beta).
Please update as we ask (see the posting guide) *before* posting about 
problems.

> without a functioning "make" command, you're not likely to get
> anything to work. Presumably, since you have a functioning R, "make"
> is there somewhere, but you need to adjust your PATH. The rest could
> well just be consequences.

Yes, of the use of a long-outdated version of R.  The 00LOCK problem was a 
Solaris (Bourne) sh vs bash problem fixed long ago, but since 2.2.0 AFAIR.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From darrenleeweber at gmail.com  Mon Sep 18 20:52:42 2006
From: darrenleeweber at gmail.com (Darren Weber)
Date: Mon, 18 Sep 2006 11:52:42 -0700
Subject: [R] currency or stock trading strategy
In-Reply-To: <450E582F.6000902@pburns.seanet.com>
References: <d2095b8c0609171437o496256aam53bbbcc25d1acf43@mail.gmail.com>
	<450E582F.6000902@pburns.seanet.com>
Message-ID: <d2095b8c0609181152m2b12a590vba51c19ee2d7dac9@mail.gmail.com>

Hi Patrick,

thanks for pointing me to your work and Rmetrics.

I have a few questions on my mind right now.  Do you have methods for
automatic download of price quote histories?  I can use python to get
XML data on FOREX price quotes from the NYRB and other sites.
Together with Rpy and matplotlib, that data download could form the
basis for an open source technical analysis platform.  I still need
some way to get open source price quote histories for stocks and
options.  Any ideas?

Best, Darren


On 9/18/06, Patrick Burns <pburns at pburns.seanet.com> wrote:
> The Finance page of the Burns Statistics website tells
> you how to sign up to R-sig-finance.
>
> You want to investigate Rmetrics.
>
> You can see an example of backtesting in R from the
> 'evalstrat' package that is in the Public Domain area of
> the Burns Statistics website.
>
> Patrick Burns
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
>
> Darren Weber wrote:
>
> >Hi,
> >
> >are there any good charting and analysis tools for use with
> >currencies, stocks, etc. in R?  I have some tools to download currency
> >data from the NYFRB using python and XML.  Can we get and parse an XML
> >download using R?  Can we have interaction in R plots?  Does anyone
> >use R for back-testing trading strategies?  Are there any forums for
> >discussion of using R for this specific purpose (apart from this
> >general list)?  Is anyone aware of any general open-source
> >developments for these purposes (I don't see any from GNU or google
> >searches)?
> >
> >Take care, Darren
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >
> >
>


From rvaradhan at jhmi.edu  Mon Sep 18 21:37:44 2006
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Mon, 18 Sep 2006 15:37:44 -0400
Subject: [R] Propensity score modeling using machine learning methods. WAS:
	RE: LARS for generalized linear models
In-Reply-To: <4B995DE98BE2724699E6D68F41B6737577EC2E@smmail9.rand.org>
Message-ID: <000e01c6db59$e9c08b40$7c94100a@win.ad.jhu.edu>

Thanks very much, Greg.  I will certainly look at glmpath.

My goal is to develop (nearly) automatic and flexible procedures for
estimating causal effects of risk factors in observational epidemiological
studies.  A major part of this is the development of a propensity score
model (when the exposure is binary).  I would like to use tools/approaches
that can do this semi-automatically so that the resulting model has both low
prediction error and good covariate balance.

I have read your paper (McCaffrey, Ridgeway and Morral 2004), which uses a
gradient boosting machine (gbm) to build a logistic regression model for
propensity score.  I was wondering whether there are other tools that can
also address this problem, for example, glmpath or MARS? 

An important question is whether these "machine learning" methods, mainly
focused on a good prediction rule, can also achieve a good covariate balance
between the treatment groups, since "balance" is not explicitly built into
the cost function.  If there is significant imbalance, incorporating such
covariates into the regression model for outcomes, and performing a weighted
least squares analysis (with estimated propensity score as weights) should
be reasonable.  Am I right?  

I would appreciate comments on these points.

Thanks very much.

Best,
Ravi.


----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------

-----Original Message-----
From: Ridgeway, Greg [mailto:gregr at rand.org] 
Sent: Monday, September 18, 2006 2:17 PM
To: r-help at stat.math.ethz.ch
Cc: Ravi Varadhan
Subject: Re: [R] LARS for generalized linear models


Check out Park & Hastie's glmpath package. They have a really clever
analysis and implementation of a generalized least angle regression.
Greg

>On Fri, 2006-09-15 at 18:49 -0400, Ravi Varadhan wrote:
> > Is there an R implementation of least angle regression for binary
response
> > modeling?  I know that this question has been asked before, and I am
also
> > aware of the "lasso2" package, but that only implements an L1
penalty, i.e.
> > the Lasso approach.
>
> > Madigan and Ridgeway in their discussion of Efron et al (2004)
describe a
> > LARS-type algorithm for generalized linear models.  Has anyone
implemented
> > this in R?



--------------------

This email message is for the sole use of the intended recip...{{dropped}}


From gregr at rand.org  Mon Sep 18 22:04:55 2006
From: gregr at rand.org (Ridgeway, Greg)
Date: Mon, 18 Sep 2006 13:04:55 -0700
Subject: [R] Propensity score modeling using machine learning methods.
	WAS: RE: LARS for generalized linear models
Message-ID: <4B995DE98BE2724699E6D68F41B6737577EC55@smmail9.rand.org>


There may be benefits to having a machine learning method that
explicitly targets covariate balance. We have experimented with
optimizing the weights directly to obtain the best covariate balance,
but got some strange solutions for simple cases that made us wary of
such methods.

Machine learning methods that yield calibrated probability estimates
should do well (e.g. those that optimize the logistic log-likelihood).
Methods that only seek a decision boundary (SVM comes to mind) can be
give great classifiers but offer poor probability estimates and then the
propensity score weights are a mess. We've had a lot of success in
practice using gbm and selecting the number of iterations to optimize
balance. You can try the ps() function in the twang package which wraps
up gbm and balance optimization in a single function. It's slow for
large datasets but it gets the job done.

Including additional variables in a weighted regression is a great
protective step. It can reduce both bias and variance and can produce
"doubly robust" estimates of the treatment effect (see Bang & Robins
2005 for an example).

Greg

 
-----Original Message-----
From: Ravi Varadhan [mailto:rvaradhan at jhmi.edu] 
Sent: Monday, September 18, 2006 12:38 PM
To: Ridgeway, Greg; r-help at stat.math.ethz.ch
Subject: Propensity score modeling using machine learning methods. WAS:
RE: [R] LARS for generalized linear models

Thanks very much, Greg.  I will certainly look at glmpath.

My goal is to develop (nearly) automatic and flexible procedures for
estimating causal effects of risk factors in observational
epidemiological
studies.  A major part of this is the development of a propensity score
model (when the exposure is binary).  I would like to use
tools/approaches
that can do this semi-automatically so that the resulting model has both
low
prediction error and good covariate balance.

I have read your paper (McCaffrey, Ridgeway and Morral 2004), which uses
a
gradient boosting machine (gbm) to build a logistic regression model for
propensity score.  I was wondering whether there are other tools that
can
also address this problem, for example, glmpath or MARS? 

An important question is whether these "machine learning" methods,
mainly
focused on a good prediction rule, can also achieve a good covariate
balance
between the treatment groups, since "balance" is not explicitly built
into
the cost function.  If there is significant imbalance, incorporating
such
covariates into the regression model for outcomes, and performing a
weighted
least squares analysis (with estimated propensity score as weights)
should
be reasonable.  Am I right?  

I would appreciate comments on these points.

Thanks very much.

Best,
Ravi.


------------------------------------------------------------------------
----
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:
http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

------------------------------------------------------------------------
----
--------

-----Original Message-----
From: Ridgeway, Greg [mailto:gregr at rand.org] 
Sent: Monday, September 18, 2006 2:17 PM
To: r-help at stat.math.ethz.ch
Cc: Ravi Varadhan
Subject: Re: [R] LARS for generalized linear models


Check out Park & Hastie's glmpath package. They have a really clever
analysis and implementation of a generalized least angle regression.
Greg

>On Fri, 2006-09-15 at 18:49 -0400, Ravi Varadhan wrote:
> > Is there an R implementation of least angle regression for binary
response
> > modeling?  I know that this question has been asked before, and I am
also
> > aware of the "lasso2" package, but that only implements an L1
penalty, i.e.
> > the Lasso approach.
>
> > Madigan and Ridgeway in their discussion of Efron et al (2004)
describe a
> > LARS-type algorithm for generalized linear models.  Has anyone
implemented
> > this in R?



--------------------

This email message is for the sole use of the intended recip...{{dropped}}


From murdoch at stats.uwo.ca  Mon Sep 18 22:46:23 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 18 Sep 2006 16:46:23 -0400
Subject: [R] uniform integer RNG 0 to t inclusive
In-Reply-To: <8ed68eed0609180037y175ea091p4927ab87f7f8ccfb@mail.gmail.com>
References: <8ed68eed0609180037y175ea091p4927ab87f7f8ccfb@mail.gmail.com>
Message-ID: <450F059F.4000203@stats.uwo.ca>

On 9/18/2006 3:37 AM, Sean O'Riordain wrote:
> Good morning,
> 
> I'm trying to concisely generate a single integer from 0 to n
> inclusive, where n might be of the order of hundreds of millions.
> This will however be used many times during the general procedure, so
> it must be "reasonably efficient" in both memory and time... (at some
> later stage in the development I hope to go vectorized)
> 
> The examples I've found through searching RSiteSearch() relating to
> generating random integers say to use : sample(0:n, 1)
> However, when n is "large" this first generates a large sequence 0:n
> before taking a sample of one... this computer doesn't have the memory
> for that!

You don't need to give the whole vector:  just give n, and you'll get 
draws from 1:n.  The man page is clear on this.

So what you want is sample(n+1, 1) - 1.  (Use "replace=TRUE" if you want 
a sample bigger than 1, or you'll get sampling without replacement.)
> 
> When I look at the documentation for runif(n, min, max) it states that
> the generated numbers will be min <= x <= max.  Note the "<= max"...

Actually it says that's the range for the uniform density.  It's silent 
on the range of the output.  But it's good defensive programming to 
assume that it's possible to get the endpoints.

> 
> How do I generate an x such that the probability of being (the
> integer) max is the same as any other integer from min (an integer) to
> max-1 (an integer) inclusive... My attempt is:
> 
> urand.int <- function(n,t) {
>   as.integer(runif(n,min=0, max=t+1-.Machine$double.eps))
> }
> # where I've included the parameter n to help testing...

Because of rounding error, t+1-.Machine$double.eps might be exactly 
equal to t+1.  I'd suggest using a rejection method if you need to use 
this approach:  but sample() is better in the cases where as.integer() 
will work.

Duncan Murdoch
> 
> is floor() "better" than as.integer?
> 
> Is this correct?  Is the probability of the integer t the same as the
> integer 1 or 0 etc... I have done some rudimentary testing and this
> appears to work, but power being what it is, I can't see how to
> realistically test this hypothesis.
> 
> Or is there a a better way of doing this?
> 
> I'm trying to implement an algorithm which samples into an array,
> hence the need for an integer - and yes I know about sample() thanks!
> :-)
> 
> { incidentally, I was surprised to note that the maximum value
> returned by summary(integer_vector) is "pretty" and appears to be
> rounded up to a "nice round number", and is not necessarily the same
> as max(integer_vector) where the value is large, i.e. of the order of
> say 50 million }
> 
> Is version etc relevant? (I'll want to be portable)
>> version               _
> platform       i386-pc-mingw32
> arch           i386
> os             mingw32
> system         i386, mingw32
> status
> major          2
> minor          3.1
> year           2006
> month          06
> day            01
> svn rev        38247
> language       R
> version.string Version 2.3.1 (2006-06-01)
> 
> Many thanks in advance for your help.
> Sean O'Riordain
> affiliation <- NULL
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From inaki.murillo at ehu.es  Mon Sep 18 19:31:46 2006
From: inaki.murillo at ehu.es (=?iso-8859-1?q?I=F1aki_Murillo_Arcos?=)
Date: Mon, 18 Sep 2006 19:31:46 +0200
Subject: [R] acos(0.5) == pi/3 FALSE
Message-ID: <200609181931.46198.inaki.murillo@ehu.es>

Hello,

  I don't know if the result of

  acos(0.5) == pi/3

is a bug or not. It looks strange to me.

   Inaki Murillo


From ott.toomet at ut.ee  Mon Sep 18 16:59:25 2006
From: ott.toomet at ut.ee (Ott-Siim Toomet)
Date: Mon, 18 Sep 2006 17:59:25 +0300 (EEST)
Subject: [R] ISO8601 week-of-year to date
Message-ID: <48151.128.230.104.196.1158591565.squirrel@mailhost.ut.ee>

Hi,

are there any way to convert ISO8601 weeks to gregorian dates?  Something
like

coverttodate(year=2006, week=38, day=1)
# Sept 18, 2006

Thanks in advance,
Ott


From dhu at medicine.ucsf.edu  Mon Sep 18 21:46:40 2006
From: dhu at medicine.ucsf.edu (Hu, Donglei)
Date: Mon, 18 Sep 2006 12:46:40 -0700
Subject: [R] Comparison of correlation coefficients
Message-ID: <FCC2A268036E2E41A1611EE0C7497648017F38B5@EXVS06.net.ucsf.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060918/8761c299/attachment.pl 

From mschwartz at mn.rr.com  Mon Sep 18 23:05:04 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Mon, 18 Sep 2006 16:05:04 -0500
Subject: [R] acos(0.5) == pi/3 FALSE
In-Reply-To: <200609181931.46198.inaki.murillo@ehu.es>
References: <200609181931.46198.inaki.murillo@ehu.es>
Message-ID: <1158613504.11572.19.camel@localhost.localdomain>

On Mon, 2006-09-18 at 19:31 +0200, I?aki Murillo Arcos wrote:
> Hello,
> 
>   I don't know if the result of
> 
>   acos(0.5) == pi/3
> 
> is a bug or not. It looks strange to me.
> 
>    Inaki Murillo

Seems reasonable to me:

> acos(0.5) == pi/3
[1] FALSE

> print(acos(0.5), 20)
[1] 1.0471975511965978534

> print(pi/3, 20)
[1] 1.0471975511965976313


See R FAQ 7.31 Why doesn't R think these numbers are equal?

HTH,

Marc Schwartz


From Charles.Annis at StatisticalEngineering.com  Mon Sep 18 23:06:01 2006
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Mon, 18 Sep 2006 17:06:01 -0400
Subject: [R] acos(0.5) == pi/3 FALSE
In-Reply-To: <200609181931.46198.inaki.murillo@ehu.es>
Message-ID: <020401c6db66$3f700190$6400a8c0@DD4XFW31>

How close do you think it should be, given finite resolution with digital
computing?

> acos(0.5) - pi/3
[1] 2.220446e-16




Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of I?aki Murillo Arcos
Sent: Monday, September 18, 2006 1:32 PM
To: r-help at stat.math.ethz.ch
Subject: [R] acos(0.5) == pi/3 FALSE

Hello,

  I don't know if the result of

  acos(0.5) == pi/3

is a bug or not. It looks strange to me.

   Inaki Murillo

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From mothsailor at googlemail.com  Mon Sep 18 23:08:18 2006
From: mothsailor at googlemail.com (David Barron)
Date: Mon, 18 Sep 2006 22:08:18 +0100
Subject: [R] acos(0.5) == pi/3 FALSE
In-Reply-To: <200609181931.46198.inaki.murillo@ehu.es>
References: <200609181931.46198.inaki.murillo@ehu.es>
Message-ID: <815b70590609181408w4e76898bvc269b104e1725b@mail.gmail.com>

If you had looked at help("==") you would have seen the following:

For numerical and complex values, remember == and != do not allow for
the finite representation of fractions, nor for rounding error. Using
all.equal with identical is almost always preferable.

Then if you had tried

> all.equal(acos(0.5),pi/3)
[1] TRUE


On 18/09/06, I?aki Murillo Arcos <inaki.murillo at ehu.es> wrote:
> Hello,
>
>   I don't know if the result of
>
>   acos(0.5) == pi/3
>
> is a bug or not. It looks strange to me.
>
>    Inaki Murillo
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From p.dalgaard at biostat.ku.dk  Mon Sep 18 23:09:34 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 Sep 2006 23:09:34 +0200
Subject: [R] acos(0.5) == pi/3 FALSE
In-Reply-To: <200609181931.46198.inaki.murillo@ehu.es>
References: <200609181931.46198.inaki.murillo@ehu.es>
Message-ID: <x2wt80j301.fsf@turmalin.kubism.ku.dk>

I?aki Murillo Arcos <inaki.murillo at ehu.es> writes:

> Hello,
> 
>   I don't know if the result of
> 
>   acos(0.5) == pi/3
> 
> is a bug or not. It looks strange to me.

Have a look in the R FAQ:

>   acos(0.5) - pi/3
[1] 2.220446e-16



-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From drf5n at maplepark.com  Mon Sep 18 23:12:21 2006
From: drf5n at maplepark.com (David Forrest)
Date: Mon, 18 Sep 2006 16:12:21 -0500 (CDT)
Subject: [R] acos(0.5) == pi/3 FALSE
In-Reply-To: <200609181931.46198.inaki.murillo@ehu.es>
References: <200609181931.46198.inaki.murillo@ehu.es>
Message-ID: <Pine.LNX.4.58.0609181600450.1518@maplepark.com>

On Mon, 18 Sep 2006, [iso-8859-1] I?aki Murillo Arcos wrote:

> Hello,
>
>   I don't know if the result of
>
>   acos(0.5) == pi/3
>
> is a bug or not. It looks strange to me.

Real numbers are strange.  Would the result of:

    acos(0.5) - pi/3

close enough to zero for you?  Try help('Comparison') ,  ?'==' for an
explanation, or 'abs(acos(0.5) - pi/3) < 1e-9' for a solution.

Dave
-- 
 Dr. David Forrest
 drf at vims.edu                                    (804)684-7900w
 drf5n at maplepark.com                             (804)642-0662h
                                   http://maplepark.com/~drf5n/


From bates at stat.wisc.edu  Mon Sep 18 23:13:17 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 18 Sep 2006 16:13:17 -0500
Subject: [R] acos(0.5) == pi/3 FALSE
In-Reply-To: <200609181931.46198.inaki.murillo@ehu.es>
References: <200609181931.46198.inaki.murillo@ehu.es>
Message-ID: <40e66e0b0609181413g215ae8afo8c089e186fe0ab8d@mail.gmail.com>

On 9/18/06, I?aki Murillo Arcos <inaki.murillo at ehu.es> wrote:
> Hello,
>
>   I don't know if the result of
>
>   acos(0.5) == pi/3
>
> is a bug or not. It looks strange to me.

However,

> all.equal(acos(0.5), pi/3)
[1] TRUE

so you may want to check the FAQ entry on comparisons involving
floating point numbers.


From sundar.dorai-raj at pdf.com  Mon Sep 18 23:18:29 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Mon, 18 Sep 2006 16:18:29 -0500
Subject: [R] acos(0.5) == pi/3 FALSE
In-Reply-To: <200609181931.46198.inaki.murillo@ehu.es>
References: <200609181931.46198.inaki.murillo@ehu.es>
Message-ID: <450F0D25.6060301@pdf.com>


I?aki Murillo Arcos said the following on 9/18/2006 12:31 PM:
> Hello,
> 
>   I don't know if the result of
> 
>   acos(0.5) == pi/3
> 
> is a bug or not. It looks strange to me.
> 
>    Inaki Murillo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

This is a FAQ:

http://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-doesn_0027t-R-think-these-numbers-are-equal_003f

 > acos(.5)
[1] 1.047198
 > pi/3
[1] 1.047198
 > acos(0.5) == pi/3
[1] FALSE
 > all.equal(acos(0.5), pi/3)
[1] TRUE

--sundar


From mothsailor at googlemail.com  Mon Sep 18 23:44:42 2006
From: mothsailor at googlemail.com (David Barron)
Date: Mon, 18 Sep 2006 22:44:42 +0100
Subject: [R] Comparison of correlation coefficients
In-Reply-To: <FCC2A268036E2E41A1611EE0C7497648017F38B5@EXVS06.net.ucsf.edu>
References: <FCC2A268036E2E41A1611EE0C7497648017F38B5@EXVS06.net.ucsf.edu>
Message-ID: <815b70590609181444x4563d587yaf1f548cf1046a5d@mail.gmail.com>

Is cor.test() in the stats packages what you mean?

On 18/09/06, Hu, Donglei <dhu at medicine.ucsf.edu> wrote:
> Hi,
>
>
>
> I calculated a few correlation coefficients.  Now I want to know whether
> they are different from each other.  Is there an R package that can do
> such a comparison?  Thanks for any suggestion.
>
>
>
> Best,
>
> Donglei Hu
>
> Department of Medicine
>
> UCSF
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From lynda_zhang at yahoo.com  Mon Sep 18 23:55:02 2006
From: lynda_zhang at yahoo.com (Lynda)
Date: Mon, 18 Sep 2006 17:55:02 -0400
Subject: [R] R questions
Message-ID: <450F15B6.8030403@yahoo.com>

 I have a few questions for R:

1. Other than using a built-in function such as mean(), how do I know if 
it is installed in my current version of R?

2. To get help in R, I can use several ways:
?sort
help.search("sort")
help(sort)
apropos("sort")
the help menu

are there any other ways to get help?

3. When I see a help menu for a function, does it mean the function is 
installed?

4. If I execute a program file that contains output files that doesn't 
have a path specified, which path is it going to output to? Is it the 
Current Working Directory that the output file is going to output? if I 
change the working directory by using setwd(), is that going to change 
where the output file path permanently until I specify it again?

Thanks a lot!
Lynda


From mkimpel at iupui.edu  Mon Sep 18 23:55:23 2006
From: mkimpel at iupui.edu (Kimpel, Mark William)
Date: Mon, 18 Sep 2006 17:55:23 -0400
Subject: [R] help with plot of prcomp object
Message-ID: <836F00680EECD340A96AD34ECFF3B53474F087@iu-mssg-mbx106.ads.iu.edu>


I need to plot a prcomp object from package stats with custom symbols suitable for B&W publication. My boss specifically wants filled and unfilled square, triangle, circle, inverted triangle, diamond to represent 5 brain regions of 2 types of rat.

Can I specify these as a parameter?

Thanks,

Mark

Mark W. Kimpel MD 

?
Official Business Address:
?
Department of Psychiatry
Indiana University School of Medicine
PR M116
Institute of Psychiatric Research
791 Union Drive
Indianapolis, IN 46202
?
Preferred Mailing Address:
?
15032 Hunter Court
Westfield, IN? 46074
?
(317) 490-5129 Work, & Mobile
?
(317) 663-0513 Home (no voice mail please)
1-(317)-536-2730 FAX


From Ted.Harding at nessie.mcc.ac.uk  Tue Sep 19 00:05:05 2006
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 18 Sep 2006 23:05:05 +0100 (BST)
Subject: [R] acos(0.5) == pi/3 FALSE
In-Reply-To: <200609181931.46198.inaki.murillo@ehu.es>
Message-ID: <XFMail.060918230505.Ted.Harding@nessie.mcc.ac.uk>

On 18-Sep-06 I?aki Murillo Arcos wrote:
> Hello,
> 
>   I don't know if the result of
> 
>   acos(0.5) == pi/3
> 
> is a bug or not. It looks strange to me.
> 
>    Inaki Murillo

It is not a bug, but a feature, in that acos(0.5) and pi/3
are not computed in the same way, so (because of the small
inaccuracies inevitable in their finite representations)
they are indeed not equal; and this is what R is telling you.

  acos(0.5) - pi/3
  [1] 2.220446e-16

The difference, as you can see is very small. However, if
for some reason (e.g. in the logic of a programming branch
which depends on such a comparison) then you could make it
work by computing them in the same way:

  pi<-3*acos(0.5)
  acos(0.5) - pi/3
  [1] 0

I.e. redefine pi in R so that its computation is compatible
with that of acos(0.5) (but this pi is only in the current
environment).

However, the usual way of comparing two numbers which should
theoretically be equal, but might differ slightly when computed
in R, is to use

  all.equal(x, y)

See "?all.equal". Also see "?identical".

  all.equal(acos(0.5),pi/3)
  [1] TRUE

(Here I'm using R's "default" value of pi, not the value
I defined above).

all.equal(x,y) tests whether x and y differ by more than
a tolerance which by default is .Machine$double.eps but
which can be set to something else if you wish.

Best wishes,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 18-Sep-06                                       Time: 23:05:01
------------------------------ XFMail ------------------------------


From rmh at temple.edu  Tue Sep 19 00:19:31 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Mon, 18 Sep 2006 18:19:31 -0400 (EDT)
Subject: [R] help with plot of prcomp object
Message-ID: <20060918181931.BIF16205@po-d.temple.edu>

yes, see ?points for the list and details.


From lockwood at rand.org  Tue Sep 19 00:27:57 2006
From: lockwood at rand.org (J.R. Lockwood)
Date: Mon, 18 Sep 2006 18:27:57 -0400 (EDT)
Subject: [R] 2007 Computing/Graphics Student Paper Competition
Message-ID: <Pine.LNX.4.58.0609181827050.4514@penguin.rand.org>

Statistical Computing and Statistical Graphics Sections
American Statistical Association
Student Paper Competition 2007

The Statistical Computing and Statistical Graphics Sections of the ASA
are co-sponsoring a student paper competition on the topics of
Statistical Computing and Statistical Graphics. Students are
encouraged to submit a paper in one of these areas, which might be
original methodological research, some novel computing or graphical
application in statistics, or any other suitable contribution (for
example, a software-related project).  The selected winners will
present their papers in a topic-contributed session at the 2007 Joint
Statistical Meetings. The Sections will pay registration fees for the
winners as well as a substantial allowance for transportation to the
meetings and lodging (which in most cases covers these expenses
completely).

Anyone who is a student (graduate or undergraduate) on or after
September 1, 2006 is eligible to participate. An entry must include an
abstract, a six page manuscript (including figures, tables and
references), a blinded version of the manuscript (with no authors and
no references that easily lead to identifying the authors), a C.V.,
and a letter from a faculty member familiar with the student's
work. The applicant must be the first author of the paper. The faculty
letter must include a verification of the applicant's student status
and, in the case of joint authorship, should indicate what fraction of
the contribution is attributable to the applicant. We prefer that
electronic submissions of papers be in Postscript or PDF. All
materials must be in English.

All application materials MUST BE RECEIVED by 5:00 PM EST, Monday,
December 18, 2006 at the address below. They will be reviewed by the
Student Paper Competition Award committee of the Statistical Computing
and Graphics Sections. The selection criteria used by the committee
will include innovation and significance of the contribution. Award
announcements will be made in late January, 2007.

Additional important information on the competition can be accessed on
the website of the Statistical Computing Section,
www.statcomputing.org. A current pointer to the website is available
from the ASA website at www.amstat.org. Inquiries and application
materials should be emailed or mailed to:

Student Paper Competition
c/o J.R. Lockwood
The RAND Corporation
4570 Fifth Avenue, Suite 600
Pittsburgh, PA 15213
lockwood at rand.org

--------------------

This email message is for the sole use of the intended recip...{{dropped}}


From bolker at zoo.ufl.edu  Tue Sep 19 00:34:34 2006
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Mon, 18 Sep 2006 22:34:34 +0000 (UTC)
Subject: [R] acos(0.5) == pi/3 FALSE
References: <200609181931.46198.inaki.murillo@ehu.es>
	<XFMail.060918230505.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <loom.20060919T003312-658@post.gmane.org>

 
  Time for folks to re-read the posting guide?

    * When responding to a very simple question, use the following algorithm:
         1. compose your response
         2. type 4*runif(1) at the R prompt, and wait this many hours
         3. check for new posts to R-help; if no similar suggestion, post your
response 
      (This is partly in jest, but if you know immediately why it is suggested,
you probably should use it! Also, it's a nice idea to replace 4 by the number of
years you have been using R or S-plus.) 

  [inspired by Romain Francois's similar post back in March]

  cheers
    Ben Bolker


From p.dalgaard at biostat.ku.dk  Tue Sep 19 01:03:55 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 19 Sep 2006 01:03:55 +0200
Subject: [R] acos(0.5) == pi/3 FALSE
In-Reply-To: <loom.20060919T003312-658@post.gmane.org>
References: <200609181931.46198.inaki.murillo@ehu.es>
	<XFMail.060918230505.Ted.Harding@nessie.mcc.ac.uk>
	<loom.20060919T003312-658@post.gmane.org>
Message-ID: <x2irjkzsis.fsf@turmalin.kubism.ku.dk>

Ben Bolker <bolker at zoo.ufl.edu> writes:

>  
>   Time for folks to re-read the posting guide?
> 
>     * When responding to a very simple question, use the following algorithm:
>          1. compose your response
>          2. type 4*runif(1) at the R prompt, and wait this many hours
>          3. check for new posts to R-help; if no similar suggestion, post your
> response 
>       (This is partly in jest, but if you know immediately why it is suggested,
> you probably should use it! Also, it's a nice idea to replace 4 by the number of
> years you have been using R or S-plus.) 

I've always wondered why step 1. - often the time-consuming bit - is not
listed last.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From p.dalgaard at biostat.ku.dk  Tue Sep 19 01:22:47 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 19 Sep 2006 01:22:47 +0200
Subject: [R] Comparison of correlation coefficients
In-Reply-To: <815b70590609181444x4563d587yaf1f548cf1046a5d@mail.gmail.com>
References: <FCC2A268036E2E41A1611EE0C7497648017F38B5@EXVS06.net.ucsf.edu>
	<815b70590609181444x4563d587yaf1f548cf1046a5d@mail.gmail.com>
Message-ID: <x2eju8zrnc.fsf@turmalin.kubism.ku.dk>

"David Barron" <mothsailor at googlemail.com> writes:

> Is cor.test() in the stats packages what you mean?

No, he wants to compare two correlation coefficients, not test that
one is zero. That's usually a misguided question, but if need be, the
Fisher z transform atanh(r) can be used to convert r to an
approximately normal variate with a known variance 1/(N-3) and
comparing r1 and r2 from two independent samples is straightforward.
The correlated case (like cor(x,y) vs cor(x,z)) is more complicated.


 
> On 18/09/06, Hu, Donglei <dhu at medicine.ucsf.edu> wrote:
> > Hi,
> >
> >
> >
> > I calculated a few correlation coefficients.  Now I want to know whether
> > they are different from each other.  Is there an R package that can do
> > such a comparison?  Thanks for any suggestion.
> >
> >
> >
> > Best,
> >
> > Donglei Hu
> >
> > Department of Medicine
> >
> > UCSF
> >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> 
> -- 
> =================================
> David Barron
> Said Business School
> University of Oxford
> Park End Street
> Oxford OX1 1HP
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From deepayan.sarkar at gmail.com  Tue Sep 19 01:23:57 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Mon, 18 Sep 2006 16:23:57 -0700
Subject: [R] dotplot/Dotplot: connecting points within factor level
	across time
In-Reply-To: <450C5E85.50803@stat.purdue.edu>
References: <450C5E85.50803@stat.purdue.edu>
Message-ID: <eb555e660609181623v60075fadq17fd245a2ab277d8@mail.gmail.com>

On 9/16/06, Benjamin Tyner <btyner at gmail.com> wrote:

> For each level of the factor in dotplot, I have time points I'd like to
> connect with a line. In the example below, 'x' represents a starting
> time and 'd' a duration, and I wish to connect 'x' to 'x+d'. Ordinarily
> I would use Dotplot from hmisc for this, but I have not been able to
> find a time class that Dotplot will allow. I can get lattice dotplot to
> put the points up, but I've not figured out how to connect them. Any
> suggestions?
>
> require(lattice)
> z<-data.frame(y=factor(letters),
>               x=structure(1:26,class=c("POSIXt","POSIXct"),tzone=""),
>               d=runif(26))
>
> # this puts the points, but does not connect them
> p<-dotplot(y~x+I(x+d),
>            data=z,
>            scales=list(x=list(format="%M:%S"))
>            )
>
> zh<-z
> zh$x<-as.numeric(zh$x)
> require(hmisc)
> # this connects them, but at the expense of the time info
> ph<-Dotplot(y~Cbind(x,x,x+d),
>             data=zh,
>             pch=" ")

If you can install packages from source, you could try installing

http://www.stat.wisc.edu/~deepayan/R/segplot_0.0-1.tar.gz

after which you could try:

--------------------

library(segplot)

segplot(y ~ x + (x+d), z,
        scales=list(x=list(format="%M:%S")))

segplot(y ~ x + (x+d), z,
        draw.bands = FALSE,
        scales=list(x=list(format="%M:%S")))

---------------------

I have been meaning to submit this to CRAN for a while, I haven't
because it's lacking a good example (I think I'll go ahead and submit
it anyway).

Deepayan


From xanga at atmail.com  Tue Sep 19 02:17:03 2006
From: xanga at atmail.com (Mr.carpenter elizebath)
Date: Tue, 19 Sep 2006 02:17:03 +0200
Subject: [R] GOLDSTAR LOTTO - Wheel E-game
Message-ID: <E1GPTIN-0005g8-Rq@bazuka.forward.pl>

       *NOTICE OF CONSOLATION PRIZE WINNING*

This email confirms that you have been notified of by the DE GOLDSTAR E-GAMES LOTTO BV DE Netherlands of your email lottery winning for 2006 
GOLDSTAR LOTTO - Wheel E-game held on 18th September 2006.

We wish to congratulate you on the selection of your email coupon number, which was selected among the 45 lucky consolation prize winners. 

Your email ID identified with Coupon No.NL132478 and was selected by Electronic Random Selection System (ERSS) with entries from the 50,000 different email addresses enrolled for the Lotto-Wheel E-game. 

Your email ID included among the 50,000 different email addresses where submitted by our partner international email provider companies.

Ref Number: 35149/337-5247/LNI
Lottery Group: Consolation Prize Group
Prize Amount: 1.500,000 (One Million Five Hundred Thousand Euro Only)

You are required to file claims for your lottery prize winning by contacting the Award Department to Processing your winning information provided above before 14 working days.
 
Award Department Officer:
Mr.carpenter elizebath
TEL:31-645-535-301
FAX:31-847-520-740 
Email:xanga at atmail.com
 
Congratulations once more from our members of staff and thank you for being part of our promotional program. 

Note: Anybody under the age of 18 is automatically disqualified. 

Yours truly,
Mrs.Mavis Cook 
(Secretary) 
For Goldstar E-games 
Netherlands.


From Robert.King at newcastle.edu.au  Tue Sep 19 02:32:04 2006
From: Robert.King at newcastle.edu.au (Robert King)
Date: Tue, 19 Sep 2006 10:32:04 +1000
Subject: [R] R CMD check fails at package dependencies check on Fedora Core
 5, works on other systems
Message-ID: <200609191032.04483.Robert.King@newcastle.edu.au>

I'm testing a FC5 machine for use in a student lab.  R 2.3.1 is installed and 
seems to work fine.  There is one peculiarity - the logins are authenticating 
to a server, and a "verbose" flag is set somewhere, leading to lots of 
spurious messages like this

request done: ld 0xa227598 msgid 1

which may be confusing R.

However, R CMD check seems to fail for packages with no dependencies at the 
dependencies check stage.

I've tried this with two packages, my own gld and also zipfR.  Both fail in 
the same way.

[Desktop]$ R CMD check zipfR_0.6-0.tar.gz
* checking for working latex ... OK
request done: ld 0x8dfb170 msgid 1
request done: ld 0x8dfb170 msgid 2
* using log directory '/home/rak776/Desktop/zipfR.Rcheck'
* using Version 2.3.1 (2006-06-01)
* checking for file 'zipfR/DESCRIPTION' ... OK
* checking extension type ... Package
* this is package 'zipfR' version '0.6-0'
* checking package dependencies ... ERROR
[Desktop]$ R CMD check gld
* checking for working latex ... OK
* using log directory '/home/rak776/Desktop/gld.Rcheck'
* using Version 2.3.1 (2006-06-01)
* checking for file 'gld/DESCRIPTION' ... OK
* this is package 'gld' version '1.8'
* checking package dependencies ... ERROR

Both these work on two other systems - one a debian/sarge AMD64 and a 
debian/sarge i386

A package with dependencies checks for dependencies correctly - for example 
ade4:

Desktop]$ R CMD check ade4_1.4-1.tar.gz
* checking for working latex ... OK
request done: ld 0x994b168 msgid 1
request done: ld 0x994b168 msgid 2
request done: ld 0x994b168 msgid 3
* using log directory '/home/rak776/Desktop/ade4.Rcheck'
* using Version 2.3.1 (2006-06-01)
* checking for file 'ade4/DESCRIPTION' ... OK
* this is package 'ade4' version '1.4-1'
* checking package dependencies ... ERROR
request done: ld 0xa458598 msgid 1
request done: ld 0xa458598 msgid 2
request done: ld 0xa458598 msgid 3
Packages required but not available:
  waveslim splancs maptools spdep pixmap ape tripack

Does anyone have ideas on what is going wrong?

Regards,
Robert King
-- 
Robert King, Statistics, School of Mathematical & Physical Sciences,
University of Newcastle, Australia
Room V133  ph +61 2 4921 5548
Robert.King at newcastle.edu.au   http://tolstoy.newcastle.edu.au/~rking/


From btyner at gmail.com  Tue Sep 19 03:38:02 2006
From: btyner at gmail.com (Benjamin Tyner)
Date: Mon, 18 Sep 2006 21:38:02 -0400
Subject: [R] dotplot/Dotplot: connecting points within factor level
 across time
In-Reply-To: <971536df0609161755q6b1db6b4gd43e7f5af7ab7db7@mail.gmail.com>
References: <450C5E85.50803@stat.purdue.edu>
	<971536df0609161755q6b1db6b4gd43e7f5af7ab7db7@mail.gmail.com>
Message-ID: <450F49FA.5080109@stat.purdue.edu>

Many thanks to Gabor and Deepayan. In case of conditioning, one can also 
do this inside the panel function, using subscripts as necessary. The 
segplot package is also a nice touch.

Ben

Gabor Grothendieck wrote:

> Try this:
>
> print(p)
> trellis.focus("panel", 1, 1)
> with(z, panel.segments(x, as.numeric(y), x+d, as.numeric(y)))
> trellis.unfocus()
>
> On 9/16/06, Benjamin Tyner <btyner at gmail.com> wrote:
>
>> For each level of the factor in dotplot, I have time points I'd like to
>> connect with a line. In the example below, 'x' represents a starting
>> time and 'd' a duration, and I wish to connect 'x' to 'x+d'. Ordinarily
>> I would use Dotplot from hmisc for this, but I have not been able to
>> find a time class that Dotplot will allow. I can get lattice dotplot to
>> put the points up, but I've not figured out how to connect them. Any
>> suggestions?
>>
>> require(lattice)
>> z<-data.frame(y=factor(letters),
>>              x=structure(1:26,class=c("POSIXt","POSIXct"),tzone=""),
>>              d=runif(26))
>>
>> # this puts the points, but does not connect them
>> p<-dotplot(y~x+I(x+d),
>>           data=z,
>>           scales=list(x=list(format="%M:%S"))
>>           )
>>
>> zh<-z
>> zh$x<-as.numeric(zh$x)
>> require(hmisc)
>> # this connects them, but at the expense of the time info
>> ph<-Dotplot(y~Cbind(x,x,x+d),
>>            data=zh,
>>            pch=" ")
>>
>>
>>
>> Thanks,
>> Ben
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>


From spluque at gmail.com  Tue Sep 19 05:47:37 2006
From: spluque at gmail.com (Sebastian P. Luque)
Date: Mon, 18 Sep 2006 22:47:37 -0500
Subject: [R] non linear modelling with nls: starting values
References: <878xkhgqzd.fsf@patagonia.sebmags.homelinux.org>
	<971536df0609180822w369f8b83jd16300522e687f0d@mail.gmail.com>
	<x2d59ti2z7.fsf@viggo.kubism.ku.dk>
Message-ID: <87ac4wecva.fsf@patagonia.sebmags.homelinux.org>

On 18 Sep 2006 17:55:24 +0200,
Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:

> "Gabor Grothendieck" <ggrothendieck at gmail.com> writes:
>> Here are some approaches:

>> - we only have 4 parameters so just use grid search to get starting
>> values as in:

>> https://stat.ethz.ch/pipermail/r-help/2005-September/079617.html

>> - there are singularities near beta_1 = beta_2 and near alpha_1 = 0 and
>> near alpha_2 = 0 so reparameterize and use the upper and lower bounds
>> to avoid those regions.  You could try a separate reduced model for
>> those.


> Or just use SSbiexp and reparametrize (it is exactly the same model)

Thank you Gabor and Peter.  With your help, I was able to fit the model
using SSbiexp as Peter suggested.  However, the dependent variable spans
both negative and positive values, and the negative values cause the
fitting procedure to fail with a NA/NaN/Inf error.  This might be related
to the singularities that Gabor mentioned.  If the dependent variable is
shifted upwards, so all values are positive, the model is fit without
problems.  Any ideas on how to get around this issue without shifting the
data, so that the parameters are expressed in the original scale?
Hopefully it's possible to do it using SSbiexp.  Thanks again.


Cheers,

-- 
Seb


From ggrothendieck at gmail.com  Tue Sep 19 06:01:46 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 19 Sep 2006 00:01:46 -0400
Subject: [R] non linear modelling with nls: starting values
In-Reply-To: <87ac4wecva.fsf@patagonia.sebmags.homelinux.org>
References: <878xkhgqzd.fsf@patagonia.sebmags.homelinux.org>
	<971536df0609180822w369f8b83jd16300522e687f0d@mail.gmail.com>
	<x2d59ti2z7.fsf@viggo.kubism.ku.dk>
	<87ac4wecva.fsf@patagonia.sebmags.homelinux.org>
Message-ID: <971536df0609182101y137aba5bnd11783763153811d@mail.gmail.com>

You could try fitting several cases:

- fit the reduced model with only one exp term, i.e. one of the alphas is 0
- fit the model with both alpha's constrained to be positive &
sufficiently away from 0
- fit the model with both alpha's negative & sufficiently away from 0
- fit the model with one positive and one negative and sufficiently away from 0

On 9/18/06, Sebastian P. Luque <spluque at gmail.com> wrote:
> On 18 Sep 2006 17:55:24 +0200,
> Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
>
> > "Gabor Grothendieck" <ggrothendieck at gmail.com> writes:
> >> Here are some approaches:
>
> >> - we only have 4 parameters so just use grid search to get starting
> >> values as in:
>
> >> https://stat.ethz.ch/pipermail/r-help/2005-September/079617.html
>
> >> - there are singularities near beta_1 = beta_2 and near alpha_1 = 0 and
> >> near alpha_2 = 0 so reparameterize and use the upper and lower bounds
> >> to avoid those regions.  You could try a separate reduced model for
> >> those.
>
>
> > Or just use SSbiexp and reparametrize (it is exactly the same model)
>
> Thank you Gabor and Peter.  With your help, I was able to fit the model
> using SSbiexp as Peter suggested.  However, the dependent variable spans
> both negative and positive values, and the negative values cause the
> fitting procedure to fail with a NA/NaN/Inf error.  This might be related
> to the singularities that Gabor mentioned.  If the dependent variable is
> shifted upwards, so all values are positive, the model is fit without
> problems.  Any ideas on how to get around this issue without shifting the
> data, so that the parameters are expressed in the original scale?
> Hopefully it's possible to do it using SSbiexp.  Thanks again.
>
>
> Cheers,
>
> --
> Seb
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mike.cheung.nus at gmail.com  Tue Sep 19 06:33:49 2006
From: mike.cheung.nus at gmail.com (Mike Cheung)
Date: Tue, 19 Sep 2006 12:33:49 +0800
Subject: [R] Comparison of correlation coefficients
In-Reply-To: <x2eju8zrnc.fsf@turmalin.kubism.ku.dk>
References: <FCC2A268036E2E41A1611EE0C7497648017F38B5@EXVS06.net.ucsf.edu>
	<815b70590609181444x4563d587yaf1f548cf1046a5d@mail.gmail.com>
	<x2eju8zrnc.fsf@turmalin.kubism.ku.dk>
Message-ID: <d06c23d50609182133s6909de90n112fbef24de98ed@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060919/5f7a3e9f/attachment.pl 

From AnupTyagi at yahoo.com  Tue Sep 19 09:03:40 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Tue, 19 Sep 2006 07:03:40 +0000 (UTC)
Subject: [R] Comparison of correlation coefficients
References: <FCC2A268036E2E41A1611EE0C7497648017F38B5@EXVS06.net.ucsf.edu>
	<815b70590609181444x4563d587yaf1f548cf1046a5d@mail.gmail.com>
	<x2eju8zrnc.fsf@turmalin.kubism.ku.dk>
Message-ID: <loom.20060919T090036-957@post.gmane.org>

Peter Dalgaard <p.dalgaard <at> biostat.ku.dk> writes:

> No, he wants to compare two correlation coefficients, not test that
> one is zero. That's usually a misguided question, but if need be, the
> Fisher z transform atanh(r) can be used to convert r to an
> approximately normal variate with a known variance 1/(N-3) and
> comparing r1 and r2 from two independent samples is straightforward.
> The correlated case (like cor(x,y) vs cor(x,z)) is more complicated.

It seem the more complicated case is often of more substantive interest in many
settings: is children's income more strongly correlated with parent's education
than parent's income?


From p.dalgaard at biostat.ku.dk  Tue Sep 19 09:07:07 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 19 Sep 2006 09:07:07 +0200
Subject: [R] R CMD check fails at package dependencies check on Fedora
	Core 5, works on other systems
In-Reply-To: <200609191032.04483.Robert.King@newcastle.edu.au>
References: <200609191032.04483.Robert.King@newcastle.edu.au>
Message-ID: <x2ac4wqqqs.fsf@turmalin.kubism.ku.dk>

Robert King <Robert.King at newcastle.edu.au> writes:

> I'm testing a FC5 machine for use in a student lab.  R 2.3.1 is installed and 
> seems to work fine.  There is one peculiarity - the logins are authenticating 
> to a server, and a "verbose" flag is set somewhere, leading to lots of 
> spurious messages like this
> 
> request done: ld 0xa227598 msgid 1
> 
> which may be confusing R.
> 
> However, R CMD check seems to fail for packages with no dependencies at the 
> dependencies check stage.
> 
> I've tried this with two packages, my own gld and also zipfR.  Both fail in 
> the same way.
> 
> [Desktop]$ R CMD check zipfR_0.6-0.tar.gz
> * checking for working latex ... OK
> request done: ld 0x8dfb170 msgid 1
> request done: ld 0x8dfb170 msgid 2
> * using log directory '/home/rak776/Desktop/zipfR.Rcheck'
> * using Version 2.3.1 (2006-06-01)
> * checking for file 'zipfR/DESCRIPTION' ... OK
> * checking extension type ... Package
> * this is package 'zipfR' version '0.6-0'
> * checking package dependencies ... ERROR
> [Desktop]$ R CMD check gld
> * checking for working latex ... OK
> * using log directory '/home/rak776/Desktop/gld.Rcheck'
> * using Version 2.3.1 (2006-06-01)
> * checking for file 'gld/DESCRIPTION' ... OK
> * this is package 'gld' version '1.8'
> * checking package dependencies ... ERROR
> 
> Both these work on two other systems - one a debian/sarge AMD64 and a 
> debian/sarge i386

And at least the former is quite happy on a vanilla FC5/ R2.3.1

What happens if you fire up R --quiet and submit

 tools:::.check_package_depends("zipfR")

(after unpacking the tarball, of course)? This gives no output for me.

 
> A package with dependencies checks for dependencies correctly - for example 
> ade4:
> 
> Desktop]$ R CMD check ade4_1.4-1.tar.gz
> * checking for working latex ... OK
> request done: ld 0x994b168 msgid 1
> request done: ld 0x994b168 msgid 2
> request done: ld 0x994b168 msgid 3
> * using log directory '/home/rak776/Desktop/ade4.Rcheck'
> * using Version 2.3.1 (2006-06-01)
> * checking for file 'ade4/DESCRIPTION' ... OK
> * this is package 'ade4' version '1.4-1'
> * checking package dependencies ... ERROR
> request done: ld 0xa458598 msgid 1
> request done: ld 0xa458598 msgid 2
> request done: ld 0xa458598 msgid 3
> Packages required but not available:
>   waveslim splancs maptools spdep pixmap ape tripack
> 
> Does anyone have ideas on what is going wrong?
> 
> Regards,
> Robert King
> -- 
> Robert King, Statistics, School of Mathematical & Physical Sciences,
> University of Newcastle, Australia
> Room V133  ph +61 2 4921 5548
> Robert.King at newcastle.edu.au   http://tolstoy.newcastle.edu.au/~rking/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From elagin at wias-berlin.de  Tue Sep 19 09:37:46 2006
From: elagin at wias-berlin.de (Mstislav Elagin)
Date: Tue, 19 Sep 2006 09:37:46 +0200
Subject: [R] printing a generated function crashes R
In-Reply-To: <x2u03brbxa.fsf@viggo.kubism.ku.dk>
References: <4508129D.4040303@wias-berlin.de>
	<x2u03brbxa.fsf@viggo.kubism.ku.dk>
Message-ID: <450F9E4A.3080105@wias-berlin.de>

Peter Dalgaard wrote:
> Mstislav Elagin <elagin at wias-berlin.de> writes:
> 
>> Dear All,
>>
>> the last expression in the following code snippet crashes R (version 
>> 2.3.1 on Windows XP) when run interactively:
>>
>> make.bad.function <- function(kind)
>> {
>>    zz <- switch(kind,
>>                 "1" = 1,
>>                 "2" = 2)
>>
>>    stopifnot( !is.null(zz) )
>>
>>    eval( bquote( function(x)
>>                 {
>>                   x + .(zz)
>>                 }))
>> }
>>
>> # bad.function <- make.bad.function("5") ## error as expected
>>
>> bad.function <- make.bad.function("1")
>> print(bad.function(10)) ## -> 11
>>
>> bad.function <- make.bad.function("2")
>> print(bad.function(10)) ## -> 12
>>
>> bad.function            ## this works if the code is source()'d
>> print(bad.function)     ## oops!
>>
>> However, it does work (i.e. prints the body of bad.function) if run 
>> non-interactively
>> (R --vanilla < bad-function.R).
>>
>> Any ideas why this happens?
> 
> Well, bquote seems to be doing nasty things if passed an expression with a
> function inside:
> 
>> f <- bquote(function(x) {
> +     x + 1
> + }
> + )
>> f
> function(x) {
>     x + 1
> }
>> eval(f)
> ?
> ?H~
> 
> ?H~
> 
> Program received signal SIGSEGV, Segmentation fault.
> 
> 
> I think the story is that the source attribute is getting messed up.
> 
>> z <- eval(f)
>> attr(z,"source")
> "function(x) {"("x+1}")
>> z
> ?X~
> ?X~
> ..poof..

Hallo,

after having played a bit more, I found out that the reason is not, or 
at least not only the source attribute.

If we try to construct a function of more than one argument in the same 
way as a function of no or one argument, we get an error message:

make.bad.function <- function()
{
   zz <- 1

   eval( bquote( function(x, y)
                {
                  x*y + .(zz)
                }))
}

bad.fun <- make.bad.function()

 > Error in eval(expr, envir, enclos) : invalid formal argument list for 
 > "function"

However, if, following an older post by Brian Ripley, we specify the 
list of formal arguments explicitly, everything works fine (albeit looks 
uglier):

make.function <- function()
{
   zz <- 1
   fun <-
     eval( bquote( function()
                  {
                    x*y + .(zz)
                  }))
   formals(fun) <- alist(x=, y=)
   return(fun)
}
fun <- make.function()
fun(3, 2) ## -> 7

Printing the function works, too:

fun

 > function (x, y)
 > {
 >    x * y + 1
 > }
 > <environment: 01A5884C>

My earlier example with the function of one argument works fine when 
rewritten in the same spirit.

In my opinion, bquote behaves "inhomogeneously" in the sense that the 
definition of the functions taking no of one params differs from that of 
the functions taking more params. I wonder whether such behaviour of 
bquote is a bug and should be reported.

Have a nice day
Mstislav Elagin


From petr.pikal at precheza.cz  Tue Sep 19 09:55:15 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 19 Sep 2006 09:55:15 +0200
Subject: [R] Question on apply() with more information...
In-Reply-To: <000e01c6db15$46bdae30$0f1e0b0a@3med.klinik.unimainz.de>
References: <450E868B.6767.D767EB@localhost>
Message-ID: <450FBE83.17190.62D053@localhost>

Hi

both lapply and mapply can give you what you want but you have to 
select only desired part.

e.g.

spec.apply<- function(sl, x) {
d<-dim(x)[1]
d2<-dim(x)[2]*2*d
vec<-seq(d,d2,2)
Z<-as.numeric(mapply(predict,sl, x))[vec]
dim(Z) <-c(d,length(vec)/d)
Z
}


but it is probably slower than simple for loop. However maybe some 
clever use of do.call can help.

Best regards
Petr


On 18 Sep 2006 at 13:26, Gunther H?ning wrote:

From:           	Gunther H?ning <gunther.hoening at ukmainz.de>
To:             	"'Petr Pikal'" <petr.pikal at precheza.cz>
Date sent:      	Mon, 18 Sep 2006 13:26:25 +0200
Copies to:      	r-help at stat.math.ethz.ch
Subject:        	Re: [R] Question on apply() with more information...

> Hi,
> 
> I tried both ideas, but it isn't that what I'm looking for.
> I want to avoid for loop, because the matrix is of big size(1200*1200
> entries)
> 
> With a loop I would do:
> 
> for ( i in seq(along = SmoothList))
> {
>  Xarry[i,] <- predict(SmoothList[[i]],Xarry[i,])$y
> } 
> 
> Actually I want to do more than just to predict a value, but it isn't
> important for the initial question...
> 
> Gunther
> 
> -----Urspr?ngliche Nachricht-----
> Von: Petr Pikal [mailto:petr.pikal at precheza.cz] 
> Gesendet: Montag, 18. September 2006 11:44
> An: Gunther H?ning
> Cc: r-help at stat.math.ethz.ch
> Betreff: Re: AW: [R] Question on apply() with more information...
> 
> Hi
> 
> If I am correct apply do not choose from SmoothList as you expected.
> Instead probably
> 
> lapply(SmoothList, predict,Xarray)
> or
> mapply(predict,SmoothList, Xarray)
> 
> can give you probably what you want.
> 
> HTH
> Petr
> 
> 
> On 18 Sep 2006 at 9:26, Gunther H?ning wrote:
> 
> From:           	Gunther H?ning <gunther.hoening at ukmainz.de>
> To:             	"'Petr Pikal'" <petr.pikal at precheza.cz>,
>  <r-help at stat.math.ethz.ch>
> Subject:        	AW: [R] Question on apply() with more information...
> Date sent:      	Mon, 18 Sep 2006 09:26:01 +0200
> 
> > Ok.
> > I tried this too, but it still doesn't work.
> > Here some more information to try out, but just an excerpt of Xarray
> > 
> > x <- c(0.11,0.25,0.45,0.65,0.80,0.95,1)
> > Y <-
> > matrix(c(15,83,57,111,150,168,175,37,207,142,277,375,420,437),nrow=2
> > )
> > 
> > sm <- function(y,x){smooth.spline(x,y)} SmoothList <-
> > apply(Y,1,sm,x) NewValues <- function(x,LIST){predict(LIST,x)}
> > Xarray <-
> > matrix(c(0.15,0.56,0.66,0.45,0.19,0.17,0.99,0.56,0.77,0.41,0.11,0.63
> > ,0 .42,0. 43),nrow=2)
> > 
> > 
> > apply(Xarray, 2, NewValues,SmoothList) apply(Xarray, 2, 
> > NewValues,LIST=SmoothList)
> > 
> > 
> > 
> > -----Urspr?ngliche Nachricht-----
> > Von: Petr Pikal [mailto:petr.pikal at precheza.cz]
> > Gesendet: Montag, 18. September 2006 08:43
> > An: Gunther H?ning; r-help at stat.math.ethz.ch
> > Betreff: Re: [R] Question on apply()
> > 
> > Hi
> > 
> > not much information about what can be wrong. As nobody knows your
> > Xarray and SmoothList it is hard to guess. You even omitted to show
> > what "does not work" So here are few guesses.
> > 
> > predict usually expects comparable data apply(Xarray, 2, 
> > NewValues,LIST=SmoothList)
> > 
> > 
> > HTH
> > Petr
> > 
> > 
> > 
> > 
> > On 18 Sep 2006 at 8:05, Gunther H?ning wrote:
> > 
> > From:           	Gunther H?ning <gunther.hoening at ukmainz.de>
> > To:             	<r-help at stat.math.ethz.ch>
> > Date sent:      	Mon, 18 Sep 2006 08:05:28 +0200
> > Subject:        	[R] Question on apply()
> > 
> > >  Dear list,
> > > 
> > > I try to do the following:
> > > I have an list of length n, with elements done by smooth.spline
> > > (SmoothList). Now I have a matrix with n rows and m columns with
> > > x-values(Xarray) Now I want ot predict the y-values. Therefor I
> > > want to take the first element of SmoothList and the first row of
> > > Xarray and predict for each element in Xarray the y value. And
> > > then take the second element of SmoothList and second row of
> > > Xarray, third row of SmoothList and third row of Xarray and so
> > > on....
> > > 
> > > I tried following:
> > > 
> > > NewValues <- function(x,LIST){predict(LIST,x)} apply(Xarray, 2,
> > > NewValues,SmoothList)
> > > 
> > > But it don't work.
> > > 
> > > Could anybody help please ?
> > > 
> > > Gunther
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list 
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html and provide commented,
> > > minimal, self-contained, reproducible code.
> > 
> > Petr Pikal
> > petr.pikal at precheza.cz
> > 
> 
> Petr Pikal
> petr.pikal at precheza.cz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From seanpor at acm.org  Tue Sep 19 10:03:32 2006
From: seanpor at acm.org (Sean O'Riordain)
Date: Tue, 19 Sep 2006 08:03:32 +0000
Subject: [R] uniform integer RNG 0 to t inclusive
In-Reply-To: <450F059F.4000203@stats.uwo.ca>
References: <8ed68eed0609180037y175ea091p4927ab87f7f8ccfb@mail.gmail.com>
	<450F059F.4000203@stats.uwo.ca>
Message-ID: <8ed68eed0609190103t5abbb8bjbf76f4d582ebc215@mail.gmail.com>

Hi Duncan,

Thanks for that.  In the light of what you've suggested, I'm now using
the following:

  # generate a random integer from 0 to t (inclusive)
  if (t < 10000000) { # to avoid memory problems...
    M <- sample(t, 1)
  } else {
    while (M > t) {
      M <- as.integer(urand(1,min=0, max=t+1-.Machine$double.eps))
    }
  }

cheers and Thanks,
Sean

On 18/09/06, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> On 9/18/2006 3:37 AM, Sean O'Riordain wrote:
> > Good morning,
> >
> > I'm trying to concisely generate a single integer from 0 to n
> > inclusive, where n might be of the order of hundreds of millions.
> > This will however be used many times during the general procedure, so
> > it must be "reasonably efficient" in both memory and time... (at some
> > later stage in the development I hope to go vectorized)
> >
> > The examples I've found through searching RSiteSearch() relating to
> > generating random integers say to use : sample(0:n, 1)
> > However, when n is "large" this first generates a large sequence 0:n
> > before taking a sample of one... this computer doesn't have the memory
> > for that!
>
> You don't need to give the whole vector:  just give n, and you'll get
> draws from 1:n.  The man page is clear on this.
>
> So what you want is sample(n+1, 1) - 1.  (Use "replace=TRUE" if you want
> a sample bigger than 1, or you'll get sampling without replacement.)
> >
> > When I look at the documentation for runif(n, min, max) it states that
> > the generated numbers will be min <= x <= max.  Note the "<= max"...
>
> Actually it says that's the range for the uniform density.  It's silent
> on the range of the output.  But it's good defensive programming to
> assume that it's possible to get the endpoints.
>
> >
> > How do I generate an x such that the probability of being (the
> > integer) max is the same as any other integer from min (an integer) to
> > max-1 (an integer) inclusive... My attempt is:
> >
> > urand.int <- function(n,t) {
> >   as.integer(runif(n,min=0, max=t+1-.Machine$double.eps))
> > }
> > # where I've included the parameter n to help testing...
>
> Because of rounding error, t+1-.Machine$double.eps might be exactly
> equal to t+1.  I'd suggest using a rejection method if you need to use
> this approach:  but sample() is better in the cases where as.integer()
> will work.
>
> Duncan Murdoch
> >
> > is floor() "better" than as.integer?
> >
> > Is this correct?  Is the probability of the integer t the same as the
> > integer 1 or 0 etc... I have done some rudimentary testing and this
> > appears to work, but power being what it is, I can't see how to
> > realistically test this hypothesis.
> >
> > Or is there a a better way of doing this?
> >
> > I'm trying to implement an algorithm which samples into an array,
> > hence the need for an integer - and yes I know about sample() thanks!
> > :-)
> >
> > { incidentally, I was surprised to note that the maximum value
> > returned by summary(integer_vector) is "pretty" and appears to be
> > rounded up to a "nice round number", and is not necessarily the same
> > as max(integer_vector) where the value is large, i.e. of the order of
> > say 50 million }
> >
> > Is version etc relevant? (I'll want to be portable)
> >> version               _
> > platform       i386-pc-mingw32
> > arch           i386
> > os             mingw32
> > system         i386, mingw32
> > status
> > major          2
> > minor          3.1
> > year           2006
> > month          06
> > day            01
> > svn rev        38247
> > language       R
> > version.string Version 2.3.1 (2006-06-01)
> >
> > Many thanks in advance for your help.
> > Sean O'Riordain
> > affiliation <- NULL
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>


From petr.pikal at precheza.cz  Tue Sep 19 10:07:27 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 19 Sep 2006 10:07:27 +0200
Subject: [R] R questions
In-Reply-To: <450F15B6.8030403@yahoo.com>
Message-ID: <450FC15F.24328.6DFB65@localhost>

Hi

please try to consult some introduction manual and see some 
suggestions below

On 18 Sep 2006 at 17:55, Lynda wrote:

Date sent:      	Mon, 18 Sep 2006 17:55:02 -0400
From:           	Lynda <lynda_zhang at yahoo.com>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] R questions

>  I have a few questions for R:
> 
> 1. Other than using a built-in function such as mean(), how do I know
> if it is installed in my current version of R?

see the difference between installed packages (e.g. those you have in 
your library directory and packages attached by library command.

> ?interp
No documentation for 'interp' in specified packages and libraries:
you could try 'help.search("interp")'
> library(akima)
> ?interp
Help for 'interp' is shown in the browser
>


> 
> 2. To get help in R, I can use several ways:
> ?sort
> help.search("sort")
> help(sort)
> apropos("sort")
> the help menu
> 
> are there any other ways to get help?

It is not enough for you?

> 
> 3. When I see a help menu for a function, does it mean the function is
> installed?

See above

> 
> 4. If I execute a program file that contains output files that doesn't
> have a path specified, which path is it going to output to? Is it the
> Current Working Directory that the output file is going to output? if
> I change the working directory by using setwd(), is that going to
> change where the output file path permanently until I specify it
> again?

AFAIK Yes.

HTH
Petr


> 
> Thanks a lot!
> Lynda
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From ligges at statistik.uni-dortmund.de  Tue Sep 19 10:23:46 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 19 Sep 2006 10:23:46 +0200
Subject: [R] R questions
In-Reply-To: <450F15B6.8030403@yahoo.com>
References: <450F15B6.8030403@yahoo.com>
Message-ID: <450FA912.9070808@statistik.uni-dortmund.de>

You might want to read
Ligges (2003): R Help Desk: Getting Help - R's Help Facilities and 
Manuals, R News 3(1), 26--28,
http://cran.r-project.org/doc/Rnews/Rnews_2003-1.pdf


Uwe Ligges


Lynda wrote:

>  I have a few questions for R:
> 
> 1. Other than using a built-in function such as mean(), how do I know if 
> it is installed in my current version of R?
> 
> 2. To get help in R, I can use several ways:
> ?sort
> help.search("sort")
> help(sort)
> apropos("sort")
> the help menu
> 
> are there any other ways to get help?
> 
> 3. When I see a help menu for a function, does it mean the function is 
> installed?
> 
> 4. If I execute a program file that contains output files that doesn't 
> have a path specified, which path is it going to output to? Is it the 
> Current Working Directory that the output file is going to output? if I 
> change the working directory by using setwd(), is that going to change 
> where the output file path permanently until I specify it again?
> 
> Thanks a lot!
> Lynda
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From cahn88 at hotmail.com  Tue Sep 19 03:32:57 2006
From: cahn88 at hotmail.com (Ahn ChaeHyung)
Date: Mon, 18 Sep 2006 21:32:57 -0400
Subject: [R] apply for list or list -> array
Message-ID: <BAY112-F247D7D5C1DE1A34D3241A6CB220@phx.gbl>

Dear all,

I have the following list, "aa", composed of two 3*3 tables.  I would like 
to use "apply" function to summarize it, but "apply" cannot handle "list".  
I want to do it without using any interation.

1. Is there any function like "apply" for list?
2. Is there any way to transform that "list" to a 2*3*3 "array"?

thanks

cahn

-----------------------------------------
>aa
[[1]]
     [,1] [,2] [,3]
[1,]    1    3    1
[2,]    2    0    1
[3,]    1    1    0

[[2]]
     [,1] [,2] [,3]
[1,]    1    3    1
[2,]    2    0    1
[3,]    1    1    0


I would like to avoid any iteration



-------------------------------------------------
Ahn, Chaehyung (cahn) Ph.D.
Alpha-Gamma Technologies Inc.
My Home: http://blog.naver.com/cahn88


From AnupTyagi at yahoo.com  Tue Sep 19 08:51:51 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Tue, 19 Sep 2006 06:51:51 +0000 (UTC)
Subject: [R] Adding percentage to Pie Charts (was (no subject))
References: <07E228A5BE53C24CAD490193A7381BBB5C48ED@LP-EXCHVS07.CO.IHC.COM>
Message-ID: <loom.20060919T083529-288@post.gmane.org>

Greg Snow <Greg.Snow <at> intermountainmail.org> writes:

> 
> You may want to rethink your whole approach here:
> 
> 1. Pie charts are usually a poor choice of graph, there are better
> choices.
> 2. Adding percentages to a pie chart is a way of admitting that the pie
> chart is not doing the job.
> 3. If you want people to compare percentages, then a table is what is
> needed.
> 4. A pie chart with percentages added is essentially a colorful but
> poorly layed out table.
> 
> Consider using a dotplot instead of a pie chart, it changes the job of
> the viewer from comparing areas/angles (done poorly by humans) to
> comparing positions along a common scale (done well by humans).

I think dot charts (plots) are very useful, but they are not substitutes for a
pie chart: they do not show a comparison between the total and the individual
value; have a different scale (linear, usually), and are visually not suitable
to answer some questions that a pie chart can answer (is the value approximately
less than a fourth of the total? Is it less than half?). For some of these
questions, even dot-charts require a value label, or the user doing mental
calculations to guess approximations.

I think I am quite attuned to getting approximate fractions from a pie-chart in
shorter time, than on a linear scale like the dot-chart.

A modification in a pie chart that draws overlapping areas with a common start
point at the top of the circle, can make is more informative than a dot-chart.
Something like:
* Start drawing at the top of the circle, as zero (degree/area).
* Draw the representation of every value starting from the top, as zero,
representing it as a labled line from the center of the circle to the boundary
(can use colors where possible).
* Use two lables for the circular axis, inside one for percentages, outside for
values.

What is the simplest way to draw this in R?

Anupam.


From ligges at statistik.uni-dortmund.de  Tue Sep 19 10:33:20 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 19 Sep 2006 10:33:20 +0200
Subject: [R] apply for list or list -> array
In-Reply-To: <BAY112-F247D7D5C1DE1A34D3241A6CB220@phx.gbl>
References: <BAY112-F247D7D5C1DE1A34D3241A6CB220@phx.gbl>
Message-ID: <450FAB50.2020206@statistik.uni-dortmund.de>

Ahn ChaeHyung wrote:

> Dear all,
> 
> I have the following list, "aa", composed of two 3*3 tables.  I would like 
> to use "apply" function to summarize it, but "apply" cannot handle "list".  
> I want to do it without using any interation.
> 
> 1. Is there any function like "apply" for list?
> 2. Is there any way to transform that "list" to a 2*3*3 "array"?


Use lapply() or sapply().
Example to transpose both matrices:
lapply(a, t)


Uwe Ligges



> thanks
> 
> cahn
> 
> -----------------------------------------
> 
>>aa
> 
> [[1]]
>      [,1] [,2] [,3]
> [1,]    1    3    1
> [2,]    2    0    1
> [3,]    1    1    0
> 
> [[2]]
>      [,1] [,2] [,3]
> [1,]    1    3    1
> [2,]    2    0    1
> [3,]    1    1    0
> 
> 
> I would like to avoid any iteration
> 
> 
> 
> -------------------------------------------------
> Ahn, Chaehyung (cahn) Ph.D.
> Alpha-Gamma Technologies Inc.
> My Home: http://blog.naver.com/cahn88
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From Roger.Bivand at nhh.no  Tue Sep 19 10:37:56 2006
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue, 19 Sep 2006 10:37:56 +0200 (CEST)
Subject: [R] apply for list or list -> array
In-Reply-To: <BAY112-F247D7D5C1DE1A34D3241A6CB220@phx.gbl>
Message-ID: <Pine.LNX.4.44.0609191036110.13705-100000@reclus.nhh.no>

On Mon, 18 Sep 2006, Ahn ChaeHyung wrote:

> Dear all,
> 
> I have the following list, "aa", composed of two 3*3 tables.  I would like 
> to use "apply" function to summarize it, but "apply" cannot handle "list".  
> I want to do it without using any interation.
> 
> 1. Is there any function like "apply" for list?

?lapply, ?sapply

> 2. Is there any way to transform that "list" to a 2*3*3 "array"?
> 

Among other possibilities:

library(abind)
aa <- list(m1=matrix(rnorm(9), 3, 3), m2=matrix(runif(9), 3, 3))
a1 <- abind(aa, along=3)
str(a1)
dim(a1)


> thanks
> 
> cahn
> 
> -----------------------------------------
> >aa
> [[1]]
>      [,1] [,2] [,3]
> [1,]    1    3    1
> [2,]    2    0    1
> [3,]    1    1    0
> 
> [[2]]
>      [,1] [,2] [,3]
> [1,]    1    3    1
> [2,]    2    0    1
> [3,]    1    1    0
> 
> 
> I would like to avoid any iteration
> 
> 
> 
> -------------------------------------------------
> Ahn, Chaehyung (cahn) Ph.D.
> Alpha-Gamma Technologies Inc.
> My Home: http://blog.naver.com/cahn88
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From dimitris.rizopoulos at med.kuleuven.be  Tue Sep 19 10:38:14 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 19 Sep 2006 10:38:14 +0200
Subject: [R] apply for list or list -> array
References: <BAY112-F247D7D5C1DE1A34D3241A6CB220@phx.gbl>
Message-ID: <010901c6dbc6$f25cbf90$0540210a@www.domain>

for lists look at ?lapply() and ?sapply(); for your 2nd question try 
something like:

array(unlist(aa), dim = c(3, 3, 2))


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Ahn ChaeHyung" <cahn88 at hotmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, September 19, 2006 3:32 AM
Subject: [R] apply for list or list -> array


> Dear all,
>
> I have the following list, "aa", composed of two 3*3 tables.  I 
> would like
> to use "apply" function to summarize it, but "apply" cannot handle 
> "list".
> I want to do it without using any interation.
>
> 1. Is there any function like "apply" for list?
> 2. Is there any way to transform that "list" to a 2*3*3 "array"?
>
> thanks
>
> cahn
>
> -----------------------------------------
>>aa
> [[1]]
>     [,1] [,2] [,3]
> [1,]    1    3    1
> [2,]    2    0    1
> [3,]    1    1    0
>
> [[2]]
>     [,1] [,2] [,3]
> [1,]    1    3    1
> [2,]    2    0    1
> [3,]    1    1    0
>
>
> I would like to avoid any iteration
>
>
>
> -------------------------------------------------
> Ahn, Chaehyung (cahn) Ph.D.
> Alpha-Gamma Technologies Inc.
> My Home: http://blog.naver.com/cahn88
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From ripley at stats.ox.ac.uk  Tue Sep 19 10:41:22 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 19 Sep 2006 09:41:22 +0100 (BST)
Subject: [R] uniform integer RNG 0 to t inclusive
In-Reply-To: <8ed68eed0609190103t5abbb8bjbf76f4d582ebc215@mail.gmail.com>
References: <8ed68eed0609180037y175ea091p4927ab87f7f8ccfb@mail.gmail.com>
	<450F059F.4000203@stats.uwo.ca>
	<8ed68eed0609190103t5abbb8bjbf76f4d582ebc215@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0609190929490.7969@gannet.stats.ox.ac.uk>

On Tue, 19 Sep 2006, Sean O'Riordain wrote:

> Hi Duncan,
>
> Thanks for that.  In the light of what you've suggested, I'm now using
> the following:
>
>  # generate a random integer from 0 to t (inclusive)
>  if (t < 10000000) { # to avoid memory problems...
>    M <- sample(t, 1)
>  } else {
>    while (M > t) {
>      M <- as.integer(urand(1,min=0, max=t+1-.Machine$double.eps))
>    }
>  }

sample(t, 1) is a sample from 1:t, not 0:t.

You need

sample(t+1, 1, replace=TRUE) - 1

which works in all cases up to INT_MAX-1, and beyond that you need to 
worry about the resolution of the RNG (and to use floor not as.integer).

There is no such thing as urand in base R ....

>
> cheers and Thanks,
> Sean
>
> On 18/09/06, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
>> On 9/18/2006 3:37 AM, Sean O'Riordain wrote:
>>> Good morning,
>>>
>>> I'm trying to concisely generate a single integer from 0 to n
>>> inclusive, where n might be of the order of hundreds of millions.
>>> This will however be used many times during the general procedure, so
>>> it must be "reasonably efficient" in both memory and time... (at some
>>> later stage in the development I hope to go vectorized)
>>>
>>> The examples I've found through searching RSiteSearch() relating to
>>> generating random integers say to use : sample(0:n, 1)
>>> However, when n is "large" this first generates a large sequence 0:n
>>> before taking a sample of one... this computer doesn't have the memory
>>> for that!
>>
>> You don't need to give the whole vector:  just give n, and you'll get
>> draws from 1:n.  The man page is clear on this.
>>
>> So what you want is sample(n+1, 1) - 1.  (Use "replace=TRUE" if you want
>> a sample bigger than 1, or you'll get sampling without replacement.)
>>>
>>> When I look at the documentation for runif(n, min, max) it states that
>>> the generated numbers will be min <= x <= max.  Note the "<= max"...
>>
>> Actually it says that's the range for the uniform density.  It's silent
>> on the range of the output.  But it's good defensive programming to
>> assume that it's possible to get the endpoints.
>>
>>>
>>> How do I generate an x such that the probability of being (the
>>> integer) max is the same as any other integer from min (an integer) to
>>> max-1 (an integer) inclusive... My attempt is:
>>>
>>> urand.int <- function(n,t) {
>>>   as.integer(runif(n,min=0, max=t+1-.Machine$double.eps))
>>> }
>>> # where I've included the parameter n to help testing...
>>
>> Because of rounding error, t+1-.Machine$double.eps might be exactly
>> equal to t+1.  I'd suggest using a rejection method if you need to use
>> this approach:  but sample() is better in the cases where as.integer()
>> will work.
>>
>> Duncan Murdoch
>>>
>>> is floor() "better" than as.integer?
>>>
>>> Is this correct?  Is the probability of the integer t the same as the
>>> integer 1 or 0 etc... I have done some rudimentary testing and this
>>> appears to work, but power being what it is, I can't see how to
>>> realistically test this hypothesis.
>>>
>>> Or is there a a better way of doing this?
>>>
>>> I'm trying to implement an algorithm which samples into an array,
>>> hence the need for an integer - and yes I know about sample() thanks!
>>> :-)
>>>
>>> { incidentally, I was surprised to note that the maximum value
>>> returned by summary(integer_vector) is "pretty" and appears to be
>>> rounded up to a "nice round number", and is not necessarily the same
>>> as max(integer_vector) where the value is large, i.e. of the order of
>>> say 50 million }
>>>
>>> Is version etc relevant? (I'll want to be portable)
>>>> version               _
>>> platform       i386-pc-mingw32
>>> arch           i386
>>> os             mingw32
>>> system         i386, mingw32
>>> status
>>> major          2
>>> minor          3.1
>>> year           2006
>>> month          06
>>> day            01
>>> svn rev        38247
>>> language       R
>>> version.string Version 2.3.1 (2006-06-01)
>>>
>>> Many thanks in advance for your help.
>>> Sean O'Riordain
>>> affiliation <- NULL
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From AnupTyagi at yahoo.com  Tue Sep 19 11:38:17 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Tue, 19 Sep 2006 09:38:17 +0000 (UTC)
Subject: [R] Comparison of correlation coefficients
References: <FCC2A268036E2E41A1611EE0C7497648017F38B5@EXVS06.net.ucsf.edu>
	<815b70590609181444x4563d587yaf1f548cf1046a5d@mail.gmail.com>
	<x2eju8zrnc.fsf@turmalin.kubism.ku.dk>
	<loom.20060919T090036-957@post.gmane.org>
Message-ID: <loom.20060919T111750-543@post.gmane.org>

Anupam Tyagi <AnupTyagi <at> yahoo.com> writes:

> It seem the more complicated case is often of more substantive interest in many
> settings: is children's income more strongly correlated with parent's education
> than parent's income?

An even better example (same measurement scale)---Questions like this get asked
quite often in practice: Is a child's income/wealth more strongly correlated
with a parent's income than parent's wealth? And some variants. I think there is
some literature on inference on marginals and conditional distributions, and
bounds that may be useful: Search: James Heckman, Charles Manski.

Anupam.


From robert.king at newcastle.edu.au  Tue Sep 19 12:02:08 2006
From: robert.king at newcastle.edu.au (Robert King)
Date: Tue, 19 Sep 2006 20:02:08 +1000
Subject: [R] R CMD check fails at package dependencies check on Fedora
 Core 5, works on other systems
In-Reply-To: <x2ac4wqqqs.fsf@turmalin.kubism.ku.dk>
References: <200609191032.04483.Robert.King@newcastle.edu.au>
	<x2ac4wqqqs.fsf@turmalin.kubism.ku.dk>
Message-ID: <450FC020.4070304@newcastle.edu.au>

No output from the tools command

Peter Dalgaard wrote:
> Robert King <Robert.King at newcastle.edu.au> writes:
> 
>> I'm testing a FC5 machine for use in a student lab.  R 2.3.1 is installed and 
>> seems to work fine.  There is one peculiarity - the logins are authenticating 
>> to a server, and a "verbose" flag is set somewhere, leading to lots of 
>> spurious messages like this
>>
>> request done: ld 0xa227598 msgid 1
>>
>> which may be confusing R.
>>
>> However, R CMD check seems to fail for packages with no dependencies at the 
>> dependencies check stage.
>>
>> I've tried this with two packages, my own gld and also zipfR.  Both fail in 
>> the same way.
>>
>> [Desktop]$ R CMD check zipfR_0.6-0.tar.gz
>> * checking for working latex ... OK
>> request done: ld 0x8dfb170 msgid 1
>> request done: ld 0x8dfb170 msgid 2
>> * using log directory '/home/rak776/Desktop/zipfR.Rcheck'
>> * using Version 2.3.1 (2006-06-01)
>> * checking for file 'zipfR/DESCRIPTION' ... OK
>> * checking extension type ... Package
>> * this is package 'zipfR' version '0.6-0'
>> * checking package dependencies ... ERROR
>> [Desktop]$ R CMD check gld
>> * checking for working latex ... OK
>> * using log directory '/home/rak776/Desktop/gld.Rcheck'
>> * using Version 2.3.1 (2006-06-01)
>> * checking for file 'gld/DESCRIPTION' ... OK
>> * this is package 'gld' version '1.8'
>> * checking package dependencies ... ERROR
>>
>> Both these work on two other systems - one a debian/sarge AMD64 and a 
>> debian/sarge i386
> 
> And at least the former is quite happy on a vanilla FC5/ R2.3.1
> 
> What happens if you fire up R --quiet and submit
> 
>  tools:::.check_package_depends("zipfR")
> 
> (after unpacking the tarball, of course)? This gives no output for me.

Similarly, no output

>> A package with dependencies checks for dependencies correctly - for example 
>> ade4:
>>
>> Desktop]$ R CMD check ade4_1.4-1.tar.gz
>> * checking for working latex ... OK
>> request done: ld 0x994b168 msgid 1
>> request done: ld 0x994b168 msgid 2
>> request done: ld 0x994b168 msgid 3
>> * using log directory '/home/rak776/Desktop/ade4.Rcheck'
>> * using Version 2.3.1 (2006-06-01)
>> * checking for file 'ade4/DESCRIPTION' ... OK
>> * this is package 'ade4' version '1.4-1'
>> * checking package dependencies ... ERROR
>> request done: ld 0xa458598 msgid 1
>> request done: ld 0xa458598 msgid 2
>> request done: ld 0xa458598 msgid 3
>> Packages required but not available:
>>   waveslim splancs maptools spdep pixmap ape tripack
>>
>> Does anyone have ideas on what is going wrong?
>>
>> Regards,
>> Robert King


From spyros.mesomeris at citigroup.com  Tue Sep 19 12:11:58 2006
From: spyros.mesomeris at citigroup.com (Mesomeris, Spyros [CIR])
Date: Tue, 19 Sep 2006 11:11:58 +0100
Subject: [R] Reading a file in R
Message-ID: <0D9D1AC09BC016428D7597A1716AFF710D990D@Exukmb73.eur.nsroot.net>

Dear R helpers,

I am trying to read a CSV file in R called EUROPE (originally an Excel
file which I have saved as a CSV file) using the command

EUROPEDATA <- read.csv("EUROPE.csv")

EUROPE.csv is basically a matrix of dimension 440*44, and has a line of
headers, i.e. each column has a name.

Using read.csv I can't load the data into R properly. Although the first
20 columns or so are read in properly, some of the data from the
remaining columns are missing, eg. For Column 29, the loaded file cannot
read the first 120 observations and puts NA in their place, whereas the
rest of the column is read in properly! I find this really strange.

I have tried to use read.table and scan commands as well, with the
header = T option, but still the problem is not solved. Please note the
columns are formatted in the same way, and contain numbers (apart form
the header row).

Does anybody have any idea how I can read the data properly into R?


From mothsailor at googlemail.com  Tue Sep 19 12:32:44 2006
From: mothsailor at googlemail.com (David Barron)
Date: Tue, 19 Sep 2006 11:32:44 +0100
Subject: [R] Reading a file in R
In-Reply-To: <0D9D1AC09BC016428D7597A1716AFF710D990D@Exukmb73.eur.nsroot.net>
References: <Acbb1ApFUBbnoV9SQi2CHJirCBahDA==>
	<0D9D1AC09BC016428D7597A1716AFF710D990D@Exukmb73.eur.nsroot.net>
Message-ID: <815b70590609190332v584e8139i37efeedea21797fe@mail.gmail.com>

I think that command should work (assuming that it is *comma* rather
than semi-colon delimited, which is used in countries where a comma is
used as a decimal point, in which case you should use read.csv2
instead).  So,  is your data definitely as clean as you think. Have
you looked at the data in a text editor?  What are the dimensions of
the resulting data frame?

On 19/09/06, Mesomeris, Spyros [CIR] <spyros.mesomeris at citigroup.com> wrote:
> Dear R helpers,
>
> I am trying to read a CSV file in R called EUROPE (originally an Excel
> file which I have saved as a CSV file) using the command
>
> EUROPEDATA <- read.csv("EUROPE.csv")
>
> EUROPE.csv is basically a matrix of dimension 440*44, and has a line of
> headers, i.e. each column has a name.
>
> Using read.csv I can't load the data into R properly. Although the first
> 20 columns or so are read in properly, some of the data from the
> remaining columns are missing, eg. For Column 29, the loaded file cannot
> read the first 120 observations and puts NA in their place, whereas the
> rest of the column is read in properly! I find this really strange.
>
> I have tried to use read.table and scan commands as well, with the
> header = T option, but still the problem is not solved. Please note the
> columns are formatted in the same way, and contain numbers (apart form
> the header row).
>
> Does anybody have any idea how I can read the data properly into R?
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From geoffrey.russell at gmail.com  Tue Sep 19 12:33:32 2006
From: geoffrey.russell at gmail.com (Geoff Russell)
Date: Tue, 19 Sep 2006 20:03:32 +0930
Subject: [R] Reading a file in R
In-Reply-To: <0D9D1AC09BC016428D7597A1716AFF710D990D@Exukmb73.eur.nsroot.net>
References: <0D9D1AC09BC016428D7597A1716AFF710D990D@Exukmb73.eur.nsroot.net>
Message-ID: <93c3eada0609190333i7fd0f7d2u70498da5b93ca7c8@mail.gmail.com>

On 9/19/06, Mesomeris, Spyros [CIR] <spyros.mesomeris at citigroup.com> wrote:
> Dear R helpers,
>
> I am trying to read a CSV file in R called EUROPE (originally an Excel
> file which I have saved as a CSV file) using the command
>
> EUROPEDATA <- read.csv("EUROPE.csv")
>
> EUROPE.csv is basically a matrix of dimension 440*44, and has a line of
> headers, i.e. each column has a name.

Check your file for unicode characters, they will get in the way. I'm
new to R myself
but have used both read.delim and read.csv. I've only had problems
when the files contained unicode. I don't know excel, but in staroffice you need
to explicitly set the character set during the save if the file had
unicode on input.

Cheers,
Geoff Russell


From murdoch at stats.uwo.ca  Tue Sep 19 12:45:11 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 19 Sep 2006 06:45:11 -0400
Subject: [R] uniform integer RNG 0 to t inclusive
In-Reply-To: <Pine.LNX.4.64.0609190929490.7969@gannet.stats.ox.ac.uk>
References: <8ed68eed0609180037y175ea091p4927ab87f7f8ccfb@mail.gmail.com>
	<450F059F.4000203@stats.uwo.ca>
	<8ed68eed0609190103t5abbb8bjbf76f4d582ebc215@mail.gmail.com>
	<Pine.LNX.4.64.0609190929490.7969@gannet.stats.ox.ac.uk>
Message-ID: <450FCA37.5070506@stats.uwo.ca>

On 9/19/2006 4:41 AM, Prof Brian Ripley wrote:
> On Tue, 19 Sep 2006, Sean O'Riordain wrote:
> 
>> Hi Duncan,
>>
>> Thanks for that.  In the light of what you've suggested, I'm now using
>> the following:
>>
>>  # generate a random integer from 0 to t (inclusive)
>>  if (t < 10000000) { # to avoid memory problems...
>>    M <- sample(t, 1)
>>  } else {
>>    while (M > t) {
>>      M <- as.integer(urand(1,min=0, max=t+1-.Machine$double.eps))
>>    }
>>  }
> 
> sample(t, 1) is a sample from 1:t, not 0:t.
> 
> You need
> 
> sample(t+1, 1, replace=TRUE) - 1
> 
> which works in all cases up to INT_MAX-1, and beyond that you need to 
> worry about the resolution of the RNG (and to use floor not as.integer).

I wonder if it would be a worthwhile optimization to treat replace as 
TRUE whenever size=1 is requested.

  - It would be a very cheap test in the C code, and  would make a large 
difference to the size=1 run time when n is very large.

  - On the other hand, using size=1 is usually not a very efficient way 
to program anything, so anyone who does it might not notice the gain...

Duncan Murdoch

> 
> There is no such thing as urand in base R ....
> 
>> cheers and Thanks,
>> Sean
>>
>> On 18/09/06, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
>>> On 9/18/2006 3:37 AM, Sean O'Riordain wrote:
>>>> Good morning,
>>>>
>>>> I'm trying to concisely generate a single integer from 0 to n
>>>> inclusive, where n might be of the order of hundreds of millions.
>>>> This will however be used many times during the general procedure, so
>>>> it must be "reasonably efficient" in both memory and time... (at some
>>>> later stage in the development I hope to go vectorized)
>>>>
>>>> The examples I've found through searching RSiteSearch() relating to
>>>> generating random integers say to use : sample(0:n, 1)
>>>> However, when n is "large" this first generates a large sequence 0:n
>>>> before taking a sample of one... this computer doesn't have the memory
>>>> for that!
>>> You don't need to give the whole vector:  just give n, and you'll get
>>> draws from 1:n.  The man page is clear on this.
>>>
>>> So what you want is sample(n+1, 1) - 1.  (Use "replace=TRUE" if you want
>>> a sample bigger than 1, or you'll get sampling without replacement.)
>>>> When I look at the documentation for runif(n, min, max) it states that
>>>> the generated numbers will be min <= x <= max.  Note the "<= max"...
>>> Actually it says that's the range for the uniform density.  It's silent
>>> on the range of the output.  But it's good defensive programming to
>>> assume that it's possible to get the endpoints.
>>>
>>>> How do I generate an x such that the probability of being (the
>>>> integer) max is the same as any other integer from min (an integer) to
>>>> max-1 (an integer) inclusive... My attempt is:
>>>>
>>>> urand.int <- function(n,t) {
>>>>   as.integer(runif(n,min=0, max=t+1-.Machine$double.eps))
>>>> }
>>>> # where I've included the parameter n to help testing...
>>> Because of rounding error, t+1-.Machine$double.eps might be exactly
>>> equal to t+1.  I'd suggest using a rejection method if you need to use
>>> this approach:  but sample() is better in the cases where as.integer()
>>> will work.
>>>
>>> Duncan Murdoch
>>>> is floor() "better" than as.integer?
>>>>
>>>> Is this correct?  Is the probability of the integer t the same as the
>>>> integer 1 or 0 etc... I have done some rudimentary testing and this
>>>> appears to work, but power being what it is, I can't see how to
>>>> realistically test this hypothesis.
>>>>
>>>> Or is there a a better way of doing this?
>>>>
>>>> I'm trying to implement an algorithm which samples into an array,
>>>> hence the need for an integer - and yes I know about sample() thanks!
>>>> :-)
>>>>
>>>> { incidentally, I was surprised to note that the maximum value
>>>> returned by summary(integer_vector) is "pretty" and appears to be
>>>> rounded up to a "nice round number", and is not necessarily the same
>>>> as max(integer_vector) where the value is large, i.e. of the order of
>>>> say 50 million }
>>>>
>>>> Is version etc relevant? (I'll want to be portable)
>>>>> version               _
>>>> platform       i386-pc-mingw32
>>>> arch           i386
>>>> os             mingw32
>>>> system         i386, mingw32
>>>> status
>>>> major          2
>>>> minor          3.1
>>>> year           2006
>>>> month          06
>>>> day            01
>>>> svn rev        38247
>>>> language       R
>>>> version.string Version 2.3.1 (2006-06-01)
>>>>
>>>> Many thanks in advance for your help.
>>>> Sean O'Riordain
>>>> affiliation <- NULL
>>>>
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>


From spyros.mesomeris at citigroup.com  Tue Sep 19 13:04:42 2006
From: spyros.mesomeris at citigroup.com (Mesomeris, Spyros [CIR])
Date: Tue, 19 Sep 2006 12:04:42 +0100
Subject: [R] Reading a file in R
Message-ID: <0D9D1AC09BC016428D7597A1716AFF710D990E@Exukmb73.eur.nsroot.net>

Thanks David,

It has actually worked, the problem was the formatting of the N/A values
in Excel. R apparently doesn't like to see #N/A that Excel produces if a
formula cannot be returned. So, saving the file as csv (comma delimited)
file and removing all the #N/A observations, leaving those cells empty,
and then uploading the file into R, works fine. 

I hope this is helpful for other users as well

Thanks again
 

-----Original Message-----
From: David Barron [mailto:mothsailor at googlemail.com] 
Sent: 19 September 2006 11:33
To: Mesomeris, Spyros [CIR]; r-help
Subject: Re: [R] Reading a file in R

I think that command should work (assuming that it is *comma* rather
than semi-colon delimited, which is used in countries where a comma is
used as a decimal point, in which case you should use read.csv2
instead).  So,  is your data definitely as clean as you think. Have you
looked at the data in a text editor?  What are the dimensions of the
resulting data frame?

On 19/09/06, Mesomeris, Spyros [CIR] <spyros.mesomeris at citigroup.com>
wrote:
> Dear R helpers,
>
> I am trying to read a CSV file in R called EUROPE (originally an Excel

> file which I have saved as a CSV file) using the command
>
> EUROPEDATA <- read.csv("EUROPE.csv")
>
> EUROPE.csv is basically a matrix of dimension 440*44, and has a line 
> of headers, i.e. each column has a name.
>
> Using read.csv I can't load the data into R properly. Although the 
> first 20 columns or so are read in properly, some of the data from the

> remaining columns are missing, eg. For Column 29, the loaded file 
> cannot read the first 120 observations and puts NA in their place, 
> whereas the rest of the column is read in properly! I find this really
strange.
>
> I have tried to use read.table and scan commands as well, with the 
> header = T option, but still the problem is not solved. Please note 
> the columns are formatted in the same way, and contain numbers (apart 
> form the header row).
>
> Does anybody have any idea how I can read the data properly into R?
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


--
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From ligges at statistik.uni-dortmund.de  Tue Sep 19 13:15:15 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 19 Sep 2006 13:15:15 +0200
Subject: [R] Reading a file in R
In-Reply-To: <0D9D1AC09BC016428D7597A1716AFF710D990E@Exukmb73.eur.nsroot.net>
References: <0D9D1AC09BC016428D7597A1716AFF710D990E@Exukmb73.eur.nsroot.net>
Message-ID: <450FD143.3060701@statistik.uni-dortmund.de>

Mesomeris, Spyros [CIR] wrote:

> Thanks David,
> 
> It has actually worked, the problem was the formatting of the N/A values
> in Excel. R apparently doesn't like to see #N/A that Excel produces if a
> formula cannot be returned. So, saving the file as csv (comma delimited)
> file and removing all the #N/A observations, leaving those cells empty,
> and then uploading the file into R, works fine. 
> 
> I hope this is helpful for other users as well


Well, you could also tell R to read it by setting arguemnts "na.strings" 
and "comment.char" appropriately.

Uwe Ligges

> Thanks again
>  
> 
> -----Original Message-----
> From: David Barron [mailto:mothsailor at googlemail.com] 
> Sent: 19 September 2006 11:33
> To: Mesomeris, Spyros [CIR]; r-help
> Subject: Re: [R] Reading a file in R
> 
> I think that command should work (assuming that it is *comma* rather
> than semi-colon delimited, which is used in countries where a comma is
> used as a decimal point, in which case you should use read.csv2
> instead).  So,  is your data definitely as clean as you think. Have you
> looked at the data in a text editor?  What are the dimensions of the
> resulting data frame?
> 
> On 19/09/06, Mesomeris, Spyros [CIR] <spyros.mesomeris at citigroup.com>
> wrote:
> 
>>Dear R helpers,
>>
>>I am trying to read a CSV file in R called EUROPE (originally an Excel
> 
> 
>>file which I have saved as a CSV file) using the command
>>
>>EUROPEDATA <- read.csv("EUROPE.csv")
>>
>>EUROPE.csv is basically a matrix of dimension 440*44, and has a line 
>>of headers, i.e. each column has a name.
>>
>>Using read.csv I can't load the data into R properly. Although the 
>>first 20 columns or so are read in properly, some of the data from the
> 
> 
>>remaining columns are missing, eg. For Column 29, the loaded file 
>>cannot read the first 120 observations and puts NA in their place, 
>>whereas the rest of the column is read in properly! I find this really
> 
> strange.
> 
>>I have tried to use read.table and scan commands as well, with the 
>>header = T option, but still the problem is not solved. Please note 
>>the columns are formatted in the same way, and contain numbers (apart 
>>form the header row).
>>
>>Does anybody have any idea how I can read the data properly into R?
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide 
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>>
> 
> 
> 
> --
> =================================
> David Barron
> Said Business School
> University of Oxford
> Park End Street
> Oxford OX1 1HP
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From H.Koffijberg at umcutrecht.nl  Tue Sep 19 14:04:27 2006
From: H.Koffijberg at umcutrecht.nl (Koffijberg, H.)
Date: Tue, 19 Sep 2006 14:04:27 +0200
Subject: [R] How to interpret these results from a simple gamma-frailty model
Message-ID: <07782D954353A543BE9291EC2155C07702BF3ECA@EX01.ds.umcutrecht.nl>


Dear R users,

I'm trying to fit a gamma-frailty model on a simulated dataset, with 6 covariates, and I'm running into some results I do not understand. I constructed an example from my simulation code, where I fit a coxph model without frailty (M1) and with frailty (M2) on a  number of data samples with a varying degree of heterogeneity (I'm running R 2.3.1, running takes ~1 min).

library(survival); set.seed(10000)
lambda <- 0.01			# Exp. hazard rate
# Beta coefficients for Age,TC,HDLC,SBP,Diab,Smok
beta <- c(0.0483,0.0064,-0.0270,0.0037,0.4284,0.5234)   
n <- 1000; Ngrp <- 2;		# Nr patients, Nr frailty groups	
# Thetas for gamma-frailty  
thetaset <- c(1,2,10,100); Ntheta <- length(thetaset);	
						
# Define the simulated population
age <-rnorm(n,48.6,11.7);tc<-rnorm(n,200,30)
hdlc<-rnorm(n,47,6);sbp<-rnorm(n,135,6) 
rtmp <- runif(0,1,n=n); diab <- rep(0, n); diab[rtmp < 0.05223] <- 1;
rtmp <- runif(0,1,n=n); smok <- rep(0, n); smok[rtmp < 0.40458] <- 1;
# Storage for the estimated coefficients in both models
cf <- 0; length(cf) <- 2*Ntheta*6; dim(cf) <- c(Ntheta,2,6); 

for (i in 1:Ntheta)
{
	l = thetaset[i]
	v=rep(rgamma(n/2,shape=l,scale=1/l),2)		# Shared frailty values
	print(paste("Theta = ",l, " variance in introduced frailty v   = ",var(v)))
	id=rep(seq(1:(n/2)),2); 			# Frailty id's, 
	c <- rep(1,n); 			 		# censoring flags
	r <- runif(n,0,1);				# For random variate generation
	t <- (-1*log(r)) / (lambda*v*exp(age*beta[1]+tc*beta[2]+hdlc*beta[3]+sbp*beta[4]+diab*beta[5]+smok*beta[6])) 
	fitM1=coxph(Surv(t,c)~age+tc+hdlc+sbp+diab+smok); 	# Model 1, no frailty
	cf[i,1,] <- round(fitM1$coef,6)				# Model 1 coefficients
	fitM2=coxph(Surv(t,c)~age+tc+hdlc+sbp+diab+smok+
		frailty(id,dist="gamma",sparse=T,method="em"))  # Model 2, frailty
	cf[i,2,] <- round(fitM2$coef,6)				# Model 2, coefficients
	
	# store estimated Variance of random effect
	varre=fitM2$history$frailty$history[length(fitM2$history$frailty$history[,1])]  		
	print(paste("Theta = ",l, " variance in estimated random effect= ",varre))
	print(paste("Theta = ",l, " variance in estimated ind. frailty = ", var(exp(fitM2$frail))))
	print(paste("Theta=",l," org beta ",beta," M1: ",cf[i,1,], " M2: ", cf[i,2,]))
	# Calculate the 'actual' 10-year risk and the risk estimated using M1 and M2
	estv <- exp(rep(fitM2$frail,2))		# Individual frailty values from M2
	# Original risk score
	rs<- age*beta[1]+tc*beta[2]+hdlc*beta[3]+sbp*beta[4]+diab*beta[5]+smok*beta[6] 		
	pevent <- exp(-10*lambda)^(exp(rs))		  	# Original 10-year risk
	rsM1<- (cf[i,1,1]*age+cf[i,1,2]*tc+cf[i,1,3]*hdlc+cf[i,1,4]*sbp+cf[i,1,5]*diab+cf[i,1,6]*smok)
	peventM1 <- exp(-10*lambda)^(exp(rsM1))
	rsM2<- (cf[i,1,1]*age+cf[i,1,2]*tc+cf[i,1,3]*hdlc+cf[i,1,4]*sbp+cf[i,1,5]*diab+cf[i,1,6]*smok)
	peventM2 <- exp(-10*lambda)^(estv*exp(rsM1))
	# Proportion of more accurate predictions
	pred <- sum(abs(pevent-peventM2) < abs(pevent-peventM1))/n	
	print(paste("Theta = ",l," proportion pred. M2 more accurate than M1 = ",pred)); 
}


corresponding output:

[1] "Theta =  1  variance in introduced frailty v   =  0.948105787326522"
[1] "Theta =  1  variance in estimated random effect=  1.04435029128552"
[1] "Theta =  1  variance in estimated ind. frailty =  0.520078219281493"
[1] "Theta= 1  org beta  0.0483  M1:  0.025015  M2:  0.045987"  
[2] "Theta= 1  org beta  0.0064  M1:  0.002274  M2:  0.005178"  
[3] "Theta= 1  org beta  -0.027  M1:  -0.014239  M2:  -0.028824"
[4] "Theta= 1  org beta  0.0037  M1:  0.00071  M2:  0.010826"   
[5] "Theta= 1  org beta  0.4284  M1:  0.421602  M2:  0.648227"  
[6] "Theta= 1  org beta  0.5234  M1:  0.360069  M2:  0.593551"  
[1] "Theta =  1  proportion pred. M2 more accurate than M1 =  0.448"
[1] "Theta =  2  variance in introduced frailty v   =  0.515806773271924"
[1] "Theta =  2  variance in estimated random effect=  0.644954876113229"
[1] "Theta =  2  variance in estimated ind. frailty =  0.277209957022078"
[1] "Theta= 2  org beta  0.0483  M1:  0.033213  M2:  0.046216"  
[2] "Theta= 2  org beta  0.0064  M1:  0.005633  M2:  0.009491"  
[3] "Theta= 2  org beta  -0.027  M1:  -0.010048  M2:  -0.016658"
[4] "Theta= 2  org beta  0.0037  M1:  -0.006032  M2:  -0.003639"
[5] "Theta= 2  org beta  0.4284  M1:  0.402992  M2:  0.403226"  
[6] "Theta= 2  org beta  0.5234  M1:  0.457665  M2:  0.691871"  
[1] "Theta =  2  proportion pred. M2 more accurate than M1 =  0.482"
[1] "Theta =  10  variance in introduced frailty v   =  0.104197974543906"
[1] "Theta =  10  variance in estimated random effect=  0.00331174766884151"
[1] "Theta =  10  variance in estimated ind. frailty =  2.30691702541282e-05"
[1] "Theta= 10  org beta  0.0483  M1:  0.048761  M2:  0.048885" 
[2] "Theta= 10  org beta  0.0064  M1:  0.002817  M2:  0.002833" 
[3] "Theta= 10  org beta  -0.027  M1:  -0.024016  M2:  -0.02411"
[4] "Theta= 10  org beta  0.0037  M1:  0.002227  M2:  0.002206" 
[5] "Theta= 10  org beta  0.4284  M1:  0.36471  M2:  0.36624"   
[6] "Theta= 10  org beta  0.5234  M1:  0.531466  M2:  0.532811" 
[1] "Theta =  10  proportion pred. M2 more accurate than M1 =  0.604"
[1] "Theta =  100  variance in introduced frailty v   =  0.0101553285954112"
[1] "Theta =  100  variance in estimated random effect=  0.0245876188637748"
[1] "Theta =  100  variance in estimated ind. frailty =  0.00113624325597910"
[1] "Theta= 100  org beta  0.0483  M1:  0.051439  M2:  0.052775"  
[2] "Theta= 100  org beta  0.0064  M1:  0.002654  M2:  0.002834"  
[3] "Theta= 100  org beta  -0.027  M1:  -0.025281  M2:  -0.025606"
[4] "Theta= 100  org beta  0.0037  M1:  0.005468  M2:  0.005903"  
[5] "Theta= 100  org beta  0.4284  M1:  0.467735  M2:  0.48221"   
[6] "Theta= 100  org beta  0.5234  M1:  0.673852  M2:  0.683147"  
[1] "Theta =  100  proportion pred. M2 more accurate than M1 =  0.564"
Warning message:
Inner loop failed to coverge for iterations 2 3 in: coxpenal.fit(X, Y, strats, offset, init = init, control, weights = weights,  
> 

I don't understand the following:
[1] The variance of the estimated individual frailty values is always much lower than both the original variance and the estimated variance of the random effect. Why is this ? Obviously the variance of the random effect is not calculated directly from the individual frailties after exponentiating them.

[2] Random  effects that are not very large are not even picked up by M2, e.g. theta = 10, yields a variance of ~1/10 in the frailty distribution used to set up the data, however, the estimated variance of that random effect equals only 0.003311. Why is frailty not picked up in this case ? Is it just that the variance of the heterogeneity inserted into the data is too small ?

[3] When I compare the predictive abilities of M1 and M2, by calculating the differences, e.g. between predicted 10-year risks, and determine the proportion of more accurate predictions over the complete sample, when using M2 instead of M1, M2 does not perform relevantly better than M1. For theta = 1 or 2 (substantial heterogeneity) M2 performs even worst than M1, while it should be able to take this heterogeneity into account and model M1 cannot do that. Can anyone explain why using M2 does not lead to better predictions in this situation ? 

[4] Intuitively it seems to me that in absence of heterogeneity (or with very little heterogeneity, e.g. theta=100) both models should be able to estimate the regression coefficients for the 6 covariates accurately, given enough events. With n=1000, the estimated coefficients are still very inaccurate (see e.g. theta=100, beta 2, 4 and 6) even though we have over 150 events/parameter. Even with lots of events (e.g. 50000) the estimates get more accurately but still are way from perfect. Is there an underlying reason for this slow convergence ? 

Is there anyone who can clarify some of these results ? Many thanks in advance for your help.
Erik.

=====================================================
Erik Koffijberg, MSc
Julius Center for Health Sciences and Primary Care
University Medical Center Utrecht Str. 6.131
P.O.Box 85500, 3508 GA Utrecht, The Netherlands
Tel: 	+31 30 250 3013,
Fax:    +31 30 250 5480
E-mail: H.Koffijberg at umcutrecht.nl


From robert.king at newcastle.edu.au  Tue Sep 19 14:16:44 2006
From: robert.king at newcastle.edu.au (Robert King)
Date: Tue, 19 Sep 2006 22:16:44 +1000
Subject: [R] R CMD check fails at package dependencies check on Fedora
 Core 5, works on other systems
In-Reply-To: <450FC020.4070304@newcastle.edu.au>
References: <200609191032.04483.Robert.King@newcastle.edu.au>
	<x2ac4wqqqs.fsf@turmalin.kubism.ku.dk>
	<450FC020.4070304@newcastle.edu.au>
Message-ID: <450FDFAC.6070209@newcastle.edu.au>

Here is another thing that might help work out what is happening.  If I 
use --no-install, ade4 actually fails as well, in the same way as zipfR.

  [Desktop]$ R CMD check --no-install ade4
  * checking for working latex ... OK
  * using log directory '/home/rak776/Desktop/ade4.Rcheck'
  * using Version 2.3.1 (2006-06-01)
  * checking for file 'ade4/DESCRIPTION' ... OK
  * this is package 'ade4' version '1.4-1'
  * checking if this is a source package ... OK
  * checking package directory ... OK
  * checking for portable file names ... OK
  * checking for sufficient/correct file permissions ... OK
  * checking DESCRIPTION meta-information ... ERROR

  [Desktop]$ R CMD check --no-install zipfR
  * checking for working latex ... OK
  * using log directory '/home/rak776/Desktop/zipfR.Rcheck'
  * using Version 2.3.1 (2006-06-01)
  * checking for file 'zipfR/DESCRIPTION' ... OK
  * checking extension type ... Package
  * this is package 'zipfR' version '0.6-0'
  * checking if this is a source package ... OK
  * checking package directory ... OK
  * checking for portable file names ... OK
  * checking for sufficient/correct file permissions ... OK
  * checking DESCRIPTION meta-information ... ERROR

Robert King wrote:
> No output from the tools command
> 
>> Robert King <Robert.King at newcastle.edu.au> writes:
>>>
>>> However, R CMD check seems to fail for packages with no dependencies 
>>> at the dependencies check stage.
>>>
>>> I've tried this with two packages, my own gld and also zipfR.  Both 
>>> fail in the same way.
>>>
>>> [Desktop]$ R CMD check zipfR_0.6-0.tar.gz
>>> * checking for working latex ... OK

...

>>> * checking package dependencies ... ERROR
>>>
>>> Both these work on two other systems - one a debian/sarge AMD64 and a 
>>> debian/sarge i386
>>
>> And at least the former is quite happy on a vanilla FC5/ R2.3.1
>>
>> What happens if you fire up R --quiet and submit
>>
>>  tools:::.check_package_depends("zipfR")
>>
>> (after unpacking the tarball, of course)? This gives no output for me.
> 
> Similarly, no output
> 
>>> A package with dependencies checks for dependencies correctly - for 
>>> example ade4:
>>>
>>> Desktop]$ R CMD check ade4_1.4-1.tar.gz
...
>>> Does anyone have ideas on what is going wrong?
>>>
>>> Regards,
>>> Robert King
>


From malfonso at telecom.com.co  Tue Sep 19 14:18:12 2006
From: malfonso at telecom.com.co (Mario Alfonso Morales Rivera)
Date: Tue, 19 Sep 2006 07:18:12 -0500
Subject: [R] odfWeave help
Message-ID: <450FE004.3080202@telecom.com.co>

Hi R users

I haven't  run odfWeave example, R give me:

Setting wd to
C:\DOCUME~1\MARIOM~1\CONFIG~1\Temp\Rtmph2Nzqb/odfWeave19070343633
  Copying  C:/ARCHIV~1/R/R-23~1.1/library/odfWeave/examples/simple.odt
  Decompressing ODF file using unzip -o "simple.odt"
Erro en odfWeave(demoFile, outputFile) : Error unzipping file
Adem?s: Warning message: unzip no encontrado

I have installed WinZip, WinRar and I have downloaded unzip too, and
uncompress into  "C:/Archivos de programa/unzip"

I don?t know how configure odfWeave so it find unzip

can you help me ?????


From sachinj.2006 at yahoo.com  Tue Sep 19 14:22:58 2006
From: sachinj.2006 at yahoo.com (Sachin J)
Date: Tue, 19 Sep 2006 05:22:58 -0700 (PDT)
Subject: [R] prediction interval for new value
In-Reply-To: <006001c6db3f$77111a70$711f210a@gne.windows.gene.com>
Message-ID: <20060919122258.20295.qmail@web37608.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060919/d0af04e0/attachment.pl 

From gianna.monti at unimib.it  Tue Sep 19 14:28:21 2006
From: gianna.monti at unimib.it (Gianna Monti)
Date: Tue, 19 Sep 2006 14:28:21 +0200
Subject: [R] help on dirichlet distribution
Message-ID: <035001c6dbe7$1848a460$44968495@statistica.unimib.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060919/d512017d/attachment.pl 

From ggrothendieck at gmail.com  Tue Sep 19 14:39:12 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 19 Sep 2006 08:39:12 -0400
Subject: [R] printing a generated function crashes R
In-Reply-To: <450F9E4A.3080105@wias-berlin.de>
References: <4508129D.4040303@wias-berlin.de>
	<x2u03brbxa.fsf@viggo.kubism.ku.dk> <450F9E4A.3080105@wias-berlin.de>
Message-ID: <971536df0609190539x1351acadx5f96ce97524c3205@mail.gmail.com>

You could alternately use substitute:


make.function2 <- function() {
	fun <- function(x, y) x*y+z
	body(fun) <- do.call(substitute, list(body(fun), list(z = 1)))
	fun
}

ff <- make.function2()
ff(3,2) # 7


On 9/19/06, Mstislav Elagin <elagin at wias-berlin.de> wrote:
> Peter Dalgaard wrote:
> > Mstislav   Elagin <elagin at wias-berlin.de> writes:
> >
> >> Dear All,
> >>
> >> the last expression in the following code snippet crashes R (version
> >> 2.3.1 on Windows XP) when run interactively:
> >>
> >> make.bad.function <- function(kind)
> >> {
> >>    zz <- switch(kind,
> >>                 "1" = 1,
> >>                 "2" = 2)
> >>
> >>    stopifnot( !is.null(zz) )
> >>
> >>    eval( bquote( function(x)
> >>                 {
> >>                   x + .(zz)
> >>                 }))
> >> }
> >>
> >> # bad.function <- make.bad.function("5") ## error as expected
> >>
> >> bad.function <- make.bad.function("1")
> >> print(bad.function(10)) ## -> 11
> >>
> >> bad.function <- make.bad.function("2")
> >> print(bad.function(10)) ## -> 12
> >>
> >> bad.function            ## this works if the code is source()'d
> >> print(bad.function)     ## oops!
> >>
> >> However, it does work (i.e. prints the body of bad.function) if run
> >> non-interactively
> >> (R --vanilla < bad-function.R).
> >>
> >> Any ideas why this happens?
> >
> > Well, bquote seems to be doing nasty things if passed an expression with a
> > function inside:
> >
> >> f <- bquote(function(x) {
> > +     x + 1
> > + }
> > + )
> >> f
> > function(x) {
> >     x + 1
> > }
> >> eval(f)
> > ?
> > ?H~
> >
> > ?H~
> >
> > Program received signal SIGSEGV, Segmentation fault.
> >
> >
> > I think the story is that the source attribute is getting messed up.
> >
> >> z <- eval(f)
> >> attr(z,"source")
> > "function(x) {"("x+1}")
> >> z
> > ?X~
> > ?X~
> > ..poof..
>
> Hallo,
>
> after having played a bit more, I found out that the reason is not, or
> at least not only the source attribute.
>
> If we try to construct a function of more than one argument in the same
> way as a function of no or one argument, we get an error message:
>
> make.bad.function <- function()
> {
>   zz <- 1
>
>   eval( bquote( function(x, y)
>                {
>                  x*y + .(zz)
>                }))
> }
>
> bad.fun <- make.bad.function()
>
>  > Error in eval(expr, envir, enclos) : invalid formal argument list for
>  > "function"
>
> However, if, following an older post by Brian Ripley, we specify the
> list of formal arguments explicitly, everything works fine (albeit looks
> uglier):
>
> make.function <- function()
> {
>   zz <- 1
>   fun <-
>     eval( bquote( function()
>                  {
>                    x*y + .(zz)
>                  }))
>   formals(fun) <- alist(x=, y=)
>   return(fun)
> }
> fun <- make.function()
> fun(3, 2) ## -> 7
>
> Printing the function works, too:
>
> fun
>
>  > function (x, y)
>  > {
>  >    x * y + 1
>  > }
>  > <environment: 01A5884C>
>
> My earlier example with the function of one argument works fine when
> rewritten in the same spirit.
>
> In my opinion, bquote behaves "inhomogeneously" in the sense that the
> definition of the functions taking no of one params differs from that of
> the functions taking more params. I wonder whether such behaviour of
> bquote is a bug and should be reported.
>
> Have a nice day
> Mstislav Elagin
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Max.Kuhn at pfizer.com  Tue Sep 19 14:41:45 2006
From: Max.Kuhn at pfizer.com (Kuhn, Max)
Date: Tue, 19 Sep 2006 08:41:45 -0400
Subject: [R] odfWeave help
Message-ID: <71257D09F114DA4A8E134DEAC70F25D306252F8E@groamrexm03.amer.pfizer.com>

Alfonso,

It sounds like unzip and zip are not in your path (as opposed to odfWeave needing additional configuration). The error message that you received is generated when odfWeave is invoked. It checks to see if unzip can be used at the command line.

After googling, I found a basic link for setting your path:

http://www.xmission.com/~comphope/issues/ch000549.htm

You'll want to add the directory where unzip and zip live to this path.

Max



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Mario Alfonso Morales Rivera
Sent: Tuesday, September 19, 2006 8:18 AM
To: r-help at stat.math.ethz.ch
Subject: [R] odfWeave help

Hi R users

I haven't  run odfWeave example, R give me:

Setting wd to
C:\DOCUME~1\MARIOM~1\CONFIG~1\Temp\Rtmph2Nzqb/odfWeave19070343633
  Copying  C:/ARCHIV~1/R/R-23~1.1/library/odfWeave/examples/simple.odt
  Decompressing ODF file using unzip -o "simple.odt"
Erro en odfWeave(demoFile, outputFile) : Error unzipping file
Adem?s: Warning message: unzip no encontrado

I have installed WinZip, WinRar and I have downloaded unzip too, and
uncompress into  "C:/Archivos de programa/unzip"

I don?t know how configure odfWeave so it find unzip

can you help me ?????

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.
----------------------------------------------------------------------
LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}


From pieterprovoost at gmail.com  Tue Sep 19 14:53:56 2006
From: pieterprovoost at gmail.com (Pieter Provoost)
Date: Tue, 19 Sep 2006 14:53:56 +0200
Subject: [R] bubble plot problems
Message-ID: <450FE864.3010207@gmail.com>

Hi,

I'm having some problems with a bubble plot (ps package). I don't want 
tick marks on all four sides (just two), I want to have a smaller font 
size, and I would like to be able to define bubble sizes shown in the 
legend (now it shows 0, 0, 0, 9.747 and 4265.757 which is not really 
convenient. Passing some of the standard plot arguments didn't help (in 
fact, nothing changed).

bubble(points,fill=FALSE,aspect='fill',main='macrofauna',col='black',scales=list(y=list(tick.number=numberspecies,labels=rev(names(data)))),ylim=c(1:numberspecies))

Thanks,
Piet


From ggrothendieck at gmail.com  Tue Sep 19 15:03:06 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 19 Sep 2006 09:03:06 -0400
Subject: [R] printing a generated function crashes R
In-Reply-To: <971536df0609190539x1351acadx5f96ce97524c3205@mail.gmail.com>
References: <4508129D.4040303@wias-berlin.de>
	<x2u03brbxa.fsf@viggo.kubism.ku.dk> <450F9E4A.3080105@wias-berlin.de>
	<971536df0609190539x1351acadx5f96ce97524c3205@mail.gmail.com>
Message-ID: <971536df0609190603h595f49fbtc2d79bfdbc0aa174@mail.gmail.com>

In fact, in this example its not really even necessary to manipulate
the function since the new function will find z in its environment
(which is the body of make.function3):

make.function3 <- function() {
	z <- 1
	function(x, y) x*y+z
}

g <- make.function2()
g(3,2) # 7


On 9/19/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> You could alternately use substitute:
>
>
> make.function2 <- function() {
>        fun <- function(x, y) x*y+z
>        body(fun) <- do.call(substitute, list(body(fun), list(z = 1)))
>        fun
> }
>
> ff <- make.function2()
> ff(3,2) # 7
>
>
> On 9/19/06, Mstislav Elagin <elagin at wias-berlin.de> wrote:
> > Peter Dalgaard wrote:
> > > Mstislav   Elagin <elagin at wias-berlin.de> writes:
> > >
> > >> Dear All,
> > >>
> > >> the last expression in the following code snippet crashes R (version
> > >> 2.3.1 on Windows XP) when run interactively:
> > >>
> > >> make.bad.function <- function(kind)
> > >> {
> > >>    zz <- switch(kind,
> > >>                 "1" = 1,
> > >>                 "2" = 2)
> > >>
> > >>    stopifnot( !is.null(zz) )
> > >>
> > >>    eval( bquote( function(x)
> > >>                 {
> > >>                   x + .(zz)
> > >>                 }))
> > >> }
> > >>
> > >> # bad.function <- make.bad.function("5") ## error as expected
> > >>
> > >> bad.function <- make.bad.function("1")
> > >> print(bad.function(10)) ## -> 11
> > >>
> > >> bad.function <- make.bad.function("2")
> > >> print(bad.function(10)) ## -> 12
> > >>
> > >> bad.function            ## this works if the code is source()'d
> > >> print(bad.function)     ## oops!
> > >>
> > >> However, it does work (i.e. prints the body of bad.function) if run
> > >> non-interactively
> > >> (R --vanilla < bad-function.R).
> > >>
> > >> Any ideas why this happens?
> > >
> > > Well, bquote seems to be doing nasty things if passed an expression with a
> > > function inside:
> > >
> > >> f <- bquote(function(x) {
> > > +     x + 1
> > > + }
> > > + )
> > >> f
> > > function(x) {
> > >     x + 1
> > > }
> > >> eval(f)
> > > ?
> > > ?H~
> > >
> > > ?H~
> > >
> > > Program received signal SIGSEGV, Segmentation fault.
> > >
> > >
> > > I think the story is that the source attribute is getting messed up.
> > >
> > >> z <- eval(f)
> > >> attr(z,"source")
> > > "function(x) {"("x+1}")
> > >> z
> > > ?X~
> > > ?X~
> > > ..poof..
> >
> > Hallo,
> >
> > after having played a bit more, I found out that the reason is not, or
> > at least not only the source attribute.
> >
> > If we try to construct a function of more than one argument in the same
> > way as a function of no or one argument, we get an error message:
> >
> > make.bad.function <- function()
> > {
> >   zz <- 1
> >
> >   eval( bquote( function(x, y)
> >                {
> >                  x*y + .(zz)
> >                }))
> > }
> >
> > bad.fun <- make.bad.function()
> >
> >  > Error in eval(expr, envir, enclos) : invalid formal argument list for
> >  > "function"
> >
> > However, if, following an older post by Brian Ripley, we specify the
> > list of formal arguments explicitly, everything works fine (albeit looks
> > uglier):
> >
> > make.function <- function()
> > {
> >   zz <- 1
> >   fun <-
> >     eval( bquote( function()
> >                  {
> >                    x*y + .(zz)
> >                  }))
> >   formals(fun) <- alist(x=, y=)
> >   return(fun)
> > }
> > fun <- make.function()
> > fun(3, 2) ## -> 7
> >
> > Printing the function works, too:
> >
> > fun
> >
> >  > function (x, y)
> >  > {
> >  >    x * y + 1
> >  > }
> >  > <environment: 01A5884C>
> >
> > My earlier example with the function of one argument works fine when
> > rewritten in the same spirit.
> >
> > In my opinion, bquote behaves "inhomogeneously" in the sense that the
> > definition of the functions taking no of one params differs from that of
> > the functions taking more params. I wonder whether such behaviour of
> > bquote is a bug and should be reported.
> >
> > Have a nice day
> > Mstislav Elagin
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From edd at debian.org  Tue Sep 19 15:09:23 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 19 Sep 2006 08:09:23 -0500
Subject: [R] [Rpy]  currency or stock trading strategy
In-Reply-To: <d2095b8c0609181152m2b12a590vba51c19ee2d7dac9@mail.gmail.com>
References: <d2095b8c0609171437o496256aam53bbbcc25d1acf43@mail.gmail.com>
	<450E582F.6000902@pburns.seanet.com>
	<d2095b8c0609181152m2b12a590vba51c19ee2d7dac9@mail.gmail.com>
Message-ID: <17679.60419.123844.491630@basebud.nulle.part>


[ rpy-help CC removed. Please don't blanket-cross post. ]

On 18 September 2006 at 11:52, Darren Weber wrote:
| Hi Patrick,
| 
| thanks for pointing me to your work and Rmetrics.
| 
| I have a few questions on my mind right now.  Do you have methods for
| automatic download of price quote histories?  I can use python to get
| XML data on FOREX price quotes from the NYRB and other sites.
| Together with Rpy and matplotlib, that data download could form the
| basis for an open source technical analysis platform.  I still need
| some way to get open source price quote histories for stocks and
| options.  Any ideas?

I hardly know where to start as you obviously haven't done a whole lot of
homework, or are seriously Google-challenged.  But for starters, consider
that

i)   the get.hist.quote() function in R's timeseries package downloads and
     plot anything Yahoo! Finance provides as data. You have the code right
     there to study and extend,

ii)  there are extensive XML facilities provided in R's XML package,

iii) for a non-R solution, my Perl-based 'beancounter' application has been
     available for more than half a decade to automatically download,
     database (to your choice of PostgreSQL, MySQL, SQLite) and analyse
     market prices.  Getting the data from the db into R is then a pretty
     low hanging fruit.

Lastly, as Patrick told you, r-sig-finance may be more on target. You need to
subscribe there before you can post, but the list archives are of course open
for perusal.

Hope this helps, Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From mothsailor at googlemail.com  Tue Sep 19 16:37:40 2006
From: mothsailor at googlemail.com (David Barron)
Date: Tue, 19 Sep 2006 15:37:40 +0100
Subject: [R] bubble plot problems
In-Reply-To: <815b70590609190737gb71eed9n8af7e6e481a2b7b@mail.gmail.com>
References: <450FE864.3010207@gmail.com>
	<815b70590609190737gb71eed9n8af7e6e481a2b7b@mail.gmail.com>
Message-ID: <815b70590609190737m7794bdaagf15dea6ddee5e352@mail.gmail.com>

The legend can be controlled with the key.entries parameter.  I don't
know about the tick marks, though note that bubble calls xyplot in the
lattice package, not the "standard" plot function.

On 19/09/06, Pieter Provoost <pieterprovoost at gmail.com> wrote:
> Hi,
>
> I'm having some problems with a bubble plot (ps package). I don't want
> tick marks on all four sides (just two), I want to have a smaller font
> size, and I would like to be able to define bubble sizes shown in the
> legend (now it shows 0, 0, 0, 9.747 and 4265.757 which is not really
> convenient. Passing some of the standard plot arguments didn't help (in
> fact, nothing changed).
>
> bubble(points,fill=FALSE,aspect='fill',main='macrofauna',col='black',scales=list(y=list(tick.number=numberspecies,labels=rev(names(data)))),ylim=c(1:numberspecies))
>
> Thanks,
> Piet
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


--
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From vito_ricci at yahoo.com  Tue Sep 19 16:42:52 2006
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Tue, 19 Sep 2006 16:42:52 +0200 (CEST)
Subject: [R] New contribute about R & Regression
Message-ID: <20060919144253.84732.qmail@web36115.mail.mud.yahoo.com>

Dear UseRs,

you can find on CRAN web site my last contribute about
R & regression techniques:

http://cran.r-project.org/doc/contrib/Ricci-regression-it.pdf

It's in Italian language.

Regards.

Vito Ricci


Se non ora, quando?
Se non qui, dove?
Se non tu, chi?


From AnupTyagi at yahoo.com  Tue Sep 19 16:44:23 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Tue, 19 Sep 2006 14:44:23 +0000 (UTC)
Subject: [R] multiple density function
References: <6358504.post@talk.nabble.com>
Message-ID: <loom.20060919T164154-982@post.gmane.org>

march <marcella.marinelli <at> uniroma1.it> writes:

> 
> 
> Hi everybody
> I'm new in R so the question will be easy for you
> I'm running multiple density functions taking account of the following
> conditions:
> mean=seq(10,1,length=10)        
> var=seq(3,1,length=10)
> 
> How can I describe the density functions on the same chart?
> thanks
> Marcella

Hi Marcella, I am not sure of what you are specifically trying to do, but if you
want more than one set of points on the same graph (chart) use "points" after
"plot". See examples in the graphics part of R documentation.
e.g.

> plot(...)
> points(...)
 
Anupam.


From mothsailor at googlemail.com  Tue Sep 19 17:03:00 2006
From: mothsailor at googlemail.com (David Barron)
Date: Tue, 19 Sep 2006 16:03:00 +0100
Subject: [R] multiple density function
In-Reply-To: <loom.20060919T164154-982@post.gmane.org>
References: <6358504.post@talk.nabble.com>
	<loom.20060919T164154-982@post.gmane.org>
Message-ID: <815b70590609190803w2265286ahd07bd7af2d9e1aaa@mail.gmail.com>

Would this do what you want:

>  m=seq(10,1,length=10)
>  v=seq(3,1,length=10)
> x <- seq(-5,20,length=500)
> plot(x,dnorm(x,m[1],v[1]))
> for (i in 1:10) plot(x,dnorm(x,m[i],v[i]),type="l",col=i)

On 19/09/06, Anupam Tyagi <AnupTyagi at yahoo.com> wrote:
> march <marcella.marinelli <at> uniroma1.it> writes:
>
> >
> >
> > Hi everybody
> > I'm new in R so the question will be easy for you
> > I'm running multiple density functions taking account of the following
> > conditions:
> > mean=seq(10,1,length=10)
> > var=seq(3,1,length=10)
> >
> > How can I describe the density functions on the same chart?
> > thanks
> > Marcella
>
> Hi Marcella, I am not sure of what you are specifically trying to do, but if you
> want more than one set of points on the same graph (chart) use "points" after
> "plot". See examples in the graphics part of R documentation.
> e.g.
>
> > plot(...)
> > points(...)
>
> Anupam.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From bbands at gmail.com  Tue Sep 19 17:06:08 2006
From: bbands at gmail.com (BBands)
Date: Tue, 19 Sep 2006 08:06:08 -0700
Subject: [R] currency or stock trading strategy
In-Reply-To: <d2095b8c0609171437o496256aam53bbbcc25d1acf43@mail.gmail.com>
References: <d2095b8c0609171437o496256aam53bbbcc25d1acf43@mail.gmail.com>
Message-ID: <6e8360ad0609190806h63411d4fqe61aad038c9733a1@mail.gmail.com>

On 9/17/06, Darren Weber <darrenleeweber at gmail.com> wrote:
> Hi,
>
> are there any good charting and analysis tools for use with
> currencies, stocks, etc. in R?  I have some tools to download currency
> data from the NYFRB using python and XML.  Can we get and parse an XML
> download using R?  Can we have interaction in R plots?  Does anyone
> use R for back-testing trading strategies?  Are there any forums for
> discussion of using R for this specific purpose (apart from this
> general list)?  Is anyone aware of any general open-source
> developments for these purposes (I don't see any from GNU or google
> searches)?
>
> Take care, Darren

I hardly know where to start either. I use R, Python, rpy, gnuplot-py
and gnuplot together and am very happy with the result. I find that
each piece has it own strengths and together they provide a very
powerful solution. I am currently working on integrating TA-lib into
this mix via a project started by Andrea Malagoli--see the
r-sig-finance archives.

http://rpy.sourceforge.net/
http://www.gnuplot.info/
http://gnuplot-py.sourceforge.net/
http://ta-lib.org/

     jab
-- 
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.


From gzhao1 at ural.wustl.edu  Tue Sep 19 17:17:18 2006
From: gzhao1 at ural.wustl.edu (Guoyan Zhao)
Date: Tue, 19 Sep 2006 10:17:18 -0500 (CDT)
Subject: [R] calculate AUC for ROC curve
Message-ID: <Pine.LNX.4.44.0609191010150.16043-100000@ural.wustl.edu>

Dear everyone,

I have only tpr (true positive rate) and fpr (false positive rate) which 
were used to plot the ROC curve. Is there any way I can calculate the AUC 
for this curve using just this information? Any help is highly 
appreciated. 

Isabel


From AnupTyagi at yahoo.com  Tue Sep 19 17:11:09 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Tue, 19 Sep 2006 15:11:09 +0000 (UTC)
Subject: [R] predict with logistic regression
References: <96507a8e0609150122g1487442clfe98df164169063@mail.gmail.com>
Message-ID: <loom.20060919T170915-111@post.gmane.org>

Jan Sabee <jan.sabee <at> gmail.com> writes:

> I know that is probability of predict for new dataset.
> My question is how can I know each probability according to class (sore).
> I mean that I need the result of predit something like (M=1, F=0):
>  1  2  3  4  5  6  7  8  9  10
>  1  0  0  0  1  0  1  1  0   1

As I understand your question: you have the probability, and you can use these
to decide whether you think it is high enough for you to think whether it is M=1
or F=0. Anupam.


From Andrew.Zachary at wetherbypartnersllc.com  Tue Sep 19 17:23:52 2006
From: Andrew.Zachary at wetherbypartnersllc.com (Andrew Zachary)
Date: Tue, 19 Sep 2006 16:23:52 +0100
Subject: [R] Problem with rpart
Message-ID: <0194357E439C294E89D51DCE03AFEAF8066134B4@grmail1.dpfm.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060919/d850a2ab/attachment.pl 

From AnupTyagi at yahoo.com  Tue Sep 19 17:25:26 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Tue, 19 Sep 2006 15:25:26 +0000 (UTC)
Subject: [R] what does Height represent?
References: <4509EEFC.4040500@u.washington.edu>
Message-ID: <loom.20060919T172250-799@post.gmane.org>

zhaoshi <zhaoshi <at> u.washington.edu> writes:

> 
> hi--
> 
> I am new to R and try to use R cluster my binary data. I use 
> hierarchical clustering
> plot (hclust (dist(x,method="binary"),method="average"),cex=0.1)
> I end up with a cluster Dendrogram. On the left of my dendrogram, there 
> is scale called Height from 0.0 to 1.0.
> I don't understand what does Height represent. If the Height represents 
> the distance scale between two different data point,
> it looks like if I add up the length of each branch, I end up with 
> distance of some pairs > 1. It is not possible the distance
> between any data point will greater than 1. Could some help me out?

Hi zhaoshi, please check the documentation for the function using ?hclust. You
may also want to look at a book that uses R/S-plus for cluster analysis.
Venables and Ripley's "Modern Applied Statistics with S-plus" Chapter 11 may be
of some help. Anupam.


From bolker at zoo.ufl.edu  Tue Sep 19 17:38:40 2006
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Tue, 19 Sep 2006 15:38:40 +0000 (UTC)
Subject: [R] help on dirichlet distribution
References: <035001c6dbe7$1848a460$44968495@statistica.unimib.it>
Message-ID: <loom.20060919T173502-756@post.gmane.org>

Gianna Monti <gianna.monti <at> unimib.it> writes:

> 
> Dear Gregory R. Warnes, 
> I'm a phd student in statistics, at the University of Milano Bicocca.
> I'm interested to the methods of estimate the parameters of the Dirichlet
distribution.
> Do you have implemented an algorithm in R? 
> If so, can you give me the script? or, in general, some helps?


  Scrolling through the results of
RSiteSearch("dirichlet") suggests some useful tools
in the VGAM package.  The gtools package and
MCMC packages also have ddirichlet() functions
that you could use to construct a (negative log) likelihood
function and optimize with optim/nlmin/etc.

   Ben Bolker


From Torsten.Hothorn at rzmail.uni-erlangen.de  Tue Sep 19 17:44:41 2006
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Tue, 19 Sep 2006 17:44:41 +0200 (CEST)
Subject: [R] Problem with rpart
In-Reply-To: <0194357E439C294E89D51DCE03AFEAF8066134B4@grmail1.dpfm.net>
References: <0194357E439C294E89D51DCE03AFEAF8066134B4@grmail1.dpfm.net>
Message-ID: <Pine.LNX.4.64.0609191743001.29403@imbe153.imbe.med.uni-erlangen.de>


On Tue, 19 Sep 2006, Andrew Zachary wrote:

> Not sure if anyone has posted on this problem ... I want to use rpart to
> build a binary tree on a relatively large dataset with ~1400 data points
> and 15 predictors. But I've noticed that rpart fails almost immediately
> in the call to C_s_to_rp, as that code returns nonsense. Looking at the
> code itself isn't terribly helpful, and there don't seem to be any hard
> limits coded anywhere. Does anyone have a suggestion for what might be
> going on?
>

Andrew,

you need to give an _executable_ example illustrating your problem. What 
means `nonsense'?

Best,

Torsten

> Thanks in advance for you help
> Andrew Zachary
>
> ----
> Wetherby Partners LLC believes the information provided herein is reliable. While every care has been taken to ensure accuracy, the information is furnished to the recipients with no warranty as to the completeness and accuracy of its contents and on condition that any errors or omissions shall not be made the basis for any claim, demand or cause for action.
> The information in this email is intended only for the named...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From f.harrell at vanderbilt.edu  Tue Sep 19 17:46:00 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 19 Sep 2006 10:46:00 -0500
Subject: [R] calculate AUC for ROC curve
In-Reply-To: <Pine.LNX.4.44.0609191010150.16043-100000@ural.wustl.edu>
References: <Pine.LNX.4.44.0609191010150.16043-100000@ural.wustl.edu>
Message-ID: <451010B8.9040807@vanderbilt.edu>

Guoyan Zhao wrote:
> Dear everyone,
> 
> I have only tpr (true positive rate) and fpr (false positive rate) which 
> were used to plot the ROC curve. Is there any way I can calculate the AUC 
> for this curve using just this information? Any help is highly 
> appreciated. 
> 
> Isabel

To get the AUC you need N pairs of (predicted probability or log odds, 
binary outcome).  AUC is quickly computed with the Hmisc somers2 
function and you can get a std. error for it from the Hmisc rcorr.cens 
function.  You can also compute AUC from the way you stated it but that 
is a roundabout way.

Frank

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From AnupTyagi at yahoo.com  Tue Sep 19 17:53:20 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Tue, 19 Sep 2006 15:53:20 +0000 (UTC)
Subject: [R] bubble plot problems
References: <450FE864.3010207@gmail.com>
Message-ID: <loom.20060919T174147-185@post.gmane.org>

Pieter Provoost <pieterprovoost <at> gmail.com> writes:

> I'm having some problems with a bubble plot (ps package). I don't want 
> tick marks on all four sides (just two), I want to have a smaller font 
> size, and I would like to be able to define bubble sizes shown in the 
> legend (now it shows 0, 0, 0, 9.747 and 4265.757 which is not really 
> convenient. Passing some of the standard plot arguments didn't help (in 
> fact, nothing changed).

In general, bubble plots are not good visual aid, except when you happen to have
data for which they work well. They try to represent an additional dimention in
2-D, so there are bound to be problems if there is not enough space to do this
on a plot; additionally the scale is not linear, or something else we are use to
for comparing large differences (in case of your data). It may be better at
times to just label the data points, and let the reader choose the
interpretation, without representing relative size graphically. 

Please see a good book on Statistical Graphics.

http://www.math.yorku.ca/SCS/Gallery/

Search: Cleveland, and Tufte. 

Anupam.


From mschwartz at mn.rr.com  Tue Sep 19 18:09:16 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Tue, 19 Sep 2006 11:09:16 -0500
Subject: [R] R CMD check fails at package dependencies check on
	Fedora	Core 5, works on other systems
In-Reply-To: <450FDFAC.6070209@newcastle.edu.au>
References: <200609191032.04483.Robert.King@newcastle.edu.au>
	<x2ac4wqqqs.fsf@turmalin.kubism.ku.dk>
	<450FC020.4070304@newcastle.edu.au>
	<450FDFAC.6070209@newcastle.edu.au>
Message-ID: <1158682156.3890.20.camel@localhost.localdomain>

On Tue, 2006-09-19 at 22:16 +1000, Robert King wrote:
> Here is another thing that might help work out what is happening.  If I 
> use --no-install, ade4 actually fails as well, in the same way as zipfR.
> 
>   [Desktop]$ R CMD check --no-install ade4
>   * checking for working latex ... OK
>   * using log directory '/home/rak776/Desktop/ade4.Rcheck'
>   * using Version 2.3.1 (2006-06-01)
>   * checking for file 'ade4/DESCRIPTION' ... OK
>   * this is package 'ade4' version '1.4-1'
>   * checking if this is a source package ... OK
>   * checking package directory ... OK
>   * checking for portable file names ... OK
>   * checking for sufficient/correct file permissions ... OK
>   * checking DESCRIPTION meta-information ... ERROR
> 
>   [Desktop]$ R CMD check --no-install zipfR
>   * checking for working latex ... OK
>   * using log directory '/home/rak776/Desktop/zipfR.Rcheck'
>   * using Version 2.3.1 (2006-06-01)
>   * checking for file 'zipfR/DESCRIPTION' ... OK
>   * checking extension type ... Package
>   * this is package 'zipfR' version '0.6-0'
>   * checking if this is a source package ... OK
>   * checking package directory ... OK
>   * checking for portable file names ... OK
>   * checking for sufficient/correct file permissions ... OK
>   * checking DESCRIPTION meta-information ... ERROR

<snip>

Robert,

I tried the process last night (my time) using the initial instructions
on my FC5 system with:

$ R --version
R version 2.3.1 Patched (2006-08-06 r38829)
Copyright (C) 2006 R Development Core Team


I could not replicate the problem.

However, this morning, with your additional communication:

$ R CMD check --no-install zipfR_0.6-0.tar.gz
* checking for working latex ... OK
* using log directory '/home/marcs/Downloads/zipfR.Rcheck'
* using Version 2.3.1 Patched (2006-08-06 r38829)
* checking for file 'zipfR/DESCRIPTION' ... OK
* checking extension type ... Package
* this is package 'zipfR' version '0.6-0'
* checking if this is a source package ... OK
* checking package directory ... OK
* checking for portable file names ... OK
* checking for sufficient/correct file permissions ... OK
* checking DESCRIPTION meta-information ... OK
* checking top-level files ... OK
* checking index information ... OK
* checking package subdirectories ... OK
* checking R files for syntax errors ... OK
* checking R files for library.dynam ... OK
* checking S3 generic/method consistency ... OK
* checking replacement functions ... OK
* checking foreign function calls ... OK
* checking Rd files ... OK
* checking Rd cross-references ... WARNING
Warning in grep(pattern, x, ignore.case, extended, value, fixed,
useBytes) :
         input string 70 is invalid in this locale
* checking for missing documentation entries ... WARNING
Warning in grep(pattern, x, ignore.case, extended, value, fixed,
useBytes) :
         input string 70 is invalid in this locale
All user-level objects in a package should have documentation entries.
See chapter 'Writing R documentation files' in manual 'Writing R
Extensions'.
* checking for code/documentation mismatches ... OK
* checking Rd \usage sections ... OK
* checking DVI version of manual ... OK

WARNING: There were 2 warnings, see
  /home/marcs/Downloads/zipfR.Rcheck/00check.log
for details



So I am wondering if this raises the possibility of a locale issue on
your FC5 system resulting in a problem reading DESCRIPTION files?  It
may be totally unrelated, but one never knows I suppose. Mine is:

$ locale
LANG=en_US.UTF-8
LC_CTYPE="en_US.UTF-8"
LC_NUMERIC="en_US.UTF-8"
LC_TIME="en_US.UTF-8"
LC_COLLATE="en_US.UTF-8"
LC_MONETARY="en_US.UTF-8"
LC_MESSAGES="en_US.UTF-8"
LC_PAPER="en_US.UTF-8"
LC_NAME="en_US.UTF-8"
LC_ADDRESS="en_US.UTF-8"
LC_TELEPHONE="en_US.UTF-8"
LC_MEASUREMENT="en_US.UTF-8"
LC_IDENTIFICATION="en_US.UTF-8"
LC_ALL=


HTH,

Marc Schwartz


From liuwensui at gmail.com  Tue Sep 19 18:18:33 2006
From: liuwensui at gmail.com (Wensui Liu)
Date: Tue, 19 Sep 2006 12:18:33 -0400
Subject: [R] Problem with rpart
In-Reply-To: <0194357E439C294E89D51DCE03AFEAF8066134B4@grmail1.dpfm.net>
References: <0194357E439C294E89D51DCE03AFEAF8066134B4@grmail1.dpfm.net>
Message-ID: <1115a2b00609190918p29c36f57hf0d48f13319f2657@mail.gmail.com>

Andrew,

Not sure what your problem is based on your email.

But data volume is not a problem if there is only 1400 obs and 15 predictors.


On 9/19/06, Andrew Zachary <Andrew.Zachary at wetherbypartnersllc.com> wrote:
> Not sure if anyone has posted on this problem ... I want to use rpart to
> build a binary tree on a relatively large dataset with ~1400 data points
> and 15 predictors. But I've noticed that rpart fails almost immediately
> in the call to C_s_to_rp, as that code returns nonsense. Looking at the
> code itself isn't terribly helpful, and there don't seem to be any hard
> limits coded anywhere. Does anyone have a suggestion for what might be
> going on?
>
> Thanks in advance for you help
> Andrew Zachary
>
> ----
> Wetherby Partners LLC believes the information provided herein is reliable. While every care has been taken to ensure accuracy, the information is furnished to the recipients with no warranty as to the completeness and accuracy of its contents and on condition that any errors or omissions shall not be made the basis for any claim, demand or cause for action.
> The information in this email is intended only for the named...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
WenSui Liu
(http://spaces.msn.com/statcompute/blog)
Senior Decision Support Analyst
Health Policy and Clinical Effectiveness
Cincinnati Children Hospital Medical Center


From Andrew.Zachary at wetherbypartnersllc.com  Tue Sep 19 18:23:47 2006
From: Andrew.Zachary at wetherbypartnersllc.com (Andrew Zachary)
Date: Tue, 19 Sep 2006 17:23:47 +0100
Subject: [R] Problem with rpart
Message-ID: <0194357E439C294E89D51DCE03AFEAF8066134B5@grmail1.dpfm.net>

 
Here is an example (though the data are too large to send ). The dataset
is (6530 x 15). Predictors are continuous N(0,1). Trying to build a
regression tree.

 fit <- rpart( y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 +
x11 + x12 + x13 + x14, data=my.data.set, weights=wts )

And the output:

 summary( fit )
Call:
rpart(formula = y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 +
x11 + x12 + x13 + x14, data =my.data.set, weights = wts)
  n= 6530 

  CP nsplit rel error
1 NA      0        NA

Node number NA: NA observationsError in if (ff$complexity[i] < cp ||
is.leaf[i]) cat("\n") else cat(",    complexity param=",  : 
        missing value where TRUE/FALSE needed


If I run this using a subset of 900 points, everything is fine.
Similarly, if I run it using 1100 points, it dies. There are no missing
values in the dataset. Is this simply a case where I should decrease cp?

Regards,
Andrew

-----Original Message-----
From: Torsten Hothorn [mailto:Torsten.Hothorn at rzmail.uni-erlangen.de] 
Sent: Tuesday, September 19, 2006 4:45 PM
To: Andrew Zachary
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Problem with rpart


On Tue, 19 Sep 2006, Andrew Zachary wrote:

> Not sure if anyone has posted on this problem ... I want to use rpart 
> to build a binary tree on a relatively large dataset with ~1400 data 
> points and 15 predictors. But I've noticed that rpart fails almost 
> immediately in the call to C_s_to_rp, as that code returns nonsense. 
> Looking at the code itself isn't terribly helpful, and there don't 
> seem to be any hard limits coded anywhere. Does anyone have a 
> suggestion for what might be going on?
>

Andrew,

you need to give an _executable_ example illustrating your problem. What
means `nonsense'?

Best,

Torsten

> Thanks in advance for you help
> Andrew Zachary
>
> ----
> Wetherby Partners LLC believes the information provided herein is
reliable. While every care has been taken to ensure accuracy, the
information is furnished to the recipients with no warranty as to the
completeness and accuracy of its contents and on condition that any
errors or omissions shall not be made the basis for any claim, demand or
cause for action.
> The information in this email is intended only for the\ > ...{{dropped}}


From gavin.simpson at ucl.ac.uk  Tue Sep 19 18:29:40 2006
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Tue, 19 Sep 2006 17:29:40 +0100
Subject: [R] How to draw a per mille symbol?
Message-ID: <1158683380.10675.38.camel@gsimpson.geog.ucl.ac.uk>

Dear list,

Following advice posted to this list a while back by Prof Ripley [1], I
have been trying to draw a per mille character [2] in an axis label.

This should give the correct character:

plot(1:10, ylab = "\u2030")

but all I get is '"S'. I'm running linux (FC5) and have fonts installed
that have the correct character (viewed in the Gnome character map at
least).

I have also tried plotting to a pdf device with a font family that the
character map tool shows I have a per mille glyph for, e.g.:

pdf("~/tmp/test_per_mille.pdf", paper = "a4", family = "URWBookman")
plot(1:10, ylab = "\u2030")
dev.off()

But all I get here is a period or a dot-like symbol.

I've tried this in R 2.4.0 alpha [4] and R 2.5.0 to be [4] as my
self-compiled R 2.3.1-patched dies when plotting Unicode characters
(fixed in 2.4.0 alpha and above [3])

Can anyone point me in the right direction to get this working?

TIA,

G

[1] http://finzi.psych.upenn.edu/R/Rhelp02a/archive/48709.html
[2] like a "%" but with 2 circles at the bottom not one, see
http://en.wikipedia.org/wiki/Permille
[3] see thread at http://article.gmane.org/gmane.comp.lang.r.devel/9704
[4] R version 2.4.0 alpha (2006-09-19 r39410)
[5] R version 2.5.0 Under development (unstable) (2006-09-19 r39410)
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC & ENSIS, UCL Geography,  [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From gavin.simpson at ucl.ac.uk  Tue Sep 19 18:33:28 2006
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Tue, 19 Sep 2006 17:33:28 +0100
Subject: [R] How to draw a per mille symbol?
In-Reply-To: <1158683380.10675.38.camel@gsimpson.geog.ucl.ac.uk>
References: <1158683380.10675.38.camel@gsimpson.geog.ucl.ac.uk>
Message-ID: <1158683608.10675.41.camel@gsimpson.geog.ucl.ac.uk>

On Tue, 2006-09-19 at 17:29 +0100, Gavin Simpson wrote:
> Dear list,
> 
> Following advice posted to this list a while back by Prof Ripley [1], I
> have been trying to draw a per mille character [2] in an axis label.
<snip />

Before I get asked for it, here's the sessionInfo() - apologies for not
supplying it originally.

> version
               _
platform       i686-pc-linux-gnu
arch           i686
os             linux-gnu
system         i686, linux-gnu
status         alpha
major          2
minor          4.0
year           2006
month          09
day            19
svn rev        39410
language       R
version.string R version 2.4.0 alpha (2006-09-19 r39410)
> sessionInfo()
R version 2.4.0 alpha (2006-09-19 r39410)
i686-pc-linux-gnu

locale:
LC_CTYPE=en_GB.UTF-8;LC_NUMERIC=C;LC_TIME=en_GB.UTF-8;LC_COLLATE=en_GB.UTF-8;LC_MONETARY=en_GB.UTF-8;LC_MESSAGES=en_GB.UTF-8;LC_PAPER=en_GB.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_GB.UTF-8;LC_IDENTIFICATION=C

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"
"datasets"
[7] "base"
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC & ENSIS, UCL Geography,  [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From sell_mirage_ne at hotmail.com  Tue Sep 19 18:46:57 2006
From: sell_mirage_ne at hotmail.com (Taka Matzmoto)
Date: Tue, 19 Sep 2006 11:46:57 -0500
Subject: [R] looking for some functions to analyze a data set.
Message-ID: <BAY110-F82547B0FE202BDE17F5C4C7220@phx.gbl>

Hi R-users
I have a data set. There are 10 products and the numbers of people who 
ranked the products.

The format of the data set is

productID   rank1 rank2 rank3 rank4 rank5 rank6 rank7 rank8 rank9 rank10
-------------------------------------------------------------------------------------------------------
1                 10
2                  3
3                  6
4                  2
5                24
6                  8
7                  3
8                  8
9                  4
10                5

Each cell has the number of people who ranked the product. For example, 4 
people who ranked 9th product best.

I would like to know how to summarze this data using some R functions. My 
goal is to figure out what is the best product based on the ranking 
information.

Easy one is to look at only rank1 column, then I do not utilize all 
information the data have.
I can also do this. For each product, the number of people * rank scores and 
then pick lowest number for the best product.

Is there any other way I can summarize this data?

Any suggestion for R fundtions or statistical methos will be appreciated.

Taka,

_________________________________________________________________
Get today's hot entertainment gossip  http://movies.msn.com/movies/hotgossip


From ripley at stats.ox.ac.uk  Tue Sep 19 18:53:49 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 19 Sep 2006 17:53:49 +0100 (BST)
Subject: [R] How to draw a per mille symbol?
In-Reply-To: <1158683380.10675.38.camel@gsimpson.geog.ucl.ac.uk>
References: <1158683380.10675.38.camel@gsimpson.geog.ucl.ac.uk>
Message-ID: <Pine.LNX.4.64.0609191742490.32551@gannet.stats.ox.ac.uk>

On Tue, 19 Sep 2006, Gavin Simpson wrote:

> Dear list,
>
> Following advice posted to this list a while back by Prof Ripley [1], I
> have been trying to draw a per mille character [2] in an axis label.
>
> This should give the correct character:
>
> plot(1:10, ylab = "\u2030")
>
> but all I get is '"S'. I'm running linux (FC5) and have fonts installed
> that have the correct character (viewed in the Gnome character map at
> least).

On what device?  If X11, this is almost always a font selection issue.
Unicode support in X11 fonts is a complex issue that seems to vary with 
every minor update.

> I have also tried plotting to a pdf device with a font family that the
> character map tool shows I have a per mille glyph for, e.g.:

What does character map have to do with postscript fonts?

> pdf("~/tmp/test_per_mille.pdf", paper = "a4", family = "URWBookman")
> plot(1:10, ylab = "\u2030")
> dev.off()
>
> But all I get here is a period or a dot-like symbol.

But see the article in R-news 2006-2 about this.  All we can do for PDF is 
to use an appropriate 8-bit font map, or a CJK font.  It seems that e.g.
encoding="CP1251" works, even for Helvetica.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From A.Robinson at ms.unimelb.edu.au  Tue Sep 19 18:54:09 2006
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Wed, 20 Sep 2006 02:54:09 +1000
Subject: [R] How to draw a per mille symbol?
In-Reply-To: <1158683380.10675.38.camel@gsimpson.geog.ucl.ac.uk>
References: <1158683380.10675.38.camel@gsimpson.geog.ucl.ac.uk>
Message-ID: <20060919165409.GI12212@ms.unimelb.edu.au>

Gavin,

the advice given here:

http://tolstoy.newcastle.edu.au/R/help/02b/4378.html

works on FreeBSD 6.1.

Try:

> pdf("~/tmp/test_per_mille.pdf", paper = "a4", family = "URWBookman",
+   encoding="WinAnsi.enc")
> plot(1:10, ylab="\211")
> dev.off() 

Cheers

Andrew


On Tue, Sep 19, 2006 at 05:29:40PM +0100, Gavin Simpson wrote:
> Dear list,
> 
> Following advice posted to this list a while back by Prof Ripley [1], I
> have been trying to draw a per mille character [2] in an axis label.
> 
> This should give the correct character:
> 
> plot(1:10, ylab = "\u2030")
> 
> but all I get is '"S'. I'm running linux (FC5) and have fonts installed
> that have the correct character (viewed in the Gnome character map at
> least).
> 
> I have also tried plotting to a pdf device with a font family that the
> character map tool shows I have a per mille glyph for, e.g.:
> 
> pdf("~/tmp/test_per_mille.pdf", paper = "a4", family = "URWBookman")
> plot(1:10, ylab = "\u2030")
> dev.off()
> 
> But all I get here is a period or a dot-like symbol.
> 
> I've tried this in R 2.4.0 alpha [4] and R 2.5.0 to be [4] as my
> self-compiled R 2.3.1-patched dies when plotting Unicode characters
> (fixed in 2.4.0 alpha and above [3])
> 
> Can anyone point me in the right direction to get this working?
> 
> TIA,
> 
> G
> 
> [1] http://finzi.psych.upenn.edu/R/Rhelp02a/archive/48709.html
> [2] like a "%" but with 2 circles at the bottom not one, see
> http://en.wikipedia.org/wiki/Permille
> [3] see thread at http://article.gmane.org/gmane.comp.lang.r.devel/9704
> [4] R version 2.4.0 alpha (2006-09-19 r39410)
> [5] R version 2.5.0 Under development (unstable) (2006-09-19 r39410)
> -- 
> %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
>  Gavin Simpson                 [t] +44 (0)20 7679 0522
>  ECRC & ENSIS, UCL Geography,  [f] +44 (0)20 7679 0565
>  Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
>  Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
>  UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
> %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
Email: a.robinson at ms.unimelb.edu.au         http://www.ms.unimelb.edu.au


From ggrothendieck at gmail.com  Tue Sep 19 19:50:49 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 19 Sep 2006 13:50:49 -0400
Subject: [R] looking for some functions to analyze a data set.
In-Reply-To: <BAY110-F82547B0FE202BDE17F5C4C7220@phx.gbl>
References: <BAY110-F82547B0FE202BDE17F5C4C7220@phx.gbl>
Message-ID: <971536df0609191050p461498d4sd7c1bf6deb7479ae@mail.gmail.com>

You could use the weighted mean (?weighted.mean) or weighted median
(?wtd.quantile in package Hmisc).

On 9/19/06, Taka Matzmoto <sell_mirage_ne at hotmail.com> wrote:
> Hi R-users
> I have a data set. There are 10 products and the numbers of people who
> ranked the products.
>
> The format of the data set is
>
> productID   rank1 rank2 rank3 rank4 rank5 rank6 rank7 rank8 rank9 rank10
> -------------------------------------------------------------------------------------------------------
> 1                 10
> 2                  3
> 3                  6
> 4                  2
> 5                24
> 6                  8
> 7                  3
> 8                  8
> 9                  4
> 10                5
>
> Each cell has the number of people who ranked the product. For example, 4
> people who ranked 9th product best.
>
> I would like to know how to summarze this data using some R functions. My
> goal is to figure out what is the best product based on the ranking
> information.
>
> Easy one is to look at only rank1 column, then I do not utilize all
> information the data have.
> I can also do this. For each product, the number of people * rank scores and
> then pick lowest number for the best product.
>
> Is there any other way I can summarize this data?
>
> Any suggestion for R fundtions or statistical methos will be appreciated.
>
> Taka,
>
> _________________________________________________________________
> Get today's hot entertainment gossip  http://movies.msn.com/movies/hotgossip
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Greg.Snow at intermountainmail.org  Tue Sep 19 20:28:57 2006
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Tue, 19 Sep 2006 12:28:57 -0600
Subject: [R] Adding percentage to Pie Charts (was (no subject))
Message-ID: <07E228A5BE53C24CAD490193A7381BBB5C4A7D@LP-EXCHVS07.CO.IHC.COM>

Have you read the books by Cleveland?

His experiments show that most people do better estimating things and
comparing things on a linear scale rather than looking at angles and
areas (also see
http://biostat.mc.vanderbilt.edu/twiki/pub/Main/StatGraphCourse/graphsco
urse.pdf)

With a dot chart you can set the axis to go from 0 to the total of all
groups (see the example I sent before, it could have had the numbers on
the x-axis, but still included the total), that means that points near
the middle of the line represent about 50%, looking at how close the
point is to the left lets you estimate the percentage and most people
(you may differ) do a better job of estimating that percentage from the
position of the dot than from an angle or area.  If you feel the need to
specify the percentages along the side of a dot chart, then at least
they are lined up vertically for easy comparison (my example would have
been better if a lot of the vertical space had been removed so the
pieces of interest were closer together), the pie chart would generally
have the percentages non-aligned causing more work for the viewer to
compare them.

Dotcharts also remove the dependence/temptation to use color and any
psycolocical influences that may have on the interpretation.

I have yet to see a pie chart that was better at conveying the true
nature of the data than a well done dot chart of the same data, I have
seen multiple cases where the dot chart showed truths about the data
that were not apparent in the corresponding pie chart.

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Anupam Tyagi
Sent: Tuesday, September 19, 2006 12:52 AM
To: r-help at stat.math.ethz.ch
Subject: Re: [R] Adding percentage to Pie Charts (was (no subject))

Greg Snow <Greg.Snow <at> intermountainmail.org> writes:

> 
> You may want to rethink your whole approach here:
> 
> 1. Pie charts are usually a poor choice of graph, there are better 
> choices.
> 2. Adding percentages to a pie chart is a way of admitting that the 
> pie chart is not doing the job.
> 3. If you want people to compare percentages, then a table is what is 
> needed.
> 4. A pie chart with percentages added is essentially a colorful but 
> poorly layed out table.
> 
> Consider using a dotplot instead of a pie chart, it changes the job of

> the viewer from comparing areas/angles (done poorly by humans) to 
> comparing positions along a common scale (done well by humans).

I think dot charts (plots) are very useful, but they are not substitutes
for a pie chart: they do not show a comparison between the total and the
individual value; have a different scale (linear, usually), and are
visually not suitable to answer some questions that a pie chart can
answer (is the value approximately less than a fourth of the total? Is
it less than half?). For some of these questions, even dot-charts
require a value label, or the user doing mental calculations to guess
approximations.

I think I am quite attuned to getting approximate fractions from a
pie-chart in shorter time, than on a linear scale like the dot-chart.

A modification in a pie chart that draws overlapping areas with a common
start point at the top of the circle, can make is more informative than
a dot-chart.
Something like:
* Start drawing at the top of the circle, as zero (degree/area).
* Draw the representation of every value starting from the top, as zero,
representing it as a labled line from the center of the circle to the
boundary (can use colors where possible).
* Use two lables for the circular axis, inside one for percentages,
outside for values.

What is the simplest way to draw this in R?

Anupam.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From friedman.steve at gmail.com  Tue Sep 19 21:10:48 2006
From: friedman.steve at gmail.com (Steve Friedman)
Date: Tue, 19 Sep 2006 15:10:48 -0400
Subject: [R] plotting question
Message-ID: <2439f5740609191210n4287b283t666f452762b72975@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060919/c227d1a1/attachment.pl 

From ageneticist at yahoo.com  Tue Sep 19 21:19:56 2006
From: ageneticist at yahoo.com (Gamal Azim)
Date: Tue, 19 Sep 2006 12:19:56 -0700 (PDT)
Subject: [R] inconsistent rows in a data frame
Message-ID: <20060919191956.84232.qmail@web33404.mail.mud.yahoo.com>

I need to identify repeated items in p$a with
different s and d entries on the same row, given that
the "0" items should not be considered in the
comparison. Here is an example:

1. Items 3 and 5 in p$a are repeated with different 
entries of s and d, should be removed. 

2. Item 2 was repeated twice but with a 0 once for s
on row 2 and a second time for d on row 6, hence 2
should be  excluded from the comparison. All items are
factor levels  and not necessarily numbers.

> p <- data.frame(a=c(1,2,3,4,5,2,3,5,3,5,3),
s=c(0,0,0,2,4,3,2,4,0,0,4),
d=c(0,1,1,1,3,0,5,11,0,0,0)
)

for(i in 1:3) p[,i] <- factor(p[,i])

> p
   a s  d
1  1 0  0
2  2 0  1
3  3 0  1
4  4 2  1
5  5 4  3
6  2 3  0
7  3 2  5
8  5 4 11
9  3 0  0
10 5 0  0
11 3 4  0

Here is my best effort, I don't like the efficiency
with large data frames! Actually,
efficiency is ridiculous with 800,000 rows!

is.unk <- function(x) {x == "0"}

p.tmp <- unique(p[,1:2])
p.tmp <- p.tmp[!is.unk(p.tmp[,1]) &
!is.unk(p.tmp[,2]),]       
dup.s <- p.tmp[duplicated(p.tmp[,1]), 1][,drop=T]

p.tmp <- unique(p[,c(1,3)])
p.tmp <- p.tmp[!is.unk(p.tmp[,1]) &
!is.unk(p.tmp[,2]),]
dup.d <- p.tmp[duplicated(p.tmp[,1]), 1][,drop=T]

dup.sd <- union(as.character(dup.d),
as.character(dup.s))

> row.names(p[is.element(p[,1],dup.sd),])
[1] "3"  "5"  "7"  "8"  "9"  "10" "11"

There must be more efficient ways, help please!!

Thanks


From johannes at huesing.name  Tue Sep 19 22:31:17 2006
From: johannes at huesing.name (Johannes =?iso-8859-1?Q?H=FCsing?=)
Date: Tue, 19 Sep 2006 22:31:17 +0200 (CEST)
Subject: [R] acos(0.5) == pi/3 FALSE
In-Reply-To: <x2irjkzsis.fsf@turmalin.kubism.ku.dk>
References: <200609181931.46198.inaki.murillo@ehu.es>
	<XFMail.060918230505.Ted.Harding@nessie.mcc.ac.uk>
	<loom.20060919T003312-658@post.gmane.org>
	<x2irjkzsis.fsf@turmalin.kubism.ku.dk>
Message-ID: <62777.91.89.65.114.1158697877.squirrel@mail.panix.com>

Peter Dalgaard:
> Ben Bolker <bolker at zoo.ufl.edu> writes:
>>          1. compose your response
> I've always wondered why step 1. - often the time-consuming bit - is not
> listed last.

The advice applies to the situation when answering immediately would be
your knee-jerk reaction. It is assumed that actually composing and sending
the mail would take very little time and thought, whereas coming around to
answering it after runif(1)*4 hours would take considerably more time, even
when mulitiplied with the probability that you are still the first one.

Looking at the submission times of questions and answers in this
particular case, though, I would be upset if the helpful guys actually
used this algorithm. Most of the answers were submitted after 3.5 to 4 h
time, thus revealing a possible flaw of the random number generator
underlying runif().


From rmh at temple.edu  Tue Sep 19 23:05:23 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Tue, 19 Sep 2006 17:05:23 -0400 (EDT)
Subject: [R] plotting question
Message-ID: <20060919170523.BIH26427@po-d.temple.edu>

Is this what you have in mind?


tmp <- data.frame(id=1:10, y1=sample(10), y2=sample(10))

tmp2 <- cbind(id=c(tmp$id, tmp$id), stack(tmp[,2:3]))

barchart(values ~ id | ind, data=tmp2)



If not, send an equally trivial example of what you want to the list
and someone will send you an optimized set of statements for it.


From Cameron.Guenther at MyFWC.com  Tue Sep 19 23:13:07 2006
From: Cameron.Guenther at MyFWC.com (Guenther, Cameron)
Date: Tue, 19 Sep 2006 17:13:07 -0400
Subject: [R] -Need help with function
Message-ID: <BA6FF017E924044A9BF748AFAEEA6F30A533F0@FWC-TLEX3.fwc.state.fl.us>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060919/64d60bfe/attachment.pl 

From Cameron.Guenther at MyFWC.com  Tue Sep 19 23:13:07 2006
From: Cameron.Guenther at MyFWC.com (Guenther, Cameron)
Date: Tue, 19 Sep 2006 17:13:07 -0400
Subject: [R] -Need help with function
Message-ID: <BA6FF017E924044A9BF748AFAEEA6F30A533F0@FWC-TLEX3.fwc.state.fl.us>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060919/64d60bfe/attachment-0001.pl 

From kartik.pappu at gmail.com  Tue Sep 19 23:18:46 2006
From: kartik.pappu at gmail.com (Kartik Pappu)
Date: Tue, 19 Sep 2006 14:18:46 -0700
Subject: [R] Union of two data frames
Message-ID: <97faa3210609191418x5b18805fr8b33f5f017ac5454@mail.gmail.com>

Hi,

I have two data frames each with 5 columns and different number of
rows.  some of the row names in one data frame are the same as the row
names in the other.  I want to be able to merge the two data frames to
get a new data frame in which the duplicated row names are only shown
once with the data for the rest of the columns used from the first
data frame.

Essentially, I want to make a union of the two data frames. I hope
this question makes sense.

Thanks


From deming.mi at vanderbilt.edu  Tue Sep 19 23:34:00 2006
From: deming.mi at vanderbilt.edu (Deming Mi)
Date: Tue, 19 Sep 2006 16:34:00 -0500
Subject: [R] R package for MALDI-TOF data pre-processing
Message-ID: <005901c6dc33$52717d50$1b0bc80a@msrc.mc.vanderbilt.edu>

Dear R user,
Is there any well-developed R package for pre-processing MALDI-TOF or 
SELDI-TOF data besides PROcess?  The package should be able to perform 
denoising, baseline correction, normalization, spectrum 
alignment/calibration, peak identification and binning.  Thank you.

Deming Mi


From jholtman at gmail.com  Wed Sep 20 00:01:53 2006
From: jholtman at gmail.com (jim holtman)
Date: Tue, 19 Sep 2006 18:01:53 -0400
Subject: [R] -Need help with function
In-Reply-To: <BA6FF017E924044A9BF748AFAEEA6F30A533F0@FWC-TLEX3.fwc.state.fl.us>
References: <BA6FF017E924044A9BF748AFAEEA6F30A533F0@FWC-TLEX3.fwc.state.fl.us>
Message-ID: <644e1f320609191501g144e131bha27f0407a2f6dc15@mail.gmail.com>

You are referencing Ct[i+1] in the loop and it is not defined.  From
then on everything is NA.

On 9/19/06, Guenther, Cameron <Cameron.Guenther at myfwc.com> wrote:
> Hello everyone,
>
> I have a function here that I wrote but doesn't seem to work quite
> right.  Attached is the code.  In the calib funcion under the for loop
> Bt[i+2]<-(1-m)*Bt[i+1]+Rt[i]*Rerr-Ct[i+1] returns NA's for everything
> after years 1983 and 1984.  However the code works when it reads
> Bt[i+2]<-(1-m)*Bt[i+1]+Rt[i]*Rerr-Ct[i].  I don't quite understand why
> since it should be calculating all of the necessary inputs prior to
> calculating Bt[i+2].  Any help would be greatly appreciated.
>
> Thanks
>
> #Model parameters
> B0<-75000000
> m<-0.3
> R0<-B0*m
> z<-0.8
> a<-B0/R0*(1-(z-0.2)/(0.8*z))
> b<-(z-0.2)/(0.8*z*R0)
> dat<-data.frame(years=seq(1983,2004),cobs=c(19032324,19032324,17531618,2
> 0533029,20298099,20793744,23519369,23131780,19922247,17274513,17034419,1
> 2448318,4551585,4226451,7183688,7407924,7538366,7336039,8869193,7902341,
> 6369089,6211886))
> stdr<-runif(100,0,0.5)
> stdc<-runif(100,0,0.5)
> BC<-runif(1000,0,100)
>
>
> #model calibration
>
> calib<-function(x){
>  v<-sample(stdr,1)
>  cr<-sample(stdc,1)
>  N<-rnorm(1)
>  Bq<-sample(BC,1)
>  Rerr<-exp(N*v-(v^2/2))
>  Cerr<-exp(N*cr-(cr^2/2))
>  Bt<-vector();Bt[1]=B0;Bt[2]=B0
>  Rt<-vector()
>  Ct<-vector()
>  for (i in 1:length(x$years)){
>  Ct[i]<-1/Bq*Bt[i]*Cerr
>  Rt[i]<-Bt[i]/(a+b*Bt[i])
>  Bt[i+2]<-(1-m)*Bt[i+1]+Rt[i]*Rerr-Ct[i+1]
>  }
>  out<-new.env()
>  out$yr<-x$years[1:length(x$years)]
>  out$Bt<-Bt[1:length(x$years)]
>  out$Rt<-Rt[1:length(x$years)]
>  out$Ct<-Ct[1:length(x$years)]
>  out$stdr<-v
>  out$stdc<-cr
>  out$Bq<-Bq
>  out$Rerr<-Rerr
>  out$Cerr<-Cerr
>  return(as.list(out))
>  }
>  test<-calib(dat)
>
>
>
> Cameron Guenther, Ph.D.
> Associate Research Scientist
> FWC/FWRI, Marine Fisheries Research
> 100 8th Avenue S.E.
> St. Petersburg, FL 33701
> (727)896-8626 Ext. 4305
> cameron.guenther at myfwc.com
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From jholtman at gmail.com  Wed Sep 20 00:01:53 2006
From: jholtman at gmail.com (jim holtman)
Date: Tue, 19 Sep 2006 18:01:53 -0400
Subject: [R] -Need help with function
In-Reply-To: <BA6FF017E924044A9BF748AFAEEA6F30A533F0@FWC-TLEX3.fwc.state.fl.us>
References: <BA6FF017E924044A9BF748AFAEEA6F30A533F0@FWC-TLEX3.fwc.state.fl.us>
Message-ID: <644e1f320609191501g144e131bha27f0407a2f6dc15@mail.gmail.com>

You are referencing Ct[i+1] in the loop and it is not defined.  From
then on everything is NA.

On 9/19/06, Guenther, Cameron <Cameron.Guenther at myfwc.com> wrote:
> Hello everyone,
>
> I have a function here that I wrote but doesn't seem to work quite
> right.  Attached is the code.  In the calib funcion under the for loop
> Bt[i+2]<-(1-m)*Bt[i+1]+Rt[i]*Rerr-Ct[i+1] returns NA's for everything
> after years 1983 and 1984.  However the code works when it reads
> Bt[i+2]<-(1-m)*Bt[i+1]+Rt[i]*Rerr-Ct[i].  I don't quite understand why
> since it should be calculating all of the necessary inputs prior to
> calculating Bt[i+2].  Any help would be greatly appreciated.
>
> Thanks
>
> #Model parameters
> B0<-75000000
> m<-0.3
> R0<-B0*m
> z<-0.8
> a<-B0/R0*(1-(z-0.2)/(0.8*z))
> b<-(z-0.2)/(0.8*z*R0)
> dat<-data.frame(years=seq(1983,2004),cobs=c(19032324,19032324,17531618,2
> 0533029,20298099,20793744,23519369,23131780,19922247,17274513,17034419,1
> 2448318,4551585,4226451,7183688,7407924,7538366,7336039,8869193,7902341,
> 6369089,6211886))
> stdr<-runif(100,0,0.5)
> stdc<-runif(100,0,0.5)
> BC<-runif(1000,0,100)
>
>
> #model calibration
>
> calib<-function(x){
>  v<-sample(stdr,1)
>  cr<-sample(stdc,1)
>  N<-rnorm(1)
>  Bq<-sample(BC,1)
>  Rerr<-exp(N*v-(v^2/2))
>  Cerr<-exp(N*cr-(cr^2/2))
>  Bt<-vector();Bt[1]=B0;Bt[2]=B0
>  Rt<-vector()
>  Ct<-vector()
>  for (i in 1:length(x$years)){
>  Ct[i]<-1/Bq*Bt[i]*Cerr
>  Rt[i]<-Bt[i]/(a+b*Bt[i])
>  Bt[i+2]<-(1-m)*Bt[i+1]+Rt[i]*Rerr-Ct[i+1]
>  }
>  out<-new.env()
>  out$yr<-x$years[1:length(x$years)]
>  out$Bt<-Bt[1:length(x$years)]
>  out$Rt<-Rt[1:length(x$years)]
>  out$Ct<-Ct[1:length(x$years)]
>  out$stdr<-v
>  out$stdc<-cr
>  out$Bq<-Bq
>  out$Rerr<-Rerr
>  out$Cerr<-Cerr
>  return(as.list(out))
>  }
>  test<-calib(dat)
>
>
>
> Cameron Guenther, Ph.D.
> Associate Research Scientist
> FWC/FWRI, Marine Fisheries Research
> 100 8th Avenue S.E.
> St. Petersburg, FL 33701
> (727)896-8626 Ext. 4305
> cameron.guenther at myfwc.com
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From zjay2006 at yahoo.com  Wed Sep 20 00:12:19 2006
From: zjay2006 at yahoo.com (Jay Z)
Date: Tue, 19 Sep 2006 15:12:19 -0700 (PDT)
Subject: [R] RODBC Connections closed automatically in background
Message-ID: <20060919221219.34046.qmail@web58411.mail.re3.yahoo.com>

I am having a problem with RODBC's connections. It appears that
my connection to the database is closed by R automatically before
I am done with it.

Here is my code:

foo <- function(dsn) {
  db <- odbcConnect(dsn)
  odbcSetAutoCommit(db, FALSE)
  data <- someDatabaseOperation(db)
  data2 <- someLongCalculation(data)
  anotherDatabaseOperation(db, data2) # This often fails b/c the db is no longer open.
  odbcClose(db)
}

I see some output:

  Warning: closing unused RODBC handle 9
  Warning: [RODBC] Error SQLDisconnect
  Warning: [RODBC] Error SQLFreeconnect
  Warning: [RODBC] Error in SQLFreeEnv
  Error in odbcGetErrMsg(channel) : first argument is not an open RODBC channel

I suspect that during the call to someLongCalculation(), R
considers the database connection as "unused", and therefore,
closes the connection, which prevents me from using the
connection in the call to anotherDatabaseOperation().

What causes the database connection to close? What can I do to
prevent the connection from closing implicitly?

I am using R 2.3.1 on Windows with the latest version of RODBC,
connnectin to SQL Server 2000.

Thanks in advance.


From alfilnegro.sv at gmail.com  Wed Sep 20 00:14:10 2006
From: alfilnegro.sv at gmail.com (Ricardo Rios)
Date: Tue, 19 Sep 2006 16:14:10 -0600
Subject: [R] How to store an array in MySQL
Message-ID: <50a711930609191514p38551ffeg90be35daf000b48e@mail.gmail.com>

Hi all, does somebody know how to store an array in MySQL with the
package RMySQL. Thanks in advance.

-- 
Web Page
http://www.geocities.com/ricardo_rios_sv/index.html


From mpg33 at drexel.edu  Wed Sep 20 00:17:31 2006
From: mpg33 at drexel.edu (Michael Gormley)
Date: Tue, 19 Sep 2006 18:17:31 -0400
Subject: [R] justRMA
Message-ID: <002101c6dc39$66b396d0$67397690@9Mike9>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060919/914f34a2/attachment.pl 

From perronbe at gmail.com  Wed Sep 20 00:27:05 2006
From: perronbe at gmail.com (Brian Edward)
Date: Tue, 19 Sep 2006 17:27:05 -0500
Subject: [R] Linux configuration (Ubuntu)
Message-ID: <1e6b5c080609191527x451a7fdjd024e660f054ce44@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060919/8274e161/attachment.pl 

From kartik.pappu at gmail.com  Wed Sep 20 00:30:51 2006
From: kartik.pappu at gmail.com (Kartik Pappu)
Date: Tue, 19 Sep 2006 15:30:51 -0700
Subject: [R] Union of two data frames
Message-ID: <97faa3210609191530ta758d93gf871ad31d480b23@mail.gmail.com>

Hi all,

I have two data frames each with 5 columns and different number of
rows.  some of the row names in one data frame are the same as the row
names in the other.  I want to be able to merge the two data frames to
get a new data frame in which the duplicated row names are only shown
once with the data for the rest of the columns used from the first
data frame.

for example:

	Pvals	PD
A	0.001	0.99
B	0.02	       0.98
C	0.05	       0.97
D	0.005	0.99

	Pvals	PD
C	0.01	       0.99
D	0.0002	0.98
E	0.05	       0.97
F	0.02	       0.99

Union
	Pvals	PD
A	0.001	0.99
B	0.02	       0.98
C	0.05	       0.97
D	0.005	0.99
E	0.05	       0.97
F	0.02  	0.99

Essentially, I want to make a union of the two data frames. I hope
this question makes sense.

Thanks


From wangtong at usc.edu  Wed Sep 20 00:47:03 2006
From: wangtong at usc.edu (Tong Wang)
Date: Tue, 19 Sep 2006 15:47:03 -0700
Subject: [R] Need help with dataset formation problem
Message-ID: <dddb923115de6.451010f7@usc.edu>

Hi, 
    I have a data set , say, X  :      [1]    [2]
                                         mean      0      .8
                                             sd        1       3
    
     I need to use it in do.call() ,   so when I refer to the first col of X, I need it to be a list with members mean, sd.
thus I constructed X as a list of list :  X[[1]]<- list(mean=0, sd=1),   X[[2]]<-.....    .  But this dataset is useless in other
cases, for example, I can't pull out all the means as a vector.    
     Can I get some suggestions on what's the best way to handle it? (except preparing two copies of this data with different formats)

Thanks a lot in advance. 

best


From ligges at statistik.uni-dortmund.de  Wed Sep 20 01:01:30 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 20 Sep 2006 01:01:30 +0200
Subject: [R] justRMA
In-Reply-To: <002101c6dc39$66b396d0$67397690@9Mike9>
References: <002101c6dc39$66b396d0$67397690@9Mike9>
Message-ID: <451076CA.9080704@statistik.uni-dortmund.de>

Michael Gormley wrote:

> I am trying to preprocess a large dataset of affymetrix data.  Creating an affybatch is not possible with the computer I am running it on, so I have used the justRMA command to run RMA.  I have read the affy document describing the justRMA command and the help documentation but I am unclear as to whether this command uses median polish after normalization.  I assume this is the case but would like to be sure before proceeding.  Could someone clear up this confusion? 


Maybe you want to ask on the Bioconductor mailing list, in fact?

Uwe Ligges


> Thanks,
> Mike
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ligges at statistik.uni-dortmund.de  Wed Sep 20 01:03:30 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 20 Sep 2006 01:03:30 +0200
Subject: [R] Linux configuration (Ubuntu)
In-Reply-To: <1e6b5c080609191527x451a7fdjd024e660f054ce44@mail.gmail.com>
References: <1e6b5c080609191527x451a7fdjd024e660f054ce44@mail.gmail.com>
Message-ID: <45107742.5060907@statistik.uni-dortmund.de>

Brian Edward wrote:

> Hello all,
> 
> I have been a R user for about a year now, running on a MS Windows machine.
> I am in the process of making a complete switch to open-source.  Linux is a
> new world to me.  Ubuntu was my selection of the various distributions.
> Please pardon this very basic question (I was unable to locate an answer on
> R or Ubuntu).  I used Synaptic to download the necessary files to run.
> However, I was unable to locate the program using the Add/Remove feature.
> So, I created a Launcher for R on the desktop and identified the executable
> file.  The path I entered into the Command Line was:  /usr/bin/R
> I can run R in the Terminal, but not as a separate desktop location.  So,
> the short question is, what is the specific command line or configuration I
> should be using to run R?  Or, am I supposed to be running R in the
> Terminal?

Short answer: yes, long answer, yes, unless you want to use some very 
capable editor as an environment such ass Emacs + ESS. Look up the 
documentation and the list archives fo details on Emacs and ESS.
Uwe Ligges



> 
> Thanks in advance,
> Brian
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ligges at statistik.uni-dortmund.de  Wed Sep 20 01:06:38 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 20 Sep 2006 01:06:38 +0200
Subject: [R] Need help with dataset formation problem
In-Reply-To: <dddb923115de6.451010f7@usc.edu>
References: <dddb923115de6.451010f7@usc.edu>
Message-ID: <451077FE.9040802@statistik.uni-dortmund.de>

Tong Wang wrote:

> Hi, I have a data set , say, X  :      [1]    [2] mean      0      .8
>  sd        1       3
> 
> I need to use it in do.call() ,   so when I refer to the first col of
> X, I need it to be a list with members mean, sd. thus I constructed X
> as a list of list :  X[[1]]<- list(mean=0, sd=1),   X[[2]]<-.....
> .  But this dataset is useless in other cases, for example, I can't
> pull out all the means as a vector. Can I get some suggestions on

You can:

sapply(X, "[[", 1)

Uwe Ligges

> what's the best way to handle it? (except preparing two copies of
> this data with different formats)
> 
> Thanks a lot in advance.
> 
> best
> 
> ______________________________________________ 
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the
> posting guide http://www.R-project.org/posting-guide.html and provide
> commented, minimal, self-contained, reproducible code.


From ligges at statistik.uni-dortmund.de  Wed Sep 20 01:19:54 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 20 Sep 2006 01:19:54 +0200
Subject: [R] Problem with rpart
In-Reply-To: <0194357E439C294E89D51DCE03AFEAF8066134B5@grmail1.dpfm.net>
References: <0194357E439C294E89D51DCE03AFEAF8066134B5@grmail1.dpfm.net>
Message-ID: <45107B1A.2080704@statistik.uni-dortmund.de>

Andrew Zachary wrote:

>  
> Here is an example (though the data are too large to send ). The dataset
> is (6530 x 15). Predictors are continuous N(0,1). Trying to build a
> regression tree.
> 
>  fit <- rpart( y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 +
> x11 + x12 + x13 + x14, data=my.data.set, weights=wts )
> 
> And the output:
> 
>  summary( fit )
> Call:
> rpart(formula = y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 +
> x11 + x12 + x13 + x14, data =my.data.set, weights = wts)
>   n= 6530 
> 
>   CP nsplit rel error
> 1 NA      0        NA


Well, I tried out a self made exmple on a 6 year old laptop right now, 
and it works perfectly!
Hence, you have to give some more setails about your data, as Torsten 
already mentioned. E.g., start with the output of
str(my.data.set)
str(wts)

and perhaps this helps, otherwise, please make a small (as small as you 
can to reproduce) example available that reproduces the problem, e.g. on 
some web page.

Uwe LIgges


> Node number NA: NA observationsError in if (ff$complexity[i] < cp ||
> is.leaf[i]) cat("\n") else cat(",    complexity param=",  : 
>         missing value where TRUE/FALSE needed
> 
> 
> If I run this using a subset of 900 points, everything is fine.
> Similarly, if I run it using 1100 points, it dies. There are no missing
> values in the dataset. Is this simply a case where I should decrease cp?
> 
> Regards,
> Andrew
> 
> -----Original Message-----
> From: Torsten Hothorn [mailto:Torsten.Hothorn at rzmail.uni-erlangen.de] 
> Sent: Tuesday, September 19, 2006 4:45 PM
> To: Andrew Zachary
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Problem with rpart
> 
> 
> On Tue, 19 Sep 2006, Andrew Zachary wrote:
> 
> 
>>Not sure if anyone has posted on this problem ... I want to use rpart 
>>to build a binary tree on a relatively large dataset with ~1400 data 
>>points and 15 predictors. But I've noticed that rpart fails almost 
>>immediately in the call to C_s_to_rp, as that code returns nonsense. 
>>Looking at the code itself isn't terribly helpful, and there don't 
>>seem to be any hard limits coded anywhere. Does anyone have a 
>>suggestion for what might be going on?
>>
> 
> 
> Andrew,
> 
> you need to give an _executable_ example illustrating your problem. What
> means `nonsense'?
> 
> Best,
> 
> Torsten
> 
> 
>>Thanks in advance for you help
>>Andrew Zachary
>>
>>----
>>Wetherby Partners LLC believes the information provided herein is
> 
> reliable. While every care has been taken to ensure accuracy, the
> information is furnished to the recipients with no warranty as to the
> completeness and accuracy of its contents and on condition that any
> errors or omissions shall not be made the basis for any claim, demand or
> cause for action.
> 
>>The information in this email is intended only for the\ > ...{{dropped}}
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ageneticist at yahoo.com  Wed Sep 20 02:31:39 2006
From: ageneticist at yahoo.com (Gamal Azim)
Date: Tue, 19 Sep 2006 17:31:39 -0700 (PDT)
Subject: [R]  Union of two data frames
Message-ID: <20060920003139.33332.qmail@web33413.mail.mud.yahoo.com>

How about, 

uxy <- union(row.names(x), row.names(y))
ixy <- intersect(row.names(x), row.names(y))

rbind(x[is.element(row.names(x),uxy),], 
y[!is.element(row.names(y),ixy),])

Note, simple rbind'ing of the two frames changes
common row.names.

Gamal


From ggrothendieck at gmail.com  Wed Sep 20 03:17:18 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 19 Sep 2006 21:17:18 -0400
Subject: [R] Union of two data frames
In-Reply-To: <97faa3210609191530ta758d93gf871ad31d480b23@mail.gmail.com>
References: <97faa3210609191530ta758d93gf871ad31d480b23@mail.gmail.com>
Message-ID: <971536df0609191817v21cf2345hac54d47a21470199@mail.gmail.com>

Try either of these:

rbind(DF1, DF2[setdiff(rownames(DF2), rownames(DF1)),])

rbind(DF1, DF2[!(rownames(DF2) %in% rownames(DF1)),])

On 9/19/06, Kartik Pappu <kartik.pappu at gmail.com> wrote:
> Hi all,
>
> I have two data frames each with 5 columns and different number of
> rows.  some of the row names in one data frame are the same as the row
> names in the other.  I want to be able to merge the two data frames to
> get a new data frame in which the duplicated row names are only shown
> once with the data for the rest of the columns used from the first
> data frame.
>
> for example:
>
>        Pvals   PD
> A       0.001   0.99
> B       0.02           0.98
> C       0.05           0.97
> D       0.005   0.99
>
>        Pvals   PD
> C       0.01           0.99
> D       0.0002  0.98
> E       0.05           0.97
> F       0.02           0.99
>
> Union
>        Pvals   PD
> A       0.001   0.99
> B       0.02           0.98
> C       0.05           0.97
> D       0.005   0.99
> E       0.05           0.97
> F       0.02    0.99
>
> Essentially, I want to make a union of the two data frames. I hope
> this question makes sense.
>
> Thanks
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mcardeal at ufba.br  Wed Sep 20 03:50:45 2006
From: mcardeal at ufba.br (Mauricio Cardeal)
Date: Tue, 19 Sep 2006 22:50:45 -0300
Subject: [R] multiple lines and plot
Message-ID: <45109E75.9070602@ufba.br>

Hi. Please, how can I put together 2 or more lines at the same 
scatterplot ? Example: measures of protein intake (quantitative) of 4 
children over 30 days, by day. How to plot all children at same graphic: 
Protein X Time ? Is there any command like "overlay" ?

Thank you,
Mauricio


From xmeng at capitalbio.com  Wed Sep 20 05:20:59 2006
From: xmeng at capitalbio.com (XinMeng)
Date: Wed, 20 Sep 2006 11:20:59 +0800
Subject: [R] about BATCH and parameter file
Message-ID: <358722459.25709@capitalbio.com>

Hello sir:
I use Rcmd to execute R code,such as :

"C:\\Program Files\\R\\R-2.2.1\\bin\\Rcmd.exe" BATCH globalLowessRun.r

globalLowessRun.r is a R function written by myself.

But I wanna make the "globalLowessRun.r" changeable,such as globalLinearRun.r or gridbasedLowessRun.r,and so on.

How can I achieve this goal via Rcmd.exe BATCH ?

The only finding is:
Rcmd BATCH [options] globalLowessRun.r [outfile]

I wonder the only way I can try is [options],but I don't know how to do it.

Thanks a lot!

My best



------------------------------
*******************************************
Xin Meng 
Capitalbio Corporation
National Engineering Research Center 
for Beijing Biochip Technology 
BioPharma-informatics & Software Dept. 
Research Engineer
Tel: +86-10-80715888/80726868-6438
Fax: +86-10-80726790
Email??xmeng at capitalbio.com 
Address:18 Life Science Parkway, 
Changping District, Beijing 102206, China


From p.murrell at auckland.ac.nz  Wed Sep 20 05:28:51 2006
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Wed, 20 Sep 2006 15:28:51 +1200
Subject: [R] How to draw a per mille symbol?
In-Reply-To: <1158683380.10675.38.camel@gsimpson.geog.ucl.ac.uk>
References: <1158683380.10675.38.camel@gsimpson.geog.ucl.ac.uk>
Message-ID: <4510B573.4080309@stat.auckland.ac.nz>

Hi


Gavin Simpson wrote:
> Dear list,
> 
> Following advice posted to this list a while back by Prof Ripley [1], I
> have been trying to draw a per mille character [2] in an axis label.
> 
> This should give the correct character:
> 
> plot(1:10, ylab = "\u2030")
> 
> but all I get is '"S'. I'm running linux (FC5) and have fonts installed
> that have the correct character (viewed in the Gnome character map at
> least).


I get the same thing (and using xfd I see the per mille character in the 
font I'm using).  I'm afraid I'm not sure why this is happening;  I can 
get a number of other "unusual" characters to work (e.g., \u20ac), but 
there appear to be some characters that do not draw correctly.  I used 
the following code to explore the default Helvetica font I've got and I 
can't see a rational pattern in the misbehaviour.

x11(width=5, height=5)
grid.prompt(TRUE)
digits <- c(0:9, letters[1:6])
for (i in c("00", "01", "02", "03", "1e", "20", "21", "22")) {
     grid.newpage()
     for (j in 1:16) {
         for (k in 1:16) {
             pushViewport(viewport(x=j/16, y=1-k/16,
                                   width=1/16, height=1/16,
                                   just=c("right", "bottom")))
             eval(parse(text=paste('grid.text("\\u',
                          i, digits[k], digits[j], '")', sep="")))
             popViewport()
         }
     }
}


> I have also tried plotting to a pdf device with a font family that the
> character map tool shows I have a per mille glyph for, e.g.:
> 
> pdf("~/tmp/test_per_mille.pdf", paper = "a4", family = "URWBookman")
> plot(1:10, ylab = "\u2030")
> dev.off()
> 
> But all I get here is a period or a dot-like symbol.


This is an encoding problem I think.  For producing PDF output, the 
character string gets converted to a single-byte encoding.   If your 
default locale is ISOLatin1 like mine then you won't see the per mille 
because that character (called perthousand in the Adobe afm's) is not in 
the ISOLatin1 encoding.  If you explicitly use an encoding that does 
include perthousand (like WinAnsi) then the conversion to single-byte 
encoding works.  For example, this works (for me at least) ...

pdf("WinAnsi_per_mille.pdf", encoding="WinAnsi")
plot(1:10, ylab = "\u2030")
dev.off()

Paul


> I've tried this in R 2.4.0 alpha [4] and R 2.5.0 to be [4] as my
> self-compiled R 2.3.1-patched dies when plotting Unicode characters
> (fixed in 2.4.0 alpha and above [3])
> 
> Can anyone point me in the right direction to get this working?
> 
> TIA,
> 
> G
> 
> [1] http://finzi.psych.upenn.edu/R/Rhelp02a/archive/48709.html
> [2] like a "%" but with 2 circles at the bottom not one, see
> http://en.wikipedia.org/wiki/Permille
> [3] see thread at http://article.gmane.org/gmane.comp.lang.r.devel/9704
> [4] R version 2.4.0 alpha (2006-09-19 r39410)
> [5] R version 2.5.0 Under development (unstable) (2006-09-19 r39410)

-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/


From maj at stats.waikato.ac.nz  Wed Sep 20 06:45:15 2006
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Wed, 20 Sep 2006 16:45:15 +1200
Subject: [R] Pooled Covariance Matrix
Message-ID: <4510C75B.8040705@stats.waikato.ac.nz>

I am in a discriminant analysis situation with a frame containing 
several variables and a grouping factor, if you like:

set.seed(200906)
exampledf <- as.data.frame(matrix(rnorm(50,5,2),nrow=10,ncol=5))
exampledf$Group <- factor(rep(c(1,2,3),c(3,3,4)))
exampledf

I'm sure there must be a simple way to get the within group pooled 
covariance matrix but I haven't found it yet.

I started thinking that one might begin by forming a frame with the same 
  dimensions but containing the group means. But then I found a thread 
from two years back called "Getting the groupmean for each person" which 
seemed to imply that doing this was a bit subtle even for ncol=1. Hence 
I will risk a question to the list.

Thanks for any help,  Murray Jorgensen

-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    Home +64 7 825 0441    Mobile 021 1395 862


From poizot at cnam.fr  Wed Sep 20 08:33:36 2006
From: poizot at cnam.fr (Poizot Emmanuel)
Date: Wed, 20 Sep 2006 08:33:36 +0200
Subject: [R] Linux configuration (Ubuntu)
In-Reply-To: <1e6b5c080609191527x451a7fdjd024e660f054ce44@mail.gmail.com>
References: <1e6b5c080609191527x451a7fdjd024e660f054ce44@mail.gmail.com>
Message-ID: <4510E0C0.1050400@cnam.fr>

Brian Edward a ?crit :
> Hello all,
>
> I have been a R user for about a year now, running on a MS Windows machine.
> I am in the process of making a complete switch to open-source.  Linux is a
> new world to me.  Ubuntu was my selection of the various distributions.
> Please pardon this very basic question (I was unable to locate an answer on
> R or Ubuntu).  I used Synaptic to download the necessary files to run.
> However, I was unable to locate the program using the Add/Remove feature.
> So, I created a Launcher for R on the desktop and identified the executable
> file.  The path I entered into the Command Line was:  /usr/bin/R
> I can run R in the Terminal, but not as a separate desktop location.  So,
> the short question is, what is the specific command line or configuration I
> should be using to run R?  Or, am I supposed to be running R in the
> Terminal?
>
> Thanks in advance,
> Brian
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>   
Dear Edward,

R under linux (Ubuntu or other distributions) is not exactly as it is 
under windows.
Under linux, you have to use a shell (or a terminal) to launch R and use it.
The main difference between windows and linux version of R, is that R 
under linux, does not provide the graphical facilities to load and 
install libraries. To do so under linux, you have to download the tar.gz 
version of the library from CRAN and use as a root user the command R 
CMD INSTALL thelibrarytoinstall.tar.gz. You have to install, of course, 
before, the needed compilers gcc and/or fortran, depend the library you 
are about to install.
Hope it 'll help.
Regards

-- 
Cordialement

------------------------------------------------
Emmanuel Poizot
Cnam/Intechmer
B.P. 324
50103 Cherbourg Cedex

Phone (Direct) : (00 33)(0)233887342
Fax : (00 33)(0)233887339
------------------------------------------------


From ripley at stats.ox.ac.uk  Wed Sep 20 08:36:33 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 20 Sep 2006 07:36:33 +0100 (BST)
Subject: [R] Pooled Covariance Matrix
In-Reply-To: <4510C75B.8040705@stats.waikato.ac.nz>
References: <4510C75B.8040705@stats.waikato.ac.nz>
Message-ID: <Pine.LNX.4.64.0609200708230.23370@gannet.stats.ox.ac.uk>

On Wed, 20 Sep 2006, Murray Jorgensen wrote:

> I am in a discriminant analysis situation with a frame containing
> several variables and a grouping factor, if you like:
>
> set.seed(200906)
> exampledf <- as.data.frame(matrix(rnorm(50,5,2),nrow=10,ncol=5))
> exampledf$Group <- factor(rep(c(1,2,3),c(3,3,4)))
> exampledf
>
> I'm sure there must be a simple way to get the within group pooled
> covariance matrix but I haven't found it yet.

There are two versions of this, weighted and unweighted, and the 
difference caused confusion in the early discriminant analysis literature. 
(See MASS4 p.333.)  The weighted version is conventional.

Suppose you have a matrix X and a grouping factor g.  Then either of

    group.means <- rowsum(X, g)/as.vector(table(g))
    group.means <- tapply(X, list(rep(g, ncol(X)), col(X)), mean)

gives the group means, and var(X - group.means[g,]) seems to be what you 
want.

> I started thinking that one might begin by forming a frame with the same
>  dimensions but containing the group means. But then I found a thread
> from two years back called "Getting the groupmean for each person" which
> seemed to imply that doing this was a bit subtle even for ncol=1. Hence
> I will risk a question to the list.

That thread seems to be about efficiency for very large matrices on R of 
two years' ago.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From wuertz at itp.phys.ethz.ch  Wed Sep 20 08:45:10 2006
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Wed, 20 Sep 2006 08:45:10 +0200
Subject: [R] currency or stock trading strategy
In-Reply-To: <d2095b8c0609181152m2b12a590vba51c19ee2d7dac9@mail.gmail.com>
References: <d2095b8c0609171437o496256aam53bbbcc25d1acf43@mail.gmail.com>	<450E582F.6000902@pburns.seanet.com>
	<d2095b8c0609181152m2b12a590vba51c19ee2d7dac9@mail.gmail.com>
Message-ID: <4510E376.4040108@itp.phys.ethz.ch>

Darren Weber wrote:

>Hi Patrick,
>
>thanks for pointing me to your work and Rmetrics.
>
>I have a few questions on my mind right now.  Do you have methods for
>automatic download of price quote histories?  
>
Rmetrics and tseries

>I can use python to get
>XML data on FOREX price quotes from the NYRB and other sites.
>Together with Rpy and matplotlib, that data download could form the
>basis for an open source technical analysis platform.  I still need
>some way to get open source price quote histories for stocks 
>
yahoo internet
Bloomberg with R interface
exchanges delayed on internet or by direct subscription or through Bloomberg


Diethelm Wuertz

>and
>options.  Any ideas?
>
>Best, Darren
>
>
>On 9/18/06, Patrick Burns <pburns at pburns.seanet.com> wrote:
>  
>
>>The Finance page of the Burns Statistics website tells
>>you how to sign up to R-sig-finance.
>>
>>You want to investigate Rmetrics.
>>
>>You can see an example of backtesting in R from the
>>'evalstrat' package that is in the Public Domain area of
>>the Burns Statistics website.
>>
>>Patrick Burns
>>patrick at burns-stat.com
>>+44 (0)20 8525 0696
>>http://www.burns-stat.com
>>(home of S Poetry and "A Guide for the Unwilling S User")
>>
>>Darren Weber wrote:
>>
>>    
>>
>>>Hi,
>>>
>>>are there any good charting and analysis tools for use with
>>>currencies, stocks, etc. in R?  I have some tools to download currency
>>>data from the NYFRB using python and XML.  Can we get and parse an XML
>>>download using R?  Can we have interaction in R plots?  Does anyone
>>>use R for back-testing trading strategies?  Are there any forums for
>>>discussion of using R for this specific purpose (apart from this
>>>general list)?  Is anyone aware of any general open-source
>>>developments for these purposes (I don't see any from GNU or google
>>>searches)?
>>>
>>>Take care, Darren
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>>>
>>>
>>>      
>>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>  
>


From ligges at statistik.uni-dortmund.de  Wed Sep 20 08:55:34 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 20 Sep 2006 08:55:34 +0200
Subject: [R] Linux configuration (Ubuntu)
In-Reply-To: <4510E0C0.1050400@cnam.fr>
References: <1e6b5c080609191527x451a7fdjd024e660f054ce44@mail.gmail.com>
	<4510E0C0.1050400@cnam.fr>
Message-ID: <4510E5E6.3020307@statistik.uni-dortmund.de>

Poizot Emmanuel wrote:
> Brian Edward a ?crit :
> 
>> Hello all,
>>
>> I have been a R user for about a year now, running on a MS Windows 
>> machine.
>> I am in the process of making a complete switch to open-source.  Linux 
>> is a
>> new world to me.  Ubuntu was my selection of the various distributions.
>> Please pardon this very basic question (I was unable to locate an 
>> answer on
>> R or Ubuntu).  I used Synaptic to download the necessary files to run.
>> However, I was unable to locate the program using the Add/Remove feature.
>> So, I created a Launcher for R on the desktop and identified the 
>> executable
>> file.  The path I entered into the Command Line was:  /usr/bin/R
>> I can run R in the Terminal, but not as a separate desktop location.  So,
>> the short question is, what is the specific command line or 
>> configuration I
>> should be using to run R?  Or, am I supposed to be running R in the
>> Terminal?
>>
>> Thanks in advance,
>> Brian
>>
>>     [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>   
> 
> Dear Edward,
> 
> R under linux (Ubuntu or other distributions) is not exactly as it is 
> under windows.
> Under linux, you have to use a shell (or a terminal) to launch R and use 
> it.
> The main difference between windows and linux version of R, is that R 
> under linux, does not provide the graphical facilities to load and 
> install libraries. To do so under linux, you have to download the tar.gz 
           ^^^^^^^^^ -> packages


> version of the library from CRAN and use as a root user the command R 
                  ^^^^^^^ -> package

> CMD INSTALL thelibrarytoinstall.tar.gz. You have to install, of course, 
                  ^^^^^^^ -> package
> before, the needed compilers gcc and/or fortran, depend the library you 
                                                   package <-  ^^^^^^^

Yes, please, it is called a *package*. And you can easily install 
packages from R by typing:

install.packages("thepackagetoinstall")
         ^^^^^^^^     ^^^^^^^^

Please see ?install.packages.
                     ^^^^^^^^

Uwe Ligges




> are about to install.
> Hope it 'll help.
> Regards
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From mothsailor at googlemail.com  Wed Sep 20 08:53:28 2006
From: mothsailor at googlemail.com (David Barron)
Date: Wed, 20 Sep 2006 07:53:28 +0100
Subject: [R] multiple lines and plot
In-Reply-To: <45109E75.9070602@ufba.br>
References: <45109E75.9070602@ufba.br>
Message-ID: <815b70590609192353o60d5932dvf52c2d5772f25ea8@mail.gmail.com>

?lines

On 20/09/06, Mauricio Cardeal <mcardeal at ufba.br> wrote:
> Hi. Please, how can I put together 2 or more lines at the same
> scatterplot ? Example: measures of protein intake (quantitative) of 4
> children over 30 days, by day. How to plot all children at same graphic:
> Protein X Time ? Is there any command like "overlay" ?
>
> Thank you,
> Mauricio
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From ligges at statistik.uni-dortmund.de  Wed Sep 20 08:58:08 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 20 Sep 2006 08:58:08 +0200
Subject: [R] about BATCH and parameter file
In-Reply-To: <358722459.25709@capitalbio.com>
References: <358722459.25709@capitalbio.com>
Message-ID: <4510E680.5070709@statistik.uni-dortmund.de>

XinMeng wrote:

> Hello sir:
> I use Rcmd to execute R code,such as :
> 
> "C:\\Program Files\\R\\R-2.2.1\\bin\\Rcmd.exe" BATCH globalLowessRun.r
> 
> globalLowessRun.r is a R function written by myself.
> 
> But I wanna make the "globalLowessRun.r" changeable,such as globalLinearRun.r or gridbasedLowessRun.r,and so on.


Can you please explain in more detail?

I do not understand wht you really would like to chenge. If it is just 
the name, why don't you simply retype the command?

Uwe Ligges

> How can I achieve this goal via Rcmd.exe BATCH ?
> 
> The only finding is:
> Rcmd BATCH [options] globalLowessRun.r [outfile]
> 
> I wonder the only way I can try is [options],but I don't know how to do it.
> 
> Thanks a lot!
> 
> My best
> 
> 
> 
> ------------------------------
> *******************************************
> Xin Meng 
> Capitalbio Corporation
> National Engineering Research Center 
> for Beijing Biochip Technology 
> BioPharma-informatics & Software Dept. 
> Research Engineer
> Tel: +86-10-80715888/80726868-6438
> Fax: +86-10-80726790
> Email??xmeng at capitalbio.com 
> Address:18 Life Science Parkway, 
> Changping District, Beijing 102206, China
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ligges at statistik.uni-dortmund.de  Wed Sep 20 08:59:07 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 20 Sep 2006 08:59:07 +0200
Subject: [R] multiple lines and plot
In-Reply-To: <45109E75.9070602@ufba.br>
References: <45109E75.9070602@ufba.br>
Message-ID: <4510E6BB.4010503@statistik.uni-dortmund.de>

Mauricio Cardeal wrote:

> Hi. Please, how can I put together 2 or more lines at the same 
> scatterplot ? Example: measures of protein intake (quantitative) of 4 
> children over 30 days, by day. How to plot all children at same graphic: 
> Protein X Time ? Is there any command like "overlay" ?


Make the plot for one child and add other with connands uch as lines() 
or segments(), don't know what is best in your case.

Uwe Ligges

> Thank you,
> Mauricio
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From petr.pikal at precheza.cz  Wed Sep 20 08:58:56 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 20 Sep 2006 08:58:56 +0200
Subject: [R] multiple lines and plot
In-Reply-To: <45109E75.9070602@ufba.br>
Message-ID: <451102D0.15050.2666BF@localhost>

Hi

Have you searched the R-project web? There are plenty of "overlay" 
commands.

Have you looked at plot help page?
The first example shows you how to put line on existing scatterplot.

So plot, lines and points are the basics for standard plot 
customisation.

HTH
Petr 

On 19 Sep 2006 at 22:50, Mauricio Cardeal wrote:

Date sent:      	Tue, 19 Sep 2006 22:50:45 -0300
From:           	Mauricio Cardeal <mcardeal at ufba.br>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] multiple lines and plot

> Hi. Please, how can I put together 2 or more lines at the same 
> scatterplot ? Example: measures of protein intake (quantitative) of 4
> children over 30 days, by day. How to plot all children at same
> graphic: Protein X Time ? Is there any command like "overlay" ?
> 
> Thank you,
> Mauricio
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From dinasaid at gmail.com  Wed Sep 20 09:03:27 2006
From: dinasaid at gmail.com (Dina Said)
Date: Wed, 20 Sep 2006 10:03:27 +0300
Subject: [R] tcltk problem
Message-ID: <4510E7BF.8010607@gmail.com>

Hi,

I'm using fedora core 3. I downloaded R and installed it. 
 After that, I invoked R by the root user and write the 
 command install.packages("Rcmdr", dependencies=TRUE). This 
 command installs Rcmdr automatically. However, whenever I 
type library(Rcmdr), it tells me that Loading required 
 package: tcltk Error in firstlib(which.lib.loc, package) :
         Tcl/Tk support is not available on this system
 Error: package 'tcltk' could not be loaded
 

tcl and tk are installed on my system

The problem is that R can't find them as I guess

Please help me to solve this problem


From antonio.fabio at gmail.com  Wed Sep 20 09:09:44 2006
From: antonio.fabio at gmail.com (Antonio, Fabio Di Narzo)
Date: Wed, 20 Sep 2006 09:09:44 +0200
Subject: [R] [Rd] Sweave processes \Sexpr in commented LaTeX source
	(2.3.1patched and 2.4.0)
In-Reply-To: <1158711280.5348.24.camel@localhost.localdomain>
References: <1158711280.5348.24.camel@localhost.localdomain>
Message-ID: <b0808fdc0609200009w2375ec5dne71a0cc2875fc553@mail.gmail.com>

Hi.

2006/9/20, Marc Schwartz <MSchwartz a mn.rr.com>:
> Hi all,
>
> On FC5, using:
>
>   Version 2.3.1 Patched (2006-08-06 r38829)
>
> and today's
>
>   R version 2.4.0 alpha (2006-09-19 r39397)
>
> with the following .Rnw file:
>
>
> \documentclass[10pt]{article}
> \begin{document}
>
>    This line should print '2': \Sexpr{1 + 1}
> %% This line should NOT print '2': \Sexpr{1 + 1}

If it's just a comment, why don't use something like:
% \ Sexpr (del the space)
or
%\sexpr (change 'sexpr' with 'Sexpr')
or
%...the 'Sexpr' command (add a backslash in latex code)
?

Antonio.

>
> \end{document}
>
>
> The \Sexpr in the second line is processed even though the line is
> commented. This results in the following .tex file content (in the case
> of R 2.4.0):
>
>
> \documentclass[10pt]{article}
> \usepackage{/home/marcs/R.Files/SourceCode/R-alpha/share/texmf/Sweave}
> \begin{document}
>
>    This line should print '2': 2
> %% This line should NOT print '2': 2
>
> \end{document}
>
>
>
> Shouldn't Sweave just generally ignore commented LaTeX code? In
> reviewing Sweave.R I did not see a check for this, so perhaps there are
> circumstances where one wants a \Sexpr in commented LaTeX code
> processed. An example escapes me at the moment however.
>
> HTH,
>
> Marc Schwartz
>
> ______________________________________________
> R-devel a r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From spencer.graves at pdf.com  Wed Sep 20 09:10:11 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 20 Sep 2006 00:10:11 -0700
Subject: [R] estimating state space with exogenous input in measurement
 eq.
In-Reply-To: <00a401c6d599$e989c2b0$2e01a8c0@m8d4477f3de884>
References: <20060911115244.90041.qmail@web25505.mail.ukl.yahoo.com>
	<00a401c6d599$e989c2b0$2e01a8c0@m8d4477f3de884>
Message-ID: <4510E953.6070805@pdf.com>

      With a bit more details:  RSiteSearch("kalman", "functions") 
produced for me just now 29 hits.  The second hit there mentions package 
'sspir'.  After installing it, I tried help(package="sspir").  The 
"Description" suggested I try 'help(sspir)', which briefly summarizes 
what it does and does not do and gives an overview of alternative 
software. 

      None of R's volunteer contributors have as yet provided a complete 
package with documentation that makes it almost trivial to do all you 
might want to do with a model like you describe.  However, several 
different capabilities are available, and virtually anything you want to 
do can probably be done without excessive effort.  If you would like 
more help from this listserve, please provide commented, minimal, 
self-contained, example with reproducible code (as suggested in the 
posting guide "www.R-project.org/posting-guide.html").  Your general 
question produced to my knowledge two general and I hope helpful replies 
(including this one).  A more specific question might generate more 
specific and perhaps more useful replies (quicker). 

      Hope this helps,
      Spencer Graves

MARK LEEDS wrote:
> below is very close to a standard kalman filter setup except for the 
> exogenous u[k] so i would check on www.r-project.org
> for any packages that do kalman filtering. (  also, good reference for state 
> space is a book by durbin and koopman but i forget the title exactly ).
>
>
>
>
> ----- Original Message ----- 
> From: "?yvind Foshaug" <oyvfos at yahoo.no>
> To: <r-help at stat.math.ethz.ch>
> Sent: Monday, September 11, 2006 7:52 AM
> Subject: [R] estimating state space with exogenous input in measurement eq.
>
>
>   
>> Anyone know how to esimate parameters in the system:
>>
>>  x[k]=Ax[k-1]+ B + Gv[k-1]
>>  y[k]=x[k]+Du[k]+Hw[k]
>>
>>  a system with exogenous u[k] in the measurement eq., v,w are iid, both 
>> eq. are gaussian.
>>
>>  Thanks,
>>  Oyvind
>>
>>
>>
>> ---------------------------------
>>
>>
>> [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>     
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From spencer.graves at pdf.com  Wed Sep 20 09:21:24 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 20 Sep 2006 00:21:24 -0700
Subject: [R] Wilcoxon Rank-Sum Test with Bonferroni's correction
In-Reply-To: <bcb1e1da0609110614m4598e2b5tb556cc552c76cd26@mail.gmail.com>
References: <bcb1e1da0609110614m4598e2b5tb556cc552c76cd26@mail.gmail.com>
Message-ID: <4510EBF4.4000203@pdf.com>

      Consider the following: 

l0 <- list(1:3, 4:5)
l1 <- list(6:8, 9:11, 12:16)
WT <- mapply(function(x,y)wilcox.test(x,y)$p.value, l0, l1)

      The simplest Bonferroni for this case would be as follows: 

length(WT)*WT

      Greater numerical precision could be obtained as follows: 

1-(1-WT)^length(WT)

      If you aren't satisfied with either of these, I suggest two 
things:  First have you tried help('Bonferroni')?  This produced 124 
hits for me just now.  Second, please expand your example slightly to 
make it self contained (as I tried to do above) and PLEASE do read the 
posting guide "www.R-project.org/posting-guide.html". 

      Hope this helps. 
      Spencer Graves

Raj, Towfique wrote:
> Dear all,
>
> I am trying to run Wilcoxon Rank-Sum Test with Bonferroni's
> correction. I have two lists: l0, l1:
>
> mapply(function(x,y)wilcox.test(x,y)$p.value, l0, l1)
>
> How do I run Bonferroni's correction on mapply? Any help is much apperciated.
> Thanks,
>
> -Raj
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ripley at stats.ox.ac.uk  Wed Sep 20 09:27:02 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 20 Sep 2006 08:27:02 +0100 (BST)
Subject: [R] Linux configuration (Ubuntu)
In-Reply-To: <4510E5E6.3020307@statistik.uni-dortmund.de>
References: <1e6b5c080609191527x451a7fdjd024e660f054ce44@mail.gmail.com>
	<4510E0C0.1050400@cnam.fr> <4510E5E6.3020307@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.64.0609200821480.25743@gannet.stats.ox.ac.uk>

On Wed, 20 Sep 2006, Uwe Ligges wrote:

> Poizot Emmanuel wrote:

>> R under linux (Ubuntu or other distributions) is not exactly as it is
>> under windows.
>> Under linux, you have to use a shell (or a terminal) to launch R and use
>> it.
>> The main difference between windows and linux version of R, is that R
>> under linux, does not provide the graphical facilities to load and
>> install libraries. To do so under linux, you have to download the tar.gz
>           ^^^^^^^^^ -> packages
>
>
>> version of the library from CRAN and use as a root user the command R
>                  ^^^^^^^ -> package
>
>> CMD INSTALL thelibrarytoinstall.tar.gz. You have to install, of course,
>                  ^^^^^^^ -> package
>> before, the needed compilers gcc and/or fortran, depend the library you
>                                                   package <-  ^^^^^^^
>
> Yes, please, it is called a *package*. And you can easily install
> packages from R by typing:
>
> install.packages("thepackagetoinstall")
>         ^^^^^^^^     ^^^^^^^^
>
> Please see ?install.packages.
>                     ^^^^^^^^

One more quick hint to ex-Windows users:  the Rgui menu item runs

install.packages(NULL, dependencies = TRUE)

and that will bring up a graphical menu to select packages even on Linux 
(provided tcltk is working).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Wed Sep 20 09:31:48 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 20 Sep 2006 08:31:48 +0100 (BST)
Subject: [R] tcltk problem
In-Reply-To: <4510E7BF.8010607@gmail.com>
References: <4510E7BF.8010607@gmail.com>
Message-ID: <Pine.LNX.4.64.0609200827470.25743@gannet.stats.ox.ac.uk>

On Wed, 20 Sep 2006, Dina Said wrote:

> Hi,
>
> I'm using fedora core 3. I downloaded R and installed it.
> After that, I invoked R by the root user and write the
> command install.packages("Rcmdr", dependencies=TRUE). This
> command installs Rcmdr automatically. However, whenever I
> type library(Rcmdr), it tells me that Loading required
> package: tcltk Error in firstlib(which.lib.loc, package) :
>         Tcl/Tk support is not available on this system
> Error: package 'tcltk' could not be loaded
>
>
> tcl and tk are installed on my system
>
> The problem is that R can't find them as I guess

It is most likely that tcl and tk are installed, but tcl-devel and 
tk-devel are not.  See the 'R Installation and Administation' manual. 
Please install these RPMs if necessary, then re-install R, keeping a note 
of the output from configure.

(As far as I know this cannot happen if you installed the RPM, so am 
assuming that you built R from the sources: you omitted to tell us.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Thierry.ONKELINX at inbo.be  Wed Sep 20 09:32:04 2006
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Wed, 20 Sep 2006 09:32:04 +0200
Subject: [R] RODBC Connections closed automatically in background
Message-ID: <2E9C414912813E4EB981326983E0A1040217D33D@inexch.instnat.be.grp>

Usually I try to do all data import before I do long calculations. A
simple workaround for your problem would be:

foo <- function(dsn) {
  db <- odbcConnect(dsn)
  odbcSetAutoCommit(db, FALSE)
  data <- someDatabaseOperation(db)
  odbcClose(db)
  data2 <- someLongCalculation(data)
  db <- odbcConnect(dsn)
  odbcSetAutoCommit(db, FALSE)
  anotherDatabaseOperation(db, data2)
  odbcClose(db)
}

An other options is to include the odbcConnect and odbcClose into
someLongCalculation() and anotherDatabaseOperation()

Cheers,

Thierry

-----Oorspronkelijk bericht-----
Van: r-help-bounces op stat.math.ethz.ch
[mailto:r-help-bounces op stat.math.ethz.ch] Namens Jay Z
Verzonden: woensdag 20 september 2006 0:12
Aan: R-help op stat.math.ethz.ch
Onderwerp: [R] RODBC Connections closed automatically in background

I am having a problem with RODBC's connections. It appears that
my connection to the database is closed by R automatically before
I am done with it.

Here is my code:

foo <- function(dsn) {
  db <- odbcConnect(dsn)
  odbcSetAutoCommit(db, FALSE)
  data <- someDatabaseOperation(db)
  data2 <- someLongCalculation(data)
  anotherDatabaseOperation(db, data2) # This often fails b/c the db is
no longer open.
  odbcClose(db)
}

I see some output:

  Warning: closing unused RODBC handle 9
  Warning: [RODBC] Error SQLDisconnect
  Warning: [RODBC] Error SQLFreeconnect
  Warning: [RODBC] Error in SQLFreeEnv
  Error in odbcGetErrMsg(channel) : first argument is not an open RODBC
channel

I suspect that during the call to someLongCalculation(), R
considers the database connection as "unused", and therefore,
closes the connection, which prevents me from using the
connection in the call to anotherDatabaseOperation().

What causes the database connection to close? What can I do to
prevent the connection from closing implicitly?

I am using R 2.3.1 on Windows with the latest version of RODBC,
connnectin to SQL Server 2000.

Thanks in advance.

______________________________________________
R-help op stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From michael.hahsler at wu-wien.ac.at  Wed Sep 20 09:44:05 2006
From: michael.hahsler at wu-wien.ac.at (Michael Hahsler)
Date: Wed, 20 Sep 2006 09:44:05 +0200 (CEST)
Subject: [R] Call for Abstracts: Tools for Intelligent Data Analysis (GfKl
	07)
Message-ID: <Pine.LNX.4.62.0609200943410.23810@seth.ai.wu-wien.ac.at>


 			*** Call for Abstracts ***

Tools for Intelligent Data Analysis
Session at the 31th Annual Conference of the German Classification Society 
(GfKl): Data Analysis, Machine Learning, and Applications
March 7-9, 2007
University of Freiburg

Providing innovative tools for data analysis is extremely important for making
techniques available for a broader community. In the session 'Tools for
Intelligent Data Analysis' we are looking for presentations of tools (in a very 
broad sense) which support any area of data analysis including data mining 
(from the data preparation to the visualization of results). We solicit 
contributions on algorithms as well as design and applications of such tools.

Some suggested topics

     * Data mining tools
     * Implementations of algorithms
     * Vizualization frameworks
     * Statistical (extension) packages

All presentes will be invited to submit a paper for a book published by 
Springer (Studies in Classification, Data Analysis, and Knowledge 
Organization).

Deadline for submission of abstracts: Fr. November 10, 2006

Session Chairs:
Michael Hahsler <Michael.Hahsler at wu-wien.ac.at>
Kurt Hornik <Kurt.Hornik at wu-wien.ac.at>

For information on how to submit your abstract, please go to 
http://www.ai.wu-wien.ac.at/~hahsler/research/tools_gfkl07/

--
   Michael Hahsler
   Institut f?r Informationswirtschaft, Wirtschaftsuniversit?t Wien
   Tel: +43-1-31336-6081                      Fax: +43-1-31336-739
   E-Mail: hahsler at ai.wu-wien.ac.at
   WWW: http://wwwai.wu-wien.ac.at/~hahsler

From Sven.Garbade at med.uni-heidelberg.de  Wed Sep 20 10:55:08 2006
From: Sven.Garbade at med.uni-heidelberg.de (Garbade, Sven)
Date: Wed, 20 Sep 2006 10:55:08 +0200
Subject: [R] random number generation from a-/symmetric distribution
Message-ID: <7BFEEFC29878C14D88B28652813BA6AA07F276@CEX10.ads.krz.uni-heidelberg.de>

Hi list,

are there any functions or ideas to compute random numbers with a specific population mean and standard deviation from symmetric (but not normal) and asymmetric distributions? My first idea was to use e.g. rf() (and other R-functions for random number generation)  and then scale the random numbers (for example: mean 300 and standard deviation 40), but I don't know if I'm wrong...

Thanks, Sven


From mel at altk.com  Wed Sep 20 11:06:40 2006
From: mel at altk.com (mel)
Date: Wed, 20 Sep 2006 11:06:40 +0200
Subject: [R] random number generation from a-/symmetric distribution
In-Reply-To: <7BFEEFC29878C14D88B28652813BA6AA07F276@CEX10.ads.krz.uni-heidelberg.de>
References: <7BFEEFC29878C14D88B28652813BA6AA07F276@CEX10.ads.krz.uni-heidelberg.de>
Message-ID: <451104A0.2030507@altk.com>

Garbade, Sven a ?crit :

> Hi list,
> are there any functions or ideas to compute random numbers with a specific population mean and standard deviation from symmetric (but not normal) and asymmetric distributions? My first idea was to use e.g. rf() (and other R-functions for random number generation)  and then scale the random numbers (for example: mean 300 and standard deviation 40), but I don't know if I'm wrong...
> Thanks, Sven

http://cran.r-project.org/src/contrib/Descriptions/boot.html
and all methods/packages related to bootstraping.
hih


From AnupTyagi at yahoo.com  Wed Sep 20 11:15:08 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Wed, 20 Sep 2006 09:15:08 +0000 (UTC)
Subject: [R] looking for some functions to analyze a data set.
References: <BAY110-F82547B0FE202BDE17F5C4C7220@phx.gbl>
Message-ID: <loom.20060920T111110-580@post.gmane.org>

Taka Matzmoto <sell_mirage_ne <at> hotmail.com> writes:

> Hi R-users
> I have a data set. There are 10 products and the numbers of people who 
> ranked the products.
.......
> 
> Is there any other way I can summarize this data?

Be sure to know what assumptions are implicit in the procedures you are using to
do this. Any procedure you use will violate some assumption that seems quite
intutively appealing. Anupam.


From mcardeal at ufba.br  Wed Sep 20 11:27:56 2006
From: mcardeal at ufba.br (Mauricio Cardeal)
Date: Wed, 20 Sep 2006 06:27:56 -0300
Subject: [R] multiple lines and plot
In-Reply-To: <815b70590609192353o60d5932dvf52c2d5772f25ea8@mail.gmail.com>
References: <45109E75.9070602@ufba.br>
	<815b70590609192353o60d5932dvf52c2d5772f25ea8@mail.gmail.com>
Message-ID: <4511099C.2020904@ufba.br>

Ok. I?ve already tried ?lines. An example:

x <- c(1,2,3,4,5,6)
y <- c(3,5,2,4,1,4)
z <- c(2,3,4,3,2,1)
plot(x,y)
lines(x,y)
lines(x,z)

Here is the point: how to show the points under the second line (x,z) ?

Thanks
Mauricio


David Barron escreveu:
> ?lines
>
> On 20/09/06, Mauricio Cardeal <mcardeal em ufba.br> wrote:
>> Hi. Please, how can I put together 2 or more lines at the same
>> scatterplot ? Example: measures of protein intake (quantitative) of 4
>> children over 30 days, by day. How to plot all children at same graphic:
>> Protein X Time ? Is there any command like "overlay" ?
>>
>> Thank you,
>> Mauricio
>>
>> ______________________________________________
>> R-help em stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>


From mothsailor at googlemail.com  Wed Sep 20 11:35:00 2006
From: mothsailor at googlemail.com (David Barron)
Date: Wed, 20 Sep 2006 10:35:00 +0100
Subject: [R] multiple lines and plot
In-Reply-To: <4511099C.2020904@ufba.br>
References: <45109E75.9070602@ufba.br>
	<815b70590609192353o60d5932dvf52c2d5772f25ea8@mail.gmail.com>
	<4511099C.2020904@ufba.br>
Message-ID: <815b70590609200235v41e79d47y17e6c0a5174552a0@mail.gmail.com>

You can specify the same type parameter in either lines() or points()
that can be used in plot().  So, if you want to show points as well as
lines, use the following:

x <- c(1,2,3,4,5,6)
y <- c(3,5,2,4,1,4)
z <- c(2,3,4,3,2,1)
plot(x,y,type="b")
lines(x,z,col="red",type="b")


On 20/09/06, Mauricio Cardeal <mcardeal at ufba.br> wrote:
> Ok. I?ve already tried ?lines. An example:
>
> x <- c(1,2,3,4,5,6)
> y <- c(3,5,2,4,1,4)
> z <- c(2,3,4,3,2,1)
> plot(x,y)
> lines(x,y)
> lines(x,z)
>
> Here is the point: how to show the points under the second line (x,z) ?
>
> Thanks
> Mauricio
>
>
> David Barron escreveu:
> > ?lines
> >
> > On 20/09/06, Mauricio Cardeal <mcardeal at ufba.br> wrote:
> >> Hi. Please, how can I put together 2 or more lines at the same
> >> scatterplot ? Example: measures of protein intake (quantitative) of 4
> >> children over 30 days, by day. How to plot all children at same graphic:
> >> Protein X Time ? Is there any command like "overlay" ?
> >>
> >> Thank you,
> >> Mauricio
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From arun.kumar.saha at gmail.com  Wed Sep 20 12:11:36 2006
From: arun.kumar.saha at gmail.com (Arun Kumar Saha)
Date: Wed, 20 Sep 2006 15:41:36 +0530
Subject: [R] Need help to estimate the Coef matrices in mAr
In-Reply-To: <44FBC786.5010703@pdf.com>
References: <d4c57560608292247h2f18d279r75061f0a4eb6cebd@mail.gmail.com>
	<44FBC786.5010703@pdf.com>
Message-ID: <d4c57560609200311u3ca3fb25n9af7971a0da3b818@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060920/8d5078b5/attachment.pl 

From gavin.simpson at ucl.ac.uk  Wed Sep 20 12:43:28 2006
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Wed, 20 Sep 2006 11:43:28 +0100
Subject: [R] How to draw a per mille symbol?
In-Reply-To: <Pine.LNX.4.64.0609191742490.32551@gannet.stats.ox.ac.uk>
References: <1158683380.10675.38.camel@gsimpson.geog.ucl.ac.uk>
	<Pine.LNX.4.64.0609191742490.32551@gannet.stats.ox.ac.uk>
Message-ID: <1158749008.20600.7.camel@gsimpson.geog.ucl.ac.uk>

On Tue, 2006-09-19 at 17:53 +0100, Prof Brian Ripley wrote:
> On Tue, 19 Sep 2006, Gavin Simpson wrote:
> 
> > Dear list,
> >
> > Following advice posted to this list a while back by Prof Ripley [1], I
> > have been trying to draw a per mille character [2] in an axis label.
> >
> > This should give the correct character:
> >
> > plot(1:10, ylab = "\u2030")
> >
> > but all I get is '"S'. I'm running linux (FC5) and have fonts installed
> > that have the correct character (viewed in the Gnome character map at
> > least).
> 
> On what device?  If X11, this is almost always a font selection issue.
> Unicode support in X11 fonts is a complex issue that seems to vary with 
> every minor update.

Yes, this was an X11 device.

> 
> > I have also tried plotting to a pdf device with a font family that the
> > character map tool shows I have a per mille glyph for, e.g.:
> 
> What does character map have to do with postscript fonts?

Given your comment, nothing. You've probably guessed I know very little
about fonts, and had naively assumed that a font of the same name as a
name displayed by postscriptFonts() or pdfFonts(), which showed a per
mille sign in the character map tool in Gnome, were one and the same
thing.

> 
> > pdf("~/tmp/test_per_mille.pdf", paper = "a4", family = "URWBookman")
> > plot(1:10, ylab = "\u2030")
> > dev.off()
> >
> > But all I get here is a period or a dot-like symbol.
> 
> But see the article in R-news 2006-2 about this.  All we can do for PDF is 
> to use an appropriate 8-bit font map, or a CJK font.  It seems that e.g.
> encoding="CP1251" works, even for Helvetica.
> 

Ok, thanks for this - I've now read this and think I understand the
issues and how to work with fonts in R a bit better now.

The ultimate aim was to be able to include relevant code in a Sweave
document. I'm not aware of a way to pass extra instructions to the pdf
generator code in Sweave, so the solution will involve generating the
pdf within embedded R code directly and inserting the figure manually
into the the document. Is my thinking correct here?

Thanks again for answering my questions, Prof. Ripley.

G

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC & ENSIS, UCL Geography,  [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From numero.primo at tele2.it  Wed Sep 20 12:47:31 2006
From: numero.primo at tele2.it (Landini Massimiliano)
Date: Wed, 20 Sep 2006 12:47:31 +0200
Subject: [R] How to compare contours
Message-ID: <j172h2dgiuolbqf0lk8ooe57guhvc16k04@4ax.com>

dear All
can anybody point me in to the right direction for this kind of operation??

Here an example.
Please consider an hilly matematical landscapa as

i<-10000

X<- runif(i, min=0, max=4*pi)
Y<- runif(i, min=0, max=4*pi)
Z<-(cos(X)+cos(Y))/2

plot(X,Y,xlim=c(0,4*pi),ylim=c(0,4*pi), xlab="X",ylab="Y", main=c(i,"  points"))
coscos.spl<-interp.new(X,Y
,Z,xo=seq(0,4*pi,length=100),yo=seq(0,4*pi,length=100))
contour(coscos.spl,add=T,col="blue",levels=c(seq(-1,1,1/5)),labcex=0.8)

in this case contour plot derived from 10000 points interpolated rappresent very
welll real trend.
now consider

par(mfrow=c(4,5) )
for (i in seq(5,195,10)) {

X<- runif(i, min=0, max=4*pi)
Y<- runif(i, min=0, max=4*pi)
Z<-(cos(X)+cos(Y))/2

plot(X,Y,xlim=c(0,4*pi),ylim=c(0,4*pi), xlab="X",ylab="Y", main=c(i,"  points"))
coscos.spl<-interp.new(X,Y
,Z,xo=seq(0,4*pi,length=100),yo=seq(0,4*pi,length=100))
contour(coscos.spl,add=T,col="blue",levels=c(seq(-1,1,1/5)),labcex=0.8)
}


How many points are necessary to fit (at 95% c.i.) true surface???
85 points???
or more than  200???

tnx in advance!!



-------------------------------------------------------------------------------------------------------------------------
Landini Massimiliano
-------------------------------------------------------------------------------------------------------------------------
Legge di Hanggi: Pi? stupida ? la tua ricerca, pi? verr? letta e approvata.
Corollario alla Legge di Hanggi: Pi? importante ? la tua ricerca, meno verr?
capita.


From MUEHGE at de.ibm.com  Wed Sep 20 13:03:45 2006
From: MUEHGE at de.ibm.com (Thorsten Muehge)
Date: Wed, 20 Sep 2006 13:03:45 +0200
Subject: [R] Variable im Data Frame Namen
Message-ID: <OF353871EA.E0807E4B-ONC12571EF.003C6F07-C12571EF.003CC4FE@de.ibm.com>


Hello R Experts,
how can I incorporate a variable in a data frame definition?

Example:
week <- 28
test(week) <- data.frame(a,b,s,c);
test28


From geoffrey.russell at gmail.com  Wed Sep 20 13:09:35 2006
From: geoffrey.russell at gmail.com (Geoff Russell)
Date: Wed, 20 Sep 2006 20:39:35 +0930
Subject: [R] Stats question - cox proportional hazards adjustments
Message-ID: <93c3eada0609200409t51b0afbcqa57f45f9d2d9db61@mail.gmail.com>

Hi useRs,

Many studies of the link between red meat and colorectal cancer use
Cox proportional
hazards with (among other things) a gender covariate.

If it is true that men eat more red meat, drink more alcohol and smoke more than
women, and if it is also true that alcohol and tobacco are known risk
factors then why does
it make sense to "adjust" for gender?   I would think that in this
case some of the
risk that should be properly attributed to the bad habits will actually end
up being attributed to being male instead.


Cheers,
Geoff Russell


From gavin.simpson at ucl.ac.uk  Wed Sep 20 13:15:44 2006
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Wed, 20 Sep 2006 12:15:44 +0100
Subject: [R] How to draw a per mille symbol?
In-Reply-To: <4510B573.4080309@stat.auckland.ac.nz>
References: <1158683380.10675.38.camel@gsimpson.geog.ucl.ac.uk>
	<4510B573.4080309@stat.auckland.ac.nz>
Message-ID: <1158750944.20600.22.camel@gsimpson.geog.ucl.ac.uk>

On Wed, 2006-09-20 at 15:28 +1200, Paul Murrell wrote:
> Hi
> 
> 
> Gavin Simpson wrote:
> > Dear list,
> > 
> > Following advice posted to this list a while back by Prof Ripley [1], I
> > have been trying to draw a per mille character [2] in an axis label.
> > 
> > This should give the correct character:
> > 
> > plot(1:10, ylab = "\u2030")
> > 
> > but all I get is '"S'. I'm running linux (FC5) and have fonts installed
> > that have the correct character (viewed in the Gnome character map at
> > least).

Thanks for your reply Paul, and also to Andrew Robinson for his earlier
reply. I had initially avoided trying to use the encoding argument and
pdf() as I had planned to include the code to produce the graphics in a
Sweave document and AFAICS there is no way to pass extra arguments to
the code generating the pdf figures in Sweave? Of course, my original
plan was ignorant of the details of font encodings and mappings to
single byte encodings in pdf and postscript devices and wouldn't have
worked anyway.

> 
> 
> I get the same thing (and using xfd I see the per mille character in the 
> font I'm using).  I'm afraid I'm not sure why this is happening;  I can 
> get a number of other "unusual" characters to work (e.g., \u20ac), but 
> there appear to be some characters that do not draw correctly.  I used 
> the following code to explore the default Helvetica font I've got and I 
> can't see a rational pattern in the misbehaviour.
> 
> x11(width=5, height=5)
> grid.prompt(TRUE)
> digits <- c(0:9, letters[1:6])
> for (i in c("00", "01", "02", "03", "1e", "20", "21", "22")) {
>      grid.newpage()
>      for (j in 1:16) {
>          for (k in 1:16) {
>              pushViewport(viewport(x=j/16, y=1-k/16,
>                                    width=1/16, height=1/16,
>                                    just=c("right", "bottom")))
>              eval(parse(text=paste('grid.text("\\u',
>                           i, digits[k], digits[j], '")', sep="")))
>              popViewport()
>          }
>      }
> }
> 

That is a nice tool for looking at the font glyphs, which I can see
being very useful in working out which unicode number matches the
character you want to display. I'm not too clued up on grid graphics
yet, would it be easy to modify the above to print out the \uXXXX code
above each glyph?

> 
> > I have also tried plotting to a pdf device with a font family that the
> > character map tool shows I have a per mille glyph for, e.g.:
> > 
> > pdf("~/tmp/test_per_mille.pdf", paper = "a4", family = "URWBookman")
> > plot(1:10, ylab = "\u2030")
> > dev.off()
> > 
> > But all I get here is a period or a dot-like symbol.
> 
> 
> This is an encoding problem I think.  For producing PDF output, the 
> character string gets converted to a single-byte encoding.   If your 
> default locale is ISOLatin1 like mine then you won't see the per mille 
> because that character (called perthousand in the Adobe afm's) is not in 
> the ISOLatin1 encoding.  If you explicitly use an encoding that does 
> include perthousand (like WinAnsi) then the conversion to single-byte 
> encoding works.  For example, this works (for me at least) ...
> 
> pdf("WinAnsi_per_mille.pdf", encoding="WinAnsi")
> plot(1:10, ylab = "\u2030")
> dev.off()
> 
> Paul

Thanks for this, which works fine for me also.

All the best,

G

> 
> 
> > I've tried this in R 2.4.0 alpha [4] and R 2.5.0 to be [4] as my
> > self-compiled R 2.3.1-patched dies when plotting Unicode characters
> > (fixed in 2.4.0 alpha and above [3])
> > 
> > Can anyone point me in the right direction to get this working?
> > 
> > TIA,
> > 
> > G
> > 
> > [1] http://finzi.psych.upenn.edu/R/Rhelp02a/archive/48709.html
> > [2] like a "%" but with 2 circles at the bottom not one, see
> > http://en.wikipedia.org/wiki/Permille
> > [3] see thread at http://article.gmane.org/gmane.comp.lang.r.devel/9704
> > [4] R version 2.4.0 alpha (2006-09-19 r39410)
> > [5] R version 2.5.0 Under development (unstable) (2006-09-19 r39410)
> 
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC & ENSIS, UCL Geography,  [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From Kurt.Hornik at wu-wien.ac.at  Wed Sep 20 13:20:45 2006
From: Kurt.Hornik at wu-wien.ac.at (Kurt Hornik)
Date: Wed, 20 Sep 2006 13:20:45 +0200
Subject: [R] R CMD check fails at package dependencies check
	on	Fedora	Core 5, works on other systems
In-Reply-To: <1158682156.3890.20.camel@localhost.localdomain>
References: <200609191032.04483.Robert.King@newcastle.edu.au>
	<x2ac4wqqqs.fsf@turmalin.kubism.ku.dk>
	<450FC020.4070304@newcastle.edu.au>
	<450FDFAC.6070209@newcastle.edu.au>
	<1158682156.3890.20.camel@localhost.localdomain>
Message-ID: <17681.9229.193443.354540@mithrandir.hornik.net>

>>>>> Marc Schwartz (via MN) writes:

> On Tue, 2006-09-19 at 22:16 +1000, Robert King wrote:
>> Here is another thing that might help work out what is happening.  If I 
>> use --no-install, ade4 actually fails as well, in the same way as zipfR.
>> 
>> [Desktop]$ R CMD check --no-install ade4
>> * checking for working latex ... OK
>> * using log directory '/home/rak776/Desktop/ade4.Rcheck'
>> * using Version 2.3.1 (2006-06-01)
>> * checking for file 'ade4/DESCRIPTION' ... OK
>> * this is package 'ade4' version '1.4-1'
>> * checking if this is a source package ... OK
>> * checking package directory ... OK
>> * checking for portable file names ... OK
>> * checking for sufficient/correct file permissions ... OK
>> * checking DESCRIPTION meta-information ... ERROR
>> 
>> [Desktop]$ R CMD check --no-install zipfR
>> * checking for working latex ... OK
>> * using log directory '/home/rak776/Desktop/zipfR.Rcheck'
>> * using Version 2.3.1 (2006-06-01)
>> * checking for file 'zipfR/DESCRIPTION' ... OK
>> * checking extension type ... Package
>> * this is package 'zipfR' version '0.6-0'
>> * checking if this is a source package ... OK
>> * checking package directory ... OK
>> * checking for portable file names ... OK
>> * checking for sufficient/correct file permissions ... OK
>> * checking DESCRIPTION meta-information ... ERROR

> <snip>

> Robert,

> I tried the process last night (my time) using the initial instructions
> on my FC5 system with:

> $ R --version
> R version 2.3.1 Patched (2006-08-06 r38829)
> Copyright (C) 2006 R Development Core Team


> I could not replicate the problem.

> However, this morning, with your additional communication:

> $ R CMD check --no-install zipfR_0.6-0.tar.gz
> * checking for working latex ... OK
> * using log directory '/home/marcs/Downloads/zipfR.Rcheck'
> * using Version 2.3.1 Patched (2006-08-06 r38829)
> * checking for file 'zipfR/DESCRIPTION' ... OK
> * checking extension type ... Package
> * this is package 'zipfR' version '0.6-0'
> * checking if this is a source package ... OK
> * checking package directory ... OK
> * checking for portable file names ... OK
> * checking for sufficient/correct file permissions ... OK
> * checking DESCRIPTION meta-information ... OK
> * checking top-level files ... OK
> * checking index information ... OK
> * checking package subdirectories ... OK
> * checking R files for syntax errors ... OK
> * checking R files for library.dynam ... OK
> * checking S3 generic/method consistency ... OK
> * checking replacement functions ... OK
> * checking foreign function calls ... OK
> * checking Rd files ... OK
> * checking Rd cross-references ... WARNING
> Warning in grep(pattern, x, ignore.case, extended, value, fixed,
> useBytes) :
>          input string 70 is invalid in this locale
> * checking for missing documentation entries ... WARNING
> Warning in grep(pattern, x, ignore.case, extended, value, fixed,
> useBytes) :
>          input string 70 is invalid in this locale
> All user-level objects in a package should have documentation entries.
> See chapter 'Writing R documentation files' in manual 'Writing R
> Extensions'.
> * checking for code/documentation mismatches ... OK
> * checking Rd \usage sections ... OK
> * checking DVI version of manual ... OK

> WARNING: There were 2 warnings, see
>   /home/marcs/Downloads/zipfR.Rcheck/00check.log
> for details



> So I am wondering if this raises the possibility of a locale issue on
> your FC5 system resulting in a problem reading DESCRIPTION files?  It
> may be totally unrelated, but one never knows I suppose. Mine is:

> $ locale
> LANG=en_US.UTF-8
> LC_CTYPE="en_US.UTF-8"
> LC_NUMERIC="en_US.UTF-8"
> LC_TIME="en_US.UTF-8"
> LC_COLLATE="en_US.UTF-8"
> LC_MONETARY="en_US.UTF-8"
> LC_MESSAGES="en_US.UTF-8"
> LC_PAPER="en_US.UTF-8"
> LC_NAME="en_US.UTF-8"
> LC_ADDRESS="en_US.UTF-8"
> LC_TELEPHONE="en_US.UTF-8"
> LC_MEASUREMENT="en_US.UTF-8"
> LC_IDENTIFICATION="en_US.UTF-8"
> LC_ALL=


> HTH,

> Marc Schwartz

That's a bug in tools:::Rd_aliases (it needs to preprocess the Rd lines,
which re-encodes if necessary and possible).

I'll commit a fix later today.

Thanks for spotting this.

Best
-k


From rocco at ds.unifi.it  Wed Sep 20 13:58:42 2006
From: rocco at ds.unifi.it (Emilia Rocco)
Date: Wed, 20 Sep 2006 13:58:42 +0200
Subject: [R] Step procedure and Akaike information criterion
Message-ID: <002201c6dcac$24d96370$9f03a8c0@STAT1.FIRENZE>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060920/a3e633e0/attachment.pl 

From mothsailor at googlemail.com  Wed Sep 20 14:05:38 2006
From: mothsailor at googlemail.com (David Barron)
Date: Wed, 20 Sep 2006 13:05:38 +0100
Subject: [R] Step procedure and Akaike information criterion
In-Reply-To: <002201c6dcac$24d96370$9f03a8c0@STAT1.FIRENZE>
References: <002201c6dcac$24d96370$9f03a8c0@STAT1.FIRENZE>
Message-ID: <815b70590609200505l315061e5j1acf71838d7f98a5@mail.gmail.com>

compare ?AIC with ?extractAIC, which explains the difference.

On 20/09/06, Emilia Rocco <rocco at ds.unifi.it> wrote:
> Please can you help me
> I have the following problem:
> I have selected an lm model through the step procedure which visualize for each step the AIC value; then I have calculated for the initial model and the selected one the AIC using the funnction AIC. The results are different.What's happened?
>
> Emilia Rocco
> Dipartimento di Statistica  "G. Parenti"
> Universit? di Firenze
> e-mail: rocco at ds.unifi.it
>
>         [[alternative HTML version deleted]]
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From chrysopa at gmail.com  Wed Sep 20 13:46:57 2006
From: chrysopa at gmail.com (Ronaldo Reis-Jr.)
Date: Wed, 20 Sep 2006 08:46:57 -0300
Subject: [R] problem in font
In-Reply-To: <450E98D8.5070400@statistik.uni-dortmund.de>
References: <200609180832.40581.chrysopa@gmail.com>
	<450E98D8.5070400@statistik.uni-dortmund.de>
Message-ID: <200609200846.57594.chrysopa@gmail.com>

Em Segunda 18 Setembro 2006 10:02, Uwe Ligges escreveu:
> Ronaldo Reis-Jr. wrote:
> > Hi,
> >
> > But, I have this warning in the enviromnent that have this problem:
> >
> > During startup - Warning message:
> > using .GlobalEnv instead of 'package:lattice'
>
> So, can you please be a bit more specific?
> R version, your startup files such as Rprofile files etc?
>
> Uwe Ligges
>
> > maybe this is the problem?
> >
> > How to fix it?
> >
> > Thanks
> > ROnaldo

Hi all,

I fix it, it was my fault.

if I make dev.copy2eps("file.eps") I give the error, but if I make 
dev.copy2eps(file="file.eps") it work fine.

Thanks for all and sorry by the fault problem :)
Inte
Ronaldo
-- 
M?sica do Mudo e do Surdo:

``Eu quis dizer, voc? n?o quis escutar.''

                  -- Paralamas do Sucesso
--
> Prof. Ronaldo Reis J?nior
|  .''`. UNIMONTES/Depto. Biologia Geral/Lab. Ecologia Evolutiva
| : :'  : Campus Universit?rio Prof. Darcy Ribeiro, Vila Mauric?ia
| `. `'` CP: 126, CEP: 39401-089, Montes Claros - MG - Brasil
|   `- Fone: (38) 3229-8190 | chrysopa em gmail.com
| ICQ#: 5692561 | LinuxUser#: 205366


From f.harrell at vanderbilt.edu  Wed Sep 20 14:21:52 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 20 Sep 2006 07:21:52 -0500
Subject: [R] Linux configuration (Ubuntu)
In-Reply-To: <45107742.5060907@statistik.uni-dortmund.de>
References: <1e6b5c080609191527x451a7fdjd024e660f054ce44@mail.gmail.com>
	<45107742.5060907@statistik.uni-dortmund.de>
Message-ID: <45113260.4040909@vanderbilt.edu>

Uwe Ligges wrote:
> Brian Edward wrote:
> 
>> Hello all,
>>
>> I have been a R user for about a year now, running on a MS Windows machine.
>> I am in the process of making a complete switch to open-source.  Linux is a
>> new world to me.  Ubuntu was my selection of the various distributions.
>> Please pardon this very basic question (I was unable to locate an answer on
>> R or Ubuntu).  I used Synaptic to download the necessary files to run.
>> However, I was unable to locate the program using the Add/Remove feature.
>> So, I created a Launcher for R on the desktop and identified the executable
>> file.  The path I entered into the Command Line was:  /usr/bin/R
>> I can run R in the Terminal, but not as a separate desktop location.  So,
>> the short question is, what is the specific command line or configuration I
>> should be using to run R?  Or, am I supposed to be running R in the
>> Terminal?
> 
> Short answer: yes, long answer, yes, unless you want to use some very 
> capable editor as an environment such ass Emacs + ESS. Look up the 
> documentation and the list archives fo details on Emacs and ESS.
> Uwe Ligges

The kate editor (a Linux KDE tool that also runs fine under Gnome) also 
works well with R.  Just set the editor option to use spaces instead of 
tab characters or the submission of code to an R session will not work 
properly.  I set a customized shortcut of Alt-r to submit code to kate's 
shell window at the bottom of the screen, after manually launching R in 
that window.

After R starts you can manage help files nicely using help.start() to 
use a browser.  I often use dillo, the world's fastest graphical 
browser, by specifying options(browser='dillo') before help.start().

Frank

> 
> 
> 
>> Thanks in advance,
>> Brian


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From p.dalgaard at biostat.ku.dk  Wed Sep 20 14:47:00 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Sep 2006 14:47:00 +0200
Subject: [R] Stats question - cox proportional hazards adjustments
In-Reply-To: <93c3eada0609200409t51b0afbcqa57f45f9d2d9db61@mail.gmail.com>
References: <93c3eada0609200409t51b0afbcqa57f45f9d2d9db61@mail.gmail.com>
Message-ID: <x2odta1z97.fsf@viggo.kubism.ku.dk>

"Geoff Russell" <geoffrey.russell at gmail.com> writes:

> Hi useRs,
> 
> Many studies of the link between red meat and colorectal cancer use
> Cox proportional
> hazards with (among other things) a gender covariate.
> 
> If it is true that men eat more red meat, drink more alcohol and smoke more than
> women, and if it is also true that alcohol and tobacco are known risk
> factors then why does
> it make sense to "adjust" for gender?   I would think that in this
> case some of the
> risk that should be properly attributed to the bad habits will actually end
> up being attributed to being male instead.

This is more than a bit off-topic for the list, but in (very) brief:
Because you need to get rid of purely gender related effects that
disturb the analysis and may create spurious association.

Otherwise you would become able to "prove" effects like stiletto heels
causing breast cancer, etc. 

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From rmh at temple.edu  Wed Sep 20 14:48:09 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Wed, 20 Sep 2006 08:48:09 -0400 (EDT)
Subject: [R] Variable im Data Frame Namen
Message-ID: <20060920084809.BII47993@po-d.temple.edu>

> assign(paste("week", 28, sep=""), data.frame(a=1:2, b=3:4, cc=letters[5:6]))
> week28
  a b cc
1 1 3  e
2 2 4  f
>


From rkrug at sun.ac.za  Wed Sep 20 15:23:59 2006
From: rkrug at sun.ac.za (Rainer M Krug)
Date: Wed, 20 Sep 2006 15:23:59 +0200
Subject: [R] Linux configuration (Ubuntu)
In-Reply-To: <45113260.4040909@vanderbilt.edu>
References: <1e6b5c080609191527x451a7fdjd024e660f054ce44@mail.gmail.com>	<45107742.5060907@statistik.uni-dortmund.de>
	<45113260.4040909@vanderbilt.edu>
Message-ID: <451140EF.5030501@sun.ac.za>



Frank E Harrell Jr wrote:
SNIP

> After R starts you can manage help files nicely using help.start() to 
> use a browser.  I often use dillo, the world's fastest graphical 
> browser, by specifying options(browser='dillo') before help.start().

WOW - dillo is BRILLIANT for this purpose.
Thanks for the tip!
Do you have any idea on how to convince dillo to reuse the same browser 
window again instead of using a new one when using ? ?

> 
> Frank
> 
>>
>>
>>> Thanks in advance,
>>> Brian
> 
> 

-- 
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
Biology (UCT)

Department of Conservation Ecology and Entomology
University of Stellenbosch
Matieland 7602
South Africa

Tel:		+27 - (0)72 808 2975 (w)
Fax:		+27 - (0)21 808 3304
Cell:		+27 - (0)83 9479 042

email:	RKrug at sun.ac.za
       	Rainer at krugs.de


From Marc.Zodet at ahrq.hhs.gov  Wed Sep 20 15:29:25 2006
From: Marc.Zodet at ahrq.hhs.gov (Zodet, Marc W. (AHRQ))
Date: Wed, 20 Sep 2006 09:29:25 -0400
Subject: [R] Simulation help
Message-ID: <1F809F62E3CEA04881B4644029484B450316CEC7@AVN3VS004.ees.hhs.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060920/01a4ff39/attachment.pl 

From maechler at stat.math.ethz.ch  Wed Sep 20 15:42:55 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 20 Sep 2006 15:42:55 +0200
Subject: [R] Linux configuration (Ubuntu)
In-Reply-To: <45107742.5060907@statistik.uni-dortmund.de>
References: <1e6b5c080609191527x451a7fdjd024e660f054ce44@mail.gmail.com>
	<45107742.5060907@statistik.uni-dortmund.de>
Message-ID: <17681.17759.24307.828937@stat.math.ethz.ch>

>>>>> "UweL" == Uwe Ligges <ligges at statistik.uni-dortmund.de>
>>>>>     on Wed, 20 Sep 2006 01:03:30 +0200 writes:

    UweL> Brian Edward wrote:
    >> Hello all,
    >> 
    >> I have been a R user for about a year now, running on a MS Windows machine.
    >> I am in the process of making a complete switch to open-source.  Linux is a
    >> new world to me.  Ubuntu was my selection of the various distributions.
    >> Please pardon this very basic question (I was unable to locate an answer on
    >> R or Ubuntu).  I used Synaptic to download the necessary files to run.
    >> However, I was unable to locate the program using the Add/Remove feature.
    >> So, I created a Launcher for R on the desktop and identified the executable
    >> file.  The path I entered into the Command Line was:  /usr/bin/R
    >> I can run R in the Terminal, but not as a separate desktop location.  So,
    >> the short question is, what is the specific command line or configuration I
    >> should be using to run R?  Or, am I supposed to be running R in the
    >> Terminal?

    UweL> Short answer: yes, long answer, yes, unless you want to use some very 
    UweL> capable editor as an environment such ass Emacs + ESS. Look up the 
    UweL> documentation and the list archives fo details on Emacs and ESS.

Of course, I'm strongly suggesting the "unless", i.e. using
Emacs + ESS.

Further note that  Ubuntu (as all other Linux distributions
derived from Debian) provides ESS as a standard package you can
simply install, e.g., via Synaptic.
Note that you need to activate the 'Universe'
{in the sources that Synaptic or other package installers
 search} for that, but I assume you've done that anyway for the
R-related ubuntu packages.

And yes, I believe Ubuntu is a very good choice when upgrading
from Windows!

Martin


From rkrug at sun.ac.za  Wed Sep 20 15:52:25 2006
From: rkrug at sun.ac.za (Rainer M Krug)
Date: Wed, 20 Sep 2006 15:52:25 +0200
Subject: [R] Calculating mean together with split
Message-ID: <45114799.3000607@sun.ac.za>

Hi

I have a table called npl containing results of simulations.

It contains about 19000 entries and the structure looks like this:

  NoPlants                  sim run year DensPlants
1        6 lng_cs99_renosterbos   1    4    0.00192
.
.
.


it has 43 different entries for sim and year goes from 1 to 100, and run 
from 1 to 5.

I would like to calculate the mean of DensPlants for each simulation and 
each year seperately, i.e. calculating the mean for all combinations of 
sim and year over run.

I can use

split(npl, npl$sim)

to split npl into different groups each containing the entries for one 
parameterset - but where to go from there?

Rainer

-- 
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
Biology (UCT)

Department of Conservation Ecology and Entomology
University of Stellenbosch
Matieland 7602
South Africa

Tel:		+27 - (0)72 808 2975 (w)
Fax:		+27 - (0)21 808 3304
Cell:		+27 - (0)83 9479 042

email:	RKrug at sun.ac.za
       	Rainer at krugs.de


From geoffrey.russell at gmail.com  Wed Sep 20 15:54:16 2006
From: geoffrey.russell at gmail.com (Geoff Russell)
Date: Wed, 20 Sep 2006 23:24:16 +0930
Subject: [R] Stats question - cox proportional hazards adjustments
In-Reply-To: <x2odta1z97.fsf@viggo.kubism.ku.dk>
References: <93c3eada0609200409t51b0afbcqa57f45f9d2d9db61@mail.gmail.com>
	<x2odta1z97.fsf@viggo.kubism.ku.dk>
Message-ID: <93c3eada0609200654r25b91e7cmd402f99ddbb3cf97@mail.gmail.com>

Peter et al,

Thanks for the reply, I did reread the posting guide before posting and figured
it was a short question and might just have a short answer. I have
Therneau's book
on order, which will probably clarify the matter in time.

I understand stratifying to deal with confounding, but not adding it
as a covariate in a regression. e.g, If one of the gender
related effects you mention happens to be
drinking, then we don't want to "get rid of it", it may well
be an additional covariate and we want its full effect embodied in the
b value for
that covariate.

I'll keep reading!

Cheers,
Geoff



On 20 Sep 2006 14:47:00 +0200, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> "Geoff Russell" <geoffrey.russell at gmail.com> writes:
>
> > Hi useRs,
> >
> > Many studies of the link between red meat and colorectal cancer use
> > Cox proportional
> > hazards with (among other things) a gender covariate.
> >
> > If it is true that men eat more red meat, drink more alcohol and smoke more than
> > women, and if it is also true that alcohol and tobacco are known risk
> > factors then why does
> > it make sense to "adjust" for gender?   I would think that in this
> > case some of the
> > risk that should be properly attributed to the bad habits will actually end
> > up being attributed to being male instead.
>
> This is more than a bit off-topic for the list, but in (very) brief:
> Because you need to get rid of purely gender related effects that
> disturb the analysis and may create spurious association.
>
> Otherwise you would become able to "prove" effects like stiletto heels
> causing breast cancer, etc.
>
> --
>    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907
>


From mothsailor at googlemail.com  Wed Sep 20 16:03:04 2006
From: mothsailor at googlemail.com (David Barron)
Date: Wed, 20 Sep 2006 15:03:04 +0100
Subject: [R] Calculating mean together with split
In-Reply-To: <45114799.3000607@sun.ac.za>
References: <45114799.3000607@sun.ac.za>
Message-ID: <815b70590609200703l39a2a317x29561f9c77865ccc@mail.gmail.com>

Have a look at the function aggregate.table in the package gtools
(part of the gregmisc bundle).

On 20/09/06, Rainer M Krug <rkrug at sun.ac.za> wrote:
> Hi
>
> I have a table called npl containing results of simulations.
>
> It contains about 19000 entries and the structure looks like this:
>
>   NoPlants                  sim run year DensPlants
> 1        6 lng_cs99_renosterbos   1    4    0.00192
> .
> .
> .
>
>
> it has 43 different entries for sim and year goes from 1 to 100, and run
> from 1 to 5.
>
> I would like to calculate the mean of DensPlants for each simulation and
> each year seperately, i.e. calculating the mean for all combinations of
> sim and year over run.
>
> I can use
>
> split(npl, npl$sim)
>
> to split npl into different groups each containing the entries for one
> parameterset - but where to go from there?
>
> Rainer
>
> --
> Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
> Biology (UCT)
>
> Department of Conservation Ecology and Entomology
> University of Stellenbosch
> Matieland 7602
> South Africa
>
> Tel:            +27 - (0)72 808 2975 (w)
> Fax:            +27 - (0)21 808 3304
> Cell:           +27 - (0)83 9479 042
>
> email:  RKrug at sun.ac.za
>         Rainer at krugs.de
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From mothsailor at googlemail.com  Wed Sep 20 16:04:38 2006
From: mothsailor at googlemail.com (David Barron)
Date: Wed, 20 Sep 2006 15:04:38 +0100
Subject: [R] Calculating mean together with split
In-Reply-To: <45114799.3000607@sun.ac.za>
References: <45114799.3000607@sun.ac.za>
Message-ID: <815b70590609200704y75d3524r535b37221080c12@mail.gmail.com>

Sorry, that should have been package gdata, not gtools...they're both
in the same bundle, though.

On 20/09/06, Rainer M Krug <rkrug at sun.ac.za> wrote:
> Hi
>
> I have a table called npl containing results of simulations.
>
> It contains about 19000 entries and the structure looks like this:
>
>   NoPlants                  sim run year DensPlants
> 1        6 lng_cs99_renosterbos   1    4    0.00192
> .
> .
> .
>
>
> it has 43 different entries for sim and year goes from 1 to 100, and run
> from 1 to 5.
>
> I would like to calculate the mean of DensPlants for each simulation and
> each year seperately, i.e. calculating the mean for all combinations of
> sim and year over run.
>
> I can use
>
> split(npl, npl$sim)
>
> to split npl into different groups each containing the entries for one
> parameterset - but where to go from there?
>
> Rainer
>
> --
> Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
> Biology (UCT)
>
> Department of Conservation Ecology and Entomology
> University of Stellenbosch
> Matieland 7602
> South Africa
>
> Tel:            +27 - (0)72 808 2975 (w)
> Fax:            +27 - (0)21 808 3304
> Cell:           +27 - (0)83 9479 042
>
> email:  RKrug at sun.ac.za
>         Rainer at krugs.de
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From Thierry.ONKELINX at inbo.be  Wed Sep 20 16:07:31 2006
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Wed, 20 Sep 2006 16:07:31 +0200
Subject: [R] Simulation help
Message-ID: <2E9C414912813E4EB981326983E0A1040217D627@inexch.instnat.be.grp>

This should work. I try to avoid for loops because large (or nested) for
loops tend to consume a lot of memory. 

one.dataset <- function(dummy = 0, slope = 0.01){
  return(c(yr1= mean(rbinom(50,1,.5)), yr2 =mean(rbinom(50,1,.5 +
slope)), yr3 =mean(rbinom(50,1,.5 + 2 * slope)), yr4
=mean(rbinom(50,1,.5 + 3 * slope)), yr5 =mean(rbinom(50,1,.5 + 4 *
slope))))
}

datasets.slope01 <- t(sapply(1:100, one.dataset, slope = 0.01))
datasets.slope05 <- t(sapply(1:100, one.dataset, slope = 0.05))

Cheers,

Thierry

------------------------------------------------------------------------
----

ir. Thierry Onkelinx

Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
and Forest

Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance

Gaverstraat 4

9500 Geraardsbergen

Belgium

tel. + 32 54/436 185

Thierry.Onkelinx op inbo.be

www.inbo.be 

-----Oorspronkelijk bericht-----
Van: r-help-bounces op stat.math.ethz.ch
[mailto:r-help-bounces op stat.math.ethz.ch] Namens Zodet, Marc W. (AHRQ)
Verzonden: woensdag 20 september 2006 15:29
Aan: r-help op stat.math.ethz.ch
Onderwerp: [R] Simulation help

I'm trying to simulate trend data over a five year period.  I want
different trend profiles...the simplest being a linear trend.  I've been
using the following code:

 

patBdta1 <- NULL

for(i in 1:100)

patBdta1 <- rbind(patBdta1,c(yr1= mean(rbinom(50,1,.50)),

                             yr2 =mean(rbinom(50,1,.51)),

                             yr3 =mean(rbinom(50,1,.52)),

                             yr4 =mean(rbinom(50,1,.53)),

                             yr5 =mean(rbinom(50,1,.54))))

 

This code creates 100 data sets each with a 5 yr binomial trend profile
with a slope of approximately .01.

 

Now, what I want to do is pass this code (or some code) in such a way
that I can simulate various trend slopes (i.e., pass in a loop or
vectorize vs. copying/repeating code for each slope).

 

Any guidance is much appreciated.  Thanks!

 

Marc

 

Marc W. Zodet, MS

Senior Health Statistician

Agency for Healthcare Research and Quality


	[[alternative HTML version deleted]]

______________________________________________
R-help op stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From mothsailor at googlemail.com  Wed Sep 20 16:10:28 2006
From: mothsailor at googlemail.com (David Barron)
Date: Wed, 20 Sep 2006 15:10:28 +0100
Subject: [R] Calculating mean together with split
In-Reply-To: <815b70590609200704y75d3524r535b37221080c12@mail.gmail.com>
References: <45114799.3000607@sun.ac.za>
	<815b70590609200704y75d3524r535b37221080c12@mail.gmail.com>
Message-ID: <815b70590609200710q62ea4c4fw30cf44abe3aa2124@mail.gmail.com>

Of course, aggregate will work too, depends on how you want the output
to be formatted.  You could also look at summarize in the Hmisc
package.

On 20/09/06, David Barron <mothsailor at googlemail.com> wrote:
> Sorry, that should have been package gdata, not gtools...they're both
> in the same bundle, though.
>
> On 20/09/06, Rainer M Krug <rkrug at sun.ac.za> wrote:
> > Hi
> >
> > I have a table called npl containing results of simulations.
> >
> > It contains about 19000 entries and the structure looks like this:
> >
> >   NoPlants                  sim run year DensPlants
> > 1        6 lng_cs99_renosterbos   1    4    0.00192
> > .
> > .
> > .
> >
> >
> > it has 43 different entries for sim and year goes from 1 to 100, and run
> > from 1 to 5.
> >
> > I would like to calculate the mean of DensPlants for each simulation and
> > each year seperately, i.e. calculating the mean for all combinations of
> > sim and year over run.
> >
> > I can use
> >
> > split(npl, npl$sim)
> >
> > to split npl into different groups each containing the entries for one
> > parameterset - but where to go from there?
> >
> > Rainer
> >
> > --
> > Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
> > Biology (UCT)
> >
> > Department of Conservation Ecology and Entomology
> > University of Stellenbosch
> > Matieland 7602
> > South Africa
> >
> > Tel:            +27 - (0)72 808 2975 (w)
> > Fax:            +27 - (0)21 808 3304
> > Cell:           +27 - (0)83 9479 042
> >
> > email:  RKrug at sun.ac.za
> >         Rainer at krugs.de
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
> --
> =================================
> David Barron
> Said Business School
> University of Oxford
> Park End Street
> Oxford OX1 1HP
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From jsorkin at grecc.umaryland.edu  Wed Sep 20 16:11:27 2006
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Wed, 20 Sep 2006 10:11:27 -0400
Subject: [R] Stats question - cox proportional hazards adjustments
In-Reply-To: <93c3eada0609200654r25b91e7cmd402f99ddbb3cf97@mail.gmail.com>
References: <93c3eada0609200409t51b0afbcqa57f45f9d2d9db61@mail.gmail.com>
	<x2odta1z97.fsf@viggo.kubism.ku.dk>
	<93c3eada0609200654r25b91e7cmd402f99ddbb3cf97@mail.gmail.com>
Message-ID: <451113D0.A712.00CB.0@grecc.umaryland.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060920/5bac0d75/attachment.pl 

From rkrug at sun.ac.za  Wed Sep 20 17:01:04 2006
From: rkrug at sun.ac.za (Rainer M Krug)
Date: Wed, 20 Sep 2006 17:01:04 +0200
Subject: [R] Calculating mean together with split
In-Reply-To: <815b70590609200710q62ea4c4fw30cf44abe3aa2124@mail.gmail.com>
References: <45114799.3000607@sun.ac.za>	
	<815b70590609200704y75d3524r535b37221080c12@mail.gmail.com>
	<815b70590609200710q62ea4c4fw30cf44abe3aa2124@mail.gmail.com>
Message-ID: <451157B0.70105@sun.ac.za>

Hi David

aggregate is what I was looking for, as I wanted to have it in the 
tabular format to plot it.

Thanks

Rainer

David Barron wrote:
> Of course, aggregate will work too, depends on how you want the output
> to be formatted.  You could also look at summarize in the Hmisc
> package.
> 
> On 20/09/06, David Barron <mothsailor at googlemail.com> wrote:
>> Sorry, that should have been package gdata, not gtools...they're both
>> in the same bundle, though.
>>
>> On 20/09/06, Rainer M Krug <rkrug at sun.ac.za> wrote:
>> > Hi
>> >
>> > I have a table called npl containing results of simulations.
>> >
>> > It contains about 19000 entries and the structure looks like this:
>> >
>> >   NoPlants                  sim run year DensPlants
>> > 1        6 lng_cs99_renosterbos   1    4    0.00192
>> > .
>> > .
>> > .
>> >
>> >
>> > it has 43 different entries for sim and year goes from 1 to 100, and 
>> run
>> > from 1 to 5.
>> >
>> > I would like to calculate the mean of DensPlants for each simulation 
>> and
>> > each year seperately, i.e. calculating the mean for all combinations of
>> > sim and year over run.
>> >
>> > I can use
>> >
>> > split(npl, npl$sim)
>> >
>> > to split npl into different groups each containing the entries for one
>> > parameterset - but where to go from there?
>> >
>> > Rainer
>> >
>> > --
>> > Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
>> > Biology (UCT)
>> >
>> > Department of Conservation Ecology and Entomology
>> > University of Stellenbosch
>> > Matieland 7602
>> > South Africa
>> >
>> > Tel:            +27 - (0)72 808 2975 (w)
>> > Fax:            +27 - (0)21 808 3304
>> > Cell:           +27 - (0)83 9479 042
>> >
>> > email:  RKrug at sun.ac.za
>> >         Rainer at krugs.de
>> >
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>> >
>>
>>
>> -- 
>> =================================
>> David Barron
>> Said Business School
>> University of Oxford
>> Park End Street
>> Oxford OX1 1HP
>>
> 
> 

-- 
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
Biology (UCT)

Department of Conservation Ecology and Entomology
University of Stellenbosch
Matieland 7602
South Africa

Tel:		+27 - (0)72 808 2975 (w)
Fax:		+27 - (0)21 808 3304
Cell:		+27 - (0)83 9479 042

email:	RKrug at sun.ac.za
       	Rainer at krugs.de


From rkrug at sun.ac.za  Wed Sep 20 17:03:56 2006
From: rkrug at sun.ac.za (Rainer M Krug)
Date: Wed, 20 Sep 2006 17:03:56 +0200
Subject: [R] Beginners manual for emacs and ess
Message-ID: <4511585C.7070307@sun.ac.za>

Hi

I heard so much about Emacs and ESS that I decided to try it out - but I 
  am stuck at the beginning.

Is there anywhere a beginners manual for Emacs & ESS to be used with R? 
even M-x S tells me it can't start S-Plus - obviously - but I want it to 
start R...

Any help welcome (otherwise I will be stuck with Eclipse and R)

Rainer

-- 
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
Biology (UCT)

Department of Conservation Ecology and Entomology
University of Stellenbosch
Matieland 7602
South Africa

Tel:		+27 - (0)72 808 2975 (w)
Fax:		+27 - (0)21 808 3304
Cell:		+27 - (0)83 9479 042

email:	RKrug at sun.ac.za
       	Rainer at krugs.de


From ande1125 at msu.edu  Wed Sep 20 17:02:09 2006
From: ande1125 at msu.edu (Gretchen)
Date: Wed, 20 Sep 2006 11:02:09 -0400
Subject: [R] variance functions in glmmPQL or glm?
Message-ID: <200609201502.k8KF29sB002076@hypatia.math.ethz.ch>

Hello R users-
I am new to R, and tried searching the archives and literature for an answer
to this - please be patient if I missed something obvious.

I am fitting a logistic regression model, and would like to include variance
functions (specifically the varIdent function).  I cannot figure out how to
do this either in glmmPQL (or something similar) for the model with random
effects, or in glm for the model without random effects.  Is it possible to
fit a varIdent function in a generalized linear model?  If so, what are the
appropriate packages/functions to use?

Any help would be appreciated. 
Thank you,
Gretchen Anderson

 

M.Sc. Candidate

Dept. of Fisheries and Wildlife

Michigan State University

13 Natural Resources Building

East Lansing, MI 48824

Phone: (517) 353-0731

E-Mail: ande1125 at msu.edu


From mschwartz at mn.rr.com  Wed Sep 20 17:02:06 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Wed, 20 Sep 2006 10:02:06 -0500
Subject: [R] R CMD check fails at package
	dependencies	check	on	Fedora	Core 5, works on other systems
In-Reply-To: <17681.9229.193443.354540@mithrandir.hornik.net>
References: <200609191032.04483.Robert.King@newcastle.edu.au>
	<x2ac4wqqqs.fsf@turmalin.kubism.ku.dk>
	<450FC020.4070304@newcastle.edu.au>
	<450FDFAC.6070209@newcastle.edu.au>
	<1158682156.3890.20.camel@localhost.localdomain>
	<17681.9229.193443.354540@mithrandir.hornik.net>
Message-ID: <1158764526.3872.17.camel@localhost.localdomain>

On Wed, 2006-09-20 at 13:20 +0200, Kurt Hornik wrote:
> >>>>> Marc Schwartz (via MN) writes:
> 
> > On Tue, 2006-09-19 at 22:16 +1000, Robert King wrote:
> >> Here is another thing that might help work out what is happening.  If I 
> >> use --no-install, ade4 actually fails as well, in the same way as zipfR.
> >> 
> >> [Desktop]$ R CMD check --no-install ade4
> >> * checking for working latex ... OK
> >> * using log directory '/home/rak776/Desktop/ade4.Rcheck'
> >> * using Version 2.3.1 (2006-06-01)
> >> * checking for file 'ade4/DESCRIPTION' ... OK
> >> * this is package 'ade4' version '1.4-1'
> >> * checking if this is a source package ... OK
> >> * checking package directory ... OK
> >> * checking for portable file names ... OK
> >> * checking for sufficient/correct file permissions ... OK
> >> * checking DESCRIPTION meta-information ... ERROR
> >> 
> >> [Desktop]$ R CMD check --no-install zipfR
> >> * checking for working latex ... OK
> >> * using log directory '/home/rak776/Desktop/zipfR.Rcheck'
> >> * using Version 2.3.1 (2006-06-01)
> >> * checking for file 'zipfR/DESCRIPTION' ... OK
> >> * checking extension type ... Package
> >> * this is package 'zipfR' version '0.6-0'
> >> * checking if this is a source package ... OK
> >> * checking package directory ... OK
> >> * checking for portable file names ... OK
> >> * checking for sufficient/correct file permissions ... OK
> >> * checking DESCRIPTION meta-information ... ERROR
> 
> > <snip>
> 
> > Robert,
> 
> > I tried the process last night (my time) using the initial instructions
> > on my FC5 system with:
> 
> > $ R --version
> > R version 2.3.1 Patched (2006-08-06 r38829)
> > Copyright (C) 2006 R Development Core Team
> 
> 
> > I could not replicate the problem.
> 
> > However, this morning, with your additional communication:
> 
> > $ R CMD check --no-install zipfR_0.6-0.tar.gz
> > * checking for working latex ... OK
> > * using log directory '/home/marcs/Downloads/zipfR.Rcheck'
> > * using Version 2.3.1 Patched (2006-08-06 r38829)
> > * checking for file 'zipfR/DESCRIPTION' ... OK
> > * checking extension type ... Package
> > * this is package 'zipfR' version '0.6-0'
> > * checking if this is a source package ... OK
> > * checking package directory ... OK
> > * checking for portable file names ... OK
> > * checking for sufficient/correct file permissions ... OK
> > * checking DESCRIPTION meta-information ... OK
> > * checking top-level files ... OK
> > * checking index information ... OK
> > * checking package subdirectories ... OK
> > * checking R files for syntax errors ... OK
> > * checking R files for library.dynam ... OK
> > * checking S3 generic/method consistency ... OK
> > * checking replacement functions ... OK
> > * checking foreign function calls ... OK
> > * checking Rd files ... OK
> > * checking Rd cross-references ... WARNING
> > Warning in grep(pattern, x, ignore.case, extended, value, fixed,
> > useBytes) :
> >          input string 70 is invalid in this locale
> > * checking for missing documentation entries ... WARNING
> > Warning in grep(pattern, x, ignore.case, extended, value, fixed,
> > useBytes) :
> >          input string 70 is invalid in this locale
> > All user-level objects in a package should have documentation entries.
> > See chapter 'Writing R documentation files' in manual 'Writing R
> > Extensions'.
> > * checking for code/documentation mismatches ... OK
> > * checking Rd \usage sections ... OK
> > * checking DVI version of manual ... OK
> 
> > WARNING: There were 2 warnings, see
> >   /home/marcs/Downloads/zipfR.Rcheck/00check.log
> > for details
> 
> 
> 
> > So I am wondering if this raises the possibility of a locale issue on
> > your FC5 system resulting in a problem reading DESCRIPTION files?  It
> > may be totally unrelated, but one never knows I suppose. Mine is:
> 
> > $ locale
> > LANG=en_US.UTF-8
> > LC_CTYPE="en_US.UTF-8"
> > LC_NUMERIC="en_US.UTF-8"
> > LC_TIME="en_US.UTF-8"
> > LC_COLLATE="en_US.UTF-8"
> > LC_MONETARY="en_US.UTF-8"
> > LC_MESSAGES="en_US.UTF-8"
> > LC_PAPER="en_US.UTF-8"
> > LC_NAME="en_US.UTF-8"
> > LC_ADDRESS="en_US.UTF-8"
> > LC_TELEPHONE="en_US.UTF-8"
> > LC_MEASUREMENT="en_US.UTF-8"
> > LC_IDENTIFICATION="en_US.UTF-8"
> > LC_ALL=
> 
> 
> > HTH,
> 
> > Marc Schwartz
> 
> That's a bug in tools:::Rd_aliases (it needs to preprocess the Rd lines,
> which re-encodes if necessary and possible).
> 
> I'll commit a fix later today.
> 
> Thanks for spotting this.
> 
> Best
> -k

Thanks for noting this Kurt!

Regards,

Marc


From pchen at uni-bielefeld.de  Wed Sep 20 17:09:59 2006
From: pchen at uni-bielefeld.de (pchen at uni-bielefeld.de)
Date: Wed, 20 Sep 2006 11:09:59 -0400
Subject: [R] local intallation of MSBVAR_0.2.0.tar.gz
In-Reply-To: <451157B0.70105@sun.ac.za>
References: <45114799.3000607@sun.ac.za>
	<815b70590609200704y75d3524r535b37221080c12@mail.gmail.com>
	<815b70590609200710q62ea4c4fw30cf44abe3aa2124@mail.gmail.com>
	<451157B0.70105@sun.ac.za>
Message-ID: <edccf41f6b8d.6b8dedccf41f@uni-bielefeld.de>


Hello, 

can someone tell me how to install a package 
like "MSBVAR_0.2.0.tar.gz" locllay ?

thanks

----- Originalnachricht -----
Von: Rainer M Krug <rkrug at sun.ac.za>
Datum: Mittwoch, September 20, 2006 11:06 am
Betreff: Re: [R] Calculating mean together with split
An: David Barron <mothsailor at googlemail.com>
CC: r-help <r-help at stat.math.ethz.ch>

> Hi David
> 
> aggregate is what I was looking for, as I wanted to have it in the 
> tabular format to plot it.
> 
> Thanks
> 
> Rainer
> 
> David Barron wrote:
> > Of course, aggregate will work too, depends on how you want the 
> output> to be formatted.  You could also look at summarize in the 
> Hmisc> package.
> > 
> > On 20/09/06, David Barron <mothsailor at googlemail.com> wrote:
> >> Sorry, that should have been package gdata, not 
> gtools...they're both
> >> in the same bundle, though.
> >>
> >> On 20/09/06, Rainer M Krug <rkrug at sun.ac.za> wrote:
> >> > Hi
> >> >
> >> > I have a table called npl containing results of simulations.
> >> >
> >> > It contains about 19000 entries and the structure looks like 
> this:>> >
> >> >   NoPlants                  sim run year DensPlants
> >> > 1        6 lng_cs99_renosterbos   1    4    0.00192
> >> > .
> >> > .
> >> > .
> >> >
> >> >
> >> > it has 43 different entries for sim and year goes from 1 to 
> 100, and 
> >> run
> >> > from 1 to 5.
> >> >
> >> > I would like to calculate the mean of DensPlants for each 
> simulation 
> >> and
> >> > each year seperately, i.e. calculating the mean for all 
> combinations of
> >> > sim and year over run.
> >> >
> >> > I can use
> >> >
> >> > split(npl, npl$sim)
> >> >
> >> > to split npl into different groups each containing the 
> entries for one
> >> > parameterset - but where to go from there?
> >> >
> >> > Rainer
> >> >
> >> > --
> >> > Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
> >> > Biology (UCT)
> >> >
> >> > Department of Conservation Ecology and Entomology
> >> > University of Stellenbosch
> >> > Matieland 7602
> >> > South Africa
> >> >
> >> > Tel:            +27 - (0)72 808 2975 (w)
> >> > Fax:            +27 - (0)21 808 3304
> >> > Cell:           +27 - (0)83 9479 042
> >> >
> >> > email:  RKrug at sun.ac.za
> >> >         Rainer at krugs.de
> >> >
> >> > ______________________________________________
> >> > R-help at stat.math.ethz.ch mailing list
> >> > https://stat.ethz.ch/mailman/listinfo/r-help
> >> > PLEASE do read the posting guide 
> >> http://www.R-project.org/posting-guide.html
> >> > and provide commented, minimal, self-contained, reproducible 
> code.>> >
> >>
> >>
> >> -- 
> >> =================================
> >> David Barron
> >> Said Business School
> >> University of Oxford
> >> Park End Street
> >> Oxford OX1 1HP
> >>
> > 
> > 
> 
> -- 
> Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
> Biology (UCT)
> 
> Department of Conservation Ecology and Entomology
> University of Stellenbosch
> Matieland 7602
> South Africa
> 
> Tel:        	+27 - (0)72 808 2975 (w)
> Fax:        	+27 - (0)21 808 3304
> Cell:        	+27 - (0)83 9479 042
> 
> email:	RKrug at sun.ac.za
>       	Rainer at krugs.de
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.htmland provide commented, minimal, self-contained, 
> reproducible code.
>


From pchen at uni-bielefeld.de  Wed Sep 20 17:12:08 2006
From: pchen at uni-bielefeld.de (pchen at uni-bielefeld.de)
Date: Wed, 20 Sep 2006 11:12:08 -0400
Subject: [R] local intallation of MSBVAR_0.2.0.tar.gz
In-Reply-To: <edccf41f6b8d.6b8dedccf41f@uni-bielefeld.de>
References: <45114799.3000607@sun.ac.za>
	<815b70590609200704y75d3524r535b37221080c12@mail.gmail.com>
	<815b70590609200710q62ea4c4fw30cf44abe3aa2124@mail.gmail.com>
	<451157B0.70105@sun.ac.za> <edccf41f6b8d.6b8dedccf41f@uni-bielefeld.de>
Message-ID: <eea6a8fd4867.4867eea6a8fd@uni-bielefeld.de>


Hello, 

can someone tell me how to install a package 
like "MSBVAR_0.2.0.tar.gz" locllay ?

thanks


From mschwartz at mn.rr.com  Wed Sep 20 17:16:56 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Wed, 20 Sep 2006 10:16:56 -0500
Subject: [R] Beginners manual for emacs and ess
In-Reply-To: <4511585C.7070307@sun.ac.za>
References: <4511585C.7070307@sun.ac.za>
Message-ID: <1158765416.3872.28.camel@localhost.localdomain>

On Wed, 2006-09-20 at 17:03 +0200, Rainer M Krug wrote:
> Hi
> 
> I heard so much about Emacs and ESS that I decided to try it out - but I 
>   am stuck at the beginning.
> 
> Is there anywhere a beginners manual for Emacs & ESS to be used with R? 
> even M-x S tells me it can't start S-Plus - obviously - but I want it to 
> start R...
> 
> Any help welcome (otherwise I will be stuck with Eclipse and R)
> 
> Rainer


There are some reference materials on the main ESS site at:

  http://ess.r-project.org/

In addition, there is a dedicated ESS mailing list, with more info here:

  https://stat.ethz.ch/mailman/listinfo/ess-help

HTH,

Marc Schwartz


From mschwartz at mn.rr.com  Wed Sep 20 17:22:44 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Wed, 20 Sep 2006 10:22:44 -0500
Subject: [R] [Rd] Sweave processes \Sexpr in commented LaTeX
	source	(2.3.1patched and 2.4.0)
In-Reply-To: <b0808fdc0609200009w2375ec5dne71a0cc2875fc553@mail.gmail.com>
References: <1158711280.5348.24.camel@localhost.localdomain>
	<b0808fdc0609200009w2375ec5dne71a0cc2875fc553@mail.gmail.com>
Message-ID: <1158765764.3872.31.camel@localhost.localdomain>

On Wed, 2006-09-20 at 09:09 +0200, Antonio, Fabio Di Narzo wrote:
> Hi.
> 
> 2006/9/20, Marc Schwartz <MSchwartz at mn.rr.com>:
> > Hi all,
> >
> > On FC5, using:
> >
> >   Version 2.3.1 Patched (2006-08-06 r38829)
> >
> > and today's
> >
> >   R version 2.4.0 alpha (2006-09-19 r39397)
> >
> > with the following .Rnw file:
> >
> >
> > \documentclass[10pt]{article}
> > \begin{document}
> >
> >    This line should print '2': \Sexpr{1 + 1}
> > %% This line should NOT print '2': \Sexpr{1 + 1}
> 
> If it's just a comment, why don't use something like:
> % \ Sexpr (del the space)
> or
> %\sexpr (change 'sexpr' with 'Sexpr')
> or
> %...the 'Sexpr' command (add a backslash in latex code)
> ?
> 
> Antonio.

See my comments in this post on r-devel, where this thread was
originally started:

  https://stat.ethz.ch/pipermail/r-devel/2006-September/039416.html

HTH,

Marc


From joe-byers at utulsa.edu  Wed Sep 20 17:23:37 2006
From: joe-byers at utulsa.edu (Joe Byers)
Date: Wed, 20 Sep 2006 10:23:37 -0500
Subject: [R] Comment blocks in R programs
Message-ID: <eermcm$jbq$1@sea.gmane.org>

All,

Is there a way to add comment blocks in an R script other than using # 
at the beginning of each line?  Or, is there anything like ndocs for C++ 
and the markup for Java that can generate documentation?

This may be a bad question, but I have searched R documentation and 
can't seem to find an Answer.  I have also looked at the R package 
documentation mark up docs for help files but that was more than I wanted.


thank you for you help.
Joe


From gavin.simpson at ucl.ac.uk  Wed Sep 20 17:28:46 2006
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Wed, 20 Sep 2006 16:28:46 +0100
Subject: [R] local intallation of MSBVAR_0.2.0.tar.gz
In-Reply-To: <eea6a8fd4867.4867eea6a8fd@uni-bielefeld.de>
References: <45114799.3000607@sun.ac.za>
	<815b70590609200704y75d3524r535b37221080c12@mail.gmail.com>
	<815b70590609200710q62ea4c4fw30cf44abe3aa2124@mail.gmail.com>
	<451157B0.70105@sun.ac.za> <edccf41f6b8d.6b8dedccf41f@uni-bielefeld.de>
	<eea6a8fd4867.4867eea6a8fd@uni-bielefeld.de>
Message-ID: <1158766126.20600.56.camel@gsimpson.geog.ucl.ac.uk>

On Wed, 2006-09-20 at 11:12 -0400, pchen at uni-bielefeld.de wrote:
> Hello, 
> 
> can someone tell me how to install a package 
> like "MSBVAR_0.2.0.tar.gz" locllay ?
> 
> thanks

Unix/Linux or Windows? If Windows, forget the tar.gz file (unless you
are set up to compile source code), you'd probably be better off with
the zip binary. Download it instead and then look at the menus in R-GUI
for the option to install from local zip file - I rarely use Windows
these days so forget which menu it is in now.

If Unix/linux, then:

R CMD INSTALL MSBVAR_0.2.0tar.gz

or

R CMD INSTALL -l /path/to/lib MSBVAR_0.2.0tar.gz

will do what you want, the -l /path thingy allows you to install to a
specified library.

All this is explained in the R Installation and Administration manual,
that you can find here (html):

http://cran.r-project.org/doc/manuals/R-admin.html

HTH

G

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC & ENSIS, UCL Geography,  [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From h.wickham at gmail.com  Wed Sep 20 17:39:28 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Wed, 20 Sep 2006 10:39:28 -0500
Subject: [R] Calculating mean together with split
In-Reply-To: <45114799.3000607@sun.ac.za>
References: <45114799.3000607@sun.ac.za>
Message-ID: <f8e6ff050609200839l697b514er4ada815f64c6fe92@mail.gmail.com>

> It contains about 19000 entries and the structure looks like this:
>
>   NoPlants                  sim run year DensPlants
> 1        6 lng_cs99_renosterbos   1    4    0.00192
> .
> .
> .
>
> it has 43 different entries for sim and year goes from 1 to 100, and run
> from 1 to 5.
>
> I would like to calculate the mean of DensPlants for each simulation and
> each year seperately, i.e. calculating the mean for all combinations of
> sim and year over run.

You can do this pretty easily with the reshape package:

library(reshape)
dfm <- rename(df, c(DensPlants = value)) # this is the form that reshape wants

# Then try one of these:

cast(dfm, year ~ sim)
cast(dfm, year + sim ~ . )
cast(dfm, year ~ sim, margins=TRUE)

Depending on what format you want the resulting summaries in.

Hadley


From ggrothendieck at gmail.com  Wed Sep 20 17:39:51 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 20 Sep 2006 11:39:51 -0400
Subject: [R] Comment blocks in R programs
In-Reply-To: <eermcm$jbq$1@sea.gmane.org>
References: <eermcm$jbq$1@sea.gmane.org>
Message-ID: <971536df0609200839g2c81e69ci2b09be294d58d554@mail.gmail.com>

Try this:

if (FALSE) {
... whatever ...
}

On 9/20/06, Joe Byers <joe-byers at utulsa.edu> wrote:
> All,
>
> Is there a way to add comment blocks in an R script other than using #
> at the beginning of each line?  Or, is there anything like ndocs for C++
> and the markup for Java that can generate documentation?
>
> This may be a bad question, but I have searched R documentation and
> can't seem to find an Answer.  I have also looked at the R package
> documentation mark up docs for help files but that was more than I wanted.
>
>
> thank you for you help.
> Joe
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Wed Sep 20 17:41:20 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 20 Sep 2006 11:41:20 -0400
Subject: [R] Comment blocks in R programs
In-Reply-To: <971536df0609200839g2c81e69ci2b09be294d58d554@mail.gmail.com>
References: <eermcm$jbq$1@sea.gmane.org>
	<971536df0609200839g2c81e69ci2b09be294d58d554@mail.gmail.com>
Message-ID: <971536df0609200841x4a625e49ld1d814ef1027021b@mail.gmail.com>

I should have noted that the ...whatever... must be valid R.
You could also do this:

" Here are some
comments.
"



On 9/20/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Try this:
>
> if (FALSE) {
> ... whatever ...
> }
>
> On 9/20/06, Joe Byers <joe-byers at utulsa.edu> wrote:
> > All,
> >
> > Is there a way to add comment blocks in an R script other than using #
> > at the beginning of each line?  Or, is there anything like ndocs for C++
> > and the markup for Java that can generate documentation?
> >
> > This may be a bad question, but I have searched R documentation and
> > can't seem to find an Answer.  I have also looked at the R package
> > documentation mark up docs for help files but that was more than I wanted.
> >
> >
> > thank you for you help.
> > Joe
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From rfrancois at mango-solutions.com  Wed Sep 20 17:49:44 2006
From: rfrancois at mango-solutions.com (Romain Francois)
Date: Wed, 20 Sep 2006 16:49:44 +0100
Subject: [R] Comment blocks in R programs
In-Reply-To: <971536df0609200839g2c81e69ci2b09be294d58d554@mail.gmail.com>
References: <eermcm$jbq$1@sea.gmane.org>
	<971536df0609200839g2c81e69ci2b09be294d58d554@mail.gmail.com>
Message-ID: <45116318.2020204@mango-solutions.com>

Gabor Grothendieck wrote:
> Try this:
>
> if (FALSE) {
> ... whatever ...
> }
>   

Problem is,
... whatever ...
must be syntactically correct to use that trick.

Why not using a text editor that does rectangular selection or 
commenting several lines at a time, there are many out there to propose 
that kind of feature.

Cheers,

Romain


> On 9/20/06, Joe Byers <joe-byers at utulsa.edu> wrote:
>   
>> All,
>>
>> Is there a way to add comment blocks in an R script other than using #
>> at the beginning of each line?  Or, is there anything like ndocs for C++
>> and the markup for Java that can generate documentation?
>>
>> This may be a bad question, but I have searched R documentation and
>> can't seem to find an Answer.  I have also looked at the R package
>> documentation mark up docs for help files but that was more than I wanted.
>>
>>
>> thank you for you help.
>> Joe
>>     

-- 
*mangosolutions*
/data analysis that delivers/

Tel   +44 1249 467 467
Fax   +44 1249 467 468


From cberry at tajo.ucsd.edu  Wed Sep 20 17:54:10 2006
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Wed, 20 Sep 2006 08:54:10 -0700
Subject: [R] acos(0.5) == pi/3 FALSE
In-Reply-To: <62777.91.89.65.114.1158697877.squirrel@mail.panix.com>
References: <200609181931.46198.inaki.murillo@ehu.es>
	<XFMail.060918230505.Ted.Harding@nessie.mcc.ac.uk>
	<loom.20060919T003312-658@post.gmane.org>
	<x2irjkzsis.fsf@turmalin.kubism.ku.dk>
	<62777.91.89.65.114.1158697877.squirrel@mail.panix.com>
Message-ID: <Pine.LNX.4.64.0609200835510.27801@tajo.ucsd.edu>

On Tue, 19 Sep 2006, Johannes H?sing wrote:

> Peter Dalgaard:
>> Ben Bolker <bolker at zoo.ufl.edu> writes:
>>>          1. compose your response
>> I've always wondered why step 1. - often the time-consuming bit - is not
>> listed last.
>
> The advice applies to the situation when answering immediately would be
> your knee-jerk reaction. It is assumed that actually composing and sending
> the mail would take very little time and thought, whereas coming around to
> answering it after runif(1)*4 hours would take considerably more time, even
> when mulitiplied with the probability that you are still the first one.
>
> Looking at the submission times of questions and answers in this
> particular case, though, I would be upset if the helpful guys actually
> used this algorithm. Most of the answers were submitted after 3.5 to 4 h
> time, thus revealing a possible flaw of the random number generator
> underlying runif().

Johannes,

Turn on 'full-headers' in your email reader.

Most of the replies were submitted within 20 minutes of the posting of the 
original query by the list-serv (to me and I assume to others) and several 
that said essentially the same thing were posted within the first 10 
minutes, I recall.

The list-serv held the initial email for a couple of hours before passing 
it on. The replies are processed more rapidly, being held at most a few 
minutes each.

Given the initial hold placed on that email, runif(1)*4 hours would have 
increased the overall response time (from time of initial posting to time 
of first response) by less than 25% (with high probability). And would 
have saved several respondents from having to type up their replies.

In this case even runif(1)*20 minutes would likely have cut the response 
traffic to one or two and would have increased the overall response time 
by less than 10 minutes.

Chuck

>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Charles C. Berry                        (858) 534-2098
                                          Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	         UC San Diego
http://biostat.ucsd.edu/~cberry/         La Jolla, San Diego 92093-0717


From AnupTyagi at yahoo.com  Wed Sep 20 18:09:44 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Wed, 20 Sep 2006 16:09:44 +0000 (UTC)
Subject: [R] Union of two data frames
References: <97faa3210609191418x5b18805fr8b33f5f017ac5454@mail.gmail.com>
Message-ID: <loom.20060920T180819-351@post.gmane.org>

Kartik Pappu <kartik.pappu <at> gmail.com> writes:

> Essentially, I want to make a union of the two data frames. I hope
> this question makes sense.

See merge(...), and have a look at R intro. Also check documentation for
"Design" package. Anupam.


From f.harrell at vanderbilt.edu  Wed Sep 20 18:15:25 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 20 Sep 2006 11:15:25 -0500
Subject: [R] Linux configuration (Ubuntu)
In-Reply-To: <451140EF.5030501@sun.ac.za>
References: <1e6b5c080609191527x451a7fdjd024e660f054ce44@mail.gmail.com>	<45107742.5060907@statistik.uni-dortmund.de>
	<45113260.4040909@vanderbilt.edu> <451140EF.5030501@sun.ac.za>
Message-ID: <4511691D.2080905@vanderbilt.edu>

Rainer M Krug wrote:
> 
> 
> Frank E Harrell Jr wrote:
> SNIP
> 
>> After R starts you can manage help files nicely using help.start() to 
>> use a browser.  I often use dillo, the world's fastest graphical 
>> browser, by specifying options(browser='dillo') before help.start().
> 
> WOW - dillo is BRILLIANT for this purpose.
> Thanks for the tip!
> Do you have any idea on how to convince dillo to reuse the same browser 
> window again instead of using a new one when using ? ?

Good question.  No, don't know.  Hope someone can figure it out.

By the way a handy use of kate is kate -u from the command line to open 
a file into an existing kate session, or start a session if needed.

Frank

> 
>>
>> Frank
>>
>>>
>>>
>>>> Thanks in advance,
>>>> Brian


From AnupTyagi at yahoo.com  Wed Sep 20 18:12:24 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Wed, 20 Sep 2006 16:12:24 +0000 (UTC)
Subject: [R] Beginners manual for emacs and ess
References: <4511585C.7070307@sun.ac.za>
Message-ID: <loom.20060920T181027-482@post.gmane.org>

Rainer M Krug <rkrug <at> sun.ac.za> writes:

> Is there anywhere a beginners manual for Emacs & ESS to be used with R? 
> even M-x S tells me it can't start S-Plus - obviously - but I want it to 
> start R...

Please also look at John Fox's Xemacs+ESS intro.
http://socserv.mcmaster.ca/jfox/Books/Companion/ESS/
Anupam.


From h.wickham at gmail.com  Wed Sep 20 18:16:14 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Wed, 20 Sep 2006 11:16:14 -0500
Subject: [R] Calculating mean together with split
In-Reply-To: <f8e6ff050609200839l697b514er4ada815f64c6fe92@mail.gmail.com>
References: <45114799.3000607@sun.ac.za>
	<f8e6ff050609200839l697b514er4ada815f64c6fe92@mail.gmail.com>
Message-ID: <f8e6ff050609200916u347c9b3ei23b0170945f9ec4f@mail.gmail.com>

> # Then try one of these:
>
> cast(dfm, year ~ sim)
> cast(dfm, year + sim ~ . )
> cast(dfm, year ~ sim, margins=TRUE)

Oops that should be:

dfm <- rename(df, c(DensPlants = "value"))

cast(dfm, year ~ sim, mean)
cast(dfm, year + sim ~ . , mean)
cast(dfm, year ~ sim, mean, margins=TRUE)

(Thanks for pointing that out Gabor!)

Hadley


From Cameron.Guenther at MyFWC.com  Wed Sep 20 18:42:14 2006
From: Cameron.Guenther at MyFWC.com (Guenther, Cameron)
Date: Wed, 20 Sep 2006 12:42:14 -0400
Subject: [R]  help with function
Message-ID: <BA6FF017E924044A9BF748AFAEEA6F30A533F5@FWC-TLEX3.fwc.state.fl.us>

Hello everyone,

I have a function here that I wrote but doesn't seem to work quite
right.  Attached is the code.  In the calib funcion under the for loop
Bt[i+2]<-(1-m)*Bt[i+1]+Rt[i]*Rerr-Ct[i+1] returns NA's for everything
after years 1983 and 1984.  However the code works when it reads
Bt[i+2]<-(1-m)*Bt[i+1]+Rt[i]*Rerr-Ct[i].  I don't quite understand why
since it should be calculating all of the necessary inputs prior to
calculating Bt[i+2].  Any help would be greatly appreciated.

Thanks

#Model parameters
B0<-75000000
m<-0.3
R0<-B0*m
z<-0.8
a<-B0/R0*(1-(z-0.2)/(0.8*z))
b<-(z-0.2)/(0.8*z*R0)
dat<-data.frame(years=seq(1983,2004),cobs=c(19032324,19032324,17531618,2
0533029,20298099,20793744,23519369,23131780,19922247,17274513,17034419,1
2448318,4551585,4226451,7183688,7407924,7538366,7336039,8869193,7902341,
6369089,6211886))
stdr<-runif(100,0,0.5)
stdc<-runif(100,0,0.5)
BC<-runif(1000,0,100)


#model calibration

calib<-function(x){
 v<-sample(stdr,1)
 cr<-sample(stdc,1)
 N<-rnorm(1)
 Bq<-sample(BC,1)
 Rerr<-exp(N*v-(v^2/2))
 Cerr<-exp(N*cr-(cr^2/2))
 Bt<-vector();Bt[1]=B0;Bt[2]=B0
 Rt<-vector()
 Ct<-vector()
 for (i in 1:length(x$years)){
  Ct[i]<-1/Bq*Bt[i]*Cerr
  Rt[i]<-Bt[i]/(a+b*Bt[i])
  Bt[i+2]<-(1-m)*Bt[i+1]+Rt[i]*Rerr-Ct[i+1]
 }
  out<-new.env()
  out$yr<-x$years[1:length(x$years)]
  out$Bt<-Bt[1:length(x$years)]
  out$Rt<-Rt[1:length(x$years)]
  out$Ct<-Ct[1:length(x$years)]
  out$stdr<-v
  out$stdc<-cr
  out$Bq<-Bq
  out$Rerr<-Rerr
  out$Cerr<-Cerr
  return(as.list(out))
 }
 test<-calib(dat)


Cameron Guenther, Ph.D. 
Associate Research Scientist
FWC/FWRI, Marine Fisheries Research
100 8th Avenue S.E.
St. Petersburg, FL 33701
(727)896-8626 Ext. 4305
cameron.guenther at myfwc.com


From Cameron.Guenther at MyFWC.com  Wed Sep 20 18:42:14 2006
From: Cameron.Guenther at MyFWC.com (Guenther, Cameron)
Date: Wed, 20 Sep 2006 12:42:14 -0400
Subject: [R]  help with function
Message-ID: <BA6FF017E924044A9BF748AFAEEA6F30A533F5@FWC-TLEX3.fwc.state.fl.us>

Hello everyone,

I have a function here that I wrote but doesn't seem to work quite
right.  Attached is the code.  In the calib funcion under the for loop
Bt[i+2]<-(1-m)*Bt[i+1]+Rt[i]*Rerr-Ct[i+1] returns NA's for everything
after years 1983 and 1984.  However the code works when it reads
Bt[i+2]<-(1-m)*Bt[i+1]+Rt[i]*Rerr-Ct[i].  I don't quite understand why
since it should be calculating all of the necessary inputs prior to
calculating Bt[i+2].  Any help would be greatly appreciated.

Thanks

#Model parameters
B0<-75000000
m<-0.3
R0<-B0*m
z<-0.8
a<-B0/R0*(1-(z-0.2)/(0.8*z))
b<-(z-0.2)/(0.8*z*R0)
dat<-data.frame(years=seq(1983,2004),cobs=c(19032324,19032324,17531618,2
0533029,20298099,20793744,23519369,23131780,19922247,17274513,17034419,1
2448318,4551585,4226451,7183688,7407924,7538366,7336039,8869193,7902341,
6369089,6211886))
stdr<-runif(100,0,0.5)
stdc<-runif(100,0,0.5)
BC<-runif(1000,0,100)


#model calibration

calib<-function(x){
 v<-sample(stdr,1)
 cr<-sample(stdc,1)
 N<-rnorm(1)
 Bq<-sample(BC,1)
 Rerr<-exp(N*v-(v^2/2))
 Cerr<-exp(N*cr-(cr^2/2))
 Bt<-vector();Bt[1]=B0;Bt[2]=B0
 Rt<-vector()
 Ct<-vector()
 for (i in 1:length(x$years)){
  Ct[i]<-1/Bq*Bt[i]*Cerr
  Rt[i]<-Bt[i]/(a+b*Bt[i])
  Bt[i+2]<-(1-m)*Bt[i+1]+Rt[i]*Rerr-Ct[i+1]
 }
  out<-new.env()
  out$yr<-x$years[1:length(x$years)]
  out$Bt<-Bt[1:length(x$years)]
  out$Rt<-Rt[1:length(x$years)]
  out$Ct<-Ct[1:length(x$years)]
  out$stdr<-v
  out$stdc<-cr
  out$Bq<-Bq
  out$Rerr<-Rerr
  out$Cerr<-Cerr
  return(as.list(out))
 }
 test<-calib(dat)


Cameron Guenther, Ph.D. 
Associate Research Scientist
FWC/FWRI, Marine Fisheries Research
100 8th Avenue S.E.
St. Petersburg, FL 33701
(727)896-8626 Ext. 4305
cameron.guenther at myfwc.com


From edd at debian.org  Wed Sep 20 18:45:47 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 20 Sep 2006 11:45:47 -0500
Subject: [R] Linux configuration (Ubuntu)
In-Reply-To: <17681.17759.24307.828937@stat.math.ethz.ch>
References: <1e6b5c080609191527x451a7fdjd024e660f054ce44@mail.gmail.com>
	<45107742.5060907@statistik.uni-dortmund.de>
	<17681.17759.24307.828937@stat.math.ethz.ch>
Message-ID: <17681.28731.422508.417055@basebud.nulle.part>


On 20 September 2006 at 15:42, Martin Maechler wrote:
| Further note that  Ubuntu (as all other Linux distributions
| derived from Debian) provides ESS as a standard package you can
| simply install, e.g., via Synaptic.
| Note that you need to activate the 'Universe'
| {in the sources that Synaptic or other package installers
|  search} for that, but I assume you've done that anyway for the
| R-related ubuntu packages.

At this point, it may be worth recalling that there are 

	-- eleven 'core' R packages incl documentation and r-mathlib
	-- around 80 CRAN packages ready to install to extend R
	-- packages for ess as mentioned in this thread
	-- RPy (R from Python) support via the python-rpy package
	-- Ggobi via the ggobi package
	-- and even R inside PostgreSQL via postgresql-$VER-plr

for Debian and Ubuntu.  Not everything may be available at all 'flavours' but
Debian testing and Ubuntu dapper are well covered. 
 
| And yes, I believe Ubuntu is a very good choice when upgrading
| from Windows!

Yup, though I personally prefer KUbuntu.  That said, all indications are that
the new Debian installer will be very powerful.  Hopefully in December ...

Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From AnupTyagi at yahoo.com  Wed Sep 20 18:48:49 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Wed, 20 Sep 2006 16:48:49 +0000 (UTC)
Subject: [R] Adding percentage to Pie Charts (was (no subject))
References: <07E228A5BE53C24CAD490193A7381BBB5C4A7D@LP-EXCHVS07.CO.IHC.COM>
Message-ID: <loom.20060920T183011-62@post.gmane.org>

Greg Snow <Greg.Snow <at> intermountainmail.org> writes:

> Have you read the books by Cleveland?

I do not recall reading Cleveland's book; I have read one by Tufte. You raise
some interesting issues there. I agree with some, I could not clearly understand
some other things you mention. 

I think visual perception is aquired, in part. So if I were presenting data to
viewers who took carpentry or other such classes in highschool I may be tempted
to use dotcarts.

An interesting experiment: have kids compare pieces of pie or bread-sticks over
a dinner, and check how they do.  They should not have taken a carpentry class.
I use dot-charts, they are useful. Sometimes pie carts are useful too, because
people are so used to using and seeing them over a long time. Ofcourse, they can
be improved.

Also, it may be possible to put points of a dot-chart on a single straight line,
label them with a pointing line, and get better perception. There is poor
perception of the horizantal distance, by having to view that extra vertical
distance in a dotchart. However, it is useful to have the vertical axis in
Lattice plots, but not in stand-alone dot-charts.

Anupam.


From mr.blacksheep at gmail.com  Wed Sep 20 18:55:28 2006
From: mr.blacksheep at gmail.com (Mike Nielsen)
Date: Wed, 20 Sep 2006 11:55:28 -0500
Subject: [R] help with function
In-Reply-To: <BA6FF017E924044A9BF748AFAEEA6F30A533F5@FWC-TLEX3.fwc.state.fl.us>
References: <BA6FF017E924044A9BF748AFAEEA6F30A533F5@FWC-TLEX3.fwc.state.fl.us>
Message-ID: <46a360560609200955n43849d60na0664522bb58094b@mail.gmail.com>

Take the case of i==1.

Ct[i]<-1/Bq*Bt[i]*Cerr                                  # Assign Ct[1]
using Bt[1]
  Rt[i]<-Bt[i]/(a+b*Bt[i])                               # Assign
Rt[1] using Bt[1]
  Bt[i+2]<-(1-m)*Bt[i+1]+Rt[i]   *Rerr-Ct[i+1]  # Assign Bt[3] using
Bt[2] and Rt[1] and **Ct[2]**


You're reading Ct[i+1] before you ever assign it, hence NA.

OSISTM

Hope this helps,

Regards,

Mike



On 9/20/06, Guenther, Cameron <Cameron.Guenther at myfwc.com> wrote:
> Hello everyone,
>
> I have a function here that I wrote but doesn't seem to work quite
> right.  Attached is the code.  In the calib funcion under the for loop
> Bt[i+2]<-(1-m)*Bt[i+1]+Rt[i]*Rerr-Ct[i+1] returns NA's for everything
> after years 1983 and 1984.  However the code works when it reads
> Bt[i+2]<-(1-m)*Bt[i+1]+Rt[i]*Rerr-Ct[i].  I don't quite understand why
> since it should be calculating all of the necessary inputs prior to
> calculating Bt[i+2].  Any help would be greatly appreciated.
>
> Thanks
>
> #Model parameters
> B0<-75000000
> m<-0.3
> R0<-B0*m
> z<-0.8
> a<-B0/R0*(1-(z-0.2)/(0.8*z))
> b<-(z-0.2)/(0.8*z*R0)
> dat<-data.frame(years=seq(1983,2004),cobs=c(19032324,19032324,17531618,2
> 0533029,20298099,20793744,23519369,23131780,19922247,17274513,17034419,1
> 2448318,4551585,4226451,7183688,7407924,7538366,7336039,8869193,7902341,
> 6369089,6211886))
> stdr<-runif(100,0,0.5)
> stdc<-runif(100,0,0.5)
> BC<-runif(1000,0,100)
>
>
> #model calibration
>
> calib<-function(x){
>  v<-sample(stdr,1)
>  cr<-sample(stdc,1)
>  N<-rnorm(1)
>  Bq<-sample(BC,1)
>  Rerr<-exp(N*v-(v^2/2))
>  Cerr<-exp(N*cr-(cr^2/2))
>  Bt<-vector();Bt[1]=B0;Bt[2]=B0
>  Rt<-vector()
>  Ct<-vector()
>  for (i in 1:length(x$years)){
>   Ct[i]<-1/Bq*Bt[i]*Cerr
>   Rt[i]<-Bt[i]/(a+b*Bt[i])
>   Bt[i+2]<-(1-m)*Bt[i+1]+Rt[i]*Rerr-Ct[i+1]
>  }
>   out<-new.env()
>   out$yr<-x$years[1:length(x$years)]
>   out$Bt<-Bt[1:length(x$years)]
>   out$Rt<-Rt[1:length(x$years)]
>   out$Ct<-Ct[1:length(x$years)]
>   out$stdr<-v
>   out$stdc<-cr
>   out$Bq<-Bq
>   out$Rerr<-Rerr
>   out$Cerr<-Cerr
>   return(as.list(out))
>  }
>  test<-calib(dat)
>
>
> Cameron Guenther, Ph.D.
> Associate Research Scientist
> FWC/FWRI, Marine Fisheries Research
> 100 8th Avenue S.E.
> St. Petersburg, FL 33701
> (727)896-8626 Ext. 4305
> cameron.guenther at myfwc.com
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Regards,

Mike Nielsen


From mr.blacksheep at gmail.com  Wed Sep 20 18:55:28 2006
From: mr.blacksheep at gmail.com (Mike Nielsen)
Date: Wed, 20 Sep 2006 11:55:28 -0500
Subject: [R] help with function
In-Reply-To: <BA6FF017E924044A9BF748AFAEEA6F30A533F5@FWC-TLEX3.fwc.state.fl.us>
References: <BA6FF017E924044A9BF748AFAEEA6F30A533F5@FWC-TLEX3.fwc.state.fl.us>
Message-ID: <46a360560609200955n43849d60na0664522bb58094b@mail.gmail.com>

Take the case of i==1.

Ct[i]<-1/Bq*Bt[i]*Cerr                                  # Assign Ct[1]
using Bt[1]
  Rt[i]<-Bt[i]/(a+b*Bt[i])                               # Assign
Rt[1] using Bt[1]
  Bt[i+2]<-(1-m)*Bt[i+1]+Rt[i]   *Rerr-Ct[i+1]  # Assign Bt[3] using
Bt[2] and Rt[1] and **Ct[2]**


You're reading Ct[i+1] before you ever assign it, hence NA.

OSISTM

Hope this helps,

Regards,

Mike



On 9/20/06, Guenther, Cameron <Cameron.Guenther at myfwc.com> wrote:
> Hello everyone,
>
> I have a function here that I wrote but doesn't seem to work quite
> right.  Attached is the code.  In the calib funcion under the for loop
> Bt[i+2]<-(1-m)*Bt[i+1]+Rt[i]*Rerr-Ct[i+1] returns NA's for everything
> after years 1983 and 1984.  However the code works when it reads
> Bt[i+2]<-(1-m)*Bt[i+1]+Rt[i]*Rerr-Ct[i].  I don't quite understand why
> since it should be calculating all of the necessary inputs prior to
> calculating Bt[i+2].  Any help would be greatly appreciated.
>
> Thanks
>
> #Model parameters
> B0<-75000000
> m<-0.3
> R0<-B0*m
> z<-0.8
> a<-B0/R0*(1-(z-0.2)/(0.8*z))
> b<-(z-0.2)/(0.8*z*R0)
> dat<-data.frame(years=seq(1983,2004),cobs=c(19032324,19032324,17531618,2
> 0533029,20298099,20793744,23519369,23131780,19922247,17274513,17034419,1
> 2448318,4551585,4226451,7183688,7407924,7538366,7336039,8869193,7902341,
> 6369089,6211886))
> stdr<-runif(100,0,0.5)
> stdc<-runif(100,0,0.5)
> BC<-runif(1000,0,100)
>
>
> #model calibration
>
> calib<-function(x){
>  v<-sample(stdr,1)
>  cr<-sample(stdc,1)
>  N<-rnorm(1)
>  Bq<-sample(BC,1)
>  Rerr<-exp(N*v-(v^2/2))
>  Cerr<-exp(N*cr-(cr^2/2))
>  Bt<-vector();Bt[1]=B0;Bt[2]=B0
>  Rt<-vector()
>  Ct<-vector()
>  for (i in 1:length(x$years)){
>   Ct[i]<-1/Bq*Bt[i]*Cerr
>   Rt[i]<-Bt[i]/(a+b*Bt[i])
>   Bt[i+2]<-(1-m)*Bt[i+1]+Rt[i]*Rerr-Ct[i+1]
>  }
>   out<-new.env()
>   out$yr<-x$years[1:length(x$years)]
>   out$Bt<-Bt[1:length(x$years)]
>   out$Rt<-Rt[1:length(x$years)]
>   out$Ct<-Ct[1:length(x$years)]
>   out$stdr<-v
>   out$stdc<-cr
>   out$Bq<-Bq
>   out$Rerr<-Rerr
>   out$Cerr<-Cerr
>   return(as.list(out))
>  }
>  test<-calib(dat)
>
>
> Cameron Guenther, Ph.D.
> Associate Research Scientist
> FWC/FWRI, Marine Fisheries Research
> 100 8th Avenue S.E.
> St. Petersburg, FL 33701
> (727)896-8626 Ext. 4305
> cameron.guenther at myfwc.com
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Regards,

Mike Nielsen


From murdoch at stats.uwo.ca  Wed Sep 20 19:42:01 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 20 Sep 2006 13:42:01 -0400
Subject: [R] Comment delay time (was:  acos(0.5) == pi/3 FALSE)
In-Reply-To: <Pine.LNX.4.64.0609200835510.27801@tajo.ucsd.edu>
References: <200609181931.46198.inaki.murillo@ehu.es>	<XFMail.060918230505.Ted.Harding@nessie.mcc.ac.uk>	<loom.20060919T003312-658@post.gmane.org>	<x2irjkzsis.fsf@turmalin.kubism.ku.dk>	<62777.91.89.65.114.1158697877.squirrel@mail.panix.com>
	<Pine.LNX.4.64.0609200835510.27801@tajo.ucsd.edu>
Message-ID: <45117D69.3020708@stats.uwo.ca>

On 9/20/2006 11:54 AM, Charles C. Berry wrote:
> On Tue, 19 Sep 2006, Johannes H?sing wrote:
> 
>> Peter Dalgaard:
>>> Ben Bolker <bolker at zoo.ufl.edu> writes:
>>>>          1. compose your response
>>> I've always wondered why step 1. - often the time-consuming bit - is not
>>> listed last.
>>
>> The advice applies to the situation when answering immediately would be
>> your knee-jerk reaction. It is assumed that actually composing and sending
>> the mail would take very little time and thought, whereas coming around to
>> answering it after runif(1)*4 hours would take considerably more time, even
>> when mulitiplied with the probability that you are still the first one.
>>
>> Looking at the submission times of questions and answers in this
>> particular case, though, I would be upset if the helpful guys actually
>> used this algorithm. Most of the answers were submitted after 3.5 to 4 h
>> time, thus revealing a possible flaw of the random number generator
>> underlying runif().
> 
> Johannes,
> 
> Turn on 'full-headers' in your email reader.
> 
> Most of the replies were submitted within 20 minutes of the posting of the 
> original query by the list-serv (to me and I assume to others) and several 
> that said essentially the same thing were posted within the first 10 
> minutes, I recall.
> 
> The list-serv held the initial email for a couple of hours before passing 
> it on. The replies are processed more rapidly, being held at most a few 
> minutes each.
> 
> Given the initial hold placed on that email, runif(1)*4 hours would have 
> increased the overall response time (from time of initial posting to time 
> of first response) by less than 25% (with high probability). And would 
> have saved several respondents from having to type up their replies.
> 
> In this case even runif(1)*20 minutes would likely have cut the response 
> traffic to one or two and would have increased the overall response time 
> by less than 10 minutes.
> 

Perhaps the list server should have a configurable user-specific random 
delay time before posting a new thread.  Users who don't want to risk 
wasting time on duplicate postings could ask not to see new threads 
until a random delay has passed.  Followups to the threads that arrived 
during this waiting period would all be sent at once, so if you see a 
message doesn't have responses, you know it's safe to write one.

Duncan Murdoch


From vincent.goulet at act.ulaval.ca  Wed Sep 20 19:41:27 2006
From: vincent.goulet at act.ulaval.ca (Vincent Goulet)
Date: Wed, 20 Sep 2006 13:41:27 -0400
Subject: [R] Building the call of an arbitrary function
In-Reply-To: <450D900D.6010702@stats.uwo.ca>
References: <200609171236.12215.vincent.goulet@act.ulaval.ca>
	<450D900D.6010702@stats.uwo.ca>
Message-ID: <200609201341.27931.vincent.goulet@act.ulaval.ca>

Le Dimanche 17 Septembre 2006 14:12, Duncan Murdoch a ?crit?:
> On 9/17/2006 12:36 PM, Vincent Goulet wrote:
> > Hy all,
> >
> > Is there a direct way to build the complete function call of an arbitrary
> > function?
> >
> > Here's what I want to do. A function will build a function which will
> > itself call a probability density function for some law given in argument
> > to the
> >
> > first function:
> >> f("gamma", 1000)
> >
> > will return, say,
> >
> > function(x, shape, rate, scale = 1/rate)
> >     dgamma(x + 1000, shape, rate, scale = 1/rate)
> >
> > (Notice that the arguments of the output function are those of dgamma().)
> >
> > I tried all sorts of combinations of call(), formals(), args() et al. to
> > no avail. But then, I avoided, so far, to build the whole thing as a
> > character string. Would it be the only option?
>
> No, do.call is what you want.
>
> dgamma(x + 1000, shape, rate, scale = 1/rate)
>
> is the same as
>
> do.call("dgamma", list(x+1000, shape, rate, scale=1/rate))
>
> But since you're going to have to look up the parameters that are
> appropriate to your target density (i.e. shape, rate, scale), I'm not
> sure how useful this will be.  It might be easier just to code the call
> to dgamma directly.
>
> Duncan Murdoch

First, thanks to both Duncan and Gabor for their useful reply. do.call() was 
part of the "et al."  functions I looked up, but I only tried it 
interactively (where the call is immediately executed after being built) and 
so dismissed it.

After some more struggling (hence the delay in my reply), I was able to do 
exactly what I want without using strings. For the record, here's my 
solution:

f <- function(dist, y)
{
    dist <- paste("d", dist, sep = "")
    args <- sapply(names(formals(dist)[-1]), as.name)

    x <- substitute(x + y, list(y = y))

    eval(substitute(FUN <- function() do.call(f, a),
                    list(f = dist, a = c(x = x, args))))
    formals(FUN) <- formals(pdf)
    FUN
}

Then, for example,

> f("gamma", 1000)
function (x, shape, rate = 1, scale = 1/rate, log = FALSE) 
do.call("dgamma", list(x = x + 1000, shape = shape, rate = rate, 
    scale = scale, log = log))
<environment: 0x8a9b330>

I think this is pretty neat! ;-)

Vincent

-- 
  Vincent Goulet, Associate Professor
  ?cole d'actuariat
  Universit? Laval, Qu?bec 
  Vincent.Goulet at act.ulaval.ca   http://vgoulet.act.ulaval.ca


From ethan.johnsons at gmail.com  Wed Sep 20 20:00:16 2006
From: ethan.johnsons at gmail.com (Ethan Johnsons)
Date: Wed, 20 Sep 2006 14:00:16 -0400
Subject: [R] binom
Message-ID: <5cd96f050609201100q3e4866devc964372cd6cf48be@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060920/17b01638/attachment.pl 

From mdjankowski at wisc.edu  Wed Sep 20 20:13:41 2006
From: mdjankowski at wisc.edu (Mark Jankowski)
Date: Wed, 20 Sep 2006 13:13:41 -0500
Subject: [R] GLM Question
Message-ID: <4DC64306-208D-4648-8A89-7FE779294BD9@wisc.edu>

Hello,

I am trying to formulate a glm model with repeated measures of viral  
blood counts where animal weight is a covariate. Animal treatment  
group is the fixed effect and subject is the random effect.  I'm  
thinking this situation calls for a mixed model in the Poisson family  
with data correlated to days post inoculation (DPI) where animal  
weight is factored out to not influence goodness of fit tests.

How might I alter the following code to reflect this situation?

glm(count~treatment,family=poisson)

If this is too much to ask, I understand!  I realize I've got a ways  
to go in writing this...

Many thanks!
Mark


From lanre.okusanya at gmail.com  Wed Sep 20 20:23:21 2006
From: lanre.okusanya at gmail.com (Lanre Okusanya)
Date: Wed, 20 Sep 2006 14:23:21 -0400
Subject: [R] Comment blocks in R programs
In-Reply-To: <45116318.2020204@mango-solutions.com>
References: <eermcm$jbq$1@sea.gmane.org>
	<971536df0609200839g2c81e69ci2b09be294d58d554@mail.gmail.com>
	<45116318.2020204@mango-solutions.com>
Message-ID: <6e25bb420609201123l5934e0e2q765e721c738eac44@mail.gmail.com>

Tinn-R has that functionality.

On 9/20/06, Romain Francois <rfrancois at mango-solutions.com> wrote:
> Gabor Grothendieck wrote:
> > Try this:
> >
> > if (FALSE) {
> > ... whatever ...
> > }
> >
>
> Problem is,
> ... whatever ...
> must be syntactically correct to use that trick.
>
> Why not using a text editor that does rectangular selection or
> commenting several lines at a time, there are many out there to propose
> that kind of feature.
>
> Cheers,
>
> Romain
>
>
> > On 9/20/06, Joe Byers <joe-byers at utulsa.edu> wrote:
> >
> >> All,
> >>
> >> Is there a way to add comment blocks in an R script other than using #
> >> at the beginning of each line?  Or, is there anything like ndocs for C++
> >> and the markup for Java that can generate documentation?
> >>
> >> This may be a bad question, but I have searched R documentation and
> >> can't seem to find an Answer.  I have also looked at the R package
> >> documentation mark up docs for help files but that was more than I wanted.
> >>
> >>
> >> thank you for you help.
> >> Joe
> >>
>
> --
> *mangosolutions*
> /data analysis that delivers/
>
> Tel   +44 1249 467 467
> Fax   +44 1249 467 468
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From rmh at temple.edu  Wed Sep 20 20:34:18 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Wed, 20 Sep 2006 14:34:18 -0400 (EDT)
Subject: [R] Beginners manual for emacs and ess
Message-ID: <20060920143418.BIJ15506@po-d.temple.edu>

I recommend you start with our JCGS article to get a sense
of what ESS does and why.  An earlier version of the article is
in the ESS distribution as file
ess-5.3.2/doc/ess-intro-graphs.pdf

The manual is in the ESS distribution as file
ess-5.3.2/doc/ess.pdf

The reference card is file
ess-5.3.2/doc/refcard/refcard.pdf


From rmh at temple.edu  Wed Sep 20 20:35:04 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Wed, 20 Sep 2006 14:35:04 -0400 (EDT)
Subject: [R] Beginners manual for emacs and ess
Message-ID: <20060920143504.BIJ15653@po-d.temple.edu>

and in answer to your specific question

You start R with
M-x R


From glen_sargeant at usgs.gov  Wed Sep 20 20:42:43 2006
From: glen_sargeant at usgs.gov (Glen A Sargeant)
Date: Wed, 20 Sep 2006 13:42:43 -0500
Subject: [R] hours() in 'chron': output != input
Message-ID: <OF801244C0.18BC0F22-ON862571EF.0062D220-862571EF.00669AB3@usgs.gov>

I encountered surprising (to me, at least) behavior while using 'hours()' 
in package 'chron.' I will be grateful if someone can point out my error 
or provide an explanation and intuitive solution (I suppose I could 
convert chron to a character vector and use substring, but I deal mostly 
with newbies and kludgy approaches don't inspire much confidence).

I used 2.3.0 to construct the dataframe below.

1) 'date.char' is a character vector

2) 'time.char' is a character vector

3) 'chron' was created by 
>chron(dates(date.char),times(time.char))

4) hours were extracted with 
>hours(chron). 

5) Note that the hours used to construct 'chron' do not match the hours 
extracted with 'hours()' when minutes and seconds are "00:00."

   date.char time.char               chron hour
1   09/20/06  00:00:00 (09/20/06 00:00:00)    0
2   09/20/06  01:00:00 (09/20/06 01:00:00)    0
3   09/20/06  02:00:00 (09/20/06 02:00:00)    2
4   09/20/06  03:00:00 (09/20/06 03:00:00)    3
5   09/20/06  04:00:00 (09/20/06 04:00:00)    3
6   09/20/06  05:00:00 (09/20/06 05:00:00)    5
7   09/20/06  06:00:00 (09/20/06 06:00:00)    6
8   09/20/06  07:00:00 (09/20/06 07:00:00)    6
9   09/20/06  08:00:00 (09/20/06 08:00:00)    8
10  09/20/06  09:00:00 (09/20/06 09:00:00)    9
11  09/20/06  10:00:00 (09/20/06 10:00:00)    9
12  09/20/06  11:00:00 (09/20/06 11:00:00)   11
13  09/20/06  12:00:00 (09/20/06 12:00:00)   12
14  09/20/06  13:00:00 (09/20/06 13:00:00)   12
15  09/20/06  14:00:00 (09/20/06 14:00:00)   14
16  09/20/06  15:00:00 (09/20/06 15:00:00)   15
17  09/20/06  16:00:00 (09/20/06 16:00:00)   15
18  09/20/06  17:00:00 (09/20/06 17:00:00)   17
19  09/20/06  18:00:00 (09/20/06 18:00:00)   18
20  09/20/06  19:00:00 (09/20/06 19:00:00)   18
21  09/20/06  20:00:00 (09/20/06 20:00:00)   20
22  09/20/06  21:00:00 (09/20/06 21:00:00)   21
23  09/20/06  22:00:00 (09/20/06 22:00:00)   21
24  09/20/06  23:00:00 (09/20/06 23:00:00)   23

This behavior is problematic if one wants to extract hours and use them to 
group data.

Regards,

Glen Sargeant


From dylan.beaudette at gmail.com  Wed Sep 20 21:08:02 2006
From: dylan.beaudette at gmail.com (Dylan Beaudette)
Date: Wed, 20 Sep 2006 12:08:02 -0700
Subject: [R] mysterious error on compile R 2.3.1
Message-ID: <200609201208.02782.dylan.beaudette@gmail.com>

Getting a very strange error with a new install of R from source on x86;

make[3]: Leaving directory `/tmp/R.INSTALL.r20887/cluster/src'
** R
** data
**  moving datasets to lazyload DB
Error in factor(c(1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1),  : 
        invalid labels; length 2 should be 1 or 1
Execution halted
ERROR: lazydata failed for package 'cluster'
** Removing '/home/dylan/src/R-2.3.1/library/cluster'
make[2]: *** [cluster.ts] Error 1
make[2]: Leaving directory `/home/dylan/src/R-2.3.1/src/library/Recommended'
make[1]: *** [recommended-packages] Error 2
make[1]: Leaving directory `/home/dylan/src/R-2.3.1/src/library/Recommended'
make: *** [stamp-recommended] Error 2

note that i am using the GCC flags:
CFLAGS=-march=opteron -ffast-math
CXXFLAGS=-march=opteron -ffast-math

any ideas?

-- 
Dylan Beaudette
Soils and Biogeochemistry Graduate Group
University of California at Davis
530.754.7341


From ggrothendieck at gmail.com  Wed Sep 20 21:06:11 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 20 Sep 2006 15:06:11 -0400
Subject: [R] hours() in 'chron': output != input
In-Reply-To: <OF801244C0.18BC0F22-ON862571EF.0062D220-862571EF.00669AB3@usgs.gov>
References: <OF801244C0.18BC0F22-ON862571EF.0062D220-862571EF.00669AB3@usgs.gov>
Message-ID: <971536df0609201206g7719fa0aqcff2fb62eb4a5487@mail.gmail.com>

I think hours should have a comparison tolerance.  Try:

   hours(x+1e-10)

as a workaround.

On 9/20/06, Glen A Sargeant <glen_sargeant at usgs.gov> wrote:
> I encountered surprising (to me, at least) behavior while using 'hours()'
> in package 'chron.' I will be grateful if someone can point out my error
> or provide an explanation and intuitive solution (I suppose I could
> convert chron to a character vector and use substring, but I deal mostly
> with newbies and kludgy approaches don't inspire much confidence).
>
> I used 2.3.0 to construct the dataframe below.
>
> 1) 'date.char' is a character vector
>
> 2) 'time.char' is a character vector
>
> 3) 'chron' was created by
> >chron(dates(date.char),times(time.char))
>
> 4) hours were extracted with
> >hours(chron).
>
> 5) Note that the hours used to construct 'chron' do not match the hours
> extracted with 'hours()' when minutes and seconds are "00:00."
>
>   date.char time.char               chron hour
> 1   09/20/06  00:00:00 (09/20/06 00:00:00)    0
> 2   09/20/06  01:00:00 (09/20/06 01:00:00)    0
> 3   09/20/06  02:00:00 (09/20/06 02:00:00)    2
> 4   09/20/06  03:00:00 (09/20/06 03:00:00)    3
> 5   09/20/06  04:00:00 (09/20/06 04:00:00)    3
> 6   09/20/06  05:00:00 (09/20/06 05:00:00)    5
> 7   09/20/06  06:00:00 (09/20/06 06:00:00)    6
> 8   09/20/06  07:00:00 (09/20/06 07:00:00)    6
> 9   09/20/06  08:00:00 (09/20/06 08:00:00)    8
> 10  09/20/06  09:00:00 (09/20/06 09:00:00)    9
> 11  09/20/06  10:00:00 (09/20/06 10:00:00)    9
> 12  09/20/06  11:00:00 (09/20/06 11:00:00)   11
> 13  09/20/06  12:00:00 (09/20/06 12:00:00)   12
> 14  09/20/06  13:00:00 (09/20/06 13:00:00)   12
> 15  09/20/06  14:00:00 (09/20/06 14:00:00)   14
> 16  09/20/06  15:00:00 (09/20/06 15:00:00)   15
> 17  09/20/06  16:00:00 (09/20/06 16:00:00)   15
> 18  09/20/06  17:00:00 (09/20/06 17:00:00)   17
> 19  09/20/06  18:00:00 (09/20/06 18:00:00)   18
> 20  09/20/06  19:00:00 (09/20/06 19:00:00)   18
> 21  09/20/06  20:00:00 (09/20/06 20:00:00)   20
> 22  09/20/06  21:00:00 (09/20/06 21:00:00)   21
> 23  09/20/06  22:00:00 (09/20/06 22:00:00)   21
> 24  09/20/06  23:00:00 (09/20/06 23:00:00)   23
>
> This behavior is problematic if one wants to extract hours and use them to
> group data.
>
> Regards,
>
> Glen Sargeant
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mothsailor at googlemail.com  Wed Sep 20 21:12:12 2006
From: mothsailor at googlemail.com (David Barron)
Date: Wed, 20 Sep 2006 20:12:12 +0100
Subject: [R] GLM Question
In-Reply-To: <4DC64306-208D-4648-8A89-7FE779294BD9@wisc.edu>
References: <4DC64306-208D-4648-8A89-7FE779294BD9@wisc.edu>
Message-ID: <815b70590609201212k7f0f5f2fl91079c75718a5b88@mail.gmail.com>

You can use the lmer function in the Matrix package or glmmPQL in the
MASS package.  The former would be used like this:

p1 <- lmer(count ~ treatment + (1|subject), family=poisson)

Dave

On 20/09/06, Mark Jankowski <mdjankowski at wisc.edu> wrote:
> Hello,
>
> I am trying to formulate a glm model with repeated measures of viral
> blood counts where animal weight is a covariate. Animal treatment
> group is the fixed effect and subject is the random effect.  I'm
> thinking this situation calls for a mixed model in the Poisson family
> with data correlated to days post inoculation (DPI) where animal
> weight is factored out to not influence goodness of fit tests.
>
> How might I alter the following code to reflect this situation?
>
> glm(count~treatment,family=poisson)
>
> If this is too much to ask, I understand!  I realize I've got a ways
> to go in writing this...
>
> Many thanks!
> Mark
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From mathieu.drapeau at mcgill.ca  Wed Sep 20 21:15:07 2006
From: mathieu.drapeau at mcgill.ca (Mathieu Drapeau)
Date: Wed, 20 Sep 2006 15:15:07 -0400
Subject: [R] [ROracle] error loading (undefined symbol: sqlclu)
Message-ID: <1158779707.4806.9.camel@mathieu2.campus.mcgill.ca>

I have this error when I load the library ROracle:
> library(ROracle)
Loading required package: DBI
Error in dyn.load(x, as.logical(local), as.logical(now)) :
        unable to load shared library
'/usr/local/lib/R/site-library/ROracle/libs/ROracle.so':
  /usr/local/lib/R/site-library/ROracle/libs/ROracle.so: undefined
symbol: sqlclu
Error in library(ROracle) : .First.lib failed for 'ROracle'
>

Also, my LD_LIBRARY_PATH seems to be set correctly:
drapeau:~> echo $LD_LIBRARY_PATH
/home/drapeau/lib:/opt/oracle/xe/app/oracle/product/10.2.0/client/lib

I installed the big database applications (10g) and ROracle 0.5-7

Your help will be very appreciated to help me solve this error,
Thank you,
Mathieu


From ripley at stats.ox.ac.uk  Wed Sep 20 21:34:14 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 20 Sep 2006 20:34:14 +0100 (BST)
Subject: [R] mysterious error on compile R 2.3.1
In-Reply-To: <200609201208.02782.dylan.beaudette@gmail.com>
References: <200609201208.02782.dylan.beaudette@gmail.com>
Message-ID: <Pine.LNX.4.64.0609202030300.771@gannet.stats.ox.ac.uk>

On Wed, 20 Sep 2006, Dylan Beaudette wrote:

> Getting a very strange error with a new install of R from source on x86;
>
> make[3]: Leaving directory `/tmp/R.INSTALL.r20887/cluster/src'
> ** R
> ** data
> **  moving datasets to lazyload DB
> Error in factor(c(1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1),  :
>        invalid labels; length 2 should be 1 or 1
> Execution halted
> ERROR: lazydata failed for package 'cluster'
> ** Removing '/home/dylan/src/R-2.3.1/library/cluster'
> make[2]: *** [cluster.ts] Error 1
> make[2]: Leaving directory `/home/dylan/src/R-2.3.1/src/library/Recommended'
> make[1]: *** [recommended-packages] Error 2
> make[1]: Leaving directory `/home/dylan/src/R-2.3.1/src/library/Recommended'
> make: *** [stamp-recommended] Error 2
>
> note that i am using the GCC flags:
> CFLAGS=-march=opteron -ffast-math
> CXXFLAGS=-march=opteron -ffast-math

We do request you do not do so in the R-admin manual.  From the gcc man 
page:

        This option should never be turned on by any -O option since it can
        result in incorrect output for programs which depend on an exact
        implementation of IEEE or ISO rules/specifications for math func-
        tions.

R is such a program.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From toby.m at mail.utexas.edu  Wed Sep 20 21:35:50 2006
From: toby.m at mail.utexas.edu (Toby Muhlhofer)
Date: Wed, 20 Sep 2006 14:35:50 -0500
Subject: [R] Unexpected behavior of apply() over a 3d array
Message-ID: <45119816.4010408@mail.utexas.edu>

Dear listeRs,

I'm finding that apply() behaves strangely when used on a 3-d array. For 
example:

 > at <- array(1:27,dim=c(3,3,3))
 > at
, , 1

      [,1] [,2] [,3]
[1,]    1    4    7
[2,]    2    5    8
[3,]    3    6    9

, , 2

      [,1] [,2] [,3]
[1,]   10   13   16
[2,]   11   14   17
[3,]   12   15   18

, , 3

      [,1] [,2] [,3]
[1,]   19   22   25
[2,]   20   23   26
[3,]   21   24   27

 > apply(at, 1, max)
[1] 25 26 27

If, for the MARGIN argument in apply() 1 is rows, I would have expected 
as output a 3x3 matrix something like

7 16 25
8 17 16
9 18 27

Either that, or maybe the transpose of that, but a single vector seems 
rather random. Especially when you go

 > apply(at, 3, max)
[1]  9 18 27

What is that the max of? Each submatrix? The diagonal? I'm confused. Can 
anyone clarify this?

Besides this, is there a function that will work on a 3d array, the way 
I'm implying, or do I need to write an explicit loop that takes 2d 
slices of my 3d array?

Thanks!

	Toby


From p.dalgaard at biostat.ku.dk  Wed Sep 20 21:38:59 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Sep 2006 21:38:59 +0200
Subject: [R] mysterious error on compile R 2.3.1
In-Reply-To: <200609201208.02782.dylan.beaudette@gmail.com>
References: <200609201208.02782.dylan.beaudette@gmail.com>
Message-ID: <x24pv2z5t8.fsf@turmalin.kubism.ku.dk>

Dylan Beaudette <dylan.beaudette at gmail.com> writes:

> Getting a very strange error with a new install of R from source on x86;
> 
> make[3]: Leaving directory `/tmp/R.INSTALL.r20887/cluster/src'
> ** R
> ** data
> **  moving datasets to lazyload DB
> Error in factor(c(1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1),  : 
>         invalid labels; length 2 should be 1 or 1
> Execution halted
> ERROR: lazydata failed for package 'cluster'
> ** Removing '/home/dylan/src/R-2.3.1/library/cluster'
> make[2]: *** [cluster.ts] Error 1
> make[2]: Leaving directory `/home/dylan/src/R-2.3.1/src/library/Recommended'
> make[1]: *** [recommended-packages] Error 2
> make[1]: Leaving directory `/home/dylan/src/R-2.3.1/src/library/Recommended'
> make: *** [stamp-recommended] Error 2
> 
> note that i am using the GCC flags:
> CFLAGS=-march=opteron -ffast-math
> CXXFLAGS=-march=opteron -ffast-math
> 
> any ideas?

Don't do that.... 

Seriously!

-ffast-math will allow the compiler to break IEEE math specifications,
 which in turn will break R all over the place.


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From chabotd at globetrotter.net  Wed Sep 20 21:42:36 2006
From: chabotd at globetrotter.net (Denis Chabot)
Date: Wed, 20 Sep 2006 15:42:36 -0400
Subject: [R] functionality of "update" in SAS
Message-ID: <29E5D33C-B243-4949-B13D-57BD06B3AE18@globetrotter.net>

Dear list,

I've tried to search the archives but found nothing, although I may  
use the wrong wording in my searches. I've also double-checked the  
upData function in Hmisc, but it does something else.

I'm wondering if one can update a dataframe by "forcing into" it a  
shorter dataframe containing the corrections, like the "update"  
provided in SAS data steps.

In this simple example:
a <- data.frame(id=c(1:5),x=rnorm(5))
b <- data.frame(id=4,x=rnorm(1))
 > a
   id          x
1  1  0.6557921
2  2  0.1897523
3  3  0.7976721
4  4  0.2107103
5  5 -0.8855786
 > b
   id         x
1  4 0.8369147

I would like the "updated" dataframe to look like (row names are not  
important to me)

    id          x
1   1  0.6557921
2   2  0.1897523
3   3  0.7976721
4   4  0.8369147
5   5 -0.8855786

I thought this could be done with merge, but this never removes the  
old version of a row, it just gives me two rows with id==4.

I thought of this solution:

reject <- a$id %in% b$id
a2 <- a[!reject,]
a3 <- rbind(a2,b)
 > a3
    id          x
1   1  0.6557921
2   2  0.1897523
3   3  0.7976721
5   5 -0.8855786
11  4  0.8369147

This works, and obviously it is not the best way to make the  
correction in a simple case like this. But providing a few lines of  
corrected data can be an effective method with large dataframes,  
especially if many identifier (grouping) variables are needed to  
identify each line that needs updating, and in this context my  
solution above rapidly becomes ugly.

Furthermore (but I can live with this constraint) this method removes  
entire rows, so I need to make sure the dataframe used to make  
corrections contains all the Y variables in the original dataframe,  
even those that do not need correcting.

If a method exists to just change one variable in 5 lines for a  
dataframe of 5000 lines and 30 variables, I'd appreciate learning  
about it. But I'll already be thrilled if I can update whole lines at  
a time.

Sincerely,

Denis Chabot


From ethan.johnsons at gmail.com  Wed Sep 20 21:43:57 2006
From: ethan.johnsons at gmail.com (Ethan Johnsons)
Date: Wed, 20 Sep 2006 15:43:57 -0400
Subject: [R] Poission distribution
Message-ID: <5cd96f050609201243m6eb921fey84ec8f33e7b346e4@mail.gmail.com>

The expected number of bladder cancer over next 20 years a tire
industry is 1.8.  Poission distribution is assumed to hold and 6
reported deaths are caused by bladder cancer among the employees.
Trying to find how unusual this event is.

> ppois(q=6, lambda=1.8, lower.tail = TRUE, log.p = FALSE)
[1] 0.9974306

not sure if ppois is the right one to use and the parameters...

thx much


From mschwartz at mn.rr.com  Wed Sep 20 21:46:53 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Wed, 20 Sep 2006 14:46:53 -0500
Subject: [R] [ROracle] error loading (undefined symbol: sqlclu)
In-Reply-To: <1158779707.4806.9.camel@mathieu2.campus.mcgill.ca>
References: <1158779707.4806.9.camel@mathieu2.campus.mcgill.ca>
Message-ID: <1158781613.13278.12.camel@localhost.localdomain>

On Wed, 2006-09-20 at 15:15 -0400, Mathieu Drapeau wrote:
> I have this error when I load the library ROracle:
> > library(ROracle)
> Loading required package: DBI
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>         unable to load shared library
> '/usr/local/lib/R/site-library/ROracle/libs/ROracle.so':
>   /usr/local/lib/R/site-library/ROracle/libs/ROracle.so: undefined
> symbol: sqlclu
> Error in library(ROracle) : .First.lib failed for 'ROracle'
> >
> 
> Also, my LD_LIBRARY_PATH seems to be set correctly:
> drapeau:~> echo $LD_LIBRARY_PATH
> /home/drapeau/lib:/opt/oracle/xe/app/oracle/product/10.2.0/client/lib
> 
> I installed the big database applications (10g) and ROracle 0.5-7
> 
> Your help will be very appreciated to help me solve this error,
> Thank you,
> Mathieu

Where did you set LD_LIBRARY_PATH?

If in one of your shell config files, it is likely that it is being
stepped on during your login and thus not being seen within the R
session. 

I had this problem previously and set the variable in /etc/ld.so.conf
(though I am using RODBC instead).

Edit that file (as root), add the path:

/opt/oracle/xe/app/oracle/product/10.2.0/client/lib

and then run [/sbin/]ldconfig to update the current settings.

Be sure also that $ORACLE_HOME is set
to /opt/oracle/xe/app/oracle/product/10.2.0/client

Then try to load ROracle in a new R session.

HTH,

Marc Schwartz


From ggrothendieck at gmail.com  Wed Sep 20 21:51:41 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 20 Sep 2006 15:51:41 -0400
Subject: [R] Unexpected behavior of apply() over a 3d array
In-Reply-To: <45119816.4010408@mail.utexas.edu>
References: <45119816.4010408@mail.utexas.edu>
Message-ID: <971536df0609201251s4a65deb0hf721f17472fb607e@mail.gmail.com>

Read ?apply carefullly.    If FUN returns as a scalar as it does here
then the result dimensions are dim(X)[MARGIN]. For example,

apply(X, 1, max) has three components which are
max(X[1,,]), max(X[2,,]) and max(X[3,,])

and apply(X, 3, max) has three components which are
max(X[,,1]), max(X[,,2]) and max(X[,,3])

Also try apply(X, 1:2, max), etc.

On 9/20/06, Toby Muhlhofer <toby.m at mail.utexas.edu> wrote:
> Dear listeRs,
>
> I'm finding that apply() behaves strangely when used on a 3-d array. For
> example:
>
>  > at <- array(1:27,dim=c(3,3,3))
>  > at
> , , 1
>
>      [,1] [,2] [,3]
> [1,]    1    4    7
> [2,]    2    5    8
> [3,]    3    6    9
>
> , , 2
>
>      [,1] [,2] [,3]
> [1,]   10   13   16
> [2,]   11   14   17
> [3,]   12   15   18
>
> , , 3
>
>      [,1] [,2] [,3]
> [1,]   19   22   25
> [2,]   20   23   26
> [3,]   21   24   27
>
>  > apply(at, 1, max)
> [1] 25 26 27
>
> If, for the MARGIN argument in apply() 1 is rows, I would have expected
> as output a 3x3 matrix something like
>
> 7 16 25
> 8 17 16
> 9 18 27
>
> Either that, or maybe the transpose of that, but a single vector seems
> rather random. Especially when you go
>
>  > apply(at, 3, max)
> [1]  9 18 27
>
> What is that the max of? Each submatrix? The diagonal? I'm confused. Can
> anyone clarify this?
>
> Besides this, is there a function that will work on a 3d array, the way
> I'm implying, or do I need to write an explicit loop that takes 2d
> slices of my 3d array?
>
> Thanks!
>
>        Toby
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mothsailor at googlemail.com  Wed Sep 20 21:53:43 2006
From: mothsailor at googlemail.com (David Barron)
Date: Wed, 20 Sep 2006 20:53:43 +0100
Subject: [R] Unexpected behavior of apply() over a 3d array
In-Reply-To: <45119816.4010408@mail.utexas.edu>
References: <45119816.4010408@mail.utexas.edu>
Message-ID: <815b70590609201253i5b198198p19837b4c19234a23@mail.gmail.com>

It's definitely not random!  Try apply(at,3,I) to see what the
marginal table is that max operates on, and you'll see where your
result comes from.

On 20/09/06, Toby Muhlhofer <toby.m at mail.utexas.edu> wrote:
> Dear listeRs,
>
> I'm finding that apply() behaves strangely when used on a 3-d array. For
> example:
>
>  > at <- array(1:27,dim=c(3,3,3))
>  > at
> , , 1
>
>       [,1] [,2] [,3]
> [1,]    1    4    7
> [2,]    2    5    8
> [3,]    3    6    9
>
> , , 2
>
>       [,1] [,2] [,3]
> [1,]   10   13   16
> [2,]   11   14   17
> [3,]   12   15   18
>
> , , 3
>
>       [,1] [,2] [,3]
> [1,]   19   22   25
> [2,]   20   23   26
> [3,]   21   24   27
>
>  > apply(at, 1, max)
> [1] 25 26 27
>
> If, for the MARGIN argument in apply() 1 is rows, I would have expected
> as output a 3x3 matrix something like
>
> 7 16 25
> 8 17 16
> 9 18 27
>
> Either that, or maybe the transpose of that, but a single vector seems
> rather random. Especially when you go
>
>  > apply(at, 3, max)
> [1]  9 18 27
>
> What is that the max of? Each submatrix? The diagonal? I'm confused. Can
> anyone clarify this?
>
> Besides this, is there a function that will work on a 3d array, the way
> I'm implying, or do I need to write an explicit loop that takes 2d
> slices of my 3d array?
>
> Thanks!
>
>         Toby
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From p.murrell at auckland.ac.nz  Wed Sep 20 21:54:00 2006
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Thu, 21 Sep 2006 07:54:00 +1200
Subject: [R] How to draw a per mille symbol?
In-Reply-To: <1158750944.20600.22.camel@gsimpson.geog.ucl.ac.uk>
References: <1158683380.10675.38.camel@gsimpson.geog.ucl.ac.uk>	
	<4510B573.4080309@stat.auckland.ac.nz>
	<1158750944.20600.22.camel@gsimpson.geog.ucl.ac.uk>
Message-ID: <45119C58.8060901@stat.auckland.ac.nz>

Hi


Gavin Simpson wrote:
> On Wed, 2006-09-20 at 15:28 +1200, Paul Murrell wrote:
>> Hi
>>
>>
>> Gavin Simpson wrote:
>>> Dear list,
>>>
>>> Following advice posted to this list a while back by Prof Ripley [1], I
>>> have been trying to draw a per mille character [2] in an axis label.
>>>
>>> This should give the correct character:
>>>
>>> plot(1:10, ylab = "\u2030")
>>>
>>> but all I get is '"S'. I'm running linux (FC5) and have fonts installed
>>> that have the correct character (viewed in the Gnome character map at
>>> least).
> 
> Thanks for your reply Paul, and also to Andrew Robinson for his earlier
> reply. I had initially avoided trying to use the encoding argument and
> pdf() as I had planned to include the code to produce the graphics in a
> Sweave document and AFAICS there is no way to pass extra arguments to
> the code generating the pdf figures in Sweave? 


In cases like this, I put an explicit pdf() call (and dev.off()) in my
Sweave code chunk and then explicitly \includegraphics{} the resulting
figure.


> Of course, my original
> plan was ignorant of the details of font encodings and mappings to
> single byte encodings in pdf and postscript devices and wouldn't have
> worked anyway.
> 
>>
>> I get the same thing (and using xfd I see the per mille character in the 
>> font I'm using).  I'm afraid I'm not sure why this is happening;  I can 
>> get a number of other "unusual" characters to work (e.g., \u20ac), but 
>> there appear to be some characters that do not draw correctly.  I used 
>> the following code to explore the default Helvetica font I've got and I 
>> can't see a rational pattern in the misbehaviour.
>>
>> x11(width=5, height=5)
>> grid.prompt(TRUE)
>> digits <- c(0:9, letters[1:6])
>> for (i in c("00", "01", "02", "03", "1e", "20", "21", "22")) {
>>      grid.newpage()
>>      for (j in 1:16) {
>>          for (k in 1:16) {
>>              pushViewport(viewport(x=j/16, y=1-k/16,
>>                                    width=1/16, height=1/16,
>>                                    just=c("right", "bottom")))
>>              eval(parse(text=paste('grid.text("\\u',
>>                           i, digits[k], digits[j], '")', sep="")))

                grid.text(paste("\\u", i, digits[k], digits[j], sep=""),
                          y=1, just="top",
                          gp=gpar(col="grey", cex=0.5))

>>              popViewport()
>>          }
>>      }
>> }
>>
> 
> That is a nice tool for looking at the font glyphs, which I can see
> being very useful in working out which unicode number matches the
> character you want to display. I'm not too clued up on grid graphics
> yet, would it be easy to modify the above to print out the \uXXXX code
> above each glyph?


Sure.  See code above.

Paul


>>> I have also tried plotting to a pdf device with a font family that the
>>> character map tool shows I have a per mille glyph for, e.g.:
>>>
>>> pdf("~/tmp/test_per_mille.pdf", paper = "a4", family = "URWBookman")
>>> plot(1:10, ylab = "\u2030")
>>> dev.off()
>>>
>>> But all I get here is a period or a dot-like symbol.
>>
>> This is an encoding problem I think.  For producing PDF output, the 
>> character string gets converted to a single-byte encoding.   If your 
>> default locale is ISOLatin1 like mine then you won't see the per mille 
>> because that character (called perthousand in the Adobe afm's) is not in 
>> the ISOLatin1 encoding.  If you explicitly use an encoding that does 
>> include perthousand (like WinAnsi) then the conversion to single-byte 
>> encoding works.  For example, this works (for me at least) ...
>>
>> pdf("WinAnsi_per_mille.pdf", encoding="WinAnsi")
>> plot(1:10, ylab = "\u2030")
>> dev.off()
>>
>> Paul
> 
> Thanks for this, which works fine for me also.
> 
> All the best,
> 
> G
> 
>>
>>> I've tried this in R 2.4.0 alpha [4] and R 2.5.0 to be [4] as my
>>> self-compiled R 2.3.1-patched dies when plotting Unicode characters
>>> (fixed in 2.4.0 alpha and above [3])
>>>
>>> Can anyone point me in the right direction to get this working?
>>>
>>> TIA,
>>>
>>> G
>>>
>>> [1] http://finzi.psych.upenn.edu/R/Rhelp02a/archive/48709.html
>>> [2] like a "%" but with 2 circles at the bottom not one, see
>>> http://en.wikipedia.org/wiki/Permille
>>> [3] see thread at http://article.gmane.org/gmane.comp.lang.r.devel/9704
>>> [4] R version 2.4.0 alpha (2006-09-19 r39410)
>>> [5] R version 2.5.0 Under development (unstable) (2006-09-19 r39410)

-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/


From p.dalgaard at biostat.ku.dk  Wed Sep 20 21:59:17 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Sep 2006 21:59:17 +0200
Subject: [R] Poission distribution
In-Reply-To: <5cd96f050609201243m6eb921fey84ec8f33e7b346e4@mail.gmail.com>
References: <5cd96f050609201243m6eb921fey84ec8f33e7b346e4@mail.gmail.com>
Message-ID: <x2venixqay.fsf@turmalin.kubism.ku.dk>

"Ethan Johnsons" <ethan.johnsons at gmail.com> writes:

> The expected number of bladder cancer over next 20 years a tire
> industry is 1.8.  Poission distribution is assumed to hold and 6
> reported deaths are caused by bladder cancer among the employees.
> Trying to find how unusual this event is.
> 
> > ppois(q=6, lambda=1.8, lower.tail = TRUE, log.p = FALSE)
> [1] 0.9974306
> 
> not sure if ppois is the right one to use and the parameters...

I think not.

However, we're not doing your homework.

Instead, do the following

x <- 0:10
cbind(x, p=round(dpois(x, lambda=1.8),4))
plot(x, dpois(x, lambda=1.8), type="h", lwd=2)
abline(v=6)

then apply(brains)...

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From toby.m at mail.utexas.edu  Wed Sep 20 22:00:16 2006
From: toby.m at mail.utexas.edu (Toby Muhlhofer)
Date: Wed, 20 Sep 2006 15:00:16 -0500
Subject: [R] Unexpected behavior of apply() over a 3d array
In-Reply-To: <971536df0609201251s4a65deb0hf721f17472fb607e@mail.gmail.com>
References: <45119816.4010408@mail.utexas.edu>
	<971536df0609201251s4a65deb0hf721f17472fb607e@mail.gmail.com>
Message-ID: <45119DD0.8030102@mail.utexas.edu>

It just seemed wierd because over a 2d array, it also returns vectors, 
but I guess breaking things down into components of the list it makes 
sense. Besides, using David's suggestion of apply(at, MARGIN, I) also 
offers the insights I need.

Gabor Grothendieck wrote:
> Read ?apply carefullly.    If FUN returns as a scalar as it does here
> then the result dimensions are dim(X)[MARGIN]. For example,
> 
> apply(X, 1, max) has three components which are
> max(X[1,,]), max(X[2,,]) and max(X[3,,])
> 
> and apply(X, 3, max) has three components which are
> max(X[,,1]), max(X[,,2]) and max(X[,,3])
> 
> Also try apply(X, 1:2, max), etc.
> 
> On 9/20/06, Toby Muhlhofer <toby.m at mail.utexas.edu> wrote:
>> Dear listeRs,
>>
>> I'm finding that apply() behaves strangely when used on a 3-d array. For
>> example:
>>
>>  > at <- array(1:27,dim=c(3,3,3))
>>  > at
>> , , 1
>>
>>      [,1] [,2] [,3]
>> [1,]    1    4    7
>> [2,]    2    5    8
>> [3,]    3    6    9
>>
>> , , 2
>>
>>      [,1] [,2] [,3]
>> [1,]   10   13   16
>> [2,]   11   14   17
>> [3,]   12   15   18
>>
>> , , 3
>>
>>      [,1] [,2] [,3]
>> [1,]   19   22   25
>> [2,]   20   23   26
>> [3,]   21   24   27
>>
>>  > apply(at, 1, max)
>> [1] 25 26 27
>>
>> If, for the MARGIN argument in apply() 1 is rows, I would have expected
>> as output a 3x3 matrix something like
>>
>> 7 16 25
>> 8 17 16
>> 9 18 27
>>
>> Either that, or maybe the transpose of that, but a single vector seems
>> rather random. Especially when you go
>>
>>  > apply(at, 3, max)
>> [1]  9 18 27
>>
>> What is that the max of? Each submatrix? The diagonal? I'm confused. Can
>> anyone clarify this?
>>
>> Besides this, is there a function that will work on a 3d array, the way
>> I'm implying, or do I need to write an explicit loop that takes 2d
>> slices of my 3d array?
>>
>> Thanks!
>>
>>        Toby
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>


From ethan.johnsons at gmail.com  Wed Sep 20 22:01:29 2006
From: ethan.johnsons at gmail.com (Ethan Johnsons)
Date: Wed, 20 Sep 2006 16:01:29 -0400
Subject: [R] ppois
Message-ID: <5cd96f050609201301kf5a152fpd4214b3189388a04@mail.gmail.com>

A quick question!

The number of episodes per year of otitis media follows a Possion
distribution with lambda = 1.6 episodes per year.   Wouldn't the
probability of getting 3 or more episodes of otitis media in the first
2 years of life be:

> ppois(q=3, lambda=1.6*2, lower.tail = TRUE, log.p = FALSE)
[1] 0.6025197

I am confused with the lambda and 3 or more..

thx much


From mothsailor at googlemail.com  Wed Sep 20 22:01:37 2006
From: mothsailor at googlemail.com (David Barron)
Date: Wed, 20 Sep 2006 21:01:37 +0100
Subject: [R] Poission distribution
In-Reply-To: <815b70590609201301s3fd390c5oa8e28a2b1d1d8b89@mail.gmail.com>
References: <5cd96f050609201243m6eb921fey84ec8f33e7b346e4@mail.gmail.com>
	<815b70590609201301s3fd390c5oa8e28a2b1d1d8b89@mail.gmail.com>
Message-ID: <815b70590609201301l53a563cct837b379b33f38507@mail.gmail.com>

Is that 6 deaths over 20 years?  If so, the probability of getting
exactly 6 deaths is given by

> dpois(6,1.8)
[1] 0.007808587

The probability of getting six or more deaths is 1 minus the result
you obtained.

On 20/09/06, Ethan Johnsons <ethan.johnsons at gmail.com> wrote:
> The expected number of bladder cancer over next 20 years a tire
> industry is 1.8.  Poission distribution is assumed to hold and 6
> reported deaths are caused by bladder cancer among the employees.
> Trying to find how unusual this event is.
>
> > ppois(q=6, lambda=1.8, lower.tail = TRUE, log.p = FALSE)
> [1] 0.9974306
>
> not sure if ppois is the right one to use and the parameters...
>
> thx much
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


--
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From hodgess at gator.dt.uh.edu  Wed Sep 20 22:07:15 2006
From: hodgess at gator.dt.uh.edu (Erin Hodgess)
Date: Wed, 20 Sep 2006 15:07:15 -0500
Subject: [R]  Dennis Chabot poss. answer
Message-ID: <200609202007.k8KK7FaB012751@gator.dt.uh.edu>

Hi Dennis!

Here is a possible solution to consider:


> a <- data.frame(x=1:25,y=rnorm(25))
> a
    x           y
1   1  1.38958705
2   2 -0.45165628
3   3  0.86517671
4   4  0.40802481
5   5 -2.00104605
6   6  0.11152748
7   7  1.84400316
8   8  0.80775204
9   9 -0.12510867
10 10 -0.13650037
11 11 -0.63498148
12 12  1.70665004
13 13 -0.42427846
14 14  0.38587832
15 15 -0.07601500
16 16  0.40442795
17 17  1.78181958
18 18 -0.07199413
19 19 -0.62582419
20 20  0.71653130
21 21 -1.07557102
22 22 -0.04874676
23 23 -0.09447060
24 24 -0.99221486
25 25  0.16559026
> # Set row numbers....
> b <- c(2,5,7,18,22)
> a[b,2] <- rnorm(length(b))
> a
    x          y
1   1  1.3895870
2   2 -0.4613160
3   3  0.8651767
4   4  0.4080248
5   5 -0.5564420
6   6  0.1115275
7   7  1.6625939
8   8  0.8077520
9   9 -0.1251087
10 10 -0.1365004
11 11 -0.6349815
12 12  1.7066500
13 13 -0.4242785
14 14  0.3858783
15 15 -0.0760150
16 16  0.4044279
17 17  1.7818196
18 18  2.0357456
19 19 -0.6258242
20 20  0.7165313
21 21 -1.0755710
22 22 -1.2000225
23 23 -0.0944706
24 24 -0.9922149
25 25  0.1655903
 

Hope this can be useful!

Note:  I accidentally deleted the original e-mail.

Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu


From deepayan.sarkar at gmail.com  Wed Sep 20 22:08:43 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Wed, 20 Sep 2006 13:08:43 -0700
Subject: [R] functionality of "update" in SAS
In-Reply-To: <29E5D33C-B243-4949-B13D-57BD06B3AE18@globetrotter.net>
References: <29E5D33C-B243-4949-B13D-57BD06B3AE18@globetrotter.net>
Message-ID: <eb555e660609201308s455caeb9oc573f71d28c6d201@mail.gmail.com>

On 9/20/06, Denis Chabot <chabotd at globetrotter.net> wrote:
> Dear list,
>
> I've tried to search the archives but found nothing, although I may
> use the wrong wording in my searches. I've also double-checked the
> upData function in Hmisc, but it does something else.
>
> I'm wondering if one can update a dataframe by "forcing into" it a
> shorter dataframe containing the corrections, like the "update"
> provided in SAS data steps.
>
> In this simple example:
> a <- data.frame(id=c(1:5),x=rnorm(5))
> b <- data.frame(id=4,x=rnorm(1))
>  > a
>    id          x
> 1  1  0.6557921
> 2  2  0.1897523
> 3  3  0.7976721
> 4  4  0.2107103
> 5  5 -0.8855786
>  > b
>    id         x
> 1  4 0.8369147
>
> I would like the "updated" dataframe to look like (row names are not
> important to me)
>
>     id          x
> 1   1  0.6557921
> 2   2  0.1897523
> 3   3  0.7976721
> 4   4  0.8369147
> 5   5 -0.8855786

Making a few assumtions (like id's being unique, b$id guaranteed to be
in a$id and all columns in b are also in a), you could do

which.id <- which(a$id %in% b$id)
a[which.id, colnames(b)] <- b

-Deepayan


From amichuang at hotmail.com  Wed Sep 20 22:09:05 2006
From: amichuang at hotmail.com (Ya-Hsiu Chuang)
Date: Wed, 20 Sep 2006 16:09:05 -0400
Subject: [R] how to delete some columns from a matrix based on some other
	indicator variable
Message-ID: <BAY101-F959CFA9EC0950E4072D46BC230@phx.gbl>

Hello,

I am not very familiar with R and need help in deleting a few columns in a 
matrix.

Suppose I have a indicator variable called r and it's defined as r = (0, 0, 
1, 1). A matrix D is a 3X4 matrix. If I want a new matrix which contains 
only the columns of D corresponding to the elements of r that equal to 1. 
how can i write a loop which creat a new matrix that contains only the last 
2 columns of D in this case? thanks


From stubben at lanl.gov  Wed Sep 20 22:15:43 2006
From: stubben at lanl.gov (Chris Stubben)
Date: Wed, 20 Sep 2006 20:15:43 +0000 (UTC)
Subject: [R] How to store an array in MySQL
References: <50a711930609191514p38551ffeg90be35daf000b48e@mail.gmail.com>
Message-ID: <loom.20060920T220920-332@post.gmane.org>

 
> Hi all, does somebody know how to store an array in MySQL with the
> package RMySQL. Thanks in advance.
> 

A similar question was asked last month. 

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/81429.html

In a normalized database, you should store the index and value in separate
columns.  Try this...


a<-array(1:3, c(2, 4))
     [,1] [,2] [,3] [,4]
[1,]    1    3    2    1
[2,]    2    1    3    2


d <-dim(a)


z<-cbind(expand.grid(r=1:d[1],c=1:d[2]),x=as.vector(a))
  r c x
1 1 1 1
2 2 1 2
3 1 2 3
4 2 2 1
5 1 3 2
6 2 3 3
7 1 4 1
8 2 4 2


library(RMySQL)

con<-dbConnect(MySQL(), dbname="test")

dbWriteTable(con, "array", z, row.names=FALSE)
[1] TRUE


---

Now in Mysql 

select * from array;
+------+------+------+
| r    | c    | x    |
+------+------+------+
|    1 |    1 |    1 |
|    2 |    1 |    2 |
|    1 |    2 |    3 |
|    2 |    2 |    1 |
|    1 |    3 |    2 |
|    2 |    3 |    3 |
|    1 |    4 |    1 |
|    2 |    4 |    2 |
+------+------+------+



select group_concat(x order by c separator ' ' ) as a from array group by r;
+---------+
| a       |
+---------+
| 1 3 2 1 |
| 2 1 3 2 |
+---------+


Chris Stubben


From dylan.beaudette at gmail.com  Wed Sep 20 22:40:49 2006
From: dylan.beaudette at gmail.com (Dylan Beaudette)
Date: Wed, 20 Sep 2006 13:40:49 -0700
Subject: [R] mysterious error on compile R 2.3.1
In-Reply-To: <x24pv2z5t8.fsf@turmalin.kubism.ku.dk>
References: <200609201208.02782.dylan.beaudette@gmail.com>
	<x24pv2z5t8.fsf@turmalin.kubism.ku.dk>
Message-ID: <200609201340.50048.dylan.beaudette@gmail.com>

On Wednesday 20 September 2006 12:38, Peter Dalgaard wrote:
> Dylan Beaudette <dylan.beaudette at gmail.com> writes:
> > Getting a very strange error with a new install of R from source on x86;
> >
> > make[3]: Leaving directory `/tmp/R.INSTALL.r20887/cluster/src'
> > ** R
> > ** data
> > **  moving datasets to lazyload DB
> > Error in factor(c(1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1), 
> > : invalid labels; length 2 should be 1 or 1
> > Execution halted
> > ERROR: lazydata failed for package 'cluster'
> > ** Removing '/home/dylan/src/R-2.3.1/library/cluster'
> > make[2]: *** [cluster.ts] Error 1
> > make[2]: Leaving directory
> > `/home/dylan/src/R-2.3.1/src/library/Recommended' make[1]: ***
> > [recommended-packages] Error 2
> > make[1]: Leaving directory
> > `/home/dylan/src/R-2.3.1/src/library/Recommended' make: ***
> > [stamp-recommended] Error 2
> >
> > note that i am using the GCC flags:
> > CFLAGS=-march=opteron -ffast-math
> > CXXFLAGS=-march=opteron -ffast-math
> >
> > any ideas?
>
> Don't do that....
>
> Seriously!
>
> -ffast-math will allow the compiler to break IEEE math specifications,
>  which in turn will break R all over the place.

yikes! i wont do that anymore.

this fixed the problem. thanks!

cheers,

-- 
Dylan Beaudette
Soils and Biogeochemistry Graduate Group
University of California at Davis
530.754.7341


From iuri at ufrgs.br  Wed Sep 20 22:21:58 2006
From: iuri at ufrgs.br (Iuri Gavronski)
Date: Wed, 20 Sep 2006 17:21:58 -0300
Subject: [R] Statitics Textbook - any recommendation?
Message-ID: <20060920172158.0i94pyn6so7qcksk@webmail.ufrgs.br>

I would like to buy a basic statistics book (experimental design,  
sampling, ANOVA, regression, etc.) with examples in R. Or download it  
in PDF or html format.
I went to the CRAN contributed documentation, but there were only R  
textbooks, that is, textbooks where R is the focus, not the  
statistics. And I would like to find the opposite.
Other text I am trying to find is multivariate data analysis (EFA,  
cluster, mult regression, MANOVA, etc.) with examples with R.
Any recommendation?

Thank you in advance,

Iuri.


From rmh at temple.edu  Wed Sep 20 22:33:22 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Wed, 20 Sep 2006 16:33:22 -0400 (EDT)
Subject: [R] Statitics Textbook - any recommendation?
Message-ID: <20060920163322.BIJ37880@po-d.temple.edu>

I recommend mine, which is listed in CRAN,

                Statistical Analysis and Data Display
                Richard M. Heiberger and Burt Holland

   http://springeronline.com/0-387-40270-5

All examples and figures in the book are included in the online files that
may be downloaded from the book's website.  The R package HH containing
the R functions from the online files is now on CRAN.


From mmalten at gmail.com  Wed Sep 20 23:04:33 2006
From: mmalten at gmail.com (Mitchell Maltenfort)
Date: Wed, 20 Sep 2006 17:04:33 -0400
Subject: [R] Statitics Textbook - any recommendation?
In-Reply-To: <20060920172158.0i94pyn6so7qcksk@webmail.ufrgs.br>
References: <20060920172158.0i94pyn6so7qcksk@webmail.ufrgs.br>
Message-ID: <8913fde30609201404y1c8355deh6112330b7f19caf7@mail.gmail.com>

Venables and Ripley's Modern Applied Statistics with S was recommended
on the CRAN site, and I like it myself.


On 9/20/06, Iuri Gavronski <iuri at ufrgs.br> wrote:
> I would like to buy a basic statistics book (experimental design,
> sampling, ANOVA, regression, etc.) with examples in R. Or download it
> in PDF or html format.
> I went to the CRAN contributed documentation, but there were only R
> textbooks, that is, textbooks where R is the focus, not the
> statistics. And I would like to find the opposite.
> Other text I am trying to find is multivariate data analysis (EFA,
> cluster, mult regression, MANOVA, etc.) with examples with R.
> Any recommendation?
>
> Thank you in advance,
>
> Iuri.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
I can answer any question.
"I don't know" is an answer.
"I don't know yet" is a better answer.


From gunter.berton at gene.com  Wed Sep 20 23:09:48 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 20 Sep 2006 14:09:48 -0700
Subject: [R] Statitics Textbook - any recommendation?
In-Reply-To: <20060920172158.0i94pyn6so7qcksk@webmail.ufrgs.br>
Message-ID: <006c01c6dcf9$1b4b3980$711f210a@gne.windows.gene.com>

Not withstanding Prof. Heiberger's admirable enthusiasm, I think the
canonical answer is probably MASS (Modern Applied Statistics with S) by
Venables and Ripley. It is very comprehensive, but depending on your
background, you may find it too telegraphic.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Iuri Gavronski
> Sent: Wednesday, September 20, 2006 1:22 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Statitics Textbook - any recommendation?
> 
> I would like to buy a basic statistics book (experimental design,  
> sampling, ANOVA, regression, etc.) with examples in R. Or 
> download it  
> in PDF or html format.
> I went to the CRAN contributed documentation, but there were only R  
> textbooks, that is, textbooks where R is the focus, not the  
> statistics. And I would like to find the opposite.
> Other text I am trying to find is multivariate data analysis (EFA,  
> cluster, mult regression, MANOVA, etc.) with examples with R.
> Any recommendation?
> 
> Thank you in advance,
> 
> Iuri.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From maj at waikato.ac.nz  Wed Sep 20 23:13:38 2006
From: maj at waikato.ac.nz (Murray Jorgensen)
Date: Thu, 21 Sep 2006 09:13:38 +1200
Subject: [R] Pooled Covariance Matrix
In-Reply-To: <Pine.LNX.4.64.0609200708230.23370@gannet.stats.ox.ac.uk>
References: <4510C75B.8040705@stats.waikato.ac.nz>
	<Pine.LNX.4.64.0609200708230.23370@gannet.stats.ox.ac.uk>
Message-ID: <4511AF02.2070502@waikato.ac.nz>

Thank you, Professor Ripley.  Murray Jorgensen

Prof Brian Ripley wrote:
> On Wed, 20 Sep 2006, Murray Jorgensen wrote:
> 
>> I am in a discriminant analysis situation with a frame containing
>> several variables and a grouping factor, if you like:
>>
>> set.seed(200906)
>> exampledf <- as.data.frame(matrix(rnorm(50,5,2),nrow=10,ncol=5))
>> exampledf$Group <- factor(rep(c(1,2,3),c(3,3,4)))
>> exampledf
>>
>> I'm sure there must be a simple way to get the within group pooled
>> covariance matrix but I haven't found it yet.
> 
> There are two versions of this, weighted and unweighted, and the 
> difference caused confusion in the early discriminant analysis 
> literature. (See MASS4 p.333.)  The weighted version is conventional.
> 
> Suppose you have a matrix X and a grouping factor g.  Then either of
> 
>    group.means <- rowsum(X, g)/as.vector(table(g))
>    group.means <- tapply(X, list(rep(g, ncol(X)), col(X)), mean)
> 
> gives the group means, and var(X - group.means[g,]) seems to be what you 
> want.
> 
>> I started thinking that one might begin by forming a frame with the same
>>  dimensions but containing the group means. But then I found a thread
>> from two years back called "Getting the groupmean for each person" which
>> seemed to imply that doing this was a bit subtle even for ncol=1. Hence
>> I will risk a question to the list.
> 
> That thread seems to be about efficiency for very large matrices on R of 
> two years' ago.
> 

-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    Home +64 7 825 0441    Mobile 021 1395 862


From devens8765 at yahoo.com  Wed Sep 20 23:19:04 2006
From: devens8765 at yahoo.com (Dave Evens)
Date: Wed, 20 Sep 2006 14:19:04 -0700 (PDT)
Subject: [R] Spliting a huge vector
Message-ID: <20060920211904.25692.qmail@web58310.mail.re3.yahoo.com>


Dear R users,

I have a huge vector that I would like to split into
unequal slices. However, the only way I can do this is
to create another huge vector to define the groups
that are used to split the original vector, e.g.

# my vector is this
a.vector <- seq(2, by=5, length=100)

# indices where I would like to slice my vector
cut.values <- c(30, 50, 100, 109, 300, 601, 803)

# so I have to create another vector of similar length
# to use the split() command, i.e.
x <- rep(1:length(cut.values), times=diff(c(0,
cut.values))

# this means I can use split()
split(a.vector, x)

This seems to be a waste in terms of memory usage as
I'm creating another vector (here "x") to split the
original vector. Is there a better way to split a huge
vector than this? Any help is much appreciated. 

Best,
Dave.


From goedman at mac.com  Thu Sep 21 00:15:06 2006
From: goedman at mac.com (Rob J Goedman)
Date: Wed, 20 Sep 2006 15:15:06 -0700
Subject: [R] Statitics Textbook - any recommendation?
In-Reply-To: <006c01c6dcf9$1b4b3980$711f210a@gne.windows.gene.com>
References: <006c01c6dcf9$1b4b3980$711f210a@gne.windows.gene.com>
Message-ID: <D097E4AD-BD86-4E20-97FC-3F216BEA6D4F@mac.com>

I would certainly consider the Michael Crawley's: Statistics, an  
introduction using R
(maybe before turning to MASS?).

Rob


On Sep 20, 2006, at 2:09 PM, Berton Gunter wrote:

> Not withstanding Prof. Heiberger's admirable enthusiasm, I think the
> canonical answer is probably MASS (Modern Applied Statistics with  
> S) by
> Venables and Ripley. It is very comprehensive, but depending on your
> background, you may find it too telegraphic.
>
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
>
> "The business of the statistician is to catalyze the scientific  
> learning
> process."  - George E. P. Box
>
>
>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Iuri Gavronski
>> Sent: Wednesday, September 20, 2006 1:22 PM
>> To: r-help at stat.math.ethz.ch
>> Subject: [R] Statitics Textbook - any recommendation?
>>
>> I would like to buy a basic statistics book (experimental design,
>> sampling, ANOVA, regression, etc.) with examples in R. Or
>> download it
>> in PDF or html format.
>> I went to the CRAN contributed documentation, but there were only R
>> textbooks, that is, textbooks where R is the focus, not the
>> statistics. And I would like to find the opposite.
>> Other text I am trying to find is multivariate data analysis (EFA,
>> cluster, mult regression, MANOVA, etc.) with examples with R.
>> Any recommendation?
>>
>> Thank you in advance,
>>
>> Iuri.
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From mothsailor at googlemail.com  Thu Sep 21 00:22:04 2006
From: mothsailor at googlemail.com (David Barron)
Date: Wed, 20 Sep 2006 23:22:04 +0100
Subject: [R] Spliting a huge vector
In-Reply-To: <20060920211904.25692.qmail@web58310.mail.re3.yahoo.com>
References: <20060920211904.25692.qmail@web58310.mail.re3.yahoo.com>
Message-ID: <815b70590609201522m65183fc6t866815ecfdd14f1a@mail.gmail.com>

It's been a long day, so my brain isn't working too well!  How about this:

cut.values <- c(1, 30, 50, 100, 109, 300, 601, 803, length(a.vector))
vec.nms <- paste("vec",1:length(cut.values+1),sep="")
for (i in 1:(length(cut.values)-1))
assign(vec.nms[i],a.vector[cut.values[i]:cut.values[i+1]])

One of the real experts, or just someone more wide awake, can probably
come up with something better!
-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From mothsailor at googlemail.com  Thu Sep 21 00:28:18 2006
From: mothsailor at googlemail.com (David Barron)
Date: Wed, 20 Sep 2006 23:28:18 +0100
Subject: [R] how to delete some columns from a matrix based on some
	other indicator variable
In-Reply-To: <BAY101-F959CFA9EC0950E4072D46BC230@phx.gbl>
References: <BAY101-F959CFA9EC0950E4072D46BC230@phx.gbl>
Message-ID: <815b70590609201528p56135b07we84e0c26c6689fb8@mail.gmail.com>

You don't need a loop.  You could try

> r <- c(0,0,1,1)
> matD <- matrix(1:12,nrow=3)
> matD
     [,1] [,2] [,3] [,4]
[1,]    1    4    7   10
[2,]    2    5    8   11
[3,]    3    6    9   12

> matD[,r==1]
> matD[,r==1]
     [,1] [,2]
[1,]    7   10
[2,]    8   11
[3,]    9   12


On 20/09/06, Ya-Hsiu Chuang <amichuang at hotmail.com> wrote:
> Hello,
>
> I am not very familiar with R and need help in deleting a few columns in a
> matrix.
>
> Suppose I have a indicator variable called r and it's defined as r = (0, 0,
> 1, 1). A matrix D is a 3X4 matrix. If I want a new matrix which contains
> only the columns of D corresponding to the elements of r that equal to 1.
> how can i write a loop which creat a new matrix that contains only the last
> 2 columns of D in this case? thanks
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From mr.blacksheep at gmail.com  Thu Sep 21 00:35:32 2006
From: mr.blacksheep at gmail.com (Mike Nielsen)
Date: Wed, 20 Sep 2006 17:35:32 -0500
Subject: [R] Statitics Textbook - any recommendation?
In-Reply-To: <006c01c6dcf9$1b4b3980$711f210a@gne.windows.gene.com>
References: <20060920172158.0i94pyn6so7qcksk@webmail.ufrgs.br>
	<006c01c6dcf9$1b4b3980$711f210a@gne.windows.gene.com>
Message-ID: <46a360560609201535obf8e326p650d4fb9df22dfe9@mail.gmail.com>

Excellent characterization.

MASS is a very good book, but I'm not sure I would describe it as a
statistics textbook, much less one of the "basic" variety.  While I
certainly wouldn't presume to speak for Prof. Ripley and Dr. Venables,
it seems unlikely their intent in writing MASS was to teach
statistics, but rather, as the name of the book might suggest, to
explain how S+ (and R) can be applied to modern statistical
techniques.  My experience with this book is that it assumes
considerable background knowledge.

By all means, buy MASS, but if you need guidance on the how and why of
statistical techniques, you may wish to shop Amazon to find a
supplement.

Regards,

Mike

On 9/20/06, Berton Gunter <gunter.berton at gene.com> wrote:
> Not withstanding Prof. Heiberger's admirable enthusiasm, I think the
> canonical answer is probably MASS (Modern Applied Statistics with S) by
> Venables and Ripley. It is very comprehensive, but depending on your
> background, you may find it too telegraphic.
>
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
>
> "The business of the statistician is to catalyze the scientific learning
> process."  - George E. P. Box
>
>
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Iuri Gavronski
> > Sent: Wednesday, September 20, 2006 1:22 PM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] Statitics Textbook - any recommendation?
> >
> > I would like to buy a basic statistics book (experimental design,
> > sampling, ANOVA, regression, etc.) with examples in R. Or
> > download it
> > in PDF or html format.
> > I went to the CRAN contributed documentation, but there were only R
> > textbooks, that is, textbooks where R is the focus, not the
> > statistics. And I would like to find the opposite.
> > Other text I am trying to find is multivariate data analysis (EFA,
> > cluster, mult regression, MANOVA, etc.) with examples with R.
> > Any recommendation?
> >
> > Thank you in advance,
> >
> > Iuri.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Regards,

Mike Nielsen


From ken.pierce at oregonstate.edu  Thu Sep 21 00:42:51 2006
From: ken.pierce at oregonstate.edu (Pierce, Ken)
Date: Wed, 20 Sep 2006 15:42:51 -0700
Subject: [R] problem coercing truncated character vector to levels
Message-ID: <4D5DA98A54374044B7CC3F40A157B98B7557D9@thuja>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060920/0761eae2/attachment.pl 

From Charles.Annis at StatisticalEngineering.com  Thu Sep 21 00:56:10 2006
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Wed, 20 Sep 2006 18:56:10 -0400
Subject: [R] Statitics Textbook - any recommendation?
In-Reply-To: <46a360560609201535obf8e326p650d4fb9df22dfe9@mail.gmail.com>
Message-ID: <03b301c6dd07$f7487bb0$6400a8c0@DD4XFW31>

Recommending a good book on statistics is like recommending a good book on
sports:  Which sports?

A good book for learning statistical concepts (and learning R at the same
time), one that assumes you understand algebra but are new to statistics, is
Peter Dalgaard's _Introductory Statistics with R_ (Springer 2002).  The
writing is relaxed and succinct, not condescending as some texts might
appear to a newcomer.  It's just a good book.

Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Mike Nielsen
Sent: Wednesday, September 20, 2006 6:36 PM
To: Berton Gunter
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Statitics Textbook - any recommendation?

Excellent characterization.

MASS is a very good book, but I'm not sure I would describe it as a
statistics textbook, much less one of the "basic" variety.  While I
certainly wouldn't presume to speak for Prof. Ripley and Dr. Venables,
it seems unlikely their intent in writing MASS was to teach
statistics, but rather, as the name of the book might suggest, to
explain how S+ (and R) can be applied to modern statistical
techniques.  My experience with this book is that it assumes
considerable background knowledge.

By all means, buy MASS, but if you need guidance on the how and why of
statistical techniques, you may wish to shop Amazon to find a
supplement.

Regards,

Mike

On 9/20/06, Berton Gunter <gunter.berton at gene.com> wrote:
> Not withstanding Prof. Heiberger's admirable enthusiasm, I think the
> canonical answer is probably MASS (Modern Applied Statistics with S) by
> Venables and Ripley. It is very comprehensive, but depending on your
> background, you may find it too telegraphic.
>
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
>
> "The business of the statistician is to catalyze the scientific learning
> process."  - George E. P. Box
>
>
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Iuri Gavronski
> > Sent: Wednesday, September 20, 2006 1:22 PM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] Statitics Textbook - any recommendation?
> >
> > I would like to buy a basic statistics book (experimental design,
> > sampling, ANOVA, regression, etc.) with examples in R. Or
> > download it
> > in PDF or html format.
> > I went to the CRAN contributed documentation, but there were only R
> > textbooks, that is, textbooks where R is the focus, not the
> > statistics. And I would like to find the opposite.
> > Other text I am trying to find is multivariate data analysis (EFA,
> > cluster, mult regression, MANOVA, etc.) with examples with R.
> > Any recommendation?
> >
> > Thank you in advance,
> >
> > Iuri.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Regards,

Mike Nielsen

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From gavin.simpson at ucl.ac.uk  Thu Sep 21 01:08:17 2006
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Thu, 21 Sep 2006 00:08:17 +0100
Subject: [R] Statitics Textbook - any recommendation?
In-Reply-To: <03b301c6dd07$f7487bb0$6400a8c0@DD4XFW31>
References: <03b301c6dd07$f7487bb0$6400a8c0@DD4XFW31>
Message-ID: <1158793697.2838.1.camel@dhcppc2.my.nat.localnet>

On Wed, 2006-09-20 at 18:56 -0400, Charles Annis, P.E. wrote:
> Recommending a good book on statistics is like recommending a good book on
> sports:  Which sports?
> 
> A good book for learning statistical concepts (and learning R at the same
> time), one that assumes you understand algebra but are new to statistics, is
> Peter Dalgaard's _Introductory Statistics with R_ (Springer 2002).  The
> writing is relaxed and succinct, not condescending as some texts might
> appear to a newcomer.  It's just a good book.

I couldn't agree more. A number of my colleagues have bought Peter
Dalgaard's book to a) learn some R and b) learn some statistics. They
have found it very useful indeed.

G

> 
> Charles Annis, P.E.
> 
> Charles.Annis at StatisticalEngineering.com
> phone: 561-352-9699
> eFax:  614-455-3265
> http://www.StatisticalEngineering.com
>  
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Mike Nielsen
> Sent: Wednesday, September 20, 2006 6:36 PM
> To: Berton Gunter
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Statitics Textbook - any recommendation?
> 
> Excellent characterization.
> 
> MASS is a very good book, but I'm not sure I would describe it as a
> statistics textbook, much less one of the "basic" variety.  While I
> certainly wouldn't presume to speak for Prof. Ripley and Dr. Venables,
> it seems unlikely their intent in writing MASS was to teach
> statistics, but rather, as the name of the book might suggest, to
> explain how S+ (and R) can be applied to modern statistical
> techniques.  My experience with this book is that it assumes
> considerable background knowledge.
> 
> By all means, buy MASS, but if you need guidance on the how and why of
> statistical techniques, you may wish to shop Amazon to find a
> supplement.
> 
> Regards,
> 
> Mike
> 
> On 9/20/06, Berton Gunter <gunter.berton at gene.com> wrote:
> > Not withstanding Prof. Heiberger's admirable enthusiasm, I think the
> > canonical answer is probably MASS (Modern Applied Statistics with S) by
> > Venables and Ripley. It is very comprehensive, but depending on your
> > background, you may find it too telegraphic.
> >
> > -- Bert Gunter
> > Genentech Non-Clinical Statistics
> > South San Francisco, CA
> >
> > "The business of the statistician is to catalyze the scientific learning
> > process."  - George E. P. Box
> >
> >
> >
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch
> > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Iuri Gavronski
> > > Sent: Wednesday, September 20, 2006 1:22 PM
> > > To: r-help at stat.math.ethz.ch
> > > Subject: [R] Statitics Textbook - any recommendation?
> > >
> > > I would like to buy a basic statistics book (experimental design,
> > > sampling, ANOVA, regression, etc.) with examples in R. Or
> > > download it
> > > in PDF or html format.
> > > I went to the CRAN contributed documentation, but there were only R
> > > textbooks, that is, textbooks where R is the focus, not the
> > > statistics. And I would like to find the opposite.
> > > Other text I am trying to find is multivariate data analysis (EFA,
> > > cluster, mult regression, MANOVA, etc.) with examples with R.
> > > Any recommendation?
> > >
> > > Thank you in advance,
> > >
> > > Iuri.
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> 
> 
> -- 
> Regards,
> 
> Mike Nielsen
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 *Note new Address and Fax and Telephone numbers from 10th April 2006*
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [t] +44 (0)20 7679 0522
ECRC                              [f] +44 (0)20 7679 0565
UCL Department of Geography
Pearson Building                  [e] gavin.simpsonATNOSPAMucl.ac.uk
Gower Street
London, UK                        [w] http://www.ucl.ac.uk/~ucfagls/cv/
WC1E 6BT                          [w] http://www.ucl.ac.uk/~ucfagls/
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From ken.pierce at oregonstate.edu  Thu Sep 21 01:07:40 2006
From: ken.pierce at oregonstate.edu (Pierce, Ken)
Date: Wed, 20 Sep 2006 16:07:40 -0700
Subject: [R] problem coercing truncated character vector to levels
In-Reply-To: <20060920230254.11801.qmail@web8603.mail.in.yahoo.com>
Message-ID: <4D5DA98A54374044B7CC3F40A157B98B7557DA@thuja>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060920/104b059a/attachment.pl 

From ageneticist at yahoo.com  Thu Sep 21 03:32:37 2006
From: ageneticist at yahoo.com (Gamal Azim)
Date: Wed, 20 Sep 2006 18:32:37 -0700 (PDT)
Subject: [R] remotely saving an R session
Message-ID: <20060921013237.60168.qmail@web33402.mail.mud.yahoo.com>

Is it possible to remotely save an R session then
terminate R? Of course the destructive task after
'then' is rather straightforward by itself.

Thanks


From ageneticist at yahoo.com  Thu Sep 21 03:49:53 2006
From: ageneticist at yahoo.com (Gamal Azim)
Date: Wed, 20 Sep 2006 18:49:53 -0700 (PDT)
Subject: [R] remotely saving an R session
In-Reply-To: <20060921013237.60168.qmail@web33402.mail.mud.yahoo.com>
Message-ID: <20060921014953.73330.qmail@web33413.mail.mud.yahoo.com>

Forgot to indicate that the remote system is Linux,
accessed remotely by ssh. 

--- Gamal Azim <ageneticist at yahoo.com> wrote:

> Is it possible to remotely save an R session then
> terminate R? Of course the destructive task after
> 'then' is rather straightforward by itself.
> 
> Thanks
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From blomsp at ozemail.com.au  Thu Sep 21 04:11:40 2006
From: blomsp at ozemail.com.au (Simon Blomberg)
Date: Thu, 21 Sep 2006 12:11:40 +1000
Subject: [R] remotely saving an R session
In-Reply-To: <20060921014953.73330.qmail@web33413.mail.mud.yahoo.com>
References: <20060921014953.73330.qmail@web33413.mail.mud.yahoo.com>
Message-ID: <4511F4DC.6000701@ozemail.com.au>

Does save.image do what you want?

Gamal Azim wrote:
> Forgot to indicate that the remote system is Linux,
> accessed remotely by ssh. 
>
> --- Gamal Azim <ageneticist at yahoo.com> wrote:
>
>   
>> Is it possible to remotely save an R session then
>> terminate R? Of course the destructive task after
>> 'then' is rather straightforward by itself.
>>
>> Thanks
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained,
>> reproducible code.
>>
>>     
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>   


-- 
Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat. 
Centre for Resource and Environmental Studies
The Australian National University              
Canberra ACT 0200                               
Australia                                       
T: +61 2 6125 7800 email: Simon.Blomberg_at_anu.edu.au
F: +61 2 6125 0757
CRICOS Provider # 00120C

The combination of some data and an aching desire for 
an answer does not ensure that a reasonable answer 
can be extracted from a given body of data.
- John Tukey.


From jeff.horner at vanderbilt.edu  Thu Sep 21 06:00:54 2006
From: jeff.horner at vanderbilt.edu (Jeffrey Horner)
Date: Wed, 20 Sep 2006 23:00:54 -0500
Subject: [R] remotely saving an R session
In-Reply-To: <20060921013237.60168.qmail@web33402.mail.mud.yahoo.com>
References: <20060921013237.60168.qmail@web33402.mail.mud.yahoo.com>
Message-ID: <45120E76.9060002@vanderbilt.edu>

Gamal Azim wrote:
> Is it possible to remotely save an R session then
> terminate R? Of course the destructive task after
> 'then' is rather straightforward by itself.

Yes. Sending the process a SIGUSR1 signal:

$ kill -USR1 pid

will save the global environment in the file .RData, but you'll need to 
remember the current working directory of the process to find it.

Good luck!

Jeff
-- 
http://biostat.mc.vanderbilt.edu/JeffreyHorner


From H.Koffijberg at umcutrecht.nl  Thu Sep 21 08:32:52 2006
From: H.Koffijberg at umcutrecht.nl (Koffijberg, H.)
Date: Thu, 21 Sep 2006 08:32:52 +0200
Subject: [R] Any examples of a frailty model actually used for prediction ?
Message-ID: <07782D954353A543BE9291EC2155C07702BF3ECB@EX01.ds.umcutrecht.nl>


Hi everyone,

I'm looking for any examples of useful frailty models, in particular any situation in which a cox proportional hazards model with frailty outperforms a regular cox proportional hazards model with respect to prediction of the time to event (or the X-year risk of an event). I have defined my own gamma-frailty cox PH model in R but on my simulated data sample it does not predict any better than a regular cox model. In fact the predictions of the frailty model are often worse, *even* when I purposely add gamma-distributed frailty to the simulated sample (as I have previously posted). Can anyone help me, e.g. by pointing me to some R code in which the performance of models with and without frailty component are assessed ? Also any examples in which the actual estimated individual frailty values are used for predictions are very much welcome.

Thanks.
Erik Koffijberg.

=====================================================
Julius Center for Health Sciences and Primary Care
University Medical Center Utrecht Str. 6.131
P.O.Box 85500, 3508 GA Utrecht, The Netherlands
Tel:	+31 30 250 3013,
Fax: 	+31 30 250 5480
E-mail: H.Koffijberg at umcutrecht.nl


From wangtong at usc.edu  Thu Sep 21 08:43:42 2006
From: wangtong at usc.edu (Tong Wang)
Date: Wed, 20 Sep 2006 23:43:42 -0700
Subject: [R] How to generating diagnal blocks ?
Message-ID: <de069bb710aed.4511d22e@usc.edu>

Hi,
     I am trying to creat a matrix with diagnal blocks, say, I have a matrix X of any dimension (nxm) ,and would like to have:
                                                   
                                                            X
                                                                X
                                                                    X
                                                                      .....
otherwise space filled with 0's.   Is there a handy way to do this ?

Thanks a lot in advance.

best


From MUEHGE at de.ibm.com  Thu Sep 21 08:36:32 2006
From: MUEHGE at de.ibm.com (Thorsten Muehge)
Date: Thu, 21 Sep 2006 08:36:32 +0200
Subject: [R] Help on Dates in R again
Message-ID: <OF37248D72.3CC3E098-ONC12571F0.0023CCF9-C12571F0.00244E18@de.ibm.com>


Hello R Experts,
I want to aggregate parameters by week. But our production week ends Friday
night instead of Sunday Night which is the default value in R.

In order to solve the problem I want to substract two days from the current
data and than use the R function

test$week<-format(test$dates,"%U");

with a test&dates format equal to "2006-09-21".

How do I substract the two days from the test$dates column in the
data.frame?

Thanks a lot for your help
Thorsten


From pratap_stat at yahoo.co.in  Thu Sep 21 00:09:35 2006
From: pratap_stat at yahoo.co.in (nalluri pratap)
Date: Wed, 20 Sep 2006 23:09:35 +0100 (BST)
Subject: [R] Pooled Covariance Matrix
In-Reply-To: <4511AF02.2070502@waikato.ac.nz>
Message-ID: <20060920220935.43319.qmail@web8602.mail.in.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060920/530545b2/attachment.pl 

From FredeA.Togersen at agrsci.dk  Thu Sep 21 09:02:03 2006
From: FredeA.Togersen at agrsci.dk (=?iso-8859-1?Q?Frede_Aakmann_T=F8gersen?=)
Date: Thu, 21 Sep 2006 09:02:03 +0200
Subject: [R] Different result from nls in R-2.2.1 and R-2.3.1
In-Reply-To: <66d34c7a0608261507t27709825g214b42ff46e200bb@mail.gmail.com>
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC047E4C70@DJFPOST01.djf.agrsci.dk>



Short story: January 2006 I did some analysis in R-2.2.1 using nls. Repeating the exercise in R-2.3.1 yesterday produced somewhat different results. 

After some debugging I found that either nls is the problem or that mine understanding of environments or scoping rules is lacking something.

This is a short reproducing example.



x <- seq(0,5,len=20)

n <- 1
y <- 2*x^2 + n + rnorm(x)

xy <- data.frame(x=x,y=y)

myf <- function(x,a,b,n){
  res <- a*x^b + n
  ## a print for debugging purpose
  print(n)
  res
}

## This works as I expect it to do in R-2.2.1 but doesn't work in R-2.3.1. 
## n is somehow sat to nrow(xy) inside nls()
## Note that x and y is defined in the dataframe xy, whereas n is found in the global environment.
fit <- nls(y ~ myf(x,a,b,n), data=xy, start=c(a=1,b=1), trace=TRUE)

## this works in both versions
## x,y,n found in the .GlobalEnv
fit <- nls(y ~ myf(x,a,b,n), start=c(a=1,b=1), trace=TRUE)

## this works in both versions.
## x, y, n found in dataframe xyn
xyn <- data.frame(xy,n=n)
fit <- nls(y ~ myf(x,a,b,n), data=xyn, start=c(a=1,b=1), trace=TRUE)

## this works in both versions
## Now using the variable .n instead of n
## .n is found in .GlobaEnv
.n <- 1
fit <- nls(y ~ myf(x,a,b,.n), data=xy, start=c(a=1,b=1), trace=TRUE)


In my real case and the example above, I do have three or more parameters of which fitting is done only on few of theme. Is this a problem? Or should I ask, why is this a problem in R-2.3.1 but not in R-2.2.1?

Is my problem related to this difference between lines of code from nls:

R-2.2.1:     mf <- as.list(eval(mf, parent.frame()))

R-2.3.1:     mf <- eval.parent(mf)
             n <- nrow(mf)
             mf <- as.list(mf)

where n is being defined in the scope of nls in the latest version?

Best regards

Frede Aakmann T?gersen



Danish Institute of Agricultural Sciences
Research Centre Foulum
Dept. of Genetics and Biotechnology
Blichers All? 20, P.O. BOX 50
DK-8830 Tjele

Phone:   +45 8999 1900
Direct:  +45 8999 1878

E-mail:  FredeA.Togersen at agrsci.dk
Web:	   http://www.agrsci.org				

This email may contain information that is confidential.
Any use or publication of this email without written permission from DIAS is not allowed.
If you are not the intended recipient, please notify DIAS immediately and delete this email.


From ripley at stats.ox.ac.uk  Thu Sep 21 09:13:09 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 21 Sep 2006 08:13:09 +0100 (BST)
Subject: [R] Help on Dates in R again
In-Reply-To: <OF37248D72.3CC3E098-ONC12571F0.0023CCF9-C12571F0.00244E18@de.ibm.com>
References: <OF37248D72.3CC3E098-ONC12571F0.0023CCF9-C12571F0.00244E18@de.ibm.com>
Message-ID: <Pine.LNX.4.64.0609210757390.13391@gannet.stats.ox.ac.uk>

On Thu, 21 Sep 2006, Thorsten Muehge wrote:

>
> Hello R Experts,
> I want to aggregate parameters by week. But our production week ends Friday
> night instead of Sunday Night which is the default value in R.

The default in ISO8601, not just in R, but that is %W, not %U as used 
below.

> In order to solve the problem I want to substract two days from the current
> data and than use the R function
>
> test$week<-format(test$dates,"%U");
>
> with a test&dates format equal to "2006-09-21".
>
> How do I substract the two days from the test$dates column in the
> data.frame?

You have not told us what class test$dates is!  Assuming it is "Date",
test$dates-2.

*However*, to do what you ask, you need to add 1:

> dates <- seq(as.Date("2006-09-21"), by=1, len=7)
> format(dates+1, "%U")
[1] "38" "38" "39" "39" "39" "39" "39"

There is a potential problem here at year ends (there is anyway in the 
ISO8601 definition).  Another way is just

(unclass(dates) - 2) %/% 7

which orders weeks across years.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From vmuggeo at dssm.unipa.it  Thu Sep 21 09:14:35 2006
From: vmuggeo at dssm.unipa.it (vito muggeo)
Date: Thu, 21 Sep 2006 09:14:35 +0200
Subject: [R] How to generating diagnal blocks ?
In-Reply-To: <de069bb710aed.4511d22e@usc.edu>
References: <de069bb710aed.4511d22e@usc.edu>
Message-ID: <45123BDB.60305@dssm.unipa.it>

If I remember well, there should be a package including the function 
bdiag() making the job..but I am not able to remember its name..

However a quick search via RSiteSearch("bdiag") yields

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/40393.html

Hope this helps you,

vito


Tong Wang wrote:
> Hi,
>      I am trying to creat a matrix with diagnal blocks, say, I have a matrix X of any dimension (nxm) ,and would like to have:
>                                                    
>                                                             X
>                                                                 X
>                                                                     X
>                                                                       .....
> otherwise space filled with 0's.   Is there a handy way to do this ?
> 
> Thanks a lot in advance.
> 
> best
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
====================================
Vito M.R. Muggeo
Dip.to Sc Statist e Matem `Vianelli'
Universit? di Palermo
viale delle Scienze, edificio 13
90128 Palermo - ITALY
tel: 091 6626240
fax: 091 485726/485612


From maechler at stat.math.ethz.ch  Thu Sep 21 09:16:21 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 21 Sep 2006 09:16:21 +0200
Subject: [R] Statitics Textbook - any recommendation?
In-Reply-To: <1158793697.2838.1.camel@dhcppc2.my.nat.localnet>
References: <03b301c6dd07$f7487bb0$6400a8c0@DD4XFW31>
	<1158793697.2838.1.camel@dhcppc2.my.nat.localnet>
Message-ID: <17682.15429.527518.809809@stat.math.ethz.ch>

>>>>> "GS" == Gavin Simpson <gavin.simpson at ucl.ac.uk>
>>>>>     on Thu, 21 Sep 2006 00:08:17 +0100 writes:

    GS> On Wed, 2006-09-20 at 18:56 -0400, Charles Annis, P.E. wrote:
    >> Recommending a good book on statistics is like recommending a good book on
    >> sports:  Which sports?
    >> 
    >> A good book for learning statistical concepts (and learning R at the same
    >> time), one that assumes you understand algebra but are new to statistics, is
    >> Peter Dalgaard's _Introductory Statistics with R_ (Springer 2002).  The
    >> writing is relaxed and succinct, not condescending as some texts might
    >> appear to a newcomer.  It's just a good book.

    GS> I couldn't agree more. A number of my colleagues have bought Peter
    GS> Dalgaard's book to a) learn some R and b) learn some statistics. They
    GS> have found it very useful indeed.

Yes!
I'm pretty sure it has been the first book of its kind ("Intro Stats + R"), 
and in my view is still the best.

Martin Maechler, ETH Zurich


    >> 
    >> Charles Annis, P.E.
    >> 
    >> Charles.Annis at StatisticalEngineering.com
    >> phone: 561-352-9699
    >> eFax:  614-455-3265
    >> http://www.StatisticalEngineering.com
    >> 
    >> 
    >> -----Original Message-----
    >> From: r-help-bounces at stat.math.ethz.ch
    >> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Mike Nielsen
    >> Sent: Wednesday, September 20, 2006 6:36 PM
    >> To: Berton Gunter
    >> Cc: r-help at stat.math.ethz.ch
    >> Subject: Re: [R] Statitics Textbook - any recommendation?
    >> 
    >> Excellent characterization.
    >> 
    >> MASS is a very good book, but I'm not sure I would describe it as a
    >> statistics textbook, much less one of the "basic" variety.  While I
    >> certainly wouldn't presume to speak for Prof. Ripley and Dr. Venables,
    >> it seems unlikely their intent in writing MASS was to teach
    >> statistics, but rather, as the name of the book might suggest, to
    >> explain how S+ (and R) can be applied to modern statistical
    >> techniques.  My experience with this book is that it assumes
    >> considerable background knowledge.
    >> 
    >> By all means, buy MASS, but if you need guidance on the how and why of
    >> statistical techniques, you may wish to shop Amazon to find a
    >> supplement.
    >> 
    >> Regards,
    >> 
    >> Mike
    >> 
    >> On 9/20/06, Berton Gunter <gunter.berton at gene.com> wrote:
    >> > Not withstanding Prof. Heiberger's admirable enthusiasm, I think the
    >> > canonical answer is probably MASS (Modern Applied Statistics with S) by
    >> > Venables and Ripley. It is very comprehensive, but depending on your
    >> > background, you may find it too telegraphic.
    >> >
    >> > -- Bert Gunter
    >> > Genentech Non-Clinical Statistics
    >> > South San Francisco, CA
    >> >
    >> > "The business of the statistician is to catalyze the scientific learning
    >> > process."  - George E. P. Box
    >> >
    >> >
    >> >
    >> > > -----Original Message-----
    >> > > From: r-help-bounces at stat.math.ethz.ch
    >> > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Iuri Gavronski
    >> > > Sent: Wednesday, September 20, 2006 1:22 PM
    >> > > To: r-help at stat.math.ethz.ch
    >> > > Subject: [R] Statitics Textbook - any recommendation?
    >> > >
    >> > > I would like to buy a basic statistics book (experimental design,
    >> > > sampling, ANOVA, regression, etc.) with examples in R. Or
    >> > > download it
    >> > > in PDF or html format.
    >> > > I went to the CRAN contributed documentation, but there were only R
    >> > > textbooks, that is, textbooks where R is the focus, not the
    >> > > statistics. And I would like to find the opposite.
    >> > > Other text I am trying to find is multivariate data analysis (EFA,
    >> > > cluster, mult regression, MANOVA, etc.) with examples with R.
    >> > > Any recommendation?
    >> > >
    >> > > Thank you in advance,
    >> > >
    >> > > Iuri.
    >> > >
    >> > > ______________________________________________
    >> > > R-help at stat.math.ethz.ch mailing list
    >> > > https://stat.ethz.ch/mailman/listinfo/r-help
    >> > > PLEASE do read the posting guide
    >> > > http://www.R-project.org/posting-guide.html
    >> > > and provide commented, minimal, self-contained, reproducible code.
    >> > >
    >> >
    >> > ______________________________________________
    >> > R-help at stat.math.ethz.ch mailing list
    >> > https://stat.ethz.ch/mailman/listinfo/r-help
    >> > PLEASE do read the posting guide
    >> http://www.R-project.org/posting-guide.html
    >> > and provide commented, minimal, self-contained, reproducible code.
    >> >
    >> 
    >> 
    >> 
    >> -- 
    >> Regards,
    >> 
    >> Mike Nielsen
    >> 
    >> ______________________________________________
    >> R-help at stat.math.ethz.ch mailing list
    >> https://stat.ethz.ch/mailman/listinfo/r-help
    >> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    >> and provide commented, minimal, self-contained, reproducible code.
    >> 
    >> ______________________________________________
    >> R-help at stat.math.ethz.ch mailing list
    >> https://stat.ethz.ch/mailman/listinfo/r-help
    >> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    >> and provide commented, minimal, self-contained, reproducible code.
    GS> -- 
    GS> %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
    GS> *Note new Address and Fax and Telephone numbers from 10th April 2006*
    GS> %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
    GS> Gavin Simpson                     [t] +44 (0)20 7679 0522
    GS> ECRC                              [f] +44 (0)20 7679 0565
    GS> UCL Department of Geography
    GS> Pearson Building                  [e] gavin.simpsonATNOSPAMucl.ac.uk
    GS> Gower Street
    GS> London, UK                        [w] http://www.ucl.ac.uk/~ucfagls/cv/
    GS> WC1E 6BT                          [w] http://www.ucl.ac.uk/~ucfagls/
    GS> %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%

    GS> ______________________________________________
    GS> R-help at stat.math.ethz.ch mailing list
    GS> https://stat.ethz.ch/mailman/listinfo/r-help
    GS> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    GS> and provide commented, minimal, self-contained, reproducible code.


From p.dalgaard at biostat.ku.dk  Thu Sep 21 09:25:58 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 21 Sep 2006 09:25:58 +0200
Subject: [R] Statitics Textbook - any recommendation?
In-Reply-To: <17682.15429.527518.809809@stat.math.ethz.ch>
References: <03b301c6dd07$f7487bb0$6400a8c0@DD4XFW31>
	<1158793697.2838.1.camel@dhcppc2.my.nat.localnet>
	<17682.15429.527518.809809@stat.math.ethz.ch>
Message-ID: <x2mz8tbrzt.fsf@turmalin.kubism.ku.dk>

Martin Maechler <maechler at stat.math.ethz.ch> writes:

>     GS> I couldn't agree more. A number of my colleagues have bought Peter
>     GS> Dalgaard's book to a) learn some R and b) learn some statistics. They
>     GS> have found it very useful indeed.
> 
> Yes!
> I'm pretty sure it has been the first book of its kind ("Intro Stats + R"), 
> and in my view is still the best.

Thanks for the kind words, but...

>     >> > > I went to the CRAN contributed documentation, but there
>     >> > > were only R textbooks, that is, textbooks where R is the
>     >> > > focus, not the statistics. And I would like to find the
>     >> > > opposite.

and that does to some extent apply to my book too. Perhaps have a look
at Maindonald & Braun as well.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From r.hankin at noc.soton.ac.uk  Thu Sep 21 09:28:18 2006
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Thu, 21 Sep 2006 08:28:18 +0100
Subject: [R] How to generating diagnal blocks ?
In-Reply-To: <45123BDB.60305@dssm.unipa.it>
References: <de069bb710aed.4511d22e@usc.edu> <45123BDB.60305@dssm.unipa.it>
Message-ID: <0658C263-3483-4976-A567-0EB84C7219B8@soc.soton.ac.uk>

Hello

You need adiag(), which
is now part of the magic package:


 > library(magic)
 > adiag(matrix(1:6,2,3),matrix(1:9,3,3))
      [,1] [,2] [,3] [,4] [,5] [,6]
[1,]    1    3    5    0    0    0
[2,]    2    4    6    0    0    0
[3,]    0    0    0    1    4    7
[4,]    0    0    0    2    5    8
[5,]    0    0    0    3    6    9
 >


the function also has extra functionality over the one Vito
points to in the archives.

HTH

rksh


On 21 Sep 2006, at 08:14, vito muggeo wrote:

> If I remember well, there should be a package including the function
> bdiag() making the job..but I am not able to remember its name..
>
> However a quick search via RSiteSearch("bdiag") yields
>
> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/40393.html
>
> Hope this helps you,
>
> vito
>
>
> Tong Wang wrote:
>> Hi,
>>      I am trying to creat a matrix with diagnal blocks, say, I  
>> have a matrix X of any dimension (nxm) ,and would like to have:
>>
>>                                                             X
>>                                                                 X
>>                                                                     X
>>                                                                       
>>  .....
>> otherwise space filled with 0's.   Is there a handy way to do this ?
>>
>> Thanks a lot in advance.
>>
>> best
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting- 
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>
> -- 
> ====================================
> Vito M.R. Muggeo
> Dip.to Sc Statist e Matem `Vianelli'
> Universit? di Palermo
> viale delle Scienze, edificio 13
> 90128 Palermo - ITALY
> tel: 091 6626240
> fax: 091 485726/485612
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From seanpor at acm.org  Thu Sep 21 09:32:15 2006
From: seanpor at acm.org (Sean O'Riordain)
Date: Thu, 21 Sep 2006 07:32:15 +0000
Subject: [R] uniform integer RNG 0 to t inclusive
In-Reply-To: <Pine.LNX.4.64.0609190929490.7969@gannet.stats.ox.ac.uk>
References: <8ed68eed0609180037y175ea091p4927ab87f7f8ccfb@mail.gmail.com>
	<450F059F.4000203@stats.uwo.ca>
	<8ed68eed0609190103t5abbb8bjbf76f4d582ebc215@mail.gmail.com>
	<Pine.LNX.4.64.0609190929490.7969@gannet.stats.ox.ac.uk>
Message-ID: <8ed68eed0609210032j460ebe72rcc0dce3dcfcf79c5@mail.gmail.com>

Prof Ripley,
You are absolutely correct, this code will not work at all - for
starters, M isn't correctly initialized, etc.  I edited my code in
tinn-r and never ran it before posting... my apologies, I always seem
to be too quick off the mark to reply - despite the 4*runif(1)
suggestion in the posting guide...

I hadn't realized before what significant difference the replace=TRUE
would make to the runtime... Now I can just use the sample() code you
suggested and remove my runif() code altogether, as  sample(t+1, 1,
replace=TRUE) - 1 will work fine with t <- 2e9 which is considerably
more that I need.

Thanks again to both Prof Ripley and Duncan Murdoch,
Sean O'Riordain
affiliation <- NULL


On 19/09/06, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> On Tue, 19 Sep 2006, Sean O'Riordain wrote:
>
> > Hi Duncan,
> >
> > Thanks for that.  In the light of what you've suggested, I'm now using
> > the following:
> >
> >  # generate a random integer from 0 to t (inclusive)
> >  if (t < 10000000) { # to avoid memory problems...
> >    M <- sample(t, 1)
> >  } else {
> >    while (M > t) {
> >      M <- as.integer(urand(1,min=0, max=t+1-.Machine$double.eps))
> >    }
> >  }
>
> sample(t, 1) is a sample from 1:t, not 0:t.
>
> You need
>
> sample(t+1, 1, replace=TRUE) - 1
>
> which works in all cases up to INT_MAX-1, and beyond that you need to
> worry about the resolution of the RNG (and to use floor not as.integer).
>
> There is no such thing as urand in base R ....
>
> >
> > cheers and Thanks,
> > Sean
> >
> > On 18/09/06, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> >> On 9/18/2006 3:37 AM, Sean O'Riordain wrote:
> >>> Good morning,
> >>>
> >>> I'm trying to concisely generate a single integer from 0 to n
> >>> inclusive, where n might be of the order of hundreds of millions.
> >>> This will however be used many times during the general procedure, so
> >>> it must be "reasonably efficient" in both memory and time... (at some
> >>> later stage in the development I hope to go vectorized)
> >>>
> >>> The examples I've found through searching RSiteSearch() relating to
> >>> generating random integers say to use : sample(0:n, 1)
> >>> However, when n is "large" this first generates a large sequence 0:n
> >>> before taking a sample of one... this computer doesn't have the memory
> >>> for that!
> >>
> >> You don't need to give the whole vector:  just give n, and you'll get
> >> draws from 1:n.  The man page is clear on this.
> >>
> >> So what you want is sample(n+1, 1) - 1.  (Use "replace=TRUE" if you want
> >> a sample bigger than 1, or you'll get sampling without replacement.)
> >>>
> >>> When I look at the documentation for runif(n, min, max) it states that
> >>> the generated numbers will be min <= x <= max.  Note the "<= max"...
> >>
> >> Actually it says that's the range for the uniform density.  It's silent
> >> on the range of the output.  But it's good defensive programming to
> >> assume that it's possible to get the endpoints.
> >>
> >>>
> >>> How do I generate an x such that the probability of being (the
> >>> integer) max is the same as any other integer from min (an integer) to
> >>> max-1 (an integer) inclusive... My attempt is:
> >>>
> >>> urand.int <- function(n,t) {
> >>>   as.integer(runif(n,min=0, max=t+1-.Machine$double.eps))
> >>> }
> >>> # where I've included the parameter n to help testing...
> >>
> >> Because of rounding error, t+1-.Machine$double.eps might be exactly
> >> equal to t+1.  I'd suggest using a rejection method if you need to use
> >> this approach:  but sample() is better in the cases where as.integer()
> >> will work.
> >>
> >> Duncan Murdoch
> >>>
> >>> is floor() "better" than as.integer?
> >>>
> >>> Is this correct?  Is the probability of the integer t the same as the
> >>> integer 1 or 0 etc... I have done some rudimentary testing and this
> >>> appears to work, but power being what it is, I can't see how to
> >>> realistically test this hypothesis.
> >>>
> >>> Or is there a a better way of doing this?
> >>>
> >>> I'm trying to implement an algorithm which samples into an array,
> >>> hence the need for an integer - and yes I know about sample() thanks!
> >>> :-)
> >>>
> >>> { incidentally, I was surprised to note that the maximum value
> >>> returned by summary(integer_vector) is "pretty" and appears to be
> >>> rounded up to a "nice round number", and is not necessarily the same
> >>> as max(integer_vector) where the value is large, i.e. of the order of
> >>> say 50 million }
> >>>
> >>> Is version etc relevant? (I'll want to be portable)
> >>>> version               _
> >>> platform       i386-pc-mingw32
> >>> arch           i386
> >>> os             mingw32
> >>> system         i386, mingw32
> >>> status
> >>> major          2
> >>> minor          3.1
> >>> year           2006
> >>> month          06
> >>> day            01
> >>> svn rev        38247
> >>> language       R
> >>> version.string Version 2.3.1 (2006-06-01)
> >>>
> >>> Many thanks in advance for your help.
> >>> Sean O'Riordain
> >>> affiliation <- NULL
> >>>
> >>> ______________________________________________
> >>> R-help at stat.math.ethz.ch mailing list
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>
> >>
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>


From petr.pikal at precheza.cz  Thu Sep 21 09:42:21 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 21 Sep 2006 09:42:21 +0200
Subject: [R] Spliting a huge vector
In-Reply-To: <20060920211904.25692.qmail@web58310.mail.re3.yahoo.com>
Message-ID: <45125E7D.16006.4B6777@localhost>

Hi

On 20 Sep 2006 at 14:19, Dave Evens wrote:

Date sent:      	Wed, 20 Sep 2006 14:19:04 -0700 (PDT)
From:           	Dave Evens <devens8765 at yahoo.com>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] Spliting a huge vector

> 
> Dear R users,
> 
> I have a huge vector that I would like to split into
> unequal slices. However, the only way I can do this is
> to create another huge vector to define the groups
> that are used to split the original vector, e.g.
> 
> # my vector is this
> a.vector <- seq(2, by=5, length=100)

should not it be

a.vector <- seq(2, by=5, length=803)

> 
> # indices where I would like to slice my vector
> cut.values <- c(30, 50, 100, 109, 300, 601, 803)
> 
> # so I have to create another vector of similar length
> # to use the split() command, i.e.
> x <- rep(1:length(cut.values), times=diff(c(0,
> cut.values))

here it throws syntactic error so I assume it shall have one more 
parentheses

> 
> # this means I can use split()
> split(a.vector, x)
then 
times <- diff(c(0,cut.values))

do.call(function(x, y, times) split(x,rep(y, times=times)), 
list(x=a.vector, y= cut.values, times=times))

or

split(a.vector,rep(1:length(cut.values), times=times))

is this what you want? However I am not sure that some vector is not 
created internally.

HTH
Petr


> 
> This seems to be a waste in terms of memory usage as
> I'm creating another vector (here "x") to split the
> original vector. Is there a better way to split a huge
> vector than this? Any help is much appreciated. 
> 
> Best,
> Dave.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From rkrug at sun.ac.za  Thu Sep 21 10:14:03 2006
From: rkrug at sun.ac.za (Rainer M Krug)
Date: Thu, 21 Sep 2006 10:14:03 +0200
Subject: [R] Beginners manual for emacs and ess
In-Reply-To: <1158765683.20600.49.camel@gsimpson.geog.ucl.ac.uk>
References: <4511585C.7070307@sun.ac.za>
	<1158765683.20600.49.camel@gsimpson.geog.ucl.ac.uk>
Message-ID: <451249CB.2090309@sun.ac.za>

Thanks for all of your information.

I will look into all of them

Rainer


Gavin Simpson wrote:
> On Wed, 2006-09-20 at 17:03 +0200, Rainer M Krug wrote:
>> Hi
>>
>> I heard so much about Emacs and ESS that I decided to try it out - but I 
>>   am stuck at the beginning.
>>
>> Is there anywhere a beginners manual for Emacs & ESS to be used with R? 
>> even M-x S tells me it can't start S-Plus - obviously - but I want it to 
>> start R...
> 
> Er, M-x R starts R (after you've chosen a working directory) unless I
> misunderstood your question? Have you tried:
> 
> http://ess.r-project.org/
> 
> And the ESS manual contained there? If not, that would be my first port
> of call.
> 
>> Any help welcome (otherwise I will be stuck with Eclipse and R)
>>
>> Rainer
> 
> G
> 

-- 
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
Biology (UCT)

Department of Conservation Ecology and Entomology
University of Stellenbosch
Matieland 7602
South Africa

Tel:		+27 - (0)72 808 2975 (w)
Fax:		+27 - (0)21 808 3304
Cell:		+27 - (0)83 9479 042

email:	RKrug at sun.ac.za
       	Rainer at krugs.de


From rdiaz at cnio.es  Thu Sep 21 10:15:32 2006
From: rdiaz at cnio.es (Ramon Diaz-Uriarte)
Date: Thu, 21 Sep 2006 10:15:32 +0200
Subject: [R] Beginners manual for emacs and ess
In-Reply-To: <1158765416.3872.28.camel@localhost.localdomain>
References: <4511585C.7070307@sun.ac.za>
	<1158765416.3872.28.camel@localhost.localdomain>
Message-ID: <200609211015.32687.rdiaz@cnio.es>

On Wednesday 20 September 2006 17:16, Marc Schwartz (via MN) wrote:
> On Wed, 2006-09-20 at 17:03 +0200, Rainer M Krug wrote:
> > Hi
> >
> > I heard so much about Emacs and ESS that I decided to try it out - but I
> >   am stuck at the beginning.
> >
> > Is there anywhere a beginners manual for Emacs & ESS to be used with R?
> > even M-x S tells me it can't start S-Plus - obviously - but I want it to
> > start R...
> >

While following Mark's suggestions, try doing 

M-x R

and that might start R.

Then, you can do:

C-x 2 ("split the screen" as is said in other editors)

move to the one without the running R, and open there an R file (or you can 
just create it on the fly:

C-x C-f 

and in the minibuffer type anything, e.g., "one-file.R"   (without the 
quotes).



Then, type

C-h m

and you'll get a list of stuff related to the ESS mode.



And I think you will then really need to look at the ESS doc and go through 
the (X)Emacs tutorial (which is available from the help, in (X)Emacs).


HTH,


R.




> > Any help welcome (otherwise I will be stuck with Eclipse and R)
> >
> > Rainer
>
> There are some reference materials on the main ESS site at:
>
>   http://ess.r-project.org/
>
> In addition, there is a dedicated ESS mailing list, with more info here:
>
>   https://stat.ethz.ch/mailman/listinfo/ess-help
>
> HTH,
>
> Marc Schwartz
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented, minimal,
> self-contained, reproducible code.

-- 
Ram?n D?az-Uriarte
Bioinformatics 
Centro Nacional de Investigaciones Oncol?gicas (CNIO)
(Spanish National Cancer Center)
Melchor Fern?ndez Almagro, 3
28029 Madrid (Spain)
Fax: +-34-91-224-6972
Phone: +-34-91-224-6900

http://ligarto.org/rdiaz
PGP KeyID: 0xE89B3462
(http://ligarto.org/rdiaz/0xE89B3462.asc)



**NOTA DE CONFIDENCIALIDAD** Este correo electr?nico, y en s...{{dropped}}


From rossiter at itc.nl  Thu Sep 21 10:17:30 2006
From: rossiter at itc.nl (D G Rossiter)
Date: Thu, 21 Sep 2006 13:47:30 +0530
Subject: [R] Significant integer digits in summary.default
Message-ID: <074AB617-01BA-4432-80DC-B90C5CD7ECCE@itc.nl>

On R 2.3.1 I was surprised to see the following results from  
summary.default():

R> x <- scan()
1: 12148 10426 10912  9116 13226 11663 11781 10680  8457 10788 12605  
10591 11040
14: 10815 12962 11644 10047 10478 10108 12353 11778 11092 11673  8758  
11145 11495
27:
Read 26 items
R> summary(x)
    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
    8460   10500   11100   11100   11800   13200
R> max(x)
[1] 13226

Clearly, these are being rounded; help(summary) shows the digits=  
option. If I now ask for all five digits (n.b. I have to know the  
magnitude of the numbers to know how many integer digits) I get:

R> summary(x, digits=5)
    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
    8457   10506   11066   11068   11752   13226

which now shows the actual maximum etc.

help(summary) shows options:  digits = max(3, getOption("digits")-3)
R> getOption("digits")
[1] 5

So in my case I get three digits... but I had thought this applied to  
decimal digits, not integer digits, and that rounding would only  
occur in decimal places, according to the digits option.

Question: is this behaviour of summary.default what is expected? If  
so, it seems non-intuitive to me.

Thank you,

D. G. Rossiter
Senior University Lecturer
Department of Earth Systems Analysis
International Institute for Geo-Information Science and Earth  
Observation (ITC)
Enschede, The Netherlands


From ripley at stats.ox.ac.uk  Thu Sep 21 10:24:30 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 21 Sep 2006 09:24:30 +0100 (BST)
Subject: [R] remotely saving an R session
In-Reply-To: <45120E76.9060002@vanderbilt.edu>
References: <20060921013237.60168.qmail@web33402.mail.mud.yahoo.com>
	<45120E76.9060002@vanderbilt.edu>
Message-ID: <Pine.LNX.4.64.0609210753350.13391@gannet.stats.ox.ac.uk>

On Wed, 20 Sep 2006, Jeffrey Horner wrote:

> Gamal Azim wrote:
>> Is it possible to remotely save an R session then
>> terminate R? Of course the destructive task after
>> 'then' is rather straightforward by itself.
>
> Yes. Sending the process a SIGUSR1 signal:
>
> $ kill -USR1 pid
>
> will save the global environment in the file .RData, but you'll need to
> remember the current working directory of the process to find it.

That is indeed what this is documented to do (and I mentioned in an 
R-devel posting yesterday).  Unfortunately it currently does not always 
work: try it on a session running

try(repeat{1})

to see the problem.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From rdiaz at cnio.es  Thu Sep 21 10:29:47 2006
From: rdiaz at cnio.es (Ramon Diaz-Uriarte)
Date: Thu, 21 Sep 2006 10:29:47 +0200
Subject: [R] Statitics Textbook - any recommendation?
In-Reply-To: <20060920172158.0i94pyn6so7qcksk@webmail.ufrgs.br>
References: <20060920172158.0i94pyn6so7qcksk@webmail.ufrgs.br>
Message-ID: <200609211029.47489.rdiaz@cnio.es>

On Wednesday 20 September 2006 22:21, Iuri Gavronski wrote:
> I would like to buy a basic statistics book (experimental design,
> sampling, ANOVA, regression, etc.) with examples in R. Or download it
> in PDF or html format.
> I went to the CRAN contributed documentation, but there were only R
> textbooks, that is, textbooks where R is the focus, not the
> statistics. And I would like to find the opposite.
> Other text I am trying to find is multivariate data analysis (EFA,
> cluster, mult regression, MANOVA, etc.) with examples with R.
> Any recommendation?


I'd say the situation is actually the opposite.  Anyway, the recent book by 
Brian Everitt and Torsten Hothorn (A handbook of statistical analyses using 
R. Chapman & Hall) is an excellent (and affordable) place to start. (I think 
that this book's context emphasizes that it is stats with R as the language: 
Everitt has (co)authored a bunch of others in other languages ---SAS, Stata, 
SPSS, etc).


Of course, there are many others that probably deserver a place on your (or 
your library's) shelves:

P. Dalgaard's
MASS
Maindonald & Braun
Heiberger & Holland
etc



HTH,


R.
>
> Thank you in advance,
>
> Iuri.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented, minimal,
> self-contained, reproducible code.

-- 
Ram?n D?az-Uriarte
Bioinformatics 
Centro Nacional de Investigaciones Oncol?gicas (CNIO)
(Spanish National Cancer Center)
Melchor Fern?ndez Almagro, 3
28029 Madrid (Spain)
Fax: +-34-91-224-6972
Phone: +-34-91-224-6900

http://ligarto.org/rdiaz
PGP KeyID: 0xE89B3462
(http://ligarto.org/rdiaz/0xE89B3462.asc)



**NOTA DE CONFIDENCIALIDAD** Este correo electr?nico, y en s...{{dropped}}


From Dietrich.Trenkler at Uni-Osnabrueck.de  Thu Sep 21 09:44:21 2006
From: Dietrich.Trenkler at Uni-Osnabrueck.de (Dietrich Trenkler)
Date: Thu, 21 Sep 2006 09:44:21 +0200
Subject: [R] Statitics Textbook - any recommendation?
Message-ID: <451242D5.9040509@uni-osnabrueck.de>

Hi all,

I am very fond of Using R for Introductory Statistics
by John Verzani, 2005, Chapman & Hall.

Regards

D. Trenkler

-- 
Dietrich Trenkler c/o Universitaet Osnabrueck 
Rolandstr. 8; D-49069 Osnabrueck, Germany    
email: Dietrich.Trenkler at Uni-Osnabrueck.de


From rkrug at sun.ac.za  Thu Sep 21 11:55:11 2006
From: rkrug at sun.ac.za (Rainer M Krug)
Date: Thu, 21 Sep 2006 11:55:11 +0200
Subject: [R] transforming factor back to numbers
Message-ID: <4512617F.9010605@sun.ac.za>

Hi

I generate a new dataframe by doing:

npl.agg <- aggregate(npl$DensPlants, list(year=npl$year, sim=npl$sim), 
mean, na.rm=TRUE )

Now I want to plot it by using

coplot(npl.agg$x ~ npl.agg$year | npl.agg$sim, type="l")

but, as npl.agg$year is seen as a factor, the order of the points on the 
x-axis (time axis) does not follow the numerical sorting 1...100, but 
rather the text sorting of the factor npl.agg$year

Is there any way that I can 'defactor' npl.agg$year so that I have again 
the numerical values for year?

Thanks

Rainer

-- 
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
Biology (UCT)

Department of Conservation Ecology and Entomology
University of Stellenbosch
Matieland 7602
South Africa

Tel:		+27 - (0)72 808 2975 (w)
Fax:		+27 - (0)21 808 3304
Cell:		+27 - (0)83 9479 042

email:	RKrug at sun.ac.za
       	Rainer at krugs.de


From mothsailor at googlemail.com  Thu Sep 21 11:54:23 2006
From: mothsailor at googlemail.com (David Barron)
Date: Thu, 21 Sep 2006 10:54:23 +0100
Subject: [R] transforming factor back to numbers
In-Reply-To: <4512617F.9010605@sun.ac.za>
References: <4512617F.9010605@sun.ac.za>
Message-ID: <815b70590609210254w4f1bda46sb4195035559c97a8@mail.gmail.com>

That's an FAQ:

7.10 How do I convert factors to numeric?

It may happen that when reading numeric data into R (usually, when
reading in a file), they come in as factors. If f is such a factor
object, you can use

     as.numeric(as.character(f))

to get the numbers back. More efficient, but harder to remember, is

     as.numeric(levels(f))[as.integer(f)]

In any case, do not call as.numeric() or their likes directly for the
task at hand (as as.numeric() or unclass() give the internal codes).

On 21/09/06, Rainer M Krug <rkrug at sun.ac.za> wrote:
> Hi
>
> I generate a new dataframe by doing:
>
> npl.agg <- aggregate(npl$DensPlants, list(year=npl$year, sim=npl$sim),
> mean, na.rm=TRUE )
>
> Now I want to plot it by using
>
> coplot(npl.agg$x ~ npl.agg$year | npl.agg$sim, type="l")
>
> but, as npl.agg$year is seen as a factor, the order of the points on the
> x-axis (time axis) does not follow the numerical sorting 1...100, but
> rather the text sorting of the factor npl.agg$year
>
> Is there any way that I can 'defactor' npl.agg$year so that I have again
> the numerical values for year?
>
> Thanks
>
> Rainer
>
> --
> Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
> Biology (UCT)
>
> Department of Conservation Ecology and Entomology
> University of Stellenbosch
> Matieland 7602
> South Africa
>
> Tel:            +27 - (0)72 808 2975 (w)
> Fax:            +27 - (0)21 808 3304
> Cell:           +27 - (0)83 9479 042
>
> email:  RKrug at sun.ac.za
>         Rainer at krugs.de
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From wl at eimb.ru  Thu Sep 21 11:54:47 2006
From: wl at eimb.ru (Vladimir Eremeev)
Date: Thu, 21 Sep 2006 13:54:47 +0400
Subject: [R] plot correlation matrix
Message-ID: <198879552.20060921135447@eimb.ru>

Dear useRs,

While exploring new R packages, I have found the Rattle.
This screenshot http://rattle.togaware.com/rattle-correlation.png
is very interesting
(others are in http://rattle.togaware.com/rattle-screenshots.html ).

Which function was used to produce this plot?
Is such plotting of the correlation matrix a unique Rattle's feature?

Unfortunately I have some weird problems with RGtk2 package and
therefore cannot run Rattle.

I have sent the question about this problem to the maintainer of the
RGtk2 a few hours ago.

When I try to load library(RGtk2) I get the message
"The procedure entry point atk_relation_add_target could not be located
in the dynamic link library libatk-1.0-0.dll".

---
Best regards,
Vladimir                mailto:wl at eimb.ru


From rkrug at sun.ac.za  Thu Sep 21 12:07:26 2006
From: rkrug at sun.ac.za (Rainer M Krug)
Date: Thu, 21 Sep 2006 12:07:26 +0200
Subject: [R] transforming factor back to numbers
In-Reply-To: <815b70590609210254w4f1bda46sb4195035559c97a8@mail.gmail.com>
References: <4512617F.9010605@sun.ac.za>
	<815b70590609210254w4f1bda46sb4195035559c97a8@mail.gmail.com>
Message-ID: <4512645E.4080502@sun.ac.za>

Thanks a lot David.

Just a suggestion - it might help if this is added to the help of factor()

Rainer

David Barron wrote:
> That's an FAQ:
> 
> 7.10 How do I convert factors to numeric?
> 
> It may happen that when reading numeric data into R (usually, when
> reading in a file), they come in as factors. If f is such a factor
> object, you can use
> 
>     as.numeric(as.character(f))
> 
> to get the numbers back. More efficient, but harder to remember, is
> 
>     as.numeric(levels(f))[as.integer(f)]
> 
> In any case, do not call as.numeric() or their likes directly for the
> task at hand (as as.numeric() or unclass() give the internal codes).
> 
> On 21/09/06, Rainer M Krug <rkrug at sun.ac.za> wrote:
>> Hi
>>
>> I generate a new dataframe by doing:
>>
>> npl.agg <- aggregate(npl$DensPlants, list(year=npl$year, sim=npl$sim),
>> mean, na.rm=TRUE )
>>
>> Now I want to plot it by using
>>
>> coplot(npl.agg$x ~ npl.agg$year | npl.agg$sim, type="l")
>>
>> but, as npl.agg$year is seen as a factor, the order of the points on the
>> x-axis (time axis) does not follow the numerical sorting 1...100, but
>> rather the text sorting of the factor npl.agg$year
>>
>> Is there any way that I can 'defactor' npl.agg$year so that I have again
>> the numerical values for year?
>>
>> Thanks
>>
>> Rainer
>>
>> -- 
>> Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
>> Biology (UCT)
>>
>> Department of Conservation Ecology and Entomology
>> University of Stellenbosch
>> Matieland 7602
>> South Africa
>>
>> Tel:            +27 - (0)72 808 2975 (w)
>> Fax:            +27 - (0)21 808 3304
>> Cell:           +27 - (0)83 9479 042
>>
>> email:  RKrug at sun.ac.za
>>         Rainer at krugs.de
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> 

-- 
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
Biology (UCT)

Department of Conservation Ecology and Entomology
University of Stellenbosch
Matieland 7602
South Africa

Tel:		+27 - (0)72 808 2975 (w)
Fax:		+27 - (0)21 808 3304
Cell:		+27 - (0)83 9479 042

email:	RKrug at sun.ac.za
       	Rainer at krugs.de


From B.Rowlingson at lancaster.ac.uk  Thu Sep 21 12:04:23 2006
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Thu, 21 Sep 2006 11:04:23 +0100
Subject: [R] remotely saving an R session
In-Reply-To: <45120E76.9060002@vanderbilt.edu>
References: <20060921013237.60168.qmail@web33402.mail.mud.yahoo.com>
	<45120E76.9060002@vanderbilt.edu>
Message-ID: <451263A7.4040201@lancaster.ac.uk>

Jeffrey Horner wrote:

> $ kill -USR1 pid
> 
> will save the global environment in the file .RData, but you'll need to 
> remember the current working directory of the process to find it.

  Remember? With a computer, you never need to remember!

  $ ls -l /proc/$pid/cwd

  is a symlink to the current working directory. Even if you change it 
with setwd() in R, it is still correct.

Barry


From syadp at yahoo.com.mx  Thu Sep 21 12:48:12 2006
From: syadp at yahoo.com.mx (Anaid Diaz)
Date: Thu, 21 Sep 2006 05:48:12 -0500 (CDT)
Subject: [R] survival function with a Weibull dist
Message-ID: <20060921104812.47030.qmail@web51913.mail.yahoo.com>

Hi
I am using R to fit a survival function to my data
(with a weibull distribution).

Data: Survival of individuals in relation to 4
treatments ('a','b','c','g') 

syntax:
----  > survreg(Surv(date2)~males2, dist='weibull')

But I have some problems interpreting the outcome and
getting the parameters for each curve.

---------              Value Std. Error      z       
p
---------  (Intercept)  2.788      0.147 19.022
1.13e-80
--------- males2b     -0.107      0.207 -0.519
6.04e-01
--------- males2c     -0.486      0.586 -0.831
4.06e-01
--------- males2g      0.580      0.207  2.798
5.15e-03
--------- Log(scale)  -1.116      0.139 -8.007
1.18e-15
--------- 
--------- Scale= 0.328 


I know from Venables & Ripley (2002) that the
parameters of this function should be two:

lambda = z (presumably "Value" in R for each
treatment)
alpha = k (scale in R)

Survival function (S):

S(t)= exp-(zt)^k

I don't quite understand how to use the intercept.
First I thought adding it to each other treatment
value, for example:

Survival Males2a (t) = exp ? ((intercept + 0) * t) ^ k
Survival Males2b (t) = exp ? ((intercept + zb)* t) ^ k

But the curves I get using this interpretation are not
similar to my data


Does anyone use this function and could help me to
interpret the results?

many thanks


Anaid


From stgries at arcor.de  Thu Sep 21 13:58:21 2006
From: stgries at arcor.de (stgries at arcor.de)
Date: Thu, 21 Sep 2006 13:58:21 +0200 (CEST)
Subject: [R] Command area in SciViews 0.8.9 - second try
Message-ID: <19387698.1158839901059.JavaMail.ngmail@webmail19>

Dear all

I am writing again with a question I posted a few weeks ago (to no avail). I have a problem with SciViews for R. It's probably a slightly stupid question but I cannot find a solution to a very elementary problem. I am using SciViews 0.8.9 on with R 2.3.1pat on a Windows XP Home machine. R is set to SDI mode, I start R, enter "library(svGUI)", SciViews starts properly, I can access the docks and everything so the installation is ok. But:

(i) the command area at the bottom cannot be found
(ii) the regular R window cannot be minimized/maximized anymore.

I don't know what to do to get the command area back. I have checked the web but all I could come up with (<http://tolstoy.newcastle.edu.au/R/help/05/12/16841.html>) is the recommendation to click on Misc: Toolbars: Command. But I did that and it's still not visible. Am I making some kind of stupid mistake? I have uploaded a screenshot to my website at <http://www.linguistics.ucsb.edu/faculty/stgries/other/sciviews.png> to show you what's happening. Any ideas would be greatly appreciated.

Thank you very much in advance for your time and all the best,
STG
-----------------------------------------------
University of California, Santa Barbara
http://www.linguistics.ucsb.edu/faculty/stgries


From lak15 at leicester.ac.uk  Thu Sep 21 14:07:43 2006
From: lak15 at leicester.ac.uk (Al-Kharusi, L.)
Date: Thu, 21 Sep 2006 13:07:43 +0100
Subject: [R] R data query
Message-ID: <93EC899E92A38749B4B93AC4319D25B60B945E9C@Saffron.cfs.le.ac.uk>

Dear Sir/Madam,


I am encountering one of those alien computer momements one finds every
so often in life. See the sequence below:

> fish3.fis <-read.csv("emperor2.csv", check.names = TRUE, strip.white =
TRUE)
> colnames(fish3.fis)
 [1] "Month"           "Year"            "FishingArea"
"SumOfTotalCatch" "CPUE"           
 [6] "rCPUE"           "PA"              "Latitude"        "Longitude"
"Depth"          
[11] "SST"            
> hist(CPUE)
Error in hist(CPUE) : Object "CPUE" not found

So, the system knows CPUE exists, but will not do a hist or a gam model
using the term - but for some strange reason it will do a plot. I have
created a completely different data file and that same problem is
happening. The imported files are exactly the same as I was using quite
happily last month. 

As you will see from the above I've tried adding check.names and
strip.white in the reading in process to avoid the unseen effect of
blank spaces.

Any ideas what I might do next? Have you come across this issue at all? 

Thanks


Lubna Al-Kharusi
PhD student (GIS)
Department of Geography
University of Leicester
University Road
Leicester
LE1 7RH
mobile # 07886990332


From rkrug at sun.ac.za  Thu Sep 21 14:12:49 2006
From: rkrug at sun.ac.za (Rainer M Krug)
Date: Thu, 21 Sep 2006 14:12:49 +0200
Subject: [R] Other editor with completion apart from emacs?
Message-ID: <451281C1.5020000@sun.ac.za>

Hi

is there any other editor for Linux, apart from emacs, which has the 
completion feature (i.e. I enter lib<TAB> and it offers me among other 
options "library")

Rainer

-- 
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
Biology (UCT)

Department of Conservation Ecology and Entomology
University of Stellenbosch
Matieland 7602
South Africa

Tel:		+27 - (0)72 808 2975 (w)
Fax:		+27 - (0)21 808 3304
Cell:		+27 - (0)83 9479 042

email:	RKrug at sun.ac.za
       	Rainer at krugs.de


From Roger.Bivand at nhh.no  Thu Sep 21 14:15:16 2006
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 21 Sep 2006 14:15:16 +0200 (CEST)
Subject: [R] R data query
In-Reply-To: <93EC899E92A38749B4B93AC4319D25B60B945E9C@Saffron.cfs.le.ac.uk>
Message-ID: <Pine.LNX.4.44.0609211412200.16156-100000@reclus.nhh.no>

On Thu, 21 Sep 2006, Al-Kharusi, L. wrote:

> Dear Sir/Madam,
> 
> 
> I am encountering one of those alien computer momements one finds every
> so often in life. See the sequence below:
> 
> > fish3.fis <-read.csv("emperor2.csv", check.names = TRUE, strip.white =
> TRUE)
> > colnames(fish3.fis)
>  [1] "Month"           "Year"            "FishingArea"
> "SumOfTotalCatch" "CPUE"           
>  [6] "rCPUE"           "PA"              "Latitude"        "Longitude"
> "Depth"          
> [11] "SST"            
> > hist(CPUE)
> Error in hist(CPUE) : Object "CPUE" not found

hist(fish3.fis$CPUE)

CPUE is inside the data.frame object and so cannot be "seen" in the 
workspace.

Reviewing:

http://cran.r-project.org/doc/manuals/R-intro.html#Data-frames

may help.

> 
> So, the system knows CPUE exists, but will not do a hist or a gam model
> using the term - but for some strange reason it will do a plot. I have
> created a completely different data file and that same problem is
> happening. The imported files are exactly the same as I was using quite
> happily last month. 
> 
> As you will see from the above I've tried adding check.names and
> strip.white in the reading in process to avoid the unseen effect of
> blank spaces.
> 
> Any ideas what I might do next? Have you come across this issue at all? 
> 
> Thanks
> 
> 
> Lubna Al-Kharusi
> PhD student (GIS)
> Department of Geography
> University of Leicester
> University Road
> Leicester
> LE1 7RH
> mobile # 07886990332
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From MSchwartz at mn.rr.com  Thu Sep 21 14:22:39 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Thu, 21 Sep 2006 07:22:39 -0500
Subject: [R] R data query
In-Reply-To: <93EC899E92A38749B4B93AC4319D25B60B945E9C@Saffron.cfs.le.ac.uk>
References: <93EC899E92A38749B4B93AC4319D25B60B945E9C@Saffron.cfs.le.ac.uk>
Message-ID: <1158841359.5693.30.camel@localhost.localdomain>

On Thu, 2006-09-21 at 13:07 +0100, Al-Kharusi, L. wrote:
> Dear Sir/Madam,
> 
> 
> I am encountering one of those alien computer momements one finds every
> so often in life. See the sequence below:
> 
> > fish3.fis <-read.csv("emperor2.csv", check.names = TRUE, strip.white =
> TRUE)
> > colnames(fish3.fis)
>  [1] "Month"           "Year"            "FishingArea"
> "SumOfTotalCatch" "CPUE"           
>  [6] "rCPUE"           "PA"              "Latitude"        "Longitude"
> "Depth"          
> [11] "SST"            
> > hist(CPUE)
> Error in hist(CPUE) : Object "CPUE" not found
> 
> So, the system knows CPUE exists, but will not do a hist or a gam model
> using the term - but for some strange reason it will do a plot. I have
> created a completely different data file and that same problem is
> happening. The imported files are exactly the same as I was using quite
> happily last month. 
> 
> As you will see from the above I've tried adding check.names and
> strip.white in the reading in process to avoid the unseen effect of
> blank spaces.
> 
> Any ideas what I might do next? Have you come across this issue at all? 
> 
> Thanks

CPUE is not found, as it is a column within the R data frame object
fish3.fis.

There are (at least) 5 ways to do a histogram on CPUE:

1.  attach() the data frame, which puts it in the search path and you
can then use the column name alone:

  attach(fish3.fis)
  hist(CPUE)
  detach(fish3.fis)

2. Identify the column using the '$' function:

  hist(fish3.fis$CPUE)

3. Use with() to evaluate the hist() call within the environment of the
data frame:

  with(fish3.fis, hist(CPUE))

4. Use the "[" function:

  hist(fish3.fis[, "CPUE"])

5. Use the "[[" function:

  hist(fish3.fis[["CPUE"]])


This is covered in "An Introduction to R".

HTH,

Marc Schwartz


From ronggui.huang at gmail.com  Thu Sep 21 14:29:57 2006
From: ronggui.huang at gmail.com (ronggui)
Date: Thu, 21 Sep 2006 20:29:57 +0800
Subject: [R] Other editor with completion apart from emacs?
In-Reply-To: <451281C1.5020000@sun.ac.za>
References: <451281C1.5020000@sun.ac.za>
Message-ID: <38b9f0350609210529l35760f05wd00f57fb24132920@mail.gmail.com>

JGR support completion. It can runs under Linux,though JGR is more
than a editor.

On 9/21/06, Rainer M Krug <rkrug at sun.ac.za> wrote:
> Hi
>
> is there any other editor for Linux, apart from emacs, which has the
> completion feature (i.e. I enter lib<TAB> and it offers me among other
> options "library")
>
> Rainer
>
> --
> Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
> Biology (UCT)
>
> Department of Conservation Ecology and Entomology
> University of Stellenbosch
> Matieland 7602
> South Africa
>
> Tel:            +27 - (0)72 808 2975 (w)
> Fax:            +27 - (0)21 808 3304
> Cell:           +27 - (0)83 9479 042
>
> email:  RKrug at sun.ac.za
>         Rainer at krugs.de
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
??????
Department of Sociology
Fudan University


From gavin.simpson at ucl.ac.uk  Thu Sep 21 14:33:58 2006
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Thu, 21 Sep 2006 13:33:58 +0100
Subject: [R] Command area in SciViews 0.8.9 - second try
In-Reply-To: <19387698.1158839901059.JavaMail.ngmail@webmail19>
References: <19387698.1158839901059.JavaMail.ngmail@webmail19>
Message-ID: <1158842038.29891.7.camel@gsimpson.geog.ucl.ac.uk>

On Thu, 2006-09-21 at 13:58 +0200, stgries at arcor.de wrote:
> Dear all
> 
> I am writing again with a question I posted a few weeks ago (to no
> avail).

STG

Perhaps the main reason you received no replies is that you failed to
read to the posting guide and act appropriately. Sciviews is *not* part
of R, it is a contributed package. As such you are asked to contact the
maintainer directly. Please do so.

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Sorry I can't be any more help - I have never used Sciviews.

G
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC & ENSIS, UCL Geography,  [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From j.m.harold at uea.ac.uk  Thu Sep 21 14:56:06 2006
From: j.m.harold at uea.ac.uk (Julie Harold)
Date: Thu, 21 Sep 2006 13:56:06 +0100
Subject: [R] newie help needed
Message-ID: <45128BE6.3050609@uea.ac.uk>

hi,

I am a system admin who has just set up R-2.3.1 in suse 9.1 (opteron)
for a previous windows user.  Please forgive any obvious stupid questions :)

We are struggling with a number of things.

1.  is there a gui interface ?  How do we use it ?  R --gui just gets us
to the command line.

2.  how do we do unix commands like cd ?

3.  can we define default directories - eg for data files ?

4.  we want to use this mainly in batch mode once we get our act
together - are there any good examples of batch scripts we can use as
crib sheets.

5.  I installed some packages earlier - but now I am obviously doing
something wrong
> install.packages(gnomeGui)
Error in install.packages(gnomeGui) : object "gnomeGui" not found


thanks,

Julie


-- 
Escience Support Team,  ITCS
Unix Support ENV (please contact envcs.unix at uea.ac.uk)


From danbebber at forestecology.co.uk  Thu Sep 21 15:02:30 2006
From: danbebber at forestecology.co.uk (Dan Bebber)
Date: Thu, 21 Sep 2006 14:02:30 +0100
Subject: [R] Statitics Textbook - any recommendation?
References: <mailman.9.1158832803.27275.r-help@stat.math.ethz.ch>
Message-ID: <003e01c6dd7e$3220dc70$d22401a3@plants.ox.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060921/7431c675/attachment.pl 

From p.dalgaard at biostat.ku.dk  Thu Sep 21 15:13:38 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 21 Sep 2006 15:13:38 +0200
Subject: [R] newie help needed
In-Reply-To: <45128BE6.3050609@uea.ac.uk>
References: <45128BE6.3050609@uea.ac.uk>
Message-ID: <x2zmct4b25.fsf@viggo.kubism.ku.dk>

Julie Harold <j.m.harold at uea.ac.uk> writes:

> hi,
> 
> I am a system admin who has just set up R-2.3.1 in suse 9.1 (opteron)
> for a previous windows user.  Please forgive any obvious stupid questions :)
> 
> We are struggling with a number of things.
> 
> 1.  is there a gui interface ?  How do we use it ?  R --gui just gets us
> to the command line.

Not really. The incantation is "R --gui=tk" or "gnome" or "X11"  but
the two former are quite sketchy (proof of concept, really). The last
one is the default, but not with menus etc., just windows for graphics.
 
> 2.  how do we do unix commands like cd ?

Any shell command can be executed via system(). This is in a subshell,
so of course cd has no effect on return: look up setwd()

> 3.  can we define default directories - eg for data files ?

Not in any "standard" way AFAIK. There is a number of ways you could
create your own convention (e.g. via environment variables), though.

> 4.  we want to use this mainly in batch mode once we get our act
> together - are there any good examples of batch scripts we can use as
> crib sheets.

Depends on what you want to do. There are quite a few documents around
with example code and several books have their scripts made public.

> 5.  I installed some packages earlier - but now I am obviously doing
> something wrong
> > install.packages(gnomeGui)
> Error in install.packages(gnomeGui) : object "gnomeGui" not found

Quotes needed.

In general, do your self a favour and read at least the Intro and
Admin manuals from.

http://cran.r-project.org/manuals.html

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From rkrug at sun.ac.za  Thu Sep 21 15:18:57 2006
From: rkrug at sun.ac.za (Rainer M Krug)
Date: Thu, 21 Sep 2006 15:18:57 +0200
Subject: [R] Other editor with completion apart from emacs?
In-Reply-To: <38b9f0350609210529l35760f05wd00f57fb24132920@mail.gmail.com>
References: <451281C1.5020000@sun.ac.za>
	<38b9f0350609210529l35760f05wd00f57fb24132920@mail.gmail.com>
Message-ID: <45129141.10903@sun.ac.za>

Hi

Thanks for the tip - sounds interesting.

I installed JGR, but I can't get it to work.

I installed it as mentioned on via install.packages() and if I try to
load it, I get the following:


> library(JGR)
Loading required package: rJava
Loading required package: JavaGD
Loading required package: iplots

 *** caught segfault ***
address 0xc, cause 'memory not mapped'

Traceback:
 1: .External("RinitJVM", classpath, parameters, PACKAGE = "rJava")
 2: .jinit(cp, parameters = "-Xmx512m", silent = TRUE)
 3: f(libname, pkgname)
 4: firstlib(which.lib.loc, package)
 5: try(firstlib(which.lib.loc, package))
 6: library(pkg, character.only = TRUE, logical = TRUE, lib.loc = lib.loc)
 7: .getRequiredPackages2(pkgInfo)
 8: library(JGR)

Possible actions:
1: abort (with core dump)
2: normal R exit
3: exit R without saving workspace
4: exit R saving workspace
Selection:


I updated the packages to the versions on http://www.rosuda.org/R , but
still the same.

Any ideas?

Rainer

ronggui wrote:
> JGR support completion. It can runs under Linux,though JGR is more
> than a editor.
> 
> On 9/21/06, Rainer M Krug <rkrug at sun.ac.za> wrote:
>> Hi
>>
>> is there any other editor for Linux, apart from emacs, which has the
>> completion feature (i.e. I enter lib<TAB> and it offers me among other
>> options "library")
>>
>> Rainer
>>
>> -- 
>> Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
>> Biology (UCT)
>>
>> Department of Conservation Ecology and Entomology
>> University of Stellenbosch
>> Matieland 7602
>> South Africa
>>
>> Tel:            +27 - (0)72 808 2975 (w)
>> Fax:            +27 - (0)21 808 3304
>> Cell:           +27 - (0)83 9479 042
>>
>> email:  RKrug at sun.ac.za
>>         Rainer at krugs.de
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> 

-- 
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
Biology (UCT)

Department of Conservation Ecology and Entomology
University of Stellenbosch
Matieland 7602
South Africa

Tel:		+27 - (0)72 808 2975 (w)
Fax:		+27 - (0)21 808 3304
Cell:		+27 - (0)83 9479 042

email:	RKrug at sun.ac.za
      	Rainer at krugs.de


From petr.pikal at precheza.cz  Thu Sep 21 15:22:55 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 21 Sep 2006 15:22:55 +0200
Subject: [R] newie help needed
In-Reply-To: <45128BE6.3050609@uea.ac.uk>
Message-ID: <4512AE4F.30086.1833D9B@localhost>

Hi

I am not familiar with linux version but

On 21 Sep 2006 at 13:56, Julie Harold wrote:

Date sent:      	Thu, 21 Sep 2006 13:56:06 +0100
From:           	Julie Harold <j.m.harold at uea.ac.uk>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] newie help needed

> hi,
> 
> I am a system admin who has just set up R-2.3.1 in suse 9.1 (opteron)
> for a previous windows user.  Please forgive any obvious stupid
> questions :)
> 
> We are struggling with a number of things.
> 
> 1.  is there a gui interface ?  How do we use it ?  R --gui just gets
> us to the command line.

AFAIK there is not a GUI in a sense of other statistical packages 
directly ot of a box. 

> 
> 2.  how do we do unix commands like cd ?

>From R command line?
see system


> 
> 3.  can we define default directories - eg for data files ?

yes, see setwd, getwd help page

> 
> 4.  we want to use this mainly in batch mode once we get our act
> together - are there any good examples of batch scripts we can use as
> crib sheets.
> 
> 5.  I installed some packages earlier - but now I am obviously doing
> something wrong > install.packages(gnomeGui) Error in
> install.packages(gnomeGui) : object "gnomeGui" not found

install.packages("gnomeGui")

Try to go through some manuals located in doc directory and/or on web 
pages of R project

HTH
Petr


> 
> 
> thanks,
> 
> Julie
> 
> 
> -- 
> Escience Support Team,  ITCS
> Unix Support ENV (please contact envcs.unix at uea.ac.uk)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From UriShimron at optiver.com  Thu Sep 21 15:27:35 2006
From: UriShimron at optiver.com (Uri Shimron)
Date: Thu, 21 Sep 2006 15:27:35 +0200
Subject: [R] Creating a new Access database with R
Message-ID: <061264DF11708C44BAF02C0F3F4EA28F01359FF7@opamms0002.comp.optiver.com>

First of all, since this is my first posting, I would like to thank
anybody who works/has worked on R, and made it such a beautiful open
source package!

My question is: how do I create a new Access database with R? I need a
channel before I can do anything, but if the mdb-file doesn't exist, I
can't connect to it with odbcConnectAccess.

I've looked at the RODBC.pdf on CRAN, searched the mailing-lists, and
looked at test.R file in the package. But probably I've overlooked
something.

It is of course possible to keep a clean new mdb-file somewhere and then
copy it to the required directory with: 
shell("copy EmptyDB.mdb NewLocation.mdb")

But that isn't very elegant...

Thanks in advance,

Uri Shimron

***************************************************************************
This email and any files transmitted with it are confidentia...{{dropped}}


From f.harrell at vanderbilt.edu  Thu Sep 21 15:44:20 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 21 Sep 2006 08:44:20 -0500
Subject: [R] Covariance matrix for first canonical variate
Message-ID: <45129734.8080208@vanderbilt.edu>

Does anyone know how to get a meaningful estimate of the covariance 
matrix for the first canonical variate coefficients for the right-hand 
(X) side of a canonical regression?  cancor returns coefficients but not 
precisions.  This would have to be subject to some constraint because of 
identifiability problems (arbitrary scaling).  I think that SPSS 
attempts to do this but seems to condition on the Y-coefficients which 
ignores a major source of uncertainty.

Thanks
Frank
-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From AnupTyagi at yahoo.com  Thu Sep 21 16:10:28 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Thu, 21 Sep 2006 14:10:28 +0000 (UTC)
Subject: [R] newie help needed
References: <45128BE6.3050609@uea.ac.uk>
Message-ID: <loom.20060921T154748-241@post.gmane.org>

Julie Harold <j.m.harold <at> uea.ac.uk> writes:

> 1.  is there a gui interface ?  How do we use it ?  R --gui just gets us
> to the command line.

Look at SciViews. Not a 100% menu driven GUI, but can be useful to new users.

> 3.  can we define default directories - eg for data files ?

I think you can, because sys-admin at my former university did so some years
ago. May be using a symbolic link---I am not sure though.

> 4.  we want to use this mainly in batch mode once we get our act
> together - are there any good examples of batch scripts we can use as
> crib sheets.

creating a new and simplified batch command for users may be helpful, rather
than allowing them to use the system "batch", "at" or related commands.


From bbands at gmail.com  Thu Sep 21 16:30:38 2006
From: bbands at gmail.com (BBands)
Date: Thu, 21 Sep 2006 07:30:38 -0700
Subject: [R] Command area in SciViews 0.8.9 - second try
In-Reply-To: <19387698.1158839901059.JavaMail.ngmail@webmail19>
References: <19387698.1158839901059.JavaMail.ngmail@webmail19>
Message-ID: <6e8360ad0609210730o2a516f83y373290d178a21758@mail.gmail.com>

On 9/21/06, stgries at arcor.de <stgries at arcor.de> wrote:
> (i) the command area at the bottom cannot be found
> (ii) the regular R window cannot be minimized/maximized anymore.

I cannot duplicate these problems with R 2.3.1, SciViews 0.8-9, Win XP(current).

    jab
-- 
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.


From efg at stowers-institute.org  Thu Sep 21 16:28:24 2006
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Thu, 21 Sep 2006 09:28:24 -0500
Subject: [R] plot correlation matrix
References: <198879552.20060921135447@eimb.ru>
Message-ID: <eeu7ib$be5$1@sea.gmane.org>

"Vladimir Eremeev" <wl at eimb.ru> wrote in message 
news:198879552.20060921135447 at eimb.ru...
> Dear useRs,
>
> While exploring new R packages, I have found the Rattle.
> This screenshot http://rattle.togaware.com/rattle-correlation.png
> is very interesting

> Which function was used to produce this plot?

library(ellipse)
 ?plotcorr

Look at the third example for the color plot.


Earl F. Glynn
Bioinformatics
Stowers Institute for Medical Research


From maechler at stat.math.ethz.ch  Thu Sep 21 16:42:33 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 21 Sep 2006 16:42:33 +0200
Subject: [R] Mail Server timeout of 1--2 hours
Message-ID: <17682.42201.148712.432141@stat.math.ethz.ch>

Because of the replacement of the current NAS (Networked Disk Server),
there will be a mail server timeout
from 18:00 till about 20:00 Swiss time, i.e. UTC+2

This will also effect cran.CH.r-project.org,
our ftp server and http://ESS.r-project.org/
but *not* the subversion server  svn.r-project.org
(because that is completely independent with no mounted file
systems).

Martin Maechler, ETH Zurich


From HDoran at air.org  Thu Sep 21 16:40:36 2006
From: HDoran at air.org (Doran, Harold)
Date: Thu, 21 Sep 2006 10:40:36 -0400
Subject: [R] Exponentiate a matrix
Message-ID: <2323A6D37908A847A7C32F1E3662C80E3B83BB@dc1ex01.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060921/62a99d27/attachment.pl 

From maechler at stat.math.ethz.ch  Thu Sep 21 17:05:50 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 21 Sep 2006 17:05:50 +0200
Subject: [R] binom
In-Reply-To: <5cd96f050609201100q3e4866devc964372cd6cf48be@mail.gmail.com>
References: <5cd96f050609201100q3e4866devc964372cd6cf48be@mail.gmail.com>
Message-ID: <17682.43598.190675.322368@stat.math.ethz.ch>

Please stop posting your homework problems to R-help!

The posting guide quite explictly does not allow it.

M.Maechler, ETH Zurich


From murdoch at stats.uwo.ca  Thu Sep 21 17:07:52 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 21 Sep 2006 11:07:52 -0400
Subject: [R] Exponentiate a matrix
In-Reply-To: <2323A6D37908A847A7C32F1E3662C80E3B83BB@dc1ex01.air.org>
References: <2323A6D37908A847A7C32F1E3662C80E3B83BB@dc1ex01.air.org>
Message-ID: <4512AAC8.4000601@stats.uwo.ca>

On 9/21/2006 10:40 AM, Doran, Harold wrote:
> Suppose I have a square matrix P
> 
> P <- matrix(c(.3,.7, .7, .3), ncol=2)
> 
> I know that 
> 
>> P * P 
> 
> Returns the element by element product, whereas
> 
>> P%*%P
> 
> Returns the matrix product.
> 
> Now, P^2 also returns the element by element product. But, is there a
> slick way to write
> 
> P %*% P %*% P
> 
> Obviously, P^3 does not return the result I expect.


I don't think there's anything built in, but it's easy to write your own:

"%^%" <- function(mat, pow) {
   stopifnot(length(pow) == 1, all.equal(pow, round(pow)), nrow(mat) == 
ncol(mat))
   pow <- round(pow)
   if (pow < 0) {
     mat <- solve(mat)
     pow <- abs(pow)
   }
   result <- diag(nrow(mat))
   while (pow > 0) {
     result <- result %*% mat
     pow <- pow - 1
   }
   result
}

Now P %^% 3 will give you the matrix cube.

Duncan Murdoch


From bbands at gmail.com  Thu Sep 21 17:09:22 2006
From: bbands at gmail.com (BBands)
Date: Thu, 21 Sep 2006 08:09:22 -0700
Subject: [R] plot correlation matrix
In-Reply-To: <eeu7ib$be5$1@sea.gmane.org>
References: <198879552.20060921135447@eimb.ru> <eeu7ib$be5$1@sea.gmane.org>
Message-ID: <6e8360ad0609210809m877c5b0m9d3550f58ad81102@mail.gmail.com>

On 9/21/06, Earl F. Glynn <efg at stowers-institute.org> wrote:
> > Which function was used to produce this plot?
>
> library(ellipse)
>  ?plotcorr
>
> Look at the third example for the color plot.

That's pretty nice. I have been using symnum(). Are there any other
neat visualisations for correlation matrices?

     jab
-- 
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.


From lamac_k at hotmail.com  Thu Sep 21 17:15:02 2006
From: lamac_k at hotmail.com (lamack lamack)
Date: Thu, 21 Sep 2006 15:15:02 +0000
Subject: [R] frequency table
Message-ID: <BAY113-F349A8221F845D5A68E37CF99200@phx.gbl>

Dear all, I have a vector like this:

z = rep(c("M","F"),c(50,60))

How can I get the following frequency table:

Sex             counts     %
F                60           54.5
M               50           45.5

I try:

DD<-   function(data,...)
{
    n       <-  nobs(data)
    out     <-  c(Frequency  = n,
                  k = n/length(data))
    return(out)
}


mApply(z,z,DD)   but ....


Best regards

_________________________________________________________________
Insta-le agora o Windows Live Messenger


From Dimitris.Rizopoulos at med.kuleuven.be  Thu Sep 21 17:26:25 2006
From: Dimitris.Rizopoulos at med.kuleuven.be (Dimitrios Rizopoulos)
Date: Thu, 21 Sep 2006 17:26:25 +0200
Subject: [R] Exponentiate a matrix
In-Reply-To: <4512AAC8.4000601@stats.uwo.ca>
References: <2323A6D37908A847A7C32F1E3662C80E3B83BB@dc1ex01.air.org>
	<4512AAC8.4000601@stats.uwo.ca>
Message-ID: <20060921172625.lsy8ruybompy8k0w@webmail4.kuleuven.be>


Quoting Duncan Murdoch <murdoch at stats.uwo.ca>:

> On 9/21/2006 10:40 AM, Doran, Harold wrote:
>> Suppose I have a square matrix P
>>
>> P <- matrix(c(.3,.7, .7, .3), ncol=2)
>>
>> I know that
>>
>>> P * P
>>
>> Returns the element by element product, whereas
>>
>>> P%*%P
>>
>> Returns the matrix product.
>>
>> Now, P^2 also returns the element by element product. But, is there a
>> slick way to write
>>
>> P %*% P %*% P
>>
>> Obviously, P^3 does not return the result I expect.
>
>
> I don't think there's anything built in, but it's easy to write your own:

I think there was function mtx.exp() in the Malmig package, but it  
seems that this package has been withdrawn from CRAN. An old version  
appears to exist in:

http://r.meteo.uni.wroc.pl/src/contrib/Descriptions/Malmig.html

Best,
Dimitris


> "%^%" <- function(mat, pow) {
>    stopifnot(length(pow) == 1, all.equal(pow, round(pow)), nrow(mat) ==
> ncol(mat))
>    pow <- round(pow)
>    if (pow < 0) {
>      mat <- solve(mat)
>      pow <- abs(pow)
>    }
>    result <- diag(nrow(mat))
>    while (pow > 0) {
>      result <- result %*% mat
>      pow <- pow - 1
>    }
>    result
> }
>
> Now P %^% 3 will give you the matrix cube.
>
> Duncan Murdoch
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>



Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From tlumley at u.washington.edu  Thu Sep 21 17:35:20 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 21 Sep 2006 08:35:20 -0700 (PDT)
Subject: [R] survival function with a Weibull dist
In-Reply-To: <20060921104812.47030.qmail@web51913.mail.yahoo.com>
References: <20060921104812.47030.qmail@web51913.mail.yahoo.com>
Message-ID: <Pine.LNX.4.64.0609210829110.27855@homer22.u.washington.edu>

On Thu, 21 Sep 2006, Anaid Diaz wrote:

> Hi
> I am using R to fit a survival function to my data
> (with a weibull distribution).
>
> Data: Survival of individuals in relation to 4
> treatments ('a','b','c','g')
>
> syntax:
> ----  > survreg(Surv(date2)~males2, dist='weibull')
>
> But I have some problems interpreting the outcome and
> getting the parameters for each curve.
>
> ---------              Value Std. Error      z
> p
> ---------  (Intercept)  2.788      0.147 19.022
> 1.13e-80
> --------- males2b     -0.107      0.207 -0.519
> 6.04e-01
> --------- males2c     -0.486      0.586 -0.831
> 4.06e-01
> --------- males2g      0.580      0.207  2.798
> 5.15e-03
> --------- Log(scale)  -1.116      0.139 -8.007
> 1.18e-15
> ---------
> --------- Scale= 0.328
>
>
> I know from Venables & Ripley (2002) that the
> parameters of this function should be two:

"this function" being which function? As help(survreg.distributions) says
      The Weibull distribution is not parameterised the
      same way as in 'rweibull'.

In your model the survival time is a variable whose logarithm has 
distribution

    2.788-0.107males2b-0.486males2c+0.580males2g+epsilon*0.328

where epsilon has cdf F=1-e^{-e^t}.

As in the example on help(survreg.distributions) shows, a survreg Weibull 
model with linear predictor M and scale S corresponds to R's weibull 
distribution with scale exp(M) and shape 1/S.

 	-thomas


From info at aghmed.fsnet.co.uk  Thu Sep 21 12:29:57 2006
From: info at aghmed.fsnet.co.uk (Michael Dewey)
Date: Thu, 21 Sep 2006 11:29:57 +0100
Subject: [R] Variable im Data Frame Namen
In-Reply-To: <OF353871EA.E0807E4B-ONC12571EF.003C6F07-C12571EF.003CC4FE@
	de.ibm.com>
References: <OF353871EA.E0807E4B-ONC12571EF.003C6F07-C12571EF.003CC4FE@de.ibm.com>
Message-ID: <7.0.0.16.0.20060921112746.019c1880@aghmed.fsnet.co.uk>

At 12:03 20/09/2006, Thorsten Muehge wrote:

>Hello R Experts,
>how can I incorporate a variable in a data frame definition?

Thorsten, you have already had a reply about this, but I wonder 
whether you _really_ want to do this. You do not say what the 
scientific question is you are trying to answer, but I find that when 
I am tempted to do this it is almost always better to make a list (in 
your case of data frames) and then access the elements of the list

>Example:
>week <- 28
>test(week) <- data.frame(a,b,s,c);
>test28

Michael Dewey
http://www.aghmed.fsnet.co.uk


From mel at altk.com  Thu Sep 21 17:40:43 2006
From: mel at altk.com (mel)
Date: Thu, 21 Sep 2006 17:40:43 +0200
Subject: [R] plot correlation matrix
In-Reply-To: <6e8360ad0609210809m877c5b0m9d3550f58ad81102@mail.gmail.com>
References: <198879552.20060921135447@eimb.ru> <eeu7ib$be5$1@sea.gmane.org>
	<6e8360ad0609210809m877c5b0m9d3550f58ad81102@mail.gmail.com>
Message-ID: <4512B27B.7090207@altk.com>

BBands a ?crit :

> That's pretty nice. I have been using symnum(). Are there any other
> neat visualisations for correlation matrices?

Although very simple, I find often image() sufficient.
Plus some ordering if needed.


From miltinho_astronauta at yahoo.com.br  Thu Sep 21 17:44:33 2006
From: miltinho_astronauta at yahoo.com.br (Milton Cezar)
Date: Thu, 21 Sep 2006 12:44:33 -0300 (ART)
Subject: [R] Plot Confidence Interval for Regression
Message-ID: <20060921154433.36128.qmail@web53404.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060921/93da3506/attachment.pl 

From p.dalgaard at biostat.ku.dk  Thu Sep 21 17:50:48 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 21 Sep 2006 17:50:48 +0200
Subject: [R] Exponentiate a matrix
In-Reply-To: <20060921172625.lsy8ruybompy8k0w@webmail4.kuleuven.be>
References: <2323A6D37908A847A7C32F1E3662C80E3B83BB@dc1ex01.air.org>
	<4512AAC8.4000601@stats.uwo.ca>
	<20060921172625.lsy8ruybompy8k0w@webmail4.kuleuven.be>
Message-ID: <x28xkdrzfr.fsf@turmalin.kubism.ku.dk>

Dimitrios Rizopoulos <Dimitris.Rizopoulos at med.kuleuven.be> writes:

> > I don't think there's anything built in, but it's easy to write your own:
> 
> I think there was function mtx.exp() in the Malmig package, but it  
> seems that this package has been withdrawn from CRAN. An old version  
> appears to exist in:
> 
> http://r.meteo.uni.wroc.pl/src/contrib/Descriptions/Malmig.html
> 

There's expm in the Matrix package and MatrixExp in msm. Bit of an
overkill for P^3 though (an you need a matrix logarithm too).

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From lawremi at iastate.edu  Thu Sep 21 14:39:19 2006
From: lawremi at iastate.edu (Michael Lawrence)
Date: Thu, 21 Sep 2006 07:39:19 -0500
Subject: [R] [R-pkgs] cairoDevice 1.2 on CRAN
Message-ID: <451287F7.90200@iastate.edu>

The cairoDevice package has recently been released on CRAN. cairoDevice 
is an R graphics device based on the cairo 
(http://www.cairographics.org) vector graphics library. It is 
distinguished from many other devices in its ability to render high 
quality anti-aliased graphics. It is available on all three major 
platforms (Windows, Mac, and Linux) and requires the GTK+ 2.8.0 library 
and its dependencies. For the current users, version 1.2 fixes many bugs.

cairoDevice is capable of rendering in three different ways: to the 
screen inside a GTK+ window, to an arbitrary GtkDrawingArea that is part 
of an RGtk2 GUI, or to any GdkDrawable, allowing for example the output 
of png and jpg images derived from R graphics using the GdkPixbuf 
bindings from RGtk2.

cairoDevice is interactive in that it supports the locator tool as well 
as the getGraphicsEvent() functionality, which previously was available 
only from the Windows device.

To obtain the requisite GTK+ (>= 2.8.0) libraries:

Windows: Download and run the installer from http://gladewin32.sf.net.
Mac: DarwinPorts
Linux: packages from your distribution

Please enjoy the package and let me know if you have any problems,

Michael Lawrence

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From u08adh at hotmail.com  Thu Sep 21 20:09:02 2006
From: u08adh at hotmail.com (Andreas Hary)
Date: Thu, 21 Sep 2006 19:09:02 +0100
Subject: [R] Exponentiate a matrix
References: <2323A6D37908A847A7C32F1E3662C80E3B83BB@dc1ex01.air.org><4512AAC8.4000601@stats.uwo.ca>
	<20060921172625.lsy8ruybompy8k0w@webmail4.kuleuven.be>
Message-ID: <BAY114-DAV747804B38D8851ED6E0C0DF200@phx.gbl>

There is also mexp in the Matrix package and MatrixExp in the msm package.

Andreas



----- Original Message ----- 
From: "Dimitrios Rizopoulos" <Dimitris.Rizopoulos at med.kuleuven.be>
To: "Duncan Murdoch" <murdoch at stats.uwo.ca>
Cc: "Doran, Harold" <HDoran at air.org>; <r-help at stat.math.ethz.ch>
Sent: Thursday, September 21, 2006 4:26 PM
Subject: Re: [R] Exponentiate a matrix


>
> Quoting Duncan Murdoch <murdoch at stats.uwo.ca>:
>
>> On 9/21/2006 10:40 AM, Doran, Harold wrote:
>>> Suppose I have a square matrix P
>>>
>>> P <- matrix(c(.3,.7, .7, .3), ncol=2)
>>>
>>> I know that
>>>
>>>> P * P
>>>
>>> Returns the element by element product, whereas
>>>
>>>> P%*%P
>>>
>>> Returns the matrix product.
>>>
>>> Now, P^2 also returns the element by element product. But, is there a
>>> slick way to write
>>>
>>> P %*% P %*% P
>>>
>>> Obviously, P^3 does not return the result I expect.
>>
>>
>> I don't think there's anything built in, but it's easy to write your own:
>
> I think there was function mtx.exp() in the Malmig package, but it
> seems that this package has been withdrawn from CRAN. An old version
> appears to exist in:
>
> http://r.meteo.uni.wroc.pl/src/contrib/Descriptions/Malmig.html
>
> Best,
> Dimitris
>
>
>> "%^%" <- function(mat, pow) {
>>    stopifnot(length(pow) == 1, all.equal(pow, round(pow)), nrow(mat) ==
>> ncol(mat))
>>    pow <- round(pow)
>>    if (pow < 0) {
>>      mat <- solve(mat)
>>      pow <- abs(pow)
>>    }
>>    result <- diag(nrow(mat))
>>    while (pow > 0) {
>>      result <- result %*% mat
>>      pow <- pow - 1
>>    }
>>    result
>> }
>>
>> Now P %^% 3 will give you the matrix cube.
>>
>> Duncan Murdoch
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>
>
>
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From bates at stat.wisc.edu  Thu Sep 21 20:08:38 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 21 Sep 2006 13:08:38 -0500
Subject: [R] Exponentiate a matrix
In-Reply-To: <20060921172625.lsy8ruybompy8k0w@webmail4.kuleuven.be>
References: <2323A6D37908A847A7C32F1E3662C80E3B83BB@dc1ex01.air.org>
	<4512AAC8.4000601@stats.uwo.ca>
	<20060921172625.lsy8ruybompy8k0w@webmail4.kuleuven.be>
Message-ID: <40e66e0b0609211108t78ab1109h142192000e81a392@mail.gmail.com>

On 9/21/06, Dimitrios Rizopoulos <Dimitris.Rizopoulos at med.kuleuven.be> wrote:
>
> Quoting Duncan Murdoch <murdoch at stats.uwo.ca>:
>
> > On 9/21/2006 10:40 AM, Doran, Harold wrote:
> >> Suppose I have a square matrix P
> >>
> >> P <- matrix(c(.3,.7, .7, .3), ncol=2)
> >>
> >> I know that
> >>
> >>> P * P
> >>
> >> Returns the element by element product, whereas
> >>
> >>> P%*%P
> >>
> >> Returns the matrix product.
> >>
> >> Now, P^2 also returns the element by element product. But, is there a
> >> slick way to write
> >>
> >> P %*% P %*% P
> >>
> >> Obviously, P^3 does not return the result I expect.
> >
> >
> > I don't think there's anything built in, but it's easy to write your own:
>
> I think there was function mtx.exp() in the Malmig package, but it
> seems that this package has been withdrawn from CRAN. An old version
> appears to exist in:
>
> http://r.meteo.uni.wroc.pl/src/contrib/Descriptions/Malmig.html
>
> Best,
> Dimitris

Is that function for matrix powers or for the exponential of a matrix
(which is what I initally thought that Harold wanted)?  There is a
function expm in the Matrix package, patterned on the octave function
of the same name, the calculates the matrix exponential for a square
matrix.

>
>
> > "%^%" <- function(mat, pow) {
> >    stopifnot(length(pow) == 1, all.equal(pow, round(pow)), nrow(mat) ==
> > ncol(mat))
> >    pow <- round(pow)
> >    if (pow < 0) {
> >      mat <- solve(mat)
> >      pow <- abs(pow)
> >    }
> >    result <- diag(nrow(mat))
> >    while (pow > 0) {
> >      result <- result %*% mat
> >      pow <- pow - 1
> >    }
> >    result
> > }
> >
> > Now P %^% 3 will give you the matrix cube.
> >
> > Duncan Murdoch
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
>
>
>
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From preuth at slf.ch  Thu Sep 21 20:01:31 2006
From: preuth at slf.ch (Thomas Preuth)
Date: Thu, 21 Sep 2006 20:01:31 +0200
Subject: [R] delete a entire vector of a dataframe
Message-ID: <4512D37B.9040100@slf.ch>

delete a entire vector of a dataframe

Hello,

i want to delete a vector and tried "rm (t.d$V712)". This did not work, 
message was, could not find variable. I thought the $ defines the vectro 
in a dataframe, when I just type "t.d$V712" the content of this vector 
is displayed.

Greetings, Thomas


From preuth at slf.ch  Thu Sep 21 19:58:38 2006
From: preuth at slf.ch (Thomas Preuth)
Date: Thu, 21 Sep 2006 19:58:38 +0200
Subject: [R] how to ignore "NA" or replace it by another value
Message-ID: <4512D2CE.40306@slf.ch>

Hello,

I`m a newbie to R so maybe this question is boring, but I have a large 
table with several empty missing values, which come out as "NA". How can 
i ignore them or replace them by another number?

Greetings, Thomas


From mothsailor at googlemail.com  Thu Sep 21 18:42:14 2006
From: mothsailor at googlemail.com (David Barron)
Date: Thu, 21 Sep 2006 17:42:14 +0100
Subject: [R] frequency table
In-Reply-To: <BAY113-F349A8221F845D5A68E37CF99200@phx.gbl>
References: <BAY113-F349A8221F845D5A68E37CF99200@phx.gbl>
Message-ID: <815b70590609210942j1f96daabx4dd33b28ca62c3bc@mail.gmail.com>

And look at ?prop.table too.

On 21/09/06, lamack lamack <lamac_k at hotmail.com> wrote:
> Dear all, I have a vector like this:
>
> z = rep(c("M","F"),c(50,60))
>
> How can I get the following frequency table:
>
> Sex             counts     %
> F                60           54.5
> M               50           45.5
>
> I try:
>
> DD<-   function(data,...)
> {
>     n       <-  nobs(data)
>     out     <-  c(Frequency  = n,
>                   k = n/length(data))
>     return(out)
> }
>
>
> mApply(z,z,DD)   but ....
>
>
> Best regards
>
> _________________________________________________________________
> Insta-le agora o Windows Live Messenger
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From mothsailor at googlemail.com  Thu Sep 21 18:35:55 2006
From: mothsailor at googlemail.com (David Barron)
Date: Thu, 21 Sep 2006 17:35:55 +0100
Subject: [R] frequency table
In-Reply-To: <815b70590609210935r52e9966at5ef59c0fec74472f@mail.gmail.com>
References: <BAY113-F349A8221F845D5A68E37CF99200@phx.gbl>
	<815b70590609210935r52e9966at5ef59c0fec74472f@mail.gmail.com>
Message-ID: <815b70590609210935j226cbeebsee4034cce7ae4251@mail.gmail.com>

You might want to look at the CrossTable function in the gmodels
package (in the gregmisc bundle).

On 21/09/06, lamack lamack <lamac_k at hotmail.com> wrote:
> Dear all, I have a vector like this:
>
> z = rep(c("M","F"),c(50,60))
>
> How can I get the following frequency table:
>
> Sex             counts     %
> F                60           54.5
> M               50           45.5
>
> I try:
>
> DD<-   function(data,...)
> {
>     n       <-  nobs(data)
>     out     <-  c(Frequency  = n,
>                   k = n/length(data))
>     return(out)
> }
>
>
> mApply(z,z,DD)   but ....
>
>
> Best regards
>
> _________________________________________________________________
> Insta-le agora o Windows Live Messenger
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


--
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From sgunnste at jhsph.edu  Thu Sep 21 18:16:58 2006
From: sgunnste at jhsph.edu (Snaebjorn Gunnsteinsson)
Date: Thu, 21 Sep 2006 12:16:58 -0400
Subject: [R] Installing packages with R v.2.3.1 on MAC OS X -- Error:
	"Cannot chdir: No such file or directory"
Message-ID: <7190356C-2172-4515-B347-DFB59383C714@jhsph.edu>

Dear all,

    I'm working on MAC OS X 10.4.7 using R version 2.3.1 (June 2006)  
and platform powerpc-apple-darwin8.6.0. Since I installed the new  
version of R I cannot install packages any more (but it worked fine  
before) and get the following message every time. I can, however,  
install packages from the command line if I get them as precompiled  
binaries.

------------------------------------------------------------------------ 
------------------------------------------------------------------
 > install.packages("foreign", lib="~/libs/Rlibs")
trying URL 'http://www.biometrics.mtu.edu/CRAN/bin/macosx/powerpc/ 
contrib/2.3/foreign_0.8-15.tgz'
Content type 'application/x-gzip' length 268096 bytes
opened URL
==================================================
downloaded 261Kb

tar: ~/libs/Rlibs/file77a4044d: Cannot chdir: No such file or directory
tar: Error is not recoverable: exiting now

gzip: stdout: Broken pipe
Error in sprintf(gettext(fmt, domain = domain), ...) :
         argument is missing, with no default
 >
------------------------------------------------------------------------ 
-------------------------------------------------------------------

The command did create the directory (i.e. ~/libs/Rlibs/file77a4044d)  
on my system and has open priviledges (i.e. drwxr-xr-x) but is empty.  
I've tried 'sudo R' but that does not help and cannot find anything  
helpful on this in the R-help archives or FAQ.

Thanks in advance for your help,
I would really appreciate any help in solving this,
best regards,
Snaebjorn


From ferri.leberl at gmx.at  Thu Sep 21 20:28:03 2006
From: ferri.leberl at gmx.at (Mag. Ferri Leberl)
Date: Thu, 21 Sep 2006 20:28:03 +0200
Subject: [R] changing the index of a histogram
Message-ID: <1158863283.14172.18.camel@vulcania>

Dear everybody!
I am trying to make up a histogram of marks of a recent test I have
corrified.
Say I have a vector "points" and a vektor "breaks" to segregate the
marks. Finally there is a vector "marks".

hist(points, breaks=breaks) leaves me with two problems:
At first the width of the bars of the histogram corresponds with the
width of the intervall. This will look confusing and not very elaborated
when in the end the index shall not be "breaks", but "marks".
At second, you guessed it, I have the problem to exchange "breaks" for
"marks" on the y-axis, at least surfacially.
A glance into old mailing list brought me 
plot(table(cut(points,breaks=breaks,right=FALSE,include.lowest=TRUE)))
as a good solution to the first matter (except that right=FALSE seems
contradictory ;)
I would be very glad if you could give me a hint to solve the second
problem, most preferably such, that the first would be circumvaded by
transforming the variable intervals of "breaks" into the uniform ones of
"marks" and then employing hist().
Thank you in advance.
Yours,
Mag. Ferri Leberl


From kjetilbrinchmannhalvorsen at gmail.com  Thu Sep 21 20:49:28 2006
From: kjetilbrinchmannhalvorsen at gmail.com (Kjetil Halvorsen)
Date: Thu, 21 Sep 2006 14:49:28 -0400
Subject: [R] Problems with making a complex graphic
Message-ID: <556e90a80609211149o3f584933x470c4202a70e5451@mail.gmail.com>

En innebygd og tegnsett-uspesifisert tekst ble skilt ut...
Navn: ikke tilgjengelig
Nettadresse: https://stat.ethz.ch/pipermail/r-help/attachments/20060921/b479a8df/attachment.ksh 

From HDoran at air.org  Thu Sep 21 20:53:21 2006
From: HDoran at air.org (Doran, Harold)
Date: Thu, 21 Sep 2006 14:53:21 -0400
Subject: [R] how to ignore "NA" or replace it by another value
Message-ID: <2323A6D37908A847A7C32F1E3662C80E3B8402@dc1ex01.air.org>

It depends a bit on what function you are using. For example,

set.seed(1)
xx <- c(NA, rnorm(10))
> mean(xx)
[1] NA
> mean(xx, na.rm=TRUE)
[1] 0.1322028

Is how you would use this to compute a mean.

Harold


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Thomas Preuth
> Sent: Thursday, September 21, 2006 1:59 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] how to ignore "NA" or replace it by another value
> 
> Hello,
> 
> I`m a newbie to R so maybe this question is boring, but I 
> have a large table with several empty missing values, which 
> come out as "NA". How can i ignore them or replace them by 
> another number?
> 
> Greetings, Thomas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Dimitris.Rizopoulos at med.kuleuven.be  Thu Sep 21 20:58:27 2006
From: Dimitris.Rizopoulos at med.kuleuven.be (Dimitrios Rizopoulos)
Date: Thu, 21 Sep 2006 20:58:27 +0200
Subject: [R] Exponentiate a matrix
In-Reply-To: <40e66e0b0609211108t78ab1109h142192000e81a392@mail.gmail.com>
References: <2323A6D37908A847A7C32F1E3662C80E3B83BB@dc1ex01.air.org>
	<4512AAC8.4000601@stats.uwo.ca>
	<20060921172625.lsy8ruybompy8k0w@webmail4.kuleuven.be>
	<40e66e0b0609211108t78ab1109h142192000e81a392@mail.gmail.com>
Message-ID: <20060921205827.org1ssb4q30g40sk@webmail4.kuleuven.be>

Quoting Douglas Bates <bates at stat.wisc.edu>:

> On 9/21/06, Dimitrios Rizopoulos <Dimitris.Rizopoulos at med.kuleuven.be> wrote:
>>
>> Quoting Duncan Murdoch <murdoch at stats.uwo.ca>:
>>
>> > On 9/21/2006 10:40 AM, Doran, Harold wrote:
>> >> Suppose I have a square matrix P
>> >>
>> >> P <- matrix(c(.3,.7, .7, .3), ncol=2)
>> >>
>> >> I know that
>> >>
>> >>> P * P
>> >>
>> >> Returns the element by element product, whereas
>> >>
>> >>> P%*%P
>> >>
>> >> Returns the matrix product.
>> >>
>> >> Now, P^2 also returns the element by element product. But, is there a
>> >> slick way to write
>> >>
>> >> P %*% P %*% P
>> >>
>> >> Obviously, P^3 does not return the result I expect.
>> >
>> >
>> > I don't think there's anything built in, but it's easy to write your own:
>>
>> I think there was function mtx.exp() in the Malmig package, but it
>> seems that this package has been withdrawn from CRAN. An old version
>> appears to exist in:
>>
>> http://r.meteo.uni.wroc.pl/src/contrib/Descriptions/Malmig.html
>>
>> Best,
>> Dimitris
>
> Is that function for matrix powers or for the exponential of a matrix
> (which is what I initally thought that Harold wanted)?  There is a
> function expm in the Matrix package, patterned on the octave function
> of the same name, the calculates the matrix exponential for a square
> matrix.

this function calculates the n-th power of a matrix, and this is what  
I thought Harold wanted, i.e.,

P %*% P %*% P %*% P

should be equal to

mtx.exp(P, 4)


>>
>>
>> > "%^%" <- function(mat, pow) {
>> >    stopifnot(length(pow) == 1, all.equal(pow, round(pow)), nrow(mat) ==
>> > ncol(mat))
>> >    pow <- round(pow)
>> >    if (pow < 0) {
>> >      mat <- solve(mat)
>> >      pow <- abs(pow)
>> >    }
>> >    result <- diag(nrow(mat))
>> >    while (pow > 0) {
>> >      result <- result %*% mat
>> >      pow <- pow - 1
>> >    }
>> >    result
>> > }
>> >
>> > Now P %^% 3 will give you the matrix cube.
>> >
>> > Duncan Murdoch
>> >
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide   
>> http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>> >
>> >
>>
>>
>>
>> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>



Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From gunter.berton at gene.com  Thu Sep 21 20:59:34 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 21 Sep 2006 11:59:34 -0700
Subject: [R] how to ignore "NA" or replace it by another value
In-Reply-To: <4512D2CE.40306@slf.ch>
Message-ID: <006001c6ddb0$147b8620$711f210a@gne.windows.gene.com>

Most R functions have arguments of the form "na.action" or "na.rm" that
allow you to specify how you treat NA's. In general, it's not a good idea to
replace NA's with numbers.

See also ?na.omit, ?na.action.



-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Thomas Preuth
> Sent: Thursday, September 21, 2006 10:59 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] how to ignore "NA" or replace it by another value
> 
> Hello,
> 
> I`m a newbie to R so maybe this question is boring, but I 
> have a large 
> table with several empty missing values, which come out as 
> "NA". How can 
> i ignore them or replace them by another number?
> 
> Greetings, Thomas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mathieu.drapeau at mcgill.ca  Thu Sep 21 21:12:46 2006
From: mathieu.drapeau at mcgill.ca (Mathieu Drapeau)
Date: Thu, 21 Sep 2006 15:12:46 -0400
Subject: [R] [ROracle] error loading (undefined symbol: sqlclu)
In-Reply-To: <1158781613.13278.12.camel@localhost.localdomain>
References: <1158779707.4806.9.camel@mathieu2.campus.mcgill.ca>
	<1158781613.13278.12.camel@localhost.localdomain>
Message-ID: <4512E42E.4060305@mcgill.ca>

Hi Marc,
thanks for the answer, but that wasn't the problem.
I set my LD_LIBRARY_PATH in my .cshrc.

I solve my problem by compiling the library manually. It created me a 
file (ROracle.so) of ~14mb (instead of the one of ~150K) that I replaced 
with the other one.
I guess R compilation did something wrong.

Thanks,
Mathieu

Marc Schwartz (via MN) wrote:

>On Wed, 2006-09-20 at 15:15 -0400, Mathieu Drapeau wrote:
>  
>
>>I have this error when I load the library ROracle:
>>    
>>
>>>library(ROracle)
>>>      
>>>
>>Loading required package: DBI
>>Error in dyn.load(x, as.logical(local), as.logical(now)) :
>>        unable to load shared library
>>'/usr/local/lib/R/site-library/ROracle/libs/ROracle.so':
>>  /usr/local/lib/R/site-library/ROracle/libs/ROracle.so: undefined
>>symbol: sqlclu
>>Error in library(ROracle) : .First.lib failed for 'ROracle'
>>    
>>
>>Also, my LD_LIBRARY_PATH seems to be set correctly:
>>drapeau:~> echo $LD_LIBRARY_PATH
>>/home/drapeau/lib:/opt/oracle/xe/app/oracle/product/10.2.0/client/lib
>>
>>I installed the big database applications (10g) and ROracle 0.5-7
>>
>>Your help will be very appreciated to help me solve this error,
>>Thank you,
>>Mathieu
>>    
>>
>
>Where did you set LD_LIBRARY_PATH?
>
>If in one of your shell config files, it is likely that it is being
>stepped on during your login and thus not being seen within the R
>session. 
>
>I had this problem previously and set the variable in /etc/ld.so.conf
>(though I am using RODBC instead).
>
>Edit that file (as root), add the path:
>
>/opt/oracle/xe/app/oracle/product/10.2.0/client/lib
>
>and then run [/sbin/]ldconfig to update the current settings.
>
>Be sure also that $ORACLE_HOME is set
>to /opt/oracle/xe/app/oracle/product/10.2.0/client
>
>Then try to load ROracle in a new R session.
>
>HTH,
>
>Marc Schwartz
>
>
>
>  
>


From mothsailor at googlemail.com  Thu Sep 21 21:14:53 2006
From: mothsailor at googlemail.com (David Barron)
Date: Thu, 21 Sep 2006 20:14:53 +0100
Subject: [R] delete a entire vector of a dataframe
In-Reply-To: <4512D37B.9040100@slf.ch>
References: <4512D37B.9040100@slf.ch>
Message-ID: <815b70590609211214s1507b57aq87486d8652ed5950@mail.gmail.com>

There is a function remove.vars in the gdata package, or you can do this:

> > dt<-data.frame(a=(1:5),b=(6:10),c=(11:15))
> dt
  a  b  c
1 1  6 11
2 2  7 12
3 3  8 13
4 4  9 14
5 5 10 15

> dt[["a"]] <- NULL
> dt
> dt
   b  c
1  6 11
2  7 12
3  8 13
4  9 14
5 10 15

If you know it is the first column that you want to remove, you can
also use dt[,-1].

On 21/09/06, Thomas Preuth <preuth at slf.ch> wrote:
> delete a entire vector of a dataframe
>
> Hello,
>
> i want to delete a vector and tried "rm (t.d$V712)". This did not work,
> message was, could not find variable. I thought the $ defines the vectro
> in a dataframe, when I just type "t.d$V712" the content of this vector
> is displayed.
>
> Greetings, Thomas
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From gavin.simpson at ucl.ac.uk  Thu Sep 21 21:17:05 2006
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Thu, 21 Sep 2006 20:17:05 +0100
Subject: [R] how to ignore "NA" or replace it by another value
In-Reply-To: <4512D2CE.40306@slf.ch>
References: <4512D2CE.40306@slf.ch>
Message-ID: <1158866226.2871.4.camel@dhcppc2.my.nat.localnet>

On Thu, 2006-09-21 at 19:58 +0200, Thomas Preuth wrote:
> Hello,
> 
> I`m a newbie to R so maybe this question is boring, but I have a large 
> table with several empty missing values, which come out as "NA". How can 
> i ignore them or replace them by another number?
> 
> Greetings, Thomas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

Ignoring NAs depends on what functions you want to ignore them in.

As for replacing NAs with another number, this will replace all NAs with
0

# some example data
dat <- as.data.frame(matrix(rnorm(100), nrow = 10))
# add some NAs
dat[sample(1:10, 3), sample(1:10, 3)] <- NA
dat
# replace missing values with 0
dat <- sapply(dat, function(x) {x[is.na(x)] <- 0; x})
dat

HTH

G
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 *Note new Address and Fax and Telephone numbers from 10th April 2006*
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [t] +44 (0)20 7679 0522
ECRC                              [f] +44 (0)20 7679 0565
UCL Department of Geography
Pearson Building                  [e] gavin.simpsonATNOSPAMucl.ac.uk
Gower Street
London, UK                        [w] http://www.ucl.ac.uk/~ucfagls/cv/
WC1E 6BT                          [w] http://www.ucl.ac.uk/~ucfagls/
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From gavin.simpson at ucl.ac.uk  Thu Sep 21 21:28:10 2006
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Thu, 21 Sep 2006 20:28:10 +0100
Subject: [R] delete a entire vector of a dataframe
In-Reply-To: <4512D37B.9040100@slf.ch>
References: <4512D37B.9040100@slf.ch>
Message-ID: <1158866890.2871.16.camel@dhcppc2.my.nat.localnet>

On Thu, 2006-09-21 at 20:01 +0200, Thomas Preuth wrote:
> delete a entire vector of a dataframe
> 
> Hello,
> 
> i want to delete a vector and tried "rm (t.d$V712)". This did not work, 
> message was, could not find variable. I thought the $ defines the vectro 
> in a dataframe, when I just type "t.d$V712" the content of this vector 
> is displayed.
> 
> Greetings, Thomas

You can't do that, and that is not what the error message said exactly -
which should have told you something was wrong with your thinking as it
also said "1: remove: variable "$" was not found". Instead, copy over
the object, minus the column you want to delete:

dat <- as.data.frame(matrix(rnorm(100), nrow = 10))
names(dat) <- paste("Var", 1:10, sep = "_")
dat
# now we don't want column Var_6
dat <- dat[, -6]
# or if we don't know which column is Var_6 you could do
not.want <- which(names(dat) %in% "Var_7") # now don't want Var_7
dat <- dat[, -not.want]
dat

This can be extended to many variables:

not.want <- which(names(dat) %in% c("Var_10", "Var_2", "Var_8"))
dat <- dat[, -not.want]
dat # only 1, 3, 4, 5, 9 left

HTH

G
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 *Note new Address and Fax and Telephone numbers from 10th April 2006*
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [t] +44 (0)20 7679 0522
ECRC                              [f] +44 (0)20 7679 0565
UCL Department of Geography
Pearson Building                  [e] gavin.simpsonATNOSPAMucl.ac.uk
Gower Street
London, UK                        [w] http://www.ucl.ac.uk/~ucfagls/cv/
WC1E 6BT                          [w] http://www.ucl.ac.uk/~ucfagls/
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From attenka at utu.fi  Thu Sep 21 21:30:59 2006
From: attenka at utu.fi (kone)
Date: Thu, 21 Sep 2006 22:30:59 +0300
Subject: [R] Matrix from a vector?
Message-ID: <B67F17BC-B990-4D91-8778-CED7060A797B@local>

Hi,

Is there some function, which generates this kind of n x n -matrix  
from a vector?

 > rhset
[1]  1792   256 13312   512  1024  2048  8192  4096
 > 	m=matrix(nrow=length(rhset),ncol=length(rhset))
 > 	for(i in 1:length(rhset))
+ 	{
+ 		m[,i]=rhset
+ 		rhset=c(rhset[length(rhset)], rhset[2:length(rhset)-1])
+ 	}
 > m
       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]
[1,]  1792  4096  8192  2048  1024   512 13312   256
[2,]   256  1792  4096  8192  2048  1024   512 13312
[3,] 13312   256  1792  4096  8192  2048  1024   512
[4,]   512 13312   256  1792  4096  8192  2048  1024
[5,]  1024   512 13312   256  1792  4096  8192  2048
[6,]  2048  1024   512 13312   256  1792  4096  8192
[7,]  8192  2048  1024   512 13312   256  1792  4096
[8,]  4096  8192  2048  1024   512 13312   256  1792


Atte Tenkanen
University of Turku, Finland


From mothsailor at googlemail.com  Thu Sep 21 21:32:29 2006
From: mothsailor at googlemail.com (David Barron)
Date: Thu, 21 Sep 2006 20:32:29 +0100
Subject: [R] Problems with making a complex graphic
In-Reply-To: <556e90a80609211149o3f584933x470c4202a70e5451@mail.gmail.com>
References: <556e90a80609211149o3f584933x470c4202a70e5451@mail.gmail.com>
Message-ID: <815b70590609211232u7904fb71h10dd54248788ae64@mail.gmail.com>

I'm not sure why it doesn't work, perhaps something to do with how you
have set the mar and oma parameters?  But why not use

> mtext(side=1,text="aleles(bp)",outer=TRUE,col="snow3")

On 21/09/06, Kjetil Halvorsen <kjetilbrinchmannhalvorsen at gmail.com> wrote:
> Hola!
>
> maybe someone can see what is wrong in the following graphics code.
> The data needed for the example (printed with dump) is at the end of the
> message.
> I make 3 pages with multipanel plot, and want each page to have a subtitle
> explaining the common x-axis, with the command:
> title(sub="aleles(bp)", outer=TRUE,adj=0.5,col="snow3")
> but nothing appears. What is wrong?
>
> Here is the code:
>
> par(ask=TRUE)
>
> par(mfcol=c(6,2),mar=c(2,2,3,0)+0.1 ,bg="gray3", col.axis="snow3",
> col.lab="snow3",
> col.main="snow3", col.sub="snow3",
>         oma=c(4,3,4,0), cex.axis=0.6, cex.main=0.8, cex.sub=0.6,
> cex.lab=0.6, yaxt="n")
>
> for (i in 1:6) {
>    barplot( tab107[,i], names.arg=c(182, 196, 200, 202, 206, 210, 220, 248)
> ,
>                col="snow3" , main=pops[i] ,xlab="", ylab="" ) }
>
>
> for (i in 1:6) {
>    barplot(tab02[,i], names.arg=c(140, 148, 152, 172, 178, 180, 196, 198,
> 206, 210, 230),
>                col="snow3", main=pops[i] , xlab="", ylab="")}
>
>
> title(main=paste(alelos[1],"
> ",alelos[2]), outer=TRUE, cex=1.5, col="red")
>    title(sub="aleles(bp)", outer=TRUE,adj=0.5,col="snow3")
>    mtext(side=2,text="Frecuencias", outer=TRUE,adj=0.5)
>
> par(mfcol=c(6,2),mar=c(2,2,3,0)+0.1 ,bg="gray3", col.axis="snow3",
> col.lab="snow3",
> col.main="snow3", col.sub="snow3",
>         oma=c(4,3,4,0), cex.axis=0.6, cex.main=0.8, cex.sub=0.6,
> cex.lab=0.6,yaxt="n")
>
> for (i in 1:6) {
>    barplot( tab137[,i], names.arg=c(128, 130, 132, 134, 136, 138, 140, 142,
> 144, 146, 148, 150, 164, 180),
>                 col="snow3", main=pops[i] ,xlab="", ylab="")}
>
>
> for (i in 1:6) {
>    barplot( tab110[,i], names.arg=c(158, 162, 164, 168, 172, 176, 180, 198)
> ,
>                 col="snow3", main=pops[i], xlab="",ylab="") }
>
>
> title(main=paste(alelos[3],"
> ",alelos[4]), outer=TRUE,cex=1.5, col="red")
>    title(sub="aleles(bp)", outer=TRUE, adj=0.5, col="snow3")
>    mtext(side=2,text="Frecuencias", outer=TRUE,adj=0.5)
>
> par(mfcol=c(6,2),mar=c(2,2,3,0)+0.1 ,bg="gray3", col.axis="snow3",
> col.lab="snow3",
> col.main="snow3", col.sub="snow3",
>         oma=c(4,3,4,0), cex.axis=0.6, cex.main=0.8,
> cex.sub=0.6,cex.lab=0.6, yaxt="n")
>
> for (i in 1:6) {
>    barplot( tab28[,i], names.arg=c(120, 124, 128, 130, 132, 134) ,
>                 col="snow3", main=pops[i], xlab="", ylab="") }
>
>
> title(main=paste(alelos[5],"
> "), outer=TRUE,cex=1.5, col='red')
>    title(sub="aleles(bp)", outer=TRUE,adj=0.5, col="snow3")
>    mtext(side=2,text="Frecuencias", outer=TRUE,adj=0.5)
>
> Here is the sessionInfo:
>
> R version 2.4.0 alpha (2006-09-16 r39365)
> i386-pc-mingw32
>
> locale:
> LC_COLLATE=Spanish_Bolivia.1252;LC_CTYPE=Spanish_Bolivia.1252;LC_MONETARY=Spanish_Bolivia.1252;LC_NUMERIC=C;LC_TIME=Spanish_Bolivia.1252
>
> attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
> [7] "base"
>
> Here is the necessary data:
>
> alelos <-
> c("ADC107", "ADC02", "ADC137", "ADC110", "ADC28")
> pops <-
> c("Prado", "P. Rico", "Guayaramerin", "C. Esperanza", "R. de Yata",
> "S. Miguel")
> tab02 <-
> structure(list(V3 = c(0.012, 0.183, 0, 0.195, 0.012, 0.049, 0.012,
> 0.305, 0.232, 0, 0), V4 = c(0.024, 0.476, 0, 0.012, 0, 0.073,
> 0.024, 0.171, 0.171, 0.049, 0), V5 = c(0.014, 0.111, 0.014, 0.236,
> 0.111, 0.194, 0, 0.042, 0.236, 0.028, 0.014), V6 = c(0.024, 0.214,
> 0, 0.202, 0.024, 0.167, 0.06, 0.071, 0.202, 0.036, 0), V7 = c(0.038,
> 0.449, 0, 0.128, 0.077, 0.038, 0, 0, 0.192, 0.077, 0), V8 = c(0,
> 0.338, 0, 0.113, 0.125, 0.138, 0, 0.038, 0.125, 0.075, 0.05)), .Names =
> c("V3",
> "V4", "V5", "V6", "V7", "V8"), row.names = c("1", "2", "3", "4",
> "5", "6", "7", "8", "9", "10", "11"), class = "data.frame")
> tab107 <-
> structure(list(V3 = c(0.079, 0.421, 0.118, 0.158, 0.079, 0.079,
> 0.066, 0), V4 = c(0.053, 0.5, 0.066, 0.118, 0.211, 0.039, 0.013,
> 0), V5 = c(0.222, 0.444, 0.069, 0.181, 0.042, 0.014, 0.028, 0
> ), V6 = c(0.138, 0.625, 0.113, 0.1, 0.013, 0.013, 0, 0), V7 = c(0.135,
> 0.541, 0.041, 0.095, 0.149, 0.014, 0.014, 0.014), V8 = c(0.212,
> 0.53, 0.061, 0.076, 0.03, 0.03, 0.03, 0.03)), .Names = c("V3",
> "V4", "V5", "V6", "V7", "V8"), row.names = c("1", "2", "3", "4",
> "5", "6", "7", "8"), class = "data.frame")
> tab110 <-
> structure(list(V3 = c(0.088, 0.25, 0.338, 0.213, 0.038, 0, 0.075,
> 0), V4 = c(0.107, 0.393, 0.333, 0.095, 0.024, 0, 0.048, 0), V5 = c(0.162,
> 0.351, 0.27, 0.149, 0.014, 0.014, 0.041, 0), V6 = c(0.012, 0.14,
> 0.535, 0.186, 0.07, 0, 0.058, 0), V7 = c(0.027, 0.23, 0.392,
> 0.149, 0.095, 0, 0.081, 0.027), V8 = c(0, 0.125, 0.475, 0.263,
> 0.088, 0, 0.025, 0.025)), .Names = c("V3", "V4", "V5", "V6",
> "V7", "V8"), row.names = c("1", "2", "3", "4", "5", "6", "7",
> "8"), class = "data.frame")
> tab137 <-
> structure(list(V3 = c(0.111, 0.069, 0.097, 0.167, 0.014, 0.069,
> 0.278, 0.153, 0.028, 0, 0.014, 0, 0, 0), V4 = c(0.05, 0.063,
> 0.038, 0.213, 0, 0.075, 0.3, 0.125, 0.1, 0, 0.013, 0, 0.013,
> 0.013), V5 = c(0.103, 0.167, 0.064, 0.115, 0, 0.064, 0.333, 0.064,
> 0.026, 0.013, 0.051, 0, 0, 0), V6 = c(0, 0.026, 0.079, 0.171,
> 0.026, 0.118, 0.289, 0.158, 0.053, 0.013, 0.039, 0.026, 0, 0),
>     V7 = c(0, 0.063, 0.075, 0.2, 0.1, 0.125, 0.225, 0.163, 0.038,
>     0, 0, 0.013, 0, 0), V8 = c(0, 0.075, 0.038, 0.113, 0.063,
>     0.188, 0.188, 0.238, 0.075, 0, 0.025, 0, 0, 0)), .Names = c("V3",
> "V4", "V5", "V6", "V7", "V8"), row.names = c("1", "2", "3", "4",
> "5", "6", "7", "8", "9", "10", "11", "12", "13", "14"), class = "data.frame
> ")
> tab28 <-
> structure(list(V3 = c(0, 0.167, 0.444, 0.097, 0.097, 0.194),
>     V4 = c(0, 0.125, 0.4, 0.238, 0.088, 0.15), V5 = c(0.013,
>     0.064, 0.487, 0.231, 0.051, 0.154), V6 = c(0, 0.122, 0.537,
>     0.122, 0.061, 0.159), V7 = c(0, 0.09, 0.577, 0.141, 0.026,
>     0.167), V8 = c(0, 0.158, 0.579, 0.171, 0.013, 0.079)), .Names = c("V3",
> "V4", "V5", "V6", "V7", "V8"), row.names = c("1", "2", "3", "4",
> "5", "6"), class = "data.frame")
>
> Thanks,Kjetil Halvorsen
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From sundar.dorai-raj at pdf.com  Thu Sep 21 21:53:52 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 21 Sep 2006 14:53:52 -0500
Subject: [R] Matrix from a vector?
In-Reply-To: <B67F17BC-B990-4D91-8778-CED7060A797B@local>
References: <B67F17BC-B990-4D91-8778-CED7060A797B@local>
Message-ID: <4512EDD0.4070401@pdf.com>



kone said the following on 9/21/2006 2:30 PM:
> Hi,
> 
> Is there some function, which generates this kind of n x n -matrix  
> from a vector?
> 
>  > rhset
> [1]  1792   256 13312   512  1024  2048  8192  4096
>  > 	m=matrix(nrow=length(rhset),ncol=length(rhset))
>  > 	for(i in 1:length(rhset))
> + 	{
> + 		m[,i]=rhset
> + 		rhset=c(rhset[length(rhset)], rhset[2:length(rhset)-1])
> + 	}
>  > m
>        [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]
> [1,]  1792  4096  8192  2048  1024   512 13312   256
> [2,]   256  1792  4096  8192  2048  1024   512 13312
> [3,] 13312   256  1792  4096  8192  2048  1024   512
> [4,]   512 13312   256  1792  4096  8192  2048  1024
> [5,]  1024   512 13312   256  1792  4096  8192  2048
> [6,]  2048  1024   512 13312   256  1792  4096  8192
> [7,]  8192  2048  1024   512 13312   256  1792  4096
> [8,]  4096  8192  2048  1024   512 13312   256  1792
> 
> 
> Atte Tenkanen
> University of Turku, Finland
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


Does this work for you?

rhsel <- c(1792, 256, 13312, 512, 1024, 2048, 8192, 4096)
n <- length(rhsel)
i <- sapply(1:n, function(i) (1:n - i)%%n + 1)
matrix(rhsel[i], n, n)

HTH,

--sundar


From macq at llnl.gov  Thu Sep 21 21:59:30 2006
From: macq at llnl.gov (Don MacQueen)
Date: Thu, 21 Sep 2006 12:59:30 -0700
Subject: [R] Installing packages with R v.2.3.1 on MAC OS X -- Error:
 "Cannot chdir: No such file or directory"
In-Reply-To: <7190356C-2172-4515-B347-DFB59383C714@jhsph.edu>
References: <7190356C-2172-4515-B347-DFB59383C714@jhsph.edu>
Message-ID: <p0623090bc1389e35ffba@[128.115.153.6]>

Are you trying to install a package from source?
Try adding type='source' when you call install.packages().
The help page for install.packages() indicates the need for this on a Mac.

You might also check and see if ~/libs exists, and you have write 
privileges, but take care of the source vs. binary issue first. 
Notice the 'bin' in the URL path it mentions.

-Don

At 12:16 PM -0400 9/21/06, Snaebjorn Gunnsteinsson wrote:
>Dear all,
>
>     I'm working on MAC OS X 10.4.7 using R version 2.3.1 (June 2006) 
>and platform powerpc-apple-darwin8.6.0. Since I installed the new 
>version of R I cannot install packages any more (but it worked fine 
>before) and get the following message every time. I can, however, 
>install packages from the command line if I get them as precompiled 
>binaries.
>
>------------------------------------------------------------------------
>------------------------------------------------------------------
>  > install.packages("foreign", lib="~/libs/Rlibs")
>trying URL 'http://www.biometrics.mtu.edu/CRAN/bin/macosx/powerpc/
>contrib/2.3/foreign_0.8-15.tgz'
>Content type 'application/x-gzip' length 268096 bytes
>opened URL
>==================================================
>downloaded 261Kb
>
>tar: ~/libs/Rlibs/file77a4044d: Cannot chdir: No such file or directory
>tar: Error is not recoverable: exiting now
>
>gzip: stdout: Broken pipe
>Error in sprintf(gettext(fmt, domain = domain), ...) :
>          argument is missing, with no default
>  >
>------------------------------------------------------------------------
>-------------------------------------------------------------------
>
>The command did create the directory (i.e. ~/libs/Rlibs/file77a4044d) 
>on my system and has open priviledges (i.e. drwxr-xr-x) but is empty. 
>I've tried 'sudo R' but that does not help and cannot find anything 
>helpful on this in the R-help archives or FAQ.
>
>Thanks in advance for your help,
>I would really appreciate any help in solving this,
>best regards,
>Snaebjorn
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA


From sgunnste at jhsph.edu  Thu Sep 21 22:20:53 2006
From: sgunnste at jhsph.edu (Snaebjorn Gunnsteinsson)
Date: Thu, 21 Sep 2006 16:20:53 -0400
Subject: [R] Installing packages with R v.2.3.1 on MAC OS X -- Error:
	"Cannot chdir: No such file or directory"
In-Reply-To: <p0623090bc1389e35ffba@[128.115.153.6]>
References: <7190356C-2172-4515-B347-DFB59383C714@jhsph.edu>
	<p0623090bc1389e35ffba@[128.115.153.6]>
Message-ID: <0F7FC1DF-7F26-449B-9237-FF06A600AFFD@jhsph.edu>

Hi Don,

     type="source" solved the problem.

Thanks very much for your help,
best,
Snaebjorn

On Sep 21, 2006, at 3:59 PM, Don MacQueen wrote:

> Are you trying to install a package from source?
> Try adding type='source' when you call install.packages().
> The help page for install.packages() indicates the need for this on  
> a Mac.
>
> You might also check and see if ~/libs exists, and you have write  
> privileges, but take care of the source vs. binary issue first.  
> Notice the 'bin' in the URL path it mentions.
>
> -Don
>
> At 12:16 PM -0400 9/21/06, Snaebjorn Gunnsteinsson wrote:
>> Dear all,
>>
>>     I'm working on MAC OS X 10.4.7 using R version 2.3.1 (June  
>> 2006) and platform powerpc-apple-darwin8.6.0. Since I installed  
>> the new version of R I cannot install packages any more (but it  
>> worked fine before) and get the following message every time. I  
>> can, however, install packages from the command line if I get them  
>> as precompiled binaries.
>>
>> --------------------------------------------------------------------- 
>> ---
>> ------------------------------------------------------------------
>>  > install.packages("foreign", lib="~/libs/Rlibs")
>> trying URL 'http://www.biometrics.mtu.edu/CRAN/bin/macosx/powerpc/
>> contrib/2.3/foreign_0.8-15.tgz'
>> Content type 'application/x-gzip' length 268096 bytes
>> opened URL
>> ==================================================
>> downloaded 261Kb
>>
>> tar: ~/libs/Rlibs/file77a4044d: Cannot chdir: No such file or  
>> directory
>> tar: Error is not recoverable: exiting now
>>
>> gzip: stdout: Broken pipe
>> Error in sprintf(gettext(fmt, domain = domain), ...) :
>>          argument is missing, with no default
>>  >
>> --------------------------------------------------------------------- 
>> ---
>> -------------------------------------------------------------------
>>
>> The command did create the directory (i.e. ~/libs/Rlibs/ 
>> file77a4044d) on my system and has open priviledges (i.e. drwxr-xr- 
>> x) but is empty. I've tried 'sudo R' but that does not help and  
>> cannot find anything helpful on this in the R-help archives or FAQ.
>>
>> Thanks in advance for your help,
>> I would really appreciate any help in solving this,
>> best regards,
>> Snaebjorn
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting- 
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>
> -- 
> --------------------------------------
> Don MacQueen
> Environmental Protection Department
> Lawrence Livermore National Laboratory
> Livermore, CA, USA
> --------------------------------------


From rbaer at atsu.edu  Thu Sep 21 22:32:18 2006
From: rbaer at atsu.edu (Robert Baer)
Date: Thu, 21 Sep 2006 15:32:18 -0500
Subject: [R] frequency table
References: <BAY113-F349A8221F845D5A68E37CF99200@phx.gbl>
	815b70590609210942j1f96daabx4dd33b28ca62c3bc@mail.gmail.com
Message-ID: <011c01c6ddbd$086d8330$a00c010a@BigBaer>



____________________________
Robert W. Baer, Ph.D.
Associate Professor
Department of Physiology
A. T. Still University of Health Science
800 W. Jefferson St. 
Kirksville, MO 63501-1497 USA


----- Original Message ----- > > 

> z = rep(c("M","F"),c(50,60))
>How can I get the following frequency table:
> >
> > Sex             counts     %
> > F                60           54.5
> > M               50           45.5

How about:
data.frame(table(z),percent=100*as.vector(table(z)/sum(table(z))))

HTH,
Rob


From devens8765 at yahoo.com  Thu Sep 21 22:43:49 2006
From: devens8765 at yahoo.com (Dave Evens)
Date: Thu, 21 Sep 2006 13:43:49 -0700 (PDT)
Subject: [R] Spliting a huge vector
Message-ID: <20060921204349.44323.qmail@web58306.mail.re3.yahoo.com>


Dear R users,

Sorry, I made a mistake in my specification of
problem, should be:

# my vector is this
a.vector <- seq(2, by=5, length=1000)

# so my cut values are
cut.values <- c(30, 50, 100, 109, 300, 601, 803, 1000)

# and x should have an extra ")" at the end
x <- rep(1:length(cut.values), times=diff(c(0,
cut.values)))

Thanks for all the responses so far. Any additional
responses are welcome.

Many thanks,
Dave





--- Dave Evens <devens8765 at yahoo.com> wrote:

> 
> Dear R users,
> 
> I have a huge vector that I would like to split into
> unequal slices. However, the only way I can do this
> is
> to create another huge vector to define the groups
> that are used to split the original vector, e.g.
> 
> # my vector is this
> a.vector <- seq(2, by=5, length=100)
> 
> # indices where I would like to slice my vector
> cut.values <- c(30, 50, 100, 109, 300, 601, 803)
> 
> # so I have to create another vector of similar
> length
> # to use the split() command, i.e.
> x <- rep(1:length(cut.values), times=diff(c(0,
> cut.values))
> 
> # this means I can use split()
> split(a.vector, x)
> 
> This seems to be a waste in terms of memory usage as
> I'm creating another vector (here "x") to split the
> original vector. Is there a better way to split a
> huge
> vector than this? Any help is much appreciated. 
> 
> Best,
> Dave.
> 
> 
> __________________________________________________
> Do You Yahoo!?

> protection around 
> http://mail.yahoo.com 
>


From lawremi at iastate.edu  Thu Sep 21 14:39:19 2006
From: lawremi at iastate.edu (Michael Lawrence)
Date: Thu, 21 Sep 2006 07:39:19 -0500
Subject: [R] [R-pkgs] cairoDevice 1.2 on CRAN
Message-ID: <451287F7.90200@iastate.edu>

The cairoDevice package has recently been released on CRAN. cairoDevice 
is an R graphics device based on the cairo 
(http://www.cairographics.org) vector graphics library. It is 
distinguished from many other devices in its ability to render high 
quality anti-aliased graphics. It is available on all three major 
platforms (Windows, Mac, and Linux) and requires the GTK+ 2.8.0 library 
and its dependencies. For the current users, version 1.2 fixes many bugs.

cairoDevice is capable of rendering in three different ways: to the 
screen inside a GTK+ window, to an arbitrary GtkDrawingArea that is part 
of an RGtk2 GUI, or to any GdkDrawable, allowing for example the output 
of png and jpg images derived from R graphics using the GdkPixbuf 
bindings from RGtk2.

cairoDevice is interactive in that it supports the locator tool as well 
as the getGraphicsEvent() functionality, which previously was available 
only from the Windows device.

To obtain the requisite GTK+ (>= 2.8.0) libraries:

Windows: Download and run the installer from http://gladewin32.sf.net.
Mac: DarwinPorts
Linux: packages from your distribution

Please enjoy the package and let me know if you have any problems,

Michael Lawrence

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From tilling at gmail.com  Thu Sep 21 23:01:26 2006
From: tilling at gmail.com (John Tillinghast)
Date: Thu, 21 Sep 2006 14:01:26 -0700
Subject: [R] Adding .R to source file keeps R from reading it?
Message-ID: <73bcebe0609211401ua0783aem12217b92eb053d81@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060921/041ac1c5/attachment.ksh 

From tilling at gmail.com  Thu Sep 21 23:06:43 2006
From: tilling at gmail.com (John Tillinghast)
Date: Thu, 21 Sep 2006 14:06:43 -0700
Subject: [R] 'help' information not modified when I modify man files
Message-ID: <73bcebe0609211406i11a22627j828ea0ef82187ad4@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060921/7b711178/attachment.ksh 

From ruser2006 at yahoo.com  Thu Sep 21 23:10:54 2006
From: ruser2006 at yahoo.com (r user)
Date: Thu, 21 Sep 2006 14:10:54 -0700 (PDT)
Subject: [R] "summarry.lm" and NA values
In-Reply-To: <000c01c6c082$6b7885e0$711f210a@gne.windows.gene.com>
Message-ID: <20060921211054.30200.qmail@web37008.mail.mud.yahoo.com>

Gentlemen,

(I am using R 2.2.1 in a Windows environment.)

I apologize but I did not fully comprehend all of your
answer.  I have a dataframe called ?data1?.  I run
several liner regression using the lm function similar
to:

reg <- ( lm(lm(data1[,2] ~., data1[,2:4])) )


I see from generous answers below how I can use 
"coef(reg)" to extract the coefficient estimates.  (If
the coefficient for a variable is for some reason NA,
"coef(reg)"  returns  NA for that coefficient, which
is what I want.)

My question: 
What is the best way to get the standard errors,
including NA values that ?go with? each of these
coefficient estimates?  (i.e. If the coefficient
estimate is NA, I similarly want the standard error to
come back as NA, so that the length of coef(reg) is
the same as the length of the vector that contains the
standard errors. )

Thanks very much for all your help, and I apologize
for my need of additional assistance.






--- Berton Gunter <gunter.berton at gene.com> wrote:

> "Is there a way to..." always has the answer "yes"
> in R (or C or any
> language for that matter). The question is: "Is
> there a GOOD way...?" where
> "good" depends on the specifics of the situation. So
> after that polemic,
> below is an effort to answer, (adding to what Petr
> Pikal already said):
> 
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
>  
> "The business of the statistician is to catalyze the
> scientific learning
> process."  - George E. P. Box
>  
>  
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On
> Behalf Of r user
> > Sent: Tuesday, August 15, 2006 7:01 AM
> > To: rhelp
> > Subject: [R] question re: "summarry.lm" and NA
> values
> > 
> > Is there a way to get the following code to
> include
> > NA values where the coefficients are "NA"?
> > 
> > ((summary(reg))$coefficients)
> BAAAD! Don't so this. Use the extractor on the
> object: coef(reg) 
> This suggests that you haven't read the
> documentation carefully, which tends
> to arouse the ire of would-be helpers.
> 
> > 
> > explanation:
> > 
> > Using a loop, I am running regressions on several
> > "subsets" of "data1".
> > 
> > "reg <- ( lm(lm(data1[,1] ~., data1[,2:l])) )"
> ??? There's an error here I think. Do you mean
> update()? Do you have your
> subscripting correct?
> 
> > 
> > My regression has 10 independent variables, and I
> > therefore expect 11 coefficients.
> > After each regression, I wish to save the
> coefficients
> > and standard errors of the coefficients in a table
> > with 22 columns.
> > 
> > I successfully extract the coefficients using the
> > following code:
> > "reg$coefficients"
> Use the extractor, coef()
> 
> > 
> > I attempt to extract the standard errors using :
> > 
> > aperm((summary(reg))$coefficients)[2,]
> 
> BAAAD! Use the extractor vcov():
> sqrt(diag(vcov(reg)))
> > 
> > ((summary(reg))$coefficients)
> > 
> > My problem:
> > For some of my subsets, I am missing data for one
> or
> > more of the independent variables.  This of course
> > causes the coefficients and standard erros for
> this
> > variable to be "NA".
> Not it doesn't, as Petr said.
> 
> One possible approach: Assuming that a variable is
> actually missing (all
> NA's), note that coef(reg) is a named vector, so
> that the character string
> names of the regressors actually used are available.
> You can thus check for
> what's missing and add them as NA's at each return.
> Though I confess that I
> see no reason to put things ina matrix rather than
> just using a list. But
> that's a matter of personal taste I suppose.
> 
> > 
> > Is there a way to include the NA standard errors,
> so
> > that I have the same number of standard erros and
> > coefficients for each regression, and can then
> store
> > the coefficients and standard erros in my table of
> 22
> > columns?
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide 
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained,
> reproducible code.
> > 
> 
>


From gunter.berton at gene.com  Thu Sep 21 23:20:14 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 21 Sep 2006 14:20:14 -0700
Subject: [R] "summarry.lm" and NA values
In-Reply-To: <20060921211054.30200.qmail@web37008.mail.mud.yahoo.com>
Message-ID: <007301c6ddc3$bae970e0$711f210a@gne.windows.gene.com>

?vcov

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r user [mailto:ruser2006 at yahoo.com] 
> Sent: Thursday, September 21, 2006 2:11 PM
> To: Berton Gunter; 'rhelp'
> Subject: "summarry.lm" and NA values
> 
> Gentlemen,
> 
> (I am using R 2.2.1 in a Windows environment.)
> 
> I apologize but I did not fully comprehend all of your
> answer.  I have a dataframe called "data1".  I run
> several liner regression using the lm function similar
> to:
> 
> reg <- ( lm(lm(data1[,2] ~., data1[,2:4])) )
> 
> 
> I see from generous answers below how I can use 
> "coef(reg)" to extract the coefficient estimates.  (If
> the coefficient for a variable is for some reason NA,
> "coef(reg)"  returns  NA for that coefficient, which
> is what I want.)
> 
> My question: 
> What is the best way to get the standard errors,
> including NA values that "go with" each of these
> coefficient estimates?  (i.e. If the coefficient
> estimate is NA, I similarly want the standard error to
> come back as NA, so that the length of coef(reg) is
> the same as the length of the vector that contains the
> standard errors. )
> 
> Thanks very much for all your help, and I apologize
> for my need of additional assistance.
> 
> 
> 
> 
> 
> 
> --- Berton Gunter <gunter.berton at gene.com> wrote:
> 
> > "Is there a way to..." always has the answer "yes"
> > in R (or C or any
> > language for that matter). The question is: "Is
> > there a GOOD way...?" where
> > "good" depends on the specifics of the situation. So
> > after that polemic,
> > below is an effort to answer, (adding to what Petr
> > Pikal already said):
> > 
> > -- Bert Gunter
> > Genentech Non-Clinical Statistics
> > South San Francisco, CA
> >  
> > "The business of the statistician is to catalyze the
> > scientific learning
> > process."  - George E. P. Box
> >  
> >  
> > 
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch 
> > > [mailto:r-help-bounces at stat.math.ethz.ch] On
> > Behalf Of r user
> > > Sent: Tuesday, August 15, 2006 7:01 AM
> > > To: rhelp
> > > Subject: [R] question re: "summarry.lm" and NA
> > values
> > > 
> > > Is there a way to get the following code to
> > include
> > > NA values where the coefficients are "NA"?
> > > 
> > > ((summary(reg))$coefficients)
> > BAAAD! Don't so this. Use the extractor on the
> > object: coef(reg) 
> > This suggests that you haven't read the
> > documentation carefully, which tends
> > to arouse the ire of would-be helpers.
> > 
> > > 
> > > explanation:
> > > 
> > > Using a loop, I am running regressions on several
> > > "subsets" of "data1".
> > > 
> > > "reg <- ( lm(lm(data1[,1] ~., data1[,2:l])) )"
> > ??? There's an error here I think. Do you mean
> > update()? Do you have your
> > subscripting correct?
> > 
> > > 
> > > My regression has 10 independent variables, and I
> > > therefore expect 11 coefficients.
> > > After each regression, I wish to save the
> > coefficients
> > > and standard errors of the coefficients in a table
> > > with 22 columns.
> > > 
> > > I successfully extract the coefficients using the
> > > following code:
> > > "reg$coefficients"
> > Use the extractor, coef()
> > 
> > > 
> > > I attempt to extract the standard errors using :
> > > 
> > > aperm((summary(reg))$coefficients)[2,]
> > 
> > BAAAD! Use the extractor vcov():
> > sqrt(diag(vcov(reg)))
> > > 
> > > ((summary(reg))$coefficients)
> > > 
> > > My problem:
> > > For some of my subsets, I am missing data for one
> > or
> > > more of the independent variables.  This of course
> > > causes the coefficients and standard erros for
> > this
> > > variable to be "NA".
> > Not it doesn't, as Petr said.
> > 
> > One possible approach: Assuming that a variable is
> > actually missing (all
> > NA's), note that coef(reg) is a named vector, so
> > that the character string
> > names of the regressors actually used are available.
> > You can thus check for
> > what's missing and add them as NA's at each return.
> > Though I confess that I
> > see no reason to put things ina matrix rather than
> > just using a list. But
> > that's a matter of personal taste I suppose.
> > 
> > > 
> > > Is there a way to include the NA standard errors,
> > so
> > > that I have the same number of standard erros and
> > > coefficients for each regression, and can then
> > store
> > > the coefficients and standard erros in my table of
> > 22
> > > columns?
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide 
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained,
> > reproducible code.
> > > 
> > 
> >
> 
> 
> __________________________________________________
> Do You Yahoo!?
> Tired of spam?  Yahoo! Mail has the best spam protection around 
> http://mail.yahoo.com 
>


From deepayan.sarkar at gmail.com  Thu Sep 21 23:58:06 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 21 Sep 2006 14:58:06 -0700
Subject: [R] Adding .R to source file keeps R from reading it?
In-Reply-To: <73bcebe0609211401ua0783aem12217b92eb053d81@mail.gmail.com>
References: <73bcebe0609211401ua0783aem12217b92eb053d81@mail.gmail.com>
Message-ID: <eb555e660609211458u3f94cc57i9d53aa9d1e9187f9@mail.gmail.com>

On 9/21/06, John Tillinghast <tilling at gmail.com> wrote:
> Hi,
>
> I'm updating the LMGene package from Bioconductor. "Writing R Extensions"
> suggests
> that all source files (the ones in the R directory) have a .R ending, so I
> added it to the (one) source file.
> The next time I installed and ran R, R didn't understand any of the
> functions.
> I tried various things and eventually went back to the file and dropped the
> .R ending, installed, ran R. It worked!
> For purposes of distributing the package, do I want to leave the name
> without the .R, or add the .R and change something else?

I'm guessing that the "source" you are working on has been obtained by
unzipping the windows binary zip file. Despite appearances, that is
not the source code. For the proper source code, download the file
that's marked as source. In this case,

http://bioconductor.org/packages/1.8/bioc/html/LMGene.html

clearly labels the following as "Source" (and the corresponding zip
file as "Windows Binary")

http://bioconductor.org/packages/1.8/bioc/src/contrib/LMGene_1.0.0.tar.gz

This likely answers your other question as well.

-Deepayan


From murdoch at stats.uwo.ca  Fri Sep 22 00:46:07 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 21 Sep 2006 18:46:07 -0400
Subject: [R] Adding .R to source file keeps R from reading it?
In-Reply-To: <73bcebe0609211401ua0783aem12217b92eb053d81@mail.gmail.com>
References: <73bcebe0609211401ua0783aem12217b92eb053d81@mail.gmail.com>
Message-ID: <4513162F.2060000@stats.uwo.ca>

On 9/21/2006 5:01 PM, John Tillinghast wrote:
> Hi,
> 
> I'm updating the LMGene package from Bioconductor. "Writing R Extensions"
> suggests
> that all source files (the ones in the R directory) have a .R ending, so I
> added it to the (one) source file.
> The next time I installed and ran R, R didn't understand any of the
> functions.
> I tried various things and eventually went back to the file and dropped the
> .R ending, installed, ran R. It worked!
> For purposes of distributing the package, do I want to leave the name
> without the .R, or add the .R and change something else?

You aren't giving us very much information, but I would guess you edited 
the installed copy of the package rather than the source.  (Where did 
you find the file with the source in it?  If it was in
$(RHOME)/library/LMGene/R, then that's the installed copy, and you 
shouldn't edit it.)  When R installs a package, it copies all the source 
into one file and doesn't put the .R extension on it.  I imagine if you 
try running after renaming such a file, R wouldn't see it.

Your other message about seeing a "help" directory confirms this.

You should get the source to the LMGene package, and make modifications 
to that, or just write your own functions and put them in your own file.

Duncan Murdoch


From ggrothendieck at gmail.com  Fri Sep 22 01:07:48 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 21 Sep 2006 19:07:48 -0400
Subject: [R] Matrix from a vector?
In-Reply-To: <B67F17BC-B990-4D91-8778-CED7060A797B@local>
References: <B67F17BC-B990-4D91-8778-CED7060A797B@local>
Message-ID: <971536df0609211607u3b1c761ld30cc14f8ceae302@mail.gmail.com>

Try this:

matrix(rhset[outer(1:8, 1:8, "-") %% 8 + 1], 8)


On 9/21/06, kone <attenka at utu.fi> wrote:
> Hi,
>
> Is there some function, which generates this kind of n x n -matrix
> from a vector?
>
>  > rhset
> [1]  1792   256 13312   512  1024  2048  8192  4096
>  >      m=matrix(nrow=length(rhset),ncol=length(rhset))
>  >      for(i in 1:length(rhset))
> +       {
> +               m[,i]=rhset
> +               rhset=c(rhset[length(rhset)], rhset[2:length(rhset)-1])
> +       }
>  > m
>       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]
> [1,]  1792  4096  8192  2048  1024   512 13312   256
> [2,]   256  1792  4096  8192  2048  1024   512 13312
> [3,] 13312   256  1792  4096  8192  2048  1024   512
> [4,]   512 13312   256  1792  4096  8192  2048  1024
> [5,]  1024   512 13312   256  1792  4096  8192  2048
> [6,]  2048  1024   512 13312   256  1792  4096  8192
> [7,]  8192  2048  1024   512 13312   256  1792  4096
> [8,]  4096  8192  2048  1024   512 13312   256  1792
>
>
> Atte Tenkanen
> University of Turku, Finland
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From hb at stat.berkeley.edu  Fri Sep 22 02:35:46 2006
From: hb at stat.berkeley.edu (Henrik Bengtsson)
Date: Thu, 21 Sep 2006 17:35:46 -0700
Subject: [R] Adding .R to source file keeps R from reading it?
In-Reply-To: <73bcebe0609211401ua0783aem12217b92eb053d81@mail.gmail.com>
References: <73bcebe0609211401ua0783aem12217b92eb053d81@mail.gmail.com>
Message-ID: <59d7961d0609211735o74ccca73y9828aa4670fc33a4@mail.gmail.com>

Hmm...  sounds like you're on Windows and have the Explorer setup such
that it is hiding file extensions.  You can try to use list.files()
from R to see if the files actually have file extension.

If this is your problem, open My Computer -> Tools -> Folder
Options... and select tab View.  Make sure that "Hide extensions for
known file types" is *NOT* selected.

/H



On 9/21/06, John Tillinghast <tilling at gmail.com> wrote:
> Hi,
>
> I'm updating the LMGene package from Bioconductor. "Writing R Extensions"
> suggests
> that all source files (the ones in the R directory) have a .R ending, so I
> added it to the (one) source file.
> The next time I installed and ran R, R didn't understand any of the
> functions.
> I tried various things and eventually went back to the file and dropped the
> .R ending, installed, ran R. It worked!
> For purposes of distributing the package, do I want to leave the name
> without the .R, or add the .R and change something else?
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From tilling at gmail.com  Fri Sep 22 03:55:57 2006
From: tilling at gmail.com (John Tillinghast)
Date: Thu, 21 Sep 2006 18:55:57 -0700
Subject: [R] Adding .R to source file keeps R from reading it?
In-Reply-To: <eb555e660609211458u3f94cc57i9d53aa9d1e9187f9@mail.gmail.com>
References: <73bcebe0609211401ua0783aem12217b92eb053d81@mail.gmail.com>
	<eb555e660609211458u3f94cc57i9d53aa9d1e9187f9@mail.gmail.com>
Message-ID: <73bcebe0609211855s7cb58984x591ffe5dc0865cd2@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060921/65af90a6/attachment.pl 

From xchen_stat at hotmail.com  Fri Sep 22 07:07:03 2006
From: xchen_stat at hotmail.com (X.H Chen)
Date: Thu, 21 Sep 2006 23:07:03 -0600
Subject: [R] how to store recursive results
Message-ID: <BAY23-F139B9DF4D701209ACD94DDFA210@phx.gbl>

Hi all,

How to store recursive resutls from a function for each step without using 
global operators <<-? Thanks ahead.

Xiaohui Chen

Dept. of Statistics
UBC, Canada

_________________________________________________________________
Don?t waste time standing in line?try shopping online. Visit Sympatico / MSN


From parrinel at med.unibs.it  Fri Sep 22 08:41:19 2006
From: parrinel at med.unibs.it (Giovanni Parrinello)
Date: Fri, 22 Sep 2006 08:41:19 +0200
Subject: [R] Propensity score and three treatments
Message-ID: <4513858F.6000900@med.unibs.it>

Dear All,
I would like to find something ( references, code,..)  to implement a
comparison of three
treatments in an observational study using the 'Propensity Score'.
Any help is much appreciated. Thanks!
Giovanni

-- 
dr. Giovanni Parrinello
Department of Biotecnologies
Medical Statistics Unit
University of Brescia
Viale Europa, 11 25123 Brescia
email: parrinel at med.unibs.it
Phone: +390303717528
Fax: +390303717488


From ripley at stats.ox.ac.uk  Fri Sep 22 08:44:39 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 22 Sep 2006 07:44:39 +0100 (BST)
Subject: [R] Different result from nls in R-2.2.1 and R-2.3.1
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC047E4C70@DJFPOST01.djf.agrsci.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC047E4C70@DJFPOST01.djf.agrsci.dk>
Message-ID: <Pine.LNX.4.64.0609220722420.3525@gannet.stats.ox.ac.uk>

The way nls looks for variables in its formula is very unusual.  For 
non-parameters it has

     for(var in varNames[!varIndex])
         mf[[var]] <- eval(as.name(var), data)

inside nls, and that means that the scoping rules are first 'data' then 
the body of nls().  That's surely not what one would expect here: 
fitting functions using model.frame look in 'data' and then the 
environment of the formula (if non-null) or the parent frame.

So to avoid surprises, put fixed values in 'data'.  (Your example did not 
work in 2.2.1 when run from a function.)

I'll try to find a bug fix for 2.4.0.


On Thu, 21 Sep 2006, Frede Aakmann T?gersen wrote:

>
>
> Short story: January 2006 I did some analysis in R-2.2.1 using nls. Repeating the exercise in R-2.3.1 yesterday produced somewhat different results.
>
> After some debugging I found that either nls is the problem or that mine understanding of environments or scoping rules is lacking something.
>
> This is a short reproducing example.
>
>
>
> x <- seq(0,5,len=20)
>
> n <- 1
> y <- 2*x^2 + n + rnorm(x)
>
> xy <- data.frame(x=x,y=y)
>
> myf <- function(x,a,b,n){
>  res <- a*x^b + n
>  ## a print for debugging purpose
>  print(n)
>  res
> }
>
> ## This works as I expect it to do in R-2.2.1 but doesn't work in R-2.3.1.
> ## n is somehow sat to nrow(xy) inside nls()
> ## Note that x and y is defined in the dataframe xy, whereas n is found in the global environment.
> fit <- nls(y ~ myf(x,a,b,n), data=xy, start=c(a=1,b=1), trace=TRUE)
>
> ## this works in both versions
> ## x,y,n found in the .GlobalEnv
> fit <- nls(y ~ myf(x,a,b,n), start=c(a=1,b=1), trace=TRUE)
>
> ## this works in both versions.
> ## x, y, n found in dataframe xyn
> xyn <- data.frame(xy,n=n)
> fit <- nls(y ~ myf(x,a,b,n), data=xyn, start=c(a=1,b=1), trace=TRUE)
>
> ## this works in both versions
> ## Now using the variable .n instead of n
> ## .n is found in .GlobaEnv
> .n <- 1
> fit <- nls(y ~ myf(x,a,b,.n), data=xy, start=c(a=1,b=1), trace=TRUE)
>
>
> In my real case and the example above, I do have three or more parameters of which fitting is done only on few of theme. Is this a problem? Or should I ask, why is this a problem in R-2.3.1 but not in R-2.2.1?
>
> Is my problem related to this difference between lines of code from nls:
>
> R-2.2.1:     mf <- as.list(eval(mf, parent.frame()))
>
> R-2.3.1:     mf <- eval.parent(mf)
>             n <- nrow(mf)
>             mf <- as.list(mf)
>
> where n is being defined in the scope of nls in the latest version?
>
> Best regards
>
> Frede Aakmann T?gersen
>
>
>
> Danish Institute of Agricultural Sciences
> Research Centre Foulum
> Dept. of Genetics and Biotechnology
> Blichers All? 20, P.O. BOX 50
> DK-8830 Tjele
>
> Phone:   +45 8999 1900
> Direct:  +45 8999 1878
>
> E-mail:  FredeA.Togersen at agrsci.dk
> Web:	   http://www.agrsci.org
>
> This email may contain information that is confidential.
> Any use or publication of this email without written permission from DIAS is not allowed.
> If you are not the intended recipient, please notify DIAS immediately and delete this email.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From RKrug at sun.ac.za  Fri Sep 22 08:54:26 2006
From: RKrug at sun.ac.za (Rainer M Krug)
Date: Fri, 22 Sep 2006 08:54:26 +0200
Subject: [R] Update to Dillo browser question
Message-ID: <451388A2.5040802@sun.ac.za>

Hi

I asked about if there is any way of opening URLs from the help browser 
in the same window of the same dillo browser - here is the answer.

Just to reiterate: dillo is for me the perfect browser for the help of R 
when you use ?...

Rainer

-------- Original Message --------
Subject: Re: [Dillo-dev] Opening new URL in same instance and -s option
Date: Thu, 21 Sep 2006 21:31:57 -0400
From: Jorge Arellano Cid <jcid at dillo.org>
To: Rainer M Krug <rkrug at sun.ac.za>
References: <45128134.60709 at sun.ac.za>

On Thu, Sep 21, 2006 at 02:10:28PM +0200, Rainer M Krug wrote:
> Hi

   Hi.

> 
> I have two questions
> 
> first: I would like to be able to have only one instance of dillo open 
> and when I try to open a new url (from outside dillo), that this url is 
> opened in the old instance. Is this possible, and if yes, how?

   Not now.

   The idea is to implement this with dpip (dillo plugin protocol)
but it has not being done yet, becuase of lack of manpower.


> second: I have seen emails concerning running dillo in server mode 
> (dillo -s server). I tried it, but dillo didn't know -s. Is this feature 
> imp[lemented, and if yes, how can I access it?

   Not in the official dillo.

> 
> Thanks a lot for a brilliant lightning fast browser,

   :-)


-- 
   Cheers
   Jorge.-

-- 
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
Biology (UCT)

Department of Conservation Ecology and Entomology
University of Stellenbosch
Matieland 7602
South Africa

Tel:		+27 - (0)72 808 2975 (w)
Fax:		+27 - (0)21 808 3304
Cell:		+27 - (0)83 9479 042

email:	RKrug at sun.ac.za
       	Rainer at krugs.de


From petr.pikal at precheza.cz  Fri Sep 22 08:58:48 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 22 Sep 2006 08:58:48 +0200
Subject: [R] Matrix from a vector?
In-Reply-To: <4512EDD0.4070401@pdf.com>
References: <B67F17BC-B990-4D91-8778-CED7060A797B@local>
Message-ID: <4513A5C8.27117.25504E@localhost>

Hi

other approach is to use embed

embed(c(rhsel,rhsel),length(rhsel))[-1,]
which is a little bit quicker

rhsel<-rnorm(1000)
n <- length(rhsel)

system.time({
i <- sapply(1:n, function(i) (1:n - i)%%n + 1)
x2<-matrix(rhsel[i], n, n)
})

system.time(x1<-embed(c(rhsel,rhsel),length(rhsel))[-1,])

all.equal(x1,x2)


HTH
Petr


On 21 Sep 2006 at 14:53, Sundar Dorai-Raj wrote:

Date sent:      	Thu, 21 Sep 2006 14:53:52 -0500
From:           	Sundar Dorai-Raj <sundar.dorai-raj at pdf.com>
Organization:   	PDF Solutions, Inc.
To:             	kone <attenka at utu.fi>
Copies to:      	r-help at stat.math.ethz.ch
Subject:        	Re: [R] Matrix from a vector?

> 
> 
> kone said the following on 9/21/2006 2:30 PM:
> > Hi,
> > 
> > Is there some function, which generates this kind of n x n -matrix 
> > from a vector?
> > 
> >  > rhset
> > [1]  1792   256 13312   512  1024  2048  8192  4096
> >  > 	m=matrix(nrow=length(rhset),ncol=length(rhset))
> >  > 	for(i in 1:length(rhset))
> > + 	{
> > + 		m[,i]=rhset
> > + 		rhset=c(rhset[length(rhset)], rhset[2:length(rhset)-1])
> > + 	}
> >  > m
> >        [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]
> > [1,]  1792  4096  8192  2048  1024   512 13312   256
> > [2,]   256  1792  4096  8192  2048  1024   512 13312
> > [3,] 13312   256  1792  4096  8192  2048  1024   512
> > [4,]   512 13312   256  1792  4096  8192  2048  1024
> > [5,]  1024   512 13312   256  1792  4096  8192  2048
> > [6,]  2048  1024   512 13312   256  1792  4096  8192
> > [7,]  8192  2048  1024   512 13312   256  1792  4096
> > [8,]  4096  8192  2048  1024   512 13312   256  1792
> > 
> > 
> > Atte Tenkanen
> > University of Turku, Finland
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented,
> > minimal, self-contained, reproducible code.
> 
> 
> Does this work for you?
> 
> rhsel <- c(1792, 256, 13312, 512, 1024, 2048, 8192, 4096)
> n <- length(rhsel)
> i <- sapply(1:n, function(i) (1:n - i)%%n + 1)
> matrix(rhsel[i], n, n)
> 
> HTH,
> 
> --sundar
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From lorenzo.isella at gmail.com  Fri Sep 22 09:23:11 2006
From: lorenzo.isella at gmail.com (Lorenzo Isella)
Date: Fri, 22 Sep 2006 09:23:11 +0200
Subject: [R] Creating Movies with R
Message-ID: <a2b3004b0609220023ic3eb836k27c363e54416359c@mail.gmail.com>

Dear All,

I'd like to know if it is possible to create animations with R.
To be specific, I attach a code I am using for my research to plot
some analytical results in 3D using the lattice package. It is not
necessary to go through the code.
Simply, it plots some 3D density profiles at two different times
selected by the user.
I wonder if it is possible to use the data generated for different
times to create something like an .avi file.

Here is the script:

rm(list=ls())
library(lattice)

# I start defining the analytical functions needed to get the density
as a function of time

expect_position <- function(t,lam1,lam2,pos_ini,vel_ini)
{1/(lam1-lam2)*(lam1*exp(lam2*t)-lam2*exp(lam1*t))*pos_ini+
1/(lam1-lam2)*(exp(lam1*t)-exp(lam2*t))*vel_ini
}

sigma_pos<-function(t,q,lam1,lam2)
{
q/(lam1-lam2)^2*(
(exp(2*lam1*t)-1)/(2*lam1)-2/(lam1+lam2)*(exp(lam1*t+lam2*t)-1) +
(exp(2*lam2*t)-1)/(2*lam2) )
}

rho_x<-function(x,expect_position,sigma_pos)
{
1/sqrt(2*pi*sigma_pos)*exp(-1/2*(x-expect_position)^2/sigma_pos)
}

#### Now the physical parameters
tau<-0.1
beta<-1/tau
St<-tau ### since I am in dimensionless units and tau is already in
units of 1/|alpha|
D=2e-2
q<-2*beta^2*D
############### Now the grid in space and time
time<-5  # time extent
tsteps<-501 # time steps
newtime<-seq(0,time,len=tsteps)
#### Now the things specific for the dynamics along x
lam1<- -beta/2*(1+sqrt(1+4*St))
lam2<- -beta/2*(1-sqrt(1+4*St))
xmin<- -0.5
xmax<-0.5
x0<-0.1
vx0<-x0
nx<-101 ## grid intervals along x
newx<-seq(xmin,xmax,len=nx) # grid along x

# M1 <- do.call("g", c(list(x = newx), mypar))


mypar<-c(q,lam1,lam2)
sig_xx<-do.call("sigma_pos",c(list(t=newtime),mypar))
mypar<-c(lam1,lam2,x0,vx0)
exp_x<-do.call("expect_position",c(list(t=newtime),mypar))

#rho_x<-function(x,expect_position,sigma_pos)

#NB: at t=0, the density blows up, since I have a delta as the initial state!
# At any t>0, instead, the result is finite.
#for this reason I now redefine time by getting rid of the istant t=0
to work out
# the density


rho_x_t<-matrix(ncol=nx,nrow=tsteps-1)
for (i in 2:tsteps)
{mypar<-c(exp_x[i],sig_xx[i])
myrho_x<-do.call("rho_x",c(list(x=newx),mypar))
rho_x_t[ i-1, ]<-myrho_x
}

### Now I also define a scaled density

rho_x_t_scaled<-matrix(ncol=nx,nrow=tsteps-1)
for (i in 2:tsteps)
{mypar<-c(exp_x[i],sig_xx[i])
myrho_x<-do.call("rho_x",c(list(x=newx),mypar))
rho_x_t_scaled[ i-1, ]<-myrho_x/max(myrho_x)
}

###########Now I deal with the dynamics along y

lam1<- -beta/2*(1+sqrt(1-4*St))
lam2<- -beta/2*(1-sqrt(1-4*St))
ymin<- 0
ymax<- 1
y0<-ymax
vy0<- -y0

mypar<-c(q,lam1,lam2)
sig_yy<-do.call("sigma_pos",c(list(t=newtime),mypar))
mypar<-c(lam1,lam2,y0,vy0)
exp_y<-do.call("expect_position",c(list(t=newtime),mypar))


# now I introduce the function giving the density along y: this has to
include the BC of zero
# density at wall

rho_y<-function(y,expect_position,sigma_pos)
{
1/sqrt(2*pi*sigma_pos)*exp(-1/2*(y-expect_position)^2/sigma_pos)-
1/sqrt(2*pi*sigma_pos)*exp(-1/2*(y+expect_position)^2/sigma_pos)
}

newy<-seq(ymin,ymax,len=nx) # grid along y with the same # of points
as the one along x


rho_y_t<-matrix(ncol=nx,nrow=tsteps-1)
for (i in 2:tsteps)
{mypar<-c(exp_y[i],sig_yy[i])
myrho_y<-do.call("rho_y",c(list(y=newy),mypar))
rho_y_t[ i-1, ]<-myrho_y
}

rho_y_t_scaled<-matrix(ncol=nx,nrow=tsteps-1)
for (i in 2:tsteps)
{mypar<-c(exp_y[i],sig_yy[i])
myrho_y<-do.call("rho_y",c(list(y=newy),mypar))
rho_y_t_scaled[ i-1, ]<-myrho_y/max(myrho_y)
}


# The following 2 plots are an example of the plots I'd like to use to
make an animation


g <- expand.grid(x = newx, y = newy)

instant<-100
mydens<-rho_x_t[ instant, ]%o%rho_y_t[ instant, ]/(max(rho_x_t[
instant, ]%o%rho_y_t[ instant, ]))


lentot<-nx^2
dim(mydens)<-c(lentot,1)

g$z<-mydens
jpeg("dens-t-3.jpeg")
print(wireframe(z ~ x * y, g, drape = TRUE,shade=TRUE,
scales = list(arrows = FALSE),pretty=FALSE, aspect = c(1,1), colorkey = TRUE
,zoom=0.8, main=expression("Density at t=2"), zlab =
list(expression("density"),rot = 90),distance=0.0,
perspective=TRUE,#screen = list(z = 150, x = -55,y= 0)
,zlim=range(c(0,1))))
dev.off()


instant<-300
mydens<-rho_x_t[ instant, ]%o%rho_y_t[ instant, ]/(max(rho_x_t[
instant, ]%o%rho_y_t[ instant, ]))


lentot<-nx^2
dim(mydens)<-c(lentot,1)

g$z<-mydens
jpeg("dens-t-3.jpeg")
print(wireframe(z ~ x * y, g, drape = TRUE,shade=TRUE,
scales = list(arrows = FALSE),pretty=FALSE, aspect = c(1,1), colorkey = TRUE
,zoom=0.8, main=expression("Density at t=3"), zlab =
list(expression("density"),rot = 90),distance=0.0,
perspective=TRUE,#screen = list(z = 150, x = -55,y= 0)
,zlim=range(c(0,1))))
dev.off()




Kind Regards

Lorenzo


From i.visser at uva.nl  Fri Sep 22 09:27:01 2006
From: i.visser at uva.nl (Ingmar Visser)
Date: Fri, 22 Sep 2006 09:27:01 +0200
Subject: [R] Exponentiate a matrix
In-Reply-To: <20060921205827.org1ssb4q30g40sk@webmail4.kuleuven.be>
Message-ID: <C1395CE5.11B61%i.visser@uva.nl>

Out of curiosity and possibly for later use: is there an R-function that
does matrix logarithms?
Best, Ingmar

On 9/21/06 8:58 PM, "Dimitrios Rizopoulos"
<Dimitris.Rizopoulos at med.kuleuven.be> wrote:

> Quoting Douglas Bates <bates at stat.wisc.edu>:
> 
>> On 9/21/06, Dimitrios Rizopoulos <Dimitris.Rizopoulos at med.kuleuven.be> wrote:
>>> 
>>> Quoting Duncan Murdoch <murdoch at stats.uwo.ca>:
>>> 
>>>> On 9/21/2006 10:40 AM, Doran, Harold wrote:
>>>>> Suppose I have a square matrix P
>>>>> 
>>>>> P <- matrix(c(.3,.7, .7, .3), ncol=2)
>>>>> 
>>>>> I know that
>>>>> 
>>>>>> P * P
>>>>> 
>>>>> Returns the element by element product, whereas
>>>>> 
>>>>>> P%*%P
>>>>> 
>>>>> Returns the matrix product.
>>>>> 
>>>>> Now, P^2 also returns the element by element product. But, is there a
>>>>> slick way to write
>>>>> 
>>>>> P %*% P %*% P
>>>>> 
>>>>> Obviously, P^3 does not return the result I expect.
>>>> 
>>>> 
>>>> I don't think there's anything built in, but it's easy to write your own:
>>> 
>>> I think there was function mtx.exp() in the Malmig package, but it
>>> seems that this package has been withdrawn from CRAN. An old version
>>> appears to exist in:
>>> 
>>> http://r.meteo.uni.wroc.pl/src/contrib/Descriptions/Malmig.html
>>> 
>>> Best,
>>> Dimitris
>> 
>> Is that function for matrix powers or for the exponential of a matrix
>> (which is what I initally thought that Harold wanted)?  There is a
>> function expm in the Matrix package, patterned on the octave function
>> of the same name, the calculates the matrix exponential for a square
>> matrix.
> 
> this function calculates the n-th power of a matrix, and this is what
> I thought Harold wanted, i.e.,
> 
> P %*% P %*% P %*% P
> 
> should be equal to
> 
> mtx.exp(P, 4)
> 
> 
>>> 
>>> 
>>>> "%^%" <- function(mat, pow) {
>>>>    stopifnot(length(pow) == 1, all.equal(pow, round(pow)), nrow(mat) ==
>>>> ncol(mat))
>>>>    pow <- round(pow)
>>>>    if (pow < 0) {
>>>>      mat <- solve(mat)
>>>>      pow <- abs(pow)
>>>>    }
>>>>    result <- diag(nrow(mat))
>>>>    while (pow > 0) {
>>>>      result <- result %*% mat
>>>>      pow <- pow - 1
>>>>    }
>>>>    result
>>>> }
>>>> 
>>>> Now P %^% 3 will give you the matrix cube.
>>>> 
>>>> Duncan Murdoch
>>>> 
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> 
>>>> 
>>> 
>>> 
>>> 
>>> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
>>> 
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> 
> 
> 
> 
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Ingmar Visser
Department of Psychology, University of Amsterdam
Roetersstraat 15, 1018 WB Amsterdam
The Netherlands
http://users.fmg.uva.nl/ivisser/
tel: +31-20-5256735


From albart.coster at wur.nl  Fri Sep 22 09:32:28 2006
From: albart.coster at wur.nl (Albart Coster)
Date: Fri, 22 Sep 2006 09:32:28 +0200
Subject: [R] Extracting and using Lattice x or y axis
Message-ID: <1158910349.2926.12.camel@localhost.localdomain>

Dear list,

I am using Lattice graphics together with Grid. In order to get
completely good results, I would like to copy the X and Y axes from the
Lattice graphic and draw these in another Grid Viewport. Is there a way
to achieve this? 

I would be gratefull for your help,

Albart Coster


From mothsailor at googlemail.com  Fri Sep 22 09:33:25 2006
From: mothsailor at googlemail.com (David Barron)
Date: Fri, 22 Sep 2006 08:33:25 +0100
Subject: [R] Creating Movies with R
In-Reply-To: <a2b3004b0609220023ic3eb836k27c363e54416359c@mail.gmail.com>
References: <a2b3004b0609220023ic3eb836k27c363e54416359c@mail.gmail.com>
Message-ID: <815b70590609220033n7b408b45n16ce9d9d410f6f3a@mail.gmail.com>

You can make animated gifs using the write.gif function in the caTools package.

On 22/09/06, Lorenzo Isella <lorenzo.isella at gmail.com> wrote:
> Dear All,
>
> I'd like to know if it is possible to create animations with R.
> To be specific, I attach a code I am using for my research to plot
> some analytical results in 3D using the lattice package. It is not
> necessary to go through the code.
> Simply, it plots some 3D density profiles at two different times
> selected by the user.
> I wonder if it is possible to use the data generated for different
> times to create something like an .avi file.
>
> Here is the script:
>
> rm(list=ls())
> library(lattice)
>
> # I start defining the analytical functions needed to get the density
> as a function of time
>
> expect_position <- function(t,lam1,lam2,pos_ini,vel_ini)
> {1/(lam1-lam2)*(lam1*exp(lam2*t)-lam2*exp(lam1*t))*pos_ini+
> 1/(lam1-lam2)*(exp(lam1*t)-exp(lam2*t))*vel_ini
> }
>
> sigma_pos<-function(t,q,lam1,lam2)
> {
> q/(lam1-lam2)^2*(
> (exp(2*lam1*t)-1)/(2*lam1)-2/(lam1+lam2)*(exp(lam1*t+lam2*t)-1) +
> (exp(2*lam2*t)-1)/(2*lam2) )
> }
>
> rho_x<-function(x,expect_position,sigma_pos)
> {
> 1/sqrt(2*pi*sigma_pos)*exp(-1/2*(x-expect_position)^2/sigma_pos)
> }
>
> #### Now the physical parameters
> tau<-0.1
> beta<-1/tau
> St<-tau ### since I am in dimensionless units and tau is already in
> units of 1/|alpha|
> D=2e-2
> q<-2*beta^2*D
> ############### Now the grid in space and time
> time<-5  # time extent
> tsteps<-501 # time steps
> newtime<-seq(0,time,len=tsteps)
> #### Now the things specific for the dynamics along x
> lam1<- -beta/2*(1+sqrt(1+4*St))
> lam2<- -beta/2*(1-sqrt(1+4*St))
> xmin<- -0.5
> xmax<-0.5
> x0<-0.1
> vx0<-x0
> nx<-101 ## grid intervals along x
> newx<-seq(xmin,xmax,len=nx) # grid along x
>
> # M1 <- do.call("g", c(list(x = newx), mypar))
>
>
> mypar<-c(q,lam1,lam2)
> sig_xx<-do.call("sigma_pos",c(list(t=newtime),mypar))
> mypar<-c(lam1,lam2,x0,vx0)
> exp_x<-do.call("expect_position",c(list(t=newtime),mypar))
>
> #rho_x<-function(x,expect_position,sigma_pos)
>
> #NB: at t=0, the density blows up, since I have a delta as the initial state!
> # At any t>0, instead, the result is finite.
> #for this reason I now redefine time by getting rid of the istant t=0
> to work out
> # the density
>
>
> rho_x_t<-matrix(ncol=nx,nrow=tsteps-1)
> for (i in 2:tsteps)
> {mypar<-c(exp_x[i],sig_xx[i])
> myrho_x<-do.call("rho_x",c(list(x=newx),mypar))
> rho_x_t[ i-1, ]<-myrho_x
> }
>
> ### Now I also define a scaled density
>
> rho_x_t_scaled<-matrix(ncol=nx,nrow=tsteps-1)
> for (i in 2:tsteps)
> {mypar<-c(exp_x[i],sig_xx[i])
> myrho_x<-do.call("rho_x",c(list(x=newx),mypar))
> rho_x_t_scaled[ i-1, ]<-myrho_x/max(myrho_x)
> }
>
> ###########Now I deal with the dynamics along y
>
> lam1<- -beta/2*(1+sqrt(1-4*St))
> lam2<- -beta/2*(1-sqrt(1-4*St))
> ymin<- 0
> ymax<- 1
> y0<-ymax
> vy0<- -y0
>
> mypar<-c(q,lam1,lam2)
> sig_yy<-do.call("sigma_pos",c(list(t=newtime),mypar))
> mypar<-c(lam1,lam2,y0,vy0)
> exp_y<-do.call("expect_position",c(list(t=newtime),mypar))
>
>
> # now I introduce the function giving the density along y: this has to
> include the BC of zero
> # density at wall
>
> rho_y<-function(y,expect_position,sigma_pos)
> {
> 1/sqrt(2*pi*sigma_pos)*exp(-1/2*(y-expect_position)^2/sigma_pos)-
> 1/sqrt(2*pi*sigma_pos)*exp(-1/2*(y+expect_position)^2/sigma_pos)
> }
>
> newy<-seq(ymin,ymax,len=nx) # grid along y with the same # of points
> as the one along x
>
>
> rho_y_t<-matrix(ncol=nx,nrow=tsteps-1)
> for (i in 2:tsteps)
> {mypar<-c(exp_y[i],sig_yy[i])
> myrho_y<-do.call("rho_y",c(list(y=newy),mypar))
> rho_y_t[ i-1, ]<-myrho_y
> }
>
> rho_y_t_scaled<-matrix(ncol=nx,nrow=tsteps-1)
> for (i in 2:tsteps)
> {mypar<-c(exp_y[i],sig_yy[i])
> myrho_y<-do.call("rho_y",c(list(y=newy),mypar))
> rho_y_t_scaled[ i-1, ]<-myrho_y/max(myrho_y)
> }
>
>
> # The following 2 plots are an example of the plots I'd like to use to
> make an animation
>
>
> g <- expand.grid(x = newx, y = newy)
>
> instant<-100
> mydens<-rho_x_t[ instant, ]%o%rho_y_t[ instant, ]/(max(rho_x_t[
> instant, ]%o%rho_y_t[ instant, ]))
>
>
> lentot<-nx^2
> dim(mydens)<-c(lentot,1)
>
> g$z<-mydens
> jpeg("dens-t-3.jpeg")
> print(wireframe(z ~ x * y, g, drape = TRUE,shade=TRUE,
> scales = list(arrows = FALSE),pretty=FALSE, aspect = c(1,1), colorkey = TRUE
> ,zoom=0.8, main=expression("Density at t=2"), zlab =
> list(expression("density"),rot = 90),distance=0.0,
> perspective=TRUE,#screen = list(z = 150, x = -55,y= 0)
> ,zlim=range(c(0,1))))
> dev.off()
>
>
> instant<-300
> mydens<-rho_x_t[ instant, ]%o%rho_y_t[ instant, ]/(max(rho_x_t[
> instant, ]%o%rho_y_t[ instant, ]))
>
>
> lentot<-nx^2
> dim(mydens)<-c(lentot,1)
>
> g$z<-mydens
> jpeg("dens-t-3.jpeg")
> print(wireframe(z ~ x * y, g, drape = TRUE,shade=TRUE,
> scales = list(arrows = FALSE),pretty=FALSE, aspect = c(1,1), colorkey = TRUE
> ,zoom=0.8, main=expression("Density at t=3"), zlab =
> list(expression("density"),rot = 90),distance=0.0,
> perspective=TRUE,#screen = list(z = 150, x = -55,y= 0)
> ,zlim=range(c(0,1))))
> dev.off()
>
>
>
>
> Kind Regards
>
> Lorenzo
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From kaniovsk at wifo.ac.at  Fri Sep 22 10:44:38 2006
From: kaniovsk at wifo.ac.at (Serguei Kaniovski)
Date: Fri, 22 Sep 2006 10:44:38 +0200
Subject: [R] Compiling a contingency table of counts by case
Message-ID: <4513A276.7030802@wifo.ac.at>

I have asked a similar question before but this time
the problem is somewhat more involved. I have the
following data:

case;name;x
1;Joe;1
1;Mike;1
1;Zoe;1
2;Joe;1
2;Mike;0
2;Zoe;1
2;John;1
3;Mike;1
3;Zoe;0
3;Karl;0

I would like to count the number of "case"
in which any two "name"

a. both have "x=1",
b. the first has "x=0" - the second has "x=1",
c. the first has "x=1" - the second has "x=0",
d. both have "x=0",

The difficulty is that the number of "names" and their
identity changes from case to case.

Thanks a lot for you help,
Serguei Kaniovski
-- 
___________________________________________________________________

Austrian Institute of Economic Research (WIFO)

Name: Serguei Kaniovski			P.O.Box 91
Tel.: +43-1-7982601-231			Arsenal Objekt 20
Fax:  +43-1-7989386			1103 Vienna, Austria
Mail: Serguei.Kaniovski at wifo.ac.at

http://www.wifo.ac.at/Serguei.Kaniovski


From ligges at statistik.uni-dortmund.de  Fri Sep 22 10:47:02 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 22 Sep 2006 10:47:02 +0200
Subject: [R] 'help' information not modified when I modify man files
In-Reply-To: <73bcebe0609211406i11a22627j828ea0ef82187ad4@mail.gmail.com>
References: <73bcebe0609211406i11a22627j828ea0ef82187ad4@mail.gmail.com>
Message-ID: <4513A306.90307@statistik.uni-dortmund.de>



John Tillinghast wrote:
> I am updating the Bioconductor package, LMGene. Thus I am modifying someone
> else's package, editing or writing new documentation, etc.
> 
> When I modify the man files in LMGene  and install the library, it doesn't
> change the 'help' that R gives you. The 'help' you actually get in R is the
> same as before.
> 
> I haven't been able to find an explanation in "Writing R Extensions". The
> package I'm working with also contains a 'help' directory, which is not
> mentioned in WRE.
> 
> When installing a package, how do I make sure that the 'man' files get used
> to generate the 'help' files?


The files in ./man are used automatically.
Are you sure you are using the modified version of package LMGene rather 
than the old one (perhaps installed into a different library?)?

Uwe Ligges


> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From suprtova at ucla.edu  Fri Sep 22 11:09:26 2006
From: suprtova at ucla.edu (Tova Fuller)
Date: Fri, 22 Sep 2006 02:09:26 -0700
Subject: [R] Merge problem
Message-ID: <666CBEFA-ABD7-4477-9CCF-97EBF8B88DF7@ucla.edu>

Hello all,

I have read as many merge issues as I possibly could tonight and  
although I presume this is a small error, I have not found the  
solution to my problem.

I'm trying to merge two data sets: dat0 and TransTable.  As you can  
see below, dat0 has 8000 rows, whereas TransTable has 47296 rows.  I  
would expect when I merge the two data sets, with all.x=F, and  
all.y=F, that the intersection would yield 8000 rows, considering  
dat0 is a subset of TransTable.

However, I get a neat little surprise when I check the dimensions of  
the resultant data frame - dat0merge, the merged data frame has 8007  
rows!  How can this be?  Where did these extra 7 rows come from?   
This appears to defy logic!

Thank you in advance for your help.  I've put my code below for  
reference.

Tova Fuller

 > dim(dat0)
[1] 8000   60
 > dim(TransTable)
[1] 47296     9
 > dat0merge=merge(TransTable,dat0,  
by.x="Target",by.y="TargetID",all.x=F,all.y=F)
 > dim(dat0merge)
[1] 8007   68


From jacques.veslot at good.ibl.fr  Fri Sep 22 11:32:14 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Fri, 22 Sep 2006 11:32:14 +0200
Subject: [R] Compiling a contingency table of counts by case
In-Reply-To: <4513A276.7030802@wifo.ac.at>
References: <4513A276.7030802@wifo.ac.at>
Message-ID: <4513AD9E.1090707@good.ibl.fr>

 > dat <- read.delim("clipboard", sep=";")
 > dat <- dat[order(dat$case, dat$name), ]

 > res <- apply(combinations(nlevels(dat$name), 2), 1, function(x) with(dat[dat$name %in% 
levels(dat$name)[x],], table(unlist(sapply(split(x, case), function(y) ifelse(length(y) == 2, 
paste(y, collapse=""), NA))))))

 > names(res) <- apply(combinations(nlevels(dat$name), 2), 1, function(x) paste(levels(dat$name)[x], 
collapse="."))

 > res
$Joe.John

11
  1

$Joe.Karl
character(0)

$Joe.Mike

10 11
  1  1

$Joe.Zoe

11
  2

$John.Karl
character(0)

$John.Mike

10
  1

$John.Zoe

11
  1

$Karl.Mike

01
  1

$Karl.Zoe

00
  1

$Mike.Zoe

01 10 11
  1  1  1

-------------------------------------------------------------------
Jacques VESLOT

CNRS UMR 8090
I.B.L (2?me ?tage)
1 rue du Professeur Calmette
B.P. 245
59019 Lille Cedex

Tel : 33 (0)3.20.87.10.44
Fax : 33 (0)3.20.87.10.31

http://www-good.ibl.fr
-------------------------------------------------------------------

Serguei Kaniovski a ?crit :
> I have asked a similar question before but this time
> the problem is somewhat more involved. I have the
> following data:
> 
> case;name;x
> 1;Joe;1
> 1;Mike;1
> 1;Zoe;1
> 2;Joe;1
> 2;Mike;0
> 2;Zoe;1
> 2;John;1
> 3;Mike;1
> 3;Zoe;0
> 3;Karl;0
> 
> I would like to count the number of "case"
> in which any two "name"
> 
> a. both have "x=1",
> b. the first has "x=0" - the second has "x=1",
> c. the first has "x=1" - the second has "x=0",
> d. both have "x=0",
> 
> The difficulty is that the number of "names" and their
> identity changes from case to case.
> 
> Thanks a lot for you help,
> Serguei Kaniovski


From buser at stat.math.ethz.ch  Fri Sep 22 11:36:09 2006
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Fri, 22 Sep 2006 11:36:09 +0200
Subject: [R] Merge problem
In-Reply-To: <666CBEFA-ABD7-4477-9CCF-97EBF8B88DF7@ucla.edu>
References: <666CBEFA-ABD7-4477-9CCF-97EBF8B88DF7@ucla.edu>
Message-ID: <17683.44681.439951.453835@stat.math.ethz.ch>

Dear Tova

There is no reason why the merged data.frame should have
exactely 8000 or less rows.

The "all=FALSE" options only says that now new rows are
generated for cases that appear only in one of the two
data.frames.

Have a look at this sample

> dat1 <- data.frame(a = c(1,2,3,4), b = letters[1:4])
> dat2 <- data.frame(a = c(1,2,3,4,5,6,7,8,1), b = LETTERS[1:9])

> merge(dat1,dat2, by = "a", all = FALSE)
1 1    a   A
2 1    a   I
3 2    b   B
4 3    c   C
5 4    d   D


Since "1" appears twice in the large data.frame it is repeated
as the help page ?merge says:

"If there is more than one match, all possible  matches
contribute one row each."

To compare have a look what the option "all = TRUE" changes

> merge(dat1,dat2, by = "a", all = TRUE)

Probably in your large data frame some rows have identical
target ids and get repeated. It should be easy to check it with
unique()

Hope this helps

Christoph

--------------------------------------------------------------

Credit and Surety PML study: visit our web page www.cs-pml.org

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH Zurich	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------

Tova Fuller writes:
 > Hello all,
 > 
 > I have read as many merge issues as I possibly could tonight and  
 > although I presume this is a small error, I have not found the  
 > solution to my problem.
 > 
 > I'm trying to merge two data sets: dat0 and TransTable.  As you can  
 > see below, dat0 has 8000 rows, whereas TransTable has 47296 rows.  I  
 > would expect when I merge the two data sets, with all.x=F, and  
 > all.y=F, that the intersection would yield 8000 rows, considering  
 > dat0 is a subset of TransTable.
 > 
 > However, I get a neat little surprise when I check the dimensions of  
 > the resultant data frame - dat0merge, the merged data frame has 8007  
 > rows!  How can this be?  Where did these extra 7 rows come from?   
 > This appears to defy logic!
 > 
 > Thank you in advance for your help.  I've put my code below for  
 > reference.
 > 
 > Tova Fuller
 > 
 >  > dim(dat0)
 > [1] 8000   60
 >  > dim(TransTable)
 > [1] 47296     9
 >  > dat0merge=merge(TransTable,dat0,  
 > by.x="Target",by.y="TargetID",all.x=F,all.y=F)
 >  > dim(dat0merge)
 > [1] 8007   68
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
 > and provide commented, minimal, self-contained, reproducible code.


From ripley at stats.ox.ac.uk  Fri Sep 22 12:02:48 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 22 Sep 2006 11:02:48 +0100 (BST)
Subject: [R] Merge problem
In-Reply-To: <666CBEFA-ABD7-4477-9CCF-97EBF8B88DF7@ucla.edu>
References: <666CBEFA-ABD7-4477-9CCF-97EBF8B88DF7@ucla.edu>
Message-ID: <Pine.LNX.4.64.0609221058080.22898@gannet.stats.ox.ac.uk>

On Fri, 22 Sep 2006, Tova Fuller wrote:

> Hello all,
>
> I have read as many merge issues as I possibly could tonight and
> although I presume this is a small error, I have not found the
> solution to my problem.
>
> I'm trying to merge two data sets: dat0 and TransTable.  As you can
> see below, dat0 has 8000 rows, whereas TransTable has 47296 rows.  I
> would expect when I merge the two data sets, with all.x=F, and
> all.y=F, that the intersection would yield 8000 rows, considering
> dat0 is a subset of TransTable.
>
> However, I get a neat little surprise when I check the dimensions of
> the resultant data frame - dat0merge, the merged data frame has 8007
> rows!  How can this be?  Where did these extra 7 rows come from?
> This appears to defy logic!

Not the help page, though.

      joined together.  If there is more than one match, all possible
      matches contribute one row each.

I presume you think you are doing a 1:1 match, but probably you have 
multiple matches for up to 7 ids.

This is one of the commonest misconceptions about merge that you will find 
frequently in the list archives.

>
> Thank you in advance for your help.  I've put my code below for
> reference.
>
> Tova Fuller
>
> > dim(dat0)
> [1] 8000   60
> > dim(TransTable)
> [1] 47296     9
> > dat0merge=merge(TransTable,dat0,
> by.x="Target",by.y="TargetID",all.x=F,all.y=F)
> > dim(dat0merge)
> [1] 8007   68
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From czucz at nimbus.elte.hu  Fri Sep 22 12:21:02 2006
From: czucz at nimbus.elte.hu (=?ISO-8859-1?Q?B=E1lint_Cz=FAcz?=)
Date: Fri, 22 Sep 2006 12:21:02 +0200
Subject: [R] how to store recursive results
In-Reply-To: <BAY23-F139B9DF4D701209ACD94DDFA210@phx.gbl>
References: <BAY23-F139B9DF4D701209ACD94DDFA210@phx.gbl>
Message-ID: <fab4bcf70609220321j535ac92cq7972c09d818785b3@mail.gmail.com>

One suggestion:

as a recursive list. For example have a look at the 'party' package
and the BinaryTree class.
I hope this helps.

B?lint


On 22/09/06, X.H Chen <xchen_stat at hotmail.com> wrote:
> Hi all,
>
> How to store recursive resutls from a function for each step without using
> global operators <<-? Thanks ahead.
>
> Xiaohui Chen
>
> Dept. of Statistics
> UBC, Canada
>
> _________________________________________________________________
> Don't waste time standing in line?try shopping online. Visit Sympatico / MSN
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


From ggrothendieck at gmail.com  Fri Sep 22 12:49:22 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 22 Sep 2006 06:49:22 -0400
Subject: [R] how to store recursive results
In-Reply-To: <BAY23-F139B9DF4D701209ACD94DDFA210@phx.gbl>
References: <BAY23-F139B9DF4D701209ACD94DDFA210@phx.gbl>
Message-ID: <971536df0609220349u1bde0bfer42a575545c3be43d@mail.gmail.com>

Note that <<- is not necessarily global:

if (exists("x")) rm(x)
f <- function() {
	x <- 2
	g <- function() x <<- 3
	g()
	x
}
f() # 3
exists("x") # FALSE

On 9/22/06, X.H Chen <xchen_stat at hotmail.com> wrote:
> Hi all,
>
> How to store recursive resutls from a function for each step without using
> global operators <<-? Thanks ahead.
>
> Xiaohui Chen
>
> Dept. of Statistics
> UBC, Canada
>
> _________________________________________________________________
> Don't waste time standing in line?try shopping online. Visit Sympatico / MSN
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


From murdoch at stats.uwo.ca  Fri Sep 22 12:54:45 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 22 Sep 2006 06:54:45 -0400
Subject: [R] Creating Movies with R
In-Reply-To: <a2b3004b0609220023ic3eb836k27c363e54416359c@mail.gmail.com>
References: <a2b3004b0609220023ic3eb836k27c363e54416359c@mail.gmail.com>
Message-ID: <4513C0F5.8000305@stats.uwo.ca>

On 9/22/2006 3:23 AM, Lorenzo Isella wrote:
> Dear All,
> 
> I'd like to know if it is possible to create animations with R.
> To be specific, I attach a code I am using for my research to plot
> some analytical results in 3D using the lattice package. It is not
> necessary to go through the code.
> Simply, it plots some 3D density profiles at two different times
> selected by the user.
> I wonder if it is possible to use the data generated for different
> times to create something like an .avi file.

There's an example of an animation in the rgl package (see 
?rgl.snapshot); it needs ImageMagick to put the frames together.  It 
supports GIF; I think it also supports MPEG and maybe some other 
animated formats.

Duncan Murdoch

> 
> Here is the script:
> 
> rm(list=ls())
> library(lattice)
> 
> # I start defining the analytical functions needed to get the density
> as a function of time
> 
> expect_position <- function(t,lam1,lam2,pos_ini,vel_ini)
> {1/(lam1-lam2)*(lam1*exp(lam2*t)-lam2*exp(lam1*t))*pos_ini+
> 1/(lam1-lam2)*(exp(lam1*t)-exp(lam2*t))*vel_ini
> }
> 
> sigma_pos<-function(t,q,lam1,lam2)
> {
> q/(lam1-lam2)^2*(
> (exp(2*lam1*t)-1)/(2*lam1)-2/(lam1+lam2)*(exp(lam1*t+lam2*t)-1) +
> (exp(2*lam2*t)-1)/(2*lam2) )
> }
> 
> rho_x<-function(x,expect_position,sigma_pos)
> {
> 1/sqrt(2*pi*sigma_pos)*exp(-1/2*(x-expect_position)^2/sigma_pos)
> }
> 
> #### Now the physical parameters
> tau<-0.1
> beta<-1/tau
> St<-tau ### since I am in dimensionless units and tau is already in
> units of 1/|alpha|
> D=2e-2
> q<-2*beta^2*D
> ############### Now the grid in space and time
> time<-5  # time extent
> tsteps<-501 # time steps
> newtime<-seq(0,time,len=tsteps)
> #### Now the things specific for the dynamics along x
> lam1<- -beta/2*(1+sqrt(1+4*St))
> lam2<- -beta/2*(1-sqrt(1+4*St))
> xmin<- -0.5
> xmax<-0.5
> x0<-0.1
> vx0<-x0
> nx<-101 ## grid intervals along x
> newx<-seq(xmin,xmax,len=nx) # grid along x
> 
> # M1 <- do.call("g", c(list(x = newx), mypar))
> 
> 
> mypar<-c(q,lam1,lam2)
> sig_xx<-do.call("sigma_pos",c(list(t=newtime),mypar))
> mypar<-c(lam1,lam2,x0,vx0)
> exp_x<-do.call("expect_position",c(list(t=newtime),mypar))
> 
> #rho_x<-function(x,expect_position,sigma_pos)
> 
> #NB: at t=0, the density blows up, since I have a delta as the initial state!
> # At any t>0, instead, the result is finite.
> #for this reason I now redefine time by getting rid of the istant t=0
> to work out
> # the density
> 
> 
> rho_x_t<-matrix(ncol=nx,nrow=tsteps-1)
> for (i in 2:tsteps)
> {mypar<-c(exp_x[i],sig_xx[i])
> myrho_x<-do.call("rho_x",c(list(x=newx),mypar))
> rho_x_t[ i-1, ]<-myrho_x
> }
> 
> ### Now I also define a scaled density
> 
> rho_x_t_scaled<-matrix(ncol=nx,nrow=tsteps-1)
> for (i in 2:tsteps)
> {mypar<-c(exp_x[i],sig_xx[i])
> myrho_x<-do.call("rho_x",c(list(x=newx),mypar))
> rho_x_t_scaled[ i-1, ]<-myrho_x/max(myrho_x)
> }
> 
> ###########Now I deal with the dynamics along y
> 
> lam1<- -beta/2*(1+sqrt(1-4*St))
> lam2<- -beta/2*(1-sqrt(1-4*St))
> ymin<- 0
> ymax<- 1
> y0<-ymax
> vy0<- -y0
> 
> mypar<-c(q,lam1,lam2)
> sig_yy<-do.call("sigma_pos",c(list(t=newtime),mypar))
> mypar<-c(lam1,lam2,y0,vy0)
> exp_y<-do.call("expect_position",c(list(t=newtime),mypar))
> 
> 
> # now I introduce the function giving the density along y: this has to
> include the BC of zero
> # density at wall
> 
> rho_y<-function(y,expect_position,sigma_pos)
> {
> 1/sqrt(2*pi*sigma_pos)*exp(-1/2*(y-expect_position)^2/sigma_pos)-
> 1/sqrt(2*pi*sigma_pos)*exp(-1/2*(y+expect_position)^2/sigma_pos)
> }
> 
> newy<-seq(ymin,ymax,len=nx) # grid along y with the same # of points
> as the one along x
> 
> 
> rho_y_t<-matrix(ncol=nx,nrow=tsteps-1)
> for (i in 2:tsteps)
> {mypar<-c(exp_y[i],sig_yy[i])
> myrho_y<-do.call("rho_y",c(list(y=newy),mypar))
> rho_y_t[ i-1, ]<-myrho_y
> }
> 
> rho_y_t_scaled<-matrix(ncol=nx,nrow=tsteps-1)
> for (i in 2:tsteps)
> {mypar<-c(exp_y[i],sig_yy[i])
> myrho_y<-do.call("rho_y",c(list(y=newy),mypar))
> rho_y_t_scaled[ i-1, ]<-myrho_y/max(myrho_y)
> }
> 
> 
> # The following 2 plots are an example of the plots I'd like to use to
> make an animation
> 
> 
> g <- expand.grid(x = newx, y = newy)
> 
> instant<-100
> mydens<-rho_x_t[ instant, ]%o%rho_y_t[ instant, ]/(max(rho_x_t[
> instant, ]%o%rho_y_t[ instant, ]))
> 
> 
> lentot<-nx^2
> dim(mydens)<-c(lentot,1)
> 
> g$z<-mydens
> jpeg("dens-t-3.jpeg")
> print(wireframe(z ~ x * y, g, drape = TRUE,shade=TRUE,
> scales = list(arrows = FALSE),pretty=FALSE, aspect = c(1,1), colorkey = TRUE
> ,zoom=0.8, main=expression("Density at t=2"), zlab =
> list(expression("density"),rot = 90),distance=0.0,
> perspective=TRUE,#screen = list(z = 150, x = -55,y= 0)
> ,zlim=range(c(0,1))))
> dev.off()
> 
> 
> instant<-300
> mydens<-rho_x_t[ instant, ]%o%rho_y_t[ instant, ]/(max(rho_x_t[
> instant, ]%o%rho_y_t[ instant, ]))
> 
> 
> lentot<-nx^2
> dim(mydens)<-c(lentot,1)
> 
> g$z<-mydens
> jpeg("dens-t-3.jpeg")
> print(wireframe(z ~ x * y, g, drape = TRUE,shade=TRUE,
> scales = list(arrows = FALSE),pretty=FALSE, aspect = c(1,1), colorkey = TRUE
> ,zoom=0.8, main=expression("Density at t=3"), zlab =
> list(expression("density"),rot = 90),distance=0.0,
> perspective=TRUE,#screen = list(z = 150, x = -55,y= 0)
> ,zlim=range(c(0,1))))
> dev.off()
> 
> 
> 
> 
> Kind Regards
> 
> Lorenzo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jlvw at na.rau.ac.za  Fri Sep 22 12:57:09 2006
From: jlvw at na.rau.ac.za (Jacob van Wyk)
Date: Fri, 22 Sep 2006 12:57:09 +0200
Subject: [R] A simple resampling problem
Message-ID: <4513DDA5020000DE0000A53C@rauzen.rau.ac.za>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060922/28cc0cc7/attachment.pl 

From Matthias.Templ at statistik.gv.at  Fri Sep 22 13:00:21 2006
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Fri, 22 Sep 2006 13:00:21 +0200
Subject: [R] Creating Movies with R
Message-ID: <83536658864BC243BE3C06D7E936ABD5047574E8@xchg1.statistik.local>

There is also a write.gif function in package caTools.
See the example there.

Best, 
Matthias

> 
> Dear All,
> 
> I'd like to know if it is possible to create animations with R.
> To be specific, I attach a code I am using for my research to 
> plot some analytical results in 3D using the lattice package. 
> It is not necessary to go through the code.
> Simply, it plots some 3D density profiles at two different 
> times selected by the user.
> I wonder if it is possible to use the data generated for 
> different times to create something like an .avi file.
> 
> Here is the script:
> 
> rm(list=ls())
> library(lattice)
> 
> # I start defining the analytical functions needed to get the 
> density as a function of time
> 
> expect_position <- function(t,lam1,lam2,pos_ini,vel_ini)
> {1/(lam1-lam2)*(lam1*exp(lam2*t)-lam2*exp(lam1*t))*pos_ini+
> 1/(lam1-lam2)*(exp(lam1*t)-exp(lam2*t))*vel_ini
> }
> 
> sigma_pos<-function(t,q,lam1,lam2)
> {
> q/(lam1-lam2)^2*(
> (exp(2*lam1*t)-1)/(2*lam1)-2/(lam1+lam2)*(exp(lam1*t+lam2*t)-1) +
> (exp(2*lam2*t)-1)/(2*lam2) )
> }
> 
> rho_x<-function(x,expect_position,sigma_pos)
> {
> 1/sqrt(2*pi*sigma_pos)*exp(-1/2*(x-expect_position)^2/sigma_pos)
> }
> 
> #### Now the physical parameters
> tau<-0.1
> beta<-1/tau
> St<-tau ### since I am in dimensionless units and tau is 
> already in units of 1/|alpha|
> D=2e-2
> q<-2*beta^2*D
> ############### Now the grid in space and time
> time<-5  # time extent
> tsteps<-501 # time steps
> newtime<-seq(0,time,len=tsteps)
> #### Now the things specific for the dynamics along x
> lam1<- -beta/2*(1+sqrt(1+4*St))
> lam2<- -beta/2*(1-sqrt(1+4*St))
> xmin<- -0.5
> xmax<-0.5
> x0<-0.1
> vx0<-x0
> nx<-101 ## grid intervals along x
> newx<-seq(xmin,xmax,len=nx) # grid along x
> 
> # M1 <- do.call("g", c(list(x = newx), mypar))
> 
> 
> mypar<-c(q,lam1,lam2)
> sig_xx<-do.call("sigma_pos",c(list(t=newtime),mypar))
> mypar<-c(lam1,lam2,x0,vx0)
> exp_x<-do.call("expect_position",c(list(t=newtime),mypar))
> 
> #rho_x<-function(x,expect_position,sigma_pos)
> 
> #NB: at t=0, the density blows up, since I have a delta as 
> the initial state!
> # At any t>0, instead, the result is finite.
> #for this reason I now redefine time by getting rid of the 
> istant t=0 to work out # the density
> 
> 
> rho_x_t<-matrix(ncol=nx,nrow=tsteps-1)
> for (i in 2:tsteps)
> {mypar<-c(exp_x[i],sig_xx[i])
> myrho_x<-do.call("rho_x",c(list(x=newx),mypar))
> rho_x_t[ i-1, ]<-myrho_x
> }
> 
> ### Now I also define a scaled density
> 
> rho_x_t_scaled<-matrix(ncol=nx,nrow=tsteps-1)
> for (i in 2:tsteps)
> {mypar<-c(exp_x[i],sig_xx[i])
> myrho_x<-do.call("rho_x",c(list(x=newx),mypar))
> rho_x_t_scaled[ i-1, ]<-myrho_x/max(myrho_x) }
> 
> ###########Now I deal with the dynamics along y
> 
> lam1<- -beta/2*(1+sqrt(1-4*St))
> lam2<- -beta/2*(1-sqrt(1-4*St))
> ymin<- 0
> ymax<- 1
> y0<-ymax
> vy0<- -y0
> 
> mypar<-c(q,lam1,lam2)
> sig_yy<-do.call("sigma_pos",c(list(t=newtime),mypar))
> mypar<-c(lam1,lam2,y0,vy0)
> exp_y<-do.call("expect_position",c(list(t=newtime),mypar))
> 
> 
> # now I introduce the function giving the density along y: 
> this has to include the BC of zero # density at wall
> 
> rho_y<-function(y,expect_position,sigma_pos)
> {
> 1/sqrt(2*pi*sigma_pos)*exp(-1/2*(y-expect_position)^2/sigma_pos)-
> 1/sqrt(2*pi*sigma_pos)*exp(-1/2*(y+expect_position)^2/sigma_pos)
> }
> 
> newy<-seq(ymin,ymax,len=nx) # grid along y with the same # of 
> points as the one along x
> 
> 
> rho_y_t<-matrix(ncol=nx,nrow=tsteps-1)
> for (i in 2:tsteps)
> {mypar<-c(exp_y[i],sig_yy[i])
> myrho_y<-do.call("rho_y",c(list(y=newy),mypar))
> rho_y_t[ i-1, ]<-myrho_y
> }
> 
> rho_y_t_scaled<-matrix(ncol=nx,nrow=tsteps-1)
> for (i in 2:tsteps)
> {mypar<-c(exp_y[i],sig_yy[i])
> myrho_y<-do.call("rho_y",c(list(y=newy),mypar))
> rho_y_t_scaled[ i-1, ]<-myrho_y/max(myrho_y) }
> 
> 
> # The following 2 plots are an example of the plots I'd like 
> to use to make an animation
> 
> 
> g <- expand.grid(x = newx, y = newy)
> 
> instant<-100
> mydens<-rho_x_t[ instant, ]%o%rho_y_t[ instant, 
> ]/(max(rho_x_t[ instant, ]%o%rho_y_t[ instant, ]))
> 
> 
> lentot<-nx^2
> dim(mydens)<-c(lentot,1)
> 
> g$z<-mydens
> jpeg("dens-t-3.jpeg")
> print(wireframe(z ~ x * y, g, drape = TRUE,shade=TRUE, scales 
> = list(arrows = FALSE),pretty=FALSE, aspect = c(1,1), 
> colorkey = TRUE ,zoom=0.8, main=expression("Density at t=2"), 
> zlab = list(expression("density"),rot = 90),distance=0.0, 
> perspective=TRUE,#screen = list(z = 150, x = -55,y= 0)
> ,zlim=range(c(0,1))))
> dev.off()
> 
> 
> instant<-300
> mydens<-rho_x_t[ instant, ]%o%rho_y_t[ instant, 
> ]/(max(rho_x_t[ instant, ]%o%rho_y_t[ instant, ]))
> 
> 
> lentot<-nx^2
> dim(mydens)<-c(lentot,1)
> 
> g$z<-mydens
> jpeg("dens-t-3.jpeg")
> print(wireframe(z ~ x * y, g, drape = TRUE,shade=TRUE, scales 
> = list(arrows = FALSE),pretty=FALSE, aspect = c(1,1), 
> colorkey = TRUE ,zoom=0.8, main=expression("Density at t=3"), 
> zlab = list(expression("density"),rot = 90),distance=0.0, 
> perspective=TRUE,#screen = list(z = 150, x = -55,y= 0)
> ,zlim=range(c(0,1))))
> dev.off()
> 
> 
> 
> 
> Kind Regards
> 
> Lorenzo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From adi at roda.ro  Fri Sep 22 13:18:11 2006
From: adi at roda.ro (Adrian DUSA)
Date: Fri, 22 Sep 2006 14:18:11 +0300
Subject: [R] delete a entire vector of a dataframe
In-Reply-To: <1158866890.2871.16.camel@dhcppc2.my.nat.localnet>
References: <4512D37B.9040100@slf.ch>
	<1158866890.2871.16.camel@dhcppc2.my.nat.localnet>
Message-ID: <200609221418.11956.adi@roda.ro>

This works too:
t.d$V712 <- NULL

On Thursday 21 September 2006 22:28, Gavin Simpson wrote:
> On Thu, 2006-09-21 at 20:01 +0200, Thomas Preuth wrote:
> > delete a entire vector of a dataframe
> >
> > Hello,
> >
> > i want to delete a vector and tried "rm (t.d$V712)". This did not work,
> > message was, could not find variable. I thought the $ defines the vectro
> > in a dataframe, when I just type "t.d$V712" the content of this vector
> > is displayed.
> >
> > Greetings, Thomas
>
> You can't do that, and that is not what the error message said exactly -
> which should have told you something was wrong with your thinking as it
> also said "1: remove: variable "$" was not found". Instead, copy over
> the object, minus the column you want to delete:
>
> dat <- as.data.frame(matrix(rnorm(100), nrow = 10))
> names(dat) <- paste("Var", 1:10, sep = "_")
> dat
> # now we don't want column Var_6
> dat <- dat[, -6]
> # or if we don't know which column is Var_6 you could do
> not.want <- which(names(dat) %in% "Var_7") # now don't want Var_7
> dat <- dat[, -not.want]
> dat
>
> This can be extended to many variables:
>
> not.want <- which(names(dat) %in% c("Var_10", "Var_2", "Var_8"))
> dat <- dat[, -not.want]
> dat # only 1, 3, 4, 5, 9 left
>
> HTH
>
> G

-- 
Adrian DUSA
Arhiva Romana de Date Sociale
Bd. Schitu Magureanu nr.1
050025 Bucuresti sectorul 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101


From dimitris.rizopoulos at med.kuleuven.be  Fri Sep 22 13:17:22 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Fri, 22 Sep 2006 13:17:22 +0200
Subject: [R] A simple resampling problem
References: <4513DDA5020000DE0000A53C@rauzen.rau.ac.za>
Message-ID: <015701c6de38$acd6c910$0540210a@www.domain>

try the following:

B <- 10000
index <- rowSums(matrix(rbinom(B*2, 1, 0.5), ncol = 2))
sum(index == 2) / sum(index > 0)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Jacob van Wyk" <jlvw at na.rau.ac.za>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, September 22, 2006 12:57 PM
Subject: [R] A simple resampling problem


> Dear UseRs
>
> I would like to show my students how to use "resampling" to solve 
> the
> following simple problem:
>
> If a family has two children of which one is a boy, what is the
> probability that the other child is also a boy.
> The answer is (obviously) 1/3, and can be show easily using the 
> usual
> methods.
> But I would like to get the students to think of resampling, by 
> doing
> the following:
> Flip two coins repeatedly, denoted 0 and 1 (1 for boy, say). Discard
> those pairs that both contain 0.
>>From those left over, count how many pairs are (1,1).
> Divide this number by the number available to choose from (i.e. all
> pairs, except (0,0)).
> This will then give 1/3 (more or less of course).
>
> Can somebody help me to code this efficiently, or elegantly (and
> smartly) in R, without loops etc.
> It is intended for first year students that are only starting to 
> learn
> about statistics (or probability theory), and R of course.
>
> Thank you for your time.
> Regards
> Jacob
>
>
>
>
> Jacob L van Wyk
> Department of Statistics
> University of Johannesburg, APK
> P O Box 524
> Auckland Park 2006
> South Africa
> Tel: +27 11 489 3080
> Fax: +27 11 489 2832
>
>
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From ggrothendieck at gmail.com  Fri Sep 22 13:23:39 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 22 Sep 2006 07:23:39 -0400
Subject: [R] A simple resampling problem
In-Reply-To: <4513DDA5020000DE0000A53C@rauzen.rau.ac.za>
References: <4513DDA5020000DE0000A53C@rauzen.rau.ac.za>
Message-ID: <971536df0609220423k4261297ds61b44e6f22f13574@mail.gmail.com>

Try this:


set.seed(1)
n <- 100

first <- sample(0:1, n, replace = TRUE)
second <- sample(0:1, n, replace = TRUE)

sum(first == 1 & second == 1) / sum(first == 1 | second == 1)

# The last could be written more compactly like this
# as 1 and 0 will be treated as TRUE and FALSE by the
# logical operators and then TRUE and FALSE will be
# regarded as 1 and 0 by sum
sum(first & second) / sum(first | second)


On 9/22/06, Jacob van Wyk <jlvw at na.rau.ac.za> wrote:
> Dear UseRs
>
> I would like to show my students how to use "resampling" to solve the
> following simple problem:
>
> If a family has two children of which one is a boy, what is the
> probability that the other child is also a boy.
> The answer is (obviously) 1/3, and can be show easily using the usual
> methods.
> But I would like to get the students to think of resampling, by doing
> the following:
> Flip two coins repeatedly, denoted 0 and 1 (1 for boy, say). Discard
> those pairs that both contain 0.
> >From those left over, count how many pairs are (1,1).
> Divide this number by the number available to choose from (i.e. all
> pairs, except (0,0)).
> This will then give 1/3 (more or less of course).
>
> Can somebody help me to code this efficiently, or elegantly (and
> smartly) in R, without loops etc.
> It is intended for first year students that are only starting to learn
> about statistics (or probability theory), and R of course.
>
> Thank you for your time.
> Regards
> Jacob
>
>
>
>
> Jacob L van Wyk
> Department of Statistics
> University of Johannesburg, APK
> P O Box 524
> Auckland Park 2006
> South Africa
> Tel: +27 11 489 3080
> Fax: +27 11 489 2832
>
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From f.harrell at vanderbilt.edu  Fri Sep 22 13:59:17 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 22 Sep 2006 06:59:17 -0500
Subject: [R] Propensity score and three treatments
In-Reply-To: <4513858F.6000900@med.unibs.it>
References: <4513858F.6000900@med.unibs.it>
Message-ID: <4513D015.6080805@vanderbilt.edu>

Giovanni Parrinello wrote:
> Dear All,
> I would like to find something ( references, code,..)  to implement a
> comparison of three
> treatments in an observational study using the 'Propensity Score'.
> Any help is much appreciated. Thanks!
> Giovanni
> 
@Article{tch05use,
   author =               {Tchernis, Rusty and {Horvitz-Lennon}, Marcela and
Normand, {Sharon-Lise} T.},
   title =                {On the use of discrete choice models for 
causal infere
nce},
   journal =      Stat in Med,
   year =                 2005,
   volume =               24,
   pages =                {2197-2212},
   annote =               {causal inference;discrete choice models;matching
estimator;multi-valued treatment propensity model using discrete
choice model}
}

In one application I created 2 propensity scores and adjusted for those 
using spline functions of the logits.  See

@ARTICLE{mar94con,
   author = {Mark, D. B. and Nelson, C. L. and Califf, R. M. and 
Harrell, F. E.
            and Lee, K. L. and Jones, R. H. and Fortin, D. F. and Stack, 
R. S.
            and Glower, D. D. and Smith, L. R. and {DeLong}, E. R. and 
Smith,
            P. K. and Reves, J. G. and Jollis, J. G. and Tcheng, J. E. and
            Muhlbaier, L. H. and Lowe, J. E. and Phillips, H. R. and 
Pryor, D.
            B.},
   year = 1994,
   title = {The continuing evolution of therapy for coronary artery disease:
           {Initial} results from the era of coronary angioplasty},
   journal = {Circulation},
   volume = 89,
   pages = {2015-2025},
   annote = {CABG; PTCA; propensity; observational study; Cox model
            applications; adjusted survival curves;two propensity
   scores for three treatments}
}

I have many annotated rreferences about propensity scores in my 
bibliographic database - see the bottom of biostat.mc.vanderbilt.edu/rms
-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From kaniovsk at wifo.ac.at  Fri Sep 22 15:04:01 2006
From: kaniovsk at wifo.ac.at (Serguei Kaniovski)
Date: Fri, 22 Sep 2006 15:04:01 +0200
Subject: [R] Compiling a contingency table of counts by case
Message-ID: <4513DF41.2040309@wifo.ac.at>

Thanks Jacques, this works!

Serguei
-- 
___________________________________________________________________

Austrian Institute of Economic Research (WIFO)

Name: Serguei Kaniovski			P.O.Box 91
Tel.: +43-1-7982601-231			Arsenal Objekt 20
Fax:  +43-1-7989386			1103 Vienna, Austria
Mail: Serguei.Kaniovski at wifo.ac.at

http://www.wifo.ac.at/Serguei.Kaniovski


From ktawo at free.fr  Fri Sep 22 15:17:23 2006
From: ktawo at free.fr (Octave Julien)
Date: Fri, 22 Sep 2006 15:17:23 +0200
Subject: [R] Variable as color in a barplot
Message-ID: <5b27d8b150c7f97c1ecdb687d6129118@free.fr>

Dear wise ones,

I have a problem assigning different colors to bars in a barplot.
The data I'm using is the following dataframe (truncated) :

 > L0
       r n p   t
[...]
18   19 1 1 RFM
19   20 1 1 RFM
20   21 2 1 RFM
21   23 6 1 RIH
22   24 2 1 ROC
23   25 1 1 ROC
24   26 1 1 ROC
25   27 2 1 ROC
26   28 2 1 RFT
27   29 1 1 RFT
28   30 2 1 RFT
29   31 1 1 ROH
[...]

My barplot should display ascending bars according to L0$n. Their width 
depends on L0$p, which can be greater than 1. Last, I want a different 
color for each of the five values of L0$t.
I use the following array to assign a colour for each type given in 
L0$t :

 > coul <- array()
 > coul['ROC'] <- 'khaki1'
 > coul['RFM'] <- 'palegreen'
 > coul['RFT'] <- 'lightorange'
 > coul['RIH'] <- 'blue'
 > coul['ROH'] <- 'lightblue'

And here's the barplot command I'm using :

 > barplot(sort(L0$n), ylim=c(0,10), width=L0$p[order(L0$n)], 
col=coul[L0$t[order(L0$n)]], main="Nombre de genres", sub="Livrets")

The barplot uses the sort() and order() functions to display ascending 
bars with the right values. The width parameter works fine, but the 
color doesn't. I get :

Erreur dans rect(y1, x1, y2, x2, ...) : nom de couleur incorrect
(error in rect(y1, x1, y2, x2, ...) : incorrect color name)

I tried to fiddle with coul[] or its indice but it didn't help.
Does anybody know what's wrong ?
Many thanks,

Octave


From mothsailor at googlemail.com  Fri Sep 22 15:24:04 2006
From: mothsailor at googlemail.com (David Barron)
Date: Fri, 22 Sep 2006 14:24:04 +0100
Subject: [R] Variable as color in a barplot
In-Reply-To: <5b27d8b150c7f97c1ecdb687d6129118@free.fr>
References: <5b27d8b150c7f97c1ecdb687d6129118@free.fr>
Message-ID: <815b70590609220624sfe36e7cj96c970319688e01b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060922/9ff9afb6/attachment.pl 

From tilling at gmail.com  Fri Sep 22 16:20:10 2006
From: tilling at gmail.com (John Tillinghast)
Date: Fri, 22 Sep 2006 07:20:10 -0700
Subject: [R] 'help' information not modified when I modify man files
In-Reply-To: <4513A306.90307@statistik.uni-dortmund.de>
References: <73bcebe0609211406i11a22627j828ea0ef82187ad4@mail.gmail.com>
	<4513A306.90307@statistik.uni-dortmund.de>
Message-ID: <73bcebe0609220720w192b9f41k82b3203c48092c12@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060922/3e8d8c33/attachment.pl 

From mikewolfgang at gmail.com  Fri Sep 22 16:45:13 2006
From: mikewolfgang at gmail.com (Mike Wolfgang)
Date: Fri, 22 Sep 2006 10:45:13 -0400
Subject: [R] extract data from lm object and then use again?
Message-ID: <e668df8c0609220745x74afe115s1e89b41fe080aa32@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060922/d312e179/attachment.pl 

From tkellermann at ukaachen.de  Fri Sep 22 17:04:56 2006
From: tkellermann at ukaachen.de (Thilo Kellermann)
Date: Fri, 22 Sep 2006 17:04:56 +0200
Subject: [R] extract data from lm object and then use again?
In-Reply-To: <e668df8c0609220745x74afe115s1e89b41fe080aa32@mail.gmail.com>
References: <e668df8c0609220745x74afe115s1e89b41fe080aa32@mail.gmail.com>
Message-ID: <200609221704.56776.tkellermann@ukaachen.de>

Hi,

the data of the model fit is stored in lm$model and should work....

Good luck,
Thilo

On Friday 22 September 2006 16:45, Mike Wolfgang wrote:
> Hi list,
>
> I want to write a general function so that it would take an lm object,
> extract its data element, then use the data at another R function (eg,
> glm). I searched R-help list, and found this would do the trick of the
> first part: a.lm$call$data
> this would return a name object but could not be recognized as a
> data.frameby glm. I also tried
> call(as.character(a.lm$call$data))
> or
> eval(call(as.character(a.lm$call$data)))
> neither works.
>
> By eval(call(...)), it acts as evaluating of a function, but what I want is
> just a data frame object which could be inserted into glm function. Anyone
> could help? Thanks,
>
> Mike
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented, minimal,
> self-contained, reproducible code.

-- 
________________________
Thilo Kellermann
Department of Psychiatry und Psychotherapy
RWTH Aachen University
Pauwelstr. 30
52074 Aachen
Tel.: +49 (0)241 / 8089977
Fax.: +49 (0)241 / 8082401
E-Mail: tkellermann at ukaachen.de


From xchen_stat at hotmail.com  Fri Sep 22 17:14:41 2006
From: xchen_stat at hotmail.com (X.H Chen)
Date: Fri, 22 Sep 2006 09:14:41 -0600
Subject: [R] how to store recursive results
In-Reply-To: <4513AC4B.1030001@pburns.seanet.com>
Message-ID: <BAY23-F13A8631310B32F0B045036FA210@phx.gbl>

Hi Patrick,

Thanks for your suggestion. I find your method works for the functions with 
integer paramters. For example,

If we have function f:
f<-function(i)
{
	if(i>1)
		i*f(i-1)
	else
		1
}

and then using:

ans <- vector("list", n)
for(i in 1:5) {
ans[[i]] <- f(i)
}

the ans should be:
[[1]]
[1] 1

[[2]]
[1] 2

[[3]]
[1] 6

[[4]]
[1] 24

[[5]]
[1] 120

But actually, we there is no such a "i" can be referrenced in f(), no 
parametric function for example, this will be a problem. Anyway, thanks a 
lot for your suggestions.

Cheers,

Xiaohui Chen

Dept. of Statistics
UBC, Canada




>From: Patrick Burns <pburns at pburns.seanet.com>
>To: "X.H Chen" <xchen_stat at hotmail.com>
>Subject: Re: [R] how to store recursive results
>Date: Fri, 22 Sep 2006 10:26:35 +0100
>
>It isn't clear to me exactly what you are asking, but
>I think that a list might be what you are after. Something
>like:
>
>ans <- vector("list", n)
>for(i in 1:n) {
>ans[[i]] <- ....
>}
>
>X.H Chen wrote:
>
>>Hi all,
>>
>>How to store recursive resutls from a function for each step without using 
>>global operators <<-? Thanks ahead.
>>
>>Xiaohui Chen
>>
>>Dept. of Statistics
>>UBC, Canada
>>
>>_________________________________________________________________
>>Don?t waste time standing in line?try shopping online. Visit Sympatico / 
>>MSN
>>
>>------------------------------------------------------------------------
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide 
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>>
>>


From mschwartz at mn.rr.com  Fri Sep 22 17:21:40 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Fri, 22 Sep 2006 10:21:40 -0500
Subject: [R] extract data from lm object and then use again?
In-Reply-To: <e668df8c0609220745x74afe115s1e89b41fe080aa32@mail.gmail.com>
References: <e668df8c0609220745x74afe115s1e89b41fe080aa32@mail.gmail.com>
Message-ID: <1158938500.3966.10.camel@localhost.localdomain>

On Fri, 2006-09-22 at 10:45 -0400, Mike Wolfgang wrote:
> Hi list,
> 
> I want to write a general function so that it would take an lm object,
> extract its data element, then use the data at another R function (eg, glm).
> I searched R-help list, and found this would do the trick of the first part:
> a.lm$call$data
> this would return a name object but could not be recognized as a
> data.frameby glm. I also tried
> call(as.character(a.lm$call$data))
> or
> eval(call(as.character(a.lm$call$data)))
> neither works.
> 
> By eval(call(...)), it acts as evaluating of a function, but what I want is
> just a data frame object which could be inserted into glm function. Anyone
> could help? Thanks,
> 
> Mike

If the 'data' argument in lm() is used, then this approach could work:

> Iris2 <- eval(lm(Sepal.Length ~ Species, data = iris)$call$data)

> str(Iris2)
`data.frame':   150 obs. of  5 variables:
 $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...
 $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...
 $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...
 $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...
 $ Species     : Factor w/ 3 levels "setosa","versicolor",..: 1 1 1 1 1
1 1 1 1 1 ...


However, note that you do not get the actual data used within the lm()
function (the model frame) but the entire source data frame.

What you likely want instead is the model frame containing the columns
actually used in the model formula:

> Iris3 <- lm(Sepal.Length ~ Species, data = iris)$model

> str(Iris3)
`data.frame':   150 obs. of  2 variables:
 $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...
 $ Species     : Factor w/ 3 levels "setosa","versicolor",..: 1 1 1 1 1
1 1 1 1 1 ...
 - attr(*, "terms")=Classes 'terms', 'formula' length 3 Sepal.Length ~
Species
  .. ..- attr(*, "variables")= language list(Sepal.Length, Species)
  .. ..- attr(*, "factors")= int [1:2, 1] 0 1
  .. .. ..- attr(*, "dimnames")=List of 2
  .. .. .. ..$ : chr [1:2] "Sepal.Length" "Species"
  .. .. .. ..$ : chr "Species"
  .. ..- attr(*, "term.labels")= chr "Species"
  .. ..- attr(*, "order")= int 1
  .. ..- attr(*, "intercept")= int 1
  .. ..- attr(*, "response")= int 1
  .. ..- attr(*, ".Environment")=length 15 <environment>
  .. ..- attr(*, "predvars")= language list(Sepal.Length, Species)
  .. ..- attr(*, "dataClasses")= Named chr [1:2] "numeric" "factor"
  .. .. ..- attr(*, "names")= chr [1:2] "Sepal.Length" "Species"


HTH,

Marc Schwartz


From xchen_stat at hotmail.com  Fri Sep 22 17:31:23 2006
From: xchen_stat at hotmail.com (X.H Chen)
Date: Fri, 22 Sep 2006 09:31:23 -0600
Subject: [R] how to store recursive results
In-Reply-To: <971536df0609220349u1bde0bfer42a575545c3be43d@mail.gmail.com>
Message-ID: <BAY23-F19FF58544C722DD135B9DCFA210@phx.gbl>

Hi Gabor,

Thanks for pointing out this for me. However, what I try to get is how to 
construct such form a function f that

ret<-f(...),

where ret contains the each recursive result from f, and meantime f consists 
of no <<- operator. Do you have any idea how to implemet this. Thanks a lot 
for your suggestions.

Cheer

Xiaohui Chen

Dept. of Statistics
UBC, Canada




>From: "Gabor Grothendieck" <ggrothendieck at gmail.com>
>To: "X.H Chen" <xchen_stat at hotmail.com>
>CC: r-help at stat.math.ethz.ch
>Subject: Re: [R] how to store recursive results
>Date: Fri, 22 Sep 2006 06:49:22 -0400
>
>Note that <<- is not necessarily global:
>
>if (exists("x")) rm(x)
>f <- function() {
>	x <- 2
>	g <- function() x <<- 3
>	g()
>	x
>}
>f() # 3
>exists("x") # FALSE
>
>On 9/22/06, X.H Chen <xchen_stat at hotmail.com> wrote:
>>Hi all,
>>
>>How to store recursive resutls from a function for each step without using
>>global operators <<-? Thanks ahead.
>>
>>Xiaohui Chen
>>
>>Dept. of Statistics
>>UBC, Canada
>>
>>_________________________________________________________________
>>Don't waste time standing in line?try shopping online. Visit Sympatico / 
>>MSN
>>
>>
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide 
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>

_________________________________________________________________
Buy what you want when you want it on Sympatico / MSN Shopping


From xchen_stat at hotmail.com  Fri Sep 22 17:37:47 2006
From: xchen_stat at hotmail.com (X.H Chen)
Date: Fri, 22 Sep 2006 09:37:47 -0600
Subject: [R] how to store recursive results
In-Reply-To: <fab4bcf70609220321j535ac92cq7972c09d818785b3@mail.gmail.com>
Message-ID: <BAY23-F115CB643513A7718C10917FA210@phx.gbl>

Hi B?lint,

Thanks very much for your suggestions. The party package is a little bit 
complicated to use. Do you have a much simpler example for this problem? 
Anyway, I am gonna try this package.

Cheers,

Xiaohui Chen

Dept. of Statistics
UBC, Canada




>From: "B?lint Cz?cz" <czucz at nimbus.elte.hu>
>To: "X.H Chen" <xchen_stat at hotmail.com>
>CC: r-help at stat.math.ethz.ch
>Subject: Re: [R] how to store recursive results
>Date: Fri, 22 Sep 2006 12:21:02 +0200
>
>One suggestion:
>
>as a recursive list. For example have a look at the 'party' package
>and the BinaryTree class.
>I hope this helps.
>
>B?lint
>
>
>On 22/09/06, X.H Chen <xchen_stat at hotmail.com> wrote:
>>Hi all,
>>
>>How to store recursive resutls from a function for each step without using
>>global operators <<-? Thanks ahead.
>>
>>Xiaohui Chen
>>
>>Dept. of Statistics
>>UBC, Canada
>>
>>_________________________________________________________________
>>Don't waste time standing in line?try shopping online. Visit Sympatico / 
>>MSN
>>
>>
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide 
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>


From richard.c.yeh at bankofamerica.com  Fri Sep 22 17:45:49 2006
From: richard.c.yeh at bankofamerica.com (Yeh, Richard C)
Date: Fri, 22 Sep 2006 11:45:49 -0400
Subject: [R] How to retrieve results of most recent command?
Message-ID: <31AFBE76660ED2459C6F75E6EDFD203C037F5D5F@ex2k.bankofamerica.com>

In R, is there an automatic variable that stores the results of the most
recent command or commands?  (I am thinking of a behavior like
Mathematica's % result-history substitution syntax.)


(I am using R 2.3.1 on Linux and R 2.3.1 on Windows XP.)


This is a pretty basic question, so I tried to do an extensive version
of the recommended pre-posting homework.

> help.search('substitution')
failed to turn up anything relevant.  This is more an environmental
question, not an R-language programming question.

> help.search('history')
was interesting, but focused on command history.  'savehistory' appears
to be an atomic function, so the temporary-file manipulation done by
'history' seems required.  Perhaps I could try store the desired line in
a string and then evaluate it, but this method feels terribly clunky.
On the other hand, that may be the price to pay for avoidance of
creeping featurism.

The top 10 RSiteSearch results also focus on saving the command
histories to a file or script.  The 'Emacs Speaks Statistics' (ESS)
package seems to offer shortcuts for command history expansion, but not
results history.  I'm a bit reluctant to start using Emacs again, having
quit it for VI over a decade ago.



212-933-3305 / richard.c.yeh at bankofamerica.com 


NOTICE TO RECIPIENTS: Any information contained in or attached
to this message is intended solely for the use of the intended
recipient(s). If you are not the intended recipient of this
transmittal, you are hereby notified that you received this
transmittal in error and we request that you please delete and
destroy all copies and attachments in your possession, notify
the sender that you have received this communication in error,
and note that any review or dissemination of, or the taking of
any action in reliance on, this communication is expressly
prohibited. 

Banc of America Securities LLC ("BAS") does not accept time
sensitive, action-oriented messages or transaction orders,
including orders to purchase or sell securities, via e-mail.

Regular internet e-mail transmission cannot be guaranteed to be
secure or error-free.  Therefore, we do not represent that this
information is complete or accurate and it should not be relied
upon as such.  If you prefer to communicate with BAS using secure
(i.e., encrypted) e-mail transmission, please notify the sender.
Otherwise, you will be deemed to have consented to communicate
with BAS via regular internet e-mail transmission.  Please note
that BAS reserves the right to intercept, monitor, and retain all
e-mail messages (including secure e-mail messages) sent to or
from its systems as permitted by applicable law.

-------------------------------------------------------------------
IRS Circular 230 Disclosure:

Bank of America Corporation and its affiliates, including BAS,
("Bank of America") do not provide tax advice. Accordingly, any
statements contained herein as to tax matters were neither written
nor intended by the sender or Bank of America to be used and cannot
be used by any taxpayer for the purpose of avoiding tax penalties
that may be imposed on such taxpayer. If any person uses or refers
to any such tax statement in promoting, marketing or recommending a
partnership or other entity, investment plan or arrangement to any
taxpayer, then the statement expressed above is being delivered to
support the promotion or marketing of the transaction or matter
addressed and the recipient should seek advice based on its
particular circumstances from an independent tax advisor.


From B.Rowlingson at lancaster.ac.uk  Fri Sep 22 17:58:42 2006
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 22 Sep 2006 16:58:42 +0100
Subject: [R] How to retrieve results of most recent command?
In-Reply-To: <31AFBE76660ED2459C6F75E6EDFD203C037F5D5F@ex2k.bankofamerica.com>
References: <31AFBE76660ED2459C6F75E6EDFD203C037F5D5F@ex2k.bankofamerica.com>
Message-ID: <45140832.7050003@lancaster.ac.uk>

Yeh, Richard C wrote:
> In R, is there an automatic variable that stores the results of the most
> recent command or commands?  (I am thinking of a behavior like
> Mathematica's % result-history substitution syntax.)
> 

Something like .Last.value:

  > x=sqrt(2)
  > .Last.value
  [1] 1.414214

which might get more usage if it was less fiddly to type.


Barry


From ggrothendieck at gmail.com  Fri Sep 22 18:00:33 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 22 Sep 2006 12:00:33 -0400
Subject: [R] how to store recursive results
In-Reply-To: <BAY23-F19FF58544C722DD135B9DCFA210@phx.gbl>
References: <971536df0609220349u1bde0bfer42a575545c3be43d@mail.gmail.com>
	<BAY23-F19FF58544C722DD135B9DCFA210@phx.gbl>
Message-ID: <971536df0609220900w4bb372acwbb70ee55206202e9@mail.gmail.com>

1. The point is that you can use <<- and still not pollute the
global environment with any variables so its not clear
why there should be any requirement not to use it.

Other possiblities are

2. to pass the info around through the return value or through an environment:

fact <- function(n) {
	if (n == 1) list(n, c("1" = 1))
	else {
		f <- fact(n-1)
		out <- list(n * f[[1]], unlist(f))
		names(out[[2]])[1] <- n
		out
	}
}
		
fact(4)

3. or through an environment:

fact <- function(n, e) {
	if (n == 1) e[["1"]] <- 1
	else n * (e[[format(n)]] <- fact(n-1, e))
}

e <- new.env()
fact(4, e)
as.list(e)


On 9/22/06, X.H Chen <xchen_stat at hotmail.com> wrote:
> Hi Gabor,
>
> Thanks for pointing out this for me. However, what I try to get is how to
> construct such form a function f that
>
> ret<-f(...),
>
> where ret contains the each recursive result from f, and meantime f consists
> of no <<- operator. Do you have any idea how to implemet this. Thanks a lot
> for your suggestions.
>
> Cheer
>
> Xiaohui Chen
>
> Dept. of Statistics
> UBC, Canada
>
>
>
>
> >From: "Gabor Grothendieck" <ggrothendieck at gmail.com>
> >To: "X.H Chen" <xchen_stat at hotmail.com>
> >CC: r-help at stat.math.ethz.ch
> >Subject: Re: [R] how to store recursive results
> >Date: Fri, 22 Sep 2006 06:49:22 -0400
> >
> >Note that <<- is not necessarily global:
> >
> >if (exists("x")) rm(x)
> >f <- function() {
> >       x <- 2
> >       g <- function() x <<- 3
> >       g()
> >       x
> >}
> >f() # 3
> >exists("x") # FALSE
> >
> >On 9/22/06, X.H Chen <xchen_stat at hotmail.com> wrote:
> >>Hi all,
> >>
> >>How to store recursive resutls from a function for each step without using
> >>global operators <<-? Thanks ahead.
> >>
> >>Xiaohui Chen
> >>
> >>Dept. of Statistics
> >>UBC, Canada
> >>
> >>_________________________________________________________________
> >>Don't waste time standing in line?try shopping online. Visit Sympatico /
> >>MSN
> >>
> >>
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide
> >>http://www.R-project.org/posting-guide.html
> >>and provide commented, minimal, self-contained, reproducible code.
> >>
> >>
> >>
>
> _________________________________________________________________
> Buy what you want when you want it on Sympatico / MSN Shopping
> http://shopping.sympatico.msn.ca/content/shp/?ctId=2,ptnrid=176,ptnrdata=081805
>
>


From tlumley at u.washington.edu  Fri Sep 22 18:03:23 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 22 Sep 2006 09:03:23 -0700 (PDT)
Subject: [R] extract data from lm object and then use again?
In-Reply-To: <200609221704.56776.tkellermann@ukaachen.de>
References: <e668df8c0609220745x74afe115s1e89b41fe080aa32@mail.gmail.com>
	<200609221704.56776.tkellermann@ukaachen.de>
Message-ID: <Pine.LNX.4.64.0609220854270.27097@homer21.u.washington.edu>

On Fri, 22 Sep 2006, Thilo Kellermann wrote:

> Hi,
>
> the data of the model fit is stored in lm$model and should work....
>

Not reliably. In the first place, you should use the accessor function 
model.frame(model) rather than model$model, which works even if the model 
was fitted with model=FALSE.

But even then,
   glm(formula(model), data=model.frame(model))
will not work reliably.
Consider
   model <- lm(log(Volume)~log(Height)+log(Girth),data=trees)

The model frame has variables called eg "log(Volume)" rather than 
"Volume".

When you need the source data frame you need to do something like
    eval(model$call$data, environment(formula(model)))
and even this might not work, eg if the model had no data 
argument.

However, if the model had no data argument then the variables 
must be available in environment(formula(model)), in which case any data 
frame of the right size will do.

If there are no missing observations or the model  was fitted with 
na.action="na.exclude" then a fairly reliable approach is to use
   eval(model$call$data, environment(formula(model)))
if it is not NULL and to fall back to model.frame(model). This is what 
termplot() does.


 	-thomas


From h.wickham at gmail.com  Fri Sep 22 18:04:02 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Fri, 22 Sep 2006 11:04:02 -0500
Subject: [R] Compiling a contingency table of counts by case
In-Reply-To: <4513A276.7030802@wifo.ac.at>
References: <4513A276.7030802@wifo.ac.at>
Message-ID: <f8e6ff050609220904md2b56f9g80b38049e58219a8@mail.gmail.com>

> I have asked a similar question before but this time
> the problem is somewhat more involved. I have the
> following data:
>
> case;name;x
> 1;Joe;1
> 1;Mike;1
> 1;Zoe;1
> 2;Joe;1
> 2;Mike;0
> 2;Zoe;1
> 2;John;1
> 3;Mike;1
> 3;Zoe;0
> 3;Karl;0
>
> I would like to count the number of "case"
> in which any two "name"
>
> a. both have "x=1",
> b. the first has "x=0" - the second has "x=1",
> c. the first has "x=1" - the second has "x=0",
> d. both have "x=0",

One way is to use the reshape package:

dat <- read.delim("clipboard", sep=";")
dat <- rename(dat, c(x=value))
cast(dat, name ~ case)

(which doesn't give you exactly what you want, but I think might be
more illuminating)

Hadley


From richard.c.yeh at bankofamerica.com  Fri Sep 22 18:09:05 2006
From: richard.c.yeh at bankofamerica.com (Yeh, Richard C)
Date: Fri, 22 Sep 2006 12:09:05 -0400
Subject: [R] How to retrieve results of most recent command?
In-Reply-To: <45140832.7050003@lancaster.ac.uk>
Message-ID: <31AFBE76660ED2459C6F75E6EDFD203C037F5D9C@ex2k.bankofamerica.com>

That's perfect!  Thanks!

I now see that .Last.value is mentioned in the 'Introduction to R',
footnote 5.  Sorry, I hadn't read it that carefully.

(Before posting, I even did read
> help('.Last')
but thought that that function wasn't what I wanted.
I guess I wasn't using the right keywords.
> help.search('last')
returns .Last.value as the top hit.

(I also apologize for the long disclaimer at the end.  It's
automatically generated.) 


212-933-3305 / richard.c.yeh at bankofamerica.com 
-----Original Message-----
From: Barry Rowlingson [mailto:B.Rowlingson at lancaster.ac.uk] 
Sent: Friday, September 22, 2006 11:59 AM
To: Yeh, Richard C
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] How to retrieve results of most recent command?

Yeh, Richard C wrote:
> In R, is there an automatic variable that stores the results of the
most
> recent command or commands?  (I am thinking of a behavior like
> Mathematica's % result-history substitution syntax.)
> 

Something like .Last.value:

  > x=sqrt(2)
  > .Last.value
  [1] 1.414214

which might get more usage if it was less fiddly to type.


Barry


NOTICE TO RECIPIENTS: Any information contained in or attached
to this message is intended solely for the use of the intended
recipient(s). If you are not the intended recipient of this
transmittal, you are hereby notified that you received this
transmittal in error and we request that you please delete and
destroy all copies and attachments in your possession, notify
the sender that you have received this communication in error,
and note that any review or dissemination of, or the taking of
any action in reliance on, this communication is expressly
prohibited. 

Banc of America Securities LLC ("BAS") does not accept time
sensitive, action-oriented messages or transaction orders,
including orders to purchase or sell securities, via e-mail.

Regular internet e-mail transmission cannot be guaranteed to be
secure or error-free.  Therefore, we do not represent that this
information is complete or accurate and it should not be relied
upon as such.  If you prefer to communicate with BAS using secure
(i.e., encrypted) e-mail transmission, please notify the sender.
Otherwise, you will be deemed to have consented to communicate
with BAS via regular internet e-mail transmission.  Please note
that BAS reserves the right to intercept, monitor, and retain all
e-mail messages (including secure e-mail messages) sent to or
from its systems as permitted by applicable law.

-------------------------------------------------------------------
IRS Circular 230 Disclosure:

Bank of America Corporation and its affiliates, including BAS,
("Bank of America") do not provide tax advice. Accordingly, any
statements contained herein as to tax matters were neither written
nor intended by the sender or Bank of America to be used and cannot
be used by any taxpayer for the purpose of avoiding tax penalties
that may be imposed on such taxpayer. If any person uses or refers
to any such tax statement in promoting, marketing or recommending a
partnership or other entity, investment plan or arrangement to any
taxpayer, then the statement expressed above is being delivered to
support the promotion or marketing of the transaction or matter
addressed and the recipient should seek advice based on its
particular circumstances from an independent tax advisor.


From pgilbert at bank-banque-canada.ca  Fri Sep 22 18:07:52 2006
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Fri, 22 Sep 2006 12:07:52 -0400
Subject: [R] Exponentiate a matrix
In-Reply-To: <2323A6D37908A847A7C32F1E3662C80E3B83BB@dc1ex01.air.org>
References: <2323A6D37908A847A7C32F1E3662C80E3B83BB@dc1ex01.air.org>
Message-ID: <45140A58.40700@bank-banque-canada.ca>

I am getting a bit rusty on some of these things, but I seem to recall 
that there is a numerical advantage (speed and/or accuracy?) to 
diagonalizing:

 > expM <- function(X,e) { v <- La.svd(X); v$u %*% diag(v$d^e) %*% v$vt }

 > P <- matrix(c(.3,.7, .7, .3), ncol=2)
 > P %*% P %*% P
      [,1]  [,2]
[1,] 0.468 0.532
[2,] 0.532 0.468
 > expM(P,3)
      [,1]  [,2]
[1,] 0.468 0.532
[2,] 0.532 0.468

I think this also works for non-integer, negative, large, and complex 
exponents:

 > expM(P, 1.5)
          [,1]      [,2]
[1,] 0.3735089 0.6264911
[2,] 0.6264911 0.3735089
 > expM(P, 1000)
     [,1] [,2]
[1,]  0.5  0.5
[2,]  0.5  0.5
 > expM(P, -3)
        [,1]    [,2]
[1,] -7.3125  8.3125
[2,]  8.3125 -7.3125
 > expM(P, 3+.5i)
                  [,1]              [,2]
[1,] 0.4713+0.0141531i 0.5287-0.0141531i
[2,] 0.5287-0.0141531i 0.4713+0.0141531i
 >

Paul Gilbert

Doran, Harold wrote:

>Suppose I have a square matrix P
>
>P <- matrix(c(.3,.7, .7, .3), ncol=2)
>
>I know that 
>
>  
>
>>P * P 
>>    
>>
>
>Returns the element by element product, whereas
>
>  
>
>>P%*%P
>>    
>>
>
>Returns the matrix product.
>
>Now, P^2 also returns the element by element product. But, is there a
>slick way to write
>
>P %*% P %*% P
>
>Obviously, P^3 does not return the result I expect.
>
>Thanks,
>Harold
>
>
>
>
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>  
>
====================================================================================

La version fran?aise suit le texte anglais.

------------------------------------------------------------------------------------

This email may contain privileged and/or confidential inform...{{dropped}}


From jacques.veslot at good.ibl.fr  Fri Sep 22 18:18:15 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Fri, 22 Sep 2006 18:18:15 +0200
Subject: [R] Compiling a contingency table of counts by case
In-Reply-To: <f8e6ff050609220904md2b56f9g80b38049e58219a8@mail.gmail.com>
References: <4513A276.7030802@wifo.ac.at>
	<f8e6ff050609220904md2b56f9g80b38049e58219a8@mail.gmail.com>
Message-ID: <45140CC7.9050305@good.ibl.fr>

what's different from:
 > with(dat, tapply(x, list(name,case), sum))
       1  2  3
Joe   1  1 NA
John NA  1 NA
Karl NA NA  0
Mike  1  0  1

and how to deal with this table ?
-------------------------------------------------------------------
Jacques VESLOT

CNRS UMR 8090
I.B.L (2?me ?tage)
1 rue du Professeur Calmette
B.P. 245
59019 Lille Cedex

Tel : 33 (0)3.20.87.10.44
Fax : 33 (0)3.20.87.10.31

http://www-good.ibl.fr
-------------------------------------------------------------------


hadley wickham a ?crit :
>>I have asked a similar question before but this time
>>the problem is somewhat more involved. I have the
>>following data:
>>
>>case;name;x
>>1;Joe;1
>>1;Mike;1
>>1;Zoe;1
>>2;Joe;1
>>2;Mike;0
>>2;Zoe;1
>>2;John;1
>>3;Mike;1
>>3;Zoe;0
>>3;Karl;0
>>
>>I would like to count the number of "case"
>>in which any two "name"
>>
>>a. both have "x=1",
>>b. the first has "x=0" - the second has "x=1",
>>c. the first has "x=1" - the second has "x=0",
>>d. both have "x=0",
> 
> 
> One way is to use the reshape package:
> 
> dat <- read.delim("clipboard", sep=";")
> dat <- rename(dat, c(x=value))
> cast(dat, name ~ case)
> 
> (which doesn't give you exactly what you want, but I think might be
> more illuminating)
> 
> Hadley
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From tlumley at u.washington.edu  Fri Sep 22 18:19:34 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 22 Sep 2006 09:19:34 -0700 (PDT)
Subject: [R] how to store recursive results
In-Reply-To: <BAY23-F19FF58544C722DD135B9DCFA210@phx.gbl>
References: <BAY23-F19FF58544C722DD135B9DCFA210@phx.gbl>
Message-ID: <Pine.LNX.4.64.0609220904420.27097@homer21.u.washington.edu>

On Fri, 22 Sep 2006, X.H Chen wrote:

> Hi Gabor,
>
> Thanks for pointing out this for me. However, what I try to get is how to 
> construct such form a function f that
>
> ret<-f(...),
>
> where ret contains the each recursive result from f, and meantime f consists 
> of no <<- operator. Do you have any idea how to implemet this. Thanks a lot 
> for your suggestions.
>

It depends on the situation. You can always pass the results back in a 
list or vector, eg

    cumfactorial<-function(n){
 	           if (n==0)
                       1
                    else c(1, n*cumfactorial(n-1))
    }


If you want to get the results out then you have to either accumulate and 
return them like this or use <<-, since return() and <<- are the only ways 
to get results out of a function.  As long as you don't use <<- to assign 
to variables outside the function it is a perfectly reasonable thing to do

If you were doing something like a Fibonacci sequence then assigning would 
be preferable

     fib<-function(n){
             memo<-new.env(hash=TRUE)
             fibrec<-function(m){
               if (m<=2) return(1)
               vm<-paste("v",m,sep="")
 	      if(exists(vm,envir=memo,inherits=FALSE))
                   return(get(vm,envir=memo,inherits=FALSE))
               rval<-fibrec(m-1)+fibrec(m-2)
 	      assign(vm,rval,envir=memo)
               rval
            }
           fibrec(n)
           sapply(ls(envir=memo),get, envir=memo)
          }

since the memoization converts the algorithm from exponential time to 
linear time.


From h.wickham at gmail.com  Fri Sep 22 18:33:00 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Fri, 22 Sep 2006 11:33:00 -0500
Subject: [R] Compiling a contingency table of counts by case
In-Reply-To: <45140CC7.9050305@good.ibl.fr>
References: <4513A276.7030802@wifo.ac.at>
	<f8e6ff050609220904md2b56f9g80b38049e58219a8@mail.gmail.com>
	<45140CC7.9050305@good.ibl.fr>
Message-ID: <f8e6ff050609220933p102f8926g7ed8b606bdfeb089@mail.gmail.com>

> what's different from:
>  > with(dat, tapply(x, list(name,case), sum))
>        1  2  3
> Joe   1  1 NA
> John NA  1 NA
> Karl NA NA  0
> Mike  1  0  1
>
> and how to deal with this table ?

Well, the syntax is easier (once you have the data in the correct,
molten, form), and more flexible for other tasks.  It is surely better
to learn a general purpose tool rather than a tool for a specific
task.

To use that table to answer the original question, you just need to
look column by column for the desired patterns of 0's and 1's, eg. in
case 1, Joe, Mike and Zoe all had ones.   Perhaps I misunderstood the
original question.

Hadley


From ripley at stats.ox.ac.uk  Fri Sep 22 19:03:43 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 22 Sep 2006 18:03:43 +0100 (BST)
Subject: [R] extract data from lm object and then use again?
In-Reply-To: <Pine.LNX.4.64.0609220854270.27097@homer21.u.washington.edu>
References: <e668df8c0609220745x74afe115s1e89b41fe080aa32@mail.gmail.com>
	<200609221704.56776.tkellermann@ukaachen.de>
	<Pine.LNX.4.64.0609220854270.27097@homer21.u.washington.edu>
Message-ID: <Pine.LNX.4.64.0609221758530.23969@gannet.stats.ox.ac.uk>

On Fri, 22 Sep 2006, Thomas Lumley wrote:

> On Fri, 22 Sep 2006, Thilo Kellermann wrote:
>
>> Hi,
>>
>> the data of the model fit is stored in lm$model and should work....
>>
>
> Not reliably. In the first place, you should use the accessor function
> model.frame(model) rather than model$model, which works even if the model
> was fitted with model=FALSE.
>
> But even then,
>   glm(formula(model), data=model.frame(model))
> will not work reliably.
> Consider
>   model <- lm(log(Volume)~log(Height)+log(Girth),data=trees)
>
> The model frame has variables called eg "log(Volume)" rather than
> "Volume".
>
> When you need the source data frame you need to do something like
>    eval(model$call$data, environment(formula(model)))
> and even this might not work, eg if the model had no data
> argument.
>
> However, if the model had no data argument then the variables
> must be available in environment(formula(model)), in which case any data
> frame of the right size will do.

to be picky ... must have been available.  You or some other command could 
very easily have changed them.  That's actually why we store the model 
frame by default: there is no other 100% reliable way to get at the data 
used in the fitting (as distinct from the data originally supplied).

> If there are no missing observations or the model  was fitted with
> na.action="na.exclude" then a fairly reliable approach is to use
>   eval(model$call$data, environment(formula(model)))
> if it is not NULL and to fall back to model.frame(model). This is what
> termplot() does.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From wildscop at yahoo.com  Fri Sep 22 19:06:42 2006
From: wildscop at yahoo.com (Mohammad Ehsanul Karim)
Date: Fri, 22 Sep 2006 22:36:42 +0530
Subject: [R] $theta of frailty in coxph
Message-ID: <c2a71e600609221006u746e0farbc4414ec19cba876@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060922/ae498d5c/attachment.pl 

From justin_bem at yahoo.fr  Fri Sep 22 19:46:14 2006
From: justin_bem at yahoo.fr (justin bem)
Date: Fri, 22 Sep 2006 17:46:14 +0000 (GMT)
Subject: [R] Re :  Statitics Textbook - any recommendation?
In-Reply-To: <17683.34077.632649.350455@stat.math.ethz.ch>
Message-ID: <20060922174614.2500.qmail@web23011.mail.ird.yahoo.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20060922/3b0902fa/attachment.pl 

From ferri.leberl at gmx.at  Fri Sep 22 20:16:57 2006
From: ferri.leberl at gmx.at (Mag. Ferri Leberl)
Date: Fri, 22 Sep 2006 20:16:57 +0200
Subject: [R] inequality with NA
Message-ID: <1158949017.5919.7.camel@vulcania>

Dear everybody!
take a<-c(5,3,NA,6).

if(a[1]!=NA){b<-7}
if(a[3]!=5){b<-7}
if(a[3]!=NA){b<-7}
if(a[3]==NA){b<-7}

will alltogeather return

Fehler in if (a[1] != NA) { : Fehlender Wert, wo TRUE/FALSE n?tig ist

(or simularly). Somehow this is logical. But how else should I get out,
whether a certain vector-component has an existing value?
Thank you in advance!
Yours,
Mag. Ferri Leberl


From ligges at statistik.uni-dortmund.de  Fri Sep 22 20:25:25 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 22 Sep 2006 20:25:25 +0200
Subject: [R] inequality with NA
In-Reply-To: <1158949017.5919.7.camel@vulcania>
References: <1158949017.5919.7.camel@vulcania>
Message-ID: <45142A95.9070006@statistik.uni-dortmund.de>



Mag. Ferri Leberl wrote:
> Dear everybody!
> take a<-c(5,3,NA,6).
> 
> if(a[1]!=NA){b<-7}
> if(a[3]!=5){b<-7}
> if(a[3]!=NA){b<-7}
> if(a[3]==NA){b<-7}
> 
> will alltogeather return
> 
> Fehler in if (a[1] != NA) { : Fehlender Wert, wo TRUE/FALSE n?tig ist
> 
> (or simularly). Somehow this is logical. But how else should I get out,
> whether a certain vector-component has an existing value?

see ?is.na

Hence:
   if(!is.na(a[1])) b <- 7

Uwe Ligges


> Thank you in advance!
> Yours,
> Mag. Ferri Leberl
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From mschwartz at mn.rr.com  Fri Sep 22 20:32:57 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Fri, 22 Sep 2006 13:32:57 -0500
Subject: [R] inequality with NA
In-Reply-To: <1158949017.5919.7.camel@vulcania>
References: <1158949017.5919.7.camel@vulcania>
Message-ID: <1158949977.9548.9.camel@localhost.localdomain>

On Fri, 2006-09-22 at 20:16 +0200, Mag. Ferri Leberl wrote:
> Dear everybody!
> take a<-c(5,3,NA,6).
> 
> if(a[1]!=NA){b<-7}
> if(a[3]!=5){b<-7}
> if(a[3]!=NA){b<-7}
> if(a[3]==NA){b<-7}
> 
> will alltogeather return
> 
> Fehler in if (a[1] != NA) { : Fehlender Wert, wo TRUE/FALSE n?tig ist
> 
> (or simularly). Somehow this is logical. But how else should I get out,
> whether a certain vector-component has an existing value?
> Thank you in advance!
> Yours,
> Mag. Ferri Leberl

NA is not defined, so you cannot predictably perform equality/inequality
tests with it. There are specific functions in place for dealing with
this.

See ?is.na and ?na.omit

> a
[1]  5  3 NA  6

> a[is.na(a)]
[1] NA

> a[!is.na(a)]
[1] 5 3 6


You can also use which() to find the indices:

> which(is.na(a))
[1] 3

> which(!is.na(a))
[1] 1 2 4


Finally, use na.omit() to remove all NA's:

> na.omit(a)
[1] 5 3 6
attr(,"na.action")
[1] 3
attr(,"class")
[1] "omit"

Note that the object attribute 'na.action' shows that a[3] was removed:

> a.omit <- na.omit(a)

> as.vector(attr(a.omit, "na.action"))
[1] 3

HTH,

Marc Schwartz


From whit.armstrong at hcmny.com  Fri Sep 22 20:39:48 2006
From: whit.armstrong at hcmny.com (Armstrong, Whit)
Date: Fri, 22 Sep 2006 14:39:48 -0400
Subject: [R] behavior of [<-.foo
Message-ID: <E58BE6136618CF4C964F6EC7773AE569B4FE87@ex4.nyc.hcmny.com>

Can someone help me understand the following behavior of "[<-" ?

If I define a simple class based on a matrix, the [<- operation only
inserts into the first column:


> x <- matrix(rnorm(10),nrow=5,ncol=2)
>  class(x) <- "foo"
> "[<-.foo" <- function(x, i, j, value) {
+     if(missing(i)) i <- 1:nrow(x)
+     if(missing(j)) j <- 1:ncol(x)
+ 
+     x <- unclass(x)
+     x <- NextMethod(.Generic)
+     class(x) <- "foo"
+     x
+ }
> 
> x[] <- 100.0
> x
     [,1]       [,2]
[1,]  100 -0.1465296
[2,]  100 -0.2615796
[3,]  100 -0.8882629
[4,]  100 -0.2886357
[5,]  100 -0.9565273
attr(,"class")
[1] "foo"

Based on the behavior of [<- for a matrix, I would have thought that the
data for the whole object would be replaced.

for instance:


> y <- matrix(rnorm(10),nrow=5,ncol=2)
> y
            [,1]       [,2]
[1,] -0.55297049 -1.1896488
[2,]  0.06157438 -0.6628254
[3,] -0.28184208 -2.5260177
[4,]  0.61204398 -0.3492488
[5,]  0.43971216  1.8990789
> y[] <- 100
> y
     [,1] [,2]
[1,]  100  100
[2,]  100  100
[3,]  100  100
[4,]  100  100
[5,]  100  100
> 


Thanks,
Whit


code for above:

x <- matrix(rnorm(10),nrow=5,ncol=2)
x
 class(x) <- "foo"
"[<-.foo" <- function(x, i, j, value) {
    if(missing(i)) i <- 1:nrow(x)
    if(missing(j)) j <- 1:ncol(x)
    x <- unclass(x)
    x <- NextMethod(.Generic)
    class(x) <- "foo"
    x
}
x[] <- 100.0
x

> R.Version()
$platform
[1] "i686-pc-linux-gnu"

$arch
[1] "i686"

$os
[1] "linux-gnu"

$system
[1] "i686, linux-gnu"

$status
[1] ""

$major
[1] "2"

$minor
[1] "3.1"

$year
[1] "2006"

$month
[1] "06"

$day
[1] "01"

$`svn rev`
[1] "38247"

$language
[1] "R"

$version.string
[1] "Version 2.3.1 (2006-06-01)"




This e-mail message is intended only for the named recipient(s) above. It may contain confidential information. If you are not the intended recipient you are hereby notified that any dissemination, distribution or copying of this e-mail and any attachment(s) is strictly prohibited. If you have received this e-mail in error, please immediately notify the sender by replying to this e-mail and delete the message and any attachment(s) from your system. Thank you.


From jeff.horner at vanderbilt.edu  Fri Sep 22 20:46:52 2006
From: jeff.horner at vanderbilt.edu (Jeffrey Horner)
Date: Fri, 22 Sep 2006 13:46:52 -0500
Subject: [R] Creating Movies with R
In-Reply-To: <a2b3004b0609220023ic3eb836k27c363e54416359c@mail.gmail.com>
References: <a2b3004b0609220023ic3eb836k27c363e54416359c@mail.gmail.com>
Message-ID: <45142F9C.7020606@vanderbilt.edu>

If you run R on Linux, then you can run the ImageMagick command called 
convert. I place this in an R function to use a sequence of PNG plots as 
movie frames:

make.mov.plotcol3d <- function(){
     unlink("plotcol3d.mpg")
     system("convert -delay 10 plotcol3d*.png plotcol3d.mpg")
}

Examples can be seen here:

http://biostat.mc.vanderbilt.edu/JrhRgbColorSpace

Look for the 'Download Movie' links.

Cheers,

Jeff

Lorenzo Isella wrote:
> Dear All,
> 
> I'd like to know if it is possible to create animations with R.
> To be specific, I attach a code I am using for my research to plot
> some analytical results in 3D using the lattice package. It is not
> necessary to go through the code.
> Simply, it plots some 3D density profiles at two different times
> selected by the user.
> I wonder if it is possible to use the data generated for different
> times to create something like an .avi file.
> 
> Here is the script:
> 
> rm(list=ls())
> library(lattice)
> 
> # I start defining the analytical functions needed to get the density
> as a function of time
> 
> expect_position <- function(t,lam1,lam2,pos_ini,vel_ini)
> {1/(lam1-lam2)*(lam1*exp(lam2*t)-lam2*exp(lam1*t))*pos_ini+
> 1/(lam1-lam2)*(exp(lam1*t)-exp(lam2*t))*vel_ini
> }
> 
> sigma_pos<-function(t,q,lam1,lam2)
> {
> q/(lam1-lam2)^2*(
> (exp(2*lam1*t)-1)/(2*lam1)-2/(lam1+lam2)*(exp(lam1*t+lam2*t)-1) +
> (exp(2*lam2*t)-1)/(2*lam2) )
> }
> 
> rho_x<-function(x,expect_position,sigma_pos)
> {
> 1/sqrt(2*pi*sigma_pos)*exp(-1/2*(x-expect_position)^2/sigma_pos)
> }
> 
> #### Now the physical parameters
> tau<-0.1
> beta<-1/tau
> St<-tau ### since I am in dimensionless units and tau is already in
> units of 1/|alpha|
> D=2e-2
> q<-2*beta^2*D
> ############### Now the grid in space and time
> time<-5  # time extent
> tsteps<-501 # time steps
> newtime<-seq(0,time,len=tsteps)
> #### Now the things specific for the dynamics along x
> lam1<- -beta/2*(1+sqrt(1+4*St))
> lam2<- -beta/2*(1-sqrt(1+4*St))
> xmin<- -0.5
> xmax<-0.5
> x0<-0.1
> vx0<-x0
> nx<-101 ## grid intervals along x
> newx<-seq(xmin,xmax,len=nx) # grid along x
> 
> # M1 <- do.call("g", c(list(x = newx), mypar))
> 
> 
> mypar<-c(q,lam1,lam2)
> sig_xx<-do.call("sigma_pos",c(list(t=newtime),mypar))
> mypar<-c(lam1,lam2,x0,vx0)
> exp_x<-do.call("expect_position",c(list(t=newtime),mypar))
> 
> #rho_x<-function(x,expect_position,sigma_pos)
> 
> #NB: at t=0, the density blows up, since I have a delta as the initial state!
> # At any t>0, instead, the result is finite.
> #for this reason I now redefine time by getting rid of the istant t=0
> to work out
> # the density
> 
> 
> rho_x_t<-matrix(ncol=nx,nrow=tsteps-1)
> for (i in 2:tsteps)
> {mypar<-c(exp_x[i],sig_xx[i])
> myrho_x<-do.call("rho_x",c(list(x=newx),mypar))
> rho_x_t[ i-1, ]<-myrho_x
> }
> 
> ### Now I also define a scaled density
> 
> rho_x_t_scaled<-matrix(ncol=nx,nrow=tsteps-1)
> for (i in 2:tsteps)
> {mypar<-c(exp_x[i],sig_xx[i])
> myrho_x<-do.call("rho_x",c(list(x=newx),mypar))
> rho_x_t_scaled[ i-1, ]<-myrho_x/max(myrho_x)
> }
> 
> ###########Now I deal with the dynamics along y
> 
> lam1<- -beta/2*(1+sqrt(1-4*St))
> lam2<- -beta/2*(1-sqrt(1-4*St))
> ymin<- 0
> ymax<- 1
> y0<-ymax
> vy0<- -y0
> 
> mypar<-c(q,lam1,lam2)
> sig_yy<-do.call("sigma_pos",c(list(t=newtime),mypar))
> mypar<-c(lam1,lam2,y0,vy0)
> exp_y<-do.call("expect_position",c(list(t=newtime),mypar))
> 
> 
> # now I introduce the function giving the density along y: this has to
> include the BC of zero
> # density at wall
> 
> rho_y<-function(y,expect_position,sigma_pos)
> {
> 1/sqrt(2*pi*sigma_pos)*exp(-1/2*(y-expect_position)^2/sigma_pos)-
> 1/sqrt(2*pi*sigma_pos)*exp(-1/2*(y+expect_position)^2/sigma_pos)
> }
> 
> newy<-seq(ymin,ymax,len=nx) # grid along y with the same # of points
> as the one along x
> 
> 
> rho_y_t<-matrix(ncol=nx,nrow=tsteps-1)
> for (i in 2:tsteps)
> {mypar<-c(exp_y[i],sig_yy[i])
> myrho_y<-do.call("rho_y",c(list(y=newy),mypar))
> rho_y_t[ i-1, ]<-myrho_y
> }
> 
> rho_y_t_scaled<-matrix(ncol=nx,nrow=tsteps-1)
> for (i in 2:tsteps)
> {mypar<-c(exp_y[i],sig_yy[i])
> myrho_y<-do.call("rho_y",c(list(y=newy),mypar))
> rho_y_t_scaled[ i-1, ]<-myrho_y/max(myrho_y)
> }
> 
> 
> # The following 2 plots are an example of the plots I'd like to use to
> make an animation
> 
> 
> g <- expand.grid(x = newx, y = newy)
> 
> instant<-100
> mydens<-rho_x_t[ instant, ]%o%rho_y_t[ instant, ]/(max(rho_x_t[
> instant, ]%o%rho_y_t[ instant, ]))
> 
> 
> lentot<-nx^2
> dim(mydens)<-c(lentot,1)
> 
> g$z<-mydens
> jpeg("dens-t-3.jpeg")
> print(wireframe(z ~ x * y, g, drape = TRUE,shade=TRUE,
> scales = list(arrows = FALSE),pretty=FALSE, aspect = c(1,1), colorkey = TRUE
> ,zoom=0.8, main=expression("Density at t=2"), zlab =
> list(expression("density"),rot = 90),distance=0.0,
> perspective=TRUE,#screen = list(z = 150, x = -55,y= 0)
> ,zlim=range(c(0,1))))
> dev.off()
> 
> 
> instant<-300
> mydens<-rho_x_t[ instant, ]%o%rho_y_t[ instant, ]/(max(rho_x_t[
> instant, ]%o%rho_y_t[ instant, ]))
> 
> 
> lentot<-nx^2
> dim(mydens)<-c(lentot,1)
> 
> g$z<-mydens
> jpeg("dens-t-3.jpeg")
> print(wireframe(z ~ x * y, g, drape = TRUE,shade=TRUE,
> scales = list(arrows = FALSE),pretty=FALSE, aspect = c(1,1), colorkey = TRUE
> ,zoom=0.8, main=expression("Density at t=3"), zlab =
> list(expression("density"),rot = 90),distance=0.0,
> perspective=TRUE,#screen = list(z = 150, x = -55,y= 0)
> ,zlim=range(c(0,1))))
> dev.off()
> 
> 
> 
> 
> Kind Regards
> 
> Lorenzo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


-- 
http://biostat.mc.vanderbilt.edu/JeffreyHorner


From ggrothendieck at gmail.com  Fri Sep 22 20:59:49 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 22 Sep 2006 14:59:49 -0400
Subject: [R] behavior of [<-.foo
In-Reply-To: <E58BE6136618CF4C964F6EC7773AE569B4FE87@ex4.nyc.hcmny.com>
References: <E58BE6136618CF4C964F6EC7773AE569B4FE87@ex4.nyc.hcmny.com>
Message-ID: <971536df0609221159k4f2fb232m41ad634954a28c5e@mail.gmail.com>

Try this:



x <- matrix(rnorm(10),nrow=5,ncol=2)
class(x) <- "foo"

"[<-.foo" <- function(x, i = TRUE, j = TRUE, ..., value) {
    x <- unclass(x)
    x <- NextMethod()
    class(x) <- "foo"
    x
}

x[] <- 100.0




On 9/22/06, Armstrong, Whit <whit.armstrong at hcmny.com> wrote:
> Can someone help me understand the following behavior of "[<-" ?
>
> If I define a simple class based on a matrix, the [<- operation only
> inserts into the first column:
>
>
> > x <- matrix(rnorm(10),nrow=5,ncol=2)
> >  class(x) <- "foo"
> > "[<-.foo" <- function(x, i, j, value) {
> +     if(missing(i)) i <- 1:nrow(x)
> +     if(missing(j)) j <- 1:ncol(x)
> +
> +     x <- unclass(x)
> +     x <- NextMethod(.Generic)
> +     class(x) <- "foo"
> +     x
> + }
> >
> > x[] <- 100.0
> > x
>     [,1]       [,2]
> [1,]  100 -0.1465296
> [2,]  100 -0.2615796
> [3,]  100 -0.8882629
> [4,]  100 -0.2886357
> [5,]  100 -0.9565273
> attr(,"class")
> [1] "foo"
>
> Based on the behavior of [<- for a matrix, I would have thought that the
> data for the whole object would be replaced.
>
> for instance:
>
>
> > y <- matrix(rnorm(10),nrow=5,ncol=2)
> > y
>            [,1]       [,2]
> [1,] -0.55297049 -1.1896488
> [2,]  0.06157438 -0.6628254
> [3,] -0.28184208 -2.5260177
> [4,]  0.61204398 -0.3492488
> [5,]  0.43971216  1.8990789
> > y[] <- 100
> > y
>     [,1] [,2]
> [1,]  100  100
> [2,]  100  100
> [3,]  100  100
> [4,]  100  100
> [5,]  100  100
> >
>
>
> Thanks,
> Whit
>
>
> code for above:
>
> x <- matrix(rnorm(10),nrow=5,ncol=2)
> x
>  class(x) <- "foo"
> "[<-.foo" <- function(x, i, j, value) {
>    if(missing(i)) i <- 1:nrow(x)
>    if(missing(j)) j <- 1:ncol(x)
>    x <- unclass(x)
>    x <- NextMethod(.Generic)
>    class(x) <- "foo"
>    x
> }
> x[] <- 100.0
> x
>
> > R.Version()
> $platform
> [1] "i686-pc-linux-gnu"
>
> $arch
> [1] "i686"
>
> $os
> [1] "linux-gnu"
>
> $system
> [1] "i686, linux-gnu"
>
> $status
> [1] ""
>
> $major
> [1] "2"
>
> $minor
> [1] "3.1"
>
> $year
> [1] "2006"
>
> $month
> [1] "06"
>
> $day
> [1] "01"
>
> $`svn rev`
> [1] "38247"
>
> $language
> [1] "R"
>
> $version.string
> [1] "Version 2.3.1 (2006-06-01)"
>
>
>
>
> This e-mail message is intended only for the named recipient(s) above. It may contain confidential information. If you are not the intended recipient you are hereby notified that any dissemination, distribution or copying of this e-mail and any attachment(s) is strictly prohibited. If you have received this e-mail in error, please immediately notify the sender by replying to this e-mail and delete the message and any attachment(s) from your system. Thank you.
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


From lockwood at rand.org  Fri Sep 22 21:02:42 2006
From: lockwood at rand.org (J.R. Lockwood)
Date: Fri, 22 Sep 2006 15:02:42 -0400 (EDT)
Subject: [R] Creating Movies with R
In-Reply-To: <45142F9C.7020606@vanderbilt.edu>
References: <a2b3004b0609220023ic3eb836k27c363e54416359c@mail.gmail.com>
	<45142F9C.7020606@vanderbilt.edu>
Message-ID: <Pine.LNX.4.58.0609221456480.21306@penguin.rand.org>

An alternative that I've used a few times is the jpg() function to
create the sequence of images, and then converting these to an mpeg
movie using "mencoder" distributed with "mplayer".  This works on both
windows and linux.  I have a pretty self-contained example file
written up that I can send to anyone who is interested.  Oddly, the
most challenging part was creating a sequence of file names that would
be correctly ordered - for this I use:

lex <- function(N){
  ## produce vector of N lexicograpically ordered strings
  ndig <- nchar(N)
  substr(formatC((1:N)/10^ndig,digits=ndig,format="f"),3,10000000)
}


On Fri, 22 Sep 2006, Jeffrey Horner wrote:

> Date: Fri, 22 Sep 2006 13:46:52 -0500
> From: Jeffrey Horner <jeff.horner at vanderbilt.edu>
> To: Lorenzo Isella <lorenzo.isella at gmail.com>, r-help at stat.math.ethz.ch
> Subject: Re: [R] Creating Movies with R
> 
> If you run R on Linux, then you can run the ImageMagick command called 
> convert. I place this in an R function to use a sequence of PNG plots as 
> movie frames:
> 
> make.mov.plotcol3d <- function(){
>      unlink("plotcol3d.mpg")
>      system("convert -delay 10 plotcol3d*.png plotcol3d.mpg")
> }
> 
> Examples can be seen here:
> 
> http://biostat.mc.vanderbilt.edu/JrhRgbColorSpace
> 
> Look for the 'Download Movie' links.
> 
> Cheers,
> 
> Jeff
> 
> Lorenzo Isella wrote:
> > Dear All,
> > 
> > I'd like to know if it is possible to create animations with R.
> > To be specific, I attach a code I am using for my research to plot
> > some analytical results in 3D using the lattice package. It is not
> > necessary to go through the code.
> > Simply, it plots some 3D density profiles at two different times
> > selected by the user.
> > I wonder if it is possible to use the data generated for different
> > times to create something like an .avi file.
> > 
> > Here is the script:
> > 
> > rm(list=ls())
> > library(lattice)
> > 
> > # I start defining the analytical functions needed to get the density
> > as a function of time
> > 
> > expect_position <- function(t,lam1,lam2,pos_ini,vel_ini)
> > {1/(lam1-lam2)*(lam1*exp(lam2*t)-lam2*exp(lam1*t))*pos_ini+
> > 1/(lam1-lam2)*(exp(lam1*t)-exp(lam2*t))*vel_ini
> > }
> > 
> > sigma_pos<-function(t,q,lam1,lam2)
> > {
> > q/(lam1-lam2)^2*(
> > (exp(2*lam1*t)-1)/(2*lam1)-2/(lam1+lam2)*(exp(lam1*t+lam2*t)-1) +
> > (exp(2*lam2*t)-1)/(2*lam2) )
> > }
> > 
> > rho_x<-function(x,expect_position,sigma_pos)
> > {
> > 1/sqrt(2*pi*sigma_pos)*exp(-1/2*(x-expect_position)^2/sigma_pos)
> > }
> > 
> > #### Now the physical parameters
> > tau<-0.1
> > beta<-1/tau
> > St<-tau ### since I am in dimensionless units and tau is already in
> > units of 1/|alpha|
> > D=2e-2
> > q<-2*beta^2*D
> > ############### Now the grid in space and time
> > time<-5  # time extent
> > tsteps<-501 # time steps
> > newtime<-seq(0,time,len=tsteps)
> > #### Now the things specific for the dynamics along x
> > lam1<- -beta/2*(1+sqrt(1+4*St))
> > lam2<- -beta/2*(1-sqrt(1+4*St))
> > xmin<- -0.5
> > xmax<-0.5
> > x0<-0.1
> > vx0<-x0
> > nx<-101 ## grid intervals along x
> > newx<-seq(xmin,xmax,len=nx) # grid along x
> > 
> > # M1 <- do.call("g", c(list(x = newx), mypar))
> > 
> > 
> > mypar<-c(q,lam1,lam2)
> > sig_xx<-do.call("sigma_pos",c(list(t=newtime),mypar))
> > mypar<-c(lam1,lam2,x0,vx0)
> > exp_x<-do.call("expect_position",c(list(t=newtime),mypar))
> > 
> > #rho_x<-function(x,expect_position,sigma_pos)
> > 
> > #NB: at t=0, the density blows up, since I have a delta as the initial state!
> > # At any t>0, instead, the result is finite.
> > #for this reason I now redefine time by getting rid of the istant t=0
> > to work out
> > # the density
> > 
> > 
> > rho_x_t<-matrix(ncol=nx,nrow=tsteps-1)
> > for (i in 2:tsteps)
> > {mypar<-c(exp_x[i],sig_xx[i])
> > myrho_x<-do.call("rho_x",c(list(x=newx),mypar))
> > rho_x_t[ i-1, ]<-myrho_x
> > }
> > 
> > ### Now I also define a scaled density
> > 
> > rho_x_t_scaled<-matrix(ncol=nx,nrow=tsteps-1)
> > for (i in 2:tsteps)
> > {mypar<-c(exp_x[i],sig_xx[i])
> > myrho_x<-do.call("rho_x",c(list(x=newx),mypar))
> > rho_x_t_scaled[ i-1, ]<-myrho_x/max(myrho_x)
> > }
> > 
> > ###########Now I deal with the dynamics along y
> > 
> > lam1<- -beta/2*(1+sqrt(1-4*St))
> > lam2<- -beta/2*(1-sqrt(1-4*St))
> > ymin<- 0
> > ymax<- 1
> > y0<-ymax
> > vy0<- -y0
> > 
> > mypar<-c(q,lam1,lam2)
> > sig_yy<-do.call("sigma_pos",c(list(t=newtime),mypar))
> > mypar<-c(lam1,lam2,y0,vy0)
> > exp_y<-do.call("expect_position",c(list(t=newtime),mypar))
> > 
> > 
> > # now I introduce the function giving the density along y: this has to
> > include the BC of zero
> > # density at wall
> > 
> > rho_y<-function(y,expect_position,sigma_pos)
> > {
> > 1/sqrt(2*pi*sigma_pos)*exp(-1/2*(y-expect_position)^2/sigma_pos)-
> > 1/sqrt(2*pi*sigma_pos)*exp(-1/2*(y+expect_position)^2/sigma_pos)
> > }
> > 
> > newy<-seq(ymin,ymax,len=nx) # grid along y with the same # of points
> > as the one along x
> > 
> > 
> > rho_y_t<-matrix(ncol=nx,nrow=tsteps-1)
> > for (i in 2:tsteps)
> > {mypar<-c(exp_y[i],sig_yy[i])
> > myrho_y<-do.call("rho_y",c(list(y=newy),mypar))
> > rho_y_t[ i-1, ]<-myrho_y
> > }
> > 
> > rho_y_t_scaled<-matrix(ncol=nx,nrow=tsteps-1)
> > for (i in 2:tsteps)
> > {mypar<-c(exp_y[i],sig_yy[i])
> > myrho_y<-do.call("rho_y",c(list(y=newy),mypar))
> > rho_y_t_scaled[ i-1, ]<-myrho_y/max(myrho_y)
> > }
> > 
> > 
> > # The following 2 plots are an example of the plots I'd like to use to
> > make an animation
> > 
> > 
> > g <- expand.grid(x = newx, y = newy)
> > 
> > instant<-100
> > mydens<-rho_x_t[ instant, ]%o%rho_y_t[ instant, ]/(max(rho_x_t[
> > instant, ]%o%rho_y_t[ instant, ]))
> > 
> > 
> > lentot<-nx^2
> > dim(mydens)<-c(lentot,1)
> > 
> > g$z<-mydens
> > jpeg("dens-t-3.jpeg")
> > print(wireframe(z ~ x * y, g, drape = TRUE,shade=TRUE,
> > scales = list(arrows = FALSE),pretty=FALSE, aspect = c(1,1), colorkey = TRUE
> > ,zoom=0.8, main=expression("Density at t=2"), zlab =
> > list(expression("density"),rot = 90),distance=0.0,
> > perspective=TRUE,#screen = list(z = 150, x = -55,y= 0)
> > ,zlim=range(c(0,1))))
> > dev.off()
> > 
> > 
> > instant<-300
> > mydens<-rho_x_t[ instant, ]%o%rho_y_t[ instant, ]/(max(rho_x_t[
> > instant, ]%o%rho_y_t[ instant, ]))
> > 
> > 
> > lentot<-nx^2
> > dim(mydens)<-c(lentot,1)
> > 
> > g$z<-mydens
> > jpeg("dens-t-3.jpeg")
> > print(wireframe(z ~ x * y, g, drape = TRUE,shade=TRUE,
> > scales = list(arrows = FALSE),pretty=FALSE, aspect = c(1,1), colorkey = TRUE
> > ,zoom=0.8, main=expression("Density at t=3"), zlab =
> > list(expression("density"),rot = 90),distance=0.0,
> > perspective=TRUE,#screen = list(z = 150, x = -55,y= 0)
> > ,zlim=range(c(0,1))))
> > dev.off()
> > 
> > 
> > 
> > 
> > Kind Regards
> > 
> > Lorenzo
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> 
> -- 
> http://biostat.mc.vanderbilt.edu/JeffreyHorner
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

J.R. Lockwood
412-683-2300 x4941
lockwood at rand.org
http://www.rand.org/statistics/bios/

--------------------

This email message is for the sole use of the intended recip...{{dropped}}


From Ted.Harding at nessie.mcc.ac.uk  Fri Sep 22 21:25:15 2006
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 22 Sep 2006 20:25:15 +0100 (BST)
Subject: [R] "logistic" + "neg binomial" + ...
Message-ID: <XFMail.060922202515.Ted.Harding@nessie.mcc.ac.uk>

Hi Folks,

I've just come across a kind of problem which leads
me to wonder how to approach it in R.

Basically, each a set of items is subjected to a series
of "impacts" until it eventually "fails". The "force"
of each impact would depend on  covariates X,Y say;
but as a result of preceding impacts an item would be
expected to have a "cumulative frailty" such that the
probability of failure due to a particular impact would
possibly increase according to the series of impacts
already survived.

Without the "cumulative frailty" one could envisage
something like a logistic model for the probabiliy
of failure at each impact, leading to a kind of
generalised "exponential distribution" -- that is,
the likelihood for each item would be of the form

  (1-P[1])*(1-P[2])*...*(1-P[n-1])*P[n]

where P[i] could have a logistic model in terms of
the values of X[i] and Y[i], and n is the index of
the impact at which failure occurred. That is then
a solvable problem.

Even so, I'm not (so far) finding in the R resources
the appropriate analogue of glm for this kind of
model. I dare say a prolonged trawl through the various
"survival" resources might lead to something applicable,
but ...

And then there's the cumulative frailty ... !

Suggestions welcome!

With thanks,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 22-Sep-06                                       Time: 20:25:12
------------------------------ XFMail ------------------------------


From ggrothendieck at gmail.com  Fri Sep 22 21:26:02 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 22 Sep 2006 15:26:02 -0400
Subject: [R] Creating Movies with R
In-Reply-To: <Pine.LNX.4.58.0609221456480.21306@penguin.rand.org>
References: <a2b3004b0609220023ic3eb836k27c363e54416359c@mail.gmail.com>
	<45142F9C.7020606@vanderbilt.edu>
	<Pine.LNX.4.58.0609221456480.21306@penguin.rand.org>
Message-ID: <971536df0609221226k39be7dednef9dd2220150d040@mail.gmail.com>

See the flag= argument on formatC:

n <- 10
formatC(1:n, dig = nchar(n)-1, flag = 0)

# Here is another way:

n <- 10
sprintf(paste("%0", nchar(n), ".0f", sep = ""), 1:n)


On 9/22/06, J.R. Lockwood <lockwood at rand.org> wrote:
> An alternative that I've used a few times is the jpg() function to
> create the sequence of images, and then converting these to an mpeg
> movie using "mencoder" distributed with "mplayer".  This works on both
> windows and linux.  I have a pretty self-contained example file
> written up that I can send to anyone who is interested.  Oddly, the
> most challenging part was creating a sequence of file names that would
> be correctly ordered - for this I use:
>
> lex <- function(N){
>  ## produce vector of N lexicograpically ordered strings
>  ndig <- nchar(N)
>  substr(formatC((1:N)/10^ndig,digits=ndig,format="f"),3,10000000)
> }
>
>
> On Fri, 22 Sep 2006, Jeffrey Horner wrote:
>
> > Date: Fri, 22 Sep 2006 13:46:52 -0500
> > From: Jeffrey Horner <jeff.horner at vanderbilt.edu>
> > To: Lorenzo Isella <lorenzo.isella at gmail.com>, r-help at stat.math.ethz.ch
> > Subject: Re: [R] Creating Movies with R
> >
> > If you run R on Linux, then you can run the ImageMagick command called
> > convert. I place this in an R function to use a sequence of PNG plots as
> > movie frames:
> >
> > make.mov.plotcol3d <- function(){
> >      unlink("plotcol3d.mpg")
> >      system("convert -delay 10 plotcol3d*.png plotcol3d.mpg")
> > }
> >
> > Examples can be seen here:
> >
> > http://biostat.mc.vanderbilt.edu/JrhRgbColorSpace
> >
> > Look for the 'Download Movie' links.
> >
> > Cheers,
> >
> > Jeff
> >
> > Lorenzo Isella wrote:
> > > Dear All,
> > >
> > > I'd like to know if it is possible to create animations with R.
> > > To be specific, I attach a code I am using for my research to plot
> > > some analytical results in 3D using the lattice package. It is not
> > > necessary to go through the code.
> > > Simply, it plots some 3D density profiles at two different times
> > > selected by the user.
> > > I wonder if it is possible to use the data generated for different
> > > times to create something like an .avi file.
> > >
> > > Here is the script:
> > >
> > > rm(list=ls())
> > > library(lattice)
> > >
> > > # I start defining the analytical functions needed to get the density
> > > as a function of time
> > >
> > > expect_position <- function(t,lam1,lam2,pos_ini,vel_ini)
> > > {1/(lam1-lam2)*(lam1*exp(lam2*t)-lam2*exp(lam1*t))*pos_ini+
> > > 1/(lam1-lam2)*(exp(lam1*t)-exp(lam2*t))*vel_ini
> > > }
> > >
> > > sigma_pos<-function(t,q,lam1,lam2)
> > > {
> > > q/(lam1-lam2)^2*(
> > > (exp(2*lam1*t)-1)/(2*lam1)-2/(lam1+lam2)*(exp(lam1*t+lam2*t)-1) +
> > > (exp(2*lam2*t)-1)/(2*lam2) )
> > > }
> > >
> > > rho_x<-function(x,expect_position,sigma_pos)
> > > {
> > > 1/sqrt(2*pi*sigma_pos)*exp(-1/2*(x-expect_position)^2/sigma_pos)
> > > }
> > >
> > > #### Now the physical parameters
> > > tau<-0.1
> > > beta<-1/tau
> > > St<-tau ### since I am in dimensionless units and tau is already in
> > > units of 1/|alpha|
> > > D=2e-2
> > > q<-2*beta^2*D
> > > ############### Now the grid in space and time
> > > time<-5  # time extent
> > > tsteps<-501 # time steps
> > > newtime<-seq(0,time,len=tsteps)
> > > #### Now the things specific for the dynamics along x
> > > lam1<- -beta/2*(1+sqrt(1+4*St))
> > > lam2<- -beta/2*(1-sqrt(1+4*St))
> > > xmin<- -0.5
> > > xmax<-0.5
> > > x0<-0.1
> > > vx0<-x0
> > > nx<-101 ## grid intervals along x
> > > newx<-seq(xmin,xmax,len=nx) # grid along x
> > >
> > > # M1 <- do.call("g", c(list(x = newx), mypar))
> > >
> > >
> > > mypar<-c(q,lam1,lam2)
> > > sig_xx<-do.call("sigma_pos",c(list(t=newtime),mypar))
> > > mypar<-c(lam1,lam2,x0,vx0)
> > > exp_x<-do.call("expect_position",c(list(t=newtime),mypar))
> > >
> > > #rho_x<-function(x,expect_position,sigma_pos)
> > >
> > > #NB: at t=0, the density blows up, since I have a delta as the initial state!
> > > # At any t>0, instead, the result is finite.
> > > #for this reason I now redefine time by getting rid of the istant t=0
> > > to work out
> > > # the density
> > >
> > >
> > > rho_x_t<-matrix(ncol=nx,nrow=tsteps-1)
> > > for (i in 2:tsteps)
> > > {mypar<-c(exp_x[i],sig_xx[i])
> > > myrho_x<-do.call("rho_x",c(list(x=newx),mypar))
> > > rho_x_t[ i-1, ]<-myrho_x
> > > }
> > >
> > > ### Now I also define a scaled density
> > >
> > > rho_x_t_scaled<-matrix(ncol=nx,nrow=tsteps-1)
> > > for (i in 2:tsteps)
> > > {mypar<-c(exp_x[i],sig_xx[i])
> > > myrho_x<-do.call("rho_x",c(list(x=newx),mypar))
> > > rho_x_t_scaled[ i-1, ]<-myrho_x/max(myrho_x)
> > > }
> > >
> > > ###########Now I deal with the dynamics along y
> > >
> > > lam1<- -beta/2*(1+sqrt(1-4*St))
> > > lam2<- -beta/2*(1-sqrt(1-4*St))
> > > ymin<- 0
> > > ymax<- 1
> > > y0<-ymax
> > > vy0<- -y0
> > >
> > > mypar<-c(q,lam1,lam2)
> > > sig_yy<-do.call("sigma_pos",c(list(t=newtime),mypar))
> > > mypar<-c(lam1,lam2,y0,vy0)
> > > exp_y<-do.call("expect_position",c(list(t=newtime),mypar))
> > >
> > >
> > > # now I introduce the function giving the density along y: this has to
> > > include the BC of zero
> > > # density at wall
> > >
> > > rho_y<-function(y,expect_position,sigma_pos)
> > > {
> > > 1/sqrt(2*pi*sigma_pos)*exp(-1/2*(y-expect_position)^2/sigma_pos)-
> > > 1/sqrt(2*pi*sigma_pos)*exp(-1/2*(y+expect_position)^2/sigma_pos)
> > > }
> > >
> > > newy<-seq(ymin,ymax,len=nx) # grid along y with the same # of points
> > > as the one along x
> > >
> > >
> > > rho_y_t<-matrix(ncol=nx,nrow=tsteps-1)
> > > for (i in 2:tsteps)
> > > {mypar<-c(exp_y[i],sig_yy[i])
> > > myrho_y<-do.call("rho_y",c(list(y=newy),mypar))
> > > rho_y_t[ i-1, ]<-myrho_y
> > > }
> > >
> > > rho_y_t_scaled<-matrix(ncol=nx,nrow=tsteps-1)
> > > for (i in 2:tsteps)
> > > {mypar<-c(exp_y[i],sig_yy[i])
> > > myrho_y<-do.call("rho_y",c(list(y=newy),mypar))
> > > rho_y_t_scaled[ i-1, ]<-myrho_y/max(myrho_y)
> > > }
> > >
> > >
> > > # The following 2 plots are an example of the plots I'd like to use to
> > > make an animation
> > >
> > >
> > > g <- expand.grid(x = newx, y = newy)
> > >
> > > instant<-100
> > > mydens<-rho_x_t[ instant, ]%o%rho_y_t[ instant, ]/(max(rho_x_t[
> > > instant, ]%o%rho_y_t[ instant, ]))
> > >
> > >
> > > lentot<-nx^2
> > > dim(mydens)<-c(lentot,1)
> > >
> > > g$z<-mydens
> > > jpeg("dens-t-3.jpeg")
> > > print(wireframe(z ~ x * y, g, drape = TRUE,shade=TRUE,
> > > scales = list(arrows = FALSE),pretty=FALSE, aspect = c(1,1), colorkey = TRUE
> > > ,zoom=0.8, main=expression("Density at t=2"), zlab =
> > > list(expression("density"),rot = 90),distance=0.0,
> > > perspective=TRUE,#screen = list(z = 150, x = -55,y= 0)
> > > ,zlim=range(c(0,1))))
> > > dev.off()
> > >
> > >
> > > instant<-300
> > > mydens<-rho_x_t[ instant, ]%o%rho_y_t[ instant, ]/(max(rho_x_t[
> > > instant, ]%o%rho_y_t[ instant, ]))
> > >
> > >
> > > lentot<-nx^2
> > > dim(mydens)<-c(lentot,1)
> > >
> > > g$z<-mydens
> > > jpeg("dens-t-3.jpeg")
> > > print(wireframe(z ~ x * y, g, drape = TRUE,shade=TRUE,
> > > scales = list(arrows = FALSE),pretty=FALSE, aspect = c(1,1), colorkey = TRUE
> > > ,zoom=0.8, main=expression("Density at t=3"), zlab =
> > > list(expression("density"),rot = 90),distance=0.0,
> > > perspective=TRUE,#screen = list(z = 150, x = -55,y= 0)
> > > ,zlim=range(c(0,1))))
> > > dev.off()
> > >
> > >
> > >
> > >
> > > Kind Regards
> > >
> > > Lorenzo
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> >
> > --
> > http://biostat.mc.vanderbilt.edu/JeffreyHorner
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> J.R. Lockwood
> 412-683-2300 x4941
> lockwood at rand.org
> http://www.rand.org/statistics/bios/
>
> --------------------
>
> This email message is for the sole use of the intended recip...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From rduarte at ipimar.pt  Fri Sep 22 21:36:40 2006
From: rduarte at ipimar.pt (Rafael Duarte)
Date: Fri, 22 Sep 2006 20:36:40 +0100
Subject: [R] Lattice strip labels for two factors
Message-ID: <45143B48.3020601@ipimar.pt>

Dear list,
My problem is to change the strip text of lattice panels when using two 
factors.
I have a data frame with two factors:

df <- expand.grid( "fact1"=c("y","b","r"), 
"fact2"=c("far","por","lis","set"), "year"=1991:2000, "value"= NA)
df[,"value"] <- sample(1:50, 120, replace=TRUE)

I can make simple xyplot and change the text of the factor levels with 
strip.custom:

require("lattice")
xyplot( value ~ year | fact1, data=df, type="b", subset= fact2=="far",
strip = strip.custom(bg=gray.colors(1,0.95), factor.levels=c("yellow", 
"black", "red")),
layout=c(1,3)
)

But how can I change the text of the factor levels when using both 
factors as in this plot:
xyplot( value ~ year | fact1*fact2, data=df, type="b")

(fact2 levels text should change to: c("faro","porto","lisbon","setubal"))

I read the help for strip.default and the emails archive, tried with 
"which.given" but could not find out how to accomplish this.

Many thanks,
Rafael Duarte

-- 
Rafael Duarte
Marine Resources Department - DRM
IPIMAR -  National Research Institute for Agriculture and Fisheries
Av. Bras?lia, 1449-006 Lisbon  -  Portugal
Tel:+351 21 302 7000      Fax:+351 21 301 5948
e-mail: rduarte at ipimar.pt


From ggrothendieck at gmail.com  Fri Sep 22 22:00:02 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 22 Sep 2006 16:00:02 -0400
Subject: [R] Lattice strip labels for two factors
In-Reply-To: <45143B48.3020601@ipimar.pt>
References: <45143B48.3020601@ipimar.pt>
Message-ID: <971536df0609221300i4a0a3a21w2914588f6b787b7e@mail.gmail.com>

Try this:

levels(df$fact2) <- c("faro","porto","lisbon","setubal")
xyplot( value ~ year | fact1*fact2, data=df, type="b")


On 9/22/06, Rafael Duarte <rduarte at ipimar.pt> wrote:
> Dear list,
> My problem is to change the strip text of lattice panels when using two
> factors.
> I have a data frame with two factors:
>
> df <- expand.grid( "fact1"=c("y","b","r"),
> "fact2"=c("far","por","lis","set"), "year"=1991:2000, "value"= NA)
> df[,"value"] <- sample(1:50, 120, replace=TRUE)
>
> I can make simple xyplot and change the text of the factor levels with
> strip.custom:
>
> require("lattice")
> xyplot( value ~ year | fact1, data=df, type="b", subset= fact2=="far",
> strip = strip.custom(bg=gray.colors(1,0.95), factor.levels=c("yellow",
> "black", "red")),
> layout=c(1,3)
> )
>
> But how can I change the text of the factor levels when using both
> factors as in this plot:
> xyplot( value ~ year | fact1*fact2, data=df, type="b")
>
> (fact2 levels text should change to: c("faro","porto","lisbon","setubal"))
>
> I read the help for strip.default and the emails archive, tried with
> "which.given" but could not find out how to accomplish this.
>
> Many thanks,
> Rafael Duarte
>
> --
> Rafael Duarte
> Marine Resources Department - DRM
> IPIMAR -  National Research Institute for Agriculture and Fisheries
> Av. Bras?lia, 1449-006 Lisbon  -  Portugal
> Tel:+351 21 302 7000      Fax:+351 21 301 5948
> e-mail: rduarte at ipimar.pt
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From exonintron at gmail.com  Fri Sep 22 22:02:40 2006
From: exonintron at gmail.com (Sender)
Date: Fri, 22 Sep 2006 13:02:40 -0700
Subject: [R] setMethod
Message-ID: <686bf0c50609221302v5f8727cdl4750fcd35ab6074b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060922/2203e547/attachment.pl 

From bli1 at bcm.tmc.edu  Fri Sep 22 22:04:01 2006
From: bli1 at bcm.tmc.edu (Bingshan Li)
Date: Fri, 22 Sep 2006 15:04:01 -0500
Subject: [R] legends in a plot
Message-ID: <E9EE229A-5239-4299-9964-967B6F0F552E@bcm.tmc.edu>

Hi there,

I have the following plot. The circles and the line do not cross in  
the main plot. But in the legend, the circle and the line cross. I am  
wondering if there is a way to make the legend look like the plot  
without crossing. I looked around but did not find a way to do that.  
Is it doable?

plot(x,x^1.5, pch=1, lty=1, type='b')
legend(1,25, legend="hello",pch=1, lty=1)

Thanks and have a great weekend!


From ggrothendieck at gmail.com  Fri Sep 22 22:17:30 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 22 Sep 2006 16:17:30 -0400
Subject: [R] legends in a plot
In-Reply-To: <E9EE229A-5239-4299-9964-967B6F0F552E@bcm.tmc.edu>
References: <E9EE229A-5239-4299-9964-967B6F0F552E@bcm.tmc.edu>
Message-ID: <971536df0609221317obbf368ewa97a67b2a16fb790@mail.gmail.com>

If the main thing you want is just to ensure that the legend and
plot are consistent it would be easier to change the plot than
change the legend:

x <- 1:10
plot(x,x^1.5, pch=1)
lines(x, x^1.5,lty=1)
legend(1,25, legend="hello",pch=1, lty=1)



On 9/22/06, Bingshan Li <bli1 at bcm.tmc.edu> wrote:
> Hi there,
>
> I have the following plot. The circles and the line do not cross in
> the main plot. But in the legend, the circle and the line cross. I am
> wondering if there is a way to make the legend look like the plot
> without crossing. I looked around but did not find a way to do that.
> Is it doable?
>
> plot(x,x^1.5, pch=1, lty=1, type='b')
> legend(1,25, legend="hello",pch=1, lty=1)
>
> Thanks and have a great weekend!
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From LindnerW at t-online.de  Fri Sep 22 23:02:14 2006
From: LindnerW at t-online.de (Wolfgang Lindner)
Date: Fri, 22 Sep 2006 23:02:14 +0200
Subject: [R] Statitics Textbook - any recommendation?
References: <20060920172158.0i94pyn6so7qcksk@webmail.ufrgs.br>
Message-ID: <1GQsA2-0n2Cie0@fwd28.sul.t-online.de>

Iuri Gavronski schrieb:
> Other text I am trying to find is multivariate data analysis (EFA,  
> cluster, mult regression, MANOVA, etc.) with examples with R.

Hi Iuri,

for your second answer I would recommend 
B. Everitt: An R and S-PLUS Companion to Multivariate Analysis. Springer 2005.
isbn 1-85233-882-2.

Best
      Wolfgang
--
privat:      Wolfgang Lindner, Stieglitzweg 6, D-42799 Leichlingen


From pberming at arts.ryerson.ca  Fri Sep 22 23:16:13 2006
From: pberming at arts.ryerson.ca (Philip Bermingham)
Date: Fri, 22 Sep 2006 17:16:13 -0400
Subject: [R] proj4R library will not install
Message-ID: <4CB9D91A4AD04647BCE0305E391423F702A1F6@mail3.arts.ryerson.ca>

I'm hoping someone can help me.  I have downloaded the proj4R.zip and under my version of R (2.3.1) I install the package from local zip file. This worked great.  I then type library(proj4R) to load the library and I get the error: Error in library(proj4R) : 'proj4R' is not a valid package -- installed < 2.0.0?  I have read through the install documentation and have downloaded and unpacked PROJ.4 to c:\proj\ so the bin is located at C:\proj\bin.  I then set the environmental variables PATH which now looks like : %SystemRoot%\system32;%SystemRoot%;%SystemRoot%\System32\Wbem;C:\Program Files\Intel\DMIX;C:\Program Files\UltraEdit;C:\proj and I created a new user variable PROJ_LIB to c:\proj\nad.  I'm not sure if I am missing anything here but I still get the <2.0.0 error.  If you can help me in any way I would truly appreciate it.

Thanks in advance,

Philip Bermingham


From c_naber at yahoo.com.br  Fri Sep 22 23:28:58 2006
From: c_naber at yahoo.com.br (Caio Lucidius Naberezny Azevedo)
Date: Fri, 22 Sep 2006 21:28:58 +0000 (GMT)
Subject: [R] Double integral
Message-ID: <20060922212858.64763.qmail@web56812.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060922/ffafa822/attachment.pl 

From c_naber at yahoo.com.br  Fri Sep 22 23:40:00 2006
From: c_naber at yahoo.com.br (Caio Lucidius Naberezny Azevedo)
Date: Fri, 22 Sep 2006 21:40:00 +0000 (GMT)
Subject: [R] Double integral
Message-ID: <20060922214000.92336.qmail@web56801.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060922/6085d8cf/attachment.pl 

From mschwartz at mn.rr.com  Fri Sep 22 23:52:37 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Fri, 22 Sep 2006 16:52:37 -0500
Subject: [R] proj4R library will not install
In-Reply-To: <4CB9D91A4AD04647BCE0305E391423F702A1F6@mail3.arts.ryerson.ca>
References: <4CB9D91A4AD04647BCE0305E391423F702A1F6@mail3.arts.ryerson.ca>
Message-ID: <1158961957.9548.42.camel@localhost.localdomain>

On Fri, 2006-09-22 at 17:16 -0400, Philip Bermingham wrote:
> I'm hoping someone can help me.  I have downloaded the proj4R.zip and
> under my version of R (2.3.1) I install the package from local zip
> file. This worked great.  I then type library(proj4R) to load the
> library and I get the error: Error in library(proj4R) : 'proj4R' is
> not a valid package -- installed < 2.0.0?  I have read through the
> install documentation and have downloaded and unpacked PROJ.4 to c:
> \proj\ so the bin is located at C:\proj\bin.  I then set the
> environmental variables PATH which now looks like : %SystemRoot%
> \system32;%SystemRoot%;%SystemRoot%\System32\Wbem;C:\Program Files
> \Intel\DMIX;C:\Program Files\UltraEdit;C:\proj and I created a new
> user variable PROJ_LIB to c:\proj\nad.  I'm not sure if I am missing
> anything here but I still get the <2.0.0 error.  If you can help me in
> any way I would truly appreciate it.
> 
> Thanks in advance,
> 
> Philip Bermingham


proj4R is not a base or CRAN R package. Some Googling suggests that the
R package might be deprecated, as it has not been updated for some time
(hence the error msgs) based upon a review of the R related archive
files available at:

http://spatial.nhh.no/R/Devel/

I would suggest communicating with Roger Bivand (who I have cc'd here)
as to the status of the package and any subsequent replacements.

HTH,

Marc Schwartz


From sundar.dorai-raj at pdf.com  Sat Sep 23 00:32:06 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 22 Sep 2006 17:32:06 -0500
Subject: [R] Double integral
In-Reply-To: <20060922214000.92336.qmail@web56801.mail.re3.yahoo.com>
References: <20060922214000.92336.qmail@web56801.mail.re3.yahoo.com>
Message-ID: <45146466.2040002@pdf.com>


Caio Lucidius Naberezny Azevedo said the following on 9/22/2006 4:40 PM:
> Hi all,
>    
>   I need to solve double integrals with no closed solution. Calling x and y the two variables we have x ~ Normal(y*v,1) and y ~Half-Normal(0,1). In fact, given a joint funcion g(x,y), I need evaluate the integral of this function under that random structure. Could anyone suggest me a package or even a suitable method to solve this problem?
>    
>    
>   Thanks all,
>    
>   Caio
> 
>  		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

Have you tried

RSiteSearch("double integral")

as the posting guide suggests?

--sundar


From murdoch at stats.uwo.ca  Sat Sep 23 00:40:01 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 22 Sep 2006 18:40:01 -0400
Subject: [R] setMethod
In-Reply-To: <686bf0c50609221302v5f8727cdl4750fcd35ab6074b@mail.gmail.com>
References: <686bf0c50609221302v5f8727cdl4750fcd35ab6074b@mail.gmail.com>
Message-ID: <45146641.30308@stats.uwo.ca>

On 9/22/2006 4:02 PM, Sender wrote:
> Hello R-help:
> 
> I was hoping someone could help me understand a particular function i came
> across in a package:
> 
> "$.myClass" <- function( x, name ) {
>         sym = paste( "foo", name, sep = "_" )
>          if( is.loaded(sym) )
>              .Call(sym,x)
> }
> 
> I understand the paste, and .Call part, but I'm not sure how this function
> would get called? What exactly is $. ? is it a regular expression? Would a
> user call this method directly, or is it an internal function that gets
> called according to a class/type.

It's the implementation of the "$" function for myClass objects, so if x 
was a myClass object, it would be called when something like

x$name

was used in an expression.  It would call an external function named 
foo_name, passing x as an argument.

Duncan Murdoch

> 
> something similar to:
> 
> plot.lm()
> 
> Thanks in advance !
> 
> Greg
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From helprhelp at gmail.com  Sat Sep 23 01:27:46 2006
From: helprhelp at gmail.com (Weiwei Shi)
Date: Fri, 22 Sep 2006 19:27:46 -0400
Subject: [R] two questions associated with heatmap
Message-ID: <cdf817830609221627r2cbfef7ey4dd72e5ab03580cd@mail.gmail.com>

hi, there:
i have 2 questions associated with heatmap

in heatmap.2{gplot}, there is a bar called "raw z-score" showing the
coloring legend for each pixel. Where can I find that z-score's
formulae?

question 2,
for example, I have 5 groups and I want to label each group with a
color name from "#FF0000" to "#0000FF" evenly. so basically i need a
vector like this:
c("#FF0000", ?, ?, ?, "#0000FF")

the number of groups can be 10 or whatever.

thanks.
-- 
Weiwei Shi, Ph.D
Research Scientist
GeneGO, Inc.

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III


From izmirlig at mail.nih.gov  Sat Sep 23 02:47:35 2006
From: izmirlig at mail.nih.gov (Izmirlian, Grant (NIH/NCI) [E])
Date: Fri, 22 Sep 2006 20:47:35 -0400
Subject: [R] Adding .R to source file keeps R from reading it?
References: <73bcebe0609211401ua0783aem12217b92eb053d81@mail.gmail.com><eb555e660609211458u3f94cc57i9d53aa9d1e9187f9@mail.gmail.com>
	<73bcebe0609211855s7cb58984x591ffe5dc0865cd2@mail.gmail.com>
Message-ID: <13F8170A4373B44286C0AAE19807E8CC697386@NIHCESMLBX11.nih.gov>

So...are you trying to modify a contributed package by adding a *.R file
to the 'R' subdirectory in the package?  One thing to consider besides
the previous tip is that the package might be using a NAMESPACE, which
lives in the package root directory (one directory up from the 'R' 
subdirectory).  If so, then you must add the name of your function
to the argument list of the call to 'export' in the NAMESPACE file

e.g.  export(sortgenes, pvalues, MyFunction)


-----Original Message-----
From: John Tillinghast [mailto:tilling at gmail.com]
Sent: Thu 9/21/2006 9:55 PM
To: r-help at stat.math.ethz.ch
Subject: Re: [R] Adding .R to source file keeps R from reading it?
 
Yes, this was exactly the problem: I was using the unzipped package, not the
source.
Now it works!

---------- Forwarded message ----------
From: Deepayan Sarkar <deepayan.sarkar at gmail.com>
Date: Sep 21, 2006 2:58 PM
Subject: Re: [R] Adding .R to source file keeps R from reading it?
To: John Tillinghast <tilling at gmail.com>
Cc: r-help at stat.math.ethz.ch

On 9/21/06, John Tillinghast <tilling at gmail.com> wrote:
> Hi,
>
> I'm updating the LMGene package from Bioconductor. "Writing R Extensions"
> suggests
> that all source files (the ones in the R directory) have a .R ending, so I
> added it to the (one) source file.
> The next time I installed and ran R, R didn't understand any of the
> functions.
> I tried various things and eventually went back to the file and dropped
the
> .R ending, installed, ran R. It worked!
> For purposes of distributing the package, do I want to leave the name
> without the .R, or add the .R and change something else?

I'm guessing that the "source" you are working on has been obtained by
unzipping the windows binary zip file. Despite appearances, that is
not the source code. For the proper source code, download the file
that's marked as source. In this case,

http://bioconductor.org/packages/1.8/bioc/html/LMGene.html

clearly labels the following as "Source" (and the corresponding zip
file as "Windows Binary")

http://bioconductor.org/packages/1.8/bioc/src/contrib/LMGene_1.0.0.tar.gz

This likely answers your other question as well.

-Deepayan

	[[alternative HTML version deleted]]


From xchen_stat at hotmail.com  Sat Sep 23 04:29:01 2006
From: xchen_stat at hotmail.com (X.H Chen)
Date: Fri, 22 Sep 2006 20:29:01 -0600
Subject: [R]  install error uder Cygwin
Message-ID: <BAY23-F23A495F6892DFF70F296AAFA260@phx.gbl>

Dear R users,

I have a question about R installation under Cygwin. When running 
./configure, I can't pass the checking phase due to an error: 
--with-readline=yes (default) and headers/libs are not available... Anybody 
who can tell me why this error arises? Thanks a lot.

Xiaohui Chen

Dept. of Statistics
UBC, Canada

_________________________________________________________________
Buy what you want when you want it on Sympatico / MSN Shopping


From murdoch at stats.uwo.ca  Sat Sep 23 04:42:28 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 22 Sep 2006 22:42:28 -0400
Subject: [R] install error uder Cygwin
In-Reply-To: <BAY23-F23A495F6892DFF70F296AAFA260@phx.gbl>
References: <BAY23-F23A495F6892DFF70F296AAFA260@phx.gbl>
Message-ID: <45149F14.6050505@stats.uwo.ca>

On 9/22/2006 10:29 PM, X.H Chen wrote:
> Dear R users,
> 
> I have a question about R installation under Cygwin. When running 
> ./configure, I can't pass the checking phase due to an error: 
> --with-readline=yes (default) and headers/libs are not available... Anybody 
> who can tell me why this error arises? Thanks a lot.

We don't support builds under Cygwin.

Duncan Murdoch


From sdavis2 at mail.nih.gov  Sat Sep 23 17:13:26 2006
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Sat, 23 Sep 2006 11:13:26 -0400
Subject: [R] [BioC] two questions associated with heatmap
In-Reply-To: <cdf817830609221627r2cbfef7ey4dd72e5ab03580cd@mail.gmail.com>
References: <cdf817830609221627r2cbfef7ey4dd72e5ab03580cd@mail.gmail.com>
Message-ID: <45154F16.2010607@mail.nih.gov>

Weiwei Shi wrote:
> hi, there:
> i have 2 questions associated with heatmap
>
> in heatmap.2{gplot}, there is a bar called "raw z-score" showing the
> coloring legend for each pixel. Where can I find that z-score's
> formulae?
>   
A Z-score is the mean of the group divided by the standard deviation.
> question 2,
> for example, I have 5 groups and I want to label each group with a
> color name from "#FF0000" to "#0000FF" evenly. so basically i need a
> vector like this:
> c("#FF0000", ?, ?, ?, "#0000FF")
>   
You could look at RColorBrewer or geneplotter for some ideas on how to 
do this.

If you don't have a copy of the Bioconductor book, it may be a good idea 
to pick one up.  It is quite helpful for getting started using 
bioconductor and gene expression data.

Hope this helps.
Sean


From sdavis2 at mail.nih.gov  Sat Sep 23 17:30:04 2006
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Sat, 23 Sep 2006 11:30:04 -0400
Subject: [R] [BioC] two questions associated with heatmap
In-Reply-To: <45154F16.2010607@mail.nih.gov>
References: <cdf817830609221627r2cbfef7ey4dd72e5ab03580cd@mail.gmail.com>
	<45154F16.2010607@mail.nih.gov>
Message-ID: <451552FC.9060206@mail.nih.gov>

Sean Davis wrote:
> Weiwei Shi wrote:
>   
>> hi, there:
>> i have 2 questions associated with heatmap
>>
>> in heatmap.2{gplot}, there is a bar called "raw z-score" showing the
>> coloring legend for each pixel. Where can I find that z-score's
>> formulae?
>>   
>>     
> A Z-score is the mean of the group divided by the standard deviation.
>   
It's a bit late--what I meant to say is that the z-score is the ("actual 
value" minus the mean of the group) divided by the standard deviation.

Sean


From papenfus at gmail.com  Sat Sep 23 05:59:01 2006
From: papenfus at gmail.com (michael papenfus)
Date: Fri, 22 Sep 2006 22:59:01 -0500
Subject: [R] Fatal error: unable to restore saved data in .RData --- no
	package called 'nlme'
Message-ID: <e936d1ed0609222059r53c47ba4nc6509a4410ef85ee@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060922/6b414964/attachment.pl 

From vokey at uleth.ca  Sat Sep 23 06:49:21 2006
From: vokey at uleth.ca (John Vokey)
Date: Fri, 22 Sep 2006 22:49:21 -0600
Subject: [R] contrasts in aov
Message-ID: <A9595913-44FB-477B-9826-EA8C64111056@uleth.ca>

useRs,
   A no doubt simple question, but I am baffled.  Indeed, I think I  
once knew the answer, but can't recover it.  The default contrasts  
for aov (and lm, and...) are contr.treatment and contr.poly for  
unordered and ordered factors, respectively.  But, how does one  
invoke the latter?  That is, in a data.frame, how does one indicate  
that a factor is an *ordered* factor such that contr.poly is invoked  
in the aov or lm call?
--
Please avoid sending me Word or PowerPoint attachments.
See <http://www.gnu.org/philosophy/no-word-attachments.html>

-Dr. John R. Vokey


From ethan.johnsons at gmail.com  Sat Sep 23 07:37:08 2006
From: ethan.johnsons at gmail.com (Ethan Johnsons)
Date: Sat, 23 Sep 2006 01:37:08 -0400
Subject: [R] Y-axis on pbinom
Message-ID: <5cd96f050609222237t70995b8cp68099bfb02e0e170@mail.gmail.com>

With this:
> plot(seq(from=0,to=10,by=1),1-pbinom (seq(from=0,to=10,by=1),size=6,prob=0.50),pch=15)

How do you change the Y-axis from 0 ~ 0.6?
I only get 0.0 ~1.0.

thx much


From ripley at stats.ox.ac.uk  Sat Sep 23 08:29:02 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 23 Sep 2006 07:29:02 +0100 (BST)
Subject: [R] install error uder Cygwin
In-Reply-To: <45149F14.6050505@stats.uwo.ca>
References: <BAY23-F23A495F6892DFF70F296AAFA260@phx.gbl>
	<45149F14.6050505@stats.uwo.ca>
Message-ID: <Pine.LNX.4.64.0609230717530.18401@gannet.stats.ox.ac.uk>

On Fri, 22 Sep 2006, Duncan Murdoch wrote:

> On 9/22/2006 10:29 PM, X.H Chen wrote:
>> Dear R users,
>>
>> I have a question about R installation under Cygwin. When running
>> ./configure, I can't pass the checking phase due to an error:
>> --with-readline=yes (default) and headers/libs are not available... Anybody
>> who can tell me why this error arises? Thanks a lot.
>
> We don't support builds under Cygwin.

But we don't preclude them either.  If you did succeed in building R that 
way (it has been done in the past, but not recently to my knowledge), you 
would have a Unix-alike version of R using the X11 graphics device, and 
not a Windows port of R.

>From the INSTALL file:

   The main source of information on installation is the `R Installation
   and Administration Manual', an HTML copy of which is available as file
   `doc/html/R-admin.html'.  Please read that before installing R.  But
   if you are impatient, read on but please refer to the manual to
   resolve any problems.

The answer to the original question is indeed in that manual.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Sat Sep 23 08:39:21 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 23 Sep 2006 07:39:21 +0100 (BST)
Subject: [R] "logistic" + "neg binomial" + ...
In-Reply-To: <XFMail.060922202515.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.060922202515.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.LNX.4.64.0609230729120.18401@gannet.stats.ox.ac.uk>

On Fri, 22 Sep 2006, Ted.Harding at nessie.mcc.ac.uk wrote:

> Hi Folks,
>
> I've just come across a kind of problem which leads
> me to wonder how to approach it in R.
>
> Basically, each a set of items is subjected to a series
> of "impacts" until it eventually "fails". The "force"
> of each impact would depend on  covariates X,Y say;
> but as a result of preceding impacts an item would be
> expected to have a "cumulative frailty" such that the
> probability of failure due to a particular impact would
> possibly increase according to the series of impacts
> already survived.

So this is a discrete-time survival model.

> Without the "cumulative frailty" one could envisage
> something like a logistic model for the probabiliy
> of failure at each impact, leading to a kind of
> generalised "exponential distribution" -- that is,
> the likelihood for each item would be of the form
>
>  (1-P[1])*(1-P[2])*...*(1-P[n-1])*P[n]
>
> where P[i] could have a logistic model in terms of
> the values of X[i] and Y[i], and n is the index of
> the impact at which failure occurred. That is then
> a solvable problem.
>
> Even so, I'm not (so far) finding in the R resources
> the appropriate analogue of glm for this kind of
> model. I dare say a prolonged trawl through the various
> "survival" resources might lead to something applicable,
> but ...

What is inadequate about glm itself?  The log-likelihood is a sum of terms 
over impacts, so fitting logisitic models for each impact can be done 
separately for each model. However, nnet() can fit them simultaneously 
(and couple them if you want).

> And then there's the cumulative frailty ... !

Add the history to that point into the model.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From RKrug at sun.ac.za  Sat Sep 23 09:33:43 2006
From: RKrug at sun.ac.za (Rainer M Krug)
Date: Sat, 23 Sep 2006 09:33:43 +0200
Subject: [R] Update to Dillo browser question
In-Reply-To: <200609221738.28515.debeaudette@ucdavis.edu>
References: <451388A2.5040802@sun.ac.za>
	<200609221738.28515.debeaudette@ucdavis.edu>
Message-ID: <4514E357.5040703@sun.ac.za>


I start the html help with

 > help.start(browser="dillo")

and all subsequent requests with ? open in dillo


Rainer

Dylan Beaudette wrote:
> hi,
> 
> how exactly do you get R to open the page in dillo ... ?
> 
> thanks!
> 
> On Thursday 21 September 2006 23:54, Rainer M Krug wrote:
>> Hi
>>
>> I asked about if there is any way of opening URLs from the help browser
>> in the same window of the same dillo browser - here is the answer.
>>
>> Just to reiterate: dillo is for me the perfect browser for the help of R
>> when you use ?...
>>
>> Rainer
>>
>> -------- Original Message --------
>> Subject: Re: [Dillo-dev] Opening new URL in same instance and -s option
>> Date: Thu, 21 Sep 2006 21:31:57 -0400
>> From: Jorge Arellano Cid <jcid at dillo.org>
>> To: Rainer M Krug <rkrug at sun.ac.za>
>> References: <45128134.60709 at sun.ac.za>
>>
>> On Thu, Sep 21, 2006 at 02:10:28PM +0200, Rainer M Krug wrote:
>>> Hi
>>    Hi.
>>
>>> I have two questions
>>>
>>> first: I would like to be able to have only one instance of dillo open
>>> and when I try to open a new url (from outside dillo), that this url is
>>> opened in the old instance. Is this possible, and if yes, how?
>>    Not now.
>>
>>    The idea is to implement this with dpip (dillo plugin protocol)
>> but it has not being done yet, becuase of lack of manpower.
>>
>>> second: I have seen emails concerning running dillo in server mode
>>> (dillo -s server). I tried it, but dillo didn't know -s. Is this feature
>>> imp[lemented, and if yes, how can I access it?
>>    Not in the official dillo.
>>
>>> Thanks a lot for a brilliant lightning fast browser,
>>>
>>    :-)
>>
>> --
>>    Cheers
>>    Jorge.-
> 


-- 
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
Biology (UCT)

Department of Conservation Ecology and Entomology
University of Stellenbosch
Matieland 7602
South Africa

Tel:		+27 - (0)72 808 2975 (w)
Fax:		+27 - (0)21 808 3304
Cell:		+27 - (0)83 9479 042

email:	RKrug at sun.ac.za
       	Rainer at krugs.de


From spencer.graves at pdf.com  Sat Sep 23 09:52:30 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 23 Sep 2006 00:52:30 -0700
Subject: [R] nls
In-Reply-To: <4506A44D.294B67C8@stats.uct.ac.za>
References: <4506A44D.294B67C8@stats.uct.ac.za>
Message-ID: <4514E7BE.3010904@pdf.com>

      I used "debug" to walk through your example line by line,  I found 
that the error message was misleading.  By making 
as.vector(semivariance) and as.vector(h)  columns of a data.frame, I got 
it to work.  My revised code appears below. 

      Thanks for providing a self-contained and relatively simple 
example.  Without it, I could have done little more that suggest 
'traceback' (which provided zero information when I tried it) and 'debug'. 

      Hope this helps. 
      Spencer Graves

fit.gaus<-function(coordinates,values,guess.c0,guess.c1,guess.a)
{
         long<-rep(coordinates[,1],each=length(coordinates[,1]))
       
lag.long<-t(matrix(long,nrow=length(coordinates[,1]),byrow=TRUE))
         dif.long <-(lag.long-t(lag.long))^2
         lat <-rep(coordinates[,2],each=length(coordinates[,2]))
         lag.lat<-t(matrix(lat,nrow=length(coordinates[,2]),byrow=TRUE))
         dif.lat <-(lag.lat-t(lag.lat))^2
         h <-sqrt(dif.long+dif.lat)
        
                                       
         if( length(values[1,])>1)
               {
                     y.m <-apply(values,1,sum,na.rm=TRUE)
                     y.m <-as.matrix(y.m)
                     y.mod <-(1/length(values[1,]))*(y.m)
            }
         else
               {
                      y.mod <-as.matrix(values)
            }

        semi <-rep(y.mod,each=length(y.mod))
        mat1<-t(matrix(semi,nrow=length(y.mod),byrow=TRUE))
        mat2<-t(mat1)
        semivariance <-(1/2)*(mat1-mat2)^2

#        model <-semivariance ~c0+c1*(1-exp(-(h^2)/a^2))
         DF <- data.frame(smvar=as.vector(semivariance),
                          h.=as.vector(h) )
         mdl <- smvar~c0+c1*(1-exp(-(h.^2)/a^2))
        
#        parameters <-nls(model,start =
#list(c0=guess.c0,c1=guess.c1,a=guess.a),trace=TRUE)
        parameters <-nls(mdl,start =
list(c0=guess.c0,c1=guess.c1,a=guess.a),trace=TRUE,
                         data=DF )
        results <-summary(parameters)
                      print(results)
}

################################
mzabalazo ngwenya wrote:
> Hello everyone !
>
> I am trying to write a short program to estimate  semivariogram
> parameters. But I keep running into a problem when using the nls
> function.
>
> Could you please shed some light. I have put a sample of one of the
> codes and ran a short example so you see what I mean.
>
> -----------------
>
>
>
> fit.gaus<-function(coordinates,values,guess.c0,guess.c1,guess.a)
> {
>          long<-rep(coordinates[,1],each=length(coordinates[,1]))
>         
> lag.long<-t(matrix(long,nrow=length(coordinates[,1]),byrow=TRUE))
>          dif.long <-(lag.long-t(lag.long))^2
>          lat <-rep(coordinates[,2],each=length(coordinates[,2]))
>          lag.lat<-t(matrix(lat,nrow=length(coordinates[,2]),byrow=TRUE))
>          dif.lat <-(lag.lat-t(lag.lat))^2
>          h <-sqrt(dif.long+dif.lat) 
>          
>                                         
>          if( length(values[1,])>1)
>                {
>                      y.m <-apply(values,1,sum,na.rm=TRUE)
>                      y.m <-as.matrix(y.m)
>                      y.mod <-(1/length(values[1,]))*(y.m)
>             }
>          else
>                {
>                       y.mod <-as.matrix(values)
>             }
>
>         semi <-rep(y.mod,each=length(y.mod))
>         mat1<-t(matrix(semi,nrow=length(y.mod),byrow=TRUE))
>         mat2<-t(mat1)
>         semivariance <-(1/2)*(mat1-mat2)^2
>
>         model <-semivariance ~c0+c1*(1-exp(-(h^2)/a^2)) 
>         parameters <-nls(model,start =
> list(c0=guess.c0,c1=guess.c1,a=guess.a),trace=TRUE)
>         results <-summary(parameters)
>                       print(results)
> } 
> --------------------------
>
>   
>>  don <-matrix(c(2,3,9,6,5,2,7,9,5,3),5,2)
>> don
>>     
>      [,1] [,2]
> [1,]    2    2
> [2,]    3    7
> [3,]    9    9
> [4,]    6    5
> [5,]    5    3
>   
>>  data <-matrix(c(3,4,2,4,6))
>> data
>>     
>      [,1]
> [1,]    3
> [2,]    4
> [3,]    2
> [4,]    4
> [5,]    6
>   
>> fit.gaus(don,data,2,3,5)
>>     
>          [,1]     [,2]     [,3]     [,4]     [,5]
> [1,] 0.000000 5.099020 9.899495 5.000000 3.162278
> [2,] 5.099020 0.000000 6.324555 3.605551 4.472136
> [3,] 9.899495 6.324555 0.000000 5.000000 7.211103
> [4,] 5.000000 3.605551 5.000000 0.000000 2.236068
> [5,] 3.162278 4.472136 7.211103 2.236068 0.000000
> 178.9113 :  2 3 5 
> Error in qr.qty(QR, resid) : 'qr' and 'y' must have the same number of
> rows
>   
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From wildscop at yahoo.com  Thu Sep 21 19:37:03 2006
From: wildscop at yahoo.com (Mohammad Ehsanul Karim)
Date: Thu, 21 Sep 2006 23:07:03 +0530
Subject: [R] frailty in coxph
Message-ID: <c2a71e600609211037q5ff9ef0dg4284acba95b79fe3@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060921/a756d5cd/attachment.pl 

From xchen_stat at hotmail.com  Fri Sep 22 03:17:34 2006
From: xchen_stat at hotmail.com (X.H Chen)
Date: Thu, 21 Sep 2006 19:17:34 -0600
Subject: [R] recursive question
Message-ID: <BAY23-F19BCC7F7658D1B3D9722A6FA210@phx.gbl>

Hi all,

How can store the recursive results from each step without using global 
operator <<-? Thanks ahead.

Xiaohui Chen

Dept. of Statistics
UBC, Canada

_________________________________________________________________
Don?t waste time standing in line?try shopping online. Visit Sympatico / MSN


From p.dalgaard at biostat.ku.dk  Sat Sep 23 10:37:41 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 23 Sep 2006 10:37:41 +0200
Subject: [R] recursive question
In-Reply-To: <BAY23-F19BCC7F7658D1B3D9722A6FA210@phx.gbl>
References: <BAY23-F19BCC7F7658D1B3D9722A6FA210@phx.gbl>
Message-ID: <x2hcyz2d2i.fsf@turmalin.kubism.ku.dk>

"X.H Chen" <xchen_stat at hotmail.com> writes:

> Hi all,
> 
> How can store the recursive results from each step without using
> global operator <<-? Thanks ahead.

By returning them to the caller, recursively...

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From rduarte at ipimar.pt  Sat Sep 23 11:47:58 2006
From: rduarte at ipimar.pt (Rafael Duarte)
Date: Sat, 23 Sep 2006 10:47:58 +0100
Subject: [R] Lattice strip labels for two factors
In-Reply-To: <971536df0609221300i4a0a3a21w2914588f6b787b7e@mail.gmail.com>
References: <45143B48.3020601@ipimar.pt>
	<971536df0609221300i4a0a3a21w2914588f6b787b7e@mail.gmail.com>
Message-ID: <451502CE.6040902@ipimar.pt>

Thank you for your suggestion.
This could be a solution that I didn't think of.

But I forgot to say that I didn't want to change the original data frame 
(I have other code that depends on the original df and on the original 
factor levels).
I was looking more for an implementation directly in the xyplot call 
(same as I did for one factor). Is it possible/simple to do?

Thank you,
Rafael


Gabor Grothendieck wrote:

> Try this:
>
> levels(df$fact2) <- c("faro","porto","lisbon","setubal")
> xyplot( value ~ year | fact1*fact2, data=df, type="b")
>
>
> On 9/22/06, Rafael Duarte <rduarte at ipimar.pt> wrote:
>
>> Dear list,
>> My problem is to change the strip text of lattice panels when using two
>> factors.
>> I have a data frame with two factors:
>>
>> df <- expand.grid( "fact1"=c("y","b","r"),
>> "fact2"=c("far","por","lis","set"), "year"=1991:2000, "value"= NA)
>> df[,"value"] <- sample(1:50, 120, replace=TRUE)
>>
>> I can make simple xyplot and change the text of the factor levels with
>> strip.custom:
>>
>> require("lattice")
>> xyplot( value ~ year | fact1, data=df, type="b", subset= fact2=="far",
>> strip = strip.custom(bg=gray.colors(1,0.95), factor.levels=c("yellow",
>> "black", "red")),
>> layout=c(1,3)
>> )
>>
>> But how can I change the text of the factor levels when using both
>> factors as in this plot:
>> xyplot( value ~ year | fact1*fact2, data=df, type="b")
>>
>> (fact2 levels text should change to: 
>> c("faro","porto","lisbon","setubal"))
>>
>> I read the help for strip.default and the emails archive, tried with
>> "which.given" but could not find out how to accomplish this.
>>
>> Many thanks,
>> Rafael Duarte
>>
>> -- 
>> Rafael Duarte
>> Marine Resources Department - DRM
>> IPIMAR -  National Research Institute for Agriculture and Fisheries
>> Av. Bras?lia, 1449-006 Lisbon  -  Portugal
>> Tel:+351 21 302 7000      Fax:+351 21 301 5948
>> e-mail: rduarte at ipimar.pt
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>


-- 
Rafael Duarte
Marine Resources Department - DRM
IPIMAR -  National Research Institute for Agriculture and Fisheries
Av. Bras?lia, 1449-006 Lisbon  -  Portugal
Tel:+351 21 302 7000      Fax:+351 21 301 5948
e-mail: rduarte at ipimar.pt


From ripley at stats.ox.ac.uk  Sat Sep 23 11:50:09 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 23 Sep 2006 10:50:09 +0100 (BST)
Subject: [R] Fatal error: unable to restore saved data in .RData --- no
 package called 'nlme'
In-Reply-To: <e936d1ed0609222059r53c47ba4nc6509a4410ef85ee@mail.gmail.com>
References: <e936d1ed0609222059r53c47ba4nc6509a4410ef85ee@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0609231046050.29945@gannet.stats.ox.ac.uk>

On Fri, 22 Sep 2006, michael papenfus wrote:

> I am using Windows XP and R 2.3.1.
> During my lastest session I updated my packages and now when I try to start
> R 2.3.1
> I get the following error message:
>
> Fatal error: unable to restore saved data in .RData in an error window and
> Error in loadNamespace(name): there is no package called 'nlme' in the R
> console.
>
> I had been using nlme and lme4 when I updated my packages.

Uh, oh, don't do that.  You cannot update a currently used package.

> I did a search on this error but unfortunatley I don't really understand the
> solution.
>
> I would like to continue using R and 'nlme'.

Find the file .RData in your R working directory, and rename it.
Start R, and run

install.packages("nlme")
load("whatever you renamed .RData to")

At the end of the session, save the workspace.


> Any suggestions,
> Mike
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From m.blizinski at wit.edu.pl  Sat Sep 23 11:54:19 2006
From: m.blizinski at wit.edu.pl (Maciej =?UTF-8?Q?Blizi=C5=84ski?=)
Date: Sat, 23 Sep 2006 11:54:19 +0200
Subject: [R] Y-axis on pbinom
In-Reply-To: <5cd96f050609222237t70995b8cp68099bfb02e0e170@mail.gmail.com>
References: <5cd96f050609222237t70995b8cp68099bfb02e0e170@mail.gmail.com>
Message-ID: <1159005259.27617.12.camel@localhost.localnet>

On Sat, 2006-09-23 at 01:37 -0400, Ethan Johnsons wrote:
> How do you change the Y-axis from 0 ~ 0.6?

plot(..., ylim = c(0, 0.6))

-- 
Maciej Blizi?ski
http://automatthias.wordpress.com


From Roger.Bivand at nhh.no  Sat Sep 23 12:15:11 2006
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Sat, 23 Sep 2006 12:15:11 +0200 (CEST)
Subject: [R] proj4R library will not install
In-Reply-To: <4CB9D91A4AD04647BCE0305E391423F702A1F6@mail3.arts.ryerson.ca>
Message-ID: <Pine.LNX.4.44.0609231214140.31075-100000@reclus.nhh.no>

On Fri, 22 Sep 2006, Philip Bermingham wrote:

proj4R was never a CRAN package. Its functionality is included in the CRAN 
package rgdal, please use that instead.


> I'm hoping someone can help me.  I have downloaded the proj4R.zip and
> under my version of R (2.3.1) I install the package from local zip file.
> This worked great.  I then type library(proj4R) to load the library and
> I get the error: Error in library(proj4R) : 'proj4R' is not a valid
> package -- installed < 2.0.0?  I have read through the install
> documentation and have downloaded and unpacked PROJ.4 to c:\proj\ so the
> bin is located at C:\proj\bin.  I then set the environmental variables
> PATH which now looks like :
> %SystemRoot%\system32;%SystemRoot%;%SystemRoot%\System32\Wbem;C:\Program
> Files\Intel\DMIX;C:\Program Files\UltraEdit;C:\proj and I created a new
> user variable PROJ_LIB to c:\proj\nad.  I'm not sure if I am missing
> anything here but I still get the <2.0.0 error.  If you can help me in
> any way I would truly appreciate it.
> 
> Thanks in advance,
> 
> Philip Bermingham
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From telloli at cs.unibo.it  Sat Sep 23 12:35:10 2006
From: telloli at cs.unibo.it (Luca Telloli)
Date: Sat, 23 Sep 2006 12:35:10 +0200
Subject: [R] Fitdistr() versus nls()
Message-ID: <731DA923-3AEB-4516-B6B6-65E31DD49A79@cs.unibo.it>

Hello R-Users,
	I'm new to R so I apologize in advance for any big mistake I might  
be doing. I'm trying to fit a set of samples with some probabilistic  
curve, and I have an important question to ask; in particular I have  
some data, from which I calculate manually the CDF, and then I import  
them into R and try to fit: I have the x values (my original samples)  
and the y values (P(X<x)).
	To attempt the fit I've both fitdistr() and nls(), in the way you  
can read in the piece of code at the end of the email. Because the  
fit with all data doesn't work very well, I decided to take a subset  
of samples randomly chosen (for some random x, the correspondant y is  
chosen).
	The first big problem is that fitdistr and nls, in the way I use  
them in the code, they don't get me similar results. I think I'm  
making a mistake but I can't really understand which one.
	From this first issue, a second one arises because the plot with nls  
is similar (maybe not a great fit bust still...) to my original CDF  
while the plot of fitdistr is basically a straight line of constant  
value y=1. On the other side, the fitdistr outperforms in the  
Kolmogorov-Smirnov test, which for nls has values of probability  
around 0 (not a good score huh?).
	Can u please tell me if there's a major mistake in the code?

Thanks in advance,
Luca


------ BEGINNING OF CODE  
----------------------------------------------------------------
cdf.all=read.table("all_failures.cdf", header=FALSE, col.names=c 
("ttr", "cdf"), sep=":" )

allvals.x=array(t(cdf.all[1]))
allvals.y=array(t(cdf.all[2]))
library(MASS)
bestval.exp.nls=bestval.exp.fit=-1
plot(allvals.x, allvals.y)

for(it in 1:100){
	#extract random samples
	random=sort(sample(1:length(allvals.x), 15))
	somevals.x=allvals.x[c(random)]
	somevals.y=allvals.y[c(random)]
	#fit with nls and fitdistr
	fit.exp = fitdistr(somevals.y, "exponential")
	nls.exp <- nls(somevals.y ~ pexp(somevals.x, rate), start=list(rate=. 
0001), model=TRUE)
	#plot what you get out of the two fits
	lines(allvals.x, pexp(allvals.x, coef(fit.exp)), col=it)
	lines(allvals.x, pexp(allvals.x, coef(nls.exp)), col=it)
	#perform kolmogorov-smirnov test on your fit
	ks.exp.nls = ks.test(somevals.y, "pexp", coef(nls.exp))
	ks.exp.fit = ks.test(somevals.y, "pexp", coef(fit.exp))
	
	bestval.exp.fit = max(bestval.exp.fit, ks.exp.fit$p.value)
	bestval.exp.nls = max(bestval.exp.nls, ks.exp.nls$p.value)
}

print(bestval.exp.fit)
print(bestval.exp.nls)

----------END OF  
CODE--------------------------------------------------------------------


From maechler at stat.math.ethz.ch  Sat Sep 23 12:39:03 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 23 Sep 2006 12:39:03 +0200
Subject: [R] nls
In-Reply-To: <4514E7BE.3010904@pdf.com>
References: <4506A44D.294B67C8@stats.uct.ac.za>
	<4514E7BE.3010904@pdf.com>
Message-ID: <17685.3783.83912.750322@stat.math.ethz.ch>

>>>>> "SpG" == Spencer Graves <spencer.graves at pdf.com>
>>>>>     on Sat, 23 Sep 2006 00:52:30 -0700 writes:

    SpG> I used "debug" to walk through your example line by line,  I found 
    SpG> that the error message was misleading.  By making 
    SpG> as.vector(semivariance) and as.vector(h)  columns of a data.frame, I got 
    SpG> it to work.  My revised code appears below. 

    SpG> Thanks for providing a self-contained and relatively simple 
    SpG> example.  Without it, I could have done little more that suggest 
    SpG> 'traceback' (which provided zero information when I tried it) and 'debug'. 

Thank you, Spencer, for picking this up.
Your solution is probably fine for  Mzabalazo Ngwenya.

However your note about the unhelpful error message made me dig
a bit further.
My current diagnosis:  nls() is at first doing a *multivariate* model
in this case which model.frame() happily does.
Later in nlsModel();  qr() does not deal with such multivariateness
and hence produces a 'QR' result which does not "fit" the other
parts of the computations.

I think we should decide if it is easy enough to make nls() work
with multivariate models and do it if it's "easy" and otherwise
disallow it consciously -- both via a better 
error message and the help page.

Well, for R 2.4.0 it should be possible to do the latter and I
think we should... 
This is now becoming an "R-devel" rather than "R-help" topic..

Martin

    SpG> Hope this helps. 
    SpG> Spencer Graves

    SpG> fit.gaus<-function(coordinates,values,guess.c0,guess.c1,guess.a)
    SpG> {
    SpG> long<-rep(coordinates[,1],each=length(coordinates[,1]))
       
    SpG> lag.long<-t(matrix(long,nrow=length(coordinates[,1]),byrow=TRUE))
    SpG> dif.long <-(lag.long-t(lag.long))^2
    SpG> lat <-rep(coordinates[,2],each=length(coordinates[,2]))
    SpG> lag.lat<-t(matrix(lat,nrow=length(coordinates[,2]),byrow=TRUE))
    SpG> dif.lat <-(lag.lat-t(lag.lat))^2
    SpG> h <-sqrt(dif.long+dif.lat)
        
                                       
    SpG> if( length(values[1,])>1)
    SpG> {
    SpG> y.m <-apply(values,1,sum,na.rm=TRUE)
    SpG> y.m <-as.matrix(y.m)
    SpG> y.mod <-(1/length(values[1,]))*(y.m)
    SpG> }
    SpG> else
    SpG> {
    SpG> y.mod <-as.matrix(values)
    SpG> }

    SpG> semi <-rep(y.mod,each=length(y.mod))
    SpG> mat1<-t(matrix(semi,nrow=length(y.mod),byrow=TRUE))
    SpG> mat2<-t(mat1)
    SpG> semivariance <-(1/2)*(mat1-mat2)^2

    SpG> #        model <-semivariance ~c0+c1*(1-exp(-(h^2)/a^2))
    SpG> DF <- data.frame(smvar=as.vector(semivariance),
    SpG> h.=as.vector(h) )
    SpG> mdl <- smvar~c0+c1*(1-exp(-(h.^2)/a^2))
        
    SpG> #        parameters <-nls(model,start =
    SpG> #list(c0=guess.c0,c1=guess.c1,a=guess.a),trace=TRUE)
    SpG> parameters <-nls(mdl,start =
    SpG> list(c0=guess.c0,c1=guess.c1,a=guess.a),trace=TRUE,
    SpG> data=DF )
    SpG> results <-summary(parameters)
    SpG> print(results)
    SpG> }

    SpG> ################################
    SpG> mzabalazo ngwenya wrote:
    >> Hello everyone !
    >> 
    >> I am trying to write a short program to estimate  semivariogram
    >> parameters. But I keep running into a problem when using the nls
    >> function.
    >> 
    >> Could you please shed some light. I have put a sample of one of the
    >> codes and ran a short example so you see what I mean.
    >> 
    >> -----------------
    >> 
    >> 
    >> 
    >> fit.gaus<-function(coordinates,values,guess.c0,guess.c1,guess.a)
    >> {
    >> long<-rep(coordinates[,1],each=length(coordinates[,1]))
    >> 
    >> lag.long<-t(matrix(long,nrow=length(coordinates[,1]),byrow=TRUE))
    >> dif.long <-(lag.long-t(lag.long))^2
    >> lat <-rep(coordinates[,2],each=length(coordinates[,2]))
    >> lag.lat<-t(matrix(lat,nrow=length(coordinates[,2]),byrow=TRUE))
    >> dif.lat <-(lag.lat-t(lag.lat))^2
    >> h <-sqrt(dif.long+dif.lat) 
    >> 
    >> 
    >> if( length(values[1,])>1)
    >> {
    >> y.m <-apply(values,1,sum,na.rm=TRUE)
    >> y.m <-as.matrix(y.m)
    >> y.mod <-(1/length(values[1,]))*(y.m)
    >> }
    >> else
    >> {
    >> y.mod <-as.matrix(values)
    >> }
    >> 
    >> semi <-rep(y.mod,each=length(y.mod))
    >> mat1<-t(matrix(semi,nrow=length(y.mod),byrow=TRUE))
    >> mat2<-t(mat1)
    >> semivariance <-(1/2)*(mat1-mat2)^2
    >> 
    >> model <-semivariance ~c0+c1*(1-exp(-(h^2)/a^2)) 
    >> parameters <-nls(model,start =
    >> list(c0=guess.c0,c1=guess.c1,a=guess.a),trace=TRUE)
    >> results <-summary(parameters)
    >> print(results)
    >> } 
    >> --------------------------
    >> 
    >> 
    >>> don <-matrix(c(2,3,9,6,5,2,7,9,5,3),5,2)
    >>> don
    >>> 
    >> [,1] [,2]
    >> [1,]    2    2
    >> [2,]    3    7
    >> [3,]    9    9
    >> [4,]    6    5
    >> [5,]    5    3
    >> 
    >>> data <-matrix(c(3,4,2,4,6))
    >>> data
    >>> 
    >> [,1]
    >> [1,]    3
    >> [2,]    4
    >> [3,]    2
    >> [4,]    4
    >> [5,]    6
    >> 
    >>> fit.gaus(don,data,2,3,5)
    >>> 
    >> [,1]     [,2]     [,3]     [,4]     [,5]
    >> [1,] 0.000000 5.099020 9.899495 5.000000 3.162278
    >> [2,] 5.099020 0.000000 6.324555 3.605551 4.472136
    >> [3,] 9.899495 6.324555 0.000000 5.000000 7.211103
    >> [4,] 5.000000 3.605551 5.000000 0.000000 2.236068
    >> [5,] 3.162278 4.472136 7.211103 2.236068 0.000000
    >> 178.9113 :  2 3 5 
    >> Error in qr.qty(QR, resid) : 'qr' and 'y' must have the same number of
    >> rows
    >> 
    >> 
    >> ______________________________________________
    >> R-help at stat.math.ethz.ch mailing list
    >> https://stat.ethz.ch/mailman/listinfo/r-help
    >> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    >> and provide commented, minimal, self-contained, reproducible code.
    >> 

    SpG> ______________________________________________
    SpG> R-help at stat.math.ethz.ch mailing list
    SpG> https://stat.ethz.ch/mailman/listinfo/r-help
    SpG> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    SpG> and provide commented, minimal, self-contained, reproducible code.


From Ted.Harding at nessie.mcc.ac.uk  Sat Sep 23 13:23:48 2006
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 23 Sep 2006 12:23:48 +0100 (BST)
Subject: [R] "logistic" + "neg binomial" + ...
In-Reply-To: <Pine.LNX.4.64.0609230729120.18401@gannet.stats.ox.ac.uk>
Message-ID: <XFMail.060923122348.Ted.Harding@nessie.mcc.ac.uk>

On 23-Sep-06 Prof Brian Ripley wrote:
> On Fri, 22 Sep 2006, Ted.Harding at nessie.mcc.ac.uk wrote:
> 
>> Hi Folks,
>>
>> I've just come across a kind of problem which leads
>> me to wonder how to approach it in R.
>>
>> Basically, each a set of items is subjected to a series
>> of "impacts" until it eventually "fails". The "force"
>> of each impact would depend on  covariates X,Y say;
>> but as a result of preceding impacts an item would be
>> expected to have a "cumulative frailty" such that the
>> probability of failure due to a particular impact would
>> possibly increase according to the series of impacts
>> already survived.
> 
> So this is a discrete-time survival model.

Essentially, yes. Though without "cumulative frailty" one
need not take into account that each item is repeatedly
"hit" until it "breaks" -- it would be the same as if
each item was discarded after impact, being replaced by
an identical one.

>> Without the "cumulative frailty" one could envisage
>> something like a logistic model for the probabiliy
>> of failure at each impact, leading to a kind of
>> generalised "exponential distribution" -- that is,
>> the likelihood for each item would be of the form
>>
>>  (1-P[1])*(1-P[2])*...*(1-P[n-1])*P[n]
>>
>> where P[i] could have a logistic model in terms of
>> the values of X[i] and Y[i], and n is the index of
>> the impact at which failure occurred. That is then
>> a solvable problem.
>>
>> Even so, I'm not (so far) finding in the R resources
>> the appropriate analogue of glm for this kind of
>> model. I dare say a prolonged trawl through the various
>> "survival" resources might lead to something applicable,
>> but ...
> 
> What is inadequate about glm itself?

My concern about using glm is that it does not have an option
like "family=negbinomial", so that binary data are treated
as if binomially distributed. For example, consider the two
cases (without covariates):

A: 0 0 0 0 0 0 0 0 0 1
B: 0 0 0 0 0 0 0 0 0 1

where A results from a fixed number of 10 trials, of which
one results in "1", while B results from repeating trials
until a "1" is obtained, as it happens on the 10th trial.

Now, granted that both A and B have the same likelihood
function for p (prob of "1"), namely p*(1-p)^9, so the same
MLE of p would result, namely p=1/10. However, they have
quite different sampling properties. If, with

  y <- cbind(c(0,0,0,0,0,0,0,0,0,1),c(1,1,1,1,1,1,1,1,1,0))

you fit y~1, the estimate returned by glm is the Intercept
which is the estimated value of t = log(p/(1-p)):

  summary(glm(y~1,family=binomial))
  [...]
  Coefficients:
              Estimate Std. Error z value Pr(>|z|)  
  (Intercept)   -2.197      1.054  -2.085   0.0371 *

and indeed, with t=Intercept, you find that exp(t)/(1+exp(t)) = 0.1

However, I would not trust the "Std. Error" from this output!
It is computed on the basis that the 0/1 data are binomial with
fixed size n=10, whereas I would want a "Std. Error" computed
on the basis that the random variable is n, for fixed r=1.
(Indeed, in both cases the "Std. Error" is not quite what it seems,
since there is positive probability in case A for both t = -inf
and t = +inf when estimated, and in case B positive probability
for t = +inf).

Although in the simple cases of A and B one can work directly
with p, avoiding such problems, when one introduces covariates
for each trial and it is natural to postulate a logistic model
for P[i] in the i-th trial, then it would be very convenient
to use the 'glm' mechanism for this. Again, the same estimated
values of the coefficients would result from maximising the
likelihood function whether the data are generated as in A or
as in B, for the same reasons (and even more strongly) I would
not trust the SEs of the coefficients as output from glm.


> The log-likelihood is a sum of terms over impacts, so fitting
> logisitic models for each impact can be done separately for
> each model.

I'm not sure how to interpret this. I'm envisaging only one
model, say logit(P[i]) = a + b*X[i] + c*Y[i] for the i-th
impact; "each impact" is a single event so I woudn't envisage
fitting any model for this single event.

> However, nnet() can fit them simultaneously (and couple
> them if you want).
> 
>> And then there's the cumulative frailty ... !
> 
> Add the history to that point into the model.

I'll look futher into those suggestions.

Thanks,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 23-Sep-06                                       Time: 12:23:44
------------------------------ XFMail ------------------------------


From gwgilc at wm.edu  Sat Sep 23 14:44:44 2006
From: gwgilc at wm.edu (George W. Gilchrist)
Date: Sat, 23 Sep 2006 08:44:44 -0400
Subject: [R] Create a vector of indices from a matrix of start and end points
Message-ID: <3B5FA8D3-CA28-421E-8C0D-095F48276E4D@wm.edu>

I have a very large dataframe and wish to extract a subset of rows. I  
have a two column matrix listing the starting and ending indices of  
one subset on each row. My idea is to create a vector of indices that  
could be applied to the dataframe and I have a solution using a for  
loop (below). But surely there is some more elegant way to do this! I  
looked thorough the archives without success. Thanks for any ideas.

 > tmp1 <- matrix(c(2,5,7,9,15,20), 3,2, byrow=T)
 > tmp1
      [,1] [,2]
[1,]    2    5
[2,]    7    9
[3,]   15   20
 > t.ind <- NULL
 > for (i in 1:3) t.ind <- c(t.ind, seq(tmp1[i,1], tmp1[i,2]))
 > t.ind
[1]  2  3  4  5  7  8  9 15 16 17 18 19 20
 >

cheers, George

..................................................................
George W. Gilchrist                           Email: gwgilc at wm.edu
Director of Graduate Studies                 Phone: (757) 221-7751
Department of Biology, Box 8795                Fax: (757) 221-6483
College of William & Mary
Williamsburg, VA 23187-8795
http://gwgilc.people.wm.edu/


From jholtman at gmail.com  Sat Sep 23 14:55:05 2006
From: jholtman at gmail.com (jim holtman)
Date: Sat, 23 Sep 2006 08:55:05 -0400
Subject: [R] Create a vector of indices from a matrix of start and end
	points
In-Reply-To: <3B5FA8D3-CA28-421E-8C0D-095F48276E4D@wm.edu>
References: <3B5FA8D3-CA28-421E-8C0D-095F48276E4D@wm.edu>
Message-ID: <644e1f320609230555m2693866ck5339a5dcb25fe899@mail.gmail.com>

try this:

> x <- rbind(c(2,5), c(7,9), c(15,20))
> x
     [,1] [,2]
[1,]    2    5
[2,]    7    9
[3,]   15   20
> unlist(mapply(seq, x[,1], x[,2]))
 [1]  2  3  4  5  7  8  9 15 16 17 18 19 20
>


On 9/23/06, George W. Gilchrist <gwgilc at wm.edu> wrote:
> I have a very large dataframe and wish to extract a subset of rows. I
> have a two column matrix listing the starting and ending indices of
> one subset on each row. My idea is to create a vector of indices that
> could be applied to the dataframe and I have a solution using a for
> loop (below). But surely there is some more elegant way to do this! I
> looked thorough the archives without success. Thanks for any ideas.
>
>  > tmp1 <- matrix(c(2,5,7,9,15,20), 3,2, byrow=T)
>  > tmp1
>      [,1] [,2]
> [1,]    2    5
> [2,]    7    9
> [3,]   15   20
>  > t.ind <- NULL
>  > for (i in 1:3) t.ind <- c(t.ind, seq(tmp1[i,1], tmp1[i,2]))
>  > t.ind
> [1]  2  3  4  5  7  8  9 15 16 17 18 19 20
>  >
>
> cheers, George
>
> ..................................................................
> George W. Gilchrist                           Email: gwgilc at wm.edu
> Director of Graduate Studies                 Phone: (757) 221-7751
> Department of Biology, Box 8795                Fax: (757) 221-6483
> College of William & Mary
> Williamsburg, VA 23187-8795
> http://gwgilc.people.wm.edu/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From mothsailor at googlemail.com  Sat Sep 23 15:18:16 2006
From: mothsailor at googlemail.com (David Barron)
Date: Sat, 23 Sep 2006 14:18:16 +0100
Subject: [R] Create a vector of indices from a matrix of start and end
	points
In-Reply-To: <3B5FA8D3-CA28-421E-8C0D-095F48276E4D@wm.edu>
References: <3B5FA8D3-CA28-421E-8C0D-095F48276E4D@wm.edu>
Message-ID: <815b70590609230618r4ca4bd31oef0d1b4ea520066@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060923/71fa4e64/attachment.pl 

From eeneling at yahoo.com  Sat Sep 23 15:55:10 2006
From: eeneling at yahoo.com (Nel N.)
Date: Sat, 23 Sep 2006 06:55:10 -0700 (PDT)
Subject: [R] problem with summary of data
Message-ID: <6462497.post@talk.nabble.com>


Greetings everybody,
I'm new with R and I'm trying to do some statistical analysis with R using
my data.
It is a DNA microarray data with 69 samples(row) and 7070 (column)
variables. I read the file using the command 
'file.table' and it can be done with no error.However, when I tried running
a summary of the data, the result was not what I had expected. Below is an
extract of the result. And I knew that the command summary will give a
summary of a data's descriptive statistics such as it's median,mean,etc.So,
what i want is the statistics summary of each gene (in each column). 
     V7065          V7066        V7067        V7068            V7069   
 20     :29   20       :69   117    : 2   39     : 6   20         :69  
 25     : 4   X16699_at: 1   134    : 2   42     : 4   L49218_f_at: 1  
 30     : 4                  142    : 2   44     : 4                   
 23     : 3                  170    : 2   46     : 4                   
 26     : 3                  203    : 2   38     : 3                   
 21     : 2                  96     : 2   58     : 3                   
 (Other):25                  (Other):58   (Other):46                   
     V7070            V7071   
 30     : 5   20         :69  
 21     : 4   Z78285_f_at: 1  
 51     : 4                   
 64     : 4                   
 27     : 3                   
 36     : 3                   
 (Other):47    
My data is in .dat format. Also, I have downloaded a few .rda files to be
tried out with R but how do I view them as data per se, like the .dat file
can be viewed and edited using Notepad, how about the .rda files?
I express my greatest appreciation all who help.
Thank you.
-- 
View this message in context: http://www.nabble.com/problem-with-summary-of-data-tf2322882.html#a6462497
Sent from the R help mailing list archive at Nabble.com.


From ggrothendieck at gmail.com  Sat Sep 23 16:02:59 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 23 Sep 2006 10:02:59 -0400
Subject: [R] Lattice strip labels for two factors
In-Reply-To: <451502CE.6040902@ipimar.pt>
References: <45143B48.3020601@ipimar.pt>
	<971536df0609221300i4a0a3a21w2914588f6b787b7e@mail.gmail.com>
	<451502CE.6040902@ipimar.pt>
Message-ID: <971536df0609230702o7d7c5fei588079e7c16d76ff@mail.gmail.com>

1. You can write a custom strip function:

	my.strip <- function(which.given, ..., factor.levels) {
	   levs <- if (which.given == 1) factor.levels
		   else c("faro", "porto", "lisbon", "setubal")
	   strip.default(which.given, ..., factor.levels = levs)
	}

	xyplot(value ~ year | fact1 * fact2, data = df,
	   strip = my.strip)

2. however, its probably easier just to change the levels in
the data frame.  Just do it in a copy if you don't want to
change the original one:

	df2 <- df
	levels(df2$fact2) <- c("faro", "porto", "lisbon", "setubal")
	xyplot(value ~ year | fact1 * fact2, data = df2)

3. or you can even do it inline in the data statement which
similarly won't change the original data frame:

	levs <- c("faro", "porto", "lisbon", "setubal")
	xyplot(value ~ year | fact1 * fact2,
	   data = replace(df, "fact2", structure(df$fact2, levels = levs)))
	head(df) # unchanged





On 9/23/06, Rafael Duarte <rduarte at ipimar.pt> wrote:
> Thank you for your suggestion.
> This could be a solution that I didn't think of.
>
> But I forgot to say that I didn't want to change the original data frame
> (I have other code that depends on the original df and on the original
> factor levels).
> I was looking more for an implementation directly in the xyplot call
> (same as I did for one factor). Is it possible/simple to do?
>
> Thank you,
> Rafael
>
>
> Gabor Grothendieck wrote:
>
> > Try this:
> >
> > levels(df$fact2) <- c("faro","porto","lisbon","setubal")
> > xyplot( value ~ year | fact1*fact2, data=df, type="b")
> >
> >
> > On 9/22/06, Rafael Duarte <rduarte at ipimar.pt> wrote:
> >
> >> Dear list,
> >> My problem is to change the strip text of lattice panels when using two
> >> factors.
> >> I have a data frame with two factors:
> >>
> >> df <- expand.grid( "fact1"=c("y","b","r"),
> >> "fact2"=c("far","por","lis","set"), "year"=1991:2000, "value"= NA)
> >> df[,"value"] <- sample(1:50, 120, replace=TRUE)
> >>
> >> I can make simple xyplot and change the text of the factor levels with
> >> strip.custom:
> >>
> >> require("lattice")
> >> xyplot( value ~ year | fact1, data=df, type="b", subset= fact2=="far",
> >> strip = strip.custom(bg=gray.colors(1,0.95), factor.levels=c("yellow",
> >> "black", "red")),
> >> layout=c(1,3)
> >> )
> >>
> >> But how can I change the text of the factor levels when using both
> >> factors as in this plot:
> >> xyplot( value ~ year | fact1*fact2, data=df, type="b")
> >>
> >> (fact2 levels text should change to:
> >> c("faro","porto","lisbon","setubal"))
> >>
> >> I read the help for strip.default and the emails archive, tried with
> >> "which.given" but could not find out how to accomplish this.
> >>
> >> Many thanks,
> >> Rafael Duarte
> >>
> >> --
> >> Rafael Duarte
> >> Marine Resources Department - DRM
> >> IPIMAR -  National Research Institute for Agriculture and Fisheries
> >> Av. Bras?lia, 1449-006 Lisbon  -  Portugal
> >> Tel:+351 21 302 7000      Fax:+351 21 301 5948
> >> e-mail: rduarte at ipimar.pt
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
>
>
> --
> Rafael Duarte
> Marine Resources Department - DRM
> IPIMAR -  National Research Institute for Agriculture and Fisheries
> Av. Bras?lia, 1449-006 Lisbon  -  Portugal
> Tel:+351 21 302 7000      Fax:+351 21 301 5948
> e-mail: rduarte at ipimar.pt
>
>


From helprhelp at gmail.com  Sat Sep 23 16:27:54 2006
From: helprhelp at gmail.com (Weiwei Shi)
Date: Sat, 23 Sep 2006 10:27:54 -0400
Subject: [R] [BioC] two questions associated with heatmap
In-Reply-To: <451552FC.9060206@mail.nih.gov>
References: <cdf817830609221627r2cbfef7ey4dd72e5ab03580cd@mail.gmail.com>
	<45154F16.2010607@mail.nih.gov> <451552FC.9060206@mail.nih.gov>
Message-ID: <cdf817830609230727y2354319s134e97ac2364ecd7@mail.gmail.com>

oh, i mean how it defines the group? i just want to confirm if it is
based on the whole matrix?

On 9/23/06, Sean Davis <sdavis2 at mail.nih.gov> wrote:
> Sean Davis wrote:
> > Weiwei Shi wrote:
> >
> >> hi, there:
> >> i have 2 questions associated with heatmap
> >>
> >> in heatmap.2{gplot}, there is a bar called "raw z-score" showing the
> >> coloring legend for each pixel. Where can I find that z-score's
> >> formulae?
> >>
> >>
> > A Z-score is the mean of the group divided by the standard deviation.
> >
> It's a bit late--what I meant to say is that the z-score is the ("actual
> value" minus the mean of the group) divided by the standard deviation.
>
> Sean
>


-- 
Weiwei Shi, Ph.D
Research Scientist
GeneGO, Inc.

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III


From sdavis2 at mail.nih.gov  Sun Sep 24 04:56:40 2006
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Sat, 23 Sep 2006 22:56:40 -0400
Subject: [R] [BioC] two questions associated with heatmap
In-Reply-To: <cdf817830609230727y2354319s134e97ac2364ecd7@mail.gmail.com>
References: <cdf817830609221627r2cbfef7ey4dd72e5ab03580cd@mail.gmail.com>	
	<45154F16.2010607@mail.nih.gov> <451552FC.9060206@mail.nih.gov>
	<cdf817830609230727y2354319s134e97ac2364ecd7@mail.gmail.com>
Message-ID: <4515F3E8.1060703@mail.nih.gov>

Weiwei Shi wrote:
> oh, i mean how it defines the group? i just want to confirm if it is
> based on the whole matrix?
The process is typically done within each probe, not across the entire
matrix, but it depends a bit on the parameters given, at least for
heatmap and heatmap.2.  Reading the help pages for heatmap and heatmap.2
will be helpful here.  I generally recommend that before someone uses a
new function in R or bioconductor, that he read the help page, although
I have been guilty of not doing that myself at points.

Sean


From Ted.Harding at nessie.mcc.ac.uk  Sat Sep 23 17:15:17 2006
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 23 Sep 2006 16:15:17 +0100 (BST)
Subject: [R] "logistic" + "neg binomial" + ...
In-Reply-To: <XFMail.060922202515.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <XFMail.060923161517.Ted.Harding@nessie.mcc.ac.uk>

On 22-Sep-06 Ted Harding wrote:
> I've just come across a kind of problem which leads
> me to wonder how to approach it in R.
> 
> Basically, each a set of items is subjected to a series
> of "impacts" until it eventually "fails". The "force"
> of each impact would depend on  covariates X,Y say;
> [...]
>  ... one could envisage
> something like a logistic model for the probabiliy
> of failure at each impact, leading to a kind of
> generalised "geometric distribution" -- that is,
> the likelihood for each item would be of the form
> 
>   (1-P[1])*(1-P[2])*...*(1-P[n-1])*P[n]
> 
> where P[i] could have a logistic model in terms of
> the values of X[i] and Y[i], and n is the index of
> the impact at which failure occurred. That is then
> a solvable problem.

I may be getting closer, but am well off target still!

Starting with the case of no covariates, one has

   p*(1-p)^(n-1) (n = 1,2,...) or p*(1-p)^y (y = 0,1,...)

which is a particular case of a negative binomial, with
"target successes" = 1. In terms of the two-stage model
for a negative binomial (see V&R MASS section 7.4), this
corresponds to

   (mu^y * theta^theta)/(mu + theta)^(theta + y)
   *gamma(theta + y)/(gamma(theta)*y!)

with theta = 1 and p = theta/(mu + theta) = 1/(mu + 1).

This was in the context of having landed on glm.nb in MASS.

However, glm.nb fits theta, which I would want to fix at 1.

I don't see anything in ?glm.nb which allows theta to be
held at a fixed value.

The next snag is that it would not be straightforward, as
far as I can see, to introduce covariates. The typical data
set would be a set of sequences each of the form

   X1 Y1 0
   X2 Y2 0
   .......
   Xn Yn 1

where the value of n is random, so varies from sequence to
sequence. In the above negative binomial framework, y=(n-1)
and the covariates for that value of y would be the set

 (X1,X2,...,Xn, Y1,Y2,...,Yn)

and therefore of variable length for each observation (i.e.
sequence as above, or value of y per sequence). I don't
know how one can accomodate a variable length of covariates
per observation.

So it looks as though glm.nb, while thinking along the lines
I want, won't fit the bill!

However, other features of glm.nb would be suitable, since

  p/(1-p) = 1/mu

and a logistic model for p therefore means a linear fit to
log(mu), and glm.nb allows a log link.

Comments welcome!
With thanks,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 23-Sep-06                                       Time: 16:15:14
------------------------------ XFMail ------------------------------


From ethan.johnsons at gmail.com  Sat Sep 23 17:27:58 2006
From: ethan.johnsons at gmail.com (Ethan Johnsons)
Date: Sat, 23 Sep 2006 11:27:58 -0400
Subject: [R] Y-axis on pbinom
In-Reply-To: <1159005259.27617.12.camel@localhost.localnet>
References: <5cd96f050609222237t70995b8cp68099bfb02e0e170@mail.gmail.com>
	<1159005259.27617.12.camel@localhost.localnet>
Message-ID: <5cd96f050609230827u45dc4068m1f0ab2f13c4d3ab5@mail.gmail.com>

thx so much..Maciej .

ej

On 9/23/06, Maciej Blizi?ski <m.blizinski at wit.edu.pl> wrote:
> On Sat, 2006-09-23 at 01:37 -0400, Ethan Johnsons wrote:
> > How do you change the Y-axis from 0 ~ 0.6?
>
> plot(..., ylim = c(0, 0.6))
>
> --
> Maciej Blizi?ski
> http://automatthias.wordpress.com
>
>


From hb at stat.berkeley.edu  Sat Sep 23 18:05:49 2006
From: hb at stat.berkeley.edu (Henrik Bengtsson)
Date: Sat, 23 Sep 2006 09:05:49 -0700
Subject: [R] Creating Movies with R
In-Reply-To: <971536df0609221226k39be7dednef9dd2220150d040@mail.gmail.com>
References: <a2b3004b0609220023ic3eb836k27c363e54416359c@mail.gmail.com>
	<45142F9C.7020606@vanderbilt.edu>
	<Pine.LNX.4.58.0609221456480.21306@penguin.rand.org>
	<971536df0609221226k39be7dednef9dd2220150d040@mail.gmail.com>
Message-ID: <59d7961d0609230905k6a247be2l99b4e9e39ebd1dc2@mail.gmail.com>

On 9/22/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> See the flag= argument on formatC:
>
> n <- 10
> formatC(1:n, dig = nchar(n)-1, flag = 0)
>
> # Here is another way:
>
> n <- 10
> sprintf(paste("%0", nchar(n), ".0f", sep = ""), 1:n)

sprintf("%0*.0f", nchar(n), 1:n)

or even

sprintf("%0*d", nchar(n), 1:n)

/H

>
> On 9/22/06, J.R. Lockwood <lockwood at rand.org> wrote:
> > An alternative that I've used a few times is the jpg() function to
> > create the sequence of images, and then converting these to an mpeg
> > movie using "mencoder" distributed with "mplayer".  This works on both
> > windows and linux.  I have a pretty self-contained example file
> > written up that I can send to anyone who is interested.  Oddly, the
> > most challenging part was creating a sequence of file names that would
> > be correctly ordered - for this I use:
> >
> > lex <- function(N){
> >  ## produce vector of N lexicograpically ordered strings
> >  ndig <- nchar(N)
> >  substr(formatC((1:N)/10^ndig,digits=ndig,format="f"),3,10000000)
> > }
> >
> >
> > On Fri, 22 Sep 2006, Jeffrey Horner wrote:
> >
> > > Date: Fri, 22 Sep 2006 13:46:52 -0500
> > > From: Jeffrey Horner <jeff.horner at vanderbilt.edu>
> > > To: Lorenzo Isella <lorenzo.isella at gmail.com>, r-help at stat.math.ethz.ch
> > > Subject: Re: [R] Creating Movies with R
> > >
> > > If you run R on Linux, then you can run the ImageMagick command called
> > > convert. I place this in an R function to use a sequence of PNG plots as
> > > movie frames:
> > >
> > > make.mov.plotcol3d <- function(){
> > >      unlink("plotcol3d.mpg")
> > >      system("convert -delay 10 plotcol3d*.png plotcol3d.mpg")
> > > }
> > >
> > > Examples can be seen here:
> > >
> > > http://biostat.mc.vanderbilt.edu/JrhRgbColorSpace
> > >
> > > Look for the 'Download Movie' links.
> > >
> > > Cheers,
> > >
> > > Jeff
> > >
> > > Lorenzo Isella wrote:
> > > > Dear All,
> > > >
> > > > I'd like to know if it is possible to create animations with R.
> > > > To be specific, I attach a code I am using for my research to plot
> > > > some analytical results in 3D using the lattice package. It is not
> > > > necessary to go through the code.
> > > > Simply, it plots some 3D density profiles at two different times
> > > > selected by the user.
> > > > I wonder if it is possible to use the data generated for different
> > > > times to create something like an .avi file.
> > > >
> > > > Here is the script:
> > > >
> > > > rm(list=ls())
> > > > library(lattice)
> > > >
> > > > # I start defining the analytical functions needed to get the density
> > > > as a function of time
> > > >
> > > > expect_position <- function(t,lam1,lam2,pos_ini,vel_ini)
> > > > {1/(lam1-lam2)*(lam1*exp(lam2*t)-lam2*exp(lam1*t))*pos_ini+
> > > > 1/(lam1-lam2)*(exp(lam1*t)-exp(lam2*t))*vel_ini
> > > > }
> > > >
> > > > sigma_pos<-function(t,q,lam1,lam2)
> > > > {
> > > > q/(lam1-lam2)^2*(
> > > > (exp(2*lam1*t)-1)/(2*lam1)-2/(lam1+lam2)*(exp(lam1*t+lam2*t)-1) +
> > > > (exp(2*lam2*t)-1)/(2*lam2) )
> > > > }
> > > >
> > > > rho_x<-function(x,expect_position,sigma_pos)
> > > > {
> > > > 1/sqrt(2*pi*sigma_pos)*exp(-1/2*(x-expect_position)^2/sigma_pos)
> > > > }
> > > >
> > > > #### Now the physical parameters
> > > > tau<-0.1
> > > > beta<-1/tau
> > > > St<-tau ### since I am in dimensionless units and tau is already in
> > > > units of 1/|alpha|
> > > > D=2e-2
> > > > q<-2*beta^2*D
> > > > ############### Now the grid in space and time
> > > > time<-5  # time extent
> > > > tsteps<-501 # time steps
> > > > newtime<-seq(0,time,len=tsteps)
> > > > #### Now the things specific for the dynamics along x
> > > > lam1<- -beta/2*(1+sqrt(1+4*St))
> > > > lam2<- -beta/2*(1-sqrt(1+4*St))
> > > > xmin<- -0.5
> > > > xmax<-0.5
> > > > x0<-0.1
> > > > vx0<-x0
> > > > nx<-101 ## grid intervals along x
> > > > newx<-seq(xmin,xmax,len=nx) # grid along x
> > > >
> > > > # M1 <- do.call("g", c(list(x = newx), mypar))
> > > >
> > > >
> > > > mypar<-c(q,lam1,lam2)
> > > > sig_xx<-do.call("sigma_pos",c(list(t=newtime),mypar))
> > > > mypar<-c(lam1,lam2,x0,vx0)
> > > > exp_x<-do.call("expect_position",c(list(t=newtime),mypar))
> > > >
> > > > #rho_x<-function(x,expect_position,sigma_pos)
> > > >
> > > > #NB: at t=0, the density blows up, since I have a delta as the initial state!
> > > > # At any t>0, instead, the result is finite.
> > > > #for this reason I now redefine time by getting rid of the istant t=0
> > > > to work out
> > > > # the density
> > > >
> > > >
> > > > rho_x_t<-matrix(ncol=nx,nrow=tsteps-1)
> > > > for (i in 2:tsteps)
> > > > {mypar<-c(exp_x[i],sig_xx[i])
> > > > myrho_x<-do.call("rho_x",c(list(x=newx),mypar))
> > > > rho_x_t[ i-1, ]<-myrho_x
> > > > }
> > > >
> > > > ### Now I also define a scaled density
> > > >
> > > > rho_x_t_scaled<-matrix(ncol=nx,nrow=tsteps-1)
> > > > for (i in 2:tsteps)
> > > > {mypar<-c(exp_x[i],sig_xx[i])
> > > > myrho_x<-do.call("rho_x",c(list(x=newx),mypar))
> > > > rho_x_t_scaled[ i-1, ]<-myrho_x/max(myrho_x)
> > > > }
> > > >
> > > > ###########Now I deal with the dynamics along y
> > > >
> > > > lam1<- -beta/2*(1+sqrt(1-4*St))
> > > > lam2<- -beta/2*(1-sqrt(1-4*St))
> > > > ymin<- 0
> > > > ymax<- 1
> > > > y0<-ymax
> > > > vy0<- -y0
> > > >
> > > > mypar<-c(q,lam1,lam2)
> > > > sig_yy<-do.call("sigma_pos",c(list(t=newtime),mypar))
> > > > mypar<-c(lam1,lam2,y0,vy0)
> > > > exp_y<-do.call("expect_position",c(list(t=newtime),mypar))
> > > >
> > > >
> > > > # now I introduce the function giving the density along y: this has to
> > > > include the BC of zero
> > > > # density at wall
> > > >
> > > > rho_y<-function(y,expect_position,sigma_pos)
> > > > {
> > > > 1/sqrt(2*pi*sigma_pos)*exp(-1/2*(y-expect_position)^2/sigma_pos)-
> > > > 1/sqrt(2*pi*sigma_pos)*exp(-1/2*(y+expect_position)^2/sigma_pos)
> > > > }
> > > >
> > > > newy<-seq(ymin,ymax,len=nx) # grid along y with the same # of points
> > > > as the one along x
> > > >
> > > >
> > > > rho_y_t<-matrix(ncol=nx,nrow=tsteps-1)
> > > > for (i in 2:tsteps)
> > > > {mypar<-c(exp_y[i],sig_yy[i])
> > > > myrho_y<-do.call("rho_y",c(list(y=newy),mypar))
> > > > rho_y_t[ i-1, ]<-myrho_y
> > > > }
> > > >
> > > > rho_y_t_scaled<-matrix(ncol=nx,nrow=tsteps-1)
> > > > for (i in 2:tsteps)
> > > > {mypar<-c(exp_y[i],sig_yy[i])
> > > > myrho_y<-do.call("rho_y",c(list(y=newy),mypar))
> > > > rho_y_t_scaled[ i-1, ]<-myrho_y/max(myrho_y)
> > > > }
> > > >
> > > >
> > > > # The following 2 plots are an example of the plots I'd like to use to
> > > > make an animation
> > > >
> > > >
> > > > g <- expand.grid(x = newx, y = newy)
> > > >
> > > > instant<-100
> > > > mydens<-rho_x_t[ instant, ]%o%rho_y_t[ instant, ]/(max(rho_x_t[
> > > > instant, ]%o%rho_y_t[ instant, ]))
> > > >
> > > >
> > > > lentot<-nx^2
> > > > dim(mydens)<-c(lentot,1)
> > > >
> > > > g$z<-mydens
> > > > jpeg("dens-t-3.jpeg")
> > > > print(wireframe(z ~ x * y, g, drape = TRUE,shade=TRUE,
> > > > scales = list(arrows = FALSE),pretty=FALSE, aspect = c(1,1), colorkey = TRUE
> > > > ,zoom=0.8, main=expression("Density at t=2"), zlab =
> > > > list(expression("density"),rot = 90),distance=0.0,
> > > > perspective=TRUE,#screen = list(z = 150, x = -55,y= 0)
> > > > ,zlim=range(c(0,1))))
> > > > dev.off()
> > > >
> > > >
> > > > instant<-300
> > > > mydens<-rho_x_t[ instant, ]%o%rho_y_t[ instant, ]/(max(rho_x_t[
> > > > instant, ]%o%rho_y_t[ instant, ]))
> > > >
> > > >
> > > > lentot<-nx^2
> > > > dim(mydens)<-c(lentot,1)
> > > >
> > > > g$z<-mydens
> > > > jpeg("dens-t-3.jpeg")
> > > > print(wireframe(z ~ x * y, g, drape = TRUE,shade=TRUE,
> > > > scales = list(arrows = FALSE),pretty=FALSE, aspect = c(1,1), colorkey = TRUE
> > > > ,zoom=0.8, main=expression("Density at t=3"), zlab =
> > > > list(expression("density"),rot = 90),distance=0.0,
> > > > perspective=TRUE,#screen = list(z = 150, x = -55,y= 0)
> > > > ,zlim=range(c(0,1))))
> > > > dev.off()
> > > >
> > > >
> > > >
> > > >
> > > > Kind Regards
> > > >
> > > > Lorenzo
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible code.
> > >
> > >
> > > --
> > > http://biostat.mc.vanderbilt.edu/JeffreyHorner
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> > J.R. Lockwood
> > 412-683-2300 x4941
> > lockwood at rand.org
> > http://www.rand.org/statistics/bios/
> >
> > --------------------
> >
> > This email message is for the sole use of the intended recip...{{dropped}}
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ethan.johnsons at gmail.com  Sat Sep 23 19:27:52 2006
From: ethan.johnsons at gmail.com (Ethan Johnsons)
Date: Sat, 23 Sep 2006 13:27:52 -0400
Subject: [R] pbinom with rnorm
Message-ID: <5cd96f050609231027n7f9ef6ao1a2638c7c5776eed@mail.gmail.com>

Is there a way to plug in the random samples, rnorm(100), in this plot?
I am using 100, but want it to be randome samples.
plot(seq(from=0,to=100,by=1),1-pbinom
(seq(from=0,to=100,by=1),size=100,prob=0.05),pch=15)

thx much


From mothsailor at googlemail.com  Sat Sep 23 19:33:09 2006
From: mothsailor at googlemail.com (David Barron)
Date: Sat, 23 Sep 2006 18:33:09 +0100
Subject: [R] problem with summary of data
In-Reply-To: <6462497.post@talk.nabble.com>
References: <6462497.post@talk.nabble.com>
Message-ID: <815b70590609231033q7b520bdcqb8530a125e28f6bd@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060923/7d29d778/attachment.pl 

From p.dalgaard at biostat.ku.dk  Sat Sep 23 19:54:35 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 23 Sep 2006 19:54:35 +0200
Subject: [R] problem with summary of data
In-Reply-To: <815b70590609231033q7b520bdcqb8530a125e28f6bd@mail.gmail.com>
References: <6462497.post@talk.nabble.com>
	<815b70590609231033q7b520bdcqb8530a125e28f6bd@mail.gmail.com>
Message-ID: <x2r6y28o4k.fsf@turmalin.kubism.ku.dk>

"David Barron" <mothsailor at googlemail.com> writes:

> You haven't shown the command you used to input the data, but it looks as
> though you haven't told R not to convert the input into factors. 

Looks even more like R wasn't told that the first line of data was the
variable names. 

> summary on
> factors shows the frequencies of the levels for each variable, which is what
> your output shows.  See ?read.table for how to prevent conversion into
> factors.

And possibly also the Data Import/Export manual.

> On 23/09/06, Nel N. <eeneling at yahoo.com> wrote:
> >
> >
> > Greetings everybody,
> > I'm new with R and I'm trying to do some statistical analysis with R using
> > my data.
> > It is a DNA microarray data with 69 samples(row) and 7070 (column)
> > variables. I read the file using the command
> > 'file.table' and it can be done with no error.However, when I tried
> > running
> > a summary of the data, the result was not what I had expected. Below is an
> > extract of the result. And I knew that the command summary will give a
> > summary of a data's descriptive statistics such as it's median,mean,etc.So
> > ,
> > what i want is the statistics summary of each gene (in each column).
> >      V7065          V7066        V7067        V7068            V7069
> > 20     :29   20       :69   117    : 2   39     : 6   20         :69
> > 25     : 4   X16699_at: 1   134    : 2   42     : 4   L49218_f_at: 1
> > 30     : 4                  142    : 2   44     : 4
> > 23     : 3                  170    : 2   46     : 4
> > 26     : 3                  203    : 2   38     : 3
> > 21     : 2                  96     : 2   58     : 3
> > (Other):25                  (Other):58   (Other):46
> >      V7070            V7071
> > 30     : 5   20         :69
> > 21     : 4   Z78285_f_at: 1
> > 51     : 4
> > 64     : 4
> > 27     : 3
> > 36     : 3
> > (Other):47
> > My data is in .dat format. Also, I have downloaded a few .rda files to be
> > tried out with R but how do I view them as data per se, like the .dat file
> > can be viewed and edited using Notepad, how about the .rda files?
> > I express my greatest appreciation all who help.
> > Thank you.
> > --
> > View this message in context:
> > http://www.nabble.com/problem-with-summary-of-data-tf2322882.html#a6462497
> > Sent from the R help mailing list archive at Nabble.com.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> 
> 
> -- 
> =================================
> David Barron
> Said Business School
> University of Oxford
> Park End Street
> Oxford OX1 1HP
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From deepayan.sarkar at gmail.com  Sat Sep 23 21:30:25 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Sat, 23 Sep 2006 12:30:25 -0700
Subject: [R] Lattice strip labels for two factors
In-Reply-To: <971536df0609230702o7d7c5fei588079e7c16d76ff@mail.gmail.com>
References: <45143B48.3020601@ipimar.pt>
	<971536df0609221300i4a0a3a21w2914588f6b787b7e@mail.gmail.com>
	<451502CE.6040902@ipimar.pt>
	<971536df0609230702o7d7c5fei588079e7c16d76ff@mail.gmail.com>
Message-ID: <eb555e660609231230g70f517b4w1289afbbb67359f0@mail.gmail.com>

On 9/23/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> 1. You can write a custom strip function:
>
>         my.strip <- function(which.given, ..., factor.levels) {
>            levs <- if (which.given == 1) factor.levels
>                    else c("faro", "porto", "lisbon", "setubal")
>            strip.default(which.given, ..., factor.levels = levs)
>         }
>
>         xyplot(value ~ year | fact1 * fact2, data = df,
>            strip = my.strip)
>
> 2. however, its probably easier just to change the levels in
> the data frame.  Just do it in a copy if you don't want to
> change the original one:
>
>         df2 <- df
>         levels(df2$fact2) <- c("faro", "porto", "lisbon", "setubal")
>         xyplot(value ~ year | fact1 * fact2, data = df2)
>
> 3. or you can even do it inline in the data statement which
> similarly won't change the original data frame:
>
>         levs <- c("faro", "porto", "lisbon", "setubal")
>         xyplot(value ~ year | fact1 * fact2,
>            data = replace(df, "fact2", structure(df$fact2, levels = levs)))
>         head(df) # unchanged

or even (untested)

xyplot(value ~ year | fact1 * factor(fact2, levels = levels(fact2),
labels = levs),
       data = df)

Deepayan

> On 9/23/06, Rafael Duarte <rduarte at ipimar.pt> wrote:
> > Thank you for your suggestion.
> > This could be a solution that I didn't think of.
> >
> > But I forgot to say that I didn't want to change the original data frame
> > (I have other code that depends on the original df and on the original
> > factor levels).
> > I was looking more for an implementation directly in the xyplot call
> > (same as I did for one factor). Is it possible/simple to do?
> >
> > Thank you,
> > Rafael
> >
> >
> > Gabor Grothendieck wrote:
> >
> > > Try this:
> > >
> > > levels(df$fact2) <- c("faro","porto","lisbon","setubal")
> > > xyplot( value ~ year | fact1*fact2, data=df, type="b")
> > >
> > >
> > > On 9/22/06, Rafael Duarte <rduarte at ipimar.pt> wrote:
> > >
> > >> Dear list,
> > >> My problem is to change the strip text of lattice panels when using two
> > >> factors.
> > >> I have a data frame with two factors:
> > >>
> > >> df <- expand.grid( "fact1"=c("y","b","r"),
> > >> "fact2"=c("far","por","lis","set"), "year"=1991:2000, "value"= NA)
> > >> df[,"value"] <- sample(1:50, 120, replace=TRUE)
> > >>
> > >> I can make simple xyplot and change the text of the factor levels with
> > >> strip.custom:
> > >>
> > >> require("lattice")
> > >> xyplot( value ~ year | fact1, data=df, type="b", subset= fact2=="far",
> > >> strip = strip.custom(bg=gray.colors(1,0.95), factor.levels=c("yellow",
> > >> "black", "red")),
> > >> layout=c(1,3)
> > >> )
> > >>
> > >> But how can I change the text of the factor levels when using both
> > >> factors as in this plot:
> > >> xyplot( value ~ year | fact1*fact2, data=df, type="b")
> > >>
> > >> (fact2 levels text should change to:
> > >> c("faro","porto","lisbon","setubal"))
> > >>
> > >> I read the help for strip.default and the emails archive, tried with
> > >> "which.given" but could not find out how to accomplish this.
> > >>
> > >> Many thanks,
> > >> Rafael Duarte
> > >>
> > >> --
> > >> Rafael Duarte
> > >> Marine Resources Department - DRM
> > >> IPIMAR -  National Research Institute for Agriculture and Fisheries
> > >> Av. Bras?lia, 1449-006 Lisbon  -  Portugal
> > >> Tel:+351 21 302 7000      Fax:+351 21 301 5948
> > >> e-mail: rduarte at ipimar.pt
> > >>
> > >> ______________________________________________
> > >> R-help at stat.math.ethz.ch mailing list
> > >> https://stat.ethz.ch/mailman/listinfo/r-help
> > >> PLEASE do read the posting guide
> > >> http://www.R-project.org/posting-guide.html
> > >> and provide commented, minimal, self-contained, reproducible code.
> > >>
> >
> >
> > --
> > Rafael Duarte
> > Marine Resources Department - DRM
> > IPIMAR -  National Research Institute for Agriculture and Fisheries
> > Av. Bras?lia, 1449-006 Lisbon  -  Portugal
> > Tel:+351 21 302 7000      Fax:+351 21 301 5948
> > e-mail: rduarte at ipimar.pt
> >
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
http://www.stat.wisc.edu/~deepayan/


From jim at bitwrit.com.au  Sun Sep 24 04:10:52 2006
From: jim at bitwrit.com.au (Jim Lemon)
Date: Sat, 23 Sep 2006 22:10:52 -0400
Subject: [R] Adding percentage to Pie Charts
Message-ID: <4515E92C.4070506@bitwrit.com.au>

Hi all,

Anupam Tyagi mentioned an interesting idea a few days ago.

"A modification in a pie chart that draws overlapping areas with a 
common start point at the top of the circle, can make is more 
informative than a dot-chart.
Something like:
* Start drawing at the top of the circle, as zero (degree/area).
* Draw the representation of every value starting from the top, as zero,
representing it as a labled line from the center of the circle to the 
boundary
(can use colors where possible).
* Use two lables for the circular axis, inside one for percentages, 
outside for values."

I admit to interpreting this pretty loosely, but I would like to know 
what people think of a "fan plot".

fan.plot<-function(x,edges=200,radius=1,col=NULL,centerpos=pi/2,
  labels=NULL,...) {

  if (!is.numeric(x) || any(is.na(x) | x<=0))
   stop("fan.plot: x values must be positive.")
  # scale the values to a half circle
  x<-pi*x/sum(x)
  xorder<-order(x,decreasing=TRUE)
  nx <- length(x)
  if (is.null(col)) col<-rainbow(nx)
  else if(length(col) < nx) col<-rep(col,nx)
  oldpar<-par(no.readonly=TRUE)
  par(mar=c(0,0,4,0))
  plot(0,xlim=c(-1,1),ylim=c(-0.6,1),xlab="",ylab="",type="n",axes=FALSE)
  lside<--0.8
  for(i in 1:nx) {
   n<-edges*x[xorder[i]]/pi
   t2p<-seq(centerpos-x[xorder[i]],centerpos+x[xorder[i]],length=n)
   xc<-c(cos(t2p)*radius,0)
   yc<-c(sin(t2p)*radius,0)
   polygon(xc,yc,col=col[xorder[i]],...)
   if(!is.null(labels)) {
    xpos<-lside*sin(x[xorder[i]])*radius
    ypos<--i/10
    text(xpos,ypos,labels[xorder[i]])
    ytop<-cos(x[xorder[i]])*radius*radius
    segments(xpos,ypos+1/20,xpos,ytop)
    lside<--lside
   }
   radius<-radius-0.02
  }
}

Jim


From jgreenberg at arc.nasa.gov  Sat Sep 23 23:54:00 2006
From: jgreenberg at arc.nasa.gov (Jonathan Greenberg)
Date: Sat, 23 Sep 2006 14:54:00 -0700
Subject: [R] Data frames questions
Message-ID: <C13AFB08.C70C%jgreenberg@arc.nasa.gov>

Hi there, couple of questions on data frames:

1) Is there a way to build an empty data frame, containing nothing but the
data frame variable names?
2) Is there a way to reorder the variables in a data frame, e.g. When I go
to write out a data frame using write.table or write.matrix, I want the
output in a certain order...
3) How to I "append" to the bottom of a dataframe?

Thanks!

--j

-- 
Jonathan A. Greenberg, PhD
NRC Research Associate
NASA Ames Research Center
MS 242-4
Moffett Field, CA 94035-1000
Office: 650-604-5896
Cell: 415-794-5043
AIM: jgrn307
MSN: jgrn307 at hotmail.com


From jgreenberg at arc.nasa.gov  Sun Sep 24 00:08:22 2006
From: jgreenberg at arc.nasa.gov (Jonathan Greenberg)
Date: Sat, 23 Sep 2006 15:08:22 -0700
Subject: [R] Question about merge()
Message-ID: <C13AFE66.C70D%jgreenberg@arc.nasa.gov>

If I want to do a join based on *two* matching fields in two data frames,
can merge() handle this?  It appears to only handle a single matching column
-- do I need to make a "metacolumn" or is there some way to do this?  E.g.:

Dataframe 1 contains columns A,B,C and Dataframe 2 contains A,B,D

I want an output A,B,C,D which places C and D together if A and B match
(otherwise, make two new rows, e.g. Ax,Bx,Cx,nodata and Ay,By,nodata,Dy)

--j

-- 
Jonathan A. Greenberg, PhD
NRC Research Associate
NASA Ames Research Center
MS 242-4
Moffett Field, CA 94035-1000
Office: 650-604-5896
Cell: 415-794-5043
AIM: jgrn307
MSN: jgrn307 at hotmail.com


From afshart at exchange.sba.miami.edu  Sun Sep 24 00:09:38 2006
From: afshart at exchange.sba.miami.edu (Afshartous, David)
Date: Sat, 23 Sep 2006 18:09:38 -0400
Subject: [R] plotting grouped data object
Message-ID: <6BCB4D493A447546A8126F24332056E804300264@school1.business.edu>

 
All,
I'd like to plot the main relationship of a grouped data 
object for all levels of a factor in a single panel.
The sample code below creates a separate panel for each level
of the factor.  I realize that this could be done in other ways, 
but I'd like to do it via plotting the grouped data object.
thanks!
dave

z = rnorm(18, mean=0, sd=1)
x = rep(1:6, 3)
y = factor(rep(c("I", "C", "P"), each = 6))
dat = data.frame(x, y, z)
data.grp = groupedData(z ~ x | y, data = dat)
plot(data.grp, outer = ~ y)
### this produces 1 line each in 3 panels
### how to collapse all 3 lines into 1 panel?


From xchen_stat at hotmail.com  Sun Sep 24 00:16:13 2006
From: xchen_stat at hotmail.com (X.H Chen)
Date: Sat, 23 Sep 2006 16:16:13 -0600
Subject: [R] Data frames questions
In-Reply-To: <C13AFB08.C70C%jgreenberg@arc.nasa.gov>
Message-ID: <BAY23-F83C4EAA1F242C47FD7B0FFA260@phx.gbl>

>1) Is there a way to build an empty data frame, containing nothing but the
>data frame variable names?

Yes, you can do it one way as followings:

df1<-as.data.frame(matrix(nrow=2,ncol=2),row.names=c("R1","R2"))

>2) Is there a way to reorder the variables in a data frame, e.g. When I go
>to write out a data frame using write.table or write.matrix, I want the
>output in a certain order...

df2<-as.data.frame(matrix(1:4,nrow=2,ncol=2),row.names=c("R3","R4"))
df2<-df2[c(2,1),]

>3) How to I "append" to the bottom of a dataframe?

df2<-rbind(df2,c(5,6))

Xiaohui Chen

Dept. of Statistics
UBC, Canada




>From: Jonathan Greenberg <jgreenberg at arc.nasa.gov>
>To: R-help <r-help at stat.math.ethz.ch>
>Subject: [R] Data frames questions
>Date: Sat, 23 Sep 2006 14:54:00 -0700
>
>Hi there, couple of questions on data frames:
>
>1) Is there a way to build an empty data frame, containing nothing but the
>data frame variable names?
>2) Is there a way to reorder the variables in a data frame, e.g. When I go
>to write out a data frame using write.table or write.matrix, I want the
>output in a certain order...
>3) How to I "append" to the bottom of a dataframe?
>
>Thanks!
>
>--j
>
>--
>Jonathan A. Greenberg, PhD
>NRC Research Associate
>NASA Ames Research Center
>MS 242-4
>Moffett Field, CA 94035-1000
>Office: 650-604-5896
>Cell: 415-794-5043
>AIM: jgrn307
>MSN: jgrn307 at hotmail.com
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide 
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From xchen_stat at hotmail.com  Sun Sep 24 00:25:08 2006
From: xchen_stat at hotmail.com (X.H Chen)
Date: Sat, 23 Sep 2006 16:25:08 -0600
Subject: [R] plotting grouped data object
In-Reply-To: <6BCB4D493A447546A8126F24332056E804300264@school1.business.edu>
Message-ID: <BAY23-F126A237E28E62863AADB6CFA260@phx.gbl>

I don't get your meaning in what is in datagrp, anyway, try:

X11()
par(new=T)

before calling:
plot(data.grp, outer = ~ y)

Xiaohui Chen

Dept. of Statistics
UBC, Canada




>From: "Afshartous, David" <afshart at exchange.sba.miami.edu>
>To: <r-help at stat.math.ethz.ch>
>Subject: [R] plotting grouped data object
>Date: Sat, 23 Sep 2006 18:09:38 -0400
>
>
>All,
>I'd like to plot the main relationship of a grouped data
>object for all levels of a factor in a single panel.
>The sample code below creates a separate panel for each level
>of the factor.  I realize that this could be done in other ways,
>but I'd like to do it via plotting the grouped data object.
>thanks!
>dave
>
>z = rnorm(18, mean=0, sd=1)
>x = rep(1:6, 3)
>y = factor(rep(c("I", "C", "P"), each = 6))
>dat = data.frame(x, y, z)
>data.grp = groupedData(z ~ x | y, data = dat)
>plot(data.grp, outer = ~ y)
>### this produces 1 line each in 3 panels
>### how to collapse all 3 lines into 1 panel?
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide 
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From pfh at synapse9.com  Sun Sep 24 00:59:28 2006
From: pfh at synapse9.com (phil henshaw)
Date: Sat, 23 Sep 2006 18:59:28 -0400
Subject: [R] really irregular time series
Message-ID: <005601c6df63$ec769ab0$2f01a8c0@SavyII>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060923/5f33b9e2/attachment.pl 

From xchen_stat at hotmail.com  Sun Sep 24 01:07:39 2006
From: xchen_stat at hotmail.com (X.H Chen)
Date: Sat, 23 Sep 2006 17:07:39 -0600
Subject: [R] Question about merge()
In-Reply-To: <C13AFE66.C70D%jgreenberg@arc.nasa.gov>
Message-ID: <BAY23-F15F8AD076359C11E0AD127FA260@phx.gbl>

merge() can handle this if you specify the "by" parameters to the same 
length vector for x and y. The parameters corresponds the columns you want 
to filter out under certain conditions and add the values to the merging 
data.frame.

For example(modified from No.1 example from ?merge):

authors <- data.frame(
         surname = c("Tukey", "Venables", "Tierney", "Ripley", "McNeil"),
         nationality = c("US", "Australia", "US", "UK", "Australia"),
         deceased = c("yes", rep("no", 4)),year=c(2000,2001,2000,2001,2000))
books <- data.frame(
         name = c("Tukey", "Venables", "Tierney",
                  "Ripley", "Ripley", "McNeil", "R Core"),
         title = c("Exploratory Data Analysis",
                   "Modern Applied Statistics ...",
                   "LISP-STAT",
                   "Spatial Statistics", "Stochastic Simulation",
                   "Interactive Data Analysis",
                   "An Introduction to R"),
         other.author = c(NA, "Ripley", NA, NA, NA, NA,
                          "Venables & 
Smith"),year=c(1999,2001,2000,2001,2000,2001,2002))

#Compare the following results:
merge(authors,books,by.x="surname",by.y="name")
merge(authors,books,by.x=c("surname","year"),by.y=c("name","year"))

Hope this can be some help of you.

Xiaohui Chen

Dept. of Statistics
UBC, Canada




>From: Jonathan Greenberg <jgreenberg at arc.nasa.gov>
>To: R-help <r-help at stat.math.ethz.ch>
>Subject: [R] Question about merge()
>Date: Sat, 23 Sep 2006 15:08:22 -0700
>
>If I want to do a join based on *two* matching fields in two data frames,
>can merge() handle this?  It appears to only handle a single matching 
>column
>-- do I need to make a "metacolumn" or is there some way to do this?  E.g.:
>
>Dataframe 1 contains columns A,B,C and Dataframe 2 contains A,B,D
>
>I want an output A,B,C,D which places C and D together if A and B match
>(otherwise, make two new rows, e.g. Ax,Bx,Cx,nodata and Ay,By,nodata,Dy)
>
>--j
>
>--
>Jonathan A. Greenberg, PhD
>NRC Research Associate
>NASA Ames Research Center
>MS 242-4
>Moffett Field, CA 94035-1000
>Office: 650-604-5896
>Cell: 415-794-5043
>AIM: jgrn307
>MSN: jgrn307 at hotmail.com
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide 
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

_________________________________________________________________
Buy what you want when you want it on Sympatico / MSN Shopping


From wangtong at usc.edu  Sun Sep 24 01:15:25 2006
From: wangtong at usc.edu (Tong Wang)
Date: Sat, 23 Sep 2006 16:15:25 -0700
Subject: [R] strange  warning message
Message-ID: <dcb8ad0b1672b.45155d9d@usc.edu>

Hi Everyone,
    I recently start to get this warning message,  while loading files in to R.   Could someone tell me what does it mean ?
I am using R 2.3.0 with Emacs on WinXP.

    use of NULL environment is deprecated 



Thanks a lot for any help

best


From deming.mi at Vanderbilt.Edu  Sun Sep 24 01:17:35 2006
From: deming.mi at Vanderbilt.Edu (Mi, Deming)
Date: Sat, 23 Sep 2006 18:17:35 -0500
Subject: [R] variance-covariance structure of random effects in lme
Message-ID: <53B010B7EB099A4A930C8CC6474FDD3E2D02C2@mailbe08.mc.vanderbilt.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060923/f9725a69/attachment.pl 

From ggrothendieck at gmail.com  Sun Sep 24 02:06:45 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 23 Sep 2006 20:06:45 -0400
Subject: [R] really irregular time series
In-Reply-To: <005601c6df63$ec769ab0$2f01a8c0@SavyII>
References: <005601c6df63$ec769ab0$2f01a8c0@SavyII>
Message-ID: <971536df0609231706hb5934d9rcd18224531bc4281@mail.gmail.com>

The zoo package supports irregular time series with arbitrary
time classes.  It include na.approx and na.spline for interpolation.

library(zoo)
vignette("zoo")
vignette("zoo-quickref")


On 9/23/06, phil henshaw <pfh at synapse9.com> wrote:
> I built some reasonably successful tools in a graphical database for
> reconstructing the developmental turning points for feedback loop driven
> natural processes.    I'm trying to move it to R and am having
> difficulty with the very basics, i.e. a) defining time series using time
> as a natural number rather than a place in a table.    I was also hoping
> b) someone might have built some of the diverse kinds of interpolation
> tools needed for irregular data on evolving systems to reconstruct their
> probable naturalistic fluctuation and turning points, making good
> representations of them compatible with other analytical tools.
>
> Is there any functionality for either a) or b)?    I could be just lazy,
> but  'its' seems to require a calendar date format, and I'm interested
> in reading the unique developmental system dynamics of some subjects in
> irregular milliseconds and others in irregular millions of years.
>
> Another thing I miss is being able to 'touch'  and query the graphic
> objects produced.   In Autolisp I was able to store a complete history
> of the operations performed in the list associated with any graphic
> object and have it pop up by clicking it with the mouse.
>
> Is there any kind of interface like that available or coming?
>
>
> Phil Henshaw                       ????.?? ? `?.????
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> 680 Ft. Washington Ave
> NY NY 10040
> tel: 212-795-4844
> e-mail: pfh at synapse9.com
> explorations: www.synapse9.com <http://www.synapse9.com/>
>
>
>        [[alternative HTML version deleted]]
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


From liuwensui at gmail.com  Sun Sep 24 02:26:57 2006
From: liuwensui at gmail.com (Wensui Liu)
Date: Sat, 23 Sep 2006 20:26:57 -0400
Subject: [R] Question about merge()
In-Reply-To: <C13AFE66.C70D%jgreenberg@arc.nasa.gov>
References: <C13AFE66.C70D%jgreenberg@arc.nasa.gov>
Message-ID: <1115a2b00609231726y4f0e95cbwc2cdeb682ac5b76@mail.gmail.com>

MERGE DATA FRAMES BY 2 OR MORE VARIABLES
#######################################
# MERGE 2 DATA FRAMES BASED ON        #
# 2 OR MORE VARIABLES                 #
#######################################

data1<-data.frame(x.id1 = 1:10, x.id2 = (1:10) * 2, x = rnorm(length(1:10)));
data2<-data.frame(y.id1 = seq(1, 20, by = 2), y.id2 = seq(1, 20, by =
2) * 2, y = rnorm(length(1:10)));

merged<-merge(data1, data2, by.x = c("x.id1", "x.id2"), by.y =
c("y.id1", "y.id2"), all = TRUE);


On 9/23/06, Jonathan Greenberg <jgreenberg at arc.nasa.gov> wrote:
> If I want to do a join based on *two* matching fields in two data frames,
> can merge() handle this?  It appears to only handle a single matching column
> -- do I need to make a "metacolumn" or is there some way to do this?  E.g.:
>
> Dataframe 1 contains columns A,B,C and Dataframe 2 contains A,B,D
>
> I want an output A,B,C,D which places C and D together if A and B match
> (otherwise, make two new rows, e.g. Ax,Bx,Cx,nodata and Ay,By,nodata,Dy)
>
> --j
>
> --
> Jonathan A. Greenberg, PhD
> NRC Research Associate
> NASA Ames Research Center
> MS 242-4
> Moffett Field, CA 94035-1000
> Office: 650-604-5896
> Cell: 415-794-5043
> AIM: jgrn307
> MSN: jgrn307 at hotmail.com
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
WenSui Liu
(http://spaces.msn.com/statcompute/blog)
Senior Decision Support Analyst
Cincinnati Children Hospital Medical Center


From liuwensui at gmail.com  Sun Sep 24 02:28:43 2006
From: liuwensui at gmail.com (Wensui Liu)
Date: Sat, 23 Sep 2006 20:28:43 -0400
Subject: [R] Data frames questions
In-Reply-To: <C13AFB08.C70C%jgreenberg@arc.nasa.gov>
References: <C13AFB08.C70C%jgreenberg@arc.nasa.gov>
Message-ID: <1115a2b00609231728s63c6442dj36f548966df32970@mail.gmail.com>

> 3) How to I "append" to the bottom of a dataframe?

APPEND ROWS ITERATIVELY TO A DATA FRAME

#################################################
# APPEND ROWS ITERATIVELY TO A DATA FRAME       #
#################################################

# LOOP FROM 1 TO 10
for(i in 1:10)
{
  # SET THE RANDOM STATE
  set.seed(i);
  # CREATE A DUMMY DATA FRAME ITERATIVELY
  data<-data.frame(iter = i, x = rnorm(1), y = runif(1));
  # ASSIGN THE ROW NAME
  row.names(data)<-i;
  # APPEND NEW DATA TO THE END OF MAIN DATA FRAME
  if (i == 1) main<-data else main<-rbind(main, data);
};


From murdoch at stats.uwo.ca  Sun Sep 24 02:59:29 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 23 Sep 2006 20:59:29 -0400
Subject: [R] strange  warning message
In-Reply-To: <dcb8ad0b1672b.45155d9d@usc.edu>
References: <dcb8ad0b1672b.45155d9d@usc.edu>
Message-ID: <4515D871.9090205@stats.uwo.ca>

On 9/23/2006 7:15 PM, Tong Wang wrote:
> Hi Everyone,
>     I recently start to get this warning message,  while loading files in to R.   Could someone tell me what does it mean ?
> I am using R 2.3.0 with Emacs on WinXP.
> 
>     use of NULL environment is deprecated 

The files were saved in an earlier version of R, which used NULL to 
indicate the base environment.  R is telling you that NULL is not a 
legal environment.  It should be automatically converted to baseenv().

In a number of cases, people used NULL to indicate an empty environment 
(even though there was no such thing when NULL was used); if that's true 
for your code, then you'll need to fix it.  emptyenv() now gives you an 
empty environment if that's what you really want.

Duncan Murdoch


From ggrothendieck at gmail.com  Sun Sep 24 03:42:28 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 23 Sep 2006 21:42:28 -0400
Subject: [R] Adding percentage to Pie Charts
In-Reply-To: <4515E92C.4070506@bitwrit.com.au>
References: <4515E92C.4070506@bitwrit.com.au>
Message-ID: <971536df0609231842p1b7c3f74g80c425525221ced@mail.gmail.com>

It might also be nice to be able to align the fans at the left or right,
not just the center.

On 9/23/06, Jim Lemon <jim at bitwrit.com.au> wrote:
> Hi all,
>
> Anupam Tyagi mentioned an interesting idea a few days ago.
>
> "A modification in a pie chart that draws overlapping areas with a
> common start point at the top of the circle, can make is more
> informative than a dot-chart.
> Something like:
> * Start drawing at the top of the circle, as zero (degree/area).
> * Draw the representation of every value starting from the top, as zero,
> representing it as a labled line from the center of the circle to the
> boundary
> (can use colors where possible).
> * Use two lables for the circular axis, inside one for percentages,
> outside for values."
>
> I admit to interpreting this pretty loosely, but I would like to know
> what people think of a "fan plot".
>
> fan.plot<-function(x,edges=200,radius=1,col=NULL,centerpos=pi/2,
>  labels=NULL,...) {
>
>  if (!is.numeric(x) || any(is.na(x) | x<=0))
>   stop("fan.plot: x values must be positive.")
>  # scale the values to a half circle
>  x<-pi*x/sum(x)
>  xorder<-order(x,decreasing=TRUE)
>  nx <- length(x)
>  if (is.null(col)) col<-rainbow(nx)
>  else if(length(col) < nx) col<-rep(col,nx)
>  oldpar<-par(no.readonly=TRUE)
>  par(mar=c(0,0,4,0))
>  plot(0,xlim=c(-1,1),ylim=c(-0.6,1),xlab="",ylab="",type="n",axes=FALSE)
>  lside<--0.8
>  for(i in 1:nx) {
>   n<-edges*x[xorder[i]]/pi
>   t2p<-seq(centerpos-x[xorder[i]],centerpos+x[xorder[i]],length=n)
>   xc<-c(cos(t2p)*radius,0)
>   yc<-c(sin(t2p)*radius,0)
>   polygon(xc,yc,col=col[xorder[i]],...)
>   if(!is.null(labels)) {
>    xpos<-lside*sin(x[xorder[i]])*radius
>    ypos<--i/10
>    text(xpos,ypos,labels[xorder[i]])
>    ytop<-cos(x[xorder[i]])*radius*radius
>    segments(xpos,ypos+1/20,xpos,ytop)
>    lside<--lside
>   }
>   radius<-radius-0.02
>  }
> }
>
> Jim
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From sell_mirage_ne at hotmail.com  Sun Sep 24 07:08:55 2006
From: sell_mirage_ne at hotmail.com (Taka Matzmoto)
Date: Sun, 24 Sep 2006 00:08:55 -0500
Subject: [R] s language version and complie languages
Message-ID: <BAY110-F37DD9D01253328282EEF37C7270@phx.gbl>

Hi R users

Which version of S does the current R version use? Venables and Ripley's 
book (2000) says R is version 3 of S language. Is there any change since 
2000 ?

I searched R-help archive for using C or C++ with R. Although there are some 
postings, I am looking for up-to-date answers. Which one ( C and C++)  works 
better with R? What complier is the best suited for R ( gcc or visual c++ or 
others)?

Taka,

_________________________________________________________________
The next generation of Search?say hello!


From chenxh007 at gmail.com  Sun Sep 24 07:13:58 2006
From: chenxh007 at gmail.com (xchen)
Date: Sat, 23 Sep 2006 22:13:58 -0700
Subject: [R] s language version and complie languages
In-Reply-To: <BAY110-F37DD9D01253328282EEF37C7270@phx.gbl>
References: <BAY110-F37DD9D01253328282EEF37C7270@phx.gbl>
Message-ID: <45161416.80605@gmail.com>

Taka Matzmoto wrote:
> Hi R users
>
> Which version of S does the current R version use? Venables and 
> Ripley's book (2000) says R is version 3 of S language. Is there any 
> change since 2000 ?
>
> I searched R-help archive for using C or C++ with R. Although there 
> are some postings, I am looking for up-to-date answers. Which one ( C 
> and C++) works better with R? What complier is the best suited for R ( 
> gcc or visual c++ or others)?
>
> Taka,
>
> _________________________________________________________________
> The next generation of Search?say hello!
>
> ------------------------------------------------------------------------
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   
I usually use C++ compiler to generator dll for .C function calls


From ripley at stats.ox.ac.uk  Sun Sep 24 08:41:47 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 24 Sep 2006 07:41:47 +0100 (BST)
Subject: [R] s language version and complie languages
In-Reply-To: <BAY110-F37DD9D01253328282EEF37C7270@phx.gbl>
References: <BAY110-F37DD9D01253328282EEF37C7270@phx.gbl>
Message-ID: <Pine.LNX.4.64.0609240732420.29165@gannet.stats.ox.ac.uk>

On Sun, 24 Sep 2006, Taka Matzmoto wrote:

> Hi R users
>
> Which version of S does the current R version use? Venables and Ripley's book 
> (2000) says R is version 3 of S language. Is there any change since 2000 ?

Yes, the 'methods' package implements much of the changes in S version 4, 
and there are quite a few nods to S4 elsewhere in the code.  That is 
mentioned in the on-line errata files.

> I searched R-help archive for using C or C++ with R. Although there are some 
> postings, I am looking for up-to-date answers. Which one ( C and C++)  works 
> better with R? What complier is the best suited for R ( gcc or visual c++ or 
> others)?

You seem to be assuming Windows.  The recommended compiler (MinGW gcc) and 
toolset work best.  If you write C++, you need to use a C++ compiler, but 
remember that C++ is not binary compatible across compilers (or even 
compiler versions) and also that you need to use C bindings.  If you don't 
need C++ features, use a C compiler.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Sun Sep 24 09:08:19 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 24 Sep 2006 08:08:19 +0100 (BST)
Subject: [R] Fitdistr() versus nls()
In-Reply-To: <731DA923-3AEB-4516-B6B6-65E31DD49A79@cs.unibo.it>
References: <731DA923-3AEB-4516-B6B6-65E31DD49A79@cs.unibo.it>
Message-ID: <Pine.LNX.4.64.0609240801190.29384@gannet.stats.ox.ac.uk>

On Sat, 23 Sep 2006, Luca Telloli wrote:

> Hello R-Users,
> 	I'm new to R so I apologize in advance for any big mistake I might
> be doing. I'm trying to fit a set of samples with some probabilistic
> curve, and I have an important question to ask; in particular I have
> some data, from which I calculate manually the CDF, and then I import
> them into R and try to fit: I have the x values (my original samples)
> and the y values (P(X<x)).
> 	To attempt the fit I've both fitdistr() and nls(), in the way you
> can read in the piece of code at the end of the email. Because the
> fit with all data doesn't work very well, I decided to take a subset
> of samples randomly chosen (for some random x, the correspondant y is
> chosen).
> 	The first big problem is that fitdistr and nls, in the way I use
> them in the code, they don't get me similar results. I think I'm
> making a mistake but I can't really understand which one.
> 	From this first issue, a second one arises because the plot with nls
> is similar (maybe not a great fit bust still...) to my original CDF
> while the plot of fitdistr is basically a straight line of constant
> value y=1. On the other side, the fitdistr outperforms in the
> Kolmogorov-Smirnov test, which for nls has values of probability
> around 0 (not a good score huh?).
> 	Can u please tell me if there's a major mistake in the code?

Yes: you fit distribution models to x, not to the CDF of x.  So the call 
to fitdistr and those to ks.test should be of somevals.x not somevals.y.

Using nls here is fitting an exponentional to the CDF by least-squares.
This is not very sensible for an exponentional distribution where the MLE 
is known in closed form.

Using a KS test after estimating parameters is invalid unless you make 
adjustments (ks.test does not), and suitable adjustments are AFAIK only 
known for ML estimation.  (The help page does say so and point you to the 
literature.)

>
> Thanks in advance,
> Luca
>
>
> ------ BEGINNING OF CODE
> ----------------------------------------------------------------
> cdf.all=read.table("all_failures.cdf", header=FALSE, col.names=c
> ("ttr", "cdf"), sep=":" )
>
> allvals.x=array(t(cdf.all[1]))
> allvals.y=array(t(cdf.all[2]))

What is the intention here?  I suspect you want cdf.all[[1]] etc.

> library(MASS)
> bestval.exp.nls=bestval.exp.fit=-1
> plot(allvals.x, allvals.y)
>
> for(it in 1:100){
> 	#extract random samples
> 	random=sort(sample(1:length(allvals.x), 15))
> 	somevals.x=allvals.x[c(random)]
> 	somevals.y=allvals.y[c(random)]
> 	#fit with nls and fitdistr
> 	fit.exp = fitdistr(somevals.y, "exponential")
> 	nls.exp <- nls(somevals.y ~ pexp(somevals.x, rate), start=list(rate=.
> 0001), model=TRUE)
> 	#plot what you get out of the two fits
> 	lines(allvals.x, pexp(allvals.x, coef(fit.exp)), col=it)
> 	lines(allvals.x, pexp(allvals.x, coef(nls.exp)), col=it)
> 	#perform kolmogorov-smirnov test on your fit
> 	ks.exp.nls = ks.test(somevals.y, "pexp", coef(nls.exp))
> 	ks.exp.fit = ks.test(somevals.y, "pexp", coef(fit.exp))
>
> 	bestval.exp.fit = max(bestval.exp.fit, ks.exp.fit$p.value)
> 	bestval.exp.nls = max(bestval.exp.nls, ks.exp.nls$p.value)
> }
>
> print(bestval.exp.fit)
> print(bestval.exp.nls)
>
> ----------END OF
> CODE--------------------------------------------------------------------

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From eeneling at yahoo.com  Sun Sep 24 10:42:08 2006
From: eeneling at yahoo.com (Nel N.)
Date: Sun, 24 Sep 2006 01:42:08 -0700 (PDT)
Subject: [R] problem with summary of data
In-Reply-To: <x2r6y28o4k.fsf@turmalin.kubism.ku.dk>
References: <6462497.post@talk.nabble.com>
	<815b70590609231033q7b520bdcqb8530a125e28f6bd@mail.gmail.com>
	<x2r6y28o4k.fsf@turmalin.kubism.ku.dk>
Message-ID: <6470251.post@talk.nabble.com>


Looks like I've missed out the 'as.is' argument, I put it in and everything
was alright! Thank you so much!
In addition,there're a few .rda files that I've got that I would like to
view as file perse. How do I open it because when I tried opening it with
Notepad, it came out as unrecognised symbols.
Thanx again.


Peter Dalgaard wrote:
> 
> "David Barron" <mothsailor at googlemail.com> writes:
> 
>> You haven't shown the command you used to input the data, but it looks as
>> though you haven't told R not to convert the input into factors. 
> 
> Looks even more like R wasn't told that the first line of data was the
> variable names. 
> 
>> summary on
>> factors shows the frequencies of the levels for each variable, which is
>> what
>> your output shows.  See ?read.table for how to prevent conversion into
>> factors.
> 
> And possibly also the Data Import/Export manual.
> 
>> On 23/09/06, Nel N. <eeneling at yahoo.com> wrote:
>> >
>> >
>> > Greetings everybody,
>> > I'm new with R and I'm trying to do some statistical analysis with R
>> using
>> > my data.
>> > It is a DNA microarray data with 69 samples(row) and 7070 (column)
>> > variables. I read the file using the command
>> > 'file.table' and it can be done with no error.However, when I tried
>> > running
>> > a summary of the data, the result was not what I had expected. Below is
>> an
>> > extract of the result. And I knew that the command summary will give a
>> > summary of a data's descriptive statistics such as it's
>> median,mean,etc.So
>> > ,
>> > what i want is the statistics summary of each gene (in each column).
>> >      V7065          V7066        V7067        V7068            V7069
>> > 20     :29   20       :69   117    : 2   39     : 6   20         :69
>> > 25     : 4   X16699_at: 1   134    : 2   42     : 4   L49218_f_at: 1
>> > 30     : 4                  142    : 2   44     : 4
>> > 23     : 3                  170    : 2   46     : 4
>> > 26     : 3                  203    : 2   38     : 3
>> > 21     : 2                  96     : 2   58     : 3
>> > (Other):25                  (Other):58   (Other):46
>> >      V7070            V7071
>> > 30     : 5   20         :69
>> > 21     : 4   Z78285_f_at: 1
>> > 51     : 4
>> > 64     : 4
>> > 27     : 3
>> > 36     : 3
>> > (Other):47
>> > My data is in .dat format. Also, I have downloaded a few .rda files to
>> be
>> > tried out with R but how do I view them as data per se, like the .dat
>> file
>> > can be viewed and edited using Notepad, how about the .rda files?
>> > I express my greatest appreciation all who help.
>> > Thank you.
>> > --
>> > View this message in context:
>> >
>> http://www.nabble.com/problem-with-summary-of-data-tf2322882.html#a6462497
>> > Sent from the R help mailing list archive at Nabble.com.
>> >
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> > http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>> >
>> 
>> 
>> 
>> -- 
>> =================================
>> David Barron
>> Said Business School
>> University of Oxford
>> Park End Street
>> Oxford OX1 1HP
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 
> 
> -- 
>    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45)
> 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45)
> 35327907
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/problem-with-summary-of-data-tf2322882.html#a6470251
Sent from the R help mailing list archive at Nabble.com.


From spencer.graves at pdf.com  Sun Sep 24 12:13:31 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 24 Sep 2006 03:13:31 -0700
Subject: [R] Power analysis for repeated measures ANCOVA
In-Reply-To: <3B862D6B11ECDE458F679DDA238A135A7A2F53@lisa>
References: <3B862D6B11ECDE458F679DDA238A135A7A2F53@lisa>
Message-ID: <45165A4B.9020509@pdf.com>

      I haven't seen a reply to this post, and you may have moved beyond 
this question by now, but I will nevertheless offer one brief question:  
Have you looked at 'simulate.lme'?  This might simplify coding your 
power analysis. 

      Hope this helps. 
      Spencer Graves

Steve Su wrote:
> Dear All,
>
>  
>
> I wonder if anyone has written a code for power analysis with repeated
> measures ANCOVA? I am aware of the following reference:
>
>  
>
> Frison L and Pocock SJ: Repeated Measures in Clinical Trials: analysis
> using mean summary statistics and its implications for design in
> Statistics in Medicine, 1992, 11:1685-1704.
>
> Statistics in Medicine
>
>  
>
> It will probably be fairly easy to code this up or even work on this
> from scratch, but it would be good if people have already done this and
> can share their code.
>
>  
>
> Thanks.
>
>  
>
> Steve.
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From tura at centroin.com.br  Sun Sep 24 12:27:20 2006
From: tura at centroin.com.br (Bernardo Rangel tura)
Date: Sun, 24 Sep 2006 07:27:20 -0300
Subject: [R] plotCI
Message-ID: <7.0.0.16.2.20060924072045.043e80b0@centroin.com.br>

Hi R masters!


I need a Help with plot confidence intervals  for one equation. I use 
library gplots and plotCI command in this script:

require(gplots)

ano <-1980:2002

rf<-exp(91.37162-0.04720281*ano)

ciw.f<-sqrt(1.766073e-08)

plotCI(ano,rf,uiw=ciw.f)



But in the graph not shown the errors bar and  I have this error msg

zero-length arrow is of indeterminate angle and so skipped


Well, where is my eror?


Thanks in advance

Bernardo Rangel Tura, MD, PhD
National Institute of Cardiology Laranjeiras
Rio de Janeiro Brazil


From mothsailor at googlemail.com  Sun Sep 24 12:34:15 2006
From: mothsailor at googlemail.com (David Barron)
Date: Sun, 24 Sep 2006 11:34:15 +0100
Subject: [R] plotCI
In-Reply-To: <7.0.0.16.2.20060924072045.043e80b0@centroin.com.br>
References: <7.0.0.16.2.20060924072045.043e80b0@centroin.com.br>
Message-ID: <815b70590609240334g8fa27b7s35a8119233de7e32@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060924/cab1efc5/attachment.pl 

From bxc at steno.dk  Sun Sep 24 13:14:51 2006
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Sun, 24 Sep 2006 13:14:51 +0200
Subject: [R] Data frames questions
In-Reply-To: <BAY23-F83C4EAA1F242C47FD7B0FFA260@phx.gbl>
Message-ID: <40D3930AC1C8EA469E39536E5BC8083502B8C7C2@EXDKBA021.corp.novocorp.net>

> -----Original Message-----
> From: X.H Chen [mailto:xchen_stat at hotmail.com] 
> Sent: Sunday, September 24, 2006 12:16 AM
> To: jgreenberg at arc.nasa.gov; r-help at stat.math.ethz.ch
> Subject: Re: [R] Data frames questions
> 
> 
> >1) Is there a way to build an empty data frame, containing 
> nothing but 
> >the data frame variable names?
> 
> Yes, you can do it one way as followings:
> 
> df1<-as.data.frame(matrix(nrow=2,ncol=2),row.names=c("R1","R2"))

This results in a dataframe with two rows and two columns NA in it.
In order to build an empty dataframe one you could use:

df1 <- data.frame( X1=1, x2=3, l1=TRUE, d3=as.Date("2003-07-14")
)[NULL,]
str( df1 )

The key is the "NULL" for the rows, it removes the data but keeps the
content.

Best,
Bendix
----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc
----------------------
 
(snip)


From jim at bitwrit.com.au  Mon Sep 25 04:02:49 2006
From: jim at bitwrit.com.au (Jim Lemon)
Date: Sun, 24 Sep 2006 22:02:49 -0400
Subject: [R] plotCI
In-Reply-To: <7.0.0.16.2.20060924072045.043e80b0@centroin.com.br>
References: <7.0.0.16.2.20060924072045.043e80b0@centroin.com.br>
Message-ID: <451738C9.7000408@bitwrit.com.au>

Bernardo Rangel tura wrote:
> Hi R masters!
> 
> 
> I need a Help with plot confidence intervals  for one equation. I use 
> library gplots and plotCI command in this script:
> 
> require(gplots)
> 
> ano <-1980:2002
> 
> rf<-exp(91.37162-0.04720281*ano)
> 
> ciw.f<-sqrt(1.766073e-08)
> 
> plotCI(ano,rf,uiw=ciw.f)
> 
> 
> 
> But in the graph not shown the errors bar and  I have this error msg
> 
> zero-length arrow is of indeterminate angle and so skipped
> 
> 
> Well, where is my eror?
> 
Hi Bernardo,

As David Barron said, the error bars are very small. However, the plotCI 
function in the plotrix package will produce a graph for you.

Jim


From justin_bem at yahoo.fr  Sun Sep 24 14:10:09 2006
From: justin_bem at yahoo.fr (justin bem)
Date: Sun, 24 Sep 2006 12:10:09 +0000 (GMT)
Subject: [R] Complex survey analysis help
Message-ID: <20060924121009.11232.qmail@web23006.mail.ird.yahoo.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20060924/fd6c0fc0/attachment.pl 

From binabina at bellsouth.net  Sun Sep 24 15:48:47 2006
From: binabina at bellsouth.net (zubin)
Date: Sun, 24 Sep 2006 09:48:47 -0400
Subject: [R] R contractor Needed - Atlanta GA
In-Reply-To: <446D1C5F.9060002@bellsouth.net>
References: <446D1C5F.9060002@bellsouth.net>
Message-ID: <45168CBF.7040901@bellsouth.net>

Hello, need an R contractor to help on a project, project requires a 
strong econometrics background, especially package MSBVAR, LME, and the 
ASSIST packages.  We need to estimate some models and produce a user 
interface to enable simulation of these models.  Please email me if your 
interested and can work on this for 2-3 months.

-zubin


From ritwik.sinha at gmail.com  Sun Sep 24 15:51:01 2006
From: ritwik.sinha at gmail.com (Ritwik Sinha)
Date: Sun, 24 Sep 2006 09:51:01 -0400
Subject: [R] Statitics Textbook - any recommendation?
In-Reply-To: <1GQsA2-0n2Cie0@fwd28.sul.t-online.de>
References: <20060920172158.0i94pyn6so7qcksk@webmail.ufrgs.br>
	<1GQsA2-0n2Cie0@fwd28.sul.t-online.de>
Message-ID: <42bc98300609240651v678c3c80y3ffe5de4beb3bfaa@mail.gmail.com>

How about this book by Julian Faraway.

http://www.stat.lsa.umich.edu/~faraway/book/

It covers only regression and anova, but I really like the book. It
gives a good overview of the important topics in linear regression and
anova. Also it is on the web and hence "free".

Ritwik

On 9/22/06, Wolfgang Lindner <LindnerW at t-online.de> wrote:
> Iuri Gavronski schrieb:
> > Other text I am trying to find is multivariate data analysis (EFA,
> > cluster, mult regression, MANOVA, etc.) with examples with R.
>
> Hi Iuri,
>
> for your second answer I would recommend
> B. Everitt: An R and S-PLUS Companion to Multivariate Analysis. Springer 2005.
> isbn 1-85233-882-2.
>
> Best
>       Wolfgang
> --
> privat:      Wolfgang Lindner, Stieglitzweg 6, D-42799 Leichlingen
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Ritwik Sinha
Graduate Student
Epidemiology and Biostatistics
Case Western Reserve University

http://darwin.cwru.edu/~rsinha


From bates at stat.wisc.edu  Sun Sep 24 16:27:07 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sun, 24 Sep 2006 09:27:07 -0500
Subject: [R] Data frames questions
In-Reply-To: <BAY23-F83C4EAA1F242C47FD7B0FFA260@phx.gbl>
References: <C13AFB08.C70C%jgreenberg@arc.nasa.gov>
	<BAY23-F83C4EAA1F242C47FD7B0FFA260@phx.gbl>
Message-ID: <40e66e0b0609240727l1e3d0d87i5a25c28fe8604e7@mail.gmail.com>

On 9/23/06, X.H Chen <xchen_stat at hotmail.com> wrote:
> >1) Is there a way to build an empty data frame, containing nothing but the
> >data frame variable names?
>
> Yes, you can do it one way as followings:
>
> df1<-as.data.frame(matrix(nrow=2,ncol=2),row.names=c("R1","R2"))
>
> >2) Is there a way to reorder the variables in a data frame, e.g. When I go
> >to write out a data frame using write.table or write.matrix, I want the
> >output in a certain order...
>
> df2<-as.data.frame(matrix(1:4,nrow=2,ncol=2),row.names=c("R3","R4"))
> df2<-df2[c(2,1),]

I understood the question to be asking how to rearrange the variables
(i.e. columns of the data frame) instead of the rows (observations).
The same technique can be used but with the reordering vector on the
right hand side of the comma.  If you have named columns it may be
more accurate to use the names rather than the positions to get the
order you want.

> str(swiss)
`data.frame':	47 obs. of  6 variables:
 $ Fertility       : num  80.2 83.1 92.5 85.8 76.9 76.1 83.8 92.4 82.4 82.9 ...
 $ Agriculture     : num  17 45.1 39.7 36.5 43.5 35.3 70.2 67.8 53.3 45.2 ...
 $ Examination     : int  15 6 5 12 17 9 16 14 12 16 ...
 $ Education       : int  12 9 5 7 15 7 7 8 7 13 ...
 $ Catholic        : num   9.96 84.84 93.40 33.77  5.16 ...
 $ Infant.Mortality: num  22.2 22.2 20.2 20.3 20.6 26.6 23.6 24.9 21 24.4 ...
> head(swiss)
             Fertility Agriculture Examination Education Catholic
Infant.Mortality
Courtelary        80.2        17.0          15        12     9.96
       22.2
Delemont          83.1        45.1           6         9    84.84
       22.2
Franches-Mnt      92.5        39.7           5         5    93.40
       20.2
Moutier           85.8        36.5          12         7    33.77
       20.3
Neuveville        76.9        43.5          17        15     5.16
       20.6
Porrentruy        76.1        35.3           9         7    90.57
       26.6
> head(swiss[, c("Agriculture", "Education", "Examination", "Catholic")])
             Agriculture Education Examination Catholic
Courtelary          17.0        12          15     9.96
Delemont            45.1         9           6    84.84
Franches-Mnt        39.7         5           5    93.40
Moutier             36.5         7          12    33.77
Neuveville          43.5        15          17     5.16
Porrentruy          35.3         7           9    90.57

> >3) How to I "append" to the bottom of a dataframe?
>
> df2<-rbind(df2,c(5,6))
>
> Xiaohui Chen
>
> Dept. of Statistics
> UBC, Canada
>
>
>
>
> >From: Jonathan Greenberg <jgreenberg at arc.nasa.gov>
> >To: R-help <r-help at stat.math.ethz.ch>
> >Subject: [R] Data frames questions
> >Date: Sat, 23 Sep 2006 14:54:00 -0700
> >
> >Hi there, couple of questions on data frames:
> >
> >1) Is there a way to build an empty data frame, containing nothing but the
> >data frame variable names?
> >2) Is there a way to reorder the variables in a data frame, e.g. When I go
> >to write out a data frame using write.table or write.matrix, I want the
> >output in a certain order...
> >3) How to I "append" to the bottom of a dataframe?
> >
> >Thanks!
> >
> >--j
> >
> >--
> >Jonathan A. Greenberg, PhD
> >NRC Research Associate
> >NASA Ames Research Center
> >MS 242-4
> >Moffett Field, CA 94035-1000
> >Office: 650-604-5896
> >Cell: 415-794-5043
> >AIM: jgrn307
> >MSN: jgrn307 at hotmail.com
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ritwik.sinha at gmail.com  Sun Sep 24 16:50:25 2006
From: ritwik.sinha at gmail.com (Ritwik Sinha)
Date: Sun, 24 Sep 2006 10:50:25 -0400
Subject: [R] how to delete some columns from a matrix based on some
	other indicator variable
In-Reply-To: <815b70590609201528p56135b07we84e0c26c6689fb8@mail.gmail.com>
References: <BAY101-F959CFA9EC0950E4072D46BC230@phx.gbl>
	<815b70590609201528p56135b07we84e0c26c6689fb8@mail.gmail.com>
Message-ID: <42bc98300609240750y4c0196a8m19bdbb6bcfe0c89f@mail.gmail.com>

Hi,

The problem might be mode of the vector "r". Try this

D[,as.character(r)=="1"]

But I am not sure that is the problem. Sometimes "factors" tend to
complicate things. Look for "factor" in the R FAQ page.

Ritwik.

On 9/20/06, David Barron <mothsailor at googlemail.com> wrote:
> You don't need a loop.  You could try
>
> > r <- c(0,0,1,1)
> > matD <- matrix(1:12,nrow=3)
> > matD
>      [,1] [,2] [,3] [,4]
> [1,]    1    4    7   10
> [2,]    2    5    8   11
> [3,]    3    6    9   12
>
> > matD[,r==1]
> > matD[,r==1]
>      [,1] [,2]
> [1,]    7   10
> [2,]    8   11
> [3,]    9   12
>
>
> On 20/09/06, Ya-Hsiu Chuang <amichuang at hotmail.com> wrote:
> > Hello,
> >
> > I am not very familiar with R and need help in deleting a few columns in a
> > matrix.
> >
> > Suppose I have a indicator variable called r and it's defined as r = (0, 0,
> > 1, 1). A matrix D is a 3X4 matrix. If I want a new matrix which contains
> > only the columns of D corresponding to the elements of r that equal to 1.
> > how can i write a loop which creat a new matrix that contains only the last
> > 2 columns of D in this case? thanks
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
> --
> =================================
> David Barron
> Said Business School
> University of Oxford
> Park End Street
> Oxford OX1 1HP
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Ritwik Sinha
Graduate Student
Epidemiology and Biostatistics
Case Western Reserve University

http://darwin.cwru.edu/~rsinha


From muster at gmail.com  Sun Sep 24 19:41:47 2006
From: muster at gmail.com (T Mu)
Date: Sun, 24 Sep 2006 13:41:47 -0400
Subject: [R] R gui console problem
Message-ID: <b68812e70609241041i52b6b3c6ifa03a85f8c95c961@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060924/ca1ca621/attachment.pl 

From jgreenberg at arc.nasa.gov  Sun Sep 24 20:31:36 2006
From: jgreenberg at arc.nasa.gov (Jonathan Greenberg)
Date: Sun, 24 Sep 2006 11:31:36 -0700
Subject: [R] Print and supressing printing in function
Message-ID: <C13C1D18.C755%jgreenberg@arc.nasa.gov>

Another newbie question for you all:

In a function, say I have:

countme <- function() {
for(i in 1:10) {
i
}
}

How do I get R to print "i" as it runs (e.g. By calling "countme") -- right
now it seems to supress most output.  On a related note, my program uses
remove.vars, which always prints its output -- how to I *supress* that
output?

Thanks!

--j 

-- 
Jonathan A. Greenberg, PhD
NRC Research Associate
NASA Ames Research Center
MS 242-4
Moffett Field, CA 94035-1000
Office: 650-604-5896
Cell: 415-794-5043
AIM: jgrn307
MSN: jgrn307 at hotmail.com


From MSchwartz at mn.rr.com  Sun Sep 24 20:56:43 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Sun, 24 Sep 2006 13:56:43 -0500
Subject: [R] Print and supressing printing in function
In-Reply-To: <C13C1D18.C755%jgreenberg@arc.nasa.gov>
References: <C13C1D18.C755%jgreenberg@arc.nasa.gov>
Message-ID: <1159124203.22868.11.camel@localhost.localdomain>

On Sun, 2006-09-24 at 11:31 -0700, Jonathan Greenberg wrote:
> Another newbie question for you all:
> 
> In a function, say I have:
> 
> countme <- function() {
> for(i in 1:10) {
> i
> }
> }
> 
> How do I get R to print "i" as it runs (e.g. By calling "countme") -- right
> now it seems to supress most output.  On a related note, my program uses
> remove.vars, which always prints its output -- how to I *supress* that
> output?
> 
> Thanks!

You need to explicitly print() the value. Thus:

countme <- function() {
for(i in 1:10) {
    print(i)
  }
}

> countme()
[1] 1
[1] 2
[1] 3
[1] 4
[1] 5
[1] 6
[1] 7
[1] 8
[1] 9
[1] 10

HTH,

Marc Schwartz


From sundar.dorai-raj at pdf.com  Sun Sep 24 21:14:06 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Sun, 24 Sep 2006 14:14:06 -0500
Subject: [R] Print and supressing printing in function
In-Reply-To: <1159124203.22868.11.camel@localhost.localdomain>
References: <C13C1D18.C755%jgreenberg@arc.nasa.gov>
	<1159124203.22868.11.camel@localhost.localdomain>
Message-ID: <4516D8FE.7020500@pdf.com>



Marc Schwartz said the following on 9/24/2006 1:56 PM:
> On Sun, 2006-09-24 at 11:31 -0700, Jonathan Greenberg wrote:
>> Another newbie question for you all:
>>
>> In a function, say I have:
>>
>> countme <- function() {
>> for(i in 1:10) {
>> i
>> }
>> }
>>
>> How do I get R to print "i" as it runs (e.g. By calling "countme") -- right
>> now it seems to supress most output.  On a related note, my program uses
>> remove.vars, which always prints its output -- how to I *supress* that
>> output?
>>
>> Thanks!
> 
> You need to explicitly print() the value. Thus:
> 
> countme <- function() {
> for(i in 1:10) {
>     print(i)
>   }
> }
> 
>> countme()
> [1] 1
> [1] 2
> [1] 3
> [1] 4
> [1] 5
> [1] 6
> [1] 7
> [1] 8
> [1] 9
> [1] 10
> 
> HTH,
> 
> Marc Schwartz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

(Answering "remove.vars" question)

Please read ?remove.vars. (You neglected to mention this function is 
part of the gdata package.) There is an "info" argument you want to set 
to FALSE.

HTH,

--sundar


From MSchwartz at mn.rr.com  Sun Sep 24 21:18:47 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Sun, 24 Sep 2006 14:18:47 -0500
Subject: [R] Print and supressing printing in function
In-Reply-To: <4516D8FE.7020500@pdf.com>
References: <C13C1D18.C755%jgreenberg@arc.nasa.gov>
	<1159124203.22868.11.camel@localhost.localdomain>
	<4516D8FE.7020500@pdf.com>
Message-ID: <1159125527.22868.12.camel@localhost.localdomain>

On Sun, 2006-09-24 at 14:14 -0500, Sundar Dorai-Raj wrote:
> 
> Marc Schwartz said the following on 9/24/2006 1:56 PM:
> > On Sun, 2006-09-24 at 11:31 -0700, Jonathan Greenberg wrote:
> >> Another newbie question for you all:
> >>
> >> In a function, say I have:
> >>
> >> countme <- function() {
> >> for(i in 1:10) {
> >> i
> >> }
> >> }
> >>
> >> How do I get R to print "i" as it runs (e.g. By calling "countme") -- right
> >> now it seems to supress most output.  On a related note, my program uses
> >> remove.vars, which always prints its output -- how to I *supress* that
> >> output?
> >>
> >> Thanks!
> > 
> > You need to explicitly print() the value. Thus:
> > 
> > countme <- function() {
> > for(i in 1:10) {
> >     print(i)
> >   }
> > }
> > 
> >> countme()
> > [1] 1
> > [1] 2
> > [1] 3
> > [1] 4
> > [1] 5
> > [1] 6
> > [1] 7
> > [1] 8
> > [1] 9
> > [1] 10
> > 
> > HTH,
> > 
> > Marc Schwartz

> 
> (Answering "remove.vars" question)
> 
> Please read ?remove.vars. (You neglected to mention this function is 
> part of the gdata package.) There is an "info" argument you want to set 
> to FALSE.

Thanks for noticing my oversight Sundar.

Marc


From tlumley at u.washington.edu  Sun Sep 24 23:04:16 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Sun, 24 Sep 2006 14:04:16 -0700 (PDT)
Subject: [R] Complex survey analysis help
In-Reply-To: <20060924121009.11232.qmail@web23006.mail.ird.yahoo.com>
Message-ID: <Pine.LNX.4.43.0609241404160.32569@hymn07.u.washington.edu>

On Sun, 24 Sep 2006, justin bem wrote:

> Hi dear all,
>
> I have a complex survey data to analyse. I have a stratified survey with three
> stages level. The first stage is a PPS, but all other are SRSWOR. The solution 
>propose in survey package is to considere PPS as SRS with repetion.

No, the solution proposed is to consider it as PPS with replacement.

> I have try this
>
> E3stage<-svydesign(ids=~stage1id+stage2id+stage3id, fpc=~fpc1+fpc2+fpc3, ...)
>
> This is the value for WOR sampling at all stage. What value should I give to >fpc1 to have the same value of variance that in sampling with repetion ? I try >fpc1<-rep(0,length(data)) I get a error message.

For sampling with replacement you just omit the fpc and specify the first-stage id and the sampling weights

E3stage <-svydesign(ids=~stageid1, weights=~weights)

       -thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From murdoch at stats.uwo.ca  Sun Sep 24 23:27:11 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 24 Sep 2006 17:27:11 -0400
Subject: [R] R gui console problem
In-Reply-To: <b68812e70609241041i52b6b3c6ifa03a85f8c95c961@mail.gmail.com>
References: <b68812e70609241041i52b6b3c6ifa03a85f8c95c961@mail.gmail.com>
Message-ID: <4516F82F.5070806@stats.uwo.ca>

On 9/24/2006 1:41 PM, T Mu wrote:
> Hi all,
> 
> My R GUI got a weird perk. It loads only first page of scripts, about 28
> rows. I didn't change any configuration or installed anything special.
> 
> I use R 2.3.1, windows. Help please.

There's no way someone could help you based on the description you 
posted.  As every message in this mailing list says, "PLEASE do read the 
posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code."

Duncan Murdoch


From ritz at kvl.dk  Mon Sep 25 10:27:42 2006
From: ritz at kvl.dk (Christian Ritz)
Date: Mon, 25 Sep 2006 10:27:42 +0200
Subject: [R]  nlme with a factor in R 2.4.0beta
Message-ID: <451792FE.2050600@kvl.dk>

Hi,

the following R lines work fine in R 2.4.0 alpha (and older R versions), but not in R 
2.4.0 beta (details below):


library(drc)  # to load the dataset 'PestSci'

library(nlme)


## Starting values
sv <- c(0.328919, 1.956121, 0.097547, 1.642436, 0.208924)


## No error
m1 <- nlme(SLOPE ~ c + (d-c)/(1+exp(b*(log(DOSE)-log(e)))),
                               fixed = list(b+c+d+e~1),
                               random = d~1|CURVE,
                               start = sv[c(2,3,4,5)], data = PestSci)


## Error: attempt to select more than one element
m2 <- nlme(SLOPE ~ c + (d-c)/(1+exp(b*(log(DOSE)-log(e)))),
                               fixed = list(b~HERBICIDE, c+d+e~1),
                               random = d~1|CURVE,
                               start = sv, data = PestSci)



Output from sessionInfo() for R 2.4.0 alpha

R version 2.4.0 alpha (2006-09-16 r39365)
i386-pc-mingw32

locale:
LC_COLLATE=Danish_Denmark.1252;LC_CTYPE=Danish_Denmark.1252;LC_MONETARY=Danish_Denmark.1252;LC_NUMERIC=C;
LC_TIME=Danish_Denmark.1252

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
[7] "base"

other attached packages:
     nlme      drc
"3.1-75"  "1.0-1"



Output from sessionInfo() for R 2.4.0 beta

R version 2.4.0 beta (2006-09-24 r39497)
i386-pc-mingw32

locale:
LC_COLLATE=Danish_Denmark.1252;LC_CTYPE=Danish_Denmark.1252;LC_MONETARY=Danish_Denmark.1252;LC_NUMERIC=C;
LC_TIME=Danish_Denmark.1252

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
[7] "base"

other attached packages:
     nlme      drc
"3.1-76"  "1.0-1"





Christian


From buser at stat.math.ethz.ch  Mon Sep 25 10:32:21 2006
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Mon, 25 Sep 2006 10:32:21 +0200
Subject: [R] contrasts in aov
In-Reply-To: <A9595913-44FB-477B-9826-EA8C64111056@uleth.ca>
References: <A9595913-44FB-477B-9826-EA8C64111056@uleth.ca>
Message-ID: <17687.37909.425239.923632@stat.math.ethz.ch>

Dear John

?ordered 

will help you.

Regards,

Christoph Buser

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH Zurich	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------


John Vokey writes:
 > useRs,
 >    A no doubt simple question, but I am baffled.  Indeed, I think I  
 > once knew the answer, but can't recover it.  The default contrasts  
 > for aov (and lm, and...) are contr.treatment and contr.poly for  
 > unordered and ordered factors, respectively.  But, how does one  
 > invoke the latter?  That is, in a data.frame, how does one indicate  
 > that a factor is an *ordered* factor such that contr.poly is invoked  
 > in the aov or lm call?
 > --
 > Please avoid sending me Word or PowerPoint attachments.
 > See <http://www.gnu.org/philosophy/no-word-attachments.html>
 > 
 > -Dr. John R. Vokey
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
 > and provide commented, minimal, self-contained, reproducible code.


From g.abraham at ms.unimelb.edu.au  Mon Sep 25 10:32:58 2006
From: g.abraham at ms.unimelb.edu.au (Gad Abraham)
Date: Mon, 25 Sep 2006 18:32:58 +1000
Subject: [R] Initialising Mersenne-Twister with one integer
Message-ID: <4517943A.6050301@ms.unimelb.edu.au>

Hi,

It seems to me that the Mersenne-Twister PRNG can be initialised using 
one integer instead of 624 integers, since inside RNG.c code there's a 
function defined as MT_sgenrand(Int32).

How do I actually set this seed within R?

I've tried:

 > .Random.seed <- c(3, 1)
 > runif(1)
Error in runif(1) : .Random.seed has wrong length

In addition, is '3' actually the correct rng.kind for the Mersenne-Twister?

I'm using R version 2.2.1, 2005-12-20 on Ubuntu Dapper Linux 686.

Thanks,
Gad

-- 
Gad Abraham
Department of Mathematics and Statistics
University of Melbourne
Parkville 3010, Victoria, Australia
email: g.abraham at ms.unimelb.edu.au
web: http://www.ms.unimelb.edu.au/~gabraham


From ripley at stats.ox.ac.uk  Mon Sep 25 10:48:16 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 25 Sep 2006 09:48:16 +0100 (BST)
Subject: [R] Initialising Mersenne-Twister with one integer
In-Reply-To: <4517943A.6050301@ms.unimelb.edu.au>
References: <4517943A.6050301@ms.unimelb.edu.au>
Message-ID: <Pine.LNX.4.64.0609250943090.28507@gannet.stats.ox.ac.uk>

On Mon, 25 Sep 2006, Gad Abraham wrote:

> Hi,
>
> It seems to me that the Mersenne-Twister PRNG can be initialised using
> one integer instead of 624 integers, since inside RNG.c code there's a
> function defined as MT_sgenrand(Int32).
>
> How do I actually set this seed within R?

set.seed(), on the help page for ?.Random.seed.

> I've tried:
>
> > .Random.seed <- c(3, 1)
> > runif(1)
> Error in runif(1) : .Random.seed has wrong length

>From the help page

      '.Random.seed' is an integer vector, containing the random number
      generator (RNG) *state* for random number generation in R.  It can
      be saved and restored, but should not be altered by the user.

> In addition, is '3' actually the correct rng.kind for the Mersenne-Twister?
>
> I'm using R version 2.2.1, 2005-12-20 on Ubuntu Dapper Linux 686.

Not current, but I suspect the help page is the same in that version.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From rduarte at ipimar.pt  Mon Sep 25 11:05:58 2006
From: rduarte at ipimar.pt (Rafael Duarte)
Date: Mon, 25 Sep 2006 10:05:58 +0100
Subject: [R] Lattice strip labels for two factors
In-Reply-To: <eb555e660609231230g70f517b4w1289afbbb67359f0@mail.gmail.com>
References: <45143B48.3020601@ipimar.pt>	
	<971536df0609221300i4a0a3a21w2914588f6b787b7e@mail.gmail.com>	
	<451502CE.6040902@ipimar.pt>	
	<971536df0609230702o7d7c5fei588079e7c16d76ff@mail.gmail.com>
	<eb555e660609231230g70f517b4w1289afbbb67359f0@mail.gmail.com>
Message-ID: <45179BF6.5080503@ipimar.pt>

Dear Gabor and Deepayan,
Many thanks for your help.
I used suggestion 3 from Gabor (it worked well with my long df ) and 
will try Deepayan's suggestion.
Rafael


Deepayan Sarkar wrote:

> On 9/23/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
>
>> 1. You can write a custom strip function:
>>
>>         my.strip <- function(which.given, ..., factor.levels) {
>>            levs <- if (which.given == 1) factor.levels
>>                    else c("faro", "porto", "lisbon", "setubal")
>>            strip.default(which.given, ..., factor.levels = levs)
>>         }
>>
>>         xyplot(value ~ year | fact1 * fact2, data = df,
>>            strip = my.strip)
>>
>> 2. however, its probably easier just to change the levels in
>> the data frame.  Just do it in a copy if you don't want to
>> change the original one:
>>
>>         df2 <- df
>>         levels(df2$fact2) <- c("faro", "porto", "lisbon", "setubal")
>>         xyplot(value ~ year | fact1 * fact2, data = df2)
>>
>> 3. or you can even do it inline in the data statement which
>> similarly won't change the original data frame:
>>
>>         levs <- c("faro", "porto", "lisbon", "setubal")
>>         xyplot(value ~ year | fact1 * fact2,
>>            data = replace(df, "fact2", structure(df$fact2, levels = 
>> levs)))
>>         head(df) # unchanged
>
>
> or even (untested)
>
> xyplot(value ~ year | fact1 * factor(fact2, levels = levels(fact2),
> labels = levs),
>       data = df)
>
> Deepayan
>
>> On 9/23/06, Rafael Duarte <rduarte at ipimar.pt> wrote:
>> > Thank you for your suggestion.
>> > This could be a solution that I didn't think of.
>> >
>> > But I forgot to say that I didn't want to change the original data 
>> frame
>> > (I have other code that depends on the original df and on the original
>> > factor levels).
>> > I was looking more for an implementation directly in the xyplot call
>> > (same as I did for one factor). Is it possible/simple to do?
>> >
>> > Thank you,
>> > Rafael
>> >
>> >
>> > Gabor Grothendieck wrote:
>> >
>> > > Try this:
>> > >
>> > > levels(df$fact2) <- c("faro","porto","lisbon","setubal")
>> > > xyplot( value ~ year | fact1*fact2, data=df, type="b")
>> > >
>> > >
>> > > On 9/22/06, Rafael Duarte <rduarte at ipimar.pt> wrote:
>> > >
>> > >> Dear list,
>> > >> My problem is to change the strip text of lattice panels when 
>> using two
>> > >> factors.
>> > >> I have a data frame with two factors:
>> > >>
>> > >> df <- expand.grid( "fact1"=c("y","b","r"),
>> > >> "fact2"=c("far","por","lis","set"), "year"=1991:2000, "value"= NA)
>> > >> df[,"value"] <- sample(1:50, 120, replace=TRUE)
>> > >>
>> > >> I can make simple xyplot and change the text of the factor 
>> levels with
>> > >> strip.custom:
>> > >>
>> > >> require("lattice")
>> > >> xyplot( value ~ year | fact1, data=df, type="b", subset= 
>> fact2=="far",
>> > >> strip = strip.custom(bg=gray.colors(1,0.95), 
>> factor.levels=c("yellow",
>> > >> "black", "red")),
>> > >> layout=c(1,3)
>> > >> )
>> > >>
>> > >> But how can I change the text of the factor levels when using both
>> > >> factors as in this plot:
>> > >> xyplot( value ~ year | fact1*fact2, data=df, type="b")
>> > >>
>> > >> (fact2 levels text should change to:
>> > >> c("faro","porto","lisbon","setubal"))
>> > >>
>> > >> I read the help for strip.default and the emails archive, tried 
>> with
>> > >> "which.given" but could not find out how to accomplish this.
>> > >>
>> > >> Many thanks,
>> > >> Rafael Duarte
>> > >>
>> > >> --
>> > >> Rafael Duarte
>> > >> Marine Resources Department - DRM
>> > >> IPIMAR -  National Research Institute for Agriculture and Fisheries
>> > >> Av. Bras?lia, 1449-006 Lisbon  -  Portugal
>> > >> Tel:+351 21 302 7000      Fax:+351 21 301 5948
>> > >> e-mail: rduarte at ipimar.pt
>> > >>
>> > >> ______________________________________________
>> > >> R-help at stat.math.ethz.ch mailing list
>> > >> https://stat.ethz.ch/mailman/listinfo/r-help
>> > >> PLEASE do read the posting guide
>> > >> http://www.R-project.org/posting-guide.html
>> > >> and provide commented, minimal, self-contained, reproducible code.
>> > >>
>> >
>> >
>> > --
>> > Rafael Duarte
>> > Marine Resources Department - DRM
>> > IPIMAR -  National Research Institute for Agriculture and Fisheries
>> > Av. Bras?lia, 1449-006 Lisbon  -  Portugal
>> > Tel:+351 21 302 7000      Fax:+351 21 301 5948
>> > e-mail: rduarte at ipimar.pt
>> >
>> >
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>


From p.dalgaard at biostat.ku.dk  Mon Sep 25 11:28:21 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 25 Sep 2006 11:28:21 +0200
Subject: [R] nlme with a factor in R 2.4.0beta
In-Reply-To: <451792FE.2050600@kvl.dk>
References: <451792FE.2050600@kvl.dk>
Message-ID: <x2bqp4i9ca.fsf@viggo.kubism.ku.dk>

Christian Ritz <ritz at kvl.dk> writes:

> Hi,
> 
> the following R lines work fine in R 2.4.0 alpha (and older R versions), but not in R 
> 2.4.0 beta (details below):
> 
> 
> library(drc)  # to load the dataset 'PestSci'
> 
> library(nlme)
> 
> 
> ## Starting values
> sv <- c(0.328919, 1.956121, 0.097547, 1.642436, 0.208924)
> 
> 
> ## No error
> m1 <- nlme(SLOPE ~ c + (d-c)/(1+exp(b*(log(DOSE)-log(e)))),
>                                fixed = list(b+c+d+e~1),
>                                random = d~1|CURVE,
>                                start = sv[c(2,3,4,5)], data = PestSci)
> 
> 
> ## Error: attempt to select more than one element
> m2 <- nlme(SLOPE ~ c + (d-c)/(1+exp(b*(log(DOSE)-log(e)))),
>                                fixed = list(b~HERBICIDE, c+d+e~1),
>                                random = d~1|CURVE,
>                                start = sv, data = PestSci)
...
> other attached packages:
>      nlme      drc
> "3.1-75"  "1.0-1"
....
>      nlme      drc
> "3.1-76"  "1.0-1"

I presume this is the real issue: The upgrade of nlme, rather than the
change of R itself from alpha to beta status.

The culprit would seem to be

   pars[, nm] <- f %*% beta[[fmap[[nm]]]]

inside nlme:::getParsNlme(). fmap[[nm]] is not necessarily a scalar,
so the outer set of [[]] should likely be []. The maintainer of nlme
will know for sure.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From ripley at stats.ox.ac.uk  Mon Sep 25 12:32:42 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 25 Sep 2006 11:32:42 +0100 (BST)
Subject: [R] behavior of [<-.foo
In-Reply-To: <E58BE6136618CF4C964F6EC7773AE569B4FE87@ex4.nyc.hcmny.com>
References: <E58BE6136618CF4C964F6EC7773AE569B4FE87@ex4.nyc.hcmny.com>
Message-ID: <Pine.LNX.4.64.0609250948270.28507@gannet.stats.ox.ac.uk>

I've not seen an actual answer to this, which is that this is a 
misunderstanding as to how NextMethod works.

First,

> +     x <- unclass(x)

looks wrong.  NextMethod uses the next method at the call to the generic, 
and subsequent changes to the object 'x' do not alter the class that would 
be dispatched on.  Given that the next method might not be the default 
method, unclassing here seems potentially damaging.

Second, the matched call is

Called from: `[<-.foo`(`*tmp*`, , value = 100)

and it is that call which is going to be passed on to the next method. 
As the help page says:

      'NextMethod' works by creating a special call frame for the next
      method.  If no new arguments are supplied, the arguments will be
      the same in number, order and name as those to the current method
      but their values will be promises to evaluate their name in the
      current method and environment.

Since 'j' was not an argument of the original call, it is ignored.

Now, [<- is an internal generic without a visible default method, but it 
is as if you have called `[<-.default`(`*tmp*`, 1:5, value = 100), which 
explains the result you got.

(That NextMethod invokes the generic as a possible default method is not 
documented anywhere that I can see, and the description in 2.3.1 is all 
about methods invoked via UseMethod.  There are issues with the above 
description when the method invoked is a primitive such as [<-, as that 
uses positional matching.  I would not be confident that this works as 
intended for multi-argument primitives.)

x[,] <- 100 is perhaps what you intended for a matrix-like class, and that 
does not work (it seems because the argument matching does indeed not work 
as intended).  You need something like

`[<-.foo` <- function(x, i, j, value) {
      if(missing(i)) i <- 1:nrow(x)
      if(missing(j)) j <- 1:ncol(x)
      cl <- class(x)
      cll <- length(cl)
      m <- match("foo", cl, cll)
      oldClass(x) <- if(m == cll) NULL else cl[(m+1):cll]
      x[i,j] <- value
      class(x) <- cl
      x
}


On Fri, 22 Sep 2006, Armstrong, Whit wrote:

> Can someone help me understand the following behavior of "[<-" ?
>
> If I define a simple class based on a matrix, the [<- operation only
> inserts into the first column:
>
>
>> x <- matrix(rnorm(10),nrow=5,ncol=2)
>>  class(x) <- "foo"
>> "[<-.foo" <- function(x, i, j, value) {
> +     if(missing(i)) i <- 1:nrow(x)
> +     if(missing(j)) j <- 1:ncol(x)
> +
> +     x <- unclass(x)
> +     x <- NextMethod(.Generic)
> +     class(x) <- "foo"
> +     x
> + }
>>
>> x[] <- 100.0
>> x
>     [,1]       [,2]
> [1,]  100 -0.1465296
> [2,]  100 -0.2615796
> [3,]  100 -0.8882629
> [4,]  100 -0.2886357
> [5,]  100 -0.9565273
> attr(,"class")
> [1] "foo"
>
> Based on the behavior of [<- for a matrix, I would have thought that the
> data for the whole object would be replaced.
>
> for instance:
>
>> y <- matrix(rnorm(10),nrow=5,ncol=2)
>> y
>            [,1]       [,2]
> [1,] -0.55297049 -1.1896488
> [2,]  0.06157438 -0.6628254
> [3,] -0.28184208 -2.5260177
> [4,]  0.61204398 -0.3492488
> [5,]  0.43971216  1.8990789
>> y[] <- 100
>> y
>     [,1] [,2]
> [1,]  100  100
> [2,]  100  100
> [3,]  100  100
> [4,]  100  100
> [5,]  100  100

[...]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From wolkerstorfer at cure.at  Mon Sep 25 13:51:11 2006
From: wolkerstorfer at cure.at (Peter Wolkerstorfer - CURE)
Date: Mon, 25 Sep 2006 13:51:11 +0200
Subject: [R] Beginner question: select cases
Message-ID: <004E39EEDEC0AA47B58872F1E7D725E72B90A7@email.cure-space.cure-vienna.org>

Hello all,

I hope i chose the right list as my question is a beginner-question.

I have a data set with 3 colums  "London", "Rome" and "Vienna" - the
location is presented through a 1 like this:
London 	Rome 	Vienna	q1
0		0	1		4
0		1	0		2	
1		0	0		3
....
....
....

I just want to calculate the means of a variable q1.

I tried following script:

# calculate the mean of all locations
results <- subset(results, subset== 1 )
mean(results$q1)
# calculate the mean of London
results <- subset(results, subset== 1 , select=c(London))
mean(results$q1)
# calculate the mean of Rome
results <- subset(results, subset== 1 , select=c(Rome))
mean(results$q1)
# calcualate the mean of Vienna
results <- subset(results, subset== 1 , select=c(Vienna))
mean(results$q1)

As all results are 1.68 and there is defenitely a difference in the
three locations I wonder whats going on.
I get confused as the Rcmdr asks me to overwrite things and there is no
"just filter" option.

Any help would be apprechiated. Thank you in advance.

Regards
Peter



___CURE - Center for Usability Research & Engineering___
 
Peter Wolkerstorfer
Usability Engineer
Hauffgasse 3-5, 1110 Wien, Austria
 
[Tel]  +43.1.743 54 51.46
[Fax]  +43.1.743 54 51.30
 
[Mail] wolkerstorfer at cure.at
[Web]  http://www.cure.at


From Thierry.ONKELINX at inbo.be  Mon Sep 25 14:11:44 2006
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Mon, 25 Sep 2006 14:11:44 +0200
Subject: [R] Beginner question: select cases
Message-ID: <2E9C414912813E4EB981326983E0A1040217E0E7@inexch.instnat.be.grp>

Your problem would be a lot easier if you coded the location in one
variable instead of three variables. Then you could calculate the means
with one line of code:

by(results$q1, results$location, mean)

With your dataset you could use
by(results$London, results$location, mean)
by(results$Rome, results$location, mean)
by(results$Vienna, results$location, mean)

see ?by for more information

And take a good look at your code. You take a subset from results and
the assign it to results. This means that you replace the original
results dataframe with a subset of it. As you take the subset for the
next city, you won't take a subset from the original dataset but for the
previous subset!

Cheers,

Thierry
------------------------------------------------------------------------
----

ir. Thierry Onkelinx

Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
and Forest

Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance

Gaverstraat 4

9500 Geraardsbergen

Belgium

tel. + 32 54/436 185

Thierry.Onkelinx op inbo.be

www.inbo.be 


-----Oorspronkelijk bericht-----
Van: r-help-bounces op stat.math.ethz.ch
[mailto:r-help-bounces op stat.math.ethz.ch] Namens Peter Wolkerstorfer -
CURE
Verzonden: maandag 25 september 2006 13:51
Aan: r-help op stat.math.ethz.ch
Onderwerp: [R] Beginner question: select cases

Hello all,

I hope i chose the right list as my question is a beginner-question.

I have a data set with 3 colums  "London", "Rome" and "Vienna" - the
location is presented through a 1 like this:
London 	Rome 	Vienna	q1
0		0	1		4
0		1	0		2	
1		0	0		3
....
....
....

I just want to calculate the means of a variable q1.

I tried following script:

# calculate the mean of all locations
results <- subset(results, subset== 1 )
mean(results$q1)
# calculate the mean of London
results <- subset(results, subset== 1 , select=c(London))
mean(results$q1)
# calculate the mean of Rome
results <- subset(results, subset== 1 , select=c(Rome))
mean(results$q1)
# calcualate the mean of Vienna
results <- subset(results, subset== 1 , select=c(Vienna))
mean(results$q1)

As all results are 1.68 and there is defenitely a difference in the
three locations I wonder whats going on.
I get confused as the Rcmdr asks me to overwrite things and there is no
"just filter" option.

Any help would be apprechiated. Thank you in advance.

Regards
Peter



___CURE - Center for Usability Research & Engineering___
 
Peter Wolkerstorfer
Usability Engineer
Hauffgasse 3-5, 1110 Wien, Austria
 
[Tel]  +43.1.743 54 51.46
[Fax]  +43.1.743 54 51.30
 
[Mail] wolkerstorfer op cure.at
[Web]  http://www.cure.at

______________________________________________
R-help op stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From HDoran at air.org  Mon Sep 25 14:11:30 2006
From: HDoran at air.org (Doran, Harold)
Date: Mon, 25 Sep 2006 08:11:30 -0400
Subject: [R] Beginner question: select cases
Message-ID: <2323A6D37908A847A7C32F1E3662C80E3B84D7@dc1ex01.air.org>

Peter,

There is a much easier way to do this. First, you should consider
organizing your data as follows:

set.seed(1) # for replication only

# Here is a sample dataframe
tmp <- data.frame(city = gl(3,10, label = c("London", "Rome","Vienna"
)), q1 = rnorm(30))

# Compute the means
with(tmp, tapply(q1,city, mean))
  London       Rome     Vienna 
 0.1322028  0.2488450 -0.1336732 

I hope this helps

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Peter 
> Wolkerstorfer - CURE
> Sent: Monday, September 25, 2006 7:51 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Beginner question: select cases
> 
> Hello all,
> 
> I hope i chose the right list as my question is a beginner-question.
> 
> I have a data set with 3 colums  "London", "Rome" and 
> "Vienna" - the location is presented through a 1 like this:
> London 	Rome 	Vienna	q1
> 0		0	1		4
> 0		1	0		2	
> 1		0	0		3
> ....
> ....
> ....
> 
> I just want to calculate the means of a variable q1.
> 
> I tried following script:
> 
> # calculate the mean of all locations
> results <- subset(results, subset== 1 )
> mean(results$q1)
> # calculate the mean of London
> results <- subset(results, subset== 1 , select=c(London))
> mean(results$q1)
> # calculate the mean of Rome
> results <- subset(results, subset== 1 , select=c(Rome))
> mean(results$q1)
> # calcualate the mean of Vienna
> results <- subset(results, subset== 1 , select=c(Vienna))
> mean(results$q1)
> 
> As all results are 1.68 and there is defenitely a difference 
> in the three locations I wonder whats going on.
> I get confused as the Rcmdr asks me to overwrite things and 
> there is no "just filter" option.
> 
> Any help would be apprechiated. Thank you in advance.
> 
> Regards
> Peter
> 
> 
> 
> ___CURE - Center for Usability Research & Engineering___
>  
> Peter Wolkerstorfer
> Usability Engineer
> Hauffgasse 3-5, 1110 Wien, Austria
>  
> [Tel]  +43.1.743 54 51.46
> [Fax]  +43.1.743 54 51.30
>  
> [Mail] wolkerstorfer at cure.at
> [Web]  http://www.cure.at
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From thineomuto at yahoo.com  Mon Sep 25 14:32:25 2006
From: thineomuto at yahoo.com (CHRISTIAN OMUTO)
Date: Mon, 25 Sep 2006 05:32:25 -0700 (PDT)
Subject: [R] Selfstarting models for soil hydrology using R
Message-ID: <20060925123225.89159.qmail@web38701.mail.mud.yahoo.com>

Dear,
I have developed and tested some models in soil
hydrology with NLME library in R. I want to ask
if it could be possible to submit this to the NLME
library (with sample data) as a toolbox or something
so
that anyone downloading new components of new versions
of R may simply call (say
SSbrookscorey function to predict water retention in
the same way someone can call SSlogis to predict
logistic function in the current version)? I would be
grateful for your support. I can also give in-depth
description and capabilities for white papers
concerning the applications of R in soil hydrology.
Please advice me.
Dr. Christian Thine,
University of Nairobi, Kenya.


From wolkerstorfer at cure.at  Mon Sep 25 14:30:44 2006
From: wolkerstorfer at cure.at (Peter Wolkerstorfer - CURE)
Date: Mon, 25 Sep 2006 14:30:44 +0200
Subject: [R] Beginner Loop Question with dynamic variable names
Message-ID: <004E39EEDEC0AA47B58872F1E7D725E72B90A9@email.cure-space.cure-vienna.org>

Dear all,

I have another small scripting-beginner problem which you hopefully can
help:

I compute new variables with:

# Question 1
results$q1 <- with(results, q1_1*1+ q1_2*2+ q1_3*3+ q1_4*4+ q1_5*5)
# Question 2
results$q2 <- with(results, q2_1*1+ q2_2*2+ q2_3*3+ q2_4*4+ q2_5*5)
# Question 3
results$q3 <- with(results, q3_1*1+ q3_2*2+ q3_3*3+ q3_4*4+ q3_5*5)
# Question 4
results$q4 <- with(results, q4_1*1+ q4_2*2+ q4_3*3+ q4_4*4+ q4_5*5)

This is very inefficient so I would like to do this in a loop like:

for (i in 1:20) {results$q1 <- with(results, q1_1*1+ q1_2*2+ q1_3*3+
q1_4*4+ q1_5*5)}

My question now:
How to replace the "1"-s (results$q1, q1_1...) in the variables with the
looping variable?

Here like I like it (just for illustration - of course I still miss the
function to tell R that it should append the value of i to the variable
name):

# i is the number of questions - just an illustration, I know it does
not work this way
for (i in 1:20) {results$qi <- with(results, qi_1*1+ qi_2*2+ qi_3*3+
qi_4*4+ qi_5*5)}

Help would be greatly appreciated. Thanks in advance.

Peter


___CURE - Center for Usability Research & Engineering___
 
Peter Wolkerstorfer
Usability Engineer
Hauffgasse 3-5, 1110 Wien, Austria
 
[Tel]  +43.1.743 54 51.46
[Fax]  +43.1.743 54 51.30
 
[Mail] wolkerstorfer at cure.at
[Web]  http://www.cure.at


From e.rapsomaniki at mail.cryst.bbk.ac.uk  Mon Sep 25 15:13:07 2006
From: e.rapsomaniki at mail.cryst.bbk.ac.uk (Eleni Rapsomaniki)
Date: Mon, 25 Sep 2006 14:13:07 +0100
Subject: [R] Multiple imputation using mice with "mean"
Message-ID: <1159189987.4517d5e34f6ab@webmail.cryst.bbk.ac.uk>


Hi

I am trying to impute missing values for my data.frame. As I intend to use the
complete data for prediction I am currently measuring the success of an
imputation method by its resulting classification error in my training data.

I have tried several approaches to replace missing values:
- mean/median substitution
- substitution by a value selected from the observed values of a variable
- MLE in the mix package
- all available methods for numerical data in the MICE package (ie. pmm, sample,
mean and norm)

I found that the least classification error results using mice with the "mean"
option for numerical data. However, I am not sure how the "mean" multiple
imputatation differs from the simple mean substitution. I tried to read some of
the documentation supporting the R package, but couldn't find much theory about
the "mean" imputation method. 

Are there any good papers to explain the background behind each imputation
option in MICE? 

I would really appreciate any comments on the above, as my understanding of
statistics is very limited. 

Many thanks
Eleni Rapsomaniki
Birkbeck College, UK


From jrkrideau at yahoo.ca  Mon Sep 25 15:20:25 2006
From: jrkrideau at yahoo.ca (John Kane)
Date: Mon, 25 Sep 2006 09:20:25 -0400 (EDT)
Subject: [R] Beginner question: select cases
In-Reply-To: <004E39EEDEC0AA47B58872F1E7D725E72B90A7@email.cure-space.cure-vienna.org>
Message-ID: <20060925132025.69363.qmail@web32807.mail.mud.yahoo.com>


--- Peter Wolkerstorfer - CURE <wolkerstorfer at cure.at>
wrote:

> Hello all,
> 
> I hope i chose the right list as my question is a
> beginner-question.
> 
> I have a data set with 3 colums  "London", "Rome"
> and "Vienna" - the
> location is presented through a 1 like this:
> London 	Rome 	Vienna	q1
> 0		0	1		4
> 0		1	0		2	
> 1		0	0		3
> ....
> ....
> ....
> 
> I just want to calculate the means of a variable q1.
> 
> I tried following script:
> 
> # calculate the mean of all locations
> results <- subset(results, subset== 1 )
> mean(results$q1)
> # calculate the mean of London
> results <- subset(results, subset== 1 ,
> select=c(London))
> mean(results$q1)
> # calculate the mean of Rome
> results <- subset(results, subset== 1 ,
> select=c(Rome))
> mean(results$q1)
> # calcualate the mean of Vienna
> results <- subset(results, subset== 1 ,
> select=c(Vienna))
> mean(results$q1)
> 
> As all results are 1.68 and there is defenitely a
> difference in the
> three locations I wonder whats going on.
> I get confused as the Rcmdr asks me to overwrite
> things and there is no
> "just filter" option.
> 
> Any help would be apprechiated. Thank you in
> advance.
> 
> Regards
> Peter


I'm new at R also.  However I don't recognize your
syntax. I have not seen select used here. 

Try 
results <- subset(results, London==1 )


From f.harrell at vanderbilt.edu  Mon Sep 25 15:31:37 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Mon, 25 Sep 2006 08:31:37 -0500
Subject: [R] Best use of LaTeX listings package for pretty printing R code
Message-ID: <4517DA39.4070102@vanderbilt.edu>

This is what I have been using.  Does anyone have a better way?  In 
particular I would like to see letters in comment strings not stretched 
so much.  Thanks -Frank

\documentclass{article}
\usepackage{listings,relsize}
\lstloadlanguages{R}
\newcommand{\lil}[1]{\lstinline|#1|}

\begin{document}
\lstset{language=R,basicstyle=\smaller,commentstyle=\rmfamily\smaller,
  showstringspaces=false,%
  xleftmargin=4ex,literate={<-}{{$\leftarrow$}}1 {~}{{$\sim$}}1}
\lstset{escapeinside={(*}{*)}}   % for (*\ref{ }*) inside lstlistings (S 
code)
\begin{lstlisting}
a <- b   # this is a test line
if(i==3) {  # another line, for y^2
  y <- 3^3
  z <- 'this string'
  qqcat <- y ~ pol(x,2)
} else y <- 4
\end{lstlisting}
That was \lstinline|x <- 22| \lil{q <- 'cat'}.
\end{document}


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From mr.blacksheep at gmail.com  Mon Sep 25 15:28:31 2006
From: mr.blacksheep at gmail.com (Mike Nielsen)
Date: Mon, 25 Sep 2006 08:28:31 -0500
Subject: [R] Beginner Loop Question with dynamic variable names
In-Reply-To: <004E39EEDEC0AA47B58872F1E7D725E72B90A9@email.cure-space.cure-vienna.org>
References: <004E39EEDEC0AA47B58872F1E7D725E72B90A9@email.cure-space.cure-vienna.org>
Message-ID: <46a360560609250628l7483ce42m67d8ba8f1c0cdf21@mail.gmail.com>

Is this what you had in mind?

> j<-data.frame(q1=rnorm(10),q2=rnorm(10))
> j
           q1         q2
1  -0.9189618 -0.2832102
2   0.9394316  1.1345975
3  -0.6388848  0.6850255
4   0.4938245 -0.5825715
5  -1.2885257 -0.2654023
6  -0.5278295  0.2382791
7   0.6517268  0.8923375
8   0.4124178  1.1231630
9  -0.1604982  0.2285672
10 -0.2369713  0.6130197
> for(i in 1:3){j[,paste(sep="","res",i)]<-with(j,q1+q2)}
> j
                q1              q2            res1            res2
        res3
1  -0.9189618 -0.2832102 -1.20217207 -1.20217207 -1.20217207
2   0.9394316  1.1345975  2.07402913  2.07402913  2.07402913
3  -0.6388848  0.6850255  0.04614073  0.04614073  0.04614073
4   0.4938245 -0.5825715 -0.08874699 -0.08874699 -0.08874699
5  -1.2885257 -0.2654023 -1.55392802 -1.55392802 -1.55392802
6  -0.5278295  0.2382791 -0.28955044 -0.28955044 -0.28955044
7   0.6517268  0.8923375  1.54406433  1.54406433  1.54406433
8   0.4124178  1.1231630  1.53558084  1.53558084  1.53558084
9  -0.1604982  0.2285672  0.06806901  0.06806901  0.06806901
10 -0.2369713  0.6130197  0.37604847  0.37604847  0.37604847

Regards,

Mike
On 9/25/06, Peter Wolkerstorfer - CURE <wolkerstorfer at cure.at> wrote:
> Dear all,
>
> I have another small scripting-beginner problem which you hopefully can
> help:
>
> I compute new variables with:
>
> # Question 1
> results$q1 <- with(results, q1_1*1+ q1_2*2+ q1_3*3+ q1_4*4+ q1_5*5)
> # Question 2
> results$q2 <- with(results, q2_1*1+ q2_2*2+ q2_3*3+ q2_4*4+ q2_5*5)
> # Question 3
> results$q3 <- with(results, q3_1*1+ q3_2*2+ q3_3*3+ q3_4*4+ q3_5*5)
> # Question 4
> results$q4 <- with(results, q4_1*1+ q4_2*2+ q4_3*3+ q4_4*4+ q4_5*5)
>
> This is very inefficient so I would like to do this in a loop like:
>
> for (i in 1:20) {results$q1 <- with(results, q1_1*1+ q1_2*2+ q1_3*3+
> q1_4*4+ q1_5*5)}
>
> My question now:
> How to replace the "1"-s (results$q1, q1_1...) in the variables with the
> looping variable?
>
> Here like I like it (just for illustration - of course I still miss the
> function to tell R that it should append the value of i to the variable
> name):
>
> # i is the number of questions - just an illustration, I know it does
> not work this way
> for (i in 1:20) {results$qi <- with(results, qi_1*1+ qi_2*2+ qi_3*3+
> qi_4*4+ qi_5*5)}
>
> Help would be greatly appreciated. Thanks in advance.
>
> Peter
>
>
> ___CURE - Center for Usability Research & Engineering___
>
> Peter Wolkerstorfer
> Usability Engineer
> Hauffgasse 3-5, 1110 Wien, Austria
>
> [Tel]  +43.1.743 54 51.46
> [Fax]  +43.1.743 54 51.30
>
> [Mail] wolkerstorfer at cure.at
> [Web]  http://www.cure.at
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Regards,

Mike Nielsen


From mothsailor at googlemail.com  Mon Sep 25 15:33:40 2006
From: mothsailor at googlemail.com (David Barron)
Date: Mon, 25 Sep 2006 14:33:40 +0100
Subject: [R] Beginner Loop Question with dynamic variable names
In-Reply-To: <004E39EEDEC0AA47B58872F1E7D725E72B90A9@email.cure-space.cure-vienna.org>
References: <004E39EEDEC0AA47B58872F1E7D725E72B90A9@email.cure-space.cure-vienna.org>
Message-ID: <815b70590609250633n5acc5085x4b26ce8b6d6dc28f@mail.gmail.com>

I think this does what you are looking for:

dta <- data.frame(q1_1=rep(1,5),q1_2=rep(2,5),q2_1=rep(3,5),q2_2=rep(4,5))

for (i in 1:2) {
    e1  <- paste("q",i,"_1 + q",i,"_2 * 2",sep="")
    assign(paste("q",i,sep=""),with(dta,eval(parse(text=e1))))
}


On 25/09/06, Peter Wolkerstorfer - CURE <wolkerstorfer at cure.at> wrote:
> Dear all,
>
> I have another small scripting-beginner problem which you hopefully can
> help:
>
> I compute new variables with:
>
> # Question 1
> results$q1 <- with(results, q1_1*1+ q1_2*2+ q1_3*3+ q1_4*4+ q1_5*5)
> # Question 2
> results$q2 <- with(results, q2_1*1+ q2_2*2+ q2_3*3+ q2_4*4+ q2_5*5)
> # Question 3
> results$q3 <- with(results, q3_1*1+ q3_2*2+ q3_3*3+ q3_4*4+ q3_5*5)
> # Question 4
> results$q4 <- with(results, q4_1*1+ q4_2*2+ q4_3*3+ q4_4*4+ q4_5*5)
>
> This is very inefficient so I would like to do this in a loop like:
>
> for (i in 1:20) {results$q1 <- with(results, q1_1*1+ q1_2*2+ q1_3*3+
> q1_4*4+ q1_5*5)}
>
> My question now:
> How to replace the "1"-s (results$q1, q1_1...) in the variables with the
> looping variable?
>
> Here like I like it (just for illustration - of course I still miss the
> function to tell R that it should append the value of i to the variable
> name):
>
> # i is the number of questions - just an illustration, I know it does
> not work this way
> for (i in 1:20) {results$qi <- with(results, qi_1*1+ qi_2*2+ qi_3*3+
> qi_4*4+ qi_5*5)}
>
> Help would be greatly appreciated. Thanks in advance.
>
> Peter
>
>
> ___CURE - Center for Usability Research & Engineering___
>
> Peter Wolkerstorfer
> Usability Engineer
> Hauffgasse 3-5, 1110 Wien, Austria
>
> [Tel]  +43.1.743 54 51.46
> [Fax]  +43.1.743 54 51.30
>
> [Mail] wolkerstorfer at cure.at
> [Web]  http://www.cure.at
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From jrhodes at cyrus.psych.uiuc.edu  Mon Sep 25 15:46:09 2006
From: jrhodes at cyrus.psych.uiuc.edu (Justin Rhodes)
Date: Mon, 25 Sep 2006 08:46:09 -0500
Subject: [R] glmmPQL in 2.3.1
Message-ID: <6.2.3.4.2.20060925083827.03b49678@cyrus.psych.uiuc.edu>

Dear R-help,

I recently tried implementing glmmPQL in 2.3.1, and I discovered a 
few differences as compared to 2.2.1.  I am fitting a regression with 
fixed and random effects with Gamma error structure.  First, 2.3.1 
gives different estimates than 2.2.1, and 2.3.1, takes more 
iterations to converge.  Second, when I try using the anova function 
it says,  "'anova' is not available for PQL fits",  why?  Any help 
would be greatly appreciated.

Best wishes,

Justin


--
Justin S. Rhodes
Assistant Professor
Department of Psychology
Beckman Institute
Neuroscience Program, Institute for Genomic Biology
University of Illinois
405 N. Mathews Avenue
Urbana, IL 61801

Ph: 217-265-0021
Fax: 217-244-5180
e-mail: jrhodes at uiuc.edu

http://www.psych.uiuc.edu/people/showprofile.php?id=545


From mothsailor at googlemail.com  Mon Sep 25 15:47:26 2006
From: mothsailor at googlemail.com (David Barron)
Date: Mon, 25 Sep 2006 14:47:26 +0100
Subject: [R] Multiple imputation using mice with "mean"
In-Reply-To: <1159189987.4517d5e34f6ab@webmail.cryst.bbk.ac.uk>
References: <1159189987.4517d5e34f6ab@webmail.cryst.bbk.ac.uk>
Message-ID: <815b70590609250647y1f92e4edy7c5d10576ac3944f@mail.gmail.com>

You might find something useful at this web site:
http://www.multiple-imputation.com/

On 25/09/06, Eleni Rapsomaniki <e.rapsomaniki at mail.cryst.bbk.ac.uk> wrote:
>
> Hi
>
> I am trying to impute missing values for my data.frame. As I intend to use the
> complete data for prediction I am currently measuring the success of an
> imputation method by its resulting classification error in my training data.
>
> I have tried several approaches to replace missing values:
> - mean/median substitution
> - substitution by a value selected from the observed values of a variable
> - MLE in the mix package
> - all available methods for numerical data in the MICE package (ie. pmm, sample,
> mean and norm)
>
> I found that the least classification error results using mice with the "mean"
> option for numerical data. However, I am not sure how the "mean" multiple
> imputatation differs from the simple mean substitution. I tried to read some of
> the documentation supporting the R package, but couldn't find much theory about
> the "mean" imputation method.
>
> Are there any good papers to explain the background behind each imputation
> option in MICE?
>
> I would really appreciate any comments on the above, as my understanding of
> statistics is very limited.
>
> Many thanks
> Eleni Rapsomaniki
> Birkbeck College, UK
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From dimitris.rizopoulos at med.kuleuven.be  Mon Sep 25 15:55:01 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Mon, 25 Sep 2006 15:55:01 +0200
Subject: [R] Beginner Loop Question with dynamic variable names
References: <004E39EEDEC0AA47B58872F1E7D725E72B90A9@email.cure-space.cure-vienna.org>
	<815b70590609250633n5acc5085x4b26ce8b6d6dc28f@mail.gmail.com>
Message-ID: <009f01c6e0aa$3257b9f0$0540210a@www.domain>

----- Original Message ----- 
From: "David Barron" <mothsailor at googlemail.com>
To: "Peter Wolkerstorfer - CURE" <wolkerstorfer at cure.at>; "r-help" 
<r-help at stat.math.ethz.ch>
Sent: Monday, September 25, 2006 3:33 PM
Subject: Re: [R] Beginner Loop Question with dynamic variable names


>I think this does what you are looking for:
>
> dta <- 
> data.frame(q1_1=rep(1,5),q1_2=rep(2,5),q2_1=rep(3,5),q2_2=rep(4,5))
>
> for (i in 1:2) {
>    e1  <- paste("q",i,"_1 + q",i,"_2 * 2",sep="")
>    assign(paste("q",i,sep=""),with(dta,eval(parse(text=e1))))
> }
>

or something like the following if you want to avoid eval(parse(text = 
...)):

dta <- data.frame(q1_1 = rep(1,5), q1_2 = rep(2,5), q1_3 = rep(1,5), 
q1_4 = rep(2,5),
                  q2_1 = rep(3,5), q2_2 = rep(4,5), q2_3 = rep(3,5), 
q2_4 = rep(4,5),
                  q3_1 = rep(3,5), q3_2 = rep(4,5), q3_3 = rep(3,5), 
q3_4 = rep(4,5))

for (i in 1:3) {
    nam <- paste("q", i, sep = "")
    e1  <- data.matrix(dta[grep(nam, names(dta), fixed = TRUE)])
    dta <- cbind(dta, rowSums(e1 * rep(1:ncol(e1), each = nrow(e1))))
    names(dta)[length(dta)] <- nam
}

dta


Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


> On 25/09/06, Peter Wolkerstorfer - CURE <wolkerstorfer at cure.at> 
> wrote:
>> Dear all,
>>
>> I have another small scripting-beginner problem which you hopefully 
>> can
>> help:
>>
>> I compute new variables with:
>>
>> # Question 1
>> results$q1 <- with(results, q1_1*1+ q1_2*2+ q1_3*3+ q1_4*4+ q1_5*5)
>> # Question 2
>> results$q2 <- with(results, q2_1*1+ q2_2*2+ q2_3*3+ q2_4*4+ q2_5*5)
>> # Question 3
>> results$q3 <- with(results, q3_1*1+ q3_2*2+ q3_3*3+ q3_4*4+ q3_5*5)
>> # Question 4
>> results$q4 <- with(results, q4_1*1+ q4_2*2+ q4_3*3+ q4_4*4+ q4_5*5)
>>
>> This is very inefficient so I would like to do this in a loop like:
>>
>> for (i in 1:20) {results$q1 <- with(results, q1_1*1+ q1_2*2+ 
>> q1_3*3+
>> q1_4*4+ q1_5*5)}
>>
>> My question now:
>> How to replace the "1"-s (results$q1, q1_1...) in the variables 
>> with the
>> looping variable?
>>
>> Here like I like it (just for illustration - of course I still miss 
>> the
>> function to tell R that it should append the value of i to the 
>> variable
>> name):
>>
>> # i is the number of questions - just an illustration, I know it 
>> does
>> not work this way
>> for (i in 1:20) {results$qi <- with(results, qi_1*1+ qi_2*2+ 
>> qi_3*3+
>> qi_4*4+ qi_5*5)}
>>
>> Help would be greatly appreciated. Thanks in advance.
>>
>> Peter
>>
>>
>> ___CURE - Center for Usability Research & Engineering___
>>
>> Peter Wolkerstorfer
>> Usability Engineer
>> Hauffgasse 3-5, 1110 Wien, Austria
>>
>> [Tel]  +43.1.743 54 51.46
>> [Fax]  +43.1.743 54 51.30
>>
>> [Mail] wolkerstorfer at cure.at
>> [Web]  http://www.cure.at
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>
> -- 
> =================================
> David Barron
> Said Business School
> University of Oxford
> Park End Street
> Oxford OX1 1HP
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From afshart at exchange.sba.miami.edu  Mon Sep 25 15:56:59 2006
From: afshart at exchange.sba.miami.edu (Afshartous, David)
Date: Mon, 25 Sep 2006 09:56:59 -0400
Subject: [R] plotting grouped data object
Message-ID: <6BCB4D493A447546A8126F24332056E804300301@school1.business.edu>

Hello Xiaohui,

data.grp is just a pseudo example of a grouped data object
that is grouped according the factor y.
I just tried your suggestion, but the result is that 
three separate panels are still created, whereas I would like
to have all 3 lines in a single panel.  

cheers,
dave
 



-----Original Message-----
From: X.H Chen [mailto:xchen_stat at hotmail.com] 
Sent: Saturday, September 23, 2006 6:25 PM
To: Afshartous, David; r-help at stat.math.ethz.ch
Subject: RE: [R] plotting grouped data object

I don't get your meaning in what is in datagrp, anyway, try:

X11()
par(new=T)

before calling:
plot(data.grp, outer = ~ y)

Xiaohui Chen

Dept. of Statistics
UBC, Canada




>From: "Afshartous, David" <afshart at exchange.sba.miami.edu>
>To: <r-help at stat.math.ethz.ch>
>Subject: [R] plotting grouped data object
>Date: Sat, 23 Sep 2006 18:09:38 -0400
>
>
>All,
>I'd like to plot the main relationship of a grouped data object for all

>levels of a factor in a single panel.
>The sample code below creates a separate panel for each level of the 
>factor.  I realize that this could be done in other ways, but I'd like 
>to do it via plotting the grouped data object.
>thanks!
>dave
>
>z = rnorm(18, mean=0, sd=1)
>x = rep(1:6, 3)
>y = factor(rep(c("I", "C", "P"), each = 6)) dat = data.frame(x, y, z) 
>data.grp = groupedData(z ~ x | y, data = dat) plot(data.grp, outer = ~ 
>y) ### this produces 1 line each in 3 panels ### how to collapse all 3 
>lines into 1 panel?
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

_________________________________________________________________
Don't waste time standing in line-try shopping online. Visit Sympatico /
MSN Shopping today! http://shopping.sympatico.msn.ca


From Ted.Harding at nessie.mcc.ac.uk  Mon Sep 25 16:34:03 2006
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 25 Sep 2006 15:34:03 +0100 (BST)
Subject: [R] Multiple imputation using mice with "mean"
In-Reply-To: <1159189987.4517d5e34f6ab@webmail.cryst.bbk.ac.uk>
Message-ID: <XFMail.060925153403.Ted.Harding@nessie.mcc.ac.uk>

On 25-Sep-06 Eleni Rapsomaniki wrote:
> Hi
> 
> I am trying to impute missing values for my data.frame. As I
> intend to use the complete data for prediction I am currently
> measuring the success of an imputation method by its resulting
> classification error in my training data.
> 
> I have tried several approaches to replace missing values:
> - mean/median substitution
> - substitution by a value selected from the observed values of
> a variable
> - MLE in the mix package
> - all available methods for numerical data in the MICE package
> (ie. pmm, sample, mean and norm)
> 
> I found that the least classification error results using mice
> with the "mean" option for numerical data. However, I am not
> sure how the "mean" multiple imputatation differs from the simple
> mean substitution. I tried to read some of the documentation
> supporting the R package, but couldn't find much theory about
> the "mean" imputation method. 
> 
> Are there any good papers to explain the background behind each
> imputation option in MICE? 

I agree that the MICE documentation tends to be silent about some
imporant questions, both in the R/S "help" pages, and also in the
MICE user's manual which can be found at

  http://web.inter.nl.net/users/S.van.Buuren/mi/docs/Manual.pdf

Possibly it could be worth looking at some of the "other relevant
reports" listed at

  http://web.inter.nl.net/users/S.van.Buuren/mi/hmtl/mice.htm

but they do not look very hopeful.

That being said, my understanding relating to your query is
(glossing over the technicalities of the Gibbs sampling methods
used in (b))

a) mean/median substitution relates to the very basic method
   of substituting, for a missing value, the arithmetic mean
   of the non  missing values for that variable, possibly
   with selection of cases with non-missing values so as to
   approximately match the observed covariates of the case
   being imputed.
 
b) "mean" imputation in MICE (as far as I can infer it) means
   that the distribution of the missing value (conditional
   on its observed covariates) is inferred from the cases
   with non-missing values, and the mean of this conditional
   distribution is subsitutedfor the missing value.

These two approaches will in general give different results.

Some further comments.

1. I would suggest that you consider the full multiple imputation
approach. Filling in missing values just once, and then using
the completed results (for predicition, in your case) in some
procedure which treats them as though they were observed values,
will not take into account the uncertainty as to what values
they should have (as opposed to the values they were imputed to have).

Whe multiple imputation is used, the variation from imputation
to imputation in the imputed values will represent this
uncertainty, and so a more realistic picture of the overall
uncertainty of prediction can be obtained.

2. You stated that one method tried was "MLE in the mix package".
MLE (maximum likelihood estimation) using the EM algorithm is
implemented in the mix functions em.mix and ecm.mix, but neither
of these produces values to substitute for missing data. The
result is essentially just parameter estimation by MLE based
on the incomplete data.

Values to substitute for missing data are produced by other
functions, such as imp.mix; but these are randomly sampled
from the conditional distributions of the missing values and
therefore, each time it is done, the results are different.
In particular, the first value you sample will be random.
Hence the values you impute will be more or less good, in
terms of your "training set", depending on the "luck of the
draw" when you use (say) imp.mix.

I don't know if I have understood what you meant by "MLE in
the mix package", but if the above is a correct understanding
then the remarks under (1) apply: in particular, as just
noted, that comparing a single imputation with your "training
set" is an uncertain comparison.

Hoping this helps,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 25-Sep-06                                       Time: 15:33:59
------------------------------ XFMail ------------------------------


From justin_bem at yahoo.fr  Mon Sep 25 16:37:01 2006
From: justin_bem at yahoo.fr (justin bem)
Date: Mon, 25 Sep 2006 16:37:01 +0200 (CEST)
Subject: [R] RE :  Beginner question: select cases
In-Reply-To: <004E39EEDEC0AA47B58872F1E7D725E72B90A7@email.cure-space.cure-vienna.org>
Message-ID: <20060925143701.65348.qmail@web23008.mail.ird.yahoo.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20060925/1fc20328/attachment.pl 

From atcdias at biologia.ufrj.br  Mon Sep 25 16:42:20 2006
From: atcdias at biologia.ufrj.br (andre tavares correa dias)
Date: Mon, 25 Sep 2006 12:42:20 -0200
Subject: [R] F values for glm with binomial distribution
Message-ID: <20060925143521.M95131@biologia.ufrj.br>

Hi Rneters,
I'm running a GLM model with a full factorial design in blocks and binomial 
error distribution. I would like to have the F values for this model but I 
got a message that "using F test with a binomial family is inappropriate in: 
anova.glm(model, test = "F")". Should I not report F statistics on this kind 
of analysis? I would appreciate any comment on this.

This is my output:

> model=glm(y~agua*fert+bloco, family=binomial)
> anova.glm(model,test="F")

Analysis of Deviance Table

Model: binomial, link: logit

Response: y

Terms added sequentially (first to last)


          Df Deviance Resid. Df Resid. Dev      F  Pr(>F)  
NULL                         43     85.018                 
agua       1    0.908        42     84.110 0.9081 0.34063  
fert       1    5.673        41     78.437 5.6727 0.01723 *
bloco      1    0.044        40     78.393 0.0444 0.83320  
agua:fert  1    0.616        39     77.777 0.6160 0.43253  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
Warning message:
using F test with a binomial family is inappropriate in: anova.glm(model, 
test = "F")

Best regards,
Andr?





--
Andr? Tavares Corr?a Dias
Laborat?rio de Ecologia Vegetal
Universidade Federal do Rio de Janeiro
CCS-IB-Departamento de Ecologia
Caixa Postal 68020
21941-970 Rio de Janeiro ? RJ, Brazil
tel: +55 21 25626377
Fax: + 55 21 25626320
atcdias at biologia.ufrj.br


From fhduan at gmail.com  Mon Sep 25 18:04:48 2006
From: fhduan at gmail.com (Frank Duan)
Date: Mon, 25 Sep 2006 11:04:48 -0500
Subject: [R] Splitting a character variable into a numeric one and a
	character one?
Message-ID: <3b9172310609250904s2e008dar5b88e3c6181a9ca0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060925/afe32e31/attachment.pl 

From zvalim at gmail.com  Mon Sep 25 18:19:00 2006
From: zvalim at gmail.com (Uri Hasson)
Date: Mon, 25 Sep 2006 12:19:00 -0400
Subject: [R] Sampling distribution of correlation estimations derived from
	robust MCD and MVE methods
Message-ID: <32078b1e0609250919l568f2781w6fbd4dfa4d0f95b8@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060925/6a18680b/attachment.pl 

From ggrothendieck at gmail.com  Mon Sep 25 18:28:05 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 25 Sep 2006 12:28:05 -0400
Subject: [R] Splitting a character variable into a numeric one and a
	character one?
In-Reply-To: <3b9172310609250904s2e008dar5b88e3c6181a9ca0@mail.gmail.com>
References: <3b9172310609250904s2e008dar5b88e3c6181a9ca0@mail.gmail.com>
Message-ID: <971536df0609250928p49d77d8dm5c316d672382a977@mail.gmail.com>

strapply in package gsubfn can do that:


library(gsubfn)
s <- c("123abc", "12cd34", "1e23")

out <- strapply(s, "^([[:digit:]]+)(.*)", c)
out <- do.call(rbind, out) # as a matrix

data.frame(x = out[,1], num = as.numeric(out[,2]), char = out[,3]) #
as a data.frame


On 9/25/06, Frank Duan <fhduan at gmail.com> wrote:
> Hi All,
>
> I have a data with a variable like this:
>
> Column 1
>
> "123abc"
> "12cd34"
> "1e23"
> ...
>
> Now I want to do an operation that can split it into two variables:
>
> Column 1        Column 2         Column 3
>
> "123abc"         123                  "abc"
> "12cd34"         12                    "cd34"
> "1e23"             1                      "e23"
> ...
>
> So basically, I want to split the original variabe into a numeric one and a
> character one, while the splitting element is the first character in Column
> 1.
>
> I searched the forum with key words "strsplit"and "substr", but still can't
> solve this problem. Can anyone give me some hints?
>
> Thanks in advance,
>
> FD
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mschwartz at mn.rr.com  Mon Sep 25 18:30:47 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Mon, 25 Sep 2006 11:30:47 -0500
Subject: [R] Splitting a character variable into a numeric one
	and	a	character one?
In-Reply-To: <3b9172310609250904s2e008dar5b88e3c6181a9ca0@mail.gmail.com>
References: <3b9172310609250904s2e008dar5b88e3c6181a9ca0@mail.gmail.com>
Message-ID: <1159201847.4348.17.camel@localhost.localdomain>

On Mon, 2006-09-25 at 11:04 -0500, Frank Duan wrote:
> Hi All,
> 
> I have a data with a variable like this:
> 
> Column 1
> 
> "123abc"
> "12cd34"
> "1e23"
> ...
> 
> Now I want to do an operation that can split it into two variables:
> 
> Column 1        Column 2         Column 3
> 
> "123abc"         123                  "abc"
> "12cd34"         12                    "cd34"
> "1e23"             1                      "e23"
> ...
> 
> So basically, I want to split the original variabe into a numeric one and a
> character one, while the splitting element is the first character in Column
> 1.
> 
> I searched the forum with key words "strsplit"and "substr", but still can't
> solve this problem. Can anyone give me some hints?
> 
> Thanks in advance,
> 
> FD


Something like this using gsub() should work I think:

> DF
      V1
1 123abc
2 12cd34
3   1e23


# Replace letters and any following chars with ""
DF$V2 <- gsub("[A-Za-Z]+.*", "", DF$V1)


# Replace any initial numbers with ""
DF$V3 <- gsub("^[0-9]+", "", DF$V1)


> DF
      V1  V2   V3
1 123abc 123  abc
2 12cd34  12 cd34
3   1e23   1  e23

See ?gsub and ?regex for more information.

HTH,

Marc Schwartz


From fhduan at gmail.com  Mon Sep 25 18:34:20 2006
From: fhduan at gmail.com (Frank Duan)
Date: Mon, 25 Sep 2006 11:34:20 -0500
Subject: [R] Splitting a character variable into a numeric one and a
	character one?
In-Reply-To: <971536df0609250928p49d77d8dm5c316d672382a977@mail.gmail.com>
References: <3b9172310609250904s2e008dar5b88e3c6181a9ca0@mail.gmail.com>
	<971536df0609250928p49d77d8dm5c316d672382a977@mail.gmail.com>
Message-ID: <3b9172310609250934p442833bbha3165eaa28afcd38@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060925/878af509/attachment.pl 

From mschwartz at mn.rr.com  Mon Sep 25 18:37:44 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Mon, 25 Sep 2006 11:37:44 -0500
Subject: [R] Splitting a character variable into a numeric one
	and	a	character one?
In-Reply-To: <1159201847.4348.17.camel@localhost.localdomain>
References: <3b9172310609250904s2e008dar5b88e3c6181a9ca0@mail.gmail.com>
	<1159201847.4348.17.camel@localhost.localdomain>
Message-ID: <1159202265.4348.21.camel@localhost.localdomain>

On Mon, 2006-09-25 at 11:30 -0500, Marc Schwartz (via MN) wrote:
> On Mon, 2006-09-25 at 11:04 -0500, Frank Duan wrote:
> > Hi All,
> > 
> > I have a data with a variable like this:
> > 
> > Column 1
> > 
> > "123abc"
> > "12cd34"
> > "1e23"
> > ...
> > 
> > Now I want to do an operation that can split it into two variables:
> > 
> > Column 1        Column 2         Column 3
> > 
> > "123abc"         123                  "abc"
> > "12cd34"         12                    "cd34"
> > "1e23"             1                      "e23"
> > ...
> > 
> > So basically, I want to split the original variabe into a numeric one and a
> > character one, while the splitting element is the first character in Column
> > 1.
> > 
> > I searched the forum with key words "strsplit"and "substr", but still can't
> > solve this problem. Can anyone give me some hints?
> > 
> > Thanks in advance,
> > 
> > FD
> 
> 
> Something like this using gsub() should work I think:
> 
> > DF
>       V1
> 1 123abc
> 2 12cd34
> 3   1e23
> 
> 
> # Replace letters and any following chars with ""
> DF$V2 <- gsub("[A-Za-Z]+.*", "", DF$V1)

Quick typo correction here. It should be:

DF$V2 <- gsub("[A-Za-z]+.*", "", DF$V1)

The second 'z' should be lower case.


Marc


From ggrothendieck at gmail.com  Mon Sep 25 18:50:36 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 25 Sep 2006 12:50:36 -0400
Subject: [R] Splitting a character variable into a numeric one and a
	character one?
In-Reply-To: <971536df0609250928p49d77d8dm5c316d672382a977@mail.gmail.com>
References: <3b9172310609250904s2e008dar5b88e3c6181a9ca0@mail.gmail.com>
	<971536df0609250928p49d77d8dm5c316d672382a977@mail.gmail.com>
Message-ID: <971536df0609250950g544ecb9as74867326e57d2e5c@mail.gmail.com>

Here is one more solution:

library(gsubfn)
s <- c("123abc", "12cd34", "1e23")

out <- gsubfn("^([[:digit:]]+)(.*)", paste, s, backref = -2)
read.table(textConnection(out))

It assumes there are no spaces in the strings.  If
there are then choose a sep= that does not appear
and do this:

sep = ","
f <- function(x, y) paste(x, y, sep = sep)
out <- gsubfn("^([[:digit:]]+)(.*)", f, s, backref = -2)
read.table(textConnection(out), sep = sep)


On 9/25/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> strapply in package gsubfn can do that:
>
>
> library(gsubfn)
> s <- c("123abc", "12cd34", "1e23")
>
> out <- strapply(s, "^([[:digit:]]+)(.*)", c)
> out <- do.call(rbind, out) # as a matrix
>
> data.frame(x = out[,1], num = as.numeric(out[,2]), char = out[,3]) #
> as a data.frame
>
>
> On 9/25/06, Frank Duan <fhduan at gmail.com> wrote:
> > Hi All,
> >
> > I have a data with a variable like this:
> >
> > Column 1
> >
> > "123abc"
> > "12cd34"
> > "1e23"
> > ...
> >
> > Now I want to do an operation that can split it into two variables:
> >
> > Column 1        Column 2         Column 3
> >
> > "123abc"         123                  "abc"
> > "12cd34"         12                    "cd34"
> > "1e23"             1                      "e23"
> > ...
> >
> > So basically, I want to split the original variabe into a numeric one and a
> > character one, while the splitting element is the first character in Column
> > 1.
> >
> > I searched the forum with key words "strsplit"and "substr", but still can't
> > solve this problem. Can anyone give me some hints?
> >
> > Thanks in advance,
> >
> > FD
> >
> >        [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From ggrothendieck at gmail.com  Mon Sep 25 19:00:45 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 25 Sep 2006 13:00:45 -0400
Subject: [R] Splitting a character variable into a numeric one and a
	character one?
In-Reply-To: <971536df0609250950g544ecb9as74867326e57d2e5c@mail.gmail.com>
References: <3b9172310609250904s2e008dar5b88e3c6181a9ca0@mail.gmail.com>
	<971536df0609250928p49d77d8dm5c316d672382a977@mail.gmail.com>
	<971536df0609250950g544ecb9as74867326e57d2e5c@mail.gmail.com>
Message-ID: <971536df0609251000k138a2bcj6285599e30550983@mail.gmail.com>

And here is a third solution not using package gsubfn:

s <- c("123abc", "12cd34", "1e23")
out <- gsub("^(([[:digit:]]+)(.*))", "\\1 \\2 \\3", s)
read.table(textConnection(out), as.is = TRUE)

Again, if spaces appear in the input string choose a character
not appearing, such as comma, and do it like this:

s <- c("123abc", "12cd34", "1e23")
out <- gsub("^(([[:digit:]]+)(.*))", "\\1,\\2,\\3", s)
read.table(textConnection(out), sep = ",", as.is = TRUE)


On 9/25/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Here is one more solution:
>
> library(gsubfn)
> s <- c("123abc", "12cd34", "1e23")
>
> out <- gsubfn("^([[:digit:]]+)(.*)", paste, s, backref = -2)
> read.table(textConnection(out))
>
> It assumes there are no spaces in the strings.  If
> there are then choose a sep= that does not appear
> and do this:
>
> sep = ","
> f <- function(x, y) paste(x, y, sep = sep)
> out <- gsubfn("^([[:digit:]]+)(.*)", f, s, backref = -2)
> read.table(textConnection(out), sep = sep)
>
>
> On 9/25/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > strapply in package gsubfn can do that:
> >
> >
> > library(gsubfn)
> > s <- c("123abc", "12cd34", "1e23")
> >
> > out <- strapply(s, "^([[:digit:]]+)(.*)", c)
> > out <- do.call(rbind, out) # as a matrix
> >
> > data.frame(x = out[,1], num = as.numeric(out[,2]), char = out[,3]) #
> > as a data.frame
> >
> >
> > On 9/25/06, Frank Duan <fhduan at gmail.com> wrote:
> > > Hi All,
> > >
> > > I have a data with a variable like this:
> > >
> > > Column 1
> > >
> > > "123abc"
> > > "12cd34"
> > > "1e23"
> > > ...
> > >
> > > Now I want to do an operation that can split it into two variables:
> > >
> > > Column 1        Column 2         Column 3
> > >
> > > "123abc"         123                  "abc"
> > > "12cd34"         12                    "cd34"
> > > "1e23"             1                      "e23"
> > > ...
> > >
> > > So basically, I want to split the original variabe into a numeric one and a
> > > character one, while the splitting element is the first character in Column
> > > 1.
> > >
> > > I searched the forum with key words "strsplit"and "substr", but still can't
> > > solve this problem. Can anyone give me some hints?
> > >
> > > Thanks in advance,
> > >
> > > FD
> > >
> > >        [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
>


From M.P.M.Boks at umcutrecht.nl  Mon Sep 25 18:58:58 2006
From: M.P.M.Boks at umcutrecht.nl (Boks, M.P.M.)
Date: Mon, 25 Sep 2006 18:58:58 +0200
Subject: [R] paste? 'cmd /c "c:\\pheno\\whap --file c:\\pheno\\smri --alt 1"'
Message-ID: <2AD792FE1A79F046B908C364C29622F2014A4FF2@EX05.ds.umcutrecht.nl>

Dear R users,
 
This command works (calling a programm -called whap- with file specifiers etc.):
 
>system('cmd /c "c:\\pheno\\whap --file c:\\pheno\\smri --alt 1 --perm 500"', intern=TRUE)
 
Now I need to call it from a loop to replace the "1" by different number, however I get lost using the quotes:
 
I tried numerous versions of:
 
>i<-1
>system(paste(c("'cmd /c "c:\\pheno\\whap --file c:\\pheno\\smri --alt", i, " --perm 500"'", sep="" )), intern=TRUE)
 
However no luck! I would be gratefull for any help.
 
Thanks,
 
Marco


From B.Rowlingson at lancaster.ac.uk  Mon Sep 25 19:07:13 2006
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 25 Sep 2006 18:07:13 +0100
Subject: [R] Splitting a character variable into a numeric one and
 a	character one?
In-Reply-To: <971536df0609250950g544ecb9as74867326e57d2e5c@mail.gmail.com>
References: <3b9172310609250904s2e008dar5b88e3c6181a9ca0@mail.gmail.com>	<971536df0609250928p49d77d8dm5c316d672382a977@mail.gmail.com>
	<971536df0609250950g544ecb9as74867326e57d2e5c@mail.gmail.com>
Message-ID: <45180CC1.8090802@lancaster.ac.uk>



>>>Now I want to do an operation that can split it into two variables:
>>>
>>>Column 1        Column 2         Column 3
>>>
>>>"123abc"         123                  "abc"
>>>"12cd34"         12                    "cd34"
>>>"1e23"             1                      "e23"
>>>...
>>>
>>>So basically, I want to split the original variabe into a numeric one and a
>>>character one, while the splitting element is the first character in Column


My first thought on this was to apply the regexp "^([0-9]*)(.*)$" and 
getting the two parts out. But I dont see a way to get both matches in 
parentheses out in one go.

In Python you just do:

  >>> re.findall('^([0-9]*)(.*)$',"123abc")
  [('123', 'abc')]

  >>> re.findall('^([0-9]*)(.*)$',"1e12")
  [('1', 'e12')]

In R you can get the groups and go gsub on them:

  > r="^([0-9]*)(.*)$"
  > gsub(r,"\\1","123abc")
  [1] "123"

  But I dont see a way of getting the two values out except as part of 
one string in gsub - which is right back where you started - or doing 
gsub twice.

Barry


From AnupTyagi at yahoo.com  Mon Sep 25 19:18:31 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Mon, 25 Sep 2006 17:18:31 +0000 (UTC)
Subject: [R] Passing R connection as argument to a shell command on Windows
Message-ID: <loom.20060925T191138-266@post.gmane.org>

Hello, is there a way to pass a connection to a file in a zipped archive as
argument (instead of a file name of unzipped file) to shell command "cut". In
general, is it possible to pipe output of a R function to a shell command? How?

I want to do something like:

z = unz("zipArchive.zip", "fileASCII.ASC")
# open connection
open(z)
# cut lines of the ASCII file in zipped archive at specific postions and send
results to another file.
shell("cut -c2-3,5-8 z > test2.dat")

Anupam.


From deepayan.sarkar at gmail.com  Mon Sep 25 19:34:11 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Mon, 25 Sep 2006 10:34:11 -0700
Subject: [R] plotting grouped data object
In-Reply-To: <6BCB4D493A447546A8126F24332056E804300264@school1.business.edu>
References: <6BCB4D493A447546A8126F24332056E804300264@school1.business.edu>
Message-ID: <eb555e660609251034t5e5dfac0xdcae17d43f6a57e1@mail.gmail.com>

On 9/23/06, Afshartous, David <afshart at exchange.sba.miami.edu> wrote:
>
> All,
> I'd like to plot the main relationship of a grouped data
> object for all levels of a factor in a single panel.
> The sample code below creates a separate panel for each level
> of the factor.  I realize that this could be done in other ways,
> but I'd like to do it via plotting the grouped data object.
> thanks!
> dave
>
> z = rnorm(18, mean=0, sd=1)
> x = rep(1:6, 3)
> y = factor(rep(c("I", "C", "P"), each = 6))
> dat = data.frame(x, y, z)
> data.grp = groupedData(z ~ x | y, data = dat)
> plot(data.grp, outer = ~ y)
> ### this produces 1 line each in 3 panels
> ### how to collapse all 3 lines into 1 panel?

The closest I can get is

dat$one <- gl(1, 18)
data.grp = groupedData(z ~ x | one, data = dat)
plot(data.grp, innerGroups = ~y, strip = FALSE)

-Deepayan


From mr.blacksheep at gmail.com  Mon Sep 25 19:46:34 2006
From: mr.blacksheep at gmail.com (Mike Nielsen)
Date: Mon, 25 Sep 2006 12:46:34 -0500
Subject: [R] Passing R connection as argument to a shell command on
	Windows
In-Reply-To: <loom.20060925T191138-266@post.gmane.org>
References: <loom.20060925T191138-266@post.gmane.org>
Message-ID: <46a360560609251046y95b359i2f9d6fc0fdef2854@mail.gmail.com>

No, the "cut" command won't understand that "z" is an R connection and
not a file in the current working directory: there is no overlap
between the R object name space and the Windows object name space.

Unfortunately, you may be forced to unzip to a temporary file, and
then read from that.

One thing that you might want to try, if you're using cygwin, is to
create a named pipe, and use "shell()" with wait=FALSE to unzip and
pipe into "cut" and then output to the named pipe.  Open an R
connection for reading from the named pipe.  This leaves open the
question of how to deal with failures, and whether you can invoke a
command pipeline from R under Windows...

I haven't tried this, so if you manage to make it work, it may be
something that's of interest to the list in general.

Regards,

Mike

On 9/25/06, Anupam Tyagi <AnupTyagi at yahoo.com> wrote:
> Hello, is there a way to pass a connection to a file in a zipped archive as
> argument (instead of a file name of unzipped file) to shell command "cut". In
> general, is it possible to pipe output of a R function to a shell command? How?
>
> I want to do something like:
>
> z = unz("zipArchive.zip", "fileASCII.ASC")
> # open connection
> open(z)
> # cut lines of the ASCII file in zipped archive at specific postions and send
> results to another file.
> shell("cut -c2-3,5-8 z > test2.dat")
>
> Anupam.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Regards,

Mike Nielsen


From r-help at botelho-machado.de  Mon Sep 25 19:56:52 2006
From: r-help at botelho-machado.de (Lothar Botelho-Machado)
Date: Mon, 25 Sep 2006 19:56:52 +0200
Subject: [R] Can't mix high level and low level plot functions.
Message-ID: <45181864.1050105@botelho-machado.de>

Hey R-Comunity,


I'd like to print out an histogram of some experimental data and add a
smooth curve of a normal distribution with an ideally generated
population having the same mean and standard deviation like the
experimental data.


The experimental data is set as vector x and its name is set to
group.name. I paint the histogram as follows:

hist(data, freq=FALSE, col="lightgrey", ylab="Density", xlab=group.name)



First I did the normal distribution curve this way:

lines(x, dnorm(x, mean=mean(x), sd=sd(x)), type="l", lwd=2)

This curve just uses as many values as there are in x. When using small
amounts of sample populations the curve looks really shaky.



I tried this one using a high level plot function as well:

curve(dnorm, n=10000, add=TRUE, xlim=range(x))

The advantage is, now I can set an ideal population of 10000 to get the
ideal curve really smooth. But the big disadvantage is, I don't know how
to add "mean=mean(x),  sd=sd(x)" arguments to it? It says that it can't
mix high level with low level plot functions when I try to set some kind
of parameter like "n=10000" to the low level function, it says that
there ain't enough x values.

So my question is, how to get a smooth curve placed of dnorm over an
histogram of sample data, ideally by using the curve method?


TIA,
Lothar Rubusch


From AnupTyagi at yahoo.com  Mon Sep 25 19:55:48 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Mon, 25 Sep 2006 17:55:48 +0000 (UTC)
Subject: [R] Adding percentage to Pie Charts
References: <4515E92C.4070506@bitwrit.com.au>
	<971536df0609231842p1b7c3f74g80c425525221ced@mail.gmail.com>
Message-ID: <loom.20060925T194512-709@post.gmane.org>

Gabor Grothendieck <ggrothendieck <at> gmail.com> writes:

> 
> It might also be nice to be able to align the fans at the left or right,
> not just the center.

Fans that open only on one side: A line that moves like the minute needle of an
analog clock; with zero at the top. Movement of the needle in clock-wise
direction represents the number (precentage). Anupam.


From murdoch at stats.uwo.ca  Mon Sep 25 20:03:26 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 25 Sep 2006 14:03:26 -0400
Subject: [R] Can't mix high level and low level plot functions.
In-Reply-To: <45181864.1050105@botelho-machado.de>
References: <45181864.1050105@botelho-machado.de>
Message-ID: <451819EE.7010400@stats.uwo.ca>

On 9/25/2006 1:56 PM, Lothar Botelho-Machado wrote:
> Hey R-Comunity,
> 
> 
> I'd like to print out an histogram of some experimental data and add a
> smooth curve of a normal distribution with an ideally generated
> population having the same mean and standard deviation like the
> experimental data.
> 
> 
> The experimental data is set as vector x and its name is set to
> group.name. I paint the histogram as follows:
> 
> hist(data, freq=FALSE, col="lightgrey", ylab="Density", xlab=group.name)
> 
> 
> 
> First I did the normal distribution curve this way:
> 
> lines(x, dnorm(x, mean=mean(x), sd=sd(x)), type="l", lwd=2)
> 
> This curve just uses as many values as there are in x. When using small
> amounts of sample populations the curve looks really shaky.

This is generally the right way to do it, but you likely want to use a 
different variable for the first two occurrences of x, e.g.

x0 <- seq(from=min(x), to=max(x), len=200)
lines(x0, dnorm(x0, mean=mean(x), sd=sd(x)), type="l", lwd=2)

Duncan Murdoch

> 
> 
> I tried this one using a high level plot function as well:
> 
> curve(dnorm, n=10000, add=TRUE, xlim=range(x))
> 
> The advantage is, now I can set an ideal population of 10000 to get the
> ideal curve really smooth. But the big disadvantage is, I don't know how
> to add "mean=mean(x),  sd=sd(x)" arguments to it? It says that it can't
> mix high level with low level plot functions when I try to set some kind
> of parameter like "n=10000" to the low level function, it says that
> there ain't enough x values.
> 
> So my question is, how to get a smooth curve placed of dnorm over an
> histogram of sample data, ideally by using the curve method?
> 
> 
> TIA,
> Lothar Rubusch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From zvalim at gmail.com  Mon Sep 25 20:06:45 2006
From: zvalim at gmail.com (Uri Hasson)
Date: Mon, 25 Sep 2006 14:06:45 -0400
Subject: [R] [PlainText Attempt] Sampling distribution of correlation
	estimations derived from robust MCD and MVE methods
Message-ID: <32078b1e0609251106s588e0407gb6eb91c7a8c56875@mail.gmail.com>

Dear R users,

I am trying to use MCD and MVE methods in the analysis of functional imaging
(fMRI) data. But, before doing that, I want to understand the sampling
distribution of the correlation parameter given by MCD and MVE (cov.mcd$cor,
cov.mve$cor).

To this end, I conducted a simulation where in each of 100000 epochs, I
a.    construct a matrix from two vectors, each containing 40 numbers
randomly sampled from a normal distribution.
b.    apply cov.mve and cov.mcd to the resulting matrix.
c.    obtain the correlations in the subsets selected by cor.mve: e.g., if
the matrix is called cormat20.ans, I request:

current.mve20 <- round(cov.mve(cormat20.ans, cor=T)$cor[[2]] ,3)

At the end of the day, I have the sampling distribution for these
correlations [i.e., what correlations exist in the subsets that MVE and MCD
tend to pick up when sampling from normal distribution].

Here is my question: Because MVE and MCD select the most central 20 points
(of the 40), I wanted to compare the resulting sampling distributions to
that of a Pearson's "r" correlation coefficient (i.e., a Pearson's r with
N=20; the goal was to establish whether the significance thresholds are
similar).  However the three sampling distributions are quite different.
That is, the sampling distribution of Pearson's R (N=20) is very different
than that of cov.mve and cov.mcd (with N=20 [20 being the subset selected of
the 40 points]).  The sampling distribution of Pearson's R with N=40 is also
very different than that of MVE and MCD.

If anyone knows, or could point me to sources information that discuss the
issue of the sampling distribution of of cov.mve$cor and cov.mcd$cor and
their relations to the pearson's R, I would be very grateful.

I have put the simulation code I used here:
http://home.uchicago.edu/~uhasson/pearson-mcd-mve.R.txt

And an image of the resulting sampling distributions here:
http://home.uchicago.edu/~uhasson/correl.comparison.tiff


Sincerely,
Uri Hasson
The Brain Research Imaging Center
The University of Chicago


From albmont at centroin.com.br  Mon Sep 25 20:12:13 2006
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Mon, 25 Sep 2006 16:12:13 -0200
Subject: [R] Unidentified warning message in Portuguese
Message-ID: <20060925180809.M72157@centroin.com.br>

What is the meaning of this message?

  Warning message:
  Realizando coer??o de LHD para uma lista 

I tried to do something like this:

test <- function(x) {
  rval <- NULL
  m <- mean(x)
  s <- sd(x)
  rval$m <- m
  rval$s <- s
  y <- x[abs(x - m) > 3 * s]
  rval$y <- y # this is the critical line
  return(rval)
}

except that in the example above the critical line does not give
any error, while in the real (much bigger) example it does, for
things like:

test(c(runif(100), 100))

Alberto Monteiro


From mschwartz at mn.rr.com  Mon Sep 25 20:25:26 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Mon, 25 Sep 2006 13:25:26 -0500
Subject: [R] Can't mix high level and low level plot functions.
In-Reply-To: <45181864.1050105@botelho-machado.de>
References: <45181864.1050105@botelho-machado.de>
Message-ID: <1159208726.10554.5.camel@localhost.localdomain>

On Mon, 2006-09-25 at 19:56 +0200, Lothar Botelho-Machado wrote:
> Hey R-Comunity,
> 
> 
> I'd like to print out an histogram of some experimental data and add a
> smooth curve of a normal distribution with an ideally generated
> population having the same mean and standard deviation like the
> experimental data.
> 
> 
> The experimental data is set as vector x and its name is set to
> group.name. I paint the histogram as follows:
> 
> hist(data, freq=FALSE, col="lightgrey", ylab="Density", xlab=group.name)
> 
> 
> 
> First I did the normal distribution curve this way:
> 
> lines(x, dnorm(x, mean=mean(x), sd=sd(x)), type="l", lwd=2)
> 
> This curve just uses as many values as there are in x. When using small
> amounts of sample populations the curve looks really shaky.
> 
> 
> 
> I tried this one using a high level plot function as well:
> 
> curve(dnorm, n=10000, add=TRUE, xlim=range(x))
> 
> The advantage is, now I can set an ideal population of 10000 to get the
> ideal curve really smooth. But the big disadvantage is, I don't know how
> to add "mean=mean(x),  sd=sd(x)" arguments to it? It says that it can't
> mix high level with low level plot functions when I try to set some kind
> of parameter like "n=10000" to the low level function, it says that
> there ain't enough x values.
> 
> So my question is, how to get a smooth curve placed of dnorm over an
> histogram of sample data, ideally by using the curve method?
> 
> 
> TIA,
> Lothar Rubusch

This almost seems like it should be a FAQ. I also checked the R Graphics
Gallery (http://addictedtor.free.fr/graphiques/index.php) and didn't see
an example there either, unless I missed it.

In either case:

x <- rnorm(50)

hist(x, freq = FALSE)

# Create a sequence of x axis values with small
# increments over the range of 'x' to smooth the lines
x.hypo <- seq(min(x), max(x), length = 1000)

# Now use lines()
lines(x.hypo, dnorm(x.hypo, mean=mean(x), sd=sd(x)), type="l", lwd=2)

HTH,

Marc Schwartz


From mschwartz at mn.rr.com  Mon Sep 25 20:39:42 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Mon, 25 Sep 2006 13:39:42 -0500
Subject: [R] paste? 'cmd /c "c:\\pheno\\whap --file
	c:\\pheno\\smri	--alt 1"'
In-Reply-To: <2AD792FE1A79F046B908C364C29622F2014A4FF2@EX05.ds.umcutrecht.nl>
References: <2AD792FE1A79F046B908C364C29622F2014A4FF2@EX05.ds.umcutrecht.nl>
Message-ID: <1159209583.10554.19.camel@localhost.localdomain>

On Mon, 2006-09-25 at 18:58 +0200, Boks, M.P.M. wrote:
> Dear R users,
>  
> This command works (calling a programm -called whap- with file specifiers etc.):
>  
> >system('cmd /c "c:\\pheno\\whap --file c:\\pheno\\smri --alt 1 --perm 500"', intern=TRUE)
>  
> Now I need to call it from a loop to replace the "1" by different number, however I get lost using the quotes:
>  
> I tried numerous versions of:
>  
> >i<-1
> >system(paste(c("'cmd /c "c:\\pheno\\whap --file c:\\pheno\\smri --alt", i, " --perm 500"'", sep="" )), intern=TRUE)
>  
> However no luck! I would be gratefull for any help.
>  
> Thanks,
>  
> Marco

You need to escape the quote (") chars in the paste()d string so that
they get passed to your command properly. Also, you don't want to use
c() within the paste() function, as the paste() function already
concatenates the component vectors.

Note:

i <- 1

> paste("'cmd /c "c:\\pheno\\whap --file c:\\pheno\\smri --alt", i,  " --perm 500"'", sep="")
Error: syntax error in "paste("'cmd /c "c"

R sees the double quote before the second 'c' as the end of the string:

  "'cmd /c "


Now use "\" to escape the internal quotes:

> paste("'cmd /c \"c:\\pheno\\whap --file c:\\pheno\\smri --alt ", i,  " --perm 500\"'", sep="")
[1] "'cmd /c \"c:\\pheno\\whap --file c:\\pheno\\smri --alt 1 --perm 500\"'"


Use '\' to escape each of the double quotes within the string, so that R
can differentiate string delimiters versus characters within the string.

HTH,

Marc Schwartz


From vincent.goulet at act.ulaval.ca  Mon Sep 25 20:43:49 2006
From: vincent.goulet at act.ulaval.ca (Vincent Goulet)
Date: Mon, 25 Sep 2006 14:43:49 -0400
Subject: [R] Best use of LaTeX listings package for pretty printing R
	code
In-Reply-To: <4517DA39.4070102@vanderbilt.edu>
References: <4517DA39.4070102@vanderbilt.edu>
Message-ID: <200609251443.49264.vincent.goulet@act.ulaval.ca>

Le Lundi 25 Septembre 2006 09:31, Frank E Harrell Jr a ?crit?:
> This is what I have been using.  Does anyone have a better way?  In
> particular I would like to see letters in comment strings not stretched
> so much.  Thanks -Frank
>
> \documentclass{article}
> \usepackage{listings,relsize}
> \lstloadlanguages{R}
> \newcommand{\lil}[1]{\lstinline|#1|}
>
> \begin{document}
> \lstset{language=R,basicstyle=\smaller,commentstyle=\rmfamily\smaller,
>   showstringspaces=false,%
>   xleftmargin=4ex,literate={<-}{{$\leftarrow$}}1 {~}{{$\sim$}}1}
> \lstset{escapeinside={(*}{*)}}   % for (*\ref{ }*) inside lstlistings (S
> code)
> \begin{lstlisting}
> a <- b   # this is a test line
> if(i==3) {  # another line, for y^2
>   y <- 3^3
>   z <- 'this string'
>   qqcat <- y ~ pol(x,2)
> } else y <- 4
> \end{lstlisting}
> That was \lstinline|x <- 22| \lil{q <- 'cat'}.
> \end{document}

listings is a great package to highlight R keywords and comments and --- that 
was my main use of the package --- index those keywords. I found that I had 
to slightly redefine the list of keywords included in listings. I still did 
not take the time to submit a patch to the author, though...

In any case, here's what I use, if it can be of any help to anyone:

\lstloadlanguages{R}
\lstdefinelanguage{Renhanced}[]{R}{%
  morekeywords={acf,ar,arima,arima.sim,colMeans,colSums,is.na,is.null,%
    mapply,ms,na.rm,nlmin,replicate,row.names,rowMeans,rowSums,seasonal,%
    sys.time,system.time,ts.plot,which.max,which.min},
  deletekeywords={c},
  alsoletter={.\%},%
  alsoother={:_\$}}
\lstset{language=Renhanced,extendedchars=true,
  basicstyle=\small\ttfamily,
  commentstyle=\textsl,
  keywordstyle=\mdseries,
  showstringspaces=false,
  index=[1][keywords], 
  indexstyle=\indexfonction}

with

  \newcommand{\indexfonction}[1]{\index{#1@\texttt{#1}}}

-- 
  Vincent Goulet, Associate Professor
  ?cole d'actuariat
  Universit? Laval, Qu?bec 
  Vincent.Goulet at act.ulaval.ca   http://vgoulet.act.ulaval.ca


From damian.betebenner at bc.edu  Mon Sep 25 21:33:08 2006
From: damian.betebenner at bc.edu (Damian Betebenner)
Date: Mon, 25 Sep 2006 15:33:08 -0400
Subject: [R] Rows of a data frame to matrix
Message-ID: <web-10795332@be3.bc.edu>

useRs,

I have a data frame where four of the columns of the data frame represent the values of a two-by-two
matrix. I'd like to, row-by-row, go through the data frame and use the four columns, in matrix form, to
perform calculations necessary to create new values for variables in the data frame. My first idea was to 
use apply:

apply(as.array(data.frame[,1:4]), 1, matrix, nrow=2)

Though intuitive, this doesn't work. I've stumbled in the dark with mApply and other functions but can't
find anything in the help that works. This can't be that hard, but it has me stumped. 

Any help greatly appreciated,

Damian


From ggrothendieck at gmail.com  Mon Sep 25 21:54:22 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 25 Sep 2006 15:54:22 -0400
Subject: [R] Rows of a data frame to matrix
In-Reply-To: <web-10795332@be3.bc.edu>
References: <web-10795332@be3.bc.edu>
Message-ID: <971536df0609251254p5fd01353y9079667b41c76518@mail.gmail.com>

Using the builtin 11x8 anscombe data frame here are
some alternatives:

# 1
# list of 2x2 matrices
lapply(split(anscombe[1:4], 1:nrow(anscombe)), matrix, 2)

# 2
# 2x2x11 array
array(t(anscombe[1:4]), c(2, 2, nrow(anscombe)))

# 3
# to create matrix and perform calculations, e.g. det, all in one step
apply(anscombe[1:4], 1, function(x) det(matrix(x, 2)))



On 9/25/06, Damian Betebenner <damian.betebenner at bc.edu> wrote:
> useRs,
>
> I have a data frame where four of the columns of the data frame represent the values of a two-by-two
> matrix. I'd like to, row-by-row, go through the data frame and use the four columns, in matrix form, to
> perform calculations necessary to create new values for variables in the data frame. My first idea was to
> use apply:
>
> apply(as.array(data.frame[,1:4]), 1, matrix, nrow=2)
>
> Though intuitive, this doesn't work. I've stumbled in the dark with mApply and other functions but can't
> find anything in the help that works. This can't be that hard, but it has me stumped.
>
> Any help greatly appreciated,
>
> Damian
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From afshart at exchange.sba.miami.edu  Mon Sep 25 22:06:33 2006
From: afshart at exchange.sba.miami.edu (Afshartous, David)
Date: Mon, 25 Sep 2006 16:06:33 -0400
Subject: [R] plotting grouped data object
Message-ID: <6BCB4D493A447546A8126F24332056E80430050B@school1.business.edu>

thanks!   

-----Original Message-----
From: Deepayan Sarkar [mailto:deepayan.sarkar at gmail.com] 
Sent: Monday, September 25, 2006 1:34 PM
To: Afshartous, David
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] plotting grouped data object

On 9/23/06, Afshartous, David <afshart at exchange.sba.miami.edu> wrote:
>
> All,
> I'd like to plot the main relationship of a grouped data object for 
> all levels of a factor in a single panel.
> The sample code below creates a separate panel for each level of the 
> factor.  I realize that this could be done in other ways, but I'd like

> to do it via plotting the grouped data object.
> thanks!
> dave
>
> z = rnorm(18, mean=0, sd=1)
> x = rep(1:6, 3)
> y = factor(rep(c("I", "C", "P"), each = 6)) dat = data.frame(x, y, z) 
> data.grp = groupedData(z ~ x | y, data = dat) plot(data.grp, outer = ~

> y) ### this produces 1 line each in 3 panels ### how to collapse all 3

> lines into 1 panel?

The closest I can get is

dat$one <- gl(1, 18)
data.grp = groupedData(z ~ x | one, data = dat) plot(data.grp,
innerGroups = ~y, strip = FALSE)

-Deepayan


From ggrothendieck at gmail.com  Mon Sep 25 22:45:23 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 25 Sep 2006 16:45:23 -0400
Subject: [R] paste? 'cmd /c "c:\\pheno\\whap --file c:\\pheno\\smri
	--alt 1"'
In-Reply-To: <2AD792FE1A79F046B908C364C29622F2014A4FF2@EX05.ds.umcutrecht.nl>
References: <2AD792FE1A79F046B908C364C29622F2014A4FF2@EX05.ds.umcutrecht.nl>
Message-ID: <971536df0609251345u66c15e6bn5991aeacc58d2b88@mail.gmail.com>

Use single outer quotes so that the inner double quotes are not
interpreted as the end of the string.

cmd <- 'cmd /c "...whatever..." '
system(cmd, intern = TRUE)

On 9/25/06, Boks, M.P.M. <M.P.M.Boks at umcutrecht.nl> wrote:
> Dear R users,
>
> This command works (calling a programm -called whap- with file specifiers etc.):
>
> >system('cmd /c "c:\\pheno\\whap --file c:\\pheno\\smri --alt 1 --perm 500"', intern=TRUE)
>
> Now I need to call it from a loop to replace the "1" by different number, however I get lost using the quotes:
>
> I tried numerous versions of:
>
> >i<-1
> >system(paste(c("'cmd /c "c:\\pheno\\whap --file c:\\pheno\\smri --alt", i, " --perm 500"'", sep="" )), intern=TRUE)
>
> However no luck! I would be gratefull for any help.
>
> Thanks,
>
> Marco
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From r-help at botelho-machado.de  Mon Sep 25 23:08:16 2006
From: r-help at botelho-machado.de (Lothar Botelho-Machado)
Date: Mon, 25 Sep 2006 23:08:16 +0200
Subject: [R] Can't mix high level and low level plot functions.
In-Reply-To: <451819EE.7010400@stats.uwo.ca>
References: <45181864.1050105@botelho-machado.de>
	<451819EE.7010400@stats.uwo.ca>
Message-ID: <45184540.10002@botelho-machado.de>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Thanks for your help!!

I appreciate, now it works perfectly.

Lothar Rubusch



Duncan Murdoch wrote:
> On 9/25/2006 1:56 PM, Lothar Botelho-Machado wrote:
>> Hey R-Comunity,
>>
>>
>> I'd like to print out an histogram of some experimental data and add a
>> smooth curve of a normal distribution with an ideally generated
>> population having the same mean and standard deviation like the
>> experimental data.
>>
>>
>> The experimental data is set as vector x and its name is set to
>> group.name. I paint the histogram as follows:
>>
>> hist(data, freq=FALSE, col="lightgrey", ylab="Density", xlab=group.name)
>>
>>
>>
>> First I did the normal distribution curve this way:
>>
>> lines(x, dnorm(x, mean=mean(x), sd=sd(x)), type="l", lwd=2)
>>
>> This curve just uses as many values as there are in x. When using small
>> amounts of sample populations the curve looks really shaky.
> 
> This is generally the right way to do it, but you likely want to use a
> different variable for the first two occurrences of x, e.g.
> 
> x0 <- seq(from=min(x), to=max(x), len=200)
> lines(x0, dnorm(x0, mean=mean(x), sd=sd(x)), type="l", lwd=2)
> 
> Duncan Murdoch
> 
>>
>>
>> I tried this one using a high level plot function as well:
>>
>> curve(dnorm, n=10000, add=TRUE, xlim=range(x))
>>
>> The advantage is, now I can set an ideal population of 10000 to get the
>> ideal curve really smooth. But the big disadvantage is, I don't know how
>> to add "mean=mean(x),  sd=sd(x)" arguments to it? It says that it can't
>> mix high level with low level plot functions when I try to set some kind
>> of parameter like "n=10000" to the low level function, it says that
>> there ain't enough x values.
>>
>> So my question is, how to get a smooth curve placed of dnorm over an
>> histogram of sample data, ideally by using the curve method?
>>
>>
>> TIA,
>> Lothar Rubusch
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> 


-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.5 (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org

iD8DBQFFGEVAHRf7N9c+X7sRAtRoAJ97ft75u1etTac3Daiti1u2mlyRWgCeIAAK
81WfyDGzDdWDm11MwPiDKIA=
=W2m9
-----END PGP SIGNATURE-----


From therneau at mayo.edu  Mon Sep 25 23:32:10 2006
From: therneau at mayo.edu (Terry Therneau)
Date: Mon, 25 Sep 2006 16:32:10 -0500 (CDT)
Subject: [R] fraitly in coxph
Message-ID: <200609252132.k8PLWAi17309@hsrnfs-101.mayo.edu>

 For a nested model you want to use the coxme function, which is the much
superior successor to frailty().  It is currently found in the kinship
library.

	Terry Therneau


From bdl at fwr.on.ca  Tue Sep 26 02:09:06 2006
From: bdl at fwr.on.ca (Bruce LaZerte)
Date: Mon, 25 Sep 2006 20:09:06 -0400
Subject: [R] Sort problem with merge (again)
Message-ID: <45186FA2.6060104@fwr.on.ca>

# R version 2.3.1 (2006-06-01) Debian Linux "testing"

# Is the following behaviour a bug, feature or just a lack of
# understanding on my part? I see that this was discussed here
# last March with no apparent resolution.

d <- as.factor(c("1970-04-04","1970-08-11","1970-10-18"))
x <- c(9,10,11)
ch <- data.frame(Date=d,X=x)

d <- as.factor(c("1970-06-04","1970-08-11","1970-08-18"))
y <- c(109,110,111)
sp <- data.frame(Date=d,Y=y)

df <- merge(ch,sp,all=TRUE,by="Date")
# the rows with dates missing all ch vars are tacked on the end.
# the rows with dates missing all sp vars are sorted in with
# the row with a date with vars from both ch and sp
# is.ordered(df$Date) returns FALSE

# The rows of df are not sorted as they should be as sort=TRUE
# is the default. Adding sort=TRUE does nothing.
# So try this:
# dd <- df[order(df$Date),]
# But that doesn't work.
# Nor does sort(df$Date)
# But sort(as.vector(df$Date)) does work.
# As does order(as.vector(df$Date)), so this works:
dd <- df[order(as.vector(df$Date)),]
# ?????


From ggrothendieck at gmail.com  Tue Sep 26 02:33:04 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 25 Sep 2006 20:33:04 -0400
Subject: [R] Sort problem with merge (again)
In-Reply-To: <45186FA2.6060104@fwr.on.ca>
References: <45186FA2.6060104@fwr.on.ca>
Message-ID: <971536df0609251733l6ae56ae3m404b9b5a0af62736@mail.gmail.com>

If you want it to act like a date store it as a Date:

dx <- as.Date(c("1970-04-04","1970-08-11","1970-10-18")) ###
x <- c(9,10,11)
ch <- data.frame(Date=dx,X=x)

dy <- as.Date(c("1970-06-04","1970-08-11","1970-08-18")) ###
y <- c(109,110,111)
sp <- data.frame(Date=dy,Y=y)

merge(ch, sp, all = TRUE)

By the way you might consider using zoo objects here:

library(zoo)
chz <- zoo(x, dx)
spz <- zoo(y, dy)
merge(chz, spz)

See:
vignette("zoo")

On 9/25/06, Bruce LaZerte <bdl at fwr.on.ca> wrote:
> # R version 2.3.1 (2006-06-01) Debian Linux "testing"
>
> # Is the following behaviour a bug, feature or just a lack of
> # understanding on my part? I see that this was discussed here
> # last March with no apparent resolution.
>
> d <- as.factor(c("1970-04-04","1970-08-11","1970-10-18"))
> x <- c(9,10,11)
> ch <- data.frame(Date=d,X=x)
>
> d <- as.factor(c("1970-06-04","1970-08-11","1970-08-18"))
> y <- c(109,110,111)
> sp <- data.frame(Date=d,Y=y)
>
> df <- merge(ch,sp,all=TRUE,by="Date")
> # the rows with dates missing all ch vars are tacked on the end.
> # the rows with dates missing all sp vars are sorted in with
> # the row with a date with vars from both ch and sp
> # is.ordered(df$Date) returns FALSE
>
> # The rows of df are not sorted as they should be as sort=TRUE
> # is the default. Adding sort=TRUE does nothing.
> # So try this:
> # dd <- df[order(df$Date),]
> # But that doesn't work.
> # Nor does sort(df$Date)
> # But sort(as.vector(df$Date)) does work.
> # As does order(as.vector(df$Date)), so this works:
> dd <- df[order(as.vector(df$Date)),]
> # ?????
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From gregor.gorjanc at bfro.uni-lj.si  Tue Sep 26 03:07:27 2006
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Tue, 26 Sep 2006 01:07:27 +0000 (UTC)
Subject: [R] Creating Movies with R
References: <a2b3004b0609220023ic3eb836k27c363e54416359c@mail.gmail.com>
	<45142F9C.7020606@vanderbilt.edu>
	<Pine.LNX.4.58.0609221456480.21306@penguin.rand.org>
Message-ID: <loom.20060926T030610-993@post.gmane.org>

Hello!

J.R. Lockwood <lockwood <at> rand.org> writes:
> 
> An alternative that I've used a few times is the jpg() function to
> create the sequence of images, and then converting these to an mpeg
> movie using "mencoder" distributed with "mplayer".  This works on both
> windows and linux.  I have a pretty self-contained example file
> written up that I can send to anyone who is interested.  Oddly, the
> most challenging part was creating a sequence of file names that would
> be correctly ordered - for this I use:
> 
> lex <- function(N){
>   ## produce vector of N lexicograpically ordered strings
>   ndig <- nchar(N)
>   substr(formatC((1:N)/10^ndig,digits=ndig,format="f"),3,10000000)
> }
> 

RWiki[1] would be a very nice place for such explanation. I am looking forwrad 
to it!

Gregor

[1]http://wiki.r-project.org/


From ggrothendieck at gmail.com  Tue Sep 26 03:41:41 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 25 Sep 2006 21:41:41 -0400
Subject: [R] Splitting a character variable into a numeric one and a
	character one?
In-Reply-To: <971536df0609250928p49d77d8dm5c316d672382a977@mail.gmail.com>
References: <3b9172310609250904s2e008dar5b88e3c6181a9ca0@mail.gmail.com>
	<971536df0609250928p49d77d8dm5c316d672382a977@mail.gmail.com>
Message-ID: <971536df0609251841p3d2d85bex1d008596a3597d4b@mail.gmail.com>

Here is a slight simplification of the strapply solution using simplify = TRUE

library(gsubfn)
s <- c("123abc", "12cd34", "1e23")

out <- t(strapply(s, "^([[:digit:]]+)(.*)", c, simplify = TRUE)) # matrix
data.frame(x = out[,1], num = as.numeric(out[,2]), char = out[,3])


On 9/25/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> strapply in package gsubfn can do that:
>
>
> library(gsubfn)
> s <- c("123abc", "12cd34", "1e23")
>
> out <- strapply(s, "^([[:digit:]]+)(.*)", c)
> out <- do.call(rbind, out) # as a matrix
>
> data.frame(x = out[,1], num = as.numeric(out[,2]), char = out[,3]) #
> as a data.frame
>
>
> On 9/25/06, Frank Duan <fhduan at gmail.com> wrote:
> > Hi All,
> >
> > I have a data with a variable like this:
> >
> > Column 1
> >
> > "123abc"
> > "12cd34"
> > "1e23"
> > ...
> >
> > Now I want to do an operation that can split it into two variables:
> >
> > Column 1        Column 2         Column 3
> >
> > "123abc"         123                  "abc"
> > "12cd34"         12                    "cd34"
> > "1e23"             1                      "e23"
> > ...
> >
> > So basically, I want to split the original variabe into a numeric one and a
> > character one, while the splitting element is the first character in Column
> > 1.
> >
> > I searched the forum with key words "strsplit"and "substr", but still can't
> > solve this problem. Can anyone give me some hints?
> >
> > Thanks in advance,
> >
> > FD
> >
> >        [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From AnupTyagi at yahoo.com  Tue Sep 26 05:33:04 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Tue, 26 Sep 2006 03:33:04 +0000 (UTC)
Subject: [R] Best use of LaTeX listings package for pretty printing R
	code
References: <4517DA39.4070102@vanderbilt.edu>
Message-ID: <loom.20060926T053120-314@post.gmane.org>

Frank E Harrell Jr <f.harrell <at> vanderbilt.edu> writes:

> 
> This is what I have been using.  Does anyone have a better way?  In 
> particular I would like to see letters in comment strings not stretched 
> so much.  Thanks -Frank

It may be possible to pass on all comments to a verbatim like environment inside
the listings environment, by defining and redefining the preamble to listings. I
hope it does not interfere with something else in LaTeX. Anupam.


From bolker at zoo.ufl.edu  Tue Sep 26 05:34:56 2006
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Tue, 26 Sep 2006 03:34:56 +0000 (UTC)
Subject: [R] linear terms within a nonlinear model
Message-ID: <loom.20060926T053009-507@post.gmane.org>



   I have a complicated nonlinear function, myfun(a,b,c),
that I want to fit to data, allowing one or more of the parameters
a, b, and c in turn to have linear dependence on other covariates.
In other words, I'd like to specify something like

nls(y~myfun(a,b,c),linear=list(a~f1,b~1,c~1))

  I know would this work in nlme *if I wanted to specify
random effects as well*, but I don't -- and wasn't able
to figure out how to specify a "null" random effect.
(Have looked in Pinheiro and Bates but haven't yet found
a solution ...)
I don't see how to do it in nls() or nlsList(), short of
implementing the linear structure within myfun().  
  Looked at Jim Lindsey's gnlm package but haven't yet
been able to figure it out.
   Does anyone have any ideas or tips?

  thanks
    Ben Bolker


From ritz at kvl.dk  Tue Sep 26 07:13:29 2006
From: ritz at kvl.dk (Christian Ritz)
Date: Tue, 26 Sep 2006 07:13:29 +0200
Subject: [R] linear terms within a nonlinear model
In-Reply-To: <loom.20060926T053009-507@post.gmane.org>
References: <loom.20060926T053009-507@post.gmane.org>
Message-ID: <4518B6F9.90102@kvl.dk>

Hi,

the contributed package 'drc' allows specification of non-linear regression models with 
individual parameter models that include covariates.

For an example see section 8 the accompanying paper in J. Statist. Software 
(http://www.jstatsoft.org/v12/i05/v12i05.pdf).

Christian


From virgin at seychelles.sc  Tue Sep 26 07:40:42 2006
From: virgin at seychelles.sc (virgin)
Date: Tue, 26 Sep 2006 05:40:42 GMT
Subject: [R] cauculating dissimilarities in R
Message-ID: <4518bd5a.1ee.6b87.141100242@seychelles.sc>

Dear All,
I?ve got a statistical question on calculating
dissimilarities in R.
I want to calculate the different types of dissimilarities
on the ?flower? dataset found in the package
?cluster?. Flower is a data frame with 18 observations
on 8 variables. Variable 1 and 2 are binary, variable 3 is
asymmetric binary, variable 4 is nominal, variable 5 and 6
are ordered and variable 7 and 8 are interval scaled.

Commands to load the dataset in R.
library(cluster)
data(flower)
flower


What are the different types of dissimilarities that can be
calculated on such a dataset?  
Do I need to group the types of variables first i.e. all
binary together then run the calculation?  Do I use
dissimilarity indices such as Jaccard or should it be
classification function such as ?daisy? which should be
used? 

Many thanks,

Elvina Payet (MSc)
University of La Reunion


From ripley at stats.ox.ac.uk  Tue Sep 26 08:34:24 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 26 Sep 2006 07:34:24 +0100 (BST)
Subject: [R] Sort problem with merge (again)
In-Reply-To: <45186FA2.6060104@fwr.on.ca>
References: <45186FA2.6060104@fwr.on.ca>
Message-ID: <Pine.LNX.4.64.0609260720090.14637@gannet.stats.ox.ac.uk>

On Mon, 25 Sep 2006, Bruce LaZerte wrote:

> # R version 2.3.1 (2006-06-01) Debian Linux "testing"
>
> # Is the following behaviour a bug, feature or just a lack of
> # understanding on my part? I see that this was discussed here
> # last March with no apparent resolution.

Reference?  It is the third alternative.  A factor is sorted by its codes: 
consider

> x <- factor(1:3, levels=as.character(3:1))
> x
[1] 1 2 3
Levels: 3 2 1
> sort(x)
[1] 3 2 1
Levels: 3 2 1

and that is what is happening here: for your example the levels of df$Date 
are

> levels(df$Date)
[1] "1970-04-04" "1970-08-11" "1970-10-18" "1970-06-04" "1970-08-18"

so the result is sorted correctly.

If you want to sort a character column in lexicographic order, don't make 
it into a factor. Similarly for a date column: use class "Date".

> d <- as.factor(c("1970-04-04","1970-08-11","1970-10-18"))
> x <- c(9,10,11)
> ch <- data.frame(Date=d,X=x)
>
> d <- as.factor(c("1970-06-04","1970-08-11","1970-08-18"))
> y <- c(109,110,111)
> sp <- data.frame(Date=d,Y=y)
>
> df <- merge(ch,sp,all=TRUE,by="Date")
> # the rows with dates missing all ch vars are tacked on the end.
> # the rows with dates missing all sp vars are sorted in with
> # the row with a date with vars from both ch and sp
> # is.ordered(df$Date) returns FALSE
>
> # The rows of df are not sorted as they should be as sort=TRUE
> # is the default. Adding sort=TRUE does nothing.
> # So try this:
> # dd <- df[order(df$Date),]
> # But that doesn't work.
> # Nor does sort(df$Date)
> # But sort(as.vector(df$Date)) does work.
> # As does order(as.vector(df$Date)), so this works:
> dd <- df[order(as.vector(df$Date)),]
> # ?????

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From virgin at seychelles.sc  Tue Sep 26 07:48:01 2006
From: virgin at seychelles.sc (virgin)
Date: Tue, 26 Sep 2006 05:48:01 GMT
Subject: [R] calculating dissimilarities in R
Message-ID: <4518bf11.168.6bae.1938350623@seychelles.sc>

?Dear All,
I?ve got a statistical question on calculating
dissimilarities in R.
I want to calculate the different types of dissimilarities
on the ?flower? dataset found in the package
?cluster?. Flower is a data frame with 18 observations
on 8 variables. Variable 1 and 2 are binary, variable 3 is
asymmetric binary, variable 4 is nominal, variable 5 and 6
are ordered and variable 7 and 8 are interval scaled.

Commands to load the dataset in R.
library(cluster)
data(flower)
flower


What are the different types of dissimilarities that can be
calculated on such a dataset?  
Do I need to group the types of variables first i.e. all
binary together then run the calculation?  Do I use
dissimilarity indices such as Jaccard or should it be
classification function such as ?daisy? which should be
used? 

Many thanks,

Elvina Payet (MSc)
University of La Reunion


From rrein at pooka.otago.ac.nz  Tue Sep 26 00:14:46 2006
From: rrein at pooka.otago.ac.nz (rrein)
Date: Tue, 26 Sep 2006 10:14:46 +1200
Subject: [R] Different results in agnes and hclust
Message-ID: <E1GRyin-0000kE-IC@galadriel.otago.ac.nz>

Hello to everybody,

I have a question regarding the results obtained from the hclust and the
agnes funtion using the ward algorithm because they seem to differ from each
other. I also ran a cluster analysis using the ward algorithm in Matlab and
obtained the same results as from agnes.
I'm using the pvclust package in order to confirm the clustering results
which internally uses the hclust function. Therefore I'm not too shure what
to do with the results. This problem doesn't appear when using the average
algorithm.

Regards
Robert Rein


From feis at ohsu.edu  Sun Sep 24 07:04:18 2006
From: feis at ohsu.edu (Suzi Fei)
Date: Sat, 23 Sep 2006 22:04:18 -0700
Subject: [R] printing a variable name in a for loop
Message-ID: <000c01c6df96$e49e5cf0$0400a8c0@suzilaptop>

Hello,

How do you print a variable name in a for loop?

I'm trying to construct a csv file that looks like this:


	Hello, variable1, value_of_variable1, World,
	Hello, variable2, value_of_variable2, World,
	Hello, variable3, value_of_variable3, World,


Using this:

	for (variable in list(variable1, variable2, variable3)){

		cat("Hello,", ???variable???, variable, ", World,")
	}

This works fine if I'm trying to print the VALUE of variable, but I want to
print the NAME of variable as well.

Thanks,
Suzi


From henrigel at gmx.de  Mon Sep 25 14:55:04 2006
From: henrigel at gmx.de (henrigel at gmx.de)
Date: Mon, 25 Sep 2006 14:55:04 +0200
Subject: [R] rpart
Message-ID: <20060925125504.19430@gmx.net>

Dear r-help-list:

If I use the rpart method like

cfit<-rpart(y~.,data=data,...),

what kind of tree is stored in cfit?
Is it right that this tree is not pruned at all, that it is the full tree?

If so, it's up to me to choose a subtree by using the printcp method.
In the technical report from Atkinson and Therneau "An Introduction to recursive partitioning using the rpart routines" from 2000, one can see the following table on page 15:

      CP  nsplit  relerror  xerror   xstd
1   0.105   0     1.00000   1.0000   0.108
2   0.056   3     0.68519   1.1852   0.111
3   0.028   4     0.62963   1.0556   0.109
4   0.574   6     0.57407   1.0556   0.109
5   0.100   7     0.55556   1.0556   0.109

Some lines below it says "We see that the best tree has 5 terminal nodes (4 splits). Why that if the xerror is the lowest for the tree only consisting of the root?

Thank you very much for your help

Henri 
--


From randsnews at gmail.com  Tue Sep 26 08:12:10 2006
From: randsnews at gmail.com (S.Q. WEN)
Date: Mon, 25 Sep 2006 23:12:10 -0700
Subject: [R] About the display of matrix
Message-ID: <8b340d720609252312w40ce1d22j662592ec5978e37b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060925/5e587970/attachment.pl 

From zhpan99 at yahoo.com  Tue Sep 26 02:09:25 2006
From: zhpan99 at yahoo.com (Pan Zheng)
Date: Mon, 25 Sep 2006 17:09:25 -0700 (PDT)
Subject: [R] venn diagram with more than three vectors
Message-ID: <20060926000925.64838.qmail@web54702.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060925/c3fdc745/attachment.pl 

From gkerns at ysu.edu  Tue Sep 26 00:27:10 2006
From: gkerns at ysu.edu (G. Jay Kerns)
Date: Mon, 25 Sep 2006 18:27:10 -0400
Subject: [R] [R-pkgs] the IPSUR package
Message-ID: <a695148b0609251527r4464776cy65f15c353721a78@mail.gmail.com>

Dear useRs,

We are pleased to announce the preliminary release of the IPSUR package.

The primary audience was originally envisioned to be upper division
undergraduate mathematics/statistics/engineering majors, but other useRs may
find this material useful.

In a nutshell, this package slightly modifies and adds selected
functionality to the R Commander by John Fox.  The changes were meant to
customize Rcmdr for our Statistics classes, populated for the most part by
the audience above.  Some clever functions written by John Verzani were
translated to IPSUR from UsingR.

Downloads for the package (while the CRAN submission is pending) are at

http://www.cc.ysu.edu/~gjkerns/IPSUR/package/index.htm

Check out the "Features" page to see what the package offers.

http://www.cc.ysu.edu/~gjkerns/IPSUR/package/features.htm

Full credit must be given to John Fox, together with his diverse team of
dedicated contributors.  Indeed, without all of their countless hours of
effort the IPSUR package would not be possible.  Kudos to them for providing
excellent software to the R community.

Cheers,
Jay




***********************************************
G. Jay Kerns, Ph.D.
Department of Mathematics & Statistics
Youngstown State University
Youngstown, OH 44555-0002 USA
Office: 1035 Cushwa Hall
Phone: (330) 941-3310 Office (voice mail)
-3302 Department
-3170 FAX
E-mail: gkerns at ysu.edu
http://www.cc.ysu.edu/~gjkerns/

	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From gunther.hoening at ukmainz.de  Tue Sep 26 09:47:08 2006
From: gunther.hoening at ukmainz.de (=?iso-8859-1?Q?Gunther_H=F6ning?=)
Date: Tue, 26 Sep 2006 09:47:08 +0200
Subject: [R] Accessing C- source code of R
In-Reply-To: <20060925125504.19430@gmx.net>
Message-ID: <006301c6e13f$f7bb3bf0$0f1e0b0a@3med.klinik.unimainz.de>

Dear list,

I'm looking for the sources code of parts of R, (e.g. spline).
Does anyone know where I can access it ?

Gunther


From maechler at stat.math.ethz.ch  Tue Sep 26 09:55:50 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 26 Sep 2006 09:55:50 +0200
Subject: [R] calculating dissimilarities in R
In-Reply-To: <4518bf11.168.6bae.1938350623@seychelles.sc>
References: <4518bf11.168.6bae.1938350623@seychelles.sc>
Message-ID: <17688.56582.429049.229899@stat.math.ethz.ch>

Hi Elvina,

>>>>> "Elvina" == Elvina Payet <virgin at seychelles.sc>
>>>>>     on Tue, 26 Sep 2006 05:48:01 GMT writes:

    Elvina> ,A (BDear All,
    Elvina> I?ve got a statistical question on calculating
    Elvina> dissimilarities in R.
    Elvina> I want to calculate the different types of dissimilarities
    Elvina> on the ?flower? dataset found in the package
    Elvina> ?cluster?. Flower is a data frame with 18 observations
    Elvina> on 8 variables. Variable 1 and 2 are binary, variable 3 is
    Elvina> asymmetric binary, variable 4 is nominal, variable 5 and 6
    Elvina> are ordered and variable 7 and 8 are interval scaled.

    Elvina> Commands to load the dataset in R.

      > library(cluster)
      > data(flower)

or  data(flower, package = "cluster")


    Elvina> What are the different types of dissimilarities that can be
    Elvina> calculated on such a dataset?  
    Elvina> Do I need to group the types of variables first i.e. all
    Elvina> binary together then run the calculation?  Do I use
    Elvina> dissimilarity indices such as Jaccard or should it be
    Elvina> classification function such as ?daisy? which should be
    Elvina> used? 


Yes, you should use  daisy() to calculate dissimilarities,
particularly when you are interested in the difference between
symmetric and asymmetric binary.

Do read  help(daisy)  and look at its examples.

Maybe this will answer all your questions or then it will help
you to ask a much more specific question as suggested by the
posting guide (see link below!)

      [.........]

    virgin> ______________________________________________

      [.........]
    virgin> PLEASE do read the posting guide

    virgin> http://www.R-project.org/posting-guide.html 
	    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

    virgin> and provide commented, minimal, self-contained, reproducible code.

Regards,
Martin Maechler, ETH Zurich


From J.Oosting at lumc.nl  Tue Sep 26 10:02:55 2006
From: J.Oosting at lumc.nl (Oosting, J. (PATH))
Date: Tue, 26 Sep 2006 10:02:55 +0200
Subject: [R] venn diagram with more than three vectors
In-Reply-To: <20060926000925.64838.qmail@web54702.mail.yahoo.com>
Message-ID: <40964D91251A18469CB08B6E96FB30BABAD8F0@mailc.lumcnet.prod.intern>

I am not aware of existing functions to draw venn diagrams with more
than 3 sets, but you could have a look at
http://en.wikipedia.org/wiki/Venn_diagram to see how these can be
constructed.

Jan Oosting 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Pan Zheng
Sent: dinsdag 26 september 2006 2:09
To: r-help at stat.math.ethz.ch
Subject: [R] venn diagram with more than three vectors

Hi,
   
  I am using venn diagram function in AMDA to plot the venn diagram. But
it seems in this function, it can only plot 3 or less vectors. Is there
a way to plot the venn diagram with more than 3 vectors?
   
  Please help.
   
  Thanks.
   
  Z


From rfrancois at mango-solutions.com  Tue Sep 26 10:05:52 2006
From: rfrancois at mango-solutions.com (Romain Francois)
Date: Tue, 26 Sep 2006 09:05:52 +0100
Subject: [R] Creating Movies with R
In-Reply-To: <Pine.LNX.4.58.0609221456480.21306@penguin.rand.org>
References: <a2b3004b0609220023ic3eb836k27c363e54416359c@mail.gmail.com>	<45142F9C.7020606@vanderbilt.edu>
	<Pine.LNX.4.58.0609221456480.21306@penguin.rand.org>
Message-ID: <4518DF60.3080707@mango-solutions.com>

J.R. Lockwood wrote:
> An alternative that I've used a few times is the jpg() function to
> create the sequence of images, and then converting these to an mpeg
> movie using "mencoder" distributed with "mplayer".  This works on both
> windows and linux.  I have a pretty self-contained example file
> written up that I can send to anyone who is interested.  Oddly, the
> most challenging part was creating a sequence of file names that would
> be correctly ordered - for this I use:
>
> lex <- function(N){
>   ## produce vector of N lexicograpically ordered strings
>   ndig <- nchar(N)
>   substr(formatC((1:N)/10^ndig,digits=ndig,format="f"),3,10000000)
> }
>   
Hi,

Or you could have asked the `filename` argument of `jpeg` to do the job 
for you, ie :
filename = "something%04d" as documented in ?jpeg

jpeg(filename = "something%04.jpg", onefile = FALSE)
for(i in 1:10){
  plot(i)
}

Cheers,

Romain

PS : For those who have ideas of "movies", I once started a website "R 
Movies Gallery" as a little sister of "R Graph(ics) Gallery" ... you may 
want to send me code to populate the website, or populate the wiki with 
such examples. The idea is not to produce pretty science fiction type 
movies with R, but use R abilities to create some useful animation that 
could highlight some statistical concepts such as the LCT, ...
Plus, there's a place where grid could show its full power.

> On Fri, 22 Sep 2006, Jeffrey Horner wrote:
>
>   
>> Date: Fri, 22 Sep 2006 13:46:52 -0500
>> From: Jeffrey Horner <jeff.horner at vanderbilt.edu>
>> To: Lorenzo Isella <lorenzo.isella at gmail.com>, r-help at stat.math.ethz.ch
>> Subject: Re: [R] Creating Movies with R
>>
>> If you run R on Linux, then you can run the ImageMagick command called 
>> convert. I place this in an R function to use a sequence of PNG plots as 
>> movie frames:
>>
>> make.mov.plotcol3d <- function(){
>>      unlink("plotcol3d.mpg")
>>      system("convert -delay 10 plotcol3d*.png plotcol3d.mpg")
>> }
>>
>> Examples can be seen here:
>>
>> http://biostat.mc.vanderbilt.edu/JrhRgbColorSpace
>>
>> Look for the 'Download Movie' links.
>>
>> Cheers,
>>
>> Jeff
>>
>> Lorenzo Isella wrote:
>>     
>>> Dear All,
>>>
>>> I'd like to know if it is possible to create animations with R.
>>> To be specific, I attach a code I am using for my research to plot
>>> some analytical results in 3D using the lattice package. It is not
>>> necessary to go through the code.
>>> Simply, it plots some 3D density profiles at two different times
>>> selected by the user.
>>> I wonder if it is possible to use the data generated for different
>>> times to create something like an .avi file.
>>>
>>> Here is the script:
>>>
>>> rm(list=ls())
>>> library(lattice)
>>>
>>> # I start defining the analytical functions needed to get the density
>>> as a function of time
>>>
>>> expect_position <- function(t,lam1,lam2,pos_ini,vel_ini)
>>> {1/(lam1-lam2)*(lam1*exp(lam2*t)-lam2*exp(lam1*t))*pos_ini+
>>> 1/(lam1-lam2)*(exp(lam1*t)-exp(lam2*t))*vel_ini
>>> }
>>>
>>> sigma_pos<-function(t,q,lam1,lam2)
>>> {
>>> q/(lam1-lam2)^2*(
>>> (exp(2*lam1*t)-1)/(2*lam1)-2/(lam1+lam2)*(exp(lam1*t+lam2*t)-1) +
>>> (exp(2*lam2*t)-1)/(2*lam2) )
>>> }
>>>
>>> rho_x<-function(x,expect_position,sigma_pos)
>>> {
>>> 1/sqrt(2*pi*sigma_pos)*exp(-1/2*(x-expect_position)^2/sigma_pos)
>>> }
>>>
>>> #### Now the physical parameters
>>> tau<-0.1
>>> beta<-1/tau
>>> St<-tau ### since I am in dimensionless units and tau is already in
>>> units of 1/|alpha|
>>> D=2e-2
>>> q<-2*beta^2*D
>>> ############### Now the grid in space and time
>>> time<-5  # time extent
>>> tsteps<-501 # time steps
>>> newtime<-seq(0,time,len=tsteps)
>>> #### Now the things specific for the dynamics along x
>>> lam1<- -beta/2*(1+sqrt(1+4*St))
>>> lam2<- -beta/2*(1-sqrt(1+4*St))
>>> xmin<- -0.5
>>> xmax<-0.5
>>> x0<-0.1
>>> vx0<-x0
>>> nx<-101 ## grid intervals along x
>>> newx<-seq(xmin,xmax,len=nx) # grid along x
>>>
>>> # M1 <- do.call("g", c(list(x = newx), mypar))
>>>
>>>
>>> mypar<-c(q,lam1,lam2)
>>> sig_xx<-do.call("sigma_pos",c(list(t=newtime),mypar))
>>> mypar<-c(lam1,lam2,x0,vx0)
>>> exp_x<-do.call("expect_position",c(list(t=newtime),mypar))
>>>
>>> #rho_x<-function(x,expect_position,sigma_pos)
>>>
>>> #NB: at t=0, the density blows up, since I have a delta as the initial state!
>>> # At any t>0, instead, the result is finite.
>>> #for this reason I now redefine time by getting rid of the istant t=0
>>> to work out
>>> # the density
>>>
>>>
>>> rho_x_t<-matrix(ncol=nx,nrow=tsteps-1)
>>> for (i in 2:tsteps)
>>> {mypar<-c(exp_x[i],sig_xx[i])
>>> myrho_x<-do.call("rho_x",c(list(x=newx),mypar))
>>> rho_x_t[ i-1, ]<-myrho_x
>>> }
>>>
>>> ### Now I also define a scaled density
>>>
>>> rho_x_t_scaled<-matrix(ncol=nx,nrow=tsteps-1)
>>> for (i in 2:tsteps)
>>> {mypar<-c(exp_x[i],sig_xx[i])
>>> myrho_x<-do.call("rho_x",c(list(x=newx),mypar))
>>> rho_x_t_scaled[ i-1, ]<-myrho_x/max(myrho_x)
>>> }
>>>
>>> ###########Now I deal with the dynamics along y
>>>
>>> lam1<- -beta/2*(1+sqrt(1-4*St))
>>> lam2<- -beta/2*(1-sqrt(1-4*St))
>>> ymin<- 0
>>> ymax<- 1
>>> y0<-ymax
>>> vy0<- -y0
>>>
>>> mypar<-c(q,lam1,lam2)
>>> sig_yy<-do.call("sigma_pos",c(list(t=newtime),mypar))
>>> mypar<-c(lam1,lam2,y0,vy0)
>>> exp_y<-do.call("expect_position",c(list(t=newtime),mypar))
>>>
>>>
>>> # now I introduce the function giving the density along y: this has to
>>> include the BC of zero
>>> # density at wall
>>>
>>> rho_y<-function(y,expect_position,sigma_pos)
>>> {
>>> 1/sqrt(2*pi*sigma_pos)*exp(-1/2*(y-expect_position)^2/sigma_pos)-
>>> 1/sqrt(2*pi*sigma_pos)*exp(-1/2*(y+expect_position)^2/sigma_pos)
>>> }
>>>
>>> newy<-seq(ymin,ymax,len=nx) # grid along y with the same # of points
>>> as the one along x
>>>
>>>
>>> rho_y_t<-matrix(ncol=nx,nrow=tsteps-1)
>>> for (i in 2:tsteps)
>>> {mypar<-c(exp_y[i],sig_yy[i])
>>> myrho_y<-do.call("rho_y",c(list(y=newy),mypar))
>>> rho_y_t[ i-1, ]<-myrho_y
>>> }
>>>
>>> rho_y_t_scaled<-matrix(ncol=nx,nrow=tsteps-1)
>>> for (i in 2:tsteps)
>>> {mypar<-c(exp_y[i],sig_yy[i])
>>> myrho_y<-do.call("rho_y",c(list(y=newy),mypar))
>>> rho_y_t_scaled[ i-1, ]<-myrho_y/max(myrho_y)
>>> }
>>>
>>>
>>> # The following 2 plots are an example of the plots I'd like to use to
>>> make an animation
>>>
>>>
>>> g <- expand.grid(x = newx, y = newy)
>>>
>>> instant<-100
>>> mydens<-rho_x_t[ instant, ]%o%rho_y_t[ instant, ]/(max(rho_x_t[
>>> instant, ]%o%rho_y_t[ instant, ]))
>>>
>>>
>>> lentot<-nx^2
>>> dim(mydens)<-c(lentot,1)
>>>
>>> g$z<-mydens
>>> jpeg("dens-t-3.jpeg")
>>> print(wireframe(z ~ x * y, g, drape = TRUE,shade=TRUE,
>>> scales = list(arrows = FALSE),pretty=FALSE, aspect = c(1,1), colorkey = TRUE
>>> ,zoom=0.8, main=expression("Density at t=2"), zlab =
>>> list(expression("density"),rot = 90),distance=0.0,
>>> perspective=TRUE,#screen = list(z = 150, x = -55,y= 0)
>>> ,zlim=range(c(0,1))))
>>> dev.off()
>>>
>>>
>>> instant<-300
>>> mydens<-rho_x_t[ instant, ]%o%rho_y_t[ instant, ]/(max(rho_x_t[
>>> instant, ]%o%rho_y_t[ instant, ]))
>>>
>>>
>>> lentot<-nx^2
>>> dim(mydens)<-c(lentot,1)
>>>
>>> g$z<-mydens
>>> jpeg("dens-t-3.jpeg")
>>> print(wireframe(z ~ x * y, g, drape = TRUE,shade=TRUE,
>>> scales = list(arrows = FALSE),pretty=FALSE, aspect = c(1,1), colorkey = TRUE
>>> ,zoom=0.8, main=expression("Density at t=3"), zlab =
>>> list(expression("density"),rot = 90),distance=0.0,
>>> perspective=TRUE,#screen = list(z = 150, x = -55,y= 0)
>>> ,zlim=range(c(0,1))))
>>> dev.off()
>>>
>>>
>>>
>>>
>>> Kind Regards
>>>
>>> Lorenzo
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>       
>> -- 
>> http://biostat.mc.vanderbilt.edu/JeffreyHorner
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>     
>
> J.R. Lockwood
> 412-683-2300 x4941
> lockwood at rand.org
> http://www.rand.org/statistics/bios/
>
> --------------------
>
> This email message is for the sole use of the intended recip...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>   


-- 
*mangosolutions*
/data analysis that delivers/

Tel   +44 1249 467 467
Fax   +44 1249 467 468


From ligges at statistik.uni-dortmund.de  Tue Sep 26 10:13:20 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 26 Sep 2006 10:13:20 +0200
Subject: [R] Accessing C- source code of R
In-Reply-To: <006301c6e13f$f7bb3bf0$0f1e0b0a@3med.klinik.unimainz.de>
References: <006301c6e13f$f7bb3bf0$0f1e0b0a@3med.klinik.unimainz.de>
Message-ID: <4518E120.6060608@statistik.uni-dortmund.de>



Gunther H?ning wrote:
> Dear list,
> 
> I'm looking for the sources code of parts of R, (e.g. spline).
> Does anyone know where I can access it ?

I plan to write a corresponding R Help Desk article on "Accessing the 
source". A draft is available from:
http://www.statistik.uni-dortmund.de/~ligges/R_Help_Desk_preview.pdf

Can you please tell me if this description is sufficient?

Thanks,
Uwe Ligges


> Gunther
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From R.Nieuwenhuis at student.ru.nl  Tue Sep 26 10:21:24 2006
From: R.Nieuwenhuis at student.ru.nl (Rense Nieuwenhuis)
Date: Tue, 26 Sep 2006 10:21:24 +0200
Subject: [R] Statistical data and Map-package
Message-ID: <169DD2BA-9983-4632-8641-3B8C7A548B3C@student.ru.nl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060926/24c07dfe/attachment.pl 

From maechler at stat.math.ethz.ch  Tue Sep 26 10:28:41 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 26 Sep 2006 10:28:41 +0200
Subject: [R] About the display of matrix
In-Reply-To: <8b340d720609252312w40ce1d22j662592ec5978e37b@mail.gmail.com>
References: <8b340d720609252312w40ce1d22j662592ec5978e37b@mail.gmail.com>
Message-ID: <17688.58553.298500.777683@stat.math.ethz.ch>

>>>>> "SQW" == S Q WEN <randsnews at gmail.com>
>>>>>     on Mon, 25 Sep 2006 23:12:10 -0700 writes:

    SQW> For a matrix A, i don't want to display the zero
    SQW> elements in it , How to do with that?

(Using a fairly recent version of R)
Either use as.table() and use the print() method for "table"
explicitly, or,
if you are really working with sparse matrices, use the 'Matrix'
package:

> set.seed(1); m <- matrix(rpois(80, lambda=.8), 8,10);m
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
[1,]    0    1    1    0    1    2    1    0    1     0
[2,]    0    0    4    0    0    1    1    1    0     0
[3,]    1    0    0    0    2    1    1    1    1     1
[4,]    2    0    1    0    1    1    2    0    1     2
[5,]    0    1    2    2    1    1    0    2    0     2
[6,]    2    0    0    0    0    1    0    0    2     0
[7,]    2    1    1    1    1    0    0    1    0     1
[8,]    1    1    0    1    0    1    0    0    2     3
> print(as.table(m), zero = ".")
  A B C D E F G H I J
A . 1 1 . 1 2 1 . 1 .
B . . 4 . . 1 1 1 . .
C 1 . . . 2 1 1 1 1 1
D 2 . 1 . 1 1 2 . 1 2
E . 1 2 2 1 1 . 2 . 2
F 2 . . . . 1 . . 2 .
G 2 1 1 1 1 . . 1 . 1
H 1 1 . 1 . 1 . . 2 3

> library(Matrix)
Loading required package: lattice
> M <- Matrix(m, sparse = TRUE)
> M
8 x 10 sparse Matrix of class "dgCMatrix"
                        
[1,] . 1 1 . 1 2 1 . 1 .
[2,] . . 4 . . 1 1 1 . .
[3,] 1 . . . 2 1 1 1 1 1
[4,] 2 . 1 . 1 1 2 . 1 2
[5,] . 1 2 2 1 1 . 2 . 2
[6,] 2 . . . . 1 . . 2 .
[7,] 2 1 1 1 1 . . 1 . 1
[8,] 1 1 . 1 . 1 . . 2 3
> 

---

Martin Maechler, ETH Zurich


From mirko.sanpietrucci at email.it  Tue Sep 26 10:26:04 2006
From: mirko.sanpietrucci at email.it (mirko sanpietrucci)
Date: Tue, 26 Sep 2006 10:26:04 +0200
Subject: [R] Voung test implementation in R
Message-ID: <0f0101c6e145$691366b0$99e1cec1@dms.unina.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060926/c6de3a85/attachment.pl 

From gavin.simpson at ucl.ac.uk  Tue Sep 26 10:47:35 2006
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Tue, 26 Sep 2006 09:47:35 +0100
Subject: [R] venn diagram with more than three vectors
In-Reply-To: <40964D91251A18469CB08B6E96FB30BABAD8F0@mailc.lumcnet.prod.intern>
References: <40964D91251A18469CB08B6E96FB30BABAD8F0@mailc.lumcnet.prod.intern>
Message-ID: <1159260455.2883.11.camel@dhcppc2.my.nat.localnet>

On Tue, 2006-09-26 at 10:02 +0200, Oosting, J. (PATH) wrote:
> I am not aware of existing functions to draw venn diagrams with more
> than 3 sets, but you could have a look at
> http://en.wikipedia.org/wiki/Venn_diagram to see how these can be
> constructed.
> 
> Jan Oosting 

Package vegan has a function (varpart) and plot method that will draw
venn diagrams with up to 4 sets. It works on results from redundancy
analyses, but you could probably adapt it to your needs.

HTH

G

> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Pan Zheng
> Sent: dinsdag 26 september 2006 2:09
> To: r-help at stat.math.ethz.ch
> Subject: [R] venn diagram with more than three vectors
> 
> Hi,
>    
>   I am using venn diagram function in AMDA to plot the venn diagram. But
> it seems in this function, it can only plot 3 or less vectors. Is there
> a way to plot the venn diagram with more than 3 vectors?
>    
>   Please help.
>    
>   Thanks.
>    
>   Z
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 *Note new Address and Fax and Telephone numbers from 10th April 2006*
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [t] +44 (0)20 7679 0522
ECRC                              [f] +44 (0)20 7679 0565
UCL Department of Geography
Pearson Building                  [e] gavin.simpsonATNOSPAMucl.ac.uk
Gower Street
London, UK                        [w] http://www.ucl.ac.uk/~ucfagls/cv/
WC1E 6BT                          [w] http://www.ucl.ac.uk/~ucfagls/
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From mothsailor at googlemail.com  Tue Sep 26 10:51:14 2006
From: mothsailor at googlemail.com (David Barron)
Date: Tue, 26 Sep 2006 09:51:14 +0100
Subject: [R] printing a variable name in a for loop
In-Reply-To: <000c01c6df96$e49e5cf0$0400a8c0@suzilaptop>
References: <000c01c6df96$e49e5cf0$0400a8c0@suzilaptop>
Message-ID: <815b70590609260151y539792b1w81d30e86e14d2388@mail.gmail.com>

This would do it:

> v1 <- 5
> v2 <- 6
> v3 <- 7

> vns <- paste("v",1:3,sep="")
> for (i in 1:length(vns)) cat("Hello", vns[i], get(vns[i]), "World\n", sep=",")

Hello,v1,5,World
Hello,v2,6,World
Hello,v3,7,World


On 24/09/06, Suzi Fei <feis at ohsu.edu> wrote:
> Hello,
>
> How do you print a variable name in a for loop?
>
> I'm trying to construct a csv file that looks like this:
>
>
>         Hello, variable1, value_of_variable1, World,
>         Hello, variable2, value_of_variable2, World,
>         Hello, variable3, value_of_variable3, World,
>
>
> Using this:
>
>         for (variable in list(variable1, variable2, variable3)){
>
>                 cat("Hello,", ???variable???, variable, ", World,")
>         }
>
> This works fine if I'm trying to print the VALUE of variable, but I want to
> print the NAME of variable as well.
>
> Thanks,
> Suzi
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From ripley at stats.ox.ac.uk  Tue Sep 26 10:56:53 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 26 Sep 2006 09:56:53 +0100 (BST)
Subject: [R] rpart
In-Reply-To: <20060925125504.19430@gmx.net>
References: <20060925125504.19430@gmx.net>
Message-ID: <Pine.LNX.4.64.0609260931130.16320@gannet.stats.ox.ac.uk>

On Mon, 25 Sep 2006, henrigel at gmx.de wrote:

> Dear r-help-list:
>
> If I use the rpart method like
>
> cfit<-rpart(y~.,data=data,...),
>
> what kind of tree is stored in cfit?
> Is it right that this tree is not pruned at all, that it is the full tree?

It is an rpart object.  This contains both the tree and the instructions 
for pruning it at all values of cp: note that cp is also used in deciding 
how large a tree to grow.

> If so, it's up to me to choose a subtree by using the printcp method.

Or the plotcp method.

> In the technical report from Atkinson and Therneau "An Introduction to 
> recursive partitioning using the rpart routines" from 2000, one can see 
> the following table on page 15:
>
>      CP  nsplit  relerror  xerror   xstd
> 1   0.105   0     1.00000   1.0000   0.108
> 2   0.056   3     0.68519   1.1852   0.111
> 3   0.028   4     0.62963   1.0556   0.109
> 4   0.574   6     0.57407   1.0556   0.109
> 5   0.100   7     0.55556   1.0556   0.109
>
> Some lines below it says "We see that the best tree has 5 terminal nodes 
> (4 splits). Why that if the xerror is the lowest for the tree only 
> consisting of the root?

There are *two* reports with that name: this seems to be from minitech.ps.
The choice is explained in the rest of that para (the 1-SE rule was used).
My guess is that the authors excluded the root as not being a tree, but 
only they can answer that.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Roger.Bivand at nhh.no  Tue Sep 26 11:12:59 2006
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue, 26 Sep 2006 11:12:59 +0200 (CEST)
Subject: [R] Statistical data and Map-package
In-Reply-To: <169DD2BA-9983-4632-8641-3B8C7A548B3C@student.ru.nl>
Message-ID: <Pine.LNX.4.44.0609261057251.27649-100000@reclus.nhh.no>

On Tue, 26 Sep 2006, Rense Nieuwenhuis wrote:

> Dear helpeRs,
> 
> I'm working with the map-package and came upon a problem which I  
> couldn't solve. I hope onee of you can. If not, this can be seen as a  
> suggestion for new versions of the package.
> 
> I'm trying to create a map of some European countries, filled with  
> colors corresponding to some values. Let's say I have the following  
> countries and I assign the following colors (fictional):
> 
> country2001 <- c("Austria", "Belgium", "Switzerland",  
> "Czechoslovakia", "Germany", "Denmark", "Spain", "Finland", "France",  
> "UK", "Greece", "Hungary", "Ireland", "Israel", "Italy",  
> "Luxembourg", "Netherlands", "Norway", "Poland", "Portugal",  
> "Sweden", "Slovenia")
> color2001 <- c("green", "yellow","red","red", "red", "red", "red",  
> "red", "green", "red", "red", "red", "red", "red", "red", "red",  
> "red", "blue", "red", "red", "red", "orange")
> 
> I then let the colors and the values correspond using 'match.map',  
> like this:
> 
> match <- match.map("world",country2001)
> color <- color2001[match]
> 
> And finally I plot the map. It works perfectly fine.
> 
> map(database="world", fill=TRUE, col=color)
> 
> 
> But as I mentioned, I want to create a map of Europe. So, I use xlim  
> and ylim to let some parts of the world fall of the map. The syntax  
> becomes like this:
> 
> map(database="world", fill=TRUE, col=color, xlim=c(-25,70),ylim=c 
> (35,71))
> 
> Now, a problem arises. The regions on the map are colored by the  
> vector 'color'. It needs therefore to correspond to the order in  
> which the polygons are drawn. Since some of the full world-map isn't  
> drawn this time, the color-vector doesn't correspond anymore. This  
> results in the coloring of the wrong countries.
> 
> Does anybody know of a way to solve this?

Within the maps package:

europe <- map(database="world", fill=TRUE, plot=FALSE, 
  xlim=c(-25,70),ylim=c(35,71))
match <- match.map(europe,country2001)
color <- color2001[match]
map(database="world", fill=TRUE, col=color, xlim=c(-25,70),ylim=c(35,71))

but I'm afraid the "world" database precedes the dissolution of the Soviet 
Union, Czechoslovakia, and Yugoslavia, and doesn't code Sicily or Sardinia 
in Italy, so the result is perhaps not yet what you need:

europe$names[grep("Sicily", europe$names)] <- "Italy:Sicily"
europe$names[grep("Sardinia", europe$names)] <- "Italy:Sardinia"
match <- match.map(europe,country2001)
color <- color2001[match]
map(database="world", fill=TRUE, col=color, xlim=c(-25,70),ylim=c(35,71))

deals with Italy, but you won't get Slovenia. There was a discussion about 
this on the R-sig-geo list in March this year starting here:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/78303.html

or equivalently:

http://article.gmane.org/gmane.comp.lang.r.geo/299


> 
> Thanks very much in advance,
> 
> Rense Nieuwenhuis
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From juscio at gmail.com  Tue Sep 26 11:25:56 2006
From: juscio at gmail.com (laba diena)
Date: Tue, 26 Sep 2006 02:25:56 -0700
Subject: [R] Need help with boxplots
Message-ID: <9b3787f20609260225p1ab32760hb2bde3ca38439dfb@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060926/5c6b887e/attachment.pl 

From wl at eimb.ru  Tue Sep 26 11:32:42 2006
From: wl at eimb.ru (Vladimir Eremeev)
Date: Tue, 26 Sep 2006 13:32:42 +0400
Subject: [R] package usage statistics.
Message-ID: <1768655942.20060926133242@eimb.ru>

Dear useRs,

  Is it possible to get the R package usage statistics?
  That is, does R contain any tools to estimate which packages were
  used and how often?

  I am going to temporary change the workplace and packing the data
  and their processing scripts on my computer in order to continue my
  projects.

  During my work on the current workplace I periodically have had installed
  new R packages, have investigated them and used them in my work or did
  not used them, depending on their functionality.

  Now I am thinking about writing an R script which will automatically
  download and install everything I need from the R repository.
  So, I need a list of packages I have used in R.
  
  The first solution in my head is to scan all disks for R
  scripts and .Rhistory files, extract calls for "library" from them
  and save names of loaded packages.

  I would appreciate other variants.
  

---
Best regards,
Vladimir                mailto:wl at eimb.ru


From ripley at stats.ox.ac.uk  Tue Sep 26 11:58:19 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 26 Sep 2006 10:58:19 +0100 (BST)
Subject: [R] glmmPQL in 2.3.1
In-Reply-To: <6.2.3.4.2.20060925083827.03b49678@cyrus.psych.uiuc.edu>
References: <6.2.3.4.2.20060925083827.03b49678@cyrus.psych.uiuc.edu>
Message-ID: <Pine.LNX.4.64.0609251607020.14064@gannet.stats.ox.ac.uk>

On Mon, 25 Sep 2006, Justin Rhodes wrote:

> Dear R-help,
>
> I recently tried implementing glmmPQL in 2.3.1,

I thought *I* had implemented it: are you talking about my function in 
package MASS or your own implementation?

> and I discovered a few differences as compared to 2.2.1.

You appear to be talking about contributed packages (MASS, and glmmPQL 
also depends on nlme) without giving their version numbers.

> I am fitting a regression with fixed and random effects with Gamma error 
> structure.  First, 2.3.1 gives different estimates than 2.2.1, and 
> 2.3.1, takes more iterations to converge.

We have no idea, given the lack of reproducible example.  glmmPQL does 
give the same answers as before for the book examples for which it is 
support software.  This may well be due to an underlying change in nlme.

> Second, when I try using the anova function it says, "'anova' is not 
> available for PQL fits", why?  Any help would be greatly appreciated.

Because anova implies you are using an optimization criterion, such as 
least squares or maximum likelihood, and so there is something like a 
deviance to partition.  It was not used in the book with glmmPQL supports, 
but it seems some people were using glmmPQL without reference to that book 
so I made a number of their misuses explicit errors.  This *is* in the 
NEWS and WHATS.NEWS files for MASS and VR:

- There are anova() and logLik() methods for class "glmmPQL" to stop
   misuse.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ronggui.huang at gmail.com  Tue Sep 26 12:06:35 2006
From: ronggui.huang at gmail.com (ronggui)
Date: Tue, 26 Sep 2006 18:06:35 +0800
Subject: [R] Voung test implementation in R
In-Reply-To: <0f0101c6e145$691366b0$99e1cec1@dms.unina.it>
References: <0f0101c6e145$691366b0$99e1cec1@dms.unina.it>
Message-ID: <38b9f0350609260306t61477483i6726c6102b109d26@mail.gmail.com>

Yes, the pscl package contains that function.
>library(pscl)
>?vuong

Description

Compares two models fit to the same data that do not nest via Vuong's
non-nested test.

Usage

vuong(m1, m2, digits = getOption("digits"))


On 9/26/06, mirko sanpietrucci <mirko.sanpietrucci at email.it> wrote:
> Dear All,
> I would like to know if the Voung test (Voung; Econometrica, 1989) to compare two non-nested regression models has been implemented in R.
> Thanks in advance for your assistance,
> mirko
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
??????
Department of Sociology
Fudan University


From jim at bitwrit.com.au  Wed Sep 27 02:14:05 2006
From: jim at bitwrit.com.au (Jim Lemon)
Date: Tue, 26 Sep 2006 20:14:05 -0400
Subject: [R] printing a variable name in a for loop
In-Reply-To: <000c01c6df96$e49e5cf0$0400a8c0@suzilaptop>
References: <000c01c6df96$e49e5cf0$0400a8c0@suzilaptop>
Message-ID: <4519C24D.7000200@bitwrit.com.au>

Suzi Fei wrote:
> Hello,
> 
> How do you print a variable name in a for loop?
> 
> I'm trying to construct a csv file that looks like this:
> 
> 
> 	Hello, variable1, value_of_variable1, World,
> 	Hello, variable2, value_of_variable2, World,
> 	Hello, variable3, value_of_variable3, World,
> 
> 
> Using this:
> 
> 	for (variable in list(variable1, variable2, variable3)){
> 
> 		cat("Hello,", ???variable???, variable, ", World,")
> 	}
> 
> This works fine if I'm trying to print the VALUE of variable, but I want to
> print the NAME of variable as well.
> 
This is a teetering heap of assumptions, but is this what you wanted?

Suzi<-1
HiYa<-function(x) {
  cat("Hello",deparse(substitute(x)),x,"World\n",sep=", ")
}
HiYa(Suzi)

Jim


From ggrothendieck at gmail.com  Tue Sep 26 12:37:36 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 26 Sep 2006 06:37:36 -0400
Subject: [R] package usage statistics.
In-Reply-To: <1768655942.20060926133242@eimb.ru>
References: <1768655942.20060926133242@eimb.ru>
Message-ID: <971536df0609260337p81a765u2438b8761278c2b@mail.gmail.com>

You could just check what the last time is that the file
system was accessed on each package listing out the packages in
order of last time accessed.  From the Windows console:

cd \Program Files\R\R-2.3.1pat\library
dir /od

or the analogous cd and ls -lt commands on UNIX

On 9/26/06, Vladimir Eremeev <wl at eimb.ru> wrote:
> Dear useRs,
>
>  Is it possible to get the R package usage statistics?
>  That is, does R contain any tools to estimate which packages were
>  used and how often?
>
>  I am going to temporary change the workplace and packing the data
>  and their processing scripts on my computer in order to continue my
>  projects.
>
>  During my work on the current workplace I periodically have had installed
>  new R packages, have investigated them and used them in my work or did
>  not used them, depending on their functionality.
>
>  Now I am thinking about writing an R script which will automatically
>  download and install everything I need from the R repository.
>  So, I need a list of packages I have used in R.
>
>  The first solution in my head is to scan all disks for R
>  scripts and .Rhistory files, extract calls for "library" from them
>  and save names of loaded packages.
>
>  I would appreciate other variants.
>
>
> ---
> Best regards,
> Vladimir                mailto:wl at eimb.ru
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Tue Sep 26 12:45:47 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 26 Sep 2006 06:45:47 -0400
Subject: [R] Need help with boxplots
In-Reply-To: <9b3787f20609260225p1ab32760hb2bde3ca38439dfb@mail.gmail.com>
References: <9b3787f20609260225p1ab32760hb2bde3ca38439dfb@mail.gmail.com>
Message-ID: <971536df0609260345t2dbca417h9ec4c7b421a7fb12@mail.gmail.com>

To prevent confusion you might want to use a red dot rather than
a line:

   points(1:2, c(mean(a), mean(b)), col = "red")

and perhaps label it since its non-standard:

   text(1:2, c(mean(a), mean(b)), "Mean", pos = 4)

On 9/26/06, laba diena <juscio at gmail.com> wrote:
> How to add a mean line in the boxplot keeping the median line ?
> For example in this:
>
>
> set.seed(1)
>
> a <- rnorm(10)
>
> b <- rnorm(10)
>
> boxplot(a, b)
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From henrigel at gmx.de  Tue Sep 26 13:34:44 2006
From: henrigel at gmx.de (henrigel at gmx.de)
Date: Tue, 26 Sep 2006 13:34:44 +0200
Subject: [R] rpart
In-Reply-To: <Pine.LNX.4.64.0609260931130.16320@gannet.stats.ox.ac.uk>
References: <20060925125504.19430@gmx.net>
	<Pine.LNX.4.64.0609260931130.16320@gannet.stats.ox.ac.uk>
Message-ID: <20060926113444.244480@gmx.net>


-------- Original-Nachricht --------
Datum: Tue, 26 Sep 2006 09:56:53 +0100 (BST)
Von: Prof Brian Ripley <ripley at stats.ox.ac.uk>
An: henrigel at gmx.de
Betreff: Re: [R] rpart

> On Mon, 25 Sep 2006, henrigel at gmx.de wrote:
> 
> > Dear r-help-list:
> >
> > If I use the rpart method like
> >
> > cfit<-rpart(y~.,data=data,...),
> >
> > what kind of tree is stored in cfit?
> > Is it right that this tree is not pruned at all, that it is the full
> tree?
> 
> It is an rpart object.  This contains both the tree and the instructions 
> for pruning it at all values of cp: note that cp is also used in deciding 
> how large a tree to grow.
> 

Ok, I have to explain my problem a little bit more in detail, I'm sorry for being so vague:
I used the method in the following way:
cfit<- rpart(y~., method="class", minsplit=1, cp=0)
I got a tree with a lot of terminals nodes that contained more than 100 observations. This made me believe that the tree was already pruned.
On the other hand, the printcp method showed subtrees that were "better".
This made me believe that the tree hadn't been pruned before.
So, are the trees "a little bit" pruned? 

> > If so, it's up to me to choose a subtree by using the printcp method.
> 
> Or the plotcp method.
> 
> > In the technical report from Atkinson and Therneau "An Introduction to 
> > recursive partitioning using the rpart routines" from 2000, one can see 
> > the following table on page 15:
> >
> >      CP  nsplit  relerror  xerror   xstd
> > 1   0.105   0     1.00000   1.0000   0.108
> > 2   0.056   3     0.68519   1.1852   0.111
> > 3   0.028   4     0.62963   1.0556   0.109
> > 4   0.574   6     0.57407   1.0556   0.109
> > 5   0.100   7     0.55556   1.0556   0.109
> >
> > Some lines below it says "We see that the best tree has 5 terminal nodes
> > (4 splits). Why that if the xerror is the lowest for the tree only 
> > consisting of the root?
> 
> There are *two* reports with that name: this seems to be from minitech.ps.
> The choice is explained in the rest of that para (the 1-SE rule was used).
> My guess is that the authors excluded the root as not being a tree, but 
> only they can answer that.
> 

Are both reports from 2000? But you're right, I'm talking about the one from minitch.ps.
The 1-SE-rule only explains why they didn't choose the tree with 6 or 7 splits, but not why they didn't choose the "tree" without a split.
The exclusion of the root as not being a tree was my first explanation, too. But if the tree only consisting of the root is still better than any other tree, why would I choose a tree with 4 splits then?  

Henri

--


From wl at eimb.ru  Tue Sep 26 12:07:25 2006
From: wl at eimb.ru (Vladimir Eremeev)
Date: Tue, 26 Sep 2006 10:07:25 +0000 (UTC)
Subject: [R] package usage statistics. (UPDATE)
References: <1768655942.20060926133242@eimb.ru>
Message-ID: <loom.20060926T115659-912@post.gmane.org>

Here is the perl script with some comments

<pre>
#!/bin/perl -w

use File::Find;
# we use the standard Perl module.
# its procedure will scan the directory tree and put all package names to the 
hash
# along with counting the number of their loadings.             

%pkgs=("base"=>-1,                # won't print packages installed by default
       "datasets"=>-1,
       "grDevices"=>-1,
       "graphics"=>-1,
       "grid"=>-1,
       "methods"=>-1,
       "splines"=>-1,
       "stats"=>-1,
       "stats4"=>-1,
       "tcltk"=>-1,
       "tools"=>-1,
       "utils"=>-1,
       "MASS"=>-1
      );

sub wanted {                   # this subroutine is used by the File::Find 
procedure
                               # it adds package names to the hash above
  return if($_!~/\.[Rr]$/ && $_!~/\.[Rr]history$/);  # do nothing if this file 
doesn't contain R commands

  open IN,"< ".$File::Find::name or die("cannot open file $!");

  while(<IN>){
    if(/library\((.*)\)/){                                    # looking for 
library(...) calls
      $pkgname=$1;
      next if(! -d "C:\\Program Files\\R\\library\\$pkgname"); # don't do 
anything if the package directory doesn't exist
                                                               # simple 
protection against typos
      if(exists $pkgs{$pkgname}) {
        $pkgs{$pkgname}=$pkgs{$pkgname}+1;            # here we assume that 
basic packages are not loaded
      }else{                                          # with "library()"
        $pkgs{$pkgname}=1;
      }
    }
  }
  close(IN);
}

sub getdepends {        # this subroutine resolves the package dependencies
  $pkgname=$_[0];       # its argument is a package name. It finds the packages 
the current one depends on
                        # and adds them to the hash above
  open IN, "< C:\\Program Files\\R\\library\\$pkgname\\DESCRIPTION" or return; 
#do {print ("cannot open file C:\\Program 
Files\\R\\library\\$pkgname\\DESCRIPTION\n $!");
  while(<IN>){
    if($_=~/^Imports: (.*)/ || $_=~/^Depends: (.*)/) {
      @deplist=split(/,/,$1);
      for(@deplist) {
        next if(/R \(.*\)/);     # exclude dependencies on R version
        s/\s//g;
        if(/(.*)\(.*\)/) {
          $pkgname=$1;
        }else{
          $pkgname=$_;
        }

        if(exists $pkgs{$pkgname}) {
          $pkgs{$pkgname}=$pkgs{$pkgname}+1 if($pkgs{$pkgname}>0);  # don't add 
basic packages
        }else{
          $pkgs{$pkgname}=1;
        }
      }
    }
  }
  close(IN);
}

# now the main loop. hope, it is self-describing

print "Searching for R commands...";
find({ wanted => \&wanted, no_chdir => 1 }, '.');
print "done!\n";

print "Now resolving dependencies...";
for $p (keys %pkgs) {
  #print "$p\n";
  getdepends($p);
}
print "done!\n";

open OUT,"> install.pkgs.r" or die("cannot create file install.pkgs.r");

print OUT "install.packages(\n";
foreach(keys %pkgs){
  print OUT "                  $_,\n" if($pkgs{$_}>0);
}
print OUT " ask=FALSE)\n";

close(OUT);
</pre>


From f.calboli at imperial.ac.uk  Tue Sep 26 13:44:35 2006
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Tue, 26 Sep 2006 12:44:35 +0100
Subject: [R] putting stuff into bins...
Message-ID: <451912A3.9050008@imperial.ac.uk>

Hi All,

I have a vector of data, a vector of bin breakpoints and I want to put my data 
in the bins and then extract fanciful informations like the mean value of each bin.

I know I can write my own function, but I would have thought that R should have 
somewhere a function that took as arguments something like (data, breaks, what 
to do with the data in the bins). I surey could not find it trawling the R-help 
archives though.

If such a function exists I'd be grateful to anyone pointing it out to me.

Cheers,

Fede

-- 
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St Mary's Campus
Norfolk Place, London W2 1PG

Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com


From ripley at stats.ox.ac.uk  Tue Sep 26 13:54:22 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 26 Sep 2006 12:54:22 +0100 (BST)
Subject: [R] rpart
In-Reply-To: <20060926113444.244480@gmx.net>
References: <20060925125504.19430@gmx.net>
	<Pine.LNX.4.64.0609260931130.16320@gannet.stats.ox.ac.uk>
	<20060926113444.244480@gmx.net>
Message-ID: <Pine.LNX.4.64.0609261252360.22091@gannet.stats.ox.ac.uk>

On Tue, 26 Sep 2006, henrigel at gmx.de wrote:

>
> -------- Original-Nachricht --------
> Datum: Tue, 26 Sep 2006 09:56:53 +0100 (BST)
> Von: Prof Brian Ripley <ripley at stats.ox.ac.uk>
> An: henrigel at gmx.de
> Betreff: Re: [R] rpart
>
>> On Mon, 25 Sep 2006, henrigel at gmx.de wrote:
>>
>>> Dear r-help-list:
>>>
>>> If I use the rpart method like
>>>
>>> cfit<-rpart(y~.,data=data,...),
>>>
>>> what kind of tree is stored in cfit?
>>> Is it right that this tree is not pruned at all, that it is the full
>> tree?
>>
>> It is an rpart object.  This contains both the tree and the instructions
>> for pruning it at all values of cp: note that cp is also used in deciding
>> how large a tree to grow.
>>
>
> Ok, I have to explain my problem a little bit more in detail, I'm sorry for being so vague:
> I used the method in the following way:
> cfit<- rpart(y~., method="class", minsplit=1, cp=0)
> I got a tree with a lot of terminals nodes that contained more than 100 observations. This made me believe that the tree was already pruned.
> On the other hand, the printcp method showed subtrees that were "better".
> This made me believe that the tree hadn't been pruned before.
> So, are the trees "a little bit" pruned?

Yes, as you asked for cp=0.  Look up what that does in ?rpart.control.

>>> If so, it's up to me to choose a subtree by using the printcp method.
>>
>> Or the plotcp method.
>>
>>> In the technical report from Atkinson and Therneau "An Introduction to
>>> recursive partitioning using the rpart routines" from 2000, one can see
>>> the following table on page 15:
>>>
>>>      CP  nsplit  relerror  xerror   xstd
>>> 1   0.105   0     1.00000   1.0000   0.108
>>> 2   0.056   3     0.68519   1.1852   0.111
>>> 3   0.028   4     0.62963   1.0556   0.109
>>> 4   0.574   6     0.57407   1.0556   0.109
>>> 5   0.100   7     0.55556   1.0556   0.109
>>>
>>> Some lines below it says "We see that the best tree has 5 terminal nodes
>>> (4 splits). Why that if the xerror is the lowest for the tree only
>>> consisting of the root?
>>
>> There are *two* reports with that name: this seems to be from minitech.ps.
>> The choice is explained in the rest of that para (the 1-SE rule was used).
>> My guess is that the authors excluded the root as not being a tree, but
>> only they can answer that.
>>
>
> Are both reports from 2000? But you're right, I'm talking about the one from minitch.ps.
> The 1-SE-rule only explains why they didn't choose the tree with 6 or 7 splits, but not why they didn't choose the "tree" without a split.
> The exclusion of the root as not being a tree was my first explanation, too. But if the tree only consisting of the root is still better than any other tree, why would I choose a tree with 4 splits then?
>
> Henri
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From p.dalgaard at biostat.ku.dk  Tue Sep 26 13:58:03 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 26 Sep 2006 13:58:03 +0200
Subject: [R] putting stuff into bins...
In-Reply-To: <451912A3.9050008@imperial.ac.uk>
References: <451912A3.9050008@imperial.ac.uk>
Message-ID: <x2lko63kms.fsf@viggo.kubism.ku.dk>

Federico Calboli <f.calboli at imperial.ac.uk> writes:

> Hi All,
> 
> I have a vector of data, a vector of bin breakpoints and I want to put my data 
> in the bins and then extract fanciful informations like the mean value of each bin.
> 
> I know I can write my own function, but I would have thought that R should have 
> somewhere a function that took as arguments something like (data, breaks, what 
> to do with the data in the bins). I surey could not find it trawling the R-help 
> archives though.
> 
> If such a function exists I'd be grateful to anyone pointing it out to me.


cut, split+lapply, aggregate, by

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From stecalza at tiscali.it  Tue Sep 26 13:59:40 2006
From: stecalza at tiscali.it (Stefano Calza)
Date: Tue, 26 Sep 2006 13:59:40 +0200
Subject: [R] putting stuff into bins...
In-Reply-To: <451912A3.9050008@imperial.ac.uk>
References: <451912A3.9050008@imperial.ac.uk>
Message-ID: <20060926115940.GD3952@med.unibs.it>

I don't know about such a function, but

tapply(data,cut(data,breaks),what to do)

should give you what you need.

HIH

Ciao,
Stefano

On Tue, Sep 26, 2006 at 12:44:35PM +0100, Federico Calboli wrote:
<Federico>Hi All,
<Federico>
<Federico>I have a vector of data, a vector of bin breakpoints and I want to put my data 
<Federico>in the bins and then extract fanciful informations like the mean value of each bin.
<Federico>
<Federico>I know I can write my own function, but I would have thought that R should have 
<Federico>somewhere a function that took as arguments something like (data, breaks, what 
<Federico>to do with the data in the bins). I surey could not find it trawling the R-help 
<Federico>archives though.
<Federico>
<Federico>If such a function exists I'd be grateful to anyone pointing it out to me.
<Federico>
<Federico>Cheers,
<Federico>
<Federico>Fede
<Federico>
<Federico>-- 
<Federico>Federico C. F. Calboli
<Federico>Department of Epidemiology and Public Health
<Federico>Imperial College, St Mary's Campus
<Federico>Norfolk Place, London W2 1PG
<Federico>
<Federico>Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193
<Federico>
<Federico>f.calboli [.a.t] imperial.ac.uk
<Federico>f.calboli [.a.t] gmail.com
<Federico>
<Federico>______________________________________________
<Federico>R-help a stat.math.ethz.ch mailing list
<Federico>https://stat.ethz.ch/mailman/listinfo/r-help
<Federico>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
<Federico>and provide commented, minimal, self-contained, reproducible code.


From dimitris.rizopoulos at med.kuleuven.be  Tue Sep 26 14:00:20 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 26 Sep 2006 14:00:20 +0200
Subject: [R] putting stuff into bins...
References: <451912A3.9050008@imperial.ac.uk>
Message-ID: <016101c6e163$56cf83d0$0540210a@www.domain>

probably a combination of cut() and tapply() could be of help in this 
case, e.g.,

x <- rnorm(100)
tapply(x, cut(x, -4:4), mean)


Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Federico Calboli" <f.calboli at imperial.ac.uk>
To: "r-help" <r-help at stat.math.ethz.ch>
Sent: Tuesday, September 26, 2006 1:44 PM
Subject: [R] putting stuff into bins...


> Hi All,
>
> I have a vector of data, a vector of bin breakpoints and I want to 
> put my data
> in the bins and then extract fanciful informations like the mean 
> value of each bin.
>
> I know I can write my own function, but I would have thought that R 
> should have
> somewhere a function that took as arguments something like (data, 
> breaks, what
> to do with the data in the bins). I surey could not find it trawling 
> the R-help
> archives though.
>
> If such a function exists I'd be grateful to anyone pointing it out 
> to me.
>
> Cheers,
>
> Fede
>
> -- 
> Federico C. F. Calboli
> Department of Epidemiology and Public Health
> Imperial College, St Mary's Campus
> Norfolk Place, London W2 1PG
>
> Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193
>
> f.calboli [.a.t] imperial.ac.uk
> f.calboli [.a.t] gmail.com
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From Dietrich.Trenkler at uni-osnabrueck.de  Tue Sep 26 14:00:09 2006
From: Dietrich.Trenkler at uni-osnabrueck.de (Dietrich Trenkler)
Date: Tue, 26 Sep 2006 14:00:09 +0200
Subject: [R] putting stuff into bins...
In-Reply-To: <451912A3.9050008@imperial.ac.uk>
References: <451912A3.9050008@imperial.ac.uk>
Message-ID: <45191649.8050202@uni-osnabrueck.de>

Federico Calboli schrieb:
> Hi All,
>
> I have a vector of data, a vector of bin breakpoints and I want to put my data 
> in the bins and then extract fanciful informations like the mean value of each bin.
>
> I know I can write my own function, but I would have thought that R should have 
> somewhere a function that took as arguments something like (data, breaks, what 
> to do with the data in the bins). I surey could not find it trawling the R-help 
> archives though.
>
> If such a function exists I'd be grateful to anyone pointing it out to me.
>
> Cheers,
>
> Fede
>
>   
The following should be of help:

"bd384" <- c(2.968, 2.097, 1.611, 3.038, 7.921, 5.476, 9.858,
    1.397, 0.155, 1.301, 9.054, 1.958, 4.058, 3.918, 2.019, 3.689,
    3.081, 4.229, 4.669, 2.274, 1.971, 10.379, 3.391, 2.093,
    6.053, 4.196, 2.788, 4.511, 7.3, 5.856, 0.86, 2.093, 0.703,
    1.182, 4.114, 2.075, 2.834, 3.698, 6.48, 2.36, 5.249, 5.1,
    4.131, 0.02, 1.071, 4.455, 3.676, 2.666, 5.457, 1.046, 1.908,
    3.064, 5.392, 8.393, 0.916, 9.665, 5.564, 3.599, 2.723, 2.87,
    1.582, 5.453, 4.091, 3.716, 6.156, 2.039)
cut(bd384,0:11)
split(bd384,cut(bd384,0:11))
sapply(split(bd384,cut(bd384,0:11)),mean)

D.Trenkler


-- 
Dietrich Trenkler c/o Universitaet Osnabrueck 
Rolandstr. 8; D-49069 Osnabrueck, Germany    
email: Dietrich.Trenkler at Uni-Osnabrueck.de


From mothsailor at googlemail.com  Tue Sep 26 14:13:47 2006
From: mothsailor at googlemail.com (David Barron)
Date: Tue, 26 Sep 2006 13:13:47 +0100
Subject: [R] putting stuff into bins...
In-Reply-To: <451912A3.9050008@imperial.ac.uk>
References: <451912A3.9050008@imperial.ac.uk>
Message-ID: <815b70590609260513j556b7dc1ob3e3eb3cbe6fb00c@mail.gmail.com>

This would work.  The point is to make a factor from the breakpoints
using cut, then use this to calculate the statistics on the binned
data.

> x <- rnorm(500)
> f <- cut(x,10)
> aggregate(x,list(f),mean)
           Group.1          x
1    (-2.71,-2.09] -2.3668991
2    (-2.09,-1.46] -1.7332011
3   (-1.46,-0.834] -1.1156487
4  (-0.834,-0.208] -0.5117649
5   (-0.208,0.418]  0.1277991
6     (0.418,1.04]  0.7092500
7      (1.04,1.67]  1.2859184
8       (1.67,2.3]  1.9347327
9       (2.3,2.92]  2.5518835
10     (2.92,3.55]  3.2873698


On 26/09/06, Federico Calboli <f.calboli at imperial.ac.uk> wrote:
> Hi All,
>
> I have a vector of data, a vector of bin breakpoints and I want to put my data
> in the bins and then extract fanciful informations like the mean value of each bin.
>
> I know I can write my own function, but I would have thought that R should have
> somewhere a function that took as arguments something like (data, breaks, what
> to do with the data in the bins). I surey could not find it trawling the R-help
> archives though.
>
> If such a function exists I'd be grateful to anyone pointing it out to me.
>
> Cheers,
>
> Fede
>
> --
> Federico C. F. Calboli
> Department of Epidemiology and Public Health
> Imperial College, St Mary's Campus
> Norfolk Place, London W2 1PG
>
> Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193
>
> f.calboli [.a.t] imperial.ac.uk
> f.calboli [.a.t] gmail.com
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From Roger.Bivand at nhh.no  Tue Sep 26 14:16:38 2006
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue, 26 Sep 2006 14:16:38 +0200 (CEST)
Subject: [R] package usage statistics. (UPDATE)
In-Reply-To: <loom.20060926T115659-912@post.gmane.org>
Message-ID: <Pine.LNX.4.44.0609261408080.27649-100000@reclus.nhh.no>

On Tue, 26 Sep 2006, Vladimir Eremeev wrote:

> Here is the perl script with some comments

??

t1 <- installed.packages()
t2 <- is.na(t1[,"Priority"])
t3 <- names(t2)[t2]
t4 <- sapply(t3, function(n) file.info(system.file("R", package=n)[1])$atime)
class(t4) <- "POSIXct"
sort(t4)

though the R directory may not be the best place to look for the atime? 
(This isn't the same, but you get the idea, there is a well-known 
"fortune" ...)

Roger

> 
> <pre>
> #!/bin/perl -w
> 
> use File::Find;
> # we use the standard Perl module.
> # its procedure will scan the directory tree and put all package names to the 
> hash
> # along with counting the number of their loadings.             
> 
> %pkgs=("base"=>-1,                # won't print packages installed by default
>        "datasets"=>-1,
>        "grDevices"=>-1,
>        "graphics"=>-1,
>        "grid"=>-1,
>        "methods"=>-1,
>        "splines"=>-1,
>        "stats"=>-1,
>        "stats4"=>-1,
>        "tcltk"=>-1,
>        "tools"=>-1,
>        "utils"=>-1,
>        "MASS"=>-1
>       );
> 
> sub wanted {                   # this subroutine is used by the File::Find 
> procedure
>                                # it adds package names to the hash above
>   return if($_!~/\.[Rr]$/ && $_!~/\.[Rr]history$/);  # do nothing if this file 
> doesn't contain R commands
> 
>   open IN,"< ".$File::Find::name or die("cannot open file $!");
> 
>   while(<IN>){
>     if(/library\((.*)\)/){                                    # looking for 
> library(...) calls
>       $pkgname=$1;
>       next if(! -d "C:\\Program Files\\R\\library\\$pkgname"); # don't do 
> anything if the package directory doesn't exist
>                                                                # simple 
> protection against typos
>       if(exists $pkgs{$pkgname}) {
>         $pkgs{$pkgname}=$pkgs{$pkgname}+1;            # here we assume that 
> basic packages are not loaded
>       }else{                                          # with "library()"
>         $pkgs{$pkgname}=1;
>       }
>     }
>   }
>   close(IN);
> }
> 
> sub getdepends {        # this subroutine resolves the package dependencies
>   $pkgname=$_[0];       # its argument is a package name. It finds the packages 
> the current one depends on
>                         # and adds them to the hash above
>   open IN, "< C:\\Program Files\\R\\library\\$pkgname\\DESCRIPTION" or return; 
> #do {print ("cannot open file C:\\Program 
> Files\\R\\library\\$pkgname\\DESCRIPTION\n $!");
>   while(<IN>){
>     if($_=~/^Imports: (.*)/ || $_=~/^Depends: (.*)/) {
>       @deplist=split(/,/,$1);
>       for(@deplist) {
>         next if(/R \(.*\)/);     # exclude dependencies on R version
>         s/\s//g;
>         if(/(.*)\(.*\)/) {
>           $pkgname=$1;
>         }else{
>           $pkgname=$_;
>         }
> 
>         if(exists $pkgs{$pkgname}) {
>           $pkgs{$pkgname}=$pkgs{$pkgname}+1 if($pkgs{$pkgname}>0);  # don't add 
> basic packages
>         }else{
>           $pkgs{$pkgname}=1;
>         }
>       }
>     }
>   }
>   close(IN);
> }
> 
> # now the main loop. hope, it is self-describing
> 
> print "Searching for R commands...";
> find({ wanted => \&wanted, no_chdir => 1 }, '.');
> print "done!\n";
> 
> print "Now resolving dependencies...";
> for $p (keys %pkgs) {
>   #print "$p\n";
>   getdepends($p);
> }
> print "done!\n";
> 
> open OUT,"> install.pkgs.r" or die("cannot create file install.pkgs.r");
> 
> print OUT "install.packages(\n";
> foreach(keys %pkgs){
>   print OUT "                  $_,\n" if($pkgs{$_}>0);
> }
> print OUT " ask=FALSE)\n";
> 
> close(OUT);
> </pre>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From stefano.sofia at regione.marche.it  Tue Sep 26 14:18:16 2006
From: stefano.sofia at regione.marche.it (Stefano Sofia)
Date: Tue, 26 Sep 2006 14:18:16 +0200
Subject: [R] about the determinant of a symmetric compound matrix
Message-ID: <163CA7BD55F0B84AAD787CE6192698D70C599D@GANDALF.regionemarche.intra>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060926/23aca4a2/attachment.pl 

From john.seers at bbsrc.ac.uk  Tue Sep 26 14:52:21 2006
From: john.seers at bbsrc.ac.uk (john seers (IFR))
Date: Tue, 26 Sep 2006 13:52:21 +0100
Subject: [R] Vectorise a for loop?
Message-ID: <1CF0B26CECD746438AE02DBF7DDE1C7B03055F15@ifre2ksrv1.ifrxp.bbsrc.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060926/f7627b0f/attachment.pl 

From juscio at gmail.com  Tue Sep 26 14:56:47 2006
From: juscio at gmail.com (laba diena)
Date: Tue, 26 Sep 2006 05:56:47 -0700
Subject: [R] Need help with boxplots
Message-ID: <9b3787f20609260556y29c9c483k70b5e070adc85785@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060926/59ec5a60/attachment.pl 

From jacques.veslot at good.ibl.fr  Tue Sep 26 15:01:37 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Tue, 26 Sep 2006 15:01:37 +0200
Subject: [R] Vectorise a for loop?
In-Reply-To: <1CF0B26CECD746438AE02DBF7DDE1C7B03055F15@ifre2ksrv1.ifrxp.bbsrc.ac.uk>
References: <1CF0B26CECD746438AE02DBF7DDE1C7B03055F15@ifre2ksrv1.ifrxp.bbsrc.ac.uk>
Message-ID: <451924B1.4050004@good.ibl.fr>

tt$fold <- ifelse(tt$M < 0, 1/(2^tt$M), 2^tt$M)
-------------------------------------------------------------------
Jacques VESLOT

CNRS UMR 8090
I.B.L (2?me ?tage)
1 rue du Professeur Calmette
B.P. 245
59019 Lille Cedex

Tel : 33 (0)3.20.87.10.44
Fax : 33 (0)3.20.87.10.31

http://www-good.ibl.fr
-------------------------------------------------------------------


john seers (IFR) a ?crit :
>  
> Hi R guru coders
>  
> I wrote a bit of code to add a new column onto a "topTable" dataframe.
> That is a list of genes processed using the limma package. I used a for
> loop but I kept feeling there was a better way using a more vector
> oriented approach. I looked at several commands such as "apply", "by"
> etc but could not find a good way to do it. I have this feeling there is
> a command or technique eluding me. (Is there an expr:value1?value2
> construction in R?) 
>  
> Can anybody suggest an elegant solution? 
>  
> Details:
>  
> So, the topTable looks like this:
>  
> 
>>topa1[1:5,c(1,2,3,4)]
> 
>           ID    Name GB_accession         M
> 11195 245828 SIGKEC9     AX135029 -7.670197
> 10966    107    FHL1       B14446 -5.089926
> 6287   25744     M90    LL137340 -4.531744
> 777     2288   VSNL1     LF039555 -4.035472
> 11310 272294     M98    LL031650  3.866422
> 
> 
> I want to add a "fold" column so it will look like this:
>  
> 
>>topa1[1:5,c(1,2,3,4,10)]
> 
>           ID    Name GB_accession         M      fold
> 11195 245828 SIGKEC9     AX135029 -7.670197 203.68521
> 10966    107    FHL1       B14446 -5.089926  34.05810
> 6287   25744     M90    LL137340 -4.531744  23.13082
> 777     2288   VSNL1     LF039555 -4.035472  16.39828
> 11310 272294     M98    LL031650  3.866422  14.58508
> 
> 
>  
> The fold values is calculated from the M column which is a log2 value.
> The calculation is different depending on whether the M value is
> negative or positive. That is if the gene is down regulated the
> reciprocal value has to be used to calculate a fold value.
>  
> Here is my clunky, not vectorised code :
>  
> # Function to add a fold column to the toptable
> ttfold<-function(tt) {
>  fold<-NULL
>  for (i in 1:length(tt$M)) {
>   if (tt$M[i] < 0 ) {
>    fold[i]<-1/(2^tt$M[i])
>   } else {
>    fold[i]<-2^tt$M[i]
>   }
>  }
>  tt<-cbind(tt, fold) 
> }
> 
> # Add fold column to top tables
> topa1<-ttfold(topa1)
>  
>  
>  
>  
> Regards
>  
>  
> J
>  
>  
>  
>  
>  
>  
>  
>  
>  
> ---
> 
> John Seers
> Institute of Food Research
> Norwich Research Park
> Colney
> Norwich
> NR4 7UA
>  
> 
> tel +44 (0)1603 251497
> fax +44 (0)1603 507723
> e-mail john.seers at bbsrc.ac.uk <mailto:john.seers at bbsrc.ac.uk>
> 
> e-disclaimer at http://www.ifr.ac.uk/edisclaimer/
> <http://www.ifr.ac.uk/edisclaimer/>  
>  
> Web sites:
> 
> www.ifr.ac.uk <http://www.ifr.ac.uk/>    
> www.foodandhealthnetwork.com <http://www.foodandhealthnetwork.com/> 
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From john.seers at bbsrc.ac.uk  Tue Sep 26 15:10:12 2006
From: john.seers at bbsrc.ac.uk (john seers (IFR))
Date: Tue, 26 Sep 2006 14:10:12 +0100
Subject: [R] Vectorise a for loop?
In-Reply-To: <451924B1.4050004@good.ibl.fr>
Message-ID: <1CF0B26CECD746438AE02DBF7DDE1C7B03055F16@ifre2ksrv1.ifrxp.bbsrc.ac.uk>



Hi Jacques

Yes, that looks a whole lot better. That "ifelse" is exactly what I was searching for. 

Merci.

J

 
---

John Seers
Institute of Food Research
Norwich Research Park
Colney
Norwich
NR4 7UA
 

tel +44 (0)1603 251497
fax +44 (0)1603 507723
e-mail john.seers at bbsrc.ac.uk                         
e-disclaimer at http://www.ifr.ac.uk/edisclaimer/ 
 
Web sites:

www.ifr.ac.uk   
www.foodandhealthnetwork.com


-----Original Message-----
From: Jacques VESLOT [mailto:jacques.veslot at good.ibl.fr] 
Sent: 26 September 2006 14:02
To: john seers (IFR)
Cc: R-help
Subject: Re: [R] Vectorise a for loop?


tt$fold <- ifelse(tt$M < 0, 1/(2^tt$M), 2^tt$M)
-------------------------------------------------------------------
Jacques VESLOT

CNRS UMR 8090
I.B.L (2?me ?tage)
1 rue du Professeur Calmette
B.P. 245
59019 Lille Cedex

Tel : 33 (0)3.20.87.10.44
Fax : 33 (0)3.20.87.10.31

http://www-good.ibl.fr


From mothsailor at googlemail.com  Tue Sep 26 15:22:45 2006
From: mothsailor at googlemail.com (David Barron)
Date: Tue, 26 Sep 2006 14:22:45 +0100
Subject: [R] Need help with boxplots
In-Reply-To: <9b3787f20609260556y29c9c483k70b5e070adc85785@mail.gmail.com>
References: <9b3787f20609260556y29c9c483k70b5e070adc85785@mail.gmail.com>
Message-ID: <815b70590609260622y21370f05s1e44d968903304da@mail.gmail.com>

The problem with a line, I think, would be that the width of the boxes
can vary depending on the number of boxes in the plot, etc.  No doubt
it could be done, but you'd probably have to look into the bxp
function to see how the widths are calculated.

On 26/09/06, laba diena <juscio at gmail.com> wrote:
>  How to add a mean *line* in the boxplot keeping the median line ?
> Maybe it is possible to do using the *segments* function ?
> For example in this:
>
> set.seed(1)
>
> a <- rnorm(10)
>
> b <- rnorm(10)
>
> boxplot(a, b)
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From abjrges at chello.no  Tue Sep 26 15:24:09 2006
From: abjrges at chello.no (=?ISO-8859-1?Q?Anders_Bj=F8rges=E6ter?=)
Date: Tue, 26 Sep 2006 15:24:09 +0200
Subject: [R] crostab qry - Too many crosstab column headers
Message-ID: <451929F9.2080700@chello.no>

Hello

I guess not, but is there a way to reduce/split up a MS-access crosstab 
query resulting in more than 256 cols. by using sqlGetResults in RODBC 
to e.g. produce several dataframes of < 256 columns (that is without 
changing the query itself).
---
[1] "[RODBC] ERROR: Could not SQLExecDirect" 

[2] "S1001 -1040 [Microsoft][ODBC Microsoft Access Driver] Too many 
crosstab column headers (2270)."

R - 2.3
WinXp with Ms access 2002

Best Regards
Anders


From pratap_stat at yahoo.co.in  Tue Sep 26 15:47:16 2006
From: pratap_stat at yahoo.co.in (nalluri pratap)
Date: Tue, 26 Sep 2006 14:47:16 +0100 (BST)
Subject: [R] creation of new variables
Message-ID: <20060926134716.32908.qmail@web8610.mail.in.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060926/7bb5a9fa/attachment.pl 

From p.dalgaard at biostat.ku.dk  Tue Sep 26 15:49:14 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 26 Sep 2006 15:49:14 +0200
Subject: [R] about the determinant of a symmetric compound matrix
In-Reply-To: <163CA7BD55F0B84AAD787CE6192698D70C599D@GANDALF.regionemarche.intra>
References: <163CA7BD55F0B84AAD787CE6192698D70C599D@GANDALF.regionemarche.intra>
Message-ID: <x2d59i3fhh.fsf@viggo.kubism.ku.dk>

"Stefano Sofia" <stefano.sofia at regione.marche.it> writes:

> Dear R users,
> even if this question is not related to an issue about R, probably some of you will be able to help me.
> 
> I have a square matrix of dimension k by k with alpha on the diagonal and beta everywhee else.
> This symmetric matrix is called symmetric compound matrix and has the form
> a( I + cJ),
> where
> I is the k by k identity matrix
> J is the k by k matrix of all ones
> a = alpha - beta
> c = beta/a
> 
> I need to evaluate the determinant of this matrix. Is there any algebric formula for that?

Yes. Unusually, this is not from the famous "Rao p.33", but from p.32... [1]:

det(A+XX') = det(A)(1+X'A^{-1}X) provided det(A) != 0

now put X = sqrt(c) times a vector of ones and get det(I+cJ) = 1+ck.
Multiply by a^k for the general case. 

Quick sanity check:

> m <- matrix(.1,7,7)
> diag(m) <- .9
> det(m)
[1] 0.393216
> .8^7 * (1 + .1/.8 * 7)
[1] 0.393216

Alternatively, you can do it via eigenvalues: The off-diagonal part
(beta*J) corresponds to a single direction along the unit vector
c(1,1,...,1)/sqrt(7). The diagonal part corresponds to adding (alpha -
beta)*I, which has total sphericity so you can arrange that one
eigenvector of it points in the same direction and you end up with

  (alpha - beta)^(k-1) * (alpha - beta + k*beta) 

> (.9-.1)^6*((.9-.1)+ 7*.1)
[1] 0.393216

(Getting this right on the first try is almost impossible...)

[1] CR Rao, Linear Statistical Inference and Its Applications, 2nd ed.
Wiley 1973.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From Greg.Snow at intermountainmail.org  Tue Sep 26 17:02:10 2006
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Tue, 26 Sep 2006 09:02:10 -0600
Subject: [R] Statistical data and Map-package
Message-ID: <07E228A5BE53C24CAD490193A7381BBB5C5089@LP-EXCHVS07.CO.IHC.COM>

You may also want to look at the maptools (and sp) package, it can read
in and plot shapefiles from external sources.

Some sources of maps that maptools can plot include:

http://www.vdstech.com/map_data.htm
http://openmap.bbn.com/data/shape/timezone/ 
http://arcdata.esri.com/data_downloader/DataDownloader?part=10200&stack=
back

Hope this helps,


-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Rense Nieuwenhuis
Sent: Tuesday, September 26, 2006 2:21 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Statistical data and Map-package

Dear helpeRs,

I'm working with the map-package and came upon a problem which I
couldn't solve. I hope onee of you can. If not, this can be seen as a
suggestion for new versions of the package.

I'm trying to create a map of some European countries, filled with
colors corresponding to some values. Let's say I have the following
countries and I assign the following colors (fictional):

country2001 <- c("Austria", "Belgium", "Switzerland", "Czechoslovakia",
"Germany", "Denmark", "Spain", "Finland", "France", "UK", "Greece",
"Hungary", "Ireland", "Israel", "Italy", "Luxembourg", "Netherlands",
"Norway", "Poland", "Portugal", "Sweden", "Slovenia")
color2001 <- c("green", "yellow","red","red", "red", "red", "red",
"red", "green", "red", "red", "red", "red", "red", "red", "red", "red",
"blue", "red", "red", "red", "orange")

I then let the colors and the values correspond using 'match.map', like
this:

match <- match.map("world",country2001)
color <- color2001[match]

And finally I plot the map. It works perfectly fine.

map(database="world", fill=TRUE, col=color)


But as I mentioned, I want to create a map of Europe. So, I use xlim and
ylim to let some parts of the world fall of the map. The syntax becomes
like this:

map(database="world", fill=TRUE, col=color, xlim=c(-25,70),ylim=c
(35,71))

Now, a problem arises. The regions on the map are colored by the vector
'color'. It needs therefore to correspond to the order in which the
polygons are drawn. Since some of the full world-map isn't drawn this
time, the color-vector doesn't correspond anymore. This results in the
coloring of the wrong countries.

Does anybody know of a way to solve this?

Thanks very much in advance,

Rense Nieuwenhuis
	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From afshart at exchange.sba.miami.edu  Tue Sep 26 17:13:01 2006
From: afshart at exchange.sba.miami.edu (Afshartous, David)
Date: Tue, 26 Sep 2006 11:13:01 -0400
Subject: [R] treatment effect at specific time point within mixed effects
	model
Message-ID: <6BCB4D493A447546A8126F24332056E804300680@school1.business.edu>

 
All,

The code below is for a pseudo dataset of repeated measures on patients
where there is also a treatment factor called "drug".  Time is treated
as categorical.  

What code is necessary to test for a treatment effect at a single time
point,
e.g., time = 3?   Does the answer matter if the design is a crossover
design,
i.e, each patient received drug and placebo?

Finally, what would be a good response to someone that suggests to do a
simple t-test (paired in crossover case) instead of the test above
within a mixed model?

thanks!
dave



z = rnorm(24, mean=0, sd=1)
time = rep(1:6, 4)
Patient = rep(1:4, each = 6)
drug = factor(rep(c("I", "P"), each = 6, times = 2)) ## P = placebo, I =
Ibuprofen
dat.new = data.frame(time, drug, z, Patient)
data.grp = groupedData(z ~ time | Patient, data = dat.new)
fm1 = lme(z ~ factor(time) + drug + factor(time):drug, data = data.grp,
random = list(Patient = ~ 1) )


From wl at eimb.ru  Tue Sep 26 17:19:21 2006
From: wl at eimb.ru (Vladimir Eremeev)
Date: Tue, 26 Sep 2006 19:19:21 +0400
Subject: [R] package usage statistics. (UPDATE)
In-Reply-To: <Pine.LNX.4.44.0609261408080.27649-100000@reclus.nhh.no>
References: <loom.20060926T115659-912@post.gmane.org>
	<Pine.LNX.4.44.0609261408080.27649-100000@reclus.nhh.no>
Message-ID: <125201634.20060926191921@eimb.ru>

Dear Roger,

Tuesday, September 26, 2006, 4:16:38 PM, you wrote:

RB> On Tue, 26 Sep 2006, Vladimir Eremeev wrote:

>> Here is the perl script with some comments

RB> ??

Sorry, forgot to mention, this script is designed to run from the root
of the working directory tree.
It scans all R session histories and scripts and analyzes them.
This is performed through the call

 find({ wanted => \&wanted, no_chdir => 1 }, '.');

The second parameter to find is a list of directories.
This allows, for example, build a histogram of package usage.

---
Best regards,
Vladimir                mailto:wl at eimb.ru


From elvis at xlsolutions-corp.com  Tue Sep 26 17:28:41 2006
From: elvis at xlsolutions-corp.com (elvis at xlsolutions-corp.com)
Date: Tue, 26 Sep 2006 08:28:41 -0700
Subject: [R] October R/Splus course in Washington DC, San Francisco,
	Seattle *** R/Splus Fundamentals and Programming Techniques
Message-ID: <20060926082841.9f08cc34deb45d78e54b3b5664e21546.34925438a2.wbe@email.secureserver.net>

XLSolutions Corporation (www.xlsolutions-corp.com) is proud to
announce our 2-day October 2006 "R/S-plus Fundamentals and Programming
Techniques" : www.xlsolutions-corp.com/Rfund.htm

*** Washington DC / October 12-13, 2006
*** Seattle Wa  / October 19-20
*** San Francisco / October 26-27      

Reserve your seat now at the early bird rates! Payment due AFTER
the class

Course Description:

This two-day beginner to intermediate R/S-plus course focuses on a
broad spectrum of topics, from reading raw data to a comparison of R
and S. We will learn the essentials of data manipulation, graphical
visualization and R/S-plus programming. We will explore statistical
data analysis tools,including graphics with data sets. How to enhance
your plots, build your own packages (librairies) and connect via
ODBC,etc.
We will perform some statistical modeling and fit linear regression
models. Participants are encouraged to bring data for interactive
sessions

With the following outline:

- An Overview of R and S
- Data Manipulation and Graphics
- Using Lattice Graphics
- A Comparison of R and S-Plus
- How can R Complement SAS?
- Writing Functions
- Avoiding Loops
- Vectorization
- Statistical Modeling
- Project Management
- Techniques for Effective use of R and S
- Enhancing Plots
- Using High-level Plotting Functions
- Building and Distributing Packages (libraries)
- Connecting; ODBC, Rweb, Orca via sockets and via Rjava


Email us for group discounts.
Email Sue Turner: sue at xlsolutions-corp.com
Phone: 206-686-1578
Visit us: www.xlsolutions-corp.com/training.htm
Please let us know if you and your colleagues are interested in this
classto take advantage of group discount. Register now to secure your
seat!

Interested in R/Splus Advanced course? email us.


Cheers,
Elvis Miller, PhD
Manager Training.
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com
elvis at xlsolutions-corp.com


From hb at stat.berkeley.edu  Tue Sep 26 17:34:50 2006
From: hb at stat.berkeley.edu (Henrik Bengtsson)
Date: Tue, 26 Sep 2006 08:34:50 -0700
Subject: [R] printing a variable name in a for loop
In-Reply-To: <4519C24D.7000200@bitwrit.com.au>
References: <000c01c6df96$e49e5cf0$0400a8c0@suzilaptop>
	<4519C24D.7000200@bitwrit.com.au>
Message-ID: <59d7961d0609260834j51fe0f08lab70bd61c9de0034@mail.gmail.com>

Example:

lst <- list(variable1, variable2, variable3)
for (kk in seq(along=lst)) {
  name <- names(lst)[kk];
  value <- lst[[kk]];
  cat("Hello,", name, value, ", World,")
}

/Henrik

On 9/26/06, Jim Lemon <jim at bitwrit.com.au> wrote:
> Suzi Fei wrote:
> > Hello,
> >
> > How do you print a variable name in a for loop?
> >
> > I'm trying to construct a csv file that looks like this:
> >
> >
> >       Hello, variable1, value_of_variable1, World,
> >       Hello, variable2, value_of_variable2, World,
> >       Hello, variable3, value_of_variable3, World,
> >
> >
> > Using this:
> >
> >       for (variable in list(variable1, variable2, variable3)){
> >
> >               cat("Hello,", ???variable???, variable, ", World,")
> >       }
> >
> > This works fine if I'm trying to print the VALUE of variable, but I want to
> > print the NAME of variable as well.
> >
> This is a teetering heap of assumptions, but is this what you wanted?
>
> Suzi<-1
> HiYa<-function(x) {
>   cat("Hello",deparse(substitute(x)),x,"World\n",sep=", ")
> }
> HiYa(Suzi)
>
> Jim
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From singyee.ling at googlemail.com  Tue Sep 26 17:35:52 2006
From: singyee.ling at googlemail.com (singyee ling)
Date: Tue, 26 Sep 2006 16:35:52 +0100
Subject: [R] warning message in nlm
Message-ID: <ca33a9890609260835s12b19b56x21a789844f2a527b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060926/36fb227d/attachment.pl 

From petr.pikal at precheza.cz  Tue Sep 26 17:40:32 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 26 Sep 2006 17:40:32 +0200
Subject: [R] lapply, plot and additional arguments
Message-ID: <45196610.17010.25BC83C@localhost>

Dear all

Hopefully somebody will know the answer.

I have some list

x <- data.frame(a = 1:9, beta = exp(-4:4), logic = rep(c(TRUE,FALSE), 
c(5,4)))
x.l <- split(x, x$logic)
plot(x.l$a, x.l$beta)

and I want to plot lines color coded according to logic variable

lapply(x.l, function(x, ...) lines(x$a, x$beta, col=1:2))
lapply(x.l, function(x,...) lines(x$a,x$beta), col=1:2)
lapply(x.l, function(x,...) lines(x$a,x$beta, ...), col=1:2)

Well, lapply seems to ignore my best attempts to persuade it to use 
different colours for each part of x.l list.

Anybody knows how to code different colours when using lapply for 
such plotting?

At present time I use a loop but maybe lapply could do it too.

Best regards.
Petr

Petr Pikal
petr.pikal at precheza.cz


From ggrothendieck at gmail.com  Tue Sep 26 17:41:14 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 26 Sep 2006 11:41:14 -0400
Subject: [R] Need help with boxplots
In-Reply-To: <971536df0609260345t2dbca417h9ec4c7b421a7fb12@mail.gmail.com>
References: <9b3787f20609260225p1ab32760hb2bde3ca38439dfb@mail.gmail.com>
	<971536df0609260345t2dbca417h9ec4c7b421a7fb12@mail.gmail.com>
Message-ID: <971536df0609260841v5fe4ac14j7ef1fe859a0b4e6c@mail.gmail.com>

And if you really do want a line segment try this:

M <- c(mean(a), mean(b))
segments(1:2-0.4, M, 1:2+0.4, M, col = "red")


On 9/26/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> To prevent confusion you might want to use a red dot rather than
> a line:
>
>   points(1:2, c(mean(a), mean(b)), col = "red")
>
> and perhaps label it since its non-standard:
>
>   text(1:2, c(mean(a), mean(b)), "Mean", pos = 4)
>
> On 9/26/06, laba diena <juscio at gmail.com> wrote:
> > How to add a mean line in the boxplot keeping the median line ?
> > For example in this:
> >
> >
> > set.seed(1)
> >
> > a <- rnorm(10)
> >
> > b <- rnorm(10)
> >
> > boxplot(a, b)
> >
> >        [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From henrigel at gmx.de  Tue Sep 26 17:51:11 2006
From: henrigel at gmx.de (henrigel at gmx.de)
Date: Tue, 26 Sep 2006 17:51:11 +0200
Subject: [R] rpart
In-Reply-To: <Pine.LNX.4.64.0609261252360.22091@gannet.stats.ox.ac.uk>
References: <20060925125504.19430@gmx.net>
	<Pine.LNX.4.64.0609260931130.16320@gannet.stats.ox.ac.uk>
	<20060926113444.244480@gmx.net>
	<Pine.LNX.4.64.0609261252360.22091@gannet.stats.ox.ac.uk>
Message-ID: <20060926155111.12000@gmx.net>


-------- Original-Nachricht --------
Datum: Tue, 26 Sep 2006 12:54:22 +0100 (BST)
Von: Prof Brian Ripley <ripley at stats.ox.ac.uk>
An: henrigel at gmx.de
Betreff: Re: [R] rpart

> On Tue, 26 Sep 2006, henrigel at gmx.de wrote:
> 
> >
> > -------- Original-Nachricht --------
> > Datum: Tue, 26 Sep 2006 09:56:53 +0100 (BST)
> > Von: Prof Brian Ripley <ripley at stats.ox.ac.uk>
> > An: henrigel at gmx.de
> > Betreff: Re: [R] rpart
> >
> >> On Mon, 25 Sep 2006, henrigel at gmx.de wrote:
> >>
> >>> Dear r-help-list:
> >>>
> >>> If I use the rpart method like
> >>>
> >>> cfit<-rpart(y~.,data=data,...),
> >>>
> >>> what kind of tree is stored in cfit?
> >>> Is it right that this tree is not pruned at all, that it is the full
> >> tree?
> >>
> >> It is an rpart object.  This contains both the tree and the
> instructions
> >> for pruning it at all values of cp: note that cp is also used in
> deciding
> >> how large a tree to grow.
> >>
> >
> > Ok, I have to explain my problem a little bit more in detail, I'm sorry
> for being so vague:
> > I used the method in the following way:
> > cfit<- rpart(y~., method="class", minsplit=1, cp=0)
> > I got a tree with a lot of terminals nodes that contained more than 100
> observations. This made me believe that the tree was already pruned.
> > On the other hand, the printcp method showed subtrees that were
> "better".
> > This made me believe that the tree hadn't been pruned before.
> > So, are the trees "a little bit" pruned?
> 
> Yes, as you asked for cp=0.  Look up what that does in ?rpart.control.
> 

I thought I would get a full tree by choosing cp=0 - and it was one.
The nodes with more than 100 observations were not split further because there was no sequence of splits which made the class label change for any subset. (A bad explanation, but you probably know what I mean.) I realized that when I chose cp=-1. Thank you very much for your help!  

> >>> If so, it's up to me to choose a subtree by using the printcp method.
> >>
> >> Or the plotcp method.
> >>
> >>> In the technical report from Atkinson and Therneau "An Introduction to
> >>> recursive partitioning using the rpart routines" from 2000, one can
> see
> >>> the following table on page 15:
> >>>
> >>>      CP  nsplit  relerror  xerror   xstd
> >>> 1   0.105   0     1.00000   1.0000   0.108
> >>> 2   0.056   3     0.68519   1.1852   0.111
> >>> 3   0.028   4     0.62963   1.0556   0.109
> >>> 4   0.574   6     0.57407   1.0556   0.109
> >>> 5   0.100   7     0.55556   1.0556   0.109
> >>>
> >>> Some lines below it says "We see that the best tree has 5 terminal
> nodes
> >>> (4 splits). Why that if the xerror is the lowest for the tree only
> >>> consisting of the root?
> >>
> >> There are *two* reports with that name: this seems to be from
> minitech.ps.
> >> The choice is explained in the rest of that para (the 1-SE rule was
> used).
> >> My guess is that the authors excluded the root as not being a tree, but
> >> only they can answer that.
> >>
> >
> > Are both reports from 2000? But you're right, I'm talking about the one
> from minitch.ps.
> > The 1-SE-rule only explains why they didn't choose the tree with 6 or 7
> splits, but not why they didn't choose the "tree" without a split.
> > The exclusion of the root as not being a tree was my first explanation,
> too. But if the tree only consisting of the root is still better than any
> other tree, why would I choose a tree with 4 splits then?
> >
> > 

Henri


--


From dimitris.rizopoulos at med.kuleuven.be  Tue Sep 26 17:53:53 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 26 Sep 2006 17:53:53 +0200
Subject: [R] lapply, plot and additional arguments
References: <45196610.17010.25BC83C@localhost>
Message-ID: <022301c6e183$f76ed8c0$0540210a@www.domain>

maybe something like this could help:

x <- data.frame(a = 1:9, beta = exp(-4:4),
    logic = rep(c(TRUE, FALSE), c(5, 4)))
x.l <- split(x, x$logic)

plot(x$a, x$beta)
mapply(function(x, y) lines(x$a, x$b, col = y), x.l, 1:2)


Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Petr Pikal" <petr.pikal at precheza.cz>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, September 26, 2006 5:40 PM
Subject: [R] lapply, plot and additional arguments


> Dear all
>
> Hopefully somebody will know the answer.
>
> I have some list
>
> x <- data.frame(a = 1:9, beta = exp(-4:4), logic = 
> rep(c(TRUE,FALSE),
> c(5,4)))
> x.l <- split(x, x$logic)
> plot(x.l$a, x.l$beta)
>
> and I want to plot lines color coded according to logic variable
>
> lapply(x.l, function(x, ...) lines(x$a, x$beta, col=1:2))
> lapply(x.l, function(x,...) lines(x$a,x$beta), col=1:2)
> lapply(x.l, function(x,...) lines(x$a,x$beta, ...), col=1:2)
>
> Well, lapply seems to ignore my best attempts to persuade it to use
> different colours for each part of x.l list.
>
> Anybody knows how to code different colours when using lapply for
> such plotting?
>
> At present time I use a loop but maybe lapply could do it too.
>
> Best regards.
> Petr
>
> Petr Pikal
> petr.pikal at precheza.cz
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From Fabian.Mollet at wur.nl  Tue Sep 26 18:21:40 2006
From: Fabian.Mollet at wur.nl (Mollet, Fabian)
Date: Tue, 26 Sep 2006 18:21:40 +0200
Subject: [R] set off error messages
Message-ID: <281EC1311AF8724FA5DE2EFB6503844C01509E01@sijmu0002.wurnet.nl>

Hello there!
 
I'm creacting a loop for(i in 1:n){...}within which I build a nls model at each iteration. for some of the values of i, the algoritm in the nls function doesn't converge or cannot find a solution and consequently an error message is produced, and so my loop is interupted. The errors don't really matter to me as all the other values might still be useful and therefore I want to ignore the errors, so that that the return of the models for which no solution is found should just be NA values, so that I get a value for every i. How can I turn off the error message and make return NA values instead?
 
Thanks in advance for your help...
 
Fabian


From tlumley at u.washington.edu  Tue Sep 26 18:26:59 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 26 Sep 2006 09:26:59 -0700 (PDT)
Subject: [R] set off error messages
In-Reply-To: <281EC1311AF8724FA5DE2EFB6503844C01509E01@sijmu0002.wurnet.nl>
References: <281EC1311AF8724FA5DE2EFB6503844C01509E01@sijmu0002.wurnet.nl>
Message-ID: <Pine.LNX.4.64.0609260926280.24608@homer22.u.washington.edu>

On Tue, 26 Sep 2006, Mollet, Fabian wrote:

> Hello there!
>
> I'm creacting a loop for(i in 1:n){...}within which I build a nls model 
> at each iteration. for some of the values of i, the algoritm in the nls 
> function doesn't converge or cannot find a solution and consequently an 
> error message is produced, and so my loop is interupted. The errors 
> don't really matter to me as all the other values might still be useful 
> and therefore I want to ignore the errors, so that that the return of 
> the models for which no solution is found should just be NA values, so 
> that I get a value for every i. How can I turn off the error message and 
> make return NA values instead?
>

This is a FAQ (7.32)

 	-thomas


From mothsailor at googlemail.com  Tue Sep 26 18:27:43 2006
From: mothsailor at googlemail.com (David Barron)
Date: Tue, 26 Sep 2006 17:27:43 +0100
Subject: [R] set off error messages
In-Reply-To: <281EC1311AF8724FA5DE2EFB6503844C01509E01@sijmu0002.wurnet.nl>
References: <281EC1311AF8724FA5DE2EFB6503844C01509E01@sijmu0002.wurnet.nl>
Message-ID: <815b70590609260927q3be3fb9em683e0e20583b7f27@mail.gmail.com>

Try ?try

On 26/09/06, Mollet, Fabian <Fabian.Mollet at wur.nl> wrote:
> Hello there!
>
> I'm creacting a loop for(i in 1:n){...}within which I build a nls model at each iteration. for some of the values of i, the algoritm in the nls function doesn't converge or cannot find a solution and consequently an error message is produced, and so my loop is interupted. The errors don't really matter to me as all the other values might still be useful and therefore I want to ignore the errors, so that that the return of the models for which no solution is found should just be NA values, so that I get a value for every i. How can I turn off the error message and make return NA values instead?
>
> Thanks in advance for your help...
>
> Fabian
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From AnupTyagi at yahoo.com  Tue Sep 26 18:21:23 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Tue, 26 Sep 2006 16:21:23 +0000 (UTC)
Subject: [R] Extention of Pie Chart in R (was Re: Adding percentage to Pie
	Charts)
References: <4515E92C.4070506@bitwrit.com.au>
Message-ID: <loom.20060926T180541-731@post.gmane.org>

Jim Lemon <jim <at> bitwrit.com.au> writes:

> I admit to interpreting this pretty loosely, but I would like to know 
> what people think of a "fan plot".

Hi all, I tried the fan.plots that Jim has been very nice to provide. It made me
think if there was something like, "clock.plots" in R? Something like the
following, anything that comes close? 

The idea an extention in yet another way of Pie Charts, extending the fan.plots
provided by Jim.
* A value will be depicted on a "clock.plot" using 1 or 2 hands of an analog
clock on a circle calibrated from 0 to 100 (same as 0).
* For values between 0 and 99 use the position of only one hand of the clock
(needle).
* For values of 100, use the second hand (needle), and move it to 1.
* Some way to identify needles, and two two overlapping needles.
* Use color coding or line-types to differentiate variables.

This is basically a clock calibrated on a scale of 100, rather than 60. It can
visually depict values between 1 and 10000.

Do we have something like this R?

Anupam.


From xianjun.dong at bccs.uib.no  Tue Sep 26 18:37:00 2006
From: xianjun.dong at bccs.uib.no (Xianjun Dong)
Date: Tue, 26 Sep 2006 18:37:00 +0200
Subject: [R] Not all functions work in RSPerl package?
In-Reply-To: <4512A2B2.1040007@wald.ucdavis.edu>
References: <1158847115.3966.21.camel@lauvtre.ii.uib.no>
	<4512A2B2.1040007@wald.ucdavis.edu>
Message-ID: <1159288621.19253.35.camel@lauvtre.ii.uib.no>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060926/5e2a0aee/attachment.pl 

From AnupTyagi at yahoo.com  Tue Sep 26 18:38:08 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Tue, 26 Sep 2006 16:38:08 +0000 (UTC)
Subject: [R] read.xport: Writing and reading dataframe to disk directly
Message-ID: <loom.20060926T182149-686@post.gmane.org>

Hi All, is there a way of directly writing to disk file, the dataframe or list
of dataframes that result from read.xport function. This function converts SAS
export files to R dataframes. I would like to convert a SAS transport file to R,
but the resulting R dataframes do not fit in the memory of my computer. Is there
way to write the output of this fucntion to disk, perhaps using some pipe or
connection facility. Something like,

filexpt.lst <- lookup.xport("file.xpt")
# works very well and returns a list with all kind of information about variable
# name, format, labels, etc.

save(filexpt.df <- read.xport("file.xpt"), file="filexpt.Rdata")
# from what I can tell, this will not work.

? Is there a way to use a pipe or connection to write filexpt.df to disk as it
is being created?
? Is there a way to use a connection to an R dataframe on disk, so I can get
subsets (rows or colums) from the dataframe on disk, without having to read it
into memory?

I will be thankful for your help and suggestions.

Anupam.


From jeff.horner at vanderbilt.edu  Tue Sep 26 19:04:23 2006
From: jeff.horner at vanderbilt.edu (Jeffrey Horner)
Date: Tue, 26 Sep 2006 12:04:23 -0500
Subject: [R] New project: littler for GNU R
Message-ID: <45195D97.8010402@vanderbilt.edu>


What ?
======

   littler - Provides hash-bang (#!) capability for R (www.r-project.org)


Why ?
=====

   GNU R, a language and environment for statistical computing and
   graphics, provides a wonderful system for 'programming with data'
   as well as interactive exploratory analysis, often involving graphs.

   Sometimes, however, simple scripts are desired. While GNU R can
   be used in batch mode, and while so-called 'here' documents can be
   crafted, a long-standing need for a scripting front-end has often
   been expressed by the R Community.

   littler (pronounced 'little R' and written 'r') aims to fill
   this need.

   It can be used directly on the command-line just like, say, bc(1):


         $ echo 'cat(pi^2,"\n")' | r
         9.869604

   Equivalently, commands that are to be evaluated can be given on
   the command-line

         $ r -e 'cat(pi^2, "\n")'
         9.869604

   But unlike bc(1), GNU R has a vast number of statistical
   functions. For example, we can quickly compute a summary() and show
   a stem-and-leaf plot for file sizes in a given directory via

         $ ls -l /boot | awk '!/^total/ {print $5}' | \
              r -e 'fsizes <- as.integer(readLines());
                 print(summary(fsizes)); stem(fsizes)'
            Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
              13     512  110100  486900  768400 4735000
         Loading required package: grDevices

           The decimal point is 6 digit(s) to the right of the |

           0 | 00000000000000000011112223
           0 | 5557778899
           1 | 112233
           1 | 5
           2 |
           2 |
           3 |
           3 |
           4 |
           4 | 7


   And, last but not least, this (somewhat unwieldy) expression can
   be stored in a helper script:

         $ cat examples/fsizes.r
         #!/usr/bin/env r

         fsizes <- as.integer(readLines())
         print(summary(fsizes))
         stem(fsizes)

   (where calling /usr/bin/env is a trick from Python which allows one
   to forget whether r is installed in /usr/bin/r, /usr/local/bin/r,
   ~/bin/r, ...)

   A few examples are provided in the source directories examples/
   and tests/.

Where ?
=======

   littler can either be downloaded from

       http://biostat.mc.vanderbilt.edu/LittleR

   accessed by anonymous SVN:

       $ svn co http://littler.googlecode.com/svn/trunk/ littler

   or (soon !) be gotten from Debian mirrors via

       $ agt-get install littler

   littler is known to build and run on Linux and OS X.


Who ?
=====

   Copyright (C) 2006 Jeffrey Horner and Dirk Eddelbuettel

   littler is free software; you can redistribute it and/or modify it
   under the terms of the GNU General Public License as published by
   the Free Software Foundation; either version 2 of the License, or
   (at your option) any later version.

   This program is distributed in the hope that it will be useful,
   but WITHOUT ANY WARRANTY; without even the implied warranty of
   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
   General Public License for more details.

   You should have received a copy of the GNU General Public
   License along with this program; if not, write to the Free
   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston,
   MA  02111-1307  USA

   Comments are welcome, as are are suggestions, bug fixes, or patches.


     - Jeffrey Horner <jeff.horner at vanderbilt.edu>
     - Dirk Eddelbuettel <edd at debian.org>


From ggrothendieck at gmail.com  Tue Sep 26 19:14:14 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 26 Sep 2006 13:14:14 -0400
Subject: [R] New project: littler for GNU R
In-Reply-To: <45195D97.8010402@vanderbilt.edu>
References: <45195D97.8010402@vanderbilt.edu>
Message-ID: <971536df0609261014x12854422i8335f877913d29ce@mail.gmail.com>

Any plans for Windows?

On 9/26/06, Jeffrey Horner <jeff.horner at vanderbilt.edu> wrote:
>
> What ?
> ======
>
>   littler - Provides hash-bang (#!) capability for R (www.r-project.org)
>
>
> Why ?
> =====
>
>   GNU R, a language and environment for statistical computing and
>   graphics, provides a wonderful system for 'programming with data'
>   as well as interactive exploratory analysis, often involving graphs.
>
>   Sometimes, however, simple scripts are desired. While GNU R can
>   be used in batch mode, and while so-called 'here' documents can be
>   crafted, a long-standing need for a scripting front-end has often
>   been expressed by the R Community.
>
>   littler (pronounced 'little R' and written 'r') aims to fill
>   this need.
>
>   It can be used directly on the command-line just like, say, bc(1):
>
>
>         $ echo 'cat(pi^2,"\n")' | r
>         9.869604
>
>   Equivalently, commands that are to be evaluated can be given on
>   the command-line
>
>         $ r -e 'cat(pi^2, "\n")'
>         9.869604
>
>   But unlike bc(1), GNU R has a vast number of statistical
>   functions. For example, we can quickly compute a summary() and show
>   a stem-and-leaf plot for file sizes in a given directory via
>
>         $ ls -l /boot | awk '!/^total/ {print $5}' | \
>              r -e 'fsizes <- as.integer(readLines());
>                 print(summary(fsizes)); stem(fsizes)'
>            Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>              13     512  110100  486900  768400 4735000
>         Loading required package: grDevices
>
>           The decimal point is 6 digit(s) to the right of the |
>
>           0 | 00000000000000000011112223
>           0 | 5557778899
>           1 | 112233
>           1 | 5
>           2 |
>           2 |
>           3 |
>           3 |
>           4 |
>           4 | 7
>
>
>   And, last but not least, this (somewhat unwieldy) expression can
>   be stored in a helper script:
>
>         $ cat examples/fsizes.r
>         #!/usr/bin/env r
>
>         fsizes <- as.integer(readLines())
>         print(summary(fsizes))
>         stem(fsizes)
>
>   (where calling /usr/bin/env is a trick from Python which allows one
>   to forget whether r is installed in /usr/bin/r, /usr/local/bin/r,
>   ~/bin/r, ...)
>
>   A few examples are provided in the source directories examples/
>   and tests/.
>
> Where ?
> =======
>
>   littler can either be downloaded from
>
>       http://biostat.mc.vanderbilt.edu/LittleR
>
>   accessed by anonymous SVN:
>
>       $ svn co http://littler.googlecode.com/svn/trunk/ littler
>
>   or (soon !) be gotten from Debian mirrors via
>
>       $ agt-get install littler
>
>   littler is known to build and run on Linux and OS X.
>
>
> Who ?
> =====
>
>   Copyright (C) 2006 Jeffrey Horner and Dirk Eddelbuettel
>
>   littler is free software; you can redistribute it and/or modify it
>   under the terms of the GNU General Public License as published by
>   the Free Software Foundation; either version 2 of the License, or
>   (at your option) any later version.
>
>   This program is distributed in the hope that it will be useful,
>   but WITHOUT ANY WARRANTY; without even the implied warranty of
>   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
>   General Public License for more details.
>
>   You should have received a copy of the GNU General Public
>   License along with this program; if not, write to the Free
>   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston,
>   MA  02111-1307  USA
>
>   Comments are welcome, as are are suggestions, bug fixes, or patches.
>
>
>     - Jeffrey Horner <jeff.horner at vanderbilt.edu>
>     - Dirk Eddelbuettel <edd at debian.org>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From btyner at gmail.com  Tue Sep 26 19:22:25 2006
From: btyner at gmail.com (Benjamin Tyner)
Date: Tue, 26 Sep 2006 13:22:25 -0400
Subject: [R] dotplot, dropping unused levels of 'y'
In-Reply-To: <eb555e660609151301q3c0a4e30h8a934811d78eb81b@mail.gmail.com>
References: <450B00C3.3070005@stat.purdue.edu>
	<eb555e660609151301q3c0a4e30h8a934811d78eb81b@mail.gmail.com>
Message-ID: <451961D1.6080902@stat.purdue.edu>

Deepayan Sarkar wrote:

> On 9/15/06, Benjamin Tyner <btyner at gmail.com> wrote:
>
>> In dotplot, what's the best way to suppress the unused levels of 'y' on
>> a per-panel basis? This is useful for the case that 'y' is a factor
>> taking perhaps thousands of levels, but for a given panel, only a
>> handfull of these levels ever present.
>
>
> It's a bit problematic. Basically, you can use
> relation="free"/"sliced", but y behaves as as.numeric(y) would. So, if
> the small subset in each panel are always more or less contiguous (in
> terms of the levels being close to each other) then you would be fine.
> Otherwise you would not. In that case, you can still write your own
> prepanel and panel functions, e.g.:
> -------------
>
> library(lattice)
>
> y <- factor(sample(1:100), levels = 1:100)
> x <- 1:100
> a <- gl(9, 1, 100)
>
> dotplot(y ~ x | a)
>
> p <-
>    dotplot(y ~ x | a,
>            scales = list(y = list(relation = "free", rot = 0)),
>
>            prepanel = function(x, y, ...) {
>                yy <- y[, drop = TRUE]
>                list(ylim = levels(yy),
>                     yat = sort(unique(as.numeric(yy))))
>            },
>
>            panel = function(x, y, ...) {
>                yy <- y[, drop = TRUE]
>                panel.dotplot(x, yy, ...)
>            })
>
> ----------
>
> Hope that gives you what you want.
>
> Deepayan

I've been trying to extend this to allow groups, but am running into a 
bit of trouble. For example, the following doesn't quite work: (some of 
the unused factor levels are suppressed per panel, but not all):

set.seed(47905)
temp3<-data.frame(s_port=factor(rpois(100,10)),
                  POSIXtime=structure(1:100,class=c("POSIXt","POSIXct")),
                  l_ipn=factor(rpois(100,10)),
                  duration=runif(100),
                  locality=sample(1:4,replace=TRUE,size=100),
                  l_role=sample(c(-1,1),replace=TRUE,size=100))

plot<-dotplot(s_port~POSIXtime|l_ipn,
              data=temp3,
              layout=c(1,1),
              pch="|",
              col=1:8,
              duration=temp3$duration,
              auto.key=list(col=1:8,points=FALSE),
              groups=locality*l_role,
              prepanel = function(x, y, ...) {
                 yy <- y[, drop = TRUE]
                 list(ylim = levels(yy),
                      yat = sort(unique(as.numeric(yy))))
              },
              panel = panel.superpose,
              panel.groups = function(x, y, subscripts, duration, col, 
...) {
                 yy <- y[, drop = TRUE]
                 yy.n <- as.numeric(yy)
                 panel.abline(h=yy.n,col="lightgray")
                 panel.xyplot(x=x,y=yy.n,subscripts=subscripts,col=col,...)
                 panel.segments(x,
                                yy.n,
                                x+duration[subscripts],
                                yy.n,
                                col = col)
              },
              scales=list(y=list(relation="free"),
                          x=list(rot=45)),
              xlab="time",
              ylab="source port")



Thanks,
Ben


From edd at debian.org  Tue Sep 26 19:39:00 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 26 Sep 2006 12:39:00 -0500
Subject: [R] New project: littler for GNU R
In-Reply-To: <971536df0609261014x12854422i8335f877913d29ce@mail.gmail.com>
References: <45195D97.8010402@vanderbilt.edu>
	<971536df0609261014x12854422i8335f877913d29ce@mail.gmail.com>
Message-ID: <17689.26036.114874.833125@basebud.nulle.part>


On 26 September 2006 at 13:14, Gabor Grothendieck wrote:
| Any plans for Windows?

Someone with deeper knowledge of the Windows build process would need to help
us. Interested?

Dirk
 
-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From mr.blacksheep at gmail.com  Tue Sep 26 19:39:22 2006
From: mr.blacksheep at gmail.com (Mike Nielsen)
Date: Tue, 26 Sep 2006 12:39:22 -0500
Subject: [R] creation of new variables
In-Reply-To: <20060926134716.32908.qmail@web8610.mail.in.yahoo.com>
References: <20060926134716.32908.qmail@web8610.mail.in.yahoo.com>
Message-ID: <46a360560609261039n7ed38c16j395d50431bfae88b@mail.gmail.com>

You may not have told us quite enough to be able to help you.  It may
be worth your while investing some time in describing the problem you
are trying to solve a little bit more comprehensively.

The posting guide http://www.R-project.org/posting-guide.html can be
useful in helping you  frame a question that stands a better chance of
receiving help.

Regards,

Mike

On 9/26/06, nalluri pratap <pratap_stat at yahoo.co.in> wrote:
> Hello All,
>
>   I have 8 variables named
>
>    a b c d e f g h
>
>   I need to create four variables from these 8 vraibles in R.
>
>   the new variables are ab,cd,ef,gh.
>
>   Can anyone pleas help me
>
>   thanks,
>   Pratap
>
>
>
>
> ---------------------------------
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Regards,

Mike Nielsen


From sfalcon at fhcrc.org  Tue Sep 26 19:52:51 2006
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Tue, 26 Sep 2006 10:52:51 -0700
Subject: [R] New project: littler for GNU R
In-Reply-To: <45195D97.8010402@vanderbilt.edu> (Jeffrey Horner's message of
	"Tue, 26 Sep 2006 12:04:23 -0500")
References: <45195D97.8010402@vanderbilt.edu>
Message-ID: <m2sliey0p8.fsf@ziti.local>

Wow, looks neat.

OS X users will be unhappy with your naming choice as the default
filesystem there is not case-sensitive :-(

IOW, r and R do the same thing.  I would expect it to otherwise work
on OS X so a change of some sort might be worthwhile.

+ seth


From Cameron.Guenther at MyFWC.com  Tue Sep 26 20:08:22 2006
From: Cameron.Guenther at MyFWC.com (Guenther, Cameron)
Date: Tue, 26 Sep 2006 14:08:22 -0400
Subject: [R]  How to "Pack" a matrix
Message-ID: <BA6FF017E924044A9BF748AFAEEA6F30A53403@FWC-TLEX3.fwc.state.fl.us>

Hello,
Suppose I have a matrix a where

a=		sp1	sp2	sp3	sp4	sp5	sp6
	site1	1	0	1	1	0	1
	site2	1	0	1	1	0	1
	site3	1	1	1	1	1	1
	site4	0	1	1	1	0	1
	site5	0	0	1	0	0	1
	site6	0	0	1	0	1	0

And I want to pack that matrix so that the upper left corner contains
most of the ones and the bottom right corner contains most of the zeros
so that matrix b is

b= 		sp3	sp6	sp4	sp1	sp2	sp5
	site1	1	1	1	1	0	0
	site2	1	1	1	1	0	0
	site3	1	1	1	1	1	1
	site4	1	1	1	0	1	0
	site5	1	1	0	0	0	0
	site6	1	0	0	0	0	1

Can any of you help me with some code to accomplish this?  I have tried
different forms of order and can't seem to figure it out.  Basically I
want to order the matrix by both the rows and columns.

Thank you for your help.
Cam

Cameron Guenther, Ph.D. 
Associate Research Scientist
FWC/FWRI, Marine Fisheries Research
100 8th Avenue S.E.
St. Petersburg, FL 33701
(727)896-8626 Ext. 4305
cameron.guenther at myfwc.com


From Cameron.Guenther at MyFWC.com  Tue Sep 26 20:08:22 2006
From: Cameron.Guenther at MyFWC.com (Guenther, Cameron)
Date: Tue, 26 Sep 2006 14:08:22 -0400
Subject: [R]  How to "Pack" a matrix
Message-ID: <BA6FF017E924044A9BF748AFAEEA6F30A53403@FWC-TLEX3.fwc.state.fl.us>

Hello,
Suppose I have a matrix a where

a=		sp1	sp2	sp3	sp4	sp5	sp6
	site1	1	0	1	1	0	1
	site2	1	0	1	1	0	1
	site3	1	1	1	1	1	1
	site4	0	1	1	1	0	1
	site5	0	0	1	0	0	1
	site6	0	0	1	0	1	0

And I want to pack that matrix so that the upper left corner contains
most of the ones and the bottom right corner contains most of the zeros
so that matrix b is

b= 		sp3	sp6	sp4	sp1	sp2	sp5
	site1	1	1	1	1	0	0
	site2	1	1	1	1	0	0
	site3	1	1	1	1	1	1
	site4	1	1	1	0	1	0
	site5	1	1	0	0	0	0
	site6	1	0	0	0	0	1

Can any of you help me with some code to accomplish this?  I have tried
different forms of order and can't seem to figure it out.  Basically I
want to order the matrix by both the rows and columns.

Thank you for your help.
Cam

Cameron Guenther, Ph.D. 
Associate Research Scientist
FWC/FWRI, Marine Fisheries Research
100 8th Avenue S.E.
St. Petersburg, FL 33701
(727)896-8626 Ext. 4305
cameron.guenther at myfwc.com


From jeff.horner at vanderbilt.edu  Tue Sep 26 20:11:55 2006
From: jeff.horner at vanderbilt.edu (Jeffrey Horner)
Date: Tue, 26 Sep 2006 13:11:55 -0500
Subject: [R] New project: littler for GNU R
In-Reply-To: <m2sliey0p8.fsf@ziti.local>
References: <45195D97.8010402@vanderbilt.edu> <m2sliey0p8.fsf@ziti.local>
Message-ID: <45196D6B.3020208@vanderbilt.edu>

Seth Falcon wrote:
> Wow, looks neat.
> 
> OS X users will be unhappy with your naming choice as the default
> filesystem there is not case-sensitive :-(
> 
> IOW, r and R do the same thing.  I would expect it to otherwise work
> on OS X so a change of some sort might be worthwhile.

(I'm always amazed at how I can miss the simplest details. I probably 
knew at some point that OS X shipped with a case-sensitive file system, 
which you can turn off somehow, but forgot. Thank goodness for peer review.)

littler will install into /usr/local/bin by default, so I don't think 
there's a clash with the Mac binary provided by CRAN, right?

Jeff
-- 
http://biostat.mc.vanderbilt.edu/JeffreyHorner


From deepayan.sarkar at gmail.com  Tue Sep 26 20:28:55 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Tue, 26 Sep 2006 11:28:55 -0700
Subject: [R] dotplot, dropping unused levels of 'y'
In-Reply-To: <451961D1.6080902@stat.purdue.edu>
References: <450B00C3.3070005@stat.purdue.edu>
	<eb555e660609151301q3c0a4e30h8a934811d78eb81b@mail.gmail.com>
	<451961D1.6080902@stat.purdue.edu>
Message-ID: <eb555e660609261128y2c5a57f6oa4f3494b7302a0d6@mail.gmail.com>

On 9/26/06, Benjamin Tyner <btyner at gmail.com> wrote:
> Deepayan Sarkar wrote:
>
> > On 9/15/06, Benjamin Tyner <btyner at gmail.com> wrote:
> >
> >> In dotplot, what's the best way to suppress the unused levels of 'y' on
> >> a per-panel basis? This is useful for the case that 'y' is a factor
> >> taking perhaps thousands of levels, but for a given panel, only a
> >> handfull of these levels ever present.
> >
> >
> > It's a bit problematic. Basically, you can use
> > relation="free"/"sliced", but y behaves as as.numeric(y) would. So, if
> > the small subset in each panel are always more or less contiguous (in
> > terms of the levels being close to each other) then you would be fine.
> > Otherwise you would not. In that case, you can still write your own
> > prepanel and panel functions, e.g.:
> > -------------
> >
> > library(lattice)
> >
> > y <- factor(sample(1:100), levels = 1:100)
> > x <- 1:100
> > a <- gl(9, 1, 100)
> >
> > dotplot(y ~ x | a)
> >
> > p <-
> >    dotplot(y ~ x | a,
> >            scales = list(y = list(relation = "free", rot = 0)),
> >
> >            prepanel = function(x, y, ...) {
> >                yy <- y[, drop = TRUE]
> >                list(ylim = levels(yy),
> >                     yat = sort(unique(as.numeric(yy))))
> >            },
> >
> >            panel = function(x, y, ...) {
> >                yy <- y[, drop = TRUE]
> >                panel.dotplot(x, yy, ...)
> >            })
> >
> > ----------
> >
> > Hope that gives you what you want.
> >
> > Deepayan
>
> I've been trying to extend this to allow groups, but am running into a
> bit of trouble. For example, the following doesn't quite work: (some of
> the unused factor levels are suppressed per panel, but not all):

I don't think panel = panel.superpose is enough. Try

             panel = function(x, y, ...) {
                yy <- y[, drop = TRUE]
                yy.n <- as.numeric(yy)
                panel.superpose(x, yy.n, ...)
            },
            panel.groups =
            function(x, y, subscripts, duration, col, ...) {
                panel.abline(h = y, col = "lightgray")
                panel.xyplot(x, y, col = col, ...)
                panel.segments(x, y,
                               x + duration[subscripts], y,
                               col = col)
            },

-Deepayan

> set.seed(47905)
> temp3<-data.frame(s_port=factor(rpois(100,10)),
>                   POSIXtime=structure(1:100,class=c("POSIXt","POSIXct")),
>                   l_ipn=factor(rpois(100,10)),
>                   duration=runif(100),
>                   locality=sample(1:4,replace=TRUE,size=100),
>                   l_role=sample(c(-1,1),replace=TRUE,size=100))
>
> plot<-dotplot(s_port~POSIXtime|l_ipn,
>               data=temp3,
>               layout=c(1,1),
>               pch="|",
>               col=1:8,
>               duration=temp3$duration,
>               auto.key=list(col=1:8,points=FALSE),
>               groups=locality*l_role,
>               prepanel = function(x, y, ...) {
>                  yy <- y[, drop = TRUE]
>                  list(ylim = levels(yy),
>                       yat = sort(unique(as.numeric(yy))))
>               },
>               panel = panel.superpose,
>               panel.groups = function(x, y, subscripts, duration, col,
> ...) {
>                  yy <- y[, drop = TRUE]
>                  yy.n <- as.numeric(yy)
>                  panel.abline(h=yy.n,col="lightgray")
>                  panel.xyplot(x=x,y=yy.n,subscripts=subscripts,col=col,...)
>                  panel.segments(x,
>                                 yy.n,
>                                 x+duration[subscripts],
>                                 yy.n,
>                                 col = col)
>               },
>               scales=list(y=list(relation="free"),
>                           x=list(rot=45)),
>               xlab="time",
>               ylab="source port")
>
>
>
> Thanks,
> Ben
>


From ggrothendieck at gmail.com  Tue Sep 26 20:30:22 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 26 Sep 2006 14:30:22 -0400
Subject: [R] How to "Pack" a matrix
In-Reply-To: <BA6FF017E924044A9BF748AFAEEA6F30A53403@FWC-TLEX3.fwc.state.fl.us>
References: <BA6FF017E924044A9BF748AFAEEA6F30A53403@FWC-TLEX3.fwc.state.fl.us>
Message-ID: <971536df0609261130k6d9aa1b0w82952b71f3c0ae8b@mail.gmail.com>

It looks like your example only reorders the columns but your
discussion refers to ordering rows too.  I have only addressed
the columns part but it is hopefully clear how to extend this
or use other objective functions.  We generate every permutation
of the rows and define an objective function f which is smaller for
more desirable column permutations and then use brute force to find
the minimizer:

library(combinat)

mat <- structure(c(1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0), .Dim = c(6,
6), .Dimnames = list(c("site1", "site2", "site3", "site4", "site5",
"site6"), c("sp1", "sp2", "sp3", "sp4", "sp5", "sp6")))

f <- function(p) sum(mat[,p] * (row(mat) + col(mat)))
perms <- permn(ncol(mat))
mat[,perms[[which.min(sapply(perms, f))]]]


On 9/26/06, Guenther, Cameron <Cameron.Guenther at myfwc.com> wrote:
> Hello,
> Suppose I have a matrix a where
>
> a=              sp1     sp2     sp3     sp4     sp5     sp6
>        site1   1       0       1       1       0       1
>        site2   1       0       1       1       0       1
>        site3   1       1       1       1       1       1
>        site4   0       1       1       1       0       1
>        site5   0       0       1       0       0       1
>        site6   0       0       1       0       1       0
>
> And I want to pack that matrix so that the upper left corner contains
> most of the ones and the bottom right corner contains most of the zeros
> so that matrix b is
>
> b=              sp3     sp6     sp4     sp1     sp2     sp5
>        site1   1       1       1       1       0       0
>        site2   1       1       1       1       0       0
>        site3   1       1       1       1       1       1
>        site4   1       1       1       0       1       0
>        site5   1       1       0       0       0       0
>        site6   1       0       0       0       0       1
>
> Can any of you help me with some code to accomplish this?  I have tried
> different forms of order and can't seem to figure it out.  Basically I
> want to order the matrix by both the rows and columns.
>
> Thank you for your help.
> Cam
>
> Cameron Guenther, Ph.D.
> Associate Research Scientist
> FWC/FWRI, Marine Fisheries Research
> 100 8th Avenue S.E.
> St. Petersburg, FL 33701
> (727)896-8626 Ext. 4305
> cameron.guenther at myfwc.com
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Tue Sep 26 20:30:22 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 26 Sep 2006 14:30:22 -0400
Subject: [R] How to "Pack" a matrix
In-Reply-To: <BA6FF017E924044A9BF748AFAEEA6F30A53403@FWC-TLEX3.fwc.state.fl.us>
References: <BA6FF017E924044A9BF748AFAEEA6F30A53403@FWC-TLEX3.fwc.state.fl.us>
Message-ID: <971536df0609261130k6d9aa1b0w82952b71f3c0ae8b@mail.gmail.com>

It looks like your example only reorders the columns but your
discussion refers to ordering rows too.  I have only addressed
the columns part but it is hopefully clear how to extend this
or use other objective functions.  We generate every permutation
of the rows and define an objective function f which is smaller for
more desirable column permutations and then use brute force to find
the minimizer:

library(combinat)

mat <- structure(c(1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0), .Dim = c(6,
6), .Dimnames = list(c("site1", "site2", "site3", "site4", "site5",
"site6"), c("sp1", "sp2", "sp3", "sp4", "sp5", "sp6")))

f <- function(p) sum(mat[,p] * (row(mat) + col(mat)))
perms <- permn(ncol(mat))
mat[,perms[[which.min(sapply(perms, f))]]]


On 9/26/06, Guenther, Cameron <Cameron.Guenther at myfwc.com> wrote:
> Hello,
> Suppose I have a matrix a where
>
> a=              sp1     sp2     sp3     sp4     sp5     sp6
>        site1   1       0       1       1       0       1
>        site2   1       0       1       1       0       1
>        site3   1       1       1       1       1       1
>        site4   0       1       1       1       0       1
>        site5   0       0       1       0       0       1
>        site6   0       0       1       0       1       0
>
> And I want to pack that matrix so that the upper left corner contains
> most of the ones and the bottom right corner contains most of the zeros
> so that matrix b is
>
> b=              sp3     sp6     sp4     sp1     sp2     sp5
>        site1   1       1       1       1       0       0
>        site2   1       1       1       1       0       0
>        site3   1       1       1       1       1       1
>        site4   1       1       1       0       1       0
>        site5   1       1       0       0       0       0
>        site6   1       0       0       0       0       1
>
> Can any of you help me with some code to accomplish this?  I have tried
> different forms of order and can't seem to figure it out.  Basically I
> want to order the matrix by both the rows and columns.
>
> Thank you for your help.
> Cam
>
> Cameron Guenther, Ph.D.
> Associate Research Scientist
> FWC/FWRI, Marine Fisheries Research
> 100 8th Avenue S.E.
> St. Petersburg, FL 33701
> (727)896-8626 Ext. 4305
> cameron.guenther at myfwc.com
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From AnupTyagi at yahoo.com  Tue Sep 26 21:12:56 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Tue, 26 Sep 2006 19:12:56 +0000 (UTC)
Subject: [R] colClasses: supressed 'NA'
Message-ID: <loom.20060926T205212-554@post.gmane.org>

Hi,

The colClasses seem to be supressing 'NA' vlaues. How do I fix this?

R script and first 5 lines of output is below.

File "test2.dat" has blanks that are read as "NA" when I do not use
'colClasses', but as blanks when I use 'colClasses'.

temp.df <- read.fwf("test2.dat", width=c(10,1,1,1,1,2,2,3,3,1),  
col.names=c("psu","losewt","maintain","fewcal","phyact","age","income","weight",
"wtdesire","gender"),
colClasses=c("factor","factor","factor","factor","factor","numeric","factor",
"numeric","numeric","factor"),
nrows=270000, comment.char="")

temp.df
           psu losewt maintain fewcal phyact age income weight wtdesire gender
1   2003009323      2        2                52     05    220      220      1
2   2003005181      2        1      2      2  58     08    165      145      2
3   2003015942      2        1      4      1  76     05    142      130      2
4   2003011406      2        1      3      1  43     03    110      110      2
5   2003006786      1               4      1  49     06    178      145      2

? why am I not getting missing values when I use 'colClasses'?


From gantonaci at gmail.com  Tue Sep 26 21:39:51 2006
From: gantonaci at gmail.com (Giuseppe Antonaci)
Date: Tue, 26 Sep 2006 16:39:51 -0300
Subject: [R] Building R for Windows with ATLAS
Message-ID: <41b60e190609261239g2f547805x7da635bfb3e7a78d@mail.gmail.com>

I think this is not a R-devel question. Sorry to all if I'm wrong,
please let me know.

I managed to build R successfully with the default BLAS but when I
change the MKRULES to use ATLAS BLAS and set the path to
"C:/cygwin/home/Administrador/ATLAS/lib/WinNT_ATHLONSSE2" I got the
following error message (I'm posting only the final part, there was a
lot of compilation before this):

cp R.dll ../../bin/
-------- Building ../../bin/Rblas.dll --------
gcc  -shared -s -o ../../bin/Rblas.dll blas00.o dllversion.o Rblas.def \
   -L../../bin -lR  -L"C:/WinNT_ATHLONSSE2" -lf77blas -latlas
C:/WinNT_ATHLONSSE2/libf77blas.a(xerbla.o):xerbla.f:(.text+0xb): undefined refer
ence to `s_wsfe'
C:/WinNT_ATHLONSSE2/libf77blas.a(xerbla.o):xerbla.f:(.text+0x27): undefined refe
rence to `do_fio'
C:/WinNT_ATHLONSSE2/libf77blas.a(xerbla.o):xerbla.f:(.text+0x43): undefined refe
rence to `do_fio'
C:/WinNT_ATHLONSSE2/libf77blas.a(xerbla.o):xerbla.f:(.text+0x48): undefined refe
rence to `e_wsfe'
C:/WinNT_ATHLONSSE2/libf77blas.a(xerbla.o):xerbla.f:(.text+0x5c): undefined refe
rence to `s_stop'
collect2: ld returned 1 exit status
make[2]: *** [../../bin/Rblas.dll] Error 1
make[1]: *** [rbuild] Error 2
make: *** [all] Error 2

The ATLAS BLAS was build using Cygwin. AFTER building ATLAS BLAS I
changed the "Path" variable putting "C:\Rtools\tools\bin;C:\MinGW\bin"
before everything else.
To build R I followed "R Administration and Instalation" and Duncan
Murdoch's guide at http://www.murdoch-sutherland.com/Rtools/,
including the version of MinGW.

At ATLAS web page (http://math-atlas.sourceforge.net/errata.html) I
found the following:

Q: I'm linking with C, and getting missing symbols (such as w_wsfe,
do_fio, w_esfe or s_stop).
R: These kinds of symbols are Fortran library calls. The problem is
that the C linker does not automatically find the Fortran libraries.
The most common fix is to either link using your fortran linker, or to
rewrite your code so that Fortran routines are not called. If you know
where they are, you can also choose to link in the Fortran libraries
explicitly

Well, I can understand that there is a huge probability that this is
my problem. Unfortunately I know nothing of C or Fortran. Even if I
knew that I have these Fortran libraries I wouldn't know how to link
them. I tried to look at MinGW web page but found nothing.
Any help would be mostly welcome, please.
Giuseppe Antonaci

Sorry for English errors and lack of knowledge. I hope I made myself
understandable.


From ritwik.sinha at gmail.com  Tue Sep 26 21:42:17 2006
From: ritwik.sinha at gmail.com (Ritwik Sinha)
Date: Tue, 26 Sep 2006 15:42:17 -0400
Subject: [R] creation of new variables
In-Reply-To: <20060926134716.32908.qmail@web8610.mail.in.yahoo.com>
References: <20060926134716.32908.qmail@web8610.mail.in.yahoo.com>
Message-ID: <42bc98300609261242x426e6e3ese713c90987104e23@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060926/c8ba6457/attachment.pl 

From mothsailor at googlemail.com  Tue Sep 26 21:45:40 2006
From: mothsailor at googlemail.com (David Barron)
Date: Tue, 26 Sep 2006 20:45:40 +0100
Subject: [R] colClasses: supressed 'NA'
In-Reply-To: <loom.20060926T205212-554@post.gmane.org>
References: <loom.20060926T205212-554@post.gmane.org>
Message-ID: <815b70590609261245w49a24975gce0aa65ba02eda35@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060926/07ce620b/attachment.pl 

From murdoch at stats.uwo.ca  Tue Sep 26 21:48:33 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 26 Sep 2006 15:48:33 -0400
Subject: [R] New project: littler for GNU R
In-Reply-To: <45195D97.8010402@vanderbilt.edu>
References: <45195D97.8010402@vanderbilt.edu>
Message-ID: <45198411.7020202@stats.uwo.ca>

On 9/26/2006 1:04 PM, Jeffrey Horner wrote:
> What ?
> ======
> 
>    littler - Provides hash-bang (#!) capability for R (www.r-project.org)
> 
> 
> Why ?
> =====
> 
>    GNU R, a language and environment for statistical computing and
>    graphics, provides a wonderful system for 'programming with data'
>    as well as interactive exploratory analysis, often involving graphs.
> 
>    Sometimes, however, simple scripts are desired. While GNU R can
>    be used in batch mode, and while so-called 'here' documents can be
>    crafted, a long-standing need for a scripting front-end has often
>    been expressed by the R Community.
> 
>    littler (pronounced 'little R' and written 'r') aims to fill
>    this need.
> 
>    It can be used directly on the command-line just like, say, bc(1):
> 
> 
>          $ echo 'cat(pi^2,"\n")' | r
>          9.869604

Is there a technical reason that this couldn't work by modifying the 
script that invokes R?  That would avoid the r/R clash on MacOSX and 
Windows.  In Windows R is R.exe, not a script, so some adjustment would 
be needed there, but that shouldn't be difficult.

Duncan Murdoch

> 
>    Equivalently, commands that are to be evaluated can be given on
>    the command-line
> 
>          $ r -e 'cat(pi^2, "\n")'
>          9.869604
> 
>    But unlike bc(1), GNU R has a vast number of statistical
>    functions. For example, we can quickly compute a summary() and show
>    a stem-and-leaf plot for file sizes in a given directory via
> 
>          $ ls -l /boot | awk '!/^total/ {print $5}' | \
>               r -e 'fsizes <- as.integer(readLines());
>                  print(summary(fsizes)); stem(fsizes)'
>             Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>               13     512  110100  486900  768400 4735000
>          Loading required package: grDevices
> 
>            The decimal point is 6 digit(s) to the right of the |
> 
>            0 | 00000000000000000011112223
>            0 | 5557778899
>            1 | 112233
>            1 | 5
>            2 |
>            2 |
>            3 |
>            3 |
>            4 |
>            4 | 7
> 
> 
>    And, last but not least, this (somewhat unwieldy) expression can
>    be stored in a helper script:
> 
>          $ cat examples/fsizes.r
>          #!/usr/bin/env r
> 
>          fsizes <- as.integer(readLines())
>          print(summary(fsizes))
>          stem(fsizes)
> 
>    (where calling /usr/bin/env is a trick from Python which allows one
>    to forget whether r is installed in /usr/bin/r, /usr/local/bin/r,
>    ~/bin/r, ...)
> 
>    A few examples are provided in the source directories examples/
>    and tests/.
> 
> Where ?
> =======
> 
>    littler can either be downloaded from
> 
>        http://biostat.mc.vanderbilt.edu/LittleR
> 
>    accessed by anonymous SVN:
> 
>        $ svn co http://littler.googlecode.com/svn/trunk/ littler
> 
>    or (soon !) be gotten from Debian mirrors via
> 
>        $ agt-get install littler
> 
>    littler is known to build and run on Linux and OS X.
> 
> 
> Who ?
> =====
> 
>    Copyright (C) 2006 Jeffrey Horner and Dirk Eddelbuettel
> 
>    littler is free software; you can redistribute it and/or modify it
>    under the terms of the GNU General Public License as published by
>    the Free Software Foundation; either version 2 of the License, or
>    (at your option) any later version.
> 
>    This program is distributed in the hope that it will be useful,
>    but WITHOUT ANY WARRANTY; without even the implied warranty of
>    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
>    General Public License for more details.
> 
>    You should have received a copy of the GNU General Public
>    License along with this program; if not, write to the Free
>    Software Foundation, Inc., 59 Temple Place, Suite 330, Boston,
>    MA  02111-1307  USA
> 
>    Comments are welcome, as are are suggestions, bug fixes, or patches.
> 
> 
>      - Jeffrey Horner <jeff.horner at vanderbilt.edu>
>      - Dirk Eddelbuettel <edd at debian.org>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ligges at statistik.uni-dortmund.de  Tue Sep 26 21:51:40 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 26 Sep 2006 21:51:40 +0200
Subject: [R] colClasses: supressed 'NA'
In-Reply-To: <loom.20060926T205212-554@post.gmane.org>
References: <loom.20060926T205212-554@post.gmane.org>
Message-ID: <451984CC.20306@statistik.uni-dortmund.de>



Anupam Tyagi wrote:
> Hi,
> 
> The colClasses seem to be supressing 'NA' vlaues. How do I fix this?
> 
> R script and first 5 lines of output is below.
> 
> File "test2.dat" has blanks that are read as "NA" when I do not use
> 'colClasses', but as blanks when I use 'colClasses'.


Well, you say it should be a factor, hence " " is taken as a level. 
Otherwise you have to specify na.string = " ".

Uwe Ligges


> temp.df <- read.fwf("test2.dat", width=c(10,1,1,1,1,2,2,3,3,1),  
> col.names=c("psu","losewt","maintain","fewcal","phyact","age","income","weight",
> "wtdesire","gender"),
> colClasses=c("factor","factor","factor","factor","factor","numeric","factor",
> "numeric","numeric","factor"),
> nrows=270000, comment.char="")
> 
> temp.df
>            psu losewt maintain fewcal phyact age income weight wtdesire gender
> 1   2003009323      2        2                52     05    220      220      1
> 2   2003005181      2        1      2      2  58     08    165      145      2
> 3   2003015942      2        1      4      1  76     05    142      130      2
> 4   2003011406      2        1      3      1  43     03    110      110      2
> 5   2003006786      1               4      1  49     06    178      145      2
> 
> ? why am I not getting missing values when I use 'colClasses'?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From edd at debian.org  Tue Sep 26 22:19:29 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 26 Sep 2006 15:19:29 -0500
Subject: [R] New project: littler for GNU R
In-Reply-To: <45198411.7020202@stats.uwo.ca>
References: <45195D97.8010402@vanderbilt.edu> <45198411.7020202@stats.uwo.ca>
Message-ID: <17689.35665.468722.586503@basebud.nulle.part>


On 26 September 2006 at 15:48, Duncan Murdoch wrote:
| On 9/26/2006 1:04 PM, Jeffrey Horner wrote:
| >    It can be used directly on the command-line just like, say, bc(1):
| > 
| > 
| >          $ echo 'cat(pi^2,"\n")' | r
| >          9.869604
| 
| Is there a technical reason that this couldn't work by modifying the 
| script that invokes R?  That would avoid the r/R clash on MacOSX and 
| Windows.  In Windows R is R.exe, not a script, so some adjustment would 
| be needed there, but that shouldn't be difficult.

Quite possible. We would surely encourage it.

We'd be happy to retire littler to the dustbin when `the real R' can do this
too.  Until then, littler appear to serve one of us rather well (as R still
can't do shebang-style scripts), and may hence be of interest to others too.

Regards, Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From ripley at stats.ox.ac.uk  Tue Sep 26 22:30:41 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 26 Sep 2006 21:30:41 +0100 (BST)
Subject: [R] Building R for Windows with ATLAS
In-Reply-To: <41b60e190609261239g2f547805x7da635bfb3e7a78d@mail.gmail.com>
References: <41b60e190609261239g2f547805x7da635bfb3e7a78d@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0609262117510.5788@gannet.stats.ox.ac.uk>

On Tue, 26 Sep 2006, Giuseppe Antonaci wrote:

> I think this is not a R-devel question. Sorry to all if I'm wrong,
> please let me know.

In what sense is this not a programming question?

> I managed to build R successfully with the default BLAS but when I
> change the MKRULES to use ATLAS BLAS and set the path to
> "C:/cygwin/home/Administrador/ATLAS/lib/WinNT_ATHLONSSE2" I got the
> following error message (I'm posting only the final part, there was a
> lot of compilation before this):
>
> cp R.dll ../../bin/
> -------- Building ../../bin/Rblas.dll --------
> gcc  -shared -s -o ../../bin/Rblas.dll blas00.o dllversion.o Rblas.def \
>   -L../../bin -lR  -L"C:/WinNT_ATHLONSSE2" -lf77blas -latlas

What version of R is this?  I get

-------- Building ../../../bin/Rblas.dll --------
gcc  -shared  -o ../../../bin/Rblas.dll blas00.o 
../../gnuwin32/dllversion.o Rbl
as.def \
    -L../../../bin -lR  -L"/R/ATLAS/lib/WinNT_PM" -lf77blas -latlas -lg2c
                                                                    ^^^^^
in R-2.4.0 RC.

You probably should not need it: you need to build ATLAS without xerbla (R 
has its own), so delete xerbla.o from libf77blas.a and all should be well.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From lawrencc at debian.org  Tue Sep 26 22:50:48 2006
From: lawrencc at debian.org (Chris Lawrence)
Date: Tue, 26 Sep 2006 15:50:48 -0500
Subject: [R] New project: littler for GNU R
In-Reply-To: <m2sliey0p8.fsf@ziti.local>
References: <45195D97.8010402@vanderbilt.edu> <m2sliey0p8.fsf@ziti.local>
Message-ID: <e2e0e3d30609261350o4be61544lefd93f1c92d3473a@mail.gmail.com>

On 9/26/06, Seth Falcon <sfalcon at fhcrc.org> wrote:
> Wow, looks neat.
>
> OS X users will be unhappy with your naming choice as the default
> filesystem there is not case-sensitive :-(
>
> IOW, r and R do the same thing.  I would expect it to otherwise work
> on OS X so a change of some sort might be worthwhile.

Installing as 'littler' on OS X might be a reasonable solution.

Then again, adapting /usr/bin/R to have a python-style -c switch might
be the best long-term solution for R 2.5+.


Chris, waiting for apt-get install littler to work :-)


From rmh at temple.edu  Tue Sep 26 23:02:06 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Tue, 26 Sep 2006 17:02:06 -0400 (EDT)
Subject: [R] New project: littler for GNU R
Message-ID: <20060926170206.BIU10488@po-d.temple.edu>

I like this plan and have now played with the concept.  I did the following
on Windows in cygwin.  It would also work in Unix, and I think could be tickled
to work on the standard MS cmd line in Windows.  It would certainly work
on Windows with a Windows-native port of the basic unix utilities.

echo 'options(echo=FALSE);cat(pi^2,"\n")' | Rterm --no-save 

This produces an output file, that normally shows up in the *shell*
buffer, but could be redirected.   The obvious place to redirect it to is
awk with a script to filter out everything above the echo of the options()
line. 

The only change to R needed to remove the need for an awk script
is to suppress the display of the copyright message and startup
information.  I suppose that could be done with a new
 --suppress-startup-info argument to Rterm.

The other optimizations that Jeffrey and Dirk have, such as
suppressing the loading of many of the standard packages,
would also need to be done.

Very good work and concept.

Rich


From helprhelp at gmail.com  Tue Sep 26 23:09:09 2006
From: helprhelp at gmail.com (Weiwei Shi)
Date: Tue, 26 Sep 2006 17:09:09 -0400
Subject: [R] 5 binary_class models vs one 5-class model
Message-ID: <cdf817830609261409m3c51c872kb5e212efe9dfe78@mail.gmail.com>

Hi,

I apologize this question is not very r-related, but believe many
people using R are expertised at or interested to know the answer to
the following question.

I am having a problem in classification. In bioinformatics study, we
always ends with a limited size of samples. While in algorithms, some
specific algorithm cannot handle modeling with more than 2 classes
problem. For the time being, not considering those limitations, I just
have a general question like this:

suppose I have a problem for classification, which involves 5 classes.
I am wondering if there is a general research comparison on which
approach is more accurate: building 5 binary_class models or building
one 5-class model (suppose cost (penalty) is same when accuracy is
estimated).

An extended or more practical question, in bioinformatics, if you do
not have many samples but you are having such problem, what approach
will you take?

thanks,

-- 
Weiwei Shi, Ph.D
Research Scientist
GeneGO, Inc.

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III


From ggrothendieck at gmail.com  Tue Sep 26 23:15:36 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 26 Sep 2006 17:15:36 -0400
Subject: [R] New project: littler for GNU R
In-Reply-To: <20060926170206.BIU10488@po-d.temple.edu>
References: <20060926170206.BIU10488@po-d.temple.edu>
Message-ID: <971536df0609261415u2541ba7as600d5de065ff0fc7@mail.gmail.com>

The way it should work IMHO is that one can write any of these
(in analogy to awk/perl/etc.):

R -f myprog.R mydata.dat
R -f myprog.R < mydata.dat
cat mydata.dat | R -f myprog.R # or analogously on Windows
R -e "...some.R.code... " mydata.dat
R -e "...some.R.code... " < mydata.dat

and there should be a simple way for myprog.R to read the
input data that does not require that it know whether it
was specified on the command line or redirected.

On 9/26/06, Richard M. Heiberger <rmh at temple.edu> wrote:
> I like this plan and have now played with the concept.  I did the following
> on Windows in cygwin.  It would also work in Unix, and I think could be tickled
> to work on the standard MS cmd line in Windows.  It would certainly work
> on Windows with a Windows-native port of the basic unix utilities.
>
> echo 'options(echo=FALSE);cat(pi^2,"\n")' | Rterm --no-save
>
> This produces an output file, that normally shows up in the *shell*
> buffer, but could be redirected.   The obvious place to redirect it to is
> awk with a script to filter out everything above the echo of the options()
> line.
>
> The only change to R needed to remove the need for an awk script
> is to suppress the display of the copyright message and startup
> information.  I suppose that could be done with a new
>  --suppress-startup-info argument to Rterm.
>
> The other optimizations that Jeffrey and Dirk have, such as
> suppressing the loading of many of the standard packages,
> would also need to be done.
>
> Very good work and concept.
>
> Rich
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ripley at stats.ox.ac.uk  Tue Sep 26 23:16:19 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 26 Sep 2006 22:16:19 +0100 (BST)
Subject: [R] New project: littler for GNU R
In-Reply-To: <20060926170206.BIU10488@po-d.temple.edu>
References: <20060926170206.BIU10488@po-d.temple.edu>
Message-ID: <Pine.LNX.4.64.0609262214520.6529@gannet.stats.ox.ac.uk>

On Tue, 26 Sep 2006, Richard M. Heiberger wrote:

> I like this plan and have now played with the concept.  I did the following
> on Windows in cygwin.  It would also work in Unix, and I think could be tickled
> to work on the standard MS cmd line in Windows.  It would certainly work
> on Windows with a Windows-native port of the basic unix utilities.
>
> echo 'options(echo=FALSE);cat(pi^2,"\n")' | Rterm --no-save
>
> This produces an output file, that normally shows up in the *shell*
> buffer, but could be redirected.   The obvious place to redirect it to is
> awk with a script to filter out everything above the echo of the options()
> line.
>
> The only change to R needed to remove the need for an awk script
> is to suppress the display of the copyright message and startup
> information.  I suppose that could be done with a new
> --suppress-startup-info argument to Rterm.

It is called --slave.

> The other optimizations that Jeffrey and Dirk have, such as
> suppressing the loading of many of the standard packages,
> would also need to be done.

Rterm --slave R_DEFAULT_PACKAHES=NULL

and variables is already widely used in the R build process.

>
> Very good work and concept.
>
> Rich
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From AnupTyagi at yahoo.com  Tue Sep 26 23:19:19 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Tue, 26 Sep 2006 21:19:19 +0000 (UTC)
Subject: [R] colClasses: supressed 'NA'
References: <loom.20060926T205212-554@post.gmane.org>
	<451984CC.20306@statistik.uni-dortmund.de>
Message-ID: <loom.20060926T231629-409@post.gmane.org>

Uwe Ligges <ligges <at> statistik.uni-dortmund.de> writes:

> Well, you say it should be a factor, hence " " is taken as a level. 

And why not " " a level. Thanks for drawing my attention to it. It is common
mistake that is easy to slip attention. Thanks a lot. Anupam.


From emgt_r at hotmail.com  Tue Sep 26 23:20:27 2006
From: emgt_r at hotmail.com (Joe Moore)
Date: Tue, 26 Sep 2006 17:20:27 -0400
Subject: [R] Lattice strip labels for two factors
In-Reply-To: <eb555e660609231230g70f517b4w1289afbbb67359f0@mail.gmail.com>
Message-ID: <BAY104-F38C51F2D64AFA5AA7DADADF9250@phx.gbl>

Dear All:

In the following code which I modified from previous question, in addition 
to show the fact1 level names (y, b, r) in strips, I also want to have a 
color bar to indicate the state of every panel (in this example, y 
correspods to 1, and b, r correspond to 0). Does anyone have a quick 
solution?

Thanks





df <- expand.grid("fact1"=c("y","b","r"),
"fact2"=c"far","por","lis","set"), "year"=1991:2000, "value"= NA)
df[,"value"] <- sample(1:50, 120, replace=TRUE)
df$state <- 0
df$state[df$fact1=="y"] <- 1

require("lattice")
xyplot( value ~ year | fact1, data=df, type="b", subset= fact2=="far",
	      strip = strip.custom(bg=gray.colors(1,0.95), 
factor.levels=c("yellow",  "black", "red")), layout=c(1,3))

_________________________________________________________________
Share your special moments by uploading 500 photos per month to Windows Live 
Spaces


From deepayan.sarkar at gmail.com  Tue Sep 26 23:31:49 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Tue, 26 Sep 2006 14:31:49 -0700
Subject: [R] New project: littler for GNU R
In-Reply-To: <20060926170206.BIU10488@po-d.temple.edu>
References: <20060926170206.BIU10488@po-d.temple.edu>
Message-ID: <eb555e660609261431u58e17564tc4dc71c24af5a5aa@mail.gmail.com>

On 9/26/06, Richard M. Heiberger <rmh at temple.edu> wrote:
> I like this plan and have now played with the concept.  I did the following
> on Windows in cygwin.  It would also work in Unix, and I think could be tickled
> to work on the standard MS cmd line in Windows.  It would certainly work
> on Windows with a Windows-native port of the basic unix utilities.
>
> echo 'options(echo=FALSE);cat(pi^2,"\n")' | Rterm --no-save
>
> This produces an output file, that normally shows up in the *shell*
> buffer, but could be redirected.   The obvious place to redirect it to is
> awk with a script to filter out everything above the echo of the options()
> line.

It seems to me that a big difference between this and littler is how
stdin is treated. How would you implement the fsizer.r example using
this concept?

> The only change to R needed to remove the need for an awk script
> is to suppress the display of the copyright message and startup
> information.  I suppose that could be done with a new
>  --suppress-startup-info argument to Rterm.

I typically use

--vanilla --slave

(which I assume would work on Windows too).

> The other optimizations that Jeffrey and Dirk have, such as
> suppressing the loading of many of the standard packages,
> would also need to be done.
>
> Very good work and concept.
>
> Rich
>


From deepayan.sarkar at gmail.com  Tue Sep 26 23:47:37 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Tue, 26 Sep 2006 14:47:37 -0700
Subject: [R] Lattice strip labels for two factors
In-Reply-To: <BAY104-F38C51F2D64AFA5AA7DADADF9250@phx.gbl>
References: <eb555e660609231230g70f517b4w1289afbbb67359f0@mail.gmail.com>
	<BAY104-F38C51F2D64AFA5AA7DADADF9250@phx.gbl>
Message-ID: <eb555e660609261447ud2c8c1av624a4f0ff67dae24@mail.gmail.com>

On 9/26/06, Joe Moore <emgt_r at hotmail.com> wrote:
> Dear All:
>
> In the following code which I modified from previous question,

Perhaps you should also have checked if it runs after the modification.

> in addition
> to show the fact1 level names (y, b, r) in strips, I also want to have a
> color bar to indicate the state of every panel (in this example, y
> correspods to 1, and b, r correspond to 0). Does anyone have a quick
> solution?

No, but this might give you a hint (you need to write a suitable panel
function):

xyplot(value ~ year | fact1:factor(state),
       data=df, type="b",
       subset= fact2=="far",
       layout=c(1,3))

Deepayan

>
> Thanks
>
>
>
>
>
> df <- expand.grid("fact1"=c("y","b","r"),
> "fact2"=c"far","por","lis","set"), "year"=1991:2000, "value"= NA)
> df[,"value"] <- sample(1:50, 120, replace=TRUE)
> df$state <- 0
> df$state[df$fact1=="y"] <- 1
>
> require("lattice")
> xyplot( value ~ year | fact1, data=df, type="b", subset= fact2=="far",
>               strip = strip.custom(bg=gray.colors(1,0.95),
> factor.levels=c("yellow",  "black", "red")), layout=c(1,3))
>


From f.harrell at vanderbilt.edu  Wed Sep 27 00:58:03 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 26 Sep 2006 17:58:03 -0500
Subject: [R] Bug in formals<-
Message-ID: <4519B07B.20300@vanderbilt.edu>

I think this is new since a previous version of R:

 > h <- function(x, trantab) trantab[x]
 > w <- 6:4
 > names(w) <- c('cat','dog','giraffe')
 > w
     cat     dog giraffe
       6       5       4
 >
 > formals(h) <- list(x=numeric(0), trantab=w)
 > h
function (x = numeric(0), trantab = c(6, 5, 4))
trantab[x]

You can see that the names have been dropped from trantab's default 
values.  I don't see a workaround but it seems to need fixing.


Version 2.3.1 (2006-06-01)
i486-pc-linux-gnu

attached base packages:
[1] "grid"      "methods"   "stats"     "graphics"  "grDevices" "utils"
[7] "datasets"  "base"

other attached packages:
   lattice   acepack     Hmisc
"0.13-10" "1.3-2.2"  "3.0-12"


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From deepayan.sarkar at gmail.com  Wed Sep 27 01:09:58 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Tue, 26 Sep 2006 16:09:58 -0700
Subject: [R] Bug in formals<-
In-Reply-To: <4519B07B.20300@vanderbilt.edu>
References: <4519B07B.20300@vanderbilt.edu>
Message-ID: <eb555e660609261609i40cbd027n1d3df9a81383434c@mail.gmail.com>

On 9/26/06, Frank E Harrell Jr <f.harrell at vanderbilt.edu> wrote:
> I think this is new since a previous version of R:
>
>  > h <- function(x, trantab) trantab[x]
>  > w <- 6:4
>  > names(w) <- c('cat','dog','giraffe')
>  > w
>      cat     dog giraffe
>        6       5       4
>  >
>  > formals(h) <- list(x=numeric(0), trantab=w)
>  > h
> function (x = numeric(0), trantab = c(6, 5, 4))
> trantab[x]
>
> You can see that the names have been dropped from trantab's default
> values.

Are you sure? I get

> formals(h)
$x
numeric(0)

$trantab
    cat     dog giraffe
      6       5       4

> h(1)
cat
  6

R version 2.4.0 beta (2006-09-21 r39463)
x86_64-unknown-linux-gnu

-Deepayan


> Version 2.3.1 (2006-06-01)
> i486-pc-linux-gnu
>
> attached base packages:
> [1] "grid"      "methods"   "stats"     "graphics"  "grDevices" "utils"
> [7] "datasets"  "base"
>
> other attached packages:
>    lattice   acepack     Hmisc
> "0.13-10" "1.3-2.2"  "3.0-12"
>
>
> --
> Frank E Harrell Jr   Professor and Chair           School of Medicine
>                       Department of Biostatistics   Vanderbilt University
>


From g.abraham at ms.unimelb.edu.au  Wed Sep 27 01:29:40 2006
From: g.abraham at ms.unimelb.edu.au (Gad Abraham)
Date: Wed, 27 Sep 2006 09:29:40 +1000
Subject: [R] Initialising Mersenne-Twister with one integer
In-Reply-To: <Pine.LNX.4.64.0609250943090.28507@gannet.stats.ox.ac.uk>
References: <4517943A.6050301@ms.unimelb.edu.au>
	<Pine.LNX.4.64.0609250943090.28507@gannet.stats.ox.ac.uk>
Message-ID: <4519B7E4.4080509@ms.unimelb.edu.au>

Prof Brian Ripley wrote:
> On Mon, 25 Sep 2006, Gad Abraham wrote:
> 
>> Hi,
>>
>> It seems to me that the Mersenne-Twister PRNG can be initialised using
>> one integer instead of 624 integers, since inside RNG.c code there's a
>> function defined as MT_sgenrand(Int32).
>>
>> How do I actually set this seed within R?
> 
> set.seed(), on the help page for ?.Random.seed.
> 
>> I've tried:
>>
>> > .Random.seed <- c(3, 1)
>> > runif(1)
>> Error in runif(1) : .Random.seed has wrong length
> 
>> From the help page
> 
>      '.Random.seed' is an integer vector, containing the random number
>      generator (RNG) *state* for random number generation in R.  It can
>      be saved and restored, but should not be altered by the user.

Ah yes, now I see that .Random.seed is the state, not the actual seed.

> 
>> In addition, is '3' actually the correct rng.kind for the 
>> Mersenne-Twister?
>>
>> I'm using R version 2.2.1, 2005-12-20 on Ubuntu Dapper Linux 686.
> 
> Not current, but I suspect the help page is the same in that version.
> 

Thanks for clarifying that, now I can ask my next question:

I would like to set the same seed in R and in another implementation of 
the Mersenne-Twister (MersenneTwister.java by Sean Luke, at 
http://cs.gmu.edu/~sean/research/mersenne/MersenneTwister.java), in 
order to get the same set of random deviates.

In Java:
    MersenneTwister mt = new MersenneTwister(1); // set seed
    System.out.println("result = " + mt.nextDouble());
    // result is 0.4170220046815992

In R:

    > set.seed(1, kind = "Mersenne-Twister")
    > runif(1)
    [1] 0.2655087

Why are the random deviates different?



Thanks,
Gad


-- 
Gad Abraham
Department of Mathematics and Statistics
University of Melbourne
Parkville 3010, Victoria, Australia
email: g.abraham at ms.unimelb.edu.au
web: http://www.ms.unimelb.edu.au/~gabraham


From f.harrell at vanderbilt.edu  Wed Sep 27 01:32:48 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 26 Sep 2006 18:32:48 -0500
Subject: [R] Bug in formals<-
In-Reply-To: <eb555e660609261609i40cbd027n1d3df9a81383434c@mail.gmail.com>
References: <4519B07B.20300@vanderbilt.edu>
	<eb555e660609261609i40cbd027n1d3df9a81383434c@mail.gmail.com>
Message-ID: <4519B8A0.9030603@vanderbilt.edu>

Deepayan Sarkar wrote:
> On 9/26/06, Frank E Harrell Jr <f.harrell at vanderbilt.edu> wrote:
>> I think this is new since a previous version of R:
>>
>>  > h <- function(x, trantab) trantab[x]
>>  > w <- 6:4
>>  > names(w) <- c('cat','dog','giraffe')
>>  > w
>>      cat     dog giraffe
>>        6       5       4
>>  >
>>  > formals(h) <- list(x=numeric(0), trantab=w)
>>  > h
>> function (x = numeric(0), trantab = c(6, 5, 4))
>> trantab[x]
>>
>> You can see that the names have been dropped from trantab's default
>> values.
> 
> Are you sure? I get
> 
>> formals(h)
> $x
> numeric(0)
> 
> $trantab
>    cat     dog giraffe
>      6       5       4
> 
>> h(1)
> cat
>  6
> 
> R version 2.4.0 beta (2006-09-21 r39463)
> x86_64-unknown-linux-gnu
> 
> -Deepayan

Deepayan -

You are correct.  h('cat') is 6 as intended.  I just looked at the 
function definition - the names attribute doesn't show for some reason. 
  I was expecting function(..., trantab=c(cat=6, ..).

Thanks

Frank

> 
> 
>> Version 2.3.1 (2006-06-01)
>> i486-pc-linux-gnu
>>
>> attached base packages:
>> [1] "grid"      "methods"   "stats"     "graphics"  "grDevices" "utils"
>> [7] "datasets"  "base"
>>
>> other attached packages:
>>    lattice   acepack     Hmisc
>> "0.13-10" "1.3-2.2"  "3.0-12"
>>
>


From ggrothendieck at gmail.com  Wed Sep 27 03:46:14 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 26 Sep 2006 21:46:14 -0400
Subject: [R] Bug in formals<-
In-Reply-To: <4519B8A0.9030603@vanderbilt.edu>
References: <4519B07B.20300@vanderbilt.edu>
	<eb555e660609261609i40cbd027n1d3df9a81383434c@mail.gmail.com>
	<4519B8A0.9030603@vanderbilt.edu>
Message-ID: <971536df0609261846t3ba7a85ctd5c8b1684e174e77@mail.gmail.com>

This seems to be related to using c to define transtab.
If we use list in place of c then it displays ok:

> h <- function(x, trantab) transtab[x]
> formals(h) <- list(x = numeric(0), transtab = c(cat = 6, dog = 5))
> print(h) # bad display
function (x = numeric(0), transtab = c(6, 5))
transtab[x]
> h("cat") # runs ok
cat
  6
> formals(h) <- list(x = numeric(0), transtab = list(cat = 6, dog = 5))
> print(h) # now display is ok
function (x = numeric(0), transtab = list(cat = 6, dog = 5))
transtab[x]
> h("cat") # runs ok
$cat
[1] 6



On 9/26/06, Frank E Harrell Jr <f.harrell at vanderbilt.edu> wrote:
> Deepayan Sarkar wrote:
> > On 9/26/06, Frank E Harrell Jr <f.harrell at vanderbilt.edu> wrote:
> >> I think this is new since a previous version of R:
> >>
> >>  > h <- function(x, trantab) trantab[x]
> >>  > w <- 6:4
> >>  > names(w) <- c('cat','dog','giraffe')
> >>  > w
> >>      cat     dog giraffe
> >>        6       5       4
> >>  >
> >>  > formals(h) <- list(x=numeric(0), trantab=w)
> >>  > h
> >> function (x = numeric(0), trantab = c(6, 5, 4))
> >> trantab[x]
> >>
> >> You can see that the names have been dropped from trantab's default
> >> values.
> >
> > Are you sure? I get
> >
> >> formals(h)
> > $x
> > numeric(0)
> >
> > $trantab
> >    cat     dog giraffe
> >      6       5       4
> >
> >> h(1)
> > cat
> >  6
> >
> > R version 2.4.0 beta (2006-09-21 r39463)
> > x86_64-unknown-linux-gnu
> >
> > -Deepayan
>
> Deepayan -
>
> You are correct.  h('cat') is 6 as intended.  I just looked at the
> function definition - the names attribute doesn't show for some reason.
>  I was expecting function(..., trantab=c(cat=6, ..).
>
> Thanks
>
> Frank
>
> >
> >
> >> Version 2.3.1 (2006-06-01)
> >> i486-pc-linux-gnu
> >>
> >> attached base packages:
> >> [1] "grid"      "methods"   "stats"     "graphics"  "grDevices" "utils"
> >> [7] "datasets"  "base"
> >>
> >> other attached packages:
> >>    lattice   acepack     Hmisc
> >> "0.13-10" "1.3-2.2"  "3.0-12"
> >>
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jeff.horner at vanderbilt.edu  Wed Sep 27 03:59:33 2006
From: jeff.horner at vanderbilt.edu (Jeffrey Horner)
Date: Tue, 26 Sep 2006 20:59:33 -0500
Subject: [R] New project: littler for GNU R
In-Reply-To: <45198411.7020202@stats.uwo.ca>
References: <45195D97.8010402@vanderbilt.edu> <45198411.7020202@stats.uwo.ca>
Message-ID: <4519DB05.20909@vanderbilt.edu>

Duncan Murdoch wrote:
> On 9/26/2006 1:04 PM, Jeffrey Horner wrote:

[...]

>>    It can be used directly on the command-line just like, say, bc(1):
>>
>>
>>          $ echo 'cat(pi^2,"\n")' | r
>>          9.869604
> 
> Is there a technical reason that this couldn't work by modifying the 
> script that invokes R?  That would avoid the r/R clash on MacOSX and 
> Windows.  In Windows R is R.exe, not a script, so some adjustment would 
> be needed there, but that shouldn't be difficult.

In fact, it does work:

$ echo 'cat(pi^2,"\n")' | R --vanilla --slave
9.869604

but what's more compelling is the ability to utilize the UNIX hash-bang 
mechanism.

Jeff
-- 
http://biostat.mc.vanderbilt.edu/JeffreyHorner


From jeff.horner at vanderbilt.edu  Wed Sep 27 04:16:05 2006
From: jeff.horner at vanderbilt.edu (Jeffrey Horner)
Date: Tue, 26 Sep 2006 21:16:05 -0500
Subject: [R] New project: littler for GNU R
In-Reply-To: <m28xk6wfej.fsf@ziti.local>
References: <45195D97.8010402@vanderbilt.edu>
	<m2sliey0p8.fsf@ziti.local>	<45196D6B.3020208@vanderbilt.edu>
	<m28xk6wfej.fsf@ziti.local>
Message-ID: <4519DEE5.7090901@vanderbilt.edu>

Seth Falcon wrote:
> Jeffrey Horner <jeff.horner at vanderbilt.edu> writes:

[...]

>> littler will install into /usr/local/bin by default, so I don't think
>> there's a clash with the Mac binary provided by CRAN, right?
> 
> It depends what you mean by clash :-)
> 
> If both are on the PATH, then you get the first one, I suspect, when
> running either 'R' or 'r'.  I haven't tested this bit yet, but on my
> OS X laptop I can invoke a new R session using either 'R' or 'r'
> (using an R built from source, not the R GUI app thingie).

Good point, but the executable path can be named absolutely in hash-bang 
scripts. Relative paths work as well with the use of '/usr/bin/env 
program' as is described in the littler announcement, but then you don't
get to pass arguments to 'program', just to the hash-bang script.

> 
> So IMO, a different name or an integration into the R script in some
> way would be a big improvement.

But I'd like to know why there's an R script in the first place. Why not 
just an executable as on windows?

> 
> 'r' is cute, but going down the road of tools with the same name
> except for caps leads to confusion (for me).  For example, R CMD
> build/INSTALL still catches me up after a number of years.

That's a different problem than case-sensitivity. The word 'build' must 
have had a different semantic than INSTALL, and I'm not sure why one was 
all caps and the other isn't.

Jeff
-- 
http://biostat.mc.vanderbilt.edu/JeffreyHorner


From ggrothendieck at gmail.com  Wed Sep 27 04:17:43 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 26 Sep 2006 22:17:43 -0400
Subject: [R] New project: littler for GNU R
In-Reply-To: <45198411.7020202@stats.uwo.ca>
References: <45195D97.8010402@vanderbilt.edu> <45198411.7020202@stats.uwo.ca>
Message-ID: <971536df0609261917pd5c5518l694f47299cfb81eb@mail.gmail.com>

The real problem is that one wants to pipe the data in, not the
R source.  The idea is that one successively transforms the
data in successive elements of the pipeline.

For example one might want to write cut, grep, etc. in R rather than
in C.

This has been on my year-end wishlist for some time.

On 9/26/06, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> On 9/26/2006 1:04 PM, Jeffrey Horner wrote:
> > What ?
> > ======
> >
> >    littler - Provides hash-bang (#!) capability for R (www.r-project.org)
> >
> >
> > Why ?
> > =====
> >
> >    GNU R, a language and environment for statistical computing and
> >    graphics, provides a wonderful system for 'programming with data'
> >    as well as interactive exploratory analysis, often involving graphs.
> >
> >    Sometimes, however, simple scripts are desired. While GNU R can
> >    be used in batch mode, and while so-called 'here' documents can be
> >    crafted, a long-standing need for a scripting front-end has often
> >    been expressed by the R Community.
> >
> >    littler (pronounced 'little R' and written 'r') aims to fill
> >    this need.
> >
> >    It can be used directly on the command-line just like, say, bc(1):
> >
> >
> >          $ echo 'cat(pi^2,"\n")' | r
> >          9.869604
>
> Is there a technical reason that this couldn't work by modifying the
> script that invokes R?  That would avoid the r/R clash on MacOSX and
> Windows.  In Windows R is R.exe, not a script, so some adjustment would
> be needed there, but that shouldn't be difficult.
>
> Duncan Murdoch
>
> >
> >    Equivalently, commands that are to be evaluated can be given on
> >    the command-line
> >
> >          $ r -e 'cat(pi^2, "\n")'
> >          9.869604
> >
> >    But unlike bc(1), GNU R has a vast number of statistical
> >    functions. For example, we can quickly compute a summary() and show
> >    a stem-and-leaf plot for file sizes in a given directory via
> >
> >          $ ls -l /boot | awk '!/^total/ {print $5}' | \
> >               r -e 'fsizes <- as.integer(readLines());
> >                  print(summary(fsizes)); stem(fsizes)'
> >             Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
> >               13     512  110100  486900  768400 4735000
> >          Loading required package: grDevices
> >
> >            The decimal point is 6 digit(s) to the right of the |
> >
> >            0 | 00000000000000000011112223
> >            0 | 5557778899
> >            1 | 112233
> >            1 | 5
> >            2 |
> >            2 |
> >            3 |
> >            3 |
> >            4 |
> >            4 | 7
> >
> >
> >    And, last but not least, this (somewhat unwieldy) expression can
> >    be stored in a helper script:
> >
> >          $ cat examples/fsizes.r
> >          #!/usr/bin/env r
> >
> >          fsizes <- as.integer(readLines())
> >          print(summary(fsizes))
> >          stem(fsizes)
> >
> >    (where calling /usr/bin/env is a trick from Python which allows one
> >    to forget whether r is installed in /usr/bin/r, /usr/local/bin/r,
> >    ~/bin/r, ...)
> >
> >    A few examples are provided in the source directories examples/
> >    and tests/.
> >
> > Where ?
> > =======
> >
> >    littler can either be downloaded from
> >
> >        http://biostat.mc.vanderbilt.edu/LittleR
> >
> >    accessed by anonymous SVN:
> >
> >        $ svn co http://littler.googlecode.com/svn/trunk/ littler
> >
> >    or (soon !) be gotten from Debian mirrors via
> >
> >        $ agt-get install littler
> >
> >    littler is known to build and run on Linux and OS X.
> >
> >
> > Who ?
> > =====
> >
> >    Copyright (C) 2006 Jeffrey Horner and Dirk Eddelbuettel
> >
> >    littler is free software; you can redistribute it and/or modify it
> >    under the terms of the GNU General Public License as published by
> >    the Free Software Foundation; either version 2 of the License, or
> >    (at your option) any later version.
> >
> >    This program is distributed in the hope that it will be useful,
> >    but WITHOUT ANY WARRANTY; without even the implied warranty of
> >    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
> >    General Public License for more details.
> >
> >    You should have received a copy of the GNU General Public
> >    License along with this program; if not, write to the Free
> >    Software Foundation, Inc., 59 Temple Place, Suite 330, Boston,
> >    MA  02111-1307  USA
> >
> >    Comments are welcome, as are are suggestions, bug fixes, or patches.
> >
> >
> >      - Jeffrey Horner <jeff.horner at vanderbilt.edu>
> >      - Dirk Eddelbuettel <edd at debian.org>
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From miltinho_astronauta at yahoo.com.br  Wed Sep 27 04:30:29 2006
From: miltinho_astronauta at yahoo.com.br (Milton Cezar)
Date: Tue, 26 Sep 2006 23:30:29 -0300 (ART)
Subject: [R] matrix with additional upper, botton, left and right cells
Message-ID: <20060927023029.85728.qmail@web53403.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060926/d6f4b42c/attachment.pl 

From edd at debian.org  Wed Sep 27 05:08:17 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 26 Sep 2006 22:08:17 -0500
Subject: [R] New project: littler for GNU R
In-Reply-To: <971536df0609261917pd5c5518l694f47299cfb81eb@mail.gmail.com>
References: <45195D97.8010402@vanderbilt.edu> <45198411.7020202@stats.uwo.ca>
	<971536df0609261917pd5c5518l694f47299cfb81eb@mail.gmail.com>
Message-ID: <17689.60193.816367.78035@basebud.nulle.part>


On 26 September 2006 at 22:17, Gabor Grothendieck wrote:
| The real problem is that one wants to pipe the data in, not the
| R source.  The idea is that one successively transforms the
| data in successive elements of the pipeline.

But that is what our filesize example does::

| On 9/26/06, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
| > On 9/26/2006 1:04 PM, Jeffrey Horner wrote:
[...]
| > >    But unlike bc(1), GNU R has a vast number of statistical
| > >    functions. For example, we can quickly compute a summary() and show
| > >    a stem-and-leaf plot for file sizes in a given directory via
| > >
| > >          $ ls -l /boot | awk '!/^total/ {print $5}' | \
| > >               r -e 'fsizes <- as.integer(readLines());
| > >                  print(summary(fsizes)); stem(fsizes)'
| > >             Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
| > >               13     512  110100  486900  768400 4735000
| > >          Loading required package: grDevices
| > >
| > >            The decimal point is 6 digit(s) to the right of the |
| > >
| > >            0 | 00000000000000000011112223
| > >            0 | 5557778899
| > >            1 | 112233
| > >            1 | 5
| > >            2 |
| > >            2 |
| > >            3 |
| > >            3 |
| > >            4 |
| > >            4 | 7

Data to be processed on stdin, command via -e 'some long expression'.

To make it simpler, here is a somewhat useless example of r piping into r
(which I've indented for readability):

  $  r -e 'set.seed(42); sapply(rnorm(5),function(x) cat(x,"\n"))' |  \
		 r -e 'cat(sum(abs(as.numeric(readLines()))), "\n")'
  3.335916

Isn't that something where, to quote you, "one wants to pipe the data in, not
the R source" ?  

Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From jholtman at gmail.com  Wed Sep 27 05:28:02 2006
From: jholtman at gmail.com (jim holtman)
Date: Tue, 26 Sep 2006 23:28:02 -0400
Subject: [R] matrix with additional upper, botton, left and right cells
In-Reply-To: <20060927023029.85728.qmail@web53403.mail.yahoo.com>
References: <20060927023029.85728.qmail@web53403.mail.yahoo.com>
Message-ID: <644e1f320609262028p6ce42fd9xfb74d1d5677ff8d4@mail.gmail.com>

How about something like this:
> x <- matrix(1:100,10)
> x.1 <- array(-3, dim=c(12,12))
> x.1[2:11, 2:11] <- x
> x.1
      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]
 [1,]   -3   -3   -3   -3   -3   -3   -3   -3   -3    -3    -3    -3
 [2,]   -3    1   11   21   31   41   51   61   71    81    91    -3
 [3,]   -3    2   12   22   32   42   52   62   72    82    92    -3
 [4,]   -3    3   13   23   33   43   53   63   73    83    93    -3
 [5,]   -3    4   14   24   34   44   54   64   74    84    94    -3
 [6,]   -3    5   15   25   35   45   55   65   75    85    95    -3
 [7,]   -3    6   16   26   36   46   56   66   76    86    96    -3
 [8,]   -3    7   17   27   37   47   57   67   77    87    97    -3
 [9,]   -3    8   18   28   38   48   58   68   78    88    98    -3
[10,]   -3    9   19   29   39   49   59   69   79    89    99    -3
[11,]   -3   10   20   30   40   50   60   70   80    90   100    -3
[12,]   -3   -3   -3   -3   -3   -3   -3   -3   -3    -3    -3    -3
>

On 9/26/06, Milton Cezar <miltinho_astronauta at yahoo.com.br> wrote:
> Dear R Gurus,
>
>  I have a matrix dim(1000x1000) and I need create a second matrix with dim(1002x1002) and insert my first matrix at position col=2,line=2. Please, see an example below:
>
>  0050055050
>  5550000000
>  5000505005
>  5005000500
>  0055550555
>
>  and I need
>
>  333333333333
>  300500550503
>  355500000003
>  350005050053
>  350050005003
>  300555505553
>  333333333333
>
>  Thanks a lot,
>
>  miltinho
>
>  __________________________________________________
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From jamie.jarabek at gmail.com  Wed Sep 27 05:32:46 2006
From: jamie.jarabek at gmail.com (Jamie Jarabek)
Date: Tue, 26 Sep 2006 20:32:46 -0700
Subject: [R] histogram colors in lattice
Message-ID: <498ba7570609262032u7e4e9503ge8b464f19be617ad@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060926/6959c1c6/attachment.pl 

From ggrothendieck at gmail.com  Wed Sep 27 05:45:39 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 26 Sep 2006 23:45:39 -0400
Subject: [R] New project: littler for GNU R
In-Reply-To: <17689.60193.816367.78035@basebud.nulle.part>
References: <45195D97.8010402@vanderbilt.edu> <45198411.7020202@stats.uwo.ca>
	<971536df0609261917pd5c5518l694f47299cfb81eb@mail.gmail.com>
	<17689.60193.816367.78035@basebud.nulle.part>
Message-ID: <971536df0609262045w47be8288pc8c58523cb840119@mail.gmail.com>

I think this is quoted out of context. I was referring to Duncan's post
which shows an example of piping R code.

On 9/26/06, Dirk Eddelbuettel <edd at debian.org> wrote:
>
> On 26 September 2006 at 22:17, Gabor Grothendieck wrote:
> | The real problem is that one wants to pipe the data in, not the
> | R source.  The idea is that one successively transforms the
> | data in successive elements of the pipeline.
>
> But that is what our filesize example does::
>
> | On 9/26/06, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> | > On 9/26/2006 1:04 PM, Jeffrey Horner wrote:
> [...]
> | > >    But unlike bc(1), GNU R has a vast number of statistical
> | > >    functions. For example, we can quickly compute a summary() and show
> | > >    a stem-and-leaf plot for file sizes in a given directory via
> | > >
> | > >          $ ls -l /boot | awk '!/^total/ {print $5}' | \
> | > >               r -e 'fsizes <- as.integer(readLines());
> | > >                  print(summary(fsizes)); stem(fsizes)'
> | > >             Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
> | > >               13     512  110100  486900  768400 4735000
> | > >          Loading required package: grDevices
> | > >
> | > >            The decimal point is 6 digit(s) to the right of the |
> | > >
> | > >            0 | 00000000000000000011112223
> | > >            0 | 5557778899
> | > >            1 | 112233
> | > >            1 | 5
> | > >            2 |
> | > >            2 |
> | > >            3 |
> | > >            3 |
> | > >            4 |
> | > >            4 | 7
>
> Data to be processed on stdin, command via -e 'some long expression'.
>
> To make it simpler, here is a somewhat useless example of r piping into r
> (which I've indented for readability):
>
>  $  r -e 'set.seed(42); sapply(rnorm(5),function(x) cat(x,"\n"))' |  \
>                 r -e 'cat(sum(abs(as.numeric(readLines()))), "\n")'
>  3.335916
>
> Isn't that something where, to quote you, "one wants to pipe the data in, not
> the R source" ?
>
> Dirk
>
> --
> Hell, there are no rules here - we're trying to accomplish something.
>                                                  -- Thomas A. Edison
>


From ggrothendieck at gmail.com  Wed Sep 27 05:58:43 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 26 Sep 2006 23:58:43 -0400
Subject: [R] histogram colors in lattice
In-Reply-To: <498ba7570609262032u7e4e9503ge8b464f19be617ad@mail.gmail.com>
References: <498ba7570609262032u7e4e9503ge8b464f19be617ad@mail.gmail.com>
Message-ID: <971536df0609262058m4ec1b9aen4edc053129a5a9be@mail.gmail.com>

Try this:

library(lattice)
set.seed(1)  ## added for reproducibility
Start <- factor(rbinom(100,1,.5))
Answer <- 2 - rbinom(100,1,.7)

histogram(~Answer | Start,
         breaks=c(1, 1.4 ,1.6,2),
         scales=list(x=list(at=c(1.2,1.8),labels=c("Yes","No"))),
         panel = function(x, ..., panel.number, col) {  ## added this
panel function
              panel.histogram(x, ..., col = panel.number+1)
          },
         xlab="",ylab="")


On 9/26/06, Jamie Jarabek <jamie.jarabek at gmail.com> wrote:
> I have code that constructs a plot using the lattice package that looks
> something like the following toy example:
>
> library(lattice)
> Start <- factor(rbinom(100,1,.5))
> Answer <- 2 - rbinom(100,1,.7)
>
> histogram(~Answer | Start,
>          breaks=c(1, 1.4 ,1.6,2),
>          scales=list(x=list(at=c(1.2,1.8),labels=c("Yes","No"))),
>          xlab="",ylab="")
>
> I would like to have different colors for the bars in the left and right
> panel (say red and green) but I can't find a way to do this. Can anyone give
> me any advice on how to achieve this?
>
> Thanks,
> Jamie Jarabek
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Wed Sep 27 08:14:46 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 27 Sep 2006 02:14:46 -0400
Subject: [R] about the determinant of a symmetric compound matrix
In-Reply-To: <163CA7BD55F0B84AAD787CE6192698D70C599D@GANDALF.regionemarche.intra>
References: <163CA7BD55F0B84AAD787CE6192698D70C599D@GANDALF.regionemarche.intra>
Message-ID: <971536df0609262314m123d247ah5b8786a8ad4a8f5@mail.gmail.com>

If P = projection onto the one dimensional space
spanned by 1, the vector consisting of n 1's, then
using the usual formula for projections we have
P = 11'/1'1 = J/n

and writing I+cJ in terms of P we have:

I+cJ = (I-P) + (cn+1)P

which is an eigen expansion showing that
I+cJ has one eigenvalue of cn+1 and n-1
eigenvalues of 1 so its determinant, being
the product of the eigenvalues, is cn+1.
That is,

det(I+cJ) = cn+1

and we can verify that for n=5 and c=10
which should give cn+1 = 51:

> det(diag(5) + matrix(10, 5, 5))   # 10 * 5 + 1 = 51
[1] 51

Thus det(a(I+cJ)) = a^n (cn+1)


On 9/26/06, Stefano Sofia <stefano.sofia at regione.marche.it> wrote:
> Dear R users,
> even if this question is not related to an issue about R, probably some of you will be able to help me.
>
> I have a square matrix of dimension k by k with alpha on the diagonal and beta everywhee else.
> This symmetric matrix is called symmetric compound matrix and has the form
> a( I + cJ),
> where
> I is the k by k identity matrix
> J is the k by k matrix of all ones
> a = alpha - beta
> c = beta/a
>
> I need to evaluate the determinant of this matrix. Is there any algebric formula for that?
>
> thank you for your help
> Stefano
>
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From p.dalgaard at biostat.ku.dk  Wed Sep 27 08:23:58 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 27 Sep 2006 08:23:58 +0200
Subject: [R] Bug in formals<-
In-Reply-To: <4519B8A0.9030603@vanderbilt.edu>
References: <4519B07B.20300@vanderbilt.edu>
	<eb555e660609261609i40cbd027n1d3df9a81383434c@mail.gmail.com>
	<4519B8A0.9030603@vanderbilt.edu>
Message-ID: <x2slidaku9.fsf@turmalin.kubism.ku.dk>

Frank E Harrell Jr <f.harrell at vanderbilt.edu> writes:

> Deepayan Sarkar wrote:
> > On 9/26/06, Frank E Harrell Jr <f.harrell at vanderbilt.edu> wrote:
> >> I think this is new since a previous version of R:
> >>
> >>  > h <- function(x, trantab) trantab[x]
> >>  > w <- 6:4
> >>  > names(w) <- c('cat','dog','giraffe')
> >>  > w
> >>      cat     dog giraffe
> >>        6       5       4
> >>  >
> >>  > formals(h) <- list(x=numeric(0), trantab=w)
> >>  > h
> >> function (x = numeric(0), trantab = c(6, 5, 4))
> >> trantab[x]
> >>
> >> You can see that the names have been dropped from trantab's default
> >> values.
> > 
> > Are you sure? I get
> > 
> >> formals(h)
> > $x
> > numeric(0)
> > 
> > $trantab
> >    cat     dog giraffe
> >      6       5       4
> > 
> >> h(1)
> > cat
> >  6
> > 
> > R version 2.4.0 beta (2006-09-21 r39463)
> > x86_64-unknown-linux-gnu
> > 
> > -Deepayan
> 
> Deepayan -
> 
> You are correct.  h('cat') is 6 as intended.  I just looked at the 
> function definition - the names attribute doesn't show for some reason. 
>   I was expecting function(..., trantab=c(cat=6, ..).

It's a generic deparsing issue. Also:

> substitute(f(w),list(w=w))
f(c(6, 5, 4))

We should probably fix it at some point. Notice though that you can't
parse your way into such situations, only get there by computing on
the language.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From ozric at web.de  Tue Sep 26 18:33:56 2006
From: ozric at web.de (ozric at web.de)
Date: Tue, 26 Sep 2006 18:33:56 +0200
Subject: [R] multiple imputation
Message-ID: <545326496@web.de>

Hi,

is it correct that "multiple-Imputation" like  mice 
http://www.imputation.com can't understand as a standard data-mining 
task, beacuse i haven't a generalization mechanism perform the model on 
complete new and bigger dataset with a predict method!?

many thanks & regards,
christian


From sfalcon at fhcrc.org  Tue Sep 26 22:18:12 2006
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Tue, 26 Sep 2006 13:18:12 -0700
Subject: [R] New project: littler for GNU R
In-Reply-To: <45196D6B.3020208@vanderbilt.edu> (Jeffrey Horner's message of
	"Tue, 26 Sep 2006 13:11:55 -0500")
References: <45195D97.8010402@vanderbilt.edu> <m2sliey0p8.fsf@ziti.local>
	<45196D6B.3020208@vanderbilt.edu>
Message-ID: <m28xk6wfej.fsf@ziti.local>

Jeffrey Horner <jeff.horner at vanderbilt.edu> writes:

> Seth Falcon wrote:
>> Wow, looks neat.
>>
>> OS X users will be unhappy with your naming choice as the default
>> filesystem there is not case-sensitive :-(
>>
>> IOW, r and R do the same thing.  I would expect it to otherwise work
>> on OS X so a change of some sort might be worthwhile.
>
> (I'm always amazed at how I can miss the simplest details. I probably
> knew at some point that OS X shipped with a case-sensitive file
> system, which you can turn off somehow, but forgot. Thank goodness for
> peer review.)
>
> littler will install into /usr/local/bin by default, so I don't think
> there's a clash with the Mac binary provided by CRAN, right?

It depends what you mean by clash :-)

If both are on the PATH, then you get the first one, I suspect, when
running either 'R' or 'r'.  I haven't tested this bit yet, but on my
OS X laptop I can invoke a new R session using either 'R' or 'r'
(using an R built from source, not the R GUI app thingie).

So IMO, a different name or an integration into the R script in some
way would be a big improvement.  

'r' is cute, but going down the road of tools with the same name
except for caps leads to confusion (for me).  For example, R CMD
build/INSTALL still catches me up after a number of years.

+ seth


From xmeng at capitalbio.com  Wed Sep 27 10:12:50 2006
From: xmeng at capitalbio.com (XinMeng)
Date: Wed, 27 Sep 2006 16:12:50 +0800
Subject: [R] exact 95% confidence intervals
Message-ID: <359344770.26135@capitalbio.com>

Hello sir:
As to the 2*2 table format for reporting results comparing a new test to true diagnosis,when I got the sensitivity and specificity,how can I calculate the exact 95% confidence intervals (based on the binomial distribution) for sensitivity and specificity via R?

Thanks a  lot!

My best!



------------------------------
*******************************************
Xin Meng 
Capitalbio Corporation
National Engineering Research Center 
for Beijing Biochip Technology 
BioPharma-informatics & Software Dept. 
Research Engineer
Tel: +86-10-80715888/80726868-6438
Fax: +86-10-80726790
Email??xmeng at capitalbio.com 
Address:18 Life Science Parkway, 
Changping District, Beijing 102206, China


From wangtong at usc.edu  Wed Sep 27 10:24:24 2006
From: wangtong at usc.edu (Tong Wang)
Date: Wed, 27 Sep 2006 01:24:24 -0700
Subject: [R] How to pass expression as an argument
Message-ID: <ddac8be61bb36.4519d2c8@usc.edu>

Hi,
     I am writing a function and need to pass a function expression as an argument,   for instance, 
            myfun <-  function( express) {
                   x<- c(1,2,3)
                  y<-express
         }

    if I call the above function by myfun( x*2 ), I get  2  as the result,  instead of  2,4,6 ,  could someone help me to 
fix this problem ? 
    Furthermore,  is that possible to operate this expression on different variables?  for example, in myfun(), I 
might want to get  u*2,  z*2,  etc. without having to say x<-u, x<-z to match the expression.

Thanks a lot for your help.

best


From p.dalgaard at biostat.ku.dk  Wed Sep 27 10:57:15 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 27 Sep 2006 10:57:15 +0200
Subject: [R] How to pass expression as an argument
In-Reply-To: <ddac8be61bb36.4519d2c8@usc.edu>
References: <ddac8be61bb36.4519d2c8@usc.edu>
Message-ID: <x2bqp11yc4.fsf@viggo.kubism.ku.dk>

Tong Wang <wangtong at usc.edu> writes:

> Hi,
>      I am writing a function and need to pass a function expression as an argument,   for instance, 
>             myfun <-  function( express) {
>                    x<- c(1,2,3)
>                   y<-express
>          }
> 
>     if I call the above function by myfun( x*2 ), I get  2  as the result,  instead of  2,4,6 ,  could someone help me to 
> fix this problem ? 
>     Furthermore,  is that possible to operate this expression on different variables?  for example, in myfun(), I 
> might want to get  u*2,  z*2,  etc. without having to say x<-u, x<-z to match the expression.
> 
> Thanks a lot for your help.

Either pass the expression explicitly using myfun(quote(x*2)) or
myfun(expression(x*2)) or use substitute(express) inside the function. 

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From gunther.hoening at ukmainz.de  Wed Sep 27 11:04:35 2006
From: gunther.hoening at ukmainz.de (=?iso-8859-1?Q?Gunther_H=F6ning?=)
Date: Wed, 27 Sep 2006 11:04:35 +0200
Subject: [R] Accessing C- source code of R
In-Reply-To: <4518E120.6060608@statistik.uni-dortmund.de>
Message-ID: <001501c6e213$f404d050$0f1e0b0a@3med.klinik.unimainz.de>

That's exactly what I was looking for...

Thanks!
Gunther

-----Urspr?ngliche Nachricht-----
Von: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de] 
Gesendet: Dienstag, 26. September 2006 10:13
An: Gunther H?ning
Cc: r-help at stat.math.ethz.ch
Betreff: Re: [R] Accessing C- source code of R



Gunther H?ning wrote:
> Dear list,
> 
> I'm looking for the sources code of parts of R, (e.g. spline).
> Does anyone know where I can access it ?

I plan to write a corresponding R Help Desk article on "Accessing the
source". A draft is available from:
http://www.statistik.uni-dortmund.de/~ligges/R_Help_Desk_preview.pdf

Can you please tell me if this description is sufficient?

Thanks,
Uwe Ligges


> Gunther
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From p.dalgaard at biostat.ku.dk  Wed Sep 27 11:11:39 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 27 Sep 2006 11:11:39 +0200
Subject: [R] exact 95% confidence intervals
In-Reply-To: <359344770.26135@capitalbio.com>
References: <359344770.26135@capitalbio.com>
Message-ID: <x27izp1xo4.fsf@viggo.kubism.ku.dk>

"XinMeng" <xmeng at capitalbio.com> writes:

> Hello sir:

> As to the 2*2 table format for reporting results comparing a new
> test to true diagnosis,when I got the sensitivity and
> specificity,how can I calculate the exact 95% confidence intervals
> (based on the binomial distribution) for sensitivity and specificity
> via R?

Just run binom.test on the data from each column (or row, depending on
which way you turn the tables).

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From bonfigli at inmi.it  Wed Sep 27 11:55:44 2006
From: bonfigli at inmi.it (Bonfigli Sandro)
Date: Wed, 27 Sep 2006 11:55:44 +0200
Subject: [R] Impossible to merge with a zero rows data frame?
Message-ID: <WorldClient-F200609271155.AA55440010@inmi.it>

I'm trying to merge two data frames. One of them is a zero rows data 
frame.
I'm using the merge parameter 'all.x = TRUE' so I'd expect to obtain all
the rows of x. In fact the merge help says:

all.x: logical; if 'TRUE', then extra rows will be added to the
          output, one for each row in 'x' that has no matching row in
          'y'.  These rows will have 'NA's in those columns that are
          usually filled with values from 'y'.

To let you test the problem here is some code:

> L3 <- LETTERS[1:3]
> (d <- data.frame(cbind(x=1, y=1), fac=sample(L3, 1, repl=TRUE)))
> e <- d[-1,]

so now we have that:
> e
[1] x   y   fac
<0 rows> (or 0-length row.names)

here is the merge:
> merge(d, e, by.x = c("x"), by.y = c("x"), all.x = TRUE)

I'd expect something like:
  x y.x fac.x y.y fac.y
1 1   1     B  NA    NA
instead of:
Error in merge.data.frame(d, e, by.x = c("x"), by.y = c("x"), all.x = TRUE) : 
        no row to correspond
(I'm traslating the error message, so it could be a little different)

My questions are:
Is there a way to obtain my desired result? In the context in which I'd 
like to use the code the second data frame is the result of a query
and a lot of overwork would be added if I have to check if the dataframe
is a zero rows one BEFORE the merge (in fact I do a sequence of several
merge)
Is this behaviour of the merge command correct? Why did the developers
choose it?

P.S.: I tested this code both in R 2.3.0 and in 2.3.1

Thank you in advance.
   Sandro Bonfigli


From nelson1977 at gmail.com  Wed Sep 27 11:45:54 2006
From: nelson1977 at gmail.com (nelson -)
Date: Wed, 27 Sep 2006 11:45:54 +0200
Subject: [R] Data fitting problems...
Message-ID: <f44750160609270245l3dd7e93ew6992cac82ab3f546@mail.gmail.com>

Hi all!
  i'm trying to use R to fit a data model... the model describes the
lenght of a process.

I find that it's histogram is of this sort

*
*
*
**
***
***                        ***
******              *********
*******************************

under fisrt "peak" there is 70% of my events and under the second 30%.
Can conclude that my population is bi modal or i'm wrong? The second
peak is good fitted using a normal distribution. The first seems to be
a exponential or weibull distribution. How can i approxximate the
"added" distribution (normal + weibull"? I try nlm or optim but they
don't converge at all...  Another question. how to estimate well
starting points for parameter estimation...

thanks,
  nelson


From e.rapsomaniki at mail.cryst.bbk.ac.uk  Wed Sep 27 12:36:29 2006
From: e.rapsomaniki at mail.cryst.bbk.ac.uk (Eleni Rapsomaniki)
Date: Wed, 27 Sep 2006 11:36:29 +0100
Subject: [R] Any hot-deck imputation packages?
Message-ID: <1159353389.451a542da7503@webmail.cryst.bbk.ac.uk>

Hi

I found on google that there is an implementation of hot-deck imputation in
SAS:
http://ideas.repec.org/c/boc/bocode/s366901.html

Is there anything similar in R?

Many Thanks
Eleni Rapsomaniki


From mothsailor at googlemail.com  Wed Sep 27 12:49:59 2006
From: mothsailor at googlemail.com (David Barron)
Date: Wed, 27 Sep 2006 11:49:59 +0100
Subject: [R] Any hot-deck imputation packages?
In-Reply-To: <1159353389.451a542da7503@webmail.cryst.bbk.ac.uk>
References: <1159353389.451a542da7503@webmail.cryst.bbk.ac.uk>
Message-ID: <815b70590609270349n4f49211cl6e5ebeccc6e329ff@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060927/e79d2562/attachment.pl 

From spyros.mesomeris at citigroup.com  Wed Sep 27 12:51:08 2006
From: spyros.mesomeris at citigroup.com (Mesomeris, Spyros [CIR])
Date: Wed, 27 Sep 2006 11:51:08 +0100
Subject: [R] Constrained OLS regression
Message-ID: <0D9D1AC09BC016428D7597A1716AFF710D9940@Exukmb73.eur.nsroot.net>

Hello R helpers,

I am trying to do a linear OLS regression of y on two variables x1 and
x2. I want to constrain the coefficients of x1 and x2 to sum up to 1.
and therefore run a constrained OLS. Can anybody help with this? (I have
seen some answers to similar questions but it was not clear to me what I
need to do) - I have tried the lm function with offset but I must not
have used it properly.

Thanks,
Spyros


From jessica.gervais at tudor.lu  Wed Sep 27 13:02:08 2006
From: jessica.gervais at tudor.lu (jessica.gervais at tudor.lu)
Date: Wed, 27 Sep 2006 13:02:08 +0200
Subject: [R] exponential fitting
Message-ID: <OF83CDFD66.35578A7A-ONC12571F6.003C0974-422571F6.003C8C7B@tudor.lu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060927/b4bacfbe/attachment.pl 

From murdoch at stats.uwo.ca  Wed Sep 27 13:06:28 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 27 Sep 2006 07:06:28 -0400
Subject: [R] New project: littler for GNU R
In-Reply-To: <971536df0609262045w47be8288pc8c58523cb840119@mail.gmail.com>
References: <45195D97.8010402@vanderbilt.edu>
	<45198411.7020202@stats.uwo.ca>	<971536df0609261917pd5c5518l694f47299cfb81eb@mail.gmail.com>	<17689.60193.816367.78035@basebud.nulle.part>
	<971536df0609262045w47be8288pc8c58523cb840119@mail.gmail.com>
Message-ID: <451A5B34.6010101@stats.uwo.ca>

Gabor Grothendieck wrote:
> I think this is quoted out of context. I was referring to Duncan's post
> which shows an example of piping R code.
>   

No, that was Jeffrey's post that showed the pipe.  All that was in my 
post was a question about why this isn't a patch to the standard R 
script.  I don't think anyone has answered that, so I assume the reason 
is that it just seemed easier to write a separate script than to change 
the standard one.

If someone else designs, documents and commits the necessary changes to 
the standard script, I'd be happy to follow suit with the Windows R.exe.

Duncan Murdoch
> On 9/26/06, Dirk Eddelbuettel <edd at debian.org> wrote:
>   
>> On 26 September 2006 at 22:17, Gabor Grothendieck wrote:
>> | The real problem is that one wants to pipe the data in, not the
>> | R source.  The idea is that one successively transforms the
>> | data in successive elements of the pipeline.
>>
>> But that is what our filesize example does::
>>
>> | On 9/26/06, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
>> | > On 9/26/2006 1:04 PM, Jeffrey Horner wrote:
>> [...]
>> | > >    But unlike bc(1), GNU R has a vast number of statistical
>> | > >    functions. For example, we can quickly compute a summary() and show
>> | > >    a stem-and-leaf plot for file sizes in a given directory via
>> | > >
>> | > >          $ ls -l /boot | awk '!/^total/ {print $5}' | \
>> | > >               r -e 'fsizes <- as.integer(readLines());
>> | > >                  print(summary(fsizes)); stem(fsizes)'
>> | > >             Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>> | > >               13     512  110100  486900  768400 4735000
>> | > >          Loading required package: grDevices
>> | > >
>> | > >            The decimal point is 6 digit(s) to the right of the |
>> | > >
>> | > >            0 | 00000000000000000011112223
>> | > >            0 | 5557778899
>> | > >            1 | 112233
>> | > >            1 | 5
>> | > >            2 |
>> | > >            2 |
>> | > >            3 |
>> | > >            3 |
>> | > >            4 |
>> | > >            4 | 7
>>
>> Data to be processed on stdin, command via -e 'some long expression'.
>>
>> To make it simpler, here is a somewhat useless example of r piping into r
>> (which I've indented for readability):
>>
>>  $  r -e 'set.seed(42); sapply(rnorm(5),function(x) cat(x,"\n"))' |  \
>>                 r -e 'cat(sum(abs(as.numeric(readLines()))), "\n")'
>>  3.335916
>>
>> Isn't that something where, to quote you, "one wants to pipe the data in, not
>> the R source" ?
>>
>> Dirk
>>
>> --
>> Hell, there are no rules here - we're trying to accomplish something.
>>                                                  -- Thomas A. Edison
>>
>>     
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mothsailor at googlemail.com  Wed Sep 27 13:07:32 2006
From: mothsailor at googlemail.com (David Barron)
Date: Wed, 27 Sep 2006 12:07:32 +0100
Subject: [R] Constrained OLS regression
In-Reply-To: <0D9D1AC09BC016428D7597A1716AFF710D9940@Exukmb73.eur.nsroot.net>
References: <0D9D1AC09BC016428D7597A1716AFF710D9940@Exukmb73.eur.nsroot.net>
Message-ID: <815b70590609270407i1c552143jca718d27d4749da8@mail.gmail.com>

Have a look at the linear.hypothesis function in the car package.  For example:

> mod.duncan <- lm(prestige ~ income + education, data=Duncan)
>
> linear.hypothesis(mod.duncan, "income + education = 1")
Linear hypothesis test

Hypothesis:
income + education = 1

Model 1: prestige ~ income + education
Model 2: restricted model

  Res.Df    RSS Df Sum of Sq      F  Pr(>F)
1     42 7506.7
2     43 8045.2 -1    -538.5 3.0129 0.08994 .
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1


On 27/09/06, Mesomeris, Spyros [CIR] <spyros.mesomeris at citigroup.com> wrote:
> Hello R helpers,
>
> I am trying to do a linear OLS regression of y on two variables x1 and
> x2. I want to constrain the coefficients of x1 and x2 to sum up to 1.
> and therefore run a constrained OLS. Can anybody help with this? (I have
> seen some answers to similar questions but it was not clear to me what I
> need to do) - I have tried the lm function with offset but I must not
> have used it properly.
>
> Thanks,
> Spyros
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From murdoch at stats.uwo.ca  Wed Sep 27 13:17:03 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 27 Sep 2006 07:17:03 -0400
Subject: [R] New project: littler for GNU R
In-Reply-To: <4519DEE5.7090901@vanderbilt.edu>
References: <45195D97.8010402@vanderbilt.edu>	<m2sliey0p8.fsf@ziti.local>	<45196D6B.3020208@vanderbilt.edu>	<m28xk6wfej.fsf@ziti.local>
	<4519DEE5.7090901@vanderbilt.edu>
Message-ID: <451A5DAF.8080106@stats.uwo.ca>

Jeffrey Horner wrote:
> Seth Falcon wrote:
>   
>> Jeffrey Horner <jeff.horner at vanderbilt.edu> writes:
>>     
>
> [...]
>
>   
>>> littler will install into /usr/local/bin by default, so I don't think
>>> there's a clash with the Mac binary provided by CRAN, right?
>>>       
>> It depends what you mean by clash :-)
>>
>> If both are on the PATH, then you get the first one, I suspect, when
>> running either 'R' or 'r'.  I haven't tested this bit yet, but on my
>> OS X laptop I can invoke a new R session using either 'R' or 'r'
>> (using an R built from source, not the R GUI app thingie).
>>     
>
> Good point, but the executable path can be named absolutely in hash-bang 
> scripts. Relative paths work as well with the use of '/usr/bin/env 
> program' as is described in the littler announcement, but then you don't
> get to pass arguments to 'program', just to the hash-bang script.
>
>   
>> So IMO, a different name or an integration into the R script in some
>> way would be a big improvement.
>>     
>
> But I'd like to know why there's an R script in the first place. Why not 
> just an executable as on windows?
>   

There need to be (at least) two separate executables:  on Windows they 
are R.exe which invokes Rterm.exe. The reason for this is that "R CMD 
foo" invokes the script foo, it doesn't run Rterm at all.  On 
Unix-alikes the first is a script rather than an exe.

The reason the first is a script on Unix is that whoever wrote it 
preferred the sh script language to C for handling the things it needs 
to do, but that's not generally available in Windows, so we were stuck 
with C.

Duncan Murdoch
>   
>> 'r' is cute, but going down the road of tools with the same name
>> except for caps leads to confusion (for me).  For example, R CMD
>> build/INSTALL still catches me up after a number of years.
>>     
>
> That's a different problem than case-sensitivity. The word 'build' must 
> have had a different semantic than INSTALL, and I'm not sure why one was 
> all caps and the other isn't.
>
> Jeff
>


From dimitris.rizopoulos at med.kuleuven.be  Wed Sep 27 13:22:02 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 27 Sep 2006 13:22:02 +0200
Subject: [R] Constrained OLS regression
References: <0D9D1AC09BC016428D7597A1716AFF710D9940@Exukmb73.eur.nsroot.net>
Message-ID: <004901c6e227$27e419e0$0540210a@www.domain>

you could reparameterize, e.g.,

x1 <- runif(100, -4, 4)
x2 <- runif(100, -4, 4)
X <- cbind(1, x1 , x2)
y <-  rnorm(100, as.vector(X %*% c(5, -3, 4)), 2)
######################

fn <- function(betas){
    betas <- c(betas, 1 - betas[2])
    crossprod(y - X %*% betas)[1, ]
}

opt <- optim(c(5, -3), fn, method = "BFGS")
c(opt$par, 1 - opt$par[2])


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Mesomeris, Spyros [CIR]" <spyros.mesomeris at citigroup.com>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, September 27, 2006 12:51 PM
Subject: [R] Constrained OLS regression


> Hello R helpers,
>
> I am trying to do a linear OLS regression of y on two variables x1 
> and
> x2. I want to constrain the coefficients of x1 and x2 to sum up to 
> 1.
> and therefore run a constrained OLS. Can anybody help with this? (I 
> have
> seen some answers to similar questions but it was not clear to me 
> what I
> need to do) - I have tried the lm function with offset but I must 
> not
> have used it properly.
>
> Thanks,
> Spyros
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From ggrothendieck at gmail.com  Wed Sep 27 14:15:53 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 27 Sep 2006 08:15:53 -0400
Subject: [R] How to pass expression as an argument
In-Reply-To: <x2bqp11yc4.fsf@viggo.kubism.ku.dk>
References: <ddac8be61bb36.4519d2c8@usc.edu>
	<x2bqp11yc4.fsf@viggo.kubism.ku.dk>
Message-ID: <971536df0609270515r11de8153pf972114fe12ba21a@mail.gmail.com>

On 27 Sep 2006 10:57:15 +0200, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> Tong Wang <wangtong at usc.edu> writes:
>
> > Hi,
> >      I am writing a function and need to pass a function expression as an argument,   for instance,
> >             myfun <-  function( express) {
> >                    x<- c(1,2,3)
> >                   y<-express
> >          }
> >
> >     if I call the above function by myfun( x*2 ), I get  2  as the result,  instead of  2,4,6 ,  could someone help me to
> > fix this problem ?
> >     Furthermore,  is that possible to operate this expression on different variables?  for example, in myfun(), I
> > might want to get  u*2,  z*2,  etc. without having to say x<-u, x<-z to match the expression.
> >
> > Thanks a lot for your help.
>
> Either pass the expression explicitly using myfun(quote(x*2)) or
> myfun(expression(x*2)) or use substitute(express) inside the function.
>

Regarding the second option see the source of curve by entering:

curve


From murdoch at stats.uwo.ca  Wed Sep 27 14:22:32 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 27 Sep 2006 08:22:32 -0400
Subject: [R] New project: littler for GNU R
In-Reply-To: <451A5B34.6010101@stats.uwo.ca>
References: <45195D97.8010402@vanderbilt.edu>	<45198411.7020202@stats.uwo.ca>	<971536df0609261917pd5c5518l694f47299cfb81eb@mail.gmail.com>	<17689.60193.816367.78035@basebud.nulle.part>	<971536df0609262045w47be8288pc8c58523cb840119@mail.gmail.com>
	<451A5B34.6010101@stats.uwo.ca>
Message-ID: <451A6D08.60504@stats.uwo.ca>

On 9/27/2006 7:06 AM, Duncan Murdoch wrote:
> Gabor Grothendieck wrote:
>> I think this is quoted out of context. I was referring to Duncan's post
>> which shows an example of piping R code.
>>   
> 
> No, that was Jeffrey's post that showed the pipe.  All that was in my 
> post was a question about why this isn't a patch to the standard R 
> script.  I don't think anyone has answered that, so I assume the reason 
> is that it just seemed easier to write a separate script than to change 
> the standard one.

I've been reminded offline that this was discussed a few months ago by R 
Core, and some tricky questions were raised.  Nobody had the time to 
address them before 2.4.0, but I think the hope is that we'll have time 
after it is released to nail down the decisions on this so that 
something makes it into 2.5.0.

Expect some more comments on the "tricky questions", but not for a 
couple of weeks:  2.4.0 is due out in 6 days, and that's the priority 
right now.

Duncan Murdoch


> 
> If someone else designs, documents and commits the necessary changes to 
> the standard script, I'd be happy to follow suit with the Windows R.exe.
> 
> Duncan Murdoch
>> On 9/26/06, Dirk Eddelbuettel <edd at debian.org> wrote:
>>   
>>> On 26 September 2006 at 22:17, Gabor Grothendieck wrote:
>>> | The real problem is that one wants to pipe the data in, not the
>>> | R source.  The idea is that one successively transforms the
>>> | data in successive elements of the pipeline.
>>>
>>> But that is what our filesize example does::
>>>
>>> | On 9/26/06, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
>>> | > On 9/26/2006 1:04 PM, Jeffrey Horner wrote:
>>> [...]
>>> | > >    But unlike bc(1), GNU R has a vast number of statistical
>>> | > >    functions. For example, we can quickly compute a summary() and show
>>> | > >    a stem-and-leaf plot for file sizes in a given directory via
>>> | > >
>>> | > >          $ ls -l /boot | awk '!/^total/ {print $5}' | \
>>> | > >               r -e 'fsizes <- as.integer(readLines());
>>> | > >                  print(summary(fsizes)); stem(fsizes)'
>>> | > >             Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>>> | > >               13     512  110100  486900  768400 4735000
>>> | > >          Loading required package: grDevices
>>> | > >
>>> | > >            The decimal point is 6 digit(s) to the right of the |
>>> | > >
>>> | > >            0 | 00000000000000000011112223
>>> | > >            0 | 5557778899
>>> | > >            1 | 112233
>>> | > >            1 | 5
>>> | > >            2 |
>>> | > >            2 |
>>> | > >            3 |
>>> | > >            3 |
>>> | > >            4 |
>>> | > >            4 | 7
>>>
>>> Data to be processed on stdin, command via -e 'some long expression'.
>>>
>>> To make it simpler, here is a somewhat useless example of r piping into r
>>> (which I've indented for readability):
>>>
>>>  $  r -e 'set.seed(42); sapply(rnorm(5),function(x) cat(x,"\n"))' |  \
>>>                 r -e 'cat(sum(abs(as.numeric(readLines()))), "\n")'
>>>  3.335916
>>>
>>> Isn't that something where, to quote you, "one wants to pipe the data in, not
>>> the R source" ?
>>>
>>> Dirk
>>>
>>> --
>>> Hell, there are no rules here - we're trying to accomplish something.
>>>                                                  -- Thomas A. Edison
>>>
>>>     
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From f.harrell at vanderbilt.edu  Wed Sep 27 14:27:40 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 27 Sep 2006 07:27:40 -0500
Subject: [R] multiple imputation
In-Reply-To: <545326496@web.de>
References: <545326496@web.de>
Message-ID: <451A6E3C.3030109@vanderbilt.edu>

ozric at web.de wrote:
> Hi,
> 
> is it correct that "multiple-Imputation" like  mice 
> http://www.imputation.com can't understand as a standard data-mining 
> task, beacuse i haven't a generalization mechanism perform the model on 
> complete new and bigger dataset with a predict method!?
> 
> many thanks & regards,
> christian

This is something we need.  I have not written a predict method for 
aregImpute in the Hmisc package yet (and soon a completely re-written 
version of aregImpute will be posted) but the framework in aregImpute 
may allow such a method to be written.  Volunteers welcome.

Frank

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From f.harrell at vanderbilt.edu  Wed Sep 27 14:30:10 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 27 Sep 2006 07:30:10 -0500
Subject: [R] exact 95% confidence intervals
In-Reply-To: <x27izp1xo4.fsf@viggo.kubism.ku.dk>
References: <359344770.26135@capitalbio.com>
	<x27izp1xo4.fsf@viggo.kubism.ku.dk>
Message-ID: <451A6ED2.6080004@vanderbilt.edu>

Peter Dalgaard wrote:
> "XinMeng" <xmeng at capitalbio.com> writes:
> 
>> Hello sir:
> 
>> As to the 2*2 table format for reporting results comparing a new
>> test to true diagnosis,when I got the sensitivity and
>> specificity,how can I calculate the exact 95% confidence intervals
>> (based on the binomial distribution) for sensitivity and specificity
>> via R?
> 
> Just run binom.test on the data from each column (or row, depending on
> which way you turn the tables).
> 

But beware of exact binomial intervals - they are often not very 
accurate.  Wilson and other intervals are generally better.  For example 
see the binconf function in the Hmisc package.

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From henrik.parn at bio.ntnu.no  Wed Sep 27 14:31:35 2006
From: henrik.parn at bio.ntnu.no (Henrik Parn)
Date: Wed, 27 Sep 2006 14:31:35 +0200
Subject: [R] equivalent of model.tables for an lm.object?
Message-ID: <451A6F27.3090209@bio.ntnu.no>

Dear all,

I run a linear model with three significant explanatory variabels
x1: a factor with 4 levels
x2 and x3: factors with two levels each
x4: continuous

model <- lm(y ~ x1 + x2 * x3 + x4)
<>
The data is not perfectly balanced between the different 
factor-combinations and I use treatment contrasts.
<>
With an aov.object, I assume I could have used model.tables(aov.object, 
type = "means", se = TRUE), to get the means and se for all factor 
combinations.

<>In an lm.object like mine, I calculate the means 'manually' from the 
Estimates (for sure it could be done with a script, but fair enough). 
<>For the standard error of the means, I started out using formulas of a 
variance of a sum of two variables, but I messed things up with the 
interaction. Is there a way to calculate the standard error of the means 
from Estimates and Std.Error (or other information) from the lm.object?
<>
<>
<>Thanks in advance for any advice!
<><>
Best regards,

Henrik
<>

-- 
************************
Henrik P?rn
Department of Biology
NTNU
7491 Trondheim
Norway

+47 735 96282 (office)
+47 909 89 255 (mobile)
+47 735 96100 (fax)


From alex at transitive.com  Wed Sep 27 14:38:36 2006
From: alex at transitive.com (Alex Brown)
Date: Wed, 27 Sep 2006 13:38:36 +0100
Subject: [R] pdf and postscript sizes - change on each page?
Message-ID: <74F95F3F-9018-4BCE-BBF2-E7E37B53FA44@transitive.com>

Hi All

The device commands pdf and postscript allow you to specify the width  
and height of a page.  However, each subsequent plot is drawn on a  
separate page.  Is there a way to change the page size part way through?

For instance, is there an equivalent to the function pdfresize below?

pdf("/tmp/1.pdf", width=6, height=6)

plot(1:10)

pdf.resize(width=8, height=5)

plot(sin, -pi, pi)

dev.off()

---

So far, the only way I can find to do this is:

pdf("/tmp/1.pdf", width=6, height=6)

plot(1:10)

dev.off()

pdf("/tmp/1.pdf", width=8, height=5)

plot(sin, -pi, pi)

dev.off()

---

however, this creates a new file - it does not append to the existing  
file.

I would note that the quartz device has some resize capability, when  
you drag the window frame.

-Alex


From dimitris.rizopoulos at med.kuleuven.be  Wed Sep 27 14:39:52 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 27 Sep 2006 14:39:52 +0200
Subject: [R] exact 95% confidence intervals
References: <359344770.26135@capitalbio.com>
Message-ID: <00ac01c6e232$0782e9a0$0540210a@www.domain>

check package "binom".


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "XinMeng" <xmeng at capitalbio.com>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, September 27, 2006 10:12 AM
Subject: [R] exact 95% confidence intervals


> Hello sir:
> As to the 2*2 table format for reporting results comparing a new 
> test to true diagnosis,when I got the sensitivity and 
> specificity,how can I calculate the exact 95% confidence intervals 
> (based on the binomial distribution) for sensitivity and specificity 
> via R?
>
> Thanks a  lot!
>
> My best!
>
>
>
> ------------------------------
> *******************************************
> Xin Meng
> Capitalbio Corporation
> National Engineering Research Center
> for Beijing Biochip Technology
> BioPharma-informatics & Software Dept.
> Research Engineer
> Tel: +86-10-80715888/80726868-6438
> Fax: +86-10-80726790
> Email??xmeng at capitalbio.com
> Address:18 Life Science Parkway,
> Changping District, Beijing 102206, China
>
>


--------------------------------------------------------------------------------


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From ccleland at optonline.net  Wed Sep 27 14:47:53 2006
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 27 Sep 2006 08:47:53 -0400
Subject: [R] equivalent of model.tables for an lm.object?
In-Reply-To: <451A6F27.3090209@bio.ntnu.no>
References: <451A6F27.3090209@bio.ntnu.no>
Message-ID: <451A72F9.3070507@optonline.net>

Henrik Parn wrote:
> Dear all,
> 
> I run a linear model with three significant explanatory variabels
> x1: a factor with 4 levels
> x2 and x3: factors with two levels each
> x4: continuous
> 
> model <- lm(y ~ x1 + x2 * x3 + x4)
> <>
> The data is not perfectly balanced between the different 
> factor-combinations and I use treatment contrasts.
> <>
> With an aov.object, I assume I could have used model.tables(aov.object, 
> type = "means", se = TRUE), to get the means and se for all factor 
> combinations.
> 
> <>In an lm.object like mine, I calculate the means 'manually' from the 
> Estimates (for sure it could be done with a script, but fair enough). 
> <>For the standard error of the means, I started out using formulas of a 
> variance of a sum of two variables, but I messed things up with the 
> interaction. Is there a way to calculate the standard error of the means 
> from Estimates and Std.Error (or other information) from the lm.object?

You might have a look at the effects package by John Fox.

http://socserv.mcmaster.ca/jfox/Misc/effects/index.html

> <>
> <>
> <>Thanks in advance for any advice!
> <><>
> Best regards,
> 
> Henrik
> <>

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From ajung at gfz-potsdam.de  Wed Sep 27 14:57:57 2006
From: ajung at gfz-potsdam.de (Andre Jung)
Date: Wed, 27 Sep 2006 14:57:57 +0200
Subject: [R] Searching for keyword values in a text (configuration) file
Message-ID: <451A7555.50501@gfz-potsdam.de>

Hi,

I would like to read values from an ASCII text file that contains 
information in the following format:

DEVICE = 'PC'
CPU_SPEED = '1999', '233'
...

It's like a config file.

How can I e.g. get R to read the 2nd value of CPU_SPEED?
How do I go through text files and search for keywords and their values?

Thanks a lot,
Andre

From ajung at gfz-potsdam.de  Wed Sep 27 15:07:36 2006
From: ajung at gfz-potsdam.de (Andre Jung)
Date: Wed, 27 Sep 2006 15:07:36 +0200
Subject: [R] Searching for keyword values in a text (configuration) file
Message-ID: <451A7798.2040808@gfz-potsdam.de>

Hi,

I would like to read values from an ASCII text file that contains
information in the following format:

DEVICE = 'PC'
CPU_SPEED = '1999', '233'
...

It's like a config file.

How can I e.g. get R to read the 2nd value of CPU_SPEED?
How do I go through text files and search for keywords and their values?

Thanks a lot,
Andre


From ggrothendieck at gmail.com  Wed Sep 27 15:05:53 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 27 Sep 2006 09:05:53 -0400
Subject: [R] exponential fitting
In-Reply-To: <OF83CDFD66.35578A7A-ONC12571F6.003C0974-422571F6.003C8C7B@tudor.lu>
References: <OF83CDFD66.35578A7A-ONC12571F6.003C0974-422571F6.003C8C7B@tudor.lu>
Message-ID: <971536df0609270605x6a0784adta5ee80dec54b398f@mail.gmail.com>

> # using this test data
> set.seed(1)
> x <- 1:20/20
> y <- exp(2 + 3 * x) + rnorm(20)
>
> # if its ok to fit logs so that its linear
> exp(fitted(lm(log(y) ~ x)))
        1         2         3         4         5         6         7         8
  8.55615   9.94692  11.56376  13.44340  15.62857  18.16894  21.12223  24.55557
        9        10        11        12        13        14        15        16
 28.54699  33.18720  38.58165  44.85295  52.14363  60.61938  70.47284  81.92793
       17        18        19        20
 95.24501 110.72673 128.72494 149.64869
>
> # or to do it on original scale use linear coefs as starting values
> cc <- coef(lm(log(y) ~ x))
> fitted(nls(y ~ exp(a + b*x), start = list(a = cc[1], b = cc[2])))
 [1]   8.592270   9.984536  11.602401  13.482421  15.667073  18.205720
 [7]  21.155722  24.583734  28.567211  33.196159  38.575168  44.825776
[13]  52.089214  60.529599  70.337640  81.734946  94.979039 110.369167
[19] 128.253066 149.034820
attr(,"label")
[1] "Fitted values"

On 9/27/06, jessica.gervais at tudor.lu <jessica.gervais at tudor.lu> wrote:
> Hi,
>
> I would like to fit some experimental points by a exponential function.
> I ignore the parameters of this exponential and what I would like is to
> ask R to calculate the best fitting curve an the associated parameters (as
> the linear model function (lm) does for linear models).
> Is it possible ?
> Do anyone have an idea about how to do that ?
>
> Thanks by advance
>
> Jessica Gervais
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From p.dalgaard at biostat.ku.dk  Wed Sep 27 15:11:11 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 27 Sep 2006 15:11:11 +0200
Subject: [R] exact 95% confidence intervals
In-Reply-To: <451A6ED2.6080004@vanderbilt.edu>
References: <359344770.26135@capitalbio.com>
	<x27izp1xo4.fsf@viggo.kubism.ku.dk> <451A6ED2.6080004@vanderbilt.edu>
Message-ID: <x2y7s5zc7k.fsf@viggo.kubism.ku.dk>

Frank E Harrell Jr <f.harrell at vanderbilt.edu> writes:

> Peter Dalgaard wrote:
> > "XinMeng" <xmeng at capitalbio.com> writes:
> >
> >> Hello sir:
> >
> >> As to the 2*2 table format for reporting results comparing a new
> >> test to true diagnosis,when I got the sensitivity and
> >> specificity,how can I calculate the exact 95% confidence intervals
> >> (based on the binomial distribution) for sensitivity and specificity
> >> via R?
> > Just run binom.test on the data from each column (or row, depending
> > on
> > which way you turn the tables).
> >
> 
> But beware of exact binomial intervals - they are often not very
> accurate.  Wilson and other intervals are generally better.  For
> example see the binconf function in the Hmisc package.

I suppose that by "accurate" you mean that they are generally better
at getting the coverage rate right? 

The "exact" intervals are strictly conservative, but at least
predictably so. The whole thing is largely a matter of taste to my
mind, but I know that other people (notably Alan Agresti) have
stronger opinions.

(People taking an interest in this may want to have a look at
http://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval )

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From petr.pikal at precheza.cz  Wed Sep 27 15:41:12 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 27 Sep 2006 15:41:12 +0200
Subject: [R] Impossible to merge with a zero rows data frame?
In-Reply-To: <WorldClient-F200609271155.AA55440010@inmi.it>
Message-ID: <451A9B98.25082.14EA5BD@localhost>

Hi

you have two options:

change the source code for merge 
or use some modified function like

my.merge <- function(x,y, ...) if(all(dim(x)[1]>0, dim(y)[1]>0)) 
merge(x,y) else print ("No merge or whatever action which is 
suitable")

HTH
Petr

On 27 Sep 2006 at 11:55, Bonfigli Sandro wrote:

Date sent:      	Wed, 27 Sep 2006 11:55:44 +0200
From:           	"Bonfigli Sandro" <bonfigli at inmi.it>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] Impossible to merge with a zero rows data frame?

> I'm trying to merge two data frames. One of them is a zero rows data
> frame. I'm using the merge parameter 'all.x = TRUE' so I'd expect to
> obtain all the rows of x. In fact the merge help says:
> 
> all.x: logical; if 'TRUE', then extra rows will be added to the
>           output, one for each row in 'x' that has no matching row in
>           'y'.  These rows will have 'NA's in those columns that are
>           usually filled with values from 'y'.
> 
> To let you test the problem here is some code:
> 
> > L3 <- LETTERS[1:3]
> > (d <- data.frame(cbind(x=1, y=1), fac=sample(L3, 1, repl=TRUE))) e
> > <- d[-1,]
> 
> so now we have that:
> > e
> [1] x   y   fac
> <0 rows> (or 0-length row.names)
> 
> here is the merge:
> > merge(d, e, by.x = c("x"), by.y = c("x"), all.x = TRUE)
> 
> I'd expect something like:
>   x y.x fac.x y.y fac.y
> 1 1   1     B  NA    NA
> instead of:
> Error in merge.data.frame(d, e, by.x = c("x"), by.y = c("x"), all.x =
> TRUE) : 
>         no row to correspond
> (I'm traslating the error message, so it could be a little different)
> 
> My questions are:
> Is there a way to obtain my desired result? In the context in which
> I'd like to use the code the second data frame is the result of a
> query and a lot of overwork would be added if I have to check if the
> dataframe is a zero rows one BEFORE the merge (in fact I do a sequence
> of several merge) Is this behaviour of the merge command correct? Why
> did the developers choose it?
> 
> P.S.: I tested this code both in R 2.3.0 and in 2.3.1
> 
> Thank you in advance.
>    Sandro Bonfigli
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From bernarduse1 at yahoo.fr  Wed Sep 27 15:41:57 2006
From: bernarduse1 at yahoo.fr (Marc Bernard)
Date: Wed, 27 Sep 2006 15:41:57 +0200 (CEST)
Subject: [R] Testing the equality  of correlations
Message-ID: <20060927134157.71898.qmail@web25808.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060927/710f7d9b/attachment.pl 

From ggrothendieck at gmail.com  Wed Sep 27 15:47:31 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 27 Sep 2006 09:47:31 -0400
Subject: [R] Searching for keyword values in a text (configuration) file
In-Reply-To: <451A7798.2040808@gfz-potsdam.de>
References: <451A7798.2040808@gfz-potsdam.de>
Message-ID: <971536df0609270647w3f083febwbdd01b8411cf0a4d@mail.gmail.com>

Read in data using readLines and replace
= with comma and delete all spaces.
Then reread using read.table and set the
rownames to column 1 removing column 1.

# test data
Lines0 <- "DEVICE = 'PC'
CPU_SPEED = '1999', '233'
"

# if reading from a file then
# replace next line with something like Lines <- readLines("myfile.dat")
Lines <- readLines(textConnection(Lines0))
Lines <- gsub("=", ",", Lines)
Lines <- gsub(" ", "", Lines)
DF <- read.table(textConnection(Lines), sep = ",", fill = TRUE,
	colClasses = "character", header = FALSE)
rownames(DF) <- DF[,1]
DF <- DF[,-1]

DF["CPU_SPEED", 2]





On 9/27/06, Andre Jung <ajung at gfz-potsdam.de> wrote:
> Hi,
>
> I would like to read values from an ASCII text file that contains
> information in the following format:
>
> DEVICE = 'PC'
> CPU_SPEED = '1999', '233'
> ...
>
> It's like a config file.
>
> How can I e.g. get R to read the 2nd value of CPU_SPEED?
> How do I go through text files and search for keywords and their values?
>
> Thanks a lot,
> Andre
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mothsailor at googlemail.com  Wed Sep 27 16:09:11 2006
From: mothsailor at googlemail.com (David Barron)
Date: Wed, 27 Sep 2006 15:09:11 +0100
Subject: [R] Searching for keyword values in a text (configuration) file
In-Reply-To: <451A7798.2040808@gfz-potsdam.de>
References: <451A7798.2040808@gfz-potsdam.de>
Message-ID: <815b70590609270709w374add20n6b93bea60113c471@mail.gmail.com>

Something like this?

> library(Hmisc)
> t <- readLines("clipboard")
> t
[1] "DEVICE = 'PC'"             "CPU_SPEED = '1999', '233'"

> ix <- grep("CPU_SPEED",t)
> loc <- substring.location(t[ix],",")
> cpu <- substring(t[ix],loc$first+2)
> cpu
[1] "'233'"


On 27/09/06, Andre Jung <ajung at gfz-potsdam.de> wrote:
> Hi,
>
> I would like to read values from an ASCII text file that contains
> information in the following format:
>
> DEVICE = 'PC'
> CPU_SPEED = '1999', '233'
> ...
>
> It's like a config file.
>
> How can I e.g. get R to read the 2nd value of CPU_SPEED?
> How do I go through text files and search for keywords and their values?
>
> Thanks a lot,
> Andre
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From ggrothendieck at gmail.com  Wed Sep 27 16:11:50 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 27 Sep 2006 10:11:50 -0400
Subject: [R] Searching for keyword values in a text (configuration) file
In-Reply-To: <971536df0609270647w3f083febwbdd01b8411cf0a4d@mail.gmail.com>
References: <451A7798.2040808@gfz-potsdam.de>
	<971536df0609270647w3f083febwbdd01b8411cf0a4d@mail.gmail.com>
Message-ID: <971536df0609270711q3820b3fay1903471aca33e307@mail.gmail.com>

Here is one more solution using the same Lines0 from last time.
This one uses strapply from gsubfn to pick out all the fields in
each line creating a list named by the keywords (rather than
a data frame as in the previous solution).  The names of the
components are the keywords so we remove them from
the contents of the list itself since we don't need them twice:

library(gsubfn)

# replace next line with something like Lines <- readLines("myfile.dat")
Lines <- readLines(textConnection(Lines0))

parms <- strapply(Lines, "[^ ',=]+", c, USE.NAMES = TRUE)
parms <- lapply(parms, "[", -1)

parms[["CPU_SPEED"]][2]



On 9/27/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Read in data using readLines and replace
> = with comma and delete all spaces.
> Then reread using read.table and set the
> rownames to column 1 removing column 1.
>
> # test data
> Lines0 <- "DEVICE = 'PC'
> CPU_SPEED = '1999', '233'
> "
>
> # if reading from a file then
> # replace next line with something like Lines <- readLines("myfile.dat")
> Lines <- readLines(textConnection(Lines0))
> Lines <- gsub("=", ",", Lines)
> Lines <- gsub(" ", "", Lines)
> DF <- read.table(textConnection(Lines), sep = ",", fill = TRUE,
>        colClasses = "character", header = FALSE)
> rownames(DF) <- DF[,1]
> DF <- DF[,-1]
>
> DF["CPU_SPEED", 2]
>
>
>
>
>
> On 9/27/06, Andre Jung <ajung at gfz-potsdam.de> wrote:
> > Hi,
> >
> > I would like to read values from an ASCII text file that contains
> > information in the following format:
> >
> > DEVICE = 'PC'
> > CPU_SPEED = '1999', '233'
> > ...
> >
> > It's like a config file.
> >
> > How can I e.g. get R to read the 2nd value of CPU_SPEED?
> > How do I go through text files and search for keywords and their values?
> >
> > Thanks a lot,
> > Andre
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From Christophe.Nguyen at bordeaux.inra.fr  Wed Sep 27 16:39:22 2006
From: Christophe.Nguyen at bordeaux.inra.fr (Christophe Nguyen)
Date: Wed, 27 Sep 2006 16:39:22 +0200
Subject: [R] PDE
Message-ID: <451A8D1A.2080006@bordeaux.inra.fr>

Dear all,
Does any know how to solve PDE with R? The archive list refers to the 
use of ODE if PDE are parabolic. I am not a mathematician and this does 
not mean anything for me!
help would be very appreciated.
Many thanks

-- 
___________________________________________________

Christophe NGUYEN

UMR 1220 INRA-ENITAB
Transfert sol-plante et cycle des ?l?ments min?raux
dans les ?cosyst?mes cultiv?s"

Centre INRA de Bordeaux-Aquitaine
71, avenue Edouard Bourlaux, BP 81
33883 Villenave d'Ornon, FRANCE

Tel : 00 33 (0)5 57 12 25 07
Fax : 00 33 (0)5 57 12 25 15

email : Christophe.Nguyen at bordeaux.inra.fr
page infoservice: http://www.bordeaux.inra.fr/tcem

__________m?O?m____________________________________


From ggrothendieck at gmail.com  Wed Sep 27 16:46:47 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 27 Sep 2006 10:46:47 -0400
Subject: [R] Searching for keyword values in a text (configuration) file
In-Reply-To: <971536df0609270647w3f083febwbdd01b8411cf0a4d@mail.gmail.com>
References: <451A7798.2040808@gfz-potsdam.de>
	<971536df0609270647w3f083febwbdd01b8411cf0a4d@mail.gmail.com>
Message-ID: <971536df0609270746j59fc949pec401d7a2297c944@mail.gmail.com>

Here is a slight simplification of this one using row.names= in
the read.table to avoid the subsequent manipulations.  We
use Lines0 defined in the earlier post:


# replace next line with something like Lines <- readLines("myfile.dat")
Lines <- readLines(textConnection(Lines0))

Lines <- gsub("=", ",", Lines)
Lines <- gsub(" ", "", Lines)

DF <- read.table(textConnection(Lines), row.names = 1, sep = ",", fill = TRUE,
       colClasses = "character", header = FALSE)

DF["CPU_SPEED", 2]


On 9/27/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Read in data using readLines and replace
> = with comma and delete all spaces.
> Then reread using read.table and set the
> rownames to column 1 removing column 1.
>
> # test data
> Lines0 <- "DEVICE = 'PC'
> CPU_SPEED = '1999', '233'
> "
>
> # if reading from a file then
> # replace next line with something like Lines <- readLines("myfile.dat")
> Lines <- readLines(textConnection(Lines0))
> Lines <- gsub("=", ",", Lines)
> Lines <- gsub(" ", "", Lines)
> DF <- read.table(textConnection(Lines), sep = ",", fill = TRUE,
>        colClasses = "character", header = FALSE)
> rownames(DF) <- DF[,1]
> DF <- DF[,-1]
>
> DF["CPU_SPEED", 2]
>
>
>
>
>
> On 9/27/06, Andre Jung <ajung at gfz-potsdam.de> wrote:
> > Hi,
> >
> > I would like to read values from an ASCII text file that contains
> > information in the following format:
> >
> > DEVICE = 'PC'
> > CPU_SPEED = '1999', '233'
> > ...
> >
> > It's like a config file.
> >
> > How can I e.g. get R to read the 2nd value of CPU_SPEED?
> > How do I go through text files and search for keywords and their values?
> >
> > Thanks a lot,
> > Andre
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From IBarjis at CityTech.Cuny.Edu  Wed Sep 27 17:07:32 2006
From: IBarjis at CityTech.Cuny.Edu (Isaac Barjis)
Date: Wed, 27 Sep 2006 11:07:32 -0400
Subject: [R] t-stat Curve
Message-ID: <s51a5b94.048@email1.citytech.cuny.edu>

Number of subjects = 25
Mean of Sample = 77
Standard Deviation (s) = 12
sem = 2.4
df = 24

The claim is that population mean is less than 80
* > 80
So our H0 (null hupotheis) is * > 80


> qt(.95,24)
[1] 1.710882
> qt(0.05, 24)
[1] -1.710882

tstat = -1.25 on t24 falls between 1.711 (.95,24) and *1.711 (.005,24)


How Could I sketch t curve for the above data where my * would be at the center?

Best Regards
Isaac

Dr. I. Barjis
Assistant Professor
Summer and Evening Coordinator
Department of  Biological Sciences
Room P313
300 Jay Street
Brooklyn, NY 11201

Phone: (718)2605285
Fax: (718)2548680
Fax: (718) 254-8595 Department Office
http://websupport1.citytech.cuny.edu/Faculty/ibarjis



-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: Isaac Barjis.vcf
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060927/f9c3c876/attachment.pl 

From paul.hewson at plymouth.ac.uk  Wed Sep 27 17:40:09 2006
From: paul.hewson at plymouth.ac.uk (Paul Hewson)
Date: Wed, 27 Sep 2006 16:40:09 +0100
Subject: [R] Testing the equality  of correlations
In-Reply-To: <20060927134157.71898.qmail@web25808.mail.ukl.yahoo.com>
Message-ID: <52A8091888A23F47A013223014B6E9FE074B28C3@03-CSEXCH.uopnet.plymouth.ac.uk>

Marc,

Off the top my head (i.e. this could all be horribly wrong), I think
Anderson gave an asymptotic version for such a test, whereby under the
null hypothesis, the difference between Fisher's z for each sample, z1 -
z2, is normal with zero mean.   If I recall correctly, the 1984 edition
gave a test statistic something like:
 
 $\frac{\vert z_{1} - z_{2} \rvert}{ \sqrt{\frac{1}{N_{1} - 3} +
\frac{1}{N_{2} - 3}}}$, 

as the test.   I ***think*** N = n+1.   Assuming that part is correct,
it can be coded up quite simply (and crudely and hopefully correctly)
as:

## estimate Fisher's z for the sample
fishz <- function(x1,x2){
r <- cor(x1,x2)
z <- 0.5 * log( (1+r)/(1-r) )
 }

## apply the correlation test
cortest <- function(x1,x2,x3,x4){
   numer <- abs(fishz(x1,x2) - fishz(x3,x4) )
   denom <- sqrt( 1/( length(x1) - 2) + 1/( length(x2) - 2) )
   test.stat <- numer / denom
return(test.stat)
}


A quick demo with some simulated data:

require(MASS)
X1 <- mvrnorm(1000, c(0,0), matrix(c(1,0.7,0.7,1),2,2))
X2 <- mvrnorm(1000, c(0,0), matrix(c(1,0.9,0.9,1),2,2))

cortest(X1[,1], X1[,2], X2[,1], X2[,2])

Is above 1.96 indicating they are different

X2 <- mvrnorm(1000, c(0,0), matrix(c(1,0.7,0.7,1),2,2))
cortest(X1[,1], X1[,2], X2[,1], X2[,2])

Is below 1.96 indicating they are not different.   All that needs to be
done for these pair is to get the pooled estimate of the populations
Fisher's z $ \frac{ (N_{1} - 3)z_{1} + (N_{2} - 3)z_{2}}{N_{1} + N_{2} -
6}$ and solve fisher's z to get an estimate of rho.

Just dabbling around with this suggests either I've missed something, or
that we need quite a large sample size before the asymptotics are any
use.   If this is of any use I'll double check on N.

Paul


-=-=-=-=-=-=-=-=-=-=-=-=
Paul Hewson 
Lecturer in Statistics 
School of Mathematics and Statistics 
University of Plymouth 
Drake Circus 
Plymouth PL4 8AA 

tel (01752) 232778 (Campus) 
tel (01752) 764437 (Tamar Science Park) 
fax (01752) 232780 

email: paul.hewson at plymouth.ac.uk
web: http://www.plymouth.ac.uk/staff/phewson
-=-=-=-=-=-=-=-=-=-=-=-=




-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Marc Bernard
Sent: 27 September 2006 14:42
To: r-help at stat.math.ethz.ch
Subject: [R] Testing the equality of correlations


Dear All, 
   
  I wonder if there is any  implemented statistical test in R to test
the  equality between many correlations. As an example, let X1, X2, X3
X4 be four random  variables.  let 
  Phi(X1,X2) , Phi(X1,X3) and Phi(X1,X4) be the corresponding
correlations.  
  How to test Phi(X1,X2) = Phi(X1,X3) = P(X1,X4)?
   
  Many thanks in advance,
   
  Bernard
   
   
   
   

 		
---------------------------------

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From cooch17 at verizon.net  Wed Sep 27 18:07:48 2006
From: cooch17 at verizon.net (Evan Cooch)
Date: Wed, 27 Sep 2006 12:07:48 -0400
Subject: [R] multidimensional lists
Message-ID: <451AA1D4.7080902@verizon.net>

In the process of moving a number of my scripts from MATLAB -> R, I've 
discovered that there is no 'pure' equivalent of MATLAB's cell arrays, 
which I use quite often. Basically, I create matrices (as a cell array) 
where each element of the matrix is itself a matrix (e.g., 2x2 cell 
array where each element of the array is another matrix). I pass these 
cell arrays to various functions which then do clever things with the 
various matrices (of course) - basically, I need to be able to pass 
collections of matrices to functions to do various things, and I need to 
be able to control the dimensionality of the cell array to preserve some 
structural relationships among the matrices in the array. The cell array 
in MATLAB handles this with aplomb.

So far, in R, I've used lists. Given (say) 4 matrices (A,B,C,D), in 
MATLAB I can use

test={A,B,C,D} for a row vector cell array, or

test={A;B;C;D} for a column vector cell array.

In R, I get more or less the same thing using

test=list(A,B,C,D)

but this only gives me a row list. For a bunch of technical reasons, I 
need to be able to control the orientation (as noted)- this is 
especially true for n-dimensional cell arrays. In MATLAB, for example, I 
could generate a (say) 2x2 cell array using

test={A B;C D}

The only way I can figure out how to do this in R is using something like

test=list(A,B,C,D);
dim(test) < c(2,2);

This seems to work, but defaults to bycolumn (in other words, instead of   

A  B
C  D

I get

A  C
B  D

)

So, I follow with

test=t(test) as needed to flip the thing around to byrow.

OK, so the question is - is there a better way? This *seems* to work, 
but I'm discovering that R is a lot like working with LaTeX (something I 
know much more about) - you can do most things, but there is often a 
more elegant way if you can figure out how to find out about it.

Thanks in advance...


From NordlDJ at dshs.wa.gov  Wed Sep 27 18:21:21 2006
From: NordlDJ at dshs.wa.gov (Nordlund, Dan (DSHS))
Date: Wed, 27 Sep 2006 09:21:21 -0700
Subject: [R] t-stat Curve
Message-ID: <941871A13165C2418EC144ACB212BDB0078C4E@dshsmxoly1504g.dshs.wa.lcl>

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of Isaac Barjis
> Sent: Wednesday, September 27, 2006 8:08 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] t-stat Curve
> 
> Number of subjects = 25
> Mean of Sample = 77
> Standard Deviation (s) = 12
> sem = 2.4
> df = 24
> 
> The claim is that population mean is less than 80
> * > 80
> So our H0 (null hupotheis) is * > 80
> 
> 
> > qt(.95,24)
> [1] 1.710882
> > qt(0.05, 24)
> [1] -1.710882
> 
> tstat = -1.25 on t24 falls between 1.711 (.95,24) and *1.711 (.005,24)
> 
> 
> How Could I sketch t curve for the above data where my * would be at the
> center?
> 
> Best Regards
> Isaac
> 
> Dr. I. Barjis
> Assistant Professor
> Summer and Evening Coordinator
> Department of  Biological Sciences


Isaac,

I'm not sure that what you are asking for is reasonable (or possible).  It
is the sampling distribution of your t-statistic that is distributed as t
under the null hypothesis, not your observed data.  Could you clarify what
it is you wish to do?

Dan

Daniel J. Nordlund
Research and Data Analysis
Washington State Department of Social and Health Services
Olympia, WA  98504-5204


From vince_bioinfo at yahoo.fr  Wed Sep 27 18:26:01 2006
From: vince_bioinfo at yahoo.fr (Vincent Negre)
Date: Wed, 27 Sep 2006 18:26:01 +0200 (CEST)
Subject: [R]  package e1071 - class probabilities
Message-ID: <20060927162601.68262.qmail@web26404.mail.ukl.yahoo.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20060927/2626f115/attachment.pl 

From ripley at stats.ox.ac.uk  Wed Sep 27 18:33:26 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 27 Sep 2006 17:33:26 +0100 (BST)
Subject: [R] multidimensional lists
In-Reply-To: <451AA1D4.7080902@verizon.net>
References: <451AA1D4.7080902@verizon.net>
Message-ID: <Pine.LNX.4.64.0609271731411.18168@gannet.stats.ox.ac.uk>

matrix(some_list, nr, nc, byrow=TRUE)  may be what you are looking for.

R arrays can be of any vector type, including list.  I'd get used to R's 
Fortran ordering rather than force transposes all the time.

On Wed, 27 Sep 2006, Evan Cooch wrote:

> In the process of moving a number of my scripts from MATLAB -> R, I've
> discovered that there is no 'pure' equivalent of MATLAB's cell arrays,
> which I use quite often. Basically, I create matrices (as a cell array)
> where each element of the matrix is itself a matrix (e.g., 2x2 cell
> array where each element of the array is another matrix). I pass these
> cell arrays to various functions which then do clever things with the
> various matrices (of course) - basically, I need to be able to pass
> collections of matrices to functions to do various things, and I need to
> be able to control the dimensionality of the cell array to preserve some
> structural relationships among the matrices in the array. The cell array
> in MATLAB handles this with aplomb.
>
> So far, in R, I've used lists. Given (say) 4 matrices (A,B,C,D), in
> MATLAB I can use
>
> test={A,B,C,D} for a row vector cell array, or
>
> test={A;B;C;D} for a column vector cell array.
>
> In R, I get more or less the same thing using
>
> test=list(A,B,C,D)
>
> but this only gives me a row list. For a bunch of technical reasons, I
> need to be able to control the orientation (as noted)- this is
> especially true for n-dimensional cell arrays. In MATLAB, for example, I
> could generate a (say) 2x2 cell array using
>
> test={A B;C D}
>
> The only way I can figure out how to do this in R is using something like
>
> test=list(A,B,C,D);
> dim(test) < c(2,2);
>
> This seems to work, but defaults to bycolumn (in other words, instead of
>
> A  B
> C  D
>
> I get
>
> A  C
> B  D
>
> )
>
> So, I follow with
>
> test=t(test) as needed to flip the thing around to byrow.
>
> OK, so the question is - is there a better way? This *seems* to work,
> but I'm discovering that R is a lot like working with LaTeX (something I
> know much more about) - you can do most things, but there is often a
> more elegant way if you can figure out how to find out about it.
>
> Thanks in advance...
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ggrothendieck at gmail.com  Wed Sep 27 18:33:23 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 27 Sep 2006 12:33:23 -0400
Subject: [R] multidimensional lists
In-Reply-To: <451AA1D4.7080902@verizon.net>
References: <451AA1D4.7080902@verizon.net>
Message-ID: <971536df0609270933v1e91d34cu89333f149ac10552@mail.gmail.com>

Try this:

AA <- matrix(list(A, 10*A, 100*A, 1000*A), 2, byrow = TRUE)
AA[1,2]


On 9/27/06, Evan Cooch <cooch17 at verizon.net> wrote:
> In the process of moving a number of my scripts from MATLAB -> R, I've
> discovered that there is no 'pure' equivalent of MATLAB's cell arrays,
> which I use quite often. Basically, I create matrices (as a cell array)
> where each element of the matrix is itself a matrix (e.g., 2x2 cell
> array where each element of the array is another matrix). I pass these
> cell arrays to various functions which then do clever things with the
> various matrices (of course) - basically, I need to be able to pass
> collections of matrices to functions to do various things, and I need to
> be able to control the dimensionality of the cell array to preserve some
> structural relationships among the matrices in the array. The cell array
> in MATLAB handles this with aplomb.
>
> So far, in R, I've used lists. Given (say) 4 matrices (A,B,C,D), in
> MATLAB I can use
>
> test={A,B,C,D} for a row vector cell array, or
>
> test={A;B;C;D} for a column vector cell array.
>
> In R, I get more or less the same thing using
>
> test=list(A,B,C,D)
>
> but this only gives me a row list. For a bunch of technical reasons, I
> need to be able to control the orientation (as noted)- this is
> especially true for n-dimensional cell arrays. In MATLAB, for example, I
> could generate a (say) 2x2 cell array using
>
> test={A B;C D}
>
> The only way I can figure out how to do this in R is using something like
>
> test=list(A,B,C,D);
> dim(test) < c(2,2);
>
> This seems to work, but defaults to bycolumn (in other words, instead of
>
> A  B
> C  D
>
> I get
>
> A  C
> B  D
>
> )
>
> So, I follow with
>
> test=t(test) as needed to flip the thing around to byrow.
>
> OK, so the question is - is there a better way? This *seems* to work,
> but I'm discovering that R is a lot like working with LaTeX (something I
> know much more about) - you can do most things, but there is often a
> more elegant way if you can figure out how to find out about it.
>
> Thanks in advance...
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jessica.gervais at tudor.lu  Wed Sep 27 18:34:33 2006
From: jessica.gervais at tudor.lu (jessica.gervais at tudor.lu)
Date: Wed, 27 Sep 2006 18:34:33 +0200
Subject: [R] panel.curve
Message-ID: <OFDBD4594D.DDC375FA-ONC12571F6.0054A1A9-422571F6.005AFBA6@tudor.lu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060927/c06e24c9/attachment.pl 

From Charles.Annis at StatisticalEngineering.com  Wed Sep 27 18:34:49 2006
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Wed, 27 Sep 2006 12:34:49 -0400
Subject: [R] t-stat Curve
In-Reply-To: <s51a5b94.048@email1.citytech.cuny.edu>
Message-ID: <004e01c6e252$da2c74a0$6400a8c0@DD4XFW31>

Isaac:

You will likely find something helpful here:
http://addictedtor.free.fr/graphiques/thumbs.php

I also recently came across this code (I thought it was at the URL above,
but I can't find it now) that may be useful with modification.

I apologize to the code-writer for having lost the correct reference. (If
anyone finds it, please send the reference to me. Thanks.)
#########################################

# neighboring (not "overlapping") normal densities
dev.off() 
x<-seq(-10,10,length=400)
y1<-dnorm(x)
y2<-dnorm(x,m=3)
par(mar=c(5,4,2,1))
plot(x, y2, xlim=c(-3,8), type="n", xlab=quote(Z==frac(mu[1]-mu[2],
                 sigma/sqrt(n))), ylab="Density")
polygon(c(1.96,1.96,x[240:400],10), c(0,dnorm(1.96,m=3),y2[240:400],0),
                 col="grey80", lty=0)
lines(x, y2)
lines(x, y1)
polygon(c(-1.96,-1.96,x[161:1],-10), c(0,dnorm(-1.96,m=0), y1[161:1],0),
                 col="grey30", lty=0)
polygon(c(1.96, 1.96, x[240:400], 10), c(0,dnorm(1.96,m=0),
                 y1[240:400],0), col="grey30")
legend(4.2, .4, fill=c("grey80","grey30"),
              legend=expression(P(abs(phantom(i)*Z*phantom(i))>1.96,
H[1])==0.85,
              P(abs(phantom(i)*Z*phantom(i))>1.96,H[0])==0.05), bty="n")
text(0, .2, quote(H[0]:~~mu[1]==mu[2]))
text(3, .2, quote(H[1]:~~mu[1]==mu[2]+delta))

#########################################


Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 
-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Isaac Barjis
Sent: Wednesday, September 27, 2006 11:08 AM
To: R-help at stat.math.ethz.ch
Subject: [R] t-stat Curve

Number of subjects = 25
Mean of Sample = 77
Standard Deviation (s) = 12
sem = 2.4
df = 24

The claim is that population mean is less than 80
* > 80
So our H0 (null hupotheis) is * > 80


> qt(.95,24)
[1] 1.710882
> qt(0.05, 24)
[1] -1.710882

tstat = -1.25 on t24 falls between 1.711 (.95,24) and *1.711 (.005,24)


How Could I sketch t curve for the above data where my * would be at the
center?

Best Regards
Isaac

Dr. I. Barjis
Assistant Professor
Summer and Evening Coordinator
Department of  Biological Sciences
Room P313
300 Jay Street
Brooklyn, NY 11201

Phone: (718)2605285
Fax: (718)2548680
Fax: (718) 254-8595 Department Office
http://websupport1.citytech.cuny.edu/Faculty/ibarjis


From ggrothendieck at gmail.com  Wed Sep 27 18:51:32 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 27 Sep 2006 12:51:32 -0400
Subject: [R] panel.curve
In-Reply-To: <OFDBD4594D.DDC375FA-ONC12571F6.0054A1A9-422571F6.005AFBA6@tudor.lu>
References: <OFDBD4594D.DDC375FA-ONC12571F6.0054A1A9-422571F6.005AFBA6@tudor.lu>
Message-ID: <971536df0609270951p4a500f3eo923262c6a31195a0@mail.gmail.com>

Look at the arguments to panel.curve

   ?panel.curve

and try RSiteSearch("panel.curve") for some examples.


On 9/27/06, jessica.gervais at tudor.lu <jessica.gervais at tudor.lu> wrote:
> Hi,
>
> I am trying to fit experimental points by exponemtial curve
>
> my data are stored into a matrix data
>
> the first column is the geographical point (a number = data[,1] ) ( I
> would like to plot several graphes at  the same time)
> the second column is the time of measurement (x in the plot)
> the third column is a speed (y in the plot)
>
> if we assume the point are folowing this exponential behaviour y=exp(a+bx)
> then log y = a+ bx
> we then can determine the coefficient a and b by a linear regression with
> the lm function and get them as following : coef ( lm (log(y)~x))
> then I can use those coefficient
>
>
> if I plot ln y = ax+b , everything goes fine
>
>
> xyplot(log(data[,3])~data[,2]|data[,1],panel=function(x,y){panel.xyplot(x,y)+panel.abline(coef(lm(y~x)))})
> and I get perfect linear regression of my points
>
> ...But I would prefer to plot the exponential curves (y=exp ( a*x + b ))..
> I tried the following formula :
>
> >
> xyplot(data[,3]~data[,2]|data[,1],panel=function(x,y){panel.xyplot(x,y)+panel.curve(coef(lm(log(y)~x))[1])})
>
> and I get :
> Error in model.frame(formula, rownames, variables, varnames, extras,
> extranames,  :
>        variable lengths differ (found for 'x')
>
> ... I don't really now what goes wrong and how to correct that
> Maybe I am wrong in the use of the pannel.curve function ....
>
> Do anyone know something about that ?
>
>
> Thanks by advance
>
> Jessica Gervais
>
>
>
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From cooch17 at verizon.net  Wed Sep 27 18:53:28 2006
From: cooch17 at verizon.net (Evan Cooch)
Date: Wed, 27 Sep 2006 12:53:28 -0400
Subject: [R] multidimensional lists
In-Reply-To: <Pine.LNX.4.64.0609271731411.18168@gannet.stats.ox.ac.uk>
References: <451AA1D4.7080902@verizon.net>
	<Pine.LNX.4.64.0609271731411.18168@gannet.stats.ox.ac.uk>
Message-ID: <451AAC88.9070406@verizon.net>

Prof Brian Ripley wrote:
> matrix(some_list, nr, nc, byrow=TRUE)  may be what you are looking for.
>
> R arrays can be of any vector type, including list.  I'd get used to 
> R's Fortran ordering rather than force transposes all the time.

Thanks very much. And I thought I'd left some aspects of my FORTRAN 
coding experience in the past. Would seem not.


From cooch17 at verizon.net  Wed Sep 27 18:54:16 2006
From: cooch17 at verizon.net (Evan Cooch)
Date: Wed, 27 Sep 2006 12:54:16 -0400
Subject: [R] multidimensional lists
In-Reply-To: <971536df0609270933v1e91d34cu89333f149ac10552@mail.gmail.com>
References: <451AA1D4.7080902@verizon.net>
	<971536df0609270933v1e91d34cu89333f149ac10552@mail.gmail.com>
Message-ID: <451AACB8.7080109@verizon.net>

Gabor Grothendieck wrote:
> Try this:
>
> AA <- matrix(list(A, 10*A, 100*A, 1000*A), 2, byrow = TRUE)
> AA[1,2]
>

Seems to do the trick. Thanks!


From f.harrell at vanderbilt.edu  Wed Sep 27 19:16:31 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 27 Sep 2006 12:16:31 -0500
Subject: [R] exact 95% confidence intervals
In-Reply-To: <x2y7s5zc7k.fsf@viggo.kubism.ku.dk>
References: <359344770.26135@capitalbio.com>	<x27izp1xo4.fsf@viggo.kubism.ku.dk>
	<451A6ED2.6080004@vanderbilt.edu>
	<x2y7s5zc7k.fsf@viggo.kubism.ku.dk>
Message-ID: <451AB1EF.4040601@vanderbilt.edu>

Peter Dalgaard wrote:
> Frank E Harrell Jr <f.harrell at vanderbilt.edu> writes:
> 
>> Peter Dalgaard wrote:
>>> "XinMeng" <xmeng at capitalbio.com> writes:
>>>
>>>> Hello sir:
>>>> As to the 2*2 table format for reporting results comparing a new
>>>> test to true diagnosis,when I got the sensitivity and
>>>> specificity,how can I calculate the exact 95% confidence intervals
>>>> (based on the binomial distribution) for sensitivity and specificity
>>>> via R?
>>> Just run binom.test on the data from each column (or row, depending
>>> on
>>> which way you turn the tables).
>>>
>> But beware of exact binomial intervals - they are often not very
>> accurate.  Wilson and other intervals are generally better.  For
>> example see the binconf function in the Hmisc package.
> 
> I suppose that by "accurate" you mean that they are generally better
> at getting the coverage rate right? 

Yes

> 
> The "exact" intervals are strictly conservative, but at least
> predictably so. The whole thing is largely a matter of taste to my
> mind, but I know that other people (notably Alan Agresti) have
> stronger opinions.

I tend to side with Agresti, and emphasize the expected absolute error 
in confidence coverage.

> 
> (People taking an interest in this may want to have a look at
> http://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval )

nice to know!

Frank


From ggrothendieck at gmail.com  Wed Sep 27 19:39:55 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 27 Sep 2006 13:39:55 -0400
Subject: [R] multidimensional lists
In-Reply-To: <971536df0609270933v1e91d34cu89333f149ac10552@mail.gmail.com>
References: <451AA1D4.7080902@verizon.net>
	<971536df0609270933v1e91d34cu89333f149ac10552@mail.gmail.com>
Message-ID: <971536df0609271039h332c89a1m1284d363442b2816@mail.gmail.com>

Just one other comment if the matrices have the same dimensions:
they could alternately be represented as a 4d array:

   A <- matrix(1:4, 2)

   AA <- matrix(list(A, 10*A, 100*A, 1000*A), 2, byrow = TRUE)
   AA[1,2]

   AAA <- array(unlist(AA), c(2,2,2,2))
   AAA[,,1,2] # same

This could have an advantage if you need to do things like
easily take the top left element of each matrix,

   AAA[1,1,,]

which in terms of AA would have required the longer expression:

   replace(AA, TRUE, lapply(AA, "[", 1, 1))


On 9/27/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Try this:
>
> AA <- matrix(list(A, 10*A, 100*A, 1000*A), 2, byrow = TRUE)
> AA[1,2]
>
>
> On 9/27/06, Evan Cooch <cooch17 at verizon.net> wrote:
> > In the process of moving a number of my scripts from MATLAB -> R, I've
> > discovered that there is no 'pure' equivalent of MATLAB's cell arrays,
> > which I use quite often. Basically, I create matrices (as a cell array)
> > where each element of the matrix is itself a matrix (e.g., 2x2 cell
> > array where each element of the array is another matrix). I pass these
> > cell arrays to various functions which then do clever things with the
> > various matrices (of course) - basically, I need to be able to pass
> > collections of matrices to functions to do various things, and I need to
> > be able to control the dimensionality of the cell array to preserve some
> > structural relationships among the matrices in the array. The cell array
> > in MATLAB handles this with aplomb.
> >
> > So far, in R, I've used lists. Given (say) 4 matrices (A,B,C,D), in
> > MATLAB I can use
> >
> > test={A,B,C,D} for a row vector cell array, or
> >
> > test={A;B;C;D} for a column vector cell array.
> >
> > In R, I get more or less the same thing using
> >
> > test=list(A,B,C,D)
> >
> > but this only gives me a row list. For a bunch of technical reasons, I
> > need to be able to control the orientation (as noted)- this is
> > especially true for n-dimensional cell arrays. In MATLAB, for example, I
> > could generate a (say) 2x2 cell array using
> >
> > test={A B;C D}
> >
> > The only way I can figure out how to do this in R is using something like
> >
> > test=list(A,B,C,D);
> > dim(test) < c(2,2);
> >
> > This seems to work, but defaults to bycolumn (in other words, instead of
> >
> > A  B
> > C  D
> >
> > I get
> >
> > A  C
> > B  D
> >
> > )
> >
> > So, I follow with
> >
> > test=t(test) as needed to flip the thing around to byrow.
> >
> > OK, so the question is - is there a better way? This *seems* to work,
> > but I'm discovering that R is a lot like working with LaTeX (something I
> > know much more about) - you can do most things, but there is often a
> > more elegant way if you can figure out how to find out about it.
> >
> > Thanks in advance...
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From cooch17 at verizon.net  Wed Sep 27 19:46:35 2006
From: cooch17 at verizon.net (Evan Cooch)
Date: Wed, 27 Sep 2006 13:46:35 -0400
Subject: [R] multidimensional lists
In-Reply-To: <971536df0609271039h332c89a1m1284d363442b2816@mail.gmail.com>
References: <451AA1D4.7080902@verizon.net>
	<971536df0609270933v1e91d34cu89333f149ac10552@mail.gmail.com>
	<971536df0609271039h332c89a1m1284d363442b2816@mail.gmail.com>
Message-ID: <451AB8FB.10906@verizon.net>

Gabor Grothendieck wrote:
> Just one other comment if the matrices have the same dimensions:

Indeed - that is quite often the case - thanks for the further 
suggestion(s).

> they could alternately be represented as a 4d array:
>
>   A <- matrix(1:4, 2)
>
>   AA <- matrix(list(A, 10*A, 100*A, 1000*A), 2, byrow = TRUE)
>   AA[1,2]
>
>   AAA <- array(unlist(AA), c(2,2,2,2))
>   AAA[,,1,2] # same
>
> This could have an advantage if you need to do things like
> easily take the top left element of each matrix,
>
>   AAA[1,1,,]
>
> which in terms of AA would have required the longer expression:
>
>   replace(AA, TRUE, lapply(AA, "[", 1, 1))
>
>
> On 9/27/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
>> Try this:
>>
>> AA <- matrix(list(A, 10*A, 100*A, 1000*A), 2, byrow = TRUE)
>> AA[1,2]
>>
>>
>> On 9/27/06, Evan Cooch <cooch17 at verizon.net> wrote:
>> > In the process of moving a number of my scripts from MATLAB -> R, I've
>> > discovered that there is no 'pure' equivalent of MATLAB's cell arrays,
>> > which I use quite often. Basically, I create matrices (as a cell 
>> array)
>> > where each element of the matrix is itself a matrix (e.g., 2x2 cell
>> > array where each element of the array is another matrix). I pass these
>> > cell arrays to various functions which then do clever things with the
>> > various matrices (of course) - basically, I need to be able to pass
>> > collections of matrices to functions to do various things, and I 
>> need to
>> > be able to control the dimensionality of the cell array to preserve 
>> some
>> > structural relationships among the matrices in the array. The cell 
>> array
>> > in MATLAB handles this with aplomb.
>> >
>> > So far, in R, I've used lists. Given (say) 4 matrices (A,B,C,D), in
>> > MATLAB I can use
>> >
>> > test={A,B,C,D} for a row vector cell array, or
>> >
>> > test={A;B;C;D} for a column vector cell array.
>> >
>> > In R, I get more or less the same thing using
>> >
>> > test=list(A,B,C,D)
>> >
>> > but this only gives me a row list. For a bunch of technical reasons, I
>> > need to be able to control the orientation (as noted)- this is
>> > especially true for n-dimensional cell arrays. In MATLAB, for 
>> example, I
>> > could generate a (say) 2x2 cell array using
>> >
>> > test={A B;C D}
>> >
>> > The only way I can figure out how to do this in R is using 
>> something like
>> >
>> > test=list(A,B,C,D);
>> > dim(test) < c(2,2);
>> >
>> > This seems to work, but defaults to bycolumn (in other words, 
>> instead of
>> >
>> > A  B
>> > C  D
>> >
>> > I get
>> >
>> > A  C
>> > B  D
>> >
>> > )
>> >
>> > So, I follow with
>> >
>> > test=t(test) as needed to flip the thing around to byrow.
>> >
>> > OK, so the question is - is there a better way? This *seems* to work,
>> > but I'm discovering that R is a lot like working with LaTeX 
>> (something I
>> > know much more about) - you can do most things, but there is often a
>> > more elegant way if you can figure out how to find out about it.
>> >
>> > Thanks in advance...
>> >
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>> >
>>
>


From bzuckerb at syr.edu  Wed Sep 27 19:52:52 2006
From: bzuckerb at syr.edu (Benjamin Zuckerberg)
Date: Wed, 27 Sep 2006 13:52:52 -0400
Subject: [R] AIC Methods
Message-ID: <20060927135252.bpl80dlv38jok8k4@mymail.syr.edu>


I am having trouble with getting AIC statistics.  I have developed a  
number of ancova models using the lm function.  I am trying to get a  
number of AIC statistics suggested by Burnham and Anderson (2002)  
including raw AIC values, AICc values, Akaike weights, and Delta AIC.   
When I use the AIC function in (stats), I only get a single AIC value  
including degrees of freedom.  When I use the summary statement as  
suggested in the (aod) package, I do not get the resulting AIC  
statistics.  What am I doing wrong?  Is this a product of using the lm  
function for my models?  I have even investigated the use of  
extractAIC, but that does not provide me with the statistics I am  
looking for as well.  Do you have any suggestions?  In addition, I am  
also curious if there is a package or procedure for model averaging.

Thank you!

-- 
Benjamin Zuckerberg
Doctoral Candidate
State University of New York
College of Environmental Science and Forestry
Illick 244A, 1 Forestry Drive
Syracuse, New York 13210
Tele: (315) 470-6985
E-mail: bzuckerb at syr.edu


From rvaradhan at jhmi.edu  Wed Sep 27 19:56:09 2006
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Wed, 27 Sep 2006 13:56:09 -0400
Subject: [R] PDE
In-Reply-To: <451A8D1A.2080006@bordeaux.inra.fr>
Message-ID: <000301c6e25e$36ce22c0$7c94100a@win.ad.jhu.edu>

Hi Christophe,

What is the PDE that you are trying to solve?  Is it
parabolic/hyperbolic/elliptical/somethingelse?  Is it linear/nonlinear?  

If time is one of the independent variables, you can transform the PDE into
an initial value problem (system of ODEs) by using finite difference
approximations of the partial derivatives of other independent variables
(typically, these are spatial coordinates).  Starting with an initial set of
values on a grid of points (also known as initial conditions, which are part
of the problem specification), you update them at different times, using
fixed or varying time steps.

R has very limited functionality for handling differential equations.  So,
you should look for FORTRAN libraries, from which you can create DLLs to be
used in R.

Hope this help,
Ravi.

----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Christophe Nguyen
Sent: Wednesday, September 27, 2006 10:39 AM
To: r-help at stat.math.ethz.ch
Subject: [R] PDE

Dear all,
Does any know how to solve PDE with R? The archive list refers to the 
use of ODE if PDE are parabolic. I am not a mathematician and this does 
not mean anything for me!
help would be very appreciated.
Many thanks

-- 
___________________________________________________

Christophe NGUYEN

UMR 1220 INRA-ENITAB
Transfert sol-plante et cycle des ?l?ments min?raux
dans les ?cosyst?mes cultiv?s"

Centre INRA de Bordeaux-Aquitaine
71, avenue Edouard Bourlaux, BP 81
33883 Villenave d'Ornon, FRANCE

Tel : 00 33 (0)5 57 12 25 07
Fax : 00 33 (0)5 57 12 25 15

email : Christophe.Nguyen at bordeaux.inra.fr
page infoservice: http://www.bordeaux.inra.fr/tcem

__________m?O?m____________________________________

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From peterdlauren at yahoo.com  Wed Sep 27 22:11:10 2006
From: peterdlauren at yahoo.com (Peter Lauren)
Date: Wed, 27 Sep 2006 13:11:10 -0700 (PDT)
Subject: [R] Single Precision (4 byte) floats with readBin
Message-ID: <20060927201110.23226.qmail@web30310.mail.mud.yahoo.com>

I would like to use readBin to read a binary data
file.  Most of the data is 4-byte floating point but,
for some reason, only double precision appears to be
offered.  I tried 
fVariable=readBin(iFile,what=single());
and got 35.87879 which looks believable except that
the correct value is 3.030303.  I then tried
fVariable=readBin(iFile,what=single(),4);
and got 
[1]  3.831111e+10  6.657199e+10 -5.592394e+29
-5.592397e+29

For the second call, there were two more single
precision floats of value 3.030303 followed by two
more with values 40.46 and 0.00 respectively.

Is there any way around this problem other than to
make the input data double (which I definitely do not
want to do)? 

Many thanks in advance,
Peter.


From spluque at gmail.com  Wed Sep 27 22:12:38 2006
From: spluque at gmail.com (Sebastian P. Luque)
Date: Wed, 27 Sep 2006 15:12:38 -0500
Subject: [R] how to retain time zone when doing c(POSIXct)
Message-ID: <877izpax1l.fsf@patagonia.sebmags.homelinux.org>

Hello,

What is the best way to concatenate POSIXct objects keeping the time zone
attribute in a program?  For example:


R> xx <- as.POSIXct(strptime(c("2006-09-26 12:00:00", "2006-09-26 13:00:00"),
+                           format="%Y-%m-%d %H:%M:%S"), tz="GMT")
R> xx
[1] "2006-09-26 12:00:00 GMT" "2006-09-26 13:00:00 GMT"


but, as ?c.POSIXct explains:


R> c(xx, xx[1] - 60, xx[2] + 60)
[1] "2006-09-26 07:00:00 CDT" "2006-09-26 08:00:00 CDT"
[3] "2006-09-26 06:59:00 CDT" "2006-09-26 08:01:00 CDT"


Is there something better/safer than simply setting the "tzone" attribute
of the new object?


Cheers,

-- 
Seb


From sundar.dorai-raj at pdf.com  Wed Sep 27 22:33:59 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 27 Sep 2006 15:33:59 -0500
Subject: [R] Single Precision (4 byte) floats with readBin
In-Reply-To: <20060927201110.23226.qmail@web30310.mail.mud.yahoo.com>
References: <20060927201110.23226.qmail@web30310.mail.mud.yahoo.com>
Message-ID: <451AE037.2020007@pdf.com>



Peter Lauren said the following on 9/27/2006 3:11 PM:
> I would like to use readBin to read a binary data
> file.  Most of the data is 4-byte floating point but,
> for some reason, only double precision appears to be
> offered.  I tried 
> fVariable=readBin(iFile,what=single());
> and got 35.87879 which looks believable except that
> the correct value is 3.030303.  I then tried
> fVariable=readBin(iFile,what=single(),4);
> and got 
> [1]  3.831111e+10  6.657199e+10 -5.592394e+29
> -5.592397e+29
> 
> For the second call, there were two more single
> precision floats of value 3.030303 followed by two
> more with values 40.46 and 0.00 respectively.
> 
> Is there any way around this problem other than to
> make the input data double (which I definitely do not
> want to do)? 
> 
> Many thanks in advance,
> Peter.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


Hi, Peter,

I believe you can use

readBin(file, double(), size = 4)

Thanks,

--sundar


From peterdlauren at yahoo.com  Wed Sep 27 22:42:14 2006
From: peterdlauren at yahoo.com (Peter Lauren)
Date: Wed, 27 Sep 2006 13:42:14 -0700 (PDT)
Subject: [R] Single Precision (4 byte) floats with readBin
In-Reply-To: <451AE037.2020007@pdf.com>
Message-ID: <20060927204215.92372.qmail@web30313.mail.mud.yahoo.com>


--- Sundar Dorai-Raj <sundar.dorai-raj at pdf.com> wrote:

> 
> 
> Peter Lauren said the following on 9/27/2006 3:11
> PM:
> > I would like to use readBin to read a binary data
> > file.  Most of the data is 4-byte floating point
> but,
> > for some reason, only double precision appears to
> be
> > offered.  I tried 
> > fVariable=readBin(iFile,what=single());
> > and got 35.87879 which looks believable except
> that
> > the correct value is 3.030303.  I then tried
> > fVariable=readBin(iFile,what=single(),4);
> > and got 
> > [1]  3.831111e+10  6.657199e+10 -5.592394e+29
> > -5.592397e+29
> > 
> > For the second call, there were two more single
> > precision floats of value 3.030303 followed by two
> > more with values 40.46 and 0.00 respectively.
> > 
> > Is there any way around this problem other than to
> > make the input data double (which I definitely do
> not
> > want to do)? 
> > 
> > Many thanks in advance,
> > Peter.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained,
> reproducible code.
> 
> 
> Hi, Peter,
> 
> I believe you can use
> 
> readBin(file, double(), size = 4)
> 

It worked!  Thanks very much.

Peter.


From dchan at GFC.STATE.GA.US  Wed Sep 27 22:47:34 2006
From: dchan at GFC.STATE.GA.US (Dan Chan)
Date: Wed, 27 Sep 2006 16:47:34 -0400
Subject: [R] Converting text to numbers
Message-ID: <43679C69FEEE9C40AC8876C0FF38EF10014FEBC1@mailsvr01.gfc.state.ga.us>

Hi,

I have Forecast Class and Observed Class in a data matrix as below. 

> Sample1
  FCT OBS
1   1  5 
2   2   4
3  3-  3+
4   3   3
5  3+  3-
6   4   2
7   5   1

I want to find the difference between Observed and Forecast Classes.
How can I get this done?

I tried to following to convert the 1 through 5 classes, to 1 through 7
for both OBS and FCT column.
> Sample1$OBS2 <- Sample1$OBS
> levels(Sample1$OBS2) <- sub('5',7,levels(Sample1$OBS2),fixed=TRUE)
> levels(Sample1$OBS2) <- sub('4',6,levels(Sample1$OBS2),fixed=TRUE)
> levels(Sample1$OBS2) <- sub('3+',5,levels(Sample1$OBS2), fixed=TRUE)
> levels(Sample1$OBS2) <- sub('3',4,levels(Sample1$OBS2),fixed=TRUE)
> levels(Sample1$OBS2) <- sub('4-',3,levels(Sample1$OBS2),fixed=TRUE)
> Sample1
  FCT OBS FCT2 OBS2
1   1  5     1   7 
2   2   4    2    6
3  3-  3+    3    5
4   3   3    4    4
5  3+  3-    5    3
6   4   2    6    2
7   5   1    7    1

All looks good, but as I do the following, I encounter an error.
> Sample1$OBS2- Sample1$FCT2
[1] NA NA NA NA NA NA NA
Warning message:
- not meaningful for factors in: Ops.factor(Sample1$OBS2, Sample1$FCT2)

Then, I tried to convert them to numbers using the following.
> Sample1$FCT2 <- as.numeric(Sample1$FCT2)
> Sample1$OBS2 <- as.numeric(Sample1$OBS2)
> Sample1
  FCT OBS FCT2 OBS2
1   1  5     1    7
2   2   4    2    6
3  3-  3+    4    5
4   3   3    3    3
5  3+  3-    5    4
6   4   2    6    2
7   5   1    7    1

Sample1$FCT2[3] and Sample1$FCT2[4] switched values. 

I think it has something to do with the following: 
> Sample1$OBS
[1] 5  4  3+ 3  3- 2   1
Levels:  1 2 3 3- 3+ 4 5 

But, I don't know why and how to fix it.

Any ideas? 

Thank you. 



Daniel Chan
Meteorologist
Georgia Forestry Commission
P O Box 819
Macon, GA 
31202
Tel: 478-751-3508
Fax: 478-751-3465


From rmh at temple.edu  Wed Sep 27 22:49:09 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Wed, 27 Sep 2006 16:49:09 -0400 (EDT)
Subject: [R] t-stat Curve
Message-ID: <20060927164909.BIW21759@po-d.temple.edu>

## There is some ambiguity in your example.
## You stated a one-sided hypothesis and calculated qt() values for both sides.
## I show both the one-sided and two-sided displays.

library(HH)
## HH_1.5 is available from CRAN for R-2.3.1
##
## HH_1.5 ignores the df.t argument and interprets the request as a
## normal distribution.

## HH_1.8 has been accepted for CRAN for R-2.4.0 and will be in the
## standard places on CRAN when R-2.4.0 is released.
##
## HH_1.8 uses the df.t argument and interprets the request as a
## t-distribution.

old.par <- par(oma=c(4,0,2,5), mar=c(7,7,4,2)+.1)

crit.val.t <- qt(c(.05,.95), 24)
crit.val <- crit.val.t*(12/sqrt(25)) + 80

observed.t <- -1.25
observed.ybar <- 77

norm.setup(mean=80, n=25, sd=12, df.t=24, xlim=c(70,90),
           main="two-sided alpha=.10")
norm.curve(mean=80, n=25, sd=12, df.t=24, crit=crit.val)
abline(v=observed.ybar)
axis(side=3, at=observed.ybar, line=-.5)

norm.setup(mean=80, n=25, sd=12, df.t=24, xlim=c(70,90),
           main="one-sided alpha=.05")
norm.curve(mean=80, n=25, sd=12, df.t=24, crit=crit.val[1], shade="left")
abline(v=observed.ybar)
axis(side=3, at=observed.ybar, line=-.5)

par(old.par)


From BEN at SSANET.COM  Wed Sep 27 23:16:11 2006
From: BEN at SSANET.COM (Ben Fairbank)
Date: Wed, 27 Sep 2006 16:16:11 -0500
Subject: [R] Space required by object?
Message-ID: <CA612484A337C6479EA341DF9EEE14AC05711C1F@hercules.ssainfo>

Does R provide a function analogous to LS() or str() that reports the
storage space, on disk or in memory, required by objects?

Ben Fairbank


From mothsailor at googlemail.com  Wed Sep 27 23:28:37 2006
From: mothsailor at googlemail.com (David Barron)
Date: Wed, 27 Sep 2006 22:28:37 +0100
Subject: [R] Converting text to numbers
In-Reply-To: <43679C69FEEE9C40AC8876C0FF38EF10014FEBC1@mailsvr01.gfc.state.ga.us>
References: <43679C69FEEE9C40AC8876C0FF38EF10014FEBC1@mailsvr01.gfc.state.ga.us>
Message-ID: <815b70590609271428r47ce5302y451e96adc1909316@mail.gmail.com>

> Then, I tried to convert them to numbers using the following.
> > Sample1$FCT2 <- as.numeric(Sample1$FCT2)
> > Sample1$OBS2 <- as.numeric(Sample1$OBS2)


This is actually an FAQ.  Do the following and it should be fine:

> Sample1$FCT2 <- as.numeric(as.character(Sample1$FCT2))
> Sample1$OBS2 <- as.numeric(as.character(Sample1$OBS2))



-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From jholtman at gmail.com  Wed Sep 27 23:33:17 2006
From: jholtman at gmail.com (jim holtman)
Date: Wed, 27 Sep 2006 17:33:17 -0400
Subject: [R] Converting text to numbers
In-Reply-To: <43679C69FEEE9C40AC8876C0FF38EF10014FEBC1@mailsvr01.gfc.state.ga.us>
References: <43679C69FEEE9C40AC8876C0FF38EF10014FEBC1@mailsvr01.gfc.state.ga.us>
Message-ID: <644e1f320609271433n47be8b7dl78d7cd7da75455bd@mail.gmail.com>

> x <- read.table('clipboard', header=T, as.is=T)
> str(x)
`data.frame':   7 obs. of  2 variables:
 $ FCT: chr  "1" "2" "3-" "3" ...
 $ OBS: chr  "5" "4" "3+" "3" ...
#  define your conversion
> x.c <- c('1'=1, '2'=2, '3'=3, '3-'=2, '3+'=5, '4-'=3, '4'=6, '5'=7)
> x.c[x$FCT]
 1  2 3-  3 3+  4  5
 1  2  2  3  5  6  7
> x$FCT1 <- x.c[x$FCT]
> x$OBS1 <- x.c[x$OBS]
> x
  FCT OBS FCT1 OBS1
1   1   5    1    7
2   2   4    2    6
3  3-  3+    2    5
4   3   3    3    3
5  3+  3-    5    2
6   4   2    6    2
7   5   1    7    1
> str(x)
`data.frame':   7 obs. of  4 variables:
 $ FCT : chr  "1" "2" "3-" "3" ...
 $ OBS : chr  "5" "4" "3+" "3" ...
 $ FCT1: num  1 2 2 3 5 6 7
 $ OBS1: num  7 6 5 3 2 2 1
> x$FCT1 - x$OBS1
[1] -6 -4 -3  0  3  4  6
>


On 9/27/06, Dan Chan <dchan at gfc.state.ga.us> wrote:
> Hi,
>
> I have Forecast Class and Observed Class in a data matrix as below.
>
> > Sample1
>  FCT OBS
> 1   1  5
> 2   2   4
> 3  3-  3+
> 4   3   3
> 5  3+  3-
> 6   4   2
> 7   5   1
>
> I want to find the difference between Observed and Forecast Classes.
> How can I get this done?
>
> I tried to following to convert the 1 through 5 classes, to 1 through 7
> for both OBS and FCT column.
> > Sample1$OBS2 <- Sample1$OBS
> > levels(Sample1$OBS2) <- sub('5',7,levels(Sample1$OBS2),fixed=TRUE)
> > levels(Sample1$OBS2) <- sub('4',6,levels(Sample1$OBS2),fixed=TRUE)
> > levels(Sample1$OBS2) <- sub('3+',5,levels(Sample1$OBS2), fixed=TRUE)
> > levels(Sample1$OBS2) <- sub('3',4,levels(Sample1$OBS2),fixed=TRUE)
> > levels(Sample1$OBS2) <- sub('4-',3,levels(Sample1$OBS2),fixed=TRUE)
> > Sample1
>  FCT OBS FCT2 OBS2
> 1   1  5     1   7
> 2   2   4    2    6
> 3  3-  3+    3    5
> 4   3   3    4    4
> 5  3+  3-    5    3
> 6   4   2    6    2
> 7   5   1    7    1
>
> All looks good, but as I do the following, I encounter an error.
> > Sample1$OBS2- Sample1$FCT2
> [1] NA NA NA NA NA NA NA
> Warning message:
> - not meaningful for factors in: Ops.factor(Sample1$OBS2, Sample1$FCT2)
>
> Then, I tried to convert them to numbers using the following.
> > Sample1$FCT2 <- as.numeric(Sample1$FCT2)
> > Sample1$OBS2 <- as.numeric(Sample1$OBS2)
> > Sample1
>  FCT OBS FCT2 OBS2
> 1   1  5     1    7
> 2   2   4    2    6
> 3  3-  3+    4    5
> 4   3   3    3    3
> 5  3+  3-    5    4
> 6   4   2    6    2
> 7   5   1    7    1
>
> Sample1$FCT2[3] and Sample1$FCT2[4] switched values.
>
> I think it has something to do with the following:
> > Sample1$OBS
> [1] 5  4  3+ 3  3- 2   1
> Levels:  1 2 3 3- 3+ 4 5
>
> But, I don't know why and how to fix it.
>
> Any ideas?
>
> Thank you.
>
>
>
> Daniel Chan
> Meteorologist
> Georgia Forestry Commission
> P O Box 819
> Macon, GA
> 31202
> Tel: 478-751-3508
> Fax: 478-751-3465
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From jholtman at gmail.com  Wed Sep 27 23:42:41 2006
From: jholtman at gmail.com (jim holtman)
Date: Wed, 27 Sep 2006 17:42:41 -0400
Subject: [R] Space required by object?
In-Reply-To: <CA612484A337C6479EA341DF9EEE14AC05711C1F@hercules.ssainfo>
References: <CA612484A337C6479EA341DF9EEE14AC05711C1F@hercules.ssainfo>
Message-ID: <644e1f320609271442u65069f4aha4f823bc8f21c09f@mail.gmail.com>

Here is a function I use:

 my.ls  <-  function(pos=1, sorted=F){
   .result <- sapply(ls(pos=pos, all.names=TRUE),
       function(..x)object.size(eval(as.symbol(..x))))
   if (sorted){
       .result <- rev(sort(.result))
   }
   .ls <- as.data.frame(rbind(as.matrix(.result),"**Total"=sum(.result)))
   names(.ls) <- "Size"
   .ls$Size <- formatC(.ls$Size, big.mark=',', digits=0, format='f')
   .ls$Mode <- c(unlist(lapply(rownames(.ls)[-nrow(.ls)],
       function(x)mode(eval(as.symbol(x))))), '-------')
   .ls
}


On 9/27/06, Ben Fairbank <BEN at ssanet.com> wrote:
> Does R provide a function analogous to LS() or str() that reports the
> storage space, on disk or in memory, required by objects?
>
> Ben Fairbank
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From im at edc.pitt.edu  Wed Sep 27 23:55:56 2006
From: im at edc.pitt.edu (Im, Kelly)
Date: Wed, 27 Sep 2006 17:55:56 -0400
Subject: [R] MSM modeling and transition rates in R
Message-ID: <E3FBCB35FBB8294A9B480A3B2DB7335689E486@boa.edc3.pitt.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060927/5883d8ca/attachment.pl 

From hb at stat.berkeley.edu  Thu Sep 28 00:02:13 2006
From: hb at stat.berkeley.edu (Henrik Bengtsson)
Date: Wed, 27 Sep 2006 15:02:13 -0700
Subject: [R] Space required by object?
In-Reply-To: <CA612484A337C6479EA341DF9EEE14AC05711C1F@hercules.ssainfo>
References: <CA612484A337C6479EA341DF9EEE14AC05711C1F@hercules.ssainfo>
Message-ID: <59d7961d0609271502v1b0b2eacp6f5178b0dbcf854f@mail.gmail.com>

See ll() in the R.oo package.  /Henrik

On 9/27/06, Ben Fairbank <BEN at ssanet.com> wrote:
> Does R provide a function analogous to LS() or str() that reports the
> storage space, on disk or in memory, required by objects?
>
> Ben Fairbank
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jafarikia at gmail.com  Thu Sep 28 00:29:51 2006
From: jafarikia at gmail.com (Mohsen Jafarikia)
Date: Wed, 27 Sep 2006 18:29:51 -0400
Subject: [R] Histogram
Message-ID: <e3f2a5ab0609271529s256c5e71o9df4245b0e60933d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060927/08ebb865/attachment.pl 

From ritwik.sinha at gmail.com  Thu Sep 28 01:47:46 2006
From: ritwik.sinha at gmail.com (Ritwik Sinha)
Date: Wed, 27 Sep 2006 19:47:46 -0400
Subject: [R] Histogram
In-Reply-To: <e3f2a5ab0609271529s256c5e71o9df4245b0e60933d@mail.gmail.com>
References: <e3f2a5ab0609271529s256c5e71o9df4245b0e60933d@mail.gmail.com>
Message-ID: <42bc98300609271647o1f2ca111v86746ee180c3e78d@mail.gmail.com>

Hi,

There may be an easier way but here is one way you can do it.

# create vector that has Y[i] X[i]s
new.data <- rep(X,Y)

hist(new.data, breaks=c(0,.1,.4,.6)) # or something like that look at
what exactly breaks should be.

Ritwik.

On 9/27/06, Mohsen Jafarikia <jafarikia at gmail.com> wrote:
> Dear all,
>
> I want to design a histogram and I need to have the frequency at certain
> points. For example I have the following 2 columns:
>
> *X      Y*
>
> 0.1    25
> 0.4    22
> 0.45  11
> 0.55  21
>
> I want the chart to have 4 columns. First column is from 0.0-0.1 (on X) and
> frequency is 25. Next colum is wider and form 0.1-0.4 with 22 frequency.
> Next column is narrow with 11 frequency and the last column is the same as
> the first one with 21 frequency.
>
> Can anybody tell me how I can have this chart.
>
> Thanks,
> Mohsen
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Ritwik Sinha
Graduate Student
Epidemiology and Biostatistics
Case Western Reserve University
ritwik.sinha at gmail.com | +12163682366 | http://darwin.cwru.edu/~rsinha


From MSchwartz at mn.rr.com  Thu Sep 28 02:50:19 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Wed, 27 Sep 2006 19:50:19 -0500
Subject: [R] Histogram
In-Reply-To: <e3f2a5ab0609271529s256c5e71o9df4245b0e60933d@mail.gmail.com>
References: <e3f2a5ab0609271529s256c5e71o9df4245b0e60933d@mail.gmail.com>
Message-ID: <1159404619.4268.10.camel@localhost.localdomain>

On Wed, 2006-09-27 at 18:29 -0400, Mohsen Jafarikia wrote:
> Dear all,
> 
> I want to design a histogram and I need to have the frequency at certain
> points. For example I have the following 2 columns:
> 
> *X      Y*
> 
> 0.1    25
> 0.4    22
> 0.45  11
> 0.55  21
> 
> I want the chart to have 4 columns. First column is from 0.0-0.1 (on X) and
> frequency is 25. Next colum is wider and form 0.1-0.4 with 22 frequency.
> Next column is narrow with 11 frequency and the last column is the same as
> the first one with 21 frequency.
> 
> Can anybody tell me how I can have this chart.
> 
> Thanks,
> Mohsen

How about this:

X <- c(0, 0.1, 0.4, 0.45, 0.55)
Y <- c(25, 22, 11, 21)

barplot(Y, space = 0, width = diff(X))
axis(1)

See ?barplot and ?diff

HTH,

Marc Schwartz


From kubovy at virginia.edu  Thu Sep 28 03:35:02 2006
From: kubovy at virginia.edu (Michael Kubovy)
Date: Wed, 27 Sep 2006 21:35:02 -0400
Subject: [R] Best use of LaTeX listings package for pretty printing R
	code
In-Reply-To: <4517DA39.4070102@vanderbilt.edu>
References: <4517DA39.4070102@vanderbilt.edu>
Message-ID: <3A07FF27-463F-4868-92BF-F347A00F0008@virginia.edu>

I tried to use Frank's formatting improvements to compile Sweave- 
produced .tex, but it doesn't seem to make a difference. Advice?

On Sep 25, 2006, at 9:31 AM, Frank E Harrell Jr wrote:

> This is what I have been using.  Does anyone have a better way?  In
> particular I would like to see letters in comment strings not  
> stretched
> so much.  Thanks -Frank
>
> \documentclass{article}
> \usepackage{listings,relsize}
> \lstloadlanguages{R}
> \newcommand{\lil}[1]{\lstinline|#1|}
>
> \begin{document}
> \lstset{language=R,basicstyle=\smaller,commentstyle=\rmfamily\smaller,
>   showstringspaces=false,%
>   xleftmargin=4ex,literate={<-}{{$\leftarrow$}}1 {~}{{$\sim$}}1}
> \lstset{escapeinside={(*}{*)}}   % for (*\ref{ }*) inside  
> lstlistings (S
> code)
> \begin{lstlisting}
> a <- b   # this is a test line
> if(i==3) {  # another line, for y^2
>   y <- 3^3
>   z <- 'this string'
>   qqcat <- y ~ pol(x,2)
> } else y <- 4
> \end{lstlisting}
> That was \lstinline|x <- 22| \lil{q <- 'cat'}.
> \end{document}

_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From bgreen at dyson.brisnet.org.au  Thu Sep 28 04:27:07 2006
From: bgreen at dyson.brisnet.org.au (bgreen at dyson.brisnet.org.au)
Date: Thu, 28 Sep 2006 12:27:07 +1000 (EST)
Subject: [R] recode problem -  unexplained values
In-Reply-To: <mailman.9.1159351203.25554.r-help@stat.math.ethz.ch>
References: <mailman.9.1159351203.25554.r-help@stat.math.ethz.ch>
Message-ID: <10151.144.131.210.250.1159410427.squirrel@144.131.210.250>

I am hoping for some advice regarding the difficulties I have been having
recoding variables which are contained in a csv file.  Table 1 (below) 
shows there are two types of blanks - as reported in the first two
columns. I am using windows XP & the latets version of R.

When blanks cells are replaced with a value of n using syntax: > affect
[affect==""] <- "n"
there are still 3 blank values (Table 2).   When as.numeric is applied,
this also causes problems because values of 2,3 & 4 are generated rather
than just 1 & 2.

TABLE 1

table(group,actions)
     actions
group           n   y
    1 100   2   0   3
    2  30   1   1   0
    3  24   0   0   0



TABLE 2

>  table(group,actions)
     actions
group           n   y
    1   0   2 100   3
    2   0   1  31   0
    3   0   0  24   0


Below is another example - for some reason there are 2 types of 'aobh'
values.


> table(group, type)
     type
group aobh aobh   gbh   m  uw
    1  104      1   0   0   0
    2    0      0  15   0  17
    3    0      0   0  24   0


Any assistance is much appreciated,


Bob Green


From ssk2031 at columbia.edu  Thu Sep 28 06:02:02 2006
From: ssk2031 at columbia.edu (Suresh Krishna)
Date: Thu, 28 Sep 2006 00:02:02 -0400
Subject: [R] Nonlinear fitting - reparametrization help
Message-ID: <op.tgklhoagcrygw3@madder.mahoney.cpmc.columbia.edu>


Hi,

I am trying to fit a function of the form:

y = A0 + A1 * exp( -0.5* ( (X - Mu1) / Sigma1 )^2 ) - A2 * exp ( -0.5*  
( (X-Mu2)/Sigma2 )^2 )

i.e. a mean term (A0) + a difference between two gaussians.

The constraints are A1,A2 >0, Sigma1,Sigma2>0, and usually Sigma2>Sigma1.  
The plot looks like a "Mexican Hat".

I had trouble (poor fits) fitting this function to toy data in Matlab and  
now I am playing with R's nls and optim functions. I keep running into  
singular gradient errors with nls, even with very different starting  
values, and have not yet figured out how to interpret the trace results  
usefully.

Can someone help ? Is there a correct parameterization for this problem ?  
I have appended some R code with sample data to fit.

Thank you !!!

Suresh; please cc ssk2031 at columbia.edu if possible.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
x=seq(-10,10,length=1000)
a0=0;
ae=1;
me=0;
se=1;
ai=0.5;
mi=0;
si=3;
dogy <- function(x,a0,ae,me,se,ai,mi,si){
y=a0+ae*exp(-0.5*(((x-me)/se)^2))-ai*exp(-0.5*(((x-mi)/si)^2))
y}
y=dogy(x,a0,ae,me,se,ai,mi,si)
erval=rnorm(length(y),sd=0.02)
y=y+erval
#plot(x,y+erval)

#fit=nls(y~ae*exp((x/se)^2)-ai*exp((x/si)^2),start=c(ae=.8,se=1.1,ai=.2,si=1),trace=TRUE)    
#here I tried to unsuccesfully reduce the model by eliminating A0, Mu1,  
and Mu2


From ggrothendieck at gmail.com  Thu Sep 28 06:15:46 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 28 Sep 2006 00:15:46 -0400
Subject: [R] Nonlinear fitting - reparametrization help
In-Reply-To: <op.tgklhoagcrygw3@madder.mahoney.cpmc.columbia.edu>
References: <op.tgklhoagcrygw3@madder.mahoney.cpmc.columbia.edu>
Message-ID: <971536df0609272115j2191d5aev9c85e795d9763bf6@mail.gmail.com>

Similar question was recently asked. See this thread:
http://comments.gmane.org/gmane.comp.lang.r.general/69592

On 9/28/06, Suresh Krishna <ssk2031 at columbia.edu> wrote:
>
> Hi,
>
> I am trying to fit a function of the form:
>
> y = A0 + A1 * exp( -0.5* ( (X - Mu1) / Sigma1 )^2 ) - A2 * exp ( -0.5*
> ( (X-Mu2)/Sigma2 )^2 )
>
> i.e. a mean term (A0) + a difference between two gaussians.
>
> The constraints are A1,A2 >0, Sigma1,Sigma2>0, and usually Sigma2>Sigma1.
> The plot looks like a "Mexican Hat".
>
> I had trouble (poor fits) fitting this function to toy data in Matlab and
> now I am playing with R's nls and optim functions. I keep running into
> singular gradient errors with nls, even with very different starting
> values, and have not yet figured out how to interpret the trace results
> usefully.
>
> Can someone help ? Is there a correct parameterization for this problem ?
> I have appended some R code with sample data to fit.
>
> Thank you !!!
>
> Suresh; please cc ssk2031 at columbia.edu if possible.
>
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> x=seq(-10,10,length=1000)
> a0=0;
> ae=1;
> me=0;
> se=1;
> ai=0.5;
> mi=0;
> si=3;
> dogy <- function(x,a0,ae,me,se,ai,mi,si){
> y=a0+ae*exp(-0.5*(((x-me)/se)^2))-ai*exp(-0.5*(((x-mi)/si)^2))
> y}
> y=dogy(x,a0,ae,me,se,ai,mi,si)
> erval=rnorm(length(y),sd=0.02)
> y=y+erval
> #plot(x,y+erval)
>
> #fit=nls(y~ae*exp((x/se)^2)-ai*exp((x/si)^2),start=c(ae=.8,se=1.1,ai=.2,si=1),trace=TRUE)
> #here I tried to unsuccesfully reduce the model by eliminating A0, Mu1,
> and Mu2
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From rmh at temple.edu  Thu Sep 28 06:28:34 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Thu, 28 Sep 2006 00:28:34 -0400 (EDT)
Subject: [R] recode problem -  unexplained values
Message-ID: <20060928002834.BIZ12905@po-d.temple.edu>

I can propose a strategy.

This example shows that there are different types of blanks when you
look at character data.

    as.character(c("", " ", "  ", "   "))

Your test for "" found only one of them.

Look at the data as read.csv produces it.  That will probably give you
some clues.

mydata <- read.csv("filename")

mydata

as.character(mydata)




Rich


From mel at altk.com  Thu Sep 28 09:01:48 2006
From: mel at altk.com (mel)
Date: Thu, 28 Sep 2006 09:01:48 +0200
Subject: [R] Space required by object?
In-Reply-To: <CA612484A337C6479EA341DF9EEE14AC05711C1F@hercules.ssainfo>
References: <CA612484A337C6479EA341DF9EEE14AC05711C1F@hercules.ssainfo>
Message-ID: <451B735C.7070008@altk.com>

?object.size
hih


From ottorino-luca.pantani at unifi.it  Thu Sep 28 09:19:17 2006
From: ottorino-luca.pantani at unifi.it (8rino-Luca Pantani)
Date: Thu, 28 Sep 2006 09:19:17 +0200
Subject: [R] I: differences between R and S (reproducing a plot from a book )
Message-ID: <DNEELNJCLGBOLHCFLMHBIEHFEBAA.OLPantani@unifi.it>

Dear R-Users,
I'm currently studying the book
Statistical model in S by Chambers J.M and Hastie T.J..

At page n 3 there's a plot showing the means of a variable at each of the
levels of the factors of an experiment.
I hope to be able to reproduce it here by ASCII art.

              B6
  S           |
  |   Thin    |
  |     |     |
  |     |     B3
__|_____|_____|__......etc
  |     |     |
  |     |     |
  M     |     |
  |   Thick   |
  L           |
              |
              A3
              |
              A1.5

Is told to be obtained by the (S) expression

plot(dataframe.df)

Using the same command in R, a different plot is obtained instead.
It is a multlple xy plot where the values of each variable are plotted
against
the values of the others.

Is there an equivalent command in R to obtain a plot similar to that showed
above?
Thanks for your time in reading this mine.

Ottorino-Luca Pantani, Universit? di Firenze
Dip. Scienza del Suolo e Nutrizione della Pianta


From b.otto at uke.uni-hamburg.de  Thu Sep 28 09:37:20 2006
From: b.otto at uke.uni-hamburg.de (Benjamin Otto)
Date: Thu, 28 Sep 2006 09:37:20 +0200
Subject: [R] line break in pie chart labels
Message-ID: <000301c6e2d0$eead7d60$336f12ac@matrix.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060928/d66ea8aa/attachment.pl 

From jim at bitwrit.com.au  Thu Sep 28 23:50:50 2006
From: jim at bitwrit.com.au (Jim Lemon)
Date: Thu, 28 Sep 2006 17:50:50 -0400
Subject: [R] line break in pie chart labels
In-Reply-To: <000301c6e2d0$eead7d60$336f12ac@matrix.com>
References: <000301c6e2d0$eead7d60$336f12ac@matrix.com>
Message-ID: <451C43BA.5020506@bitwrit.com.au>

Benjamin Otto wrote:
> Hi,
> 
>  
> 
> Given a pie chart with some really long labels can R insert line breaks
> here? Why I imagine is getting a label displayed as
> 
>  
> 
> 
>>some really extraordinary 
> 
> 
>>long and non-ending label
> 
> 
>  
> 
> Instead of
> 
>  
> 
> 
>>some really extraordinary long and non-ending label
> 
Hi Benjamin,

Try:

pie(1:4,labels=c("The first\nlong label",
  "The second, even\nlonger label",
  "The third, quite\nboring and\nstill longer\nlabel",
  "The fourth, final\nflagrantly\nverbose and\nlongest label\nof the lot"))

Jim


From vmuggeo at dssm.unipa.it  Thu Sep 28 09:59:25 2006
From: vmuggeo at dssm.unipa.it (vito muggeo)
Date: Thu, 28 Sep 2006 09:59:25 +0200
Subject: [R] Constrained OLS regression
In-Reply-To: <004901c6e227$27e419e0$0540210a@www.domain>
References: <0D9D1AC09BC016428D7597A1716AFF710D9940@Exukmb73.eur.nsroot.net>
	<004901c6e227$27e419e0$0540210a@www.domain>
Message-ID: <451B80DD.3080500@dssm.unipa.it>

In addition to Dimitris's approach, probably the following is more 
straightforward..(the idea is the same, but implementation is simpler; 
you do not need starting values, for instance..)

Given the linear predictor lp:
b0+b1X1+b2X2
as b2=1-b1 the lp becomes:
b0+b1X1+(1-b1)X2 => b0+b1(X1-X2)+offset(X2)

Hence for a generic GLM you can type
glm(y~1+I(x1-x2)+offset(x2))

Hope this helps,
vito


Dimitris Rizopoulos wrote:
> you could reparameterize, e.g.,
> 
> x1 <- runif(100, -4, 4)
> x2 <- runif(100, -4, 4)
> X <- cbind(1, x1 , x2)
> y <-  rnorm(100, as.vector(X %*% c(5, -3, 4)), 2)
> ######################
> 
> fn <- function(betas){
>     betas <- c(betas, 1 - betas[2])
>     crossprod(y - X %*% betas)[1, ]
> }
> 
> opt <- optim(c(5, -3), fn, method = "BFGS")
> c(opt$par, 1 - opt$par[2])
> 
> 
> I hope it helps.
> 
> Best,
> Dimitris
> 
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
> 
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/(0)16/336899
> Fax: +32/(0)16/337015
> Web: http://med.kuleuven.be/biostat/
>      http://www.student.kuleuven.be/~m0390867/dimitris.htm
> 
> 
> ----- Original Message ----- 
> From: "Mesomeris, Spyros [CIR]" <spyros.mesomeris at citigroup.com>
> To: <r-help at stat.math.ethz.ch>
> Sent: Wednesday, September 27, 2006 12:51 PM
> Subject: [R] Constrained OLS regression
> 
> 
> 
>>Hello R helpers,
>>
>>I am trying to do a linear OLS regression of y on two variables x1 
>>and
>>x2. I want to constrain the coefficients of x1 and x2 to sum up to 
>>1.
>>and therefore run a constrained OLS. Can anybody help with this? (I 
>>have
>>seen some answers to similar questions but it was not clear to me 
>>what I
>>need to do) - I have tried the lm function with offset but I must 
>>not
>>have used it properly.
>>
>>Thanks,
>>Spyros
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide 
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>>
> 
> 
> 
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
====================================
Vito M.R. Muggeo
Dip.to Sc Statist e Matem `Vianelli'
Universit? di Palermo
viale delle Scienze, edificio 13
90128 Palermo - ITALY
tel: 091 6626240
fax: 091 485726/485612


From p.dalgaard at biostat.ku.dk  Thu Sep 28 10:02:41 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 28 Sep 2006 10:02:41 +0200
Subject: [R] line break in pie chart labels
In-Reply-To: <451C43BA.5020506@bitwrit.com.au>
References: <000301c6e2d0$eead7d60$336f12ac@matrix.com>
	<451C43BA.5020506@bitwrit.com.au>
Message-ID: <x2ven8to4e.fsf@turmalin.kubism.ku.dk>

Jim Lemon <jim at bitwrit.com.au> writes:


> > Given a pie chart with some really long labels can R insert line breaks
> > here? Why I imagine is getting a label displayed as

> pie(1:4,labels=c("The first\nlong label",
>   "The second, even\nlonger label",
>   "The third, quite\nboring and\nstill longer\nlabel",
>   "The fourth, final\nflagrantly\nverbose and\nlongest label\nof the lot"))

In 2.4.0-to-be pie() is happier with plotmath labels, so atop() could
have be another solution. In 2.3.1

> pie(c(2,3),labels=expression(pi_1,pi_2))
Error in lab != "" : comparison is not allowed for expressions
In addition: Warning message:
is.na() applied to non-(list or vector) in: is.na(lab <- labels[i])

but in the prerelease versions, you get to eat you pie() and have your
pi's too.


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From p.dalgaard at biostat.ku.dk  Thu Sep 28 10:07:26 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 28 Sep 2006 10:07:26 +0200
Subject: [R] line break in pie chart labels
In-Reply-To: <x2ven8to4e.fsf@turmalin.kubism.ku.dk>
References: <000301c6e2d0$eead7d60$336f12ac@matrix.com>
	<451C43BA.5020506@bitwrit.com.au>
	<x2ven8to4e.fsf@turmalin.kubism.ku.dk>
Message-ID: <x2r6xwtnwh.fsf@turmalin.kubism.ku.dk>

Peter Dalgaard <p.dalgaard at biostat.ku.dk> writes:

> > pie(c(2,3),labels=expression(pi_1,pi_2))
> Error in lab != "" : comparison is not allowed for expressions
> In addition: Warning message:
> is.na() applied to non-(list or vector) in: is.na(lab <- labels[i])

Argh. The curse of the non-functional example: It should of course say
expression(pi[1], pi[2]). 

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From nelson1977 at gmail.com  Thu Sep 28 10:33:36 2006
From: nelson1977 at gmail.com (nelson -)
Date: Thu, 28 Sep 2006 10:33:36 +0200
Subject: [R] starting point for non linear fitting
Message-ID: <f44750160609280133y57a9347gc6d104b1ef1710bc@mail.gmail.com>

Hi all!
 i'm trying to use nls for fitting my data. I wrote this code to find
some minimum, but it fails, returning 0 every time..... and i can't
figure out the problem... any advice?

grid <- expand.grid(A0 = seq(1000,10000,1000), A1 = seq(0,2,0.1))
exp.approx <- function(x,A0,A1) {
  A0 * exp(- x*A1)
}
ss <- function(p) {
                   sum((durata.h.freq - exp.approx(durata.h.x,p[0],p[1]))^2)}

idx <- which.min(apply(grid,1,ss))



idx is always 1,1... and that's not true...


From Wolfgang.Viechtbauer at STAT.unimaas.nl  Thu Sep 28 10:50:21 2006
From: Wolfgang.Viechtbauer at STAT.unimaas.nl (Viechtbauer Wolfgang (STAT))
Date: Thu, 28 Sep 2006 10:50:21 +0200
Subject: [R] Testing the equality  of correlations
In-Reply-To: <52A8091888A23F47A013223014B6E9FE074B28C3@03-CSEXCH.uopnet.plymouth.ac.uk>
Message-ID: <329A68716B57D54E8D39FD3F8A4A84DF057D5BC2@um-mail0136.unimaas.nl>

It's more complicated than that, since Phi(X1,X2), Phi(X1,X3), and Phi(X1,X4) are dependent. Take a look at:

Olkin, I., & Finn, J. D. (1990). Testing correlated correlations. Psychological Bulletin, 108(2), 330-333.

and

Meng, X., Rosenthal, R., & Rubin, D. B. (1992). Comparing correlated correlation coefficients. Psychological Bulletin, 111(1), 172-175.

You will probably have to implement these tests yourself. 

Best,

-- 
Wolfgang Viechtbauer 
?Department of Methodology and Statistics 
?University of Maastricht, The Netherlands 
?http://www.wvbauer.com/ 


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of Paul Hewson
> Sent: Wednesday, September 27, 2006 17:40
> To: Marc Bernard; r-help at stat.math.ethz.ch
> Subject: Re: [R] Testing the equality of correlations
> 
> Off the top my head (i.e. this could all be horribly wrong), I think
> Anderson gave an asymptotic version for such a test, whereby under the
> null hypothesis, the difference between Fisher's z for each sample, z1 -
> z2, is normal with zero mean.   

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Marc Bernard
> Sent: 27 September 2006 14:42
> To: r-help at stat.math.ethz.ch
> Subject: [R] Testing the equality of correlations
> 
> Dear All,
> 
>   I wonder if there is any  implemented statistical test in R to test
> the  equality between many correlations. As an example, let X1, X2, X3
> X4 be four random  variables.  let
>   Phi(X1,X2) , Phi(X1,X3) and Phi(X1,X4) be the corresponding
> correlations.
>   How to test Phi(X1,X2) = Phi(X1,X3) = P(X1,X4)?
> 
>   Many thanks in advance,
> 
>   Bernard


From ccleland at optonline.net  Thu Sep 28 11:20:30 2006
From: ccleland at optonline.net (Chuck Cleland)
Date: Thu, 28 Sep 2006 05:20:30 -0400
Subject: [R] I: differences between R and S (reproducing a plot from a
 book )
In-Reply-To: <DNEELNJCLGBOLHCFLMHBIEHFEBAA.OLPantani@unifi.it>
References: <DNEELNJCLGBOLHCFLMHBIEHFEBAA.OLPantani@unifi.it>
Message-ID: <451B93DE.10200@optonline.net>

8rino-Luca Pantani wrote:
> Dear R-Users,
> I'm currently studying the book
> Statistical model in S by Chambers J.M and Hastie T.J..
> 
> At page n 3 there's a plot showing the means of a variable at each of the
> levels of the factors of an experiment.
> I hope to be able to reproduce it here by ASCII art.
> 
>               B6
>   S           |
>   |   Thin    |
>   |     |     |
>   |     |     B3
> __|_____|_____|__......etc
>   |     |     |
>   |     |     |
>   M     |     |
>   |   Thick   |
>   L           |
>               |
>               A3
>               |
>               A1.5
> 
> Is told to be obtained by the (S) expression
> 
> plot(dataframe.df)
> 
> Using the same command in R, a different plot is obtained instead.
> It is a multlple xy plot where the values of each variable are plotted
> against
> the values of the others.
> 
> Is there an equivalent command in R to obtain a plot similar to that showed
> above?
> Thanks for your time in reading this mine.

Have a look at plot.design().

> Ottorino-Luca Pantani, Universit? di Firenze
> Dip. Scienza del Suolo e Nutrizione della Pianta
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From b.otto at uke.uni-hamburg.de  Thu Sep 28 11:34:15 2006
From: b.otto at uke.uni-hamburg.de (Benjamin Otto)
Date: Thu, 28 Sep 2006 11:34:15 +0200
Subject: [R] line break in pie chart labels
In-Reply-To: <451C43BA.5020506@bitwrit.com.au>
Message-ID: <000801c6e2e1$45040c50$336f12ac@matrix.com>

Hi Peter, Jim,

Thanks a lot, that solved the problem.

Regards
Benjamin


From p.dalgaard at biostat.ku.dk  Thu Sep 28 11:47:54 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 28 Sep 2006 11:47:54 +0200
Subject: [R] starting point for non linear fitting
In-Reply-To: <f44750160609280133y57a9347gc6d104b1ef1710bc@mail.gmail.com>
References: <f44750160609280133y57a9347gc6d104b1ef1710bc@mail.gmail.com>
Message-ID: <x2y7s4jp9x.fsf@viggo.kubism.ku.dk>

"nelson -" <nelson1977 at gmail.com> writes:

> Hi all!
>  i'm trying to use nls for fitting my data. I wrote this code to find
> some minimum, but it fails, returning 0 every time..... and i can't
> figure out the problem... any advice?
> 
> grid <- expand.grid(A0 = seq(1000,10000,1000), A1 = seq(0,2,0.1))
> exp.approx <- function(x,A0,A1) {
>   A0 * exp(- x*A1)
> }
> ss <- function(p) {
>                    sum((durata.h.freq - exp.approx(durata.h.x,p[0],p[1]))^2)}
> 
> idx <- which.min(apply(grid,1,ss))
> 
> 
> 
> idx is always 1,1... and that's not true...

Array indexing in R is 1-based, so p[0] is asking for trouble....

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From ggrothendieck at gmail.com  Thu Sep 28 12:23:19 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 28 Sep 2006 06:23:19 -0400
Subject: [R] how to retain time zone when doing c(POSIXct)
In-Reply-To: <877izpax1l.fsf@patagonia.sebmags.homelinux.org>
References: <877izpax1l.fsf@patagonia.sebmags.homelinux.org>
Message-ID: <971536df0609280323h20ebdbfbge111d80098d8ad3d@mail.gmail.com>

Here is one way that does not require explicit assignment of tzone:

rbind(data.frame(x=xx), data.frame(x=xx[1] - 60), data.frame(x=xx[2] + 60))[[1]]


With xx from the post we get:

> rbind(data.frame(x=xx), data.frame(x=xx[1] - 60), data.frame(x=xx[2] + 60))[[1]]
[1] "2006-09-26 12:00:00 GMT" "2006-09-26 13:00:00 GMT"
[3] "2006-09-26 11:59:00 GMT" "2006-09-26 13:01:00 GMT"



On 9/27/06, Sebastian P. Luque <spluque at gmail.com> wrote:
> Hello,
>
> What is the best way to concatenate POSIXct objects keeping the time zone
> attribute in a program?  For example:
>
>
> R> xx <- as.POSIXct(strptime(c("2006-09-26 12:00:00", "2006-09-26 13:00:00"),
> +                           format="%Y-%m-%d %H:%M:%S"), tz="GMT")
> R> xx
> [1] "2006-09-26 12:00:00 GMT" "2006-09-26 13:00:00 GMT"
>
>
> but, as ?c.POSIXct explains:
>
>
> R> c(xx, xx[1] - 60, xx[2] + 60)
> [1] "2006-09-26 07:00:00 CDT" "2006-09-26 08:00:00 CDT"
> [3] "2006-09-26 06:59:00 CDT" "2006-09-26 08:01:00 CDT"
>
>
> Is there something better/safer than simply setting the "tzone" attribute
> of the new object?
>
>
> Cheers,
>
> --
> Seb
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From j.logsdon at quantex-research.com  Thu Sep 28 12:40:10 2006
From: j.logsdon at quantex-research.com (John Logsdon)
Date: Thu, 28 Sep 2006 11:40:10 +0100 (GMT)
Subject: [R] Perspective axes
Message-ID: <Pine.LNX.4.10.10609281110580.31381-100000@mercury.quantex>

Is there a way to get the axes labels for a persp() plot to show the
actual values employed?  ticktype='detailed' only shows a scale from 0 to
1.  My values are (for example) y in 0.2-0.7, x in 450-560 and I would
like to suppress the z labels.  

How can I get the x and y values to appear on the plot?

R-2.3.1, Windows XP Home.

Best wishes

John

John Logsdon                               "Try to make things as simple
Quantex Research Ltd, Manchester UK         as possible but not simpler"
j.logsdon at quantex-research.com              a.einstein at relativity.org
+44(0)161 445 4951/G:+44(0)7717758675       www.quantex-research.com


From p_connolly at ihug.co.nz  Thu Sep 28 12:51:51 2006
From: p_connolly at ihug.co.nz (Patrick Connolly)
Date: Thu, 28 Sep 2006 22:51:51 +1200
Subject: [R] Using assign() function as I did in Splus
Message-ID: <20060928105151.GE4295@ihug.co.nz>

Years ago, when I used Splus, I used to do this sort of thing:

assign("data", data, frame = 1)

This was so that if the object data was used in, say a call to glm,
and I wished to do a summary of the glm object, data wasn't otherwise
visible.

Since I moved to R, the lexical scoping has virtually eliminated the
need to do that.  However, I've been trying to use the package samm 

http://www2.dpi.qld.gov.au/fieldcrops/14715.html

It seems to me to be developed for Splus and ported to R (and then
mostly only for Windows).  There is a binary version for Linux, but
it's for R-1.9.x which means it's virtually useless.  There's a huge
number of functions in the package, and it's not easy to work out
what's producing what error.very

I suspect the problem isn't to do with using Windows instead of Linux
which I'm used to.  The only way I can make the required objects
visible is to assign them to pos = 1 which means they're debris to be
removed after the function finishes, and not a very elegant approach.

There's probably something tricky to do with environments I'm not
understanding here.  If I could replicate the frame = 1 idea, I'd feel
less of a klutz.  What would be a better approach?

TIA

-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}          		 Great minds discuss ideas    
 _( Y )_  	  	        Middle minds discuss events 
(:_~*~_:) 	       		 Small minds discuss people  
 (_)-(_)  	                           ..... Anon
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From murdoch at stats.uwo.ca  Thu Sep 28 12:56:14 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 28 Sep 2006 06:56:14 -0400
Subject: [R] Perspective axes
In-Reply-To: <Pine.LNX.4.10.10609281110580.31381-100000@mercury.quantex>
References: <Pine.LNX.4.10.10609281110580.31381-100000@mercury.quantex>
Message-ID: <451BAA4E.1020405@stats.uwo.ca>

On 9/28/2006 6:40 AM, John Logsdon wrote:
> Is there a way to get the axes labels for a persp() plot to show the
> actual values employed?  ticktype='detailed' only shows a scale from 0 to
> 1.  My values are (for example) y in 0.2-0.7, x in 450-560 and I would
> like to suppress the z labels.  
> 
> How can I get the x and y values to appear on the plot?

You could try persp3d in the rgl package; it has more flexibility about 
labels (but not as much as most R graphics:  you can't change fonts, for 
instance).

Duncan Murdoch


From john.gavin at ubs.com  Thu Sep 28 13:42:25 2006
From: john.gavin at ubs.com (john.gavin at ubs.com)
Date: Thu, 28 Sep 2006 12:42:25 +0100
Subject: [R] calling R from within Java, using jri
Message-ID: <182544D7A3144B42994EEA5662C54E0103D0AAFF@NLDNC105PEX1.ubsw.net>

Hi,

I want to call R from within Java, using jri as per 
http://www.rosuda.org/software/jri/
So I am following the instructions in the README file for JRI 0.2-4.

I have run 'sh configure.win' and 'make' and they seemed to be
successful.
(See below for the output from make, for example.)
But when I try 'run.bat rtest' (with and without R command line
arguments)
the output that I get is 

'c:\temp\r\jri>C:/PROGRA~1/Java/JDK15~1.0_0/bin/java -cp
.;examples;src/JRI.jar rtest $* 
Creating Rengine (with arguments)'

That suggests that it gets to line 60 in rtest.java then stops on line
61, 
without producing an error. Line 61 is

Rengine re=new Rengine(args, false, null);

so it seems that I am not able to construct an Rengine.
Can someone suggest what I might try next to track down the problem,
please?

I am running Java JDK 1.5.0 and R 2.3.1 on Windows NT.

Regards,

John.

John Gavin <john.gavin at ubs.com>,
Commodities, FIRC,
UBS Investment Bank, 2nd floor, 
100 Liverpool St., London EC2M 2RH, UK.
Phone +44 (0) 207 567 4289

=== output from running make in the jri folder ====

c:\temp\r\jri>make 
make -C src JRI.jar 
make[1]: Entering directory `/cygdrive/c/temp/r/jri/src' 
gcc -c -o Rengine.o Rengine.c -DWin32 -D_JNI_IMPLEMENTATION_
-IC:/PROGRA~1/Java/JDK15~1.0_0/include
-IC:/PROGRA~1/Java/JDK15~1.0_0/include/win32 -IC:/etc/R/R-2.3.1/include 
gcc -c -o jri.o jri.c -DWin32 -D_JNI_IMPLEMENTATION_
-IC:/PROGRA~1/Java/JDK15~1.0_0/include
-IC:/PROGRA~1/Java/JDK15~1.0_0/include/win32 -IC:/etc/R/R-2.3.1/include 
gcc -c -o Rcallbacks.o Rcallbacks.c -DWin32 -D_JNI_IMPLEMENTATION_
-IC:/PROGRA~1/Java/JDK15~1.0_0/include
-IC:/PROGRA~1/Java/JDK15~1.0_0/include/win32 -IC:/etc/R/R-2.3.1/include 
gcc -c -o Rinit.o Rinit.c -DWin32 -D_JNI_IMPLEMENTATION_
-IC:/etc/R/R-2.3.1/include 
Rinit.c: In function `initR': 
Rinit.c:265: warning: assignment from incompatible pointer type 
gcc -c -o globals.o globals.c -DWin32 -D_JNI_IMPLEMENTATION_
-IC:/PROGRA~1/Java/JDK15~1.0_0/include
-IC:/PROGRA~1/Java/JDK15~1.0_0/include/win32 
gcc -o jri.dll Rengine.o jri.o Rcallbacks.o Rinit.o globals.o
win32/libjvm.dll.a  -shared -Lwin32 -ljvm
-LC:/etc/R/R-2.3.1/src/gnuwin32 -LC:/etc/R/R-2.3.1/bin -lR -Wl,--kill-at

Info: resolving _R_CStackLimit by linking to __imp__R_CStackLimit
(auto-import) 
C:/PROGRA~1/Java/JDK15~1.0_0/bin/javac -d . ../Mutex.java ../REXP.java
../RMainLoopCallbacks.java ../Rengine.java 
C:/PROGRA~1/Java/JDK15~1.0_0/bin/jar fc JRI.jar org jri.dll 
make[1]: Leaving directory `/cygdrive/c/temp/r/jri/src' 
rm -f jri.dll 
cp src/jri.dll jri.dll 
C:/PROGRA~1/Java/JDK15~1.0_0/bin/javac -classpath src/JRI.jar -d
examples examples/rtest.java 
Note: examples/rtest.java uses or overrides a deprecated API. 
Note: Recompile with -Xlint:deprecation for details. 
C:/PROGRA~1/Java/JDK15~1.0_0/bin/javac -classpath src/JRI.jar -d
examples examples/rtest2.java 
Note: examples/rtest2.java uses or overrides a deprecated API. 
Note: Recompile with -Xlint:deprecation for details. 
echo "set PATH=%PATH%;C:/etc/R/R-2.3.1\\bin;C:/etc/R/R-2.3.1\\lib" >
run.bat 
echo "C:/PROGRA~1/Java/JDK15~1.0_0/bin/java -cp .;examples;src/JRI.jar
rtest \$*" >> run.bat 
This communication is issued by UBS AG and/or affiliates to
institutional investors; it is not for private persons. This is a
product of a sales or trading desk and not the Research Dept.
Opinions expressed may differ from those of other divisions of UBS,
including Research.  UBS may trade as principal in instruments
identified herein and may accumulate/have accumulated a long or short
position in instruments or derivatives thereof.  UBS has policies
designed to negate conflicts of interest.  This e-mail is not an
official confirmation of terms and unless stated, is not a
recommendation, offer or solicitation to buy or sell.  Any prices or
quotations contained herein are indicative only.  Communications
may be monitored.

 ? 2006 UBS.  All rights reserved. 
Intended for recipient only and not for further distribution without
the consent of UBS.


From f.harrell at vanderbilt.edu  Thu Sep 28 14:07:42 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 28 Sep 2006 07:07:42 -0500
Subject: [R] Best use of LaTeX listings package for pretty printing R
 code
In-Reply-To: <3A07FF27-463F-4868-92BF-F347A00F0008@virginia.edu>
References: <4517DA39.4070102@vanderbilt.edu>
	<3A07FF27-463F-4868-92BF-F347A00F0008@virginia.edu>
Message-ID: <451BBB0E.3070402@vanderbilt.edu>

Michael Kubovy wrote:
> I tried to use Frank's formatting improvements to compile 
> Sweave-produced .tex, but it doesn't seem to make a difference. Advice?

I've been hoping that Fritz would put in hooks for pretty printing of 
code but I don't think it's there yet.  If you tried any special hooks 
and that didn't work please let us know.

Frank

> 
> On Sep 25, 2006, at 9:31 AM, Frank E Harrell Jr wrote:
> 
>> This is what I have been using.  Does anyone have a better way?  In
>> particular I would like to see letters in comment strings not stretched
>> so much.  Thanks -Frank
>>
>> \documentclass{article}
>> \usepackage{listings,relsize}
>> \lstloadlanguages{R}
>> \newcommand{\lil}[1]{\lstinline|#1|}
>>
>> \begin{document}
>> \lstset{language=R,basicstyle=\smaller,commentstyle=\rmfamily\smaller,
>>   showstringspaces=false,%
>>   xleftmargin=4ex,literate={<-}{{$\leftarrow$}}1 {~}{{$\sim$}}1}
>> \lstset{escapeinside={(*}{*)}}   % for (*\ref{ }*) inside lstlistings (S
>> code)
>> \begin{lstlisting}
>> a <- b   # this is a test line
>> if(i==3) {  # another line, for y^2
>>   y <- 3^3
>>   z <- 'this string'
>>   qqcat <- y ~ pol(x,2)
>> } else y <- 4
>> \end{lstlisting}
>> That was \lstinline|x <- 22| \lil{q <- 'cat'}.
>> \end{document}
> 
> _____________________________
> Professor Michael Kubovy
> University of Virginia
> Department of Psychology
> USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
> Parcels:    Room 102        Gilmer Hall
>         McCormick Road    Charlottesville, VA 22903
> Office:    B011    +1-434-982-4729
> Lab:        B019    +1-434-982-4751
> Fax:        +1-434-982-4766
> WWW:    http://www.people.virginia.edu/~mk9y/
>


From jjbarn at btinternet.com  Thu Sep 28 14:14:31 2006
From: jjbarn at btinternet.com (John James)
Date: Thu, 28 Sep 2006 12:14:31 +0000 (GMT)
Subject: [R] Adding graphics to R-help Files
Message-ID: <20060928121431.48085.qmail@web86211.mail.ird.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060928/5fdded9c/attachment.pl 

From assampryseley at yahoo.com  Thu Sep 28 15:01:02 2006
From: assampryseley at yahoo.com (Pryseley Assam)
Date: Thu, 28 Sep 2006 06:01:02 -0700 (PDT)
Subject: [R] Plackett-Dale Model in R
In-Reply-To: <mailman.9.1148896804.12396.r-help@stat.math.ethz.ch>
Message-ID: <20060928130102.80637.qmail@web37105.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060928/cdd562dd/attachment.pl 

From gregor.gorjanc at bfro.uni-lj.si  Thu Sep 28 15:07:41 2006
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Thu, 28 Sep 2006 13:07:41 +0000 (UTC)
Subject: [R] Adding graphics to R-help Files
References: <20060928121431.48085.qmail@web86211.mail.ird.yahoo.com>
Message-ID: <loom.20060928T150605-993@post.gmane.org>

John James <jjbarn <at> btinternet.com> writes:

> 
> Is there any way to add graphics (e.g. thumbnails in examples) to Rd files?
> 
> A picture is worth a thousand words!

No. You can provide code example that works. That is quite close. You might also
look at 

http://bg9.imslab.co.jp/Rhelp/
http://addictedtor.free.fr/graphiques/

Gregor


From f.harrell at vanderbilt.edu  Thu Sep 28 15:20:03 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 28 Sep 2006 08:20:03 -0500
Subject: [R] Looking for info on the "Regression Modeling Strategies in
 R"	course in DC area
In-Reply-To: <20060815153350.1632.qmail@web57205.mail.re3.yahoo.com>
References: <20060815153350.1632.qmail@web57205.mail.re3.yahoo.com>
Message-ID: <451BCC03.8020608@vanderbilt.edu>

Tim McDonald wrote:
> Hello list,
>    
>   A colleague of mine mentioned a great course on  "Regression Modeling Strategies in R". Anyone knows if this course is offered as public course in DC area?
>    
>   Thanks a bunch - TM
> 
>  		
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

You must have missed the announcement on r-help.  The course starts at 
10am Eastern Time in DC, i.e., in 41 minutes.  XLSolutions web site has 
details.  It's a 2-day course.

Frank Harrell


From maechler at stat.math.ethz.ch  Thu Sep 28 15:23:50 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 28 Sep 2006 15:23:50 +0200
Subject: [R] Perspective axes
In-Reply-To: <Pine.LNX.4.10.10609281110580.31381-100000@mercury.quantex>
References: <Pine.LNX.4.10.10609281110580.31381-100000@mercury.quantex>
Message-ID: <17691.52454.503069.73413@stat.math.ethz.ch>

>>>>> "John" == John Logsdon <j.logsdon at quantex-research.com>
>>>>>     on Thu, 28 Sep 2006 11:40:10 +0100 (GMT) writes:

    John> Is there a way to get the axes labels for a persp()
    John> plot to show the actual values employed?
    John> ticktype='detailed' only shows a scale from 0 to 1.

that's not true.

    John> My values are (for example) y in 0.2-0.7, x in 450-560
    John> and I would like to suppress the z labels.

    John> How can I get the x and y values to appear on the
    John> plot?

and what does the posting guide say ???

The 3rd (I think) plot   in    demo(persp)
shows axes where none of the value ranges are  "from 0 to 1".

Martin

 [.........]

  >>> PLEASE do read the posting guide
  >>> http://www.R-project.org/posting-guide.html 
  >>> and provide commented, minimal, self-contained,
  >>> reproducible code.

Please do!


From racinej at mcmaster.ca  Thu Sep 28 15:52:50 2006
From: racinej at mcmaster.ca (Jeffrey Racine)
Date: Thu, 28 Sep 2006 09:52:50 -0400
Subject: [R] cat(), Rgui, and support for carriage return \r...
In-Reply-To: <Pine.LNX.4.64.0603281658200.1639@gannet.stats.ox.ac.uk>
References: <1142606674.824.28.camel@localhost>
	<441C61E4.2000507@stats.uwo.ca> <441D878C.9050003@stats.uwo.ca>
	<Pine.LNX.4.64.0603281658200.1639@gannet.stats.ox.ac.uk>
Message-ID: <1159451570.91224.9.camel@pc-racine1.mcmaster.ca>

Dear Brian, Duncan, et al.

A followup and quick question if I may regarding support for \r. First,
the R terminal and the windows console now support \r properly
(thanks!). However, I just got my hands on a macbook, installed the
latest R and R.app gui, and it appears that \r is not supported in the
OS X gui build on the R site. Would it be possible to add this support?

Thank you for your time and contributions to the R community.

-- Jeff

On Tue, 2006-03-28 at 17:00 +0100, Prof Brian Ripley wrote:
> Rgui now supports \r in the same way as rterm.

-- 
Professor J. S. Racine         Phone:  (905) 525 9140 x 23825
Department of Economics        FAX:    (905) 521-8232
McMaster University            e-mail: racinej at mcmaster.ca
1280 Main St. W.,Hamilton,     URL:
http://www.economics.mcmaster.ca/racine/
Ontario, Canada. L8S 4M4

`The generation of random numbers is too important to be left to chance'


From gchappi at gmail.com  Thu Sep 28 15:57:08 2006
From: gchappi at gmail.com (Hans-Peter)
Date: Thu, 28 Sep 2006 15:57:08 +0200
Subject: [R] drop
Message-ID: <47fce0650609280657x5bd7f29bxae45e15dc723b80d@mail.gmail.com>

Hello,

What's the best/recommended way to drop the dimension of a data.frame?
Probably "test[,,]" but I am not sure. I would prefer an explicit
function, but drop doesn't work with frames and I didn't find
something in the helpfiles/mailarchive.

Example:

test <- data.frame( x = 1:7 )

str( drop( test ) )                     # still a data.frame

str( test[,, drop = TRUE] )             # ok
str( test[,,] )                         # ok <== IS THIS THE WAY TO DO IT?
str( as.integer( as.matrix( test ) ) )  # ok

str( drop( as.matrix( test ) ) )        # ~ok, named integer
test2 <- test
attributes( test2 ) <- NULL
str( drop( as.matrix( test2 ) ) )       # ok


Thanks and best regards,
Hans-Peter

PS: how could I lookup the code for subsetting a data.frame. I suppose
that it is in "data.frame.["


From ethan.johnsons at gmail.com  Thu Sep 28 15:57:23 2006
From: ethan.johnsons at gmail.com (Ethan Johnsons)
Date: Thu, 28 Sep 2006 09:57:23 -0400
Subject: [R] ppois
Message-ID: <5cd96f050609280657o118f4af6u4bf063bfaf712fe@mail.gmail.com>

33 died overa 10-year period from COPD, whereas only 24 such deaths
could be expected based onstatewide mortality rates.

I came up with ppois(q = 33, lambda = 24, lower.tail = FALSE),
but it seems it is ppois(q = 32, lambda = 24, lower.tail = FALSE).

I am confused with the q value.  Why is it 32 instead 33?

thx much


From ripley at stats.ox.ac.uk  Thu Sep 28 15:58:13 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 28 Sep 2006 14:58:13 +0100 (BST)
Subject: [R] cat(), Rgui, and support for carriage return \r...
In-Reply-To: <1159451570.91224.9.camel@pc-racine1.mcmaster.ca>
References: <1142606674.824.28.camel@localhost> <441C61E4.2000507@stats.uwo.ca>
	<441D878C.9050003@stats.uwo.ca>
	<Pine.LNX.4.64.0603281658200.1639@gannet.stats.ox.ac.uk>
	<1159451570.91224.9.camel@pc-racine1.mcmaster.ca>
Message-ID: <Pine.LNX.4.64.0609281457220.13040@gannet.stats.ox.ac.uk>

On Thu, 28 Sep 2006, Jeffrey Racine wrote:

> Dear Brian, Duncan, et al.
>
> A followup and quick question if I may regarding support for \r. First,
> the R terminal and the windows console now support \r properly
> (thanks!). However, I just got my hands on a macbook, installed the
> latest R and R.app gui, and it appears that \r is not supported in the
> OS X gui build on the R site. Would it be possible to add this support?

You need to ask the MacGUI maintainers (Stefano and Simon)

>
> Thank you for your time and contributions to the R community.
>
> -- Jeff
>
> On Tue, 2006-03-28 at 17:00 +0100, Prof Brian Ripley wrote:
>> Rgui now supports \r in the same way as rterm.
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Mark.Leeds at morganstanley.com  Thu Sep 28 16:07:37 2006
From: Mark.Leeds at morganstanley.com (Leeds, Mark (IED))
Date: Thu, 28 Sep 2006 10:07:37 -0400
Subject: [R] ppois
In-Reply-To: <5cd96f050609280657o118f4af6u4bf063bfaf712fe@mail.gmail.com>
Message-ID: <D3AEEDA31E57474B840BEBC25A8A8344934260@NYWEXMB23.msad.ms.com>

my guess is that ppois calculates greater than so ( since we are dealing
in a  discrete world )  greater than or equal to 33 is the same as
greater than 32.
But I am not familiar with ppois so you should check that this is what
is happening.

I think someone else already told you that you shouldn't use this site
for homework but I guess this could be not related to homework so I
answered ?




-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ethan Johnsons
Sent: Thursday, September 28, 2006 9:57 AM
To: r-help at stat.math.ethz.ch
Subject: [R] ppois

33 died overa 10-year period from COPD, whereas only 24 such deaths
could be expected based onstatewide mortality rates.

I came up with ppois(q = 33, lambda = 24, lower.tail = FALSE), but it
seems it is ppois(q = 32, lambda = 24, lower.tail = FALSE).

I am confused with the q value.  Why is it 32 instead 33?

thx much

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.
--------------------------------------------------------

This is not an offer (or solicitation of an offer) to buy/se...{{dropped}}


From im at edc.pitt.edu  Thu Sep 28 16:47:45 2006
From: im at edc.pitt.edu (Im, Kelly)
Date: Thu, 28 Sep 2006 10:47:45 -0400
Subject: [R] MSM modeling and transition rates in R
Message-ID: <E3FBCB35FBB8294A9B480A3B2DB7335689E4DA@boa.edc3.pitt.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060928/3907b997/attachment.pl 

From cborges at iqm.unicamp.br  Thu Sep 28 17:01:34 2006
From: cborges at iqm.unicamp.br (Cleber N. Borges)
Date: Thu, 28 Sep 2006 12:01:34 -0300
Subject: [R] get index of elements in vector
Message-ID: <451BE3CE.2060201@iqm.unicamp.br>


Hello all


Is There a fuction that return a index of a element in vector?
like this semantic example:

vector = c( 100, 200, 300 )

getINDEX( vector, value = 200 )

Thanks in advance for your attention.


Cleber Borges


From dimitris.rizopoulos at med.kuleuven.be  Thu Sep 28 17:08:59 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Thu, 28 Sep 2006 17:08:59 +0200
Subject: [R] get index of elements in vector
References: <451BE3CE.2060201@iqm.unicamp.br>
Message-ID: <017d01c6e310$06874cb0$0540210a@www.domain>

look at ?which(), e.g.,

which(vector == 200)


Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Cleber N. Borges" <cborges at iqm.unicamp.br>
To: "R Help" <r-help at stat.math.ethz.ch>
Sent: Thursday, September 28, 2006 5:01 PM
Subject: [R] get index of elements in vector


>
> Hello all
>
>
> Is There a fuction that return a index of a element in vector?
> like this semantic example:
>
> vector = c( 100, 200, 300 )
>
> getINDEX( vector, value = 200 )
>
> Thanks in advance for your attention.
>
>
> Cleber Borges
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From mschwartz at mn.rr.com  Thu Sep 28 17:12:38 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Thu, 28 Sep 2006 10:12:38 -0500
Subject: [R] recode problem -  unexplained values
In-Reply-To: <10151.144.131.210.250.1159410427.squirrel@144.131.210.250>
References: <mailman.9.1159351203.25554.r-help@stat.math.ethz.ch>
	<10151.144.131.210.250.1159410427.squirrel@144.131.210.250>
Message-ID: <1159456358.3881.9.camel@localhost.localdomain>

On Thu, 2006-09-28 at 12:27 +1000, bgreen at dyson.brisnet.org.au wrote:
> I am hoping for some advice regarding the difficulties I have been having
> recoding variables which are contained in a csv file.  Table 1 (below) 
> shows there are two types of blanks - as reported in the first two
> columns. I am using windows XP & the latets version of R.
> 
> When blanks cells are replaced with a value of n using syntax: > affect
> [affect==""] <- "n"
> there are still 3 blank values (Table 2).   When as.numeric is applied,
> this also causes problems because values of 2,3 & 4 are generated rather
> than just 1 & 2.
> 
> TABLE 1
> 
> table(group,actions)
>      actions
> group           n   y
>     1 100   2   0   3
>     2  30   1   1   0
>     3  24   0   0   0
> 
> 
> 
> TABLE 2
> 
> >  table(group,actions)
>      actions
> group           n   y
>     1   0   2 100   3
>     2   0   1  31   0
>     3   0   0  24   0
> 
> 
> Below is another example - for some reason there are 2 types of 'aobh'
> values.
> 
> 
> > table(group, type)
>      type
> group aobh aobh   gbh   m  uw
>     1  104      1   0   0   0
>     2    0      0  15   0  17
>     3    0      0   0  24   0
> 
> 
> Any assistance is much appreciated,
> 
> 
> Bob Green

Bob,

A quick heads up, which is the presumption that "aobh" and "aobh  " are
different values simply as a consequence of leading/trailing spaces in
the source data file within the delimited fields. This is also the
likely reason for there being multiple missing/blank values in your
imported data set.

Presuming that you used one of the read.table() family functions (ie.
read.csv() ), take note of the 'strip.white' argument in ?read.table,
which defaults to FALSE. If you change it to TRUE, the function will
strip leading and trailing blanks, likely resolving this issue.

HTH,

Marc Schwartz


From gchappi at gmail.com  Thu Sep 28 17:13:14 2006
From: gchappi at gmail.com (Hans-Peter)
Date: Thu, 28 Sep 2006 17:13:14 +0200
Subject: [R] get index of elements in vector
In-Reply-To: <451BE3CE.2060201@iqm.unicamp.br>
References: <451BE3CE.2060201@iqm.unicamp.br>
Message-ID: <47fce0650609280813p4dfc44daxa50bf58d841c1955@mail.gmail.com>

2006/9/28, Cleber N. Borges <cborges at iqm.unicamp.br>:
> Is There a fuction that return a index of a element in vector?
> like this semantic example:
> vector = c( 100, 200, 300 )
> getINDEX( vector, value = 200 )

which( vector == 200 )

-- 
Regards,
Hans-Peter


From gchappi at gmail.com  Thu Sep 28 17:39:43 2006
From: gchappi at gmail.com (Hans-Peter)
Date: Thu, 28 Sep 2006 17:39:43 +0200
Subject: [R] fractional part
Message-ID: <47fce0650609280839o194298d3tf358d9598c125a8e@mail.gmail.com>

Hi all,

is there a function to get the fractional part of a number?

-- 
Regards,
Hans-Peter


From ssj1364 at gmail.com  Thu Sep 28 17:44:28 2006
From: ssj1364 at gmail.com (Spencer Jones)
Date: Thu, 28 Sep 2006 09:44:28 -0600
Subject: [R]  safe prediction from lm
Message-ID: <1c6126db0609280844m78feb917sddd4dd30165c994e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060928/49b9877e/attachment.pl 

From edd at debian.org  Thu Sep 28 17:54:07 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 28 Sep 2006 10:54:07 -0500
Subject: [R] Comparing entire row sets at once efficiently
Message-ID: <17691.61471.872466.824208@basebud.nulle.part>


Dear useRs,

I am having a hard time coming up with a nice and efficient solution to
a problem on entires matrices or data.frames. In spirit, this is similar to
what setdiff() and setequal() do, but I need it in more dimensions.

Here's a brief description.

  * given a set of factors or sequences, expand.grid() gives me the set
    of permutations in a data.frame; 

    in my case all arguments are numeric so I could convert the data frame to
    a matrix

    let's call this one Candidates

  * I have a second matrix (or data frame) to compare to; this second 
    set may be a subset of the first, or a superset but it guaranted to
    contain the same columns

    let's call this one Comparison

  * I want know which rows in Candidates are not yet in Comparison.

A toy example:

> Comparison <- matrix(1:30, ncol=5)
> Candidates <- Comparison[c(2,4), ]
> checkRow <- function(r, M) { any( (r[1] == M[,1]) & (r[2] == M[,2]) & (r[3] == M[,3]) & (r[4] == M[,4]) ) }
> checkRow( Candidates[1,], Comparison)
[1] TRUE
> falseRow <- Candidates[1,] 
> falseRow[2] <- 42
> checkRow( falseRow, Comparison)
[1] FALSE
> 

The checkRow function works but is a) klunky, b) hardcodes the dimension and
c) works only on one row at a time.

There must be better ways, at least for a) and b).  What am I missing?  

Feel free to reply off-list and I'd gladly summarize back to the list. If you
don't want your reply (or email) summarized back, please indicate.

Thanks, Dirk



-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From sundar.dorai-raj at pdf.com  Thu Sep 28 17:55:02 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 28 Sep 2006 10:55:02 -0500
Subject: [R] safe prediction from lm
In-Reply-To: <1c6126db0609280844m78feb917sddd4dd30165c994e@mail.gmail.com>
References: <1c6126db0609280844m78feb917sddd4dd30165c994e@mail.gmail.com>
Message-ID: <451BF056.7030601@pdf.com>



Spencer Jones said the following on 9/28/2006 10:44 AM:
> I am fitting a regression model with a bs term and then making predictions
> based on the model. According to some info on the internet at
> http://www.stat.auckland.ac.nz/~yee/smartpred/DummiesGuide.txt
> 
> there are some problems with using predict.lm when you have a model with
> terms such as bs,ns,or poly. However when I used one of the examples they
> said would illustrate the problems I get virtually the same results using
> the standard predict function and "safe prediction" method they propose. Has
> lm been updated so that it can handle terms such as bs,ns, and poly
> automatically? I am using R 2.3.0
> 
> this is their example:
> 
> n <- 100
> set.seed(86) # For reproducibility of the random numbers
> x <- sort(runif(n))
> y <- sort(runif(n))
> fit <- lm(y ~ bs(x, df=5))
> plot(x, y,col="blue")
> lines(x, fitted(fit), col="black")
> newx <- seq(0, 1, len=n)
> points(newx, predict(fit, data.frame(x=newx)), type="l", col=red, err=-1)
> 
> 
> thanks,
> 
> Spencer
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

Hi, Spencer,

That website is for S-PLUS and not R. When I run the code in S-PLUS 6.2, 
I do indeed see different curves.

HTH,

--sundar


From epistat at gmail.com  Thu Sep 28 17:55:30 2006
From: epistat at gmail.com (zhijie zhang)
Date: Thu, 28 Sep 2006 23:55:30 +0800
Subject: [R] help on plots
Message-ID: <2fc17e30609280855q3545ccb6r83713ebdeccbf684@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060928/42761de8/attachment.pl 

From ggrothendieck at gmail.com  Thu Sep 28 17:57:05 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 28 Sep 2006 11:57:05 -0400
Subject: [R] safe prediction from lm
In-Reply-To: <1c6126db0609280844m78feb917sddd4dd30165c994e@mail.gmail.com>
References: <1c6126db0609280844m78feb917sddd4dd30165c994e@mail.gmail.com>
Message-ID: <971536df0609280857k2101dd2avb472c496beb5d3f6@mail.gmail.com>

See ?SafePrediction for how it works in R.

Also the example here:
http://www.mayin.org/ajayshah/KB/R/html/o9.html

On 9/28/06, Spencer Jones <ssj1364 at gmail.com> wrote:
> I am fitting a regression model with a bs term and then making predictions
> based on the model. According to some info on the internet at
> http://www.stat.auckland.ac.nz/~yee/smartpred/DummiesGuide.txt
>
> there are some problems with using predict.lm when you have a model with
> terms such as bs,ns,or poly. However when I used one of the examples they
> said would illustrate the problems I get virtually the same results using
> the standard predict function and "safe prediction" method they propose. Has
> lm been updated so that it can handle terms such as bs,ns, and poly
> automatically? I am using R 2.3.0
>
> this is their example:
>
> n <- 100
> set.seed(86) # For reproducibility of the random numbers
> x <- sort(runif(n))
> y <- sort(runif(n))
> fit <- lm(y ~ bs(x, df=5))
> plot(x, y,col="blue")
> lines(x, fitted(fit), col="black")
> newx <- seq(0, 1, len=n)
> points(newx, predict(fit, data.frame(x=newx)), type="l", col=red, err=-1)
>
>
> thanks,
>
> Spencer
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Thu Sep 28 18:05:50 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 28 Sep 2006 12:05:50 -0400
Subject: [R] Comparing entire row sets at once efficiently
In-Reply-To: <17691.61471.872466.824208@basebud.nulle.part>
References: <17691.61471.872466.824208@basebud.nulle.part>
Message-ID: <971536df0609280905x60cc2eadqdd34dec8d503b960@mail.gmail.com>

If Comparison and Candidates each have no duplicated rows (which
is the situation in the example) then try this:

tail(!duplicated(rbind(Comparison, Candidates)), nrow(Candidates))


On 9/28/06, Dirk Eddelbuettel <edd at debian.org> wrote:
>
> Dear useRs,
>
> I am having a hard time coming up with a nice and efficient solution to
> a problem on entires matrices or data.frames. In spirit, this is similar to
> what setdiff() and setequal() do, but I need it in more dimensions.
>
> Here's a brief description.
>
>  * given a set of factors or sequences, expand.grid() gives me the set
>    of permutations in a data.frame;
>
>    in my case all arguments are numeric so I could convert the data frame to
>    a matrix
>
>    let's call this one Candidates
>
>  * I have a second matrix (or data frame) to compare to; this second
>    set may be a subset of the first, or a superset but it guaranted to
>    contain the same columns
>
>    let's call this one Comparison
>
>  * I want know which rows in Candidates are not yet in Comparison.
>
> A toy example:
>
> > Comparison <- matrix(1:30, ncol=5)
> > Candidates <- Comparison[c(2,4), ]
> > checkRow <- function(r, M) { any( (r[1] == M[,1]) & (r[2] == M[,2]) & (r[3] == M[,3]) & (r[4] == M[,4]) ) }
> > checkRow( Candidates[1,], Comparison)
> [1] TRUE
> > falseRow <- Candidates[1,]
> > falseRow[2] <- 42
> > checkRow( falseRow, Comparison)
> [1] FALSE
> >
>
> The checkRow function works but is a) klunky, b) hardcodes the dimension and
> c) works only on one row at a time.
>
> There must be better ways, at least for a) and b).  What am I missing?
>
> Feel free to reply off-list and I'd gladly summarize back to the list. If you
> don't want your reply (or email) summarized back, please indicate.
>
> Thanks, Dirk
>
>
>
> --
> Hell, there are no rules here - we're trying to accomplish something.
>                                                  -- Thomas A. Edison
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From edd at debian.org  Thu Sep 28 18:21:44 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 28 Sep 2006 11:21:44 -0500
Subject: [R] Comparing entire row sets at once efficiently
In-Reply-To: <971536df0609280905x60cc2eadqdd34dec8d503b960@mail.gmail.com>
References: <17691.61471.872466.824208@basebud.nulle.part>
	<971536df0609280905x60cc2eadqdd34dec8d503b960@mail.gmail.com>
Message-ID: <17691.63128.138606.976900@basebud.nulle.part>


I should have known that  Gabor would reply within minutes with a nice
one-line solution ... :) 

On 28 September 2006 at 12:05, Gabor Grothendieck wrote:
| If Comparison and Candidates each have no duplicated rows (which
| is the situation in the example) then try this:
| 
| tail(!duplicated(rbind(Comparison, Candidates)), nrow(Candidates))

Excellent.  That will work.  Candidates has no dupes because expand.grid()
constructs it.  Comparison may have dupes, but we can ignore that.

By putting the 'larger set' against we which to compare second, we catch the
markers from duplicated(), and then subset via tail().  That's exactly what
needed.

Thanks, and chapeau for a very elegant one-liner,  Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From ssj1364 at gmail.com  Thu Sep 28 18:24:55 2006
From: ssj1364 at gmail.com (Spencer Jones)
Date: Thu, 28 Sep 2006 10:24:55 -0600
Subject: [R] safe prediction from lm
In-Reply-To: <971536df0609280857k2101dd2avb472c496beb5d3f6@mail.gmail.com>
References: <1c6126db0609280844m78feb917sddd4dd30165c994e@mail.gmail.com>
	<971536df0609280857k2101dd2avb472c496beb5d3f6@mail.gmail.com>
Message-ID: <1c6126db0609280924m23ae79a2g414cfb6e0115fc65@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060928/a2b1d1af/attachment.pl 

From ggrothendieck at gmail.com  Thu Sep 28 18:30:51 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 28 Sep 2006 12:30:51 -0400
Subject: [R] safe prediction from lm
In-Reply-To: <1c6126db0609280924m23ae79a2g414cfb6e0115fc65@mail.gmail.com>
References: <1c6126db0609280844m78feb917sddd4dd30165c994e@mail.gmail.com>
	<971536df0609280857k2101dd2avb472c496beb5d3f6@mail.gmail.com>
	<1c6126db0609280924m23ae79a2g414cfb6e0115fc65@mail.gmail.com>
Message-ID: <971536df0609280930w35e532c6uf48babc9b104d461@mail.gmail.com>

Yes.

On 9/28/06, Spencer Jones <ssj1364 at gmail.com> wrote:
> Sorry to be dense, I am somewhat new to R. But does that mean that
> SafePrediction is automatically incorporated into lm.predict?
>
>
> On 9/28/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > See ?SafePrediction for how it works in R.
> >
> > Also the example here:
> > http://www.mayin.org/ajayshah/KB/R/html/o9.html
> >
> > On 9/28/06, Spencer Jones < ssj1364 at gmail.com> wrote:
> > > I am fitting a regression model with a bs term and then making
> predictions
> > > based on the model. According to some info on the internet at
> > >
> http://www.stat.auckland.ac.nz/~yee/smartpred/DummiesGuide.txt
> > >
> > > there are some problems with using predict.lm when you have a model with
> > > terms such as bs,ns,or poly. However when I used one of the examples
> they
> > > said would illustrate the problems I get virtually the same results
> using
> > > the standard predict function and "safe prediction" method they propose.
> Has
> > > lm been updated so that it can handle terms such as bs,ns, and poly
> > > automatically? I am using R 2.3.0
> > >
> > > this is their example:
> > >
> > > n <- 100
> > > set.seed(86) # For reproducibility of the random numbers
> > > x <- sort(runif(n))
> > > y <- sort(runif(n))
> > > fit <- lm(y ~ bs(x, df=5))
> > > plot(x, y,col="blue")
> > > lines(x, fitted(fit), col="black")
> > > newx <- seq(0, 1, len=n)
> > > points(newx, predict(fit, data.frame(x=newx)), type="l", col=red,
> err=-1)
> > >
> > >
> > > thanks,
> > >
> > > Spencer
> > >
> > >        [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
>
>


From gunter.berton at gene.com  Thu Sep 28 18:33:48 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 28 Sep 2006 09:33:48 -0700
Subject: [R] fractional part
In-Reply-To: <47fce0650609280839o194298d3tf358d9598c125a8e@mail.gmail.com>
Message-ID: <003401c6e31b$e02fa9c0$0d1f210a@gne.windows.gene.com>

?"%%"

X%%1

Bert Gunter
Genentech Nonclinical Statistics
South San Francisco, CA 94404



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Hans-Peter
> Sent: Thursday, September 28, 2006 8:40 AM
> To: R Help
> Subject: [R] fractional part
> 
> Hi all,
> 
> is there a function to get the fractional part of a number?
> 
> -- 
> Regards,
> Hans-Peter
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From gunter.berton at gene.com  Thu Sep 28 18:41:13 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 28 Sep 2006 09:41:13 -0700
Subject: [R] fractional part
In-Reply-To: <47fce0650609280839o194298d3tf358d9598c125a8e@mail.gmail.com>
Message-ID: <003e01c6e31c$e96fb740$0d1f210a@gne.windows.gene.com>


Note:

I should have said for negative values, this may have to be adjusted,
depending on how you define "fractional part," as -1.25 %% 1 = .75.

-- Bert Gunter 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Hans-Peter
> Sent: Thursday, September 28, 2006 8:40 AM
> To: R Help
> Subject: [R] fractional part
> 
> Hi all,
> 
> is there a function to get the fractional part of a number?
> 
> -- 
> Regards,
> Hans-Peter
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ripley at stats.ox.ac.uk  Thu Sep 28 19:33:57 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 28 Sep 2006 18:33:57 +0100 (BST)
Subject: [R] drop
In-Reply-To: <47fce0650609280657x5bd7f29bxae45e15dc723b80d@mail.gmail.com>
References: <47fce0650609280657x5bd7f29bxae45e15dc723b80d@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0609281829370.15693@gannet.stats.ox.ac.uk>

On Thu, 28 Sep 2006, Hans-Peter wrote:

> Hello,
>
> What's the best/recommended way to drop the dimension of a data.frame?
> Probably "test[,,]" but I am not sure. I would prefer an explicit
> function, but drop doesn't work with frames and I didn't find
> something in the helpfiles/mailarchive.

The recommended way is not to do it.  If as it appears you want to extract 
a single column, do so explicitly by e.g.

if(length(test) == 1) test[[1]] else test

Your as.matrix versions will not work with general columns.

>
> Example:
>
> test <- data.frame( x = 1:7 )
>
> str( drop( test ) )                     # still a data.frame
>
> str( test[,, drop = TRUE] )             # ok
> str( test[,,] )                         # ok <== IS THIS THE WAY TO DO IT?
> str( as.integer( as.matrix( test ) ) )  # ok
>
> str( drop( as.matrix( test ) ) )        # ~ok, named integer
> test2 <- test
> attributes( test2 ) <- NULL
> str( drop( as.matrix( test2 ) ) )       # ok
>
>
> Thanks and best regards,
> Hans-Peter
>
> PS: how could I lookup the code for subsetting a data.frame. I suppose
> that it is in "data.frame.["
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From gchappi at gmail.com  Thu Sep 28 19:40:11 2006
From: gchappi at gmail.com (Hans-Peter)
Date: Thu, 28 Sep 2006 19:40:11 +0200
Subject: [R] fractional part
In-Reply-To: <003401c6e31b$e02fa9c0$0d1f210a@gne.windows.gene.com>
References: <47fce0650609280839o194298d3tf358d9598c125a8e@mail.gmail.com>
	<003401c6e31b$e02fa9c0$0d1f210a@gne.windows.gene.com>
Message-ID: <47fce0650609281040o6b50a762udb6bb57303e509fc@mail.gmail.com>

2006/9/28, Berton Gunter <gunter.berton at gene.com>:
> ?"%%"

I didn't know that operator, thanks!

But I was not clear, as I was looking more towards a lowlevel function.
E.g. "trunc", "floor", "ceiling", ... are defined in names.c and while
I currently not fully understand this I thought there could be a
similair thing for fraction.

I defined the function "frac <- function(x) abs( x - trunc(x) )"
which seems to work well.

Thanks again,
Hans-Peter


From Horace.Tso at pgn.com  Thu Sep 28 19:42:42 2006
From: Horace.Tso at pgn.com (Horace Tso)
Date: Thu, 28 Sep 2006 10:42:42 -0700
Subject: [R] Issue with Data Editor and Date objects
In-Reply-To: <971536df0609280930w35e532c6uf48babc9b104d461@mail.gmail.com>
References: <1c6126db0609280844m78feb917sddd4dd30165c994e@mail.gmail.com>
	<971536df0609280857k2101dd2avb472c496beb5d3f6@mail.gmail.com>
	<1c6126db0609280924m23ae79a2g414cfb6e0115fc65@mail.gmail.com>
	<971536df0609280930w35e532c6uf48babc9b104d461@mail.gmail.com>
Message-ID: <451BA723020000100042FDF3@pgn.com>

Dear list,

It's a minor issue but for someone who uses the Data Editor once in a
while this could be very annoying.

I have a dataframe with a Date column. When I bring it up with the
(gui) Data Editor (Edit -> Data editor...), do nothing and then close
it, the Date column turns into "numeric".

Before using Data Editor, 

>class(gas.gd$Flow.Date)
[1] "Date"

After Data Editor,

>class(gas.gd$Flow.Date)
[1] "numeric"

Any way to fix this?

TIA.

Horace W. Tso
Portland General Electric
Portland, Oregon


From helprhelp at gmail.com  Thu Sep 28 19:52:03 2006
From: helprhelp at gmail.com (Weiwei Shi)
Date: Thu, 28 Sep 2006 13:52:03 -0400
Subject: [R] mx2 contingency tables or (2^(m-1)-1)'s 2x2 contingency tables
	in the context of feature selection for random forest
Message-ID: <cdf817830609281052g27de0a3arf87185b2c5914ce7@mail.gmail.com>

Dear Listers:

I have a categorical feature selection problem for random forest.

Suppose I have a multiple-leveled category variable A, which has m=3
levels: red, green, and blue and the final target is binary
classification.

I want to evaluate its power in discrimination between 2 classes. We
know rf splits multiple-leveled category variable by considering all
combinations of its levels. So suppose again I have 1000 such
multiple-leveled category variables and I need to do some feature
selection. Then I would like to try chi-sqr tests (or information
gain).

To match the splitting method used in rf, I am thinking if I should
simply use mx2 contingency table or (2^(m-1)-1)'s 2x2 contingency
tables in which I pick the best p-value to evaluate A's power. For the
latter, I am sure it is very alike the way used in rf. But is the
former good enough?

Thanks.
-- 
Weiwei Shi, Ph.D
Research Scientist
GeneGO, Inc.

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III


From dchan at GFC.STATE.GA.US  Thu Sep 28 20:08:34 2006
From: dchan at GFC.STATE.GA.US (Dan Chan)
Date: Thu, 28 Sep 2006 14:08:34 -0400
Subject: [R] Converting text to numbers
Message-ID: <43679C69FEEE9C40AC8876C0FF38EF1003996184@mailsvr01.gfc.state.ga.us>

Hi David,

Thank you for your help.  It worked! 

Daniel Chan

-----Original Message-----
From: David Barron [mailto:mothsailor at googlemail.com] 
Sent: Wednesday, September 27, 2006 9:29 PM
To: Dan Chan; r-help
Subject: Re: [R] Converting text to numbers

> Then, I tried to convert them to numbers using the following.
> > Sample1$FCT2 <- as.numeric(Sample1$FCT2)
> > Sample1$OBS2 <- as.numeric(Sample1$OBS2)


This is actually an FAQ.  Do the following and it should be fine:

> Sample1$FCT2 <- as.numeric(as.character(Sample1$FCT2))
> Sample1$OBS2 <- as.numeric(as.character(Sample1$OBS2))



-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From bbands at gmail.com  Thu Sep 28 21:31:40 2006
From: bbands at gmail.com (BBands)
Date: Thu, 28 Sep 2006 12:31:40 -0700
Subject: [R] a decimal aligned column
Message-ID: <6e8360ad0609281231lbeec4f5la9c44fa177268597@mail.gmail.com>

Hello,

For numbers in the range 100 to 100,000,000 I'd like to decimal align
a right-justified comma-delineated column of numbers, but I haven't
been able to work out the proper format statement. format(num,
justify=right, width=15, big.mark=",") gets me close, but numbers
larger than 1,000,000 project a digit beyond the right edge of the
column, which I really don't understand. I gather I can get the
decimal alignment from sprintf(), but I am not sure about the
interaction of the two functions.

TIA,

    jab
-- 
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.


From uhkeller at web.de  Thu Sep 28 21:49:49 2006
From: uhkeller at web.de (Ulrich Keller)
Date: Thu, 28 Sep 2006 21:49:49 +0200
Subject: [R] Evaluation of defaults in functions
Message-ID: <451C275D.50901@web.de>

Hello,

and sorry if this is already explained somewhere. I couldn't find anything.

R (2.3.1, Windows) seems to perform some kind of lazy evaluation when 
evaluating defaults in function calls that, at least for me, leads to 
unexpected results. Consider the following, seemingly equivalent functions:

 > foo1 <- function(x, y=x) {
+   x <- 0
+   y
+ }
 > foo1(1)
[1] 0
 > foo2 <- function(x, y=x) {
+   y <- y
+   x <- 0
+   y
+ }
 > foo2(1)
[1] 1

Obviously, y is not evaluated until it is used in some way. I would 
expect it to be evaluated where it is defined. Is this intended behavior?
Thanks for clarifying,

Uli


From deepayan.sarkar at gmail.com  Thu Sep 28 22:06:03 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 28 Sep 2006 13:06:03 -0700
Subject: [R] Evaluation of defaults in functions
In-Reply-To: <451C275D.50901@web.de>
References: <451C275D.50901@web.de>
Message-ID: <eb555e660609281306u28085555s21ebf2a5a5072108@mail.gmail.com>

On 9/28/06, Ulrich Keller <uhkeller at web.de> wrote:
> Hello,
>
> and sorry if this is already explained somewhere. I couldn't find anything.
>
> R (2.3.1, Windows) seems to perform some kind of lazy evaluation when
> evaluating defaults in function calls that, at least for me, leads to
> unexpected results. Consider the following, seemingly equivalent functions:
>
>  > foo1 <- function(x, y=x) {
> +   x <- 0
> +   y
> + }
>  > foo1(1)
> [1] 0
>  > foo2 <- function(x, y=x) {
> +   y <- y
> +   x <- 0
> +   y
> + }
>  > foo2(1)
> [1] 1
>
> Obviously, y is not evaluated until it is used in some way. I would
> expect it to be evaluated where it is defined. Is this intended behavior?

Yes. For a similar example see the 'logplot' function and related discussion in

http://cran.r-project.org/doc/manuals/R-lang.html#Substitutions

-Deepayan


From mschwartz at mn.rr.com  Thu Sep 28 22:11:05 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Thu, 28 Sep 2006 15:11:05 -0500
Subject: [R] Evaluation of defaults in functions
In-Reply-To: <451C275D.50901@web.de>
References: <451C275D.50901@web.de>
Message-ID: <1159474265.4037.16.camel@localhost.localdomain>

On Thu, 2006-09-28 at 21:49 +0200, Ulrich Keller wrote:
> Hello,
> 
> and sorry if this is already explained somewhere. I couldn't find anything.
> 
> R (2.3.1, Windows) seems to perform some kind of lazy evaluation when 
> evaluating defaults in function calls that, at least for me, leads to 
> unexpected results. Consider the following, seemingly equivalent functions:
> 
>  > foo1 <- function(x, y=x) {
> +   x <- 0
> +   y
> + }
>  > foo1(1)
> [1] 0
>  > foo2 <- function(x, y=x) {
> +   y <- y
> +   x <- 0
> +   y
> + }
>  > foo2(1)
> [1] 1
> 
> Obviously, y is not evaluated until it is used in some way. I would 
> expect it to be evaluated where it is defined. Is this intended behavior?
> Thanks for clarifying,
> 
> Uli

Yep. This is documented in the R Language Definition Manual, which is
available via the GUI in the Windows version and/or online here:

  http://cran.r-project.org/doc/manuals/R-lang.html

Specifically in section 4.3.3 Argument Evaluation:

"R has a form of lazy evaluation of function arguments. Arguments are
not evaluated until needed. It is important to realize that in some
cases the argument will never be evaluated. Thus, it is bad style to use
arguments to functions to cause side-effects. While in C it is common to
use the form, foo(x = y) to invoke foo with the value of y and
simultaneously to assign the value of y to x this same style should not
be used in R. There is no guarantee that the argument will ever be
evaluated and hence the assignment may not take place."

You might also want to read section 2.1.8 Promise objects and section
6.2 Substitutions.

HTH,

Marc Schwartz


From LuJ at edc.pitt.edu  Thu Sep 28 22:55:06 2006
From: LuJ at edc.pitt.edu (Lu, Jiang Jane)
Date: Thu, 28 Sep 2006 16:55:06 -0400
Subject: [R] complex plots using layout()
Message-ID: <6E031567AB9AD24FA98F173673F0708F824A77@boa.edc3.pitt.edu>

Dear r-help,

I am trying to plot several scatter plots with marginal histograms on
one page. Ideally, a page is equally divided into 4 figure regions.
Within each figure region, a scatter plot with marginal histograms will
be plotted.

I followed Dr. Paul Murrell's code released online to successfully plot
the scatter plot with marginal histograms. The code applies "layout()"
to partition the page.

Right now, I want each of the 4 figure regions on one page to be plotted
a scatter plot with marginal histograms. I tried par(mfrow= ) ahead of
layout(). It does not work. Could I repeat layout() to reach my point?

Following is the code I use. Any advice is greatly appreciated.

=================================================================
x <- demog$age
y1 <- demog$mji
y2 <- demog$nles
xhist <- hist(x,  plot=FALSE)
y1hist <- hist(y1,  plot=FALSE)
y2hist <- hist(y2,  plot=FALSE)

top1 <- max(c(xhist$counts, y1hist$counts))
top2 <- max(c(xhist$counts, y2hist$counts))

xrange <- range(x,na.rm=TRUE)
y1range <- range(y1,na.rm=TRUE)
y2range <- range(y2,na.rm=TRUE)

def.par <- par(no.readonly = TRUE)

par(mfrow=c(2,2))

nf <- layout(matrix(c(2,0,1,3),2,2,byrow=TRUE), c(3,1), c(1,3), TRUE)

par(mar=c(3,3,1,1))
plot(x, y1, xlim=xrange, ylim=y1range, xlab="Age", ylab="MJI")
lines(lowess(x,y1),col=2)
par(mar=c(0,3,1,1))
barplot(xhist$counts, axes=FALSE, ylim=c(0, top1), space=0)
par(mar=c(3,0,1,1))
barplot(y1hist$counts, axes=FALSE, xlim=c(0, top1), space=0, horiz=TRUE)

par(mar=c(3,3,1,1))
plot(x, y2, xlim=xrange, ylim=y2range, xlab="Age", ylab="Numer of
Lesions")
lines(lowess(x,y2),col=2)
par(mar=c(0,3,1,1))
barplot(xhist$counts, axes=FALSE, ylim=c(0, top2), space=0)
par(mar=c(3,0,1,1))
barplot(y2hist$counts, axes=FALSE, xlim=c(0, top2), space=0, horiz=TRUE)


par(def.par)

================================================================ 


Sincerely yours,

Jiang Lu

Statistician

University of Pittsburgh
130 DeSoto Street, 127 Parran Hall
Pittsburgh, PA 15261
USA


From ggrothendieck at gmail.com  Thu Sep 28 23:32:57 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 28 Sep 2006 17:32:57 -0400
Subject: [R] Evaluation of defaults in functions
In-Reply-To: <451C275D.50901@web.de>
References: <451C275D.50901@web.de>
Message-ID: <971536df0609281432w332f786ase511e504ac35c269@mail.gmail.com>

foo2 could be written:

   foo3 <- function(x, y = x) { force(y); x <- 0; y }

to make it clear that evaluation of y is being forced.  See ?force

On 9/28/06, Ulrich Keller <uhkeller at web.de> wrote:
> Hello,
>
> and sorry if this is already explained somewhere. I couldn't find anything.
>
> R (2.3.1, Windows) seems to perform some kind of lazy evaluation when
> evaluating defaults in function calls that, at least for me, leads to
> unexpected results. Consider the following, seemingly equivalent functions:
>
>  > foo1 <- function(x, y=x) {
> +   x <- 0
> +   y
> + }
>  > foo1(1)
> [1] 0
>  > foo2 <- function(x, y=x) {
> +   y <- y
> +   x <- 0
> +   y
> + }
>  > foo2(1)
> [1] 1
>
> Obviously, y is not evaluated until it is used in some way. I would
> expect it to be evaluated where it is defined. Is this intended behavior?
> Thanks for clarifying,
>
> Uli
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mschwartz at mn.rr.com  Thu Sep 28 23:36:09 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Thu, 28 Sep 2006 16:36:09 -0500
Subject: [R] help on plots
In-Reply-To: <2fc17e30609280855q3545ccb6r83713ebdeccbf684@mail.gmail.com>
References: <2fc17e30609280855q3545ccb6r83713ebdeccbf684@mail.gmail.com>
Message-ID: <1159479369.4037.32.camel@localhost.localdomain>

On Thu, 2006-09-28 at 23:55 +0800, zhijie zhang wrote:
> Dear friends,
>  I met a problem on plotting.
> My dataset is :
> year    MHBC LHBC MHRC LURC
> 1993   11.75   4.50   0.43   0.46
> 1994    7.25   1.25   0.35   0.51
> 1995    8.67   2.17   0.54   0.44
> 1996   2.67   1.33   0.78   0.47
> 1997   3.42   4.92   0.69   0.48
> 1998   1.92   3.08   0.72   0.54
> 1999   2.33   2.58   0.74   0.41
> 2000   5.75   4.50   0.45   0.50
> 2001   3.75   4.42   0.52   0.47
> 2002   2.33   1.83   0.58   0.45
> 2003   0.25   2.83   0.50   0.39
> I want to get a plot -line with scatters, the requirement is :
> x-axis is year;
> two y-axis:
>   y1 corresponds to MHBC and LHBC;
>   y2 corresponds to MHRC and LURC;
> hope to use different symbols to differentiate the MHBC,LHBC,MHRC and  LURC.
> 
> The following is my program, but  very bad ,:
> *plot(a$year,a$MHBC,type='b')  #line1
> par(new=T)
> plot(a$year,a$LHBC,type='b')  #line2
> par(new=T)
> plot(a$year,a$MHRC,type='b')  #line3
> par(new=T)
> plot(a$year,a$LURC,type='b')   #line4
> axis(4, at=pretty(range(a$MHRC)))*
> In the figure, the labels and scales of X-axis are vague, the scale of
> y-axis is not very good.
> The better figure should be like the line1 and 2 are in the upper, and line3
> and 4 are in the bottom.
> Any suggestion are welcome!

It's not entirely clear to me what you want, so let me offer three
possibilities.


1. Do all four lines in a single plot with a common y axis:

matplot(a$year, a[, -1], type = "o", pch = 15:18)



2. Do all four lines in a single plot with the first two having a
separate left hand y axis and the second two having a separate right
hand y axis:

# Draw the first pair of lines
matplot(a$year, a[, 2:3], type = "o", pch = c(19, 20),
        lty = "solid", ann = FALSE)

# Get the current plot region boundaries
usr <- par("usr")

# Get the range of the second set of columns
range.y2 <- range(a[, 4:5])

# Change the plot region y axis range for the second
# set of columns. Extend them by 4% as per the default
par(usr = c(usr[1], usr[2], 
            range.y2[1] * 0.96 , range.y2[2] * 1.04))

# Add the second pair of lines
matlines(a$year, a[, 4:5], type = "o", pch = c(15, 18), 
         lty = "dashed", col = c("blue", "green"))

# Add the second y axis
axis(4)



3. Do the first two lines in an upper plot and the second two lines in a
lower plot, each has its own y axis range:

# Set plot region to have two rows
par(mfrow = c(2, 1))

# Adjust the plot margins
par(mar = c(2, 5, 2, 2))

# Draw the first pair of lines
matplot(a$year, a[, 2:3], type = "o", pch = c(19, 20),
        lty = "solid", ylab = "First Pair")


par(mar = c(3, 5, 2, 2))

# Add the second pair of lines
matplot(a$year, a[, 4:5], type = "o", pch = c(15, 18), 
        lty = "dashed", col = c("blue", "green"), 
        ylab = "Second Pair")



See ?matplot, ?par and ?points for more information.

HTH,

Marc Schwartz


From p.dalgaard at biostat.ku.dk  Thu Sep 28 23:42:04 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 28 Sep 2006 23:42:04 +0200
Subject: [R] Evaluation of defaults in functions
In-Reply-To: <971536df0609281432w332f786ase511e504ac35c269@mail.gmail.com>
References: <451C275D.50901@web.de>
	<971536df0609281432w332f786ase511e504ac35c269@mail.gmail.com>
Message-ID: <x2hcyrmzwz.fsf@turmalin.kubism.ku.dk>

"Gabor Grothendieck" <ggrothendieck at gmail.com> writes:

> foo2 could be written:
> 
>    foo3 <- function(x, y = x) { force(y); x <- 0; y }
> 
> to make it clear that evaluation of y is being forced.  See ?force


And, to answer the specific question: Yes, R has lazy evaluation,
everywhere. Arguments are always evaluated if and when they are
needed. 

 
> On 9/28/06, Ulrich Keller <uhkeller at web.de> wrote:
> > Hello,
> >
> > and sorry if this is already explained somewhere. I couldn't find anything.
> >
> > R (2.3.1, Windows) seems to perform some kind of lazy evaluation when
> > evaluating defaults in function calls that, at least for me, leads to
> > unexpected results. Consider the following, seemingly equivalent functions:
> >
> >  > foo1 <- function(x, y=x) {
> > +   x <- 0
> > +   y
> > + }
> >  > foo1(1)
> > [1] 0
> >  > foo2 <- function(x, y=x) {
> > +   y <- y
> > +   x <- 0
> > +   y
> > + }
> >  > foo2(1)
> > [1] 1
> >
> > Obviously, y is not evaluated until it is used in some way. I would
> > expect it to be evaluated where it is defined. Is this intended behavior?
> > Thanks for clarifying,
> >
> > Uli
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From duchesne at dms.umontreal.ca  Thu Sep 28 23:45:54 2006
From: duchesne at dms.umontreal.ca (Pierre Duchesne)
Date: Thu, 28 Sep 2006 17:45:54 -0400
Subject: [R] AIC in R
Message-ID: <000801c6e347$792af780$6901a8c0@portableudm>

Dear R users,

According Brockwell & Davis (1991, Section 9.3, p.304), the penalty term for
computing the AIC criteria is "p+q+1" in the context of a zero-mean
ARMA(p,q) time series model.  They arrived at this criterion (with this
particular penalty term) estimating the Kullback-Leibler discrepancy index.
In practice, the user usually chooses the model whose estimated index is
minimum.  Consequently, it seems that the theory and the interpretation are
only available in the case of a zero mean ARMA model, at least in the time
series context.

Concerning R, it seems that the penalty term is p+q+1 in a zero mean model,
and p+q+1+1 = p+q+2 for a ARMA(p,q) model with a constant term.  See the
following examples:

--------------------------------------------------------------
set.seed(1)
serieAR1 = arima.sim(100,model=list(ar= 0.5))

fit1AR1 = arima(serieAR1, order = c(0, 0, 0), include.mean = T) 
fit2AR1 = arima(serieAR1, order = c(1, 0, 0), include.mean = T) 
fit3AR1 = arima(serieAR1, order = c(1, 0, 0), include.mean = F) 
fit4AR1 = arima(serieAR1, order = c(1, 0, 1), include.mean = T) 
fit5AR1 = arima(serieAR1, order = c(1, 0, 1), include.mean = F)

-2* fit1AR1$loglik + 2*(1+1)
fit1AR1$aic

-2* fit2AR1$loglik + 2*(1+1+1)
fit2AR1$aic

-2* fit3AR1$loglik + 2*(1+1)
fit3AR1$aic

-2* fit4AR1$loglik + 2*(1+1+1+1)
fit4AR1$aic

-2* fit5AR1$loglik + 2*(1+1+1)
fit5AR1$aic


> set.seed(1)
> serieAR1 = arima.sim(100,model=list(ar= 0.5))
> 
> fit1AR1 = arima(serieAR1, order = c(0, 0, 0), include.mean = T) 
> fit2AR1 = arima(serieAR1, order = c(1, 0, 0), include.mean = T) 
> fit3AR1 = arima(serieAR1, order = c(1, 0, 0), include.mean = F) 
> fit4AR1 = arima(serieAR1, order = c(1, 0, 1), include.mean = T) 
> fit5AR1 = arima(serieAR1, order = c(1, 0, 1), include.mean = F)
> 
> -2* fit1AR1$loglik + 2*(1+1)
[1] 297.4670
> fit1AR1$aic
[1] 297.4670
> 
> -2* fit2AR1$loglik + 2*(1+1+1)
[1] 270.5381
> fit2AR1$aic
[1] 270.5381
> 
> -2* fit3AR1$loglik + 2*(1+1)
[1] 270.6653
> fit3AR1$aic
[1] 270.6653
> 
> -2* fit4AR1$loglik + 2*(1+1+1+1)
[1] 272.3530
> fit4AR1$aic
[1] 272.3530
> 
> -2* fit5AR1$loglik + 2*(1+1+1)
[1] 272.5564
> fit5AR1$aic
[1] 272.5564
--------------------------------------------------------------

>From the help file of extractAIC(), it seems that the criterion used is: 
AIC = - 2*log L +  k * edf,
where L is the likelihood and 'edf' the equivalent degrees of freedom (i.e.,
the number of free parameters for usual parametric models) of 'fit'.

My question is: is there any justification for computing the AIC as done by
R when a constant term is in the model?

Your help will be appreciated.

Best regards,
Pierre

Note: for differenced time series (d > 1), the penalty term seems to be
p+q+1, and there is no constant term in the fit.

-------
Pierre Duchesne,
D?partement de math?matiques et statistique,
Universit? de Montr?al,
CP 6128 Succ. Centre-Ville,
Montr?al, Qu?bec, Canada H3C 3J7.


From mwtoews at sfu.ca  Thu Sep 28 23:46:21 2006
From: mwtoews at sfu.ca (Michael Toews)
Date: Thu, 28 Sep 2006 14:46:21 -0700
Subject: [R] Documentation patch for 'match' and 'palette'
Message-ID: <451C42AD.3060506@sfu.ca>

Here is a patch to improve documentation for finding useful, yet newish, 
functions: 'findInterval' and 'colorRamp'. I think that it is worthwhile 
to mention these in the 'seealso' section of the similar 'match' and 
'palette' documents. I had difficulty finding these functions at first, 
as they have compound names. Modify the patch as needed.
+mt

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: documentation-patch.diff
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060928/42dc012d/attachment.pl 

From mschwartz at mn.rr.com  Thu Sep 28 23:49:52 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Thu, 28 Sep 2006 16:49:52 -0500
Subject: [R] a decimal aligned column
In-Reply-To: <6e8360ad0609281231lbeec4f5la9c44fa177268597@mail.gmail.com>
References: <6e8360ad0609281231lbeec4f5la9c44fa177268597@mail.gmail.com>
Message-ID: <1159480192.4037.41.camel@localhost.localdomain>

On Thu, 2006-09-28 at 12:31 -0700, BBands wrote:
> Hello,
> 
> For numbers in the range 100 to 100,000,000 I'd like to decimal align
> a right-justified comma-delineated column of numbers, but I haven't
> been able to work out the proper format statement. format(num,
> justify=right, width=15, big.mark=",") gets me close, but numbers
> larger than 1,000,000 project a digit beyond the right edge of the
> column, which I really don't understand. I gather I can get the
> decimal alignment from sprintf(), but I am not sure about the
> interaction of the two functions.
> 
> TIA,
> 
>     jab

Is this what you want?:

Nums <- 10 ^ (2:8)

Nums.Pretty <- format(Nums, width = 20, justify = "right", 
                      big.mark = ",", nsmall = 4, scientific = FALSE)


> Nums.Pretty
[1] "              100.0000" "            1,000.0000"
[3] "           10,000.0000" "          100,000.0000"
[5] "        1,000,000.0000" "       10,000,000.0000"
[7] "      100,000,000.0000"


> cat(Nums.Pretty, sep = "\n")
              100.0000
            1,000.0000
           10,000.0000
          100,000.0000
        1,000,000.0000
       10,000,000.0000
      100,000,000.0000



HTH,

Marc Schwartz


From sswami at hotmail.com  Thu Sep 28 23:53:33 2006
From: sswami at hotmail.com (S. Swaminathan)
Date: Thu, 28 Sep 2006 21:53:33 +0000
Subject: [R] multiple xyplots on the same graph
Message-ID: <BAY120-F9F7D100086AF12D4333AEDD1B0@phx.gbl>

I am new to R and am having trouble plotting multiple xyplots on the same 
graph.
I have a dataframe x1 with 3 columns month, var1, var2. Month is a factor 
while var1 and var2 are numeric.
Reading through the archives I learned to plot var1 vs month and var2 vs 
month and have them as two plots in one window by doing the following
library(lattice)
x<-xyplot(var1~month,type="b")
y<-xyplot(var2~month,type="b")
print(x,split(c(1,1,1,2),more=TRUE)
print(y,split(c(1,2,1,2))

How do I get just one plot with month as the x axis and two lines for var1 
and var2 ?

thanks
swami


From deepayan.sarkar at gmail.com  Fri Sep 29 00:33:56 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 28 Sep 2006 15:33:56 -0700
Subject: [R] multiple xyplots on the same graph
In-Reply-To: <BAY120-F9F7D100086AF12D4333AEDD1B0@phx.gbl>
References: <BAY120-F9F7D100086AF12D4333AEDD1B0@phx.gbl>
Message-ID: <eb555e660609281533p476c786egd450af76245568ec@mail.gmail.com>

On 9/28/06, S. Swaminathan <sswami at hotmail.com> wrote:
> I am new to R and am having trouble plotting multiple xyplots on the same
> graph.
> I have a dataframe x1 with 3 columns month, var1, var2. Month is a factor
> while var1 and var2 are numeric.
> Reading through the archives I learned to plot var1 vs month and var2 vs
> month and have them as two plots in one window by doing the following
> library(lattice)
> x<-xyplot(var1~month,type="b")
> y<-xyplot(var2~month,type="b")
> print(x,split(c(1,1,1,2),more=TRUE)
> print(y,split(c(1,2,1,2))
>
> How do I get just one plot with month as the x axis and two lines for var1
> and var2 ?

xyplot(var1 + var2 ~ month,
       data = x1,
       type = "b")

Incidentally, R has a help system, and you will usually get more
reliable information if you read the help page of the function you are
using rather than "read through the archives". In this case, that
would be done by typing ?xyplot or help(xyplot).

-Deepayan


From epistat at gmail.com  Fri Sep 29 03:11:02 2006
From: epistat at gmail.com (zhijie zhang)
Date: Fri, 29 Sep 2006 09:11:02 +0800
Subject: [R] help on plots
In-Reply-To: <1159479369.4037.32.camel@localhost.localdomain>
References: <2fc17e30609280855q3545ccb6r83713ebdeccbf684@mail.gmail.com>
	<1159479369.4037.32.camel@localhost.localdomain>
Message-ID: <2fc17e30609281811m66c8d103tdee9131fff7ef377@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060929/077c18d2/attachment.pl 

From Charles.Annis at StatisticalEngineering.com  Fri Sep 29 04:12:37 2006
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Thu, 28 Sep 2006 22:12:37 -0400
Subject: [R] control L to clear the Rgui screen in Windows
Message-ID: <000001c6e36c$bc36a9f0$6400a8c0@DD4XFW31>

Greetings R-ians:

Searching the Searchable Mail Archives I discovered that ctrl L will clear
the Rgui screen, which is what I'd like to do from a print (or some similar)
statement.

Is there a mechanism to use the ctrl L clear-screen sequence in a script, or
print statement?

Thanks for your counsel.

Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:? 614-455-3265
http://www.StatisticalEngineering.com
?


From jholtman at gmail.com  Fri Sep 29 04:33:00 2006
From: jholtman at gmail.com (jim holtman)
Date: Thu, 28 Sep 2006 22:33:00 -0400
Subject: [R] control L to clear the Rgui screen in Windows
In-Reply-To: <000001c6e36c$bc36a9f0$6400a8c0@DD4XFW31>
References: <000001c6e36c$bc36a9f0$6400a8c0@DD4XFW31>
Message-ID: <644e1f320609281933u13922802rea8716e02d12d4dc@mail.gmail.com>

This I got from a previous answer.  You require the package 'rcom'

 cls.console  <-  function() {
       if (.Platform$GUI[1] != "Rgui")
               return(invisible(FALSE))
       if (!require(rcom, quietly = TRUE)) # Not shown any way!
               stop("Package rcom is required for 'cls()'")
       wsh <- comCreateObject("Wscript.Shell")
       if (is.null(wsh)) {
               return(invisible(FALSE))
       } else {
               comInvoke(wsh, "SendKeys", "\014")
               return(invisible(TRUE))
       }
}


On 9/28/06, Charles Annis, P.E.
<Charles.Annis at statisticalengineering.com> wrote:
> Greetings R-ians:
>
> Searching the Searchable Mail Archives I discovered that ctrl L will clear
> the Rgui screen, which is what I'd like to do from a print (or some similar)
> statement.
>
> Is there a mechanism to use the ctrl L clear-screen sequence in a script, or
> print statement?
>
> Thanks for your counsel.
>
> Charles Annis, P.E.
>
> Charles.Annis at StatisticalEngineering.com
> phone: 561-352-9699
> eFax: 614-455-3265
> http://www.StatisticalEngineering.com
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From h.wickham at gmail.com  Fri Sep 29 04:34:19 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Thu, 28 Sep 2006 21:34:19 -0500
Subject: [R] Evaluation of defaults in functions
In-Reply-To: <x2hcyrmzwz.fsf@turmalin.kubism.ku.dk>
References: <451C275D.50901@web.de>
	<971536df0609281432w332f786ase511e504ac35c269@mail.gmail.com>
	<x2hcyrmzwz.fsf@turmalin.kubism.ku.dk>
Message-ID: <f8e6ff050609281934r69a0e60fp99f16cef986d2a3c@mail.gmail.com>

>
> And, to answer the specific question: Yes, R has lazy evaluation,
> everywhere. Arguments are always evaluated if and when they are
> needed.
>

But doesn't R has a rather limited force of lazy evaluation? - you
have no control over it, apart from that arguments are evaluated
lazily.  This rather limited compared to other languages (no lazy
lists etc)

Hadley


From ggrothendieck at gmail.com  Fri Sep 29 04:35:57 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 28 Sep 2006 22:35:57 -0400
Subject: [R] control L to clear the Rgui screen in Windows
In-Reply-To: <000001c6e36c$bc36a9f0$6400a8c0@DD4XFW31>
References: <000001c6e36c$bc36a9f0$6400a8c0@DD4XFW31>
Message-ID: <971536df0609281935j24027873y5181455bd749805d@mail.gmail.com>

See:

http://tolstoy.newcastle.edu.au/R/help/06/02/21556.html



On 9/28/06, Charles Annis, P.E.
<Charles.Annis at statisticalengineering.com> wrote:
> Greetings R-ians:
>
> Searching the Searchable Mail Archives I discovered that ctrl L will clear
> the Rgui screen, which is what I'd like to do from a print (or some similar)
> statement.
>
> Is there a mechanism to use the ctrl L clear-screen sequence in a script, or
> print statement?
>
> Thanks for your counsel.
>
> Charles Annis, P.E.
>
> Charles.Annis at StatisticalEngineering.com
> phone: 561-352-9699
> eFax: 614-455-3265
> http://www.StatisticalEngineering.com
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From dkaplan at education.wisc.edu  Fri Sep 29 05:00:03 2006
From: dkaplan at education.wisc.edu (David Kaplan)
Date: Thu, 28 Sep 2006 22:00:03 -0500
Subject: [R] time-series packages
Message-ID: <451C8C33.2060509@education.wisc.edu>

Greetings,

Are there R packages that perform time-series analyses - particularly 
estimation of ARIMA models along with unit-root tests?  I know that 
FinMetrics in the S-Plus program will do it, but I'm looking for R 
packages, as well any reference material for estimating time-series' 
models in R.

Thanks in advance,

David


-- 
========================================================================
David Kaplan, Ph.D.
Professor
Department of Educational Psychology
University of Wisconsin - Madison
Educational Sciences, Room 1061
1025 W. Johnson Street
Madison, WI 53706

email: dkaplan at education.wisc.edu
Web:   http://www.education.wisc.edu/edpsych/facstaff/kaplan/kaplan.htm
Phone: 608-262-0836
Fax:   608-262-0843


From Joe-Byers at utulsa.edu  Fri Sep 29 05:11:45 2006
From: Joe-Byers at utulsa.edu (Joe W. Byers)
Date: Thu, 28 Sep 2006 22:11:45 -0500
Subject: [R] inserting columns in the middle of a dataframe
In-Reply-To: <007501c6d784$841fcf90$711f210a@gne.windows.gene.com>
References: <C12E3F0A.425E5%timothy.c.bates@gmail.com>
	<007501c6d784$841fcf90$711f210a@gne.windows.gene.com>
Message-ID: <efi2po$gl3$1@sea.gmane.org>

Berton Gunter wrote:
> Please folks -- use indexing.
> 
> myframe<-myframe[,c(1,5,2,3,4)]
> 
> Which begs the question: why bother rearranging the columns anyway, since
> one can get them used, printed, etc. in any order you wish anytime you want
> just by specifying the indices in the order you want them. I suspect the
> question was motivated by too much Sas- or Excel -ism.
Many of the time series classes expect a date in the first column of the 
matrix or data.frame when creating the date-time object.  Retrieving 
data in a SQL query from a dB returns a character representation of the 
date that requires conversion to a date.  Performing this conversion is 
easy but inserting this converted date column is not straight forward.


> 
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
>  
> "The business of the statistician is to catalyze the scientific learning
> process."  - George E. P. Box
>  
>  
> 
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch 
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Timothy Bates
>> Sent: Wednesday, September 13, 2006 3:05 PM
>> To: Jon Minton; r-help at stat.math.ethz.ch
>> Subject: Re: [R] inserting columns in the middle of a dataframe
>>
>>
>>> Is there a built-in and simple way to insert new columns in 
>> a dataframe?
>>
>> You do this by collecting the columns in the new order you desire, and
>> making a new frame.
>>
>> oldframe           <- data.frame(matrix(0:14,ncol=3))
>> newcol              <- data.frame(20:24)
>> names(newcol) <- "newcol"
>> newframe         <- data.frame(c(oldframe[1],newcol, oldframe[2:3]))
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Charles.Annis at StatisticalEngineering.com  Fri Sep 29 05:15:09 2006
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Thu, 28 Sep 2006 23:15:09 -0400
Subject: [R] SUMMARY control L to clear the Rgui screen in Windows
In-Reply-To: <000001c6e36c$bc36a9f0$6400a8c0@DD4XFW31>
Message-ID: <000601c6e375$78671c60$6400a8c0@DD4XFW31>

Eternal thanks to Jim Holtman and to Gabor Grothendieck who pointed me to
this concise piece of code:

cls <- function() {
	require(rcom)
	wsh <- comCreateObject("Wscript.Shell")
	comInvoke(wsh, "SendKeys", "\014")
	invisible(wsh)
}


Perfect!

Thanks!

Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Charles Annis, P.E.
Sent: Thursday, September 28, 2006 10:13 PM
To: R-help at stat.math.ethz.ch
Subject: [R] control L to clear the Rgui screen in Windows

Greetings R-ians:

Searching the Searchable Mail Archives I discovered that ctrl L will clear
the Rgui screen, which is what I'd like to do from a print (or some similar)
statement.

Is there a mechanism to use the ctrl L clear-screen sequence in a script, or
print statement?

Thanks for your counsel.

Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:? 614-455-3265
http://www.StatisticalEngineering.com
?

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From edd at debian.org  Fri Sep 29 05:17:39 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 28 Sep 2006 22:17:39 -0500
Subject: [R] time-series packages
In-Reply-To: <451C8C33.2060509@education.wisc.edu>
References: <451C8C33.2060509@education.wisc.edu>
Message-ID: <17692.36947.840442.776306@basebud.nulle.part>


On 28 September 2006 at 22:00, David Kaplan wrote:
| Greetings,
| 
| Are there R packages that perform time-series analyses - particularly 
| estimation of ARIMA models along with unit-root tests?  I know that 
| FinMetrics in the S-Plus program will do it, but I'm looking for R 
| packages, as well any reference material for estimating time-series' 
| models in R.

You could start here for an overview:

http://cran.us.r-project.org/src/contrib/Views/Econometrics.html

and there is some overlap regarding time series with this one:

http://cran.us.r-project.org/src/contrib/Views/Finance.html

Hth, Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From wasquith at austin.rr.com  Fri Sep 29 05:22:48 2006
From: wasquith at austin.rr.com (William Asquith)
Date: Thu, 28 Sep 2006 22:22:48 -0500
Subject: [R]  End-of-Line Problems
Message-ID: <0C591593-AAF5-40F5-BF1E-DD19723C6913@austin.rr.com>

I am teaching grad/undergrad course in which I am introducting  
students to R as a side topic; however, I am simultaneously  
supporting Windows/MacOSX/Linux for the students.

Under some conditions, the Windows users can not read files  
originating from MacOSX. The end-of-line differences between OSs is  
the cause. Windows=\r\n; Mac=\r; Unix=\n; etc. Some of the students  
are extremely frustrated (of course).

My Windows users will not have dos2unix or perl -pi -e 's/\r\n$/\n/'  
or similar abilities for file conversion.

read.table() does not appear to have capability to modify its end-of- 
line definition--is this true?  Is there some R command sequence for  
the students to change the end-of-line?

I've scanned mail archive and the Export/Importing manual, but I am  
stuck and need some advice from other instructors of R in classroom  
environments.


THANKS!

William A.


From dkaplan at education.wisc.edu  Fri Sep 29 06:16:23 2006
From: dkaplan at education.wisc.edu (David Kaplan)
Date: Thu, 28 Sep 2006 23:16:23 -0500
Subject: [R] What is wrong with this input
Message-ID: <451C9E17.6090204@education.wisc.edu>

I can't seem to figure out why I'm getting this error.  The output is 
copied right off the screen.  Notice how in some cases the back slash is 
missing.  In other cases, it can't read a file that I know is there.

Thanks in advance



 > library(foreign)
 > hrout <- read.spss("c:\\hrab200.sav")
Error in read.spss("c:\\hrab200.sav") : unable to open file
 > library(foreign)
 > hrout <- read.spss("C:\Documents and Settings\David Kaplan\My 
Documents\Baseball\Baseball Research\hr time series paper\new time 
series hr paper\hrab200.sav")
Error in read.spss("C:Documents and SettingsDavid KaplanMy 
DocumentsBaseballBaseball Researchhr time series paper\new time series 
hr paperhrab200.sav") :
         unable to open file
 > library(foreign)
 > hrout <- read.spss("C:\Documents and Settings\David Kaplan\My 
Documents\hrab200.sav"}
Error: syntax error in "hrout <- read.spss("C:\Documents and 
Settings\David Kaplan\My Documents\hrab200.sav"}"
 > hrout <-read.spss("C:\\Documents and Settings\David Kaplan\My 
Documents\hrab200.sav")
Error in read.spss("C:\\Documents and SettingsDavid KaplanMy 
Documentshrab200.sav") :
         unable to open file



-- 
========================================================================
David Kaplan, Ph.D.
Professor
Department of Educational Psychology
University of Wisconsin - Madison
Educational Sciences, Room 1061
1025 W. Johnson Street
Madison, WI 53706

email: dkaplan at education.wisc.edu
Web:   http://www.education.wisc.edu/edpsych/facstaff/kaplan/kaplan.htm
Phone: 608-262-0836
Fax:   608-262-0843


From rhead.enion at yale.edu  Fri Sep 29 06:24:05 2006
From: rhead.enion at yale.edu (Michael Rhead Enion)
Date: Fri, 29 Sep 2006 00:24:05 -0400
Subject: [R] Loading rgdal library
Message-ID: <E4EC56EB-1824-4B51-9CC6-610340DE2D68@yale.edu>

I am trying to load the rgdal library in Mac OS X 10.4.7 (pismo g3  
500 mhz).

I already loaded sp  and compiled rgdal successfully, such that rgdal  
shows up in the package manager GUI.  But when I try to load rgdal, I  
get the following error message:

Error in dyn.load(x, as.logical(local), as.logical(now)) :
	unable to load shared library '/Users/rhead/Library/R/library/rgdal/ 
libs/ppc/rgdal.so':
   dlopen(/Users/rhead/Library/R/library/rgdal/libs/ppc/rgdal.so, 6):  
no suitable image found.  Did find:
	/Users/rhead/Library/R/library/rgdal/libs/ppc/rgdal.so: mach-o, but  
wrong architecture
Error: package/namespace load failed for 'rgdal'

Any help in fixing this problem would be appreciated.

Thanks,

Rhead


From torsten.mathies at matec-gmbh.com  Fri Sep 29 07:04:31 2006
From: torsten.mathies at matec-gmbh.com (Torsten Mathies)
Date: Fri, 29 Sep 2006 07:04:31 +0200
Subject: [R] Display figures within pareto chart
Message-ID: <001801c6e384$bff01b90$5a03a8c0@msc.de>

Dear all,
 
I would like to display the count of entries (the y-value)  placed above
each bar of a pareto chart. 
 
Greetings
 
Torsten


From ripley at stats.ox.ac.uk  Fri Sep 29 08:03:46 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 29 Sep 2006 07:03:46 +0100 (BST)
Subject: [R] Evaluation of defaults in functions
In-Reply-To: <f8e6ff050609281934r69a0e60fp99f16cef986d2a3c@mail.gmail.com>
References: <451C275D.50901@web.de>
	<971536df0609281432w332f786ase511e504ac35c269@mail.gmail.com>
	<x2hcyrmzwz.fsf@turmalin.kubism.ku.dk>
	<f8e6ff050609281934r69a0e60fp99f16cef986d2a3c@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0609290659190.15856@gannet.stats.ox.ac.uk>

On Thu, 28 Sep 2006, hadley wickham wrote:

>>
>> And, to answer the specific question: Yes, R has lazy evaluation,
>> everywhere. Arguments are always evaluated if and when they are
>> needed.
>>
>
> But doesn't R has a rather limited force of lazy evaluation? - you
> have no control over it, apart from that arguments are evaluated
> lazily.  This rather limited compared to other languages (no lazy
> lists etc)

I'd say that was a rather pure form.

However, Peter's statement is not quite true. R has several types of 
functions, and I think he is referring to closures, the functions written 
in R itself.  The argument handling in the built-in functions (primitive 
and .Internal) is different, and they will often evaluate all the 
arguments whether needed or not.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Fri Sep 29 08:18:00 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 29 Sep 2006 07:18:00 +0100 (BST)
Subject: [R] inserting columns in the middle of a dataframe
In-Reply-To: <efi2po$gl3$1@sea.gmane.org>
References: <C12E3F0A.425E5%timothy.c.bates@gmail.com>
	<007501c6d784$841fcf90$711f210a@gne.windows.gene.com>
	<efi2po$gl3$1@sea.gmane.org>
Message-ID: <Pine.LNX.4.64.0609290706160.15856@gannet.stats.ox.ac.uk>

On Thu, 28 Sep 2006, Joe W. Byers wrote:

> Berton Gunter wrote:
>> Please folks -- use indexing.
>>
>> myframe<-myframe[,c(1,5,2,3,4)]
>>
>> Which begs the question: why bother rearranging the columns anyway, since
>> one can get them used, printed, etc. in any order you wish anytime you want
>> just by specifying the indices in the order you want them. I suspect the
>> question was motivated by too much Sas- or Excel -ism.

> Many of the time series classes expect a date in the first column of the
> matrix or data.frame when creating the date-time object.  Retrieving
> data in a SQL query from a dB returns a character representation of the
> date that requires conversion to a date.  Performing this conversion is
> easy but inserting this converted date column is not straight forward.

Well-written R <--> DBMS software does return a date or date-time, and if 
it is the first column retrieved by other software, you want to _replace_ 
the _first_ column, not really relevant to the topic of your subject line.
(Doing that is basic data manipulation, covered in Chapter 2 of MASS4, for 
example.)

The initial assertion is not (necessarily) true of "ts" or "irts" or "its" 
or "zoo", so quite a few time-series class generators expect a date or 
date-time to be specified separately.


>>> -----Original Message-----
>>> From: r-help-bounces at stat.math.ethz.ch
>>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Timothy Bates
>>> Sent: Wednesday, September 13, 2006 3:05 PM
>>> To: Jon Minton; r-help at stat.math.ethz.ch
>>> Subject: Re: [R] inserting columns in the middle of a dataframe
>>>
>>>
>>>> Is there a built-in and simple way to insert new columns in
>>> a dataframe?
>>>
>>> You do this by collecting the columns in the new order you desire, and
>>> making a new frame.
>>>
>>> oldframe           <- data.frame(matrix(0:14,ncol=3))
>>> newcol              <- data.frame(20:24)
>>> names(newcol) <- "newcol"
>>> newframe         <- data.frame(c(oldframe[1],newcol, oldframe[2:3]))

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Roger.Bivand at nhh.no  Fri Sep 29 08:33:27 2006
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 29 Sep 2006 08:33:27 +0200 (CEST)
Subject: [R] Loading rgdal library
In-Reply-To: <E4EC56EB-1824-4B51-9CC6-610340DE2D68@yale.edu>
Message-ID: <Pine.LNX.4.44.0609290828360.2826-100000@reclus.nhh.no>

On Fri, 29 Sep 2006, Michael Rhead Enion wrote:

> I am trying to load the rgdal library in Mac OS X 10.4.7 (pismo g3  
> 500 mhz).

This is not an R-help question, rather for R-sig-mac and/or R-sig-geo. 
Please review the archives of those lists, plus comments on the 
rgdal package on the Rgeo (www.R-project.org/Rgeo) site first page and 
under "Maps", and if those do not resolve your problem, post to one of 
those lists, possibly also reporting how you solved the problem.

> 
> I already loaded sp  and compiled rgdal successfully, such that rgdal  
> shows up in the package manager GUI.  But when I try to load rgdal, I  
> get the following error message:
> 
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
> 	unable to load shared library '/Users/rhead/Library/R/library/rgdal/ 
> libs/ppc/rgdal.so':
>    dlopen(/Users/rhead/Library/R/library/rgdal/libs/ppc/rgdal.so, 6):  
> no suitable image found.  Did find:
> 	/Users/rhead/Library/R/library/rgdal/libs/ppc/rgdal.so: mach-o, but  
> wrong architecture
> Error: package/namespace load failed for 'rgdal'
> 
> Any help in fixing this problem would be appreciated.
> 
> Thanks,
> 
> Rhead
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From ripley at stats.ox.ac.uk  Fri Sep 29 08:48:55 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 29 Sep 2006 07:48:55 +0100 (BST)
Subject: [R] What is wrong with this input
In-Reply-To: <451C9E17.6090204@education.wisc.edu>
References: <451C9E17.6090204@education.wisc.edu>
Message-ID: <Pine.LNX.4.64.0609290745370.15856@gannet.stats.ox.ac.uk>

On Thu, 28 Sep 2006, David Kaplan wrote:

> I can't seem to figure out why I'm getting this error.  The output is
> copied right off the screen.  Notice how in some cases the back slash is
> missing.  In other cases, it can't read a file that I know is there.

This is rw-FAQ 2.16 and FAQ 7.8

Since you seem unfamiliar with the FAQs, their study will reward you.

> Thanks in advance
>
>
>
> > library(foreign)
> > hrout <- read.spss("c:\\hrab200.sav")
> Error in read.spss("c:\\hrab200.sav") : unable to open file
> > library(foreign)
> > hrout <- read.spss("C:\Documents and Settings\David Kaplan\My
> Documents\Baseball\Baseball Research\hr time series paper\new time
> series hr paper\hrab200.sav")
> Error in read.spss("C:Documents and SettingsDavid KaplanMy
> DocumentsBaseballBaseball Researchhr time series paper\new time series
> hr paperhrab200.sav") :
>         unable to open file
> > library(foreign)
> > hrout <- read.spss("C:\Documents and Settings\David Kaplan\My
> Documents\hrab200.sav"}
> Error: syntax error in "hrout <- read.spss("C:\Documents and
> Settings\David Kaplan\My Documents\hrab200.sav"}"
> > hrout <-read.spss("C:\\Documents and Settings\David Kaplan\My
> Documents\hrab200.sav")
> Error in read.spss("C:\\Documents and SettingsDavid KaplanMy
> Documentshrab200.sav") :
>         unable to open file
>
>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From justin_bem at yahoo.fr  Fri Sep 29 08:54:24 2006
From: justin_bem at yahoo.fr (justin bem)
Date: Fri, 29 Sep 2006 08:54:24 +0200 (CEST)
Subject: [R] RE :  What is wrong with this input
In-Reply-To: <451C9E17.6090204@education.wisc.edu>
Message-ID: <20060929065425.64755.qmail@web23004.mail.ird.yahoo.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20060929/5848d0b7/attachment.pl 

From i.helbig at unimelb.edu.au  Fri Sep 29 09:04:17 2006
From: i.helbig at unimelb.edu.au (Ingo)
Date: Fri, 29 Sep 2006 17:04:17 +1000
Subject: [R] problems with R and tckl/tk on Mac OS X
Message-ID: <E0E144EA-4A20-409C-BB77-2F9FFE331096@unimelb.edu.au>

Dear R-help team,
I am trying to run "R" on my Intel-based Mac.  I have installed R,  
X11 and Tcl/TK (I thought), but the GUI doesn't run.  My system  
administrator doesn't support R and therefore, I'm a little  
helpless.  What can I do?

Regards,

Ingo



From Soren.Hojsgaard at agrsci.dk  Fri Sep 29 09:06:04 2006
From: Soren.Hojsgaard at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Fri, 29 Sep 2006 09:06:04 +0200
Subject: [R] Automatical download of needed packages from CRAN
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC047E4DC5@DJFPOST01.djf.agrsci.dk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060929/b2da0410/attachment.pl 

From petr.pikal at precheza.cz  Fri Sep 29 09:07:38 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 29 Sep 2006 09:07:38 +0200
Subject: [R] What is wrong with this input
In-Reply-To: <451C9E17.6090204@education.wisc.edu>
Message-ID: <451CE25A.11389.4496B7@localhost>

Hi

>From Faq

7.8 How do file names work in Windows?
As R uses C-style string handling, \ is treated as an escape 
character, so that for example one can enter a newline as \n. When 
you really need a \, you have to escape it with another \. 

Thus, in filenames use something like "c:\\data\\money.dat". You can 
also replace \ by / ("c:/data/money.dat"). 

HTH
Petr

On 28 Sep 2006 at 23:16, David Kaplan wrote:

Date sent:      	Thu, 28 Sep 2006 23:16:23 -0500
From:           	David Kaplan <dkaplan at education.wisc.edu>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] What is wrong with this input

> I can't seem to figure out why I'm getting this error.  The output is
> copied right off the screen.  Notice how in some cases the back slash
> is missing.  In other cases, it can't read a file that I know is
> there.
> 
> Thanks in advance
> 
> 
> 
>  > library(foreign)
>  > hrout <- read.spss("c:\\hrab200.sav")
> Error in read.spss("c:\\hrab200.sav") : unable to open file
>  > library(foreign)
>  > hrout <- read.spss("C:\Documents and Settings\David Kaplan\My
> Documents\Baseball\Baseball Research\hr time series paper\new time
> series hr paper\hrab200.sav") Error in read.spss("C:Documents and
> SettingsDavid KaplanMy DocumentsBaseballBaseball Researchhr time
> series paper\new time series hr paperhrab200.sav") :
>          unable to open file
>  > library(foreign)
>  > hrout <- read.spss("C:\Documents and Settings\David Kaplan\My
> Documents\hrab200.sav"} Error: syntax error in "hrout <-
> read.spss("C:\Documents and Settings\David Kaplan\My
> Documents\hrab200.sav"}"
>  > hrout <-read.spss("C:\\Documents and Settings\David Kaplan\My
> Documents\hrab200.sav") Error in read.spss("C:\\Documents and
> SettingsDavid KaplanMy Documentshrab200.sav") :
>          unable to open file
> 
> 
> 
> -- 
> ======================================================================
> == David Kaplan, Ph.D. Professor Department of Educational Psychology
> University of Wisconsin - Madison Educational Sciences, Room 1061 1025
> W. Johnson Street Madison, WI 53706
> 
> email: dkaplan at education.wisc.edu
> Web:  
> http://www.education.wisc.edu/edpsych/facstaff/kaplan/kaplan.htm
> Phone: 608-262-0836 Fax:   608-262-0843
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From petr.pikal at precheza.cz  Fri Sep 29 09:10:42 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 29 Sep 2006 09:10:42 +0200
Subject: [R] Display figures within pareto chart
In-Reply-To: <001801c6e384$bff01b90$5a03a8c0@msc.de>
Message-ID: <451CE312.19875.476279@localhost>

Hi

why did not you do it?

On 29 Sep 2006 at 7:04, Torsten Mathies wrote:

From:           	"Torsten Mathies" <torsten.mathies at matec-gmbh.com>
To:             	<r-help at stat.math.ethz.ch>
Date sent:      	Fri, 29 Sep 2006 07:04:31 +0200
Subject:        	[R] Display figures within pareto chart

> Dear all,
> 
> I would like to display the count of entries (the y-value)  placed
> above each bar of a pareto chart. 
> 

Anyway, look at ?text.

HTH
Petr 


> Greetings
> 
> Torsten
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From petr.pikal at precheza.cz  Fri Sep 29 09:32:02 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 29 Sep 2006 09:32:02 +0200
Subject: [R] help on plots
In-Reply-To: <2fc17e30609281811m66c8d103tdee9131fff7ef377@mail.gmail.com>
References: <1159479369.4037.32.camel@localhost.localdomain>
Message-ID: <451CE812.25666.5AED96@localhost>

Hi

what is wrong with legend. How did you fail to add a legend

legend(2000, 0.8, legend=names(a)[2:5], pch=c(19,20,15,18), 
col=c(1,2,4,3))

works for me om Marc's example.

HTH
Petr


On 29 Sep 2006 at 9:11, zhijie zhang wrote:

Date sent:      	Fri, 29 Sep 2006 09:11:02 +0800
From:           	"zhijie zhang" <epistat at gmail.com>
To:             	mschwartz at mn.rr.com
Copies to:      	R-help at stat.math.ethz.ch
Subject:        	Re: [R] help on plots

> *Marc Schwartz ,*
> **  Method2 is what i need, and they are good answers. A little more
> question is how to add the legend to the plot? legend() may do it,but
> i fail to add them. Thanks again.
> 
> ----------------------------
> with kind regards
> zhijie zhang
> 
> On 9/29/06, Marc Schwartz (via MN) <mschwartz at mn.rr.com> wrote:
> >
> > On Thu, 2006-09-28 at 23:55 +0800, zhijie zhang wrote:
> > > Dear friends,
> > >  I met a problem on plotting.
> > > My dataset is :
> > > year    MHBC LHBC MHRC LURC
> > > 1993   11.75   4.50   0.43   0.46
> > > 1994    7.25   1.25   0.35   0.51
> > > 1995    8.67   2.17   0.54   0.44
> > > 1996   2.67   1.33   0.78   0.47
> > > 1997   3.42   4.92   0.69   0.48
> > > 1998   1.92   3.08   0.72   0.54
> > > 1999   2.33   2.58   0.74   0.41
> > > 2000   5.75   4.50   0.45   0.50
> > > 2001   3.75   4.42   0.52   0.47
> > > 2002   2.33   1.83   0.58   0.45
> > > 2003   0.25   2.83   0.50   0.39
> > > I want to get a plot -line with scatters, the requirement is :
> > > x-axis is year; two y-axis:
> > >   y1 corresponds to MHBC and LHBC;
> > >   y2 corresponds to MHRC and LURC;
> > > hope to use different symbols to differentiate the MHBC,LHBC,MHRC
> > and  LURC.
> > >
> > > The following is my program, but  very bad ,:
> > > *plot(a$year,a$MHBC,type='b')  #line1
> > > par(new=T)
> > > plot(a$year,a$LHBC,type='b')  #line2
> > > par(new=T)
> > > plot(a$year,a$MHRC,type='b')  #line3
> > > par(new=T)
> > > plot(a$year,a$LURC,type='b')   #line4
> > > axis(4, at=pretty(range(a$MHRC)))*
> > > In the figure, the labels and scales of X-axis are vague, the
> > > scale of y-axis is not very good. The better figure should be like
> > > the line1 and 2 are in the upper, and
> > line3
> > > and 4 are in the bottom.
> > > Any suggestion are welcome!
> >
> > It's not entirely clear to me what you want, so let me offer three
> > possibilities.
> >
> >
> > 1. Do all four lines in a single plot with a common y axis:
> >
> > matplot(a$year, a[, -1], type = "o", pch = 15:18)
> >
> >
> >
> > 2. Do all four lines in a single plot with the first two having a
> > separate left hand y axis and the second two having a separate right
> > hand y axis:
> >
> > # Draw the first pair of lines
> > matplot(a$year, a[, 2:3], type = "o", pch = c(19, 20),
> >        lty = "solid", ann = FALSE)
> >
> > # Get the current plot region boundaries
> > usr <- par("usr")
> >
> > # Get the range of the second set of columns
> > range.y2 <- range(a[, 4:5])
> >
> > # Change the plot region y axis range for the second
> > # set of columns. Extend them by 4% as per the default
> > par(usr = c(usr[1], usr[2],
> >            range.y2[1] * 0.96 , range.y2[2] * 1.04))
> >
> > # Add the second pair of lines
> > matlines(a$year, a[, 4:5], type = "o", pch = c(15, 18),
> >         lty = "dashed", col = c("blue", "green"))
> >
> > # Add the second y axis
> > axis(4)
> >
> >
> >
> > 3. Do the first two lines in an upper plot and the second two lines
> > in a lower plot, each has its own y axis range:
> >
> > # Set plot region to have two rows
> > par(mfrow = c(2, 1))
> >
> > # Adjust the plot margins
> > par(mar = c(2, 5, 2, 2))
> >
> > # Draw the first pair of lines
> > matplot(a$year, a[, 2:3], type = "o", pch = c(19, 20),
> >        lty = "solid", ylab = "First Pair")
> >
> >
> > par(mar = c(3, 5, 2, 2))
> >
> > # Add the second pair of lines
> > matplot(a$year, a[, 4:5], type = "o", pch = c(15, 18),
> >        lty = "dashed", col = c("blue", "green"),
> >        ylab = "Second Pair")
> >
> >
> >
> > See ?matplot, ?par and ?points for more information.
> >
> > HTH,
> >
> > Marc Schwartz
> >
> >
> >
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From antonio.fabio at gmail.com  Fri Sep 29 09:46:06 2006
From: antonio.fabio at gmail.com (Antonio, Fabio Di Narzo)
Date: Fri, 29 Sep 2006 09:46:06 +0200
Subject: [R] Automatical download of needed packages from CRAN
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC047E4DC5@DJFPOST01.djf.agrsci.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC047E4DC5@DJFPOST01.djf.agrsci.dk>
Message-ID: <b0808fdc0609290046r26dd1b3cyaa444da140416481@mail.gmail.com>

install.packages("foo", dep="Depends")

See online help. I think that the GUI version of install.packages
automatically does that.

Antonio.

2006/9/29, S?ren H?jsgaard <Soren.Hojsgaard a agrsci.dk>:
> I write a package foo which requires a package bar (from CRAN) to work. So in the DESCRIPTION file I write Depends: bar. I would like it to be so that when one installs foo, then it is automatically checked whether bar is installed, and if not then bar is also installed at the same time. I remember having seen packages which do so, but I can not figure out the mechanism
> 'Depends: bar' in the DESCRIPTION file does not solve the problem. Can anyone help?
>
> A related question: How to check programmatically that a specific package is loaded?
>
> Best regards
> S?ren
>
>         [[alternative HTML version deleted]]
>
>
>
> ______________________________________________
> R-help a stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


From ripley at stats.ox.ac.uk  Fri Sep 29 09:59:49 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 29 Sep 2006 08:59:49 +0100 (BST)
Subject: [R] Automatical download of needed packages from CRAN
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC047E4DC5@DJFPOST01.djf.agrsci.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC047E4DC5@DJFPOST01.djf.agrsci.dk>
Message-ID: <Pine.LNX.4.64.0609290856550.15856@gannet.stats.ox.ac.uk>

On Fri, 29 Sep 2006, S?ren H?jsgaard wrote:

> I write a package foo which requires a package bar (from CRAN) to work. 
> So in the DESCRIPTION file I write Depends: bar. I would like it to be 
> so that when one installs foo, then it is automatically checked whether 
> bar is installed, and if not then bar is also installed at the same 
> time. I remember having seen packages which do so, but I can not figure 
> out the mechanism 'Depends: bar' in the DESCRIPTION file does not solve 
> the problem. Can anyone help?

It does if you use install.packages(dependencies=TRUE), the default from 
the Windows GUI menu.

> A related question: How to check programmatically that a specific 
> package is loaded?

Via search(): there are lots of examples in the R help files.  ?termplot 
is one.

>
> Best regards
> S?ren
>
> 	[[alternative HTML version deleted]]
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From AnupTyagi at yahoo.com  Fri Sep 29 10:06:07 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Fri, 29 Sep 2006 13:36:07 +0530
Subject: [R] working with summarized data
In-Reply-To: <44F66F58.9070307@yahoo.com>
References: <F417F47A-2030-4135-BDB9-25913E7747AB@gmail.com>
	<44F66F58.9070307@yahoo.com>
Message-ID: <451CD3EF.1080902@yahoo.com>

Hi Rick,

I came across your posting that I had replied to. I had assumed from 
your posting that you had positive integer weights, and that you had a 
certain kind of stratified sampling. For a general case, you may want to 
look at "survey" package. Graphical representation of survey data, 
specially large surveys, is a good research issue in statistical 
graphics. R seems to be is suitable for doing this kind of work.

Anupam.

Anupam Tyagi wrote the following on 8/31/2006 10:40 AM:
> One solution is to simulate the population by repeating each row 
> "weight" number of times. This is inefficient. It may create a very 
> large dataset for a large sample survey. But some of graphs and other 
> things may turn out to your liking, depending upon how the functions are 
> written.
> 
> Anupam.
> 
> Rick Bischoff wrote the following on 8/30/2006 7:57 PM:
>> The data sets I am working with all have a weight variable--e.g.,  
>> each row doesn't mean 1 observation.
>>
>> With that in mind, nearly all of the graphs and summary statistics  
>> are incorrect for my data, because they don't take into account the  
>> weight.
>>
>> ****
>> For example "median" is incorrect, as the quantiles aren't calculated  
>> with weights:
>>
>> sum( weights[X < median(X)] ) / sum(weights)
>>
>> This should be 0.5... of course it's not.
>> ****
>>
>> Unfortunately, it seems that most(all?) of R's graphics and summary  
>> statistic functions don't take a weight or frequency argument.    
>> (Fortunately the models do...)
>>
>> Am I completely missing how to do this?  One way would be to  
>> replicate each row proportional to the weight (e.g. if the weight was  
>> 4, we would 3 additional copies) but this will get prohibitive pretty  
>> quickly as the dataset grows.
>>
>>
>> Thanks in advance!
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
>


From AnupTyagi at yahoo.com  Fri Sep 29 10:30:35 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Fri, 29 Sep 2006 08:30:35 +0000 (UTC)
Subject: [R] working with summarized data
References: <F417F47A-2030-4135-BDB9-25913E7747AB@gmail.com>
	<f8e6ff050608300800v1ecf59fq6f233676181474ac@mail.gmail.com>
	<F5867361-7417-4020-B3C0-C89DE185617E@gmail.com>
	<Pine.LNX.4.64.0608300958090.17647@homer23.u.washington.edu>
Message-ID: <loom.20060929T101535-616@post.gmane.org>

Thomas Lumley <tlumley <at> u.washington.edu> writes:

> 
> On Wed, 30 Aug 2006, Rick Bischoff wrote:
> 
> >>> Unfortunately, it seems that most(all?) of R's graphics and summary
> >>> statistic functions don't take a weight or frequency argument.
> >>> (Fortunately the models do...)
> >>
> >> I have been been meaning to add this functionality to my graphics
> >> package ggplot (http://had.co.nz/ggplot), but unfortunately haven't
> >> had time yet.  I'm guessing you want something like:
> >>
> >> * scatterplot: scale size of point according to weight (can do)
> >> * bar chart: bars should have height proportional to weight (can do)
> >> * histogram: area proportion to weighting variable (have some half
> >> finished code to do)
> >> * smoothers: should automatically use weights
> >> * boxplot: use weighted quantiles/letter statistics (is there a
> >> function for that?)
> >>
> >> What else is there?
> >
> > densityplot is the only other one I can think of at the moment...
> > With the rest of those, I could certainly live without it though!
> >
> 
> Density plots, scatterplot smoothers, hexbin plots, bubble plots, 
> histograms, and boxplots are available in the survey package. These are 
> probability-weighted rather than frequency-weighted but it doesn't matter 
> for graphics.  You could use them as is (which requires setting up a 
> survey design object) or rip the internals out of them.
> 
>  	-thomas
> 

I came across this posting that I had replied to earlier. I had assumed from the
original question that the data had positive integer weights, and that it had a
certain kind of stratified sampling. For a general case, "survey" package and
perhaps "ggplots" seem suitable to make these graphical extensions. "survey"
also takes into account survey design. I think graphical representation of
survey data,
specially large surveys, is a good research issue in statistical graphics. For
example, I am not convinced that making the area of a graphical symbol a
function of survey weight gives easily perceived and interpretable results: like
a bars in a bar-plot or histogram. Is there an implementation of graphical
functions that are conceptually similar to graphical respresntations of robust
statistics (which modify the "weights" of observations)? R seems to be suitable
for doing this kind of work.

Anupam.


From murdoch at stats.uwo.ca  Fri Sep 29 13:07:28 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 29 Sep 2006 07:07:28 -0400
Subject: [R] Evaluation of defaults in functions
In-Reply-To: <f8e6ff050609281934r69a0e60fp99f16cef986d2a3c@mail.gmail.com>
References: <451C275D.50901@web.de>	<971536df0609281432w332f786ase511e504ac35c269@mail.gmail.com>	<x2hcyrmzwz.fsf@turmalin.kubism.ku.dk>
	<f8e6ff050609281934r69a0e60fp99f16cef986d2a3c@mail.gmail.com>
Message-ID: <451CFE70.5030700@stats.uwo.ca>

On 9/28/2006 10:34 PM, hadley wickham wrote:
>> And, to answer the specific question: Yes, R has lazy evaluation,
>> everywhere. Arguments are always evaluated if and when they are
>> needed.
>>
> 
> But doesn't R has a rather limited force of lazy evaluation? - you
> have no control over it, apart from that arguments are evaluated
> lazily.  This rather limited compared to other languages (no lazy
> lists etc)

You do have more control than that.  You can't put a promise in a list, 
but you can put one in an environment, e.g.

 > x <- new.env()
 > y <- 1
 > delayedAssign("z", y, assign=x)
 > y <- 2
 > x$z
[1] 2

A few versions ago there was a delay() function that might have let you 
do this as

x$z <- delay(y)

but this doesn't really make sense:  since assignment is a function 
call, why wouldn't it force the evaluation of the promise?  The 
contortions necessary to work around this contradiction led to some 
strange errors, so delay was deprecated and is now defunct.  (I don't 
really know what the result of the assignment above would have been.)

Duncan Murdoch


From Christophe.Nguyen at bordeaux.inra.fr  Fri Sep 29 13:11:52 2006
From: Christophe.Nguyen at bordeaux.inra.fr (Christophe Nguyen)
Date: Fri, 29 Sep 2006 13:11:52 +0200
Subject: [R] PDE
In-Reply-To: <000301c6e25e$36ce22c0$7c94100a@win.ad.jhu.edu>
References: <000301c6e25e$36ce22c0$7c94100a@win.ad.jhu.edu>
Message-ID: <451CFF78.5010308@bordeaux.inra.fr>

Dear Ravi,
Many thanks for your help. I guess the PDE I am interested in is 
parabolic: it is a diffusion+advection equation: dc/dt=D d2c/dx2 + vdc/dx
Do you mean that I have to solve D d2c/dx2 + vdc/dx=0 for  each time 
step, taking as initial condition at step n+1 the value of c at step n?
Does the ODE package sollve second order differential equation?
Best regards,
Chris

Ravi Varadhan wrote:
> Hi Christophe,
>
> What is the PDE that you are trying to solve?  Is it
> parabolic/hyperbolic/elliptical/somethingelse?  Is it linear/nonlinear?  
>
> If time is one of the independent variables, you can transform the PDE into
> an initial value problem (system of ODEs) by using finite difference
> approximations of the partial derivatives of other independent variables
> (typically, these are spatial coordinates).  Starting with an initial set of
> values on a grid of points (also known as initial conditions, which are part
> of the problem specification), you update them at different times, using
> fixed or varying time steps.
>
> R has very limited functionality for handling differential equations.  So,
> you should look for FORTRAN libraries, from which you can create DLLs to be
> used in R.
>
> Hope this help,
> Ravi.
>
> ----------------------------------------------------------------------------
> -------
>
> Ravi Varadhan, Ph.D.
>
> Assistant Professor, The Center on Aging and Health
>
> Division of Geriatric Medicine and Gerontology 
>
> Johns Hopkins University
>
> Ph: (410) 502-2619
>
> Fax: (410) 614-9625
>
> Email: rvaradhan at jhmi.edu
>
> Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html
>
>  
>
> ----------------------------------------------------------------------------
> --------
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Christophe Nguyen
> Sent: Wednesday, September 27, 2006 10:39 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] PDE
>
> Dear all,
> Does any know how to solve PDE with R? The archive list refers to the 
> use of ODE if PDE are parabolic. I am not a mathematician and this does 
> not mean anything for me!
> help would be very appreciated.
> Many thanks
>
>   

-- 
___________________________________________________

Christophe NGUYEN

UMR 1220 INRA-ENITAB
Transfert sol-plante et cycle des ?l?ments min?raux
dans les ?cosyst?mes cultiv?s"

Centre INRA de Bordeaux-Aquitaine
71, avenue Edouard Bourlaux, BP 81
33883 Villenave d'Ornon, FRANCE

Tel : 00 33 (0)5 57 12 25 07
Fax : 00 33 (0)5 57 12 25 15

email : Christophe.Nguyen at bordeaux.inra.fr
page infoservice: http://www.bordeaux.inra.fr/tcem

__________m?O?m____________________________________


From ligges at statistik.uni-dortmund.de  Fri Sep 29 14:24:10 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 29 Sep 2006 14:24:10 +0200
Subject: [R] complex plots using layout()
In-Reply-To: <6E031567AB9AD24FA98F173673F0708F824A77@boa.edc3.pitt.edu>
References: <6E031567AB9AD24FA98F173673F0708F824A77@boa.edc3.pitt.edu>
Message-ID: <451D106A.6030308@statistik.uni-dortmund.de>



Lu, Jiang Jane wrote:
> Dear r-help,
> 
> I am trying to plot several scatter plots with marginal histograms on
> one page. Ideally, a page is equally divided into 4 figure regions.
> Within each figure region, a scatter plot with marginal histograms will
> be plotted.
> 
> I followed Dr. Paul Murrell's code released online to successfully plot
> the scatter plot with marginal histograms. The code applies "layout()"
> to partition the page.
> 
> Right now, I want each of the 4 figure regions on one page to be plotted
> a scatter plot with marginal histograms. I tried par(mfrow= ) ahead of
> layout(). It does not work. Could I repeat layout() to reach my point?

No, nested layout(), par(mfrow) and theirlike are not supported, but 
there is the other R graphics system in package grid which supports 
stuff like that extraordinary well. See For example Paul's book on "R 
Graphics".

Uwe Ligges



> Following is the code I use. Any advice is greatly appreciated.
> 
> =================================================================
> x <- demog$age
> y1 <- demog$mji
> y2 <- demog$nles
> xhist <- hist(x,  plot=FALSE)
> y1hist <- hist(y1,  plot=FALSE)
> y2hist <- hist(y2,  plot=FALSE)
> 
> top1 <- max(c(xhist$counts, y1hist$counts))
> top2 <- max(c(xhist$counts, y2hist$counts))
> 
> xrange <- range(x,na.rm=TRUE)
> y1range <- range(y1,na.rm=TRUE)
> y2range <- range(y2,na.rm=TRUE)
> 
> def.par <- par(no.readonly = TRUE)
> 
> par(mfrow=c(2,2))
> 
> nf <- layout(matrix(c(2,0,1,3),2,2,byrow=TRUE), c(3,1), c(1,3), TRUE)
> 
> par(mar=c(3,3,1,1))
> plot(x, y1, xlim=xrange, ylim=y1range, xlab="Age", ylab="MJI")
> lines(lowess(x,y1),col=2)
> par(mar=c(0,3,1,1))
> barplot(xhist$counts, axes=FALSE, ylim=c(0, top1), space=0)
> par(mar=c(3,0,1,1))
> barplot(y1hist$counts, axes=FALSE, xlim=c(0, top1), space=0, horiz=TRUE)
> 
> par(mar=c(3,3,1,1))
> plot(x, y2, xlim=xrange, ylim=y2range, xlab="Age", ylab="Numer of
> Lesions")
> lines(lowess(x,y2),col=2)
> par(mar=c(0,3,1,1))
> barplot(xhist$counts, axes=FALSE, ylim=c(0, top2), space=0)
> par(mar=c(3,0,1,1))
> barplot(y2hist$counts, axes=FALSE, xlim=c(0, top2), space=0, horiz=TRUE)
> 
> 
> par(def.par)
> 
> ================================================================ 
> 
> 
> Sincerely yours,
> 
> Jiang Lu
> 
> Statistician
> 
> University of Pittsburgh
> 130 DeSoto Street, 127 Parran Hall
> Pittsburgh, PA 15261
> USA
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From savdekar at hotmail.com  Fri Sep 29 14:53:08 2006
From: savdekar at hotmail.com (Pankaj Savdekar)
Date: Fri, 29 Sep 2006 18:23:08 +0530
Subject: [R] Build error on Windows
Message-ID: <BAY126-F30B59DA3C1B693C977EA31D2180@phx.gbl>

Hi,

I'm trying to build R-2.3.1 on windows, but make gives me following error 
while building pkg-base:
---------- Making package base ------------
  adding build stamp to DESCRIPTION
make[4]: *** [frontmatter] Error 1
make[3]: *** [all] Error 2
make[2]: *** [pkg-base] Error 2
make[1]: *** [rpackage] Error 2
make: *** [all] Error 2

Please note that R.exe, Rterm.exe, Rgui.exe, RCmd.exe are build without any 
errors.

I have three questions, can anyone please help me to resolve it?
1. How to solve (or get more details) of the above mentioned error?
2. When I run Rterm.exe it takes 25-30% CPU time, but doesn't provide me the 
command prompt, is there anyway to log/trace, to know the issues in Rterm?
3. Despite error mentioned above (Frontmatter) I tried building other 
packages and was able to build few. But still RGui.exe shows warning saying 
there is no package called 'datasets', 'utils' etc. Can someone please point 
me how to resolve this issue?

Thanks in advance.

Pankaj


From rvaradhan at jhmi.edu  Fri Sep 29 15:42:36 2006
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Fri, 29 Sep 2006 09:42:36 -0400
Subject: [R] PDE
In-Reply-To: <451CFF78.5010308@bordeaux.inra.fr>
Message-ID: <000301c6e3cd$200e3280$7c94100a@win.ad.jhu.edu>

No, you don't solve D d2c/dx2 + v dc/dx = 0.

You discretize the spatial derivatives using finite differences (typically,
central difference), at a given time.  Let us say x goes from 0 to 1.
Divide this into intervals of length delta x, such that x(i+1) = x (i) +
delta x, i = 0, ..., N-1.  Let c(i,t) denote the concentration at x(i) at
time t.  So you now have:

dc(i,t)/dt = D (c(i+1,t) - 2 c(i,t) + c(i-1,t)) / (delta x) ^2 + v (c(i+1,t)
-  c(i-1,t)) / (2 * delta x), for i = 1, ..., N.

The above is a coupled system of N equations, with only time t as the
independent variable.  You can solve this system using any of the methods
for initial value problem, e.g. Euler's or classical 4-th order Runge-Kuttta
(you can use the Odesolve package in R for this task).  The initial values
c(i,t=0) must be specified as an input, as well as the boundary conditions
at c(x=0,t) and c(x=1,t) for all t.

Hope this is clear.  You should also consult a basic numerical analysis
text, for example, Burden and Faires (2001, 7th Edition, pages 704 - ...).

Best,
Ravi.

----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------

-----Original Message-----
From: Christophe Nguyen [mailto:Christophe.Nguyen at bordeaux.inra.fr] 
Sent: Friday, September 29, 2006 7:12 AM
To: Ravi Varadhan
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] PDE

Dear Ravi,
Many thanks for your help. I guess the PDE I am interested in is 
parabolic: it is a diffusion+advection equation: dc/dt=D d2c/dx2 + vdc/dx
Do you mean that I have to solve D d2c/dx2 + vdc/dx=0 for  each time 
step, taking as initial condition at step n+1 the value of c at step n?
Does the ODE package sollve second order differential equation?
Best regards,
Chris

Ravi Varadhan wrote:
> Hi Christophe,
>
> What is the PDE that you are trying to solve?  Is it
> parabolic/hyperbolic/elliptical/somethingelse?  Is it linear/nonlinear?  
>
> If time is one of the independent variables, you can transform the PDE
into
> an initial value problem (system of ODEs) by using finite difference
> approximations of the partial derivatives of other independent variables
> (typically, these are spatial coordinates).  Starting with an initial set
of
> values on a grid of points (also known as initial conditions, which are
part
> of the problem specification), you update them at different times, using
> fixed or varying time steps.
>
> R has very limited functionality for handling differential equations.  So,
> you should look for FORTRAN libraries, from which you can create DLLs to
be
> used in R.
>
> Hope this help,
> Ravi.
>
>
----------------------------------------------------------------------------
> -------
>
> Ravi Varadhan, Ph.D.
>
> Assistant Professor, The Center on Aging and Health
>
> Division of Geriatric Medicine and Gerontology 
>
> Johns Hopkins University
>
> Ph: (410) 502-2619
>
> Fax: (410) 614-9625
>
> Email: rvaradhan at jhmi.edu
>
> Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html
>
>  
>
>
----------------------------------------------------------------------------
> --------
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Christophe Nguyen
> Sent: Wednesday, September 27, 2006 10:39 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] PDE
>
> Dear all,
> Does any know how to solve PDE with R? The archive list refers to the 
> use of ODE if PDE are parabolic. I am not a mathematician and this does 
> not mean anything for me!
> help would be very appreciated.
> Many thanks
>
>   

-- 
___________________________________________________

Christophe NGUYEN

UMR 1220 INRA-ENITAB
Transfert sol-plante et cycle des ?l?ments min?raux
dans les ?cosyst?mes cultiv?s"

Centre INRA de Bordeaux-Aquitaine
71, avenue Edouard Bourlaux, BP 81
33883 Villenave d'Ornon, FRANCE

Tel : 00 33 (0)5 57 12 25 07
Fax : 00 33 (0)5 57 12 25 15

email : Christophe.Nguyen at bordeaux.inra.fr
page infoservice: http://www.bordeaux.inra.fr/tcem

__________m?O?m____________________________________


From alex at transitive.com  Fri Sep 29 15:47:55 2006
From: alex at transitive.com (Alex Brown)
Date: Fri, 29 Sep 2006 14:47:55 +0100
Subject: [R] problems with R and tckl/tk on Mac OS X
In-Reply-To: <E0E144EA-4A20-409C-BB77-2F9FFE331096@unimelb.edu.au>
References: <E0E144EA-4A20-409C-BB77-2F9FFE331096@unimelb.edu.au>
Message-ID: <C1025C3C-C1A7-4A88-A6C0-66EDF0590E80@transitive.com>

I assume you are running MacOS X 10.4?

The Mac version of R does not require X11 or Tcl/TK since it uses a  
separate GUI interface developed exclusively for the Mac using native  
widgets and a quartz based graphing mechanism.

Try downloading the following binary installer which includes the gui  
interface:

http://cran.r-project.org/bin/macosx/R-2.3.1.dmg

-Alex Brown

On 29 Sep 2006, at 08:04, Ingo wrote:


> Dear R-help team,
> I am trying to run "R" on my Intel-based Mac.  I have installed R,  
> X11 and Tcl/TK (I thought), but the GUI doesn't run.  My system  
> administrator doesn't support R and therefore, I'm a little  
> helpless.  What can I do?
>
> Regards,
>
> Ingo
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


On 29 Sep 2006, at 08:04, Ingo wrote:

> Dear R-help team,
> I am trying to run "R" on my Intel-based Mac.  I have installed R,  
> X11 and Tcl/TK (I thought), but the GUI doesn't run.  My system  
> administrator doesn't support R and therefore, I'm a little  
> helpless.  What can I do?
>
> Regards,
>
> Ingo
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From h.wickham at gmail.com  Fri Sep 29 16:12:01 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Fri, 29 Sep 2006 09:12:01 -0500
Subject: [R] Evaluation of defaults in functions
In-Reply-To: <451CFE70.5030700@stats.uwo.ca>
References: <451C275D.50901@web.de>
	<971536df0609281432w332f786ase511e504ac35c269@mail.gmail.com>
	<x2hcyrmzwz.fsf@turmalin.kubism.ku.dk>
	<f8e6ff050609281934r69a0e60fp99f16cef986d2a3c@mail.gmail.com>
	<451CFE70.5030700@stats.uwo.ca>
Message-ID: <f8e6ff050609290712o643ad40es2b3248dbd74cb318@mail.gmail.com>

> > But doesn't R has a rather limited force of lazy evaluation? - you
> > have no control over it, apart from that arguments are evaluated
> > lazily.  This rather limited compared to other languages (no lazy
> > lists etc)
>
> You do have more control than that.  You can't put a promise in a list,
> but you can put one in an environment, e.g.
>
>  > x <- new.env()
>  > y <- 1
>  > delayedAssign("z", y, assign=x)
>  > y <- 2
>  > x$z
> [1] 2

That's interesting.  Is it possible to treat an environment like a
list in most situations?

I had experimented with lazy lists/streams in R a while go (as they
seem like a very natural way of dealing with datasets that are too
large to fit in memory) but got stuck.  This might let me get a little
further.

Hadley


From ggrothendieck at gmail.com  Fri Sep 29 16:37:33 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 29 Sep 2006 10:37:33 -0400
Subject: [R] Evaluation of defaults in functions
In-Reply-To: <f8e6ff050609290712o643ad40es2b3248dbd74cb318@mail.gmail.com>
References: <451C275D.50901@web.de>
	<971536df0609281432w332f786ase511e504ac35c269@mail.gmail.com>
	<x2hcyrmzwz.fsf@turmalin.kubism.ku.dk>
	<f8e6ff050609281934r69a0e60fp99f16cef986d2a3c@mail.gmail.com>
	<451CFE70.5030700@stats.uwo.ca>
	<f8e6ff050609290712o643ad40es2b3248dbd74cb318@mail.gmail.com>
Message-ID: <971536df0609290737m653b349w621a118fe5375558@mail.gmail.com>

On 9/29/06, hadley wickham <h.wickham at gmail.com> wrote:
> > > But doesn't R has a rather limited force of lazy evaluation? - you
> > > have no control over it, apart from that arguments are evaluated
> > > lazily.  This rather limited compared to other languages (no lazy
> > > lists etc)
> >
> > You do have more control than that.  You can't put a promise in a list,
> > but you can put one in an environment, e.g.
> >
> >  > x <- new.env()
> >  > y <- 1
> >  > delayedAssign("z", y, assign=x)
> >  > y <- 2
> >  > x$z
> > [1] 2
>
> That's interesting.  Is it possible to treat an environment like a
> list in most situations?
>
> I had experimented with lazy lists/streams in R a while go (as they
> seem like a very natural way of dealing with datasets that are too
> large to fit in memory) but got stuck.  This might let me get a little
> further.
>

Check out the g.data package.


From murdoch at stats.uwo.ca  Fri Sep 29 16:50:51 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 29 Sep 2006 10:50:51 -0400
Subject: [R] Evaluation of defaults in functions
In-Reply-To: <f8e6ff050609290712o643ad40es2b3248dbd74cb318@mail.gmail.com>
References: <451C275D.50901@web.de>	
	<971536df0609281432w332f786ase511e504ac35c269@mail.gmail.com>	
	<x2hcyrmzwz.fsf@turmalin.kubism.ku.dk>	
	<f8e6ff050609281934r69a0e60fp99f16cef986d2a3c@mail.gmail.com>	
	<451CFE70.5030700@stats.uwo.ca>
	<f8e6ff050609290712o643ad40es2b3248dbd74cb318@mail.gmail.com>
Message-ID: <451D32CB.2040307@stats.uwo.ca>

On 9/29/2006 10:12 AM, hadley wickham wrote:
>> > But doesn't R has a rather limited force of lazy evaluation? - you
>> > have no control over it, apart from that arguments are evaluated
>> > lazily.  This rather limited compared to other languages (no lazy
>> > lists etc)
>>
>> You do have more control than that.  You can't put a promise in a list,
>> but you can put one in an environment, e.g.
>>
>>  > x <- new.env()
>>  > y <- 1
>>  > delayedAssign("z", y, assign=x)
>>  > y <- 2
>>  > x$z
>> [1] 2
> 
> That's interesting.  Is it possible to treat an environment like a
> list in most situations?

There are some important differences:

Lists are regular R objects, which are copied when passed as args to 
functions.  Environments are passed by reference.

Lists have an ordering, and don't need names.  Environments only contain 
named things, and the order in which objects were put into them is not 
retained.

Lists do partial matching on named args, environments need full 
matching.  For example:

 > x <- list(abc=1)
 > x$a
[1] 1
 > y <- new.env()
 > y$abc <- 1
 > y$a
NULL
 > y$abc
[1] 1

Environments have enclosing environments, lists don't.  The $ operator 
doesn't search the enclosure, but get() does:

 > y$mean
NULL
 > get("mean", y)
function (x, ...)
UseMethod("mean")
<environment: namespace:base>

Set the parent of the environment to emptyenv() if you don't want this. 
  By default it's set to the current evaluation environment.

 > z <- new.env(parent=emptyenv())
 > get("mean", z)
Error in get(x, envir, mode, inherits) : variable "mean" was not found

Duncan Murdoch


From murdoch at stats.uwo.ca  Fri Sep 29 17:00:51 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 29 Sep 2006 11:00:51 -0400
Subject: [R] Build error on Windows
In-Reply-To: <BAY126-F30B59DA3C1B693C977EA31D2180@phx.gbl>
References: <BAY126-F30B59DA3C1B693C977EA31D2180@phx.gbl>
Message-ID: <451D3523.7020703@stats.uwo.ca>

On 9/29/2006 8:53 AM, Pankaj Savdekar wrote:
> Hi,
> 
> I'm trying to build R-2.3.1 on windows, but make gives me following error 
> while building pkg-base:
> ---------- Making package base ------------
>   adding build stamp to DESCRIPTION
> make[4]: *** [frontmatter] Error 1
> make[3]: *** [all] Error 2
> make[2]: *** [pkg-base] Error 2
> make[1]: *** [rpackage] Error 2
> make: *** [all] Error 2
> 
> Please note that R.exe, Rterm.exe, Rgui.exe, RCmd.exe are build without any 
> errors.
> 
> I have three questions, can anyone please help me to resolve it?
> 1. How to solve (or get more details) of the above mentioned error?

You need to look through the make files, to see what was happening. 
Reading the messages in reverse order:  "make all" called "make 
rpackage" and so on to "make frontmatter".  The errors don't tell you 
which makefiles these are in, but the "frontmatter" target only occurs 
in src/gnuwin32/MakePkg.  You could try deleting the "@" signs from the 
lines for that target to see exactly what was happening when the error 
was generated.

I'd guess that this is happening because your build is messed up:  the 
base package is used in later build steps.  If you start from a clean 
checkout and just call "make", you probably won't see this.


> 2. When I run Rterm.exe it takes 25-30% CPU time, but doesn't provide me the 
> command prompt, is there anyway to log/trace, to know the issues in Rterm?
> 3. Despite error mentioned above (Frontmatter) I tried building other 
> packages and was able to build few. But still RGui.exe shows warning saying 
> there is no package called 'datasets', 'utils' etc. Can someone please point 
> me how to resolve this issue?

Probably the same cause.  base is messed up, and that's going to cause 
all sorts of trouble.

Duncan Murdoch
> 
> Thanks in advance.
> 
> Pankaj
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jfox at mcmaster.ca  Fri Sep 29 17:04:08 2006
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 29 Sep 2006 11:04:08 -0400
Subject: [R] problems with R and tckl/tk on Mac OS X
In-Reply-To: <C1025C3C-C1A7-4A88-A6C0-66EDF0590E80@transitive.com>
Message-ID: <20060929150407.DPBA13241.tomts10-srv.bellnexxia.net@JohnDesktop8300>

Dear Alex,

It's possible, though not clear from the original posting, that Ingo is
trying to install the Rcmdr package. If that's the case, then he might find
the installation instructions for Mac users at
<http://socserv.socsci.mcmaster.ca/jfox/Misc/Rcmdr/installation-notes.html>
helpful.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Alex Brown
> Sent: Friday, September 29, 2006 8:48 AM
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] problems with R and tckl/tk on Mac OS X
> 
> I assume you are running MacOS X 10.4?
> 
> The Mac version of R does not require X11 or Tcl/TK since it 
> uses a separate GUI interface developed exclusively for the 
> Mac using native widgets and a quartz based graphing mechanism.
> 
> Try downloading the following binary installer which includes the gui
> interface:
> 
> http://cran.r-project.org/bin/macosx/R-2.3.1.dmg
> 
> -Alex Brown
> 
> On 29 Sep 2006, at 08:04, Ingo wrote:
> 
> 
> > Dear R-help team,
> > I am trying to run "R" on my Intel-based Mac.  I have installed R,
> > X11 and Tcl/TK (I thought), but the GUI doesn't run.  My system 
> > administrator doesn't support R and therefore, I'm a little 
> helpless.  
> > What can I do?
> >
> > Regards,
> >
> > Ingo
> >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting- 
> > guide.html and provide commented, minimal, self-contained, 
> > reproducible code.
> >
> 
> 
> On 29 Sep 2006, at 08:04, Ingo wrote:
> 
> > Dear R-help team,
> > I am trying to run "R" on my Intel-based Mac.  I have installed R,
> > X11 and Tcl/TK (I thought), but the GUI doesn't run.  My system 
> > administrator doesn't support R and therefore, I'm a little 
> helpless.  
> > What can I do?
> >
> > Regards,
> >
> > Ingo
> >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting- 
> > guide.html and provide commented, minimal, self-contained, 
> > reproducible code.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From luke at stat.uiowa.edu  Fri Sep 29 17:12:44 2006
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Fri, 29 Sep 2006 10:12:44 -0500 (CDT)
Subject: [R] Evaluation of defaults in functions
In-Reply-To: <Pine.LNX.4.64.0609290659190.15856@gannet.stats.ox.ac.uk>
References: <451C275D.50901@web.de>
	<971536df0609281432w332f786ase511e504ac35c269@mail.gmail.com>
	<x2hcyrmzwz.fsf@turmalin.kubism.ku.dk>
	<f8e6ff050609281934r69a0e60fp99f16cef986d2a3c@mail.gmail.com>
	<Pine.LNX.4.64.0609290659190.15856@gannet.stats.ox.ac.uk>
Message-ID: <Pine.LNX.4.64.0609290952150.11669@nokomis.stat.uiowa.edu>

On Fri, 29 Sep 2006, Prof Brian Ripley wrote:

> On Thu, 28 Sep 2006, hadley wickham wrote:
>
>>>
>>> And, to answer the specific question: Yes, R has lazy evaluation,
>>> everywhere. Arguments are always evaluated if and when they are
>>> needed.
>>>
>>
>> But doesn't R has a rather limited force of lazy evaluation? - you
>> have no control over it, apart from that arguments are evaluated
>> lazily.  This rather limited compared to other languages (no lazy
>> lists etc)
>
> I'd say that was a rather pure form.

There are not all that many other languages that use lazy evaluation.
Those that do are for the most part pure or nearly pure functional
languages--Haskell is probably the main example.  These go much
further in their use of lazy evaluation than R.  For analogs of the R
expressions

         x <- f(x)
 	list(f(x))
         x + f(x)

only the last one is guaranteed to result in f being called. This
makes many things conceptually cleaner and also automatically supports
lazy data structures, which allows one to express things like infinite
sequences.  Some of the downsides are a more complex (or at least
very different) implementation and that I becomes very hard to reason
about performance.  One of the reasons performance is hard to sort out
is that it is (deliberately, because of the declarative nature of
these languages) hard to know exactly when an evaluation will occur.

Contrary to Hadley's comment R actually provides quite a lot of control since
assignments are guaranteed to cause evaluation, e.g. in

     g <- function(x) {
         x <- x # forces evaluation
         ...
     }

the argument is guaranteed to be evaluated.  The function force()
makes this a little more readable (avoids the need for a comment).

> However, Peter's statement is not quite true. R has several types of
> functions, and I think he is referring to closures, the functions written
> in R itself.  The argument handling in the built-in functions (primitive
> and .Internal) is different, and they will often evaluate all the
> arguments whether needed or not.

I think Peter's statement is OK with respect to evaluation since you
can think of `+` as being implemented as

     `+` <- function(x,y) {
         force(x)
         force(y)
         ... internal code to add x, y ...
     }

so y would be guaranteed to be evaluated even if the value of x would
cause the internal code to throw an error, for example (though not if
evaluation of x causes an error).  Builtins are of course different in
that they don't do argument matching, but that is another issue.

Lazy data structures can be implemented in R on top of the limited
lazy evaluation mechanism.  I experimented with this for fun a one
point.  Some code is in http://www.stat.uiowa.edu/~luke/R/lazy/.  This
is out of date but not too hard to fix.  Insuring memory efficiency is
still a bit tricky; some comments are int he notes at this site.

Best,

luke

-- 
Luke Tierney
Chair, Statistics and Actuarial Science
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu


From spencer.graves at pdf.com  Fri Sep 29 17:30:34 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 29 Sep 2006 10:30:34 -0500
Subject: [R] plotting all subgroups with augPred
In-Reply-To: <6BCB4D493A447546A8126F24332056E8041C5599@school1.business.edu>
References: <6BCB4D493A447546A8126F24332056E8041C5599@school1.business.edu>
Message-ID: <451D3C1A.9080002@pdf.com>

      You want an 'augPred' plot with multiple lines per panel, from 
multiple factors in the model?  Because you provided such a simple, 
self-contained example, I can offer much more substantive comments than 
I might have otherwise.  First the bad news:  I don't see an easy way to 
get what you want.  Good news:  I think I can outline for you a 
moderately simple but still not trivial way to get it -- though I 
haven't completed the exercise myself. 

      The 'augPred' documentation includes an example with two lines per 
panel, but one of the lines is just the 'fixed' model, which is the same 
in all frames.  That's not what you want. 

      To dig deeper, I typed 'augPred' at a command prompt:  It's a one 
line function consisting of 'UseMethod("augPred")'.  This pushed me to 
do the following: 

 > methods("augPred")
[1] augPred.gls*    augPred.lme*    augPred.lmList*

      This led me to try "getAnywhere(augPred.lme)", which listed out 
the function.  I decided I wanted to walk through the function line by 
line, so I tried the following: 

 > debug(augPred.lme)
Error: object "augPred.lme" not found

      Since 'augPred' is in library(nlme), I refined this as follows: 

 > debug(nlme:::augPred.lme)

      This worked.  Next I tried to execute your command 
'augPred(fm1Pixel)', which put me into 'augPred.lme'.  From there, one 
can walk through the function line by line, looking at what they do, 
etc.  Later, you can do the execute your own modification to that code 
outside of a call to 'augPred'.  If you get 'object ... not found', try 
adding 'nlme:::' as a prefix to '...'.  If you do this, you will find 
that 'augPred' basically does three things: 

      1.  Creates a data.frame 'value' containing the explanatory 
variables for which predictions are needed.  You need to add a column 
for 'Side' to 'value';  I don't see a way to do this in the function call. 

      2.  Call 'pred <- predict(...)' to get the numbers required for 
the lines. 

      3.  Reorganize things so 'plot.augPred' knows what to do. 

      After you get 'pred' with all the numbers you need, you can plot 
them any way you want. 

      Hope this helps. 
      Spencer Graves

Afshartous, David wrote:
> All,
>
> I have a question RE plotting the prediction lines of a random effects
> model via augPred.  I'll illustrate via the Pixel dataset within the
> nlme package: 
>
> library(nlme)
> attach(Pixel)
> fm1Pixel = lme(pixel ~ day + I(day^2), data = Pixel, random = list(Dog =
> ~ 1))
> plot(augPred(fm1Pixel))   ### 10 fitted lines since there are 10 dogs
>
> fm2Pixel = update(fm1Pixel, . ~ . + Side)
> plot(augPred(fm2Pixel))    ## also produces 10 fitted lines
>
> For the second plot, shouldn't we have 2 fitted lines per dog, one for
> each level
> of the fixed effect Side?  
>
> 1) How does one plot insure that this is plotted accordingly?
>
> 2) How does one plot say only the first 5 dogs?
>
>
> Thanks!
> Dave
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From h.wickham at gmail.com  Fri Sep 29 17:34:59 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Fri, 29 Sep 2006 10:34:59 -0500
Subject: [R] Evaluation of defaults in functions
In-Reply-To: <Pine.LNX.4.64.0609290952150.11669@nokomis.stat.uiowa.edu>
References: <451C275D.50901@web.de>
	<971536df0609281432w332f786ase511e504ac35c269@mail.gmail.com>
	<x2hcyrmzwz.fsf@turmalin.kubism.ku.dk>
	<f8e6ff050609281934r69a0e60fp99f16cef986d2a3c@mail.gmail.com>
	<Pine.LNX.4.64.0609290659190.15856@gannet.stats.ox.ac.uk>
	<Pine.LNX.4.64.0609290952150.11669@nokomis.stat.uiowa.edu>
Message-ID: <f8e6ff050609290834y4a54c86dke224874cecbf7f60@mail.gmail.com>

> There are not all that many other languages that use lazy evaluation.
> Those that do are for the most part pure or nearly pure functional
> languages--Haskell is probably the main example.  These go much
> further in their use of lazy evaluation than R.  For analogs of the R
> expressions
>
>          x <- f(x)
>         list(f(x))
>          x + f(x)
>
> only the last one is guaranteed to result in f being called. This
> makes many things conceptually cleaner and also automatically supports
> lazy data structures, which allows one to express things like infinite
> sequences.  Some of the downsides are a more complex (or at least
> very different) implementation and that I becomes very hard to reason
> about performance.  One of the reasons performance is hard to sort out
> is that it is (deliberately, because of the declarative nature of
> these languages) hard to know exactly when an evaluation will occur.

You can argue similarly that adding state (eg. functions with side
effects) increases the complexity of reasoning about the result of a
calculcation.


> Contrary to Hadley's comment R actually provides quite a lot of control since
> assignments are guaranteed to cause evaluation, e.g. in
>
>      g <- function(x) {
>          x <- x # forces evaluation
>          ...
>      }
>
> the argument is guaranteed to be evaluated.  The function force()
> makes this a little more readable (avoids the need for a comment).

Perhaps control wasn't the word I was looking for.  Most lazy
languages provide some way to force evaluation.  Assignments forcing
evaluation seems to be a disadvantage to me - I can't do anything with
a promise apart from evaluate it.

> Lazy data structures can be implemented in R on top of the limited
> lazy evaluation mechanism.  I experimented with this for fun a one
> point.  Some code is in http://www.stat.uiowa.edu/~luke/R/lazy/.  This
> is out of date but not too hard to fix.  Insuring memory efficiency is
> still a bit tricky; some comments are int he notes at this site.

Thanks, I will take a look at that.

Regards,

Hadley


From spencer.graves at pdf.com  Fri Sep 29 17:49:40 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 29 Sep 2006 10:49:40 -0500
Subject: [R] EBAM ERROR
In-Reply-To: <30d556050609141647g385ad1a2qa722fc8c8cbacf1b@mail.gmail.com>
References: <30d556050609141647g385ad1a2qa722fc8c8cbacf1b@mail.gmail.com>
Message-ID: <451D4094.6000900@pdf.com>

      I haven't seen a reply to this post, so I will offer a couple of 
comments.  Unfortunately, the information provided is not sufficient for 
me to answer your questions.  If you'd still like help from this 
listserve, please provide commented, minimal, self-contained, 
reproducible code, as suggested in the posting guide 
"www.R-project.org/posting-guide.html".  However, have you tried 
"www.bioconductor.org"?  I believe they also have a listserve, and the 
people who follow that list should be more familiar with analysis of 
genetic data than this more general listserve. 

      Also, I suggest you send your email "From" something more 
revealing of who you are.  Some of the most knowledgeable and frequent 
contributors to this listserve rarely if ever answer questions from 
anonymous email addresses like "learningr at gmail.com". 

      Hope this helps. 
      Spencer Graves

Learning R wrote:
> Dear RUsers,
>
> I am new to R. I am learning how to use R. I am a PC user and run R on
> windows. I would appreciate if some one could guide me on a few questions I
> have:
>
> 1) I have 4 cel files (2 replicates for NORM and Disease resp). I have been
> able to run siggenes on this dataset where I have 4 labels in the class file
> groupsnhi.cl  op-> (0,0,1,1) and my data has been read into datrmanhi after
> performing rma. When I run these commands here I receive these errors:
>
>   
>> plot(samnhi.out,seq(0.1,0.6,0.1))
>> identify(samnhi.out,ll=FALSE)
>>     
> warning: no point with 0.25 inches
> warning: no point with 0.25 inches
> warning: no point with 0.25 inches
>
> Why does this happen.
>
> 2) Now I am trying to run EBAM: and when I type I get an error
>
>  > find.out<-find.a0(datrmanhi,groupsnhi.cl,rand=123)
> Loading required package: affy
> Loading required package: affyio
> EBAM Analysis for the two class unpaired case.
>
> Warning: There are 1 genes which have variance Zero or no non-missing
> values.
>          The d-value of these genes is set to NA.
>
>
>         The following object(s) are masked _by_ .GlobalEnv :
>
>          n
>
>
>         The following object(s) are masked from mat.repeat ( position 5 ) :
>
>          center log.bino n p success x1 x2 x3 x4 x5
>
> Error in optim(rep(0, 6), neglogLik.repeat, method = "BFGS") :
>         non-finite finite-difference value [1]
> In addition: Warning message:
> collapsing to unique 'x' values in: approx(Z, Z.norm, z, rule = 2)
>
>
> -------------------------------------------------------------------------------------------------------------------------------------------------------------------------
>
> I have also tried :
>   
>> find.out<-find.a0(exprs2,c(1,1,1,0,0,0),rand=123)
>>     
> EBAM Analysis for the two class unpaired case.
>
> Warning: There are 1 genes which have variance Zero or no non-missing
> values.
>          The d-value of these genes is set to NA.
>
>
>         The following object(s) are masked _by_ .GlobalEnv :
>
>          n
>
>
>         The following object(s) are masked from mat.repeat ( position 3 ) :
>
>          center log.bino n p success x1 x2 x3 x4 x5
>
>
>         The following object(s) are masked from mat.repeat ( position 6 ) :
>
>          center log.bino n p success x1 x2 x3 x4 x5
>
> Error in optim(rep(0, 6), neglogLik.repeat, method = "BFGS") :
>         non-finite finite-difference value [1]
> In addition: Warning message:
> collapsing to unique 'x' values in: approx(Z, Z.norm, z, rule = 2)
>
>
> I would greatly appreciate any solutions and help to solve this problem.
>
> Thank you,
> Appreciate your time.
> Regards,
> Lolita
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From goedman at mac.com  Fri Sep 29 17:57:28 2006
From: goedman at mac.com (Rob J Goedman)
Date: Fri, 29 Sep 2006 08:57:28 -0700
Subject: [R] problems with R and tckl/tk on Mac OS X
In-Reply-To: <20060929150407.DPBA13241.tomts10-srv.bellnexxia.net@JohnDesktop8300>
References: <20060929150407.DPBA13241.tomts10-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <CDDCD28E-9FDF-4611-9983-FF31ADBB3CBD@mac.com>

Hi John, Alex and Ingo,

Thanks for catching this question. I'd missed Ingo's original email.

Rcmdr does need X.11 and Tcl/Tk, although it uses the versions that  
come with Mac OS 10.4. Hence, as Alex indicates, there is no need to  
separately install these.

Ingo, if you can't get it to work using John's link, let me know  
where you get stuck.  R-Sig-Mac is an alias dedicated to Mac OS  
specific questions.

Regards,
Rob


On Sep 29, 2006, at 8:04 AM, John Fox wrote:

> Dear Alex,
>
> It's possible, though not clear from the original posting, that  
> Ingo is
> trying to install the Rcmdr package. If that's the case, then he  
> might find
> the installation instructions for Mac users at
> <http://socserv.socsci.mcmaster.ca/jfox/Misc/Rcmdr/installation- 
> notes.html>
> helpful.
>
> Regards,
>  John
>
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox
> --------------------------------
>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Alex Brown
>> Sent: Friday, September 29, 2006 8:48 AM
>> Cc: r-help at stat.math.ethz.ch
>> Subject: Re: [R] problems with R and tckl/tk on Mac OS X
>>
>> I assume you are running MacOS X 10.4?
>>
>> The Mac version of R does not require X11 or Tcl/TK since it
>> uses a separate GUI interface developed exclusively for the
>> Mac using native widgets and a quartz based graphing mechanism.
>>
>> Try downloading the following binary installer which includes the gui
>> interface:
>>
>> http://cran.r-project.org/bin/macosx/R-2.3.1.dmg
>>
>> -Alex Brown
>>
>> On 29 Sep 2006, at 08:04, Ingo wrote:
>>
>>
>>> Dear R-help team,
>>> I am trying to run "R" on my Intel-based Mac.  I have installed R,
>>> X11 and Tcl/TK (I thought), but the GUI doesn't run.  My system
>>> administrator doesn't support R and therefore, I'm a little
>> helpless.
>>> What can I do?
>>>
>>> Regards,
>>>
>>> Ingo
>>>
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-
>>> guide.html and provide commented, minimal, self-contained,
>>> reproducible code.
>>>
>>
>>
>> On 29 Sep 2006, at 08:04, Ingo wrote:
>>
>>> Dear R-help team,
>>> I am trying to run "R" on my Intel-based Mac.  I have installed R,
>>> X11 and Tcl/TK (I thought), but the GUI doesn't run.  My system
>>> administrator doesn't support R and therefore, I'm a little
>> helpless.
>>> What can I do?
>>>
>>> Regards,
>>>
>>> Ingo
>>>
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-
>>> guide.html and provide commented, minimal, self-contained,
>>> reproducible code.
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From luke at stat.uiowa.edu  Fri Sep 29 18:00:44 2006
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Fri, 29 Sep 2006 11:00:44 -0500 (CDT)
Subject: [R] Evaluation of defaults in functions
In-Reply-To: <f8e6ff050609290834y4a54c86dke224874cecbf7f60@mail.gmail.com>
References: <451C275D.50901@web.de>
	<971536df0609281432w332f786ase511e504ac35c269@mail.gmail.com>
	<x2hcyrmzwz.fsf@turmalin.kubism.ku.dk>
	<f8e6ff050609281934r69a0e60fp99f16cef986d2a3c@mail.gmail.com>
	<Pine.LNX.4.64.0609290659190.15856@gannet.stats.ox.ac.uk> 
	<Pine.LNX.4.64.0609290952150.11669@nokomis.stat.uiowa.edu>
	<f8e6ff050609290834y4a54c86dke224874cecbf7f60@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0609291039120.11669@nokomis.stat.uiowa.edu>

On Fri, 29 Sep 2006, hadley wickham wrote:

>> There are not all that many other languages that use lazy evaluation.
>> Those that do are for the most part pure or nearly pure functional
>> languages--Haskell is probably the main example.  These go much
>> further in their use of lazy evaluation than R.  For analogs of the R
>> expressions
>>
>>          x <- f(x)
>>         list(f(x))
>>          x + f(x)
>> 
>> only the last one is guaranteed to result in f being called. This
>> makes many things conceptually cleaner and also automatically supports
>> lazy data structures, which allows one to express things like infinite
>> sequences.  Some of the downsides are a more complex (or at least
>> very different) implementation and that I becomes very hard to reason
>> about performance.  One of the reasons performance is hard to sort out
>> is that it is (deliberately, because of the declarative nature of
>> these languages) hard to know exactly when an evaluation will occur.
>
> You can argue similarly that adding state (eg. functions with side
> effects) increases the complexity of reasoning about the result of a
> calculcation.

Of course, hence the popularity of purely functional languages in
csome circles.  That's not the point.  Reasoning about _performance_,
even in a purely functional context, is complicated by lazy
evaluations.  You can read about this in various ML vs Haskell debates
if you really want to.

>
>
>> Contrary to Hadley's comment R actually provides quite a lot of control 
>> since
>> assignments are guaranteed to cause evaluation, e.g. in
>>
>>      g <- function(x) {
>>          x <- x # forces evaluation
>>          ...
>>      }
>> 
>> the argument is guaranteed to be evaluated.  The function force()
>> makes this a little more readable (avoids the need for a comment).
>
> Perhaps control wasn't the word I was looking for.  Most lazy
> languages provide some way to force evaluation.

I actualy doubt that, except possibly as a back door the use of which
is generally frowned upon. Again, it's not like there are hundreds of
such languages out there, so "most" would mean something like 2 out of
3 (there are more but not that many more).

> Assignments forcing
> evaluation seems to be a disadvantage to me - I can't do anything with
> a promise apart from evaluate it.

You shouldn't be able to do anything with a promise, period (the
internal R promise that is). It is an internal implementation trick
that should not be accessible at the user level, and now with the
removal of delay() is not.  Having user level promise objects as in
Scheme, for example, is very useful, but those are based on the notion
that they are _only_ evaluated on explicit demand.  Such objects can
easily be implemented with the current R behavior.

Having assignments behave the way they do is most unfurtunate when it
comes to reasoning about code (e.g. in code analysis or
compilation). Analogs of

     x <- complicated expression
     f (x)

and

     f(complicated expression)

are equivalent in a language like ML with eager evaluation and they
are equivalent in Haskell with pure lazy evaluation but they are not
in R.  So in R replacing one by the other is not a
semantics-preserving transformation.

Best,

luke

>> Lazy data structures can be implemented in R on top of the limited
>> lazy evaluation mechanism.  I experimented with this for fun a one
>> point.  Some code is in http://www.stat.uiowa.edu/~luke/R/lazy/.  This
>> is out of date but not too hard to fix.  Insuring memory efficiency is
>> still a bit tricky; some comments are int he notes at this site.
>
> Thanks, I will take a look at that.
>
> Regards,
>
> Hadley
>

-- 
Luke Tierney
Chair, Statistics and Actuarial Science
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu


From b.otto at uke.uni-hamburg.de  Fri Sep 29 17:59:19 2006
From: b.otto at uke.uni-hamburg.de (Benjamin Otto)
Date: Fri, 29 Sep 2006 17:59:19 +0200
Subject: [R] List-manipulation
Message-ID: <001601c6e3e0$38db3f70$336f12ac@matrix.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060929/36d4b2bd/attachment.pl 

From patrick.giraudoux at univ-fcomte.fr  Fri Sep 29 18:20:38 2006
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux)
Date: Fri, 29 Sep 2006 18:20:38 +0200
Subject: [R] hcc not found, rcmd build
Message-ID: <451D47D6.5020605@univ-fcomte.fr>


Working under Windows XP, I am compiling a package called 'pgirmess' 
with the command

rcmd build --binary --auto-zip pgirmess

I have this message error after having listed: functions text html latex 
example chm
....
zipping help file
hcc: not found
cp: cannot stat 'c:/TEMP/Rbuild365620874/pgirmess/chm/pgirmess.chm': No 
such file or directory
make[1]: *** [chm-pgirmess] Error 1
make: *** [pkg-pgirmess] Error 2
*** Installation of pgirmess failed ***

I have recently installed MikTex 2.5 and Perl (I have been obliged to 
format my hard disk and reinstall everything after a computer crash...).

Has anyone an idea about what means hcc: not found and how to make "hcc" 
available to the programme?

Thanks in advance for any hint,

Patrick


From R.A.Sanderson at newcastle.ac.uk  Fri Sep 29 19:31:23 2006
From: R.A.Sanderson at newcastle.ac.uk (Roy Sanderson)
Date: Fri, 29 Sep 2006 17:31:23 +0000
Subject: [R] Helmert contrasts for repeated measures and split-plot expts
Message-ID: <3.0.3.32.20060929173123.0091cd60@popin.ncl.ac.uk>

Dear R-help

I have two separate experiments, one a repeated-measures design, the other
a split-plot.  In a standard ANOVA I have usually undertaken a
multiple-comparison test on a significant factor with e.g TukeyHSD, but as
I understand it such a test is inappropriate for repeated measures or
split-plot designs.

Is it therefore sensible to use Helmert contrasts for either of these
designs?  Whilst not providing all the pairwise comparisons of TukeyHSD,
presumably the P-statistic for each Helmert contrast will indicate clearly
whether that contrast is significant and should be retained in the model.
(This seems to come with the disadvantage that the parameter values are
harder to interpret than with Treatment contrasts.)  In the
repeated-measures design the factor in question has three levels, whilst in
the split-plot design it has four.

Many thanks in advance
Roy
----------------------------------------------------------------------------
-------
Roy Sanderson
Institute for Research on Environment and Sustainability
Devonshire Building
University of Newcastle
Newcastle upon Tyne
NE1 7RU
United Kingdom

Tel: +44 191 246 4835
Fax: +44 191 246 4999

http://www.ncl.ac.uk/environment/
r.a.sanderson at newcastle.ac.uk


From ligges at statistik.uni-dortmund.de  Fri Sep 29 18:31:48 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 29 Sep 2006 18:31:48 +0200
Subject: [R] hcc not found, rcmd build
In-Reply-To: <451D47D6.5020605@univ-fcomte.fr>
References: <451D47D6.5020605@univ-fcomte.fr>
Message-ID: <451D4A74.9000608@statistik.uni-dortmund.de>



Patrick Giraudoux wrote:
> Working under Windows XP, I am compiling a package called 'pgirmess' 
> with the command
> 
> rcmd build --binary --auto-zip pgirmess
> 
> I have this message error after having listed: functions text html latex 
> example chm
> ....
> zipping help file
> hcc: not found
> cp: cannot stat 'c:/TEMP/Rbuild365620874/pgirmess/chm/pgirmess.chm': No 
> such file or directory
> make[1]: *** [chm-pgirmess] Error 1
> make: *** [pkg-pgirmess] Error 2
> *** Installation of pgirmess failed ***
> 
> I have recently installed MikTex 2.5 and Perl (I have been obliged to 
> format my hard disk and reinstall everything after a computer crash...).
> 
> Has anyone an idea about what means hcc: not found and how to make "hcc" 
> available to the programme?


Microsoft's html help compiler is not in your path (or not even 
installed). See the R Installation and Administration manual.

Uwe Ligges



> Thanks in advance for any hint,
> 
> Patrick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From Roger.Bivand at nhh.no  Fri Sep 29 18:37:16 2006
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 29 Sep 2006 18:37:16 +0200 (CEST)
Subject: [R] hcc not found, rcmd build
In-Reply-To: <451D47D6.5020605@univ-fcomte.fr>
Message-ID: <Pine.LNX.4.44.0609291836060.3279-100000@reclus.nhh.no>

On Fri, 29 Sep 2006, Patrick Giraudoux wrote:

> 
> Working under Windows XP, I am compiling a package called 'pgirmess' 
> with the command
> 
> rcmd build --binary --auto-zip pgirmess
> 
> I have this message error after having listed: functions text html latex 
> example chm
> ....
> zipping help file
> hcc: not found
> cp: cannot stat 'c:/TEMP/Rbuild365620874/pgirmess/chm/pgirmess.chm': No 
> such file or directory
> make[1]: *** [chm-pgirmess] Error 1
> make: *** [pkg-pgirmess] Error 2
> *** Installation of pgirmess failed ***
> 
> I have recently installed MikTex 2.5 and Perl (I have been obliged to 
> format my hard disk and reinstall everything after a computer crash...).
> 
> Has anyone an idea about what means hcc: not found and how to make "hcc" 
> available to the programme?

See: "To build Windows help" in:

http://www.murdoch-sutherland.com/Rtools/

where there is also a link to the Microsoft HTML Help Workshop, which 
doesn't seem to be on your path.

> 
> Thanks in advance for any hint,
> 
> Patrick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From patrick.giraudoux at univ-fcomte.fr  Fri Sep 29 18:44:23 2006
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux)
Date: Fri, 29 Sep 2006 18:44:23 +0200
Subject: [R] hcc not found, rcmd build
In-Reply-To: <451D4A74.9000608@statistik.uni-dortmund.de>
References: <451D47D6.5020605@univ-fcomte.fr>
	<451D4A74.9000608@statistik.uni-dortmund.de>
Message-ID: <451D4D67.3090601@univ-fcomte.fr>

Exactly that... Shame on me, ashes on my head and all those sort of 
things...

Thanks for the hint anyway...

Patrick

Uwe Ligges a ?crit :
>
>
> Patrick Giraudoux wrote:
>> Working under Windows XP, I am compiling a package called 'pgirmess' 
>> with the command
>>
>> rcmd build --binary --auto-zip pgirmess
>>
>> I have this message error after having listed: functions text html 
>> latex example chm
>> ....
>> zipping help file
>> hcc: not found
>> cp: cannot stat 'c:/TEMP/Rbuild365620874/pgirmess/chm/pgirmess.chm': 
>> No such file or directory
>> make[1]: *** [chm-pgirmess] Error 1
>> make: *** [pkg-pgirmess] Error 2
>> *** Installation of pgirmess failed ***
>>
>> I have recently installed MikTex 2.5 and Perl (I have been obliged to 
>> format my hard disk and reinstall everything after a computer crash...).
>>
>> Has anyone an idea about what means hcc: not found and how to make 
>> "hcc" available to the programme?
>
>
> Microsoft's html help compiler is not in your path (or not even 
> installed). See the R Installation and Administration manual.
>
> Uwe Ligges
>
>
>
>> Thanks in advance for any hint,
>>
>> Patrick
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>
>


From sarosh.jamal at utoronto.ca  Fri Sep 29 18:48:05 2006
From: sarosh.jamal at utoronto.ca (Sarosh Jamal)
Date: Fri, 29 Sep 2006 12:48:05 -0400
Subject: [R] File Encoding Warnings during package installs
Message-ID: <20060929124805827.00000001084@gypsy-l01>

I've been getting the following warning on each package update/install (Rv2.4 on SunOS9):

Warning message:
'DESCRIPTION' file has 'Encoding' field and re-encoding is not possible.

This was the case on R2.3 and now on R2.4 as well - any insight would be much appreciated.

Thank you,

Sarosh Jamal 
Geo Computing & IT Specialist, Department of Geography 
University of Toronto at Mississauga 
e: sarosh.jamal at utoronto.ca 
p: (905) 569-4497


From sarosh.jamal at utoronto.ca  Fri Sep 29 18:52:37 2006
From: sarosh.jamal at utoronto.ca (Sarosh Jamal)
Date: Fri, 29 Sep 2006 12:52:37 -0400
Subject: [R] RODBC ERROR on Rcmdr install
Message-ID: <20060929125237518.00000001084@gypsy-l01>


I've been trying to install the RODBC dependency for Rcmdr on Rv2.4 on a SunOS9 system. 

It claims not to be able to create gcc output files (executables) for the installation.

This is puzzling since I've been able to install other packages with the same PATH variables and all.

Feedback would be much appreciated. Thank you!

Sarosh Jamal 
Geo Computing & IT Specialist, Department of Geography 
University of Toronto at Mississauga 
e: sarosh.jamal at utoronto.ca 
p: (905) 569-4497


From jholtman at gmail.com  Fri Sep 29 19:00:27 2006
From: jholtman at gmail.com (jim holtman)
Date: Fri, 29 Sep 2006 13:00:27 -0400
Subject: [R] List-manipulation
In-Reply-To: <001601c6e3e0$38db3f70$336f12ac@matrix.com>
References: <001601c6e3e0$38db3f70$336f12ac@matrix.com>
Message-ID: <644e1f320609291000q6ba0855as10077ed9ed82b3a7@mail.gmail.com>

Is this what you want?

> x <- list(a=1:3, b=30:34, c=40:35)
> x
$a
[1] 1 2 3

$b
[1] 30 31 32 33 34

$c
[1] 40 39 38 37 36 35

> lapply(x,'[', 1)
$a
[1] 1

$b
[1] 30

$c
[1] 40

> unlist(lapply(x,'[', 1))
 a  b  c
 1 30 40
>


On 9/29/06, Benjamin Otto <b.otto at uke.uni-hamburg.de> wrote:
> Hi,
>
>
>
> Sorry for the question, I know it should be basic knowledge but I'm
> struggling for two hours now.
>
>
>
> How do I select only the first entry of each list member and ignore the
> rest?
>
>
>
> So for
>
>
>
> > $"121_at"
>
> > -113691170
>
>
>
> > $"1255_g_at"
>
> > 42231151
>
>
>
> > $"1316_at"
>
> > 35472685 35472588
>
>
>
> > $"1320_at"
>
> > -88003869
>
>
>
> I only want to select
>
>
>
> -113691170, 42231151, 35472685 and -88003869 .?
>
>
>
> Regards
>
> Benjamin
>
> --
> Benjamin Otto
> Universitaetsklinikum Eppendorf Hamburg
> Institut fuer Klinische Chemie
> Martinistrasse 52
> 20246 Hamburg
>
>
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From rmh at temple.edu  Fri Sep 29 19:00:38 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Fri, 29 Sep 2006 13:00:38 -0400 (EDT)
Subject: [R] List-manipulation
Message-ID: <20060929130038.BJC17793@po-d.temple.edu>

You need one of the apply family of functions.
?sapply

> tmp <- list(a=1:2, b=3:5, c=5, dd=numeric(0), e=1:8)
> sapply(tmp, function(x) x[1])
 a  b  c dd  e 
 1  3  5 NA  1


From tplate at acm.org  Fri Sep 29 19:01:24 2006
From: tplate at acm.org (Tony Plate)
Date: Fri, 29 Sep 2006 11:01:24 -0600
Subject: [R] List-manipulation
In-Reply-To: <001601c6e3e0$38db3f70$336f12ac@matrix.com>
References: <001601c6e3e0$38db3f70$336f12ac@matrix.com>
Message-ID: <451D5164.7040808@acm.org>

Does this do what you want?

 > x <- list(1,2,3:7,8,9:10)
 > sapply(x, function(xx) xx[1])
[1] 1 2 3 8 9
 >

-- Tony Plate

Benjamin Otto wrote:
> Hi,
> 
>  
> 
> Sorry for the question, I know it should be basic knowledge but I'm
> struggling for two hours now.
> 
>  
> 
> How do I select only the first entry of each list member and ignore the
> rest?
> 
>  
> 
> So for 
> 
>  
> 
> 
>>$"121_at"
> 
> 
>>-113691170 
> 
> 
>  
> 
> 
>>$"1255_g_at"
> 
> 
>>42231151 
> 
> 
>  
> 
> 
>>$"1316_at"
> 
> 
>>35472685 35472588 
> 
> 
>  
> 
> 
>>$"1320_at"
> 
> 
>>-88003869
> 
> 
>  
> 
> I only want to select 
> 
>  
> 
> -113691170, 42231151, 35472685 and -88003869 .?
> 
>  
> 
> Regards
> 
> Benjamin
> 
> --
> Benjamin Otto
> Universitaetsklinikum Eppendorf Hamburg
> Institut fuer Klinische Chemie
> Martinistrasse 52
> 20246 Hamburg
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From spencer.graves at pdf.com  Fri Sep 29 19:11:27 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 29 Sep 2006 12:11:27 -0500
Subject: [R] Formula aruguments with NLS and model.frame()
In-Reply-To: <eeehnh$8hk$1@sea.gmane.org>
References: <eeehnh$8hk$1@sea.gmane.org>
Message-ID: <451D53BF.1010102@pdf.com>

      I haven't seen any replies to this post, so I will offer a couple 
of comments. 

      First, your example is entirely too complicated and not self 
contained for me to say much in the limited time I have for this.  I 
suggest you start by splitting your problem into several steps.  For 
example, will 'garchFit' allow 'formula.mean' to be something other than 
'~arma(p, q)', where p and q are nonnegative integers?  If no, I suggest 
you start by trying to produce your own modification to 'garchFit' so it 
will accept something like 'formula.mean=~z+arma(p, q)';  I suggest you 
give your local copy a different name like 'garchFitZ'.  Second, I 
suggest you cut your example data down to a minimum, e.g, 9 or 20 
observations, just enough so the algorithm won't die for some reason 
that would not occur with a larger data set but small enough so you can 
quickly print out and study every object the 'garchFit' algorithm 
produces. 

      Second, I suggest you use 'debug' to walk through your local 
version of 'garchFit' line by line.  I've found this to be a very 
powerful way to learn what happens in the internal environment of a 
function. 

      If you get stuck trying this, please submit another post including 
commented, minimal, self-contained, reproducible code, as suggested in 
the posting guide 'www.R-project.org/posting-guide.html'. 
      Hope this helps. 
      Spencer Graves

and provide commented, minimal, self-contained, reproducible code.



Joe W. Byers wrote:
> I could use some help understanding how nls parses the formula argument
> to a model.frame and estimates the model.  I am trying to utilize the
> functionality of the nls formula argument to modify garchFit() to handle
> other variables in the mean equation besides just an arma(u,v)
> specification.
>
> My nonlinear model is
>       y<-nls(t~a*sin(w*2*pi/365*id+p)+b*id+int,data=t1,
> 	start=list(w=.5,a=.1,p=.5,b=init.y$coef[2],int=init.y$coef[1] ),
> 	control=list(maxiter=1000000,minFactor=1e-18))
> where t is change in daily temperatures, id is just a time trend and the
> a*sin is a one year fourier series.
>
> I have tried to debug the nls code using the following code
> t1<-data.frame(t=as.vector(x),id=index(x))
> data=t1;
> formula <- as.formula(t ~ a *sin(w *2* pi/365 * id + p) + b * id + int);
>       varNames <- all.vars(formula)
>       algorithm<-'default';
>       mf <- match.call(definition=nls,expand.dots=FALSE,
>       call('nls',formula, data=parent.frame(),start,control = nls.control(),
>       algorithm = "default", trace = FALSE,
>       subset, weights, na.action, model = FALSE, lower = -Inf,
>       upper = Inf));
>       mWeights<-F;#missing(weights);
> 	start=list(w=.5,a=.1,p=.5,b=init.y$coef[2],int=init.y$coef[1] );
>       pnames <- names(start);
>        varNames <- varNames[is.na(match(varNames, pnames, nomatch = NA))]
>
> 	varIndex <- sapply(varNames,
> 		function(varName, data, respLength) {
>           	length(eval(as.name(varName), data))%%respLength == 0},
>           	 data, length(eval(formula[[2]], data))
>           );
> 	mf$formula <- as.formula(paste("~", paste(varNames[varIndex],
>           collapse = "+")), env = environment(formula));
> 	mf$start <- NULL;mf$control <- NULL;mf$algorithm <- NULL;
> 	mf$trace <- NULL;mf$model <- NULL;
>       mf$lower <- NULL;mf$upper <- NULL;
>       mf[[1]] <- as.name("model.frame");
>       mf<-evalq(mf,data);
>       n<-nrow(mf)
>       mf<-as.list(mf);
>       wts <- if (!mWeights)
>           model.weights(mf)
>       else rep(1, n)
>       if (any(wts < 0 | is.na(wts)))
>           stop("missing or negative weights not allowed")
>
>       m <- switch(algorithm,
>       		plinear = nlsModel.plinear(formula, mf, start, wts),
>       		port = nlsModel(formula, mf, start, wts, upper),
>       		nlsModel(formula, mf, start, wts));
>
> I am struggling with the environment issues associated with performing
> these operations.  I did not include the data because it is 9000 
> observations of temperature data.  If anyone would like the data, I can 
> provide it or a subset in a csv file.
>
>
> thank you
> Joe
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From spencer.graves at pdf.com  Fri Sep 29 19:13:12 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 29 Sep 2006 12:13:12 -0500
Subject: [R] time series simulation
In-Reply-To: <6352577.post@talk.nabble.com>
References: <6352577.post@talk.nabble.com>
Message-ID: <451D5428.8000204@pdf.com>

      First, I'd write down a model for how your stochastic process 
relates to independent, normal observations with mean 0 and standard 
deviation 1.  You want a lognormal series, so I'd start by generating a 
normal series and the compute 'exp' of that.  If you'd like more help 
from this listserve, please provide commented, minimal, self-contained, 
reproducible code, as suggested in the posting guide 
"www.R-project.org/posting-guide.html". 

      Hope this helps. 
      Spencer Graves

march wrote:
> Hi everybody
> I'm trying to simulate a stochastic process in R. I would like consider n
> log normal time series. The first time serie has a growth rate lower than
> the second and so on. the initial time of the first serie is lower than the
> initial time of the second and so on. In the long run the series have the
> same value. Do you have any idea at running such a process?
> Other question: How can I reduce the domain of a random variable?
> Thanks
> March
>                                                                   
>


From albmont at centroin.com.br  Fri Sep 29 19:24:06 2006
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Fri, 29 Sep 2006 15:24:06 -0200
Subject: [R] Heteroskedasticity test
In-Reply-To: <644e1f320609291000q6ba0855as10077ed9ed82b3a7@mail.gmail.com>
References: <001601c6e3e0$38db3f70$336f12ac@matrix.com>
	<644e1f320609291000q6ba0855as10077ed9ed82b3a7@mail.gmail.com>
Message-ID: <20060929171915.M91848@centroin.com.br>

Is there any heteroskedasticity test in the package? Something
that would flag a sample like

  x <- c(rnorm(1000), rnorm(1000, 0, 1.2))

Alberto Monteiro


From rvaradhan at jhmi.edu  Fri Sep 29 19:25:31 2006
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Fri, 29 Sep 2006 13:25:31 -0400
Subject: [R] X-axis labels in histograms drawn by the "truehist" function
Message-ID: <000601c6e3ec$4424fea0$7c94100a@win.ad.jhu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060929/759c5cfb/attachment.pl 

From Mark.Leeds at morganstanley.com  Fri Sep 29 19:34:41 2006
From: Mark.Leeds at morganstanley.com (Leeds, Mark (IED))
Date: Fri, 29 Sep 2006 13:34:41 -0400
Subject: [R] Heteroskedasticity test
In-Reply-To: <20060929171915.M91848@centroin.com.br>
Message-ID: <D3AEEDA31E57474B840BEBC25A8A8344934278@NYWEXMB23.msad.ms.com>

I think tseries has one but I don't know if it would pick that up. It's
kind of slight.



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Alberto Monteiro
Sent: Friday, September 29, 2006 1:24 PM
To: R-Help
Subject: [R] Heteroskedasticity test

Is there any heteroskedasticity test in the package? Something that
would flag a sample like

  x <- c(rnorm(1000), rnorm(1000, 0, 1.2))

Alberto Monteiro

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.
--------------------------------------------------------

This is not an offer (or solicitation of an offer) to buy/se...{{dropped}}


From spencer.graves at pdf.com  Fri Sep 29 19:37:14 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 29 Sep 2006 12:37:14 -0500
Subject: [R] Simulation of a Dirichlet Process.
In-Reply-To: <20060918071312.wrws4sdynls08wo8@mail.ucad.sn>
References: <20060918071312.wrws4sdynls08wo8@mail.ucad.sn>
Message-ID: <451D59CA.201@pdf.com>

      I haven't seen any replies to this post, and I don't know tutors 
in NYC.  However, I just got 79 hits for 'RSiteSearch("dirichlet")' and 
12 for 'RSiteSearch("dirichlet MCMC")'.  If you would like further help 
from this listserve, I suggest you review the results of 
'RSiteSearch("dirichlet MCMC")', try something.  If it is not what you 
want, please convert it to a commented, minimal, self-contained, 
reproducible code and send that to this listserve, as suggested in the 
posting guide "www.R-project.org/posting-guide.html". 

      Hope this helps. 
      Spencer Graves

pngom at ucad.sn wrote:
> I'm just getting started with R, having a lot of original work on
> modeling and exploring the simulation by MCMC. I want to simulate the 
> prior and the posterior distribution of Dirichlet Process by MCMC.
> Is there anyone in NYC that might be a good
> tutor for me?
>
>


From DAVID.BICKEL at pioneer.com  Fri Sep 29 19:50:21 2006
From: DAVID.BICKEL at pioneer.com (Bickel, David)
Date: Fri, 29 Sep 2006 12:50:21 -0500
Subject: [R] GLM information matrix
Message-ID: <508C256258222B4D9BBCD72591CCC7C1011F3D4B@jhms18.phibred.com>

Is there a function that provides the Fisher information matrix for a
generalized linear model? I do not see how to access the off-diagonal
matrix elements of the value returned by glm. (I'm particularly
interested in logistic regression.)

If not, what is a good way to use R to compute Hessians or other partial
derivatives of log likelihoods?

I would appreciate any guidance.

David
_______________________________________
David R. Bickel  http://davidbickel.com
Research Scientist
Pioneer Hi-Bred International (DuPont)
Bioinformatics
7200 NW 62nd Ave.; PO Box 184
Johnston, IA 50131-0184
515-334-4739 Tel
515-334-4473 Fax
david.bickel at pioneer.com

This communication is for use by the intended recipient and ...{{dropped}}


From rmh at temple.edu  Fri Sep 29 20:07:48 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Fri, 29 Sep 2006 14:07:48 -0400 (EDT)
Subject: [R] Heteroskedasticity test
Message-ID: <20060929140748.BJC29398@po-d.temple.edu>

The Brown-Forsyth test for homogeneity of variance is included in
the HH package, downloadable from CRAN.

library(HH)
x <- c(rnorm(1000), rnorm(1000, 0, 1.2))
tmp <- data.frame(x=x, group=rep(c("s1","s1.2"), c(1000,1000)))
plot.hov(x ~ group, data=tmp)
hov(x ~ group, data=tmp)


From Antonio_Paredes at aphis.usda.gov  Fri Sep 29 20:18:08 2006
From: Antonio_Paredes at aphis.usda.gov (Antonio_Paredes at aphis.usda.gov)
Date: Fri, 29 Sep 2006 13:18:08 -0500
Subject: [R] attributable fraction
Message-ID: <OF022DEA20.C6037026-ON862571F8.00644DF9-862571F8.006405AA@aphis.usda.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060929/8ca2c567/attachment.pl 

From Robert.McGehee at geodecapital.com  Fri Sep 29 20:17:30 2006
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Fri, 29 Sep 2006 14:17:30 -0400
Subject: [R] Plotting text with lattice
Message-ID: <67DCA285A2D7754280D3B8E88EB548020C946925@MSGBOSCLB2WIN.DMN1.FMR.COM>

Hello,
I've decided to take the leap and try my hand at the lattice package,
though I am getting stuck at what one might consider a trivial problem,
plotting text at a point in a graph. Apologies in advance if (that) I'm
missing something extremely basic.

Consider in base graphics:
> plot(1:10)
> text(2, 4, "Text")
In the above you will see text centered at the point (2, 4) on the
graph.

Now I would like to try to do the same thing using the lattice package:

> xyplot(x ~ x, data = data.frame(x = 1:10))
> ltext(x=2, y=4, labels="Text")
> panel.text(x=2, y=4, labels="Text")
> grid.text(label="Text", x=2, y=4)
> grid.text(label="Text", x=unit(2, "native"), y=unit(4, "native"))

None of the above four commands puts the "Text" at the (2, 4) point on
the graph. Any help with this would be appreciated! Also, if I have more
than one panel and would like to place text at different points on
different panels how would I do this?

Also, note that I'm hoping to use text to label interesting points in a
levelplot, but am using the above xyplot as an example.

Thanks,
Robert


Robert McGehee
Quantitative Analyst
Geode Capital Management, LLC
53 State Street, 5th Floor | Boston, MA | 02109
Tel: 617/392-8396    Fax:617/476-6389
mailto:robert.mcgehee at geodecapital.com



This e-mail, and any attachments hereto, are intended for us...{{dropped}}


From hpbenton at scripps.edu  Fri Sep 29 20:40:18 2006
From: hpbenton at scripps.edu (H. Paul Benton)
Date: Fri, 29 Sep 2006 11:40:18 -0700
Subject: [R] linear gradient in nls
Message-ID: <001901c6e3f6$b6637870$6402a8c0@lama>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060929/cc122a70/attachment.pl 

From Charles.Annis at StatisticalEngineering.com  Fri Sep 29 20:51:18 2006
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Fri, 29 Sep 2006 14:51:18 -0400
Subject: [R] GLM information matrix
In-Reply-To: <508C256258222B4D9BBCD72591CCC7C1011F3D4B@jhms18.phibred.com>
Message-ID: <006901c6e3f8$3fccf8b0$6400a8c0@DD4XFW31>

David:

I don't have what you want.  But if your model is simple (2-parameter,
binomial response, glm with a logit link) I have some code that computes and
plots the loglikelihood surface using contour() and superimposes the
asymptotic 95% confidence ellipse, for comparison with the observed contour
for qchisq(0.95, df=2)/2.  And for many datasets the agreement isn't as nice
as you might hope, and that your Hessian might require.  (That is, the
actually contour is not elliptical, or if it is its axes may not agree well
with the pseudo-elliptical contour of the observed loglikelihood surface.)

You may be looking for the resulting confidence bounds on the glm fit for
which I also have code that iteratively interrogates the loglikelihood
surface without plotting it.  

If any of this is interesting, please send me a note so we won't clog the
bandwidth.

Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Bickel, David
Sent: Friday, September 29, 2006 1:50 PM
To: r-help at stat.math.ethz.ch
Subject: [R] GLM information matrix

Is there a function that provides the Fisher information matrix for a
generalized linear model? I do not see how to access the off-diagonal
matrix elements of the value returned by glm. (I'm particularly
interested in logistic regression.)

If not, what is a good way to use R to compute Hessians or other partial
derivatives of log likelihoods?

I would appreciate any guidance.

David
_______________________________________
David R. Bickel  http://davidbickel.com
Research Scientist
Pioneer Hi-Bred International (DuPont)
Bioinformatics
7200 NW 62nd Ave.; PO Box 184
Johnston, IA 50131-0184
515-334-4739 Tel
515-334-4473 Fax
david.bickel at pioneer.com

This communication is for use by the intended recipient and ...{{dropped}}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From spencer.graves at pdf.com  Fri Sep 29 21:02:31 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 29 Sep 2006 14:02:31 -0500
Subject: [R] ISO8601 week-of-year to date
In-Reply-To: <48151.128.230.104.196.1158591565.squirrel@mailhost.ut.ee>
References: <48151.128.230.104.196.1158591565.squirrel@mailhost.ut.ee>
Message-ID: <451D6DC7.1040904@pdf.com>

      What have you tried?  Are you familiar with the 'zoo' vignette and 
the time-date articles in R News?  Have you tried 'RSiteSearch'? 

      If you'd like more help from this listserve, please submit another 
post that includes commented, minimal, self-contained, reproducible code 
describing something you've tried and did not find adequate (as 
suggested in the posting guide 'www.R-project.org/posting-guide.html'). 

      Hope this helps. 
      Spencer Graves

Ott-Siim Toomet wrote:
> Hi,
>
> are there any way to convert ISO8601 weeks to gregorian dates?  Something
> like
>
> coverttodate(year=2006, week=38, day=1)
> # Sept 18, 2006
>
> Thanks in advance,
> Ott
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Dimitris.Rizopoulos at med.kuleuven.be  Fri Sep 29 21:11:42 2006
From: Dimitris.Rizopoulos at med.kuleuven.be (Dimitrios Rizopoulos)
Date: Fri, 29 Sep 2006 21:11:42 +0200
Subject: [R] GLM information matrix
In-Reply-To: <508C256258222B4D9BBCD72591CCC7C1011F3D4B@jhms18.phibred.com>
References: <508C256258222B4D9BBCD72591CCC7C1011F3D4B@jhms18.phibred.com>
Message-ID: <20060929211142.h0fdfcw489ass4sw@webmail5.kuleuven.be>

look at summary.glm(), probably you're looking for

fit <- glm(..., family = binomial)
# the inverse Fisher Information matrix
summary(fit)$cov.scaled


I hope it helps.

Best,
Dimitris

-- 
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
      http://www.student.kuleuven.be/~m0390867/dimitris.htm


Quoting "Bickel, David" <DAVID.BICKEL at pioneer.com>:

> Is there a function that provides the Fisher information matrix for a
> generalized linear model? I do not see how to access the off-diagonal
> matrix elements of the value returned by glm. (I'm particularly
> interested in logistic regression.)
>
> If not, what is a good way to use R to compute Hessians or other partial
> derivatives of log likelihoods?
>
> I would appreciate any guidance.
>
> David
> _______________________________________
> David R. Bickel  http://davidbickel.com
> Research Scientist
> Pioneer Hi-Bred International (DuPont)
> Bioinformatics
> 7200 NW 62nd Ave.; PO Box 184
> Johnston, IA 50131-0184
> 515-334-4739 Tel
> 515-334-4473 Fax
> david.bickel at pioneer.com
>
> This communication is for use by the intended recipient and ...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>



Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From rvaradhan at jhmi.edu  Fri Sep 29 21:14:50 2006
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Fri, 29 Sep 2006 15:14:50 -0400
Subject: [R] GLM information matrix
In-Reply-To: <006901c6e3f8$3fccf8b0$6400a8c0@DD4XFW31>
Message-ID: <000b01c6e3fb$89628730$7c94100a@win.ad.jhu.edu>

David,

You can use the 'vcov' function in the "stats" package to extract the
variance-covariance matrix from the GLM object.  Inverse of this matrix will
give you the observed (not Fisher) information matrix.

You can also use the "numDeriv" package to obtain accurate Hessian of the
log-likelihood.

Ravi.

----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Charles Annis, P.E.
Sent: Friday, September 29, 2006 2:51 PM
To: 'Bickel, David'; r-help at stat.math.ethz.ch
Subject: Re: [R] GLM information matrix

David:

I don't have what you want.  But if your model is simple (2-parameter,
binomial response, glm with a logit link) I have some code that computes and
plots the loglikelihood surface using contour() and superimposes the
asymptotic 95% confidence ellipse, for comparison with the observed contour
for qchisq(0.95, df=2)/2.  And for many datasets the agreement isn't as nice
as you might hope, and that your Hessian might require.  (That is, the
actually contour is not elliptical, or if it is its axes may not agree well
with the pseudo-elliptical contour of the observed loglikelihood surface.)

You may be looking for the resulting confidence bounds on the glm fit for
which I also have code that iteratively interrogates the loglikelihood
surface without plotting it.  

If any of this is interesting, please send me a note so we won't clog the
bandwidth.

Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Bickel, David
Sent: Friday, September 29, 2006 1:50 PM
To: r-help at stat.math.ethz.ch
Subject: [R] GLM information matrix

Is there a function that provides the Fisher information matrix for a
generalized linear model? I do not see how to access the off-diagonal
matrix elements of the value returned by glm. (I'm particularly
interested in logistic regression.)

If not, what is a good way to use R to compute Hessians or other partial
derivatives of log likelihoods?

I would appreciate any guidance.

David
_______________________________________
David R. Bickel  http://davidbickel.com
Research Scientist
Pioneer Hi-Bred International (DuPont)
Bioinformatics
7200 NW 62nd Ave.; PO Box 184
Johnston, IA 50131-0184
515-334-4739 Tel
515-334-4473 Fax
david.bickel at pioneer.com

This communication is for use by the intended recipient and ...{{dropped}}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ggrothendieck at gmail.com  Fri Sep 29 21:18:09 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 29 Sep 2006 15:18:09 -0400
Subject: [R] Plotting text with lattice
In-Reply-To: <67DCA285A2D7754280D3B8E88EB548020C946925@MSGBOSCLB2WIN.DMN1.FMR.COM>
References: <67DCA285A2D7754280D3B8E88EB548020C946925@MSGBOSCLB2WIN.DMN1.FMR.COM>
Message-ID: <971536df0609291218g1495bba8l73cfbc42194166a7@mail.gmail.com>

Here are two possibilities.  The first use trellis.focus/trellis/unfocus to
add text subsequent to drawing the xyplot and the second uses a
custom panel:

xyplot(x ~ x, data = data.frame(x = 1:10))
trellis.focus("panel", 1, 1)
panel.text(x=2, y=4, labels="Text")
trellis.unfocus()

xyplot(x ~ x, data = data.frame(x = 1:10), panel = function(...) {
	panel.xyplot(...)
	panel.text(x=2, y=4, labels="Text")
})

On 9/29/06, McGehee, Robert <Robert.McGehee at geodecapital.com> wrote:
> Hello,
> I've decided to take the leap and try my hand at the lattice package,
> though I am getting stuck at what one might consider a trivial problem,
> plotting text at a point in a graph. Apologies in advance if (that) I'm
> missing something extremely basic.
>
> Consider in base graphics:
> > plot(1:10)
> > text(2, 4, "Text")
> In the above you will see text centered at the point (2, 4) on the
> graph.
>
> Now I would like to try to do the same thing using the lattice package:
>
> > xyplot(x ~ x, data = data.frame(x = 1:10))
> > ltext(x=2, y=4, labels="Text")
> > panel.text(x=2, y=4, labels="Text")
> > grid.text(label="Text", x=2, y=4)
> > grid.text(label="Text", x=unit(2, "native"), y=unit(4, "native"))
>
> None of the above four commands puts the "Text" at the (2, 4) point on
> the graph. Any help with this would be appreciated! Also, if I have more
> than one panel and would like to place text at different points on
> different panels how would I do this?
>
> Also, note that I'm hoping to use text to label interesting points in a
> levelplot, but am using the above xyplot as an example.
>
> Thanks,
> Robert
>
>
> Robert McGehee
> Quantitative Analyst
> Geode Capital Management, LLC
> 53 State Street, 5th Floor | Boston, MA | 02109
> Tel: 617/392-8396    Fax:617/476-6389
> mailto:robert.mcgehee at geodecapital.com
>
>
>
> This e-mail, and any attachments hereto, are intended for us...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Jue.Wang2 at sanofi-aventis.com  Fri Sep 29 21:19:01 2006
From: Jue.Wang2 at sanofi-aventis.com (Jue.Wang2 at sanofi-aventis.com)
Date: Fri, 29 Sep 2006 15:19:01 -0400
Subject: [R] Wilcoxon Rank test of Package Coin
Message-ID: <943529EF506BBD4797015900B6F6C25DE30727@brwsmxsusr01.pharma.aventis.com>

Hi,

I am running the following example which can be found on page 12 of the pdf file of COIN package

wt<-wilcox_test(pd~age,data=water_transfer,distribution="exact", conf.int=TRUE)

"wt" actually contains the estimate of difference in location and the confidence interval of it.  I am just wondering how can I extract these values? From the examples, I understand that the Wilcoxon statistic can be extracted as
statitic(wt,"linear") 
and the exact two-sides p-value can be extracted as 
pvalue(wt) 

But I haven't been able to find the correct function to extract the estimates of difference median of y-x) and the confidence interval of this estimates. Shall I go back to the  package ExactRankTest? I do know how to extracted these values from function wilcox.exact there.

Thank you
 

Jue Wang, Biostatistician
Contracted Position for Preclinical & Research Biostatistics
PrO Unlimited
(908) 231-3022


From bates at stat.wisc.edu  Fri Sep 29 22:09:29 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 29 Sep 2006 15:09:29 -0500
Subject: [R] linear gradient in nls
In-Reply-To: <001901c6e3f6$b6637870$6402a8c0@lama>
References: <001901c6e3f6$b6637870$6402a8c0@lama>
Message-ID: <40e66e0b0609291309vc901478u42c97a3df62cdbce@mail.gmail.com>

On 9/29/06, H. Paul Benton <hpbenton at scripps.edu> wrote:
> Hello,
>
>
>
>             I hope this doesn't turn into a statistics question but here I
> go. I am using the nls function with a Gaussian distribution, see coding
> below. When I run the nls I get an error back saying that I have a linear
> gradient. I then, of course am unable to do anything else. The data that I
> am using are intensity values from some mass spectrometry data. Is there
> something I can do to stop the linear gradient, and I dare to ask why am I
> getting a linear gradient. I apologies if I haven't supplied enough
> information or I have made some mistake in my coding. The coding below works
> on some of the data but not all.
>
>
>
>             Thank you for your time,
>
>
>
>             Paul Benton
>
>
>
> brseq <- seq(-4.0, 4.0, by=0.1)
>
> AB<- A[,1]/A[,2]
>
> lgAB<-log(AB)
>
> freq_AB <-hist(lgAB, type="o", breaks=brseq, plot=F
>
> freq.tab <- as.data.frame(cbind(brseq, freq_AB$counts)
>
> class(fo <- (x ~ (A/(sig*sqrt(2*pi)))* exp(-1*((bin-mu)^2/(2* sig^2)))))
>
> nls.AB <- nls(fo,data=freq.tab, start= list(A=0.1*len, mu=0.01, sig=0.5),
> trace=TRUE)

I think you mean "singular gradient" not "linear gradient".  This
generally indicates that there is not enough information in the data
to estimate all the parameters that you are trying to estimate.  I see
that you have set trace = TRUE, which is a good start.  Take a look at
the progress of the parameter estimates during the iterations and look
at your data plots to see if the estimates appear to be sensible.


From ggrothendieck at gmail.com  Fri Sep 29 22:11:08 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 29 Sep 2006 16:11:08 -0400
Subject: [R] X-axis labels in histograms drawn by the "truehist" function
In-Reply-To: <000601c6e3ec$4424fea0$7c94100a@win.ad.jhu.edu>
References: <000601c6e3ec$4424fea0$7c94100a@win.ad.jhu.edu>
Message-ID: <971536df0609291311u661fd0en7af452773774c440@mail.gmail.com>

Try this:

f <- function(x,xlab) truehist(x, xlab = xlab)
mapply(f, as.data.frame(X), colnames(X))


On 9/29/06, Ravi Varadhan <rvaradhan at jhmi.edu> wrote:
> Hi,
>
>
>
> I have a simple problem that I would appreciate getting some tips.  I am
> using the "truehist" function within an "apply" call to plot multiple
> histograms.  I can't figure out how to get truehist to use the column names
> of the matrix as the labels for the x-axis of the histograms.
>
>
>
> Here is a simple example:
>
>
>
>
>
> X <- matrix(runif(4000),ncol=4)
>
> colnames(X) <- c("X1","X2","X3","X4")
>
> par(mfrow=c(2,2))
>
> apply(X, 2, function(x)truehist(x))
>
>
>
> In this example, I would like the x-labels of the histograms to be "X1",
> "X2", etc.
>
>
>
> Any help is appreciated.
>
>
>
> Best,
>
> Ravi.
>
> ----------------------------------------------------------------------------
> -------
>
> Ravi Varadhan, Ph.D.
>
> Assistant Professor, The Center on Aging and Health
>
> Division of Geriatric Medicine and Gerontology
>
> Johns Hopkins University
>
> Ph: (410) 502-2619
>
> Fax: (410) 614-9625
>
> Email: rvaradhan at jhmi.edu
>
> Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html
>
>
>
> ----------------------------------------------------------------------------
> --------
>
>
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From deepayan.sarkar at gmail.com  Fri Sep 29 22:13:20 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Fri, 29 Sep 2006 13:13:20 -0700
Subject: [R] Plotting text with lattice
In-Reply-To: <971536df0609291218g1495bba8l73cfbc42194166a7@mail.gmail.com>
References: <67DCA285A2D7754280D3B8E88EB548020C946925@MSGBOSCLB2WIN.DMN1.FMR.COM>
	<971536df0609291218g1495bba8l73cfbc42194166a7@mail.gmail.com>
Message-ID: <eb555e660609291313m10743d20le1373b111531cca@mail.gmail.com>

On 9/29/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Here are two possibilities.  The first use trellis.focus/trellis/unfocus to
> add text subsequent to drawing the xyplot and the second uses a
> custom panel:
>
> xyplot(x ~ x, data = data.frame(x = 1:10))
> trellis.focus("panel", 1, 1)
> panel.text(x=2, y=4, labels="Text")
> trellis.unfocus()
>
> xyplot(x ~ x, data = data.frame(x = 1:10), panel = function(...) {
>         panel.xyplot(...)
>         panel.text(x=2, y=4, labels="Text")
> })

Right, on a more general note, this is necessary because it is not clear what

ltext(x=2, y=4, labels="Text")

should do for a multi-panel plot. On an even more general note, you
will keep getting in trouble when using lattice if you (1) try to
follow the "incremental addition" approach of standard graphics or (2)
use the par() system in any way.

Deepayan

> On 9/29/06, McGehee, Robert <Robert.McGehee at geodecapital.com> wrote:
> > Hello,
> > I've decided to take the leap and try my hand at the lattice package,
> > though I am getting stuck at what one might consider a trivial problem,
> > plotting text at a point in a graph. Apologies in advance if (that) I'm
> > missing something extremely basic.
> >
> > Consider in base graphics:
> > > plot(1:10)
> > > text(2, 4, "Text")
> > In the above you will see text centered at the point (2, 4) on the
> > graph.
> >
> > Now I would like to try to do the same thing using the lattice package:
> >
> > > xyplot(x ~ x, data = data.frame(x = 1:10))
> > > ltext(x=2, y=4, labels="Text")
> > > panel.text(x=2, y=4, labels="Text")
> > > grid.text(label="Text", x=2, y=4)
> > > grid.text(label="Text", x=unit(2, "native"), y=unit(4, "native"))
> >
> > None of the above four commands puts the "Text" at the (2, 4) point on
> > the graph. Any help with this would be appreciated! Also, if I have more
> > than one panel and would like to place text at different points on
> > different panels how would I do this?
> >
> > Also, note that I'm hoping to use text to label interesting points in a
> > levelplot, but am using the above xyplot as an example.
> >
> > Thanks,
> > Robert
> >
> >
> > Robert McGehee
> > Quantitative Analyst
> > Geode Capital Management, LLC
> > 53 State Street, 5th Floor | Boston, MA | 02109
> > Tel: 617/392-8396    Fax:617/476-6389
> > mailto:robert.mcgehee at geodecapital.com
> >
> >
> >
> > This e-mail, and any attachments hereto, are intended for us...{{dropped}}
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
http://www.stat.wisc.edu/~deepayan/


From borghor at yahoo.ca  Fri Sep 29 22:13:36 2006
From: borghor at yahoo.ca (bertrand toupin)
Date: Fri, 29 Sep 2006 16:13:36 -0400 (EDT)
Subject: [R] index vector
Message-ID: <20060929201336.12647.qmail@web42205.mail.scd.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060929/78a82803/attachment.pl 

From deming.mi at vanderbilt.edu  Fri Sep 29 22:42:22 2006
From: deming.mi at vanderbilt.edu (Deming Mi)
Date: Fri, 29 Sep 2006 15:42:22 -0500
Subject: [R] help with distance matrix
Message-ID: <004601c6e407$c3f20540$1b0bc80a@msrc.mc.vanderbilt.edu>

Dear R users,
I have computed a diagonal distance matrix and it's in a matrix format (I 
did not use the dist() function).  However hclust() does not take my 
distance matrix as correct input object.  It seems that hclust() only accept 
the distance matrix from dist() as input object.  Does anybody know how to 
make my distance matrix work with hclust()?  Thank you!

Deming  Mi


From jholtman at gmail.com  Fri Sep 29 22:46:33 2006
From: jholtman at gmail.com (jim holtman)
Date: Fri, 29 Sep 2006 16:46:33 -0400
Subject: [R] index vector
In-Reply-To: <20060929201336.12647.qmail@web42205.mail.scd.yahoo.com>
References: <20060929201336.12647.qmail@web42205.mail.scd.yahoo.com>
Message-ID: <644e1f320609291346g79360ea5g387d60051d65f55a@mail.gmail.com>

Is this what you want?

> x <- matrix(1:25,5)
> x[c(2,5),5] <- -9999
> x
     [,1] [,2] [,3] [,4]  [,5]
[1,]    1    6   11   16    21
[2,]    2    7   12   17 -9999
[3,]    3    8   13   18    23
[4,]    4    9   14   19    24
[5,]    5   10   15   20 -9999
> x[x[,5] == -9999, 5] <- NA
> x
     [,1] [,2] [,3] [,4] [,5]
[1,]    1    6   11   16   21
[2,]    2    7   12   17   NA
[3,]    3    8   13   18   23
[4,]    4    9   14   19   24
[5,]    5   10   15   20   NA
>
>


On 9/29/06, bertrand toupin <borghor at yahoo.ca> wrote:
> Hi!  1st time I'm posting here.  I'm beginning to learn R and I've encountered a problem that I'm unable to solve so far.
>
> I have  a 20 000 x 5 matrix.  In the 5th column, I have elevation.  Missing value are actually put to -99999.  I want to track down the index of those values and replace them with NA.  I've read that to replace, the command "replace" is enough.  I just don't know how to construct the index vector that contains the index of -99999 values.
>
> Hope this makes sense,
> Thanks!
> Philippe
>
>
> ---------------------------------
> Share your photos with the people who matter at Yahoo! Canada Photos
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From bbands at gmail.com  Fri Sep 29 22:49:28 2006
From: bbands at gmail.com (BBands)
Date: Fri, 29 Sep 2006 13:49:28 -0700
Subject: [R] a decimal aligned column
In-Reply-To: <1159480192.4037.41.camel@localhost.localdomain>
References: <6e8360ad0609281231lbeec4f5la9c44fa177268597@mail.gmail.com>
	<1159480192.4037.41.camel@localhost.localdomain>
Message-ID: <6e8360ad0609291349p147cc5fdjebf231258643b64f@mail.gmail.com>

Many thanks to Marc Schwartz who helped me work through this problem
in a series of off list exchanges.

     jab
-- 
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.


From borghor at yahoo.ca  Fri Sep 29 22:49:59 2006
From: borghor at yahoo.ca (bertrand toupin)
Date: Fri, 29 Sep 2006 16:49:59 -0400 (EDT)
Subject: [R] index vector
In-Reply-To: <644e1f320609291346g79360ea5g387d60051d65f55a@mail.gmail.com>
Message-ID: <20060929204959.27293.qmail@web42209.mail.scd.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060929/2752589e/attachment.pl 

From mschwartz at mn.rr.com  Fri Sep 29 22:54:43 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Fri, 29 Sep 2006 15:54:43 -0500
Subject: [R] index vector
In-Reply-To: <20060929201336.12647.qmail@web42205.mail.scd.yahoo.com>
References: <20060929201336.12647.qmail@web42205.mail.scd.yahoo.com>
Message-ID: <1159563284.20112.37.camel@localhost.localdomain>

On Fri, 2006-09-29 at 16:13 -0400, bertrand toupin wrote:
> Hi!  1st time I'm posting here.  I'm beginning to learn R and I've
> encountered a problem that I'm unable to solve so far.
> 
> I have  a 20 000 x 5 matrix.  In the 5th column, I have elevation.
> Missing value are actually put to -99999.  I want to track down the
> index of those values and replace them with NA.  I've read that to
> replace, the command "replace" is enough.  I just don't know how to
> construct the index vector that contains the index of -99999 values.
> 
> Hope this makes sense,
> Thanks!
> Philippe


See ?is.na and note the use of:

  is.na(x) <- value


Example:

> mat <- matrix(sample(50), 10, 5)

> mat
      [,1] [,2] [,3] [,4] [,5]
 [1,]   24   39   40   30    5
 [2,]    8   44    3   34   47
 [3,]   23   12   16   14   45
 [4,]   35   26    2   11    6
 [5,]   13   15   42   33   19
 [6,]    7   36   31   49   37
 [7,]   29   41    9   27    4
 [8,]   48    1   22   25   17
 [9,]   43   32   28   38   20
[10,]   18   50   46   21   10


# Set some values in column 5 to -99999
> mat[sample(10, 3), 5] <- -99999


> mat
      [,1] [,2] [,3] [,4]   [,5]
 [1,]   24   39   40   30      5
 [2,]    8   44    3   34     47
 [3,]   23   12   16   14     45
 [4,]   35   26    2   11      6
 [5,]   13   15   42   33 -99999
 [6,]    7   36   31   49 -99999
 [7,]   29   41    9   27      4
 [8,]   48    1   22   25     17
 [9,]   43   32   28   38     20
[10,]   18   50   46   21 -99999

# Use which to get the indices within column 5
# of those values which are -99999
# See ?which
> which(mat[, 5] == -99999)
[1]  5  6 10


# Now extend that and set those to NA
> is.na(mat[, 5]) <- which(mat[, 5] == -99999)

> mat
      [,1] [,2] [,3] [,4] [,5]
 [1,]   24   39   40   30    5
 [2,]    8   44    3   34   47
 [3,]   23   12   16   14   45
 [4,]   35   26    2   11    6
 [5,]   13   15   42   33   NA
 [6,]    7   36   31   49   NA
 [7,]   29   41    9   27    4
 [8,]   48    1   22   25   17
 [9,]   43   32   28   38   20
[10,]   18   50   46   21   NA



Note one other possibility, which is that if you used one of the
read.table() family functions to read in a delimited ASCII file
containing the data set, you can set the 'na.strings' argument to
"-99999" and have it set these to NA upon importing.  See ?read.table
for more information.

HTH,

Marc Schwartz


From borghor at yahoo.ca  Fri Sep 29 23:12:31 2006
From: borghor at yahoo.ca (bertrand toupin)
Date: Fri, 29 Sep 2006 17:12:31 -0400 (EDT)
Subject: [R] index vector
In-Reply-To: <1159563284.20112.37.camel@localhost.localdomain>
Message-ID: <20060929211231.56563.qmail@web42201.mail.scd.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060929/2e6e4668/attachment.pl 

From Jue.Wang2 at sanofi-aventis.com  Fri Sep 29 23:16:44 2006
From: Jue.Wang2 at sanofi-aventis.com (Jue.Wang2 at sanofi-aventis.com)
Date: Fri, 29 Sep 2006 17:16:44 -0400
Subject: [R] Confidence interval in the Wilcoxon exact test
Message-ID: <943529EF506BBD4797015900B6F6C25DE30729@brwsmxsusr01.pharma.aventis.com>

Hi,

Two functions wilcox.exact and wilcox_test give slightly different confidence intervals of the difference of the medians: for example
  
y<-c(0,0,1.081,0.594,0,0.769,0,0.009,0,0,0.798,0.405,0.498,0.946,1.35,1.149,0.528)
x<-c(rep(1,10),rep(2,7))

aa<-wilcox.exact(y~x,conf.int=TRUE)
bb<-wilcox_test(y~factor(x),distribution="exact",conf.int=TRUE)

aa
bb

Does anyone know why?

Thank you

Jue Wang, Biostatistician
Contracted Position for Preclinical & Research Biostatistics
PrO Unlimited
(908) 231-3022


From savdekar at hotmail.com  Fri Sep 29 23:41:25 2006
From: savdekar at hotmail.com (Pankaj Savdekar)
Date: Sat, 30 Sep 2006 03:11:25 +0530
Subject: [R] Build error on Windows
In-Reply-To: <451D3523.7020703@stats.uwo.ca>
Message-ID: <BAY126-F27AAA98ADEFF5C3A6F6CE0D2180@phx.gbl>

Thanks for the quick reply.

>On 9/29/2006 8:53 AM, Pankaj Savdekar wrote:
>>Hi,
>>
>>I'm trying to build R-2.3.1 on windows, but make gives me following error 
>>while building pkg-base:
>>---------- Making package base ------------
>>   adding build stamp to DESCRIPTION
>>make[4]: *** [frontmatter] Error 1
>>make[3]: *** [all] Error 2
>>make[2]: *** [pkg-base] Error 2
>>make[1]: *** [rpackage] Error 2
>>make: *** [all] Error 2
>>
>>Please note that R.exe, Rterm.exe, Rgui.exe, RCmd.exe are build without 
>>any errors.
>>
>>I have three questions, can anyone please help me to resolve it?
>>1. How to solve (or get more details) of the above mentioned error?
>
>You need to look through the make files, to see what was happening. Reading 
>the messages in reverse order:  "make all" called "make rpackage" and so on 
>to "make frontmatter".  The errors don't tell you which makefiles these are 
>in, but the "frontmatter" target only occurs in src/gnuwin32/MakePkg.  You 
>could try deleting the "@" signs from the lines for that target to see 
>exactly what was happening when the error was generated.

Yes I could figure out the source of 'frontmatter', but my problem is, there 
is no error message. I tried 'make -d' too. I tried removing '@', but no 
change.

>I'd guess that this is happening because your build is messed up:  the base 
>package is used in later build steps.  If you start from a clean checkout 
>and just call "make", you probably won't see this.

Is there any way to check what could have been wrong in building base 
package?

>
>>2. When I run Rterm.exe it takes 25-30% CPU time, but doesn't provide me 
>>the command prompt, is there anyway to log/trace, to know the issues in 
>>Rterm?
>>3. Despite error mentioned above (Frontmatter) I tried building other 
>>packages and was able to build few. But still RGui.exe shows warning 
>>saying there is no package called 'datasets', 'utils' etc. Can someone 
>>please point me how to resolve this issue?
>
>Probably the same cause.  base is messed up, and that's going to cause all 
>sorts of trouble.
>
>Duncan Murdoch
>>
>>Thanks in advance.
>>
>>Pankaj
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide 
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>


From devarshipant at aim.com  Sat Sep 30 00:03:48 2006
From: devarshipant at aim.com (ed_kowal)
Date: Fri, 29 Sep 2006 15:03:48 -0700 (PDT)
Subject: [R] Using censummary on censored data
Message-ID: <6573511.post@talk.nabble.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060929/f20a5d1c/attachment.pl 

From tlumley at u.washington.edu  Sat Sep 30 00:15:32 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 29 Sep 2006 15:15:32 -0700 (PDT)
Subject: [R] Confidence interval in the Wilcoxon exact test
In-Reply-To: <943529EF506BBD4797015900B6F6C25DE30729@brwsmxsusr01.pharma.aventis.com>
References: <943529EF506BBD4797015900B6F6C25DE30729@brwsmxsusr01.pharma.aventis.com>
Message-ID: <Pine.LNX.4.64.0609291512430.3997@homer23.u.washington.edu>

On Fri, 29 Sep 2006, Jue.Wang2 at sanofi-aventis.com wrote:

> Hi,
>
> Two functions wilcox.exact and wilcox_test give slightly different 
> confidence intervals of the difference of the medians: for example
>
> y<-c(0,0,1.081,0.594,0,0.769,0,0.009,0,0,0.798,0.405,0.498,0.946,1.35,1.149,0.528)
> x<-c(rep(1,10),rep(2,7))
>
> aa<-wilcox.exact(y~x,conf.int=TRUE)
> bb<-wilcox_test(y~factor(x),distribution="exact",conf.int=TRUE)
>
> aa
> bb
>
> Does anyone know why?
>

The help for wilcox.test says
      'wilcox.exact' in 'exactRankTests' covers much of the same ground,
      but also produces exact p-values in the presence of ties.
so that is probably the explanation.

However, since your x and y can't really have come from distributions 
differing only by a location shift, the confidence interval isn't valid 
anyway (and the test is valid only to reject the strong null hypothesis 
that the distributions are the same, not anything about medians)

 	-thomas


From angela.baldo at ARS.USDA.GOV  Sat Sep 30 00:23:01 2006
From: angela.baldo at ARS.USDA.GOV (angela baldo)
Date: Fri, 29 Sep 2006 18:23:01 -0400
Subject: [R] scatter3d() model.summary coefficients?
Message-ID: <1159568581.3108.83.camel@kadu>

Hello All,

I am a R newbie and am probably misinterpreting something really
obvious...

In the Rcmdr package there is a scatter3d() function that can fit a
curve and also provide coefficients for the model.  If I'm understanding
this right, I think it's calling the lower level stats package function
lm(), which is the part that actually does the curve fitting.

Anyway, what has me perplexed is that the model summary from scatter3d()
has different coefficients than the one generated by lm().  However, the
actual surface plotted by scatter3d() looks like the function generated
by lm().

In the scatter3d() docs I didn't see anything about transforming the
coefficients or changing them somehow - perhaps I have not been looking
in the right place?

I'm using a Linux box: 2.6.17-1.2187_FC5smp, R version 2.3.1, Rcmdr
version 1.2-0, in case that helps.

Thanks very much for any enlightenment!

anja


Here's an example of the output on the same data by both functions.  If
anyone wants the dataset, let me know:

> scatter3d(samples$x1, samples$y, samples$x2, fit="linear",
residuals=TRUE, bg="white", axis.scales=TRUE, grid=TRUE,
ellipsoid=FALSE, xlab="x1", ylab="y", zlab="x2", model.summary=TRUE)
$linear

Call:
lm(formula = y ~ x + z)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.096984 -0.022303  0.004758  0.029354  0.091188 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)  0.708945   0.007005  101.20   <2e-16 ***
x            0.278540   0.011262   24.73   <2e-16 ***
z           -0.688175   0.011605  -59.30   <2e-16 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 

Residual standard error: 0.03936 on 105 degrees of freedom
Multiple R-Squared: 0.972,	Adjusted R-squared: 0.9715 
F-statistic:  1822 on 2 and 105 DF,  p-value: < 2.2e-16 

> summary(lm(formula=samples$y~samples$x1+samples$x2))

Call:
lm(formula = samples$y ~ samples$x1 + samples$x2)

Residuals:
    Min      1Q  Median      3Q     Max 
-7865.0 -1808.6   385.8  2380.5  7394.9 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) 92204.502   1323.217   69.68   <2e-16 ***
samples$x1    225.882      9.133   24.73   <2e-16 ***
samples$x2   -558.076      9.411  -59.30   <2e-16 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 

Residual standard error: 3192 on 105 degrees of freedom
Multiple R-Squared: 0.972,	Adjusted R-squared: 0.9715 
F-statistic:  1822 on 2 and 105 DF,  p-value: < 2.2e-16 

-- 
Angela M. Baldo
Computational Biologist
USDA, ARS
Plant Genetic Resources Unit
& Grape Genetics Research Unit
New York State Agricultural Experiment Station
630 W. North Street
Geneva, NY  14456-0462
USA

voice 315 787-2413 or 607 254-9413
fax   315 787-2339 or 607 254-9339

angela.baldo at ars.usda.gov
http://www.ars.usda.gov/NAA/Geneva


From amb82 at cornell.edu  Sat Sep 30 00:25:07 2006
From: amb82 at cornell.edu (angela baldo)
Date: Fri, 29 Sep 2006 18:25:07 -0400
Subject: [R] scatter3d() model.summary coefficients?
Message-ID: <1159568707.3108.85.camel@kadu>

Hello All,

I am a R newbie and am probably misinterpreting something really
obvious...

In the Rcmdr package there is a scatter3d() function that can fit a
curve and also provide coefficients for the model.  If I'm understanding
this right, I think it's calling the lower level stats package function
lm(), which is the part that actually does the curve fitting.

Anyway, what has me perplexed is that the model summary from scatter3d()
has different coefficients than the one generated by lm().  However, the
actual surface plotted by scatter3d() looks like the function generated
by lm().

In the scatter3d() docs I didn't see anything about transforming the
coefficients or changing them somehow - perhaps I have not been looking
in the right place?

I'm using a Linux box: 2.6.17-1.2187_FC5smp, R version 2.3.1, Rcmdr
version 1.2-0, in case that helps.

Thanks very much for any enlightenment!

anja


Here's an example of the output on the same data by both functions.  If
anyone wants the dataset, let me know:

> scatter3d(samples$x1, samples$y, samples$x2, fit="linear",
residuals=TRUE, bg="white", axis.scales=TRUE, grid=TRUE,
ellipsoid=FALSE, xlab="x1", ylab="y", zlab="x2", model.summary=TRUE)
$linear

Call:
lm(formula = y ~ x + z)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.096984 -0.022303  0.004758  0.029354  0.091188 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)  0.708945   0.007005  101.20   <2e-16 ***
x            0.278540   0.011262   24.73   <2e-16 ***
z           -0.688175   0.011605  -59.30   <2e-16 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 

Residual standard error: 0.03936 on 105 degrees of freedom
Multiple R-Squared: 0.972,      Adjusted R-squared: 0.9715 
F-statistic:  1822 on 2 and 105 DF,  p-value: < 2.2e-16 

> summary(lm(formula=samples$y~samples$x1+samples$x2))

Call:
lm(formula = samples$y ~ samples$x1 + samples$x2)

Residuals:
    Min      1Q  Median      3Q     Max 
-7865.0 -1808.6   385.8  2380.5  7394.9 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) 92204.502   1323.217   69.68   <2e-16 ***
samples$x1    225.882      9.133   24.73   <2e-16 ***
samples$x2   -558.076      9.411  -59.30   <2e-16 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 

Residual standard error: 3192 on 105 degrees of freedom
Multiple R-Squared: 0.972,      Adjusted R-squared: 0.9715 
F-statistic:  1822 on 2 and 105 DF,  p-value: < 2.2e-16


From Jue.Wang2 at sanofi-aventis.com  Sat Sep 30 00:50:58 2006
From: Jue.Wang2 at sanofi-aventis.com (Jue.Wang2 at sanofi-aventis.com)
Date: Fri, 29 Sep 2006 18:50:58 -0400
Subject: [R] if then else
Message-ID: <943529EF506BBD4797015900B6F6C25DE3072A@brwsmxsusr01.pharma.aventis.com>

What is the correct form to write statement meaning:

if (a==1) {b=2; c=3}; else {b=0; c=0};

Thank you


Jue Wang, Biostatistician
Contracted Position for Preclinical & Research Biostatistics
PrO Unlimited
(908) 231-3022


From jfox at mcmaster.ca  Sat Sep 30 00:51:28 2006
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 29 Sep 2006 18:51:28 -0400
Subject: [R] scatter3d() model.summary coefficients?
In-Reply-To: <1159568581.3108.83.camel@kadu>
Message-ID: <20060929225128.KLGY13653.tomts36-srv.bellnexxia.net@JohnDesktop8300>

Dear Anja,

As you suggest, models in scatter3d() are fit via lm() and also mgcv().
scatter3d() rescales the three variables to fit in the unit cube;  I believe
that the new version of rgl makes the rescaling unnecessary, so eventually
I'll probably rework scatter3d() to avoid it. It would be better if
?scatter3d mentioned this; I've made that change in the development version
of the package.

BTW, a nice thing about R is that the source code is there, so you can look
to see what a function does.

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of angela baldo
> Sent: Friday, September 29, 2006 5:23 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] scatter3d() model.summary coefficients?
> 
> Hello All,
> 
> I am a R newbie and am probably misinterpreting something 
> really obvious...
> 
> In the Rcmdr package there is a scatter3d() function that can 
> fit a curve and also provide coefficients for the model.  If 
> I'm understanding this right, I think it's calling the lower 
> level stats package function lm(), which is the part that 
> actually does the curve fitting.
> 
> Anyway, what has me perplexed is that the model summary from 
> scatter3d() has different coefficients than the one generated 
> by lm().  However, the actual surface plotted by scatter3d() 
> looks like the function generated by lm().
> 
> In the scatter3d() docs I didn't see anything about 
> transforming the coefficients or changing them somehow - 
> perhaps I have not been looking in the right place?
> 
> I'm using a Linux box: 2.6.17-1.2187_FC5smp, R version 2.3.1, 
> Rcmdr version 1.2-0, in case that helps.
> 
> Thanks very much for any enlightenment!
> 
> anja
> 
> 
> Here's an example of the output on the same data by both 
> functions.  If anyone wants the dataset, let me know:
> 
> > scatter3d(samples$x1, samples$y, samples$x2, fit="linear",
> residuals=TRUE, bg="white", axis.scales=TRUE, grid=TRUE, 
> ellipsoid=FALSE, xlab="x1", ylab="y", zlab="x2", 
> model.summary=TRUE) $linear
> 
> Call:
> lm(formula = y ~ x + z)
> 
> Residuals:
>       Min        1Q    Median        3Q       Max 
> -0.096984 -0.022303  0.004758  0.029354  0.091188 
> 
> Coefficients:
>              Estimate Std. Error t value Pr(>|t|)    
> (Intercept)  0.708945   0.007005  101.20   <2e-16 ***
> x            0.278540   0.011262   24.73   <2e-16 ***
> z           -0.688175   0.011605  -59.30   <2e-16 ***
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
> 
> Residual standard error: 0.03936 on 105 degrees of freedom
> Multiple R-Squared: 0.972,	Adjusted R-squared: 0.9715 
> F-statistic:  1822 on 2 and 105 DF,  p-value: < 2.2e-16 
> 
> > summary(lm(formula=samples$y~samples$x1+samples$x2))
> 
> Call:
> lm(formula = samples$y ~ samples$x1 + samples$x2)
> 
> Residuals:
>     Min      1Q  Median      3Q     Max 
> -7865.0 -1808.6   385.8  2380.5  7394.9 
> 
> Coefficients:
>              Estimate Std. Error t value Pr(>|t|)    
> (Intercept) 92204.502   1323.217   69.68   <2e-16 ***
> samples$x1    225.882      9.133   24.73   <2e-16 ***
> samples$x2   -558.076      9.411  -59.30   <2e-16 ***
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
> 
> Residual standard error: 3192 on 105 degrees of freedom
> Multiple R-Squared: 0.972,	Adjusted R-squared: 0.9715 
> F-statistic:  1822 on 2 and 105 DF,  p-value: < 2.2e-16 
> 
> --
> Angela M. Baldo
> Computational Biologist
> USDA, ARS
> Plant Genetic Resources Unit
> & Grape Genetics Research Unit
> New York State Agricultural Experiment Station 630 W. North 
> Street Geneva, NY  14456-0462 USA
> 
> voice 315 787-2413 or 607 254-9413
> fax   315 787-2339 or 607 254-9339
> 
> angela.baldo at ars.usda.gov
> http://www.ars.usda.gov/NAA/Geneva
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jfox at mcmaster.ca  Sat Sep 30 00:55:07 2006
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 29 Sep 2006 18:55:07 -0400
Subject: [R] RODBC ERROR on Rcmdr install
In-Reply-To: <20060929125237518.00000001084@gypsy-l01>
Message-ID: <20060929225507.JYAO18394.tomts5-srv.bellnexxia.net@JohnDesktop8300>

Dear Sarosh,

As I understand it, RODBC isn't useful on non-Windows systems, since the
necessary ODBC drivers aren't available. (Someone will correct me, I'm sure,
if I don't have that entirely straight.) The RODBC package is used in the
Rcmdr to read Excel and some other files under Windows; in the latest
version of the Rcmdr, you won't even see this menu item in non-Windows
systems.

You should, I believe, be able to install and use the Rcmdr package on your
system without RODBC.

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Sarosh Jamal
> Sent: Friday, September 29, 2006 11:53 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] RODBC ERROR on Rcmdr install
> 
> 
> I've been trying to install the RODBC dependency for Rcmdr on 
> Rv2.4 on a SunOS9 system. 
> 
> It claims not to be able to create gcc output files 
> (executables) for the installation.
> 
> This is puzzling since I've been able to install other 
> packages with the same PATH variables and all.
> 
> Feedback would be much appreciated. Thank you!
> 
> Sarosh Jamal
> Geo Computing & IT Specialist, Department of Geography 
> University of Toronto at Mississauga
> e: sarosh.jamal at utoronto.ca
> p: (905) 569-4497
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ggrothendieck at gmail.com  Sat Sep 30 01:46:58 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 29 Sep 2006 19:46:58 -0400
Subject: [R] help with distance matrix
In-Reply-To: <004601c6e407$c3f20540$1b0bc80a@msrc.mc.vanderbilt.edu>
References: <004601c6e407$c3f20540$1b0bc80a@msrc.mc.vanderbilt.edu>
Message-ID: <971536df0609291646o241ce508jae09849991604881@mail.gmail.com>

Try:

as.dist(mymatrix)

On 9/29/06, Deming Mi <deming.mi at vanderbilt.edu> wrote:
> Dear R users,
> I have computed a diagonal distance matrix and it's in a matrix format (I
> did not use the dist() function).  However hclust() does not take my
> distance matrix as correct input object.  It seems that hclust() only accept
> the distance matrix from dist() as input object.  Does anybody know how to
> make my distance matrix work with hclust()?  Thank you!
>
> Deming  Mi
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From joe-byers at utulsa.edu  Sat Sep 30 02:04:15 2006
From: joe-byers at utulsa.edu (Joe Byers)
Date: Fri, 29 Sep 2006 19:04:15 -0500
Subject: [R] Formula aruguments with NLS and model.frame()
In-Reply-To: <451D53BF.1010102@pdf.com>
References: <eeehnh$8hk$1@sea.gmane.org> <451D53BF.1010102@pdf.com>
Message-ID: <451DB47F.7070909@utulsa.edu>

Spencer,

Thank you for taking time to reply and offer suggestions.  garchFit does 
not allow 'formula.mean=~z+arma(p, q)', nor does it allow xreg=(x,y) 
options.  Any thing is xreg is ignored with a warming or error. 

I have debugged garchFit and I know where the code should be modified to 
implement other exogenous variable in the mean equation.  Because mine 
is a NL mean equation I was looking at nls.  You are correct, a simpler 
equation that would operate like the xreg for armaFit, would be the 
place to start.  I also can use debug on my equation for the nls method 
to try and understand the model.frame and environment material.  Thank 
you for the debug suggestion.

Good luck
Joe


Spencer Graves wrote:
>      I haven't seen any replies to this post, so I will offer a couple 
> of comments.
>      First, your example is entirely too complicated and not self 
> contained for me to say much in the limited time I have for this.  I 
> suggest you start by splitting your problem into several steps.  For 
> example, will 'garchFit' allow 'formula.mean' to be something other 
> than '~arma(p, q)', where p and q are nonnegative integers?  If no, I 
> suggest you start by trying to produce your own modification to 
> 'garchFit' so it will accept something like 'formula.mean=~z+arma(p, 
> q)';  I suggest you give your local copy a different name like 
> 'garchFitZ'.  Second, I suggest you cut your example data down to a 
> minimum, e.g, 9 or 20 observations, just enough so the algorithm won't 
> die for some reason that would not occur with a larger data set but 
> small enough so you can quickly print out and study every object the 
> 'garchFit' algorithm produces.
>      Second, I suggest you use 'debug' to walk through your local 
> version of 'garchFit' line by line.  I've found this to be a very 
> powerful way to learn what happens in the internal environment of a 
> function.
>      If you get stuck trying this, please submit another post 
> including commented, minimal, self-contained, reproducible code, as 
> suggested in the posting guide 'www.R-project.org/posting-guide.html'. 
>      Hope this helps.      Spencer Graves
>
> and provide commented, minimal, self-contained, reproducible code.
>
>
>
> Joe W. Byers wrote:
>> I could use some help understanding how nls parses the formula argument
>> to a model.frame and estimates the model.  I am trying to utilize the
>> functionality of the nls formula argument to modify garchFit() to handle
>> other variables in the mean equation besides just an arma(u,v)
>> specification.
>>
>> My nonlinear model is
>>       y<-nls(t~a*sin(w*2*pi/365*id+p)+b*id+int,data=t1,
>>     start=list(w=.5,a=.1,p=.5,b=init.y$coef[2],int=init.y$coef[1] ),
>>     control=list(maxiter=1000000,minFactor=1e-18))
>> where t is change in daily temperatures, id is just a time trend and the
>> a*sin is a one year fourier series.
>>
>> I have tried to debug the nls code using the following code
>> t1<-data.frame(t=as.vector(x),id=index(x))
>> data=t1;
>> formula <- as.formula(t ~ a *sin(w *2* pi/365 * id + p) + b * id + int);
>>       varNames <- all.vars(formula)
>>       algorithm<-'default';
>>       mf <- match.call(definition=nls,expand.dots=FALSE,
>>       call('nls',formula, data=parent.frame(),start,control = 
>> nls.control(),
>>       algorithm = "default", trace = FALSE,
>>       subset, weights, na.action, model = FALSE, lower = -Inf,
>>       upper = Inf));
>>       mWeights<-F;#missing(weights);
>>     start=list(w=.5,a=.1,p=.5,b=init.y$coef[2],int=init.y$coef[1] );
>>       pnames <- names(start);
>>        varNames <- varNames[is.na(match(varNames, pnames, nomatch = 
>> NA))]
>>
>>     varIndex <- sapply(varNames,
>>         function(varName, data, respLength) {
>>               length(eval(as.name(varName), data))%%respLength == 0},
>>                data, length(eval(formula[[2]], data))
>>           );
>>     mf$formula <- as.formula(paste("~", paste(varNames[varIndex],
>>           collapse = "+")), env = environment(formula));
>>     mf$start <- NULL;mf$control <- NULL;mf$algorithm <- NULL;
>>     mf$trace <- NULL;mf$model <- NULL;
>>       mf$lower <- NULL;mf$upper <- NULL;
>>       mf[[1]] <- as.name("model.frame");
>>       mf<-evalq(mf,data);
>>       n<-nrow(mf)
>>       mf<-as.list(mf);
>>       wts <- if (!mWeights)
>>           model.weights(mf)
>>       else rep(1, n)
>>       if (any(wts < 0 | is.na(wts)))
>>           stop("missing or negative weights not allowed")
>>
>>       m <- switch(algorithm,
>>               plinear = nlsModel.plinear(formula, mf, start, wts),
>>               port = nlsModel(formula, mf, start, wts, upper),
>>               nlsModel(formula, mf, start, wts));
>>
>> I am struggling with the environment issues associated with performing
>> these operations.  I did not include the data because it is 9000 
>> observations of temperature data.  If anyone would like the data, I 
>> can provide it or a subset in a csv file.
>>
>>
>> thank you
>> Joe
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>   

From falimadhi at iq.harvard.edu  Sat Sep 30 02:07:28 2006
From: falimadhi at iq.harvard.edu (Ferdinand Alimadhi)
Date: Fri, 29 Sep 2006 20:07:28 -0400
Subject: [R] if then else
In-Reply-To: <943529EF506BBD4797015900B6F6C25DE3072A@brwsmxsusr01.pharma.aventis.com>
References: <943529EF506BBD4797015900B6F6C25DE3072A@brwsmxsusr01.pharma.aventis.com>
Message-ID: <451DB540.6080105@iq.harvard.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060929/075e24ba/attachment.pl 

From dkaplan at education.wisc.edu  Sat Sep 30 02:19:12 2006
From: dkaplan at education.wisc.edu (David Kaplan)
Date: Fri, 29 Sep 2006 19:19:12 -0500
Subject: [R] R 2.3.1 and SPSS 14.0
Message-ID: <451DB800.3080702@education.wisc.edu>

Hi,

Are there any incompatibilities with R 2.3.1 and SPSS 14.0 with regard 
to the read.spss command?

Thanks in advance.

David


-- 
========================================================================
David Kaplan, Ph.D.
Professor
Department of Educational Psychology
University of Wisconsin - Madison
Educational Sciences, Room 1061
1025 W. Johnson Street
Madison, WI 53706

email: dkaplan at education.wisc.edu
Web:   http://www.education.wisc.edu/edpsych/facstaff/kaplan/kaplan.htm
Phone: 608-262-0836
Fax:   608-262-0843


From albmont at centroin.com.br  Sat Sep 30 03:46:48 2006
From: albmont at centroin.com.br (Alberto Vieira Ferreira Monteiro)
Date: Sat, 30 Sep 2006 01:46:48 +0000
Subject: [R] Simple graphics
Message-ID: <200609300146.49216.albmont@centroin.com.br>

Is there any way that I can do something like this:

  png("file.png", width=200, height=200)
  polygon(c(50, 50, 150, 150), c(50, 150, 150, 50))
  dev.off()

and then have a png file with a 100 x 100 pixels rectangle in the middle 
of it? It seems that when I call "plot", it redefines the image coordinates
to some "optimized" value and then I lose any information to draw
exactly what I want.

Alberto Monteiro


From jholtman at gmail.com  Sat Sep 30 03:53:59 2006
From: jholtman at gmail.com (jim holtman)
Date: Fri, 29 Sep 2006 21:53:59 -0400
Subject: [R] Simple graphics
In-Reply-To: <200609300146.49216.albmont@centroin.com.br>
References: <200609300146.49216.albmont@centroin.com.br>
Message-ID: <644e1f320609291853o423ed9c8od5fb50868f593231@mail.gmail.com>

Is this what you want?  You have to reset the margins:

png("file.png", width=200, height=200)
par(mar=c(0,0,0,0))  # reset margins
plot(0, xlim=c(0,200), ylim=c(0,200), type='n')
 polygon(c(50, 50, 150, 150), c(50, 150, 150, 50))
 dev.off()



On 9/29/06, Alberto Vieira Ferreira Monteiro <albmont at centroin.com.br> wrote:
> Is there any way that I can do something like this:
>
>  png("file.png", width=200, height=200)
>  polygon(c(50, 50, 150, 150), c(50, 150, 150, 50))
>  dev.off()
>
> and then have a png file with a 100 x 100 pixels rectangle in the middle
> of it? It seems that when I call "plot", it redefines the image coordinates
> to some "optimized" value and then I lose any information to draw
> exactly what I want.
>
> Alberto Monteiro
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From wangtong at usc.edu  Sat Sep 30 07:00:53 2006
From: wangtong at usc.edu (Tong Wang)
Date: Fri, 29 Sep 2006 22:00:53 -0700
Subject: [R] strange  warning message
In-Reply-To: <4515D871.9090205@stats.uwo.ca>
References: <dcb8ad0b1672b.45155d9d@usc.edu> <4515D871.9090205@stats.uwo.ca>
Message-ID: <ddebfcbe1e233.451d9795@usc.edu>

Hi Duncan:
     Thank you for your help last time,  since I do not use NULL to indicate empty enviroment, I think I'm fine. 
And yes, I did upgrade my R version recently,  but how comes I still get this warning for new files created
and saved after that update ? Is there anyway to get rid of this message ?
     Thanks a lot . 

best  

----- Original Message -----
From: Duncan Murdoch <murdoch at stats.uwo.ca>
Date: Saturday, September 23, 2006 5:59 pm
Subject: Re: [R] strange  warning message
To: Tong Wang <wangtong at usc.edu>
Cc: r-help at stat.math.ethz.ch

> On 9/23/2006 7:15 PM, Tong Wang wrote:
> > Hi Everyone,
> >     I recently start to get this warning message,  while loading 
> files in to R.   Could someone tell me what does it mean ?
> > I am using R 2.3.0 with Emacs on WinXP.
> > 
> >     use of NULL environment is deprecated 
> 
> The files were saved in an earlier version of R, which used NULL to 
> indicate the base environment.  R is telling you that NULL is not a 
> legal environment.  It should be automatically converted to baseenv().
> 
> In a number of cases, people used NULL to indicate an empty 
> environment 
> (even though there was no such thing when NULL was used); if that's 
> true 
> for your code, then you'll need to fix it.  emptyenv() now gives 
> you an 
> empty environment if that's what you really want.
> 
> Duncan Murdoch
>


From wangtong at usc.edu  Sat Sep 30 08:23:48 2006
From: wangtong at usc.edu (Tong Wang)
Date: Fri, 29 Sep 2006 23:23:48 -0700
Subject: [R] How to repeat vectors ?
Message-ID: <dab0b29f1e5c7.451dab04@usc.edu>

Hi,
    If I have a matrix  , say       a11   a12
                                                   a21  a22
    Is there a routine to get:      a11  a12
                                                     a11  a12
                                                     a21   a22
                                                     a21   a22

     Thanks a lot for any help.

best


From wangtong at usc.edu  Sat Sep 30 08:33:36 2006
From: wangtong at usc.edu (Tong Wang)
Date: Fri, 29 Sep 2006 23:33:36 -0700
Subject: [R] How to repeat vectors ?
In-Reply-To: <dab0b29f1e5c7.451dab04@usc.edu>
References: <dab0b29f1e5c7.451dab04@usc.edu>
Message-ID: <dab0842a18049.451dad50@usc.edu>

I just figured out a way to do this: 
          rep.vec <- function(X,n)    return(t(array(rep(X,n),c(length(X),n))))
    
   Then,    apply(MyMatrix, 2, rep.vec,2)

Is there a better way ?  Is there an internal function to repeat a vector or matrix ?

Thanks a lot.


----- Original Message -----
From: Tong Wang <wangtong at usc.edu>
Date: Friday, September 29, 2006 11:23 pm
Subject: How to repeat vectors ?
To: r-help at stat.math.ethz.ch

> Hi,
>    If I have a matrix  , say       a11   a12
>                                                   a21  a22
>    Is there a routine to get:      a11  a12
>                                                     a11  a12
>                                                     a21   a22
>                                                     a21   a22
> 
>     Thanks a lot for any help.
> 
> best
>


From murdoch at stats.uwo.ca  Sat Sep 30 12:21:20 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 30 Sep 2006 06:21:20 -0400
Subject: [R] strange  warning message
In-Reply-To: <ddebfcbe1e233.451d9795@usc.edu>
References: <dcb8ad0b1672b.45155d9d@usc.edu> <4515D871.9090205@stats.uwo.ca>
	<ddebfcbe1e233.451d9795@usc.edu>
Message-ID: <451E4520.20804@stats.uwo.ca>

On 9/30/2006 1:00 AM, Tong Wang wrote:
> Hi Duncan:
>      Thank you for your help last time,  since I do not use NULL to indicate empty enviroment, I think I'm fine. 
> And yes, I did upgrade my R version recently,  but how comes I still get this warning for new files created
> and saved after that update ? Is there anyway to get rid of this message ?

Please show some code that leads to the message.  You shouldn't get it 
in new objects, but there could be a bug in R or in a package you're using.

Duncan Murdoch

>      Thanks a lot . 
> 
> best  
> 
> ----- Original Message -----
> From: Duncan Murdoch <murdoch at stats.uwo.ca>
> Date: Saturday, September 23, 2006 5:59 pm
> Subject: Re: [R] strange  warning message
> To: Tong Wang <wangtong at usc.edu>
> Cc: r-help at stat.math.ethz.ch
> 
>> On 9/23/2006 7:15 PM, Tong Wang wrote:
>>> Hi Everyone,
>>>     I recently start to get this warning message,  while loading 
>> files in to R.   Could someone tell me what does it mean ?
>>> I am using R 2.3.0 with Emacs on WinXP.
>>>
>>>     use of NULL environment is deprecated 
>> The files were saved in an earlier version of R, which used NULL to 
>> indicate the base environment.  R is telling you that NULL is not a 
>> legal environment.  It should be automatically converted to baseenv().
>>
>> In a number of cases, people used NULL to indicate an empty 
>> environment 
>> (even though there was no such thing when NULL was used); if that's 
>> true 
>> for your code, then you'll need to fix it.  emptyenv() now gives 
>> you an 
>> empty environment if that's what you really want.
>>
>> Duncan Murdoch
>>


From ligges at statistik.uni-dortmund.de  Sat Sep 30 12:29:44 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 30 Sep 2006 12:29:44 +0200
Subject: [R] if then else
In-Reply-To: <943529EF506BBD4797015900B6F6C25DE3072A@brwsmxsusr01.pharma.aventis.com>
References: <943529EF506BBD4797015900B6F6C25DE3072A@brwsmxsusr01.pharma.aventis.com>
Message-ID: <451E4718.90403@statistik.uni-dortmund.de>



Jue.Wang2 at sanofi-aventis.com wrote:
> What is the correct form to write statement meaning:
> 
> if (a==1) {b=2; c=3}; else {b=0; c=0};


if (a==1) {b=2; c=3} else {b=0; c=0};

;-)

Uwe Ligges


> Thank you
> 
> 
> Jue Wang, Biostatistician
> Contracted Position for Preclinical & Research Biostatistics
> PrO Unlimited
> (908) 231-3022
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From alex at transitive.com  Sat Sep 30 13:15:59 2006
From: alex at transitive.com (Alex Brown)
Date: Sat, 30 Sep 2006 12:15:59 +0100
Subject: [R] How to repeat vectors ?
In-Reply-To: <dab0842a18049.451dad50@usc.edu>
References: <dab0b29f1e5c7.451dab04@usc.edu> <dab0842a18049.451dad50@usc.edu>
Message-ID: <39FA1A6C-210D-4292-B15A-65BF9BA940C9@transitive.com>

Solution:

m[rep(1:nrow(m),each=2),]

Explanation:

There is a simple and effective way to do this, using array slices.

for your input matrix, m:

 > m=matrix(paste("a",c(11,12,21,22),sep=""),2)
 > m
      [,1]  [,2]
[1,] "a11" "a21"
[2,] "a12" "a22"

you want to create

      [,1]  [,2]
[1,] "a11" "a21"
[2,] "a11" "a21"
[3,] "a12" "a22"
[3,] "a12" "a22"

First, let's just consider the simpler problem of vectors - taking  
the first column as an example:

 > v=m[,1]
 > v
[1] "a11" "a12"

and you want:

[1] "a11" "a11" "a12" "a12"

which is the first element, followed by another copy of the first,  
and then the second, followed by another copy of the second, ie:

 > v[c(1,1,2,2)]
[1] "a11" "a11" "a12" "a12"

can we generate the sequence c(1,1,2,2) automatically?  yes:

 > rep(c(1,2),each=2)
[1] 1 1 2 2

or:

 > rep(1:length(v),each=2)
[1] 1 1 2 2

So let's apply that to the vector:

 > v[rep(1:length(v),each=2)]
[1] "a11" "a11" "a12" "a12"

Going back to the matrix, we can see that we want to do the same  
thing, but to the rows of the matrix, instead of the elements of the  
vector:

Instead of length, we use nrow, and we use the row specifier [r,]

 > m[rep(1:nrow(m),each=2),]
      [,1]  [,2]
[1,] "a11" "a21"
[2,] "a11" "a21"
[3,] "a12" "a22"
[4,] "a12" "a22"


-Alex

On 30 Sep 2006, at 07:33, Tong Wang wrote:

> I just figured out a way to do this:
>           rep.vec <- function(X,n)    return(t(array(rep(X,n),c 
> (length(X),n))))
>
>    Then,    apply(MyMatrix, 2, rep.vec,2)
>
> Is there a better way ?  Is there an internal function to repeat a  
> vector or matrix ?
>
> Thanks a lot.
>
>
> ----- Original Message -----
> From: Tong Wang <wangtong at usc.edu>
> Date: Friday, September 29, 2006 11:23 pm
> Subject: How to repeat vectors ?
> To: r-help at stat.math.ethz.ch
>
>> Hi,
>>    If I have a matrix  , say       a11   a12
>>                                                   a21  a22
>>    Is there a routine to get:      a11  a12
>>                                                     a11  a12
>>                                                     a21   a22
>>                                                     a21   a22
>>
>>     Thanks a lot for any help.
>>
>> best
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From adi at roda.ro  Sat Sep 30 13:48:54 2006
From: adi at roda.ro (Adrian DUSA)
Date: Sat, 30 Sep 2006 14:48:54 +0300
Subject: [R] How to repeat vectors ?
In-Reply-To: <dab0842a18049.451dad50@usc.edu>
References: <dab0b29f1e5c7.451dab04@usc.edu> <dab0842a18049.451dad50@usc.edu>
Message-ID: <200609301448.54898.adi@roda.ro>

Maybe this one?

> MyMatrix <- matrix(1:4, nrow=2)

> MyMatrix
     [,1] [,2]
[1,]    1    3
[2,]    2    4

> MyMatrix[rep(seq(nrow(MyMatrix)), each=2), ]
     [,1] [,2]
[1,]    1    3
[2,]    1    3
[3,]    2    4
[4,]    2    4


HTH,
Adrian

On Saturday 30 September 2006 09:33, Tong Wang wrote:
> I just figured out a way to do this:
>           rep.vec <- function(X,n)   
> return(t(array(rep(X,n),c(length(X),n))))
>
>    Then,    apply(MyMatrix, 2, rep.vec,2)
>
> Is there a better way ?  Is there an internal function to repeat a vector
> or matrix ?
>
> Thanks a lot.
>
>
> ----- Original Message -----
> From: Tong Wang <wangtong at usc.edu>
> Date: Friday, September 29, 2006 11:23 pm
> Subject: How to repeat vectors ?
> To: r-help at stat.math.ethz.ch
>
> > Hi,
> >    If I have a matrix  , say       a11   a12
> >                                                   a21  a22
> >    Is there a routine to get:      a11  a12
> >                                                     a11  a12
> >                                                     a21   a22
> >                                                     a21   a22
> >
> >     Thanks a lot for any help.
> >
> > best

-- 
Adrian DUSA
Arhiva Romana de Date Sociale
Bd. Schitu Magureanu nr.1
050025 Bucuresti sectorul 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101


From ggrothendieck at gmail.com  Sat Sep 30 13:44:43 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 30 Sep 2006 07:44:43 -0400
Subject: [R] How to repeat vectors ?
In-Reply-To: <dab0842a18049.451dad50@usc.edu>
References: <dab0b29f1e5c7.451dab04@usc.edu> <dab0842a18049.451dad50@usc.edu>
Message-ID: <971536df0609300444o79e28dd1gc8752a90d8be1065@mail.gmail.com>

Here are 4 approaches in order from most compact
to least.  #1 only works for numeric matrices, # 2 is
a shorter versio of your solution using rep.vec and # 3
is from Alex's post and is likely what I would
use in practice.

m <- matrix(1:4, 2) # test matrix

# 1 - m must be numeric for this one to work
kronecker(m, rep(1,2))

# 2
apply(m, 2, rep, each = 2) # 2

# 3 - from Alex's post
m[rep(1:nrow(m), each = 2),]

# 4
matrix(rbind(c(m), c(m)), nc = ncol(m))

On 9/30/06, Tong Wang <wangtong at usc.edu> wrote:
> I just figured out a way to do this:
>          rep.vec <- function(X,n)    return(t(array(rep(X,n),c(length(X),n))))
>
>   Then,    apply(MyMatrix, 2, rep.vec,2)
>
> Is there a better way ?  Is there an internal function to repeat a vector or matrix ?
>
> Thanks a lot.
>
>
> ----- Original Message -----
> From: Tong Wang <wangtong at usc.edu>
> Date: Friday, September 29, 2006 11:23 pm
> Subject: How to repeat vectors ?
> To: r-help at stat.math.ethz.ch
>
> > Hi,
> >    If I have a matrix  , say       a11   a12
> >                                                   a21  a22
> >    Is there a routine to get:      a11  a12
> >                                                     a11  a12
> >                                                     a21   a22
> >                                                     a21   a22
> >
> >     Thanks a lot for any help.
> >
> > best
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From sara at gmesintra.com  Sat Sep 30 12:56:20 2006
From: sara at gmesintra.com (Sara Mouro)
Date: Sat, 30 Sep 2006 11:56:20 +0100
Subject: [R] autologistic model? - what package?
Message-ID: <200609301056.k8UAuPaT001586@hypatia.math.ethz.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060930/903aa9a0/attachment.pl 

From Achim.Zeileis at wu-wien.ac.at  Sat Sep 30 14:25:29 2006
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Sat, 30 Sep 2006 14:25:29 +0200 (CEST)
Subject: [R] Heteroskedasticity test
In-Reply-To: <20060929171915.M91848@centroin.com.br>
References: <001601c6e3e0$38db3f70$336f12ac@matrix.com>
	<644e1f320609291000q6ba0855as10077ed9ed82b3a7@mail.gmail.com>
	<20060929171915.M91848@centroin.com.br>
Message-ID: <Pine.LNX.4.64.0609301419310.6234@eowyn>

On Fri, 29 Sep 2006, Alberto Monteiro wrote:

> Is there any heteroskedasticity test in the package? Something
> that would flag a sample like
>
>  x <- c(rnorm(1000), rnorm(1000, 0, 1.2))

The package lmtest contains several tests for heteroskedasticity, in 
particular the Breusch-Pagan test (and also the Goldfeld-Quandt test for 
known change point). Furthermore, some of the structural change tests in 
strucchange can be used to test for non-constant variances, e.g, the 
Nyblom-Hansen test.
Z

> Alberto Monteiro
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From myotisone at gmail.com  Sat Sep 30 14:38:39 2006
From: myotisone at gmail.com (Graham Smith)
Date: Sat, 30 Sep 2006 13:38:39 +0100
Subject: [R] Textmate project drawer: is there a Windows alternative?
Message-ID: <2c75873c0609300538n400bb0bal37ce7090c0d4283b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060930/19506b82/attachment.pl 

From Roger.Bivand at nhh.no  Sat Sep 30 14:39:44 2006
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Sat, 30 Sep 2006 14:39:44 +0200 (CEST)
Subject: [R] autologistic model? - what package?
In-Reply-To: <200609301056.k8UAuPaT001586@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.44.0609301430490.3978-100000@reclus.nhh.no>

On Sat, 30 Sep 2006, Sara Mouro wrote:

> Dear all,
> 
> Could you pleas advise me on the following?
> 
> I need to use general(ized) linear models (binomial distribution + logit
> link function) , to describe the preferred environment of each species (each
> sample is an individual in which I have measured several variables and also
> recorded the species it belongs to) 
> 
>  
> 
> However,  must account for the spatial autrefoocorrelation between
> individuals.
> 
>  
> 
> -> So I think I need something like the so called "autologistic" models.
> isn't it?
> 
> -> What package would you advise me to use for that?
> 

RSiteSearch("autologistic")

tells you what is known about this, which I'm afraid seems to be that no 
such function is available. Your question is very similar to the first hit 
in the site search, and that received no answer. 

If you are willing to try a different framework, the ME() Moran
eigenvector function in the spdep package is a possibility, albeit not yet
well-proven. It adds selected eigenvectors from a centred spatial weights
matrix to the RHS of the glm to "whiten out" spatial dependence, but it
will also "whiten out" or paint over other spatially patterned
mis-specification problems.

>  
> 
> Thank you in advance.
> 
>  
> 
> Best regards,
> 
> Sara Mouro
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From M.P.M.Boks at umcutrecht.nl  Sat Sep 30 15:09:15 2006
From: M.P.M.Boks at umcutrecht.nl (Boks, M.P.M.)
Date: Sat, 30 Sep 2006 15:09:15 +0200
Subject: [R] only need the p-value
References: <2c75873c0609300538n400bb0bal37ce7090c0d4283b@mail.gmail.com>
Message-ID: <2AD792FE1A79F046B908C364C29622F2014A5022@EX05.ds.umcutrecht.nl>

 

Dear R users,

 

I am calculating several cox proportional hazard models after each other (I know this is unusual, but I am just exploring the data). For the purpose of multiple testing correction I need to construct an array of these p-values. However since the output is not an array in itself, I cannot find a way to obtain the p-value only.

 

> attach(tms)      

> 

> goal<-rep(0.7*FREQUENC[1:13],6)

> event<- Surv(TIJD,FREQUENC<goal)

> results<-coxph(event~ TYPETREA)

> summary(results)

 

Call:

coxph(formula = event ~ TYPETREA)

 

  n=76 (2 observations deleted due to missing)

                     coef exp(coef) se(coef)     z     p

TYPETREAnon-guided -0.826     0.438    0.484 -1.71 0.088

 

                   exp(coef) exp(-coef) lower .95 upper .95

TYPETREAnon-guided     0.438       2.28     0.169      1.13

 

Rsquare= 0.041   (max possible= 0.829 )

Likelihood ratio test= 3.2  on 1 df,   p=0.0737

Wald test            = 2.91  on 1 df,   p=0.088

Score (logrank) test = 3.08  on 1 df,   p=0.0794

 

Does anyone now how to extract the p-value?

Many thanks!,

Marco


From murdoch at stats.uwo.ca  Sat Sep 30 15:13:36 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 30 Sep 2006 09:13:36 -0400
Subject: [R] Build error on Windows
In-Reply-To: <BAY126-F27AAA98ADEFF5C3A6F6CE0D2180@phx.gbl>
References: <BAY126-F27AAA98ADEFF5C3A6F6CE0D2180@phx.gbl>
Message-ID: <451E6D80.7000604@stats.uwo.ca>

On 9/29/2006 5:41 PM, Pankaj Savdekar wrote:
> Thanks for the quick reply.
> 
>> On 9/29/2006 8:53 AM, Pankaj Savdekar wrote:
>>> Hi,
>>>
>>> I'm trying to build R-2.3.1 on windows, but make gives me following error 
>>> while building pkg-base:
>>> ---------- Making package base ------------
>>>   adding build stamp to DESCRIPTION
>>> make[4]: *** [frontmatter] Error 1
>>> make[3]: *** [all] Error 2
>>> make[2]: *** [pkg-base] Error 2
>>> make[1]: *** [rpackage] Error 2
>>> make: *** [all] Error 2
>>>
>>> Please note that R.exe, Rterm.exe, Rgui.exe, RCmd.exe are build without 
>>> any errors.
>>>
>>> I have three questions, can anyone please help me to resolve it?
>>> 1. How to solve (or get more details) of the above mentioned error?
>> You need to look through the make files, to see what was happening. Reading 
>> the messages in reverse order:  "make all" called "make rpackage" and so on 
>> to "make frontmatter".  The errors don't tell you which makefiles these are 
>> in, but the "frontmatter" target only occurs in src/gnuwin32/MakePkg.  You 
>> could try deleting the "@" signs from the lines for that target to see 
>> exactly what was happening when the error was generated.
> 
> Yes I could figure out the source of 'frontmatter', but my problem is, there 
> is no error message. I tried 'make -d' too. I tried removing '@', but no 
> change.
> 
>> I'd guess that this is happening because your build is messed up:  the base 
>> package is used in later build steps.  If you start from a clean checkout 
>> and just call "make", you probably won't see this.
> 
> Is there any way to check what could have been wrong in building base 
> package?

You could look at whatever command in the makefile failed, and try it 
outside of the makefile, try variations on it, etc.  I can't give more 
specific advice without more specific information on where the error 
happened.

If you're not used to working in Windows, remember that it is not like 
Unix in several ways.  In particular, you can't delete an open file, 
because it's considered an error for a file to exist unless it has a 
valid directory entry.  If you try to replace a file that is open, the 
replacement will fail, and that may lead to other errors later.

Duncan Murdoch


From murdoch at stats.uwo.ca  Sat Sep 30 15:26:09 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 30 Sep 2006 09:26:09 -0400
Subject: [R] if then else
In-Reply-To: <451E4718.90403@statistik.uni-dortmund.de>
References: <943529EF506BBD4797015900B6F6C25DE3072A@brwsmxsusr01.pharma.aventis.com>
	<451E4718.90403@statistik.uni-dortmund.de>
Message-ID: <451E7071.1040206@stats.uwo.ca>

On 9/30/2006 6:29 AM, Uwe Ligges wrote:
> 
> Jue.Wang2 at sanofi-aventis.com wrote:
>> What is the correct form to write statement meaning:
>>
>> if (a==1) {b=2; c=3}; else {b=0; c=0};
> 
> 
> if (a==1) {b=2; c=3} else {b=0; c=0};

That's valid, but is it "correct form"?  The semicolon at the end is not 
needed.  I'd say it's a bad idea to use one, because it might give a 
mistaken impression about the meaning of something like

  a = 1
      + 2;

It's better to avoid semi-colons whenever possible, to make sure the C 
parser in your brain throws an exception and lets the R parser take over.

Duncan Murdoch


From murdoch at stats.uwo.ca  Sat Sep 30 15:29:19 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 30 Sep 2006 09:29:19 -0400
Subject: [R] Textmate project drawer: is there a Windows alternative?
In-Reply-To: <2c75873c0609300538n400bb0bal37ce7090c0d4283b@mail.gmail.com>
References: <2c75873c0609300538n400bb0bal37ce7090c0d4283b@mail.gmail.com>
Message-ID: <451E712F.8020803@stats.uwo.ca>

On 9/30/2006 8:38 AM, Graham Smith wrote:
> I was reading about the project drawer feature in Textmate, which is Mac
> only.
> 
> Is there a similar feature in a Windows based text editor that works with R.
> This feature sounds really useful.

If you don't get an answer, it would probably be a good idea to describe 
what a "project drawer" is.  The population of people who know Textmate 
and Windows text editors is probably pretty small.

Duncan Murdoch

> 
> Thanks,
> 
> Graham
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From myotisone at gmail.com  Sat Sep 30 15:34:41 2006
From: myotisone at gmail.com (Graham Smith)
Date: Sat, 30 Sep 2006 14:34:41 +0100
Subject: [R] Textmate project drawer: is there a Windows alternative?
In-Reply-To: <451E712F.8020803@stats.uwo.ca>
References: <2c75873c0609300538n400bb0bal37ce7090c0d4283b@mail.gmail.com>
	<451E712F.8020803@stats.uwo.ca>
Message-ID: <2c75873c0609300634j252a483o8dcef0466b3e3ff@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060930/27c066de/attachment.pl 

From ligges at statistik.uni-dortmund.de  Sat Sep 30 15:35:02 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 30 Sep 2006 15:35:02 +0200
Subject: [R] if then else
In-Reply-To: <451E7071.1040206@stats.uwo.ca>
References: <943529EF506BBD4797015900B6F6C25DE3072A@brwsmxsusr01.pharma.aventis.com>
	<451E4718.90403@statistik.uni-dortmund.de>
	<451E7071.1040206@stats.uwo.ca>
Message-ID: <451E7286.7080505@statistik.uni-dortmund.de>



Duncan Murdoch wrote:
> On 9/30/2006 6:29 AM, Uwe Ligges wrote:
>>
>> Jue.Wang2 at sanofi-aventis.com wrote:
>>> What is the correct form to write statement meaning:
>>>
>>> if (a==1) {b=2; c=3}; else {b=0; c=0};
>>
>>
>> if (a==1) {b=2; c=3} else {b=0; c=0};
> 
> That's valid, but is it "correct form"?  The semicolon at the end is not 
> needed.  I'd say it's a bad idea to use one, because it might give a 
> mistaken impression about the meaning of something like
> 
>  a = 1
>      + 2;
> 
> It's better to avoid semi-colons whenever possible, to make sure the C 
> parser in your brain throws an exception and lets the R parser take over.
> 
> Duncan Murdoch


Of course, Duncan is right, and shame on me for posting it in such a 
pedagogically bad way on R-help, but I could not resist to simply remove 
a single semicolon from the original question.

Thank you, Duncan, for pointing it out.

Uwe


From ritwik.sinha at gmail.com  Sat Sep 30 15:38:48 2006
From: ritwik.sinha at gmail.com (Ritwik Sinha)
Date: Sat, 30 Sep 2006 09:38:48 -0400
Subject: [R] only need the p-value
In-Reply-To: <2AD792FE1A79F046B908C364C29622F2014A5022@EX05.ds.umcutrecht.nl>
References: <2c75873c0609300538n400bb0bal37ce7090c0d4283b@mail.gmail.com>
	<2AD792FE1A79F046B908C364C29622F2014A5022@EX05.ds.umcutrecht.nl>
Message-ID: <42bc98300609300638w6c8bf03ai8da61adf784df626@mail.gmail.com>

This is how you go about doing this.

summary(results)$coefficients[1,5]

You will have to check this for you code. But the idea is that
summary(results) is a list (?) and one of its components is called
"coefficients", which is a matrix. So the problem is just to extract
one element of this matrix.

I am not well versed with coxph so there may be some minor details I
am missing, but that is the general idea (same as with lm, glm etc.).

Ritwik.

On 9/30/06, Boks, M.P.M. <M.P.M.Boks at umcutrecht.nl> wrote:
>
>
> Dear R users,
>
>
>
> I am calculating several cox proportional hazard models after each other (I know this is unusual, but I am just exploring the data). For the purpose of multiple testing correction I need to construct an array of these p-values. However since the output is not an array in itself, I cannot find a way to obtain the p-value only.
>
>
>
> > attach(tms)
>
> >
>
> > goal<-rep(0.7*FREQUENC[1:13],6)
>
> > event<- Surv(TIJD,FREQUENC<goal)
>
> > results<-coxph(event~ TYPETREA)
>
> > summary(results)
>
>
>
> Call:
>
> coxph(formula = event ~ TYPETREA)
>
>
>
>   n=76 (2 observations deleted due to missing)
>
>                      coef exp(coef) se(coef)     z     p
>
> TYPETREAnon-guided -0.826     0.438    0.484 -1.71 0.088
>
>
>
>                    exp(coef) exp(-coef) lower .95 upper .95
>
> TYPETREAnon-guided     0.438       2.28     0.169      1.13
>
>
>
> Rsquare= 0.041   (max possible= 0.829 )
>
> Likelihood ratio test= 3.2  on 1 df,   p=0.0737
>
> Wald test            = 2.91  on 1 df,   p=0.088
>
> Score (logrank) test = 3.08  on 1 df,   p=0.0794
>
>
>
> Does anyone now how to extract the p-value?
>
> Many thanks!,
>
> Marco
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Ritwik Sinha
Graduate Student
Epidemiology and Biostatistics
Case Western Reserve University
ritwik.sinha at gmail.com | +12163682366 | http://darwin.cwru.edu/~rsinha


From ritwik.sinha at gmail.com  Sat Sep 30 16:10:13 2006
From: ritwik.sinha at gmail.com (Ritwik Sinha)
Date: Sat, 30 Sep 2006 10:10:13 -0400
Subject: [R] Print/Save/Cat/Write list
Message-ID: <42bc98300609300710o3def33fbs26b46fc735d96796@mail.gmail.com>

Hi,

I would like to write a list to an ascii file.
I tried the following

y <- list(a = 1, b = c(TRUE,FALSE), c = "oops")
save(y, file="y.data", ascii=TRUE)
# Not satisfactory

print does not have a file="" option
cat cannot handle lists.
write does not handle lists
write.table converts it to a d.f

Perhaps I could loop through the elements of a list and keep appending
its elements to a file, but that will have a problem if any of the
elements of the list is a list. I suppose there must be a simple
function that does what I need. Sorry if I have missed anything
obvious, my searches did not return anything useful.

Thanks and regards,
Ritwik.

Here is my version

platform i686-redhat-linux-gnu
arch     i686
os       linux-gnu
system   i686, linux-gnu
status
major    2
minor    2.1
year     2005
month    12
day      20
svn rev  36812
language R

-- 
Ritwik Sinha
Graduate Student
Epidemiology and Biostatistics
Case Western Reserve University
ritwik.sinha at gmail.com | +12163682366 | http://darwin.cwru.edu/~rsinha


From rolf at erdos.math.unb.ca  Sat Sep 30 16:14:31 2006
From: rolf at erdos.math.unb.ca (Rolf Turner)
Date: Sat, 30 Sep 2006 11:14:31 -0300 (ADT)
Subject: [R] Print/Save/Cat/Write list
Message-ID: <200609301414.k8UEEVqb003308@erdos.math.unb.ca>


?sink


From jholtman at gmail.com  Sat Sep 30 16:18:32 2006
From: jholtman at gmail.com (jim holtman)
Date: Sat, 30 Sep 2006 10:18:32 -0400
Subject: [R] Print/Save/Cat/Write list
In-Reply-To: <42bc98300609300710o3def33fbs26b46fc735d96796@mail.gmail.com>
References: <42bc98300609300710o3def33fbs26b46fc735d96796@mail.gmail.com>
Message-ID: <644e1f320609300718n3d307611uf398f504267cceb7@mail.gmail.com>

try:

sink("y.data")
y
sink()

On 9/30/06, Ritwik Sinha <ritwik.sinha at gmail.com> wrote:
> Hi,
>
> I would like to write a list to an ascii file.
> I tried the following
>
> y <- list(a = 1, b = c(TRUE,FALSE), c = "oops")
> save(y, file="y.data", ascii=TRUE)
> # Not satisfactory
>
> print does not have a file="" option
> cat cannot handle lists.
> write does not handle lists
> write.table converts it to a d.f
>
> Perhaps I could loop through the elements of a list and keep appending
> its elements to a file, but that will have a problem if any of the
> elements of the list is a list. I suppose there must be a simple
> function that does what I need. Sorry if I have missed anything
> obvious, my searches did not return anything useful.
>
> Thanks and regards,
> Ritwik.
>
> Here is my version
>
> platform i686-redhat-linux-gnu
> arch     i686
> os       linux-gnu
> system   i686, linux-gnu
> status
> major    2
> minor    2.1
> year     2005
> month    12
> day      20
> svn rev  36812
> language R
>
> --
> Ritwik Sinha
> Graduate Student
> Epidemiology and Biostatistics
> Case Western Reserve University
> ritwik.sinha at gmail.com | +12163682366 | http://darwin.cwru.edu/~rsinha
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From ritwik.sinha at gmail.com  Sat Sep 30 16:33:03 2006
From: ritwik.sinha at gmail.com (Ritwik Sinha)
Date: Sat, 30 Sep 2006 10:33:03 -0400
Subject: [R] Print/Save/Cat/Write list
In-Reply-To: <644e1f320609300718n3d307611uf398f504267cceb7@mail.gmail.com>
References: <42bc98300609300710o3def33fbs26b46fc735d96796@mail.gmail.com>
	<644e1f320609300718n3d307611uf398f504267cceb7@mail.gmail.com>
Message-ID: <42bc98300609300733s7dd4d9epe92194e067931c0c@mail.gmail.com>

thanks.

Ritwik.

On 9/30/06, jim holtman <jholtman at gmail.com> wrote:
> try:
>
> sink("y.data")
> y
> sink()
>
> On 9/30/06, Ritwik Sinha <ritwik.sinha at gmail.com> wrote:
> > Hi,
> >
> > I would like to write a list to an ascii file.
> > I tried the following
> >
> > y <- list(a = 1, b = c(TRUE,FALSE), c = "oops")
> > save(y, file="y.data", ascii=TRUE)
> > # Not satisfactory
> >
> > print does not have a file="" option
> > cat cannot handle lists.
> > write does not handle lists
> > write.table converts it to a d.f
> >
> > Perhaps I could loop through the elements of a list and keep appending
> > its elements to a file, but that will have a problem if any of the
> > elements of the list is a list. I suppose there must be a simple
> > function that does what I need. Sorry if I have missed anything
> > obvious, my searches did not return anything useful.
> >
> > Thanks and regards,
> > Ritwik.
> >
> > Here is my version
> >
> > platform i686-redhat-linux-gnu
> > arch     i686
> > os       linux-gnu
> > system   i686, linux-gnu
> > status
> > major    2
> > minor    2.1
> > year     2005
> > month    12
> > day      20
> > svn rev  36812
> > language R
> >
> > --
> > Ritwik Sinha
> > Graduate Student
> > Epidemiology and Biostatistics
> > Case Western Reserve University
> > ritwik.sinha at gmail.com | +12163682366 | http://darwin.cwru.edu/~rsinha
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
> --
> Jim Holtman
> Cincinnati, OH
> +1 513 646 9390
>
> What is the problem you are trying to solve?
>


-- 
Ritwik Sinha
Graduate Student
Epidemiology and Biostatistics
Case Western Reserve University
ritwik.sinha at gmail.com | +12163682366 | http://darwin.cwru.edu/~rsinha


From ritwik.sinha at gmail.com  Sat Sep 30 16:37:41 2006
From: ritwik.sinha at gmail.com (Ritwik Sinha)
Date: Sat, 30 Sep 2006 10:37:41 -0400
Subject: [R] Heteroskedasticity test
In-Reply-To: <Pine.LNX.4.64.0609301419310.6234@eowyn>
References: <001601c6e3e0$38db3f70$336f12ac@matrix.com>
	<644e1f320609291000q6ba0855as10077ed9ed82b3a7@mail.gmail.com>
	<20060929171915.M91848@centroin.com.br>
	<Pine.LNX.4.64.0609301419310.6234@eowyn>
Message-ID: <42bc98300609300737v4a92410fqbc62d91558fbf2cc@mail.gmail.com>

you may also try to levene test. Once again i think it is for a known
change point.

http://finzi.psych.upenn.edu/R/library/car/html/levene.test.html

On 9/30/06, Achim Zeileis <Achim.Zeileis at wu-wien.ac.at> wrote:
> On Fri, 29 Sep 2006, Alberto Monteiro wrote:
>
> > Is there any heteroskedasticity test in the package? Something
> > that would flag a sample like
> >
> >  x <- c(rnorm(1000), rnorm(1000, 0, 1.2))
>
> The package lmtest contains several tests for heteroskedasticity, in
> particular the Breusch-Pagan test (and also the Goldfeld-Quandt test for
> known change point). Furthermore, some of the structural change tests in
> strucchange can be used to test for non-constant variances, e.g, the
> Nyblom-Hansen test.
> Z
>
> > Alberto Monteiro
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Ritwik Sinha
Graduate Student
Epidemiology and Biostatistics
Case Western Reserve University
ritwik.sinha at gmail.com | +12163682366 | http://darwin.cwru.edu/~rsinha


From ggrothendieck at gmail.com  Sat Sep 30 16:55:30 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 30 Sep 2006 10:55:30 -0400
Subject: [R] Print/Save/Cat/Write list
In-Reply-To: <42bc98300609300710o3def33fbs26b46fc735d96796@mail.gmail.com>
References: <42bc98300609300710o3def33fbs26b46fc735d96796@mail.gmail.com>
Message-ID: <971536df0609300755n5915065ck223a4e3a3d566a58@mail.gmail.com>

Check out ?dput

On 9/30/06, Ritwik Sinha <ritwik.sinha at gmail.com> wrote:
> Hi,
>
> I would like to write a list to an ascii file.
> I tried the following
>
> y <- list(a = 1, b = c(TRUE,FALSE), c = "oops")
> save(y, file="y.data", ascii=TRUE)
> # Not satisfactory
>
> print does not have a file="" option
> cat cannot handle lists.
> write does not handle lists
> write.table converts it to a d.f
>
> Perhaps I could loop through the elements of a list and keep appending
> its elements to a file, but that will have a problem if any of the
> elements of the list is a list. I suppose there must be a simple
> function that does what I need. Sorry if I have missed anything
> obvious, my searches did not return anything useful.
>
> Thanks and regards,
> Ritwik.
>
> Here is my version
>
> platform i686-redhat-linux-gnu
> arch     i686
> os       linux-gnu
> system   i686, linux-gnu
> status
> major    2
> minor    2.1
> year     2005
> month    12
> day      20
> svn rev  36812
> language R
>
> --
> Ritwik Sinha
> Graduate Student
> Epidemiology and Biostatistics
> Case Western Reserve University
> ritwik.sinha at gmail.com | +12163682366 | http://darwin.cwru.edu/~rsinha
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From AnupTyagi at yahoo.com  Sat Sep 30 17:51:34 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Sat, 30 Sep 2006 15:51:34 +0000 (UTC)
Subject: [R] plotting
Message-ID: <loom.20060930T174723-793@post.gmane.org>

Is there something in R that will display both observed values and their
influence on calculated statistics?
Anupam.


From bbands at gmail.com  Sat Sep 30 18:07:21 2006
From: bbands at gmail.com (BBands)
Date: Sat, 30 Sep 2006 09:07:21 -0700
Subject: [R] a decimal aligned column
In-Reply-To: <1159480192.4037.41.camel@localhost.localdomain>
References: <6e8360ad0609281231lbeec4f5la9c44fa177268597@mail.gmail.com>
	<1159480192.4037.41.camel@localhost.localdomain>
Message-ID: <6e8360ad0609300907x24051d25o2cb03491a0c5a327@mail.gmail.com>

As requested:

The alignment problem came from calling format many times. Marc
Schwartz suggested a solution of putting my results in a vector and
then formatting. As I understand it the problem is that fixed-width
fields are only available from sprintf, while comma delineation is
only available from format, formatC and prettyNum. The
interrelationships are complicated (format calls prettyNum) and
require _very_ careful study. Here is Marc's solution with a few
changes. It left aligns the symbols and truncates, right aligns and
comma delineates the numbers. In short you get a nice table that is
easy to scan.

If I had one wish for format, it would be that it could set fixed
field width as well as minimum.

library(tseries)
# the symbols
symbols <- c('spy', 'ise', 'oih', 'mot', 'pbj', 'qqqq')
# set the start date to a year ago
Start <- Sys.Date() - 366
# Pre-allocate dolVol as a vector
dolVol <- numeric(length(symbols))
# Now get the values, assign to dolVol by indexing
for(line in seq(along = symbols))
{
 a <- get.hist.quote(instrument=symbols[line], start=Start,
                     compression="w", quote=c("Close", "Volume"),
                     quiet=TRUE)

 dolVol[line] <- mean(a[,1]) * mean(a[,2])
}
# Now for the initial common formatting,
# truncating the dolVol values to whole numbers
dolVol.pretty <- format(trunc(dolVol), big.mark=",", scientific=FALSE,
                       justify="right", width=15)
# Now output
cat(paste(sprintf('%-4s',symbols), dolVol.pretty,
    collapse = "\n", sep = ""), "\n")

spy      8,770,399,023
ise         21,296,087
oih      1,415,206,983
mot        416,923,148
pbj            246,700
qqqq     4,077,543,493

    jab
-- 
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.


From singyee.ling at googlemail.com  Sat Sep 30 18:09:06 2006
From: singyee.ling at googlemail.com (singyee ling)
Date: Sat, 30 Sep 2006 17:09:06 +0100
Subject: [R] Gradient problem in nlm
Message-ID: <ca33a9890609300909j4eaddd75mef41aea6a38a8d69@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060930/3b3946de/attachment.pl 

From ggrothendieck at gmail.com  Sat Sep 30 18:43:52 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 30 Sep 2006 12:43:52 -0400
Subject: [R] a decimal aligned column
In-Reply-To: <6e8360ad0609300907x24051d25o2cb03491a0c5a327@mail.gmail.com>
References: <6e8360ad0609281231lbeec4f5la9c44fa177268597@mail.gmail.com>
	<1159480192.4037.41.camel@localhost.localdomain>
	<6e8360ad0609300907x24051d25o2cb03491a0c5a327@mail.gmail.com>
Message-ID: <971536df0609300943u6710e0b2v49f74702f5bd2523@mail.gmail.com>

For the last line you could also consider the print.data.frame method:

   data.frame(Symbol = symbols, dolVol = dolVol.pretty)

or

   data.frame(row.names = symbols, dolVol = dolVol.pretty)

capture.output or sink could be used if you want to direct it to a file.

On 9/30/06, BBands <bbands at gmail.com> wrote:
> As requested:
>
> The alignment problem came from calling format many times. Marc
> Schwartz suggested a solution of putting my results in a vector and
> then formatting. As I understand it the problem is that fixed-width
> fields are only available from sprintf, while comma delineation is
> only available from format, formatC and prettyNum. The
> interrelationships are complicated (format calls prettyNum) and
> require _very_ careful study. Here is Marc's solution with a few
> changes. It left aligns the symbols and truncates, right aligns and
> comma delineates the numbers. In short you get a nice table that is
> easy to scan.
>
> If I had one wish for format, it would be that it could set fixed
> field width as well as minimum.
>
> library(tseries)
> # the symbols
> symbols <- c('spy', 'ise', 'oih', 'mot', 'pbj', 'qqqq')
> # set the start date to a year ago
> Start <- Sys.Date() - 366
> # Pre-allocate dolVol as a vector
> dolVol <- numeric(length(symbols))
> # Now get the values, assign to dolVol by indexing
> for(line in seq(along = symbols))
> {
>  a <- get.hist.quote(instrument=symbols[line], start=Start,
>                     compression="w", quote=c("Close", "Volume"),
>                     quiet=TRUE)
>
>  dolVol[line] <- mean(a[,1]) * mean(a[,2])
> }
> # Now for the initial common formatting,
> # truncating the dolVol values to whole numbers
> dolVol.pretty <- format(trunc(dolVol), big.mark=",", scientific=FALSE,
>                       justify="right", width=15)
> # Now output
> cat(paste(sprintf('%-4s',symbols), dolVol.pretty,
>    collapse = "\n", sep = ""), "\n")
>
> spy      8,770,399,023
> ise         21,296,087
> oih      1,415,206,983
> mot        416,923,148
> pbj            246,700
> qqqq     4,077,543,493
>
>    jab
> --
> John Bollinger, CFA, CMT
> www.BollingerBands.com
>
> If you advance far enough, you arrive at the beginning.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From AnupTyagi at yahoo.com  Sat Sep 30 18:47:38 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Sat, 30 Sep 2006 16:47:38 +0000 (UTC)
Subject: [R] Setting NA
Message-ID: <loom.20060930T184513-30@post.gmane.org>

Is there a way to set NA values in R, without changing the dataframe? I would
like to use different combinations of non-response values, as if they were NA
for some of the computations. I don't want to change the dataframe each time I
have to do this?
Anupam.


From spencer.graves at pdf.com  Sat Sep 30 19:03:19 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 30 Sep 2006 12:03:19 -0500
Subject: [R] Need help to estimate the Coef matrices in mAr
In-Reply-To: <d4c57560609200311u3ca3fb25n9af7971a0da3b818@mail.gmail.com>
References: <d4c57560608292247h2f18d279r75061f0a4eb6cebd@mail.gmail.com>	
	<44FBC786.5010703@pdf.com>
	<d4c57560609200311u3ca3fb25n9af7971a0da3b818@mail.gmail.com>
Message-ID: <451EA357.90807@pdf.com>

      I'm sorry, but I can't follow what you are asking.  If you'd like 
more help, please provide commented, minimal, self-contained, 
reproducible code, as suggested in the posting guide 
"www.R-project.org/posting-guide.html". 
<http://www.R-project.org/posting-guide.html>  Please include a minimal 
data set, e.g., 10 simulated observations on 2 variables, with comments 
describing what you tried and what you don't understand about it. 

      Hope this helps. 
      Spencer Graves

Arun Kumar Saha wrote:
>
> Dear Spencer,
>
>  
>
> Thank you very much for your attention on my problem. According to 
> your advice I did some home work on this problem, but unfortunately I 
> could not solve my problem.
>
>  
>
>  
>
> Suppose I have a dataset of length 300 with 2 variables. And I want to 
> fit a VAR model on this of order 2.
>
>  
>
> I went through the function mAr.est and got understand that, here 'K' 
> is a matrix with (300-2) rows and 7 columns. the first col. consists 
> only 1, next two columns consist of lagged values of two variables 
> with lag-length 2, next two col. consist of lagged value with lag 
> length-1, and next two cols are for lag-length-0.
>
>  
>
> Next, they add additional a 7-7 matrix to K. For this matrix diagonal 
> elements are the square root of sum of square of elements of K (col. 
> wise) and rest of the elements are 0.
>
>  
>
> I feel that this matrix, that is added to K, is the key matrix for any 
> type of modification that you prescribed. Therefore for experimental 
> purpose I put NA against one of its off-diagonal elements. But I got 
> error.
>
>  
>
> However I cannot understand why they put such figures for diagonal and 
> off-diagonal elements of that matrix.
>
>  
>
> Can you suggest me any solution more specifically?
>
>  
>
>  
>
> Thanks and regards,
>
> Arun
>
>
>
> On 9/4/06, *Spencer Graves* <spencer.graves at pdf.com 
> <mailto:spencer.graves at pdf.com>> wrote:
>
>           Have you tried 'RSiteSearch("multivariate autoregression",
>     "functions")'?  This produced 14 hits for me just now, the first of
>     which mentions a package 'MSBVAR'.  Have you looked at that?
>
>           If that failed, I don't think it would be too hard to modify
>     'mAr.est' to do what you want.  If it were my problem, I might a local
>     copy of the function, then add an argument accepting a 2 or
>     3-dimensional array with numbers for AR coefficients to be fixed
>     and NAs
>     for the coefficients.  Then I'd use 'debug' to walk through the
>     function
>     line by line until I figured out how to modify the function to do
>     what I
>     wanted.  I haven't checked all the details, so I don't know for
>     sure if
>     this would work, but the function contains a line 'R =
>     qr.R(qr((rbind(K,
>     diag(scale)))), complete = TRUE)' which I would start by decomposing,
>     possibly starting as follows:
>
>           Z <-     rbind(K, diag(scale)
>
>     I'd figure out how the different columns of Z relate to my
>     problem, then
>     modify it appropriately to get what I wanted.
>
>           Another alternative would be to program it from scratch using
>     something like 'optim' to minimize the sum of squares of residuals
>     over
>     the free parameters in my AR matrices.   I'm confident I could
>     make this
>     work, even if the I somehow could not get it with either of the
>     other two.
>
>           There may be something else  better, e.g., a Kalman filter
>     representation, but I can't think how to do that off the top if my
>     head.
>
>           Hope this helps.
>           Spencer Graves
>
>     Arun Kumar Saha wrote:
>     > Dear R users,
>     >
>     > I am using mAr package to fit a Vector autoregressive model to
>     my data. But
>     > here I want to put some predetermined values for some elements in
>     > coefficient matrix that mAr.est going to estimate. For example
>     if p=3 then I
>     > want to put A3[1,3] = 0 and keep rest of the elements of
>     coefficient
>     > matrices to be determined by mAr.est.
>     >
>     > Can anyone please tell me how can I do that?
>     >
>     > Sincerely yours,
>     > Arun
>     >
>     >       [[alternative HTML version deleted]]
>     >
>     > ______________________________________________
>     > R-help at stat.math.ethz.ch <mailto:R-help at stat.math.ethz.ch>
>     mailing list
>     > https://stat.ethz.ch/mailman/listinfo/r-help
>     <https://stat.ethz.ch/mailman/listinfo/r-help>
>     > PLEASE do read the posting guide
>     http://www.R-project.org/posting-guide.html
>     > and provide commented, minimal, self-contained, reproducible code.
>     >
>
>
>
>
> -- 
> Arun Kumar Saha, M.Sc.[C.U.]
> S T A T I S T I C I A N    [Analyst]
> RISK  MANAGEMENT  DIVISION
> Transgraph Consulting [ www.transgraph.com <http://www.transgraph.com>]
> Hyderabad, INDIA
> Contact #  Home: (91-033) 25558038
>                 Office: (91-040) 30685012 Ext. 17
>                   FAX: (91-040) 55755003
>                Mobile: 919989122010
> E-Mail: arun.riskanalyst at transgraph.com 
> <mailto:arun.riskanalyst at transgraph.com>
>             arun.kumar.saha at gmail.com <mailto:arun.kumar.saha at gmail.com>


From mothsailor at googlemail.com  Sat Sep 30 19:19:42 2006
From: mothsailor at googlemail.com (David Barron)
Date: Sat, 30 Sep 2006 18:19:42 +0100
Subject: [R] Setting NA
In-Reply-To: <loom.20060930T184513-30@post.gmane.org>
References: <loom.20060930T184513-30@post.gmane.org>
Message-ID: <815b70590609301019q3fab125w2ad86d64c16a11a8@mail.gmail.com>

Would using the subset argument that is available in many functions
(eg lm) achieve what you want?

On 30/09/06, Anupam Tyagi <AnupTyagi at yahoo.com> wrote:
> Is there a way to set NA values in R, without changing the dataframe? I would
> like to use different combinations of non-response values, as if they were NA
> for some of the computations. I don't want to change the dataframe each time I
> have to do this?
> Anupam.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From david.meyer at wu-wien.ac.at  Sat Sep 30 20:44:51 2006
From: david.meyer at wu-wien.ac.at (David Meyer)
Date: Sat, 30 Sep 2006 20:44:51 +0200
Subject: [R]   package e1071 - class probabilities
Message-ID: <451EBB23.2060507@wu-wien.ac.at>

Vince:

the implementations for both are different, so this might happen
(although undesirably).

Can you provide me an example with data (off-list)?

David


From frainj at gmail.com  Sat Sep 30 21:31:57 2006
From: frainj at gmail.com (John C Frain)
Date: Sat, 30 Sep 2006 20:31:57 +0100
Subject: [R] strange warning message
In-Reply-To: <451E4520.20804@stats.uwo.ca>
References: <dcb8ad0b1672b.45155d9d@usc.edu> <4515D871.9090205@stats.uwo.ca>
	<ddebfcbe1e233.451d9795@usc.edu> <451E4520.20804@stats.uwo.ca>
Message-ID: <fad888a10609301231k2e0aa5dbq31ff4f45db2e4437@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060930/f58a5cee/attachment.pl 

From hpbenton at scripps.edu  Sat Sep 30 22:09:32 2006
From: hpbenton at scripps.edu (H. Paul Benton)
Date: Sat, 30 Sep 2006 13:09:32 -0700
Subject: [R] fitting a gaussian to some x,y data
Message-ID: <000701c6e4cc$582ea0f0$d7fa1ac0@lama>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060930/42ed8756/attachment.pl 

From murdoch at stats.uwo.ca  Sat Sep 30 22:13:30 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 30 Sep 2006 16:13:30 -0400
Subject: [R] strange warning message
In-Reply-To: <fad888a10609301231k2e0aa5dbq31ff4f45db2e4437@mail.gmail.com>
References: <dcb8ad0b1672b.45155d9d@usc.edu>
	<4515D871.9090205@stats.uwo.ca>	<ddebfcbe1e233.451d9795@usc.edu>
	<451E4520.20804@stats.uwo.ca>
	<fad888a10609301231k2e0aa5dbq31ff4f45db2e4437@mail.gmail.com>
Message-ID: <451ECFEA.6090700@stats.uwo.ca>

On 9/30/2006 3:31 PM, John C Frain wrote:
> I get a similar message when i start Sciviews R console.  I do not get the
> message when I start R directly or through Tinn-r .  If I load the libraries
> one by one the message is returned after svViews is loaded.  I presume there
> is some problem with svViews.  However it does not appear to have any
> consequences for my work.  I use R 2.3.1 with Windows XP and I should have
> the latest versions of packages installed

This does look like an svViews problem, possibly because it's loading an 
  old binary copy of some object, or because it hasn't been recompiled 
for the release you're using.

It may become a more serious problem when the use becomes "defunct" 
instead of "deprecated":  then the usage will generate an error. 
However, I believe we intend to maintain the ability to read old data 
files as long as possible, so you might not see any consequences.

Duncan Murdoch

> 
> John C Frain
> ****************************************
> output of R session started with Sciviews
> R is a collaborative project with many contributors.
> Type 'contributors()' for more information and
> 'citation()' on how to cite R or R packages in publications.
> 
> Type 'demo()' for some demos, 'help()' for on-line help, or
> 'help.start()' for an HTML browser interface to help.
> Type 'q()' to quit R.
> 
> [Previously saved workspace restored]
> 
> Loading required package: datasets
> Loading required package: utils
> Loading required package: grDevices
> Loading required package: graphics
> Loading required package: stats
> Loading required package: methods
> Loading required package: tcltk
> Loading Tcl/Tk interface ... done
> Loading required package: R2HTML
> Loading required package: svMisc
> Loading required package: svIO
> Loading required package: svViews
> During startup - Warning message:
> use of NULL environment is deprecated
> 
> 
> On 30/09/06, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
>> On 9/30/2006 1:00 AM, Tong Wang wrote:
>>> Hi Duncan:
>>>      Thank you for your help last time,  since I do not use NULL to
>> indicate empty enviroment, I think I'm fine.
>>> And yes, I did upgrade my R version recently,  but how comes I still get
>> this warning for new files created
>>> and saved after that update ? Is there anyway to get rid of this message
>> ?
>>
>> Please show some code that leads to the message.  You shouldn't get it
>> in new objects, but there could be a bug in R or in a package you're
>> using.
>>
>> Duncan Murdoch
>>
>>>      Thanks a lot .
>>>
>>> best
>>>
>>> ----- Original Message -----
>>> From: Duncan Murdoch <murdoch at stats.uwo.ca>
>>> Date: Saturday, September 23, 2006 5:59 pm
>>> Subject: Re: [R] strange  warning message
>>> To: Tong Wang <wangtong at usc.edu>
>>> Cc: r-help at stat.math.ethz.ch
>>>
>>>> On 9/23/2006 7:15 PM, Tong Wang wrote:
>>>>> Hi Everyone,
>>>>>     I recently start to get this warning message,  while loading
>>>> files in to R.   Could someone tell me what does it mean ?
>>>>> I am using R 2.3.0 with Emacs on WinXP.
>>>>>
>>>>>     use of NULL environment is deprecated
>>>> The files were saved in an earlier version of R, which used NULL to
>>>> indicate the base environment.  R is telling you that NULL is not a
>>>> legal environment.  It should be automatically converted to baseenv().
>>>>
>>>> In a number of cases, people used NULL to indicate an empty
>>>> environment
>>>> (even though there was no such thing when NULL was used); if that's
>>>> true
>>>> for your code, then you'll need to fix it.  emptyenv() now gives
>>>> you an
>>>> empty environment if that's what you really want.
>>>>
>>>> Duncan Murdoch
>>>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> 
>


From darbarisonal at gmail.com  Sat Sep 30 22:49:34 2006
From: darbarisonal at gmail.com (Sonal Darbari)
Date: Sat, 30 Sep 2006 16:49:34 -0400
Subject: [R] Inner product
Message-ID: <5c460bc30609301349x38d43218x7f83c98f22564a47@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060930/b9358be6/attachment.pl 

From yonghai at science.oregonstate.edu  Sat Sep 30 21:11:54 2006
From: yonghai at science.oregonstate.edu (yonghai at science.oregonstate.edu)
Date: Sat, 30 Sep 2006 12:11:54 -0700
Subject: [R] error from pmvnorm
Message-ID: <1159643514.451ec17a9a977@webmail.oregonstate.edu>

Hi all,

Can anyone tell me what the following error message means?
" Error in mvt(lower = lower, upper = upper, df = 0, corr = corr, delta = mean, 
:  NA/NaN/Inf in foreign function call (arg 6)"

It was generated when I used the 'pmvnorm' function in the 'mvtnorm' package.

Thanks a lot.

Yonghai Li


From mothsailor at googlemail.com  Sat Sep 30 23:33:08 2006
From: mothsailor at googlemail.com (David Barron)
Date: Sat, 30 Sep 2006 22:33:08 +0100
Subject: [R] Inner product
In-Reply-To: <5c460bc30609301349x38d43218x7f83c98f22564a47@mail.gmail.com>
References: <5c460bc30609301349x38d43218x7f83c98f22564a47@mail.gmail.com>
Message-ID: <815b70590609301433r4cbddf95ga3a4e7de81730193@mail.gmail.com>

For inner product see ?"%*%".  There is a norm function in the Matrix package.

On 30/09/06, Sonal Darbari <darbarisonal at gmail.com> wrote:
> Hi,
>
> How do we find out the inner product & norm of eigen vectors in R?
>
> Lets say we have eigen vectors :
> x1 = 1,2,3 and x2 = 2,-3,4
>
> are there any functions buit in R which directly calculate the inner product
> & norm of vectors?
>
> Thanks,
> Sonal.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From h.wickham at gmail.com  Sat Sep 30 23:49:29 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Sat, 30 Sep 2006 16:49:29 -0500
Subject: [R] [R-pkgs] Reshape version 0.7.1
Message-ID: <f8e6ff050609301449j7f4cca00kcb99315ff275955a@mail.gmail.com>

Reshape version 0.7.1
=====================

Reshape is an R package for flexibly restructuring and aggregating
data.  It's inspired by Excel's pivot tables, and it (hopefully) makes
it very easy to get your data into the shape that you want.  You can find out
more at http://had.co.nz/reshape

What's new in this version?

 * A 25 page introductory vignette, also available at
http://had.co.nz/reshape/introduction.pdf, which shows some of the
many ways that you can use reshape.

 * The biggest change is that reshape now outputs regular data.frames,
which should make it easier to use them for further analysis and
transformation.

 * Added a fill argument to cast which specifies what value should be
used for structural missings

Various bug fixes and completion missing features:

 * fun.aggregate will always be applied if specified, even if no
aggregation occurs

 * margins now work for non-aggregated data

 * cast now accepts a list of functions for fun.aggregate

 * very long formulas will now work in cast

 * fixed bug in rbind.fill

 * should be able to melt any cast form

Please let me know if you have any comments, questions, or something
isn't working right.

Regards,

Hadley

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From p.dalgaard at biostat.ku.dk  Sat Sep 30 23:59:50 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Sep 2006 23:59:50 +0200
Subject: [R] Inner product
In-Reply-To: <815b70590609301433r4cbddf95ga3a4e7de81730193@mail.gmail.com>
References: <5c460bc30609301349x38d43218x7f83c98f22564a47@mail.gmail.com>
	<815b70590609301433r4cbddf95ga3a4e7de81730193@mail.gmail.com>
Message-ID: <x2psddaucp.fsf@turmalin.kubism.ku.dk>

"David Barron" <mothsailor at googlemail.com> writes:

> For inner product see ?"%*%".  There is a norm function in the Matrix package.

Or crossprod(). Notice that this gives _squared_ norms when applied to
a single vector.

 
> On 30/09/06, Sonal Darbari <darbarisonal at gmail.com> wrote:
> > Hi,
> >
> > How do we find out the inner product & norm of eigen vectors in R?
> >
> > Lets say we have eigen vectors :
> > x1 = 1,2,3 and x2 = 2,-3,4
> >
> > are there any functions buit in R which directly calculate the inner product
> > & norm of vectors?
> >
> > Thanks,
> > Sonal.
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> 
> -- 
> =================================
> David Barron
> Said Business School
> University of Oxford
> Park End Street
> Oxford OX1 1HP
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From vdemart1 at tin.it  Sat Sep 30 19:40:28 2006
From: vdemart1 at tin.it (vittorio)
Date: Sat, 30 Sep 2006 17:40:28 +0000
Subject: [R] RODBC ERROR on Rcmdr install
In-Reply-To: <20060929225507.JYAO18394.tomts5-srv.bellnexxia.net@JohnDesktop8300>
References: <20060929225507.JYAO18394.tomts5-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <200609301740.28744.vdemart1@tin.it>

Alle 22:55, venerd? 29 settembre 2006, John Fox ha scritto:
> As I understand it, RODBC isn't useful on non-Windows systems, since the
> necessary ODBC drivers aren't available. (Someone will correct me, I'm
> sure, if I don't have that entirely straight.) The RODBC package is used in
> the Rcmdr to read Excel and some other files under Windows; in the latest
> version of the Rcmdr, you won't even see this menu item in non-Windows
> systems.

As a matter of fact RODBC can be profitably used under *nix OS together with 
unixODBC to connect to many DBs.
I've been using RODBC with unixODBC on linux, freebsd and win xp  to connect 
smoothly to postgresql, mysql  and oracle (somewhat tricky to me under *nix, 
you need ** to buy ** a driver) and I know that connections are possible to 
many other *nix DBs under *nix itself. 

Ciao
Vittorio


