From pisharm at cs.sunysb.edu  Mon Dec  1 00:27:40 2003
From: pisharm at cs.sunysb.edu (Piyush Sharma)
Date: Sun, 30 Nov 2003 18:27:40 -0500 (EST)
Subject: [R] fitting a theoretical distribution with truncated tails
Message-ID: <Pine.GSO.4.53.0311301823310.25221@compserv1>

Hi,
  I have recently started working with R and am not really fluent in it. I
am plotting a few graphs using the qqplot function. Is there a method for
fitting a theoretical distribution (e.g Weibull) with truncated tails in R?

Thanks for any help!
Piyush



From spencer.graves at pdf.com  Mon Dec  1 01:33:45 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 30 Nov 2003 16:33:45 -0800
Subject: [R] fitting a theoretical distribution with truncated tails
In-Reply-To: <Pine.GSO.4.53.0311301823310.25221@compserv1>
References: <Pine.GSO.4.53.0311301823310.25221@compserv1>
Message-ID: <3FCA8C69.905@pdf.com>

      Have you considered "fitdistr"?  The documentation says that the 
second argument is "Either a character string or a function returning a 
density evaluated at its first argument."  It should be easy enough to 
write something like the following: 

dweibull.trunc <-
function(x, shape, scale=1, trunc.=Inf, log=FALSE){
    ln.dens <- (dweibull(x, shape, scale, log=TRUE)
        -pweibull(trunc., shape, scale = 1, lower.tail = TRUE, log.p = 
TRUE))
    if(any(oops <- (x>trunc.)))
        ln.dens[oops] <- (-Inf)   
    if(log)ln.dens else exp(ln.dens)
}

x <- rweibull(100, 1)
range(x)
x4 <- x[x<=4]
fitdistr(x4, dweibull.trunc, start=list(shape=1, scale=1), trunc=4)

      If you want to estimate the truncation point, that will be a more 
difficult problem.  For that, I suggest you compute the max of your data 
and parameterize the truncated density with a parameter like 
"log.trunc.over.max" so "trunc." in the above example is computed as 
(max+exp(log.trunc.over.max)). 

      hope this helps.  spencer graves

Piyush Sharma wrote:

>Hi,
>  I have recently started working with R and am not really fluent in it. I
>am plotting a few graphs using the qqplot function. Is there a method for
>fitting a theoretical distribution (e.g Weibull) with truncated tails in R?
>
>Thanks for any help!
>Piyush
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From ggrothendieck at myway.com  Mon Dec  1 02:55:49 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 30 Nov 2003 20:55:49 -0500 (EST)
Subject: [R] Discovering methods
Message-ID: <20031201015549.433C5397D@mprdmxin.myway.com>



Thanks.  I guess we have to be content to approximate this via:

   apropos("POSIXt$|POSIXct$")

although this supposes we know that POSIXct inherits from POSIXt
and its not clear that there is a reliable way to discover that
for S3 classes.

--- 
Date: Sun, 30 Nov 2003 13:10:52 -0800 (PST) 
From: Thomas Lumley <tlumley at u.washington.edu>
To: Gabor Grothendieck <ggrothendieck at myway.com> 
Cc: <kjetil at entelnet.bo>, <R-help at stat.math.ethz.ch> 
Subject: Re: [R] Discovering methods 

 
 
On Fri, 28 Nov 2003, Gabor Grothendieck wrote:

>
>
> Sure, but why are some methods found using methods("POSIXct")
> while other methods not found?
>
> It would be nice to have some reliable documentation-independent
> way to discover all the methods for a class.

Indeed it would, but that requires registration of methods using either
the S4 approach or the functions for handling S3 methods in namespaces.

Without this, it is simply not possible to decide, for example, whether
t.test.formula is a method for t() or for t.test() or a separate function.


     -thomas



From dmurdoch at pair.com  Mon Dec  1 04:43:36 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Sun, 30 Nov 2003 22:43:36 -0500
Subject: [R] Discovering methods
In-Reply-To: <20031201015549.433C5397D@mprdmxin.myway.com>
References: <20031201015549.433C5397D@mprdmxin.myway.com>
Message-ID: <j1elsvskn477k7a69mtssfm4vb337377e8@4ax.com>

On Sun, 30 Nov 2003 20:55:49 -0500 (EST), you wrote:

>
>
>Thanks.  I guess we have to be content to approximate this via:
>
>   apropos("POSIXt$|POSIXct$")
>
>although this supposes we know that POSIXct inherits from POSIXt
>and its not clear that there is a reliable way to discover that
>for S3 classes.
>

I don't think there's a concept of inheritance of classes in S3.
That's one of the advantages of S4.

Duncan Murdoch



From Toby.Patterson at csiro.au  Mon Dec  1 05:20:29 2003
From: Toby.Patterson at csiro.au (Toby.Patterson@csiro.au)
Date: Mon, 1 Dec 2003 15:20:29 +1100
Subject: [R] hdf library for windows
Message-ID: <C4178DC99E08604EA5E2BDB989F093800131EC29@extas2-hba.tas.csiro.au>

Hi, 
Is there a version of the hdf library for windows? 

Cheers 
Toby



From Tom.Mulholland at health.wa.gov.au  Mon Dec  1 06:44:21 2003
From: Tom.Mulholland at health.wa.gov.au (Mulholland, Tom)
Date: Mon, 1 Dec 2003 13:44:21 +0800
Subject: [R] hdf library for windows
Message-ID: <74E242B6968AA0469B632C5A3EFC1EFD03D5700D@nt207mesep.corporate.hdwa.health.wa.gov.au>

The question puzzled me at first, because of your use of "library". It
looks as if the hdf5 r "package" utilises the "windows hdf5 library"
binary. 

My reading is that you will have to compile the package yourself after
you have downloaded the hdf windows dll from hdf.ncsa.uiuc.edu The
instructions are in win.readme.txt of the package source which you can
download at planetmirror or aarnet.

I think the use of the hdf dll is the reason a windows binary cannot be
made available for direct download.

Ciao, Tom

_________________________________________________
 
Tom Mulholland
Senior Policy Officer
WA Country Health Service
Tel: (08) 9222 4062
 
The contents of this e-mail transmission are confidential an...{{dropped}}



From kwan022 at stat.auckland.ac.nz  Mon Dec  1 07:23:30 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Mon, 1 Dec 2003 19:23:30 +1300 (NZDT)
Subject: [R] strptime Usage
In-Reply-To: <20031126053411.B27DF39C2@mprdmxin.myway.com>
Message-ID: <Pine.LNX.4.44.0312011921440.9876-100000@stat61.stat.auckland.ac.nz>

Thanks!

On Wed, 26 Nov 2003, Gabor Grothendieck wrote:

> Date: Wed, 26 Nov 2003 00:34:11 -0500 (EST)
> From: Gabor Grothendieck <ggrothendieck at myway.com>
> To: kwan022 at stat.auckland.ac.nz, r-help at stat.math.ethz.ch
> Subject: RE: [R] strptime Usage
> 
> 
> 
> strptime takes a character input and produces a POSIXlt output so
> the format you specify to strptime is the format of the input, 
> not the output:
> 
>    format( strptime("10/22/1986", "%m/%d/%Y"), "%Y-%m" )

It worked perfect.  Just out of interest, if I want to convert (either 
from the original form, i.e. mm/dd/yyyy, or the yyyy-mm form, to quarterly 
format, e.g.:
  1999-1
  1999-2
  1999-3
  1999-4
is it possible to do with strptime?  Or do I have to do something 
creative? ;-D

-- 
Cheers,

Kevin

---------------------------------------------------------------
"Try not.  Do, do!  Or do not.  There is no try"
   Jedi Master Yoda

----
Ko-Kang Kevin Wang
Master of Science (MSc) Student
SLC Tutor and Lab Demonstrator
Department of Statistics
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599
    x88475 (City)
    x88480 (Tamaki)



From ripley at stats.ox.ac.uk  Mon Dec  1 08:37:15 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 1 Dec 2003 07:37:15 +0000 (GMT)
Subject: [R] Discovering methods
In-Reply-To: <Pine.A41.4.58.0311301308110.142760@homer01.u.washington.edu>
Message-ID: <Pine.LNX.4.44.0312010732410.4150-100000@gannet.stats>

On Sun, 30 Nov 2003, Thomas Lumley wrote:

> On Fri, 28 Nov 2003, Gabor Grothendieck wrote:
>
> > Sure, but why are some methods found using methods("POSIXct")
> > while other methods not found?
> >
> > It would be nice to have some reliable documentation-independent
> > way to discover all the methods for a class.
> 
> Indeed it would, but that requires registration of methods using either
> the S4 approach or the functions for handling S3 methods in namespaces.
> 
> Without this, it is simply not possible to decide, for example, whether
> t.test.formula is a method for t() or for t.test() or a separate function.

It also needs registration of generics.  No one thought to list the 
members of the group generics (such as "+") as individual generics until 
recently, and methods() does try to find a corresponding generic to at 
least find out if t() and t.test() are generic functions.

methods() is R-devel does a better job -- even that in R-patched does a 
better job now a couple of long-standing bugs in find() have been fixed.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Dec  1 08:45:49 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 1 Dec 2003 07:45:49 +0000 (GMT)
Subject: [R] strptime Usage
In-Reply-To: <Pine.LNX.4.44.0312011921440.9876-100000@stat61.stat.auckland.ac.nz>
Message-ID: <Pine.LNX.4.44.0312010742430.4150-100000@gannet.stats>

On Mon, 1 Dec 2003, Ko-Kang Kevin Wang wrote:

> Thanks!
> 
> On Wed, 26 Nov 2003, Gabor Grothendieck wrote:
> 
> > Date: Wed, 26 Nov 2003 00:34:11 -0500 (EST)
> > From: Gabor Grothendieck <ggrothendieck at myway.com>
> > To: kwan022 at stat.auckland.ac.nz, r-help at stat.math.ethz.ch
> > Subject: RE: [R] strptime Usage
> > 
> > 
> > 
> > strptime takes a character input and produces a POSIXlt output so
> > the format you specify to strptime is the format of the input, 
> > not the output:
> > 
> >    format( strptime("10/22/1986", "%m/%d/%Y"), "%Y-%m" )
> 
> It worked perfect.  Just out of interest, if I want to convert (either 
> from the original form, i.e. mm/dd/yyyy, or the yyyy-mm form, to quarterly 
> format, e.g.:
>   1999-1
>   1999-2
>   1999-3
>   1999-4
> is it possible to do with strptime?  Or do I have to do something 
> creative? ;-D

Well, strptime converts from strings to internal date format, so it is 
logically impossible to do this with strptime.

As for quarters: how do you define them?  Does the quarters() function do 
what you want?  If almost, look at

> quarters.POSIXt
function (x, ...)
{
    x <- (as.POSIXlt(x)$mon)%/%3
    paste("Q", x + 1, sep = "")
}

for ideas.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Torsten.Hothorn at rzmail.uni-erlangen.de  Mon Dec  1 10:16:55 2003
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Mon, 1 Dec 2003 10:16:55 +0100 (CET)
Subject: [R] significance in difference of proportions: What problema
In-Reply-To: <XFMail.031129100918.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.031129100918.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.LNX.4.51.0312010935000.30657@artemis.imbe.med.uni-erlangen.de>


> On 28-Nov-03 Torsten Hothorn wrote:
> > yes, thats my understanding too. The "enumerative techniques" as
> > you call it condition on the data actually observed and determine
> > the null distribution of the associated test statistic from the data.
> > In contrast, unconditional procedures require some assumptions to the
> > underlying data generating process from which the null distribution is
> > derived. The appropriate choice depends of the kind of experiment
> > under test: In a randomized trial we would like to see all possible
> > outcomes of the trial caused by "rerandomization" and the enumerative
> > techniques are natural here. When we draw many samples from predefined
> > populations, men and women, say, "rerandomization" of gender is of
> > course not that easy and we may assume something about the data
> > generating process :-)
>
> Nice example, but it depends on how you look at it!
>

indeed.

> Suppose you have samples of n1 Men and n2 Women and record, for instance,
> whether or not each is suffering from a cold (r1 and r2 respectively).
> Do M & W differ in their risk of catching cold?
>
> NH: No difference; implies that the R = (r1+r2) colds have selected
> a random subset of the N=(n1+n2) individuals as victims; implies
> that the n1 Men out of N are a random subset of the R+(N-R)
> Colds/NonColds. So you then have a hypergeometric distribution and are
> back with an "exact" test. But are we "assuming somthing about the
> data generating process" here?
>

This is the exact conditional approach where both the row and
column marginal totals are fixed and, because in this "simple" 2x2 case
the distribution of the test statistic is known to be hypergeometric,
there is no need for explicit enumeration (and `fisher.test()' computes the
corresponding P-values) and, indeed, we do not need to make any
assumtion.

But is fixing both the number of women and men (rows) AND the numbers of
colds (columns) intuitive when we would like to "learn" something about
the two different populations, i.e. the distribution of colds in women
and the distribution of colds in men? If we only assume the sample sizes
in both populations to be fixed, this reduces to a comparison of two
binomial parameters for two independent samples. And in this special
situation, we assume something about the data generating process: we
assume "cold" to be distributed according to binomial law
(OK: this is not really a restrictive assumption, but for continuous
responses people tend to assume something like normality). And
the comparison of two binomial distribtions leads to exact
unconditional inference: this is explained in a very nice way in Agresti
(StatMed 20, 2001, 2709-2722) and I hope read it correctly :-)

> (Of course, in the background lurks the Ogre of Exchangeability,
> in that the probability of catching cold may vary from person to
> person, whether Man or Woman, but nothing in the information
> plus NH suggests any reason to distinguish any arrangement of the
> N people from any other; equivalent to a re-randomisation of
> gender ... ??).

yes, looks like that. But re-randomization of gender ( = fixed column
marginal totals) is maybe hard to sell to our customers :-)

Best,

Torsten

>
> Best wishes,
> Ted.
>
>
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 167 1972
> Date: 29-Nov-03                                       Time: 10:09:18
> ------------------------------ XFMail ------------------------------
>
>



From Pascal.Niklaus at unibas.ch  Mon Dec  1 10:28:22 2003
From: Pascal.Niklaus at unibas.ch (Pascal A. Niklaus)
Date: Mon, 01 Dec 2003 10:28:22 +0100
Subject: [R] Indexing ANOVA table
In-Reply-To: <Pine.LNX.4.44.0311291154460.13346-100000@gannet.stats>
References: <Pine.LNX.4.44.0311291154460.13346-100000@gannet.stats>
Message-ID: <3FCB09B6.1070001@unibas.ch>

Prof Brian Ripley wrote:

>On Sat, 29 Nov 2003 Pascal.Niklaus at unibas.ch wrote:
>
>  
>
>>Hi all,
>>
>>I'd like to extract a value from an ANOVA table, but experience the following
>>problem:
>>
>>### This works:
>>
>>    
>>
>>>s.pseudo <- summary(aov(m ~ block + mix*graz,data=split1))
>>>s.pseudo
>>>      
>>>
>>            Df  Sum Sq Mean Sq F value  Pr(>F)
>>block        2 1114.66  557.33  4.4296 0.04192 *
>>mix          1    6.14    6.14  0.0488 0.82956
>>graz         2    1.45    0.72  0.0057 0.99427
>>mix:graz     2    3.82    1.91  0.0152 0.98495
>>Residuals   10 1258.19  125.82
>>---
>>Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
>>    
>>
>>>s.pseudo[[1]]["mix ","Pr(>F)"]
>>>      
>>>
>>[1] 0.8295556
>>
>>### But this doesn't -- why?
>>    
>>
>
>Why should it?
>
>  
>
>>>s.split <-  summary(aov(m ~ block + mix*graz + Error(Plot),data=split1))
>>>s.split
>>>      
>>>
>>Error: Plot
>>          Df  Sum Sq Mean Sq F value Pr(>F)
>>block      2 1114.66  557.33  0.8994 0.5265
>>mix        1    6.14    6.14  0.0099 0.9298
>>Residuals  2 1239.37  619.68
>>
>>Error: Within
>>          Df  Sum Sq Mean Sq F value Pr(>F)
>>graz       2  1.4464  0.7232  0.3073 0.7437
>>mix:graz   2  3.8206  1.9103  0.8117 0.4776
>>Residuals  8 18.8278  2.3535
>>    
>>
>>>s.split[["Error: Plot"]]  ## extracting first list element works
>>>      
>>>
>>          Df  Sum Sq Mean Sq F value Pr(>F)
>>block      2 1114.66  557.33  0.8994 0.5265
>>mix        1    6.14    6.14  0.0099 0.9298
>>Residuals  2 1239.37  619.68
>>    
>>
>>>s.split[["Error: Plot"]]["mix ","Pr(>F)"]  ### <== FAILS
>>>      
>>>
>>Error in s.split[["Error: Plot"]]["mix ", "Pr(>F)"] :
>>        incorrect number of dimensions
>>    
>>
>
>You should have written
>
>s.split[["Error: Plot"]][[1]]["mix ","Pr(>F)"]
>
>  
>
>>So where is the difference between the two?
>>    
>>
>
>One is an "aov" object, the other an "aovlist" object.  Take a closer look 
>at print.summary.aov, for example.
>
>More generally, learn how to look at R objects instead of assuming you
>know what you are doing: unclass(s.split[["Error: Plot"]]) would have been
>informative.
>  
>
Thanks for the help - it works now.

I had of course tried str(s.split) and str(s.split[["Error: Plot"]]) and also [ ] and [[ ]], but wrongly thought that the remaining "List of 1" was because I extracted a slice of the list instead of a specific list element only... Need to look better next time...

Pascal



From nielssteenkrogh at hotmail.com  Mon Dec  1 11:51:59 2003
From: nielssteenkrogh at hotmail.com (Niels Steen Krogh)
Date: Mon, 01 Dec 2003 11:51:59 +0100
Subject: [R] wilcoxon-pratt signed rank  test in R -  drug-effiacy  
Message-ID: <BAY7-F43VkFV9kXk6uS0001fb1e@hotmail.com>

Hi.
I'm going to introduce the R-package for a group of medical doctors later 
this week and is a little confused about there use of a test named 
"willcoxon-pratt"  for testing if the clinical and biochemical markers has 
decreased significantly after the use of some drugs for a group of patients.

Looking into the R-functions I would in R recommand using a matched-pairs 
Wilcoxon test with a formula like:
wilcox.test(pre,post,alternative='greater',paired=T)

Looking deeper into the writings of Pratt I found some 1964-stuff "Pratt JW. 
Remarks on zeros and ties in the Wilcoxon signed rank procedures.  
J.Americ.Statistical Assoc. 1959; 54: 655-67. "

Do'es any of you know what is the wilcoxon-pratt test compared with the 
formula described above and how it should be implemented in R.

Thanks in advance

/Niels

Cand. Polit.
Niels Steen Krogh
Solsortvej 44
2000 F.

Tlf: 3888 8613

ZiteLab / EmpoweR youR data with R, Zope and SOAP



From p.dalgaard at biostat.ku.dk  Mon Dec  1 13:04:15 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 01 Dec 2003 13:04:15 +0100
Subject: [R] wilcoxon-pratt signed rank  test in R -  drug-effiacy
In-Reply-To: <BAY7-F43VkFV9kXk6uS0001fb1e@hotmail.com>
References: <BAY7-F43VkFV9kXk6uS0001fb1e@hotmail.com>
Message-ID: <x2brqshhhc.fsf@biostat.ku.dk>

"Niels Steen Krogh" <nielssteenkrogh at hotmail.com> writes:

> Hi.
> I'm going to introduce the R-package for a group of medical doctors
                             ^^^^^^^^^

Nitpick: R is a "language and environment (for statististical
computation and graphics)". An R-package is something that you load
into R, as in the contrib sections on CRAN.

> later this week and is a little confused about there use of a test
> named "willcoxon-pratt"  for testing if the clinical and biochemical
> markers has decreased significantly after the use of some drugs for a
> group of patients.
> 
> Looking into the R-functions I would in R recommand using a
> matched-pairs Wilcoxon test with a formula like:
> wilcox.test(pre,post,alternative='greater',paired=T)
> 
> Looking deeper into the writings of Pratt I found some 1964-stuff
> "Pratt JW. Remarks on zeros and ties in the Wilcoxon signed rank
> procedures.  J.Americ.Statistical Assoc. 1959; 54: 655-67. "
> 
> Do'es any of you know what is the wilcoxon-pratt test compared with
> the formula described above and how it should be implemented in R.

wilcox.test does the original Wilcoxon procedure, discarding any tied
pairs (zero difference). The Pratt procedure (rank all differences
first, *then* discard tied pairs) is not immediately available, but
would be fairly easy to implement, at least for the large-sample case.
I wouldn't be surprised if the exact procedure could be performed
using Torsten's ExactRankTests package (perm.test or wilcox.exact),
but I haven't gone deeply into it.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From mikewhite.diu at tiscali.co.uk  Mon Dec  1 13:07:47 2003
From: mikewhite.diu at tiscali.co.uk (Mike White)
Date: Mon, 1 Dec 2003 12:07:47 -0000
Subject: [R] Changing distance scale in plclust()
Message-ID: <003a01c3b803$bc4d6fb0$be62e150@FSSFQCV7BGDVED>

I want to plot the cluster trees from the results of hclust() on different
datasets, but all with the same distance scale corresponding to the dataset
with the largest distance range.  However, plclust() does not accept ylim
values.  Does anyone know of a way around this problem?

Mike White



From wolski at molgen.mpg.de  Mon Dec  1 13:21:42 2003
From: wolski at molgen.mpg.de (Wolski)
Date: Mon, 01 Dec 2003 13:21:42 +0100
Subject: [R] Rd Files?
Message-ID: <200312011321420649.00C26205@harry.molgen.mpg.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031201/a6d79c7e/attachment.pl

From savano at superig.com.br  Mon Dec  1 13:37:51 2003
From: savano at superig.com.br (Savano)
Date: Mon, 1 Dec 2003 10:37:51 -0200
Subject: [R] Sampling
Message-ID: <200312011037.51244.savano@superig.com.br>

UseRs,

I imported a table using "read.table". It has 209 observations, I want to 
select a sample with 106 observations.  What function I use?

thanks.



From kjetil at entelnet.bo  Mon Dec  1 13:44:41 2003
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Mon, 01 Dec 2003 08:44:41 -0400
Subject: [R] Rd Files?
In-Reply-To: <200312011321420649.00C26205@harry.molgen.mpg.de>
Message-ID: <3FCAFF79.3183.282A59@localhost>

On 1 Dec 2003 at 13:21, Wolski wrote:

It is not entirely clear what you want, but if you want to translate 
S[-Plus] help files to Rd format, there is [windows]
Rcmd Sd2Rd
[unix]
R cmd Sd2Rd

Kjetil Halvorsen

> Hi!
> 
> Are there scripts or packages for generating Rd files out of "S3" code
> on cran? Has any one written such scripts (e.g. in Perl?) and are
> willing to share them?
> 
> Thanks in advance.
> 
> Eryk.
> 
> Dipl. bio-chem. Eryk Witold Wolski    @    MPI-MG Dep. Vertebrate
> Genomics Ihnestrasse 73 14195 Berlin          'v' tel:
> 0049-30-84131285               /   \ mail: wolski at molgen.mpg.de       
> ---W-W----
> 
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From wolski at molgen.mpg.de  Mon Dec  1 13:50:32 2003
From: wolski at molgen.mpg.de (Wolski)
Date: Mon, 01 Dec 2003 13:50:32 +0100
Subject: [R] Rd Files?
In-Reply-To: <3FCAFF79.3183.282A59@localhost>
References: <3FCAFF79.3183.282A59@localhost>
Message-ID: <200312011350320396.00DCC6D9@harry.molgen.mpg.de>

Hi Kjetil!

I have an *.R file with a lot of functions and I am looking for a script which would generate the skeleton of an Rd file for all functions found in the R file.
It will be nice if the can already generate the "name","usage" and the "arguments" field from the source so that i have to edit only the description and examples....

Eryk



*********** REPLY SEPARATOR  ***********

On 12/1/2003 at 8:44 AM kjetil at entelnet.bo wrote:

>On 1 Dec 2003 at 13:21, Wolski wrote:
>
>It is not entirely clear what you want, but if you want to translate 
>S[-Plus] help files to Rd format, there is [windows]
>Rcmd Sd2Rd
>[unix]
>R cmd Sd2Rd
>
>Kjetil Halvorsen
>
>> Hi!
>> 
>> Are there scripts or packages for generating Rd files out of "S3" code
>> on cran? Has any one written such scripts (e.g. in Perl?) and are
>> willing to share them?
>> 
>> Thanks in advance.
>> 
>> Eryk.
>> 
>> Dipl. bio-chem. Eryk Witold Wolski    @    MPI-MG Dep. Vertebrate
>> Genomics Ihnestrasse 73 14195 Berlin          'v' tel:
>> 0049-30-84131285               /   \ mail: wolski at molgen.mpg.de       
>> ---W-W----
>> 
>> 
>>  [[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



Dipl. bio-chem. Eryk Witold Wolski    @    MPI-MG Dep. Vertebrate Genomics   
Ihnestrasse 73 14195 Berlin          'v'    
tel: 0049-30-84131285               /   \    
mail: wolski at molgen.mpg.de        ---W-W----    http://www.molgen.mpg.de/~wolski



From rost at zf.jcu.cz  Mon Dec  1 13:57:20 2003
From: rost at zf.jcu.cz (Ing. Michael Rost)
Date: Mon, 1 Dec 2003 13:57:20 +0100
Subject: [R] Cluster analysis for multinominal data?
Message-ID: <003601c3b80a$a8637560$75a7d9a0@kmi07>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031201/554ffefe/attachment.pl

From H.RINNER at tirol.gv.at  Mon Dec  1 14:07:47 2003
From: H.RINNER at tirol.gv.at (RINNER Heinrich)
Date: Mon, 1 Dec 2003 14:07:47 +0100 
Subject: AW: [R] Sampling
Message-ID: <6A6B3B547E312840A98A9DD31516B32118045D@mxs1.tirol.local>

Hi,

you can use something like

> example.data <- data.frame(x=rnorm(209), y=rnorm(209))
> selected.sample <- example.data[sample(209,106), ]

See ?sample.

-Heinrich.

> -----Urspr?ngliche Nachricht-----
> Von: Savano [mailto:savano at superig.com.br] 
> Gesendet: Montag, 01. Dezember 2003 13:38
> An: Lista R
> Betreff: [R] Sampling
> 
> 
> UseRs,
> 
> I imported a table using "read.table". It has 209 
> observations, I want to 
> select a sample with 106 observations.  What function I use?
> 
> thanks.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From ripley at stats.ox.ac.uk  Mon Dec  1 14:08:03 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 1 Dec 2003 13:08:03 +0000 (GMT)
Subject: [R] Rd Files?
In-Reply-To: <200312011350320396.00DCC6D9@harry.molgen.mpg.de>
Message-ID: <Pine.LNX.4.44.0312011306220.1393-100000@gannet.stats>

?prompt
?package.skeleton

and do read `Writing R Extensions'


On Mon, 1 Dec 2003, Wolski wrote:

> Hi Kjetil!
> 
> I have an *.R file with a lot of functions and I am looking for a script
> which would generate the skeleton of an Rd file for all functions found
> in the R file. It will be nice if the can already generate the
> "name","usage" and the "arguments" field from the source so that i have
> to edit only the description and examples....
> 
> Eryk
> 
> 
> 
> *********** REPLY SEPARATOR  ***********
> 
> On 12/1/2003 at 8:44 AM kjetil at entelnet.bo wrote:
> 
> >On 1 Dec 2003 at 13:21, Wolski wrote:
> >
> >It is not entirely clear what you want, but if you want to translate 
> >S[-Plus] help files to Rd format, there is [windows]
> >Rcmd Sd2Rd
> >[unix]
> >R cmd Sd2Rd
> >
> >Kjetil Halvorsen
> >
> >> Hi!
> >> 
> >> Are there scripts or packages for generating Rd files out of "S3" code
> >> on cran? Has any one written such scripts (e.g. in Perl?) and are
> >> willing to share them?
> >> 
> >> Thanks in advance.
> >> 
> >> Eryk.
> >> 
> >> Dipl. bio-chem. Eryk Witold Wolski    @    MPI-MG Dep. Vertebrate
> >> Genomics Ihnestrasse 73 14195 Berlin          'v' tel:
> >> 0049-30-84131285               /   \ mail: wolski at molgen.mpg.de       
> >> ---W-W----
> >> 
> >> 
> >>  [[alternative HTML version deleted]]
> >> 
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 
> 
> Dipl. bio-chem. Eryk Witold Wolski    @    MPI-MG Dep. Vertebrate Genomics   
> Ihnestrasse 73 14195 Berlin          'v'    
> tel: 0049-30-84131285               /   \    
> mail: wolski at molgen.mpg.de        ---W-W----    http://www.molgen.mpg.de/~wolski
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From paradis at univ-montp2.fr  Mon Dec  1 13:54:34 2003
From: paradis at univ-montp2.fr (paradis@univ-montp2.fr)
Date: Mon, 1 Dec 2003 13:54:34 +0100
Subject: [R] Sampling
References: <200312011037.51244.savano@superig.com.br>
Message-ID: <3fcb3a0a35ee09.31111193@univ-montp2.fr>

> UseRs,
> 
> I imported a table using "read.table". It has 209 observations, I want to
> select a sample with 106 observations.  What function I use?

sample(), of course. See the help page (?sample) for the details and the options (with or without replacement). I guess you want something like:

DF[sample(1:209, 106), ]

where DF is your imported table.


Emmanuel Paradis
 
> thanks.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From wolski at molgen.mpg.de  Mon Dec  1 14:23:58 2003
From: wolski at molgen.mpg.de (Wolski)
Date: Mon, 01 Dec 2003 14:23:58 +0100
Subject: [R] Rd Files?
In-Reply-To: <Pine.LNX.4.44.0312011306220.1393-100000@gannet.stats>
References: <Pine.LNX.4.44.0312011306220.1393-100000@gannet.stats>
Message-ID: <200312011423580384.00FB61E7@harry.molgen.mpg.de>

Hi!


Seems to me that package.skeleton(...) does not like assignment functions.
System: Windows 2000


package.skeleton(name="testpack",list=ls(),path="c:/home/wolski/devel",force=T)
Creating directories ...
Creating DESCRIPTION ...
Creating READMEs ...
Saving functions and data ...
Error in file(file, ifelse(append, "a", "w")) : 
	unable to open connection
In addition: Warning message: 
cannot open file `c:/home/wolski/devel/testpack/R/[[<-.caliblist.R' 

Eryk


*********** REPLY SEPARATOR  ***********

On 12/1/2003 at 1:08 PM Prof Brian Ripley wrote:

>?prompt
>?package.skeleton
>
>and do read `Writing R Extensions'
>
>
>On Mon, 1 Dec 2003, Wolski wrote:
>
>> Hi Kjetil!
>> 
>> I have an *.R file with a lot of functions and I am looking for a script
>> which would generate the skeleton of an Rd file for all functions found
>> in the R file. It will be nice if the can already generate the
>> "name","usage" and the "arguments" field from the source so that i have
>> to edit only the description and examples....
>> 
>> Eryk
>> 
>> 
>> 
>> *********** REPLY SEPARATOR  ***********
>> 
>> On 12/1/2003 at 8:44 AM kjetil at entelnet.bo wrote:
>> 
>> >On 1 Dec 2003 at 13:21, Wolski wrote:
>> >
>> >It is not entirely clear what you want, but if you want to translate 
>> >S[-Plus] help files to Rd format, there is [windows]
>> >Rcmd Sd2Rd
>> >[unix]
>> >R cmd Sd2Rd
>> >
>> >Kjetil Halvorsen
>> >
>> >> Hi!
>> >> 
>> >> Are there scripts or packages for generating Rd files out of "S3" code
>> >> on cran? Has any one written such scripts (e.g. in Perl?) and are
>> >> willing to share them?
>> >> 
>> >> Thanks in advance.
>> >> 
>> >> Eryk.
>> >> 
>> >> Dipl. bio-chem. Eryk Witold Wolski    @    MPI-MG Dep. Vertebrate
>> >> Genomics Ihnestrasse 73 14195 Berlin          'v' tel:
>> >> 0049-30-84131285               /   \ mail: wolski at molgen.mpg.de       
>> >> ---W-W----
>> >> 
>> >> 
>> >>  [[alternative HTML version deleted]]
>> >> 
>> >> ______________________________________________
>> >> R-help at stat.math.ethz.ch mailing list
>> >> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> 
>> 
>> 
>> Dipl. bio-chem. Eryk Witold Wolski    @    MPI-MG Dep. Vertebrate
>Genomics   
>> Ihnestrasse 73 14195 Berlin          'v'    
>> tel: 0049-30-84131285               /   \    
>> mail: wolski at molgen.mpg.de        ---W-W----   
>http://www.molgen.mpg.de/~wolski
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> 
>> 
>
>-- 
>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>University of Oxford,             Tel:  +44 1865 272861 (self)
>1 South Parks Road,                     +44 1865 272866 (PA)
>Oxford OX1 3TG, UK                Fax:  +44 1865 272595



Dipl. bio-chem. Eryk Witold Wolski    @    MPI-MG Dep. Vertebrate Genomics   
Ihnestrasse 73 14195 Berlin          'v'    
tel: 0049-30-84131285               /   \    
mail: wolski at molgen.mpg.de        ---W-W----    http://www.molgen.mpg.de/~wolski



From ripley at stats.ox.ac.uk  Mon Dec  1 14:26:36 2003
From: ripley at stats.ox.ac.uk (Prof Brian D Ripley)
Date: Mon, 1 Dec 2003 13:26:36 +0000 (GMT Standard Time)
Subject: [R] Rd Files?
In-Reply-To: <200312011423580384.00FB61E7@harry.molgen.mpg.de>
Message-ID: <Pine.WNT.4.44.0312011324410.2872-100000@petrel>

On Mon, 1 Dec 2003, Wolski wrote:

> Hi!
>
>
> Seems to me that package.skeleton(...) does not like assignment functions.
> System: Windows 2000

And that's your problem!  It's a problem with your OS, not
package.skeleton. And it is known.

>
>
> package.skeleton(name="testpack",list=ls(),path="c:/home/wolski/devel",force=T)
> Creating directories ...
> Creating DESCRIPTION ...
> Creating READMEs ...
> Saving functions and data ...
> Error in file(file, ifelse(append, "a", "w")) :
> 	unable to open connection
> In addition: Warning message:
> cannot open file `c:/home/wolski/devel/testpack/R/[[<-.caliblist.R'
>
> Eryk
>
>
> *********** REPLY SEPARATOR  ***********
>
> On 12/1/2003 at 1:08 PM Prof Brian Ripley wrote:
>
> >?prompt
> >?package.skeleton
> >
> >and do read `Writing R Extensions'
> >
> >
> >On Mon, 1 Dec 2003, Wolski wrote:
> >
> >> Hi Kjetil!
> >>
> >> I have an *.R file with a lot of functions and I am looking for a script
> >> which would generate the skeleton of an Rd file for all functions found
> >> in the R file. It will be nice if the can already generate the
> >> "name","usage" and the "arguments" field from the source so that i have
> >> to edit only the description and examples....
> >>
> >> Eryk
> >>
> >>
> >>
> >> *********** REPLY SEPARATOR  ***********
> >>
> >> On 12/1/2003 at 8:44 AM kjetil at entelnet.bo wrote:
> >>
> >> >On 1 Dec 2003 at 13:21, Wolski wrote:
> >> >
> >> >It is not entirely clear what you want, but if you want to translate
> >> >S[-Plus] help files to Rd format, there is [windows]
> >> >Rcmd Sd2Rd
> >> >[unix]
> >> >R cmd Sd2Rd
> >> >
> >> >Kjetil Halvorsen
> >> >
> >> >> Hi!
> >> >>
> >> >> Are there scripts or packages for generating Rd files out of "S3" code
> >> >> on cran? Has any one written such scripts (e.g. in Perl?) and are
> >> >> willing to share them?
> >> >>
> >> >> Thanks in advance.
> >> >>
> >> >> Eryk.
> >> >>
> >> >> Dipl. bio-chem. Eryk Witold Wolski    @    MPI-MG Dep. Vertebrate
> >> >> Genomics Ihnestrasse 73 14195 Berlin          'v' tel:
> >> >> 0049-30-84131285               /   \ mail: wolski at molgen.mpg.de
> >> >> ---W-W----
> >> >>
> >> >>
> >> >>  [[alternative HTML version deleted]]
> >> >>
> >> >> ______________________________________________
> >> >> R-help at stat.math.ethz.ch mailing list
> >> >> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >>
> >>
> >>
> >> Dipl. bio-chem. Eryk Witold Wolski    @    MPI-MG Dep. Vertebrate
> >Genomics
> >> Ihnestrasse 73 14195 Berlin          'v'
> >> tel: 0049-30-84131285               /   \
> >> mail: wolski at molgen.mpg.de        ---W-W----
> >http://www.molgen.mpg.de/~wolski
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >>
> >>
> >
> >--
> >Brian D. Ripley,                  ripley at stats.ox.ac.uk
> >Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> >University of Oxford,             Tel:  +44 1865 272861 (self)
> >1 South Parks Road,                     +44 1865 272866 (PA)
> >Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
>
>
> Dipl. bio-chem. Eryk Witold Wolski    @    MPI-MG Dep. Vertebrate Genomics
> Ihnestrasse 73 14195 Berlin          'v'
> tel: 0049-30-84131285               /   \
> mail: wolski at molgen.mpg.de        ---W-W----    http://www.molgen.mpg.de/~wolski
>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From wolski at molgen.mpg.de  Mon Dec  1 14:53:19 2003
From: wolski at molgen.mpg.de (Wolski)
Date: Mon, 01 Dec 2003 14:53:19 +0100
Subject: [R] Rd Files?
In-Reply-To: <Pine.WNT.4.44.0312011324410.2872-100000@petrel>
References: <Pine.WNT.4.44.0312011324410.2872-100000@petrel>
Message-ID: <200312011453190786.01164261@harry.molgen.mpg.de>

Hi!
I can generate the package skeleton under unix. 
But i am not shure what are further consequences?  
Would it be still possible to
rcmd install ...
the package under windows?

If I stay under windows how than to name the Rd files for assignement functions? Does it matter how the name of the Rd file are?

Eryk




I would like to have a windows version of the package. 
*********** REPLY SEPARATOR  ***********

On 12/1/2003 at 1:26 PM Prof Brian D Ripley wrote:

>On Mon, 1 Dec 2003, Wolski wrote:
>
>> Hi!
>>
>>
>> Seems to me that package.skeleton(...) does not like assignment
>functions.
>> System: Windows 2000
>
>And that's your problem!  It's a problem with your OS, not
>package.skeleton. And it is known.
>
>>
>>
>>
>package.skeleton(name="testpack",list=ls(),path="c:/home/wolski/devel",force=T)
>> Creating directories ...
>> Creating DESCRIPTION ...
>> Creating READMEs ...
>> Saving functions and data ...
>> Error in file(file, ifelse(append, "a", "w")) :
>> 	unable to open connection
>> In addition: Warning message:
>> cannot open file `c:/home/wolski/devel/testpack/R/[[<-.caliblist.R'
>>
>> Eryk
>>
>>
>> *********** REPLY SEPARATOR  ***********
>>
>> On 12/1/2003 at 1:08 PM Prof Brian Ripley wrote:
>>
>> >?prompt
>> >?package.skeleton
>> >
>> >and do read `Writing R Extensions'
>> >
>> >
>> >On Mon, 1 Dec 2003, Wolski wrote:
>> >
>> >> Hi Kjetil!
>> >>
>> >> I have an *.R file with a lot of functions and I am looking for a
>script
>> >> which would generate the skeleton of an Rd file for all functions
>found
>> >> in the R file. It will be nice if the can already generate the
>> >> "name","usage" and the "arguments" field from the source so that i
>have
>> >> to edit only the description and examples....
>> >>
>> >> Eryk
>> >>
>> >>
>> >>
>> >> *********** REPLY SEPARATOR  ***********
>> >>
>> >> On 12/1/2003 at 8:44 AM kjetil at entelnet.bo wrote:
>> >>
>> >> >On 1 Dec 2003 at 13:21, Wolski wrote:
>> >> >
>> >> >It is not entirely clear what you want, but if you want to translate
>> >> >S[-Plus] help files to Rd format, there is [windows]
>> >> >Rcmd Sd2Rd
>> >> >[unix]
>> >> >R cmd Sd2Rd
>> >> >
>> >> >Kjetil Halvorsen
>> >> >
>> >> >> Hi!
>> >> >>
>> >> >> Are there scripts or packages for generating Rd files out of "S3"
>code
>> >> >> on cran? Has any one written such scripts (e.g. in Perl?) and are
>> >> >> willing to share them?
>> >> >>
>> >> >> Thanks in advance.
>> >> >>
>> >> >> Eryk.
>> >> >>
>> >> >> Dipl. bio-chem. Eryk Witold Wolski    @    MPI-MG Dep. Vertebrate
>> >> >> Genomics Ihnestrasse 73 14195 Berlin          'v' tel:
>> >> >> 0049-30-84131285               /   \ mail: wolski at molgen.mpg.de
>> >> >> ---W-W----
>> >> >>
>> >> >>
>> >> >>  [[alternative HTML version deleted]]
>> >> >>
>> >> >> ______________________________________________
>> >> >> R-help at stat.math.ethz.ch mailing list
>> >> >> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> >>
>> >>
>> >>
>> >> Dipl. bio-chem. Eryk Witold Wolski    @    MPI-MG Dep. Vertebrate
>> >Genomics
>> >> Ihnestrasse 73 14195 Berlin          'v'
>> >> tel: 0049-30-84131285               /   \
>> >> mail: wolski at molgen.mpg.de        ---W-W----
>> >http://www.molgen.mpg.de/~wolski
>> >>
>> >> ______________________________________________
>> >> R-help at stat.math.ethz.ch mailing list
>> >> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> >>
>> >>
>> >
>> >--
>> >Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> >Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>> >University of Oxford,             Tel:  +44 1865 272861 (self)
>> >1 South Parks Road,                     +44 1865 272866 (PA)
>> >Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>
>>
>>
>> Dipl. bio-chem. Eryk Witold Wolski    @    MPI-MG Dep. Vertebrate
>Genomics
>> Ihnestrasse 73 14195 Berlin          'v'
>> tel: 0049-30-84131285               /   \
>> mail: wolski at molgen.mpg.de        ---W-W----   
>http://www.molgen.mpg.de/~wolski
>>
>>
>>
>
>-- 
>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>University of Oxford,             Tel:  +44 1865 272861 (self)
>1 South Parks Road,                     +44 1865 272860 (secr)
>Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help



Dipl. bio-chem. Eryk Witold Wolski    @    MPI-MG Dep. Vertebrate Genomics   
Ihnestrasse 73 14195 Berlin          'v'    
tel: 0049-30-84131285               /   \    
mail: wolski at molgen.mpg.de        ---W-W----    http://www.molgen.mpg.de/~wolski



From ripley at stats.ox.ac.uk  Mon Dec  1 14:54:53 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 1 Dec 2003 13:54:53 +0000 (GMT)
Subject: [R] Rd Files?
In-Reply-To: <200312011453190786.01164261@harry.molgen.mpg.de>
Message-ID: <Pine.LNX.4.44.0312011354160.1503-100000@gannet.stats>

On Mon, 1 Dec 2003, Wolski wrote:

> Hi!
> I can generate the package skeleton under unix. 
> But i am not shure what are further consequences?  
> Would it be still possible to
> rcmd install ...
> the package under windows?
> 
> If I stay under windows how than to name the Rd files for assignement
> functions? Does it matter how the name of the Rd file are?

It does not matter.

> 
> Eryk
> 
> 
> 
> 
> I would like to have a windows version of the package. 
> *********** REPLY SEPARATOR  ***********
> 
> On 12/1/2003 at 1:26 PM Prof Brian D Ripley wrote:
> 
> >On Mon, 1 Dec 2003, Wolski wrote:
> >
> >> Hi!
> >>
> >>
> >> Seems to me that package.skeleton(...) does not like assignment
> >functions.
> >> System: Windows 2000
> >
> >And that's your problem!  It's a problem with your OS, not
> >package.skeleton. And it is known.
> >
> >>
> >>
> >>
> >package.skeleton(name="testpack",list=ls(),path="c:/home/wolski/devel",force=T)
> >> Creating directories ...
> >> Creating DESCRIPTION ...
> >> Creating READMEs ...
> >> Saving functions and data ...
> >> Error in file(file, ifelse(append, "a", "w")) :
> >> 	unable to open connection
> >> In addition: Warning message:
> >> cannot open file `c:/home/wolski/devel/testpack/R/[[<-.caliblist.R'
> >>
> >> Eryk
> >>
> >>
> >> *********** REPLY SEPARATOR  ***********
> >>
> >> On 12/1/2003 at 1:08 PM Prof Brian Ripley wrote:
> >>
> >> >?prompt
> >> >?package.skeleton
> >> >
> >> >and do read `Writing R Extensions'
> >> >
> >> >
> >> >On Mon, 1 Dec 2003, Wolski wrote:
> >> >
> >> >> Hi Kjetil!
> >> >>
> >> >> I have an *.R file with a lot of functions and I am looking for a
> >script
> >> >> which would generate the skeleton of an Rd file for all functions
> >found
> >> >> in the R file. It will be nice if the can already generate the
> >> >> "name","usage" and the "arguments" field from the source so that i
> >have
> >> >> to edit only the description and examples....
> >> >>
> >> >> Eryk
> >> >>
> >> >>
> >> >>
> >> >> *********** REPLY SEPARATOR  ***********
> >> >>
> >> >> On 12/1/2003 at 8:44 AM kjetil at entelnet.bo wrote:
> >> >>
> >> >> >On 1 Dec 2003 at 13:21, Wolski wrote:
> >> >> >
> >> >> >It is not entirely clear what you want, but if you want to translate
> >> >> >S[-Plus] help files to Rd format, there is [windows]
> >> >> >Rcmd Sd2Rd
> >> >> >[unix]
> >> >> >R cmd Sd2Rd
> >> >> >
> >> >> >Kjetil Halvorsen
> >> >> >
> >> >> >> Hi!
> >> >> >>
> >> >> >> Are there scripts or packages for generating Rd files out of "S3"
> >code
> >> >> >> on cran? Has any one written such scripts (e.g. in Perl?) and are
> >> >> >> willing to share them?
> >> >> >>
> >> >> >> Thanks in advance.
> >> >> >>
> >> >> >> Eryk.
> >> >> >>
> >> >> >> Dipl. bio-chem. Eryk Witold Wolski    @    MPI-MG Dep. Vertebrate
> >> >> >> Genomics Ihnestrasse 73 14195 Berlin          'v' tel:
> >> >> >> 0049-30-84131285               /   \ mail: wolski at molgen.mpg.de
> >> >> >> ---W-W----
> >> >> >>
> >> >> >>
> >> >> >>  [[alternative HTML version deleted]]
> >> >> >>
> >> >> >> ______________________________________________
> >> >> >> R-help at stat.math.ethz.ch mailing list
> >> >> >> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >> >>
> >> >>
> >> >>
> >> >> Dipl. bio-chem. Eryk Witold Wolski    @    MPI-MG Dep. Vertebrate
> >> >Genomics
> >> >> Ihnestrasse 73 14195 Berlin          'v'
> >> >> tel: 0049-30-84131285               /   \
> >> >> mail: wolski at molgen.mpg.de        ---W-W----
> >> >http://www.molgen.mpg.de/~wolski
> >> >>
> >> >> ______________________________________________
> >> >> R-help at stat.math.ethz.ch mailing list
> >> >> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >> >>
> >> >>
> >> >
> >> >--
> >> >Brian D. Ripley,                  ripley at stats.ox.ac.uk
> >> >Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> >> >University of Oxford,             Tel:  +44 1865 272861 (self)
> >> >1 South Parks Road,                     +44 1865 272866 (PA)
> >> >Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> >>
> >>
> >>
> >> Dipl. bio-chem. Eryk Witold Wolski    @    MPI-MG Dep. Vertebrate
> >Genomics
> >> Ihnestrasse 73 14195 Berlin          'v'
> >> tel: 0049-30-84131285               /   \
> >> mail: wolski at molgen.mpg.de        ---W-W----   
> >http://www.molgen.mpg.de/~wolski
> >>
> >>
> >>
> >
> >-- 
> >Brian D. Ripley,                  ripley at stats.ox.ac.uk
> >Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> >University of Oxford,             Tel:  +44 1865 272861 (self)
> >1 South Parks Road,                     +44 1865 272860 (secr)
> >Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 
> 
> Dipl. bio-chem. Eryk Witold Wolski    @    MPI-MG Dep. Vertebrate Genomics   
> Ihnestrasse 73 14195 Berlin          'v'    
> tel: 0049-30-84131285               /   \    
> mail: wolski at molgen.mpg.de        ---W-W----    http://www.molgen.mpg.de/~wolski 
> 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From wolski at molgen.mpg.de  Mon Dec  1 15:00:00 2003
From: wolski at molgen.mpg.de (Wolski)
Date: Mon, 01 Dec 2003 15:00:00 +0100
Subject: [R] Rd Files?
In-Reply-To: <Pine.LNX.4.44.0312011354160.1503-100000@gannet.stats>
References: <Pine.LNX.4.44.0312011354160.1503-100000@gannet.stats>
Message-ID: <200312011500000945.011C6076@harry.molgen.mpg.de>

Hallo!

If it does not matter how the name of the Rd file are,
would it be difficult than to modify the package.skeleton function, so that it works under windows 2000 too?

Eryk


*********** REPLY SEPARATOR  ***********

On 12/1/2003 at 1:54 PM Prof Brian Ripley wrote:

>On Mon, 1 Dec 2003, Wolski wrote:
>
>> Hi!
>> I can generate the package skeleton under unix. 
>> But i am not shure what are further consequences?  
>> Would it be still possible to
>> rcmd install ...
>> the package under windows?
>> 
>> If I stay under windows how than to name the Rd files for assignement
>> functions? Does it matter how the name of the Rd file are?
>
>It does not matter.
>
>> 
>> Eryk
>> 
>> 
>> 
>> 
>> I would like to have a windows version of the package. 
>> *********** REPLY SEPARATOR  ***********
>> 
>> On 12/1/2003 at 1:26 PM Prof Brian D Ripley wrote:
>> 
>> >On Mon, 1 Dec 2003, Wolski wrote:
>> >
>> >> Hi!
>> >>
>> >>
>> >> Seems to me that package.skeleton(...) does not like assignment
>> >functions.
>> >> System: Windows 2000
>> >
>> >And that's your problem!  It's a problem with your OS, not
>> >package.skeleton. And it is known.
>> >
>> >>
>> >>
>> >>
>>
>>package.skeleton(name="testpack",list=ls(),path="c:/home/wolski/devel",force=T)
>> >> Creating directories ...
>> >> Creating DESCRIPTION ...
>> >> Creating READMEs ...
>> >> Saving functions and data ...
>> >> Error in file(file, ifelse(append, "a", "w")) :
>> >> 	unable to open connection
>> >> In addition: Warning message:
>> >> cannot open file `c:/home/wolski/devel/testpack/R/[[<-.caliblist.R'
>> >>
>> >> Eryk
>> >>
>> >>
>> >> *********** REPLY SEPARATOR  ***********
>> >>
>> >> On 12/1/2003 at 1:08 PM Prof Brian Ripley wrote:
>> >>
>> >> >?prompt
>> >> >?package.skeleton
>> >> >
>> >> >and do read `Writing R Extensions'
>> >> >
>> >> >
>> >> >On Mon, 1 Dec 2003, Wolski wrote:
>> >> >
>> >> >> Hi Kjetil!
>> >> >>
>> >> >> I have an *.R file with a lot of functions and I am looking for a
>> >script
>> >> >> which would generate the skeleton of an Rd file for all functions
>> >found
>> >> >> in the R file. It will be nice if the can already generate the
>> >> >> "name","usage" and the "arguments" field from the source so that i
>> >have
>> >> >> to edit only the description and examples....
>> >> >>
>> >> >> Eryk
>> >> >>
>> >> >>
>> >> >>
>> >> >> *********** REPLY SEPARATOR  ***********
>> >> >>
>> >> >> On 12/1/2003 at 8:44 AM kjetil at entelnet.bo wrote:
>> >> >>
>> >> >> >On 1 Dec 2003 at 13:21, Wolski wrote:
>> >> >> >
>> >> >> >It is not entirely clear what you want, but if you want to
>translate
>> >> >> >S[-Plus] help files to Rd format, there is [windows]
>> >> >> >Rcmd Sd2Rd
>> >> >> >[unix]
>> >> >> >R cmd Sd2Rd
>> >> >> >
>> >> >> >Kjetil Halvorsen
>> >> >> >
>> >> >> >> Hi!
>> >> >> >>
>> >> >> >> Are there scripts or packages for generating Rd files out of
>"S3"
>> >code
>> >> >> >> on cran? Has any one written such scripts (e.g. in Perl?) and
>are
>> >> >> >> willing to share them?
>> >> >> >>
>> >> >> >> Thanks in advance.
>> >> >> >>
>> >> >> >> Eryk.
>> >> >> >>
>> >> >> >> Dipl. bio-chem. Eryk Witold Wolski    @    MPI-MG Dep.
>Vertebrate
>> >> >> >> Genomics Ihnestrasse 73 14195 Berlin          'v' tel:
>> >> >> >> 0049-30-84131285               /   \ mail: wolski at molgen.mpg.de
>> >> >> >> ---W-W----
>> >> >> >>
>> >> >> >>
>> >> >> >>  [[alternative HTML version deleted]]
>> >> >> >>
>> >> >> >> ______________________________________________
>> >> >> >> R-help at stat.math.ethz.ch mailing list
>> >> >> >> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> >> >>
>> >> >>
>> >> >>
>> >> >> Dipl. bio-chem. Eryk Witold Wolski    @    MPI-MG Dep. Vertebrate
>> >> >Genomics
>> >> >> Ihnestrasse 73 14195 Berlin          'v'
>> >> >> tel: 0049-30-84131285               /   \
>> >> >> mail: wolski at molgen.mpg.de        ---W-W----
>> >> >http://www.molgen.mpg.de/~wolski
>> >> >>
>> >> >> ______________________________________________
>> >> >> R-help at stat.math.ethz.ch mailing list
>> >> >> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> >> >>
>> >> >>
>> >> >
>> >> >--
>> >> >Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> >> >Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>> >> >University of Oxford,             Tel:  +44 1865 272861 (self)
>> >> >1 South Parks Road,                     +44 1865 272866 (PA)
>> >> >Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>> >>
>> >>
>> >>
>> >> Dipl. bio-chem. Eryk Witold Wolski    @    MPI-MG Dep. Vertebrate
>> >Genomics
>> >> Ihnestrasse 73 14195 Berlin          'v'
>> >> tel: 0049-30-84131285               /   \
>> >> mail: wolski at molgen.mpg.de        ---W-W----   
>> >http://www.molgen.mpg.de/~wolski
>> >>
>> >>
>> >>
>> >
>> >-- 
>> >Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> >Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>> >University of Oxford,             Tel:  +44 1865 272861 (self)
>> >1 South Parks Road,                     +44 1865 272860 (secr)
>> >Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>> >
>> >______________________________________________
>> >R-help at stat.math.ethz.ch mailing list
>> >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> 
>> 
>> 
>> Dipl. bio-chem. Eryk Witold Wolski    @    MPI-MG Dep. Vertebrate
>Genomics   
>> Ihnestrasse 73 14195 Berlin          'v'    
>> tel: 0049-30-84131285               /   \    
>> mail: wolski at molgen.mpg.de        ---W-W----   
>http://www.molgen.mpg.de/~wolski 
>> 
>> 
>> 
>
>-- 
>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>University of Oxford,             Tel:  +44 1865 272861 (self)
>1 South Parks Road,                     +44 1865 272866 (PA)
>Oxford OX1 3TG, UK                Fax:  +44 1865 272595



Dipl. bio-chem. Eryk Witold Wolski    @    MPI-MG Dep. Vertebrate Genomics   
Ihnestrasse 73 14195 Berlin          'v'    
tel: 0049-30-84131285               /   \    
mail: wolski at molgen.mpg.de        ---W-W----    http://www.molgen.mpg.de/~wolski



From ripley at stats.ox.ac.uk  Mon Dec  1 15:23:49 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 1 Dec 2003 14:23:49 +0000 (GMT)
Subject: [R] Rd Files?
In-Reply-To: <200312011500000945.011C6076@harry.molgen.mpg.de>
Message-ID: <Pine.LNX.4.44.0312011419370.1554-100000@gannet.stats>

On Mon, 1 Dec 2003, Wolski wrote:

> If it does not matter how the name of the Rd file are, would it be
> difficult than to modify the package.skeleton function, so that it works
> under windows 2000 too?

Why don't you try to do it and find out how difficult it is *for you*?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From kmw at mail.rockefeller.edu  Mon Dec  1 15:38:52 2003
From: kmw at mail.rockefeller.edu (Knut M. Wittkowski)
Date: Mon, 01 Dec 2003 09:38:52 -0500
Subject: [R] wilcoxon-pratt signed rank  test in R -  drug-effiacy
In-Reply-To: <x2brqshhhc.fsf@biostat.ku.dk>
References: <BAY7-F43VkFV9kXk6uS0001fb1e@hotmail.com>
	<BAY7-F43VkFV9kXk6uS0001fb1e@hotmail.com>
Message-ID: <5.1.0.14.0.20031201092052.02201548@imap.rockefeller.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031201/588705ef/attachment.pl

From tlumley at u.washington.edu  Mon Dec  1 16:15:47 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 1 Dec 2003 07:15:47 -0800 (PST)
Subject: [R] Discovering methods
In-Reply-To: <20031201015549.433C5397D@mprdmxin.myway.com>
References: <20031201015549.433C5397D@mprdmxin.myway.com>
Message-ID: <Pine.A41.4.58.0312010714310.134678@homer12.u.washington.edu>

On Sun, 30 Nov 2003, Gabor Grothendieck wrote:

>
>
> Thanks.  I guess we have to be content to approximate this via:
>
>    apropos("POSIXt$|POSIXct$")
>
> although this supposes we know that POSIXct inherits from POSIXt
> and its not clear that there is a reliable way to discover that
> for S3 classes.

It's actually easier for S3 classes, since there it is objects that
inherit, not classes: eg

> class(.leap.seconds)
[1] "POSIXt"  "POSIXct"

	-thomas



From bruno at speech.kth.se  Mon Dec  1 16:55:29 2003
From: bruno at speech.kth.se (Bruno Giordano)
Date: Mon, 1 Dec 2003 16:55:29 +0100
Subject: [R] cclust - cindex - binary data
References: <1070017824.3fc72d20eaba8@webmail.unipd.it>
Message-ID: <00a201c3b823$8b15e560$ba43ed82@brungio>

If anybody had the same problem I had in calculating the cindex with the
clustIndex function (binary data),
replace this line of code:

if (length(unique(x))==2)

with this one:

if (length(unique(as.vector(x)))==2)

located in the call to the procedure for calculating the cindex, in the
cclust file, in the library/cclust/R folder.

    Bruno


----- Original Message ----- 
From: <bruno.giordano at unipd.it>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, November 28, 2003 12:10 PM
Subject: [R] cclust - cindex - binary data


> Hi,
> I'm trying to debug a function I wrote to calculate the cindex for a
> hierarchical tree.
> For this it is useful to compare my calculations with those in output from
> the clustindex function, in the cclust library.
> There's no way, however, to have the cindex value for a given output of
the
> cclust function, as a NA value is always returned.
> This happens almost surely because the cindex in clustIndex is calculated
> only for binary data, but, in turn, I can't find a way to specify either
> with the cclust function or with the clustIndex function, that an eventual
> input data set is binary.
>
> Thanks a lot
>     Bruno
>
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> Bruno L. Giordano - Ph. D. student
> Dipartimento di Psicologia Generale
> Via Venezia 8 - 35131 Padova, Italy
>
> currently hosted by
>
> KTH - Royal Institute of Technology
> TMH - Department of Speech, Music and Hearing
> Drottning Kristinas v. 31
> SE-100 44 Stockholm, Sweden
>
> -------------------------------------------------
> This mail sent through IMP: webmail.unipd.it



From rpeng at jhsph.edu  Mon Dec  1 16:59:17 2003
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Mon, 01 Dec 2003 10:59:17 -0500
Subject: [R] with for objects
In-Reply-To: <3FC9B643.6050700@auckland.ac.nz>
References: <3FC9B643.6050700@auckland.ac.nz>
Message-ID: <3FCB6555.2010808@jhsph.edu>

I'm not sure S4 objects are really meant to be used this way.  Maybe you 
  could write a plot method?

-roger

Hadley Wickham wrote:
> Is there a form of with (or an equivalent function) that can be applied 
> to objects?
> 
> I'd like to be able to write something like (drawing from bioconductor + 
> trellis as an example)
> xyplot(maA ~ maM | maPrintTip, object = swirl[,1]) which would be 
> interpreted as xyplot(swirl[,1]@maA ~ swirl[,1]@maM | 
> swirl[,1]@maPrintTip) (or even better, match functions not slots eg. as 
> xyplot(maA(swirl[,1]) ~ maM(swirl[,1]) | maPrintTip(swirl[,1])))
> 
> If there isn't, can any one offer any tips on writing such a function.  
> I presume I'd have to deparse the formula, match the text with the 
> slots/methods of the function, create the appropriate call objects and 
> then call them?
> 
> Thanks for you help,
> 
> Hadley
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From jarrod.hadfield at imperial.ac.uk  Mon Dec  1 17:36:19 2003
From: jarrod.hadfield at imperial.ac.uk (jarrod.hadfield@imperial.ac.uk)
Date: Mon, 1 Dec 2003 16:36:19 +0000
Subject: [R] matrix bending
Message-ID: <a06010201bbf11cffd878@imperial.ac.uk>

Dear All,

I was wondering whether any one knows of a matrix bending function in 
R that can turn non-positive definite matrices into the nearest 
positive definite matrix.  I was hoping there would be something akin 
to John Henshall's flbend program 
(http://agbu.une.edu.au/~kmeyer/pdmatrix.html), which allows the 
standard errors of the estimated matrix elements to be considered in 
the bending process.

Thanks,

Jarrod.



From savano at superig.com.br  Mon Dec  1 17:44:09 2003
From: savano at superig.com.br (Savano)
Date: Mon, 1 Dec 2003 14:44:09 -0200
Subject: [R] white's general heterocedasticity test
Message-ID: <200312011444.09227.savano@superig.com.br>

UseRs,

I want to do the white's general heterocedasticity test, but I don't find that 
function. I find "white.test" that's ' White Neural Network Test for 
Nonlinearity'. Anybody help me?



From wolski at molgen.mpg.de  Mon Dec  1 18:11:56 2003
From: wolski at molgen.mpg.de (Wolski)
Date: Mon, 01 Dec 2003 18:11:56 +0100
Subject: [R] Rd Files?
In-Reply-To: <Pine.LNX.4.44.0312011419370.1554-100000@gannet.stats>
References: <Pine.LNX.4.44.0312011419370.1554-100000@gannet.stats>
Message-ID: <200312011811560288.01CC12F7@harry.molgen.mpg.de>

Hi!

Thanks for suggestions.

Sincerely
Eryk

Ps.
In the attachment are prompt and package.skeleton modified so that it works on windows 2000 too.
It substitutes <- in file names = before writing if sys.info()["sysname"]=="Windows".


*********** REPLY SEPARATOR  ***********

On 12/1/2003 at 2:23 PM Prof Brian Ripley wrote:

>On Mon, 1 Dec 2003, Wolski wrote:
>
>> If it does not matter how the name of the Rd file are, would it be
>> difficult than to modify the package.skeleton function, so that it works
>> under windows 2000 too?
>
>Why don't you try to do it and find out how difficult it is *for you*?
>
>-- 
>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>University of Oxford,             Tel:  +44 1865 272861 (self)
>1 South Parks Road,                     +44 1865 272866 (PA)
>Oxford OX1 3TG, UK                Fax:  +44 1865 272595



Dipl. bio-chem. Eryk Witold Wolski    @    MPI-MG Dep. Vertebrate Genomics   
Ihnestrasse 73 14195 Berlin          'v'    
tel: 0049-30-84131285               /   \    
mail: wolski at molgen.mpg.de        ---W-W----    http://www.molgen.mpg.de/~wolski 

From Arne.Muller at aventis.com  Mon Dec  1 18:41:06 2003
From: Arne.Muller at aventis.com (Arne.Muller@aventis.com)
Date: Mon, 1 Dec 2003 18:41:06 +0100
Subject: [R] significance in difference of proportions
Message-ID: <C80ECAFA2ACC1B45BE45D133ED660ADE010BF107@crbsmxsusr04.pharma.aventis.com>

Hello,

thanks for the replies to this subject. I'm using a fisher.test to test if
the proportions of my 2 samples are different (see Ted's example below).
 
The assumption was that the two samples are from the same population and that
they may contain a different number of "positives" (due to different
treatment). 

I may be able to figues out the true probability to get a "positive", since I
for some of my experiments I know the entire population. E.g. the samples
(111 items, and 10 items) come from a population of 10,000 items, and I know
that there are 200 positives in the population.

Is it possible to use the fisher test for testing equallity of proportions
and to include the known probability to find a positive - would that make
sense at all? If the two samples come from the same population the
probability to find a positive shouldn't influence the test for difference of
proportions, should it? 

At some point I'd like to extend the statistics so that the two samples can
come from 2 different populations (with known probability for the positives).

I'm happy to receive suggestions and comments on this.

	thanks a lot again for your help,

	Arne 

> 
> On 27-Nov-03 Arne.Muller at aventis.com wrote:
> > I've 2 samples A (111 items) and B (10 items) drawn from the same
> > unknown population. Witihn A I find 9 "positives" and in B 0
> > positives. I'd like to know if the 2 samples A and B are different,
> > ie is there a way to find out whether the number of "positives" is
> > significantly different in A and B?
> 
> Pretty obviously not, just from looking at the numbers:
> 
> 9 out of 111 -> p = P(positive) approx = 1/10
> 
> P(0 out of 10 when p = 1/10) is not unlikely (in fact = 0.35).
> 
> However, a Fisher exact test will give you a respectable P-value:
> 
> > library(ctest)
> > ?fisher.test
> > fisher.test(matrix(c(102,9,10,0),nrow=2))
>   [...]
>   p-value = 1
>   alternative hypothesis: true odds ratio is not equal to 1 
>   95 percent confidence interval:
>    0.000000 6.088391 
> > fisher.test(matrix(c(102,9,9,1),nrow=2))
>   p-value = 0.5926
> > fisher.test(matrix(c(102,9,8,2),nrow=2))
>   p-value = 0.2257
> > fisher.test(matrix(c(102,9,7,3),nrow=2))
>   p-value = 0.0605
> > fisher.test(matrix(c(102,9,6,4),nrow=2))
>   p-value = 0.01202
> 
> So there's a 95% CI (0,6.1) for the odds ratio which, for
> identical probabilities of "+", is 1.0 hence well within the CI.
> And, keeping the numbers for the larger sample fixed for
> simplicity, you have to go quite a way with the smaller one to get
> a result significant at 5%:
> 
> (102,9):(7,3) -> P = 0.06
> (102,9):(6,4) -> P = 0.01
> 
> and, to have 80% power (0.8 probability of this event), the
> probability of "+" in the second sample would have to be as
> high as 0.41.
> 
> Conclusion: your second sample size is quite inadequate except
> to detect rather large differences between the true proportions
> in the two cases!
> 
> Best wishes,
> Ted.
> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 167 1972
> Date: 27-Nov-03                                       Time: 17:43:00
> ------------------------------ XFMail ------------------------------
>



From ripley at stats.ox.ac.uk  Mon Dec  1 19:08:08 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 1 Dec 2003 18:08:08 +0000 (GMT)
Subject: [R] Rd Files?
In-Reply-To: <200312011811560288.01CC12F7@harry.molgen.mpg.de>
Message-ID: <Pine.LNX.4.44.0312011753460.7849-100000@gannet.stats>

On Mon, 1 Dec 2003, Wolski wrote:

> In the attachment are prompt and package.skeleton modified so that it
> works on windows 2000 too. It substitutes <- in file names = before
> writing if sys.info()["sysname"]=="Windows".

However, that is nowhere near sufficient: please take a look at the Perl
file `check' for a list of conditions you would need to fulfil on the file
names.  Did you actually try to find out what are valid filenames on your
half-implemented filesystem?  Did you consider what happens with names
which differ only by case?


> *********** REPLY SEPARATOR  ***********
> 
> On 12/1/2003 at 2:23 PM Prof Brian Ripley wrote:
> 
> >On Mon, 1 Dec 2003, Wolski wrote:
> >
> >> If it does not matter how the name of the Rd file are, would it be
> >> difficult than to modify the package.skeleton function, so that it works
> >> under windows 2000 too?
> >
> >Why don't you try to do it and find out how difficult it is *for you*?

Too difficult ....

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From lschwei at ucla.edu  Mon Dec  1 19:13:23 2003
From: lschwei at ucla.edu (Lisa Schweitzer)
Date: Mon, 01 Dec 2003 10:13:23 -0800
Subject: [R] Re: using shapefile as owin 
In-Reply-To: <200312011103.hB1B37Ck026010@hypatia.math.ethz.ch>
Message-ID: <BBF0C4C3.4CDC%lschwei@ucla.edu>

My sincerest apologies, as this is a very elementary problem, but I have
searched through archives and FAQs and on the web, and I am at the end of my
own resources.

I need to do analysis of a spatial point process occuring with four
counties, using spatstat. I've had no trouble importing the shapefiles, but
I can't seem to figure out what I need to do in order to use the boundary as
an owin. Why I use the Map2poly command, it appears to work, but not when I
attempt define the window using the shapefile; it also gives me errors when
I try to define a ppp.object.  I am using the latest release of Raqua on G4.
My process looks like this:

#begin code
> library(maptools)
> Shapefile<-read.shape("county.shp", dbf.data=TRUE)
Shapefile Type: Polygon   # of Shapes: 4
> plot(Shapefile, fg="white")
> rm(Map)
 
> shapepoly<-Map2poly(Shapefile, region.id = NULL)
     Map2lines(Shapefile)
     Map2points(Shapefile)
     Map2bbs(Shapefile)

[[1]]
             [,1]     [,2]
   [1,] -116.1655 34.03370
   [2,] -116.1672 34.03370
   [3,] -116.1819 34.03364

#R does its thing for a gillion more lines, then,

Error in Map2lines(Shapefile) : maptype not line/arc

> plot(shapepoly)
Error in plot.window(xlim, ylim, log, asp, ...) :
    need finite xlim values
In addition: Warning messages:
1: no finite arguments to min; returning Inf
2: no finite arguments to max; returning -Inf
3: no finite arguments to min; returning Inf
4: no finite arguments to max; returning -Inf
> library(spatstat)
spatstat 1.3-3
Type "demo(spatstat)" for a demonstration
See the Introduction and Quick Reference in
/Users/lschwei/Library/RAqua/library/spatstat/doc
> owin(shapepoly)
Error in owin(shapepoly) : If one of xrange, yrange is specified then both
must be.
#end code


Lisa
-------
Lisa Schweitzer 
Center for the Study of Urban Poverty
Dept. of Urban Planning
University of California, Los Angeles
(lschwei at ucla.edu)



From dmurdoch at pair.com  Mon Dec  1 19:30:37 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Mon, 01 Dec 2003 13:30:37 -0500
Subject: [R] Discovering methods
In-Reply-To: <Pine.A41.4.58.0312010714310.134678@homer12.u.washington.edu>
References: <20031201015549.433C5397D@mprdmxin.myway.com>
	<Pine.A41.4.58.0312010714310.134678@homer12.u.washington.edu>
Message-ID: <eq1nsv8fifkqc3v1ognsgf9sa5h8j5jofi@4ax.com>

On Mon, 1 Dec 2003 07:15:47 -0800 (PST), Thomas Lumley
<tlumley at u.washington.edu> wrote :

>On Sun, 30 Nov 2003, Gabor Grothendieck wrote:
>
>>
>>
>> Thanks.  I guess we have to be content to approximate this via:
>>
>>    apropos("POSIXt$|POSIXct$")
>>
>> although this supposes we know that POSIXct inherits from POSIXt
>> and its not clear that there is a reliable way to discover that
>> for S3 classes.
>
>It's actually easier for S3 classes, since there it is objects that
>inherit, not classes: eg
>
>> class(.leap.seconds)
>[1] "POSIXt"  "POSIXct"

But then the obvious guess at what Gabor wants doesn't work:

>> apropos(class(.leap.seconds))
> [1] ".__C__POSIXt"        "-.POSIXt"            "+.POSIXt"           
> [4] "as.character.POSIXt" "cut.POSIXt"          "diff.POSIXt"        
> [7] "hist.POSIXt"         "julian.POSIXt"       "Math.POSIXt"        
>[10] "months.POSIXt"       "Ops.POSIXt"          "quarters.POSIXt"    
>[13] "round.POSIXt"        "seq.POSIXt"          "str.POSIXt"         
>[16] "trunc.POSIXt"        "weekdays.POSIXt"    
>Warning messages: 
>1: the condition has length > 1 and only the first element will be used in: if (is.na(pattern)) { 
>2: the condition has length > 1 and only the first element will be used in: if (is.na(pattern)) { 
>3: the condition has length > 1 and only the first element will be used in: if (is.na(pattern)) { 
>4: the condition has length > 1 and only the first element will be used in: if (is.na(pattern)) { 
>5: the condition has length > 1 and only the first element will be used in: if (is.na(pattern)) { 
>6: the condition has length > 1 and only the first element will be used in: if (is.na(pattern)) { 
>7: the condition has length > 1 and only the first element will be used in: if (is.na(pattern)) { 
>8: the condition has length > 1 and only the first element will be used in: if (is.na(pattern)) { 
>9: the condition has length > 1 and only the first element will be used in: if (is.na(pattern)) { 
>> 

What works is this:

 apropos(paste(class(.leap.seconds),'$',sep='',collapse='|'))

Duncan Murdoch



From pburns at pburns.seanet.com  Mon Dec  1 19:30:39 2003
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Mon, 01 Dec 2003 18:30:39 +0000
Subject: [R] matrix bending
References: <a06010201bbf11cffd878@imperial.ac.uk>
Message-ID: <3FCB88CF.9070400@pburns.seanet.com>

Here is a function that I use for symmetric matrices:

make.positive.definite <-
function(x, tol=1e-6) {
    eig <- eigen(x, symmetric=TRUE)
    rtol <- tol * eig$values[1]
    if(min(eig$values) < rtol) {
        vals <- eig$values
        vals[vals < rtol} <- rtol
        srev <- eig$vectors %*% (vals * t(eig$vectors))
        dimnames(srev) <- dimnames(x)
        return(srev)
    } else {
        return(x)
    }
}


Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")


jarrod.hadfield at imperial.ac.uk wrote:

> Dear All,
>
> I was wondering whether any one knows of a matrix bending function in 
> R that can turn non-positive definite matrices into the nearest 
> positive definite matrix.  I was hoping there would be something akin 
> to John Henshall's flbend program 
> (http://agbu.une.edu.au/~kmeyer/pdmatrix.html), which allows the 
> standard errors of the estimated matrix elements to be considered in 
> the bending process.
>
> Thanks,
>
> Jarrod.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>



From xiaoliu at jhmi.edu  Mon Dec  1 20:59:25 2003
From: xiaoliu at jhmi.edu (XIAO LIU)
Date: Mon, 01 Dec 2003 14:59:25 -0500
Subject: [R] Kurtosis function
Message-ID: <67d2bf67da4a.67da4a67d2bf@jhmimail.jhmi.edu>

Hi,

Which R package contains Kurtosis function?  Where can I get it?

Thank you very much.

Xiao



From abunn at montana.edu  Mon Dec  1 21:05:44 2003
From: abunn at montana.edu (Andy Bunn)
Date: Mon, 1 Dec 2003 13:05:44 -0700
Subject: [R] Kurtosis function
In-Reply-To: <67d2bf67da4a.67da4a67d2bf@jhmimail.jhmi.edu>
Message-ID: <001901c3b846$930d9bf0$78f05a99@msu.montana.edu>

The e1071 package does kurtosis.
kurtosis(x, na.rm=FALSE)

HTH, Andy



From faheem at email.unc.edu  Mon Dec  1 21:23:18 2003
From: faheem at email.unc.edu (Faheem Mitha)
Date: Mon, 1 Dec 2003 15:23:18 -0500 (EST)
Subject: [R] help with random numbers and Rmpi
Message-ID: <Pine.LNX.4.58.0312011520250.26318@Chrestomanci>


Dear People,

This may not be the right place to ask a question about Rmpi, but I don't
know of a better one.

I am trying to get a simple program working using Rmpi with the model of 1
R master and n C slaves. What I am trying to do is have each of the C
slaves generate a random number from U[0,1], and then have the master
collect all n numbers as a vector and output it. However even doing this
is rather over my head. I'm trying to use rsprng and sprng, but I am sure
what I am currently doing is wrong.

I enclose a first attempt. Any suggestions would be appreciated. Thanks in
advance.

                                                                  Faheem.

*************************************************************************
rand.R
*************************************************************************
library(Rmpi)

rand <- function ()
{
  if (mpi.comm.size(1) > 1)
    stop ("It seems some slaves running on comm 1.")
  mpi.comm.spawn("./rand")
  mpi.intercomm.merge(2,0,1)

  mpi.init.sprng()
  free.sprng() #does this function exist here?

  rdata <- double(sum(mpi.comm.size(1)))
  out <- mpi.gather(0, 2, rdata) ##this isn't right
  mpi.comm.free()
  out
}

*************************************************************************
rand.c
*************************************************************************
#include <mpi.h>
#include <sprng.h>

int main(int argc, char **argv)
{
  double rand;
  double* randarray;

  MPI_Comm slavecomm, all_processes;

  /*Initialize MPI*/
  MPI_Init(&argc, &argv);

  MPI_Comm_get_parent(&slavecomm);
  MPI_Intercomm_merge(slavecomm, 1, &all_processes);

  /*How many processes are there?*/
  MPI_Comm_size(all_processes, &size);

  /*Which one am I?*/
  MPI_Comm_rank(all_processes, &rank);

  init_sprng()
  rand = sprng();
  free.sprng()

  randarray = (double *)malloc(sizeof(double)*size);

  /*Gather random numbers from all C slave processes*/
  /* Using randarray doesn't make sense since this should correspond
  to the rdata vector in the R master process */
  MPI_GAther(&rand, 1, 1, MPI_DOUBLE, randarray, 1, MPI_DOUBLE, 0, all_processes);

  /*All done*/
  MPI_Comm_free(&all_processes);
  MPI_Finalize();
  exit(0);
}



From h.wickham at auckland.ac.nz  Mon Dec  1 21:33:40 2003
From: h.wickham at auckland.ac.nz (Hadley Wickham)
Date: Tue, 02 Dec 2003 09:33:40 +1300
Subject: [R] with for objects
In-Reply-To: <3FCB6555.2010808@jhsph.edu>
References: <3FC9B643.6050700@auckland.ac.nz> <3FCB6555.2010808@jhsph.edu>
Message-ID: <3FCBA5A4.6040906@auckland.ac.nz>

Why not? A data frame is a convenient way of grouping related data 
together.  So is an object.  Writing with(expr, a + b) is just shorthand 
for writing expr$a + expr$b, so why shouldn't I be able to write 
with(obj, a + b) for obj at a + obj at b? 

I'm not keen to write a plot method because I'm trying to create a more 
generalised way of exploring data stored in objects.  For example, with 
microarray data from bioconductor, I might want to do one MA for all the 
data, or one for each printtip.

Hadley

Roger D. Peng wrote:

> I'm not sure S4 objects are really meant to be used this way.  Maybe 
> you  could write a plot method?
>
> -roger



From tlumley at u.washington.edu  Mon Dec  1 21:41:27 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 1 Dec 2003 12:41:27 -0800 (PST)
Subject: [R] Discovering methods
In-Reply-To: <eq1nsv8fifkqc3v1ognsgf9sa5h8j5jofi@4ax.com>
References: <20031201015549.433C5397D@mprdmxin.myway.com>
	<Pine.A41.4.58.0312010714310.134678@homer12.u.washington.edu>
	<eq1nsv8fifkqc3v1ognsgf9sa5h8j5jofi@4ax.com>
Message-ID: <Pine.A41.4.58.0312011240520.30536@homer18.u.washington.edu>

On Mon, 1 Dec 2003, Duncan Murdoch wrote:

> But then the obvious guess at what Gabor wants doesn't work:
>
> >> apropos(class(.leap.seconds))
> > [1] ".__C__POSIXt"        "-.POSIXt"            "+.POSIXt"
> > [4] "as.character.POSIXt" "cut.POSIXt"          "diff.POSIXt"
> > [7] "hist.POSIXt"         "julian.POSIXt"       "Math.POSIXt"
> >[10] "months.POSIXt"       "Ops.POSIXt"          "quarters.POSIXt"
> >[13] "round.POSIXt"        "seq.POSIXt"          "str.POSIXt"
> >[16] "trunc.POSIXt"        "weekdays.POSIXt"
> >Warning messages:
> >1: the condition has length > 1 and only the first element will be used in: if (is.na(pattern)) {
> >2: the condition has length > 1 and only the first element will be used in: if (is.na(pattern)) {
> >3: the condition has length > 1 and only the first element will be used in: if (is.na(pattern)) {
> >4: the condition has length > 1 and only the first element will be used in: if (is.na(pattern)) {
> >5: the condition has length > 1 and only the first element will be used in: if (is.na(pattern)) {
> >6: the condition has length > 1 and only the first element will be used in: if (is.na(pattern)) {
> >7: the condition has length > 1 and only the first element will be used in: if (is.na(pattern)) {
> >8: the condition has length > 1 and only the first element will be used in: if (is.na(pattern)) {
> >9: the condition has length > 1 and only the first element will be used in: if (is.na(pattern)) {
> >>
>
> What works is this:
>
>  apropos(paste(class(.leap.seconds),'$',sep='',collapse='|'))
>

or
    sapply(class(.leap.seconds), apropos)


	-thomas



From rossini at blindglobe.net  Mon Dec  1 21:45:48 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Mon, 01 Dec 2003 12:45:48 -0800
Subject: [R] help with random numbers and Rmpi
In-Reply-To: <Pine.LNX.4.58.0312011520250.26318@Chrestomanci> (Faheem
	Mitha's message of "Mon, 1 Dec 2003 15:23:18 -0500 (EST)")
References: <Pine.LNX.4.58.0312011520250.26318@Chrestomanci>
Message-ID: <85fzg4jmgz.fsf@blindglobe.net>


use snow.

The general approach highlighted in 

http://www.analytics.washington.edu/~rossini/courses/cph-statcomp

in Lab 4 works with Rmpi as well.


Faheem Mitha <faheem at email.unc.edu> writes:

> Dear People,
>
> This may not be the right place to ask a question about Rmpi, but I don't
> know of a better one.
>
> I am trying to get a simple program working using Rmpi with the model of 1
> R master and n C slaves. What I am trying to do is have each of the C
> slaves generate a random number from U[0,1], and then have the master
> collect all n numbers as a vector and output it. However even doing this
> is rather over my head. I'm trying to use rsprng and sprng, but I am sure
> what I am currently doing is wrong.
>
> I enclose a first attempt. Any suggestions would be appreciated. Thanks in
> advance.
>
>                                                                   Faheem.
>
> *************************************************************************
> rand.R
> *************************************************************************
> library(Rmpi)
>
> rand <- function ()
> {
>   if (mpi.comm.size(1) > 1)
>     stop ("It seems some slaves running on comm 1.")
>   mpi.comm.spawn("./rand")
>   mpi.intercomm.merge(2,0,1)
>
>   mpi.init.sprng()
>   free.sprng() #does this function exist here?
>
>   rdata <- double(sum(mpi.comm.size(1)))
>   out <- mpi.gather(0, 2, rdata) ##this isn't right
>   mpi.comm.free()
>   out
> }
>
> *************************************************************************
> rand.c
> *************************************************************************
> #include <mpi.h>
> #include <sprng.h>
>
> int main(int argc, char **argv)
> {
>   double rand;
>   double* randarray;
>
>   MPI_Comm slavecomm, all_processes;
>
>   /*Initialize MPI*/
>   MPI_Init(&argc, &argv);
>
>   MPI_Comm_get_parent(&slavecomm);
>   MPI_Intercomm_merge(slavecomm, 1, &all_processes);
>
>   /*How many processes are there?*/
>   MPI_Comm_size(all_processes, &size);
>
>   /*Which one am I?*/
>   MPI_Comm_rank(all_processes, &rank);
>
>   init_sprng()
>   rand = sprng();
>   free.sprng()
>
>   randarray = (double *)malloc(sizeof(double)*size);
>
>   /*Gather random numbers from all C slave processes*/
>   /* Using randarray doesn't make sense since this should correspond
>   to the rdata vector in the R master process */
>   MPI_GAther(&rand, 1, 1, MPI_DOUBLE, randarray, 1, MPI_DOUBLE, 0, all_processes);
>
>   /*All done*/
>   MPI_Comm_free(&all_processes);
>   MPI_Finalize();
>   exit(0);
> }
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From faheem at email.unc.edu  Mon Dec  1 22:10:18 2003
From: faheem at email.unc.edu (Faheem Mitha)
Date: Mon, 1 Dec 2003 16:10:18 -0500 (EST)
Subject: [R] help with random numbers and Rmpi
In-Reply-To: <85fzg4jmgz.fsf@blindglobe.net>
References: <Pine.LNX.4.58.0312011520250.26318@Chrestomanci>
	<85fzg4jmgz.fsf@blindglobe.net>
Message-ID: <Pine.LNX.4.58.0312011552290.26318@Chrestomanci>



On Mon, 1 Dec 2003, A.J. Rossini wrote:

>
> use snow.
>
> The general approach highlighted in
>
> http://www.analytics.washington.edu/~rossini/courses/cph-statcomp
>
> in Lab 4 works with Rmpi as well.

Forgive my cluelessness, but are the slaves spawned by snow R slaves or C
slaves? I need to work low level with C.

I'm looking at (for example)

http://www.stat.uiowa.edu/~luke/R/cluster/cluster.html

Thanks.

                                                       Faheem.



From p.dalgaard at biostat.ku.dk  Mon Dec  1 22:15:57 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 01 Dec 2003 22:15:57 +0100
Subject: [R] Rd Files?
In-Reply-To: <Pine.LNX.4.44.0312011753460.7849-100000@gannet.stats>
References: <Pine.LNX.4.44.0312011753460.7849-100000@gannet.stats>
Message-ID: <x2wu9gs0he.fsf@biostat.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> On Mon, 1 Dec 2003, Wolski wrote:
> 
> > In the attachment are prompt and package.skeleton modified so that it
> > works on windows 2000 too. It substitutes <- in file names = before
> > writing if sys.info()["sysname"]=="Windows".
> 
> However, that is nowhere near sufficient: please take a look at the Perl
> file `check' for a list of conditions you would need to fulfil on the file
> names.  Did you actually try to find out what are valid filenames on your
> half-implemented filesystem?  Did you consider what happens with names
> which differ only by case?
> > >
> > >Why don't you try to do it and find out how difficult it is *for you*?
> 
> Too difficult ....

Brian, if you pull any more, it will come off at the hip! 

Just one thing to add: If Wolski gets sufficiently annoyed to actually
try and do this right, he'll want a naming scheme that works on *all*
platforms, so branching on sys.info()["sysname"]=="Windows" is surely
wrong. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From rossini at blindglobe.net  Mon Dec  1 22:20:58 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Mon, 01 Dec 2003 13:20:58 -0800
Subject: [R] help with random numbers and Rmpi
In-Reply-To: <Pine.LNX.4.58.0312011552290.26318@Chrestomanci> (Faheem
	Mitha's message of "Mon, 1 Dec 2003 16:10:18 -0500 (EST)")
References: <Pine.LNX.4.58.0312011520250.26318@Chrestomanci>
	<85fzg4jmgz.fsf@blindglobe.net>
	<Pine.LNX.4.58.0312011552290.26318@Chrestomanci>
Message-ID: <853cc4jkud.fsf@blindglobe.net>

Faheem Mitha <faheem at email.unc.edu> writes:

> On Mon, 1 Dec 2003, A.J. Rossini wrote:
>
>>
>> use snow.
>>
>> The general approach highlighted in
>>
>> http://www.analytics.washington.edu/~rossini/courses/cph-statcomp
>>
>> in Lab 4 works with Rmpi as well.
>
> Forgive my cluelessness, but are the slaves spawned by snow R slaves or C
> slaves? I need to work low level with C.

R slaves.  

If you need to work with C slaves, you need to be pretty careful with
SPRNG.

Is there a reason you can't use C from R code?  If so, you get SPRNG
for free, if not, it's a royal pain.  

I'd be careful about SPRNG's MPI code, as well -- works with MPICH,
but it's been touchy with LAM-MPI, at least with the versions (LAM,
MPICH) I've worked with.  And versions seem to be somewhat important.

best,
-tony




-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From Roger.Bivand at nhh.no  Mon Dec  1 22:25:21 2003
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 1 Dec 2003 22:25:21 +0100 (CET)
Subject: [R] Re: using shapefile as owin 
In-Reply-To: <BBF0C4C3.4CDC%lschwei@ucla.edu>
Message-ID: <Pine.LNX.4.44.0312012205301.1327-100000@reclus.nhh.no>

On Mon, 1 Dec 2003, Lisa Schweitzer wrote:

> My sincerest apologies, as this is a very elementary problem, but I have
> searched through archives and FAQs and on the web, and I am at the end of my
> own resources.
> 
> I need to do analysis of a spatial point process occuring with four
> counties, using spatstat. I've had no trouble importing the shapefiles, but
> I can't seem to figure out what I need to do in order to use the boundary as
> an owin. Why I use the Map2poly command, it appears to work, but not when I
> attempt define the window using the shapefile; it also gives me errors when
> I try to define a ppp.object.  I am using the latest release of Raqua on G4.
> My process looks like this:
> 
> #begin code
> > library(maptools)
> > Shapefile<-read.shape("county.shp", dbf.data=TRUE)
> Shapefile Type: Polygon   # of Shapes: 4
> > plot(Shapefile, fg="white")
> > rm(Map)

Why this?

>  
> > shapepoly<-Map2poly(Shapefile, region.id = NULL)

This should create a polylist object as a list with four elements, each 
a ring of county boundary points, an n by 2 matrix with a number of 
attributes.

>      Map2lines(Shapefile)
>      Map2points(Shapefile)
>      Map2bbs(Shapefile)

Why this?

> 
> [[1]]
>              [,1]     [,2]
>    [1,] -116.1655 34.03370
>    [2,] -116.1672 34.03370
>    [3,] -116.1819 34.03364
> 
> #R does its thing for a gillion more lines, then,
> 
> Error in Map2lines(Shapefile) : maptype not line/arc
> 

Obviously, since it is contains polygons ...

> > plot(shapepoly)
> Error in plot.window(xlim, ylim, log, asp, ...) :
>     need finite xlim values
> In addition: Warning messages:
> 1: no finite arguments to min; returning Inf
> 2: no finite arguments to max; returning -Inf
> 3: no finite arguments to min; returning Inf
> 4: no finite arguments to max; returning -Inf

No, not (yet) plot.polylist(), but plotpolys().

> > library(spatstat)
> spatstat 1.3-3
> Type "demo(spatstat)" for a demonstration
> See the Introduction and Quick Reference in
> /Users/lschwei/Library/RAqua/library/spatstat/doc
> > owin(shapepoly)
> Error in owin(shapepoly) : If one of xrange, yrange is specified then both
> must be.

Why would you think that owin() accepts a polylist object as its default 
argument? Although help(owin.object) doesn't tell you, you can finf out 
from:

http://www.maths.uwa.edu.au/~adrian/spatstat/doc/Intro/

that:

"If the window boundary is a single polygon, then p should be a list with 
components x and y giving the coordinates of the vertices of the window 
boundary, traversed anticlockwise. Note that polygons should not be 
closed, i.e. the last vertex should not equal the first vertex. If the 
window boundary consists of several separate polygons, then p should be a 
list, each of whose components p[[i]] is a list with components x and y 
describing one of the polygons. The vertices of each polygon should be 
traversed anticlockwise for external boundaries and clockwise for internal 
boundaries (holes)"

>From the ESRI shapefile definition, we read that the vertices of a single 
ringed polygon are in clockwise order.

So your object does not conform at all to the structure owin() seems to 
need, although there are ways of doing this - contributions welcome 
(owin.polylist() for example).

Roger Bivand

> #end code
> 
> 
> Lisa
> -------
> Lisa Schweitzer 
> Center for the Study of Urban Poverty
> Dept. of Urban Planning
> University of California, Los Angeles
> (lschwei at ucla.edu)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From faheem at email.unc.edu  Mon Dec  1 22:39:59 2003
From: faheem at email.unc.edu (Faheem Mitha)
Date: Mon, 1 Dec 2003 16:39:59 -0500 (EST)
Subject: [R] help with random numbers and Rmpi
In-Reply-To: <853cc4jkud.fsf@blindglobe.net>
References: <Pine.LNX.4.58.0312011520250.26318@Chrestomanci>
	<85fzg4jmgz.fsf@blindglobe.net>
	<Pine.LNX.4.58.0312011552290.26318@Chrestomanci>
	<853cc4jkud.fsf@blindglobe.net>
Message-ID: <Pine.LNX.4.58.0312011627380.26318@Chrestomanci>



On Mon, 1 Dec 2003, A.J. Rossini wrote:

> Faheem Mitha <faheem at email.unc.edu> writes:
>
> > On Mon, 1 Dec 2003, A.J. Rossini wrote:
> >
> >>
> >> use snow.
> >>
> >> The general approach highlighted in
> >>
> >> http://www.analytics.washington.edu/~rossini/courses/cph-statcomp
> >>
> >> in Lab 4 works with Rmpi as well.
> >
> > Forgive my cluelessness, but are the slaves spawned by snow R slaves or C
> > slaves? I need to work low level with C.
>
> R slaves.
>
> If you need to work with C slaves, you need to be pretty careful with
> SPRNG.
>
> Is there a reason you can't use C from R code?  If so, you get SPRNG
> for free, if not, it's a royal pain.

It would be great if I could do it that way. I've used the .C interface to
R (for example) for ages. However, I need to parallise the code at the C
level, which is where I do nearly all of the heavy lifting. More
explicitly, I want the C code to do different things on different
processors, pass data back and forth and so on, where everything would be
controlled from the R level.

My impression is that if I was to use C from R, then the parallelization
would only be done at the R level.

So, can this (parallelization at the C level) be done without running a
bunch of C slaves along the lines I had previously written? Any examples
would be helpful.

> I'd be careful about SPRNG's MPI code, as well -- works with MPICH,
> but it's been touchy with LAM-MPI, at least with the versions (LAM,
> MPICH) I've worked with.  And versions seem to be somewhat important.

Yes, the whole SPRNG thing seems rather difficult. Thanks for your help.

                                                                Faheem.



From mathieu.drapeau at bioneq.qc.ca  Mon Dec  1 22:44:45 2003
From: mathieu.drapeau at bioneq.qc.ca (Mathieu Drapeau)
Date: Mon, 01 Dec 2003 16:44:45 -0500
Subject: [R] histogram density division
Message-ID: <3FCBB64D.2090300@bioneq.qc.ca>

Hi,
I would like to plot an histogram with modified density values.
My Y-axis represent the occurences of the ranges (specified by the 
breaks argument) of my data (represented by a big vector). Now, I would 
like to divide by X the number of occurences in each bands and do a plot 
of "lower" occurences.
How can I do that?

Thanks,
Mathieu



From tlumley at u.washington.edu  Mon Dec  1 23:12:46 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 1 Dec 2003 14:12:46 -0800 (PST)
Subject: [R] with for objects
In-Reply-To: <3FCBA5A4.6040906@auckland.ac.nz>
References: <3FC9B643.6050700@auckland.ac.nz> <3FCB6555.2010808@jhsph.edu>
	<3FCBA5A4.6040906@auckland.ac.nz>
Message-ID: <Pine.A41.4.58.0312011354110.30536@homer18.u.washington.edu>

On Tue, 2 Dec 2003, Hadley Wickham wrote:

> Why not? A data frame is a convenient way of grouping related data
> together.  So is an object.  Writing with(expr, a + b) is just shorthand
> for writing expr$a + expr$b, so why shouldn't I be able to write
> with(obj, a + b) for obj at a + obj at b?

It is a bad idea because it has to break the information hiding that is an
important point of objects.

If you define a class "A" that either includes or inherits from another
class "B" then you don't know what slots your object has (without looking
at the internals of the implementation of class "B"). Suppose you define
slots a and b in addition to whatever you have inherited.
You don't know what
  with(obj, a+x)
will do: is it obj at a+x or obj at a+obj@x for some inherited slot x? And even
worse, if someone extends your class and adds a slot x in the subclass,
what should it do?

Now, as it happens, the function slotNames() will give you all the slots,
so it is possible to work out where x is, but this doesn't make it a good
idea.

Without rewriting the internal code for eval() it is also hard to do: it
would require either a recursive search through the expression changing
slot names to obj at slot, or a function to convert objects into environments
that could be fed to eval().


	-thomas



From h.wickham at auckland.ac.nz  Mon Dec  1 23:54:51 2003
From: h.wickham at auckland.ac.nz (Hadley Wickham)
Date: Tue, 02 Dec 2003 11:54:51 +1300
Subject: [R] with for objects
In-Reply-To: <Pine.A41.4.58.0312011354110.30536@homer18.u.washington.edu>
References: <3FC9B643.6050700@auckland.ac.nz> <3FCB6555.2010808@jhsph.edu>
	<3FCBA5A4.6040906@auckland.ac.nz>
	<Pine.A41.4.58.0312011354110.30536@homer18.u.washington.edu>
Message-ID: <3FCBC6BB.6010706@auckland.ac.nz>


>>Why not? A data frame is a convenient way of grouping related data
>>together.  So is an object.  Writing with(expr, a + b) is just shorthand
>>for writing expr$a + expr$b, so why shouldn't I be able to write
>>with(obj, a + b) for obj at a + obj at b?
>>    
>>
>
>It is a bad idea because it has to break the information hiding that is an
>important point of objects.
>  
>
Good point - but with can be abused in this way too, eg. with(lm(...), 
plot(fitted.values ~ residuals)). 

What about if with(obj, a + b) was shorthand for a(obj) + b(obj)?  ie. 
it uses accessor functions instead of the slots (assuming that the 
accessor functions take no other arguments).

>If you define a class "A" that either includes or inherits from another
>class "B" then you don't know what slots your object has (without looking
>at the internals of the implementation of class "B"). 
>
I thought you did.  If I say x at A I expect that R will find the correct 
slot regardless of where in the class heirachy it is, though I suppose 
that is a slightly different question.

>Suppose you define
>slots a and b in addition to whatever you have inherited.
>You don't know what
>  with(obj, a+x)
>will do: is it obj at a+x or obj at a+obj@x for some inherited slot x? 
>
I'm not sure I understand the problem - I would expect it to try the 
slot in the object "environment" (if such a thing existed) first, and 
the parent environment etc.  Isn't this effectively what with() does?

>And even worse, if someone extends your class and adds a slot x in the subclass,
>what should it do?
>  
>
Yes, that is a (big) problem.  What do you suggest as an alternative? 

I am experimenting visualising microarray data using trellis.  I end up 
with a whole lot of calls like xyplot(maA(swirl[,1]) ~ maM(swirl[,1]) | 
maPrintTip(swirl[,1])) and then if I want to plot the same thing for 
array 2, I need xyplot(maA(swirl[,2]) ~ maM(swirl[,2]) | 
maPrintTip(swirl[,2]))...  I don't want to limit myself to a fixed set 
of plots because I'm still exploring different ways to look at the data, 
but I would like to save myself some typing.

>Without rewriting the internal code for eval() it is also hard to do: it
>would require either a recursive search through the expression changing
>slot names to obj at slot, or a function to convert objects into environments
>that could be fed to eval().
>  
>
I did end up writing something (very rough) to create an environment 
containing the results from the necessary function calls.  It works for 
my needs, but is very frail against changes in the objects I'm using.

Hadley



From rossini at blindglobe.net  Tue Dec  2 00:35:46 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Mon, 01 Dec 2003 15:35:46 -0800
Subject: [R] help with random numbers and Rmpi
In-Reply-To: <Pine.LNX.4.58.0312011627380.26318@Chrestomanci> (Faheem
	Mitha's message of "Mon, 1 Dec 2003 16:39:59 -0500 (EST)")
References: <Pine.LNX.4.58.0312011520250.26318@Chrestomanci>
	<85fzg4jmgz.fsf@blindglobe.net>
	<Pine.LNX.4.58.0312011552290.26318@Chrestomanci>
	<853cc4jkud.fsf@blindglobe.net>
	<Pine.LNX.4.58.0312011627380.26318@Chrestomanci>
Message-ID: <85llpwi019.fsf@blindglobe.net>

Faheem Mitha <faheem at email.unc.edu> writes:

> So, can this (parallelization at the C level) be done without running a
> bunch of C slaves along the lines I had previously written? Any examples
> would be helpful.

How much heavy lifting happens before you spawn the slaves, and can
that not be moved to R?  

Your best bet is to read the SNOW code for handling SPRNG/RSPRNG,
otherwise.

Good luck!

best,
-tony

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From baron at psych.upenn.edu  Tue Dec  2 00:58:33 2003
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Mon, 1 Dec 2003 18:58:33 -0500
Subject: [R] search site for R (http://finzi.psych.upenn.edu)
Message-ID: <20031201235833.GA11587@mail2.sas.upenn.edu>

My search site, http://finzi.psych.upenn.edu, has had several
problems recently, all my fault, for which I apologize.  But it
now seems to be running reliably, on a new computer that is much
faster than the old one.

It uses htdig to permit search of the Rhelp mailing list, R
documents, R functions, and various combinations of these.
Search has several options, including Boolean search (with AND,
etc.).

Suggestions are welcome.
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron



From spencer.graves at pdf.com  Tue Dec  2 01:13:32 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 01 Dec 2003 16:13:32 -0800
Subject: [R] search site for R (http://finzi.psych.upenn.edu)
In-Reply-To: <20031201235833.GA11587@mail2.sas.upenn.edu>
References: <20031201235833.GA11587@mail2.sas.upenn.edu>
Message-ID: <3FCBD92C.5020701@pdf.com>

It's a great service, which I use on average several times each day.  
Thanks for providing this.  Spencer Graves

Jonathan Baron wrote:

>My search site, http://finzi.psych.upenn.edu, has had several
>problems recently, all my fault, for which I apologize.  But it
>now seems to be running reliably, on a new computer that is much
>faster than the old one.
>
>It uses htdig to permit search of the Rhelp mailing list, R
>documents, R functions, and various combinations of these.
>Search has several options, including Boolean search (with AND,
>etc.).
>
>Suggestions are welcome.
>  
>



From christian_mora at vtr.net  Tue Dec  2 01:37:27 2003
From: christian_mora at vtr.net (Christian Mora)
Date: Mon, 1 Dec 2003 21:37:27 -0300
Subject: [R] Sampling
In-Reply-To: <200312011037.51244.savano@superig.com.br>
Message-ID: <000601c3b86c$76b70330$b43d68c8@CPQ28661778111>

YourData[sample(209,106),]

Regards
Christian Mora


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Savano
Sent: Monday, December 01, 2003 9:38 AM
To: Lista R
Subject: [R] Sampling


UseRs,

I imported a table using "read.table". It has 209 observations, I want
to 
select a sample with 106 observations.  What function I use?

thanks.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From edward at ratree.psu.ac.th  Tue Dec  2 01:45:17 2003
From: edward at ratree.psu.ac.th (Edward McNeil)
Date: Tue, 2 Dec 2003 07:45:17 +0700
Subject: [R] R and Memory
References: <3FC9B643.6050700@auckland.ac.nz> <3FCB6555.2010808@jhsph.edu>
	<3FCBA5A4.6040906@auckland.ac.nz>
Message-ID: <001501c3b86d$8e7cde80$3e021dac@psu.ac.th>

Dear all,
This is my first post.
We have started to use R here and have also started teaching it to our PhD
students. Our unit will be the HQ for developing R throughout Thailand.

I would like some help with a problem we are having. We have one sample of
data that is quite large in fact - over 2 million records (ok ok it's more
like a population!). The data is stored in SPSS. The file is over 350Mb but
SPSS happily stores this much data. Now when I try to read it into R it
grunts and groans for a few seconds and then reports that there is not
enough memory (the computer has 250MB RAM). I have tried setting the memory
in the command line (--max-vsize and --max-mem-size) but all to no avail.

Any help would be muchly appreciated!

Edward McNeil (son of Don)
Epidemiology Unit
Faculty of Medicine
Prince of Songkhla University
Hat Yai  90110
THAILAND



From apv at capital.net  Tue Dec  2 02:19:54 2003
From: apv at capital.net (Arend P. van der Veen)
Date: 01 Dec 2003 20:19:54 -0500
Subject: [R] Vector Assignments
Message-ID: <1070327994.13910.8.camel@redtail.mydomain.home>

Hi,

I have simple R question.  

I have a vector x that contains real numbers.  I would like to create
another vector col that is the same length of x such that:

if x[i] < 250 then col[i] = "red"
else if x[i] < 500 then col[i] = "blue"
else if x[i] < 750 then col[i] = "green"
else col[i] = "black" for all i

I am convinced that there is probably a very efficient way to do this in
R but I am not able to figure it out.  Any help would be greatly
appreciated.

Thanks in advance,
Arend van der Veen



From spencer.graves at pdf.com  Tue Dec  2 02:32:47 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 01 Dec 2003 17:32:47 -0800
Subject: [R] Vector Assignments
In-Reply-To: <1070327994.13910.8.camel@redtail.mydomain.home>
References: <1070327994.13910.8.camel@redtail.mydomain.home>
Message-ID: <3FCBEBBF.7070505@pdf.com>

      One way is to use "ifelse": 

x <- seq(100, 1000, by=100)
x.col <- ifelse(x < 250, "red",
    ifelse(x<500, "blue", ifelse(x<750, "green", "black")))
data.frame(x, x.col)
      x x.col
1   100   red
2   200   red
3   300  blue
4   400  blue
5   500 green
6   600 green
7   700 green
8   800 black
9   900 black
10 1000 black

      Does this seem reasonable? 
      spencer graves

Arend P. van der Veen wrote:

>Hi,
>
>I have simple R question.  
>
>I have a vector x that contains real numbers.  I would like to create
>another vector col that is the same length of x such that:
>
>if x[i] < 250 then col[i] = "red"
>else if x[i] < 500 then col[i] = "blue"
>else if x[i] < 750 then col[i] = "green"
>else col[i] = "black" for all i
>
>I am convinced that there is probably a very efficient way to do this in
>R but I am not able to figure it out.  Any help would be greatly
>appreciated.
>
>Thanks in advance,
>Arend van der Veen
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From r-eugenesalinas at comcast.net  Tue Dec  2 02:35:37 2003
From: r-eugenesalinas at comcast.net (Eugene Salinas (R))
Date: Mon, 01 Dec 2003 20:35:37 -0500
Subject: [R] smoothing functions
Message-ID: <3FCBEC69.5060009@comcast.net>

Dear all,

I am trying to program an estimator which maximizes a likelihood type 
objective function which is basically just lots of sums of indicator 
functions of data and parameters. In order to make the optimization I 
would like to smooth these functions. Since they are either 0 or 1, one 
possibility is to use the normal cdf.

I am wondering whether anyone is aware of a less arbitrary choice of a 
smoothing function? (is there any theory that suggests what's best to 
use?) Does anyone have any recommendations on what works best numerically?

Thanks, Eugene.



From h.wickham at auckland.ac.nz  Tue Dec  2 02:38:55 2003
From: h.wickham at auckland.ac.nz (Hadley Wickham)
Date: Tue, 02 Dec 2003 14:38:55 +1300
Subject: [R] Vector Assignments
In-Reply-To: <1070327994.13910.8.camel@redtail.mydomain.home>
References: <1070327994.13910.8.camel@redtail.mydomain.home>
Message-ID: <3FCBED2F.2050204@auckland.ac.nz>

One way would be to create a vector of colours and then cut() to index 
the vector:

colours <- c("red", "blue", "green","back")
colours[cut(x, c(min(x),250,500,700,max(x)),lab=F)]

Hadley


Arend P. van der Veen wrote:

>Hi,
>
>I have simple R question.  
>
>I have a vector x that contains real numbers.  I would like to create
>another vector col that is the same length of x such that:
>
>if x[i] < 250 then col[i] = "red"
>else if x[i] < 500 then col[i] = "blue"
>else if x[i] < 750 then col[i] = "green"
>else col[i] = "black" for all i
>
>I am convinced that there is probably a very efficient way to do this in
>R but I am not able to figure it out.  Any help would be greatly
>appreciated.
>
>Thanks in advance,
>Arend van der Veen
>



From tblackw at umich.edu  Tue Dec  2 02:40:08 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Mon, 1 Dec 2003 20:40:08 -0500 (EST)
Subject: [R] Vector Assignments
In-Reply-To: <1070327994.13910.8.camel@redtail.mydomain.home>
References: <1070327994.13910.8.camel@redtail.mydomain.home>
Message-ID: <Pine.SOL.4.58.0312012023240.7186@mspacman.gpcc.itd.umich.edu>

Arend  -

Here is a sequence of commands which will do it.
These first build a vector of (4+1) cutpoints,
then  cut()  returns a factor whose labels are
the colors and codes are determined by x.  Last,
as.character()  turns the factor into the character
vector which you ask for.  Or, perhaps the factor
data structure is more useful directly.  (Factors
are sort of an acquired taste.)

Note that in the call to cut, I am passing many
arguments into the function by their position in
the call.  You will need to look at  help("cut")
to figure out which argument is which.  Note also
that by monkeying with the two logical arguments,
("include.lowest" and "right"), I didn't need to
fudge any of the cutpoints.

tmp <- range(x)
tmp <- c(tmp[1], 250, 500, 750, tmp[2])
fac <- cut(x, tmp, c("red","blue","green","black"), TRUE, FALSE)
col <- as.character(fac)

HTH  -  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Mon, 1 Dec 2003, Arend P. van der Veen wrote:

> Hi,
>
> I have simple R question.
>
> I have a vector x that contains real numbers.  I would like to create
> another vector col that is the same length of x such that:
>
> if x[i] < 250 then col[i] = "red"
> else if x[i] < 500 then col[i] = "blue"
> else if x[i] < 750 then col[i] = "green"
> else col[i] = "black" for all i
>
> I am convinced that there is probably a very efficient way to do this in
> R but I am not able to figure it out.  Any help would be greatly
> appreciated.
>
> Thanks in advance,
> Arend van der Veen
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From Tom.Mulholland at health.wa.gov.au  Tue Dec  2 02:56:20 2003
From: Tom.Mulholland at health.wa.gov.au (Mulholland, Tom)
Date: Tue, 2 Dec 2003 09:56:20 +0800
Subject: [R] white's general heteroscedasticity test
Message-ID: <74E242B6968AA0469B632C5A3EFC1EFD03D57010@nt207mesep.corporate.hdwa.health.wa.gov.au>

I have a vague idea about heteroscedasticity so I thought I would see
what I could come up with. Unfortunately it's going to take me a while
before I am well enough versed in the ways in which it is used within
different packages. But I thought that if you haven't already tried you
should try searching the R-archives. I found 55 references to
heteroscedasticity and one which looks particularly relevant
http://finzi.psych.upenn.edu/R/Rhelp02/archive/1460.html

Ciao, Tom

_________________________________________________
 
Tom Mulholland
Senior Policy Officer
WA Country Health Service
Tel: (08) 9222 4062
 
The contents of this e-mail transmission are confidential an...{{dropped}}



From tblackw at umich.edu  Tue Dec  2 03:05:03 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Mon, 1 Dec 2003 21:05:03 -0500 (EST)
Subject: [R] smoothing functions
In-Reply-To: <3FCBEC69.5060009@comcast.net>
References: <3FCBEC69.5060009@comcast.net>
Message-ID: <Pine.SOL.4.58.0312012058420.7186@mspacman.gpcc.itd.umich.edu>

Eugene  -

Is the estimand in your problem (the parameter which you seek
to estimate) discrete-valued or continuous-valued ?  If it is
discrete-valued, then you are heading in the wrong direction,
because no matter how smooth you make the objective function,
you will not be able to differentiate it with respect to the
parameter !   I think I don't have quite enough information
to give a helpful answer to your question  . . .  but more
important is for you to find the answer yourself.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Mon, 1 Dec 2003, Eugene Salinas (R) wrote:

> Dear all,
>
> I am trying to program an estimator which maximizes a likelihood type
> objective function which is basically just lots of sums of indicator
> functions of data and parameters. In order to make the optimization I
> would like to smooth these functions. Since they are either 0 or 1, one
> possibility is to use the normal cdf.
>
> I am wondering whether anyone is aware of a less arbitrary choice of a
> smoothing function? (is there any theory that suggests what's best to
> use?) Does anyone have any recommendations on what works best numerically?
>
> Thanks, Eugene.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From so13839 at alltel.net  Tue Dec  2 03:32:11 2003
From: so13839 at alltel.net (Stephen Opiyo)
Date: Mon, 01 Dec 2003 20:32:11 -0600
Subject: [R] Help with this topic
Message-ID: <3FCBF9AB.7050607@alltel.net>

Dear ladies and gentlemen,

I would like to calculate autocovarinace and cross-covariance scores 1, 
2 and 3 of four classes A, B, C and D. I am using acf and ccf from time 
sires library.  My problem is that I can not separate my data among the 
classes A, B, C and D.  When I calculated acf for Score 1, I got a wrong 
result.  The reason being that instead of using ony 60, 40 and 20, the 
program use all the data in column under Score 1. What should I do to 
calculate acf and ccf scores for each class A, B, C and D according to 
he data below?

Class        Score 1  Score 2  Score 3
A                60          11          21
A                40          21          16
A                20          16          18
B                10          23          62
B                16            8          13
B                14          13          18
C                22          15          22
C                24            5          18
C                24          12          12
D                16             6         16
D                12             3          8
D                15             2         13
 
Thanks for your help.

SO



From tblackw at umich.edu  Tue Dec  2 04:01:40 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Mon, 1 Dec 2003 22:01:40 -0500 (EST)
Subject: [R] Help with this topic
In-Reply-To: <3FCBF9AB.7050607@alltel.net>
References: <3FCBF9AB.7050607@alltel.net>
Message-ID: <Pine.SOL.4.58.0312012153500.7186@mspacman.gpcc.itd.umich.edu>

Stephen  -

If the four columns shown below are in this order in a data
frame named 'data', then use

covariances <- by(data[ ,-1], data$Class, cov)

to get the covariance matrices within each of the four classes.
Alternative functions would be  tapply()  or  aggregate(), but
the syntax for  by()  is easiest to understand.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Mon, 1 Dec 2003, Stephen Opiyo wrote:

> Dear ladies and gentlemen,
>
> I would like to calculate autocovarinace and cross-covariance scores 1,
> 2 and 3 of four classes A, B, C and D. I am using acf and ccf from time
> sires library.  My problem is that I can not separate my data among the
> classes A, B, C and D.  When I calculated acf for Score 1, I got a wrong
> result.  The reason being that instead of using ony 60, 40 and 20, the
> program use all the data in column under Score 1. What should I do to
> calculate acf and ccf scores for each class A, B, C and D according to
> he data below?
>
> Class        Score 1  Score 2  Score 3
> A                60          11          21
> A                40          21          16
> A                20          16          18
> B                10          23          62
> B                16            8          13
> B                14          13          18
> C                22          15          22
> C                24            5          18
> C                24          12          12
> D                16             6         16
> D                12             3          8
> D                15             2         13
>
> Thanks for your help.
>
> SO
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From jeff.hamann at forestinformatics.com  Tue Dec  2 05:14:47 2003
From: jeff.hamann at forestinformatics.com (Jeff D. Hamann)
Date: Mon, 1 Dec 2003 20:14:47 -0800
Subject: [R] check WARNING...
Message-ID: <003601c3b88a$d5ad2e00$0a00a8c0@rodan>

I've been developing a package and have been getting the following warning
when running the check command:

* checking S3 generic/method consistency ... WARNING
plot:
  function(x, ...)
plot.summaries:
  function(trees, sp)

* checking for replacement functions with final arg not named 'value' ... OK
* checking Rd files ... OK

...blah, blah, blah...

* checking examples ... OK
* creating cruisepak-manual.tex ... OK
* checking cruisepak-manual.tex ... OK

WARNING: There were 3 warnings, see
  C://cruisepak.Rcheck/00check.log
for details


C:\>

I can fix the other two warnings with no problem, but the S3 generic/method
consistency warning is puzzling me... .

I have a function called

plot.summaries <- function( trees, sp=NULL ) {
...yak, kay, yak...
}

and I'm not sure if this is causing the problem. I'm developing a package
for forestry and the field makes use of many terms commonly found in
technology (logs, trees, plots, points, etc) and would like to know if the
nomenclature will cause a problem.

Thanks,
Jeff.


---
Jeff D. Hamann
Forest Informatics, Inc.
PO Box 1421
Corvallis, Oregon USA 97339-1421
(office) 541-754-1428
(cell) 541-740-5988
jeff.hamann at forestinformatics.com
www.forestinformatics.com


---
Jeff D. Hamann
Forest Informatics, Inc.
PO Box 1421
Corvallis, Oregon USA 97339-1421
(office) 541-754-1428
(cell) 541-740-5988
jeff.hamann at forestinformatics.com
www.forestinformatics.com



From ggrothendieck at myway.com  Tue Dec  2 05:47:39 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon,  1 Dec 2003 23:47:39 -0500 (EST)
Subject: [R] Vector Assignments
Message-ID: <20031202044739.60CA7395E@mprdmxin.myway.com>



Just some small refinements/corrections:

   colours <- c("red", "blue", "green","back")
   colours[cut(x, c(-Inf,250,500,700,Inf),right=F,lab=F)]

---
Date: Tue, 02 Dec 2003 14:38:55 +1300 
From: Hadley Wickham <h.wickham at auckland.ac.nz>
To: Arend P. van der Veen <apv at capital.net> 
Cc: R HELP <r-help at stat.math.ethz.ch> 
Subject: Re: [R] Vector Assignments 

 
 
One way would be to create a vector of colours and then cut() to index 
the vector:

colours <- c("red", "blue", "green","back")
colours[cut(x, c(min(x),250,500,700,max(x)),lab=F)]

Hadley


Arend P. van der Veen wrote:

>Hi,
>
>I have simple R question. 
>
>I have a vector x that contains real numbers. I would like to create
>another vector col that is the same length of x such that:
>
>if x[i] < 250 then col[i] = "red"
>else if x[i] < 500 then col[i] = "blue"
>else if x[i] < 750 then col[i] = "green"
>else col[i] = "black" for all i
>
>I am convinced that there is probably a very efficient way to do this in
>R but I am not able to figure it out. Any help would be greatly
>appreciated.
>
>Thanks in advance,
>Arend van der Veen
>



From nali at biostat.umn.edu  Tue Dec  2 06:32:26 2003
From: nali at biostat.umn.edu (Na Li)
Date: Mon, 01 Dec 2003 23:32:26 -0600
Subject: [R] help with random numbers and Rmpi
In-Reply-To: <Pine.LNX.4.58.0312011520250.26318@Chrestomanci> (Faheem
	Mitha's message of "Mon, 1 Dec 2003 15:23:18 -0500 (EST)")
References: <Pine.LNX.4.58.0312011520250.26318@Chrestomanci>
Message-ID: <ioekvnokd1.fsf@dhcp48.biostat.umn.edu>

On 1 Dec 2003, Faheem Mitha spake thusly:
  
>  I am trying to get a simple program working using Rmpi with the model of 1
>  R master and n C slaves. What I am trying to do is have each of the C
>  slaves generate a random number from U[0,1], and then have the master
>  collect all n numbers as a vector and output it. However even doing this
>  is rather over my head. I'm trying to use rsprng and sprng, but I am sure
>  what I am currently doing is wrong.
>  
>  *************************************************************************
>  rand.R
>  *************************************************************************
>  library(Rmpi)
>  
>  rand <- function ()
>  {
>  if (mpi.comm.size(1) > 1)
>  stop ("It seems some slaves running on comm 1.")
>  mpi.comm.spawn("./rand")
>  mpi.intercomm.merge(2,0,1)
>  
>X  mpi.init.sprng()
>X  free.sprng() #does this function exist here?

I'm not familiar with Rmpi code.  But here you don't need call any SPRNG
function.  Instead, generate a seed (an integer) can pass it on to the
slaves. 
  
>  rdata <- double(sum(mpi.comm.size(1)))
>  out <- mpi.gather(0, 2, rdata) ##this isn't right
>  mpi.comm.free()
>  out
>  }
>  
>  *************************************************************************
>  rand.c
>  *************************************************************************
>  #include <mpi.h>
>  #include <sprng.h>
>  
>  int main(int argc, char **argv)
>  {
>  double rand;
>  double* randarray;
>  
>  MPI_Comm slavecomm, all_processes;
>  
>  /*Initialize MPI*/
>  MPI_Init(&argc, &argv);
>  
>  MPI_Comm_get_parent(&slavecomm);
>  MPI_Intercomm_merge(slavecomm, 1, &all_processes);
>  
>  /*How many processes are there?*/
>  MPI_Comm_size(all_processes, &size);
>  
>  /*Which one am I?*/
>  MPI_Comm_rank(all_processes, &rank);
>  
>X  init_sprng()
>X  rand = sprng();
>X  free.sprng()

Receive the seed from the master and call

   int * stream_id;

   stream_id = init_sprng (gtype, rank, size, seed, param);

where gtype and param can be predefined or got from the master as well.

Now you can generate random numbers by:

   rand = sprng (stream_id);
   free_sprng (stream_id);

>  randarray = (double *)malloc(sizeof(double)*size);
>  
>  /*Gather random numbers from all C slave processes*/
>  /* Using randarray doesn't make sense since this should correspond
>  to the rdata vector in the R master process */
>  MPI_GAther(&rand, 1, 1, MPI_DOUBLE, randarray, 1, MPI_DOUBLE, 0,
>  all_processes);
>  
>  /*All done*/
>  MPI_Comm_free(&all_processes);
>  MPI_Finalize();
>  exit(0);
>  }

There is nothing mysterious about the MPI code in SPRNG, don't use it.
Generating and passing seeds around yourself.

Michael

-- 
Na (Michael) Li, Ph.D.
Assistant Professor
Division of Biostatistics, University of Minnesota
A443 Mayo Bldg, MMC 303         Phone: (612) 626-4765
420 Delaware St SE              Fax:   (612) 626-0660



From ggrothendieck at myway.com  Tue Dec  2 06:32:30 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue,  2 Dec 2003 00:32:30 -0500 (EST)
Subject: [R] Vector Assignments
Message-ID: <20031202053230.6F3153957@mprdmxin.myway.com>



And one other thing.  Are you sure you want character variables
as the result of all this?  A column whose entries are each one
of four colors seems like a good job for a factor:

colours <- c("red", "blue", "green","black")
cut(x, c(-Inf,250,500,700,Inf),right=F,lab=colours)



---
Date: Mon, 1 Dec 2003 23:47:39 -0500 (EST) 
From: Gabor Grothendieck <ggrothendieck at myway.com>
To: <h.wickham at auckland.ac.nz>, <apv at capital.net> 
Cc: <r-help at stat.math.ethz.ch> 
Subject: Re: [R] Vector Assignments 

 
 


Just some small refinements/corrections:

colours <- c("red", "blue", "green","back")
colours[cut(x, c(-Inf,250,500,700,Inf),right=F,lab=F)]

---
Date: Tue, 02 Dec 2003 14:38:55 +1300 
From: Hadley Wickham <h.wickham at auckland.ac.nz>
To: Arend P. van der Veen <apv at capital.net> 
Cc: R HELP <r-help at stat.math.ethz.ch> 
Subject: Re: [R] Vector Assignments 



One way would be to create a vector of colours and then cut() to index 
the vector:

colours <- c("red", "blue", "green","back")
colours[cut(x, c(min(x),250,500,700,max(x)),lab=F)]

Hadley


Arend P. van der Veen wrote:

>Hi,
>
>I have simple R question. 
>
>I have a vector x that contains real numbers. I would like to create
>another vector col that is the same length of x such that:
>
>if x[i] < 250 then col[i] = "red"
>else if x[i] < 500 then col[i] = "blue"
>else if x[i] < 750 then col[i] = "green"
>else col[i] = "black" for all i
>
>I am convinced that there is probably a very efficient way to do this in
>R but I am not able to figure it out. Any help would be greatly
>appreciated.
>
>Thanks in advance,
>Arend van der Veen
>



From jasont at indigoindustrial.co.nz  Tue Dec  2 06:35:42 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Tue, 02 Dec 2003 18:35:42 +1300
Subject: [R] check WARNING...
In-Reply-To: <003601c3b88a$d5ad2e00$0a00a8c0@rodan>
References: <003601c3b88a$d5ad2e00$0a00a8c0@rodan>
Message-ID: <3FCC24AE.70905@indigoindustrial.co.nz>

Jeff D. Hamann wrote:

> I've been developing a package and have been getting the following warning
> when running the check command:
> 
> * checking S3 generic/method consistency ... WARNING
> plot:
>   function(x, ...)
> plot.summaries:
>   function(trees, sp)
> 

I'm unclear; is "summaries" a class?  If not, try naming the function 
plotSummaries, or some such thing (no dot ".")

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From H.Andersson at nioo.knaw.nl  Tue Dec  2 09:29:01 2003
From: H.Andersson at nioo.knaw.nl (Andersson, Henrik)
Date: Tue, 2 Dec 2003 09:29:01 +0100 
Subject: [R] Two axes with different scales
Message-ID: <E0EAEA12C52AD6119C6800306E073297043107@yerseke.cemo.nioo.knaw.nl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031202/c8709f8d/attachment.pl

From jeff.hamann at forestinformatics.com  Tue Dec  2 09:40:25 2003
From: jeff.hamann at forestinformatics.com (Jeff D. Hamann)
Date: Tue, 2 Dec 2003 00:40:25 -0800
Subject: [R] names of parameters from nonlinear model?
Message-ID: <001801c3b8af$f79fabd0$0a00a8c0@rodan>

I've been trying to figure out how to build a list of terms from a nonlinear
model (terms() returns a error). I need to compute and evaluate the partial
derivatives (Jacobian) for each equaiton in  a set of equations.

For example:

> eqn <- q ~ s0 + s1 * p + s2 * f + s3 * a
> sv2 <- c(d0=3,d1=4.234,d2=4,s0=-2.123,s1=0.234,s2=2.123,s3=4.234)
> names( sv2 )
[1] "d0" "d2" "d1" "s0" "s2" "s3" "s1"
...after some processing....
> sv2
        d0         d2         d1         s0         s2         s3         s1
99.8954229  0.3346356 -0.3162988 58.2754311  0.2481333  0.2483023  0.1603666

# error becuase of the gradient
> nlols.derivs <- eval( deriv( eqn, names( sv2 ), hessian=T ) )
> J <- attr( nlols.derivs, "gradient" )

in this case J is

> J
      d0 d2 d1 s0    s2 s3      s1
 [1,]  0  0  0  1  98.0  1 100.323
 [2,]  0  0  0  1  99.1  2 104.264
...blah, blah, blah...
[6,]  0  0  0  1 108.2  6  99.456
 19,]  0  0  0  1  89.3 19 105.769
[20,]  0  0  0  1  93.0 20 113.490
> se <- sqrt( nlols$eq[[2]]$mse * diag( solve( crossprod( J ) ) ) )
Error in solve.default(crossprod(J)) : Lapack routine dgesv: system is
exactly singular
> print( se )

## gives the correct results becuase the J matrix is full rank
nlols.derivs <- eval( deriv( nlols$eq[[2]]$formula, c( "s0", "s1", "s2",
"s3" ), hessian=T ) )
J <- attr( nlols.derivs, "gradient" )
se <- sqrt( nlols$eq[[2]]$mse * diag( solve( crossprod( J ) ) ) )
print( se )


So I can do the following:

1) parse the matrix, column by column, and perform some operation to test to
see if the column is all zeros and tally up a character vector with the
names of the columns to obtain the terms in equation i

        eqn.terms <- vector()
        for( v in 1:length( est$estimate ) ) {
          j <- attr( eval( deriv( as.formula( eqns[[i]] ), names(
startvals ) ) ), "gradient" )
          if( qr( j[,v] )$rank > 0 ) {
            eqn.terms <- rbind( eqn.terms,
                               name <- names( est$estimate )[v] )
          }
        }

        derivs[[i]] <- deriv( as.formula( eqns[[i]] ), eqn.terms,
hessian=T )
        jacobian <- attr( eval( derivs[[i]] ), "gradient" )
        se <- sqrt( mse[i] * diag( solve( crossprod( jacobian ) ) ) )


2) decomopse the matrix (svd(crossprod(J)) or qr(crossprod(J)) ?
3) ask for help and see if there's a magic R function I can't find in the
code/faq/help docs/etc...

Sorry for the lame question but I'm not seeing an "elegant" solution...

Jeff.


---
Jeff D. Hamann
Forest Informatics, Inc.
PO Box 1421
Corvallis, Oregon USA 97339-1421
(office) 541-754-1428
(cell) 541-740-5988
jeff.hamann at forestinformatics.com
www.forestinformatics.com



From mineoeli at unipa.it  Tue Dec  2 09:52:18 2003
From: mineoeli at unipa.it (Elio Mineo)
Date: Tue, 02 Dec 2003 09:52:18 +0100
Subject: [R] Kurtosis function
In-Reply-To: <67d2bf67da4a.67da4a67d2bf@jhmimail.jhmi.edu>
References: <67d2bf67da4a.67da4a67d2bf@jhmimail.jhmi.edu>
Message-ID: <3FCC52C2.2060800@unipa.it>

The normalp package contains a kurtosis function.
All the best,
Elio Mineo

XIAO LIU wrote:

>Hi,
>
>Which R package contains Kurtosis function?  Where can I get it?
>
>Thank you very much.
>
>Xiao
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>  
>

-- 
--------------------------------------------------------------------------
Elio Mineo
Dipartimento di Scienze Statistiche e Matematiche "Silvio Vianelli"
Universit? degli Studi di Palermo
Viale delle Scienze
90128 Palermo
URL: http://dssm.unipa.it/elio



From ripley at stats.ox.ac.uk  Tue Dec  2 09:55:45 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 2 Dec 2003 08:55:45 +0000 (GMT)
Subject: [R] Two axes with different scales
In-Reply-To: <E0EAEA12C52AD6119C6800306E073297043107@yerseke.cemo.nioo.knaw.nl>
Message-ID: <Pine.LNX.4.44.0312020845310.12965-100000@gannet.stats>

On Tue, 2 Dec 2003, Andersson, Henrik wrote:

> Hello I would like to make a graph with two variables, with different units,
> so I would like to have two y-axes, one on the left and one on the right
> with different scales. Is this possible?

Yes.  There is an example on weight loss in the MASS/scripts/ch08.R script
which should be part of your R installation.

> I have searched through all the documentation I could find on the R homepage
> without luck and I like R very much and would rather not resort to using
> something like SigmaPlot for this seemingly simple thing.

The most comprehensive documentation on R is in real books, such as MASS 
(see the R FAQ).

> E.g. plotting the concentration of  Oxygen and it's isotopic composition in
> the same graph.

Ah, so the `different units' are actually different quantities, not just 
units?  Then the same principles apply, but you will need to change the 
plotting coordinates for the plot region before adding the second variable 
and the axis on side 4.  That is covered in `An Introduction to R' (you 
can either reset par("usr") or call plot.window()).

I suspect you actually have *three* variables, and want to plot y vs x and
z vs x, and that's the scenario I have addressed here.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Tue Dec  2 09:56:29 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 2 Dec 2003 09:56:29 +0100
Subject: [R] search site for R (http://finzi.psych.upenn.edu)
In-Reply-To: <20031201235833.GA11587@mail2.sas.upenn.edu>
References: <20031201235833.GA11587@mail2.sas.upenn.edu>
Message-ID: <16332.21437.959215.423643@gargle.gargle.HOWL>

Dear Professor Baron,
Thank you very much for this service!

>>>>> "Jon" == Jonathan Baron <baron at psych.upenn.edu>
>>>>>     on Mon, 1 Dec 2003 18:58:33 -0500 writes:

    Jon> My search site, http://finzi.psych.upenn.edu, has had
    Jon> several problems recently, all my fault, for which I
    Jon> apologize.  But it now seems to be running reliably, on
    Jon> a new computer that is much faster than the old one.

    Jon> It uses htdig to permit search of the Rhelp mailing
    Jon> list, R documents, R functions, and various
    Jon> combinations of these.  Search has several options,
    Jon> including Boolean search (with AND, etc.).

    Jon> Suggestions are welcome.  -- Jonathan Baron, Professor
    Jon> of Psychology, University of Pennsylvania

I've been asked more than once:
While it's cumbersome to provide a link to this from the
"pipermail" archives, I have now added a link to the finzi page
on the main R-help mailing list page --- the one at the end of
every R-help posting.

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From allan at stats.uct.ac.za  Tue Dec  2 09:56:54 2003
From: allan at stats.uct.ac.za (allan clark)
Date: Tue, 02 Dec 2003 10:56:54 +0200
Subject: [R]: GLIM PROBLEMS
Message-ID: <3FCC53D6.F18DD0FF@stats.uct.ac.za>


   Hi all

   I have another GLIM question.

   I have been using R as well as Genstat (version 6) in order to fit
   GLIM models to the data (displayed below).

   The same models are fitted but the answers supplied by the two
   packages are not the same.

   Why? Can anyone help?

   A discription of the data and the type of model/s fitted can be found
   below.

   Regards
   Allan


   The problem is taken from Bennet (1978)  (I dont have any more of the
   reference.)

   In this example we wish to model the probability of a car insurance
   policyholder claiming insurance on his/her car given that we know
   certain information about him/her. The explanatory variables used in
   this analysis are: the age of the policyholder (age), the year of
   registration (reg) and a measure of the policyholders claim history
   called the no claim discount (ncd).

   Define p(i,j,k)  as the probability of a policyholder in level i of
   age, level j of reg and level k of ncd making a claim (for i= 1 (age=
   17-22), 2 (23-26), 3 (27-65), 4 (66-80) ; j=1 (registration after
   1964), 2 (63-64), 3 (60-62), 4 (earlier than 1960) and ncd= 1 (1-1
   claims), 2 (2-3), 3 (more than three).).

   Similarly define r(i,j,k)  as the proportion of policyholders in level
   i of age, level j of reg and level k of ncd making a claim.

   We can thus model r(i,j,k) by means of a binomial distribution with
   parameters p(i,j,k) and N(i,j,k)  where N(i,j,k)  is the total number
   of policyholders that falls into group i, j, k such that
   log(p(i,j,k)/(1-p(i,j,k))) = some function of age, reg and ncd .

   The baseline chosen is age (17-22), reg (65) and ncd (0-1).


         age     reg   ncd claims.r exp.n
   1  (17-22)   (65-) (0-1)      475  1800
   2  (17-22)   (65-) (2-3)      150   700
   3  (17-22)   (65-)  (4+)       35   200
   4  (17-22) (63-64) (0-1)      680  2650
   5  (17-22) (63-64) (2-3)      215  1000
   6  (17-22) (63-64)  (4+)       55   250
   7  (17-22) (60-62) (0-1)      710  2950
   8  (17-22) (60-62) (2-3)      220  1100
   9  (17-22) (60-62)  (4+)       60   250
   10 (17-22)   (-59) (0-1)      230  1050
   11 (17-22)   (-59) (2-3)       75   450
   12 (17-22)   (-59)  (4+)       25   250
   13 (23-26)   (65-) (0-1)      240  1300
   14 (23-26)   (65-) (2-3)      140   900
   15 (23-26)   (65-)  (4+)      150   900
   16 (23-26) (63-64) (0-1)      310  1500
   17 (23-26) (63-64) (2-3)      185  1050
   18 (23-26) (63-64)  (4+)      170  1050
   19 (23-26) (60-62) (0-1)      240  1405
   20 (23-26) (60-62) (2-3)      160  1000
   21 (23-26) (60-62)  (4+)      130  1050
   22 (23-26)   (-59) (0-1)       80   600
   23 (23-26)   (-59) (2-3)       60   500
   24 (23-26)   (-59)  (4+)       70   550
   25 (27-65)   (65-) (0-1)     1650 10300
   26 (27-65)   (65-) (2-3)     1450  9900
   27 (27-65)   (65-)  (4+)     3400 28900
   28 (27-65) (63-64) (0-1)     1550  9900
   29 (27-65) (63-64) (2-3)     1450 10000
   30 (27-65) (63-64)  (4+)     3200 27700
   31 (27-65) (60-62) (0-1)     1250  9300
   32 (27-65) (60-62) (2-3)     1250  9200
   33 (27-65) (60-62)  (4+)     2500 25600
   34 (27-65)   (-59) (0-1)      500  4700
   35 (27-65)   (-59) (2-3)      550  5300
   36 (27-65)   (-59)  (4+)     1400 18100
   37 (66-80)   (65-) (0-1)       55   275
   38 (66-80)   (65-) (2-3)       40   250
   39 (66-80)   (65-)  (4+)      180  1400
   40 (66-80) (63-64) (0-1)       35   225
   41 (66-80) (63-64) (2-3)       30   225
   42 (66-80) (63-64)  (4+)      155  1450
   43 (66-80) (60-62) (0-1)       25   200
   44 (66-80) (60-62) (2-3)       40   300
   45 (66-80) (60-62)  (4+)      130  1500
   46 (66-80)   (-59) (0-1)       25   175
   47 (66-80)   (-59) (2-3)       30   300
   48 (66-80)   (-59)  (4+)      180  2400

   claims.r =the number of claims made in a particular group.
   exp.n=the total number of policyholders in a particular group.

   EXAMPLE

   As an example if we use age as an explanatory variable and fit the glm
   model we get the following results:

   cars<-read.table("c:/a.dat",header=T)
   attach(cars)
   y<-cbind(claims.r,exp.n)
   cars.age<-glm(y~age,family=binomial)
   summary(cars.age)

   Call:
   glm(formula = y ~ age, family = binomial)

   Deviance Residuals:
         Min         1Q     Median         3Q        Max
   -16.62952   -1.84073   -0.04282    2.07028   10.72502

   Coefficients:
               Estimate Std. Error z value Pr(>|z|)
   (Intercept) -1.46265    0.02050  -71.34   <2e-16 ***
   age(23-26)  -0.34576    0.03197  -10.82   <2e-16 ***
   age(27-65)  -0.66345    0.02182  -30.41   <2e-16 ***
   age(66-80)  -0.77863    0.04020  -19.37   <2e-16 ***
   ---
   Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1

   (Dispersion parameter for binomial family taken to be 1)

       Null deviance: 1837.4  on 47  degrees of freedom
   Residual deviance:  883.8  on 44  degrees of freedom
   AIC: 1228.4

   Number of Fisher Scoring iterations: 4

   The Genstat output is displayed below.  (well only some of it!!!)


   ***** Regression Analysis *****

   Response variate: claims_r
   Binomial totals: exp_n
   Distribution: Binomial
   Link function: Logit
   Fitted terms: Constant, age


   *** Summary of analysis ***

                                           mean  deviance approx
                 d.f.     deviance     deviance     ratio chi pr
   Regression       3        1298.       432.70    432.70  <.001
   Residual        44        1136.        25.83
   Total           47        2434.        51.80
   * MESSAGE: ratios are based on dispersion parameter with value 1

   Dispersion parameter is fixed at 1.00


   *** Estimates of parameters ***
                                                            antilog of
                     estimate         s.e.      t(*)  t pr.   estimate
   Constant           -1.1992       0.0211    -56.90  <.001     0.3014
   age (23-26)        -0.4302       0.0326    -13.20  <.001     0.6504
   age (27-65)        -0.7999       0.0224    -35.75  <.001     0.4494
   age (66-80)        -0.9297       0.0407    -22.86  <.001     0.3947
   * MESSAGE: s.e.s are based on dispersion parameter with value 1

   Parameters for factors are differences compared with the reference
   level:
                 Factor  Reference level
                    age  (17-22)


From p.dalgaard at biostat.ku.dk  Tue Dec  2 10:22:14 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 02 Dec 2003 10:22:14 +0100
Subject: [R]: GLIM PROBLEMS
In-Reply-To: <3FCC53D6.F18DD0FF@stats.uct.ac.za>
References: <3FCC53D6.F18DD0FF@stats.uct.ac.za>
Message-ID: <x2oeurshfd.fsf@biostat.ku.dk>

allan clark <allan at stats.uct.ac.za> writes:

>    Hi all
> 
>    I have another GLIM question.
> 
>    I have been using R as well as Genstat (version 6) in order to fit
>    GLIM models to the data (displayed below).
> 
>    The same models are fitted but the answers supplied by the two
>    packages are not the same.
> 
>    Why? Can anyone help?

It's not the same model!
....

>    cars<-read.table("c:/a.dat",header=T)
>    attach(cars)
>    y<-cbind(claims.r,exp.n)
        *********************

from help(glm)

     For 'binomial' models the response can also be specified as a
     'factor' (when the first level denotes failure and all others
     success) or as a two-column matrix with the columns giving the
     numbers of successes and failures.      


>    cars.age<-glm(y~age,family=binomial)
>    summary(cars.age)

....

>    ***** Regression Analysis *****
> 
>    Response variate: claims_r
>    Binomial totals: exp_n
              ******

>    Distribution: Binomial
>    Link function: Logit
>    Fitted terms: Constant, age


I.e. try  y <- cbind(claims.r, exp.n - claims.r)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From lecoutre at stat.ucl.ac.be  Tue Dec  2 10:34:11 2003
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Tue, 02 Dec 2003 10:34:11 +0100
Subject: [R] search site for R (http://finzi.psych.upenn.edu)
In-Reply-To: <20031201235833.GA11587@mail2.sas.upenn.edu>
References: <20031201235833.GA11587@mail2.sas.upenn.edu>
Message-ID: <6.0.1.1.2.20031202102522.0207bec0@stat4ux.stat.ucl.ac.be>


Hi,

I didn't know your search engine. Thank you very much for providing this 
tool. As far as I am concerned, I think that something like that was 
missing: R is growing so fast that it becomes difficult for the poor human 
to know all available facilities. I know many people using SPlus that wont 
change to R because they would have difficulties to find equivalent 
functions. They dont know the poweR!
It would be very great to have a portal that allows a quick access to all 
the available documentation/stuff about R.
For the students here, I also prepared a support page allowing to search 
across R functions (R Help Center: 
http://www.stat.ucl.ac.be/ISdidactique/Rhelp/).
I created a FileMaker database containing more than 11000 functions (name, 
description, library and code). Doing research on the code and having the 
possibility to browse it is very usefull. Nervertheless, I dont think I 
will have time to update it on a regular basis.
Thus, my suggestion is to add function's code to your search engine. If I 
understand well what you use, for that, you will have to generate a flat 
file for each function. If you are interestd, I have pieces of code that 
could do the stuff.

Eric

At 00:58 2/12/2003, Jonathan Baron wrote:
>My search site, http://finzi.psych.upenn.edu, has had several
>problems recently, all my fault, for which I apologize.  But it
>now seems to be running reliably, on a new computer that is much
>faster than the old one.
>It uses htdig to permit search of the Rhelp mailing list, R
>documents, R functions, and various combinations of these.
>Search has several options, including Boolean search (with AND,
>etc.).
>Suggestions are welcome.
>--
>Jonathan Baron, Professor of Psychology, University of Pennsylvania
>Home page:            http://www.sas.upenn.edu/~baron



--------------------------------------------------
L'erreur est certes humaine, mais un vrai d?sastre
n?cessite un ou deux ordinateurs. Citation anonyme
--------------------------------------------------
Eric Lecoutre
Informaticien/Statisticien
Institut de Statistique / UCL

TEL (+32)(0)10473050       lecoutre at stat.ucl.ac.be
URL http://www.stat.ucl.ac.be/ISpersonnel/lecoutre



From B.Rowlingson at lancaster.ac.uk  Tue Dec  2 11:09:00 2003
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 02 Dec 2003 10:09:00 +0000
Subject: [R] Re: using shapefile as owin
In-Reply-To: <Pine.LNX.4.44.0312012205301.1327-100000@reclus.nhh.no>
References: <Pine.LNX.4.44.0312012205301.1327-100000@reclus.nhh.no>
Message-ID: <3FCC64BC.80008@lancaster.ac.uk>


> So your object does not conform at all to the structure owin() seems to 
> need, although there are ways of doing this - contributions welcome 
> (owin.polylist() for example).


Here's how to convert a list of 2-column matrices to a list of lists of 
x and y element vectors and feed it to owin:

owin.polylist = function(list2vecs){
  owin(poly=lapply(list2vecs,function(xy){list(x=xy[,1],y=xy[,2])}))
}

  But there will still be problems if the polygon directions aren't what 
spatstat expects.

Baz



From luke.keele at politics-and-international-relations.oxford.ac.uk  Tue Dec  2 11:33:52 2003
From: luke.keele at politics-and-international-relations.oxford.ac.uk (Luke Keele)
Date: Tue, 2 Dec 2003 10:33:52 -0000
Subject: [R] RWinEdt
Message-ID: <000201c3b8bf$c7f6cb10$903701a3@politics.ox.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031202/5ef2dae9/attachment.pl

From karlknoblich at yahoo.de  Tue Dec  2 11:34:54 2003
From: karlknoblich at yahoo.de (=?iso-8859-1?q?Karl=20Knoblick?=)
Date: Tue, 2 Dec 2003 11:34:54 +0100 (CET)
Subject: [R] lme: reproducing example
Message-ID: <20031202103454.44203.qmail@web10001.mail.yahoo.com>

Dear R-community!

I still have the problem reproducing the following
example using lme.

id<-factor(rep(rep(1:5,rep(3,5)),3))
factA <- factor(rep(c("a1","a2","a3"),rep(15,3)))
factB <- factor(rep(c("B1","B2","B3"),15))
Y<-numeric(length=45)
Y[ 1: 9]<-c(56,52,48,57,54,46,55,51,51)
Y[10:18]<-c(58,51,50,54,53,46,54,50,49)
Y[19:27]<-c(53,49,48,56,48,52,52,52,50)
Y[28:36]<-c(55,51,46,57,49,50,55,51,47)
Y[37:45]<-c(56,48,51,58,50,48,58,46,52)
df<-data.frame(id, factA, factB, Y) 
df.aov <- aov(Y ~ factA*factB + Error(factA:id),
data=df)
summary(df.aov)

Is there a way to get the same results with lme as 
with aov with Error()? HOW???

One idea was the following:
df$factAid=factor(paste(as.character(df$factA),":",as.character(df$id),sep=""))
df.lme <-
lme(Y~factA*factB,df,random=~1|factAid,method="REML") 
The degrees of freedom look right, but the F values
don't match aov.

Hope somebody can help! Thanks!!

Karl



From gavin.simpson at ucl.ac.uk  Tue Dec  2 11:39:57 2003
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Tue, 02 Dec 2003 10:39:57 +0000
Subject: [R] with for objects
In-Reply-To: <3FCBC6BB.6010706@auckland.ac.nz>
References: <3FC9B643.6050700@auckland.ac.nz> <3FCB6555.2010808@jhsph.edu>
	<3FCBA5A4.6040906@auckland.ac.nz>
	<Pine.A41.4.58.0312011354110.30536@homer18.u.washington.edu>
	<3FCBC6BB.6010706@auckland.ac.nz>
Message-ID: <3FCC6BFD.7010409@ucl.ac.uk>

Hadley Wickham wrote:
> Yes, that is a (big) problem.  What do you suggest as an alternative?
> I am experimenting visualising microarray data using trellis.  I end up 
> with a whole lot of calls like xyplot(maA(swirl[,1]) ~ maM(swirl[,1]) | 
> maPrintTip(swirl[,1])) and then if I want to plot the same thing for 
> array 2, I need xyplot(maA(swirl[,2]) ~ maM(swirl[,2]) | 
> maPrintTip(swirl[,2]))...  I don't want to limit myself to a fixed set 
> of plots because I'm still exploring different ways to look at the data, 
> but I would like to save myself some typing.
> 

Could you not build the functions in a loop, using combinations of 
paste, eval and do.call ? I tried this method recently to fit a few 
thousand response curves to my species data using glm() [yes, most were 
completely uninteresting].

There is an excellent example of building function calls in RNews Vol2 
Issue 2 by Bill Venables (Programmer's Niche pages 24--26):

http://cran.r-project.org/doc/Rnews/Rnews_2002-2.pdf

This has the advantage of using the variable names in your plots/calls 
rather than the swirl[,1] format you have used.

Alternatively, grab the number of columns in swirl and then loop over 
your call replacing swirl[,1] with swirl[,i] e.g. [not tried]:

for (i in 1:length(swirl))
{
   xyplot(maA(swirl[,i]) ~ maM(swirl[,i]) | maPrintTip(swirl[,i]))
}

Wrap the whole thing in postscript() or pdf() so all the plots end up in 
a file you can look at later. Not sure whether you need to wrap the 
xyplot() in print() to get lattice to produce the plot in a loop (you do 
in a function for example) as I haven't used lattice much yet, but I'm 
sure someone on the list will correct/advise if needed.

HTH

Gav
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpson at ucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From paradis at isem.univ-montp2.fr  Tue Dec  2 11:45:15 2003
From: paradis at isem.univ-montp2.fr (Emmanuel Paradis)
Date: Tue, 02 Dec 2003 11:45:15 +0100
Subject: [R] check WARNING...
In-Reply-To: <3FCC24AE.70905@indigoindustrial.co.nz>
References: <003601c3b88a$d5ad2e00$0a00a8c0@rodan>
	<003601c3b88a$d5ad2e00$0a00a8c0@rodan>
Message-ID: <4.2.0.58.20031202113531.00365c78@isem.isem.univ-montp2.fr>

A 18:35 02/12/2003 +1300, Jason Turner a ?crit:
>Jeff D. Hamann wrote:
>
>>I've been developing a package and have been getting the following warning
>>when running the check command:
>>* checking S3 generic/method consistency ... WARNING
>>plot:
>>   function(x, ...)
>>plot.summaries:
>>   function(trees, sp)
>
>I'm unclear; is "summaries" a class?  If not, try naming the function 
>plotSummaries, or some such thing (no dot ".")
>
>Cheers

And if yes, you can rename the main argument inside the function:

plot.summaries <- function(x, sp, ...)
{
trees <- x
rm(x)
[...]
}

This allows you to keep the object name 'trees' inside the function.

Note that the "dot dot dot" (...) argument MUST be included in this case 
(use of the generic function plot). Have a look at the Writing R Extensions 
manual: there are very useful information there when writing a package.

Emmanuel Paradis



>Jason
>--
>Indigo Industrial Controls Ltd.
>http://www.indigoindustrial.co.nz
>64-21-343-545
>jasont at indigoindustrial.co.nz
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From gavin.simpson at ucl.ac.uk  Tue Dec  2 11:49:29 2003
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Tue, 02 Dec 2003 10:49:29 +0000
Subject: [R] RWinEdt
In-Reply-To: <000201c3b8bf$c7f6cb10$903701a3@politics.ox.ac.uk>
References: <000201c3b8bf$c7f6cb10$903701a3@politics.ox.ac.uk>
Message-ID: <3FCC6E39.6030605@ucl.ac.uk>

Hi Luke,

Did you install the RWinEdt library as well? You don't say so, and the 
error message would suggest you didn't (or that the installation did not 
work)

> Error in library(WinEdt) : there is no package called 'RWinEdt'
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

HTH

Gav

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpson at ucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From pascal.bernard at sc-eco.univ-nantes.fr  Tue Dec  2 12:11:15 2003
From: pascal.bernard at sc-eco.univ-nantes.fr (bernard)
Date: 02 Dec 2003 12:11:15 +0100
Subject: [R] model of fish over exploitation
Message-ID: <1070363474.2940.51.camel@localhost>

Dear all,

I have a serious problem to solve my model. I study over exploitation of
fish in the bay of biscay (france). I know only the level of catch and
the fishing effort (see data below) by year.

My model is composed by the following equations:

* the growth function
Gt(St) = r*St*(1-St/sbar)
with Gt the growth of each period t
r intrinsec growth of the stock
sbar carriyng capacity of the stock
St the fish stock of each period t

* the production function
Ht(St, Xt) = alpha*St*Xt
Ht the catch for each period t
Xt fishing effort for each period t
alpha parameter of boat productivity

* the dynamic of the fish stock
S(t+1) = S(t) + Gt - Ht


I would like to modelise the following system:

S_(t+1) = S_t + G_t - H_t
G_t = r*S_t*(1-S_t / sbar)
H_t = alpha * S_t * X_t
S_1961 = S_0

I know only H_t on period (1961 - 1994) and X_t on the same period.
I don't know r, sbar, alpha and S_0 (the initial level of the stock)
(and of course S_t on this period) and I want to estimate this four
parameters.

I have written something like that:

*******************************************
library(nls)
data <- read.table("fish.dat", header=TRUE)
attach(data)
names(data)

fct <- function(x, param)
{
  r     <- param[1]
  sbar  <- param[2]
  alpha <- param[3]
  s0    <- param[4]

for (i in 1:length(year))
  g[i] <- r*S[i]*(1-s[i] / sbar)
  h[i] <- alpha * s[i] * x[i]
  s[i+1] <- s[i] + g[i] - h[i]
}

mod <- nls (catch ~ fct(effort, catch, c(r, sbar, alpha, s0)), start =
c(r = 1.0, sbar = 100.0, alpha = 0.5, S0 = 70.0))

******************************************* 

... but, of course, it doesn't work !!

Could you help me please and could you answerd in my personnal email ? I
have put at the end data of my problem.

best regards to all of you

Pascal BERNARD

----------------------------------------
Researcher
LEN-CORRAIL
University of Nantes
Faculty of Economy
Chemin de la Censive du Tertre
44322 Nantes cedex 03
France
pascal.bernard at sc-eco.univ-nantes.fr
----------------------------------------

fish.dat:
************************
year catch effort
1961 41.400 45.03512 
1962 51.800 50.05673 
1963 44.300 44.30000 
1964 48.000 44.54000 
1965 44.826 59.97878 
1966 39.208 45.37687 
1967 48.278 46.60833 
1968 37.819 52.24526 
1969 31.992 54.11967 
1970 29.894 35.60816 
1971 39.406 61.24754 
1972 34.279 54.76161 
1973 27.958 46.56643 
1974 36.407 28.51477 
1975 27.827 27.16532 
1976 33.710 38.83327 
1977 32.888 22.07106 
1978 35.804 31.36205 
1979 38.950 25.68735 
1980 29.157 19.38004 
1981 23.748 21.78884 
1982 28.333 20.10470 
1983 31.945 27.18085 
1984 18.434 17.92367 
1985 22.531 18.97028 
1986 25.587 22.37778 
1987 29.777 16.89844 
1988 27.906 20.19613 
1989 25.757 16.42839 
1990 24.503 15.57284 
1991 16.608 17.14402 
1992 18.162 15.78574 
1993 18.371 12.12064 
1994 16.993 10.31185



From ripley at stats.ox.ac.uk  Tue Dec  2 12:16:06 2003
From: ripley at stats.ox.ac.uk (Prof Brian D Ripley)
Date: Tue, 2 Dec 2003 11:16:06 +0000 (GMT Standard Time)
Subject: [R]: GLIM PROBLEMS
In-Reply-To: <3FCC53D6.F18DD0FF@stats.uct.ac.za>
Message-ID: <Pine.WNT.4.44.0312021113280.3820-100000@gannet.stats.ox.ac.uk>

In R you use cbind(successes, failures), not cbind(successes, total) as you
appear to have done.

And GLIM is a program (with I for interactive): these are GLMs, not GLIMs
in th ecommonly accepted terminology (but not that of SAS).

On Tue, 2 Dec 2003, allan clark wrote:

>
>    Hi all
>
>    I have another GLIM question.
>
>    I have been using R as well as Genstat (version 6) in order to fit
>    GLIM models to the data (displayed below).
>
>    The same models are fitted but the answers supplied by the two
>    packages are not the same.
>
>    Why? Can anyone help?
>
>    A discription of the data and the type of model/s fitted can be found
>    below.
>
>    Regards
>    Allan
>
>
>    The problem is taken from Bennet (1978)  (I dont have any more of the
>    reference.)
>
>    In this example we wish to model the probability of a car insurance
>    policyholder claiming insurance on his/her car given that we know
>    certain information about him/her. The explanatory variables used in
>    this analysis are: the age of the policyholder (age), the year of
>    registration (reg) and a measure of the policyholders claim history
>    called the no claim discount (ncd).
>
>    Define p(i,j,k)  as the probability of a policyholder in level i of
>    age, level j of reg and level k of ncd making a claim (for i= 1 (age=
>    17-22), 2 (23-26), 3 (27-65), 4 (66-80) ; j=1 (registration after
>    1964), 2 (63-64), 3 (60-62), 4 (earlier than 1960) and ncd= 1 (1-1
>    claims), 2 (2-3), 3 (more than three).).
>
>    Similarly define r(i,j,k)  as the proportion of policyholders in level
>    i of age, level j of reg and level k of ncd making a claim.
>
>    We can thus model r(i,j,k) by means of a binomial distribution with
>    parameters p(i,j,k) and N(i,j,k)  where N(i,j,k)  is the total number
>    of policyholders that falls into group i, j, k such that
>    log(p(i,j,k)/(1-p(i,j,k))) = some function of age, reg and ncd .
>
>    The baseline chosen is age (17-22), reg (65) and ncd (0-1).
>
>
>          age     reg   ncd claims.r exp.n
>    1  (17-22)   (65-) (0-1)      475  1800
>    2  (17-22)   (65-) (2-3)      150   700
>    3  (17-22)   (65-)  (4+)       35   200
>    4  (17-22) (63-64) (0-1)      680  2650
>    5  (17-22) (63-64) (2-3)      215  1000
>    6  (17-22) (63-64)  (4+)       55   250
>    7  (17-22) (60-62) (0-1)      710  2950
>    8  (17-22) (60-62) (2-3)      220  1100
>    9  (17-22) (60-62)  (4+)       60   250
>    10 (17-22)   (-59) (0-1)      230  1050
>    11 (17-22)   (-59) (2-3)       75   450
>    12 (17-22)   (-59)  (4+)       25   250
>    13 (23-26)   (65-) (0-1)      240  1300
>    14 (23-26)   (65-) (2-3)      140   900
>    15 (23-26)   (65-)  (4+)      150   900
>    16 (23-26) (63-64) (0-1)      310  1500
>    17 (23-26) (63-64) (2-3)      185  1050
>    18 (23-26) (63-64)  (4+)      170  1050
>    19 (23-26) (60-62) (0-1)      240  1405
>    20 (23-26) (60-62) (2-3)      160  1000
>    21 (23-26) (60-62)  (4+)      130  1050
>    22 (23-26)   (-59) (0-1)       80   600
>    23 (23-26)   (-59) (2-3)       60   500
>    24 (23-26)   (-59)  (4+)       70   550
>    25 (27-65)   (65-) (0-1)     1650 10300
>    26 (27-65)   (65-) (2-3)     1450  9900
>    27 (27-65)   (65-)  (4+)     3400 28900
>    28 (27-65) (63-64) (0-1)     1550  9900
>    29 (27-65) (63-64) (2-3)     1450 10000
>    30 (27-65) (63-64)  (4+)     3200 27700
>    31 (27-65) (60-62) (0-1)     1250  9300
>    32 (27-65) (60-62) (2-3)     1250  9200
>    33 (27-65) (60-62)  (4+)     2500 25600
>    34 (27-65)   (-59) (0-1)      500  4700
>    35 (27-65)   (-59) (2-3)      550  5300
>    36 (27-65)   (-59)  (4+)     1400 18100
>    37 (66-80)   (65-) (0-1)       55   275
>    38 (66-80)   (65-) (2-3)       40   250
>    39 (66-80)   (65-)  (4+)      180  1400
>    40 (66-80) (63-64) (0-1)       35   225
>    41 (66-80) (63-64) (2-3)       30   225
>    42 (66-80) (63-64)  (4+)      155  1450
>    43 (66-80) (60-62) (0-1)       25   200
>    44 (66-80) (60-62) (2-3)       40   300
>    45 (66-80) (60-62)  (4+)      130  1500
>    46 (66-80)   (-59) (0-1)       25   175
>    47 (66-80)   (-59) (2-3)       30   300
>    48 (66-80)   (-59)  (4+)      180  2400
>
>    claims.r =the number of claims made in a particular group.
>    exp.n=the total number of policyholders in a particular group.
>
>    EXAMPLE
>
>    As an example if we use age as an explanatory variable and fit the glm
>    model we get the following results:
>
>    cars<-read.table("c:/a.dat",header=T)
>    attach(cars)
>    y<-cbind(claims.r,exp.n)
>    cars.age<-glm(y~age,family=binomial)
>    summary(cars.age)
>
>    Call:
>    glm(formula = y ~ age, family = binomial)
>
>    Deviance Residuals:
>          Min         1Q     Median         3Q        Max
>    -16.62952   -1.84073   -0.04282    2.07028   10.72502
>
>    Coefficients:
>                Estimate Std. Error z value Pr(>|z|)
>    (Intercept) -1.46265    0.02050  -71.34   <2e-16 ***
>    age(23-26)  -0.34576    0.03197  -10.82   <2e-16 ***
>    age(27-65)  -0.66345    0.02182  -30.41   <2e-16 ***
>    age(66-80)  -0.77863    0.04020  -19.37   <2e-16 ***
>    ---
>    Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
>
>    (Dispersion parameter for binomial family taken to be 1)
>
>        Null deviance: 1837.4  on 47  degrees of freedom
>    Residual deviance:  883.8  on 44  degrees of freedom
>    AIC: 1228.4
>
>    Number of Fisher Scoring iterations: 4
>
>    The Genstat output is displayed below.  (well only some of it!!!)
>
>
>    ***** Regression Analysis *****
>
>    Response variate: claims_r
>    Binomial totals: exp_n
>    Distribution: Binomial
>    Link function: Logit
>    Fitted terms: Constant, age
>
>
>    *** Summary of analysis ***
>
>                                            mean  deviance approx
>                  d.f.     deviance     deviance     ratio chi pr
>    Regression       3        1298.       432.70    432.70  <.001
>    Residual        44        1136.        25.83
>    Total           47        2434.        51.80
>    * MESSAGE: ratios are based on dispersion parameter with value 1
>
>    Dispersion parameter is fixed at 1.00
>
>
>    *** Estimates of parameters ***
>                                                             antilog of
>                      estimate         s.e.      t(*)  t pr.   estimate
>    Constant           -1.1992       0.0211    -56.90  <.001     0.3014
>    age (23-26)        -0.4302       0.0326    -13.20  <.001     0.6504
>    age (27-65)        -0.7999       0.0224    -35.75  <.001     0.4494
>    age (66-80)        -0.9297       0.0407    -22.86  <.001     0.3947
>    * MESSAGE: s.e.s are based on dispersion parameter with value 1
>
>    Parameters for factors are differences compared with the reference
>    level:
>                  Factor  Reference level
>                     age  (17-22)
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From turk at math.montana.edu  Tue Dec  2 14:02:39 2003
From: turk at math.montana.edu (Philip Turk)
Date: Tue, 2 Dec 2003 06:02:39 -0700 (MST)
Subject: [R] question regarding variance components
Message-ID: <Pine.GSO.4.44.0312020551160.16377-100000@newton1.math.montana.edu>

I am using a two-factor ANOVA model with random factor effects including
the interaction, i.e. the factors are crossed.  I would like to be able to
generate all four variance components along with approximate confidence
intervals using the NLME package.  However, I do not know how to specify
the random option because of two problems.  First, I do not know how to
enter the interaction term into the option.  Second, I do not know how
to enter in the covariance matrix (see Neter et. al., Applied Linear
Statistical Models, 4th ed., page 977).  Is there anyone who has
confronted this problem before that would be willing to help?  Thank you.


Philip Turk



From r-eugenesalinas at comcast.net  Tue Dec  2 14:08:04 2003
From: r-eugenesalinas at comcast.net (Eugene Salinas (R))
Date: Tue, 02 Dec 2003 08:08:04 -0500
Subject: [R] smoothing functions
In-Reply-To: <Pine.SOL.4.58.0312012058420.7186@mspacman.gpcc.itd.umich.edu>
References: <3FCBEC69.5060009@comcast.net>
	<Pine.SOL.4.58.0312012058420.7186@mspacman.gpcc.itd.umich.edu>
Message-ID: <3FCC8EB4.6030003@comcast.net>

Dear Thomas,

The short answer is both. Some of the parameters of interest to be 
estimated are discrete while others are are continuous. Similarly for 
the support of the data variables that go into the objective.

Let me try and make this more concrete without typing all the maths 
down. Consider a set of functions that are well defined (eg exponentials 
etc) f_i (x, a) where x is some given data and a is a parameter of 
interest. Then we look at comparing different combinations of these 
functions (possibly functions of functions) but in the simplest case it 
would be 1| f_i (x,a)> f_ j (x,a)], where 1| returns 1 if the inequality 
is satisfied, 0 otherwise. Now consider building an objective function 
out of lots and lots of these indicator functions. So the issue is how 
to smooth this objective since it has no continuous elements because of 
the indicator structure?

thanks, Eugene.

Thomas W Blackwell wrote:

>Eugene  -
>
>Is the estimand in your problem (the parameter which you seek
>to estimate) discrete-valued or continuous-valued ?  If it is
>discrete-valued, then you are heading in the wrong direction,
>because no matter how smooth you make the objective function,
>you will not be able to differentiate it with respect to the
>parameter !   I think I don't have quite enough information
>to give a helpful answer to your question  . . .  but more
>important is for you to find the answer yourself.
>
>-  tom blackwell  -  u michigan medical school  -  ann arbor  -
>
>On Mon, 1 Dec 2003, Eugene Salinas (R) wrote:
>
>  
>
>>Dear all,
>>
>>I am trying to program an estimator which maximizes a likelihood type
>>objective function which is basically just lots of sums of indicator
>>functions of data and parameters. In order to make the optimization I
>>would like to smooth these functions. Since they are either 0 or 1, one
>>possibility is to use the normal cdf.
>>
>>I am wondering whether anyone is aware of a less arbitrary choice of a
>>smoothing function? (is there any theory that suggests what's best to
>>use?) Does anyone have any recommendations on what works best numerically?
>>
>>Thanks, Eugene.
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>
>>    
>>
>
>  
>



From Pascal.Niklaus at unibas.ch  Tue Dec  2 14:58:22 2003
From: Pascal.Niklaus at unibas.ch (Pascal A. Niklaus)
Date: Tue, 02 Dec 2003 14:58:22 +0100
Subject: [R] lme: reproducing example
In-Reply-To: <20031202103454.44203.qmail@web10001.mail.yahoo.com>
References: <20031202103454.44203.qmail@web10001.mail.yahoo.com>
Message-ID: <3FCC9A7E.70309@unibas.ch>

Karl Knoblick wrote:

>Dear R-community!
>
>I still have the problem reproducing the following
>example using lme.
>
>id<-factor(rep(rep(1:5,rep(3,5)),3))
>factA <- factor(rep(c("a1","a2","a3"),rep(15,3)))
>factB <- factor(rep(c("B1","B2","B3"),15))
>Y<-numeric(length=45)
>Y[ 1: 9]<-c(56,52,48,57,54,46,55,51,51)
>Y[10:18]<-c(58,51,50,54,53,46,54,50,49)
>Y[19:27]<-c(53,49,48,56,48,52,52,52,50)
>Y[28:36]<-c(55,51,46,57,49,50,55,51,47)
>Y[37:45]<-c(56,48,51,58,50,48,58,46,52)
>df<-data.frame(id, factA, factB, Y) 
>df.aov <- aov(Y ~ factA*factB + Error(factA:id),
>data=df)
>summary(df.aov)
>
>Is there a way to get the same results with lme as 
>with aov with Error()? HOW???
>
>One idea was the following:
>df$factAid=factor(paste(as.character(df$factA),":",as.character(df$id),sep=""))
>df.lme <-
>lme(Y~factA*factB,df,random=~1|factAid,method="REML") 
>The degrees of freedom look right, but the F values
>don't match aov.
>
>Hope somebody can help! Thanks!!
>
>Karl
>  
>
Hmmm, strange, it works if I use factB:id as plot... it also works when 
I use factA:id as plot and replace your Y's by random numbers... is this 
a problem with convergence?

Pascal


 > df$Y=rnorm(45)
 > summary(aov(Y ~ factB*factA + Error(id:factA),data=df))

Error: id:factA
          Df  Sum Sq Mean Sq F value Pr(>F)
factA      2  2.9398  1.4699  0.9014 0.4318
Residuals 12 19.5675  1.6306

Error: Within
            Df  Sum Sq Mean Sq F value   Pr(>F)
factB        2  7.1431  3.5716  7.4964 0.002956 **
factB:factA  4  4.2411  1.0603  2.2254 0.096377 .
Residuals   24 11.4345  0.4764
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1

 > anova(lme(Y ~ factB*factA ,data=df, random = ~ 1 | plot))
            numDF denDF  F-value p-value
(Intercept)     1    24 0.014294  0.9058
factB           2    24 7.496097  0.0030
factA           2    12 0.901489  0.4318
factB:factA     4    24 2.225317  0.0964

Pascal


 > summary(aov(Y ~ factA*factB + Error(factB:id)))

Error: factB:id
          Df Sum Sq Mean Sq F value    Pr(>F)
factB      2 370.71  185.36  51.488 1.293e-06 ***
Residuals 12  43.20    3.60
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1

Error: Within
            Df Sum Sq Mean Sq F value  Pr(>F)
factA        2  9.911   4.956  1.6248 0.21788
factA:factB  4 45.556  11.389  3.7341 0.01686 *
Residuals   24 73.200   3.050

 > df$plot <- factor(paste(df$factB,df$id))
 > anova(lme(Y ~ factB*factA , data=df, random = ~1 | plot))
            numDF denDF  F-value p-value
(Intercept)     1    24 33296.02  <.0001
factB           2    12    51.47  <.0001
factA           2    24     1.63  0.2178
factB:factA     4    24     3.73  0.0168
 >



From Ted.Harding at nessie.mcc.ac.uk  Tue Dec  2 15:25:52 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 02 Dec 2003 14:25:52 -0000 (GMT)
Subject: [R] model of fish over exploitation
In-Reply-To: <1070363474.2940.51.camel@localhost>
Message-ID: <XFMail.031202142552.Ted.Harding@nessie.mcc.ac.uk>


On 02-Dec-03 bernard wrote:
> Dear all,
> 
> I have a serious problem to solve my model. I study over
> exploitation of fish in the bay of biscay (france). I know
> only the level of catch and the fishing effort (see data
> below) by year.
> 
> My model is composed by the following equations:
> 
> * the growth function
> Gt(St) = r*St*(1-St/sbar)
> with Gt the growth of each period t
> r intrinsec growth of the stock
> sbar carriyng capacity of the stock
> St the fish stock of each period t
> 
> * the production function
> Ht(St, Xt) = alpha*St*Xt
> Ht the catch for each period t
> Xt fishing effort for each period t
> alpha parameter of boat productivity
> 
> * the dynamic of the fish stock
> S(t+1) = S(t) + Gt - Ht
> 
> 
> I would like to modelise the following system:
> 
> S_(t+1) = S_t + G_t - H_t
> G_t = r*S_t*(1-S_t / sbar)
> H_t = alpha * S_t * X_t
> S_1961 = S_0
> 
> I know only H_t on period (1961 - 1994) and X_t on the same period.
> I don't know r, sbar, alpha and S_0 (the initial level of the stock)
> (and of course S_t on this period) and I want to estimate this four
> parameters.

I don't know (though no doubt others do) about fitting non-linear
dynamic models to time-series data in R. However, I can suggest
the following initial approach, which while rather crude may
be useful.

Let R[t] = H[t]/X[t] be the CPUE (catch per unit effort). Proceeding 
for the moment as though your equations were exactly deterministic,
you get

  R[t] = alpha*S[t]

from which S[t] = R[t]/alpha, and then

  R[t+1] = R[t] + r*R[t]*(1 - R[t]/(alpha*sbar)) - alpha*H[t]

which is an equation which involves only variables which can be
evaluated from your data, and the parameters r, alpha and sbar.

The real-life deviation of data from model can be represented
in this by adding a stochastic term.

As a first approximation, consider linearising this equation
by ignoring the non-linear term:

  R[t+1] = R[t] + r*R[t] - alpha*H[t]
         = R[t](1+r) - alpha*H[t]

(this is tantamount to assuming that the fish stock S[t] is well
 below the "carrying capacity" which, for a thoroughly expoited
 fishery in the Bay of Biscay, may be a realistic assumption ... ).

You are now in the domain of linear AR time series with a known
driving term, and I'm sure people can point you to good procedures
in R for estimating the paranetrs (1+r) and alpha in this.

At this stage, what you have obtained only has the status of an
initial indication of realistic values, since it is based on a
somewhat brutal manipulation of your original model.

However, you can now go back to your original equations. Your
most important task is to think of realistic statistical models
for the difference between your data and your equations, i.e. how
you will incorporate the stochastic errors. I would suggest here
that (based on experience with fisheries data) you work with
the logarithms of the variables, and make the errors additive
with respect to these; and "normally distributed" will probably
be an assumption which you can't disprove ...

Now, with a given hypothetical S[0] (actually I think you should
call this S[1] if you mean the Stock in 1961), you can explore the
likelihood function for the parameters. Find the maximum by some
method. Now you can vary S[0], and similarly get the maximising
value of this. A feasible starting value for S[0] could be
what is implied by S[0] = H[0]/(alpha*X[0]).

You may well find that r and alpha do not differ much from the
values you got in the first round. Your only information about
sbar will come from the non-linear term, whose coefficient is
likely to be poorly estimated (I'm guessing here ... ).

Incidentally, if you look at a plot of log(CPUE) against year:

  plot(year,log(catch)-log(effort))

it seems that something interesting happened around 1973-1975
(or, perhaps, something interesting was going on prior to this
period), since CPUE makes a sudden jump (it's not clear whether
1961-1967 is similar to 1975+ or whether the whole series is on
the low side prior to 1974; but there is at least a series of
low values from 1968-1973). Are these due to low stocks?
To mismatch between where people fished and where the fish were?
To competition from other fishing fleets? To non-declaration
of catches? ... ? Whatever the reason, this range of years is
going to complicate your model fitting!

I hoped this helps, and apologies for not knowing enough about
what's available on this front in R to be specific about routines
which you could usefully apply. I hope others can provide this
information.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 02-Dec-03                                       Time: 14:25:52
------------------------------ XFMail ------------------------------



From tlumley at u.washington.edu  Tue Dec  2 16:12:53 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 2 Dec 2003 07:12:53 -0800 (PST)
Subject: [R] check WARNING...
In-Reply-To: <003601c3b88a$d5ad2e00$0a00a8c0@rodan>
References: <003601c3b88a$d5ad2e00$0a00a8c0@rodan>
Message-ID: <Pine.A41.4.58.0312020708070.95454@homer12.u.washington.edu>

On Mon, 1 Dec 2003, Jeff D. Hamann wrote:
> I have a function called
>
> plot.summaries <- function( trees, sp=NULL ) {
> ...yak, kay, yak...
> }
>
> and I'm not sure if this is causing the problem. I'm developing a package
> for forestry and the field makes use of many terms commonly found in
> technology (logs, trees, plots, points, etc) and would like to know if the
> nomenclature will cause a problem.
>

Yes, that's the problem. Yes, it's annoying.  Yes, there's a good reason
for it.

If you define a method for plot, it has to be compatible with the generic
and it has to be able to be extended if someone else develops a subclass
of your class. As long as no-one extends your code, and the function is
always called as  plot(some.tree.thing) there isn't a problem.  But if
someone calls
   plot(trees=some.tree.thing)
this doesn't match the generic.

If plot.summaries() isn't a method for plot then it should be called
something else, like plotSummaries().

	-thomas



From r.knell at qmul.ac.uk  Tue Dec  2 16:19:24 2003
From: r.knell at qmul.ac.uk (Rob Knell)
Date: Tue, 02 Dec 2003 15:19:24 +0000
Subject: [R] changing axis font size in a pairs plot?
Message-ID: <4.1.20031202151619.0117f6a0@pop.qmw.ac.uk>

Hello

Can anyone let me know how I might change the size of the numbers on the
axes in a pairs plot? The normal cex.axis call doesn't do anything,
cex.labels only seems to change the font size in the diagonal labels. Using
R 1.8.0 on a Mac with OS X.3.

Thanks

Rob Knell



From cmoffet at nwrc.ars.usda.gov  Tue Dec  2 16:33:32 2003
From: cmoffet at nwrc.ars.usda.gov (Corey Moffet)
Date: Tue, 02 Dec 2003 08:33:32 -0700
Subject: [R] Is there a way to use CGIwithR in Windows?
Message-ID: <3.0.6.32.20031202083332.0108eaf8@nwrc.ars.usda.gov>

Dear R-Help,

Does anyone know of a way to use CGIwithR in Windows?  David Firth the 
author of CGIwithR states "It ought to be possible to adapt it for use also
in conjunction with web servers running under Windows NT ... but the author
has no plans for that."  I have developed some applications in R that I would
like to make available through a web interface using CGIwithR but at this
time I am a Windows user.  Any suggestions?
With best wishes and kind regards I am

Sincerely,

Corey A. Moffet
Rangeland Scientist

USDA-ARS
Northwest Watershed Research Center
800 Park Blvd, Plaza IV, Suite 105
Boise, ID 83712-7716

Voice: (208) 422-0718
FAX:   (208) 334-1502



From song.baiyi at udo.edu  Tue Dec  2 16:34:14 2003
From: song.baiyi at udo.edu (Song Baiyi)
Date: Tue, 02 Dec 2003 16:34:14 +0100
Subject: [R] Two questions about the creating new package
Message-ID: <3FCCB0F6.1000508@udo.edu>

Hello everyone,

I am just trying to colloct all my function into a new packages. I met 
two questions which hurt me so much:

1. when I use the "prompt" to help to write Rd file for a variable x 
which is character vector, say x <- c("a","b"), it always give the error 
informaion:
Error in get(x, envir, mode, inherits) : variable "a" was not found.
Obvious it regards the "a" as the variable name instead of the item of a 
vector. So how can I put a character vector into the pacakge, or it must 
be a data.frame to put into a package?

2. Also about the constant character vector. I have a constant vector 
which record the column names. I hope when the package is loaded, this 
vector will be loaded without explict writing "data(***)". Therefore the 
users and other functions can use it. How can I do it? A more generate 
question is where should I put those R codes which will be excuted after 
the package is loaded?

Thank you very much!

Baiyi Song
Computer engineering institute
Dortmund University



From rsoerum80 at hotmail.com  Tue Dec  2 17:04:20 2003
From: rsoerum80 at hotmail.com (=?iso-8859-1?B?UmFnbmhpbGQgU/hydW0=?=)
Date: Tue, 02 Dec 2003 16:04:20 +0000
Subject: [R] generate random number 
Message-ID: <BAY10-F122m5Oeg3Bvy000028df@hotmail.com>

Hi

I would like to generate some(ss) random numbers from a density distribution 
f(x).
In my case f(x) = a*exp(b*x),  ex. a=2, b=0.5,   and x is only defined 
between x1 and x2,  ex.  from -6 to -2.5.

Any suggestions? Can I use rexp?

Thanks,
Ragnhild



From dmurdoch at pair.com  Tue Dec  2 17:09:49 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Tue, 02 Dec 2003 11:09:49 -0500
Subject: [R] Rd Files?
In-Reply-To: <x2wu9gs0he.fsf@biostat.ku.dk>
References: <Pine.LNX.4.44.0312011753460.7849-100000@gannet.stats>
	<x2wu9gs0he.fsf@biostat.ku.dk>
Message-ID: <mvdpsvk2ala6b0qtebmf601i3i5ogdu9ug@4ax.com>

On 01 Dec 2003 22:15:57 +0100, Peter Dalgaard
<p.dalgaard at biostat.ku.dk> wrote :

>Just one thing to add: If Wolski gets sufficiently annoyed to actually
>try and do this right, he'll want a naming scheme that works on *all*
>platforms, so branching on sys.info()["sysname"]=="Windows" is surely
>wrong. 

Probably the right way to do it is to write a Sys.legal.filenames()
function that takes a vector of strings and creates legal filenames
out of them.  I'm not sure what optional args it would want, but
possibilities are

  - Avoid existing filenames.
  - Do it in a certain path.
  - Do it in a way that's likely to be portable to other systems.
  - Create filenames that are convenient, not just legal (e.g. even on
Unix, don't allow asterisks or other characters that shells pay
attention to; on Windows, avoid embedded spaces, etc.).

Duncan Murdoch



From ripley at stats.ox.ac.uk  Tue Dec  2 17:36:31 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 2 Dec 2003 16:36:31 +0000 (GMT)
Subject: [R] Two questions about the creating new package
In-Reply-To: <3FCCB0F6.1000508@udo.edu>
Message-ID: <Pine.LNX.4.44.0312021629480.16240-100000@gannet.stats>

On Tue, 2 Dec 2003, Song Baiyi wrote:

> Hello everyone,
> 
> I am just trying to colloct all my function into a new packages. I met 
> two questions which hurt me so much:
> 
> 1. when I use the "prompt" to help to write Rd file for a variable x 
> which is character vector, say x <- c("a","b"), it always give the error 
> informaion:
> Error in get(x, envir, mode, inherits) : variable "a" was not found.
> Obvious it regards the "a" as the variable name instead of the item of a 
> vector. So how can I put a character vector into the pacakge, or it must 
> be a data.frame to put into a package?

are you saying prompt(x) or prompt("x") or prompt(name="x")?  The latter
two work.  Note also the Warning on the help page for prompt.

> 2. Also about the constant character vector. I have a constant vector 
> which record the column names. I hope when the package is loaded, this 
> vector will be loaded without explict writing "data(***)". Therefore the 
> users and other functions can use it. How can I do it? A more generate 
> question is where should I put those R codes which will be excuted after 
> the package is loaded?

If you put code in a file say MyPkgSources/R/zzz.R it is executed on
loading, and that can assign a constant character vector.  It is often
better to put code in a .First.lib function, though.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Dec  2 17:39:50 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 2 Dec 2003 16:39:50 +0000 (GMT)
Subject: [R] changing axis font size in a pairs plot?
In-Reply-To: <4.1.20031202151619.0117f6a0@pop.qmw.ac.uk>
Message-ID: <Pine.LNX.4.44.0312021637220.16240-100000@gannet.stats>

par(cex.axis=0.5)
data(iris)
pairs(iris[1:4])

seems to work for me, although ideally

pairs(iris[1:4], cex.axis=0.5) would work.

On Tue, 2 Dec 2003, Rob Knell wrote:

> Can anyone let me know how I might change the size of the numbers on the
> axes in a pairs plot? The normal cex.axis call doesn't do anything,
> cex.labels only seems to change the font size in the diagonal labels. Using
> R 1.8.0 on a Mac with OS X.3.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Dec  2 17:41:05 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 2 Dec 2003 16:41:05 +0000 (GMT)
Subject: [R] generate random number 
In-Reply-To: <BAY10-F122m5Oeg3Bvy000028df@hotmail.com>
Message-ID: <Pine.LNX.4.44.0312021640190.16240-100000@gannet.stats>

On Tue, 2 Dec 2003, Ragnhild S?rum wrote:

> I would like to generate some(ss) random numbers from a density distribution 
> f(x).
> In my case f(x) = a*exp(b*x),  ex. a=2, b=0.5,   and x is only defined 
> between x1 and x2,  ex.  from -6 to -2.5.
> 
> Any suggestions? Can I use rexp?

The inversion method is very simple here: see any good book on Simulation.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From wolski at molgen.mpg.de  Tue Dec  2 17:45:46 2003
From: wolski at molgen.mpg.de (Wolski)
Date: Tue, 02 Dec 2003 17:45:46 +0100
Subject: [R] Rd Files?
In-Reply-To: <mvdpsvk2ala6b0qtebmf601i3i5ogdu9ug@4ax.com>
References: <Pine.LNX.4.44.0312011753460.7849-100000@gannet.stats>
	<x2wu9gs0he.fsf@biostat.ku.dk>
	<mvdpsvk2ala6b0qtebmf601i3i5ogdu9ug@4ax.com>
Message-ID: <200312021745460781.06DA643E@harry.molgen.mpg.de>

Hi!

It would be great If the problem with prompt or package.skeleton would be only the file naming conventions on different platforms. For me the hack with gsub(<-,=,functionname) and sys.info()[sysname] worked and I generated Rd files for all my functions. 

I have seen the output and it does not matter to me anymore if prompt or package.skeleton works on any platform. I hope it wasn't a too big heresy.
If someone would ask me what are the week point of R, then the only one that pops up immediately, is that the documentation to functions have to be stored in a separate file than the code.  I am a big R/S fan. 
But its a pity that comments above or below the function declaration are not recognized by the help system. Therefore "prompt" or "package.skeleton" how they are implemented now are not of big use to me. 
They do not cope with comments in the source. And its good to comment the code anyway. So prompt and package.skeleton are not the functionality for which I am looking. 
I am looking for functions that enable me a documentation concept how it was introduced, or I encountered first, by javadoc. I also know that it is now implemented for many other languages by  such tools like doxygen which run on any platform. I  heard that they can be adapted to other languages too and to generate specific output probably also Rd files.
 
For now, to get started with any documentation, I decided to use a perl hack that parses the code and generates Rd files. And this was for what i was looking for.

Eryk


*********** REPLY SEPARATOR  ***********

On 12/2/2003 at 11:09 AM Duncan Murdoch wrote:

>On 01 Dec 2003 22:15:57 +0100, Peter Dalgaard
><p.dalgaard at biostat.ku.dk> wrote :
>
>>Just one thing to add: If Wolski gets sufficiently annoyed to actually
>>try and do this right, he'll want a naming scheme that works on *all*
>>platforms, so branching on sys.info()["sysname"]=="Windows" is surely
>>wrong. 
>
>Probably the right way to do it is to write a Sys.legal.filenames()
>function that takes a vector of strings and creates legal filenames
>out of them.  I'm not sure what optional args it would want, but
>possibilities are
>
>  - Avoid existing filenames.
>  - Do it in a certain path.
>  - Do it in a way that's likely to be portable to other systems.
>  - Create filenames that are convenient, not just legal (e.g. even on
>Unix, don't allow asterisks or other characters that shells pay
>attention to; on Windows, avoid embedded spaces, etc.).
>
>Duncan Murdoch



Dipl. bio-chem. Eryk Witold Wolski    @    MPI-MG Dep. Vertebrate Genomics   
Ihnestrasse 73 14195 Berlin          'v'    
tel: 0049-30-84131285               /   \    
mail: wolski at molgen.mpg.de        ---W-W----    http://www.molgen.mpg.de/~wolski



From spencer.graves at pdf.com  Tue Dec  2 18:00:14 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 02 Dec 2003 09:00:14 -0800
Subject: [R] generate random number
In-Reply-To: <BAY10-F122m5Oeg3Bvy000028df@hotmail.com>
References: <BAY10-F122m5Oeg3Bvy000028df@hotmail.com>
Message-ID: <3FCCC51E.5060805@pdf.com>

      Please note that "a" is given by "b" (or vice versa), because the 
integral from x1 to x2 of f(x)dx must be 1. 

      I have solved this kind of problem by writing density functions, 
cumulative distribution functions, quantile and random number functions 
as a set of 4, using different ones to help check the others.  If your 
distribution is "ss", then the standard S naming convention would have 
dss, pss, qss, and rss for density, probability (cdf), quantile (inverse 
cdf) and random number generation.  See, e.g., "?runif" for a typical 
argument convention. 

      Note that rss <- function(n, ...)pss(runif(n), ...)

      hope this helps.  spencer graves

Ragnhild S?rum wrote:

> Hi
>
> I would like to generate some(ss) random numbers from a density 
> distribution f(x).
> In my case f(x) = a*exp(b*x),  ex. a=2, b=0.5,   and x is only defined 
> between x1 and x2,  ex.  from -6 to -2.5.
>
> Any suggestions? Can I use rexp?
>
> Thanks,
> Ragnhild
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From jasont at indigoindustrial.co.nz  Tue Dec  2 18:01:40 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Wed, 03 Dec 2003 06:01:40 +1300
Subject: [R] Two questions about the creating new package
In-Reply-To: <3FCCB0F6.1000508@udo.edu>
References: <3FCCB0F6.1000508@udo.edu>
Message-ID: <3FCCC574.2090104@indigoindustrial.co.nz>

Song Baiyi wrote:

> Hello everyone,
> 
> I am just trying to colloct all my function into a new packages. I met 
> two questions which hurt me so much:
> 
> 1. when I use the "prompt" to help to write Rd file for a variable x 
> which is character vector, say x <- c("a","b"), it always give the error 
> informaion:
> Error in get(x, envir, mode, inherits) : variable "a" was not found.
> Obvious it regards the "a" as the variable name instead of the item of a 
> vector. So how can I put a character vector into the pacakge, or it must 
> be a data.frame to put into a package?

Use the "name" argument to prompt.

prompt(name="x")

should work.

> 2. Also about the constant character vector. I have a constant vector 
> which record the column names. I hope when the package is loaded, this 
> vector will be loaded without explict writing "data(***)". Therefore the 
> users and other functions can use it. How can I do it? A more generate 
> question is where should I put those R codes which will be excuted after 
> the package is loaded?

See help(.First.lib) .  In this case, you'd want something like....

.First.lib <- function(lib,pkg){
   data(foo)
}

When writing a package, these functions are traditionally (but not 
manditorily) stored in a file called zzz.R .

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From rossini at blindglobe.net  Tue Dec  2 18:08:05 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Tue, 02 Dec 2003 09:08:05 -0800
Subject: [R] Rd Files?
In-Reply-To: <200312021745460781.06DA643E@harry.molgen.mpg.de>
	(wolski@molgen.mpg.de's
	message of "Tue, 02 Dec 2003 17:45:46 +0100")
References: <Pine.LNX.4.44.0312011753460.7849-100000@gannet.stats>
	<x2wu9gs0he.fsf@biostat.ku.dk>
	<mvdpsvk2ala6b0qtebmf601i3i5ogdu9ug@4ax.com>
	<200312021745460781.06DA643E@harry.molgen.mpg.de>
Message-ID: <857k1fb11m.fsf@blindglobe.net>

"Wolski" <wolski at molgen.mpg.de> writes:


> I have seen the output and it does not matter to me anymore if prompt
> or package.skeleton works on any platform. I hope it wasn't a too big
> heresy.  If someone would ask me what are the week point of R, then
> the only one that pops up immediately, is that the documentation to
> functions have to be stored in a separate file than the code.  I am a
> big R/S fan.  But its a pity that comments above or below the function
> declaration are not recognized by the help system. Therefore "prompt"

Doug Bates commented on the possibility of patching Doxygen to do
this, once long ago.  Not sure if anyone took it anywhere, though.
It's a reasonable system for assisting with documentation in a number
of languages, though it could be improved.  That would be nice, since
then one would get C docs "for free".

Another alternative is to write a noweb lit-prog file, and then generate
your package via noweb (NOT Sweave, though you get double duty, since
if it's written right, you can stick the original doc in as a
vignette).  

best,
-tony

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From d.firth at warwick.ac.uk  Tue Dec  2 18:19:32 2003
From: d.firth at warwick.ac.uk (David Firth)
Date: Tue, 2 Dec 2003 17:19:32 +0000
Subject: [R] Is there a way to use CGIwithR in Windows?
In-Reply-To: <3.0.6.32.20031202083332.0108eaf8@nwrc.ars.usda.gov>
Message-ID: <B18EB77A-24EB-11D8-B41A-0050E4C03977@warwick.ac.uk>

I guess it's possible to run bash and standard unix-type tools in 
recent versions of Windows?  (see for example http://www.cygwin.com ?  
I know nothing of such things, though.)  If so then with perhaps some 
minor modifications it "ought" to be possible to get the wrapper 
shell-script R.cgi to work -- provided your http server supports CGI of 
course.  Do let me know if you succeed!

Regards,
David

On Tuesday, Dec 2, 2003, at 15:33 Europe/London, Corey Moffet wrote:

> Dear R-Help,
>
> Does anyone know of a way to use CGIwithR in Windows?  David Firth the
> author of CGIwithR states "It ought to be possible to adapt it for use 
> also
> in conjunction with web servers running under Windows NT ... but the 
> author
> has no plans for that."  I have developed some applications in R that 
> I would
> like to make available through a web interface using CGIwithR but at 
> this
> time I am a Windows user.  Any suggestions?
> With best wishes and kind regards I am
>
> Sincerely,
>
> Corey A. Moffet
> Rangeland Scientist
>
> USDA-ARS
> Northwest Watershed Research Center
> 800 Park Blvd, Plaza IV, Suite 105
> Boise, ID 83712-7716
>
> Voice: (208) 422-0718
> FAX:   (208) 334-1502
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From statho3 at web.de  Tue Dec  2 19:06:05 2003
From: statho3 at web.de (Thomas Stabla)
Date: Tue, 2 Dec 2003 19:06:05 +0100 (CET)
Subject: [R] setMethod("min", "myclass", ...)
Message-ID: <Pine.LNX.4.44.0312021845550.3778-100000@spock.vulcan>

Hello,

I have defined a new class

> setClass("myclass", representation(min = "numeric", max = "numeric"))

and want to write accessor functions, so that for

> foo = new("myclass", min = 0, max = 1)
> min(foo) # prints 0
> max(foo) # prints 1

At first i created a generic function for "min"

> setGeneric("min", function(..., na.rm = FALSE) standardGeneric("min"))

and then tried to use setMethod. And there's my problem, I don't know the
name of the first argument which is to be passed to "min".
I can't just write:

> setMethod("min", "myclass", function(..., na.rm = FALSE) ... at min)

The same problem occurs with "max".

Thanks for your help.

Greetings,
Thomas Stabla



From elvis at xlsolutions-corp.com  Tue Dec  2 22:18:09 2003
From: elvis at xlsolutions-corp.com (elvis@xlsolutions-corp.com)
Date: Tue,  2 Dec 2003 14:18:09 -0700
Subject: [R] Course***R/S-plus Programming Techniques II in January 2004 @ 4
	locations near you.
Message-ID: <20031202211809.12390.qmail@webmail-2-2.mesa1.secureserver.net>


   Happy New Year! XLSolutions Corporation ([1]www.xlsolutions-corp.com)
   is proud
   to announce a 2-day "R/S-plus Programming Techniques II" course:

   ****San Francisco, CA ----------> January 15-16
   ****Boston, MA -----------------> January 29-30
   ****Princeton, NJ --------------> January 29-30
   **** Washington, DC -----------> TBD


   Course Description:

   This intermediate level course with some advanced topis is recommended
   for those users
   who have some previous experience using R/S-plus and wish to hone
   their skills.
   Casual users can "fill in the gaps" of their knowledge to make better
   use of R/S-PLus.
   This course will focus on intermediate/advanced topics that help to
   work
   with large objest; Vectorization, resource management, calling C and
   Fortran,
   classes and methods.

   Course Outline:
   - Overview of R/S-Plus
   - Techniques for Effective use of R and S
   - Taking advantage of fast objects and fast functions
   - Avoiding Loops
   - Vectorization
   - Working with large objects; Resource Management
   - Calling C and Fortran programs
   - Introduction to Classes (S4/S-Plus) and Methods
   - Librarie (Packages) and help files
   Payments are due AFTER the course and early-bird ends December 20!
   Early bird research group: $995
   Registration:
   Email Sue Turner: sue at xlsolutions-corp.com
   Phone: 206-686-1578 x221
   Visit us: [2]www.xlsolutions-corp.com/training.htm

   Course Format:
   This course consists of a series of short lectures with
   demonstrations and interactive sessions for the participants.
   Each student is provided with bound copies of the notes and
   a CD-ROM containing all examples, exercises and software used
   on the course.

   Share Your Thoughts:
   Are there any additional topics you would like for this course to
   address?
   Would you like for this course to be offered in another city?
   Please let us know by contributing to our recommendation list:
   training at xlsolutions-corp.com.

   ======================================================================
   ==
   R/S-Plus Programming Techniques II
   Pre-registration Form (Please email or print and fax: 206-686-1578)
   XLsolutions Corporation: For your Solutions needs, Consulting and
   Training.
   [3]www.xlsolutions-corp.com

   Title...... First Name ................. Last Name....................
   Organization..........................................................
   Mailing Address.......................................................
   .....................................................................
   .....................................................................
   Zip Code...................... Country.............................
   Telephone........................... Fax
   ...............................
   E-mail................................................................
   Payment will be made by: (1) Check (2) Invoice (3) Credit Card

   Elvis Miller, PhD
   Manager Training and Technical Support
   North American Division
   XLSolutions Corporation
   Email: elvis at xlsolutions-corp.com
   Phone: 206-686-1578
   Web: [4]www.xlsolutions-corp.com

References

   1. http://www.xlsolutions-corp.com/
   2. http://www.xlsolutions-corp.com/training.htm
   3. http://www.xlsolutions-corp.com/
   4. http://www.xlsolutions-corp.com/


From HStevens at muohio.edu  Tue Dec  2 22:28:48 2003
From: HStevens at muohio.edu (Hank Stevens)
Date: Tue, 02 Dec 2003 16:28:48 -0500
Subject: [R] contributed packages not found
Message-ID: <5.1.0.14.2.20031202162400.029a6a68@po.muohio.edu>


   Windows 2000, R 1.6.1
   When I try use the menu to download and install new packages, the
   result looks like this:
   {a <- CRAN.packages()
   + install.packages(select.list(a[,1],,TRUE), .libPaths()[1],
   available=a)}
   trying URL `[1]http://cran.r-project.org/bin/windows/contrib/PACKAGES'
   Error in download.file(url = paste(contriburl, "PACKAGES", sep = "/"),
   :
   cannot open: HTTP status was `404'
   Is this no longer the path to the packages? Can I fix this without
   updating R?
   Many thanks,
   Hank Stevens

   Dr. Martin Henry H. Stevens, Assistant Professor
   338 Pearson Hall
   Botany Department
   Miami University
   Oxford, OH 45056
   Office: (513) 529-4206
   Lab: (513) 529-4262
   FAX: (513) 529-4243
   [2]http://www.cas.muohio.edu/botany/bot/henry.html
   [3]http://www.muohio.edu/ecology
   

References

   1. http://cran.r-project.org/bin/windows/contrib/PACKAGES
   2. http://www.cas.muohio.edu/botany/bot/henry.html
   3. http://www.muohio.edu/ecology


From MSchwartz at medanalytics.com  Tue Dec  2 22:38:53 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Tue, 02 Dec 2003 15:38:53 -0600
Subject: [R] contributed packages not found
In-Reply-To: <5.1.0.14.2.20031202162400.029a6a68@po.muohio.edu>
References: <5.1.0.14.2.20031202162400.029a6a68@po.muohio.edu>
Message-ID: <1070401132.4985.14.camel@localhost.localdomain>

On Tue, 2003-12-02 at 15:28, Hank Stevens wrote:
>    Windows 2000, R 1.6.1
>    When I try use the menu to download and install new packages, the
>    result looks like this:
>    {a <- CRAN.packages()
>    + install.packages(select.list(a[,1],,TRUE), .libPaths()[1],
>    available=a)}
>    trying URL
> `[1]http://cran.r-project.org/bin/windows/contrib/PACKAGES'
>    Error in download.file(url = paste(contriburl, "PACKAGES", sep =
> "/"),
>    :
>    cannot open: HTTP status was `404'
>    Is this no longer the path to the packages? Can I fix this without
>    updating R?
>    Many thanks,
>    Hank Stevens


Quoting from http://cran.r-project.org/bin/windows/contrib/ReadMe:

"Packages for R < 1.7.0 are no longer available at this location for
various reasons. Binary packages for the outdated versions R-1.6.x
have to be downloaded from subdirectory ./1.6 manually."

You will need to go to

http://cran.r-project.org/bin/windows/contrib/1.6/

and download the ZIP files directly. Then use the Install from Local Zip
File option on the menu.

Better yet, would be to update to 1.8.1, since you are a few versions
out of date.

HTH,

Marc Schwartz



From p.dalgaard at biostat.ku.dk  Tue Dec  2 22:57:42 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 02 Dec 2003 22:57:42 +0100
Subject: [R] contributed packages not found
In-Reply-To: <5.1.0.14.2.20031202162400.029a6a68@po.muohio.edu>
References: <5.1.0.14.2.20031202162400.029a6a68@po.muohio.edu>
Message-ID: <x2brqqc27d.fsf@biostat.ku.dk>

Hank Stevens <HStevens at muohio.edu> writes:

>    Windows 2000, R 1.6.1
>    When I try use the menu to download and install new packages, the
>    result looks like this:
>    {a <- CRAN.packages()
>    + install.packages(select.list(a[,1],,TRUE), .libPaths()[1],
>    available=a)}
>    trying URL `[1]http://cran.r-project.org/bin/windows/contrib/PACKAGES'
>    Error in download.file(url = paste(contriburl, "PACKAGES", sep = "/"),
>    :
>    cannot open: HTTP status was `404'
>    Is this no longer the path to the packages? Can I fix this without
>    updating R?
>    Many thanks,
>    Hank Stevens

As checking with a browser might have told you, we now have 

http://cran.r-project.org/bin/windows/contrib/1.6
http://cran.r-project.org/bin/windows/contrib/1.7
http://cran.r-project.org/bin/windows/contrib/1.8
http://cran.r-project.org/bin/windows/contrib/1.9

which might give you an idea what to fix. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From wind0w_glass at yahoo.com  Tue Dec  2 23:10:58 2003
From: wind0w_glass at yahoo.com (Window Glass)
Date: Tue, 2 Dec 2003 14:10:58 -0800 (PST)
Subject: [R] Trouble with syntax
Message-ID: <20031202221058.83083.qmail@web40510.mail.yahoo.com>

Hello,

axsize <- max(i)
allsize <- axsize * axsize
x <- array(c(1:allsize), dim=c(axsize, axsize))
x[1:allsize] <- 0
x[i] <- 1

for (c in 1:axsize)
{
        ourgraph <- data.frame(edges=x,
vertices=c(1:axsize), ith=c,
                        components=list(1),
communities=list(1), row.names=NULL)
        browser()

When I get to browser, printing ourgraph$edges gives
NULL while printing x gives the matrix with the
correct values.
What am I writing wrong?

Regards,
Wind0w Glass



From Simon.Gatehouse at csiro.au  Tue Dec  2 23:47:34 2003
From: Simon.Gatehouse at csiro.au (Simon.Gatehouse@csiro.au)
Date: Wed, 3 Dec 2003 09:47:34 +1100 
Subject: [R] Getting rid of loops?
Message-ID: <FFE02AF26875734B82A728403821CB2EFB05DB@asp-RI.riverside.CSIRO.AU>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031203/e59c881d/attachment.pl

From p.dalgaard at biostat.ku.dk  Wed Dec  3 00:57:46 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 03 Dec 2003 00:57:46 +0100
Subject: [R] Getting rid of loops?
In-Reply-To: <FFE02AF26875734B82A728403821CB2EFB05DB@asp-RI.riverside.CSIRO.AU>
References: <FFE02AF26875734B82A728403821CB2EFB05DB@asp-RI.riverside.CSIRO.AU>
Message-ID: <x27k1ebwn9.fsf@biostat.ku.dk>

Simon.Gatehouse at csiro.au writes:

> I think this will do what you want, though there may be ways of speeding it
> up further.
> 
> theta.dist <- function(x)
> as.dist(acos(crossprod(t(x))/sqrt(crossprod(t(rowSums(x*x)))))/pi*180)

Or,

theta.dist <- function(x)
  as.dist(acos(cov2cor(crossprod(t(x))))/pi*180)

Now, if only there was a way to tell cor() not to center the
variables, we'd have 

  as.dist(acos(cor(t(x),center=F))/pi*180)

Unfortunately there's no such argument.

> 
> theta.dist <- function(x){
> 
>   res <- matrix(NA, nrow(x), nrow(x))
> 
>   for (i in 1:nrow(x)){
>     for(j in 1:nrow(x)){
>       if (i > j)
>         res[i, j] <- res[j, i]
>       else {
>         v1 <- x[i,]
>         v2 <- x[j,]
>         good <- !is.na(v1) & !is.na(v2)
>         v1 <- v1[good]
>         v2 <- v2[good]
>         theta <- acos(v1%*%v2 / sqrt(v1%*%v1 * v2%*%v2 )) / pi * 180
>         res[i,j] <- theta
>       }
>     }
>   }
>   as.dist(res)
> }
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> <https://www.stat.math.ethz.ch/mailman/listinfo/r-help> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Tom.Mulholland at health.wa.gov.au  Wed Dec  3 04:23:30 2003
From: Tom.Mulholland at health.wa.gov.au (Mulholland, Tom)
Date: Wed, 3 Dec 2003 11:23:30 +0800
Subject: [R] R and Memory
Message-ID: <74E242B6968AA0469B632C5A3EFC1EFD03D57012@nt207mesep.corporate.hdwa.health.wa.gov.au>

I would suggest that you make a more thorough search of the R-Archives.
(http://finzi.psych.upenn.edu/search.html) If you do you will find this
discussion has been had several times and that the type of machine you
are running will have an impact upon what you can do. My feeling is that
you are going have to knuckle down with the documentation and understand
how R works and then when you have specific issues that show you have
read all the appropriate documentation, you might try another message to
the list.

Ciao, Tom

_________________________________________________
 
Tom Mulholland
Senior Policy Officer
WA Country Health Service
Tel: (08) 9222 4062
 
The contents of this e-mail transmission are confidential and may be
protected by professional privilege. The contents are intended only for
the named recipients of this e-mail. If you are not the intended
recipient, you are hereby notified that any use, reproduction,
disclosure or distribution of the information contained in this e-mail
is prohibited. Please notify the sender immediately.


-----Original Message-----
From: Edward McNeil [mailto:edward at ratree.psu.ac.th] 
Sent: Tuesday, 2 December 2003 8:45 AM
To: r-help at stat.math.ethz.ch
Subject: [R] R and Memory


Dear all,
This is my first post.
We have started to use R here and have also started teaching it to our
PhD students. Our unit will be the HQ for developing R throughout
Thailand.

I would like some help with a problem we are having. We have one sample
of data that is quite large in fact - over 2 million records (ok ok it's
more like a population!). The data is stored in SPSS. The file is over
350Mb but SPSS happily stores this much data. Now when I try to read it
into R it grunts and groans for a few seconds and then reports that
there is not enough memory (the computer has 250MB RAM). I have tried
setting the memory in the command line (--max-vsize and --max-mem-size)
but all to no avail.

Any help would be muchly appreciated!

Edward McNeil (son of Don)
Epidemiology Unit
Faculty of Medicine
Prince of Songkhla University
Hat Yai  90110
THAILAND

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From tchur at optushome.com.au  Wed Dec  3 04:46:13 2003
From: tchur at optushome.com.au (Tim Churches)
Date: Wed, 03 Dec 2003 14:46:13 +1100
Subject: [R] R and Memory
Message-ID: <200312030346.hB33kED25213@mail013.syd.optusnet.com.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031203/3285af8f/attachment.pl

From kjetil at entelnet.bo  Wed Dec  3 04:53:53 2003
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Tue, 02 Dec 2003 23:53:53 -0400
Subject: [R] pushBack
Message-ID: <3FCD2611.11134.F7CE89@localhost>

Hola!

I have been fighting some with pushBack, well, at latest I gor it to 
do what I want, but the documentation could be clearer. 

>From the documentation:

data a character vector.

but the simple use pushes back each element of the character vector, 
so that pushing back a character vector of length 5 gives 5 lines in 
the pushback stack. Neither does the argument newLine=FALSE help, it 
pastes all together as if used with paste( , collapse="").

If I have read a character vector with 
test <- scan( myfile, what=character(0), nlines=1 )

to push it back as it was needs

pushBack( paste(test, collapse=" "), myfile )

could this be mentioned in the documentation , for example added to 
the examples part?

Kjetil Halvorsen



From Alexander.Herr at csiro.au  Wed Dec  3 06:25:44 2003
From: Alexander.Herr at csiro.au (Alexander.Herr@csiro.au)
Date: Wed, 3 Dec 2003 16:25:44 +1100
Subject: [R] npmc output
Message-ID: <062AE320EF971E40ACD0F6C93391D7690A52C9@exqld1-tsv.nexus.csiro.au>

Hi List,

Can anyone help with further documentation on the npmc output (library npmc). I can't find details in Munzel& Hothorn's paper. Specifically I am looking for differences (wrt interpretation) of the 

p-value 1s: the 1-sided p-value and 
p-value 2s: the 1-sided p-value.

Any help appreciated
Herry
(please also cc your response to my email (alexander.herr at csiro.au)



From phgrosjean at sciviews.org  Wed Dec  3 09:38:40 2003
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Wed, 3 Dec 2003 09:38:40 +0100
Subject: [R] Rd Files?
In-Reply-To: <857k1fb11m.fsf@blindglobe.net>
Message-ID: <MABBLJDICACNFOLGIHJOKEJLDPAA.phgrosjean@sciviews.org>

>"Wolski" <wolski at molgen.mpg.de> wrotes:
>> I have seen the output and it does not matter to me anymore if prompt
>> or package.skeleton works on any platform. I hope it wasn't a too big
>> heresy.  If someone would ask me what are the week point of R, then
>> the only one that pops up immediately, is that the documentation to
>> functions have to be stored in a separate file than the code.  I am a
>> big R/S fan.  But its a pity that comments above or below the function
>> declaration are not recognized by the help system. Therefore "prompt"

Toni Rossini answered:
>Doug Bates commented on the possibility of patching Doxygen to do
>this, once long ago.  Not sure if anyone took it anywhere, though.
>It's a reasonable system for assisting with documentation in a number
>of languages, though it could be improved.  That would be nice, since
>then one would get C docs "for free".

>Another alternative is to write a noweb lit-prog file, and then generate
>your package via noweb (NOT Sweave, though you get double duty, since
>if it's written right, you can stick the original doc in as a
>vignette).

Well, writing a quick and durty help for a function with a few lines of
comment above or below the function code ("a la Matlab") should be nice. I
don't think that it should be a good idea to provide a complex alternative
solution for documenting the functions than the current mechanism which is
both powerful and efficient (but, of course, a little bit complex). Here is
a "quick and durty" implementation of a mechanism to include "quick and
durty" help messages inside the code of an R function. I guess this is
enough.

qhelp <- function(topic) {
    if (is.character(topic))
        topic <- get(topic)
    if (!is.function(topic))
        stop("`topic` must be a function, or the name of a function")
    fcode <- sub("    ", "", deparse(topic)) # Because 4 spaces are added by
deparse
    # Look for quick help text, that is, strings starting with `#`
    qhlp <- fcode[grep("^\"\#", fcode)]
    qhlp <- as.character(parse(text=qhlp))
    cat(paste(qhlp, "\n", sep=""), sep="")
    return(invisible())

    # Quick help
    "# `qhelp()` provides a mechanism to include \"quick help\""
    "# embedded inside the code of an R function."
    "#"
    "# Just end the function code with return(res)"
    "# and add some strings starting with `#` after it"
    "# with the content of your quick help message..."
}

# An example of a very simple function with quick help
cube <- function(x) {
    # This is some comment that will appear only when I print the
function...
    return(x^3)

    # Quick help
    "# `cube(x)` returns the cube of its `x` argument"
    "# Version 0.1, by Ph. Grosjean (phgrosjean at sciviews.org)"
}

qhelp(cube)    # Should return quick help
qhelp("qhelp") # Strings also allowed for `topic` argument
qhelp("log")   # No quick help, should print just an empty lines

Best,

Philippe

...........]<(({?<...............<?}))><...............................
 ) ) ) ) )
( ( ( ( (   Prof. Philippe Grosjean
 ) ) ) ) )
( ( ( ( (   Numerical Ecology Laboratory
 ) ) ) ) )  Mons-Hainaut University
( ( ( ( (   8, Av. du Champ de Mars, 7000 Mons, Belgium
 ) ) ) ) )
( ( ( ( (   phone: 00-32-65.37.34.97
 ) ) ) ) )  email: Philippe.Grosjean at umh.ac.be; phgrosjean at sciviews.org
( ( ( ( (   SciViews project coordinator (http://www.sciviews.org)
 ) ) ) ) )
.......................................................................



From John.Marsland at CommerzbankIB.com  Wed Dec  3 09:41:06 2003
From: John.Marsland at CommerzbankIB.com (Marsland, John)
Date: Wed, 3 Dec 2003 08:41:06 -0000 
Subject: [R] Rblas for dual Xeon
Message-ID: <8CBAA121CEB4D5118CB200508BB2BBEF0317EAC3@xmx8lonib.lonib.commerzbank.com>

Does anybody have a tuned Rblas.dll compiled against ALTLAS for a dual Xeon
system?

Unfortunately, we have very strict security that does not allow compilers of
any sort on production desktops - we only have Pentium III development PCs.

Regards,

John Marsland


********************************************************************** 
This is a commercial communication from Commerzbank AG.\ \ T...{{dropped}}



From rossini at blindglobe.net  Wed Dec  3 10:01:12 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Wed, 03 Dec 2003 01:01:12 -0800
Subject: [R] Rd Files?
In-Reply-To: <MABBLJDICACNFOLGIHJOKEJLDPAA.phgrosjean@sciviews.org> (Philippe
	Grosjean's message of "Wed, 3 Dec 2003 09:38:40 +0100")
References: <MABBLJDICACNFOLGIHJOKEJLDPAA.phgrosjean@sciviews.org>
Message-ID: <857k1e8ecn.fsf@blindglobe.net>

"Philippe Grosjean" <phgrosjean at sciviews.org> writes:

> Well, writing a quick and durty help for a function with a few lines of
> comment above or below the function code ("a la Matlab") should be nice. I
> don't think that it should be a good idea to provide a complex alternative
> solution for documenting the functions than the current mechanism which is
> both powerful and efficient (but, of course, a little bit complex). Here is
> a "quick and durty" implementation of a mechanism to include "quick and
> durty" help messages inside the code of an R function. I guess this is
> enough.

That's a nice quicky and dirty solution.  Works in simple cases, but
fails the "works in all cases".  But a 90-percent solution is probably
enough for the task at hand, especially for software limited to
individual deployment.

However, note that it's the basic idea behind the Doxygen framework, 
which does a more robust job of parsing and documenting.

best,
-tony


-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From cvencatasawmy at yahoo.co.uk  Wed Dec  3 11:08:04 2003
From: cvencatasawmy at yahoo.co.uk (=?iso-8859-1?q?Coomaren=20Vencatasawmy?=)
Date: Wed, 3 Dec 2003 10:08:04 +0000 (GMT)
Subject: [R] Simulating correlated distributions
Message-ID: <20031203100804.75903.qmail@web86108.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031203/bd5f8b0d/attachment.pl

From savano at superig.com.br  Wed Dec  3 11:12:18 2003
From: savano at superig.com.br (Savano)
Date: Wed, 3 Dec 2003 08:12:18 -0200
Subject: [R] FONT TYPE
Message-ID: <200312030812.18035.savano@superig.com.br>

Hello,

Can anyone let me know how I might change the font of name on axis in pairs 
plot? I want the font "times new roman".

thanks.



From ripley at stats.ox.ac.uk  Wed Dec  3 12:05:20 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 3 Dec 2003 11:05:20 +0000 (GMT)
Subject: [R] FONT TYPE
In-Reply-To: <200312030812.18035.savano@superig.com.br>
Message-ID: <Pine.LNX.4.44.0312031102210.11281-100000@gannet.stats>

In which OS and on which graphics device?

For postscript/pdf see the `family' argument.

On Windows devices see the Rdevga file (?Rdevga for details) and the 
README.

For X11() on Unix you will need to compile up the R-devel version (see the 
FAQ for how to get it) as it is an upcoming feature.

Please do try to give sufficient details for us to be able to answer your 
question without guessing.


On Wed, 3 Dec 2003, Savano wrote:

> Can anyone let me know how I might change the font of name on axis in pairs 
> plot? I want the font "times new roman".

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From f.calboli at ucl.ac.uk  Wed Dec  3 13:55:19 2003
From: f.calboli at ucl.ac.uk (Federico Calboli)
Date: 03 Dec 2003 12:55:19 +0000
Subject: [R] Re:  question regarding variance components
Message-ID: <1070456118.2950.21.camel@monkey>

Assuming you are measuring Y and you have factor A fixed and factor B
random, I would create a model like:

mod<-lme(Y ~ A, random=~1|B/A, mydata)
VarCorr(mod1)

the term "random=~1|B" tells the model that B is a random factor, adding
the "/A" to get "random =~1|B/A" tells the model you want the
interaction between the fixed and random factors.

VarCorr gives you the variance components of the model.

All is answered much better (and with examples) in Pinheiro and Bates
2000 (it's in the first chapter) and in Crawley 2002.

I have posted a question similar to yours times ago, and got an
excellent reply from Prof Bates; search the archives for it.


If ALL your factors are random try something:

mod<-lme(Y~1,random=~1|A/B, mydata)
VarCorr(mod)

but here I am more guessing than anything. Get Pinheiro and Bates 2000
for this.


Cheers,

Federico


-- 



=================================

Federico C. F. Calboli

PLEASE NOTE NEW ADDRESS

Dipartimento di Biologia
Via Selmi 3
40126 Bologna
Italy

tel (+39) 051 209 4187
fax (+39) 051 251 208

f.calboli at ucl.ac.uk



From thomas.finnie at imperial.ac.uk  Wed Dec  3 13:26:39 2003
From: thomas.finnie at imperial.ac.uk (Finnie, Thomas)
Date: Wed, 3 Dec 2003 12:26:39 -0000
Subject: [R] amap : hclust agglomeration
Message-ID: <7293B801619DA2458B64C069BB25E8A6178ED3@icex31.ic.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031203/064e2cff/attachment.pl

From karlknoblich at yahoo.de  Wed Dec  3 13:46:05 2003
From: karlknoblich at yahoo.de (=?iso-8859-1?q?Karl=20Knoblick?=)
Date: Wed, 3 Dec 2003 13:46:05 +0100 (CET)
Subject: [R] lme: reproducing example
In-Reply-To: <3FCC9A7E.70309@unibas.ch>
Message-ID: <20031203124605.38560.qmail@web10003.mail.yahoo.com>

Thanks!
I think the minor differences taking the values with
rnorm result of the homogen distribution without an
effect. But the results of aov and lme should be
similiar for data with effects, too (at least for
simple and balanced designs).

Karl

 --- "Pascal A. Niklaus" <Pascal.Niklaus at unibas.ch>
schrieb: > Karl Knoblick wrote:
> 
> >Dear R-community!
> >
> >I still have the problem reproducing the following
> >example using lme.
> >
> >id<-factor(rep(rep(1:5,rep(3,5)),3))
> >factA <- factor(rep(c("a1","a2","a3"),rep(15,3)))
> >factB <- factor(rep(c("B1","B2","B3"),15))
> >Y<-numeric(length=45)
> >Y[ 1: 9]<-c(56,52,48,57,54,46,55,51,51)
> >Y[10:18]<-c(58,51,50,54,53,46,54,50,49)
> >Y[19:27]<-c(53,49,48,56,48,52,52,52,50)
> >Y[28:36]<-c(55,51,46,57,49,50,55,51,47)
> >Y[37:45]<-c(56,48,51,58,50,48,58,46,52)
> >df<-data.frame(id, factA, factB, Y) 
> >df.aov <- aov(Y ~ factA*factB + Error(factA:id),
> >data=df)
> >summary(df.aov)
> >
> >Is there a way to get the same results with lme as 
> >with aov with Error()? HOW???
> >
> >One idea was the following:
>
>df$factAid=factor(paste(as.character(df$factA),":",as.character(df$id),sep=""))
> >df.lme <-
>
>lme(Y~factA*factB,df,random=~1|factAid,method="REML")
> 
> >The degrees of freedom look right, but the F values
> >don't match aov.
> >
> >Hope somebody can help! Thanks!!
> >
> >Karl
> >  
> >
> Hmmm, strange, it works if I use factB:id as plot...
> it also works when 
> I use factA:id as plot and replace your Y's by
> random numbers... is this 
> a problem with convergence?
> 
> Pascal
> 
> 
>  > df$Y=rnorm(45)
>  > summary(aov(Y ~ factB*factA +
> Error(id:factA),data=df))
> 
> Error: id:factA
>           Df  Sum Sq Mean Sq F value Pr(>F)
> factA      2  2.9398  1.4699  0.9014 0.4318
> Residuals 12 19.5675  1.6306
> 
> Error: Within
>             Df  Sum Sq Mean Sq F value   Pr(>F)
> factB        2  7.1431  3.5716  7.4964 0.002956 **
> factB:factA  4  4.2411  1.0603  2.2254 0.096377 .
> Residuals   24 11.4345  0.4764
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.'
> 0.1 ` ' 1
> 
>  > anova(lme(Y ~ factB*factA ,data=df, random = ~ 1
> | plot))
>             numDF denDF  F-value p-value
> (Intercept)     1    24 0.014294  0.9058
> factB           2    24 7.496097  0.0030
> factA           2    12 0.901489  0.4318
> factB:factA     4    24 2.225317  0.0964
> 
> Pascal
> 
> 
>  > summary(aov(Y ~ factA*factB + Error(factB:id)))
> 
> Error: factB:id
>           Df Sum Sq Mean Sq F value    Pr(>F)
> factB      2 370.71  185.36  51.488 1.293e-06 ***
> Residuals 12  43.20    3.60
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.'
> 0.1 ` ' 1
> 
> Error: Within
>             Df Sum Sq Mean Sq F value  Pr(>F)
> factA        2  9.911   4.956  1.6248 0.21788
> factA:factB  4 45.556  11.389  3.7341 0.01686 *
> Residuals   24 73.200   3.050
> 
>  > df$plot <- factor(paste(df$factB,df$id))
>  > anova(lme(Y ~ factB*factA , data=df, random = ~1
> | plot))
>             numDF denDF  F-value p-value
> (Intercept)     1    24 33296.02  <.0001
> factB           2    12    51.47  <.0001
> factA           2    24     1.63  0.2178
> factB:factA     4    24     3.73  0.0168
>  >
>



From dmurdoch at pair.com  Wed Dec  3 13:59:04 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Wed, 03 Dec 2003 07:59:04 -0500
Subject: [R] Simulating correlated distributions
In-Reply-To: <20031203100804.75903.qmail@web86108.mail.ukl.yahoo.com>
References: <20031203100804.75903.qmail@web86108.mail.ukl.yahoo.com>
Message-ID: <tdnrsvoseqjfrc2ads2u686njomp67atdk@4ax.com>

On Wed, 3 Dec 2003 10:08:04 +0000 (GMT), you wrote:

>Hi
> 
>How can one simulate correlated distributions in R for windows?

I'm not sure exactly what you're asking, but maybe the MASS function
mvrnorm() is what you want.

Duncan Murdoch



From Lars.Peters at uni-konstanz.de  Wed Dec  3 14:09:33 2003
From: Lars.Peters at uni-konstanz.de (Lars Peters)
Date: Wed, 3 Dec 2003 14:09:33 +0100
Subject: [R] Changing Colors
Message-ID: <LBELKNGGJOINKPAFNOOLCELDCAAA.Lars.Peters@Uni-Konstanz.de>

Hello,

I've got a big problem. I'm using R for geostatistical analyses, especially
the field-package.
I try to generate plots after the kriging process with help of
image.plot(..., col=terrain.colors, ...). Everything works fine, but I want
to reverse the color-palettes (heat.colors, topo.colors or gray()) to get
darkest colors at highest data-values instead the other way round.

Could anyone give me hints or some syntax to resolve that problem??


Thanks and best regards,

Lars Peters


-----
Lars Peters

University of Konstanz
Limnological Institute
D-78457 Konstanz
Germany

phone: +49 (0)7531 88-2930
fax:   +49 (0)7531 88-3533
e-mail: Lars.Peters at Uni-Konstanz.de
http://www.uni-konstanz.de/sfb454/tp_eng/A1/doc/peters/peters.html
http://www.uni-konstanz.de/sfb454/tp_eng/A1/index.htm



From Roger.Bivand at nhh.no  Wed Dec  3 14:21:14 2003
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 3 Dec 2003 14:21:14 +0100 (CET)
Subject: [R] Changing Colors
In-Reply-To: <LBELKNGGJOINKPAFNOOLCELDCAAA.Lars.Peters@Uni-Konstanz.de>
Message-ID: <Pine.LNX.4.44.0312031420470.2128-100000@reclus.nhh.no>

On Wed, 3 Dec 2003, Lars Peters wrote:

> Hello,
> 
> I've got a big problem. I'm using R for geostatistical analyses, especially
> the field-package.
> I try to generate plots after the kriging process with help of
> image.plot(..., col=terrain.colors, ...). Everything works fine, but I want
> to reverse the color-palettes (heat.colors, topo.colors or gray()) to get
> darkest colors at highest data-values instead the other way round.
> 
> Could anyone give me hints or some syntax to resolve that problem??

rev()?

> 
> 
> Thanks and best regards,
> 
> Lars Peters
> 
> 
> -----
> Lars Peters
> 
> University of Konstanz
> Limnological Institute
> D-78457 Konstanz
> Germany
> 
> phone: +49 (0)7531 88-2930
> fax:   +49 (0)7531 88-3533
> e-mail: Lars.Peters at Uni-Konstanz.de
> http://www.uni-konstanz.de/sfb454/tp_eng/A1/doc/peters/peters.html
> http://www.uni-konstanz.de/sfb454/tp_eng/A1/index.htm
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From apv at capital.net  Wed Dec  3 14:26:05 2003
From: apv at capital.net (Arend P. van der Veen)
Date: 03 Dec 2003 08:26:05 -0500
Subject: [R] Vector Assignments
In-Reply-To: <20031202053230.6F3153957@mprdmxin.myway.com>
References: <20031202053230.6F3153957@mprdmxin.myway.com>
Message-ID: <1070457965.3446.9.camel@redtail.mydomain.home>

Your recommendations have worked great.  I have found both cut and
ifelse to be useful.

I have one more question. When should I use factors over a character
vector.  I know that they have different uses.  However, I am still
trying to figure out how I can best take advantage of factors. 

The following is what I am really trying to do:

colors <- c("red","blue","green","black")
y.col <- colors[cut(y,c(-Inf,250,500,700,Inf),right=F,lab=F)]
plot(x,y,col=y.col)

Would using factors make this any cleaner?  I think a character vector
is all I need but I thought I would ask.

Thanks for your help,
Arend van der Veen



On Tue, 2003-12-02 at 00:32, Gabor Grothendieck wrote:
> And one other thing.  Are you sure you want character variables
> as the result of all this?  A column whose entries are each one
> of four colors seems like a good job for a factor:
> 
> colours <- c("red", "blue", "green","black")
> cut(x, c(-Inf,250,500,700,Inf),right=F,lab=colours)
> 
> 
> 
> ---
> Date: Mon, 1 Dec 2003 23:47:39 -0500 (EST) 
> From: Gabor Grothendieck <ggrothendieck at myway.com>
> To: <h.wickham at auckland.ac.nz>, <apv at capital.net> 
> Cc: <r-help at stat.math.ethz.ch> 
> Subject: Re: [R] Vector Assignments 
> 
>  
> 
> 
> 
> Just some small refinements/corrections:
> 
> colours <- c("red", "blue", "green","back")
> colours[cut(x, c(-Inf,250,500,700,Inf),right=F,lab=F)]
> 
> ---
> Date: Tue, 02 Dec 2003 14:38:55 +1300 
> From: Hadley Wickham <h.wickham at auckland.ac.nz>
> To: Arend P. van der Veen <apv at capital.net> 
> Cc: R HELP <r-help at stat.math.ethz.ch> 
> Subject: Re: [R] Vector Assignments 
> 
> 
> 
> One way would be to create a vector of colours and then cut() to index 
> the vector:
> 
> colours <- c("red", "blue", "green","back")
> colours[cut(x, c(min(x),250,500,700,max(x)),lab=F)]
> 
> Hadley
> 
> 
> Arend P. van der Veen wrote:
> 
> >Hi,
> >
> >I have simple R question. 
> >
> >I have a vector x that contains real numbers. I would like to create
> >another vector col that is the same length of x such that:
> >
> >if x[i] < 250 then col[i] = "red"
> >else if x[i] < 500 then col[i] = "blue"
> >else if x[i] < 750 then col[i] = "green"
> >else col[i] = "black" for all i
> >
> >I am convinced that there is probably a very efficient way to do this in
> >R but I am not able to figure it out. Any help would be greatly
> >appreciated.
> >
> >Thanks in advance,
> >Arend van der Veen
> >
> 
> 
> _______________________________________________
> No banners. No pop-ups. No kidding.
> Introducing My Way - http://www.myway.com
>



From Arne.Muller at aventis.com  Wed Dec  3 14:34:32 2003
From: Arne.Muller at aventis.com (Arne.Muller@aventis.com)
Date: Wed, 3 Dec 2003 14:34:32 +0100
Subject: [R] multidimensional Fisher or Chi square test
Message-ID: <C80ECAFA2ACC1B45BE45D133ED660ADE010BF10B@crbsmxsusr04.pharma.aventis.com>

Hello,

Is there a test for independence available based on a multidimensional
contingency table?

I've about 300 processes, and for each of them I get numbers for failures and
successes. I've two or more conditions under which I test these processes.

If I had just one process to test I could just perform a fisher or chisquare
test on a 2x2 contigency table, like this:

for one process:
        conditionA	conditionB
ok      20			6
failed  190			156

>From the table I can figure out if the outcome (ok/failed) is bound to one of
the conditions for a process. However, I'd like to know how different the 2
conditions are from each other considering all 300 processes, and I consider
the processes to be an additional dimension. 

My H0 is that both conditions are overall (considering all processes) the
same.

Could you give me a hint what kind of test of package I should look into?

	kind regars + thanks for your help,

	Arne



From veronique.verhoeven at ua.ac.be  Wed Dec  3 14:35:24 2003
From: veronique.verhoeven at ua.ac.be (Veronique Verhoeven)
Date: Wed, 3 Dec 2003 14:35:24 +0100
Subject: [R] intraclass correlation
Message-ID: <005d01c3b9a2$506bfa00$ce02a98f@uia.ac.be>

Hi,


Can R calculate an intraclass correlation coefficient for clustered data,
when the outcome variable is dichotomous?
By now I calculate it by hand, estimating between- and intracluster variance
by one-way ANOVA - however I don't feel very comfortable about this, since
the distributional assumptions are not really met....
Maybe anyone can help me?

Best regards and many many thanks,

Veronique Verhoeven
University of Antwerp



From ggrothendieck at myway.com  Wed Dec  3 14:51:57 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed,  3 Dec 2003 08:51:57 -0500 (EST)
Subject: [R] Vector Assignments
Message-ID: <20031203135157.299313980@mprdmxin.myway.com>



If you were using the colours in a model matrix
using a factor would be best but since your case
is plotting using characters is best.


 
Date: 03 Dec 2003 08:26:05 -0500 
From: Arend P. van der Veen <apv at capital.net>
To: R HELP <r-help at stat.math.ethz.ch> 
Subject: Re: [R] Vector Assignments 

 
 
Your recommendations have worked great. I have found both cut and
ifelse to be useful.

I have one more question. When should I use factors over a character
vector. I know that they have different uses. However, I am still
trying to figure out how I can best take advantage of factors. 

The following is what I am really trying to do:

colors <- c("red","blue","green","black")
y.col <- colors[cut(y,c(-Inf,250,500,700,Inf),right=F,lab=F)]
plot(x,y,col=y.col)

Would using factors make this any cleaner? I think a character vector
is all I need but I thought I would ask.

Thanks for your help,
Arend van der Veen



On Tue, 2003-12-02 at 00:32, Gabor Grothendieck wrote:
> And one other thing. Are you sure you want character variables
> as the result of all this? A column whose entries are each one
> of four colors seems like a good job for a factor:
> 
> colours <- c("red", "blue", "green","black")
> cut(x, c(-Inf,250,500,700,Inf),right=F,lab=colours)
> 
> 
> 
> ---
> Date: Mon, 1 Dec 2003 23:47:39 -0500 (EST) 
> From: Gabor Grothendieck <ggrothendieck at myway.com>
> To: <h.wickham at auckland.ac.nz>, <apv at capital.net> 
> Cc: <r-help at stat.math.ethz.ch> 
> Subject: Re: [R] Vector Assignments 
> 
> 
> 
> 
> 
> Just some small refinements/corrections:
> 
> colours <- c("red", "blue", "green","back")
> colours[cut(x, c(-Inf,250,500,700,Inf),right=F,lab=F)]
> 
> ---
> Date: Tue, 02 Dec 2003 14:38:55 +1300 
> From: Hadley Wickham <h.wickham at auckland.ac.nz>
> To: Arend P. van der Veen <apv at capital.net> 
> Cc: R HELP <r-help at stat.math.ethz.ch> 
> Subject: Re: [R] Vector Assignments 
> 
> 
> 
> One way would be to create a vector of colours and then cut() to index 
> the vector:
> 
> colours <- c("red", "blue", "green","back")
> colours[cut(x, c(min(x),250,500,700,max(x)),lab=F)]
> 
> Hadley
> 
> 
> Arend P. van der Veen wrote:
> 
> >Hi,
> >
> >I have simple R question. 
> >
> >I have a vector x that contains real numbers. I would like to create
> >another vector col that is the same length of x such that:
> >
> >if x[i] < 250 then col[i] = "red"
> >else if x[i] < 500 then col[i] = "blue"
> >else if x[i] < 750 then col[i] = "green"
> >else col[i] = "black" for all i
> >
> >I am convinced that there is probably a very efficient way to do this in
> >R but I am not able to figure it out. Any help would be greatly
> >appreciated.
> >
> >Thanks in advance,
> >Arend van der Veen
> >
> 
> 
> _______________________________________________
> No banners. No pop-ups. No kidding.
> Introducing My Way - http://www.myway.com
>



From tblackw at umich.edu  Wed Dec  3 14:56:38 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Wed, 3 Dec 2003 08:56:38 -0500 (EST)
Subject: [R] reason for Factors -- was -- Vector Assignments
In-Reply-To: <1070457965.3446.9.camel@redtail.mydomain.home>
References: <20031202053230.6F3153957@mprdmxin.myway.com>
	<1070457965.3446.9.camel@redtail.mydomain.home>
Message-ID: <Pine.SOL.4.58.0312030846570.21693@robotron.gpcc.itd.umich.edu>


On Wed, 3 Dec 2003, Arend P. van der Veen wrote:

> Your recommendations have worked great.  I have found both cut and
> ifelse to be useful.
>
> I have one more question. When should I use factors over a character
> vector.  I know that they have different uses.  However, I am still
> trying to figure out how I can best take advantage of factors.
>
> The following is what I am really trying to do:
>
> colors <- c("red","blue","green","black")
> y.col <- colors[cut(y,c(-Inf,250,500,700,Inf),right=F,lab=F)]
> plot(x,y,col=y.col)
>
> Would using factors make this any cleaner?  I think a character vector
> is all I need but I thought I would ask.
>
> Thanks for your help,
> Arend van der Veen

Arend  -

When setting the colors of plotted points, you definitely want
a vector of character strings as the color names.  "Factor" was
invented so that regression and analysis of variance functions
would properly recognize a grouping variable and not fit simply
a linear coefficient to the integer codes.  In the context of a
linear (or similar) model, each factor or interaction has to be
expanded from a single column of integer codes into a matrix of
[0,1] indicator variables, with a separate column for each possible
level of the factor.  (I oversimplify a bit here: some columns
are omitted, to keep the design matrix from being over-specified.)

-  tom blackwell  -  u michigan medical school  -  ann arbor  -



From tblackw at umich.edu  Wed Dec  3 15:06:29 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Wed, 3 Dec 2003 09:06:29 -0500 (EST)
Subject: [R] amap : hclust agglomeration
In-Reply-To: <7293B801619DA2458B64C069BB25E8A6178ED3@icex31.ic.ac.uk>
References: <7293B801619DA2458B64C069BB25E8A6178ED3@icex31.ic.ac.uk>
Message-ID: <Pine.SOL.4.58.0312030901180.21693@robotron.gpcc.itd.umich.edu>

Thomas  -

"sup" stands for "supremum" or "maximum".  The criterion for
complete linkage clustering is that the two groups with the
smallest maximum distance between any of their members will
be joined at each stage.

(I dare say you will have recieved many similar responses
already, but none on-list, so I venture to add one more.)

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Wed, 3 Dec 2003, Finnie, Thomas wrote:

> Hi,
>
> I'm trying to understand the complete linkage method in hclust.
> Can anyone provide a breakdown of the formula (p9 of the pdf
> documentation) or tell me what the "sup" operator does/means?
>
> thanks in advance
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From bruno at speech.kth.se  Wed Dec  3 15:32:40 2003
From: bruno at speech.kth.se (Bruno Giordano)
Date: Wed, 3 Dec 2003 15:32:40 +0100
Subject: [R] non-uniqueness in cluster analysis
Message-ID: <009001c3b9aa$4e00ccc0$ba43ed82@brungio>

Hi,
I'm clustering objects defined by categorical variables with a hierarchical
algorithm - average linkage.
My distance matrix (general dissimilarity coefficient) includes several
distances with exactly the same values.
As I see, a standard agglomerative procedure ignores this problems, simply
selecting, above equal distances, the one that comes first.
For this reason the analysis in output depends strongly on the orderings of
the objects within the raw data matrix.
Is there a standard procedure to deal with this?
Thanks
    Bruno



From christian.schulz at questico.de  Wed Dec  3 15:42:08 2003
From: christian.schulz at questico.de (Christian Schulz)
Date: Wed, 3 Dec 2003 15:42:08 +0100
Subject: [R] Error in randomForest.default(m, y,
	...) : negative length vectors are not allowed
Message-ID: <JAEELBHBOPKJDMMCNHKMMENJCBAA.christian.schulz@questico.de>

Hi,

what i'm doing wrong?
I'm using a data.frame with ~ 90.000 instances
and 7 attributes, 5 are binary recoded 
1 independend variable are a real one 
and the target is a real one,too.

The distributions are not very skewed in the dummy variables
,but in the real variables are ~ 60.000 
zero values instances, but zero means
no money is payed and is a important value!

Many thanks for help & suggestions,
regards,christian



From dd-list at nomans.de  Wed Dec  3 15:46:48 2003
From: dd-list at nomans.de (Dennis Alexis Valin Dittrich)
Date: Wed, 03 Dec 2003 15:46:48 +0100
Subject: [R] multidimensional Fisher or Chi square test
In-Reply-To: <C80ECAFA2ACC1B45BE45D133ED660ADE010BF10B@crbsmxsusr04.pharma.aventis.com>
References: <C80ECAFA2ACC1B45BE45D133ED660ADE010BF10B@crbsmxsusr04.pharma.aventis.com>
Message-ID: <1070462745.32196.10.camel@private.nomans.n2411.de>

On Wed, 2003-12-03 at 14:34, Arne.Muller at aventis.com wrote:
> Is there a test for independence available based on a multidimensional
> contingency table?
> I've about 300 processes, and for each of them I get numbers for failures and
> successes. I've two or more conditions under which I test these processes.

You may look for ?mantelhaen.test 

Dennis



From tblackw at umich.edu  Wed Dec  3 15:49:18 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Wed, 3 Dec 2003 09:49:18 -0500 (EST)
Subject: [R] non-uniqueness in cluster analysis
In-Reply-To: <009001c3b9aa$4e00ccc0$ba43ed82@brungio>
References: <009001c3b9aa$4e00ccc0$ba43ed82@brungio>
Message-ID: <Pine.SOL.4.58.0312030941380.21693@robotron.gpcc.itd.umich.edu>

Bruno  -

Many people add a tiny random number to each of the distances,
or deliberately randomize the input order.  This means that
any clustering is not reproducible, unless you go back to the
original randoms, but it forces you not to pay attention to
minor differences.

Ah, I think you're asking about bootstrap confidence intervals
for the set of descendants from each interior vertex.  This is
certainly routine procedure when inferring evolutionary trees,
but I'm not sure any of that code has been re-implemented in R
or Splus.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Wed, 3 Dec 2003, Bruno Giordano wrote:

> Hi,
> I'm clustering objects defined by categorical variables with a hierarchical
> algorithm - average linkage.
> My distance matrix (general dissimilarity coefficient) includes several
> distances with exactly the same values.
> As I see, a standard agglomerative procedure ignores this problems, simply
> selecting, above equal distances, the one that comes first.
> For this reason the analysis in output depends strongly on the orderings of
> the objects within the raw data matrix.
> Is there a standard procedure to deal with this?
> Thanks
>     Bruno



From ripley at stats.ox.ac.uk  Wed Dec  3 15:53:22 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 3 Dec 2003 14:53:22 +0000 (GMT)
Subject: [R] non-uniqueness in cluster analysis
In-Reply-To: <009001c3b9aa$4e00ccc0$ba43ed82@brungio>
Message-ID: <Pine.LNX.4.44.0312031452540.13706-100000@gannet.stats>

On Wed, 3 Dec 2003, Bruno Giordano wrote:

> Hi,
> I'm clustering objects defined by categorical variables with a hierarchical
> algorithm - average linkage.
> My distance matrix (general dissimilarity coefficient) includes several
> distances with exactly the same values.
> As I see, a standard agglomerative procedure ignores this problems, simply
> selecting, above equal distances, the one that comes first.
> For this reason the analysis in output depends strongly on the orderings of
> the objects within the raw data matrix.
> Is there a standard procedure to deal with this?

Don't use average linkage!

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From fm3a004 at math.uni-hamburg.de  Wed Dec  3 16:19:42 2003
From: fm3a004 at math.uni-hamburg.de (Christian Hennig)
Date: Wed, 3 Dec 2003 16:19:42 +0100 (MET)
Subject: [R] non-uniqueness in cluster analysis
In-Reply-To: <009001c3b9aa$4e00ccc0$ba43ed82@brungio>
Message-ID: <Pine.GSO.3.95q.1031203160216.29986E-100000@sun35.math.uni-hamburg.de>

Hi,

Brian Ripley already replied "don't use average linkage"... You
may think about k-medoid (pam) in package cluster instead.
However, often average linkage is not such a bad choice, and if you really
want to use it for your data, you may try the following:
Among the hierarchical methods, single linkage has the smallest problem
with equal distances, because possible agglomerations based on equal
distances between clusters are all carried out regardless of the order.
If at some step the smallest between cluster-distance 
is d(a,b)=d(a,c)<d(b,c), it may happen that a and b are merged first, or
a and c are merged first, but before merging anything else with distance
larger than d(a,b), a, b *and* c are merged. Thus, you have order
dependence only between the steps where you merge clusters with the same
distance, but not afterwards.

If your problem occurs only at a low level of agglomeration
(and you don't have
situations where d(a,b) and d(a,c) are small and d(b,c) is very large; I 
do not know if the triangle inequality holds for your data), you may do
some first steps with Single Linkage and then continue with average
linkage (I haven't thought about if this can be done in R without extra
effort). 

But if you have already observed that the averarge linkage outcome depends
critically (from the viewpoint of interpretation) on the order of points, 
then it seems that you are in an unstable situation, if you are able to
define a unique clustering or not.

Christian

On Wed, 3 Dec 2003, Bruno Giordano wrote:

> Hi,
> I'm clustering objects defined by categorical variables with a hierarchical
> algorithm - average linkage.
> My distance matrix (general dissimilarity coefficient) includes several
> distances with exactly the same values.
> As I see, a standard agglomerative procedure ignores this problems, simply
> selecting, above equal distances, the one that comes first.
> For this reason the analysis in output depends strongly on the orderings of
> the objects within the raw data matrix.
> Is there a standard procedure to deal with this?
> Thanks
>     Bruno
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

***********************************************************************
Christian Hennig
Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
#######################################################################
ich empfehle www.boag-online.de



From bruno at speech.kth.se  Wed Dec  3 16:53:01 2003
From: bruno at speech.kth.se (Bruno Giordano)
Date: Wed, 3 Dec 2003 16:53:01 +0100
Subject: [R] non-uniqueness in cluster analysis
References: <009001c3b9aa$4e00ccc0$ba43ed82@brungio>
	<Pine.SOL.4.58.0312030941380.21693@robotron.gpcc.itd.umich.edu>
Message-ID: <00dc01c3b9b5$8851fec0$ba43ed82@brungio>

What I did was, in presence of equal values distances, to randomize the
selection of them, and compute the distortion of the solution using
cophenetic correlation.
I computed 10000 "random" trees for each of three methods: average, single
and complete linkage.
Among the "randomly" selected solutions, for the three methods, average
linkage was able to give the highest cophenetic correlation, followed by
complete and then by single linkage. Among the "random" trees single
linkage, for obvious reasons, gave a constant cophenetic correlation.
My data set is rather small (25 objects). I'm seriously thinking of
calculating all the possible solutions (I guess about 30000), picking the
ones that give the highest cophenetic correlation, and analyzing the
consistency among those solutions, after establishing a "natural" number of
clusters.

    Bruno



From jmc at research.bell-labs.com  Wed Dec  3 17:03:50 2003
From: jmc at research.bell-labs.com (John Chambers)
Date: Wed, 03 Dec 2003 11:03:50 -0500
Subject: [R] setMethod("min", "myclass", ...)
References: <Pine.LNX.4.44.0312021845550.3778-100000@spock.vulcan>
Message-ID: <3FCE0966.E2720D2E@research.bell-labs.com>

Thomas Stabla wrote:
> 
> Hello,
> 
> I have defined a new class
> 
> > setClass("myclass", representation(min = "numeric", max = "numeric"))
> 
> and want to write accessor functions, so that for
> 
> > foo = new("myclass", min = 0, max = 1)
> > min(foo) # prints 0
> > max(foo) # prints 1
> 
> At first i created a generic function for "min"
> 
> > setGeneric("min", function(..., na.rm = FALSE) standardGeneric("min"))
> 
> and then tried to use setMethod. And there's my problem, I don't know the
> name of the first argument which is to be passed to "min".
> I can't just write:
> 
> > setMethod("min", "myclass", function(..., na.rm = FALSE) ... at min)
> 
> The same problem occurs with "max".

Generally, it's not a good idea to take a well-known function name and
make it into an accessor function.

In a functional language, basic function calls such as min(x), sin(x),
etc. should have a natural interpretation.  Defining methods for these
functions is meant to do what it says:  provide a method to achieve the
general purpose of the function for particular objects.

If you want accessor functions, they should probably have names that
make their purpose obvious.  One convention, a la Java properties, would
be getMin(x), etc. (the capitalizing is potentially an issue since slot
names are case sensitive).

If you do really want a method for the min() function for myclass,
that's a different problem.

The argument "..." is different from all other argument names, and it
can't be used in a signature.  To define methods for functions such as
min(), the formal arguments of the basic function would have to be
changed to, say, function(x, ..., na.rm)

Then methods can be defined for argument "x".

The R versions of these functions don't currently allow methods.  If
methods are needed, a package could currently define its own version of
the (non-generic) functions to include argument "x".

More than this is needed to handle multiple arguments generally.  The
problem is that defining a method for argument "x" does not cause that
method to be called if the object appears as a later argument.

If you had a method for min() for "myclass" and myX was an object from
that class
  min(myX, 1)
would work, but
  min(1, myX)
would not.  To get all examples right would require changes to the basic
code for these functions.  There is a brief discussion of one approach
in "Programming with Data", pages 343 and 351.






> 
> Thanks for your help.
> 
> Greetings,
> Thomas Stabla
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-- 
John M. Chambers                  jmc at bell-labs.com
Bell Labs, Lucent Technologies    office: (908)582-2681
700 Mountain Avenue, Room 2C-282  fax:    (908)582-3340
Murray Hill, NJ  07974            web: http://www.cs.bell-labs.com/~jmc



From clists at perrin.socsci.unc.edu  Wed Dec  3 17:06:48 2003
From: clists at perrin.socsci.unc.edu (Andrew Perrin)
Date: Wed, 3 Dec 2003 11:06:48 -0500 (EST)
Subject: [R] intraclass correlation
In-Reply-To: <005d01c3b9a2$506bfa00$ce02a98f@uia.ac.be>
References: <005d01c3b9a2$506bfa00$ce02a98f@uia.ac.be>
Message-ID: <Pine.LNX.4.53.0312031105580.5277@perrin.socsci.unc.edu>

I have been using a little function I wrote myself; look at
http://www.unc.edu/home/aperrin/tips/src/icc.R for the code.  Not pretty,
but it works.

ap

----------------------------------------------------------------------
Andrew J Perrin - http://www.unc.edu/~aperrin
Assistant Professor of Sociology, U of North Carolina, Chapel Hill
clists at perrin.socsci.unc.edu * andrew_perrin (at) unc.edu


On Wed, 3 Dec 2003, Veronique Verhoeven wrote:

> Hi,
>
>
> Can R calculate an intraclass correlation coefficient for clustered data,
> when the outcome variable is dichotomous?
> By now I calculate it by hand, estimating between- and intracluster variance
> by one-way ANOVA - however I don't feel very comfortable about this, since
> the distributional assumptions are not really met....
> Maybe anyone can help me?
>
> Best regards and many many thanks,
>
> Veronique Verhoeven
> University of Antwerp
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From matthew_wiener at merck.com  Wed Dec  3 17:26:18 2003
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Wed, 03 Dec 2003 11:26:18 -0500
Subject: [R] Error in randomForest.default(m, y, ...) : negative
	lengt	h vectors are not allowed
Message-ID: <AEBD81486231A343B1813FE62D335225077D1F3A@usrymx15.merck.com>

Christian -- 

You don't provide enough information (like a call) to answer this.  I
suspect, though, that you may be subsetting in a way that passes
randomForest no data.

I'm not aware offhand of an easy way to get this error from randomForest.  I
tried creating some data superficially similar to yours to see whether
something would break if there were only a single value in the variable to
be explained, but everything worked fine (though it does give a reasonable
warning).

> test.dat <- data.frame(a = rep(0, 1000), b = runif(1000), c = sample(0:1,
1000, replace = TRUE, p = c(.8, .2))
> t8 <- randomForest(a ~ b + c, data = test.dat)
Warning message: 
The response has five or fewer unique values.  Are you sure you want to do
regression? in: randomForest.default(m, y, ...) 
> test.dat[sample(1:1000, 100),"a"] <- runif(100, 1, 200)
> t8 <- randomForest(a ~ b + c, data = test.dat)

Some other generated data might come up with the error, but I'd bet on the
subsetting problem.

Hope this helps,  -Matt

Matthew Wiener
RY84-202
Applied Computer Science & Mathematics Dept.
Merck Research Labs
126 E. Lincoln Ave.
Rahway, NJ 07065
732-594-5303 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Christian Schulz
Sent: Wednesday, December 03, 2003 9:42 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Error in randomForest.default(m, y, ...) : negative length
vectors are not allowed


Hi,

what i'm doing wrong?
I'm using a data.frame with ~ 90.000 instances
and 7 attributes, 5 are binary recoded 
1 independend variable are a real one 
and the target is a real one,too.

The distributions are not very skewed in the dummy variables
,but in the real variables are ~ 60.000 
zero values instances, but zero means
no money is payed and is a important value!

Many thanks for help & suggestions,
regards,christian

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From tmurph6 at po-box.mcgill.ca  Wed Dec  3 17:53:44 2003
From: tmurph6 at po-box.mcgill.ca (Tanya Murphy)
Date: Wed, 3 Dec 2003 11:53:44 -0500
Subject: [R] HMisc describe -- error with dates
Message-ID: <3FD6016D@oldwebmail.mcgill.ca>

Thank you Frank and Gabor for the fixes and checking and rechecking! 
Everything seems to work well with the Hmisc functions tried--upData, describe 
and summary.

To summarize:
1. Add the testDateTime and formatDateTime functions (copied from Frank's 
messages) to the Hmisc file (or run prior to loading Hmisc)


testDateTime <- function(x, what=c('either','both','timeVaries')) {
  what <- match.arg(what)
  cl <- class(x) # was oldClass 22jun03
  if(!length(cl)) return(FALSE)

  dc <- if(.R.) c('POSIXt','POSIXct','dates','times','chron') else
  c('timeDate','date','dates','times','chron')
  dtc <- if(.R.) c('POSIXt','POSIXct','chron') else
  c('timeDate','chron')
  switch(what,
  either = any(cl %in% dc),
  both = any(cl %in% dtc),
  timeVaries = {
  if('chron' %in% cl || !.R.) { ## chron or S+ timeDate
  y <- as.numeric(x)
  length(unique(round(y - floor(y),13))) > 1
  } else if(.R.) length(unique(format(x,'%H%M%S'))) > 1 else
  FALSE
  })
  }

formatDateTime <- function(x, at, roundDay=FALSE) {
cl <- at$class
w <- if(any(cl %in% c('chron','dates','times'))) {
attributes(x) <- at
fmt <- at$format
if(roundDay) {
if(length(fmt)==2 && is.character(fmt))
format.dates(x, fmt[1]) else format.dates(x)
} else x
} else if(.R.) {
attributes(x) <- at
if(roundDay) as.POSIXct(round(x, 'days')) else x
} else timeDate(julian=if(roundDay)round(x) else x)
format(w)
}

2. Replace the decribe function with the new one (available as an attachment 
in Frank's most recent message on the subject). Instead of editing the 
original Hmisc file, this could be run after the Hmisc library is loaded.

Right?


Tanya



From christian.schulz at questico.de  Wed Dec  3 18:22:51 2003
From: christian.schulz at questico.de (Christian Schulz)
Date: Wed, 3 Dec 2003 18:22:51 +0100
Subject: AW: [R] Error in randomForest.default(m, y,
	...) : negative length vectors are not allowed
In-Reply-To: <AEBD81486231A343B1813FE62D335225077D1F3A@usrymx15.merck.com>
Message-ID: <JAEELBHBOPKJDMMCNHKMMENLCBAA.christian.schulz@questico.de>

Hmmm, thanks for your suggestions i'm in the
same opinion with any subsetting problem, but curious is
that my model i.e. with library(gbm) or simple lm works,
because my task is to find out the weights/importance values
for the attributes and i would like compare the results between
the randomForest classifier and a linear approach.

I check it with your suggestions and code snippets in detail
and feedback you the problem, if i found the solution.

regards,Christian



-----Urspr?ngliche Nachricht-----
Von: Wiener, Matthew [mailto:matthew_wiener at merck.com]
Gesendet: Mittwoch, 3. Dezember 2003 17:26
An: 'Christian Schulz'; r-help at stat.math.ethz.ch
Betreff: RE: [R] Error in randomForest.default(m, y, ...) : negative
lengt h vectors are not allowed


Christian --

You don't provide enough information (like a call) to answer this.  I
suspect, though, that you may be subsetting in a way that passes
randomForest no data.

I'm not aware offhand of an easy way to get this error from randomForest.  I
tried creating some data superficially similar to yours to see whether
something would break if there were only a single value in the variable to
be explained, but everything worked fine (though it does give a reasonable
warning).

> test.dat <- data.frame(a = rep(0, 1000), b = runif(1000), c = sample(0:1,
1000, replace = TRUE, p = c(.8, .2))
> t8 <- randomForest(a ~ b + c, data = test.dat)
Warning message:
The response has five or fewer unique values.  Are you sure you want to do
regression? in: randomForest.default(m, y, ...)
> test.dat[sample(1:1000, 100),"a"] <- runif(100, 1, 200)
> t8 <- randomForest(a ~ b + c, data = test.dat)

Some other generated data might come up with the error, but I'd bet on the
subsetting problem.

Hope this helps,  -Matt

Matthew Wiener
RY84-202
Applied Computer Science & Mathematics Dept.
Merck Research Labs
126 E. Lincoln Ave.
Rahway, NJ 07065
732-594-5303

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Christian Schulz
Sent: Wednesday, December 03, 2003 9:42 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Error in randomForest.default(m, y, ...) : negative length
vectors are not allowed


Hi,

what i'm doing wrong?
I'm using a data.frame with ~ 90.000 instances
and 7 attributes, 5 are binary recoded
1 independend variable are a real one
and the target is a real one,too.

The distributions are not very skewed in the dummy variables
,but in the real variables are ~ 60.000
zero values instances, but zero means
no money is payed and is a important value!

Many thanks for help & suggestions,
regards,christian

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From rxg218 at psu.edu  Wed Dec  3 19:06:10 2003
From: rxg218 at psu.edu (Rajarshi Guha)
Date: 03 Dec 2003 13:06:10 -0500
Subject: [R] checking for identical columns in a mxn matrix
Message-ID: <1070474770.21090.10.camel@ra.chem.psu.edu>

Hi,
  I have a rectangular matrix and I need to check whether any columns
are identical or not. Currently I'm looping over the columns and
checking each column with all the others with identical().

However, as experience has shown me, getting rid of loops is a good idea
:) Would anybody have any suggestions as to how I could do this job more
efficiently.

(It would be nice to know which columns are identical but thats not a
necessity.)

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
Entropy isn't what it used to be.



From rxg218 at psu.edu  Wed Dec  3 19:17:42 2003
From: rxg218 at psu.edu (Rajarshi Guha)
Date: 03 Dec 2003 13:17:42 -0500
Subject: [R] nameless functions in R
Message-ID: <1070475462.21090.16.camel@ra.chem.psu.edu>

Hi,
  I have an apply statement that looks like:

> check.cols <- function(v1, v2) {
+     return( identical(v1,v2) );
+ }
> x
     [,1] [,2] [,3]
[1,]    1    3    3
[2,]    4    5    4
[3,]    2    7    6
> apply(x, c(2), check.cols, v2=c(7,8,9))
[1] FALSE FALSE FALSE

Is it possible to make the function check.cols() inline to the apply
statement. Some thing like this:

apply(x, c(2), funtion(v1,v2){ identical(v1,v2) }, v2=c(1,4,2))

The above gives me a syntax error. I also tried:

apply(x, c(2), fun <- funtion(v1,v2){ return(identical(v1,v2)) },
v2=c(1,4,2))

and I still get a syntax error.

Is this type of syntax allowed in R?

Thanks,

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
He who is in love with himself has at least this advantage -- he won't
encounter many rivals.
-- Georg Lichtenberg, "Aphorisms"



From rxg218 at psu.edu  Wed Dec  3 19:23:37 2003
From: rxg218 at psu.edu (Rajarshi Guha)
Date: 03 Dec 2003 13:23:37 -0500
Subject: [R] checking for identical columns in a mxn matrix
In-Reply-To: <Pine.LNX.4.33.0312031316380.21285-100000@penguin.rand.org>
References: <Pine.LNX.4.33.0312031316380.21285-100000@penguin.rand.org>
Message-ID: <1070475817.21090.19.camel@ra.chem.psu.edu>

On Wed, 2003-12-03 at 13:18, J.R. Lockwood wrote:

> list will come up with something clever.  the other issues is that you
> need to be careful when doing equality comparisons with floating point
> numbers.  unless your matrix consists of characters or integers,
> you'll need to think about some level of numerical tolerance of your
> comparison.

Yes, the matrix will always be integer.

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
All theoretical chemistry is really physics; and all theoretical
chemists 
know it.
-- Richard P. Feynman



From MSchwartz at medanalytics.com  Wed Dec  3 19:33:05 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Wed, 03 Dec 2003 12:33:05 -0600
Subject: [R] checking for identical columns in a mxn matrix
In-Reply-To: <1070474770.21090.10.camel@ra.chem.psu.edu>
References: <1070474770.21090.10.camel@ra.chem.psu.edu>
Message-ID: <1070476384.4985.69.camel@localhost.localdomain>

On Wed, 2003-12-03 at 12:06, Rajarshi Guha wrote:
> Hi,
>   I have a rectangular matrix and I need to check whether any columns
> are identical or not. Currently I'm looping over the columns and
> checking each column with all the others with identical().
> 
> However, as experience has shown me, getting rid of loops is a good idea
> :) Would anybody have any suggestions as to how I could do this job more
> efficiently.
> 
> (It would be nice to know which columns are identical but thats not a
> necessity.)


If your matrix is 'x' and contains text and/or integer values (since
float comparisons can be problematic) you can use:

any(duplicated(x, MARGIN = 2))

to find out if any of the columns are duplicated and  

which(duplicated(x, MARGIN = 2))

to get the column numbers that are duplicates in the matrix.

If you want to extract the unique columns, you can use:

unique(x, MARGIN = 2)

See ?duplicated and ?unique for more information.

Example:

> x <- matrix(c(1:3, 4:6, 1:3, 7:9), ncol = 4)
> x
     [,1] [,2] [,3] [,4]
[1,]    1    4    1    7
[2,]    2    5    2    8
[3,]    3    6    3    9

> any(duplicated(x, MARGIN = 2))
[1] TRUE

> which(duplicated(x, MARGIN = 2))
[1] 3

> unique(x, MARGIN = 2)
     [,1] [,2] [,3]
[1,]    1    4    7
[2,]    2    5    8
[3,]    3    6    9

HTH,

Marc Schwartz



From tlumley at u.washington.edu  Wed Dec  3 19:39:16 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 3 Dec 2003 10:39:16 -0800 (PST)
Subject: [R] nameless functions in R
In-Reply-To: <1070475462.21090.16.camel@ra.chem.psu.edu>
References: <1070475462.21090.16.camel@ra.chem.psu.edu>
Message-ID: <Pine.A41.4.58.0312031038320.71294@homer09.u.washington.edu>

On Wed, 3 Dec 2003, Rajarshi Guha wrote:

> Hi,
>   I have an apply statement that looks like:
>
> > check.cols <- function(v1, v2) {
> +     return( identical(v1,v2) );
> + }
> > x
>      [,1] [,2] [,3]
> [1,]    1    3    3
> [2,]    4    5    4
> [3,]    2    7    6
> > apply(x, c(2), check.cols, v2=c(7,8,9))
> [1] FALSE FALSE FALSE
>
> Is it possible to make the function check.cols() inline to the apply
> statement. Some thing like this:
>
> apply(x, c(2), funtion(v1,v2){ identical(v1,v2) }, v2=c(1,4,2))
>
> The above gives me a syntax error. I also tried:
>
> apply(x, c(2), fun <- funtion(v1,v2){ return(identical(v1,v2)) },
> v2=c(1,4,2))
>
> and I still get a syntax error.
>
> Is this type of syntax allowed in R?
>

Yes, anonymous functions are allowed. Anonymous funtions aren't -- you
appear to have a typographical problem.

	-thomas



From sibert at hawaii.edu  Wed Dec  3 19:36:47 2003
From: sibert at hawaii.edu (John Sibert)
Date: Wed, 03 Dec 2003 08:36:47 -1000
Subject: [R] model of fish over exploitation
Message-ID: <6.0.0.22.2.20031203082852.02a35580@mail.hawaii.edu>

It looks like you are trying to fit Schaeffer model (a special case of the 
Pella-Tomlinsion general production model) to the data. Such models can be 
solved in a completely general way using ADModel Builder, and an example of 
the general production model application can be found at
http://otter-rsch.com/examples.htm#docs

Bon courage,
John

____________________________________

John Sibert, Manager
Pelagic Fisheries Research Program
University of Hawaii at Manoa
1000 Pope Road, MSB 313
Honolulu, HI 96822
United States

Phone: (808) 956-4109
Fax: (808) 956-4104
____________________________________

Washington DC
Phone: (202) 861 2363
Fax: (202) 861 4767
____________________________________

PFRP Web Site:   http://www.soest.hawaii.edu/PFRP/
email:  sibert at hawaii.edu



From andy_liaw at merck.com  Wed Dec  3 19:54:16 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 03 Dec 2003 13:54:16 -0500
Subject: [R] checking for identical columns in a mxn matrix
Message-ID: <3A822319EB35174CA3714066D590DCD50205CED7@usrymx25.merck.com>

> From: Rajarshi Guha

> On Wed, 2003-12-03 at 13:18, J.R. Lockwood wrote:
> 
> > list will come up with something clever.  the other issues 
> is that you
> > need to be careful when doing equality comparisons with 
> floating point
> > numbers.  unless your matrix consists of characters or integers,
> > you'll need to think about some level of numerical tolerance of your
> > comparison.
> 
> Yes, the matrix will always be integer.

Other than what J.R. and Marc suggested, you might could try to use

dist(t(x), method="manhattan")

and see which entries are 0 (or close enough to 0).

HTH,
Andy


 
> -------------------------------------------------------------------
> Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
> GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
> -------------------------------------------------------------------
> All theoretical chemistry is really physics; and all theoretical
> chemists 
> know it.
> -- Richard P. Feynman
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From bhx2 at mevik.net  Wed Dec  3 20:27:00 2003
From: bhx2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Wed, 03 Dec 2003 20:27:00 +0100
Subject: [R] nameless functions in R
In-Reply-To: <1070475462.21090.16.camel@ra.chem.psu.edu> (Rajarshi Guha's
	message of "03 Dec 2003 13:17:42 -0500")
References: <1070475462.21090.16.camel@ra.chem.psu.edu>
Message-ID: <7oad69snwb.fsf@foo.nemo-project.org>

Rajarshi Guha <rxg218 at psu.edu> writes:

> apply(x, c(2), funtion(v1,v2){ identical(v1,v2) }, v2=c(1,4,2))
>
> The above gives me a syntax error. I also tried:

No wonder!  Try with `function' instead of `funtion'.

-- 
Bj?rn-Helge Mevik



From elsawy at ysbl.york.ac.uk  Wed Dec  3 21:46:15 2003
From: elsawy at ysbl.york.ac.uk (Karim Elsawy)
Date: Wed, 03 Dec 2003 20:46:15 +0000
Subject: [R] volume of an irregular grid
Message-ID: <3FCE4B97.B00350AB@ysbl.york.ac.uk>

I have a 3d irregular grid of a surface (closed surface)
I would like to calculate the volume enclosed inside this surface
can this be done in R
any help is very much appreciated
best regards
karim
Karim



From elsawy at ysbl.york.ac.uk  Wed Dec  3 21:56:47 2003
From: elsawy at ysbl.york.ac.uk (Karim Elsawy)
Date: Wed, 03 Dec 2003 20:56:47 +0000
Subject: [R] volume of an irregular grid
Message-ID: <3FCE4E0F.6ADDE203@ysbl.york.ac.uk>

I have a 3d irregular grid of a surface (closed surface)
I would like to calculate the volume enclosed inside this surface
can this be done in R
any help is very much appreciated
best regards
karim
Karim



From jonathan_li at agilent.com  Wed Dec  3 23:31:04 2003
From: jonathan_li at agilent.com (jonathan_li@agilent.com)
Date: Wed, 3 Dec 2003 14:31:04 -0800 
Subject: [R] add a point to regression line and cook's distance
Message-ID: <FC0B9DA2600ED4118F76009027AA5DDD09FEDA80@ALEX2>

Hi, 

This is more a statistics question rather than R question. But I thought people on this list may have some pointers.

MY question is like the following:
I would like to have a robust regression line. The data I have are mostly clustered around a small range. So
the regression line tend to be influenced strongly by outlier points (with large cook's distance). From the application
's background, I know that the line should pass (0,0), which is far away from the data cloud. I would like to add this
point to have a more robust line. The question is: does it make sense to do this? what are the negative impacts if any?

thanks,
jonathan



From spencer.graves at pdf.com  Wed Dec  3 23:50:51 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 03 Dec 2003 14:50:51 -0800
Subject: [R] add a point to regression line and cook's distance
In-Reply-To: <FC0B9DA2600ED4118F76009027AA5DDD09FEDA80@ALEX2>
References: <FC0B9DA2600ED4118F76009027AA5DDD09FEDA80@ALEX2>
Message-ID: <3FCE68CB.3050307@pdf.com>

      What is the context?  What do the "outliers" represent?  If you 
think carefully about the context, you may find the answer. 

      hope this helps.  spencer graves
p.s.  I know statisticians who worked for HP before the split and who 
still work for either HP or Agilent, I'm not certain which.  If you want 
to contact me off-line, I can give you a couple of names if that might 
help. 

jonathan_li at agilent.com wrote:

>Hi, 
>
>This is more a statistics question rather than R question. But I thought people on this list may have some pointers.
>
>MY question is like the following:
>I would like to have a robust regression line. The data I have are mostly clustered around a small range. So
>the regression line tend to be influenced strongly by outlier points (with large cook's distance). From the application
>'s background, I know that the line should pass (0,0), which is far away from the data cloud. I would like to add this
>point to have a more robust line. The question is: does it make sense to do this? what are the negative impacts if any?
>
>thanks,
>jonathan
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From rbonneau at systemsbiology.org  Thu Dec  4 01:07:48 2003
From: rbonneau at systemsbiology.org (Richard Bonneau)
Date: Wed, 3 Dec 2003 16:07:48 -0800
Subject: [R] predict.gl1ce question
Message-ID: <E4B3C7C4-25ED-11D8-8CEC-000A95AFB68E@systemsbiology.org>

Hi,

I'm using gl1ce with family=binomial like so:
 >yy
       succ fail
  [1,]   76   23
  [2,]   32   67
  [3,]   56   43
  ...
[24,]   81   18

 >xx
          c1219       c643
X1  0.04545455 0.64274145
X2  0.17723669 0.90392792
...
X24 0.80629054 0.12239320

 >test.gl1ce <- gl1ce(yy ~ xx, family = binomial(link=logit), bound = 
0.5 )
or
 >omit <- c(2,3)
 >test.gl1ce <- gl1ce(yy[-omit,] ~ xx[-omit,], family = 
binomial(link=logit), bound = 1 )

this seems to work fine and as i change the shrinkage parameter 
everything behaves as expected.

if i try to get the fitted values (y-hat) using predict i have no 
problems:
 > predict.gl1ce(test.gl1ce)
  [1] 0.38129977 0.16513661 0.47666779 0.45348757 0.09916513 0.18167674
  [7] 0.11047684 0.15786664 0.14765670 0.40657031 0.19072570 0.80259477
[13] 0.36317090 0.35930557 0.23700520 0.17579282 0.18835043 0.52306049
[19] 0.28388953 0.41262864 0.29933710 0.43556139 0.15276727 0.73017401

***
I have problems, however, when i try to use predict.gl1ce() with 
newdata.

so, the following tries all give errors:
 > predict.gl1ce(test.gl1ce, xx, family=binomial(link-logit))
 > predict.gl1ce(test.gl1ce, xx)
 > predict.gl1ce(test.gl1ce, xx[omit,], family=binomial(link-logit))
Error in predict.l1ce(test.gl1ce, xx, family = binomial(link - logit)) :
         Argument `newdata' is not a data frame, and cannot be coerced 
to an appropriate model matrix

the following weak try also bombs:
 > predict.gl1ce(test.gl1ce, data.frame(xx), family=binomial(link-logit))
Error in eval(expr, envir, enclos) : attempt to apply non-function


I've tried quite a few variations. It seems I'm missing something, but 
if glm or gl1ce take a certain
data format then the corresponding predict me,thods should (what am i 
missing).

thanks,
Rico


Richard Bonneau
Institute for Systems Biology
Seattle, WA
206-732-1463
206-732-1299 (fax)



From p.dalgaard at biostat.ku.dk  Thu Dec  4 01:29:40 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 04 Dec 2003 01:29:40 +0100
Subject: [R] predict.gl1ce question
In-Reply-To: <E4B3C7C4-25ED-11D8-8CEC-000A95AFB68E@systemsbiology.org>
References: <E4B3C7C4-25ED-11D8-8CEC-000A95AFB68E@systemsbiology.org>
Message-ID: <x2zne9l91n.fsf@biostat.ku.dk>

Richard Bonneau <rbonneau at systemsbiology.org> writes:

> Hi,
> 
> I'm using gl1ce with family=binomial like so:
>  >yy
>        succ fail
>   [1,]   76   23
>   [2,]   32   67
>   [3,]   56   43
>   ...
> [24,]   81   18
> 
>  >xx
>           c1219       c643
> X1  0.04545455 0.64274145
> X2  0.17723669 0.90392792
> ...
> X24 0.80629054 0.12239320
> 
>  >test.gl1ce <- gl1ce(yy ~ xx, family = binomial(link=logit), bound =
> 0.5 )
> or
>  >omit <- c(2,3)
>  >test.gl1ce <- gl1ce(yy[-omit,] ~ xx[-omit,], family =
> binomial(link=logit), bound = 1 )
> 
> this seems to work fine and as i change the shrinkage parameter
> everything behaves as expected.
> 
> if i try to get the fitted values (y-hat) using predict i have no
> problems:
>  > predict.gl1ce(test.gl1ce)
>   [1] 0.38129977 0.16513661 0.47666779 0.45348757 0.09916513 0.18167674
>   [7] 0.11047684 0.15786664 0.14765670 0.40657031 0.19072570 0.80259477
> [13] 0.36317090 0.35930557 0.23700520 0.17579282 0.18835043 0.52306049
> [19] 0.28388953 0.41262864 0.29933710 0.43556139 0.15276727 0.73017401
> 
> ***
> I have problems, however, when i try to use predict.gl1ce() with
> newdata.
> 
> so, the following tries all give errors:
>  > predict.gl1ce(test.gl1ce, xx, family=binomial(link-logit))
>  > predict.gl1ce(test.gl1ce, xx)
>  > predict.gl1ce(test.gl1ce, xx[omit,], family=binomial(link-logit))
> Error in predict.l1ce(test.gl1ce, xx, family = binomial(link - logit)) :
>          Argument `newdata' is not a data frame, and cannot be coerced
> to an appropriate model matrix
> 
> the following weak try also bombs:
>  > predict.gl1ce(test.gl1ce, data.frame(xx), family=binomial(link-logit))
> Error in eval(expr, envir, enclos) : attempt to apply non-function
> 
> 
> I've tried quite a few variations. It seems I'm missing something, but
> if glm or gl1ce take a certain
> data format then the corresponding predict me,thods should (what am i
> missing).

1. try link=logit, as opposed to what you typed
2. xx is probably not a data frame (or yy~xx would unlikely work), so
   as.data.frame might do the trick.
3. what is gl1ce? I you're having trouble with an add-on package, you
   might have the courtesy to tell us which one. Not everyone uses
   lasso2 on a daily basis, you know.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From msb1129 at bellsouth.net  Thu Dec  4 01:31:49 2003
From: msb1129 at bellsouth.net (Michael Benjamin)
Date: Wed, 3 Dec 2003 19:31:49 -0500
Subject: [R] R performance--referred from Bioconductor listserv
Message-ID: <010101c3b9fe$04a834b0$7a05fea9@amd>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031203/e09629f7/attachment.pl

From kjetil at entelnet.bo  Thu Dec  4 01:48:32 2003
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Wed, 03 Dec 2003 20:48:32 -0400
Subject: [R] predict.gl1ce question
In-Reply-To: <E4B3C7C4-25ED-11D8-8CEC-000A95AFB68E@systemsbiology.org>
Message-ID: <3FCE4C20.8942.827249@localhost>

On 3 Dec 2003 at 16:07, Richard Bonneau wrote:

Could you tell us where you found gl1ec ?

Kjetil Halvorsen

> Hi,
> 
> I'm using gl1ce with family=binomial like so:
>  >yy
>        succ fail
>   [1,]   76   23
>   [2,]   32   67
>   [3,]   56   43
>   ...
> [24,]   81   18
> 
>  >xx
>           c1219       c643
> X1  0.04545455 0.64274145
> X2  0.17723669 0.90392792
> ...
> X24 0.80629054 0.12239320
> 
>  >test.gl1ce <- gl1ce(yy ~ xx, family = binomial(link=logit), bound =
> 0.5 ) or
>  >omit <- c(2,3)
>  >test.gl1ce <- gl1ce(yy[-omit,] ~ xx[-omit,], family = 
> binomial(link=logit), bound = 1 )
> 
> this seems to work fine and as i change the shrinkage parameter 
> everything behaves as expected.
> 
> if i try to get the fitted values (y-hat) using predict i have no
> problems:
>  > predict.gl1ce(test.gl1ce)
>   [1] 0.38129977 0.16513661 0.47666779 0.45348757 0.09916513
>   0.18167674 [7] 0.11047684 0.15786664 0.14765670 0.40657031
>   0.19072570 0.80259477
> [13] 0.36317090 0.35930557 0.23700520 0.17579282 0.18835043 0.52306049
> [19] 0.28388953 0.41262864 0.29933710 0.43556139 0.15276727 0.73017401
> 
> ***
> I have problems, however, when i try to use predict.gl1ce() with
> newdata.
> 
> so, the following tries all give errors:
>  > predict.gl1ce(test.gl1ce, xx, family=binomial(link-logit))
>  > predict.gl1ce(test.gl1ce, xx)
>  > predict.gl1ce(test.gl1ce, xx[omit,], family=binomial(link-logit))
> Error in predict.l1ce(test.gl1ce, xx, family = binomial(link - logit))
> :
>          Argument `newdata' is not a data frame, and cannot be coerced
>          
> to an appropriate model matrix
> 
> the following weak try also bombs:
>  > predict.gl1ce(test.gl1ce, data.frame(xx),
>  family=binomial(link-logit))
> Error in eval(expr, envir, enclos) : attempt to apply non-function
> 
> 
> I've tried quite a few variations. It seems I'm missing something, but
> if glm or gl1ce take a certain data format then the corresponding
> predict me,thods should (what am i missing).
> 
> thanks,
> Rico
> 
> 
> Richard Bonneau
> Institute for Systems Biology
> Seattle, WA
> 206-732-1463
> 206-732-1299 (fax)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From matthew_wiener at merck.com  Thu Dec  4 02:03:37 2003
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Wed, 03 Dec 2003 20:03:37 -0500
Subject: [R] add a point to regression line and cook's distance
Message-ID: <AEBD81486231A343B1813FE62D335225077D1F3E@usrymx15.merck.com>

If you know that the line should pass through (0,0), would it make sense to
do a regression without an intercept?  You can do that by putting "-1" in
the formula, like:  lm(y ~ x - 1).

Hope this helps,

Matt

Matthew Wiener
RY84-202
Applied Computer Science & Mathematics Dept.
Merck Research Labs
126 E. Lincoln Ave.
Rahway, NJ 07065
732-594-5303 


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Spencer Graves
Sent: Wednesday, December 03, 2003 5:51 PM
To: jonathan_li at agilent.com
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] add a point to regression line and cook's distance


      What is the context?  What do the "outliers" represent?  If you 
think carefully about the context, you may find the answer. 

      hope this helps.  spencer graves
p.s.  I know statisticians who worked for HP before the split and who 
still work for either HP or Agilent, I'm not certain which.  If you want 
to contact me off-line, I can give you a couple of names if that might 
help. 

jonathan_li at agilent.com wrote:

>Hi, 
>
>This is more a statistics question rather than R question. But I thought
people on this list may have some pointers.
>
>MY question is like the following:
>I would like to have a robust regression line. The data I have are mostly
clustered around a small range. So
>the regression line tend to be influenced strongly by outlier points (with
large cook's distance). From the application
>'s background, I know that the line should pass (0,0), which is far away
from the data cloud. I would like to add this
>point to have a more robust line. The question is: does it make sense to do
this? what are the negative impacts if any?
>
>thanks,
>jonathan
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From rgentlem at jimmy.harvard.edu  Thu Dec  4 02:07:40 2003
From: rgentlem at jimmy.harvard.edu (Robert Gentleman)
Date: Wed, 3 Dec 2003 20:07:40 -0500
Subject: [R] R performance--referred from Bioconductor listserv
In-Reply-To: <010101c3b9fe$04a834b0$7a05fea9@amd>;
	from msb1129@bellsouth.net on Wed, Dec 03, 2003 at 07:31:49PM
	-0500
References: <010101c3b9fe$04a834b0$7a05fea9@amd>
Message-ID: <20031203200740.R6265@jimmy.harvard.edu>

Hi,
 Speed is an issue and large data sets are problematic. But I don't
 think that they are the entire problem here. Much more of the problem
 is that we don't yet know how to efficiently normalize microarrays
 and to estimate gene expression data. We're still trying to get it
 right rather than get it fast. There is not a lot of point in
 optimizing an algorithm that has a short shelf-life. And I don't
 think that anyone yet knows just which one will win.

 So, some of the issues are whether algorithms can be improved (and
they probably can; some form of binning would undoubtedly help with a
lot of what is going on in microarray analyses, but that requires that
the technology be somewhat more mature than it is now, at least that
is my view). 

Some gains can be made by cleaning up inefficient code (and newer
versions of the affy package have had some of that done). You can
explore this yourself (and I expect it is a bit more interesting than
benchmarking). The commands below profile the example and the output
(cut short) shows where time is being spent (interested readers are
referred to the man page). 

library(affy)
Rprof()
example(expresso)
Rprof(NULL)
summaryRprof()



$by.self
                                 self.time self.pct total.time
				 total.pct
"fft"                                 1.45     16.8       1.55
17.9
"read.dcf"                            0.42      4.9       0.63
7.3
".C"                                  0.35      4.1       0.35
4.1
"*"                                   0.30      3.5       0.30
3.5
"ifelse"                              0.28      3.3       0.87
10.1
"unique.default"                      0.28      3.3       0.30
3.5
":"                                   0.26      3.0       0.26
3.0
"names<-"                             0.23      2.7       0.28
3.3
"rep.default"                         0.23      2.7       0.23
2.7
"structure"                           0.23      2.7       1.08
12.5


So the bulk of the time is spent if fft; I think the first f is
important so you are unlikely to gain much there, the rest of the
self.time numbers suggest that there are not many gains to be had,
maybe a 20% gain with some serious reworking, maybe more.

But in other cases profiling is a great help (we recently made pretty
minor changes that resulted in major improvements),

 Robert

On Wed, Dec 03, 2003 at 07:31:49PM -0500, Michael Benjamin wrote:
> Hi, all--
>  
> I wanted to start a (new) thread on R speed/benchmarking.  There is a
> nice R benchmarking overview at
> http://www.sciviews.org/other/benchmark.htm, along with a free script so
> you can see how your machine stacks up.
>  
> Looks like R is substantially faster than S-plus.
>  
> My problem is this: with 512Mb and an overclocked AMD Athlon XP 1800+,
> running at 588 SPEC-FP 2000, it still takes me 30 minutes to analyze 4Mb
> .cel files x 120 files using affy (expresso).  Running svm takes a
> mighty long time with more than 500 genes, 150 samples.
>  
> Questions:
> 1) Would adding RAM or processing speed improve performance the most?
> 2) Is it possible to run R on a cluster without rewriting my high-level
> code?  In other words,
> 3) What are we going to do when we start collecting terabytes of array
> data to analyze?  There will come a "breaking point" at which desktop
> systems can't perform these analyses fast enough for large quantities of
> data.  What then?
>  
> Michael Benjamin, MD
> Winship Cancer Institute
> Emory University,
> Atlanta, GA
>  
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-- 
+---------------------------------------------------------------------------+
| Robert Gentleman                 phone : (617) 632-5250                   |
| Associate Professor              fax:   (617)  632-2444                   |
| Department of Biostatistics      office: M1B20                            |
| Harvard School of Public Health  email: rgentlem at jimmy.harvard.edu        |
+---------------------------------------------------------------------------+



From maj at stats.waikato.ac.nz  Thu Dec  4 02:17:47 2003
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Thu, 04 Dec 2003 14:17:47 +1300
Subject: [R] add a point to regression line and cook's distance
In-Reply-To: <AEBD81486231A343B1813FE62D335225077D1F3E@usrymx15.merck.com>
References: <AEBD81486231A343B1813FE62D335225077D1F3E@usrymx15.merck.com>
Message-ID: <3FCE8B3B.6030201@stats.waikato.ac.nz>

Not a good idea, unless the regression function is *known* to be linear. 
More likely it is only approximately linear over small ranges.

Murray Jorgensen

Wiener, Matthew wrote:

> If you know that the line should pass through (0,0), would it make sense to
> do a regression without an intercept?  You can do that by putting "-1" in
> the formula, like:  lm(y ~ x - 1).
> 
> Hope this helps,
> 
> Matt
> 
> Matthew Wiener
> RY84-202
> Applied Computer Science & Mathematics Dept.
> Merck Research Labs
> 126 E. Lincoln Ave.
> Rahway, NJ 07065
> 732-594-5303 
> 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Spencer Graves
> Sent: Wednesday, December 03, 2003 5:51 PM
> To: jonathan_li at agilent.com
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] add a point to regression line and cook's distance
> 
> 
>       What is the context?  What do the "outliers" represent?  If you 
> think carefully about the context, you may find the answer. 
> 
>       hope this helps.  spencer graves
> p.s.  I know statisticians who worked for HP before the split and who 
> still work for either HP or Agilent, I'm not certain which.  If you want 
> to contact me off-line, I can give you a couple of names if that might 
> help. 
> 
> jonathan_li at agilent.com wrote:
> 
> 
>>Hi, 
>>
>>This is more a statistics question rather than R question. But I thought
> 
> people on this list may have some pointers.
> 
>>MY question is like the following:
>>I would like to have a robust regression line. The data I have are mostly
> 
> clustered around a small range. So
> 
>>the regression line tend to be influenced strongly by outlier points (with
> 
> large cook's distance). From the application
> 
>>'s background, I know that the line should pass (0,0), which is far away
> 
> from the data cloud. I would like to add this
> 
>>point to have a more robust line. The question is: does it make sense to do
> 
> this? what are the negative impacts if any?
> 
>>thanks,
>>jonathan
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> 
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862



From jonathan_li at agilent.com  Thu Dec  4 02:40:21 2003
From: jonathan_li at agilent.com (jonathan_li@agilent.com)
Date: Wed, 3 Dec 2003 17:40:21 -0800 
Subject: [R] add a point to regression line and cook's distance
Message-ID: <FC0B9DA2600ED4118F76009027AA5DDD09FEDA83@ALEX2>


It is likely that the "true" relationship is nonlinear. There isn't a priori knowledge about linearity. In the small range where we do have enough data, the relationship
looks linear. Outside the range, the data are very scarse and have high level of noises too.
This is why adding (0,0) to the data can potentially improve the fit a great deal. But at the
same time, I have never heard people doing it this way. 

Jonathan

-----Original Message-----
From: Murray Jorgensen [mailto:maj at stats.waikato.ac.nz]
Sent: Wednesday, December 03, 2003 5:18 PM
To: Wiener, Matthew
Cc: jonathan_li at agilent.com; r-help at stat.math.ethz.ch
Subject: Re: [R] add a point to regression line and cook's distance


Not a good idea, unless the regression function is *known* to be linear. 
More likely it is only approximately linear over small ranges.

Murray Jorgensen

Wiener, Matthew wrote:

> If you know that the line should pass through (0,0), would it make sense to
> do a regression without an intercept?  You can do that by putting "-1" in
> the formula, like:  lm(y ~ x - 1).
> 
> Hope this helps,
> 
> Matt
> 
> Matthew Wiener
> RY84-202
> Applied Computer Science & Mathematics Dept.
> Merck Research Labs
> 126 E. Lincoln Ave.
> Rahway, NJ 07065
> 732-594-5303 
> 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Spencer Graves
> Sent: Wednesday, December 03, 2003 5:51 PM
> To: jonathan_li at agilent.com
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] add a point to regression line and cook's distance
> 
> 
>       What is the context?  What do the "outliers" represent?  If you 
> think carefully about the context, you may find the answer. 
> 
>       hope this helps.  spencer graves
> p.s.  I know statisticians who worked for HP before the split and who 
> still work for either HP or Agilent, I'm not certain which.  If you want 
> to contact me off-line, I can give you a couple of names if that might 
> help. 
> 
> jonathan_li at agilent.com wrote:
> 
> 
>>Hi, 
>>
>>This is more a statistics question rather than R question. But I thought
> 
> people on this list may have some pointers.
> 
>>MY question is like the following:
>>I would like to have a robust regression line. The data I have are mostly
> 
> clustered around a small range. So
> 
>>the regression line tend to be influenced strongly by outlier points (with
> 
> large cook's distance). From the application
> 
>>'s background, I know that the line should pass (0,0), which is far away
> 
> from the data cloud. I would like to add this
> 
>>point to have a more robust line. The question is: does it make sense to do
> 
> this? what are the negative impacts if any?
> 
>>thanks,
>>jonathan
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> 
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862



From maj at stats.waikato.ac.nz  Thu Dec  4 03:07:59 2003
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Thu, 04 Dec 2003 15:07:59 +1300
Subject: [R] add a point to regression line and cook's distance
In-Reply-To: <FC0B9DA2600ED4118F76009027AA5DDD09FEDA83@ALEX2>
References: <FC0B9DA2600ED4118F76009027AA5DDD09FEDA83@ALEX2>
Message-ID: <3FCE96FF.6080000@stats.waikato.ac.nz>

I suspect that the only way that adding a point at (0,0) would 'improve 
the fit' is by giving R^2 a boost. But this would be a spurious measure 
of fit, including as it does the invented point. The residual sum of 
squares calculated over the actual data would be increased, probably 
only by a modest amount, though.

You say  "Outside the range, the data are very scarse and have high 
level of noises too." Does this mean that you think that the error in 
these points is likely to be larger than the others? You might try a 
weighted regression in which you downweighted these points whicle still 
leaving them with relatively high leverage. Another thing to consider 
might be fitting a function like  y = ax + bx^2   ie y ~ x + I(x^2) -1.

All of this is ad hoc, though, and a bit of understanding about the 
science underlying the data and the likely functional form of the 
regression function would let you get much further, possibly using a 
nonlinear regression approach.

Murray

jonathan_li at agilent.com wrote:

> It is likely that the "true" relationship is nonlinear. There isn't a priori knowledge about linearity. In the small range where we do have enough data, the relationship
> looks linear. Outside the range, the data are very scarse and have high level of noises too.
> This is why adding (0,0) to the data can potentially improve the fit a great deal. But at the
> same time, I have never heard people doing it this way. 
> 
> Jonathan
> 
> -----Original Message-----
> From: Murray Jorgensen [mailto:maj at stats.waikato.ac.nz]
> Sent: Wednesday, December 03, 2003 5:18 PM
> To: Wiener, Matthew
> Cc: jonathan_li at agilent.com; r-help at stat.math.ethz.ch
> Subject: Re: [R] add a point to regression line and cook's distance
> 
> 
> Not a good idea, unless the regression function is *known* to be linear. 
> More likely it is only approximately linear over small ranges.
> 
> Murray Jorgensen
> 
> Wiener, Matthew wrote:
> 
> 
>>If you know that the line should pass through (0,0), would it make sense to
>>do a regression without an intercept?  You can do that by putting "-1" in
>>the formula, like:  lm(y ~ x - 1).
>>
>>Hope this helps,
>>
>>Matt
>>
>>Matthew Wiener
>>RY84-202
>>Applied Computer Science & Mathematics Dept.
>>Merck Research Labs
>>126 E. Lincoln Ave.
>>Rahway, NJ 07065
>>732-594-5303 
>>
>>
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Spencer Graves
>>Sent: Wednesday, December 03, 2003 5:51 PM
>>To: jonathan_li at agilent.com
>>Cc: r-help at stat.math.ethz.ch
>>Subject: Re: [R] add a point to regression line and cook's distance
>>
>>
>>      What is the context?  What do the "outliers" represent?  If you 
>>think carefully about the context, you may find the answer. 
>>
>>      hope this helps.  spencer graves
>>p.s.  I know statisticians who worked for HP before the split and who 
>>still work for either HP or Agilent, I'm not certain which.  If you want 
>>to contact me off-line, I can give you a couple of names if that might 
>>help. 
>>
>>jonathan_li at agilent.com wrote:
>>
>>
>>
>>>Hi, 
>>>
>>>This is more a statistics question rather than R question. But I thought
>>
>>people on this list may have some pointers.
>>
>>
>>>MY question is like the following:
>>>I would like to have a robust regression line. The data I have are mostly
>>
>>clustered around a small range. So
>>
>>
>>>the regression line tend to be influenced strongly by outlier points (with
>>
>>large cook's distance). From the application
>>
>>
>>>'s background, I know that the line should pass (0,0), which is far away
>>
>>from the data cloud. I would like to add this
>>
>>
>>>point to have a more robust line. The question is: does it make sense to do
>>
>>this? what are the negative impacts if any?
>>
>>
>>>thanks,
>>>jonathan
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>
>>>
>>
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>
>>
> 
> 

-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862



From msb1129 at bellsouth.net  Thu Dec  4 04:21:02 2003
From: msb1129 at bellsouth.net (Michael Benjamin)
Date: Wed, 3 Dec 2003 22:21:02 -0500
Subject: [R] RE: R performance questions
In-Reply-To: <3A822319EB35174CA3714066D590DCD50205CEDE@usrymx25.merck.com>
Message-ID: <011101c3ba15$a7f867e0$7a05fea9@amd>

Hi--

While I agree that we cannot agree on the ideal algorithms, we should be
taking practical steps to implement microarrays in the clinic.  I think
we can all agree that our algorithms have some degree of efficacy over
and above conventional diagnostic techniques.  If patients are dying
from lack of diagnostic accuracy, I think we have to work hard to use
this technology to help them, if we can.  I think we can, even now.

What if I offer, in my clinic, a service for cancer patients to compare
their affy data to an existing set of data, to predict their prognosis
or response to chemotherapy?  I think people will line up out the door
for such a service.  Knowing what we as a group of array analyzers know,
wouldn't we all want this kind of service available if we or a loved one
got cancer?

Can our programs deal with 1,000 .cel files?  10,000 files?  

I think our programs are pretty good, but what we need is DATA.  We must
be careful what we wish for--we might get it!  So how do we measure
whether analyzing 10,000 .cel files with library(affy) is feasible?  I'm
assuming that advanced hardware would be required for such a task.  What
are the critical components of such a platform?  How much money would a
feasible system for array analysis cost?

I was just looking ahead two or three years--where is all this genomic
array research headed?  I guess I'm concerned about scalability.  

Is anyone really working on implementing affy on a cluster/Beowulf?
That sounds like a real challenge.

Regards,
Michael Benjamin, MD
-----Original Message-----
From: Liaw, Andy [mailto:andy_liaw at merck.com] 
Sent: Wednesday, December 03, 2003 9:47 PM
To: 'Michael Benjamin'
Subject: RE: [BioC] R performance questions

Another point about benchmarking:  As has been discussed on R-help
before,
benchmarks can be misleading, as the one you mentioned.  It measures
linear
algebra tasks, etc., but that typically account for very small portion
of
"average" tasks.  Doug Bates also pointed out that the eigen() example
used
in that benchmark is computing mostly meaningless results.

In our experience, learning to use R more efficiently gives us the most
mileage, but large and fast hardware wouldn't hurt...

Cheers,
Andy

> -----Original Message-----
> From: Michael Benjamin [mailto:msb1129 at bellsouth.net] 
> Sent: Wednesday, December 03, 2003 7:32 PM
> To: 'Liaw, Andy'
> Subject: RE: [BioC] R performance questions
> 
> 
> Thanks.
> Mike
> 
> -----Original Message-----
> From: Liaw, Andy [mailto:andy_liaw at merck.com] 
> Sent: Wednesday, December 03, 2003 8:17 AM
> To: 'Michael Benjamin'
> Subject: RE: [BioC] R performance questions
> 
> Hi Michael,
> 
> Just one comment about SVM.  If you use the svm() function in 
> the e1071
> package to train linear SVM, it will be rather slow.  That's a known
> limitation of libsvm, of which the svm() function uses.  If you are
> willing
> to go outside of R, the "bsvm" package by C.J. Lin (same person who
> wrote
> libsvm) will train linear svm in much more efficient manner.
> 
> HTH,
> Andy
> 
> > -----Original Message-----
> > From: bioconductor-bounces at stat.math.ethz.ch 
> > [mailto:bioconductor-bounces at stat.math.ethz.ch] On Behalf Of 
> > Michael Benjamin
> > Sent: Tuesday, December 02, 2003 10:30 PM
> > To: bioconductor at stat.math.ethz.ch
> > Subject: [BioC] R performance questions
> > 
> > 
> > Hi, all--
> > 
> > I wanted to start a thread on R speed/benchmarking.  There 
> is a nice R
> > benchmarking overview at 
> http://www.sciviews.org/other/benchmark.htm,
> > along with a 
> free script so you can see how your machine stacks up.
> > 
> > Looks like R is substantially faster than S-plus.
> > 
> > My problem is this: with 512Mb and an overclocked AMD 
> Athlon XP 1800+,
> > running at 588 SPEC-FP 2000, it still takes FOREVER to 
> > analyze multiple
> > .cel files using affy (expresso).  Running svm takes a mighty 
> > long time
> > with more than 500 genes, 150 samples.
> > 
> > Questions:
> > 1) Would adding RAM or processing speed improve performance 
> the most?
> > 2) Is it possible to run R on a cluster without rewriting my 
> > high-level
> > code?  In other words,
> > 3) What are we going to do when we start collecting 
> terabytes of array
> > data to analyze?  There will come a "breaking point" at 
> which desktop
> > systems can't perform these analyses fast enough for large 
> > quantities of
> > data.  What then?
> > 
> > Michael Benjamin, MD
> > Winship Cancer Institute
> > Emory University,
> > Atlanta, GA
> > 
> > _______________________________________________
> > Bioconductor mailing list
> > Bioconductor at stat.math.ethz.ch
> > https://www.stat.math.ethz.ch/mailman/listinfo/bioconductor
> > 
> 
> 
> 
>



From rossini at blindglobe.net  Thu Dec  4 05:56:35 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Wed, 03 Dec 2003 20:56:35 -0800
Subject: [R] RE: R performance questions
In-Reply-To: <011101c3ba15$a7f867e0$7a05fea9@amd> (Michael Benjamin's
	message of "Wed, 3 Dec 2003 22:21:02 -0500")
References: <011101c3ba15$a7f867e0$7a05fea9@amd>
Message-ID: <85smk1tc3g.fsf@blindglobe.net>

"Michael Benjamin" <msb1129 at bellsouth.net> writes:

> I was just looking ahead two or three years--where is all this genomic
> array research headed?  I guess I'm concerned about scalability.  

Me too -- but at least in the near future, data will be growing more
than the capacity to process it.

> Is anyone really working on implementing affy on a cluster/Beowulf?
> That sounds like a real challenge.

Yes and no.  Depends on which components you want to deal with, and
how you want to work with the data.   

Everything (with respect to speed/capacity/etc) is especially
contextual -- applications and approximations will be quite
important. 

best,
-tony

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From jasont at indigoindustrial.co.nz  Thu Dec  4 06:17:39 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Thu, 04 Dec 2003 18:17:39 +1300
Subject: [R] add a point to regression line and cook's distance
In-Reply-To: <FC0B9DA2600ED4118F76009027AA5DDD09FEDA80@ALEX2>
References: <FC0B9DA2600ED4118F76009027AA5DDD09FEDA80@ALEX2>
Message-ID: <3FCEC373.90903@indigoindustrial.co.nz>

jonathan_li at agilent.com wrote:

> Hi, 
> 
> 
> MY question is like the following:
> I would like to have a robust regression line. The data I have are 
 > mostly clustered around a small range. So
> the regression line tend to be influenced strongly by outlier points 
 > (with large cook's distance). From the application's
 > background, I know that the line should pass (0,0), which is far
 > away from the data cloud. I would like to add this
> point to have a more robust line. The question is: 
 > does it make sense to do this? what are the negative impacts if any?

Have you tried a more robust fit (ltsreg() in the package lqs springs to 
mind)?  Using this, without forcing the intercept to zero, might give 
you some idea if your idea makes sense.  Venables and Ripley (Modern 
Applied Statistics with S, Springer-Verlag, 2002) give a good 
introduction to robust linear models, and how to estimate their error 
distribution.  Julian Faraway also gives an overview of the same, in his 
"Practical Regression and ANOVA using R".
http://cran.r-project.org/doc/contrib/Faraway-PRA.pdf

Hope that helps

Jason
-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From h.wickham at auckland.ac.nz  Thu Dec  4 08:20:40 2003
From: h.wickham at auckland.ac.nz (Hadley Wickham)
Date: Thu, 04 Dec 2003 07:20:40 -0000
Subject: [R] with for objects
In-Reply-To: <3FCC6BFD.7010409@ucl.ac.uk>
References: <3FC9B643.6050700@auckland.ac.nz> <3FCB6555.2010808@jhsph.edu>
	<3FCBA5A4.6040906@auckland.ac.nz>
	<Pine.A41.4.58.0312011354110.30536@homer18.u.washington.edu>
	<3FCBC6BB.6010706@auckland.ac.nz> <3FCC6BFD.7010409@ucl.ac.uk>
Message-ID: <3FA8AF69.7090608@auckland.ac.nz>

Hi Gavin,

Thanks for your suggestion - it wasn't quite what I wanted, but at least 
it got me thinking in the right direction.  I realised I didn't really 
need automatic access to all the slots/methods of a function, just to 
some I used commonly.  So I wrote a function that generates a list 
containing the bits I want from the object, and then passed that to 
with().  Simple and something I probably should have thought of before 
trying to be excessively complicated!

Hadley



From aconesa at ivia.es  Thu Dec  4 09:34:12 2003
From: aconesa at ivia.es (Ana Conesa)
Date: Thu, 04 Dec 2003 09:34:12 +0100
Subject: [R] Table to pdf
Message-ID: <6.0.0.22.0.20031204083628.051ad2cc@master.ivia.es>


   Hi all,
   I am new in R world and I haven't been able to find the answer to my
   question in the documentation I looked up so far. I hope someone can
   help. In the R function I am writing I have set the graphical output
   to be saved into a pdf file. I would also like to include in this file
   a table with some data, however I can't find the way to direct a table
   to a graphics file.  Does anyone know the way to address this?
   Thanks
   Ana

      O@@@@@    Ana Conesa, PhD.
     @@@O@@O@   Center for Citrus Genomics
     @O@@@@O@   Instituto Valenciano de Investigaciones Agrarias (IVIA)
     @@@O@@@@   Carretera Moncada - Naquera, Km. 4,5
      @@@@O@    46113 Moncada (Valencia) SPAIN
        ||      Tel. +34 963424000 ext.70161; Fax. +34 963424001
        ||      email: aconesa at ivia.es


From uth at zhwin.ch  Thu Dec  4 09:46:47 2003
From: uth at zhwin.ch (=?iso-8859-1?Q?=22Untern=E4hrer_Thomas=2C_uth=22?=)
Date: Thu, 4 Dec 2003 09:46:47 +0100
Subject: AW: [R] Table to pdf
Message-ID: <53A181E56FB0694ABFD212F8AEDA7F6F258A46@langouste.zhwin.ch>



Hi,

See ?Sweave in library(tools).

HTH

Thomas


-----Urspr?ngliche Nachricht-----
Von: Ana Conesa [mailto:aconesa at ivia.es] 
Gesendet: Donnerstag, 4. Dezember 2003 09:34
An: r-help at stat.math.ethz.ch
Betreff: [R] Table to pdf



   Hi all,
   I am new in R world and I haven't been able to find the answer to my
   question in the documentation I looked up so far. I hope someone can
   help. In the R function I am writing I have set the graphical output
   to be saved into a pdf file. I would also like to include in this file
   a table with some data, however I can't find the way to direct a table
   to a graphics file.  Does anyone know the way to address this?
   Thanks
   Ana

      O@@@@@    Ana Conesa, PhD.
     @@@O@@O@   Center for Citrus Genomics
     @O@@@@O@   Instituto Valenciano de Investigaciones Agrarias (IVIA)
     @@@O@@@@   Carretera Moncada - Naquera, Km. 4,5
      @@@@O@    46113 Moncada (Valencia) SPAIN
        ||      Tel. +34 963424000 ext.70161; Fax. +34 963424001
        ||      email: aconesa at ivia.es
______________________________________________
R-help at stat.math.ethz.ch mailing list https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From cvencatasawmy at yahoo.co.uk  Thu Dec  4 09:53:52 2003
From: cvencatasawmy at yahoo.co.uk (=?iso-8859-1?q?Coomaren=20Vencatasawmy?=)
Date: Thu, 4 Dec 2003 08:53:52 +0000 (GMT)
Subject: [R] Matrix Decomposition
In-Reply-To: <tdnrsvoseqjfrc2ads2u686njomp67atdk@4ax.com>
Message-ID: <20031204085352.29389.qmail@web86110.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031204/c1f9014b/attachment.pl

From ripley at stats.ox.ac.uk  Thu Dec  4 10:11:18 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 4 Dec 2003 09:11:18 +0000 (GMT)
Subject: [R] Table to pdf
In-Reply-To: <6.0.0.22.0.20031204083628.051ad2cc@master.ivia.es>
Message-ID: <Pine.LNX.4.44.0312040906220.27024-100000@gannet.stats>

On Thu, 4 Dec 2003, Ana Conesa wrote:

>    I am new in R world and I haven't been able to find the answer to my
>    question in the documentation I looked up so far. I hope someone can
>    help. In the R function I am writing I have set the graphical output
>    to be saved into a pdf file. I would also like to include in this file
>    a table with some data, however I can't find the way to direct a table
>    to a graphics file.  Does anyone know the way to address this?

But a table is not a graphic!  Think of the graphics pdf file as something 
to be included in another document, not the final file.

Setting tables in PDF is not an R question.  But there is some help 
available in packages Hmisc and xtable to set tables in latex, which 
pdflatex can then turn into PDF.

The alternative would be to make a graphical representation of the table 
via calls to text etc, but that makes it much harder to achieve good 
typography.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From pburns at pburns.seanet.com  Thu Dec  4 12:13:54 2003
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Thu, 04 Dec 2003 11:13:54 +0000
Subject: [R] Matrix Decomposition
References: <20031204085352.29389.qmail@web86110.mail.ukl.yahoo.com>
Message-ID: <3FCF16F2.6030207@pburns.seanet.com>

I think perhaps you are looking for something like
"symsqrt" which is given on page 285 of S Poetry.

Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Coomaren Vencatasawmy wrote:

>Hi,
> 
>How can one decompose the matrix M into
> 
>M= W * t(W)
> 
>I know of the chol() command but I am looking more for hypersphere decomposition and spectral decomposition.
> 
>Thanks.
> 
>Coomaren P. Vencatasawmy 
>
>
>
>
>---------------------------------
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>
>  
>



From yukangtu at hotmail.com  Thu Dec  4 12:53:16 2003
From: yukangtu at hotmail.com (Tu Yu-Kang)
Date: Thu, 04 Dec 2003 11:53:16 +0000
Subject: [R] extracting p value from GEE
Message-ID: <Law15-F51sFFsAitVGe0000a61f@hotmail.com>

Dear R users,

If anyone can tell me how to extract the p values from the output of gee?

Many thanks in advance.

Yu-Kang

_________________________________________________________________
KO MSN ^yGMuHvuW^ http://www.msn.com.tw/english/



From pburns at pburns.seanet.com  Thu Dec  4 13:05:57 2003
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Thu, 04 Dec 2003 12:05:57 +0000
Subject: [R] Matrix Decomposition
Message-ID: <3FCF2325.4070406@pburns.seanet.com>

  I should add that there is a "shar" file of functions from
S Poetry, so you need not type them in yourself.

Pat

I think perhaps you are looking for something like
"symsqrt" which is given on page 285 of S Poetry.

Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Coomaren Vencatasawmy wrote:

>Hi,
> 
>How can one decompose the matrix M into
> 
>M= W * t(W)
> 
>I know of the chol() command but I am looking more for hypersphere decomposition and spectral decomposition.
> 
>Thanks.
> 
>Coomaren P. Vencatasawmy 
>
>
>
>
>---------------------------------
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>
>  
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From christian_mora at vtr.net  Thu Dec  4 13:18:26 2003
From: christian_mora at vtr.net (christian_mora@vtr.net)
Date: Thu, 4 Dec 2003 08:18:26 -0400
Subject: [R] Selecting subsamples
Message-ID: <3FC6E2A0000104CF@hudson.vtr.net>

Hi all,
I?m working with a dataset with 9 columns and 2000 rows. Each row represents
an individual and one of the columns represents the volume of that individual
(measured in cubic meters). I?d like to select a sample from this dataset
(without considering any probability of the rows) in which the sum of the
volume of the individuals in that sample >= 100 cubic m.
I?ll appreciate any suggestion
Thanks
CM



From christian.schulz at questico.de  Thu Dec  4 13:24:51 2003
From: christian.schulz at questico.de (Christian Schulz)
Date: Thu, 4 Dec 2003 13:24:51 +0100
Subject: [R] Selecting subsamples
In-Reply-To: <3FC6E2A0000104CF@hudson.vtr.net>
Message-ID: <JAEELBHBOPKJDMMCNHKMEEOACBAA.christian.schulz@questico.de>

CM,

maybe
s <- which(data.frame$attribute >= 100)
is a starting point!?

regards,christian



-----Urspr?ngliche Nachricht-----
Von: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]Im Auftrag von
christian_mora at vtr.net
Gesendet: Donnerstag, 4. Dezember 2003 13:18
An: r-help at stat.math.ethz.ch
Betreff: [R] Selecting subsamples


Hi all,
I?m working with a dataset with 9 columns and 2000 rows. Each row represents
an individual and one of the columns represents the volume of that
individual
(measured in cubic meters). I?d like to select a sample from this dataset
(without considering any probability of the rows) in which the sum of the
volume of the individuals in that sample >= 100 cubic m.
I?ll appreciate any suggestion
Thanks
CM

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From petr.pikal at precheza.cz  Thu Dec  4 13:41:48 2003
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 04 Dec 2003 13:41:48 +0100
Subject: [R] Selecting subsamples
In-Reply-To: <3FC6E2A0000104CF@hudson.vtr.net>
Message-ID: <3FCF399C.508.14BDAD5@localhost>

Hallo

I assume you want equal size samples

x<-runif(1000)

this construction gives you "y" to be set if sum(y) > 5

while(sum(y<-sample(x,10))<5) y<-sample(x,10)

Cheers


On 4 Dec 2003 at 8:18, christian_mora at vtr.net wrote:

> Hi all,
> I?m working with a dataset with 9 columns and 2000 rows. Each row
> represents an individual and one of the columns represents the volume
> of that individual (measured in cubic meters). I?d like to select a
> sample from this dataset (without considering any probability of the
> rows) in which the sum of the volume of the individuals in that sample
> >= 100 cubic m. I?ll appreciate any suggestion Thanks CM
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

Petr Pikal
petr.pikal at precheza.cz



From annie_lenox at hotmail.com  Thu Dec  4 13:55:58 2003
From: annie_lenox at hotmail.com (annie lenox)
Date: Thu, 04 Dec 2003 12:55:58 +0000
Subject: [R] Irregular Time series
Message-ID: <Law14-F44Ur5qWn3GCI00008bcd@hotmail.com>

Could you please give me some help on R?

I have got an irregular time series: 28.8 1962, 27.6.1977, 19.7.1989, 
26.6.1995, 26.7.1999.
For these days, I know the surface of different vegetal formations.

How may I use these data?

Is the pastecs library appropritate?

Thanks for your help

Virginie

_________________________________________________________________

http://search.fr.msn.ch

From wantia at ifi.unizh.ch  Thu Dec  4 14:03:57 2003
From: wantia at ifi.unizh.ch (Jan Wantia)
Date: 4 Dec 2003 14:03:57 +0100
Subject: [R] get mean of several rows
Message-ID: <3FCF30BD.3040302@ifi.unizh.ch>

Dear all!

After hours of trying around, I gave up:

I have a 2-dimensional array, and I know how to split it into its rows 
and how to get the mean for every row using 'sapply'.
But what I want is to calculate the mean over the first n rows, and then 
the second n rows, etc., so that I get a vector like:

v == mean1(row 1:5), mean2(row6:10),...

(trivial, you might say. I find it rather mind-boggling, though: I tried 
to get the mean from the array before splitting it, after splitting it, 
looping through it with for-loops...I feel like an idiot by now; looks 
like I missed a crucial point of how 'R' works.)

Thanks a lot in advance!
-- 

______________________________________________________

Jan Wantia
Dept. of Information Technology, University of Z?rich
Andreasstr. 15
CH 8050 Z?rich
Switzerland

Tel.:     +41 (0) 1 635 4315
Fax:     +41 (0) 1 635 45 07
email: wantia at ifi.unizh.ch



From andy_liaw at merck.com  Thu Dec  4 14:25:27 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 04 Dec 2003 08:25:27 -0500
Subject: [R] get mean of several rows
Message-ID: <3A822319EB35174CA3714066D590DCD50205CEE1@usrymx25.merck.com>

1. Using rowMeans() is more efficient for computing row means.

2. "Mean of five rows" is the same as "mean of the five row means"
(exception: if you have NAs, and set na.rm=TRUE).  So you can just do
something like:

k <- ceiling(nrow(x) / 5)
ktimes <- if (rem <- nrow(x) %% 5) c(rep(5, k-1), rem) else rep(5, k)
mean5row <- tapply(rowMeans(x), rep(1:k, ktimes))

(This is untested!)

HTH,
Andy

> From: Jan Wantia
> 
> Dear all!
> 
> After hours of trying around, I gave up:
> 
> I have a 2-dimensional array, and I know how to split it into 
> its rows 
> and how to get the mean for every row using 'sapply'.
> But what I want is to calculate the mean over the first n 
> rows, and then 
> the second n rows, etc., so that I get a vector like:
> 
> v == mean1(row 1:5), mean2(row6:10),...
> 
> (trivial, you might say. I find it rather mind-boggling, 
> though: I tried 
> to get the mean from the array before splitting it, after 
> splitting it, 
> looping through it with for-loops...I feel like an idiot by 
> now; looks 
> like I missed a crucial point of how 'R' works.)
> 
> Thanks a lot in advance!
> -- 
> 
> ______________________________________________________
> 
> Jan Wantia
> Dept. of Information Technology, University of Z?rich
> Andreasstr. 15
> CH 8050 Z?rich
> Switzerland
> 
> Tel.:     +41 (0) 1 635 4315
> Fax:     +41 (0) 1 635 45 07
> email: wantia at ifi.unizh.ch



From lecoutre at stat.ucl.ac.be  Thu Dec  4 14:43:25 2003
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Thu, 04 Dec 2003 14:43:25 +0100
Subject: [R] get mean of several rows
In-Reply-To: <3FCF30BD.3040302@ifi.unizh.ch>
References: <3FCF30BD.3040302@ifi.unizh.ch>
Message-ID: <6.0.1.1.2.20031204143216.0212bf88@stat4ux.stat.ucl.ac.be>


Take a look at tapply and %/% which could help you to create indices for 
groups.

 > x=cbind(id=1:23,var1=rnorm(23))
 > tapply(x[,"var1"],1:length(x[,"var1"])%/%5,rowMeans)
            0            1            2            3            4
  0.393473633  0.412297253 -0.221925003 -0.005212217 -1.564881727

If your data has severall variables, also look at aggregate

 > x=cbind(var1=rnorm(23),var2=rnorm(23))
 > aggregate(x,by=list(groups=1:dim(x)[1]%/%5),FUN=mean)

(and then you can call rowMeans on the result)

Eric


At 14:03 4/12/2003, Jan Wantia wrote:
>Dear all!
>
>After hours of trying around, I gave up:
>
>I have a 2-dimensional array, and I know how to split it into its rows and 
>how to get the mean for every row using 'sapply'.
>But what I want is to calculate the mean over the first n rows, and then 
>the second n rows, etc., so that I get a vector like:
>
>v == mean1(row 1:5), mean2(row6:10),...
>
>(trivial, you might say. I find it rather mind-boggling, though: I tried 
>to get the mean from the array before splitting it, after splitting it, 
>looping through it with for-loops...I feel like an idiot by now; looks 
>like I missed a crucial point of how 'R' works.)
>
>Thanks a lot in advance!
>--
>
>______________________________________________________
>
>Jan Wantia
>Dept. of Information Technology, University of Z?rich
>Andreasstr. 15
>CH 8050 Z?rich
>Switzerland
>
>Tel.:     +41 (0) 1 635 4315
>Fax:     +41 (0) 1 635 45 07
>email: wantia at ifi.unizh.ch
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help



--------------------------------------------------
L'erreur est certes humaine, mais un vrai d?sastre
n?cessite un ou deux ordinateurs. Citation anonyme
--------------------------------------------------
Eric Lecoutre
Informaticien/Statisticien
Institut de Statistique / UCL

TEL (+32)(0)10473050       lecoutre at stat.ucl.ac.be
URL http://www.stat.ucl.ac.be/ISpersonnel/lecoutre



From ggrothendieck at myway.com  Thu Dec  4 14:56:47 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu,  4 Dec 2003 08:56:47 -0500 (EST)
Subject: [R] get mean of several rows
Message-ID: <20031204135647.3AF2E396E@mprdmxin.myway.com>



The following:

   c( tapply( x, (row(x)-1)%/%5, mean ) )

gives a vector whose first element is the mean of every
element in the rows 1 through 5 inclusive, whose second 
element is the mean of every element rows 6 through 10 
inclusive, etc.

--- 
Date: 4 Dec 2003 14:03:57 +0100 
From: Jan Wantia <wantia at ifi.unizh.ch>
To: <r-help at stat.math.ethz.ch> 
Subject: [R] get mean of several rows 

 
 
Dear all!

After hours of trying around, I gave up:

I have a 2-dimensional array, and I know how to split it into its rows 
and how to get the mean for every row using 'sapply'.
But what I want is to calculate the mean over the first n rows, and then 
the second n rows, etc., so that I get a vector like:

v == mean1(row 1:5), mean2(row6:10),...

(trivial, you might say. I find it rather mind-boggling, though: I tried 
to get the mean from the array before splitting it, after splitting it, 
looping through it with for-loops...I feel like an idiot by now; looks 
like I missed a crucial point of how 'R' works.)

Thanks a lot in advance!
-- 

______________________________________________________

Jan Wantia
Dept. of Information Technology, University of Zrich
Andreasstr. 15
CH 8050 Zrich
Switzerland



From rpeng at jhsph.edu  Thu Dec  4 14:57:03 2003
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Thu, 04 Dec 2003 08:57:03 -0500
Subject: [R] R performance--referred from Bioconductor listserv
In-Reply-To: <010101c3b9fe$04a834b0$7a05fea9@amd>
References: <010101c3b9fe$04a834b0$7a05fea9@amd>
Message-ID: <3FCF3D2F.7090401@jhsph.edu>

Please see below.

Michael Benjamin wrote:
> Hi, all--
>  
> I wanted to start a (new) thread on R speed/benchmarking.  There is a
> nice R benchmarking overview at
> http://www.sciviews.org/other/benchmark.htm, along with a free script so
> you can see how your machine stacks up.
>  
> Looks like R is substantially faster than S-plus.
>  
> My problem is this: with 512Mb and an overclocked AMD Athlon XP 1800+,
> running at 588 SPEC-FP 2000, it still takes me 30 minutes to analyze 4Mb
> .cel files x 120 files using affy (expresso).  Running svm takes a
> mighty long time with more than 500 genes, 150 samples.
>  
> Questions:
> 1) Would adding RAM or processing speed improve performance the most?

I usually find adding RAM makes a big difference, especially for Windows 
boxes.

> 2) Is it possible to run R on a cluster without rewriting my high-level
> code?  In other words,

I think the answer is most likely "no".  The `snow' package of 
Tierney/Rossini/Li on CRAN has gone a long way in making parallel 
computing in R much easier.

> 3) What are we going to do when we start collecting terabytes of array
> data to analyze?  There will come a "breaking point" at which desktop
> systems can't perform these analyses fast enough for large quantities of
> data.  What then?

Hasn't that "breaking point" always existed in some form or another?  If 
large datasets can be broken up then clusters can be useful because 
smaller chunks can be parceled out to the cluster nodes and processed. 
Another thing to think about is that as R moves into the world of 64 bit 
processors, we will be able to load much larger datasets into RAM.  I 
didn't think it was possible, but I recently loaded an 8GB dataset into 
R running on a Solaris/Sparc box!

-roger

>  
> Michael Benjamin, MD
> Winship Cancer Institute
> Emory University,
> Atlanta, GA
>  
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From pauljohn at ku.edu  Thu Dec  4 13:57:25 2003
From: pauljohn at ku.edu (Paul E. Johnson)
Date: Thu, 04 Dec 2003 06:57:25 -0600
Subject: [R] Comparing Negative Binomial Regression in Stata and R. Constants
 differ?
Message-ID: <3FCF2F35.5030007@ku.edu>

I looked for examples of count data that might interest the students and 
found this project about dropout rates in Los Angeles High Schools.  It 
is discussed in the UCLA stats help pages for the Stata users:
http://www.ats.ucla.edu/stat/stata/library/count.htm
and
See: http://www.ats.ucla.edu/stat/stata/library/longutil.htm

To replicate those results, I used R's excellent foreign package to 
bring the lahigh data in, then did
poisReg1 <- glm(daysabs~gender+ 
mathnce+langnce,family=poisson(link=log), data=lahigh)
library(MASS)
negbinReg1 <- glm.nb(daysabs~gender+ mathnce+langnce,link=log, data=lahigh)

The parameter estimates of the coefficients are the just about the same, 
except for the intercept estimates.  Below I pasted in the Negative 
Binomial results I got from R along with the Stata results that they 
report.  In the Stata output, they report alpha, same as 1/theta from 
the R glm.nb output.  Except for minor differences in standard errors, 
only the intercept estimates markedly differ.

Can anybody explain this?

-----------------------------------------------------------
Stata:

nbreg daysabs gender mathnce langnce

Negative binomial regression                      Number of obs   
=        316
                                                  LR chi2(3)      =      
20.74
                                                  Prob > chi2     =     
0.0001
Log likelihood = -880.87312                       Pseudo R2       =     
0.0116
------------------------------------------------------------------------------
 daysabs |      Coef.   Std. Err.       z     P>|z|       [95% Conf. 
Interval]
---------+--------------------------------------------------------------------
  gender  |  -.4311844   .1396656     -3.087   0.002       -.704924   
-.1574448
 mathnce |   -.001601     .00485     -0.330   0.741      -.0111067    
.0079048
 langnce  |  -.0143475   .0055815     -2.571   0.010      -.0252871    
-.003408
   _cons   |   3.147254   .3211669      9.799   0.000       2.517778    
3.776729
---------+--------------------------------------------------------------------
/lnalpha  |   .2533877   .0955362                          .0661402    
.4406351
---------+--------------------------------------------------------------------
   alpha  |   1.288383   .1230871     10.467   0.000       1.068377    
1.553694
------------------------------------------------------------------------------

Likelihood ratio test of alpha=0:    chi2(1) =  1334.20   Prob > chi2 = 
0.0000



Here is the R glm.nb output:

Deviance Residuals:
    Min       1Q   Median       3Q      Max 
-1.9785  -1.0627  -0.4147   0.2865   2.8193 

Coefficients:
             Estimate Std. Error z value Pr(>|z|)   
(Intercept)  2.716069   0.234174  11.598  < 2e-16 ***
gendermale  -0.431185   0.139516  -3.091  0.00200 **
mathnce     -0.001601   0.005300  -0.302  0.76259   
langnce     -0.014348   0.005372  -2.671  0.00756 **
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1

(Dispersion parameter for Negative Binomial(0.7762) family taken to be 1)

    Null deviance: 378.43  on 315  degrees of freedom
Residual deviance: 356.93  on 312  degrees of freedom
AIC: 1771.7

Number of Fisher Scoring iterations: 1

Correlation of Coefficients:
           (Intercept) gendermale mathnce
gendermale -0.40                        
mathnce    -0.28       -0.09            
langnce    -0.43        0.19      -0.69 


              Theta:  0.7762
          Std. Err.:  0.0742

 2 x log-likelihood:  -1761.7460  

----------------------------------------------------------



-- 
Paul E. Johnson                       email: pauljohn at ukans.edu
Dept. of Political Science            http://lark.cc.ukans.edu/~pauljohn
University of Kansas                  Office: (785) 864-9086
Lawrence, Kansas 66045                FAX: (785) 864-5700



From Ted.Harding at nessie.mcc.ac.uk  Thu Dec  4 15:08:48 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 04 Dec 2003 14:08:48 -0000 (GMT)
Subject: [R] Selecting subsamples
In-Reply-To: <3FC6E2A0000104CF@hudson.vtr.net>
Message-ID: <XFMail.031204140848.Ted.Harding@nessie.mcc.ac.uk>

On 04-Dec-03 christian_mora at vtr.net wrote:
> Hi all,
> I?m working with a dataset with 9 columns and 2000 rows. Each row
> represents an individual and one of the columns represents the volume
> of that individual (measured in cubic meters). I?d like to select a
> sample from this dataset (without considering any probability of the
> rows) in which the sum of the volume of the individuals in that sample
> >= 100 cubic m.

let X be the dataset. For N=2000:

  ix<-sort(rnorm(N),index.return=TRUE)$ix

  M<-max(which(cumsum(volume[ix])<100))+1 ## Assumes volume > 0

  X[ix[1:M],]

If you can't assume volume > 0, then somthing like

  M<-min( which(sum(volume)-cumsum(volume[ix]) <= sum(volume) - 100) )

Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 04-Dec-03                                       Time: 14:08:48
------------------------------ XFMail ------------------------------



From feh3k at spamcop.net  Wed Dec  3 18:59:23 2003
From: feh3k at spamcop.net (Frank E Harrell Jr)
Date: Wed, 3 Dec 2003 12:59:23 -0500
Subject: [R] HMisc describe -- error with dates
In-Reply-To: <3FD6016D@oldwebmail.mcgill.ca>
References: <3FD6016D@oldwebmail.mcgill.ca>
Message-ID: <20031203125923.433703c7.feh3k@spamcop.net>

On Wed, 3 Dec 2003 11:53:44 -0500
Tanya Murphy <tmurph6 at po-box.mcgill.ca> wrote:

> Thank you Frank and Gabor for the fixes and checking and rechecking! 
> Everything seems to work well with the Hmisc functions tried--upData,
> describe and summary.
> 
> To summarize:
> 1. Add the testDateTime and formatDateTime functions (copied from
> Frank's messages) to the Hmisc file (or run prior to loading Hmisc)
> 
> 
> testDateTime <- function(x, what=c('either','both','timeVaries')) {
>   what <- match.arg(what)
>   cl <- class(x) # was oldClass 22jun03
>   if(!length(cl)) return(FALSE)
> 
>   dc <- if(.R.) c('POSIXt','POSIXct','dates','times','chron') else
>   c('timeDate','date','dates','times','chron')
>   dtc <- if(.R.) c('POSIXt','POSIXct','chron') else
>   c('timeDate','chron')
>   switch(what,
>   either = any(cl %in% dc),
>   both = any(cl %in% dtc),
>   timeVaries = {
>   if('chron' %in% cl || !.R.) { ## chron or S+ timeDate
>   y <- as.numeric(x)
>   length(unique(round(y - floor(y),13))) > 1
>   } else if(.R.) length(unique(format(x,'%H%M%S'))) > 1 else
>   FALSE
>   })
>   }
> 
> formatDateTime <- function(x, at, roundDay=FALSE) {
> cl <- at$class
> w <- if(any(cl %in% c('chron','dates','times'))) {
> attributes(x) <- at
> fmt <- at$format
> if(roundDay) {
> if(length(fmt)==2 && is.character(fmt))
> format.dates(x, fmt[1]) else format.dates(x)
> } else x
> } else if(.R.) {
> attributes(x) <- at
> if(roundDay) as.POSIXct(round(x, 'days')) else x
> } else timeDate(julian=if(roundDay)round(x) else x)
> format(w)
> }
> 
> 2. Replace the decribe function with the new one (available as an
> attachment in Frank's most recent message on the subject). Instead of
> editing the original Hmisc file, this could be run after the Hmisc
> library is loaded.
> 
> Right?

Right, or if you use Linux I can send you a new .tar.gz file with an
update to Hmisc.  Once you do update Hmisc you can drop all the above.

Frank

> 
> 
> Tanya
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help


---
Frank E Harrell Jr    Professor and Chair            School of Medicine
                      Department of Biostatistics    Vanderbilt University



From szank at web.de  Thu Dec  4 16:08:42 2003
From: szank at web.de (Sebastian Zank)
Date: Thu, 4 Dec 2003 16:08:42 +0100
Subject: [R] Power-function of the F-Test
Message-ID: <003701c3ba78$81fab940$7193fea9@zankt28ydsx7go>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031204/dedef4e2/attachment.pl

From andy_liaw at merck.com  Thu Dec  4 16:15:46 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 04 Dec 2003 10:15:46 -0500
Subject: [R] Power-function of the F-Test
Message-ID: <3A822319EB35174CA3714066D590DCD50205CEE5@usrymx25.merck.com>

If you really only have two samples, power.t.test will do.  (An F with ndf=1
and ddf=p is the same as the square of a t with df=p.)


Andy

> From: Sebastian Zank
> 
> Hello,
> 
> does anybody know the procedure how to write a function to 
> generate the power of the two sample F-Test ? 
> 
> Would be glad if somebody can help me.
> 
> S.Zank
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From kimm at pet.auh.dk  Thu Dec  4 16:38:49 2003
From: kimm at pet.auh.dk (Kim Mouridsen)
Date: Thu, 4 Dec 2003 16:38:49 +0100
Subject: [R] regression with limited range response
Message-ID: <001001c3ba7c$b6b3e5e0$ce65030a@pckim>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031204/72399691/attachment.pl

From p.dalgaard at biostat.ku.dk  Thu Dec  4 16:42:37 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 04 Dec 2003 16:42:37 +0100
Subject: [R] Power-function of the F-Test
In-Reply-To: <003701c3ba78$81fab940$7193fea9@zankt28ydsx7go>
References: <003701c3ba78$81fab940$7193fea9@zankt28ydsx7go>
Message-ID: <x2n0a8fv2q.fsf@biostat.ku.dk>

"Sebastian Zank" <szank at web.de> writes:

> Hello,
> 
> does anybody know the procedure how to write a function to generate the power of the two sample F-Test ? 
> 
> Would be glad if somebody can help me.

Which F-test? For means or for variances? In the former case
power.t.test should if there are only two groups, or power.anova.test. 

        -p

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Thu Dec  4 17:16:27 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 4 Dec 2003 16:16:27 +0000 (GMT)
Subject: [R] Comparing Negative Binomial Regression in Stata and R.
	Constants differ?
In-Reply-To: <3FCF2F35.5030007@ku.edu>
Message-ID: <Pine.LNX.4.44.0312041614100.28557-100000@gannet.stats>

On Thu, 4 Dec 2003, Paul E. Johnson wrote:

> I looked for examples of count data that might interest the students and 
> found this project about dropout rates in Los Angeles High Schools.  It 
> is discussed in the UCLA stats help pages for the Stata users:
> http://www.ats.ucla.edu/stat/stata/library/count.htm
> and
> See: http://www.ats.ucla.edu/stat/stata/library/longutil.htm
> 
> To replicate those results, I used R's excellent foreign package to 
> bring the lahigh data in, then did
> poisReg1 <- glm(daysabs~gender+ 
> mathnce+langnce,family=poisson(link=log), data=lahigh)
> library(MASS)
> negbinReg1 <- glm.nb(daysabs~gender+ mathnce+langnce,link=log, data=lahigh)
> 
> The parameter estimates of the coefficients are the just about the same, 
> except for the intercept estimates.  Below I pasted in the Negative 
> Binomial results I got from R along with the Stata results that they 

Actually, from V&R's MASS package, excellent or otherwise but worthy of 
credit!

> report.  In the Stata output, they report alpha, same as 1/theta from 
> the R glm.nb output.  Except for minor differences in standard errors, 
> only the intercept estimates markedly differ.

What are the variable codings used?  Intercepts depend on coding of 
factors, and that applies to any sort of regression.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Dec  4 17:18:04 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 4 Dec 2003 16:18:04 +0000 (GMT)
Subject: [R] regression with limited range response
In-Reply-To: <001001c3ba7c$b6b3e5e0$ce65030a@pckim>
Message-ID: <Pine.LNX.4.44.0312041617050.28557-100000@gannet.stats>

On Thu, 4 Dec 2003, Kim Mouridsen wrote:

> Dear R experts
> 
>  
> 
> How can you perform a regression analysis in R when the dependent
> variable is countiuous but bounded, say between 0 and 100?
> 
> I would be grateful for pointers to R-functions but also for hints to
> relavant litterature since I have never worked with this problem before.

The usual way is to perform a logistic regression, and nls() has code to 
fit various logistic regression: glm could also be used with response/100.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From spencer.graves at pdf.com  Thu Dec  4 17:24:09 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 04 Dec 2003 08:24:09 -0800
Subject: [R] regression with limited range response
In-Reply-To: <001001c3ba7c$b6b3e5e0$ce65030a@pckim>
References: <001001c3ba7c$b6b3e5e0$ce65030a@pckim>
Message-ID: <3FCF5FA9.5040805@pdf.com>

      What are your assumptions about the distribution of the dependent 
variable and its relationship to independent variables?  Have you 
considered "glm" with family = quasibinomial? 

      hope this helps.  spencer graves

Kim Mouridsen wrote:

>Dear R experts
>
> 
>
>How can you perform a regression analysis in R when the dependent
>variable is countiuous but bounded, say between 0 and 100?
>
>I would be grateful for pointers to R-functions but also for hints to
>relavant litterature since I have never worked with this problem before.
>
> 
>
>Thanks in advance.
>
>Kim Mouridsen. 
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From maechler at stat.math.ethz.ch  Thu Dec  4 17:28:37 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 4 Dec 2003 17:28:37 +0100
Subject: [R] Comparing Negative Binomial Regression in Stata and R.
	Constants differ?
In-Reply-To: <3FCF2F35.5030007@ku.edu>
References: <3FCF2F35.5030007@ku.edu>
Message-ID: <16335.24757.401789.838469@gargle.gargle.HOWL>

Without going into details,  did you remember that the intercept
depends very much on the **contrasts** you use (for your factors)?

Regards,
Martin



From tlumley at u.washington.edu  Thu Dec  4 17:31:26 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 4 Dec 2003 08:31:26 -0800 (PST)
Subject: [R] get mean of several rows
In-Reply-To: <3FCF30BD.3040302@ifi.unizh.ch>
References: <3FCF30BD.3040302@ifi.unizh.ch>
Message-ID: <Pine.A41.4.58.0312040830510.67888@homer35.u.washington.edu>

On Thu, 4 Dec 2003, Jan Wantia wrote:

> Dear all!
>
> After hours of trying around, I gave up:
>
> I have a 2-dimensional array, and I know how to split it into its rows
> and how to get the mean for every row using 'sapply'.
> But what I want is to calculate the mean over the first n rows, and then
> the second n rows, etc., so that I get a vector like:
>
> v == mean1(row 1:5), mean2(row6:10),...
>
> (trivial, you might say. I find it rather mind-boggling, though: I tried
> to get the mean from the array before splitting it, after splitting it,
> looping through it with for-loops...I feel like an idiot by now; looks
> like I missed a crucial point of how 'R' works.)
>

There is also a function rowsum() for doing precisely this (it's quite a
bit faster than the tapply solution).

	-thomas



From p.dalgaard at biostat.ku.dk  Thu Dec  4 17:35:52 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 04 Dec 2003 17:35:52 +0100
Subject: [R] Comparing Negative Binomial Regression in Stata and R.
	Constants differ?
In-Reply-To: <Pine.LNX.4.44.0312041614100.28557-100000@gannet.stats>
References: <Pine.LNX.4.44.0312041614100.28557-100000@gannet.stats>
Message-ID: <x2ekvkfslz.fsf@biostat.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> > report.  In the Stata output, they report alpha, same as 1/theta from 
> > the R glm.nb output.  Except for minor differences in standard errors, 
> > only the intercept estimates markedly differ.
> 
> What are the variable codings used?  Intercepts depend on coding of 
> factors, and that applies to any sort of regression.

..and the fact that the difference between the two intercepts equals
the gender effect quite strongly suggests that Stata has gender coded
1 for females and 2 for males.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From flom at ndri.org  Thu Dec  4 17:39:49 2003
From: flom at ndri.org (Peter Flom)
Date: Thu, 04 Dec 2003 11:39:49 -0500
Subject: [R] regression with limited range response
Message-ID: <sfcf1d13.020@MAIL.NDRI.ORG>

Kim

This would depend a lot on how the DV is distributed within the 0-100
range.  It's even conceivable that the usual linear model could be used
without much problem (e.g. if the mean is 50 and the sd is small, then
the bounds of 0 and 100 will exist more in theory than in practice; of
course, you'd still want to do a lot of diagnostics).  

0 to 100 makes me think of a percent, there are methods for dealing
with percents.  

More generally, you may want to look at 
Long, JS (1997). Regression Models for Categorical and Limited
Dependent Variables. Sage.

which I find useful frequently.

HTH

Peter




Kim Mouridsen wrote:

>Dear R experts
>
> 
>
>How can you perform a regression analysis in R when the dependent
>variable is countiuous but bounded, say between 0 and 100?
>
>I would be grateful for pointers to R-functions but also for hints to
>relavant litterature since I have never worked with this problem
before.
>
> 
>
>Thanks in advance.
>
>Kim Mouridsen. 
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help 
>  
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help

Peter L. Flom, PhD
Assistant Director, Statistics and Data Analysis Core
Center for Drug Use and HIV Research
National Development and Research Institutes
71 W. 23rd St
www.peterflom.com
New York, NY 10010
(212) 845-4485 (voice)
(917) 438-0894 (fax)



From tlumley at u.washington.edu  Thu Dec  4 17:43:49 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 4 Dec 2003 08:43:49 -0800 (PST)
Subject: [R] Comparing Negative Binomial Regression in Stata and R.
	Constants differ?
In-Reply-To: <x2ekvkfslz.fsf@biostat.ku.dk>
References: <Pine.LNX.4.44.0312041614100.28557-100000@gannet.stats>
	<x2ekvkfslz.fsf@biostat.ku.dk>
Message-ID: <Pine.A41.4.58.0312040841460.67888@homer35.u.washington.edu>

On Thu, 4 Dec 2003, Peter Dalgaard wrote:

> Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:
>
> > > report.  In the Stata output, they report alpha, same as 1/theta from
> > > the R glm.nb output.  Except for minor differences in standard errors,
> > > only the intercept estimates markedly differ.
> >
> > What are the variable codings used?  Intercepts depend on coding of
> > factors, and that applies to any sort of regression.
>
> ..and the fact that the difference between the two intercepts equals
> the gender effect quite strongly suggests that Stata has gender coded
> 1 for females and 2 for males.
>

and this turns out to be the case.

In stata after fitting the model

lincom _cons+gender
 ( 1)  [daysabs]gender + [daysabs]_cons = 0

------------------------------------------------------------------------------
     daysabs |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
         (1) |   2.716069    .232576    11.68   0.000     2.260229     3.17191
------------------------------------------------------------------------------

which agrees almost exactly with the R intercept.


	-thomas



From Giles.Heywood at CommerzbankIB.com  Thu Dec  4 17:48:06 2003
From: Giles.Heywood at CommerzbankIB.com (Heywood, Giles)
Date: Thu, 4 Dec 2003 16:48:06 -0000 
Subject: [R] Irregular Time series
Message-ID: <8CBAA121CEB4D5118CB200508BB2BBEF05BF73A0@xmx8lonib.lonib.commerzbank.com>

You could take a look at the irregular time-series (its) package
on CRAN.  Your series is certainly irregular, and possibly a little
er... sparse.  Anyway, the following might get you started:

require(its)
mydates <- c("28.8.1962","27.6.1977","19.7.1989","26.6.1995","26.7.1999")
data <- matrix(1:5,dimnames=list(mydates,NULL))
its.format("%d.%m.%Y")
its(data)

- Giles

> -----Original Message-----
> From: annie lenox [mailto:annie_lenox at hotmail.com]
> Sent: 04 December 2003 12:56
> To: r-help at stat.math.ethz.ch
> Subject: [R] Irregular Time series
> 
> 
> Could you please give me some help on R?
> 
> I have got an irregular time series: 28.8 1962, 27.6.1977, 19.7.1989, 
> 26.6.1995, 26.7.1999.
> For these days, I know the surface of different vegetal formations.
> 
> How may I use these data?
> 
> Is the pastecs library appropritate?
> 
> Thanks for your help
> 
> Virginie
> 
> _________________________________________________________________
> 
> http://search.fr.msn.ch
> 


********************************************************************** 
This is a commercial communication from Commerzbank AG.\ \ T...{{dropped}}



From RBaskin at ahrq.gov  Thu Dec  4 18:00:14 2003
From: RBaskin at ahrq.gov (RBaskin@ahrq.gov)
Date: Thu, 4 Dec 2003 12:00:14 -0500 
Subject: [R] regression with limited range response
Message-ID: <3598558AD728D41183350008C7CF291C0F16B9E6@exchange1.ahrq.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031204/b6d5964e/attachment.pl

From subianto at cs.uu.nl  Thu Dec  4 18:04:34 2003
From: subianto at cs.uu.nl (Muhammad Subianto)
Date: Thu, 04 Dec 2003 18:04:34 +0100
Subject: [R] convert data
Message-ID: <3FCF6922.9020806@cs.uu.nl>

Dear R-helper,
I have a data set like:

OLDa
ALL
OLDc
OLDa
OLDb
NEW
OLDb
OLDa
ALL
. . .
ALL
OLDc
NEW

I want to convert that data as OLDa=1, OLDb=2, OLDc=3, NEW=4 and ALL=5 
or the result like:

1
5
3
1
2
4
2
1
5
. . .
5
3
4

How can I do it. Thanks you for your help.

Best regards,
Muhammad Subianto



From Lars-Konstanz at t-online.de  Thu Dec  4 18:03:58 2003
From: Lars-Konstanz at t-online.de (Lars Peters)
Date: Thu, 4 Dec 2003 18:03:58 +0100
Subject: [R] Font-style
Message-ID: <LBELKNGGJOINKPAFNOOLEEMDCAAA.Lars-Konstanz@t-online.de>

Hi,

how to change the font-style (e.g. Arial or Times) in Plots??

Thank 

Lars Peters


-----
Lars Peters

University of Konstanz
Limnological Institute
D-78457 Konstanz
Germany

phone: +49 (0)7531 88-2930
fax:   +49 (0)7531 88-3533
e-mail: Lars.Peters at Uni-Konstanz.de
http://www.uni-konstanz.de/sfb454/tp_eng/A1/doc/peters/peters.html
http://www.uni-konstanz.de/sfb454/tp_eng/A1/index.htm



From tplate at acm.org  Thu Dec  4 18:09:15 2003
From: tplate at acm.org (Tony Plate)
Date: Thu, 04 Dec 2003 10:09:15 -0700
Subject: [R] add a point to regression line and cook's distance
In-Reply-To: <FC0B9DA2600ED4118F76009027AA5DDD09FEDA80@ALEX2>
Message-ID: <5.2.1.1.2.20031204095749.040ef4a0@mailhost.blackmesacapital.com>

One way of implementing some Bayesian techniques is to add data points 
based on prior knowledge.  E.g., see Gelman, Carlin, Stern & Rubin, in 
"Bayesian Data Analysis" (1997) for how a prior on a regression parameter 
can be interpreted as an additional data point.  (Section 8.9 in my 2000 
reprint).

hope this helps,

Tony Plate


At Wednesday 02:31 PM 12/3/2003 -0800, jonathan_li at agilent.com wrote:
>Hi,
>
>This is more a statistics question rather than R question. But I thought 
>people on this list may have some pointers.
>
>MY question is like the following:
>I would like to have a robust regression line. The data I have are mostly 
>clustered around a small range. So
>the regression line tend to be influenced strongly by outlier points (with 
>large cook's distance). From the application
>'s background, I know that the line should pass (0,0), which is far away 
>from the data cloud. I would like to add this
>point to have a more robust line. The question is: does it make sense to 
>do this? what are the negative impacts if any?
>
>thanks,
>jonathan
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help

Tony Plate   tplate at acm.org



From p.dalgaard at biostat.ku.dk  Thu Dec  4 18:17:07 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 04 Dec 2003 18:17:07 +0100
Subject: [R] convert data
In-Reply-To: <3FCF6922.9020806@cs.uu.nl>
References: <3FCF6922.9020806@cs.uu.nl>
Message-ID: <x27k1cfqp8.fsf@biostat.ku.dk>

Muhammad Subianto <subianto at cs.uu.nl> writes:

> Dear R-helper,
> I have a data set like:
> 
> OLDa
> ALL
> OLDc
> OLDa
> OLDb
> NEW
> OLDb
> OLDa
> ALL
> . . .
> ALL
> OLDc
> NEW
> 
> I want to convert that data as OLDa=1, OLDb=2, OLDc=3, NEW=4 and ALL=5
> or the result like:
> 
> 1
> 5
> 3
> 1
> 2
> 4
> 2
> 1
> 5
> . . .
> 5
> 3
> 4
> 
> How can I do it. Thanks you for your help.


I'd do it like this:

> x <- scan(what="")
1: OLDa
2: ALL
3: OLDc
4: OLDa
5: OLDb
6: NEW
7: OLDb
8: OLDa
9: ALL
10:
Read 9 items
> f <- factor(x,levels=c("OLDa", "OLDb", "OLDc", "NEW", "ALL") )
> as.integer(f)
[1] 1 5 3 1 2 4 2 1 5


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From apjaworski at mmm.com  Thu Dec  4 18:20:15 2003
From: apjaworski at mmm.com (apjaworski@mmm.com)
Date: Thu, 4 Dec 2003 11:20:15 -0600
Subject: [R] bug in as.POSIXct ?
Message-ID: <OF2610D5B8.B91B50EC-ON86256DF2.00049B85-86256DF2.005F3D09@mmm.com>

I think that there is a bug in the as.POSIXct function on Windows.

 Here is what I get on Win2000, Pentium III machine in R 1.8.1.

> dd1 <- ISOdatetime(2003, 10, 26, 0, 59, 59)
> dd2 <- ISOdatetime(2003, 10, 26, 1, 0, 0)
> dd2 - dd1
Time difference of 1.000278 hours

Now, the 26th of October was the day that change to the standard time
occurred, so I suspect that this has something to do with that.  In fact

> dd1
[1] "2003-10-26 00:59:59 Central Daylight Time"
> dd2
[1] "2003-10-26 01:00:00 Central Standard Time"

so it looks like the switch from CDT to CST happens at 1:00 (instead of
2:00 ?).

Since the only thing the difftime function does is unclass the as.POSIXct
values of its two arguments, the error seems to be in the as.POSIXct code.
I looked at the C code of the as.POSIXct function but I do not know enough
about R code and C handling of the time structures to find an error there.
Perhaps, the error is actually in the C library handling of time structures
on Windows machines.


All this seems to work fine  in version 1.8.1 patched 11-24 on a Linux
machine, i.e. the above difference is 1second.  However, the swicth between
CDT and CSD also accurs at 1:00.


Andy

__________________________________
Andy Jaworski
518-1-01
Process Laboratory
3M Corporate Research Laboratory
-----
E-mail: apjaworski at mmm.com
Tel:  (651) 733-6092
Fax:  (651) 736-3122



From abunn at montana.edu  Thu Dec  4 18:19:56 2003
From: abunn at montana.edu (Andy Bunn)
Date: Thu, 4 Dec 2003 10:19:56 -0700
Subject: [R] convert data
In-Reply-To: <3FCF6922.9020806@cs.uu.nl>
Message-ID: <002d01c3ba8a$e8dd0d40$78f05a99@msu.montana.edu>

Do you just want to out the data vectors together?

Try ?scan, ?c, ?data.frame

my.data.frame <- data.frame( names = c(OLDa, ALL, OLDc, OLDa, OLDb,
NEW......), values = c(1,5,3,1,2,4.....)

HTH,
-Andy



From ripley at stats.ox.ac.uk  Thu Dec  4 18:55:24 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 4 Dec 2003 17:55:24 +0000 (GMT)
Subject: [R] Font-style
In-Reply-To: <LBELKNGGJOINKPAFNOOLEEMDCAAA.Lars-Konstanz@t-online.de>
Message-ID: <Pine.LNX.4.44.0312041753170.9128-100000@gannet.stats>

That depends on the plotting device: I answered this _yesterday_, 
so please consult the list archives.

Date: Wed, 3 Dec 2003 11:05:20 +0000 (GMT)
From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
To: Savano <savano at superig.com.br>
Subject: Re: [R] FONT TYPE


On Thu, 4 Dec 2003, Lars Peters wrote:

> how to change the font-style (e.g. Arial or Times) in Plots??

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andy_liaw at merck.com  Thu Dec  4 19:20:06 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 04 Dec 2003 13:20:06 -0500
Subject: [R] performance gap between R 1.7.1 and 1.8.0
Message-ID: <3A822319EB35174CA3714066D590DCD50205CEE8@usrymx25.merck.com>

> From: Martin Maechler [mailto:maechler at stat.math.ethz.ch] 
[...]
> A very first step of diagnosis might be to activate
>   trace(read.dcf)
>   trace(library)
>   options(verbose = TRUE)
> 
> A step further might be to patch read.dcf such that it prints
> info 
>        if(getOption("verbose")) { <<print what I am trying to read>> }
> 
> Martin

To refresh people's memory, the problem is that the identical code took more
than twice as long in R-1.8.x than in R-1.7.x.  I've managed to strip the
code down to the following and still see the same problem.  It basically
bootstraps mixture models to groups of data.  It uses boot() from the boot
package and flexmix() with intercept only, in the flexmix package.

=============================================
fitmix <- function(x, verbosefit=FALSE) {
  bpunlist <- function(x) {
    unlisted <- numeric()
    for(i in 1:length(x at components)) {
      unlisted <- c(unlisted, unlist(parameters(x, component=i)))
    }
    unlisted
  }
  unifit <-  flexmix(x ~ 1, k = 1, control=list(verbose=0))
  unlist(unlist(list(G = 1, proportion = unifit at prior,
                     param=matrix(bpunlist(unifit), nrow=2),
                     bic=BIC(unifit), loglik=unifit at logLik)))
}

bootmix <- function(data, i) {
  d <- data
  d.grps <- split.data.frame(d[i,], d[i, "id"])
  comps <- vector("list", length=length(d.grps))
  for (j in seq(along=comps)) {
    thefit <- try(fitmix(d.grps[[j]][,1], verbosefit=FALSE))
    comps[[j]] <- if (inherits(thefit, "try-error")) rep(NA, 48) else thefit
  }
  return(unlist(comps))
}

## The actual commands:
require(flexmix)
require(boot)
set.seed(76421)
x <- rnorm(3e3)
id <- gl(10, 300)
mydf <- data.frame(x, id)
Rprof(filename=paste("Rprof.out", system("hostname -s", int=TRUE), sep="."))
system.time(res <- boot(mydf, bootmix, strata=mydf[,"id"], R=5))
Rprof(NULL)
summaryRprof(filename=paste("Rprof.out", system("hostname -s", int=TRUE),
               sep="."))$by.self[1:20,]
=============================================

I ran this on a Xeon 2.4GHz running Mandrake 9.0, R compiled from source.

This took 2 seconds in R-1.7.1, with the output:

                  self.time self.pct total.time total.pct
"FUN"                  0.18        9       0.24        12
"names"                0.16        8       0.16         8
"initialize"           0.10        5       0.80        40
"apply"                0.08        4       0.30        15
"[.data.frame"         0.06        3       0.22        11
"inherits"             0.06        3       0.18         9
"is.null"              0.06        3       0.06         3
"match"                0.06        3       0.22        11
"seq.default"          0.06        3       0.06         3
"structure"            0.06        3       0.08         4
"any"                  0.04        2       0.14         7
"el"                   0.04        2       0.04         2
"FLXfit"               0.04        2       1.18        59
"lapply"               0.04        2       0.22        11
"lm.wfit"              0.04        2       0.16         8
"names<-.default"      0.04        2       0.04         2
"slot<-"               0.04        2       0.18         9
"<"                    0.02        1       0.02         1
"|"                    0.02        1       0.02         1
":"                    0.02        1       0.02         1

... and took over 7 seconds in R-1.8.1, with the output:

                self.time self.pct total.time total.pct
"paste"              0.52      6.7       1.10      14.1
"read.dcf"           0.38      4.9       0.96      12.3
"exists"             0.36      4.6       0.50       6.4
"lapply"             0.36      4.6       1.36      17.4
"names"              0.34      4.4       0.40       5.1
"names<-"            0.32      4.1       0.44       5.6
"inherits"           0.22      2.8       0.34       4.4
"=="                 0.20      2.6       0.20       2.6
".Call"              0.20      2.6       0.20       2.6
"seq"                0.18      2.3       0.50       6.4
"seq.default"        0.18      2.3       0.28       3.6
"dynGet"             0.16      2.1       0.46       5.9
"topenv"             0.16      2.1       0.18       2.3
"unique"             0.16      2.1       1.00      12.8
"apply"              0.14      1.8       0.20       2.6
"as.list"            0.14      1.8       0.18       2.3
".find.package"      0.14      1.8       1.88      24.1
"sapply"             0.14      1.8       1.54      19.7
"any"                0.12      1.5       1.68      21.5
"initialize"         0.12      1.5       4.98      63.8

I then did the following:

> trace(read.dcf, recover)
[1] "read.dcf"
> res <- boot(mydf, bootmix, strata=mydf[,"id"], R=1)
Tracing read.dcf(file.path(package.lib, package, "DESCRIPTION"), fields =
"Namespace") on entry 

Enter a frame number, or 0 to exit   
1:boot(mydf, bootmix, strata = mydf[, "id"], R = 1) 
2:statistic(data, original, ...) 
3:try(fitmix(d.grps[[j]][, 1], verbosefit = FALSE)) 
4:fitmix(d.grps[[j]][, 1], verbosefit = FALSE) 
5:flexmix(x ~ 1, k = 1, control = list(verbose = 0)) 
6:flexmix(x ~ 1, k = 1, control = list(verbose = 0)) 
7:flexmix(formula = formula, data = data, k = k, cluster = cluster, model =
list(F 
8:FLXglm() 
9:new("FLXmodel", weighted = TRUE, formula = formula, name = paste("FLXglm",
famil 
10:initialize(value, ...) 
11:initialize(value, ...) 
12:getClass(Class) 
13:.requirePackage(package) 
14:trySilent(loadNamespace(package)) 
15:eval.parent(call) 
16:eval(expr, p) 
17:eval(expr, envir, enclos) 
18:try(loadNamespace(package)) 
19:loadNamespace(package) 
20:packageHasNamespace(package, package.lib) 
21:read.dcf(file.path(package.lib, package, "DESCRIPTION"), fields =
"Namespace") 

I'm really out of ideas at this point.  Can anyone see the problem given the
above, or at least tell me what to try next?

Best,
Andy



From p.dalgaard at biostat.ku.dk  Thu Dec  4 19:44:18 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 04 Dec 2003 19:44:18 +0100
Subject: [R] performance gap between R 1.7.1 and 1.8.0
In-Reply-To: <3A822319EB35174CA3714066D590DCD50205CEE8@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50205CEE8@usrymx25.merck.com>
Message-ID: <x2zne8ifst.fsf@biostat.ku.dk>

"Liaw, Andy" <andy_liaw at merck.com> writes:

> I'm really out of ideas at this point.  Can anyone see the problem given the
> above, or at least tell me what to try next?

Hmm. Looking into the actual Rprof.out file is sometimes revealing.
Could you provide a link to it (I assume it might be a bit large for
email).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From andy_liaw at merck.com  Thu Dec  4 20:04:19 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 04 Dec 2003 14:04:19 -0500
Subject: [R] performance gap between R 1.7.1 and 1.8.0
Message-ID: <3A822319EB35174CA3714066D590DCD50205CEE9@usrymx25.merck.com>

> From: Peter Dalgaard [mailto:p.dalgaard at biostat.ku.dk] 
> "Liaw, Andy" <andy_liaw at merck.com> writes:
> 
> > I'm really out of ideas at this point.  Can anyone see the 
> problem given the
> > above, or at least tell me what to try next?
> 
> Hmm. Looking into the actual Rprof.out file is sometimes revealing.
> Could you provide a link to it (I assume it might be a bit large for
> email).

The files aren't that big, but it certainly isn't pretty (not intended for
human consumption, I suppose).

I don't have a place to post it, so I'm attaching it as .tgz.  Hopefully it
at least makes it to some...

Best,
Andy

 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: 
> (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: 
> (+45) 35327907
> 


From jasont at indigoindustrial.co.nz  Thu Dec  4 20:14:27 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Fri, 05 Dec 2003 08:14:27 +1300
Subject: [R] bug in as.POSIXct ?
In-Reply-To: <OF2610D5B8.B91B50EC-ON86256DF2.00049B85-86256DF2.005F3D09@mmm.com>
References: <OF2610D5B8.B91B50EC-ON86256DF2.00049B85-86256DF2.005F3D09@mmm.com>
Message-ID: <3FCF8793.8010800@indigoindustrial.co.nz>

apjaworski at mmm.com wrote:

> I think that there is a bug in the as.POSIXct function on Windows.
> 
>  Here is what I get on Win2000, Pentium III machine in R 1.8.1.
> 
> 
>>dd1 <- ISOdatetime(2003, 10, 26, 0, 59, 59)
>>dd2 <- ISOdatetime(2003, 10, 26, 1, 0, 0)
>>dd2 - dd1
> 
> Time difference of 1.000278 hours
> 
> Now, the 26th of October was the day that change to the standard time
> occurred, so I suspect that this has something to do with that.  In fact
> 
> 
>>dd1
> 
> [1] "2003-10-26 00:59:59 Central Daylight Time"
> 
>>dd2
> 
> [1] "2003-10-26 01:00:00 Central Standard Time"
> 
> so it looks like the switch from CDT to CST happens at 1:00 (instead of
> 2:00 ?).
> 

Or, it did happen at 2:00 CDT, when the time fell back one hour to 1:00 
CST.  1:00 am occured twice on that day, once as CDT and once as CST.  R 
picked the last one.  A bit pathological at first glance, but 
date-handling often is.

As for the dd2 - dd1 value, the "correct" value depends which 1:00 am 
was chosen.  On Windows, this should be 1 hour, 1 second, no?  I'm 
thinking 1:00 am CST == 2:00 am CDT, so in CDT entirely, your expression 
is basicly 02:00:00 CDT - 00:59:59 CDT.

This makes me suspect that Linux picked the former 1:00 am, from your 
report.  Since R gets its date intricacies from the OS, there really 
isn't much that can be done about this, until someone builds a full 
POSIX time implementation that takes all the world's locales and time 
zones into account, and welds it into R.  Volunteers?

It's things like this that make me convert everything to UCT (GMT, or 
Zulu, if you prefer).  Not R's fault; stupid calendar tricks are to 
blame here.

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From apjaworski at mmm.com  Thu Dec  4 20:42:24 2003
From: apjaworski at mmm.com (apjaworski@mmm.com)
Date: Thu, 4 Dec 2003 13:42:24 -0600
Subject: [R] bug in as.POSIXct ?
Message-ID: <OFB2692808.8088F01C-ON86256DF2.006B1AC4-86256DF2.006C40A4@mmm.com>


Thanks very much for your response and the earlier one by Professor Ripley.

The general problem is indeed a tricky one.

My particular problem was much simpler.  I had a bunch of data from a data
acquisition system with sampling interval of 1 minute.  The system used a
simple compression scheme, where a data point was reported only when the
change in response was sufficiently large.  For example, a fragment like
this

      Oct. 26 0:01:00       y1
      Oct. 26 0:05:00       y2

means that the values for 0:02, 0:03 0:04 where essentially y1.

I needed to "decompress" the data set, i.e., fill in the gaps, so I was
checking for differences of 1 minute and that is when I discovered the
"error".

I am not sure what the difference between Oct. 26 0:59:00 and Oct. 26 1:00
should really be, but in this particular application it had to be 1 minute.
Otherwise I generated 60 spurious gaps between these two times.

Andy


__________________________________
Andy Jaworski
518-1-01
Process Laboratory
3M Corporate Research Laboratory
-----
E-mail: apjaworski at mmm.com
Tel:  (651) 733-6092
Fax:  (651) 736-3122


|---------+------------------------------->
|         |           Jason Turner        |
|         |           <jasont at indigoindust|
|         |           rial.co.nz>         |
|         |                               |
|         |           12/05/2003 02:14    |
|         |                               |
|---------+------------------------------->
  >-----------------------------------------------------------------------------------------------------------------------------|
  |                                                                                                                             |
  |      To:       apjaworski at mmm.com                                                                                           |
  |      cc:       r-help at stat.math.ethz.ch                                                                                     |
  |      Subject:  Re: [R] bug in as.POSIXct ?                                                                                  |
  >-----------------------------------------------------------------------------------------------------------------------------|




apjaworski at mmm.com wrote:

> I think that there is a bug in the as.POSIXct function on Windows.
>
>  Here is what I get on Win2000, Pentium III machine in R 1.8.1.
>
>
>>dd1 <- ISOdatetime(2003, 10, 26, 0, 59, 59)
>>dd2 <- ISOdatetime(2003, 10, 26, 1, 0, 0)
>>dd2 - dd1
>
> Time difference of 1.000278 hours
>
> Now, the 26th of October was the day that change to the standard time
> occurred, so I suspect that this has something to do with that.  In fact
>
>
>>dd1
>
> [1] "2003-10-26 00:59:59 Central Daylight Time"
>
>>dd2
>
> [1] "2003-10-26 01:00:00 Central Standard Time"
>
> so it looks like the switch from CDT to CST happens at 1:00 (instead of
> 2:00 ?).
>

Or, it did happen at 2:00 CDT, when the time fell back one hour to 1:00
CST.  1:00 am occured twice on that day, once as CDT and once as CST.  R
picked the last one.  A bit pathological at first glance, but
date-handling often is.

As for the dd2 - dd1 value, the "correct" value depends which 1:00 am
was chosen.  On Windows, this should be 1 hour, 1 second, no?  I'm
thinking 1:00 am CST == 2:00 am CDT, so in CDT entirely, your expression
is basicly 02:00:00 CDT - 00:59:59 CDT.

This makes me suspect that Linux picked the former 1:00 am, from your
report.  Since R gets its date intricacies from the OS, there really
isn't much that can be done about this, until someone builds a full
POSIX time implementation that takes all the world's locales and time
zones into account, and welds it into R.  Volunteers?

It's things like this that make me convert everything to UCT (GMT, or
Zulu, if you prefer).  Not R's fault; stupid calendar tricks are to
blame here.

Cheers

Jason
--
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From jasont at indigoindustrial.co.nz  Thu Dec  4 21:04:55 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Fri, 05 Dec 2003 09:04:55 +1300
Subject: [R] bug in as.POSIXct ?
In-Reply-To: <OFB2692808.8088F01C-ON86256DF2.006B1AC4-86256DF2.006C40A4@mmm.com>
References: <OFB2692808.8088F01C-ON86256DF2.006B1AC4-86256DF2.006C40A4@mmm.com>
Message-ID: <3FCF9367.7010204@indigoindustrial.co.nz>

apjaworski at mmm.com wrote:
> My particular problem was much simpler.  I had a bunch of data from a data
> acquisition system with sampling interval of 1 minute.  The system used a
> simple compression scheme, where a data point was reported only when the
> change in response was sufficiently large.  For example, a fragment like
> this
> 
>       Oct. 26 0:01:00       y1
>       Oct. 26 0:05:00       y2
> 
> means that the values for 0:02, 0:03 0:04 where essentially y1.
> 
> I needed to "decompress" the data set, i.e., fill in the gaps, so I was
> checking for differences of 1 minute and that is when I discovered the
> "error".

I presume the data acq. system doesn't know about time zone changes? In 
cases like this, I specify the time zone as UTC for all analysis, and 
convert it back to my time zone for reporting.  If it does know time 
zones, you have to force the CST/CDT zone.  Import, convert to text, 
paste the appropriate string, convert to POSIXct.

Who wants the world to switch to "metric time"?  (raises hand). :)

Cheers

Jason


-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From munoz at cs.wisc.edu  Thu Dec  4 21:37:40 2003
From: munoz at cs.wisc.edu (Alejandro Munoz Del Rio)
Date: Thu,  4 Dec 2003 14:37:40 -0600
Subject: [R] assigning colors to barplot when beside=TRUE
Message-ID: <1070570260.3fcf9b148abe4@www-auth.cs.wisc.edu>

dear list,

i am having trouble coloring the bars in a barplot. my data have two
groups, which i would like to plot side by side. within each group i
want to sort the observations in decreasing order, like a pareto
chart. the bar colors would relfect the value of a third variable.

below i have generated a reproducible example. the bar heights are a
given pig's "gain", "type" identifies groups, and the color depends
on "day".

i would expect to get the five bars colored red, green, red, followed
by two whites. instead, i see: red red white red white. (i am aware
that in the code below "colv", the color vector, contains NAs, but
that does not seem to be the source of the problem).

i would be grateful for any help. 

alejandro

# pigs.dat
pig type day gain 
1   1    1   1
2   1    1   6
10  1    3   7
a   B    NA  3
b   B    NA  4

pigs <- read.table("~/pigs.dat", h=TRUE)
attach(pigs)
tbl <- tapply(gain, list(type=="B", pig), I)	# generate matrix
o1 <- rev(order(tbl[1, 1:3]))		# sort order for group 1
oB <- rev(order(tbl[1, 4:5]))+3		# ditto, group B
oday <- day[match(levels(pig), pig)]	# day of sacrifice ordered as pig/tbl
colv <- match(oday, c(1, 3))+1
barplot(tbl[,c(o1, oB)], beside=TRUE, ylim=c(0,8), xlab="pig", 
                ylab="weight gain (g)", col=colv)
   legend(12, 8, fill=c(2:3,NA), legend=c(paste("day", c(1,3)), "uninjured"))
   box()
detach()
rm(pigs, tbl, o1, oB, oday, colv)



-------------------------------------------------
This mail sent through IMP: http://horde.org/imp/



From kovac at stat-math.uni-essen.de  Thu Dec  4 22:02:19 2003
From: kovac at stat-math.uni-essen.de (Arne Kovac)
Date: Thu, 4 Dec 2003 22:02:19 +0100 (CET)
Subject: [R] Interrupt handling
Message-ID: <Pine.LNX.4.58.0312042148230.27455@zeus.stat-math.uni-essen.de>

Hi everyone,

I recently installed R 1.8.1 on some of our machines and noted quickly
that the handling of interrupts has changed since version 1.8.0. Are there
any plans to reintroduce the old behaviour as a feature when compiling R
or even better as a command line option? The problem is that we are
extensively calling C functions from R which take quite a large time to
finish. With the new version we had to kill the R process quite often and
at the end we sadly decided to reinstall version 1.7.1 on all of our
computers.

Sorry about my complaints, apart from this particular problem we are very
grateful for all the people developing this fantastic software.

Cheers, Arne



From ggrothendieck at myway.com  Thu Dec  4 22:03:00 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu,  4 Dec 2003 16:03:00 -0500 (EST)
Subject: [R] bug in as.POSIXct ?
Message-ID: <20031204210300.41B4439C8@mprdmxin.myway.com>



Not sure but perhaps this is related to bug PR#3646 reported in:

http://maths.newcastle.edu.au/~rking/R/devel/03b/0195.html


--- 
Date: Thu, 4 Dec 2003 11:20:15 -0600 
From: <apjaworski at mmm.com>
To: <r-help at stat.math.ethz.ch> 
Subject: [R] bug in as.POSIXct ? 

 
 
I think that there is a bug in the as.POSIXct function on Windows.

Here is what I get on Win2000, Pentium III machine in R 1.8.1.

> dd1 <- ISOdatetime(2003, 10, 26, 0, 59, 59)
> dd2 <- ISOdatetime(2003, 10, 26, 1, 0, 0)
> dd2 - dd1
Time difference of 1.000278 hours

Now, the 26th of October was the day that change to the standard time
occurred, so I suspect that this has something to do with that. In fact

> dd1
[1] "2003-10-26 00:59:59 Central Daylight Time"
> dd2
[1] "2003-10-26 01:00:00 Central Standard Time"

so it looks like the switch from CDT to CST happens at 1:00 (instead of
2:00 ?).

Since the only thing the difftime function does is unclass the as.POSIXct
values of its two arguments, the error seems to be in the as.POSIXct code.
I looked at the C code of the as.POSIXct function but I do not know enough
about R code and C handling of the time structures to find an error there.
Perhaps, the error is actually in the C library handling of time structures
on Windows machines.


All this seems to work fine in version 1.8.1 patched 11-24 on a Linux
machine, i.e. the above difference is 1second. However, the swicth between
CDT and CSD also accurs at 1:00.


Andy

__________________________________
Andy Jaworski
518-1-01
Process Laboratory
3M Corporate Research Laboratory
-----
E-mail: apjaworski at mmm.com
Tel: (651) 733-6092
Fax: (651) 736-3122



From ggrothendieck at myway.com  Thu Dec  4 22:55:35 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu,  4 Dec 2003 16:55:35 -0500 (EST)
Subject: [R] bug in as.POSIXct ?
Message-ID: <20031204215535.DFB0139DE@mprdmxin.myway.com>



The other strategy is to use the chron package for 
representing times as it does not use timezones.

---
Date: Thu, 4 Dec 2003 13:42:24 -0600 
From: <apjaworski at mmm.com>
To: Jason Turner <jasont at indigoindustrial.co.nz> 
Cc: <r-help at stat.math.ethz.ch> 
Subject: Re: [R] bug in as.POSIXct ? 

 
 

Thanks very much for your response and the earlier one by Professor Ripley.

The general problem is indeed a tricky one.

My particular problem was much simpler. I had a bunch of data from a data
acquisition system with sampling interval of 1 minute. The system used a
simple compression scheme, where a data point was reported only when the
change in response was sufficiently large. For example, a fragment like
this

Oct. 26 0:01:00 y1
Oct. 26 0:05:00 y2

means that the values for 0:02, 0:03 0:04 where essentially y1.

I needed to "decompress" the data set, i.e., fill in the gaps, so I was
checking for differences of 1 minute and that is when I discovered the
"error".

I am not sure what the difference between Oct. 26 0:59:00 and Oct. 26 1:00
should really be, but in this particular application it had to be 1 minute.
Otherwise I generated 60 spurious gaps between these two times.

Andy


__________________________________
Andy Jaworski
518-1-01
Process Laboratory
3M Corporate Research Laboratory
-----
E-mail: apjaworski at mmm.com
Tel: (651) 733-6092
Fax: (651) 736-3122


|---------+------------------------------->
| | Jason Turner |
| | <jasont at indigoindust|
| | rial.co.nz> |
| | |
| | 12/05/2003 02:14 |
| | |
|---------+------------------------------->
>-----------------------------------------------------------------------------------------------------------------------------|
| |
| To: apjaworski at mmm.com |
| cc: r-help at stat.math.ethz.ch |
| Subject: Re: [R] bug in as.POSIXct ? |
>-----------------------------------------------------------------------------------------------------------------------------|




apjaworski at mmm.com wrote:

> I think that there is a bug in the as.POSIXct function on Windows.
>
> Here is what I get on Win2000, Pentium III machine in R 1.8.1.
>
>
>>dd1 <- ISOdatetime(2003, 10, 26, 0, 59, 59)
>>dd2 <- ISOdatetime(2003, 10, 26, 1, 0, 0)
>>dd2 - dd1
>
> Time difference of 1.000278 hours
>
> Now, the 26th of October was the day that change to the standard time
> occurred, so I suspect that this has something to do with that. In fact
>
>
>>dd1
>
> [1] "2003-10-26 00:59:59 Central Daylight Time"
>
>>dd2
>
> [1] "2003-10-26 01:00:00 Central Standard Time"
>
> so it looks like the switch from CDT to CST happens at 1:00 (instead of
> 2:00 ?).
>

Or, it did happen at 2:00 CDT, when the time fell back one hour to 1:00
CST. 1:00 am occured twice on that day, once as CDT and once as CST. R
picked the last one. A bit pathological at first glance, but
date-handling often is.

As for the dd2 - dd1 value, the "correct" value depends which 1:00 am
was chosen. On Windows, this should be 1 hour, 1 second, no? I'm
thinking 1:00 am CST == 2:00 am CDT, so in CDT entirely, your expression
is basicly 02:00:00 CDT - 00:59:59 CDT.

This makes me suspect that Linux picked the former 1:00 am, from your
report. Since R gets its date intricacies from the OS, there really
isn't much that can be done about this, until someone builds a full
POSIX time implementation that takes all the world's locales and time
zones into account, and welds it into R. Volunteers?

It's things like this that make me convert everything to UCT (GMT, or
Zulu, if you prefer). Not R's fault; stupid calendar tricks are to
blame here.

Cheers

Jason
--
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From ripley at stats.ox.ac.uk  Thu Dec  4 23:09:36 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 4 Dec 2003 22:09:36 +0000 (GMT)
Subject: [R] Interrupt handling
In-Reply-To: <Pine.LNX.4.58.0312042148230.27455@zeus.stat-math.uni-essen.de>
Message-ID: <Pine.LNX.4.44.0312042200260.3946-100000@gannet.stats>

The short answer is `no', nothing has changed since 1.8,0.  There was a 
change at 1.8.0, and that was deliberate and described in the NEWS file.
You seem rather confused about this!

A slighter longer answer is that this depended on your unnamed OS having 
such interrupts, so for many R users there was no such facility.
Surely the right thing to do is to make your C code interruptible by 
inserting calls to R_CheckUserInterrupt() so interrupts occur at planned 
places and work on all platforms.  (That's a little harder to do with 
Fortran code, something I plan to address before 1.9.0.)

On Thu, 4 Dec 2003, Arne Kovac wrote:

> I recently installed R 1.8.1 on some of our machines and noted quickly
> that the handling of interrupts has changed since version 1.8.0. Are there
> any plans to reintroduce the old behaviour as a feature when compiling R
> or even better as a command line option? The problem is that we are
> extensively calling C functions from R which take quite a large time to
> finish. With the new version we had to kill the R process quite often and
> at the end we sadly decided to reinstall version 1.7.1 on all of our
> computers.
> 
> Sorry about my complaints, apart from this particular problem we are very
> grateful for all the people developing this fantastic software.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From atosuner at ixir.com  Wed Dec  3 23:38:50 2003
From: atosuner at ixir.com (ayhan tosuner)
Date: Thu, 4 Dec 2003 00:38:50 +0200
Subject: [R] Code for Hodrick-Prescott Filter
Message-ID: <002d01c3b9ee$39777170$164883d9@detay5sjh3y3o6>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031204/c1f76cca/attachment.pl

From p.murrell at auckland.ac.nz  Thu Dec  4 23:45:06 2003
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Fri, 05 Dec 2003 11:45:06 +1300
Subject: [R] assigning colors to barplot when beside=TRUE
References: <1070570260.3fcf9b148abe4@www-auth.cs.wisc.edu>
Message-ID: <3FCFB8F2.1030302@stat.auckland.ac.nz>

Hi


Alejandro Munoz Del Rio wrote:
> dear list,
> 
> i am having trouble coloring the bars in a barplot. my data have two
> groups, which i would like to plot side by side. within each group i
> want to sort the observations in decreasing order, like a pareto
> chart. the bar colors would relfect the value of a third variable.
> 
> below i have generated a reproducible example. the bar heights are a
> given pig's "gain", "type" identifies groups, and the color depends
> on "day".
> 
> i would expect to get the five bars colored red, green, red, followed
> by two whites. instead, i see: red red white red white. (i am aware
> that in the code below "colv", the color vector, contains NAs, but
> that does not seem to be the source of the problem).


I haven't figured out exactly what your code is supposed to produce, but 
I suspect that you are calculating colv incorrectly.  In your example, 
you get a colv with 5 elements.  There are a total of 10 bars plotted 
(some are zero height because the value they are plotting is NA).  The 
colours get applied to bars from left to right so in your case we get:

bar height:  7 NA  6 NA  1 NA NA  4 NA  3
bar colour:  2  3  2 NA NA  2  3  2 NA NA

(note that the colours are recycled).  I cannot yet suggest what the 
correct calculation of colv should be for your example, but the 
following colv setting would do the job in this case:

colv <- c(2, NA, 3, NA, 2, NA, NA, NA, NA, NA)

Now we get:

bar height:  7 NA  6 NA  1 NA NA  4 NA  3
bar colour:  2 NA  3 NA  2 NA NA NA NA NA

Hope that helps

Paul



> alejandro
> 
> # pigs.dat
> pig type day gain 
> 1   1    1   1
> 2   1    1   6
> 10  1    3   7
> a   B    NA  3
> b   B    NA  4
> 
> pigs <- read.table("~/pigs.dat", h=TRUE)
> attach(pigs)
> tbl <- tapply(gain, list(type=="B", pig), I)	# generate matrix
> o1 <- rev(order(tbl[1, 1:3]))		# sort order for group 1
> oB <- rev(order(tbl[1, 4:5]))+3		# ditto, group B
> oday <- day[match(levels(pig), pig)]	# day of sacrifice ordered as pig/tbl
> colv <- match(oday, c(1, 3))+1
> barplot(tbl[,c(o1, oB)], beside=TRUE, ylim=c(0,8), xlab="pig", 
>                 ylab="weight gain (g)", col=colv)
>    legend(12, 8, fill=c(2:3,NA), legend=c(paste("day", c(1,3)), "uninjured"))
>    box()
> detach()
> rm(pigs, tbl, o1, oB, oday, colv)
> 
> 
> 
> -------------------------------------------------
> This mail sent through IMP: http://horde.org/imp/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help


-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From p.dalgaard at biostat.ku.dk  Thu Dec  4 23:53:42 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 04 Dec 2003 23:53:42 +0100
Subject: [R] performance gap between R 1.7.1 and 1.8.0
In-Reply-To: <3A822319EB35174CA3714066D590DCD50205CEE9@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50205CEE9@usrymx25.merck.com>
Message-ID: <x2vfowi495.fsf@biostat.ku.dk>

"Liaw, Andy" <andy_liaw at merck.com> writes:

> > From: Peter Dalgaard [mailto:p.dalgaard at biostat.ku.dk] 
> > "Liaw, Andy" <andy_liaw at merck.com> writes:
> > 
> > > I'm really out of ideas at this point.  Can anyone see the 
> > problem given the
> > > above, or at least tell me what to try next?
> > 
> > Hmm. Looking into the actual Rprof.out file is sometimes revealing.
> > Could you provide a link to it (I assume it might be a bit large for
> > email).
> 
> The files aren't that big, but it certainly isn't pretty (not intended for
> human consumption, I suppose).
> 
> I don't have a place to post it, so I'm attaching it as .tgz.  Hopefully it
> at least makes it to some...

Everyone directly addressed, I'd expect (i.e. not r-help). As far as I
can see the main story is the pattern

... "loadNamespace" "try"...".requirePackage" "getClass"..."flexmix" ...

i.e lots of calls to getClass trigger an attempt to load the namespace
for the package

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Ted.Harding at nessie.mcc.ac.uk  Thu Dec  4 23:09:06 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 04 Dec 2003 22:09:06 -0000 (GMT)
Subject: [R] volume of an irregular grid
In-Reply-To: <3FCE4E0F.6ADDE203@ysbl.york.ac.uk>
Message-ID: <XFMail.031204220906.Ted.Harding@nessie.mcc.ac.uk>

On 03-Dec-03 Karim Elsawy wrote:
> I have a 3d irregular grid of a surface (closed surface)
> I would like to calculate the volume enclosed inside this surface
> can this be done in R
> any help is very much appreciated
> best regards
> karim

Hi Karim,

You should be able to create your own function for this fairly
straightforwardly, on the following lines.

First you will need to pre-process the data of the grid on the
surface. I assume that the grid is equivalent to approximating
the surface as a set of planar facets.

First, determine for each facet the coordinates (X,Y,Z) of a
point (perhaps, conveniently, its centroid) in the plane of the
facet.

Next, determine the area A of each facet.

Next, determine for each facet the direction of the normal to
the plane of the facet which points _out of_ the surface,
as a unit vector (L,M,N).

You can then set up a vector A and two matrices:
  P with columns (X,Y,Z), and
  D with columns (L,M,N).
each with as many rows as there are facets.

Now take an arbitrary point C; but, for best accuracy, it should
be well within the surface and could be at the centroid of the
facets:

  C<-colMeans(A*X)

Now, for each facet, you want the line joining C to P=(X,Y,Z):

  CP<-(X,Y,X)-C

and also as a unit vector:

  CP1<-CP/sqrt(sum(CP*CP))

Next get the cosine of the angle between CP and the normal
D=(L,M,N):

  gamma<-sum(D*CP1)

Finally, sum

  (1/3)*gamma*A*sqrt(sum(CP*CP))

over all the facets.

This will give you the volume contained within the surface
constituted by the facets.

NB: the above outline calculations refer to each facet separately,
i.e. as if taking the matrices row by row. Vectorising it so as to
do the whole thing in one pass, without loops, should be a nice
exercise in the use of 'sweep'

Hoping this helps,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 04-Dec-03                                       Time: 22:09:06
------------------------------ XFMail ------------------------------



From munoz at cs.wisc.edu  Fri Dec  5 00:12:54 2003
From: munoz at cs.wisc.edu (Alejandro Munoz Del Rio)
Date: Thu,  4 Dec 2003 17:12:54 -0600
Subject: [R] assigning colors to barplot when beside=TRUE
In-Reply-To: <3FCFB8F2.1030302@stat.auckland.ac.nz>
References: <1070570260.3fcf9b148abe4@www-auth.cs.wisc.edu>
	<3FCFB8F2.1030302@stat.auckland.ac.nz>
Message-ID: <1070579574.3fcfbf76b4d17@www-auth.cs.wisc.edu>

Quoting Paul Murrell <p.murrell at auckland.ac.nz>:
> I haven't figured out exactly what your code is supposed to produce, but 
> I suspect that you are calculating colv incorrectly.  In your example, 
> you get a colv with 5 elements.  There are a total of 10 bars plotted 
> (some are zero height because the value they are plotting is NA).  The 
> colours get applied to bars from left to right so in your case we get:
> 
> bar height:  7 NA  6 NA  1 NA NA  4 NA  3
> bar colour:  2  3  2 NA NA  2  3  2 NA NA
[snip]
your hunch is correct. setting:

colv <- rep(colv, each=2)

to "echo" each entry in the old colv solves my problem. i was not aware of the 
zero-height bars.

thanks!

alejandro

-------------------------------------------------
This mail sent through IMP: http://horde.org/imp/



From kovac at stat-math.uni-essen.de  Fri Dec  5 00:31:05 2003
From: kovac at stat-math.uni-essen.de (Arne Kovac)
Date: Fri, 5 Dec 2003 00:31:05 +0100 (CET)
Subject: [R] Interrupt handling
In-Reply-To: <Pine.LNX.4.44.0312042200260.3946-100000@gannet.stats>
References: <Pine.LNX.4.44.0312042200260.3946-100000@gannet.stats>
Message-ID: <Pine.LNX.4.58.0312042353430.27725@zeus.stat-math.uni-essen.de>

I had noticed that the change was deliberate, otherwise I would have
thought that this was a bug...

Your suggestion to use R_CheckUserInterrupt() might certainly be a
reasonable solution for many users of R. However, I would still prefer to
be able to interrupt my C code by just pressing CTRL-C and not to worry
about putting R_CheckUserInterrupt() commands into several places of my
program when searching long and complicated source code for some bug. And
I also prefer to press CTRL-C when I'm working with some function from
some CRAN package, start to think that my data vector was too large for
that function and want to try out the function on a subset of the data.  
Finally, as you already noted, there are people around who write their
programs in Fortran.

I'm afraid I will stick to the old version 1.7.1 until the advantages of 
the current versions become more compelling :-(

BTW: The unnamed OS was Linux.


> The short answer is `no', nothing has changed since 1.8,0.  There was a 
> change at 1.8.0, and that was deliberate and described in the NEWS file.
> You seem rather confused about this!
> 
> A slighter longer answer is that this depended on your unnamed OS having 
> such interrupts, so for many R users there was no such facility.
> Surely the right thing to do is to make your C code interruptible by 
> inserting calls to R_CheckUserInterrupt() so interrupts occur at planned 
> places and work on all platforms.  (That's a little harder to do with 
> Fortran code, something I plan to address before 1.9.0.)
> 
> On Thu, 4 Dec 2003, Arne Kovac wrote:
> 
> > I recently installed R 1.8.1 on some of our machines and noted quickly
> > that the handling of interrupts has changed since version 1.8.0. Are there
> > any plans to reintroduce the old behaviour as a feature when compiling R
> > or even better as a command line option? The problem is that we are
> > extensively calling C functions from R which take quite a large time to
> > finish. With the new version we had to kill the R process quite often and
> > at the end we sadly decided to reinstall version 1.7.1 on all of our
> > computers.
> > 
> > Sorry about my complaints, apart from this particular problem we are very
> > grateful for all the people developing this fantastic software.
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From j.byrne at mackillop.acu.edu.au  Fri Dec  5 00:40:29 2003
From: j.byrne at mackillop.acu.edu.au (john byrne)
Date: Fri, 5 Dec 2003 10:40:29 +1100
Subject: [R] Processing calendar dates with R
Message-ID: <OF3CD10F94.464D7B6C-ONCA256DEA.001370F2-CA256DF2.00820C25@mackillop.acu.edu.au>





I am a beginner in R with a background in SAS.

Are there built-in R methods of reading dates for calculating elapsed days
between two calendar dates?  If so, are there any examples I can browse?

Thanks in anticipation.

John Byrne.
Lecturer in Information Systems.
Australian Catholic University.



From abunn at montana.edu  Fri Dec  5 00:39:52 2003
From: abunn at montana.edu (Andy Bunn)
Date: Thu, 4 Dec 2003 16:39:52 -0700
Subject: [R] Processing calendar dates with R
In-Reply-To: <OF3CD10F94.464D7B6C-ONCA256DEA.001370F2-CA256DF2.00820C25@mackillop.acu.edu.au>
Message-ID: <001001c3babf$fbd36180$78f05a99@msu.montana.edu>

See:
?DateTimeClasses
?as.POSIXct
?strptime
?difftime 


The example for  difftime should help:

(z <- Sys.time() - 3600)
Sys.time() - z                # just over 3600 seconds.

## time interval between releases of 1.2.2 and 1.2.3.
ISOdate(2001, 4, 26) - ISOdate(2001, 2, 26)

as.difftime(c("0:3:20", "11:23:15"))
as.difftime(c("3:20", "23:15", "2:"), format= "%H:%M")# 3rd gives NA

HTH, Andy



From abunn at montana.edu  Fri Dec  5 00:40:22 2003
From: abunn at montana.edu (Andy Bunn)
Date: Thu, 4 Dec 2003 16:40:22 -0700
Subject: [R] Processing calendar dates with R
In-Reply-To: <OF3CD10F94.464D7B6C-ONCA256DEA.001370F2-CA256DF2.00820C25@mackillop.acu.edu.au>
Message-ID: <001101c3babf$fbf98720$78f05a99@msu.montana.edu>

See:
?DateTimeClasses
?as.POSIXct
?strptime
?difftime 


The example for  difftime should help:

(z <- Sys.time() - 3600)
Sys.time() - z                # just over 3600 seconds.

## time interval between releases of 1.2.2 and 1.2.3.
ISOdate(2001, 4, 26) - ISOdate(2001, 2, 26)

as.difftime(c("0:3:20", "11:23:15"))
as.difftime(c("3:20", "23:15", "2:"), format= "%H:%M")# 3rd gives NA

HTH, Andy



From york at zipcon.net  Fri Dec  5 00:58:32 2003
From: york at zipcon.net (Anne York)
Date: Thu, 4 Dec 2003 15:58:32 -0800 (PST)
Subject: [R] passing par() options to pdf under Sweave
Message-ID: <Pine.LNX.4.44.0312041546530.26419-100000@localhost.localdomain>

I'm using Sweave to generate LaTeX output from R. Thank-you 
very much for this capability. It is very useful.

Could someone please tell me how to pass non-default 
graphics parameters from par() to the pdf driver when it is 
called by Sweave? 

(I need to change some of the margin defaults. )

Hoping there is an easy solution to this question,

Anne York



From Catherine.Wang at infotech.monash.edu.au  Fri Dec  5 00:52:05 2003
From: Catherine.Wang at infotech.monash.edu.au (Xiaozhe Wang)
Date: Fri, 05 Dec 2003 10:52:05 +1100
Subject: [R] R code for estimating Hurst exponent
Message-ID: <c5334dc5448c.c5448cc5334d@mail1.monash.edu.au>

Has anyone writen R code for estimating Hurst exponent with R/S method 
or other methods?
or any other source of R code available?

Many thanks 

Catherine Wang



From karlknoblich at yahoo.de  Fri Dec  5 01:13:08 2003
From: karlknoblich at yahoo.de (=?iso-8859-1?q?Karl=20Knoblick?=)
Date: Fri, 5 Dec 2003 01:13:08 +0100 (CET)
Subject: [R] Model (aov): CI of interaction 
Message-ID: <20031205001308.98449.qmail@web10006.mail.yahoo.com>


Hallo!

I have the a model with 3 time points, 2 treatments
and N subjects. I can calculate an ANOVA but I can not
calculate the CI of the interaction term (time and
treatment), which I need for a closer look at the
effect of the treatment to the 3 time points. I do NOT
want to use lme because I can not manage it to
reproduce text book examples (see my posting [R] lme:
reproducing example Karl Knoblick (Tue 02 Dec 2003 -
21:34:54 EST)).

Here some sample data:

# Data
ID<-factor(rep(1:35,each=3)) # 35 subjects
TREAT<-factor(c(rep("A", 60), rep("B", 45)))
TIME<-factor(rep(1:3, 35))
Y<-numeric(length=105)
set.seed(1234)
Y<-rnorm(105)
Y[TREAT=="A" & TIME==2]<-Y[TREAT=="A" & TIME==2] + 1 #
want to see an effect!
DF<-data.frame(Y, ID, TREAT, TIME)

# 2 possible designs:
# Design 1 with random term
DF.aov1<-aov(Y ~ TIME*TREAT + Error(TREAT:ID),
data=DF)
summary(DF.aov1)
# Design 2 without random term
DF.aov2<-aov(Y ~ TIME*TREAT, data=DF)
summary(DF.aov2)

I am also not sure about the design - I think design 1
is more appropriate.

What I have tried is to calculate the CI of the
coefficients:
confint(DF.aov1[[2]])
confint(DF.aov1[[3]])

(or:
confint(DF.aov2)
)

But how can I get the CI for a concrete difference for
example between the treatments at time point 2?

I really hope, sombody can help!

Karl



From jc at or.psychology.dal.ca  Fri Dec  5 01:17:44 2003
From: jc at or.psychology.dal.ca (John Christie)
Date: Thu, 04 Dec 2003 20:17:44 -0400
Subject: [R] simple sphericity test using bartlett and cov?
Message-ID: <72C5F5B2-26B8-11D8-AD7E-000A956DE534@or.psychology.dal.ca>

Hey,
	Is this right for a sphericity test?

x <- mymatrix
mycov <- cov(x)
bartlett.text (array (mycov), myconditions)



From york at zipcon.net  Fri Dec  5 01:31:46 2003
From: york at zipcon.net (Anne York)
Date: Thu, 4 Dec 2003 16:31:46 -0800 (PST)
Subject: [R] Re: passing par() options to pdf under Sweave
In-Reply-To: <Pine.LNX.4.44.0312041546530.26419-100000@localhost.localdomain>
Message-ID: <Pine.LNX.4.44.0312041630000.27112-100000@localhost.localdomain>

Sorry for wasting the list for this. Had a blind spot.  It 
is very easy. Just  include the par() commands inside the R 
chunks that generate the plots. 

Anne

On Thu, 4 Dec 2003, Anne York wrote:

> I'm using Sweave to generate LaTeX output from R. Thank-you 
> very much for this capability. It is very useful.
> 
> Could someone please tell me how to pass non-default 
> graphics parameters from par() to the pdf driver when it is 
> called by Sweave? 
> 
> (I need to change some of the margin defaults. )
> 
> Hoping there is an easy solution to this question,
> 
> Anne York
> 
> 
>



From ok at cs.otago.ac.nz  Fri Dec  5 04:05:48 2003
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Fri, 5 Dec 2003 16:05:48 +1300 (NZDT)
Subject: [R] Selecting subsamples
Message-ID: <200312050305.hB535mf1005848@atlas.otago.ac.nz>

christian_mora at vtr.net wrote
    [that he has a data set with 9 variables (columns) measured on 2000
     individuals (rows) and wants a sample] in which the sum of the
    volume of the individuals in that sample >= 100 cubic m.

Let's suppose that this information is held in d, a data frame, and that
the volume column is d$vol.

If sum(d$vol) < 100, there is no sample which satisfies your condition.
If sum(d$vol) >= 100, then d is such a sample as it stands.

If you want the smallest number of rows, then

    indices <- order(d$vol, decreasing=TRUE)

gives you the row indices sorted by decreasing volume;

    d$vol[indices]	=> the volumes in decreasing order
    cumsum(")           => the cumulative sum
    sum(" < 100.0)	=> 1 less than then number of rows you want

so

    indices <- order(d$vol, decreasing=TRUE)
    d[indices[1:(sum(cumsum(d$vol[indices]) < 100.0) + 1)]]

should be the answer you want.

This is O(n.lg n) where n is the number of rows; in your case n is 2000.

If you don't need the smallest sample, but just any old haphazard answer,

    indices <- sample(nrow(d))
    d[indices[1:(sum(cumsum(d$vol[indices]) < 100.0) + 1)]]

should be useful.



From p.connolly at hortresearch.co.nz  Fri Dec  5 04:19:55 2003
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Fri, 5 Dec 2003 16:19:55 +1300
Subject: [R] grid packages since R-1.7.1
Message-ID: <20031205031955.GR28127@hortresearch.co.nz>

I'm having a spot of bother using code that worked with R-1.7.1 but
will not work with 1.8.1.

The beginning of the saga is with grid.polygon ostensibly not
findable.  One does exist in ..../R-1.8.1/library/grid/R, and when I
specifically load the grid package (which probably isn't a good idea),
it starts finding fault with the length of vectors being unequal.

I suspect the lengths of the vectors has more to do with a scoping
error.  If I understood why grid.polygon wasn't being found, and what
I should do about it instead of what I did, I suspect the length of
vectors problem would vanish.  It works as I'd expect in 1.7.1.

I've been using this same .Rprofile for all versions:

options(defaultPackages =  c("mva", "lattice"), keep.source.pkgs = TRUE)

It's rather unlikely that I should specifically load grid also, but
perhaps some other tricky dependency is happening.

I probably could do the rest of the project with 1.7.1, but I'd like
to be tidier than that.

Ideas appreciated.

Thanks


-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From ok at cs.otago.ac.nz  Fri Dec  5 04:27:48 2003
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Fri, 5 Dec 2003 16:27:48 +1300 (NZDT)
Subject: [R] get mean of several rows
Message-ID: <200312050327.hB53RmPJ006261@atlas.otago.ac.nz>

"Jan Wantia" <wantia at ifi.unizh.ch> asked:
	I have a 2-dimensional array, and I know how to split it into its rows 
	and how to get the mean for every row using 'sapply'.
	But what I want is to calculate the mean over the first n rows, and then 
	the second n rows, etc., so that I get a vector like:
	
	v == mean1(row 1:5), mean2(row6:10),...
	
	(trivial, you might say.

There are lots of ways to do it.
Let Arr be a p=nrow(Arr) by q=ncol(Arr) array.
Let p %% n = 0, for simplicity.

Let 1 <= k <= p %/% n.
Then the rows which should contribute to the kth mean are
(k-1)*n+1 .. k*n.
mean(Arr[((k-1)*n+1):(k*n)])
will therefore give you the kth mean.

So

    sapply(1:(nrow(Arr)%/%n), function (k) mean(Arr[((k-1)*n+1):(k*n),]))

should do the trick.  I've tested it on a small example where I knew the
answers, and it worked.  This assumes that I've understood the question...

I'm a bit annoyed, because I thought of several really cute ways to do
this, one involving cumsum(t(Arr)) and diff(), but this is so direct that
the others might only be confusing.



From deepayan at stat.wisc.edu  Fri Dec  5 06:23:12 2003
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 4 Dec 2003 23:23:12 -0600
Subject: [R] grid packages since R-1.7.1
In-Reply-To: <20031205031955.GR28127@hortresearch.co.nz>
References: <20031205031955.GR28127@hortresearch.co.nz>
Message-ID: <200312042323.12324.deepayan@stat.wisc.edu>

On Thursday 04 December 2003 21:19, Patrick Connolly wrote:
> I'm having a spot of bother using code that worked with R-1.7.1 but
> will not work with 1.8.1.
>
> The beginning of the saga is with grid.polygon ostensibly not
> findable.  One does exist in ..../R-1.8.1/library/grid/R, and when I
> specifically load the grid package (which probably isn't a good idea),
> it starts finding fault with the length of vectors being unequal.
>
> I suspect the lengths of the vectors has more to do with a scoping
> error.  If I understood why grid.polygon wasn't being found, and what
> I should do about it instead of what I did, I suspect the length of
> vectors problem would vanish.  It works as I'd expect in 1.7.1.
>
> I've been using this same .Rprofile for all versions:
>
> options(defaultPackages =  c("mva", "lattice"), keep.source.pkgs = TRUE)
>
> It's rather unlikely that I should specifically load grid also, but

That's exactly what you need to do (i.e., load grid explicitly). This has to 
do with namespaces, which makes some earlier notions obsolete. lattice 
'imports' grid function definitions but by itself no longer makes them 
available/visible to the user.

This of course doesn't explain any problems in grid.polygon after loading 
grid. grid.polygon has changed in 1.8.1, but I think it is supposed to be 
back-compatible. Could you provide a reproducible example ? 

Deepayan



From dolic72 at net.hr  Fri Dec  5 07:56:10 2003
From: dolic72 at net.hr (D. Dolic)
Date: Fri, 5 Dec 2003 07:56:10 +0100
Subject: [R] Processing calendar dates with R
Message-ID: <3fd02c0a.2938.0@net.hr>

Hi John.

I'm a beginner in SAS with a background in R ;-)

Browse the HTML Help "help.start()" and search for keywords Date or time.
You will find several function managing this task. You can do Your MDY that
You know from SAS but the R functions are a bit superior and more flexible
to those given by SAS...

>
>
>
>
>I am a beginner in R with a background in SAS.
>
>Are there built-in R methods of reading dates for calculating elapsed days
>between two calendar dates?  If so, are there any examples I can browse?
>
>Thanks in anticipation.
>
>John Byrne.
>Lecturer in Information Systems.
>Australian Catholic University.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>

--
Trebate bolji pristup internetu?
Nazovite IskonInternet na 0800 1000 ili pogledajte
http://www.iskon.biz/individualni/usluge/dialup/



From Friedrich.Leisch at ci.tuwien.ac.at  Fri Dec  5 08:15:22 2003
From: Friedrich.Leisch at ci.tuwien.ac.at (Friedrich.Leisch@ci.tuwien.ac.at)
Date: Fri, 5 Dec 2003 08:15:22 +0100
Subject: [R] passing par() options to pdf under Sweave
In-Reply-To: <Pine.LNX.4.44.0312041546530.26419-100000@localhost.localdomain>
References: <Pine.LNX.4.44.0312041546530.26419-100000@localhost.localdomain>
Message-ID: <16336.12426.383469.216383@celeborn.leisch.at>

>>>>> On Thu, 4 Dec 2003 15:58:32 -0800 (PST),
>>>>> Anne York (AY) wrote:

  > I'm using Sweave to generate LaTeX output from R. Thank-you 
  > very much for this capability. It is very useful.

  > Could someone please tell me how to pass non-default 
  > graphics parameters from par() to the pdf driver when it is 
  > called by Sweave? 

  > (I need to change some of the margin defaults. )

  > Hoping there is an easy solution to this question,

Yes, simply define a hook function for figure chunks: The effect of

options(SweaveHooks=list(fig=function() par(bg="red", fg="blue")))


should be easy to see :-) The hook function will be executed at the
beginning of each chunk where fig=true (i.e., all figure chunks).


What you want is something like

options(SweaveHooks=list(fig=function() par(mar=c(1,2,3,4))))


Hope this helps,

-- 
-------------------------------------------------------------------
                        Friedrich  Leisch 
Institut f?r Statistik                     Tel: (+43 1) 58801 10715
Technische Universit?t Wien                Fax: (+43 1) 58801 10798
Wiedner Hauptstra?e 8-10/1071      Friedrich.Leisch at ci.tuwien.ac.at
A-1040 Wien, Austria             http://www.ci.tuwien.ac.at/~leisch



From raftery at stat.washington.edu  Fri Dec  5 04:34:41 2003
From: raftery at stat.washington.edu (Adrian Raftery)
Date: Thu, 4 Dec 2003 19:34:41 -0800 (PST)
Subject: [R] [R-pkgs] Robust Covariance Estimation (NNVE) Package Released
Message-ID: <Pine.OSF.4.58.0312041933390.334531@lisbon2.stat.washington.edu>

 Robust Covariance Estimation Software via Nearest Neighbor Variance Estimation (NNVE)

Software to carry out robust covariance estimation by Nearest Neighbor
Variance Estimation (NNVE) [Wang and Raftery (2002, J. Amer. Statist. Ass.)]
is now available for R and Splus. In the simulation studies published in JASA,
this had mean squared error at least 100 times smaller than that of
other leading covariance estimators when the proportion of outliers was high.

cov.nnve is now available in the covRobust contributed package at
http://cran.r-project.org/src/contrib/PACKAGES.html#covRobust

An Splus version is also available on the S archive of Statlib
http://lib.stat.cmu.edu/S/
under the function name cov.nnve

cov.nnve is by Naisyin Wang and Adrian Raftery, with contributions by Chris Fraley.

References:

Wang, N. and Raftery. A.E. (December 2002). Nearest-neighbor variance estimation (NNVE):
 Robust covariance estimation via nearest-neighbor cleaning (with Discussion).
 Journal of the American Statistical Association 97(460): 994-1019.

Wang, N. and Raftery. A.E. (2000). Nearest-neighbor variance estimation (NNVE):
 Robust covariance estimation via nearest-neighbor cleaning.
 Technical Report no. 368, Department of Statistics, University of Washington.
 Available at www.stat.washington.edu/www/research/reports


 -------------------------------------------------------------------
 Adrian E. Raftery
 Professor of Statistics and Sociology
 Director, Center for Statistics and the Social Sciences
 University of Washington, Box 354320	 Phone: (206) 543-4505
 Seattle, WA 98195-4320.		 FAX:   (206) 221-6873
 Web: www.stat.washington.edu/raftery;   www.csss.washington.edu
 -------------------------------------------------------------------

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://www.stat.math.ethz.ch/mailman/listinfo/r-packages



From luke at stat.uiowa.edu  Fri Dec  5 09:11:09 2003
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Fri, 5 Dec 2003 02:11:09 -0600 (CST)
Subject: [R] performance gap between R 1.7.1 and 1.8.0
In-Reply-To: <3A822319EB35174CA3714066D590DCD50205CEE8@usrymx25.merck.com>
Message-ID: <Pine.LNX.4.44.0312050120020.5227-100000@itasca2.stat.uiowa.edu>

If you look at the by.total components what you get is that for 1.7.1

>  summaryRprof("Rprof.out.1.7.1")$by.total[1:30,]
                   total.time total.pct self.time self.pct
"boot"                   1.72     100.0      0.00      0.0
...
"validObject"            0.14       8.1      0.00      0.0

and for R-devel

>  summaryRprof("Rprof.out.1.9.0")$by.total[1:30,]
                  total.time total.pct self.time self.pct
"source"                3.52     100.0      0.00      0.0
"boot"                  3.42      97.2      0.00      0.0
...
"validObject"           1.52      43.2      0.04      1.1
"findClass"             1.30      36.9      0.00      0.0
".requirePackage"       1.24      35.2      0.02      0.6

So it seems most of the increased time is coming from validObject, and
in particular from calls to findClass and then .requirePackage that
occur there.  The .requirePackage code does

> methods:::.requirePackage
function (package, useNamespace = FALSE) 
{
    ...
    if (is.character(package) && package != "flexmix") 
        value <- trySilent(getNamespace(package))
    ...
}

In this case it is called with "flexmix".  If flexmix had a name space
this would be a quick operation since the name space would be found in
the internal cache (it would be better to use getNamespace here since
that returns faster if the name space is loaded). But since flexmix
does not have a name space, the load fails every time, but not until
it has gone through a fair amount of effort to (re)discover that there
is no name space.

So the methods code needs to do some more caching of information here.
If I fiddle .requirePackage to not attempt to load flexmix as a name
space then the timings are

R-devel:	2.02
1.7.1:		1.72

validObject is still twice as slow in R-devel as in 1.7.1, which
accounts for about half the remaining speed loss, but that may be due
to its doing more useful validation.  Plotting the total timings for
the common functions under 1.7.1 and the modified R-devel that doesn't
try to load a flexmix name space suggest there may be a few otehr
places in methods that could use a bit of tuning, but minor compared
to the main issue here.

Luke

On Thu, 4 Dec 2003, Liaw, Andy wrote:

> > From: Martin Maechler [mailto:maechler at stat.math.ethz.ch] 
> [...]
> > A very first step of diagnosis might be to activate
> >   trace(read.dcf)
> >   trace(library)
> >   options(verbose = TRUE)
> > 
> > A step further might be to patch read.dcf such that it prints
> > info 
> >        if(getOption("verbose")) { <<print what I am trying to read>> }
> > 
> > Martin
> 
> To refresh people's memory, the problem is that the identical code took more
> than twice as long in R-1.8.x than in R-1.7.x.  I've managed to strip the
> code down to the following and still see the same problem.  It basically
> bootstraps mixture models to groups of data.  It uses boot() from the boot
> package and flexmix() with intercept only, in the flexmix package.
> 
> =============================================
> fitmix <- function(x, verbosefit=FALSE) {
>   bpunlist <- function(x) {
>     unlisted <- numeric()
>     for(i in 1:length(x at components)) {
>       unlisted <- c(unlisted, unlist(parameters(x, component=i)))
>     }
>     unlisted
>   }
>   unifit <-  flexmix(x ~ 1, k = 1, control=list(verbose=0))
>   unlist(unlist(list(G = 1, proportion = unifit at prior,
>                      param=matrix(bpunlist(unifit), nrow=2),
>                      bic=BIC(unifit), loglik=unifit at logLik)))
> }
> 
> bootmix <- function(data, i) {
>   d <- data
>   d.grps <- split.data.frame(d[i,], d[i, "id"])
>   comps <- vector("list", length=length(d.grps))
>   for (j in seq(along=comps)) {
>     thefit <- try(fitmix(d.grps[[j]][,1], verbosefit=FALSE))
>     comps[[j]] <- if (inherits(thefit, "try-error")) rep(NA, 48) else thefit
>   }
>   return(unlist(comps))
> }
> 
> ## The actual commands:
> require(flexmix)
> require(boot)
> set.seed(76421)
> x <- rnorm(3e3)
> id <- gl(10, 300)
> mydf <- data.frame(x, id)
> Rprof(filename=paste("Rprof.out", system("hostname -s", int=TRUE), sep="."))
> system.time(res <- boot(mydf, bootmix, strata=mydf[,"id"], R=5))
> Rprof(NULL)
> summaryRprof(filename=paste("Rprof.out", system("hostname -s", int=TRUE),
>                sep="."))$by.self[1:20,]
> =============================================
> 
> I ran this on a Xeon 2.4GHz running Mandrake 9.0, R compiled from source.
> 
> This took 2 seconds in R-1.7.1, with the output:
> 
>                   self.time self.pct total.time total.pct
> "FUN"                  0.18        9       0.24        12
> "names"                0.16        8       0.16         8
> "initialize"           0.10        5       0.80        40
> "apply"                0.08        4       0.30        15
> "[.data.frame"         0.06        3       0.22        11
> "inherits"             0.06        3       0.18         9
> "is.null"              0.06        3       0.06         3
> "match"                0.06        3       0.22        11
> "seq.default"          0.06        3       0.06         3
> "structure"            0.06        3       0.08         4
> "any"                  0.04        2       0.14         7
> "el"                   0.04        2       0.04         2
> "FLXfit"               0.04        2       1.18        59
> "lapply"               0.04        2       0.22        11
> "lm.wfit"              0.04        2       0.16         8
> "names<-.default"      0.04        2       0.04         2
> "slot<-"               0.04        2       0.18         9
> "<"                    0.02        1       0.02         1
> "|"                    0.02        1       0.02         1
> ":"                    0.02        1       0.02         1
> 
> ... and took over 7 seconds in R-1.8.1, with the output:
> 
>                 self.time self.pct total.time total.pct
> "paste"              0.52      6.7       1.10      14.1
> "read.dcf"           0.38      4.9       0.96      12.3
> "exists"             0.36      4.6       0.50       6.4
> "lapply"             0.36      4.6       1.36      17.4
> "names"              0.34      4.4       0.40       5.1
> "names<-"            0.32      4.1       0.44       5.6
> "inherits"           0.22      2.8       0.34       4.4
> "=="                 0.20      2.6       0.20       2.6
> ".Call"              0.20      2.6       0.20       2.6
> "seq"                0.18      2.3       0.50       6.4
> "seq.default"        0.18      2.3       0.28       3.6
> "dynGet"             0.16      2.1       0.46       5.9
> "topenv"             0.16      2.1       0.18       2.3
> "unique"             0.16      2.1       1.00      12.8
> "apply"              0.14      1.8       0.20       2.6
> "as.list"            0.14      1.8       0.18       2.3
> ".find.package"      0.14      1.8       1.88      24.1
> "sapply"             0.14      1.8       1.54      19.7
> "any"                0.12      1.5       1.68      21.5
> "initialize"         0.12      1.5       4.98      63.8
> 
> I then did the following:
> 
> > trace(read.dcf, recover)
> [1] "read.dcf"
> > res <- boot(mydf, bootmix, strata=mydf[,"id"], R=1)
> Tracing read.dcf(file.path(package.lib, package, "DESCRIPTION"), fields =
> "Namespace") on entry 
> 
> Enter a frame number, or 0 to exit   
> 1:boot(mydf, bootmix, strata = mydf[, "id"], R = 1) 
> 2:statistic(data, original, ...) 
> 3:try(fitmix(d.grps[[j]][, 1], verbosefit = FALSE)) 
> 4:fitmix(d.grps[[j]][, 1], verbosefit = FALSE) 
> 5:flexmix(x ~ 1, k = 1, control = list(verbose = 0)) 
> 6:flexmix(x ~ 1, k = 1, control = list(verbose = 0)) 
> 7:flexmix(formula = formula, data = data, k = k, cluster = cluster, model =
> list(F 
> 8:FLXglm() 
> 9:new("FLXmodel", weighted = TRUE, formula = formula, name = paste("FLXglm",
> famil 
> 10:initialize(value, ...) 
> 11:initialize(value, ...) 
> 12:getClass(Class) 
> 13:.requirePackage(package) 
> 14:trySilent(loadNamespace(package)) 
> 15:eval.parent(call) 
> 16:eval(expr, p) 
> 17:eval(expr, envir, enclos) 
> 18:try(loadNamespace(package)) 
> 19:loadNamespace(package) 
> 20:packageHasNamespace(package, package.lib) 
> 21:read.dcf(file.path(package.lib, package, "DESCRIPTION"), fields =
> "Namespace") 
> 
> I'm really out of ideas at this point.  Can anyone see the problem given the
> above, or at least tell me what to try next?
> 
> Best,
> Andy
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Luke Tierney
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
   Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu



From s-plus at wiwi.uni-bielefeld.de  Fri Dec  5 09:19:24 2003
From: s-plus at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Fri, 05 Dec 2003 09:19:24 +0100
Subject: [R] convert data
References: <3FCF6922.9020806@cs.uu.nl> <x27k1cfqp8.fsf@biostat.ku.dk>
Message-ID: <3FD03F8C.5030707@wiwi.uni-bielefeld.de>

 > y  <- c( "OLDa", "ALL",  "OLDc", "OLDa", "OLDb", "NEW", "OLDb", 
"OLDa", "ALL","...")
 > el <- c("OLDa", "OLDb", "OLDc", "NEW", "ALL")
 > match(y,el)
 [1]  1  5  3  1  2  4  2  1  5 NA

Peter Wolf

-----------------------

Peter Dalgaard wrote:

>Muhammad Subianto <subianto at cs.uu.nl> writes:
>
>  
>
>>Dear R-helper,
>>I have a data set like:
>>
>>OLDa
>>ALL
>>OLDc
>>OLDa
>>OLDb
>>NEW
>>OLDb
>>OLDa
>>ALL
>>. . .
>>ALL
>>OLDc
>>NEW
>>
>>I want to convert that data as OLDa=1, OLDb=2, OLDc=3, NEW=4 and ALL=5
>>or the result like:
>>
>>1
>>5
>>3
>>1
>>2
>>4
>>2
>>1
>>5
>>. . .
>>5
>>3
>>4
>>
>>How can I do it. Thanks you for your help.
>>    
>>
>
>
>I'd do it like this:
>
>  
>
>>x <- scan(what="")
>>    
>>
>1: OLDa
>2: ALL
>3: OLDc
>4: OLDa
>5: OLDb
>6: NEW
>7: OLDb
>8: OLDa
>9: ALL
>10:
>Read 9 items
>  
>
>>f <- factor(x,levels=c("OLDa", "OLDb", "OLDc", "NEW", "ALL") )
>>as.integer(f)
>>    
>>
>[1] 1 5 3 1 2 4 2 1 5
>
>
>  
>



From maechler at stat.math.ethz.ch  Fri Dec  5 11:03:57 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 5 Dec 2003 11:03:57 +0100
Subject: [R] R code for estimating Hurst exponent
In-Reply-To: <c5334dc5448c.c5448cc5334d@mail1.monash.edu.au>
References: <c5334dc5448c.c5448cc5334d@mail1.monash.edu.au>
Message-ID: <16336.22541.350060.333759@gargle.gargle.HOWL>

>>>>> "CathW" == Xiaozhe Wang <Catherine.Wang at infotech.monash.edu.au>
>>>>>     on Fri, 05 Dec 2003 10:52:05 +1100 writes:

    CathW> Has anyone writen R code for estimating Hurst exponent with R/S method 
    CathW> or other methods?
    CathW> or any other source of R code available?

There is the  fracdiff  package on CRAN for fitting

frARIMA(p,d,q) models with fractional "d" and there's a
one_to_one relationship between  'd' and the Hurst parameter for
these models: d = H - 1/2.

The R/S method is known to be far from optimal for many years
now, and there are better methods for which I'd recommend at
least the book 

@Book{BerJ94,
  author =       {Jan Beran},
  title =        {Statistics for Long-Memory Processes},
  publisher =    {Chapman \& Hall},
  year =         1994,
  series =       {Monographs on Statistics and Applied Probability 61},
  address =      NY
}

--------

In the book appendix, he has S-plus code (functions and a
script, not all really ok, using global variables, ...)  that I
had started to package for R many months ago, package name
"longmemo".

The version was never close to fulfill any CRAN quality
standards on R packages --- though I can make it available if
you are interested; it's already quite a bit more useful than
the original fortran code {I'm also sending this (BCC) to Prof.Beran
to solicit comments}. 

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From bitwrit at ozemail.com.au  Fri Dec  5 12:27:45 2003
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Fri, 5 Dec 2003 22:27:45 +1100
Subject: [R] Selecting subsamples
In-Reply-To: <3FCF399C.508.14BDAD5@localhost>
References: <3FCF399C.508.14BDAD5@localhost>
Message-ID: <20031205111839.YZFI21896.smta00.mail.ozemail.net@there>

christian_mora at vtr.net wrote:
> Hi all,
> I?m working with a dataset with 9 columns and 2000 rows. Each row
> represents an individual and one of the columns represents the volume
> of that individual (measured in cubic meters). I?d like to select a
> sample from this dataset (without considering any probability of the
> rows) in which the sum of the volume of the individuals in that sample
> >= 100 cubic m. I?ll appreciate any suggestion Thanks CM
>
I think Petr has the right idea, but I'll suggest the following, which 
allows you to draw samples without replacement until you run out of rows.
Assume your data frame is call mydata.df and the volume variable is called 
"M3"

shuffled.rows<-sample(1:2000,2000)
rowindex<-0
volume.sum<-0
while(volume.sum < 100) {
 rowindex<-rowindex+1
 volume.sum<-volume.sum+mydata.df[shuffled.rows[rowindex],]$M3
}
this.sample<-mydata.df[shuffled.rows[1:rowindex],]

add another loop to collect as many samples as you need.

Jim



From jano at mail-box.cz  Fri Dec  5 12:34:47 2003
From: jano at mail-box.cz (Jano Kula)
Date: 05 Dec 2003 12:34:47 +0100
Subject: [R] How to use Sys.setlocale("LC_NUMERIC")?
Message-ID: <1070624079.1264.86.camel@citadela>

Can you help me to use Sys.setlocale("LC_NUMERIC", "cs_CZ") (comma as a
decimal point) in some useful way, without all the workarounds?

After switching to Sys.setlocale("LC_NUMERIC", "cs_CZ"):

-- How do I set attributes in read.csv2() not to get columns of real
numbers (decimal point = comma, field separator = semicolon) as factors?

Wokrkaround: I can go backward with
"as.numeric(levels(table$column))[table$column]".

-- How do I separte values in a vector then? For example ylim=c(2.4,
3.5) -> ylim=c(2,4 ? 3,5), not to get a syntax error with the
comma-decimal numbers?

Workaround: I can set the value before I switch to "cz_CZ".

-- And finally, how do I plot it to the postscript device with commas in
labels (2,54), but with points in the postcsript commands (0.75
setlinewidth)? I can understand this is a problem of postscript
language/interpreter which doesn't use locales, but R's postscript
driver should know, it can't use commas in PS-command arguments.

Workaround: I can use "pretty()" and "as.character()" with default "C"
locales but all in all: what for is Sys.setlocale("LC_NUMERIC") good for
then?

Thanks

Jano



From spencer.graves at pdf.com  Fri Dec  5 13:33:04 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 05 Dec 2003 04:33:04 -0800
Subject: [R] Difficult experimental design questions
Message-ID: <3FD07B00.80706@pdf.com>

  What is available to help design experiments with non-standard 
requirements?

I have a recurring need to solve these kinds of problems, with deadlines 
of next Wednesday for two sample cases. The first of the two is "mission 
impossible", while the second is merely difficult. The following 
outlines briefly the two problems and the approach I'm currently 
considering. I'd appreciate suggestions either of available software or 
of general approaches. I also have a recurring need to solve this kind 
of problem, so ideas that would take longer to develop could also be 
useful.

MISSION IMPOSSIBLE: 4 factors, 3 levels each, in either 6 or 8 plots 
split in 2 using one of the 4 factors. Because of the split plot 
structure, any model estimated from the 3 between-plot factors will have 
only 6 or 8 distinct combinations available. However, a full quadratic 
model in 3 factors has 10 coefficients. This means that we could only 
estimate models containing subsets of the coefficients. I therefore plan 
to compare alternative designs primarily in terms of their "estimation 
capacity" = percent of models of certain types that are actually 
estimable, following Li and Nachtsheim (2000) ?Model Robust Factorial 
Designs?, Technometrics, pp. 345-352. I propose to start with a 
half-fraction of a 12-run Plackett-Burman in 6 runs and a 2^3 in 8 runs, 
then move selected points to a middle value to obtain 3-level designs to 
compare in terms of estimation capacity. After I get the 3-factor 
design, then I can split each of those runs into 2 plots for the 4th 
factor. The problem is complicated because the client already knows that 
at least 2 of the between-plot factors should be highly significant.

MERELY DIFFICULT: 10 factors with 6 at 3 levels and 4 at 2 levels in 
either 12 or 24 plots split in 2 on one of the 3-level factors. This 
problem is easier, because we have more runs and can rely more on effect 
sparsity / tapering of effect sizes, following Burnham and Anderson 
(2002 ) Model Selection and Multi-Model Inference, 2nd ed.; (Springer)

Any ideas, references, etc., would be greatly appreciated.

Thanks,
Spencer Graves



From ripley at stats.ox.ac.uk  Fri Dec  5 14:22:40 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 5 Dec 2003 13:22:40 +0000 (GMT)
Subject: [R] How to use Sys.setlocale("LC_NUMERIC")?
In-Reply-To: <1070624079.1264.86.camel@citadela>
Message-ID: <Pine.LNX.4.44.0312051317270.3321-100000@gannet.stats>

Have you consulted the help page?

On 5 Dec 2003, Jano Kula wrote:

> Can you help me to use Sys.setlocale("LC_NUMERIC", "cs_CZ") (comma as a
> decimal point) in some useful way, without all the workarounds?
> 
> After switching to Sys.setlocale("LC_NUMERIC", "cs_CZ"):
> 
> -- How do I set attributes in read.csv2() not to get columns of real
> numbers (decimal point = comma, field separator = semicolon) as factors?
> 
> Wokrkaround: I can go backward with
> "as.numeric(levels(table$column))[table$column]".
> 
> -- How do I separte values in a vector then? For example ylim=c(2.4,
> 3.5) -> ylim=c(2,4 ? 3,5), not to get a syntax error with the
> comma-decimal numbers?
> 
> Workaround: I can set the value before I switch to "cz_CZ".
> 
> -- And finally, how do I plot it to the postscript device with commas in
> labels (2,54), but with points in the postcsript commands (0.75
> setlinewidth)? I can understand this is a problem of postscript
> language/interpreter which doesn't use locales, but R's postscript
> driver should know, it can't use commas in PS-command arguments.

How is R's postscript driver supposed to know this?  At least of its 
authors would have expected that you wanted commas in PostScript (sic) 
commands under that locale setting.  You `should' not blame your tools for 
obeying your instructions.

> Workaround: I can use "pretty()" and "as.character()" with default "C"
> locales but all in all: what for is Sys.setlocale("LC_NUMERIC") good for
> then?

Who said it was `good for' anything?  Not the help page which warns
against this, although you give us no credit for that.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From junwen at astro.ocis.temple.edu  Fri Dec  5 14:40:37 2003
From: junwen at astro.ocis.temple.edu (Junwen wang)
Date: Fri, 5 Dec 2003 08:40:37 -0500 (EST)
Subject: [R] data fitting
Message-ID: <Pine.OSF.4.53.0312050839130.572877@gs873ps>

Hi,
I got a data set and want to find if the bin bar is normal distribution or
extreme value distribution. Is there any function to do the fitting?

Thanks



From ggrothendieck at myway.com  Fri Dec  5 15:08:25 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri,  5 Dec 2003 09:08:25 -0500 (EST)
Subject: [R] Processing calendar dates with R
Message-ID: <20031205140825.65710398E@mprdmxin.myway.com>


There are two datetime systems available.  

1. The POSIXt system in the R base, encompassing the 
POSIXlt and POSIXct classes. Others have already described 
where to get info on these.

2. The Bell Labs chron package.  Info available at:
require(chron)
?chron
You may also find this article on chron useful:
http://cm.bell-labs.com/cm/ms/departments/sia/dj/papers/chron.pdf

A key difference between the two, aside from the specific names and
arguments of the functions involved, is that POSIXt works 
with timezones and with daylight savings time whereas chron 
does not.  I personally use chron for all my stats problems
in order to avoid introducing certain subtleties of timezones
and daylight savings times.  I use POSIXt when I need to 
manipulate time stamps of files from the operating system since
POSIXt provides a close fit to how operating systems handle dates
and time.  Based on books and documentation for R I believe
that most people use POSIXt.

---
Date: Fri, 5 Dec 2003 10:40:29 +1100 
From: john byrne <j.byrne at mackillop.acu.edu.au>
To: <r-help at stat.math.ethz.ch> 
Subject: [R] Processing calendar dates with R 

 
 




I am a beginner in R with a background in SAS.

Are there built-in R methods of reading dates for calculating elapsed days
between two calendar dates? If so, are there any examples I can browse?

Thanks in anticipation.

John Byrne.
Lecturer in Information Systems.
Australian Catholic University.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From paradis at isem.univ-montp2.fr  Fri Dec  5 15:48:41 2003
From: paradis at isem.univ-montp2.fr (Emmanuel Paradis)
Date: Fri, 05 Dec 2003 15:48:41 +0100
Subject: [R] extracting p value from GEE
In-Reply-To: <Law15-F51sFFsAitVGe0000a61f@hotmail.com>
Message-ID: <4.2.0.58.20031205150802.00b1a4d0@isem.isem.univ-montp2.fr>

At 11:53 04/12/2003 +0000, vous avez ?crit:
>Dear R users,
>
>If anyone can tell me how to extract the p values from the output of gee?

They are easily computed from the output of summary(gee(...)) which prints 
either a "z" or a "t" depending in the "family" option. z follows, under 
the null hypothesis, a normal distribution N(0, 1), you have the 
corresponding P-value with (for a two-tailed test):

2 * (1 - pnorm(abs(z)))

t follows a 'Student' distribution with df degrees of freedom given by N- k 
- 1, where N is the number of observations, and k is the number of 
estimated paramaters. I think, but am not definitely sure, that N is 
counted among all clusters, and k is the number of parameters in the GLM 
eventually included the estimated scale (correlation parameters are not 
counted). As above, you have the P-value with:

2 * (1 - pnorm(abs(t), df))

HTH

Emmanuel Paradis


>Many thanks in advance.
>
>Yu-Kang
>
>_________________________________________________________________
>?K?O???? MSN ?^?y?????G?M?u?H???v?u?W???^?? http://www.msn.com.tw/english/
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From paradis at isem.univ-montp2.fr  Fri Dec  5 16:01:10 2003
From: paradis at isem.univ-montp2.fr (Emmanuel Paradis)
Date: Fri, 05 Dec 2003 16:01:10 +0100
Subject: [R] data fitting
In-Reply-To: <Pine.OSF.4.53.0312050839130.572877@gs873ps>
Message-ID: <4.2.0.58.20031205160000.00b557e8@isem.isem.univ-montp2.fr>

At 08:40 05/12/2003 -0500, vous avez ?crit:
>Hi,
>I got a data set and want to find if the bin bar is normal distribution or
>extreme value distribution. Is there any function to do the fitting?

see fitdistr() in package MASS

Emmanuel Paradis

>Thanks
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From karlknoblich at yahoo.de  Fri Dec  5 16:41:21 2003
From: karlknoblich at yahoo.de (=?iso-8859-1?q?Karl=20Knoblick?=)
Date: Fri, 5 Dec 2003 16:41:21 +0100 (CET)
Subject: [R] Processing calendar dates with R
Message-ID: <20031205154121.59981.qmail@web10001.mail.yahoo.com>

Hi!

Don't konw much, but I worked with package "chron",
function "dates":

d2<-"2002-01-21"
d1<-"2001-01-01"
dates(d2, format="y-m-d") -dates(d1, format="y-m-d")
# gives:
# Time in days:
# [1] 385

Actually I used something like 
dates(as.character(d2), format="y-m-d") 
cause the variable was read form a table as factor.

Good luck!
Karl



From Gerald.Jean at spgdag.ca  Fri Dec  5 17:32:30 2003
From: Gerald.Jean at spgdag.ca (Gerald.Jean@spgdag.ca)
Date: Fri, 5 Dec 2003 11:32:30 -0500
Subject: [R] GUI's for R
Message-ID: <OF4A7273A9.8DCA1BD1-ON85256DF3.00554D4C@spgdag.ca>

Hello,

I am not currently using R but I have been using S+ for sevaral years.  I
think that I fit in the category of power users, never using the GUI
prefering the flexibility of a command line interface (CLI).  In a short
while I will try to move to R, that is if we can convince our IT people
that installing a freeware package on a Unix server will not damaged the
companie's network!  The main reasons for which I want to move to R, at
least give it a try, are 1) David Smith of Insightful annonced us a couple
months ago that there is not plan to further develop the 6.0.4 Release 1
for Sun SPARC, SunOS 5.8, 64-bit version of S+ and as we deal with very
large problems, addressing well over 4GB of memory, a 64 bit version is
required and we bought that Sun machine for this sole purpose. 2) From what
I read and heard R manages its memory a little better than S+ and is a
little faster, non negligeable aspects when dealing with large problems.

All I said so far has little to do with the title of this email.  I am
getting to it.  While I am pretty happy with a CLI I was asked to evaluate
a few statistical packages to be used by casual users in my group.  One of
the main requirements is ease of use, hence the GUI, furthermore, it will
be a lot easier for me to support them in their analyses if we use the same
software.

I spent the two last days reading the R-FAQ and searching the R groups mail
archives, including the R-GUI mail group and as a consequence of all this
reading I am a bit confused.  At first I gathered that there was no GUI for
R, that R was strickly a CLI language.  What brought me to that conclusion
is the fact that on the R-project pages they refer the readers to external
builders of GUIs like the R-Commander of John Fox, Brodgar, TeXmacs,
SciViews etc..  On the other hand searching the mail archives for the word
"R-GUI" yielded well over 500 hits.  After reading a few tens of those I
realized that there existed, after building R, an executable called
"Rgui.exe", what is that? is that a GUI version of R or not? can someone
set my bearings straight on that issue?

Thanks in advance for your valuable comments,

G?rald Jean
Analyste-conseil (statistiques), Actuariat
t?lephone            : (418) 835-4900 poste (7639)
t?lecopieur          : (418) 835-6657
courrier ?lectronique: gerald.jean at spgdag.ca

"In God we trust all others must bring data"  W. Edwards Deming



From maketo at sdf.lonestar.org  Fri Dec  5 17:33:15 2003
From: maketo at sdf.lonestar.org (Ognen Duzlevski)
Date: Fri, 5 Dec 2003 16:33:15 +0000 (UTC)
Subject: [R] s-plus to R
Message-ID: <Pine.NEB.4.58.0312051631260.8483@otaku.freeshell.org>

Hi, I have a piece of code originally written for s-plus - I am trying to
run it in R now. The code was obtained from someone who is now not
available to give any pointers and I am a beginner in R. Here is where it
is getting stuck:

> +names(good.motifs[,1])
Error in +names(good.motifs[, 1]) : Invalid argument to unary operator

here is now names(good.motifs,1]) looks:
> names(good.motifs[,1])
[1] "Motif.P1.8.3"  "Motif.P1.9.14" "Motif.P1.10.1" "Motif.P1.11.8"
[5] "Motif.P1.15.1"

Can anyone help with an equivalent in R?

Thanks,
Ognen



From dmurdoch at pair.com  Fri Dec  5 17:42:45 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Fri, 05 Dec 2003 11:42:45 -0500
Subject: [R] GUI's for R
In-Reply-To: <OF4A7273A9.8DCA1BD1-ON85256DF3.00554D4C@spgdag.ca>
References: <OF4A7273A9.8DCA1BD1-ON85256DF3.00554D4C@spgdag.ca>
Message-ID: <v6d1tvc74ticqmr86tb93nadv51gk81a8d@4ax.com>

On Fri, 5 Dec 2003 11:32:30 -0500, Gerald.Jean at spgdag.ca wrote :
>After reading a few tens of those I
>realized that there existed, after building R, an executable called
>"Rgui.exe", what is that? is that a GUI version of R or not? can someone
>set my bearings straight on that issue?

That's currently only on Windows.  It's a GUI, but not in the sense
you're looking for:  it still needs external add-ons to get buttons,
etc. to do most statistics.  

Duncan Murdoch



From dmurdoch at pair.com  Fri Dec  5 17:47:07 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Fri, 05 Dec 2003 11:47:07 -0500
Subject: [R] s-plus to R
In-Reply-To: <Pine.NEB.4.58.0312051631260.8483@otaku.freeshell.org>
References: <Pine.NEB.4.58.0312051631260.8483@otaku.freeshell.org>
Message-ID: <9dd1tv05vuk8152qd21i555da40541dhhi@4ax.com>

On Fri, 5 Dec 2003 16:33:15 +0000 (UTC), Ognen Duzlevski
<maketo at sdf.lonestar.org> wrote :

>Hi, I have a piece of code originally written for s-plus - I am trying to
>run it in R now. The code was obtained from someone who is now not
>available to give any pointers and I am a beginner in R. Here is where it
>is getting stuck:
>
>> +names(good.motifs[,1])
>Error in +names(good.motifs[, 1]) : Invalid argument to unary operator
>
>here is now names(good.motifs,1]) looks:
>> names(good.motifs[,1])
>[1] "Motif.P1.8.3"  "Motif.P1.9.14" "Motif.P1.10.1" "Motif.P1.11.8"
>[5] "Motif.P1.15.1"
>
>Can anyone help with an equivalent in R?

I think in S-PLUS the "+" does nothing on a character vector, so the
equivalent is 

 names(good.motifs[,1])

However, it seems likely that some formatting or other error has
messed up the original code; why would the original author have added
a redundant +?

Duncan Murdoch



From MSchwartz at medanalytics.com  Fri Dec  5 17:47:48 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Fri, 05 Dec 2003 10:47:48 -0600
Subject: [R] GUI's for R
In-Reply-To: <OF4A7273A9.8DCA1BD1-ON85256DF3.00554D4C@spgdag.ca>
References: <OF4A7273A9.8DCA1BD1-ON85256DF3.00554D4C@spgdag.ca>
Message-ID: <1070642867.19312.87.camel@localhost.localdomain>

On Fri, 2003-12-05 at 10:32, Gerald.Jean at spgdag.ca wrote:
> Hello,
> 
> I am not currently using R but I have been using S+ for sevaral years.  I
> think that I fit in the category of power users, never using the GUI
> prefering the flexibility of a command line interface (CLI).  In a short
> while I will try to move to R, that is if we can convince our IT people
> that installing a freeware package on a Unix server will not damaged the
> companie's network!  The main reasons for which I want to move to R, at
> least give it a try, are 1) David Smith of Insightful annonced us a couple
> months ago that there is not plan to further develop the 6.0.4 Release 1
> for Sun SPARC, SunOS 5.8, 64-bit version of S+ and as we deal with very
> large problems, addressing well over 4GB of memory, a 64 bit version is
> required and we bought that Sun machine for this sole purpose. 2) From what
> I read and heard R manages its memory a little better than S+ and is a
> little faster, non negligeable aspects when dealing with large problems.
> 
> All I said so far has little to do with the title of this email.  I am
> getting to it.  While I am pretty happy with a CLI I was asked to evaluate
> a few statistical packages to be used by casual users in my group.  One of
> the main requirements is ease of use, hence the GUI, furthermore, it will
> be a lot easier for me to support them in their analyses if we use the same
> software.
> 
> I spent the two last days reading the R-FAQ and searching the R groups mail
> archives, including the R-GUI mail group and as a consequence of all this
> reading I am a bit confused.  At first I gathered that there was no GUI for
> R, that R was strickly a CLI language.  What brought me to that conclusion
> is the fact that on the R-project pages they refer the readers to external
> builders of GUIs like the R-Commander of John Fox, Brodgar, TeXmacs,
> SciViews etc..  On the other hand searching the mail archives for the word
> "R-GUI" yielded well over 500 hits.  After reading a few tens of those I
> realized that there existed, after building R, an executable called
> "Rgui.exe", what is that? is that a GUI version of R or not? can someone
> set my bearings straight on that issue?
> 
> Thanks in advance for your valuable comments,

I will defer to others with more experience on your particular platform
queries.

Rgui.exe is the Windows 'front end' environment for R that provides a
command line console for entering commands to the R interpreter, seeing
the textual output of those commands and for displaying the output of
plots. It supports a MDI/SDI type of interface. It does have some menus
for simple operations (like installing and updating packages) but not
for performing analyses. It is Windows OS only.

The default R installation is a CLI interface. If you wish to add a GUI
for performing "point and click" analyses, the sources that you list
would be appropriate for consideration. If you require a cross-platform
GUI for a multi-OS environment, John Fox's RCmdr would be a very good
choice since it is built with Tcl/Tk and is not OS specific.

HTH,

Marc Schwartz



From cstrato at aon.at  Fri Dec  5 17:48:39 2003
From: cstrato at aon.at (cstrato)
Date: Fri, 05 Dec 2003 17:48:39 +0100
Subject: [R] RE: R performance questions
In-Reply-To: <011101c3ba15$a7f867e0$7a05fea9@amd>
References: <011101c3ba15$a7f867e0$7a05fea9@amd>
Message-ID: <3FD0B6E7.1080005@aon.at>

Dear Michael

You rise a very good question. The number of microarray data is ever
increasing and dealing with 10,000 .cel files is quite challenging.

R and Bioconductor are great for developing and testing novel
algorithms, however, personally, I do not think that R will ever
be able to deal with massive amounts of data. 10,000 .cel files using
the newest GeneChips are equivalent to more than 200 Gigabyte of data,
so we are eventually talking about data in the Terabyte range.

Maybe, it is time to look how scientists used to handle large data
deal with this problem, such as the high energy physicists. Having
done this, I have decided to start to write my own expression analyisis
program which is no longer based on R but on C++ using a framework,
called ROOT, which is currently under development at CERN to deal
with Petabytes (!!) of data, see:
http://www.ci.tuwien.ac.at/Conferences/DSC-2003/Proceedings/Stratowa.pdf

Sorrowly, it takes me longer than expected to develop this software,
but you are looking ahead two or three years anyhow :-)

If microarray data would be stored in the way described, i.e. in the
same way as high energy physics data, this would already be a step
in the right direction.

However, this is only my personal opinion. In our company I still
use mainly R to analyse our microraay data.

Best regards
Christian Stratowa
Vienna     Austria


Michael Benjamin wrote:
> Hi--
> 
> While I agree that we cannot agree on the ideal algorithms, we should be
> taking practical steps to implement microarrays in the clinic.  I think
> we can all agree that our algorithms have some degree of efficacy over
> and above conventional diagnostic techniques.  If patients are dying
> from lack of diagnostic accuracy, I think we have to work hard to use
> this technology to help them, if we can.  I think we can, even now.
> 
> What if I offer, in my clinic, a service for cancer patients to compare
> their affy data to an existing set of data, to predict their prognosis
> or response to chemotherapy?  I think people will line up out the door
> for such a service.  Knowing what we as a group of array analyzers know,
> wouldn't we all want this kind of service available if we or a loved one
> got cancer?
> 
> Can our programs deal with 1,000 .cel files?  10,000 files?  
> 
> I think our programs are pretty good, but what we need is DATA.  We must
> be careful what we wish for--we might get it!  So how do we measure
> whether analyzing 10,000 .cel files with library(affy) is feasible?  I'm
> assuming that advanced hardware would be required for such a task.  What
> are the critical components of such a platform?  How much money would a
> feasible system for array analysis cost?
> 
> I was just looking ahead two or three years--where is all this genomic
> array research headed?  I guess I'm concerned about scalability.  
> 
> Is anyone really working on implementing affy on a cluster/Beowulf?
> That sounds like a real challenge.
> 
> Regards,
> Michael Benjamin, MD
> -----Original Message-----
> From: Liaw, Andy [mailto:andy_liaw at merck.com] 
> Sent: Wednesday, December 03, 2003 9:47 PM
> To: 'Michael Benjamin'
> Subject: RE: [BioC] R performance questions
> 
> Another point about benchmarking:  As has been discussed on R-help
> before,
> benchmarks can be misleading, as the one you mentioned.  It measures
> linear
> algebra tasks, etc., but that typically account for very small portion
> of
> "average" tasks.  Doug Bates also pointed out that the eigen() example
> used
> in that benchmark is computing mostly meaningless results.
> 
> In our experience, learning to use R more efficiently gives us the most
> mileage, but large and fast hardware wouldn't hurt...
> 
> Cheers,
> Andy
> 
> 
>>-----Original Message-----
>>From: Michael Benjamin [mailto:msb1129 at bellsouth.net] 
>>Sent: Wednesday, December 03, 2003 7:32 PM
>>To: 'Liaw, Andy'
>>Subject: RE: [BioC] R performance questions
>>
>>
>>Thanks.
>>Mike
>>
>>-----Original Message-----
>>From: Liaw, Andy [mailto:andy_liaw at merck.com] 
>>Sent: Wednesday, December 03, 2003 8:17 AM
>>To: 'Michael Benjamin'
>>Subject: RE: [BioC] R performance questions
>>
>>Hi Michael,
>>
>>Just one comment about SVM.  If you use the svm() function in 
>>the e1071
>>package to train linear SVM, it will be rather slow.  That's a known
>>limitation of libsvm, of which the svm() function uses.  If you are
>>willing
>>to go outside of R, the "bsvm" package by C.J. Lin (same person who
>>wrote
>>libsvm) will train linear svm in much more efficient manner.
>>
>>HTH,
>>Andy
>>
>>
>>>-----Original Message-----
>>>From: bioconductor-bounces at stat.math.ethz.ch 
>>>[mailto:bioconductor-bounces at stat.math.ethz.ch] On Behalf Of 
>>>Michael Benjamin
>>>Sent: Tuesday, December 02, 2003 10:30 PM
>>>To: bioconductor at stat.math.ethz.ch
>>>Subject: [BioC] R performance questions
>>>
>>>
>>>Hi, all--
>>>
>>>I wanted to start a thread on R speed/benchmarking.  There 
>>
>>is a nice R
>>
>>>benchmarking overview at 
>>
>>http://www.sciviews.org/other/benchmark.htm,
>>
>>>along with a 
>>
>>free script so you can see how your machine stacks up.
>>
>>>Looks like R is substantially faster than S-plus.
>>>
>>>My problem is this: with 512Mb and an overclocked AMD 
>>
>>Athlon XP 1800+,
>>
>>>running at 588 SPEC-FP 2000, it still takes FOREVER to 
>>>analyze multiple
>>>.cel files using affy (expresso).  Running svm takes a mighty 
>>>long time
>>>with more than 500 genes, 150 samples.
>>>
>>>Questions:
>>>1) Would adding RAM or processing speed improve performance 
>>
>>the most?
>>
>>>2) Is it possible to run R on a cluster without rewriting my 
>>>high-level
>>>code?  In other words,
>>>3) What are we going to do when we start collecting 
>>
>>terabytes of array
>>
>>>data to analyze?  There will come a "breaking point" at 
>>
>>which desktop
>>
>>>systems can't perform these analyses fast enough for large 
>>>quantities of
>>>data.  What then?
>>>
>>>Michael Benjamin, MD
>>>Winship Cancer Institute
>>>Emory University,
>>>Atlanta, GA
>>>
>>>_______________________________________________
>>>Bioconductor mailing list
>>>Bioconductor at stat.math.ethz.ch
>>>https://www.stat.math.ethz.ch/mailman/listinfo/bioconductor
>>>
>>
>>
>>
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
>



From baron at psych.upenn.edu  Fri Dec  5 17:56:06 2003
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Fri, 5 Dec 2003 11:56:06 -0500
Subject: [R] GUI's for R
In-Reply-To: <OF4A7273A9.8DCA1BD1-ON85256DF3.00554D4C@spgdag.ca>
References: <OF4A7273A9.8DCA1BD1-ON85256DF3.00554D4C@spgdag.ca>
Message-ID: <20031205165606.GB28661@mail1.sas.upenn.edu>

If you look on the R site, under "Related project" in the left
frame near the bottom, you will find a list of various attempts
at GUIs.  (It seems to be down right now, but usually it works.)

I've tried Rcmdr and Rweb.  There are others in various states of
development, some apparently not being developed.

It is my impression from reading the mailing list that many of
these are useful for what might be called canned analyses.  That
is, you want to make a data set available to lots of people
(e.g., students, co-workers) who do not know much about R and do
not want to learn, but they do want to do certain standard things
like analysis of variance, and they want some flexibility in what
variables they use, etc. etc.  But I think the level of
development is nowhere near what you would find in Splus or (what
I used to use) Systat, where you could use the GUI for a lot (but
not everything).

FWIW, I find Rweb to be very useful.  I use it with my students.
You can execute actual R commands, or even a batch file of them,
on an arbitrary data set (in the right format).  It runs in any
web browser.  Although a Google search shows that a few others
are using it, it is not being developed.  The person who wrote it
has not answered my email.  (His web page says he travels a lot.)
It is a bit limited because of its security features.  Your
sysadmin will like that, but others will want to disable them.

I think Rcmdr has great potential too, but I gave up on it
because its behavior is a bit dependent on the OS and monitor
settings, so I felt I could not be as helpful to my students
(being myself an oddball in both categories).

BUT THIS IS OPEN SOURCE.  If you like it and want to make it
better, fix it and share what you do.  I will do that with Rweb
when I get some time.  (Ha.)

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron



From tplate at acm.org  Fri Dec  5 17:57:56 2003
From: tplate at acm.org (Tony Plate)
Date: Fri, 05 Dec 2003 09:57:56 -0700
Subject: [R] s-plus to R
In-Reply-To: <Pine.NEB.4.58.0312051631260.8483@otaku.freeshell.org>
Message-ID: <5.2.1.1.2.20031205095030.04112690@mailhost.blackmesacapital.com>

I'd suggest just removing the "+" and see if that works.  It's somewhat 
difficult to say whether that would be safe without seeing more of your 
code, but "+" as a unary operator on character vectors seems to be an 
identity function in S-plus 6.1, e.g.:

 > print(+ letters[1:3])
[1] "a" "b" "c"
 > deparse(+ letters[1:3])
[1] "c(\"a\", \"b\", \"c\")"
 >

Maybe it has some side effects in S-plus (like invisible()?), but if the 
code depends on these you will probably have to code these in a different 
way in R anyway.  I don't know -- I haven't before seen "+" used as a unary 
operator on character vectors.

hope this helps,

Tony Plate


At Friday 04:33 PM 12/5/2003 +0000, Ognen Duzlevski wrote:
>Hi, I have a piece of code originally written for s-plus - I am trying to
>run it in R now. The code was obtained from someone who is now not
>available to give any pointers and I am a beginner in R. Here is where it
>is getting stuck:
>
> > +names(good.motifs[,1])
>Error in +names(good.motifs[, 1]) : Invalid argument to unary operator
>
>here is now names(good.motifs,1]) looks:
> > names(good.motifs[,1])
>[1] "Motif.P1.8.3"  "Motif.P1.9.14" "Motif.P1.10.1" "Motif.P1.11.8"
>[5] "Motif.P1.15.1"
>
>Can anyone help with an equivalent in R?
>
>Thanks,
>Ognen
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help

Tony Plate   tplate at acm.org



From paradis at univ-montp2.fr  Fri Dec  5 17:34:58 2003
From: paradis at univ-montp2.fr (paradis@univ-montp2.fr)
Date: Fri, 5 Dec 2003 17:34:58 +0100
Subject: [R] GUI\'s for R
References: <1070642867.19312.87.camel@localhost.localdomain>
Message-ID: <3fd0b3b2779b39.19994528@univ-montp2.fr>

> On Fri, 2003-12-05 at 10:32, Gerald.Jean at spgdag.ca wrote:
> > Hello,
> >
> > I am not currently using R but I have been using S+ for sevaral years.  I
> > think that I fit in the category of power users, never using the GUI
> > prefering the flexibility of a command line interface (CLI).  In a short
> > while I will try to move to R, that is if we can convince our IT people
> > that installing a freeware package on a Unix server will not damaged the
> > companie's network!  The main reasons for which I want to move to R, at
> > least give it a try, are 1) David Smith of Insightful annonced us a couple
> > months ago that there is not plan to further develop the 6.0.4 Release 1
> > for Sun SPARC, SunOS 5.8, 64-bit version of S+ and as we deal with very
> > large problems, addressing well over 4GB of memory, a 64 bit version is
> > required and we bought that Sun machine for this sole purpose. 2) From what
> > I read and heard R manages its memory a little better than S+ and is a
> > little faster, non negligeable aspects when dealing with large problems.
> >
> > All I said so far has little to do with the title of this email.  I am
> > getting to it.  While I am pretty happy with a CLI I was asked to evaluate
> > a few statistical packages to be used by casual users in my group.  One of
> > the main requirements is ease of use, hence the GUI, furthermore, it will
> > be a lot easier for me to support them in their analyses if we use the same
> > software.
> >
> > I spent the two last days reading the R-FAQ and searching the R groups mail
> > archives, including the R-GUI mail group and as a consequence of all this
> > reading I am a bit confused.  At first I gathered that there was no GUI for
> > R, that R was strickly a CLI language.  What brought me to that conclusion
> > is the fact that on the R-project pages they refer the readers to external
> > builders of GUIs like the R-Commander of John Fox, Brodgar, TeXmacs,
> > SciViews etc..  On the other hand searching the mail archives for the word
> > "R-GUI" yielded well over 500 hits.  After reading a few tens of those I
> > realized that there existed, after building R, an executable called
> > "Rgui.exe", what is that? is that a GUI version of R or not? can someone
> > set my bearings straight on that issue?
> >
> > Thanks in advance for your valuable comments,
> 
> I will defer to others with more experience on your particular platform
> queries.
> 
> Rgui.exe is the Windows 'front end' environment for R that provides a
> command line console for entering commands to the R interpreter, seeing
> the textual output of those commands and for displaying the output of
> plots. It supports a MDI/SDI type of interface. It does have some menus
> for simple operations (like installing and updating packages) but not
> for performing analyses. It is Windows OS only.

There is a very similar "GUI" under Linux that runs under the Gnome desktop environment, and I believe Gnome can run under SunOS.

Emmanuel Paradis

> The default R installation is a CLI interface. If you wish to add a GUI
> for performing "point and click" analyses, the sources that you list
> would be appropriate for consideration. If you require a cross-platform
> GUI for a multi-OS environment, John Fox's RCmdr would be a very good
> choice since it is built with Tcl/Tk and is not OS specific.
> 
> HTH,
> 
> Marc Schwartz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From maketo at sdf.lonestar.org  Fri Dec  5 18:07:33 2003
From: maketo at sdf.lonestar.org (Ognen Duzlevski)
Date: Fri, 5 Dec 2003 17:07:33 +0000 (UTC)
Subject: [R] ANother s-plus to R problem
In-Reply-To: <5.2.1.1.2.20031205095030.04112690@mailhost.blackmesacapital.com>
References: <5.2.1.1.2.20031205095030.04112690@mailhost.blackmesacapital.com>
Message-ID: <Pine.NEB.4.58.0312051704540.25438@otaku.freeshell.org>

Hi all, thank you for replying so quickly!

I have another problem:

 step.wise <- stepwise(data.table[,names(good.motifs[,1])], data.table[1],
f.crit=fval.cutoff)

s-plus has the stepwise() formula but R has step() and stepAIC() from base
and MASS packages. I cannot seem to figure out how to convert the above
stepwise to either step() or stepAIC().

> data.table[,names(good.motifs[,1])]
SO0404 -9.861773e-02 -3.280285e-02 -1.005750e-01 -7.363912e-02
-5.880977e-02
SO0403  3.962532e-02  9.791646e-03  2.854144e-03  3.393920e-02
5.433459e-02
SO0314  4.286539e-02  4.075057e-02  1.594184e-02  2.591711e-02
-6.343293e-04
SO4348  4.027333e-02  3.888057e-02  2.521229e-02 -4.118126e-04
-3.077392e-03
SO3919  1.219271e-02 -2.675035e-03  5.320541e-02 -4.118126e-04
2.410169e-02

> data.table[1]
SO0404 -0.11244436
SO0403 -0.11963379
SO0314 -0.13039490
SO4348 -0.14484288
SO3919 -0.14548809

Can anyone help?

Best regards,
Ognen

On Fri, 5 Dec 2003, Tony Plate wrote:

> Date: Fri, 05 Dec 2003 09:57:56 -0700
> From: Tony Plate <tplate at acm.org>
> To: Ognen Duzlevski <maketo at sdf.lonestar.org>, r-help at stat.math.ethz.ch
> Subject: Re: [R] s-plus to R
>
> I'd suggest just removing the "+" and see if that works.  It's somewhat
> difficult to say whether that would be safe without seeing more of your
> code, but "+" as a unary operator on character vectors seems to be an
> identity function in S-plus 6.1, e.g.:
>
>  > print(+ letters[1:3])
> [1] "a" "b" "c"
>  > deparse(+ letters[1:3])
> [1] "c(\"a\", \"b\", \"c\")"
>  >
>
> Maybe it has some side effects in S-plus (like invisible()?), but if the
> code depends on these you will probably have to code these in a different
> way in R anyway.  I don't know -- I haven't before seen "+" used as a unary
> operator on character vectors.
>
> hope this helps,
>
> Tony Plate
>
>
> At Friday 04:33 PM 12/5/2003 +0000, Ognen Duzlevski wrote:
> >Hi, I have a piece of code originally written for s-plus - I am trying to
> >run it in R now. The code was obtained from someone who is now not
> >available to give any pointers and I am a beginner in R. Here is where it
> >is getting stuck:
> >
> > > +names(good.motifs[,1])
> >Error in +names(good.motifs[, 1]) : Invalid argument to unary operator
> >
> >here is now names(good.motifs,1]) looks:
> > > names(good.motifs[,1])
> >[1] "Motif.P1.8.3"  "Motif.P1.9.14" "Motif.P1.10.1" "Motif.P1.11.8"
> >[5] "Motif.P1.15.1"
> >
> >Can anyone help with an equivalent in R?
> >
> >Thanks,
> >Ognen
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
> Tony Plate   tplate at acm.org
>
>

maketo at sdf.lonestar.org
SDF Public Access UNIX System - http://sdf.lonestar.org



From jfox at mcmaster.ca  Fri Dec  5 18:14:28 2003
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 05 Dec 2003 12:14:28 -0500
Subject: [R] GUI's for R
In-Reply-To: <1070642867.19312.87.camel@localhost.localdomain>
References: <OF4A7273A9.8DCA1BD1-ON85256DF3.00554D4C@spgdag.ca>
	<OF4A7273A9.8DCA1BD1-ON85256DF3.00554D4C@spgdag.ca>
Message-ID: <5.1.0.14.2.20031205120930.01fd3460@127.0.0.1>

Dear Gerald,

Although R doesn't come with a GUI, in the same sense as S-PLUS does, the 
tcltk package, which is part of the standard R distribution, provides 
facilities for building GUIs. The Rcmdr package is an example. Although 
it's not as extensive as the S-PLUS GUI, the Rcmdr package might satisfy 
the needs of the kinds of users who require a GUI (and it is extensible). 
I'm close to a new release of the package, which is described at 
<http://socserv.socsci.mcmaster.ca/jfox/Misc/Rcmdr/index.html>, where you 
can also find the development version.

I hope that this helps,
  John

At 10:47 AM 12/5/2003 -0600, Marc Schwartz wrote:
>On Fri, 2003-12-05 at 10:32, Gerald.Jean at spgdag.ca wrote:
> > Hello,
> >
> > I am not currently using R but I have been using S+ for sevaral years.  I
> > think that I fit in the category of power users, never using the GUI
> > prefering the flexibility of a command line interface (CLI).  In a short
> > while I will try to move to R, that is if we can convince our IT people
> > that installing a freeware package on a Unix server will not damaged the
> > companie's network!  The main reasons for which I want to move to R, at
> > least give it a try, are 1) David Smith of Insightful annonced us a couple
> > months ago that there is not plan to further develop the 6.0.4 Release 1
> > for Sun SPARC, SunOS 5.8, 64-bit version of S+ and as we deal with very
> > large problems, addressing well over 4GB of memory, a 64 bit version is
> > required and we bought that Sun machine for this sole purpose. 2) From what
> > I read and heard R manages its memory a little better than S+ and is a
> > little faster, non negligeable aspects when dealing with large problems.
> >
> > All I said so far has little to do with the title of this email.  I am
> > getting to it.  While I am pretty happy with a CLI I was asked to evaluate
> > a few statistical packages to be used by casual users in my group.  One of
> > the main requirements is ease of use, hence the GUI, furthermore, it will
> > be a lot easier for me to support them in their analyses if we use the same
> > software.
> >
> > I spent the two last days reading the R-FAQ and searching the R groups mail
> > archives, including the R-GUI mail group and as a consequence of all this
> > reading I am a bit confused.  At first I gathered that there was no GUI for
> > R, that R was strickly a CLI language.  What brought me to that conclusion
> > is the fact that on the R-project pages they refer the readers to external
> > builders of GUIs like the R-Commander of John Fox, Brodgar, TeXmacs,
> > SciViews etc..  On the other hand searching the mail archives for the word
> > "R-GUI" yielded well over 500 hits.  After reading a few tens of those I
> > realized that there existed, after building R, an executable called
> > "Rgui.exe", what is that? is that a GUI version of R or not? can someone
> > set my bearings straight on that issue?
> >
> > Thanks in advance for your valuable comments,
>
>I will defer to others with more experience on your particular platform
>queries.
>
>Rgui.exe is the Windows 'front end' environment for R that provides a
>command line console for entering commands to the R interpreter, seeing
>the textual output of those commands and for displaying the output of
>plots. It supports a MDI/SDI type of interface. It does have some menus
>for simple operations (like installing and updating packages) but not
>for performing analyses. It is Windows OS only.
>
>The default R installation is a CLI interface. If you wish to add a GUI
>for performing "point and click" analyses, the sources that you list
>would be appropriate for consideration. If you require a cross-platform
>GUI for a multi-OS environment, John Fox's RCmdr would be a very good
>choice since it is built with Tcl/Tk and is not OS specific.

-----------------------------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada L8S 4M4
email: jfox at mcmaster.ca
phone: 905-525-9140x23604
web: www.socsci.mcmaster.ca/jfox



From MSchwartz at medanalytics.com  Fri Dec  5 18:15:50 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Fri, 05 Dec 2003 11:15:50 -0600
Subject: [R] GUI\'s for R
In-Reply-To: <3fd0b3b2779b39.19994528@univ-montp2.fr>
References: <1070642867.19312.87.camel@localhost.localdomain>
	<3fd0b3b2779b39.19994528@univ-montp2.fr>
Message-ID: <1070644550.19312.96.camel@localhost.localdomain>

On Fri, 2003-12-05 at 10:34, paradis at univ-montp2.fr wrote:
  Marc Schwartz wrote:
> > Rgui.exe is the Windows 'front end' environment for R that provides a
> > command line console for entering commands to the R interpreter, seeing
> > the textual output of those commands and for displaying the output of
> > plots. It supports a MDI/SDI type of interface. It does have some menus
> > for simple operations (like installing and updating packages) but not
> > for performing analyses. It is Windows OS only.
> 
> There is a very similar "GUI" under Linux that runs under the Gnome
> desktop environment, and I believe Gnome can run under SunOS.


Yeah...you need to compile from source and use the

./configure --with-gnome

option to get it.  I tried that once some time ago. Like RGui.exe under
Windows, IIRC, it provides some basic menus for package installation,
etc.

It does not however provide any access to analytic/plotting functions.
More information is available in the R-admin manual.

Best regards,

Marc



From wolski at molgen.mpg.de  Fri Dec  5 18:37:42 2003
From: wolski at molgen.mpg.de (Wolski)
Date: Fri, 05 Dec 2003 18:37:42 +0100
Subject: [R] rcmd check question.
Message-ID: <200312051837420746.167CB84F@harry.molgen.mpg.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031205/899455be/attachment.pl

From ripley at stats.ox.ac.uk  Fri Dec  5 18:47:31 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 5 Dec 2003 17:47:31 +0000 (GMT)
Subject: [R] ANother s-plus to R problem
In-Reply-To: <Pine.NEB.4.58.0312051704540.25438@otaku.freeshell.org>
Message-ID: <Pine.LNX.4.44.0312051741020.2077-100000@gannet.stats>

On Fri, 5 Dec 2003, Ognen Duzlevski wrote:

> Hi all, thank you for replying so quickly!
> 
> I have another problem:
> 
>  step.wise <- stepwise(data.table[,names(good.motifs[,1])], data.table[1],
> f.crit=fval.cutoff)
> 
> s-plus has the stepwise() formula but R has step() and stepAIC() from base
> and MASS packages. I cannot seem to figure out how to convert the above
> stepwise to either step() or stepAIC().

You can't.  This is an old-fashioned approach, and the closest equivalent
in R is probably that of regsubsets in package leaps (in one of its
stepwise modes: it is a little short of detail)

I can't help asking: If you have S-PLUS (sic) code, why not use S-PLUS?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From phgrosjean at sciviews.org  Fri Dec  5 18:48:35 2003
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Fri, 5 Dec 2003 18:48:35 +0100
Subject: [R] GUI's for R
In-Reply-To: <20031205165606.GB28661@mail1.sas.upenn.edu>
Message-ID: <MABBLJDICACNFOLGIHJOGELKDPAA.phgrosjean@sciviews.org>

Johnathan Baron wrote:
>If you look on the R site, under "Related project" in the left
>frame near the bottom, you will find a list of various attempts
>at GUIs.  (It seems to be down right now, but usually it works.)

Uhh! I am the maintainer of the GUI section of the R site. If you cannot
access the site, please, send me an email... I have to fix it! I just try
right now (05/12/2003 18:16 French time), and I can access the site without
any problem.

>I've tried Rcmdr and Rweb.  There are others in various states of
>development, some apparently not being developed.

Yes, the original idea of this section, together with the R-SIG-GUI mailing
list, which is not very active since its beginning, is to support the
development of GUIs on top of R. My politic is not to favor any project, but
to collect together in a single place differents "attempts" (let's call it
like that, without any pejorative view), to build GUIs. Consequently, there
are several projects, at different levels of achievement, and maintenance,
yes. Some are idle now, yes. I keep a link to them in order to give them a
second chance...

>It is my impression from reading the mailing list that many of
>these are useful for what might be called canned analyses.  That
>is, you want to make a data set available to lots of people
>(e.g., students, co-workers) who do not know much about R and do
>not want to learn, but they do want to do certain standard things
>like analysis of variance, and they want some flexibility in what
>variables they use, etc. etc.  But I think the level of
>development is nowhere near what you would find in Splus or (what
>I used to use) Systat, where you could use the GUI for a lot (but
>not everything).

Personnally, I am envolved in SciViews. I made some progress towards a more
complete GUI, but not as fast as I would like, because we are only three,
part-time programmers on this project, currently. Also, I had very little
time to save for SciViews this year. Furthermore, a big problem is: it is
not platform-independent and it is mainly written in Visual Basic.

I suppose other projects face similar problems. That is probably why they do
not progress very rapidly. In a word: many people waiting and impatient, but
very few volonteers to actually make things work. This is not a critic, this
is just a constatation.

As a consequence, the less ambitious the project, and the more chances it
has to complete, like R-Commander that is clearly focused on canned analyses
aiming students in statistic. I think this project was developped by no more
than two people in an incredibly short amount of time. Kudos! But yes, it is
still canned analyses.

So, what next? Well, I can only speak for myself. I just got a position this
week as Professor in biostatistics. This means I will have more time (and
needs!) to develop a GUI for my students and colleagues (I don't need it for
myself). I am also developing some original ideas with Eric Lecoutre
(maintainer of the R2HTML library) that we intend to present to the UseR!
workshop in Vienna next May... However, I loose a lot of time trying to make
things platform-independent, while I have now what begin to be a good
starting basis, but for Windows only: SciViews.

Conclusion: either participate(if you want to further develop RWeb, good
idea), or be patient because it moves... but verrrry slowly.

Best,

Philippe Grosjean

...........]<(({?<...............<?}))><...............................
 ) ) ) ) )
( ( ( ( (   Prof. Philippe Grosjean
 ) ) ) ) )
( ( ( ( (   Numerical Ecology Laboratory
 ) ) ) ) )  Mons-Hainaut University
( ( ( ( (   8, Av. du Champ de Mars, 7000 Mons, Belgium
 ) ) ) ) )
( ( ( ( (   phone: 00-32-65.37.34.97
 ) ) ) ) )  email: Philippe.Grosjean at umh.ac.be; phgrosjean at sciviews.org
( ( ( ( (   SciViews project coordinator (http://www.sciviews.org)
 ) ) ) ) )
.......................................................................



From turk at math.montana.edu  Fri Dec  5 18:49:27 2003
From: turk at math.montana.edu (Philip Turk)
Date: Fri, 5 Dec 2003 10:49:27 -0700 (MST)
Subject: [R] Can anyone help me reproduce this SAS Mixed output??
Message-ID: <Pine.GSO.4.44.0312051036570.18479-100000@newton1.math.montana.edu>


I asked this before and I am going to try again in more applied terms.  I
am trying to use R to extract variance components for a two-factor random
effects model with both factors crossed.  It would also be nice to
generate some confidence intervals as well.  For example, a data set
using SAS Proc Mixed is below followed by the four variance component
estimates and the respective confidence intervals.  Currently, I have been
unable to reproduce this in NLME but I am sure I have not correctly
specified the "random" option.

Any help and/or ideas would be greatly appreciated!

## SAS PROGRAM WITH DATA FOLLOWS

data hw7;
input mpg driver car obs;
cards;
25.3	1	1	1
25.2	1	1	2
28.9	1	2	1
30	1	2	2
24.8	1	3	1
25.1	1	3	2
28.4	1	4	1
27.9	1	4	2
27.1	1	5	1
26.6	1	5	2
33.6	2	1	1
32.9	2	1	2
36.7	2	2	1
36.5	2	2	2
31.7	2	3	1
31.9	2	3	2
35.6	2	4	1
35	2	4	2
33.7	2	5	1
33.9	2	5	2
27.7	3	1	1
28.5	3	1	2
30.7	3	2	1
30.4	3	2	2
26.9	3	3	1
26.3	3	3	2
29.7	3	4	1
30.2	3	4	2
29.2	3	5	1
28.9	3	5	2
29.2	4	1	1
29.3	4	1	2
32.4	4	2	1
32.4	4	2	2
27.7	4	3	1
28.9	4	3	2
31.8	4	4	1
30.7	4	4	2
30.3	4	5	1
29.9	4	5	2
;

proc mixed data = hw7 method = reml cl asycov;
class driver car;
model mpg =;
random driver car driver*car;
run;
quit;

## SELECTED OUTPUT FOLLOWS

Covariance Parameter Estimates

Cov Parm       Estimate     Alpha       Lower       Upper

driver           9.3224      0.05      2.9864      130.79
car              2.9343      0.05      1.0464     24.9038
driver*car      0.01406      0.05    0.001345    3.592E17
Residual         0.1757      0.05      0.1029      0.3665



From maketo at sdf.lonestar.org  Fri Dec  5 18:52:56 2003
From: maketo at sdf.lonestar.org (Ognen Duzlevski)
Date: Fri, 5 Dec 2003 17:52:56 +0000 (UTC)
Subject: [R] ANother s-plus to R problem
In-Reply-To: <Pine.LNX.4.44.0312051741020.2077-100000@gannet.stats>
References: <Pine.LNX.4.44.0312051741020.2077-100000@gannet.stats>
Message-ID: <Pine.NEB.4.58.0312051749040.9852@otaku.freeshell.org>

On Fri, 5 Dec 2003, Prof Brian Ripley wrote:

> Date: Fri, 5 Dec 2003 17:47:31 +0000 (GMT)
> From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
> To: Ognen Duzlevski <maketo at sdf.lonestar.org>
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] ANother s-plus to R problem
>
> On Fri, 5 Dec 2003, Ognen Duzlevski wrote:
>
> > Hi all, thank you for replying so quickly!
> >
> > I have another problem:
> >
> >  step.wise <- stepwise(data.table[,names(good.motifs[,1])], data.table[1],
> > f.crit=fval.cutoff)
> >
> > s-plus has the stepwise() formula but R has step() and stepAIC() from base
> > and MASS packages. I cannot seem to figure out how to convert the above
> > stepwise to either step() or stepAIC().
>
> You can't.  This is an old-fashioned approach, and the closest equivalent
> in R is probably that of regsubsets in package leaps (in one of its
> stepwise modes: it is a little short of detail)
>
> I can't help asking: If you have S-PLUS (sic) code, why not use S-PLUS?

:) Money? R is free and s-plus is not and it also is not cheap. We got the
code from some researcher who used s-plus and we would like to run it
under R. I started playing with R yesterday and was able to figure out the
basics and to get almost all of the things going in the code we
inherited but this stepwise() thing doesn't look too good...

Thanks for your help.
Ognen



From macq at llnl.gov  Fri Dec  5 19:00:46 2003
From: macq at llnl.gov (Don MacQueen)
Date: Fri, 5 Dec 2003 10:00:46 -0800
Subject: [R] bug in as.POSIXct ?
In-Reply-To: <OFB2692808.8088F01C-ON86256DF2.006B1AC4-86256DF2.006C40A4@mmm.com>
References: <OFB2692808.8088F01C-ON86256DF2.006B1AC4-86256DF2.006C40A4@mmm.com>
Message-ID: <p06002001bbf664044eda@[128.115.153.6]>

I also have a data acquisition system with a sampling interval of one minute.
Since I'm working on a unix-based platform I don't know if my method 
of loading this kind of data
into R will work for you, but here it is.

This method *requires* that the incoming data ignore any changes to 
and from daylight savings time.

My time-date information is contained in a character vector named 
'dstr', formatted
     yyyy-mm-dd hh:mm:ss
and always with an 8 hour offset from UTC.

     x <- Sys.getenv("TZ")
     Sys.putenv(TZ="GMT")
     tm <- as.POSIXct(dstr)+28800
     Sys.putenv(TZ=x)

Note that 28800 is the number of seconds in 8 hours.

The result, tm, is a POSIXct object that represents local time, 
respecting daylight savings time.
When printed or used for plot axis labels, it is formatted for 
standard time or daylight savings time, as appropriate, and time 
interval calculations are correct.

Using your example (and assuming the times are both 8 hours behind UTC)

ds1 <- '2003-10-26 0:59:59'
ds2 <- '2003-10-26 1:00:00'

x <- Sys.getenv("TZ")
Sys.putenv(TZ="GMT")
dt1 <- as.POSIXct(ds1)+28800
dt2 <- as.POSIXct(ds2)+28800
Sys.putenv(TZ=x)


>  dt2-dt1
Time difference of 1 secs
>  dt1
[1] "2003-10-26 01:59:59 PDT"
>  dt2
[1] "2003-10-26 01:00:00 PST"

You may or may not want your times displayed in standard and daylight 
savings time as appropriate for the current local time. I did.

I suppose that one way to view this situation is that, as others 
pointed out, "2003-10-26 1:00:00" is a time that exists in both 
standard and daylight savings time (in the U.S., at least; I wouldn't 
assume that the transition takes place at the same time elsewhere). 
Since it exists in both it is best to explicitly tell it which one to 
use. If you don't tell it, a choice will be made for you, but no 
matter which choice is made, there is no guarantee that it will be 
the same as what the data acquisition system is doing. The above code 
is a way of telling R which one to use. In my limited experience, 
data acquisition systems (data loggers) don't deal with the 
standard/daylight savings time issue at all.

Note also that if you can arrange it so that your data acquisition 
system writes its date-times in ISO standard format, your R code will 
be simpler and easier.

At the time when I started working on this project, R was the only 
software available to me that correctly handled the transitions to 
and from standard time and daylight savings time, and this is the 
primary reason I started using R. The authors of the POSIX time 
classes deserve a great deal of credit for their contributions in 
this area--and based on the article in the June 2001 issue of R News 
that (I believe) introduced the classes, they are Brian Ripley and 
Kurt Hornik. That article also clearly describes the issues that 
motivated the design of the POSIX time classes.

-Don

p.s.
Here's what it looks like assuming a constant 7 hours behind UTC, 
i.e., always in pacific daylight savings time:

x <- Sys.getenv("TZ")
Sys.putenv(TZ="GMT")
dt1 <- as.POSIXct(ds1)+25200
dt2 <- as.POSIXct(ds2)+25200
Sys.putenv(TZ=x)

>  print(dt2-dt1)
Time difference of 1 secs
>  print(dt1)
[1] "2003-10-26 00:59:59 PDT"
>  print(dt2)
[1] "2003-10-26 01:00:00 PDT"

At 1:42 PM -0600 12/4/03, apjaworski at mmm.com wrote:
>Thanks very much for your response and the earlier one by Professor Ripley.
>
>The general problem is indeed a tricky one.
>
>My particular problem was much simpler.  I had a bunch of data from a data
>acquisition system with sampling interval of 1 minute.  The system used a
>simple compression scheme, where a data point was reported only when the
>change in response was sufficiently large.  For example, a fragment like
>this
>
>       Oct. 26 0:01:00       y1
>       Oct. 26 0:05:00       y2
>
>means that the values for 0:02, 0:03 0:04 where essentially y1.
>
>I needed to "decompress" the data set, i.e., fill in the gaps, so I was
>checking for differences of 1 minute and that is when I discovered the
>"error".
>
>I am not sure what the difference between Oct. 26 0:59:00 and Oct. 26 1:00
>should really be, but in this particular application it had to be 1 minute.
>Otherwise I generated 60 spurious gaps between these two times.
>
>Andy
>
>
>__________________________________
>Andy Jaworski
>518-1-01
>Process Laboratory
>3M Corporate Research Laboratory
>-----
>E-mail: apjaworski at mmm.com
>Tel:  (651) 733-6092
>Fax:  (651) 736-3122
>
>
>|---------+------------------------------->
>|         |           Jason Turner        |
>|         |           <jasont at indigoindust|
>|         |           rial.co.nz>         |
>|         |                               |
>|         |           12/05/2003 02:14    |
>|         |                               |
>|---------+------------------------------->
> 
>>-----------------------------------------------------------------------------------------------------------------------------|
>   | 
>|
>   |      To:       apjaworski at mmm.com 
>|
>   |      cc:       r-help at stat.math.ethz.ch 
>|
>   |      Subject:  Re: [R] bug in as.POSIXct ? 
>|
> 
>>-----------------------------------------------------------------------------------------------------------------------------|
>
>
>
>
>apjaworski at mmm.com wrote:
>
>>  I think that there is a bug in the as.POSIXct function on Windows.
>>
>>   Here is what I get on Win2000, Pentium III machine in R 1.8.1.
>>
>>
>>>dd1 <- ISOdatetime(2003, 10, 26, 0, 59, 59)
>>>dd2 <- ISOdatetime(2003, 10, 26, 1, 0, 0)
>>>dd2 - dd1
>>
>>  Time difference of 1.000278 hours
>>
>>  Now, the 26th of October was the day that change to the standard time
>>  occurred, so I suspect that this has something to do with that.  In fact
>>
>>
>>>dd1
>>
>>  [1] "2003-10-26 00:59:59 Central Daylight Time"
>>
>>>dd2
>>
>>  [1] "2003-10-26 01:00:00 Central Standard Time"
>>
>>  so it looks like the switch from CDT to CST happens at 1:00 (instead of
>>  2:00 ?).
>>
>
>Or, it did happen at 2:00 CDT, when the time fell back one hour to 1:00
>CST.  1:00 am occured twice on that day, once as CDT and once as CST.  R
>picked the last one.  A bit pathological at first glance, but
>date-handling often is.
>
>As for the dd2 - dd1 value, the "correct" value depends which 1:00 am
>was chosen.  On Windows, this should be 1 hour, 1 second, no?  I'm
>thinking 1:00 am CST == 2:00 am CDT, so in CDT entirely, your expression
>is basicly 02:00:00 CDT - 00:59:59 CDT.
>
>This makes me suspect that Linux picked the former 1:00 am, from your
>report.  Since R gets its date intricacies from the OS, there really
>isn't much that can be done about this, until someone builds a full
>POSIX time implementation that takes all the world's locales and time
>zones into account, and welds it into R.  Volunteers?
>
>It's things like this that make me convert everything to UCT (GMT, or
>Zulu, if you prefer).  Not R's fault; stupid calendar tricks are to
>blame here.
>
>Cheers
>
>Jason
>--
>Indigo Industrial Controls Ltd.
>http://www.indigoindustrial.co.nz
>64-21-343-545
>jasont at indigoindustrial.co.nz
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From s-plus at wiwi.uni-bielefeld.de  Fri Dec  5 19:16:03 2003
From: s-plus at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Fri, 05 Dec 2003 19:16:03 +0100
Subject: [R] GUI's for R
References: <OF4A7273A9.8DCA1BD1-ON85256DF3.00554D4C@spgdag.ca>
Message-ID: <3FD0CB63.5040208@wiwi.uni-bielefeld.de>

Gerald.Jean at spgdag.ca wrote:

>Hello,
>
>...
>
>All I said so far has little to do with the title of this email.  I am
>getting to it.  While I am pretty happy with a CLI I was asked to evaluate
>a few statistical packages to be used by casual users in my group.  One of
>the main requirements is ease of use, hence the GUI, furthermore, it will
>be a lot easier for me to support them in their analyses if we use the same
>software.
>
>  
>
One idea to archive easy use is to construct an editor from which you 
can start
R-commands. If you have saved frequently used R-expressions in a text file
you can load this file into such an editor,  locate the correct formula 
and eval it.
For this task I am developing an editor that uses the tcltk package of 
R: rwined().

Here is a screen shot:

   http://www.wiwi.uni-bielefeld.de/~wolf/software/revweb/rwinedwin.jpg

For experimentation you can download the code (window and linux)  from

   http://www.wiwi.uni-bielefeld.de/~wolf/software/revweb/rtrevive.exe

after installation the package, you have to type:
 
 > library(rtrevive)
 > rwined()

Peter Wolf



From jasont at indigoindustrial.co.nz  Fri Dec  5 19:24:39 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Sat, 06 Dec 2003 07:24:39 +1300
Subject: [R] rcmd check question.
In-Reply-To: <200312051837420746.167CB84F@harry.molgen.mpg.de>
References: <200312051837420746.167CB84F@harry.molgen.mpg.de>
Message-ID: <3FD0CD67.5020100@indigoindustrial.co.nz>

Wolski wrote:
...
> Can anyone tell what may causes the following errors?
> They printed during a
> 
> 
>>rcmd check packagename
> 
> * checking S3 generic/method consistency ... WARNING
> Error in .tryQuietly({ : Error in file(file, "r") : unable to open connection
> Execution halted
...

In the directory where you issue this command, does "dir" list 
"packagename" as one of the directories?  And is it set up according to 
the specification in "Writing R Extensions"?  That is, subdirectories 
"R", "man", ... ?

That's the simplest thing I could think of that would cause this.

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From srusso at oeb.harvard.edu  Fri Dec  5 20:00:05 2003
From: srusso at oeb.harvard.edu (Sabrina Russo)
Date: Fri, 05 Dec 2003 14:00:05 -0500
Subject: [R] Odds ratios for categorical variable
Message-ID: <5.1.0.14.2.20031205135738.02d7e008@oeb.harvard.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031205/e1c641ea/attachment.pl

From rpeng at jhsph.edu  Fri Dec  5 20:09:39 2003
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Fri, 05 Dec 2003 14:09:39 -0500
Subject: [R] Odds ratios for categorical variable
In-Reply-To: <5.1.0.14.2.20031205135738.02d7e008@oeb.harvard.edu>
References: <5.1.0.14.2.20031205135738.02d7e008@oeb.harvard.edu>
Message-ID: <3FD0D7F3.7060207@jhsph.edu>

Is hab coded as a factor or a numeric variable?

-roger

Sabrina Russo wrote:
> Dear R-users:
> How does one calculate in R  the odds ratios for a CATEGORICAL predictor 
> variable that has 4 levels.  I see r-help inquiries regarding odds ratios 
> for what looked like a continuous predictor variable.  I was wondering how 
> to get the pairwise odds ratios for comparisons of levels of a categorical 
> predictor variable.  I can't seem to get the correct output using:
>  > sp.glm=glm(cohort$logreg~cohort$hab, family=binomial)
>  > summary(sp.glm)
> 
> This gives me the coefficient for the effect of hab overall, but not the 
> coefficients for the comparisons of each of the levels. I suspect this has 
> something to do with the contrasts statement, but I can't figure it out, 
> and would be very appreciative of any help you can provide.
> Thank you,
> Sabrina
> 
> ______________________________________________________________
> Sabrina E. Russo
> Postdoctoral Fellow
> Center for Tropical Forest Science - Arnold Arboretum Asia Program
> Harvard University
> 22 Divinity Avenue
> Cambridge, MA 02138 USA
> 
> 617-496-2380 phone
> 617-495-9484 FAX
> srusso at oeb.harvard.edu
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From feh3k at spamcop.net  Fri Dec  5 20:23:40 2003
From: feh3k at spamcop.net (Frank E Harrell Jr)
Date: Fri, 5 Dec 2003 14:23:40 -0500
Subject: [R] Odds ratios for categorical variable
In-Reply-To: <5.1.0.14.2.20031205135738.02d7e008@oeb.harvard.edu>
References: <5.1.0.14.2.20031205135738.02d7e008@oeb.harvard.edu>
Message-ID: <20031205142340.4d97629e.feh3k@spamcop.net>

On Fri, 05 Dec 2003 14:00:05 -0500
Sabrina Russo <srusso at oeb.harvard.edu> wrote:

> Dear R-users:
> How does one calculate in R  the odds ratios for a CATEGORICAL predictor
> 
> variable that has 4 levels.  I see r-help inquiries regarding odds
> ratios for what looked like a continuous predictor variable.  I was
> wondering how to get the pairwise odds ratios for comparisons of levels
> of a categorical predictor variable.  I can't seem to get the correct
> output using:
>  > sp.glm=glm(cohort$logreg~cohort$hab, family=binomial)
>  > summary(sp.glm)
> 
> This gives me the coefficient for the effect of hab overall, but not the
> 
> coefficients for the comparisons of each of the levels. I suspect this
> has something to do with the contrasts statement, but I can't figure it
> out, and would be very appreciative of any help you can provide.
> Thank you,
> Sabrina
> 
> ______________________________________________________________
> Sabrina E. Russo
> Postdoctoral Fellow
> Center for Tropical Forest Science - Arnold Arboretum Asia Program
> Harvard University
> 22 Divinity Avenue
> Cambridge, MA 02138 USA
> 
> 617-496-2380 phone
> 617-495-9484 FAX
> srusso at oeb.harvard.edu
> 

library(Design)
f <- lrm(. . .)
summary(f)
---
Frank E Harrell Jr    Professor and Chair            School of Medicine
                      Department of Biostatistics    Vanderbilt University



From davison at uchicago.edu  Fri Dec  5 20:27:58 2003
From: davison at uchicago.edu (Dan Davison)
Date: Fri, 5 Dec 2003 13:27:58 -0600 (CST)
Subject: [R] .C() memory allocation
Message-ID: <Pine.GSO.4.21.0312051251000.15543-100000@harper.uchicago.edu>

I would like to retrieve a vector of integers from a call to .C(), but I
don't know its length in advance. How do I do this without making an ugly
safe guess?

My vector is called "sequences".
I am passing the argument sequences = integer(0) in the call to .C(),
then declaring the corresponding argument as int *sequences in my C code.

I tried R_alloc()ing the storage in C and assigning the pointer to
sequences but that segfaulted.

Then I tried Realloc()ing sequences:
sequences = Realloc(sequences, *nsam * *totalnmuts, int) ;

But that also segfaults.

Thanks very much for replies and R in general.

Dan



btw I think this is misleading / a typo in section 5.1.2 p.58 ch5 of
"Writing R Extensions", I hope I haven't just misunderstood.

It reads:
The interface functions are
	type* Calloc(size_t n, type)
 		     ^^^^^^
	type* Realloc(any *p, size_t n, type)
                              ^^^^^^

But RS.h makes me think that n should just be an integer giving the number
of elements and not necessarily be of type size_t:

extern void *R_chk_calloc(size_t, size_t);
extern void *R_chk_realloc(void *, size_t);

#define Calloc(n, t)   (t *) R_chk_calloc( (size_t) (n), sizeof(t) )
#define Realloc(p,n,t) (t *) R_chk_realloc( (void *)(p), (size_t)((n) * sizeof(t)) )




--------------------------------------------------
Dan Davison	
Committee on Evolutionary Biology, University of Chicago, USA
Field Museum of Natural History, Chicago, USA
http://home.uchicago.edu/~davison/



From ripley at stats.ox.ac.uk  Fri Dec  5 20:49:46 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 5 Dec 2003 19:49:46 +0000 (GMT)
Subject: [R] .C() memory allocation
In-Reply-To: <Pine.GSO.4.21.0312051251000.15543-100000@harper.uchicago.edu>
Message-ID: <Pine.LNX.4.44.0312051943540.9654-100000@gannet.stats>

On Fri, 5 Dec 2003, Dan Davison wrote:

> I would like to retrieve a vector of integers from a call to .C(), but I
> don't know its length in advance. How do I do this without making an ugly
> safe guess?
> 
> My vector is called "sequences".
> I am passing the argument sequences = integer(0) in the call to .C(),
> then declaring the corresponding argument as int *sequences in my C code.
> 
> I tried R_alloc()ing the storage in C and assigning the pointer to
> sequences but that segfaulted.
> 
> Then I tried Realloc()ing sequences:
> sequences = Realloc(sequences, *nsam * *totalnmuts, int) ;
> 
> But that also segfaults.

Yes.  You can't do this with .C, so use .Call instead.

> btw I think this is misleading / a typo in section 5.1.2 p.58 ch5 of
> "Writing R Extensions", I hope I haven't just misunderstood.

> It reads:
> The interface functions are
> 	type* Calloc(size_t n, type)
>  		     ^^^^^^
> 	type* Realloc(any *p, size_t n, type)
>                               ^^^^^^
> 
> But RS.h makes me think that n should just be an integer giving the number
> of elements and not necessarily be of type size_t:

A size_t is a non-negative integer, and it may not be an int (it could
well be larger on a 64-bit machine).  Calloc and Realloc are macros, and
they inform the standard C coercion rules, just as those declarations
imply.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ihaka at stat.auckland.ac.nz  Fri Dec  5 20:56:13 2003
From: ihaka at stat.auckland.ac.nz (Ross Ihaka)
Date: Sat, 06 Dec 2003 08:56:13 +1300
Subject: [R] .C() memory allocation
In-Reply-To: <Pine.GSO.4.21.0312051251000.15543-100000@harper.uchicago.edu>
References: <Pine.GSO.4.21.0312051251000.15543-100000@harper.uchicago.edu>
Message-ID: <3FD0E2DD.6050006@stat.auckland.ac.nz>

Dan Davison wrote:

> I would like to retrieve a vector of integers from a call to .C(), but I
> don't know its length in advance. How do I do this without making an ugly
> safe guess?

You should probably look into using .Call instead.  There's a bit of
learning overhead, but the payoff is that it's much more flexible.

-- 
Ross Ihaka                         Email:  ihaka at stat.auckland.ac.nz
Department of Statistics           Phone:  (64-9) 373-7599 x 85054
University of Auckland             Fax:    (64-9) 373-7018
Private Bag 92019, Auckland
New Zealand



From tlumley at u.washington.edu  Fri Dec  5 21:02:55 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 5 Dec 2003 12:02:55 -0800 (PST)
Subject: [R] ANother s-plus to R problem
In-Reply-To: <Pine.LNX.4.44.0312051741020.2077-100000@gannet.stats>
References: <Pine.LNX.4.44.0312051741020.2077-100000@gannet.stats>
Message-ID: <Pine.A41.4.58.0312051154170.54746@homer14.u.washington.edu>

On Fri, 5 Dec 2003, Prof Brian Ripley wrote:

> On Fri, 5 Dec 2003, Ognen Duzlevski wrote:
>
> > Hi all, thank you for replying so quickly!
> >
> > I have another problem:
> >
> >  step.wise <- stepwise(data.table[,names(good.motifs[,1])], data.table[1],
> > f.crit=fval.cutoff)
> >
> > s-plus has the stepwise() formula but R has step() and stepAIC() from base
> > and MASS packages. I cannot seem to figure out how to convert the above
> > stepwise to either step() or stepAIC().
>
> You can't.  This is an old-fashioned approach, and the closest equivalent
> in R is probably that of regsubsets in package leaps (in one of its
> stepwise modes: it is a little short of detail)
>

The default method for stepwise() in S-PLUS seems to be "efroymson". This
is not provided by regsubsets(), but it is in the Fortran code, so it
could be added.

	-thomas



From tlumley at u.washington.edu  Fri Dec  5 21:06:12 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 5 Dec 2003 12:06:12 -0800 (PST)
Subject: [R] Odds ratios for categorical variable
In-Reply-To: <5.1.0.14.2.20031205135738.02d7e008@oeb.harvard.edu>
References: <5.1.0.14.2.20031205135738.02d7e008@oeb.harvard.edu>
Message-ID: <Pine.A41.4.58.0312051204370.54746@homer14.u.washington.edu>

On Fri, 5 Dec 2003, Sabrina Russo wrote:

> Dear R-users:
> How does one calculate in R  the odds ratios for a CATEGORICAL predictor
> variable that has 4 levels.  I see r-help inquiries regarding odds ratios
> for what looked like a continuous predictor variable.  I was wondering how
> to get the pairwise odds ratios for comparisons of levels of a categorical
> predictor variable.  I can't seem to get the correct output using:
>  > sp.glm=glm(cohort$logreg~cohort$hab, family=binomial)
>  > summary(sp.glm)
>

If the predictor isn't already coded as a factor, use

   sp.glm=glm(cohort$logreg~factor(cohort$hab), family=binomial)

This will give contrasts with the first level of the factor.  A tidier way
to specify the same thing is

  sp.glm <- glm(logreg~factor(hab), data=cohort, family=binomial)


	-thomas



From bates at stat.wisc.edu  Fri Dec  5 21:12:29 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: 05 Dec 2003 14:12:29 -0600
Subject: [R] .C() memory allocation
In-Reply-To: <Pine.GSO.4.21.0312051251000.15543-100000@harper.uchicago.edu>
References: <Pine.GSO.4.21.0312051251000.15543-100000@harper.uchicago.edu>
Message-ID: <6rad673txu.fsf@bates4.stat.wisc.edu>

Dan Davison <davison at uchicago.edu> writes:

> I would like to retrieve a vector of integers from a call to .C(), but I
> don't know its length in advance. How do I do this without making an ugly
> safe guess?
> 
> My vector is called "sequences".
> I am passing the argument sequences = integer(0) in the call to .C(),
> then declaring the corresponding argument as int *sequences in my C code.

For something like this it is better to use the .Call interface than
to use the .C interface.  See the description in "Writing R
Extensions".  If that documentation is insufficient then I can send
you copies of the slides from our short course on "Advanced
Programming in R" from DSC-2003.



From kjetil at entelnet.bo  Fri Dec  5 22:23:03 2003
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Fri, 05 Dec 2003 17:23:03 -0400
Subject: [R] data fitting
In-Reply-To: <Pine.OSF.4.53.0312050839130.572877@gs873ps>
Message-ID: <3FD0BEF7.32603.494B00@localhost>

On 5 Dec 2003 at 8:40, Junwen wang wrote:

Your question could have been clearer, but:

Did you consider qqnorm/qqplot?

extreme value distributions are in the evd package on CRAN. 

if your data are in the vector bar, try:

qqnorm(bar)
qqline(bar, col="red")

then (after download/innstall) 

library(evd)
n <- length(bar)
qqplot( qgumbel(ppoints(n), sort(bar) )
# maybe estimate parameters first?

(By the way, if n is not to low, even a histogram should tell you if 
this might be normal or extrem value --- what modelling situation 
makes you doubt?)

Kjetil Halvorsen

> Hi,
> I got a data set and want to find if the bin bar is normal
> distribution or extreme value distribution. Is there any function to
> do the fitting?
> 
> Thanks
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From mitch.01 at wanadoo.fr  Sat Dec  6 13:56:26 2003
From: mitch.01 at wanadoo.fr (mitch.01)
Date: Sat, 6 Dec 2003 13:56:26 +0100
Subject: [R] (no subject)
Message-ID: <MBBBKEGHCNFLFKFBIEFHKEHECAAA.mitch.01@wanadoo.fr>

I am a french student and I want to make an histogram with R.But I have a
problem: I cant' change the frequencies in percent
 and I can't change the number of classes or intervals of my variable.Can
you help me please,in french if it's possible?
My mail is : fhenge at ulp.u-strasbg.fr

                           Thanks a lot.
HENGE FREDERIQUE



From kjetil at entelnet.bo  Sat Dec  6 15:18:26 2003
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Sat, 06 Dec 2003 10:18:26 -0400
Subject: [R] (no subject)
In-Reply-To: <MBBBKEGHCNFLFKFBIEFHKEHECAAA.mitch.01@wanadoo.fr>
Message-ID: <3FD1ACF2.16327.105EEB@localhost>

On 6 Dec 2003 at 13:56, mitch.01 wrote:

> I am a french student and I want to make an histogram with R.But I
> have a problem: I cant' change the frequencies in percent
>  and I can't change the number of classes or intervals of my
>  variable.Can
> you help me please,in french if it's possible?
> My mail is : fhenge at ulp.u-strasbg.fr
> 

I'm afraid I cannot help in french. Have you read 
?hist

If your data are in x, to get density scale and not counts, 
hist(, prob=TRUE)
To change the number of braeks see the arguments    breaks=

Kjetil Halvorsen

>                            Thanks a lot.
> HENGE FREDERIQUE
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From spencer.graves at pdf.com  Sat Dec  6 16:43:14 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 06 Dec 2003 07:43:14 -0800
Subject: [R] s-plus to R
In-Reply-To: <5.2.1.1.2.20031205095030.04112690@mailhost.blackmesacapital.com>
References: <5.2.1.1.2.20031205095030.04112690@mailhost.blackmesacapital.com>
Message-ID: <3FD1F912.5040902@pdf.com>

Is the "+" actually a unitary operator, or is it somehow a continuation 
from a previous line?  I don't know how this would explain the error 
message, but I wonder if there might be another problem? 

hope this helps,
spencer graves

Tony Plate wrote:

> I'd suggest just removing the "+" and see if that works.  It's 
> somewhat difficult to say whether that would be safe without seeing 
> more of your code, but "+" as a unary operator on character vectors 
> seems to be an identity function in S-plus 6.1, e.g.:
>
> > print(+ letters[1:3])
> [1] "a" "b" "c"
> > deparse(+ letters[1:3])
> [1] "c(\"a\", \"b\", \"c\")"
> >
>
> Maybe it has some side effects in S-plus (like invisible()?), but if 
> the code depends on these you will probably have to code these in a 
> different way in R anyway.  I don't know -- I haven't before seen "+" 
> used as a unary operator on character vectors.
>
> hope this helps,
>
> Tony Plate
>
>
> At Friday 04:33 PM 12/5/2003 +0000, Ognen Duzlevski wrote:
>
>> Hi, I have a piece of code originally written for s-plus - I am 
>> trying to
>> run it in R now. The code was obtained from someone who is now not
>> available to give any pointers and I am a beginner in R. Here is 
>> where it
>> is getting stuck:
>>
>> > +names(good.motifs[,1])
>> Error in +names(good.motifs[, 1]) : Invalid argument to unary operator
>>
>> here is now names(good.motifs,1]) looks:
>> > names(good.motifs[,1])
>> [1] "Motif.P1.8.3"  "Motif.P1.9.14" "Motif.P1.10.1" "Motif.P1.11.8"
>> [5] "Motif.P1.15.1"
>>
>> Can anyone help with an equivalent in R?
>>
>> Thanks,
>> Ognen
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>
> Tony Plate   tplate at acm.org
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From spencer.graves at pdf.com  Sat Dec  6 17:07:35 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 06 Dec 2003 08:07:35 -0800
Subject: [R] ANother s-plus to R problem
In-Reply-To: <Pine.A41.4.58.0312051154170.54746@homer14.u.washington.edu>
References: <Pine.LNX.4.44.0312051741020.2077-100000@gannet.stats>
	<Pine.A41.4.58.0312051154170.54746@homer14.u.washington.edu>
Message-ID: <3FD1FEC7.5070406@pdf.com>

      Have you tried something like the following: 

      y.name <- names(data.table)[1]
      null.mdl <- formula(y.name, "~")
      null.fit <- lm(null.mdl, data.table)
      x.names <- paste(names(good.motifs[,1]), collapse="+")
      mdl <- formula(paste("~", x.names))
      library(MASS)
      fit <- stepAIC(null.fit, mdl)

      I have not tried this specific code, but I have done many things 
like this successfully in both S-Plus and R.  If it doesn't produce what 
you want immediately, I suggest you first step through this code one 
line at a time, examine the inputs and outputs to see what you have and 
modify the code accordingly.  Also, have you looked at the documentation 
on "stepAIC", including that in Venables and Ripley (2002) Modern 
Applied Statistics with S, 4th ed. (Springer)? 

      hope this helps. 
      spencer graves

Thomas Lumley wrote:

>On Fri, 5 Dec 2003, Prof Brian Ripley wrote:
>
>  
>
>>On Fri, 5 Dec 2003, Ognen Duzlevski wrote:
>>
>>    
>>
>>>Hi all, thank you for replying so quickly!
>>>
>>>I have another problem:
>>>
>>> step.wise <- stepwise(data.table[,names(good.motifs[,1])], data.table[1],
>>>f.crit=fval.cutoff)
>>>
>>>s-plus has the stepwise() formula but R has step() and stepAIC() from base
>>>and MASS packages. I cannot seem to figure out how to convert the above
>>>stepwise to either step() or stepAIC().
>>>      
>>>
>>You can't.  This is an old-fashioned approach, and the closest equivalent
>>in R is probably that of regsubsets in package leaps (in one of its
>>stepwise modes: it is a little short of detail)
>>
>>    
>>
>
>The default method for stepwise() in S-PLUS seems to be "efroymson". This
>is not provided by regsubsets(), but it is in the Fortran code, so it
>could be added.
>
>	-thomas
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From spencer.graves at pdf.com  Sat Dec  6 17:13:11 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 06 Dec 2003 08:13:11 -0800
Subject: [R] Can anyone help me reproduce this SAS Mixed output??
In-Reply-To: <Pine.GSO.4.44.0312051036570.18479-100000@newton1.math.montana.edu>
References: <Pine.GSO.4.44.0312051036570.18479-100000@newton1.math.montana.edu>
Message-ID: <3FD20017.4010405@pdf.com>

      Have you also tried "intervals" on the output of "nlme"?  I have 
not used "nlme", but it works in "lme".  Also, have you looked at 
Pinhiero and Bates (2000) Mixed-Effects Models in S and S-PLUS 
(Springer)?  I had to read and carefully work through a portion of this 
book before I was able to use "lme" successfully.  However, I found it 
well worth the effort, both for how to use "lme" and for understanding 
the theory behind it.  Bates and his graduate students including 
Pinhiero wrote "lme" and "nlme", and I know of no better source on this 
subject. 

      hope this helps.  spencer graves

Philip Turk wrote:

>I asked this before and I am going to try again in more applied terms.  I
>am trying to use R to extract variance components for a two-factor random
>effects model with both factors crossed.  It would also be nice to
>generate some confidence intervals as well.  For example, a data set
>using SAS Proc Mixed is below followed by the four variance component
>estimates and the respective confidence intervals.  Currently, I have been
>unable to reproduce this in NLME but I am sure I have not correctly
>specified the "random" option.
>
>Any help and/or ideas would be greatly appreciated!
>
>## SAS PROGRAM WITH DATA FOLLOWS
>
>data hw7;
>input mpg driver car obs;
>cards;
>25.3	1	1	1
>25.2	1	1	2
>28.9	1	2	1
>30	1	2	2
>24.8	1	3	1
>25.1	1	3	2
>28.4	1	4	1
>27.9	1	4	2
>27.1	1	5	1
>26.6	1	5	2
>33.6	2	1	1
>32.9	2	1	2
>36.7	2	2	1
>36.5	2	2	2
>31.7	2	3	1
>31.9	2	3	2
>35.6	2	4	1
>35	2	4	2
>33.7	2	5	1
>33.9	2	5	2
>27.7	3	1	1
>28.5	3	1	2
>30.7	3	2	1
>30.4	3	2	2
>26.9	3	3	1
>26.3	3	3	2
>29.7	3	4	1
>30.2	3	4	2
>29.2	3	5	1
>28.9	3	5	2
>29.2	4	1	1
>29.3	4	1	2
>32.4	4	2	1
>32.4	4	2	2
>27.7	4	3	1
>28.9	4	3	2
>31.8	4	4	1
>30.7	4	4	2
>30.3	4	5	1
>29.9	4	5	2
>;
>
>proc mixed data = hw7 method = reml cl asycov;
>class driver car;
>model mpg =;
>random driver car driver*car;
>run;
>quit;
>
>## SELECTED OUTPUT FOLLOWS
>
>Covariance Parameter Estimates
>
>Cov Parm       Estimate     Alpha       Lower       Upper
>
>driver           9.3224      0.05      2.9864      130.79
>car              2.9343      0.05      1.0464     24.9038
>driver*car      0.01406      0.05    0.001345    3.592E17
>Residual         0.1757      0.05      0.1029      0.3665
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From alex_s_42 at yahoo.com  Sat Dec  6 17:10:55 2003
From: alex_s_42 at yahoo.com (Alexander Sirotkin [at Yahoo])
Date: Sat, 6 Dec 2003 08:10:55 -0800 (PST)
Subject: [R] Difference between summary.lm() and summary.aov()
Message-ID: <20031206161055.9355.qmail@web60001.mail.yahoo.com>

I have a simple linear model (fitted with lm()) with 2
independant
variables : one categorical and one integer.

When I run summary.lm() on this model, I get a
standard linear
regression summary (in which one categorical variable
has to be
converted into many indicator variables) which looks
like :

            Estimate Std. Error t value Pr(>|t|)
(Intercept)  -3595.3     2767.1  -1.299   0.2005
physicianB     802.0     2289.5   0.350   0.7277
physicianC    4906.8     2419.8   2.028   0.0485 *
severity      7554.4      906.3   8.336 1.12e-10 ***

and when I run summary.aov() I get similar ANOVA table
: 

           Df     Sum Sq    Mean Sq F value    Pr(>F)
physician    2  294559803  147279901  3.3557   0.04381
*
severity     1 3049694210 3049694210 69.4864 1.124e-10
***
Residuals   45 1975007569   43889057

What is absolutely unclear to me is how F-value and
Pr(>F) for the
categorical "physician" variable of the summary.aov()
is calculated
from the t-value of the summary.lm() table.

I looked at the summary.aov() source code but still
could not figure
it.

Thanks a lot.

__________________________________

New Yahoo! Photos - easier uploading and sharing.



From spencer.graves at pdf.com  Sat Dec  6 18:17:15 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 06 Dec 2003 09:17:15 -0800
Subject: [R] Difference between summary.lm() and summary.aov()
In-Reply-To: <20031206161055.9355.qmail@web60001.mail.yahoo.com>
References: <20031206161055.9355.qmail@web60001.mail.yahoo.com>
Message-ID: <3FD20F1B.702@pdf.com>

      The square of a Student's t with "df" degrees of freedom is an F 
distribution with 1 and "df" degrees of freedom. 
     
      hope this helps.  spencer graves

Alexander Sirotkin [at Yahoo] wrote:

>I have a simple linear model (fitted with lm()) with 2
>independant
>variables : one categorical and one integer.
>
>When I run summary.lm() on this model, I get a
>standard linear
>regression summary (in which one categorical variable
>has to be
>converted into many indicator variables) which looks
>like :
>
>            Estimate Std. Error t value Pr(>|t|)
>(Intercept)  -3595.3     2767.1  -1.299   0.2005
>physicianB     802.0     2289.5   0.350   0.7277
>physicianC    4906.8     2419.8   2.028   0.0485 *
>severity      7554.4      906.3   8.336 1.12e-10 ***
>
>and when I run summary.aov() I get similar ANOVA table
>: 
>
>           Df     Sum Sq    Mean Sq F value    Pr(>F)
>physician    2  294559803  147279901  3.3557   0.04381
>*
>severity     1 3049694210 3049694210 69.4864 1.124e-10
>***
>Residuals   45 1975007569   43889057
>
>What is absolutely unclear to me is how F-value and
>Pr(>F) for the
>categorical "physician" variable of the summary.aov()
>is calculated
>from the t-value of the summary.lm() table.
>
>I looked at the summary.aov() source code but still
>could not figure
>it.
>
>Thanks a lot.
>
>__________________________________
>
>New Yahoo! Photos - easier uploading and sharing.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From mendigo at netcabo.pt  Sat Dec  6 18:48:06 2003
From: mendigo at netcabo.pt (M. M. Palhoto N. Rodrigues)
Date: Sat, 6 Dec 2003 17:48:06 -0000
Subject: [R] Axe time of series in format yy-mm-dd
Message-ID: <001201c3bc21$1de20440$772c5451@galactic>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031206/e4f24423/attachment.pl

From forkusam at yahoo.com  Sat Dec  6 19:07:25 2003
From: forkusam at yahoo.com (forkusam)
Date: Sat, 6 Dec 2003 10:07:25 -0800 (PST)
Subject: [R] Precision
Message-ID: <20031206180725.57797.qmail@web10504.mail.yahoo.com>

 Hi, 
I would like to increase the Precision of R  by
increasing the Number of decimal places in 
calcultaions. I get about 7 decimal places and would
like to have over 15. is the a means to inflence this?
Thanks

=====
=====================
Sylvie B. Forkusam
Eppelheimer Str.52/A2-5-2
69115 Heidelberg, Germany
Tel: (0049)-06221/346913
Mobile: 0179-6816276



From rpugh at mango-solutions.com  Sat Dec  6 19:41:18 2003
From: rpugh at mango-solutions.com (Richard Pugh)
Date: Sat, 6 Dec 2003 18:41:18 -0000
Subject: [R] Windows Memory Issues
Message-ID: <000001c3bc28$a0f10690$3e01fc3e@vsn.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031206/be2d2211/attachment.pl

From rpeng at jhsph.edu  Sat Dec  6 19:46:28 2003
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Sat, 06 Dec 2003 13:46:28 -0500
Subject: [R] Precision
In-Reply-To: <20031206180725.57797.qmail@web10504.mail.yahoo.com>
References: <20031206180725.57797.qmail@web10504.mail.yahoo.com>
Message-ID: <3FD22404.7030201@jhsph.edu>

I think you want to change the number of digits that are *printed*.  Try 
doing

options(digits = 15)

-roger

forkusam wrote:
>  Hi, 
> I would like to increase the Precision of R  by
> increasing the Number of decimal places in 
> calcultaions. I get about 7 decimal places and would
> like to have over 15. is the a means to inflence this?
> Thanks
> 
> =====
> =====================
> Sylvie B. Forkusam
> Eppelheimer Str.52/A2-5-2
> 69115 Heidelberg, Germany
> Tel: (0049)-06221/346913
> Mobile: 0179-6816276
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From dominique.couturier at unine.ch  Sat Dec  6 19:57:14 2003
From: dominique.couturier at unine.ch (Dominique Couturier)
Date: Sat, 06 Dec 2003 19:57:14 +0100
Subject: [R] pdf() function, screen command and graphs
Message-ID: <0182CF18-281E-11D8-8EAF-0003931DD6AE@unine.ch>

Dear [R]-list,

I am trying to do a pdf() of the following graphs but don't understand 
why the pdf() function does produce an empty pdf file.
(I use R1.7.0 on MacOS 10.2.8)
any idea? Is pdf() incompatible with screen?
Thanks a lot,
DLC

## create dataset
x=rnorm(1000,10,2)
y=rpois(1000,5)

## graphs
  pdf()
   par(col.main=4,omi=c(0,0,1.25,0))
         split.screen(c(1,2))
         screen(1);split.screen(c(2,1))

         screen(3) ## gauche en haut
         boxplot(x,col=3,main="my boxplot")

         screen(2) ## droite seul
         barplot(table(y),col=4,main="my barplot")

         screen(4) ## gauche en bas
         qqnorm(x,main="mon qqnorm");qqline(x,col=5)
         mtext(paste("Graphics are important","\n","I love 
R"),side=3,col=2,cex=1.5,outer=T)
   dev.off()

## remove dataset
  rm(x,y)



From jasont at indigoindustrial.co.nz  Sat Dec  6 19:54:48 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Sun, 07 Dec 2003 07:54:48 +1300
Subject: [R] Precision
In-Reply-To: <20031206180725.57797.qmail@web10504.mail.yahoo.com>
References: <20031206180725.57797.qmail@web10504.mail.yahoo.com>
Message-ID: <3FD225F8.30007@indigoindustrial.co.nz>

forkusam wrote:
>  Hi, 
> I would like to increase the Precision of R  by
> increasing the Number of decimal places in 
> calcultaions. I get about 7 decimal places and would
> like to have over 15. is the a means to inflence this?

R uses double-precision  for *all* internal floating point numbers. 
These are 64 bits, on most machines these days.

If you want to change the *display* of your results, use format()  e.g.

pi
format(pi, digits=15)

If you find that you're getting rounding error in your calculations when 
using R, check your calculation or algorithm, and see if there's a more 
numerically stable way to do it.

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From ripley at stats.ox.ac.uk  Sat Dec  6 20:00:43 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 6 Dec 2003 19:00:43 +0000 (GMT)
Subject: [R] Windows Memory Issues
In-Reply-To: <000001c3bc28$a0f10690$3e01fc3e@vsn.local>
Message-ID: <Pine.LNX.4.44.0312061854400.29769-100000@gannet.stats>

I think you misunderstand how R uses memory.  gc() does not free up all 
the memory used for the objects it frees, and repeated calls will free 
more.  Don't speculate about how memory management works: do your 
homework!

In any case, you are using an outdated version of R, and your first
course of action should be to compile up R-devel and try that, as there 
has been improvements to memory management under Windows.  You could also 
try compiling using the native malloc (and that *is* described in the 
INSTALL file) as that has different compromises.


On Sat, 6 Dec 2003, Richard Pugh wrote:

> Hi all,
>  
> I am currently building an application based on R 1.7.1 (+ compiled
> C/C++ code + MySql + VB).  I am building this application to work on 2
> different platforms (Windows XP Professional (500mb memory) and Windows
> NT 4.0 with service pack 6 (1gb memory)).  This is a very memory
> intensive application performing sophisticated operations on "large"
> matrices (typically 5000x1500 matrices).
>  
> I have run into some issues regarding the way R handles its memory,
> especially on NT.  In particular, R does not seem able to recollect some
> of the memory used following the creation and manipulation of large data
> objects.  For example, I have a function which receives a (large)
> numeric matrix, matches against more data (maybe imported from MySql)
> and returns a large list structure for further analysis.  A typical call
> may look like this .
>  
> > myInputData <- matrix(sample(1:100, 7500000, T), nrow=5000)
> > myPortfolio <- createPortfolio(myInputData)
>  
> It seems I can only repeat this code process 2/3 times before I have to
> restart R (to get the memory back).  I use the same object names
> (myInputData and myPortfolio) each time, so I am not create more large
> objects ..
>  
> I think the problems I have are illustrated with the following example
> from a small R session .
>  
> > # Memory usage for Rui process = 19,800
> > testData <- matrix(rnorm(10000000), 1000) # Create big matrix
> > # Memory usage for Rgui process = 254,550k
> > rm(testData)
> > # Memory usage for Rgui process = 254,550k
> > gc()
>          used (Mb) gc trigger  (Mb)
> Ncells 369277  9.9     667722  17.9
> Vcells  87650  0.7   24286664 185.3
> > # Memory usage for Rgui process = 20,200k
>  
> In the above code, R cannot recollect all memory used, so the memory
> usage increases from 19.8k to 20.2.  However, the following example is
> more typical of the environments I use .
>  
> > # Memory 128,100k
> > myTestData <- matrix(rnorm(10000000), 1000)
> > # Memory 357,272k
> > rm(myTestData)
> > # Memory 357,272k
> > gc()
>           used (Mb) gc trigger  (Mb)
> Ncells  478197 12.8     818163  21.9
> Vcells 9309525 71.1   31670210 241.7
> > # Memory 279,152k
>  
> Here, the memory usage increases from 128.1k to 279.1k
>  
> Could anyone point out what I could do to rectify this (if anything), or
> generally what strategy I could take to improve this?
>  
> Many thanks,
> Rich.
>  
> Mango Solutions
> Tel : (01628) 418134
> Mob : (07967) 808091
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sat Dec  6 20:06:01 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 6 Dec 2003 19:06:01 +0000 (GMT)
Subject: [R] pdf() function, screen command and graphs
In-Reply-To: <0182CF18-281E-11D8-8EAF-0003931DD6AE@unine.ch>
Message-ID: <Pine.LNX.4.44.0312061903180.29769-100000@gannet.stats>

Works perfectly (if garishly) for me in R 1.8.1.
Perhaps you could benefit from an R upgrade?

[I presume a line got wrapped, BTW, and if you had made good use of your
space bar it would not have produced invalid code ....]

On Sat, 6 Dec 2003, Dominique Couturier wrote:

> Dear [R]-list,
> 
> I am trying to do a pdf() of the following graphs but don't understand 
> why the pdf() function does produce an empty pdf file.
> (I use R1.7.0 on MacOS 10.2.8)
> any idea? Is pdf() incompatible with screen?
> Thanks a lot,
> DLC
> 
> ## create dataset
> x=rnorm(1000,10,2)
> y=rpois(1000,5)
> 
> ## graphs
>   pdf()
>    par(col.main=4,omi=c(0,0,1.25,0))
>          split.screen(c(1,2))
>          screen(1);split.screen(c(2,1))
> 
>          screen(3) ## gauche en haut
>          boxplot(x,col=3,main="my boxplot")
> 
>          screen(2) ## droite seul
>          barplot(table(y),col=4,main="my barplot")
> 
>          screen(4) ## gauche en bas
>          qqnorm(x,main="mon qqnorm");qqline(x,col=5)
>          mtext(paste("Graphics are important","\n","I love 
> R"),side=3,col=2,cex=1.5,outer=T)
>    dev.off()
> 
> ## remove dataset
>   rm(x,y)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at myway.com  Sat Dec  6 20:06:49 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat,  6 Dec 2003 14:06:49 -0500 (EST)
Subject: [R] Axe time of series in format yy-mm-dd
Message-ID: <20031206190649.DAF1339A5@mprdmxin.myway.com>



get.hist.quote produces times relative to Dec 31, 1899, so try this:

require(tseries)
require(chron)

ibm <- get.hist.quote( "ibm", start = "2003-01-01", quote = "Close" )

time.ibm <- chron( time(ibm), out.format="y-m-d", 
   origin = c( month = 12, day = 31, year = 1899 ) )
plot( time.ibm, ibm, type="l", simplify = F )

The simplify=F flag on plot forces the year to appear.  Without
it, only the month and day are displayed.

---
Date: Sat, 6 Dec 2003 17:48:06 -0000 
From: M. M. Palhoto N. Rodrigues <mendigo at netcabo.pt>
To: R Help <r-help at stat.math.ethz.ch> 
Subject: [R] Axe time of series in format yy-mm-dd 

 
 
I'm trying to plot a ibm stock time series.
I made the download of that series,
ibm <- get.hist.quote(instrument = "ibm", start = "2003-01-01",quote=c("CL"))
And ibm is a serie wiht this characteristic:
Start = 37623 
End = 37960 
Frequency = 1 
When I try to plot it,
ts.plot(ibm)
In the graphic the axe time is represented by 37623 ... 37960, How can I put the time in the format,
yy-mm-dd ?

Thanks a lot



From jasont at indigoindustrial.co.nz  Sat Dec  6 20:23:14 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Sun, 07 Dec 2003 08:23:14 +1300
Subject: [R] Windows Memory Issues
In-Reply-To: <000001c3bc28$a0f10690$3e01fc3e@vsn.local>
References: <000001c3bc28$a0f10690$3e01fc3e@vsn.local>
Message-ID: <3FD22CA2.9020808@indigoindustrial.co.nz>

Richard Pugh wrote:
...
> I have run into some issues regarding the way R handles its memory,
> especially on NT.  
...

Actually, you've run into NT's nasty memory management.  Welcome! :)
R-core have worked very hard to work around Windows memory issues, so 
they've probably got a better answer than I can give.  I'll give you a 
few quick answers, and then wait for correction when one them replies.

> A typical call
> may look like this .
>  
> 
>>myInputData <- matrix(sample(1:100, 7500000, T), nrow=5000)
>>myPortfolio <- createPortfolio(myInputData)
> 
>  
> It seems I can only repeat this code process 2/3 times before I have to
> restart R (to get the memory back).  I use the same object names
> (myInputData and myPortfolio) each time, so I am not create more large
> objects ..

Actually, you do.  Re-using a name does not re-use the same blocks of 
memory.  The size of the object may change, for example.

>  
> I think the problems I have are illustrated with the following example
> from a small R session .
>  
> 
>># Memory usage for Rui process = 19,800
>>testData <- matrix(rnorm(10000000), 1000) # Create big matrix
>># Memory usage for Rgui process = 254,550k
>>rm(testData)
>># Memory usage for Rgui process = 254,550k
>>gc()
> 
>          used (Mb) gc trigger  (Mb)
> Ncells 369277  9.9     667722  17.9
> Vcells  87650  0.7   24286664 185.3
> 
>># Memory usage for Rgui process = 20,200k
> 
>  
> In the above code, R cannot recollect all memory used, so the memory
> usage increases from 19.8k to 20.2.  However, the following example is
> more typical of the environments I use .
>  
> 
>># Memory 128,100k
>>myTestData <- matrix(rnorm(10000000), 1000)
>># Memory 357,272k
>>rm(myTestData)
>># Memory 357,272k
>>gc()
> 
>           used (Mb) gc trigger  (Mb)
> Ncells  478197 12.8     818163  21.9
> Vcells 9309525 71.1   31670210 241.7
> 
>># Memory 279,152k

R can return memory to Windows, but it cannot *make* Windows take it 
back.  Exiting the app is the only guaranteed way to do this, for any 
application.

The fact that you get this with matricies makes me suspect 
fragmentation issues with memory, rather than pure lack of memory. 
Here, the memory is disorganised, thanks to some programmers in Redmond. 
  When a matrix gets assigned, it needs all its memory to be contiguous. 
  If the memory on your machine has, say, 250 MB free, but only in 1 MB 
chunks, and you need to build a 2 MB matrix, you're out of luck.

 From the sounds of your calculations, they *must* be done as big 
matricies (true?).  If not, try a data structure that isn't a matrix or 
array; these require *contiguous* blocks of memory.  Lists, by 
comparison, can store their components in separate blocks.  Would a list 
of smaller matricies work?

> Could anyone point out what I could do to rectify this (if anything), or
> generally what strategy I could take to improve this?

Some suggestions:

1) call gc() somewhere inside your routines regularly.  Not guaranteed 
to help, but worth a try.

2) Get even more RAM, and hope it stabilises.

3) Change data structures to something other than one huge matrix. 
Matricies have huge computational advantages, but are pigs for memory.

4) Export the data crunching part of the application to an operating 
system that isn't notorious for bad memory management.  <opinion, 
subjective=yes> I've almost never had anguish from Solaris.  Linux and 
FreeBSD are not bad. </opinion> Have you considered running the results 
on a different machine, and storing the results in a fresh table on the 
same database as where you get the raw data?

Hope that helps.

Jason
-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From edd at debian.org  Sat Dec  6 20:30:22 2003
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sat, 6 Dec 2003 13:30:22 -0600
Subject: [R] Axe time of series in format yy-mm-dd
In-Reply-To: <001201c3bc21$1de20440$772c5451@galactic>
References: <001201c3bc21$1de20440$772c5451@galactic>
Message-ID: <20031206193022.GA30951@sonny.eddelbuettel.com>

On Sat, Dec 06, 2003 at 05:48:06PM -0000, M. M. Palhoto N. Rodrigues wrote:
> I'm trying to plot a ibm stock time series.
> I made the download of that series,
>  ibm <- get.hist.quote(instrument = "ibm",  start = "2003-01-01",quote=c("CL"))
> And  ibm is a serie wiht this characteristic:
> Start = 37623 
> End = 37960 
> Frequency = 1 
> When I try to plot it,
> ts.plot(ibm)
> In the graphic the axe time is represented by  37623 ... 37960, How can I put the time in the format,
> yy-mm-dd ?

As you certainly read the content of 'help(get.hist.quote)', you will know
that the date is stored in Julian days since the 'origin' variable.

So to display dates in a more human-readable format, you need to convert
from Julian dates to a proper date representation and then label the axis
accordingly.  Now, luckily, there are examples -- as e.g. the plotOHLC()
function which is also in the tseries package.  So do 

   > IBM <- get.hist.quote(instrument = "ibm", start='2003-01-01')
   trying URL
   http://chart.yahoo.com/table.csv?s=ibm&a=11&b=31&c=2002&d=11&e=04&f=2003&g=d&q=q&y=0&z=ibm&x=.csv'
   Content type application/octet-stream' length unknown
   opened URL
   .......... .
   downloaded 11Kb

   time series starts 2002-12-30
   time series ends   2003-12-03
   > plotOHLC(IBM)

which works swimmingly for mye on Linux, and displays five dates spanning
from '2002-12-31' to '2003-12-04'.  One caveat: plotOHLC is both a) dog-slow
for larger datasets and b) meaningless as you can't see the open/close ticks
anyway.


Hope this helps,  Dirk

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx



From xma at arcturusag.com  Sat Dec  6 22:31:44 2003
From: xma at arcturusag.com (Xiao-Jun Ma)
Date: Sat, 6 Dec 2003 13:31:44 -0800 
Subject: [R] Getting rid of loops?
Message-ID: <BBAF0DEC119BD41193C100B0D0788DFE3FCF5D@GENOME>

Simon and Peter,

Thanks for your help. Peter's function speeds it up 25x vs. my naive code!

XiaoJun


-----Original Message-----
From: Peter Dalgaard
To: Simon.Gatehouse at csiro.au
Cc: r-help at stat.math.ethz.ch; Xiao-Jun Ma
Sent: 02-12-03 15.57
Subject: Re: [R] Getting rid of loops?

Simon.Gatehouse at csiro.au writes:

> I think this will do what you want, though there may be ways of
speeding it
> up further.
> 
theta.dist2 <- function(x)
as.dist(acos(crossprod(t(x))/sqrt(crossprod(t(rowSums(x*x)))))/pi*180)

Or,

theta.dist <- function(x)
  as.dist(acos(cov2cor(crossprod(t(x))))/pi*180)

Now, if only there was a way to tell cor() not to center the
variables, we'd have 

  as.dist(acos(cor(t(x),center=F))/pi*180)

Unfortunately there's no such argument.

> 
> theta.dist <- function(x){
> 
>   res <- matrix(NA, nrow(x), nrow(x))
> 
>   for (i in 1:nrow(x)){
>     for(j in 1:nrow(x)){
>       if (i > j)
>         res[i, j] <- res[j, i]
>       else {
>         v1 <- x[i,]
>         v2 <- x[j,]
>         good <- !is.na(v1) & !is.na(v2)
>         v1 <- v1[good]
>         v2 <- v2[good]
>         theta <- acos(v1%*%v2 / sqrt(v1%*%v1 * v2%*%v2 )) / pi * 180
>         res[i,j] <- theta
>       }
>     }
>   }
>   as.dist(res)
> }
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> <https://www.stat.math.ethz.ch/mailman/listinfo/r-help> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From julieremold at yahoo.com.br  Sat Dec  6 23:57:40 2003
From: julieremold at yahoo.com.br (Julie Remold)
Date: Sat, 6 Dec 2003 20:57:40 -0200
Subject: [R] recode{car} does not work as expected?
Message-ID: <2003126205740.662209@eMachines>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031206/dad24cf6/attachment.pl

From jfox at mcmaster.ca  Sun Dec  7 00:24:27 2003
From: jfox at mcmaster.ca (John Fox)
Date: Sat, 06 Dec 2003 18:24:27 -0500
Subject: [R] Difference between summary.lm() and summary.aov()
In-Reply-To: <3FD20F1B.702@pdf.com>
References: <20031206161055.9355.qmail@web60001.mail.yahoo.com>
	<20031206161055.9355.qmail@web60001.mail.yahoo.com>
Message-ID: <5.1.0.14.2.20031206182135.01fd3a80@127.0.0.1>

Dear Spencer and Alexander,

In this case, physician is apparently a factor with three levels, so 
summary.aov() gives you a sequential ANOVA, equivalent to what you'd get 
from anova(). There no simple relationship between the F-statistic for 
physician, which has 2 df in the numerator, and the two t's. (By the way, I 
doubt whether a sequential ANOVA is what's wanted here.)

Regards,
  John

At 09:17 AM 12/6/2003 -0800, Spencer Graves wrote:
>      The square of a Student's t with "df" degrees of freedom is an F 
> distribution with 1 and "df" degrees of freedom.
>      hope this helps.  spencer graves
>
>Alexander Sirotkin [at Yahoo] wrote:
>
>>I have a simple linear model (fitted with lm()) with 2
>>independant
>>variables : one categorical and one integer.
>>
>>When I run summary.lm() on this model, I get a
>>standard linear
>>regression summary (in which one categorical variable
>>has to be
>>converted into many indicator variables) which looks
>>like :
>>
>>            Estimate Std. Error t value Pr(>|t|)
>>(Intercept)  -3595.3     2767.1  -1.299   0.2005
>>physicianB     802.0     2289.5   0.350   0.7277
>>physicianC    4906.8     2419.8   2.028   0.0485 *
>>severity      7554.4      906.3   8.336 1.12e-10 ***
>>
>>and when I run summary.aov() I get similar ANOVA table
>>:
>>           Df     Sum Sq    Mean Sq F value    Pr(>F)
>>physician    2  294559803  147279901  3.3557   0.04381
>>*
>>severity     1 3049694210 3049694210 69.4864 1.124e-10
>>***
>>Residuals   45 1975007569   43889057
>>
>>What is absolutely unclear to me is how F-value and
>>Pr(>F) for the
>>categorical "physician" variable of the summary.aov()
>>is calculated
>>from the t-value of the summary.lm() table.
>>
>>I looked at the summary.aov() source code but still
>>could not figure
>>it.
>>
>>Thanks a lot.
>>
>>__________________________________
>>
>>New Yahoo! Photos - easier uploading and sharing.
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-----------------------------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada L8S 4M4
email: jfox at mcmaster.ca
phone: 905-525-9140x23604
web: www.socsci.mcmaster.ca/jfox



From jfox at mcmaster.ca  Sun Dec  7 00:27:39 2003
From: jfox at mcmaster.ca (John Fox)
Date: Sat, 06 Dec 2003 18:27:39 -0500
Subject: [R] recode{car} does not work as expected?
In-Reply-To: <2003126205740.662209@eMachines>
Message-ID: <5.1.0.14.2.20031206182503.02048208@127.0.0.1>

Dear Julie,

At 08:57 PM 12/6/2003 -0200, Julie Remold wrote:
>Hi to all,
>
>I am having a problem when using recode from the car package. I have
>a variable that is char and when I use recode to create a new
>variable that is supposed to be numeric I get a factor variable. Here
>is the code I am using:
>
> > FCI$PRE1 <- recode (PRE1RES, " c('C', 'c')=1 ; else = 0 ;
>as.factor.result=FALSE ")
>
>So, if I understand correctly the recode command, the line above
>should  create FCI$PRE1 variable with 1 if PRE1RES is c and zero
>otherwise. However, if I type
>
> > FCI$PRE1[0]
>
>I get as result:
>
>factor(0)
>Levels: 0 1
>
>FCI$PRE1 is not a numeric variable, and I get instead a factor.
>Recode seems to work fine as it correctly replaces the c's for 1 and
>other results for 0, but the fact that the recoded variable is a
>factor does not work for me.
>
>Any suggestions on what I am doing wrong?

You're putting quotation marks around the argument is.factor.result as if 
it's part of the recode specifications. I believe that you want

         FCI$PRE1 <- recode (PRE1RES, " c('C', 'c')=1 ; else = 0 ", 
as.factor.result=FALSE )

I hope that this helps,
  John

-----------------------------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada L8S 4M4
email: jfox at mcmaster.ca
phone: 905-525-9140x23604
web: www.socsci.mcmaster.ca/jfox



From dominique.couturier at unine.ch  Sun Dec  7 01:21:04 2003
From: dominique.couturier at unine.ch (Dominique Couturier)
Date: Sun, 07 Dec 2003 01:21:04 +0100
Subject: Rep: [R] pdf() function, screen command and graphs
In-Reply-To: <Pine.LNX.4.44.0312061903180.29769-100000@gannet.stats>
Message-ID: <3E6E8D0B-284B-11D8-AE7F-0003931DD6AE@unine.ch>


Hi all,
There was no unwanted space bar in my code.
Screen seems not to like... (it works with the mfrow argument of par() 
but is (in my point of view) less flexible)
I upgrade then to 1.8.1 and it now works!
thanks
DLC

> Works perfectly (if garishly) for me in R 1.8.1.
> Perhaps you could benefit from an R upgrade?
>
> [I presume a line got wrapped, BTW, and if you had made good use of 
> your
> space bar it would not have produced invalid code ....]
>
> On Sat, 6 Dec 2003, Dominique Couturier wrote:
>
>> Dear [R]-list,
>>
>> I am trying to do a pdf() of the following graphs but don't understand
>> why the pdf() function does produce an empty pdf file.
>> (I use R1.7.0 on MacOS 10.2.8)
>> any idea? Is pdf() incompatible with screen?
>> Thanks a lot,
>> DLC
>>
>> ## create dataset
>> x=rnorm(1000,10,2)
>> y=rpois(1000,5)
>>
>> ## graphs
>>   pdf()
>>    par(col.main=4,omi=c(0,0,1.25,0))
>>          split.screen(c(1,2))
>>          screen(1);split.screen(c(2,1))
>>
>>          screen(3) ## gauche en haut
>>          boxplot(x,col=3,main="my boxplot")
>>
>>          screen(2) ## droite seul
>>          barplot(table(y),col=4,main="my barplot")
>>
>>          screen(4) ## gauche en bas
>>          qqnorm(x,main="mon qqnorm");qqline(x,col=5)
>>          mtext(paste("Graphics are important","\n","I love
>> R"),side=3,col=2,cex=1.5,outer=T)
>>    dev.off()
>>
>> ## remove dataset
>>   rm(x,y)
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>
>>
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From v.demart at libero.it  Sun Dec  7 08:38:17 2003
From: v.demart at libero.it (v.demart@libero.it)
Date: Sun,  7 Dec 2003 08:38:17 +0100
Subject: [R] A hint to start ESS-xemacs
Message-ID: <HPIL7T$F171A1CC47ACACE634CA47E5CDC179D6@libero.it>

I'm trying to use ESS & xemacs under debian linux testing and KDE.

My problem is that I didn't find any document in the internet explaining a
**step by step** session with R and xemacs.

The (wrong) procedure I follow (to no avail!) is:
1)  I start R in a terminal window;
2) I start xemacs and open a file with the extension .R (test.R);
3) I issue M-x R RET and I can see an Rd new item in the menu;

And WHAT NEXT?

I mean if I want to start an R interactive session under xemacs what should I do?
And, to record the session?

Please help.

Ciao from Rome, Italy

Vittorio



From Detlef.Steuer at unibw-hamburg.de  Sun Dec  7 11:56:08 2003
From: Detlef.Steuer at unibw-hamburg.de (Detlef Steuer)
Date: Sun, 7 Dec 2003 11:56:08 +0100
Subject: [R] A hint to start ESS-xemacs
In-Reply-To: <HPIL7T$F171A1CC47ACACE634CA47E5CDC179D6@libero.it>
References: <HPIL7T$F171A1CC47ACACE634CA47E5CDC179D6@libero.it>
Message-ID: <20031207115608.6815f98d.steuer@unibw-hamburg.de>

On Sun,  7 Dec 2003 08:38:17 +0100
"v\.demart\@libero\.it" <v.demart at libero.it> wrote:

> I'm trying to use ESS & xemacs under debian linux testing and KDE.
> 
> My problem is that I didn't find any document in the internet explaining a
> **step by step** session with R and xemacs.
> 
> The (wrong) procedure I follow (to no avail!) is:
> 1)  I start R in a terminal window;
> 2) I start xemacs and open a file with the extension .R (test.R);
> 3) I issue M-x R RET and I can see an Rd new item in the menu;

Just do
1) start xemacs
2) M-x R RET

If ESS is installed correctly, you now find the R command prompt 
inside your xemacs.


> 
> And WHAT NEXT?
> 
> I mean if I want to start an R interactive session under xemacs what should I do?
> And, to record the session?
> 
> Please help.
> 
> Ciao from Rome, Italy
> 
> Vittorio
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 


-- 
"Die herrschenden Ideen sind die Ideen der Herrschenden."
--- K. Marx

Detlef Steuer --- http://fawn.unibw-hamburg.de/steuer.html
***** Encrypted mail preferred *****



From Detlef.Steuer at unibw-hamburg.de  Sun Dec  7 12:05:37 2003
From: Detlef.Steuer at unibw-hamburg.de (Detlef Steuer)
Date: Sun, 7 Dec 2003 12:05:37 +0100
Subject: [R] GUI\'s for R
In-Reply-To: <1070644550.19312.96.camel@localhost.localdomain>
References: <1070642867.19312.87.camel@localhost.localdomain>
	<3fd0b3b2779b39.19994528@univ-montp2.fr>
	<1070644550.19312.96.camel@localhost.localdomain>
Message-ID: <20031207120537.771a9b8c.steuer@unibw-hamburg.de>

On Fri, 05 Dec 2003 11:15:50 -0600
Marc Schwartz <MSchwartz at medanalytics.com> wrote:

> On Fri, 2003-12-05 at 10:34, paradis at univ-montp2.fr wrote:
>   Marc Schwartz wrote:
> > > Rgui.exe is the Windows 'front end' environment for R that provides a
> > > command line console for entering commands to the R interpreter, seeing
> > > the textual output of those commands and for displaying the output of
> > > plots. It supports a MDI/SDI type of interface. It does have some menus
> > > for simple operations (like installing and updating packages) but not
> > > for performing analyses. It is Windows OS only.
> > 
> > There is a very similar "GUI" under Linux that runs under the Gnome
> > desktop environment, and I believe Gnome can run under SunOS.
> 
> 
> Yeah...you need to compile from source and use the
> 
> ./configure --with-gnome

You may need to give the paths to gnome and libglade-config.
(At least I have to, configure fails for me)

./configure --help tells the details.

detlef

> 
> option to get it.  I tried that once some time ago. Like RGui.exe under
> Windows, IIRC, it provides some basic menus for package installation,
> etc.
> 
> It does not however provide any access to analytic/plotting functions.
> More information is available in the R-admin manual.
> 
> Best regards,
> 
> Marc
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 


-- 
"Die herrschenden Ideen sind die Ideen der Herrschenden."
--- K. Marx

Detlef Steuer --- http://fawn.unibw-hamburg.de/steuer.html
***** Encrypted mail preferred *****



From alex_s_42 at yahoo.com  Sun Dec  7 13:21:47 2003
From: alex_s_42 at yahoo.com (Alexander Sirotkin [at Yahoo])
Date: Sun, 7 Dec 2003 04:21:47 -0800 (PST)
Subject: [R] Difference between summary.lm() and summary.aov()
In-Reply-To: <5.1.0.14.2.20031206182135.01fd3a80@127.0.0.1>
Message-ID: <20031207122147.4592.qmail@web60002.mail.yahoo.com>

John,

What you are saying is that any conclusion I can make
from summary.aov (for instance, to answer a question
if physician is a significant variable) will not be
correct ?


--- John Fox <jfox at mcmaster.ca> wrote:
> Dear Spencer and Alexander,
> 
> In this case, physician is apparently a factor with
> three levels, so 
> summary.aov() gives you a sequential ANOVA,
> equivalent to what you'd get 
> from anova(). There no simple relationship between
> the F-statistic for 
> physician, which has 2 df in the numerator, and the
> two t's. (By the way, I 
> doubt whether a sequential ANOVA is what's wanted
> here.)
> 
> Regards,
>   John
> 
> At 09:17 AM 12/6/2003 -0800, Spencer Graves wrote:
> >      The square of a Student's t with "df" degrees
> of freedom is an F 
> > distribution with 1 and "df" degrees of freedom.
> >      hope this helps.  spencer graves
> >
> >Alexander Sirotkin [at Yahoo] wrote:
> >
> >>I have a simple linear model (fitted with lm())
> with 2
> >>independant
> >>variables : one categorical and one integer.
> >>
> >>When I run summary.lm() on this model, I get a
> >>standard linear
> >>regression summary (in which one categorical
> variable
> >>has to be
> >>converted into many indicator variables) which
> looks
> >>like :
> >>
> >>            Estimate Std. Error t value Pr(>|t|)
> >>(Intercept)  -3595.3     2767.1  -1.299   0.2005
> >>physicianB     802.0     2289.5   0.350   0.7277
> >>physicianC    4906.8     2419.8   2.028   0.0485 *
> >>severity      7554.4      906.3   8.336 1.12e-10
> ***
> >>
> >>and when I run summary.aov() I get similar ANOVA
> table
> >>:
> >>           Df     Sum Sq    Mean Sq F value   
> Pr(>F)
> >>physician    2  294559803  147279901  3.3557  
> 0.04381
> >>*
> >>severity     1 3049694210 3049694210 69.4864
> 1.124e-10
> >>***
> >>Residuals   45 1975007569   43889057
> >>
> >>What is absolutely unclear to me is how F-value
> and
> >>Pr(>F) for the
> >>categorical "physician" variable of the
> summary.aov()
> >>is calculated
> >>from the t-value of the summary.lm() table.
> >>
> >>I looked at the summary.aov() source code but
> still
> >>could not figure
> >>it.
> >>
> >>Thanks a lot.
> >>
> >>__________________________________
> >>

> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
>
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >>
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
>
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
>
-----------------------------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario, Canada L8S 4M4
> email: jfox at mcmaster.ca
> phone: 905-525-9140x23604
> web: www.socsci.mcmaster.ca/jfox
>
-----------------------------------------------------
>



From baron at psych.upenn.edu  Sun Dec  7 13:28:05 2003
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Sun, 7 Dec 2003 07:28:05 -0500
Subject: [R] A hint to start ESS-xemacs
In-Reply-To: <HPIL7T$F171A1CC47ACACE634CA47E5CDC179D6@libero.it>
References: <HPIL7T$F171A1CC47ACACE634CA47E5CDC179D6@libero.it>
Message-ID: <20031207122805.GA9981@mail1.sas.upenn.edu>

On 12/07/03 08:38, v.demart at libero.it wrote:
>I'm trying to use ESS & xemacs under debian linux testing and KDE.
>
>My problem is that I didn't find any document in the internet explaining a
>**step by step** session with R and xemacs.

Here is how I do it.  (I admit that it is much easier to tell you
this than to find out where I learned it.)

1. Start Xemacs.  (I do this about once a day and keep it in one
   viewport.  I don't know if KDE has viewports the way that
   Metacity does under Gnome.)

2. In Xemacs, give the command M-x R (which for me is alt-x R).
   This starts R as a function in Xemacs.  It prompts for a
   directory and I answer the prompt question with wherever the
   relevant .R file is.

3. In another viewport (or window, or whatever), open the .R file
   with gnuclient, e.g. "gnuclient myfile.R".  (I have a key
   aliaised to gnuclient in my .cshrc.)

The order of steps 2 and 3 can be reversed.  And I don't think
you need to use gnuclient, although it is faster than starting
Xemacs.

As for saving the session, I think you can do that when you exit
R in the Xemacs window, in the usual way.  But I don't.  Instead,
I save commands that I like (about 10% of the things I try, which
is why I don't save the session) by cutting and pasting them from
the first window to the second, and then I save the second.
Sometimes I put # in front of them (e.g., when they involve
making and saving a figure, which I don't want to remake every
time I run the file).  Sometimes I put readline() or stop()
commands in the file, so that I don't have to run the whole
thing.

When you are done, you can quit R in the first window with q(),
then use Xemacs for something else.  When I write a paper, I
usually have at least two gnuclient windows, one containing the
paper, one the .R file.

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron
R page: http://finzi.psych.upenn.edu



From uth at zhwin.ch  Sun Dec  7 13:44:48 2003
From: uth at zhwin.ch (=?utf-8?Q?=22Untern=C3=A4hrer_Thomas=2C_uth=22?=)
Date: Sun, 7 Dec 2003 13:44:48 +0100
Subject: AW: [R] A hint to start ESS-xemacs
Message-ID: <53A181E56FB0694ABFD212F8AEDA7F6F1AB8E4@langouste.zhwin.ch>

Hi,
 
Have a look at http://stat.ethz.ch/ESS/.
 
First of all you have to edit the inite.el  (require 'ess-site)
 
HTH
 
Thomas
 

	-----Urspr?ngliche Nachricht----- 
	Von: v.demart at libero.it [mailto:v.demart at libero.it] 
	Gesendet: So 07.12.2003 08:38 
	An: r-help r-help 
	Cc: 
	Betreff: [R] A hint to start ESS-xemacs
	
	

	I'm trying to use ESS & xemacs under debian linux testing and KDE.
	
	My problem is that I didn't find any document in the internet explaining a
	**step by step** session with R and xemacs.
	
	The (wrong) procedure I follow (to no avail!) is:
	1)  I start R in a terminal window;
	2) I start xemacs and open a file with the extension .R (test.R);
	3) I issue M-x R RET and I can see an Rd new item in the menu;
	
	And WHAT NEXT?
	
	I mean if I want to start an R interactive session under xemacs what should I do?
	And, to record the session?
	
	Please help.
	
	Ciao from Rome, Italy
	
	Vittorio
	
	______________________________________________
	R-help at stat.math.ethz.ch mailing list
	https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Sun Dec  7 13:57:10 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 7 Dec 2003 12:57:10 +0000 (GMT)
Subject: [R] Difference between summary.lm() and summary.aov()
In-Reply-To: <20031207122147.4592.qmail@web60002.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0312071246270.26882-100000@gannet.stats>

On Sun, 7 Dec 2003, Alexander Sirotkin [at Yahoo] wrote:

> John,
> 
> What you are saying is that any conclusion I can make
> from summary.aov (for instance, to answer a question
> if physician is a significant variable) will not be
> correct ?

If that is your question *both* are incorrect.  The correct function to 
use is drop1() (or equivalently Anova from car with the right options).

For a detailed comparison of two t tests and the F test (for a term fitted
last) see Largey & Spencer (1996) _The Statistician_ 45, 105-9.

Once again, aov() and its methods are designed for classical AoV problems 
which are balanced and in which sequential anova (as implemented here, 
that is with a common denominator) is appropriate and interpreting 
coefficients (as in summary.lm) is not.


> --- John Fox <jfox at mcmaster.ca> wrote:
> > Dear Spencer and Alexander,
> > 
> > In this case, physician is apparently a factor with
> > three levels, so 
> > summary.aov() gives you a sequential ANOVA,
> > equivalent to what you'd get 
> > from anova(). There no simple relationship between
> > the F-statistic for 
> > physician, which has 2 df in the numerator, and the
> > two t's. (By the way, I 
> > doubt whether a sequential ANOVA is what's wanted
> > here.)
> > 
> > Regards,
> >   John
> > 
> > At 09:17 AM 12/6/2003 -0800, Spencer Graves wrote:
> > >      The square of a Student's t with "df" degrees
> > of freedom is an F 
> > > distribution with 1 and "df" degrees of freedom.
> > >      hope this helps.  spencer graves
> > >
> > >Alexander Sirotkin [at Yahoo] wrote:
> > >
> > >>I have a simple linear model (fitted with lm())
> > with 2
> > >>independant
> > >>variables : one categorical and one integer.
> > >>
> > >>When I run summary.lm() on this model, I get a
> > >>standard linear
> > >>regression summary (in which one categorical
> > variable
> > >>has to be
> > >>converted into many indicator variables) which
> > looks
> > >>like :
> > >>
> > >>            Estimate Std. Error t value Pr(>|t|)
> > >>(Intercept)  -3595.3     2767.1  -1.299   0.2005
> > >>physicianB     802.0     2289.5   0.350   0.7277
> > >>physicianC    4906.8     2419.8   2.028   0.0485 *
> > >>severity      7554.4      906.3   8.336 1.12e-10
> > ***
> > >>
> > >>and when I run summary.aov() I get similar ANOVA
> > table
> > >>:
> > >>           Df     Sum Sq    Mean Sq F value   
> > Pr(>F)
> > >>physician    2  294559803  147279901  3.3557  
> > 0.04381
> > >>*
> > >>severity     1 3049694210 3049694210 69.4864
> > 1.124e-10
> > >>***
> > >>Residuals   45 1975007569   43889057
> > >>
> > >>What is absolutely unclear to me is how F-value
> > and
> > >>Pr(>F) for the
> > >>categorical "physician" variable of the
> > summary.aov()
> > >>is calculated
> > >>from the t-value of the summary.lm() table.
> > >>
> > >>I looked at the summary.aov() source code but
> > still
> > >>could not figure
> > >>it.
> > >>
> > >>Thanks a lot.
> > >>
> > >>__________________________________
> > >>
> 
> > >>
> > >>______________________________________________
> > >>R-help at stat.math.ethz.ch mailing list
> >
> >>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > >>
> > >
> > >______________________________________________
> > >R-help at stat.math.ethz.ch mailing list
> >
> >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > 
> >
> -----------------------------------------------------
> > John Fox
> > Department of Sociology
> > McMaster University
> > Hamilton, Ontario, Canada L8S 4M4
> > email: jfox at mcmaster.ca
> > phone: 905-525-9140x23604
> > web: www.socsci.mcmaster.ca/jfox
> >
> -----------------------------------------------------
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Sun Dec  7 14:17:11 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 07 Dec 2003 14:17:11 +0100
Subject: [R] Difference between summary.lm() and summary.aov()
In-Reply-To: <20031207122147.4592.qmail@web60002.mail.yahoo.com>
References: <20031207122147.4592.qmail@web60002.mail.yahoo.com>
Message-ID: <x23cbwix7s.fsf@biostat.ku.dk>

"Alexander Sirotkin [at Yahoo]" <alex_s_42 at yahoo.com> writes:

> John,
> 
> What you are saying is that any conclusion I can make
> from summary.aov (for instance, to answer a question
> if physician is a significant variable) will not be
> correct ?

Summary.aov is for summarizing aov objects, so you're lucky to get
something that is sensible at all. You should use anova() to get
analysis of variance tables. These are sequential so that you can use
them (give or take some quibbles about the residual variance) for
reducing the model from the "bottom up". I.e. if you place "physician"
last, you get the F test for whether that variable is significant.
However, a more convenient way of getting that result is to use
drop1(). Even then there's no simple relation to the two
t-tests, except that the F test tests the hypothesis that *both*
coefficients are zero, where the t-tests do so individually. 
 

> --- John Fox <jfox at mcmaster.ca> wrote:
> > Dear Spencer and Alexander,
> > 
> > In this case, physician is apparently a factor with
> > three levels, so 
> > summary.aov() gives you a sequential ANOVA,
> > equivalent to what you'd get 
> > from anova(). There no simple relationship between
> > the F-statistic for 
> > physician, which has 2 df in the numerator, and the
> > two t's. (By the way, I 
> > doubt whether a sequential ANOVA is what's wanted
> > here.)
> > 
> > Regards,
> >   John
> > 
> > At 09:17 AM 12/6/2003 -0800, Spencer Graves wrote:
> > >      The square of a Student's t with "df" degrees
> > of freedom is an F 
> > > distribution with 1 and "df" degrees of freedom.
> > >      hope this helps.  spencer graves
> > >
> > >Alexander Sirotkin [at Yahoo] wrote:
> > >
> > >>I have a simple linear model (fitted with lm())
> > with 2
> > >>independant
> > >>variables : one categorical and one integer.
> > >>
> > >>When I run summary.lm() on this model, I get a
> > >>standard linear
> > >>regression summary (in which one categorical
> > variable
> > >>has to be
> > >>converted into many indicator variables) which
> > looks
> > >>like :
> > >>
> > >>            Estimate Std. Error t value Pr(>|t|)
> > >>(Intercept)  -3595.3     2767.1  -1.299   0.2005
> > >>physicianB     802.0     2289.5   0.350   0.7277
> > >>physicianC    4906.8     2419.8   2.028   0.0485 *
> > >>severity      7554.4      906.3   8.336 1.12e-10
> > ***
> > >>
> > >>and when I run summary.aov() I get similar ANOVA
> > table
> > >>:
> > >>           Df     Sum Sq    Mean Sq F value   
> > Pr(>F)
> > >>physician    2  294559803  147279901  3.3557  
> > 0.04381
> > >>*
> > >>severity     1 3049694210 3049694210 69.4864
> > 1.124e-10
> > >>***
> > >>Residuals   45 1975007569   43889057
> > >>
> > >>What is absolutely unclear to me is how F-value
> > and
> > >>Pr(>F) for the
> > >>categorical "physician" variable of the
> > summary.aov()
> > >>is calculated
> > >>from the t-value of the summary.lm() table.
> > >>
> > >>I looked at the summary.aov() source code but
> > still
> > >>could not figure
> > >>it.
> > >>
> > >>Thanks a lot.
> > >>
> > >>__________________________________
> > >>
> 
> > >>
> > >>______________________________________________
> > >>R-help at stat.math.ethz.ch mailing list
> >
> >>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > >>
> > >
> > >______________________________________________
> > >R-help at stat.math.ethz.ch mailing list
> >
> >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > 
> >
> -----------------------------------------------------
> > John Fox
> > Department of Sociology
> > McMaster University
> > Hamilton, Ontario, Canada L8S 4M4
> > email: jfox at mcmaster.ca
> > phone: 905-525-9140x23604
> > web: www.socsci.mcmaster.ca/jfox
> >
> -----------------------------------------------------
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From rossini at blindglobe.net  Sun Dec  7 14:46:45 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Sun, 07 Dec 2003 05:46:45 -0800
Subject: [R] A hint to start ESS-xemacs
In-Reply-To: <HPIL7T$F171A1CC47ACACE634CA47E5CDC179D6@libero.it>
	(v.demart@libero.it's
	message of "Sun,  7 Dec 2003 08:38:17 +0100")
References: <HPIL7T$F171A1CC47ACACE634CA47E5CDC179D6@libero.it>
Message-ID: <85vfossptm.fsf@blindglobe.net>

"v\.demart\@libero\.it" <v.demart at libero.it> writes:

> I'm trying to use ESS & xemacs under debian linux testing and KDE.
>
> My problem is that I didn't find any document in the internet explaining a
> **step by step** session with R and xemacs.

What is missing from

  http://www.analytics.washington.edu/~rossini/courses/cph-statcomp/cph-lab-1.html

?  Admittedly, it mentions starting using "emacs -f R", but 
"xemacs -f R" works as well.   I can update if you provide more
details.  (in private, no need to bore the rest of the list with the
discussion).

best,
-tony

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From rossini at blindglobe.net  Sun Dec  7 14:49:17 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Sun, 07 Dec 2003 05:49:17 -0800
Subject: [R] A hint to start ESS-xemacs
In-Reply-To: <HPIL7T$F171A1CC47ACACE634CA47E5CDC179D6@libero.it>
	(v.demart@libero.it's
	message of "Sun,  7 Dec 2003 08:38:17 +0100")
References: <HPIL7T$F171A1CC47ACACE634CA47E5CDC179D6@libero.it>
Message-ID: <85r7zgsppe.fsf@blindglobe.net>


Whoops.  Sorry.  

"v\.demart\@libero\.it" <v.demart at libero.it> writes:

> I'm trying to use ESS & xemacs under debian linux testing and KDE.
>
> My problem is that I didn't find any document in the internet explaining a
> **step by step** session with R and xemacs.
>
> The (wrong) procedure I follow (to no avail!) is:
> 1)  I start R in a terminal window;
> 2) I start xemacs and open a file with the extension .R (test.R);
> 3) I issue M-x R RET and I can see an Rd new item in the menu;

This should  have given you an R shell (similar to typing R in a
terminal window.   Does it?

>
> And WHAT NEXT?
>
> I mean if I want to start an R interactive session under xemacs what should I do?
> And, to record the session?

Better to move this discussion to the ESS-help mailing list.

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From edd at debian.org  Sun Dec  7 15:27:46 2003
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 7 Dec 2003 08:27:46 -0600
Subject: [R] A hint to start ESS-xemacs
In-Reply-To: <HPIL7T$F171A1CC47ACACE634CA47E5CDC179D6@libero.it>
References: <HPIL7T$F171A1CC47ACACE634CA47E5CDC179D6@libero.it>
Message-ID: <20031207142746.GA4915@sonny.eddelbuettel.com>

On Sun, Dec 07, 2003 at 08:38:17AM +0100, v.demart at libero.it wrote:
> I'm trying to use ESS & xemacs under debian linux testing and KDE.
> 
> My problem is that I didn't find any document in the internet explaining a
> **step by step** session with R and xemacs.

I think what you are looking for is probably contained in one or more files
of the ESS source distribution. The Debian package I use comes with a host
of documents:

   edd at chibud:/usr/share/doc/ess> ls
   ChangeLog.gz          README.elsewhere           changelog.gz
   ESS_intro.tex.gz      README.gz                  copyright
   NEWS.gz               README.noweb.ESS           ess.dvi.gz
   README.S.gz           TODO.gz                    html
   README.SAS.gz         Why_R_mode_not_S_mode.txt  readme.dvi.gz
   README.SPLUS4COMMAND  Why_S-mode_Rocks.DMS       rmh-talk.tex.gz
   README.XLispStat.gz   ajr-talk.tex.gz            slverb.sty
   README.additions      changelog.Debian.gz

which your installation of ESS may or may not carry too.

For example, README.S contains the section below which I found useful. It
talsk more about S/S-PLUS than R but these days most things can be expected
to carry over.

Last but not least, are you aware that ESS comes with an extensive manual?

Hth,  Dirk


   [...]
   Scenarios for use
   =================
   
   We present some basic suggestions for using ESS to interact with S.
   These are just a subset of approaches, many better approaches are
   possible.  Contributions of examples of how you work with ESS are
   appreciated (especially since it helps us determine priorities on
   future enhancements)! (comments as to what should be happening are
   prefixed by "##").
   
   1:  ##    Data Analysis Example (source code is real)
       ## Load the file you want to work with
       C-x C-f myfile.s
   
       ## Edit as appropriate, and then start up S-PLUS 3.x
       M-x S+3
   
       ## A new buffer *S+3:1* will appear.  Splus will have been started
       ## in this buffer.  The buffer is in iESS [S+3:1] mode.
   
       ## Split the screen and go back to the file editing buffer.
       C-x 2 C-x b myfile.s
   
       ## Send regions, lines, or the entire file contents to S-PLUS.  For regions,
       ## highlight a region with keystrokes or mouse and then send with:
       C-c C-r
   
       ## Re-edit myfile.s as necessary to correct any difficulties.  Add
       ## new commands here.  Send them to S by region with C-c C-r, or
       ## one line at a time with C-c C-n.
   
       ## Save the revised myfile.s with C-x C-s.
   
       ## Save the entire *S+3:1* interaction buffer with C-c C-s.  You
       ## will be prompted for a file name.  The recommended name is
       ## myfile.St.  With the *.St suffix, the file will come up in ESS
       ## Transcript mode the next time it is accessed from Emacs.
   
   
   
   2:  ## Program revision example (source code is real)
   
       ## Start up S-PLUS 3.x in a process buffer (this will be *S+3:1*) 
       M-x S+3
   
       ## Load the file you want to work with
       C-x C-f myfile.s
       
       ## edit program, functions, and code in myfile.s, and send revised
       ## functions to S when ready with
       C-c C-f
       ## or highlighted regions with
       C-c C-r
       ## or individual lines with
       C-c C-n
       ## or load the entire buffer with 
       C-c C-l
   
       ## save the revised myfile.s when you have finished
       C-c C-s
   
   
   
   3:  ## Program revision example (S object is real)
   
       ## Start up S-PLUS 3.x in a process buffer (this will be *S+3:1*) 
       M-x S+3
   
       ## Dump an existing S object my.function into a buffer to work with
       C-c C-d my.function
       ## a new buffer named yourloginname.my.function.S will be created with
       ## an editable copy of the object.  The buffer is associated with the
       ## pathname /tmp/yourloginname.my.function.S and will amlost certainly not
       ## exist after you log off.
   
       ## enter program, functions, and code into work buffer, and send
       ## entire contents to S-PLUS when ready
       C-c C-b
   
       ## Go to *S+3:1* buffer, which is the process buffer, and examine
       ## the results.
       C-c C-y
       ## The sequence C-c C-y is a shortcut for:  C-x b *S+3:1*
   
       ## Return to the work buffer (may/may not be prefixed)
       C-x C-b yourloginname.my.function.S
       ## Fix the function that didn't work, and resubmit by placing the
       ## cursor somewhere in the function and
       C-c C-f
       ## Or you could've selected a region (using the mouse, or keyboard 
       ## via setting point/mark) and 
       C-c C-r
       ## Or you could step through, line by line, using 
       C-c C-n
       ## Or just send a single line (without moving to the next) using
       C-c C-j
       ## To fix that error in syntax for the "rchisq" command, get help
       ## by
       C-c C-v rchisq
   
   
   4:    Data Analysis (S object is real)
       ## Start up S-PLUS 3.x, in a process buffer (this will be *S+3:1*) 
       M-x S+3
   
       ## Work in the process buffer.  When you find an object that needs 
       ## to be changed (this could be a data frame, or a variable, or a 
       ## function), dump it to a buffer:
       C-c C-d my.cool.function
   
       ## Edit the function as appropriate, and dump back in to the
       ## process buffer  
       C-c C-b
   
       ## Return to the S-PLUS process buffer
       C-c C-y
       ## Continue working.
   
       ## When you need help, use 
       C-c C-v rchisq
       ## instead of entering:   help("rchisq")
   
   
   
   Customization Examples and Solutions to Problems
   ================================================
   
   1. Suppose that you are primarily an SPLUS 3.4 user, occasionally
      using S version 4, and sick and tired of the buffer-name *S+3*
      we've stuck you with.  Simply edit the "ess-dialect" alist entry in 
      the essd-s+3.el and essd-s4.el files to be "S" instead of "S4" and
      "S+3".  This will insure that all the inferior process buffer names 
      are "*S*".
   
   2. Suppose that you WANT to have the first buffer name indexed by
      ":1", in the same manner as your S-PLUS processes 2,3,4, and 5 (for
      you heavy simulation people).  Then uncomment the line in ess-site
      (or add after your (require 'ess-site) or (load "ess-site") command 
       in your .emacs file, the line:
          
          (setq ess-plain-first-buffername nil)
      )
   
   3. Fontlocking sometimes fails to behave nicely upon errors.  When
      Splus dumps, a mis-matched "  (double-quote) can result in the
      wrong font-lock face being used for the remainder of the buffer.  
   
      Solution: add a " at the end of the "Dumped..." statement, to
      revert the font-lock face back to normal.


-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx



From ligges at statistik.uni-dortmund.de  Sun Dec  7 16:05:43 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 07 Dec 2003 16:05:43 +0100
Subject: [R] Trouble with syntax
In-Reply-To: <20031202221058.83083.qmail@web40510.mail.yahoo.com>
References: <20031202221058.83083.qmail@web40510.mail.yahoo.com>
Message-ID: <3FD341C7.4050108@statistik.uni-dortmund.de>

Window Glass wrote:


[Looks like there was no answer on R-help yet]

> Hello,
>
> axsize <- max(i)

Let's assume i is a numeric (integer) vector:


> allsize <- axsize * axsize
> x <- array(c(1:allsize), dim=c(axsize, axsize))

You are going to construct a matrix, maybe it's simple to say

x <- matrix(1:allsize, axsize)


> x[1:allsize] <- 0

So, you are setting the whole matrix to 0 now? Isn't it better to 
initialize with 0 at once? E.g.:

x <- matrix(0, axsize, axsize)


> x[i] <- 1

Are you sure you want those 1s only in the first column?
You are indexing x still as a vector, not as a matrix!



> for (c in 1:axsize)

It's not a good idea to use a variable called "c" - there is a function 
c() (works here, though).


> {
>         ourgraph <- data.frame(edges=x,
> vertices=c(1:axsize), ith=c,
>                         components=list(1),
> communities=list(1), row.names=NULL)
>         browser()
> 
> When I get to browser, printing ourgraph$edges gives
> NULL while printing x gives the matrix with the
> correct values.
> What am I writing wrong?

A data.frame must contain an equal number of observations for each 
"column", so data.frame constructs the data.frame so that it fits. You 
are going to use a list!

  names(ourgraph)
or
  str(ourgraph)
will result in some deeper insights.

It doesn't make sense to overwrite ourgraph axsize times, BTW.

Uwe Ligges

> Regards,
> Wind0w Glass



From ligges at statistik.uni-dortmund.de  Sun Dec  7 16:16:03 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 07 Dec 2003 16:16:03 +0100
Subject: [R] Rblas for dual Xeon
In-Reply-To: <8CBAA121CEB4D5118CB200508BB2BBEF0317EAC3@xmx8lonib.lonib.commerzbank.com>
References: <8CBAA121CEB4D5118CB200508BB2BBEF0317EAC3@xmx8lonib.lonib.commerzbank.com>
Message-ID: <3FD34433.1030305@statistik.uni-dortmund.de>

Marsland, John wrote:

> Does anybody have a tuned Rblas.dll compiled against ALTLAS for a dual Xeon
> system?

Not that I know of.

It would be a great thing having some more Rblas.dll on CRAN,
e.g. for Xeon and Opteron (unfortunately, we don't have got such 
machines yet).


> Unfortunately, we have very strict security that does not allow compilers of
> any sort on production desktops - we only have Pentium III development PCs.

So using compilers is not allowed, but getting DLLs compiled by others 
from the Internet is allowed ... ?!

Uwe Ligges


> Regards,
> 
> John Marsland
> 
> 
> ********************************************************************** 
> This is a commercial communication from Commerzbank AG.\ \ T...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Sun Dec  7 16:27:15 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 7 Dec 2003 15:27:15 +0000 (GMT)
Subject: [R] Rblas for dual Xeon
In-Reply-To: <3FD34433.1030305@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.44.0312071523400.20244-100000@gannet.stats>

On Sun, 7 Dec 2003, Uwe Ligges wrote:

> Marsland, John wrote:
> 
> > Does anybody have a tuned Rblas.dll compiled against ALTLAS for a dual Xeon
> > system?
> 
> Not that I know of.
> 
> It would be a great thing having some more Rblas.dll on CRAN,
> e.g. for Xeon and Opteron (unfortunately, we don't have got such 
> machines yet).

Yes!  We do, but we run Linux only on them.  It looks like a PIII blas 
works pretty well on Xeon (dual is irrelevant on Windows as ATLAS is not 
multithreaded under MinGW) and on Opteron, though, under Linux so I guess 
it would under Windows.

I will shortly have a P4 HT as my home machine, to produce a Rblas.dll 
for.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From kim.mouridsen at ofir.dk  Sun Dec  7 18:45:38 2003
From: kim.mouridsen at ofir.dk (Kim Mouridsen)
Date: Sun, 7 Dec 2003 18:45:38 +0100
Subject: [R] logistic regression
Message-ID: <20031207174539.9D06114EC6E@postfix2.ofir.com>

Dear R-experts

A binary response is observed for patients receiving one of three different 
drugs injected in different doses. That is for each drug (treatment) we have a 
dose-response model. Additionlly the patient's age and gender is recorded.

Initially I ran three logistic regressions, one for each drug type with gender 
as categorical variable and age and dose as continuous variables. If I want to 
know the effect of, say, gender on the response I now have three odds ratios - 
one for each dose response model.

My question is: How can I compare the three odds ratios? Is it possible in R 
to combine the three dose-response models into a single model to get an 
overall estimate of the effect of age?
Can I do something like
lr <- glm(vom ~ therapy*age + therapy*gender + 
therapy*cisdose+therapy*cardose+therapy*cycdose,family=binomial,data=emrisk)

Thanks in advance!
Kim Mouridsen.

_________________________________________________________________________
OFiR Spil - Vind 1.000 vis af kroner! Bes?g http://spil.ofir.dk
OFiR Kontakt - Find en at dele julen med - Bes?g http://kontakt.ofir.dk



From p.connolly at hortresearch.co.nz  Sun Dec  7 22:03:49 2003
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Mon, 8 Dec 2003 10:03:49 +1300
Subject: [R] grid packages since R-1.7.1
In-Reply-To: <200312042323.12324.deepayan@stat.wisc.edu>
References: <20031205031955.GR28127@hortresearch.co.nz>
	<200312042323.12324.deepayan@stat.wisc.edu>
Message-ID: <20031207210349.GT28127@hortresearch.co.nz>

On Thu, 04-Dec-2003 at 11:23PM -0600, Deepayan Sarkar wrote:

|> On Thursday 04 December 2003 21:19, Patrick Connolly wrote:

[...]

|> > options(defaultPackages =  c("mva", "lattice"), keep.source.pkgs = TRUE)
|> >
|> > It's rather unlikely that I should specifically load grid also, but
|> 

|> That's exactly what you need to do (i.e., load grid explicitly).
|> This has to do with namespaces, which makes some earlier notions
|> obsolete. lattice 'imports' grid function definitions but by itself
|> no longer makes them available/visible to the user.


I understand from that that there is functionality in grid that
lattice doesn't normally require and so isn't in what is 'imported' to
lattice.  Evidently, all the time I used R-1.8.0, I was not requiring
that functionality.

|> 
|> This of course doesn't explain any problems in grid.polygon after loading 
|> grid. grid.polygon has changed in 1.8.1, but I think it is supposed to be 
|> back-compatible. Could you provide a reproducible example ? 

My hunch was completely off course -- nothing to do with scoping.  I
tracked it down to a deficiency in the error messages in 1.7.1 which
proceded to go on with what it could do without complaining about what
it couldn't do (because of no data in some cases).  1.8.1 is more
up-front and bawks when it can't do what was asked.  I simply added an
if statement to avoid trying to do the impossible.

Thanks for helping me clear that up.


-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From tord.snall at ebc.uu.se  Sun Dec  7 22:39:59 2003
From: tord.snall at ebc.uu.se (Tord Snall)
Date: Sun, 07 Dec 2003 22:39:59 +0100
Subject: [R] par(las = 1) not possible in polymap(), library(splancs)?
Message-ID: <3.0.6.32.20031207223959.00d4fed8@mail.anst.uu.se>

Dear all, 
I want my PhD thesis which I hand in tomorrow to look even nicer: 

Does polymap in the splancs library not allow horizontal plotting of
y-labels? I have tried 

polymap(studyarea, xlab = "x (m)", ylab = "y (m)", las = 1)

but it doesn't change the labels?

Mayby some function in library(spatstat) support las?


Thanks!

Sincerely,
Tord


-----------------------------------------------------------------------
Tord Sn?ll
Avd. f v?xtekologi, Evolutionsbiologiskt centrum, Uppsala universitet
Dept. of Plant Ecology, Evolutionary Biology Centre, Uppsala University
Villav?gen 14			
SE-752 36 Uppsala, Sweden
Tel: 018-471 28 82 (int +46 18 471 28 82) (work)
Tel: 018-25 71 33 (int +46 18 25 71 33) (home)
Fax: 018-55 34 19 (int +46 18 55 34 19) (work)
E-mail: Tord.Snall at ebc.uu.se
Check this: http://www.vaxtbio.uu.se/resfold/snall.htm!



From ligges at statistik.uni-dortmund.de  Sun Dec  7 22:51:13 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 07 Dec 2003 22:51:13 +0100
Subject: [R] par(las = 1) not possible in polymap(), library(splancs)?
References: <3.0.6.32.20031207223959.00d4fed8@mail.anst.uu.se>
Message-ID: <3FD3A0D1.444BFCEB@statistik.uni-dortmund.de>



Tord Snall wrote:
> 
> Dear all,
> I want my PhD thesis which I hand in tomorrow to look even nicer:
> Does polymap in the splancs library not allow horizontal plotting of
> y-labels? I have tried
> 
> polymap(studyarea, xlab = "x (m)", ylab = "y (m)", las = 1)
> 
> but it doesn't change the labels?

Use 
  par(las = 1)
  polymap(.....)

Not all the arguments of par() do work as arguments in all highlevel
functions ...

Uwe Ligges


> Mayby some function in library(spatstat) support las?
> 
> Thanks!
> 
> Sincerely,
> Tord
> 
> -----------------------------------------------------------------------
> Tord Sn?ll
> Avd. f v?xtekologi, Evolutionsbiologiskt centrum, Uppsala universitet
> Dept. of Plant Ecology, Evolutionary Biology Centre, Uppsala University
> Villav?gen 14
> SE-752 36 Uppsala, Sweden
> Tel: 018-471 28 82 (int +46 18 471 28 82) (work)
> Tel: 018-25 71 33 (int +46 18 25 71 33) (home)
> Fax: 018-55 34 19 (int +46 18 55 34 19) (work)
> E-mail: Tord.Snall at ebc.uu.se
> Check this: http://www.vaxtbio.uu.se/resfold/snall.htm!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From p.murrell at auckland.ac.nz  Sun Dec  7 22:56:08 2003
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Mon, 08 Dec 2003 10:56:08 +1300
Subject: [R] grid packages since R-1.7.1
References: <20031205031955.GR28127@hortresearch.co.nz>	<200312042323.12324.deepayan@stat.wisc.edu>
	<20031207210349.GT28127@hortresearch.co.nz>
Message-ID: <3FD3A1F8.1030802@stat.auckland.ac.nz>

Hi


Patrick Connolly wrote:
> On Thu, 04-Dec-2003 at 11:23PM -0600, Deepayan Sarkar wrote:
> 
> |> On Thursday 04 December 2003 21:19, Patrick Connolly wrote:
> 
> [...]
> 
> |> > options(defaultPackages =  c("mva", "lattice"), keep.source.pkgs = TRUE)
> |> >
> |> > It's rather unlikely that I should specifically load grid also, but
> |> 
> 
> |> That's exactly what you need to do (i.e., load grid explicitly).
> |> This has to do with namespaces, which makes some earlier notions
> |> obsolete. lattice 'imports' grid function definitions but by itself
> |> no longer makes them available/visible to the user.
> 
> 
> I understand from that that there is functionality in grid that
> lattice doesn't normally require and so isn't in what is 'imported' to
> lattice.  Evidently, all the time I used R-1.8.0, I was not requiring
> that functionality.


No.  lattice imports grid, which only *loads* grid so that lattice gets 
to see what grid exports;  grid is not *attached* to the search path so 
grid's exports are not visible from the command line.  If your code 
directly uses grid, you should require(grid) explicitly to place grid on 
the search path.  Luke Tierney's R News article 
(http://cran.r-project.org/doc/Rnews/Rnews_2003-1.pdf)
explains the difference between loading and attaching packages.

Paul


> |> This of course doesn't explain any problems in grid.polygon after loading 
> |> grid. grid.polygon has changed in 1.8.1, but I think it is supposed to be 
> |> back-compatible. Could you provide a reproducible example ? 
> 
> My hunch was completely off course -- nothing to do with scoping.  I
> tracked it down to a deficiency in the error messages in 1.7.1 which
> proceded to go on with what it could do without complaining about what
> it couldn't do (because of no data in some cases).  1.8.1 is more
> up-front and bawks when it can't do what was asked.  I simply added an
> if statement to avoid trying to do the impossible.
> 
> Thanks for helping me clear that up.
> 
> 


-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From tord.snall at ebc.uu.se  Sun Dec  7 23:04:12 2003
From: tord.snall at ebc.uu.se (Tord Snall)
Date: Sun, 07 Dec 2003 23:04:12 +0100
Subject: [R] par(las = 1) not possible in polymap(),
  library(splancs)?
In-Reply-To: <3FD3A0D1.444BFCEB@statistik.uni-dortmund.de>
References: <3.0.6.32.20031207223959.00d4fed8@mail.anst.uu.se>
Message-ID: <3.0.6.32.20031207230412.00d4fed8@mail.anst.uu.se>


>Use 
>  par(las = 1)
>  polymap(.....)
>
>Not all the arguments of par() do work as arguments in all highlevel
>functions ...

Dear Uwe Ligges,

Thanks very much! This was new to me.

Sincerely,
Tord


>
>Uwe Ligges
>
>
>> Mayby some function in library(spatstat) support las?
>> 
>> Thanks!
>> 
>> Sincerely,
>> Tord
>> 
>> -----------------------------------------------------------------------
>> Tord Sn?ll
>> Avd. f v?xtekologi, Evolutionsbiologiskt centrum, Uppsala universitet
>> Dept. of Plant Ecology, Evolutionary Biology Centre, Uppsala University
>> Villav?gen 14
>> SE-752 36 Uppsala, Sweden
>> Tel: 018-471 28 82 (int +46 18 471 28 82) (work)
>> Tel: 018-25 71 33 (int +46 18 25 71 33) (home)
>> Fax: 018-55 34 19 (int +46 18 55 34 19) (work)
>> E-mail: Tord.Snall at ebc.uu.se
>> Check this: http://www.vaxtbio.uu.se/resfold/snall.htm!
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>

-----------------------------------------------------------------------
Tord Sn?ll
Avd. f v?xtekologi, Evolutionsbiologiskt centrum, Uppsala universitet
Dept. of Plant Ecology, Evolutionary Biology Centre, Uppsala University
Villav?gen 14			
SE-752 36 Uppsala, Sweden
Tel: 018-471 28 82 (int +46 18 471 28 82) (work)
Tel: 018-25 71 33 (int +46 18 25 71 33) (home)
Fax: 018-55 34 19 (int +46 18 55 34 19) (work)
E-mail: Tord.Snall at ebc.uu.se
Check this: http://www.vaxtbio.uu.se/resfold/snall.htm!



From maj at stats.waikato.ac.nz  Mon Dec  8 02:51:32 2003
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Mon, 08 Dec 2003 14:51:32 +1300
Subject: [R] Character graphics
Message-ID: <3FD3D924.6050500@stats.waikato.ac.nz>

Does anyone else miss email-friendly character graphics such as the 
following example, produced using Minitab?

Histogram of C6   N = 478   N* = 21
Each * represents 2 observation(s)

Midpoint        Count
      -12           16  ********
      -11           53  ***************************
      -10           63  ********************************
       -9           83  ******************************************
       -8           93  ***********************************************
       -7           74  *************************************
       -6           45  ***********************
       -5           13  *******
       -4            6  ***
       -3            2  *
       -2           10  *****
       -1           18  *********
        0            1  *

BTW, Minitab 13 protests when you want to do this:

MTB > gstd
* NOTE  * Character graphs are obsolete.

* NOTE  * Standard Graphics are enabled.
           Professional Graphics are disabled.
           Use the GPRO command to enable Professional Graphics.

Has anyone tried to get similar unprofessional displays out of R?

Cheers,  Murray

-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862



From rossini at blindglobe.net  Mon Dec  8 03:04:30 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Sun, 07 Dec 2003 18:04:30 -0800
Subject: [R] Character graphics
In-Reply-To: <3FD3D924.6050500@stats.waikato.ac.nz> (Murray Jorgensen's
	message of "Mon, 08 Dec 2003 14:51:32 +1300")
References: <3FD3D924.6050500@stats.waikato.ac.nz>
Message-ID: <85llporro1.fsf@blindglobe.net>


Murray Jorgensen <maj at stats.waikato.ac.nz> writes:

> Does anyone else miss email-friendly character graphics such as the
> following example, produced using Minitab?

I don't.

> Histogram of C6   N = 478   N* = 21
> Each * represents 2 observation(s)
>
> Midpoint        Count
>       -12           16  ********
>       -11           53  ***************************
>       -10           63  ********************************
>        -9           83  ******************************************
>        -8           93  ***********************************************
>        -7           74  *************************************
>        -6           45  ***********************
>        -5           13  *******
>        -4            6  ***
>        -3            2  *
>        -2           10  *****
>        -1           18  *********
>         0            1  *
>
> BTW, Minitab 13 protests when you want to do this:
>
> MTB > gstd
> * NOTE  * Character graphs are obsolete.
>
> * NOTE  * Standard Graphics are enabled.
>            Professional Graphics are disabled.
>            Use the GPRO command to enable Professional Graphics.
>
> Has anyone tried to get similar unprofessional displays out of R?

A stem and leaf plot?  Here's one:

> stem(rnorm(10))

  The decimal point is at the |

  -1 | 2
  -0 | 
   0 | 2226
   1 | 22559

(it's in base).

best,
-tony

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From ggrothendieck at myway.com  Mon Dec  8 06:09:20 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon,  8 Dec 2003 00:09:20 -0500 (EST)
Subject: [R] Character graphics
Message-ID: <20031208050920.73FE5399E@mprdmxin.myway.com>


I think you would have to roll your own or make use
of a second package.  With the free Macanova package, 
try this:

In R,

   write( x, file = "/temp.dat" )   # x is vector, e.g. x <- 1:20

In MacAnova,

   x <- vecread( "/temp.dat" )
   hist( x, freq:T, dumb:T, new:T, file:"/hist.out" )
   quit(F)  # exit Macanova without prompting

If you are using the Windows version of Macanova you could even transfer
the data through the clipboard (replacing the write and read parts above with
write(x,file="clipboard") and vecread(string:CLIPBOARD) ).  You could even
spawn Macanoava from R (system("macanowx -batch /hist.mac")) if you place
the Macanova commands in a file, hist.mac.

Here is the hist.out file:


                    Frequency histogram of vecread(string:CLIPBOARD)
           +---------------+--------------+---------------+---------------+---+
          4+..................................................................+
           |.            .            .            .            .            .|
           |.            .            .            .            .            .|
        3.5+.            .            .            .            .            .+
           |.            .            .            .            .            .|
          3+.            .            .            .            .            .+
 F         |.            .            .            .            .            .|
 r         |.            .            .            .            .            .|
 e      2.5+.            .            .            .            .            .+
 q         |.            .            .            .            .            .|
 u        2+.            .            .            .            .            .+
 e         |.            .            .            .            .            .|
 n      1.5+.            .            .            .            .            .+
 c         |.            .            .            .            .            .|
 y         |.            .            .            .            .            .|
          1+.            .            .            .            .            .+
           |.            .            .            .            .            .|
        0.5+.            .            .            .            .            .+
           |.            .            .            .            .            .|
          0+..................................................................+
           +---------------+--------------+---------------+---------------+---+
           0               5             10              15              20
                                vecread(string:CLIPBOARD)



---
Date: Mon, 08 Dec 2003 14:51:32 +1300 
From: Murray Jorgensen <maj at stats.waikato.ac.nz>
To: R-help <r-help at stat.math.ethz.ch> 
Subject: [R] Character graphics 

 
 
Does anyone else miss email-friendly character graphics such as the 
following example, produced using Minitab?

Histogram of C6 N = 478 N* = 21
Each * represents 2 observation(s)

Midpoint Count
-12 16 ********
-11 53 ***************************
-10 63 ********************************
-9 83 ******************************************
-8 93 ***********************************************
-7 74 *************************************
-6 45 ***********************
-5 13 *******
-4 6 ***
-3 2 *
-2 10 *****
-1 18 *********
0 1 *

BTW, Minitab 13 protests when you want to do this:

MTB > gstd
* NOTE * Character graphs are obsolete.

* NOTE * Standard Graphics are enabled.
Professional Graphics are disabled.
Use the GPRO command to enable Professional Graphics.

Has anyone tried to get similar unprofessional displays out of R?

Cheers, Murray

-- 
Dr Murray Jorgensen http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz Fax 7 838 4155
Phone +64 7 838 4773 wk +64 7 849 6486 home Mobile 021 1395 862



From karlknoblich at yahoo.de  Mon Dec  8 11:33:35 2003
From: karlknoblich at yahoo.de (=?iso-8859-1?q?Karl=20Knoblick?=)
Date: Mon, 8 Dec 2003 11:33:35 +0100 (CET)
Subject: [R] Confidence intervals in ANOVA
Message-ID: <20031208103335.82612.qmail@web10005.mail.yahoo.com>


Hallo! 


I have the a model with 3 time points, 2 treatments 
and N subjects. I can calculate an ANOVA but I can not

calculate the CI of the interaction term (time and 
treatment), which I need for a closer look at the 
effect of the treatment to the 3 time points. I do NOT

want to use lme because I can not manage it to 
reproduce text book examples (see my posting [R] lme: 
reproducing example Karl Knoblick (Tue 02 Dec 2003 - 
21:34:54 EST)). 


Here some sample data: 


# Data 
# 35 subjects 
ID<-factor(rep(1:35,each=3)) 
TREAT<-factor(c(rep("A", 60), rep("B", 45))) 
TIME<-factor(rep(1:3, 35)) 
Y<-numeric(length=105) 
set.seed(1234) 
Y<-rnorm(105) 
# want to see an effect:
Y[TREAT=="A" & TIME==2]<-Y[TREAT=="A" & TIME==2] - 1
DF<-data.frame(Y, ID, TREAT, TIME) 


# 2 possible designs: 
# Design 1 with random term 
DF.aov1<-aov(Y ~ TIME*TREAT + Error(TREAT:ID), 
data=DF) 
summary(DF.aov1) 
# Design 2 without random term 
DF.aov2<-aov(Y ~ TIME*TREAT, data=DF) 
summary(DF.aov2) 


I am also not sure about the design - I think design 1

is more appropriate. 


What I have tried is to calculate the CI of the 
coefficients: 
confint(DF.aov1[[2]]) 
confint(DF.aov1[[3]]) 


(or: 
confint(DF.aov2) 
) 


But how can I get the CI for a concrete difference for

example between the treatments at time point 2? 


I really hope, sombody can help! 


Karl



From Pascal.Niklaus at unibas.ch  Mon Dec  8 11:47:02 2003
From: Pascal.Niklaus at unibas.ch (Pascal A. Niklaus)
Date: Mon, 08 Dec 2003 11:47:02 +0100
Subject: [R] Add row to data frame
Message-ID: <3FD456A6.6050906@unibas.ch>

Hi all,

is there an easy way to build up a data frame by sequentially adding 
individual rows? The data frame consists of numeric and character 
columns. I thought of rbind, but I ended up with numeric values for the 
character columns.

Pascal



From anne.piotet at m-td.com  Mon Dec  8 11:50:44 2003
From: anne.piotet at m-td.com (Anne Piotet)
Date: Mon, 8 Dec 2003 11:50:44 +0100
Subject: [R] graphical parametres...
Message-ID: <002901c3bd79$22346f20$6c00a8c0@mtd4>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031208/0dca48a7/attachment.pl

From maechler at stat.math.ethz.ch  Mon Dec  8 12:09:15 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 8 Dec 2003 12:09:15 +0100
Subject: [R] Add row to data frame
In-Reply-To: <3FD456A6.6050906@unibas.ch>
References: <3FD456A6.6050906@unibas.ch>
Message-ID: <16340.23515.14859.742758@gargle.gargle.HOWL>

>>>>> "Pascal" == Pascal A Niklaus <Pascal.Niklaus at unibas.ch>
>>>>>     on Mon, 08 Dec 2003 11:47:02 +0100 writes:

    Pascal> Hi all, is there an easy way to build up a data
    Pascal> frame by sequentially adding individual rows? 

yes, pretty easy, but usually not recommended because quite
inefficient.

rbind() does work with data frames in the cases we know.
Have a look at help(rbind.data.frame)


    Pascal> The data frame consists of numeric and character
    Pascal> columns. I thought of rbind, but I ended up with
    Pascal> numeric values for the character columns.

We'd need to see [i.e give a reproducible example!]
how you "ended up with numeric values for the  character
columns" -- which I guess were *factor* instead of character ?

Regards to Basel,
Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From han at math.sc.edu  Sun Dec  7 10:27:23 2003
From: han at math.sc.edu (Jun Han)
Date: Sun, 7 Dec 2003 04:27:23 -0500
Subject: [R] help
Message-ID: <000801c3bca4$528ee220$2f0ffc81@randomwalk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031207/9afe4ee6/attachment.pl

From so13839 at alltel.net  Sun Dec  7 03:56:25 2003
From: so13839 at alltel.net (Stephen Opiyo)
Date: Sat, 06 Dec 2003 20:56:25 -0600
Subject: [R] Help
Message-ID: <3FD296D9.6060003@alltel.net>

Hi,

I have a data set (data frame) approx. 50 rows * 600 columns. The 
columns are separated by commas. I would like to know how to remove 
those commas  between the columns. What should I do to remove those 
commas? Secondly, if I want only to use part of the (data frame), say  
(50 rows * 300 columns) instead of (50 rows * 600 columns), what should 
I do?

Thanks,

Stephen



From eric.esposito at gazdefrance.com  Mon Dec  8 12:18:13 2003
From: eric.esposito at gazdefrance.com (Eric ESPOSITO)
Date: Mon, 8 Dec 2003 12:18:13 +0100
Subject: [R] tkrplot with grid lattice plots
Message-ID: <OFF9AC0266.736B680D-ON41256DF6.003D9444@notes.edfgdf.fr>

Hello,
i tried to use a lattice graphics with tkrplot it seems that it doen't
work, here is the exemple used:

> library(tkrplot)
> tt <- tktoplevel()
> tktitle(tt)<-"Exemple"
>
> randdata<-data.frame(x=rnorm(100), y=rnorm(100), idobs=rep(1:10,
each=10))
>
> plot.graph<-function() {
+ plot(randdata$x, randdata$y)
+ }
>
> plot.graph2<-function() {
+ print(xyplot(x ~ y | idobs, data = randdata, as.table=F, type="p",
panel=panel.xyplot, main="", lty=1))
+ }
>
> img <-tkrplot(tt, fun=plot.graph)
> tkgrid(img)
<Tcl>
> img2 <-tkrplot(tt, fun=plot.graph2)
Error in check.length(gparname) : gpar element fontsize must not be length
0


I had a look at the tkrplot function but can't understand what is wrong.
Thanks for any help!

Eric Esposito



From krcabrer at epm.net.co  Mon Dec  8 13:08:44 2003
From: krcabrer at epm.net.co (Kenneth Cabrera)
Date: Mon, 08 Dec 2003 07:08:44 -0500
Subject: [R] Reading formated databases
Message-ID: <3FD469CC.9030508@epm.net.co>

Hi R-users:

How can I read an ascii database that is controled by the column number?
For example:

7349593Luis Miguel      Ariza Gutierrez           32342123
9394583XXXXX XX         YYYYYYYYYYYY              34234930
39483  KKKKKKKK KKKKKKKKCCCCCCC CCCCCCCCCCCCCCCCCC39203230
3484932YYYYYYYYYY YYYYYYZZZZZZ ZZZZZZZZZZZZZZZ    39402343
  39203WWWWWWWWWWWW  WWWVVVVVVVVVVVV VVVVVVVVVVVVV  342343

There are 4 variables, ID, Name, Last Name, Numeric Variable.
1 Variable column 1-8
2 Variable column 9-25
3 Variable column 26-51
4 Variable column 51-59

Thank you for your help

Kenneth

-- 
Kenneth Roy Cabrera Torres
Celular +57 (315) 405 9339



From ccleland at optonline.net  Mon Dec  8 12:58:58 2003
From: ccleland at optonline.net (Chuck Cleland)
Date: Mon, 08 Dec 2003 06:58:58 -0500
Subject: [R] Reading formated databases
In-Reply-To: <3FD469CC.9030508@epm.net.co>
References: <3FD469CC.9030508@epm.net.co>
Message-ID: <3FD46782.8090008@optonline.net>

Kenneth Cabrera wrote:
> How can I read an ascii database that is controled by the column number?

?read.fwf

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From ligges at statistik.uni-dortmund.de  Mon Dec  8 13:03:14 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 08 Dec 2003 13:03:14 +0100
Subject: [R] graphical parametres...
In-Reply-To: <002901c3bd79$22346f20$6c00a8c0@mtd4>
References: <002901c3bd79$22346f20$6c00a8c0@mtd4>
Message-ID: <3FD46882.7030206@statistik.uni-dortmund.de>

Anne Piotet wrote:

> Hi...it is probably trivial, but I do not know how to do the following:
> I want the output of a xy plot to be plotted in different colors according to a given condition ...
> I want to plot temperature dependency of flow stress ; for some (rare) occurencies I've got a special condition (chemical composition change) , and I want these points to appear on the same graph...I do know how to change the appearance of all the plots with the par() command or the points()...
> 
> Thanks 
> Anne
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

I guess you are looking for something like:

  y <- sample(1:10)
  plot(1:10, y, col = ifelse(y > 5, "red", "black"))

Uwe Ligges



From ligges at statistik.uni-dortmund.de  Mon Dec  8 13:05:52 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 08 Dec 2003 13:05:52 +0100
Subject: [R] Help
In-Reply-To: <3FD296D9.6060003@alltel.net>
References: <3FD296D9.6060003@alltel.net>
Message-ID: <3FD46920.9000900@statistik.uni-dortmund.de>

Stephen Opiyo wrote:

> Hi,
> 
> I have a data set (data frame) approx. 50 rows * 600 columns. The 
> columns are separated by commas. I would like to know how to remove 
> those commas  between the columns. What should I do to remove those 
> commas? 


I guess during the import of the data?
Please read the R Data Import/Export manual and the help page 
?read.table. It tells you how to use the argument "sep".


> Secondly, if I want only to use part of the (data frame), say  
> (50 rows * 300 columns) instead of (50 rows * 600 columns), what should 
> I do?

Index the data frame appropriately.

See the manual "An Introduction to R". Also, the help page ?data.frame 
points you to subsetting methods in its "See Also" Section.

Uwe Ligges


> Thanks,
> 
> Stephen
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ligges at statistik.uni-dortmund.de  Mon Dec  8 13:06:36 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 08 Dec 2003 13:06:36 +0100
Subject: [R] Reading formated databases
In-Reply-To: <3FD469CC.9030508@epm.net.co>
References: <3FD469CC.9030508@epm.net.co>
Message-ID: <3FD4694C.6030702@statistik.uni-dortmund.de>

Kenneth Cabrera wrote:

> Hi R-users:
> 
> How can I read an ascii database that is controled by the column number?
> For example:
> 
> 7349593Luis Miguel      Ariza Gutierrez           32342123
> 9394583XXXXX XX         YYYYYYYYYYYY              34234930
> 39483  KKKKKKKK KKKKKKKKCCCCCCC CCCCCCCCCCCCCCCCCC39203230
> 3484932YYYYYYYYYY YYYYYYZZZZZZ ZZZZZZZZZZZZZZZ    39402343
>  39203WWWWWWWWWWWW  WWWVVVVVVVVVVVV VVVVVVVVVVVVV  342343
> 
> There are 4 variables, ID, Name, Last Name, Numeric Variable.
> 1 Variable column 1-8
> 2 Variable column 9-25
> 3 Variable column 26-51
> 4 Variable column 51-59
> 
> Thank you for your help
> 
> Kenneth
> 

See ?read.fwf

Uwe Ligges



From pallier at lscp.ehess.fr  Mon Dec  8 13:17:06 2003
From: pallier at lscp.ehess.fr (Christophe Pallier)
Date: Mon, 08 Dec 2003 13:17:06 +0100
Subject: [R] aggregate and names of factors 
Message-ID: <3FD46BC2.3090301@lscp.ehess.fr>

Hello,

I use the function 'aggregate' a lot.

One small annoyance is that it is necessary to name the factors in the
'by' list to get the names in the resulting data.frame (else, they
appear as Group.1, Group.2...etc). For example, I am forced to
write:

aggregate(y,list(f1=f1,f2=f2),mean)

instead of aggregate(y,list(f1,f2),mean)

(for two factors with short names, it is not such a big deal, but I
ususally have about 8 factors with long names...)

I wrote a modified 'aggregate.data.frame' function (see the code
below) so that it parses the names of the factors and uses them in the 
output
data.frame. I can now typer aggregate(y,list(f1,f2),mean) ans the 
resulting data.frame
has variables with names 'f1' and 'f2'.

However, I have a few questions:

1. Is is a good idea at all? When expressions rather than variables are
   used as factors, this will probably result in a mess. Can one test
   if an argument within a list, is just a variable name or a more
   complex expression?). Is there a better way?

2. I would also like to keep the name of the data when it is a
   vector, and not a data.frame. The current version transforms it into 'x'.
   I have not managed to modify this behavior, so I am forced to use
    aggregate(data.frame(y),list(f1,f2),mean)

3. I would love to have yet another a version that handles formula so
   that I could type:

   aggregate(y~f1*f2)

   I have a provisory version (see below), but it does not work very
   well.  I would be grateful for any suggestions. In particular, I
   would love to have a 'subset' parameter, as in the lm
   function)

Here is the small piece of code fot the embryo of aggregate.formula:

my.aggregate.formula = function(formula,FUN=mean) {
{
    d=model.frame(formula)

    factor.names=lapply(names(d)[sapply(d,is.factor)],as.name)
    factor.list=lapply(factor.names,eval)
    names(factor.list)=factor.names
    aggregate(d[1],factor.list,FUN)
}



Christophe Pallier
http://www.pallier.org

---------------

HEre is the code for aggregate.data.frame that recovers the name sof the 
factors:

my.aggregate.data.frame <- function (x, by, FUN, ...)
{
 
   if (!is.data.frame(x)) {
        x <- as.data.frame(x)
      }
        
    if (!is.list(by))
        stop("`by' must be a list")

    if (is.null(names(by))) {
      #  names(by) <- paste("Group", seq(along = by), sep = ".")
        names(by)=lapply(substitute(by)[-1],deparse)
    }
    else {
        nam <- names(by)
        ind <- which(nchar(nam) == 0)
        if (any(ind)) {
          names(by)[ind] <- lapply(substitute(by)[c(-1,-(ind))],deparse)
        }
    }
    y <- lapply(x, tapply, by, FUN, ..., simplify = FALSE)
    if (any(sapply(unlist(y, recursive = FALSE), length) > 1))
        stop("`FUN' must always return a scalar")
    z <- y[[1]]
    d <- dim(z)
    w <- NULL
    for (i in seq(along = d)) {
        j <- rep(rep(seq(1:d[i]), prod(d[seq(length = i - 1)]) *
            rep(1, d[i])), prod(d[seq(from = i + 1, length = length(d) -
            i)]))
        w <- cbind(w, dimnames(z)[[i]][j])
    }
    w <- w[which(!unlist(lapply(z, is.null))), ]
    y <- data.frame(w, lapply(y, unlist, use.names = FALSE))
    names(y) <- c(names(by), names(x))
    y
}



From yukangtu at hotmail.com  Mon Dec  8 13:48:57 2003
From: yukangtu at hotmail.com (Tu Yu-Kang)
Date: Mon, 08 Dec 2003 12:48:57 +0000
Subject: [R] extracting p value from GEE
Message-ID: <Law15-F77JUwby8WYBt00017edc@hotmail.com>


Hi,

Many thanks for your kind help.

best regards,

Yu-Kang

>From: Emmanuel Paradis <paradis at isem.univ-montp2.fr>
>To: "Tu Yu-Kang" <yukangtu at hotmail.com>
>CC: r-help at stat.math.ethz.ch
>Subject: Re: [R] extracting p value from GEE
>Date: Fri, 05 Dec 2003 15:48:41 +0100
>
>At 11:53 04/12/2003 +0000, vous avez crit:
>>Dear R users,
>>
>>If anyone can tell me how to extract the p values from the output 
>>of gee?
>
>They are easily computed from the output of summary(gee(...)) which 
>prints either a "z" or a "t" depending in the "family" option. z 
>follows, under the null hypothesis, a normal distribution N(0, 1), 
>you have the corresponding P-value with (for a two-tailed test):
>
>2 * (1 - pnorm(abs(z)))
>
>t follows a 'Student' distribution with df degrees of freedom given 
>by N- k - 1, where N is the number of observations, and k is the 
>number of estimated paramaters. I think, but am not definitely sure, 
>that N is counted among all clusters, and k is the number of 
>parameters in the GLM eventually included the estimated scale 
>(correlation parameters are not counted). As above, you have the 
>P-value with:
>
>2 * (1 - pnorm(abs(t), df))
>
>HTH
>
>Emmanuel Paradis
>
>
>>Many thanks in advance.
>>
>>Yu-Kang
>>
>>_________________________________________________________________
>>KO MSN ^yGMuHvuW^ 

>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>

_________________________________________________________________
}} MSN eShopGKQBuaAHufPPz
I http://msn.com.tw/eshop



From p.dalgaard at biostat.ku.dk  Mon Dec  8 14:13:59 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 08 Dec 2003 14:13:59 +0100
Subject: [R] aggregate and names of factors
In-Reply-To: <3FD46BC2.3090301@lscp.ehess.fr>
References: <3FD46BC2.3090301@lscp.ehess.fr>
Message-ID: <x265gr78q0.fsf@biostat.ku.dk>

Christophe Pallier <pallier at lscp.ehess.fr> writes:

> Hello,
> 
> I use the function 'aggregate' a lot.
> 
> One small annoyance is that it is necessary to name the factors in the
> 'by' list to get the names in the resulting data.frame (else, they
> appear as Group.1, Group.2...etc). For example, I am forced to
> write:
> 
> aggregate(y,list(f1=f1,f2=f2),mean)
> 
> instead of aggregate(y,list(f1,f2),mean)
> 
> (for two factors with short names, it is not such a big deal, but I
> ususally have about 8 factors with long names...)
> 
> I wrote a modified 'aggregate.data.frame' function (see the code
> below) so that it parses the names of the factors and uses them in the
> output
> data.frame. I can now typer aggregate(y,list(f1,f2),mean) ans the
> resulting data.frame
> has variables with names 'f1' and 'f2'.
> 
> However, I have a few questions:
> 
> 1. Is is a good idea at all? When expressions rather than variables are
>    used as factors, this will probably result in a mess. Can one test
>    if an argument within a list, is just a variable name or a more
>    complex expression?). Is there a better way?

This issue is not just relevant for aggregate. There are a couple of
other places where you want a named list to get names on output -
lapply(list(foo,bar,baz) function(x) lm(x~age)), say. One option that
I've been toying around with is to clone the code from data.frame and
have a function namedList() or nlist() which automagically supplies
names by deparsing the call. Now where did I put that code sketch...

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From wantia at ifi.unizh.ch  Mon Dec  8 14:58:56 2003
From: wantia at ifi.unizh.ch (Jan Wantia)
Date: 8 Dec 2003 14:58:56 +0100
Subject: [R] thanks!! get mean of several rows
In-Reply-To: <6.0.1.1.2.20031204143216.0212bf88@stat4ux.stat.ucl.ac.be>
References: <3FCF30BD.3040302@ifi.unizh.ch>
	<6.0.1.1.2.20031204143216.0212bf88@stat4ux.stat.ucl.ac.be>
Message-ID: <3FD483A0.2000708@ifi.unizh.ch>

Dear all!

Thanks to all who replied to my question on getting the means of several 
rows, and the one with the standard error + mean-plot!
Many of them worked fine just as they were, others had to be adapted a bit.
However, I can finally do my calculations, and find myself happy as a 
man could be, plotting fancy graphs whole day long.
Moreover, I think I have learned quite a bit on R, seeing so many ways 
to do the same thing.
Thanks a lot, again, I was really at the edge of going home and get drunk!

Cheers, Jan

-- 

______________________________________________________

Jan Wantia
Dept. of Information Technology, University of Z?rich
Andreasstr. 15
CH 8050 Z?rich
Switzerland

Tel.:     +41 (0) 1 635 4315
Fax:     +41 (0) 1 635 45 07
email: wantia at ifi.unizh.ch



From Pascal.Niklaus at unibas.ch  Mon Dec  8 14:59:32 2003
From: Pascal.Niklaus at unibas.ch (Pascal A. Niklaus)
Date: Mon, 08 Dec 2003 14:59:32 +0100
Subject: [R] Add row to data frame
In-Reply-To: <16340.23515.14859.742758@gargle.gargle.HOWL>
References: <3FD456A6.6050906@unibas.ch>
	<16340.23515.14859.742758@gargle.gargle.HOWL>
Message-ID: <3FD483C4.7040602@unibas.ch>

Martin Maechler wrote:

>>>>>>"Pascal" == Pascal A Niklaus <Pascal.Niklaus at unibas.ch>
>>>>>>    on Mon, 08 Dec 2003 11:47:02 +0100 writes:
>>>>>>            
>>>>>>
>
>    Pascal> Hi all, is there an easy way to build up a data
>    Pascal> frame by sequentially adding individual rows? 
>
>yes, pretty easy, but usually not recommended because quite
>inefficient.
>
>rbind() does work with data frames in the cases we know.
>Have a look at help(rbind.data.frame)
>
>
>    Pascal> The data frame consists of numeric and character
>    Pascal> columns. I thought of rbind, but I ended up with
>    Pascal> numeric values for the character columns.
>
>We'd need to see [i.e give a reproducible example!]
>how you "ended up with numeric values for the  character
>columns" -- which I guess were *factor* instead of character ?
>  
>
Yes, there was a factor...  There's one problem left, though... The row 
names are 1, 11, 111 etc, instead of 1,2,3...

 > df <- NULL;
 >
 > df <- rbind(df,data.frame(A=1,B="abc",C=rnorm(1)))
 > df <- rbind(df,data.frame(A=1,B="abc",C=rnorm(1)))
 > df <- rbind(df,data.frame(A=1,B="abc",C=rnorm(1)))
 > df
    A   B          C
1   1 abc  1.3540030
11  1 abc -0.7229597
111 1 abc -0.4922653

Of course, I can do a  attr(df,"row.names") <- 1:3 at the end, but is 
there an easier way? Ideally, I would like to name the row already when 
adding it to the data frame. Is there another way than setting the 
row.names attribute "manually" with the attr command?

Pascal



From ripley at stats.ox.ac.uk  Mon Dec  8 15:33:14 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 8 Dec 2003 14:33:14 +0000 (GMT)
Subject: [R] Add row to data frame
In-Reply-To: <3FD483C4.7040602@unibas.ch>
Message-ID: <Pine.LNX.4.44.0312081430570.19668-100000@gannet.stats>

On Mon, 8 Dec 2003, Pascal A. Niklaus wrote:

> Martin Maechler wrote:
> 
> >>>>>>"Pascal" == Pascal A Niklaus <Pascal.Niklaus at unibas.ch>
> >>>>>>    on Mon, 08 Dec 2003 11:47:02 +0100 writes:
> >>>>>>            
> >>>>>>
> >
> >    Pascal> Hi all, is there an easy way to build up a data
> >    Pascal> frame by sequentially adding individual rows? 
> >
> >yes, pretty easy, but usually not recommended because quite
> >inefficient.
> >
> >rbind() does work with data frames in the cases we know.
> >Have a look at help(rbind.data.frame)
> >
> >
> >    Pascal> The data frame consists of numeric and character
> >    Pascal> columns. I thought of rbind, but I ended up with
> >    Pascal> numeric values for the character columns.
> >
> >We'd need to see [i.e give a reproducible example!]
> >how you "ended up with numeric values for the  character
> >columns" -- which I guess were *factor* instead of character ?
> >  
> >
> Yes, there was a factor...  There's one problem left, though... The row 
> names are 1, 11, 111 etc, instead of 1,2,3...
> 
>  > df <- NULL;
>  >
>  > df <- rbind(df,data.frame(A=1,B="abc",C=rnorm(1)))
>  > df <- rbind(df,data.frame(A=1,B="abc",C=rnorm(1)))
>  > df <- rbind(df,data.frame(A=1,B="abc",C=rnorm(1)))
>  > df
>     A   B          C
> 1   1 abc  1.3540030
> 11  1 abc -0.7229597
> 111 1 abc -0.4922653
> 
> Of course, I can do a  attr(df,"row.names") <- 1:3 at the end, but is 
> there an easier way? Ideally, I would like to name the row already when 
> adding it to the data frame.

Easy!  Create the data frame with the row names you want -- see 
?data.frame.

> Is there another way than setting the 
> row.names attribute "manually" with the attr command?

What do you think row.names<- does?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andrewr at uidaho.edu  Mon Dec  8 16:51:21 2003
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Mon, 8 Dec 2003 07:51:21 -0800
Subject: [R] TukeyHSD changes if I create interaction term
Message-ID: <200312080751.21462.andrewr@uidaho.edu>

Dear R community,

I'm trying to understand this behavior of TukeyHSD.  My goal is to obtain 
defensible, labelled multiple comparisons of an interaction term.

Firstly, if I plot the TukeyHSD from the model that calculates its own 
interactions, then the y-axis labels appear to be reflected on their median 
when compared to the text output of the TukeyHSD statement.  The labels are 
integers.

Secondly, if I provide an interaction term for the model, to try to coerce 
TukeyHSD to label the comparisons, then the multiple comparison outcome is 
quite different, as is the output from coefficients().  It must be using a 
different parameterization, because the anova statements that summarize the 
model are identical.  

However, if two different parameterizations give rise to two different sets of 
multiple comparisons, how ought we choose between them?

The following snippet illustrates.   

=======================================================================

data(warpbreaks)
warpbreaks$WT <- interaction(warpbreaks$wool, warpbreaks$tension)

summary(fm1 <- aov(breaks ~ wool * tension, data = warpbreaks))
summary(fm2 <- aov(breaks ~ wool + tension + WT, data = warpbreaks))

summary(fm1) #  Identical
summary(fm2) #  Identical

coefficients(fm1) #  Different
coefficients(fm2) #  Different

TukeyHSD(fm1, "wool:tension") 

# (1) My first concern is that the y-axis labels seem to be reflected
#     in their median. 

plot(TukeyHSD(fm1, "wool:tension")) # Labels are upside down?

# (2) My second concern is that the estimates and intervals are 
#     different from fm2

TukeyHSD(fm2, "WT") 

=======================================================================

I would appreciate any advice.  

Andrew
-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From Alexandre.Irrthum at icr.ac.uk  Mon Dec  8 16:51:42 2003
From: Alexandre.Irrthum at icr.ac.uk (Alexandre Irrthum)
Date: Mon, 08 Dec 2003 15:51:42 +0000
Subject: [R] stripchart problem
Message-ID: <sfd49e2a.098@icr.ac.uk>

Hello,

I am trying to plot age distribution data for a certain condition that
runs in families. Below is a simplified view of the dataset, i.e. in
this case there are four families, each line corresponding to one
individual with age at diagnosis and sex.

> famdata
   family age sex
1    fam1 2.1   f
2    fam1 2.3   f
3    fam1 1.0   m
4    fam2 7.3   f
5    fam2 4.1   f
6    fam2 1.2   f
7    fam2 0.6   m
8    fam3 3.5   m
9    fam3 2.5   m
10   fam3 2.9   m
11   fam3 5.6   m
12   fam3 4.4   f
13  fam10 1.1   f
14  fam10 1.2   f
15  fam10 2.9   f
16  fam10 2.2   f
17  fam10 4.7   m

I can nicely plot the age distribution by families with

> stripchart(famdata$age~famdata$family)

I would like to plot datapoints according to the sex of the person, e.g.
circle for a girl and square for a boy, like this:

> stripchart(famdata$age~famdata$family, pch=ifelse(famdata$sex=="m",
22, 1))

But this command doesn't work as I expected. Datapoints from fam2 are
shown as squares, all the rest as circles. Still , this seems OK:

> ifelse(famdata$sex=="m", 22, 1)
 [1]  1  1 22  1  1  1 22 22 22 22 22  1  1  1  1  1 22

Any clues ?

Thanks a lot for your help,

alex



From fs at fs-analyse.dk  Mon Dec  8 17:56:18 2003
From: fs at fs-analyse.dk (Finn Sando)
Date: Mon, 08 Dec 2003 17:56:18 +0100
Subject: [R] WinMenus - is there a way of knowing if a WinMenu or
	WinMenuItem already exists?
Message-ID: <3FD4BB42.22530.75ECE0@localhost>

I am developing a menusystem using the functions WinMenuAdd and 
WinMenuAddItem etc. 

I want to be able to shift between different interfaces (ie. different sets of menu-trees).
Therefore I would like to be able to ask whether a specific menu already exists in order 
to remove or add it Without errors. That is I am looking for something like:

WinMenuExist(menuname) and WinMenuExistItem(menuname,itemname)

Are there any way asking such question of R?
--
Finn



From ripley at stats.ox.ac.uk  Mon Dec  8 18:05:06 2003
From: ripley at stats.ox.ac.uk (Prof Brian D Ripley)
Date: Mon, 8 Dec 2003 17:05:06 +0000 (GMT Standard Time)
Subject: [R] WinMenus - is there a way of knowing if a WinMenu or
	WinMenuItem already exists?
In-Reply-To: <3FD4BB42.22530.75ECE0@localhost>
Message-ID: <Pine.WNT.4.44.0312081704240.2792-100000@petrel>

On Mon, 8 Dec 2003, Finn Sando wrote:

> I am developing a menusystem using the functions WinMenuAdd and
> WinMenuAddItem etc.
>
> I want to be able to shift between different interfaces (ie. different sets of menu-trees).
> Therefore I would like to be able to ask whether a specific menu already exists in order
> to remove or add it Without errors. That is I am looking for something like:
>
> WinMenuExist(menuname) and WinMenuExistItem(menuname,itemname)
>
> Are there any way asking such question of R?

No.  But you could contribute such functionality.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From paradis at univ-montp2.fr  Mon Dec  8 17:54:29 2003
From: paradis at univ-montp2.fr (paradis@univ-montp2.fr)
Date: Mon, 8 Dec 2003 17:54:29 +0100
Subject: [R] stripchart problem
References: <sfd49e2a.098@icr.ac.uk>
Message-ID: <3fd4acc5a93d28.31981641@univ-montp2.fr>

> Hello,
> 
> I am trying to plot age distribution data for a certain condition that
> runs in families. Below is a simplified view of the dataset, i.e. in
> this case there are four families, each line corresponding to one
> individual with age at diagnosis and sex.
> 
> > famdata
>    family age sex
> 1    fam1 2.1   f
> 2    fam1 2.3   f
> 3    fam1 1.0   m
> 4    fam2 7.3   f
> 5    fam2 4.1   f
> 6    fam2 1.2   f
> 7    fam2 0.6   m
> 8    fam3 3.5   m
> 9    fam3 2.5   m
> 10   fam3 2.9   m
> 11   fam3 5.6   m
> 12   fam3 4.4   f
> 13  fam10 1.1   f
> 14  fam10 1.2   f
> 15  fam10 2.9   f
> 16  fam10 2.2   f
> 17  fam10 4.7   m
> 
> I can nicely plot the age distribution by families with
> 
> > stripchart(famdata$age~famdata$family)
> 
> I would like to plot datapoints according to the sex of the person, e.g.
> circle for a girl and square for a boy, like this:
> 
> > stripchart(famdata$age~famdata$family, pch=ifelse(famdata$sex=="m",
> 22, 1))

Try this:

stripchart(famdata$age ~ famdata$family, pch = c(1, 22)[unclass(famdata$sex)])

(maybe you need to have "c(22, 1)")

HTH

Emmanuel Paradis


> 
> But this command doesn't work as I expected. Datapoints from fam2 are
> shown as squares, all the rest as circles. Still , this seems OK:
> 
> > ifelse(famdata$sex=="m", 22, 1)
>  [1]  1  1 22  1  1  1 22 22 22 22 22  1  1  1  1  1 22
> 
> Any clues ?
> 
> Thanks a lot for your help,
> 
> alex
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From dgrove at fhcrc.org  Mon Dec  8 18:51:12 2003
From: dgrove at fhcrc.org (Douglas Grove)
Date: Mon, 8 Dec 2003 09:51:12 -0800 (PST)
Subject: [R] Windows Memory Issues
In-Reply-To: <Pine.LNX.4.44.0312061854400.29769-100000@gannet.stats>
Message-ID: <Pine.LNX.4.44.0312080921260.27288-100000@echidna.fhcrc.org>

On Sat, 6 Dec 2003, Prof Brian Ripley wrote:

> I think you misunderstand how R uses memory.  gc() does not free up all 
> the memory used for the objects it frees, and repeated calls will free 
> more.  Don't speculate about how memory management works: do your 
> homework!

Are you saying that consecutive calls to gc() will free more memory than
a single call, or am I misunderstanding?   Reading ?gc and ?Memory I don't
see anything about this mentioned.  Where should I be looking to find 
more comprehensive info on R's memory management??  I'm not writing any
packages, just would like to have a better handle on efficiently using
memory as it is usually the limiting factor with R.  FYI, I'm running
R1.8.1 and RedHat9 on a P4 with 2GB of RAM in case there is any platform
specific info that may be applicable.

Thanks,

Doug Grove
Statistical Research Associate
Fred Hutchinson Cancer Research Center




 
> In any case, you are using an outdated version of R, and your first
> course of action should be to compile up R-devel and try that, as there 
> has been improvements to memory management under Windows.  You could also 
> try compiling using the native malloc (and that *is* described in the 
> INSTALL file) as that has different compromises.
> 
> 
> On Sat, 6 Dec 2003, Richard Pugh wrote:
> 
> > Hi all,
> >  
> > I am currently building an application based on R 1.7.1 (+ compiled
> > C/C++ code + MySql + VB).  I am building this application to work on 2
> > different platforms (Windows XP Professional (500mb memory) and Windows
> > NT 4.0 with service pack 6 (1gb memory)).  This is a very memory
> > intensive application performing sophisticated operations on "large"
> > matrices (typically 5000x1500 matrices).
> >  
> > I have run into some issues regarding the way R handles its memory,
> > especially on NT.  In particular, R does not seem able to recollect some
> > of the memory used following the creation and manipulation of large data
> > objects.  For example, I have a function which receives a (large)
> > numeric matrix, matches against more data (maybe imported from MySql)
> > and returns a large list structure for further analysis.  A typical call
> > may look like this .
> >  
> > > myInputData <- matrix(sample(1:100, 7500000, T), nrow=5000)
> > > myPortfolio <- createPortfolio(myInputData)
> >  
> > It seems I can only repeat this code process 2/3 times before I have to
> > restart R (to get the memory back).  I use the same object names
> > (myInputData and myPortfolio) each time, so I am not create more large
> > objects ..
> >  
> > I think the problems I have are illustrated with the following example
> > from a small R session .
> >  
> > > # Memory usage for Rui process = 19,800
> > > testData <- matrix(rnorm(10000000), 1000) # Create big matrix
> > > # Memory usage for Rgui process = 254,550k
> > > rm(testData)
> > > # Memory usage for Rgui process = 254,550k
> > > gc()
> >          used (Mb) gc trigger  (Mb)
> > Ncells 369277  9.9     667722  17.9
> > Vcells  87650  0.7   24286664 185.3
> > > # Memory usage for Rgui process = 20,200k
> >  
> > In the above code, R cannot recollect all memory used, so the memory
> > usage increases from 19.8k to 20.2.  However, the following example is
> > more typical of the environments I use .
> >  
> > > # Memory 128,100k
> > > myTestData <- matrix(rnorm(10000000), 1000)
> > > # Memory 357,272k
> > > rm(myTestData)
> > > # Memory 357,272k
> > > gc()
> >           used (Mb) gc trigger  (Mb)
> > Ncells  478197 12.8     818163  21.9
> > Vcells 9309525 71.1   31670210 241.7
> > > # Memory 279,152k
> >  
> > Here, the memory usage increases from 128.1k to 279.1k
> >  
> > Could anyone point out what I could do to rectify this (if anything), or
> > generally what strategy I could take to improve this?
> >  
> > Many thanks,
> > Rich.
> >  
> > Mango Solutions
> > Tel : (01628) 418134
> > Mob : (07967) 808091
> >  
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > 
> > 
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From VINCENT.STOLIAROFF at sgam.com  Mon Dec  8 18:59:29 2003
From: VINCENT.STOLIAROFF at sgam.com (STOLIAROFF VINCENT)
Date: Mon, 8 Dec 2003 18:59:29 +0100 
Subject: [R] test for arima coef's significancy
Message-ID: <F8F9E8240570AB4E86DC49B64FDCA2C40101BE1F@FR-MAILBOX1.fr.sgam.socgen>

Dear sirs, 

I would like to know if there is a function to compute the pvalue for the
significancy of arima coef in an arima object created by 
the arima function.

I have written this one:

pvalueArima<-function(x,arima)
{
t<-(arima$coef)/(diag(arima$var.coef)^0.5)
df<-length(x)-length(arima$coef)
1-pt(t,df)
}

Has somebody already implemented something equivalent ?

thank you for your help and comments

Vincent S.


*************************************************************************
Ce message et toutes les pieces jointes (ci-apres le "message") sont
confidentiels et etablis a l'intention exclusive de ses destinataires.
Toute utilisation ou diffusion non autorisee est interdite. 
Tout message electronique est susceptible d'alteration. 
SG Asset Management et ses filiales declinent toute responsabilite au titre
de ce message s'il a ete altere, deforme ou falsifie.

D?couvrez l'offre et les services de SG Asset Management sur le site
www.sgam.fr 

				********

This message and any attachments (the "message") are confide...{{dropped}}



From clayton.springer at pharma.novartis.com  Mon Dec  8 19:48:11 2003
From: clayton.springer at pharma.novartis.com (clayton.springer@pharma.novartis.com)
Date: Mon, 8 Dec 2003 13:48:11 -0500
Subject: [R] trouble with predict.l1ce
Message-ID: <OFC9DE508A.503CD7AC-ON85256DF6.005B53E9-85256DF6.0067777C@EU.novartis.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031208/de0e451b/attachment.pl

From flom at ndri.org  Mon Dec  8 20:02:15 2003
From: flom at ndri.org (Peter Flom)
Date: Mon, 08 Dec 2003 14:02:15 -0500
Subject: [R] Recoding problem
Message-ID: <sfd48488.028@MAIL.NDRI.ORG>

Hello

I have the following variables, all of which are logicals

fmar15   fcoc15   fher15    fcrk15    fidu15

what I would like is a variable drug15 which equals 
idu if fidu15 is T; crk if fidu15 is F but fcrk is T, her if fher15 is
T but fcrk15 and fidu15 are F and so on

What's the best way to do this?  

Thanks in advance

Peter L. Flom, PhD
Assistant Director, Statistics and Data Analysis Core
Center for Drug Use and HIV Research
National Development and Research Institutes
71 W. 23rd St
www.peterflom.com
New York, NY 10010
(212) 845-4485 (voice)
(917) 438-0894 (fax)



From ripley at stats.ox.ac.uk  Mon Dec  8 20:04:43 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 8 Dec 2003 19:04:43 +0000 (GMT)
Subject: [R] test for arima coef's significancy
In-Reply-To: <F8F9E8240570AB4E86DC49B64FDCA2C40101BE1F@FR-MAILBOX1.fr.sgam.socgen>
Message-ID: <Pine.LNX.4.44.0312081858170.30662-100000@gannet.stats>

On Mon, 8 Dec 2003, STOLIAROFF VINCENT wrote:

> Dear sirs, 
> 
> I would like to know if there is a function to compute the pvalue for the
> significancy of arima coef in an arima object created by 
> the arima function.
> 
> I have written this one:
> 
> pvalueArima<-function(x,arima)
> {
> t<-(arima$coef)/(diag(arima$var.coef)^0.5)
> df<-length(x)-length(arima$coef)
> 1-pt(t,df)
> }
> 
> Has somebody already implemented something equivalent ?

Can you explain how you managed to derive a t distribution for this 
statistic, yet none of the references mentioned in the various help pages 
contain such a result?

Could you also explain why a one-sided p-value is appropriate, and how the 
bounded space containing the coefficients is not relevant, nor are 
missing values in the series?

It's hard for the R-developers to write a function to compute something we 
do not know how to find theoretically, so please share your exceptional 
insights with us.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at myway.com  Mon Dec  8 20:12:15 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon,  8 Dec 2003 14:12:15 -0500 (EST)
Subject: [R] stripchart problem
Message-ID: <20031208191215.DF8E639A5@mprdmxin.myway.com>




stripchart always plots all the points in a given group with the same
symbol, so you can't do what you want with it.  Here are some
alternatives:

1. Its not very nice but coplot can get you a chart somewhat
in the vein you are looking for:

 with(famdata, coplot(age~family|family, pch=ifelse(sex=="m",22,1))) 

2. With a bit more work, you can try creating it yourself using plot:

 with(famdata, plot(age, as.numeric(family), pch=ifelse(sex=="m",22,1),axes=F))
 axis(1)
 lv <- levels(famdata$family)
 axis(2,seq(lv),lv)

I have not examined the above closely so you might want to double
check them.



--- 
 
Hello,

I am trying to plot age distribution data for a certain condition that
runs in families. Below is a simplified view of the dataset, i.e. in
this case there are four families, each line corresponding to one
individual with age at diagnosis and sex.

> famdata
family age sex
1 fam1 2.1 f
2 fam1 2.3 f
3 fam1 1.0 m
4 fam2 7.3 f
5 fam2 4.1 f
6 fam2 1.2 f
7 fam2 0.6 m
8 fam3 3.5 m
9 fam3 2.5 m
10 fam3 2.9 m
11 fam3 5.6 m
12 fam3 4.4 f
13 fam10 1.1 f
14 fam10 1.2 f
15 fam10 2.9 f
16 fam10 2.2 f
17 fam10 4.7 m

I can nicely plot the age distribution by families with

> stripchart(famdata$age~famdata$family)

I would like to plot datapoints according to the sex of the person, e.g.
circle for a girl and square for a boy, like this:

> stripchart(famdata$age~famdata$family, pch=ifelse(famdata$sex=="m",
22, 1))

But this command doesn't work as I expected. Datapoints from fam2 are
shown as squares, all the rest as circles. Still , this seems OK:

> ifelse(famdata$sex=="m", 22, 1)
[1] 1 1 22 1 1 1 22 22 22 22 22 1 1 1 1 1 22

Any clues ?

Thanks a lot for your help,

alex



From pospiech at de.ibm.com  Mon Dec  8 20:11:32 2003
From: pospiech at de.ibm.com (Dr. Christoph Pospiech)
Date: Mon, 8 Dec 2003 20:11:32 +0100
Subject: [R] Re: Compiling R in 64-bit mode on AIX
Message-ID: <200312082011.32673.pospiech@de.ibm.com>

On Tuesday 14 October 2003 21:05, you wrote:
> Hi.  I saw your post from earlier this year in which you were soliciting
> help on compiling R as a 64-bit application under AIX.  We have been having
> trouble with the same problem.
>
> Have you gotten anywhere?
>
> Thanks for any help.
>
> Best regards,
>
> Matthew Wiener
> RY84-202
> Applied Computer Science & Mathematics Dept.
> Merck Research Labs
> 126 E. Lincoln Ave.
> Rahway, NJ 07065
> 732-594-5303

Matthew, Liu

first sorry for the delay in answering, but I was buried in an important 
project.

I can offer you a solution that was actually found by one of our customers, 
Tobias Reber from DKFZ (German Cancer Research Center).

I am appending a patch (generated by cvs diff -c ...) that I applied to R 
v1.6.2.

Then I typed the following two commands.

OBJECT_MODE=64 \
MAKE=gmake \
CC="cc -DSTDC" \
CXX="xlC" \
MAIN_LDFLAGS="-Wl,-brtl" \
SHLIB_LDFLAGS="-Wl,-G" \
F77="xlf" \
./configure \
--without-x \
--without-blas

OBJECT_MODE=64 gmake

After this I tested with "gmake check" and found no errors.

I have just retried the above on the following setup.
h/w p690+ 1.7GHz
[tstposp at j36 R]$ oslevel -r
5200-02
[tstposp at j36 R]$ lslpp -l | grep vac.C
  vac.C                      6.0.0.5  APPLIED    C for AIX Compiler
  vac.C.readme.ibm           6.0.0.1  COMMITTED  C for AIX iFOR/LS Information
  vac.C                      6.0.0.5  APPLIED    C for AIX Compiler
[tstposp at j36 R]$ lslpp -L | grep xlfcmp
  xlfcmp                     8.1.1.3    A     F    XL Fortran Compiler
  xlfcmp.html.en_US          8.1.1.0    C     F    XL Fortran Compiler
  xlfcmp.idebug.html.en_US   8.1.1.0    C     F    Distributed Debugger
  xlfcmp.msg.en_US           8.1.1.1    A     F    XL Fortran Compiler 
Messages -
  xlfcmp.pdf.en_US           8.1.1.0    C     F    XL Fortran Compiler
  xlfcmp.ps.en_US            8.1.1.0    C     F    XL Fortran Compiler

I copied the r-help mailing list, but I am not sure whether I can post there. 
Could you please post this if you don't see it posted ? Thanks.
-- 

Mit freundlichen Gruessen/Best Regards

Dr. Christoph Pospiech
High Performance & Parallel Computing
Advanced Computing Technology Center
Phone +49-621-469450, Fax: ...-469200, eMail: pospiech at de.ibm.com
Mobile +49-171 765 5871
---- Please Note new Tel+FAX Number ----

From VINCENT.STOLIAROFF at sgam.com  Mon Dec  8 20:19:01 2003
From: VINCENT.STOLIAROFF at sgam.com (STOLIAROFF VINCENT)
Date: Mon, 8 Dec 2003 20:19:01 +0100 
Subject: [R] test for arima coef's significancy
Message-ID: <F8F9E8240570AB4E86DC49B64FDCA2C40101BE20@FR-MAILBOX1.fr.sgam.socgen>



>Can you explain how you managed to derive a t distribution for this 
>statistic, yet none of the references mentioned in the various help pages 
>contain such a result?

OK, let's say it was a naive and not well thought attempt.
I tried it because I used to work with SAS and with the proc ARIMA, I think
I was able to get a pvalue for significancy test for each coefficient. I
have seen in one of your previous mail on the help mailing list that I could
use the wald test or a likelyhood ratio test for the global significancy of
the coef but nothing about each coef significancy

>Could you also explain why a one-sided p-value is appropriate, and how the 
>bounded space containing the coefficients is not relevant, nor are 
>missing values in the series?

Do you mean it is sufficient to check if zero belongs to the confidence
interval centered on the coef value?

>It's hard for the R-developers to write a function to compute something we 
>do not know how to find theoretically, so please share your exceptional 
>insights with us.

I'll do my best for the next contribution.


*************************************************************************
Ce message et toutes les pieces jointes (ci-apres le "message") sont
confidentiels et etablis a l'intention exclusive de ses destinataires.
Toute utilisation ou diffusion non autorisee est interdite. 
Tout message electronique est susceptible d'alteration. 
SG Asset Management et ses filiales declinent toute responsabilite au titre
de ce message s'il a ete altere, deforme ou falsifie.

Decouvrez l'offre et les services de SG Asset Management sur le site
www.sgam.fr 

				********

This message and any attachments (the "message") are confide...{{dropped}}



From hodgess at gator.uhd.edu  Mon Dec  8 20:42:38 2003
From: hodgess at gator.uhd.edu (Erin Hodgess)
Date: Mon, 8 Dec 2003 13:42:38 -0600
Subject: [R] Durbin Watson test
Message-ID: <200312081942.hB8Jgc804458@gator.dt.uh.edu>

Hi R People:

Where is the Durbin Watson test located, please?

I tried looking in the ctest library, but to no avail.

Version 1.8.0 for Windows.

Thanks for the help!


Sincerely,
Erin Hodgess
mailto: hodgess at gator.uhd.edu



From hodgess at gator.uhd.edu  Mon Dec  8 20:45:19 2003
From: hodgess at gator.uhd.edu (Erin Hodgess)
Date: Mon, 8 Dec 2003 13:45:19 -0600
Subject: [R] Durbin Watson
Message-ID: <200312081945.hB8JjJ522115@gator.dt.uh.edu>

the Durbin Watson function is in the car library.

thanks,
Erin



From andy_liaw at merck.com  Mon Dec  8 20:35:30 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 8 Dec 2003 14:35:30 -0500
Subject: [R] Durbin Watson test
Message-ID: <3A822319EB35174CA3714066D590DCD50205CF0E@usrymx25.merck.com>

help.search("durbin") on my box gives:

durbin.watson(car)       Durbin-Watson Test for Autocorrelated Errors
dwtest(lmtest)           Durbin-Watson Test

so you'll need either the `car' or `lmtest' package.

HTH,
Andy

> From: Of Erin Hodgess
> 
> Hi R People:
> 
> Where is the Durbin Watson test located, please?
> 
> I tried looking in the ctest library, but to no avail.
> 
> Version 1.8.0 for Windows.
> 
> Thanks for the help!
> 
> 
> Sincerely,
> Erin Hodgess
> mailto: hodgess at gator.uhd.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
>



From h.wickham at auckland.ac.nz  Mon Dec  8 20:39:52 2003
From: h.wickham at auckland.ac.nz (Hadley Wickham)
Date: Tue, 09 Dec 2003 08:39:52 +1300
Subject: [R] stripchart problem
In-Reply-To: <sfd49e2a.098@icr.ac.uk>
References: <sfd49e2a.098@icr.ac.uk>
Message-ID: <3FD4D388.7080404@auckland.ac.nz>


>I would like to plot datapoints according to the sex of the person, e.g.
>circle for a girl and square for a boy, like this:
>
>  
>
Have you tried using stripplot (a lattice plot) instead?  You can then 
use the group argument to display different groups differently.

eg.  stripplot(age ~ family, data=famdata, group=sex)

Hadley



From feh3k at spamcop.net  Mon Dec  8 20:51:40 2003
From: feh3k at spamcop.net (Frank E Harrell Jr)
Date: Mon, 8 Dec 2003 14:51:40 -0500
Subject: [R] aggregate and names of factors
In-Reply-To: <x265gr78q0.fsf@biostat.ku.dk>
References: <3FD46BC2.3090301@lscp.ehess.fr>
	<x265gr78q0.fsf@biostat.ku.dk>
Message-ID: <20031208145140.361bb2bb.feh3k@spamcop.net>

On 08 Dec 2003 14:13:59 +0100
Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:

> Christophe Pallier <pallier at lscp.ehess.fr> writes:
> 
> > Hello,
> > 
> > I use the function 'aggregate' a lot.
> > 
> > One small annoyance is that it is necessary to name the factors in the
> > 'by' list to get the names in the resulting data.frame (else, they
> > appear as Group.1, Group.2...etc). For example, I am forced to
> > write:
> > 
> > aggregate(y,list(f1=f1,f2=f2),mean)
> > 
> > instead of aggregate(y,list(f1,f2),mean)
> > 
> > (for two factors with short names, it is not such a big deal, but I
> > ususally have about 8 factors with long names...)
> > 
> > I wrote a modified 'aggregate.data.frame' function (see the code
> > below) so that it parses the names of the factors and uses them in the
> > output
> > data.frame. I can now typer aggregate(y,list(f1,f2),mean) ans the
> > resulting data.frame
> > has variables with names 'f1' and 'f2'.
> > 
> > However, I have a few questions:
> > 
> > 1. Is is a good idea at all? When expressions rather than variables
> > are
> >    used as factors, this will probably result in a mess. Can one test
> >    if an argument within a list, is just a variable name or a more
> >    complex expression?). Is there a better way?
> 
> This issue is not just relevant for aggregate. There are a couple of
> other places where you want a named list to get names on output -
> lapply(list(foo,bar,baz) function(x) lm(x~age)), say. One option that
> I've been toying around with is to clone the code from data.frame and
> have a function namedList() or nlist() which automagically supplies
> names by deparsing the call. Now where did I put that code sketch...

llist in the Hmisc packages does that

Frank


> 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help


---
Frank E Harrell Jr    Professor and Chair            School of Medicine
                      Department of Biostatistics    Vanderbilt University
---
Frank E Harrell Jr    Professor and Chair            School of Medicine
                      Department of Biostatistics    Vanderbilt University



From hodgess at gator.uhd.edu  Mon Dec  8 21:18:25 2003
From: hodgess at gator.uhd.edu (Erin Hodgess)
Date: Mon, 8 Dec 2003 14:18:25 -0600
Subject: [R] Durbin Watson thanks!
Message-ID: <200312082018.hB8KIPd30875@gator.dt.uh.edu>

Hi all
The Durbin Watson is also in the lmtest library 
as dwtest.

Thanks to all of you who answered so promptly!!!!!!

R Help rocks!

Sincerely,
Erin



From ray at mcs.vuw.ac.nz  Mon Dec  8 21:23:49 2003
From: ray at mcs.vuw.ac.nz (Ray Brownrigg)
Date: Tue, 9 Dec 2003 09:23:49 +1300 (NZDT)
Subject: [R] Recoding problem
Message-ID: <200312082023.hB8KNnRc019654@tahi.mcs.vuw.ac.nz>

> I have the following variables, all of which are logicals
> 
> fmar15   fcoc15   fher15    fcrk15    fidu15
> 
> what I would like is a variable drug15 which equals 
> idu if fidu15 is T; crk if fidu15 is F but fcrk is T, her if fher15 is
> T but fcrk15 and fidu15 are F and so on
> 
> What's the best way to do this?  
> 
I don't know about the best way, but if I understand your question, the
following achieves what you want:

> tab <- cbind(fidu15, fcrk15, fher15, fcoc15, fmar15)
> substring(colnames(tab), 2, 4)[apply(tab, 1, match, x = T)]

HTH
Ray Brownrigg



From umberto_maggiore at hotmail.com  Mon Dec  8 22:23:40 2003
From: umberto_maggiore at hotmail.com (Umberto Maggiore)
Date: Mon, 08 Dec 2003 21:23:40 +0000
Subject: [R] Design functions after Multiple Imputation
Message-ID: <BAY8-F30ToukzUMObBW000077fc@hotmail.com>

I am a new user of R for Windows, enthusiast about the many functions
of the Design and Hmisc libraries.
I combined the results of a Cox regression model after multiple imputation
(of missing values in some covariates).
Now I got my vector of coefficients (and of standard errors).
My question is: How could I use directly that vector to run programs such
as 'nomogram', 'calibrate', 'validate.cph' which, in contrast, call for
the saved results form 'cph' ?
I did not use 'aregImpute' for multiple imputation. However, even if
I did it, 'fit.mult.impute'  seems not to allow specifying the option
'surv=TRUE' (essential to get a nomogram) or 'x=TRUE, y=TRUE' (which
are essential for 'calibrate' and 'validate.cph'. Therefore, I dont't see
how I could get a nomogram or run other Design functions after 'aregImpute'.

thank you so much in advance
Umberto

_________________________________________________________________
MSN Extra Storage! Hotmail all'ennesima potenza. Provalo!



From hodgess at gator.uhd.edu  Mon Dec  8 22:46:44 2003
From: hodgess at gator.uhd.edu (Erin Hodgess)
Date: Mon, 8 Dec 2003 15:46:44 -0600
Subject: [R] Matrix to Dates
Message-ID: <200312082146.hB8Lkin21808@gator.dt.uh.edu>

Hello again R People:

If I have a matrix with 2 columns

> z1
1960 1
1960 9
1961 6



From hodgess at gator.uhd.edu  Mon Dec  8 22:48:20 2003
From: hodgess at gator.uhd.edu (Erin Hodgess)
Date: Mon, 8 Dec 2003 15:48:20 -0600
Subject: [R] Matrix to dates
Message-ID: <200312082148.hB8LmKK28562@gator.dt.uh.edu>

Let's try again!

I have a matrix in which the first column is a four digit year, and the 
second column is a 2 digit month.

How do I convert the matrix to a date function, please?

Thanks,
Erin
Version 1.8.0
mailto: hodgess at gator.uhd.edu



From abunn at montana.edu  Mon Dec  8 22:57:28 2003
From: abunn at montana.edu (Andy Bunn)
Date: Mon, 8 Dec 2003 14:57:28 -0700
Subject: [R] Matrix to dates
In-Reply-To: <200312082148.hB8LmKK28562@gator.dt.uh.edu>
Message-ID: <002901c3bdd6$57a09aa0$78f05a99@msu.montana.edu>

See ?DateTimeClasses, ?strptime, and ?as.character

This example from strptime should get you going:

## read in date/time info in format 'm/d/y h:m:s'
     dates <- c("02/27/92", "02/27/92", "01/14/92",
                "02/28/92", "02/01/92")
     times <- c("23:03:20", "22:29:56", "01:03:30",
                "18:21:03", "16:56:26")
     x <- paste(dates, times)
     z <- strptime(x, "%m/%d/%y %H:%M:%S")
     z


Good luck, Andy



From christoph.lehmann at gmx.ch  Mon Dec  8 23:41:10 2003
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: 08 Dec 2003 23:41:10 +0100
Subject: [R] R^2 analogue in polr() and prerequisites for polr()
Message-ID: <1070923270.1331.18.camel@christophl>

Hi

(1)In polr(), is there any way to calculate a pseudo analogue to the
R^2. Just for use as a purely descriptive statistic of the goodness of
fit?

(2) And: what are the assumptions which must be fulfilled, so that the
results of polr() (t-values, etc.) are valid? How can I test these
prerequisites most easily: I have a three-level (ordered factor)
response and four metric variables.

many thanks

Christoph

-- 
Christoph Lehmann <christoph.lehmann at gmx.ch>



From r.darnell at uq.edu.au  Tue Dec  9 00:39:16 2003
From: r.darnell at uq.edu.au (Ross Darnell)
Date: Tue, 09 Dec 2003 09:39:16 +1000
Subject: [R] Frequent crash printing graphics windows and wavethresh
Message-ID: <he0ayj4r.fsf@uq.edu.au>


I often have R (1.8.1) crash after I generate several graphics windows using
windows() or X11(), print a graphics window and then rerun the same
script. A windows message comes up saying "Program error Rterm.exe .." 
(I can get the same problem using Rgui.)

Xemacs tells me 
"Process R trace trap" at time  and date

The entry in the Dr Watson logs starts with


Application exception occurred:
        App:  (pid=134697032)
        When: 26/11/2002 @ 21:12:10.057
        Exception number: c0000005 (access violation)


The situation in which I can consistently produce this is when I am
using the wavethresh3 dll and associated functions. I have tried to
generate the same problem without using wavelet functions (just
producing several graphics windows with plots of random numbers) but
cannot reproduce this problem consistently from a new session.

I was wondering if anyone else is experiencing this problem

Regards

Ross Darnell
-- 
University of Queensland, Brisbane QLD 4067 AUSTRALIA
Email: <r.darnell at uq.edu.au>



From hec.villafuerte at telgua.com.gt  Tue Dec  9 03:37:16 2003
From: hec.villafuerte at telgua.com.gt (=?ISO-8859-1?Q?=22H=E9ctor_Villafuerte_D=2E=22?=)
Date: Mon, 08 Dec 2003 18:37:16 -0800
Subject: [R] Interfacing R and Python in MS Windows
Message-ID: <3FD5355C.6090503@telgua.com.gt>

Hi all,
I need the power of R from within some of my Python programs...
I use debian linux (woody) at home and windows XP at work (the
latter is where I need to get things done!)

This are my packages:
R 1.8.0
Python 2.3
RSPython 0.5-3

This is what I've done:
(1) Since the Windows Binary of RSPython is compiled against
Python 2.2 I downloaded the tarball
(2) Followed the instructions in INSTALL.win (with pexports and
everything)
(3) In the RGUI "Install package(s) from local zip files..."
(4) NO errors reported during this process
(5) When I try to "Load package" in R it show this error:
 > local({pkg <- select.list(sort(.packages(all.available = TRUE)))
+ if(nchar(pkg)) library(pkg, character.only=TRUE)})
Error in testRversion(descfields) : This package has not been installed 
properly
 See the Note in ?library

(6) In Python
 >>> import RS
Traceback (most recent call last):
  File "<stdin>", line 1, in ?
ImportError: No module named RS

Please help me to get this excelent tools going on in Windows.
Thanks in advance,
Hector



From savano at superig.com.br  Tue Dec  9 04:24:10 2003
From: savano at superig.com.br (Savano)
Date: Tue, 9 Dec 2003 01:24:10 -0200
Subject: [R] Font
Message-ID: <200312090124.10766.savano@superig.com.br>

UseR's,

I run R on Red Hat linux 8.  I did some graphics using:

	ps.options(paper="a4",horizontal=T,family="Times");

	postscript(file="boxplotdistancia.eps");
	boxplot(distancia);
	dev.off()

How I change the font size?

Thanks for helping.

Savano



From jeffrey.chang at duke.edu  Tue Dec  9 04:53:10 2003
From: jeffrey.chang at duke.edu (Jeffrey Chang)
Date: Mon, 8 Dec 2003 22:53:10 -0500
Subject: [R] p-value from chisq.test working strangely on 1.8.1
Message-ID: <34D43DCE-29FB-11D8-9DB9-000A956845CE@duke.edu>

Hello everybody,

I'm seeing some strange behavior on R 1.8.1 on Intel/Linux compiled  
with gcc 3.2.2.  The p-value calculated from the chisq.test function is  
incorrect for some input values:


 > chisq.test(matrix(c(0, 1, 1, 12555), 2, 2), simulate.p.value=TRUE)

         Pearson's Chi-squared test with simulated p-value (based on 2000
         replicates)

data:  matrix(c(0, 1, 1, 12555), 2, 2)
X-squared = 1e-04, df = NA, p-value = 1

 > chisq.test(matrix(c(0, 1, 1, 12556), 2, 2), simulate.p.value=TRUE)
[...]
data:  matrix(c(0, 1, 1, 12556), 2, 2)
X-squared = 1e-04, df = NA, p-value = < 2.2e-16

 > chisq.test(matrix(c(0, 1, 1, 12557), 2, 2), simulate.p.value=TRUE)
[...]
data:  matrix(c(0, 1, 1, 12557), 2, 2)
X-squared = 1e-04, df = NA, p-value = 1


In these three calls to chisq.test, I'm varying the input matrix by  
only 1 observation, but the p-value changes by 16 orders of magnitude.   
This is reproducible on my system.  Please let me know if any other  
information would be useful.

chisq.test works properly for these inputs on Mac OS X 10.3.1 with R  
1.8.0.  I don't know if the problem is with Linux or 1.8.1.

This bug looks very similar to bug 4718, which was reported in R 1.8.0  
and fixed in R 1.8.1.  They may be related.
http://r-bugs.biostat.ku.dk/cgi-bin/R/Analyses-fixed?id=4718; 
user=guest;selectid=4718

Jeff



From naumov at buffalo.edu  Tue Dec  9 05:34:26 2003
From: naumov at buffalo.edu (Aleksey Naumov)
Date: Mon, 8 Dec 2003 23:34:26 -0500 (EST)
Subject: [R] Font
In-Reply-To: <200312090124.10766.savano@superig.com.br>
Message-ID: <Pine.GSO.4.05.10312082329140.1959-100000@callisto.acsu.buffalo.edu>

Use 'pointsize' argument, see ?postscript. This will proportionately scale
all plot elements: title, axes labels and annotations, and plotting
symbols (pch). If you want to control them separately, you may have to use
the cex* parameters directly: cex, cex.axis, cex.lab, etc, see ?par.

Best
Aleksey

On Tue, 9 Dec 2003, Savano wrote:

> UseR's,
> 
> I run R on Red Hat linux 8.  I did some graphics using:
> 
> 	ps.options(paper="a4",horizontal=T,family="Times");
> 
> 	postscript(file="boxplotdistancia.eps");
> 	boxplot(distancia);
> 	dev.off()
> 
> How I change the font size?
> 
> Thanks for helping.
> 
> Savano
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
>



From kreil at ebi.ac.uk  Tue Dec  9 06:02:36 2003
From: kreil at ebi.ac.uk (David Kreil)
Date: Tue, 09 Dec 2003 05:02:36 +0000
Subject: [R] How to append to a data.frame?
Message-ID: <200312090502.hB952am26154@puffin.ebi.ac.uk>


Hi,

I have a data.frame that I need to construct iteratively.

At the moment, I'm doing:

d<-data.frame(x=c(),y=c(),z=());

# {and, within some loop}

  d<-rbind(d,data.frame(x=newx,y=newy,z=newz);


While this works, it is horribly verbose and probably not efficient, either. 
My real data.frame has, of course, many more columns, which can be of 
different modes.

I vaguely recall that in much earlier R versions the following worked

  d[dim(d)[1]+1,]<-c(newx,newy,newz);

but not anymore (both 1.7 and 1.8 give "subscript out of bounds").

Can anyone suggest a more elegant and/or efficient way of achieving this, 
please? Cc to this address highly appreciated.

With many thanks,

David.


------------------------------------------------------------------------
Dr David Philip Kreil                 ("`-''-/").___..--''"`-._
Research Fellow                        `6_ 6  )   `-.  (     ).`-.__.`)
University of Cambridge                (_Y_.)'  ._   )  `._ `. ``-..-'
++44 1223 764107, fax 333992         _..`--'_..-_/  /--'_.' ,'
www.inference.phy.cam.ac.uk/dpk20   (il),-''  (li),'  ((!.-'



From HStevens at muohio.edu  Tue Dec  9 07:17:45 2003
From: HStevens at muohio.edu (Hank Stevens)
Date: Tue, 09 Dec 2003 01:17:45 -0500
Subject: [R] axes that meet
Message-ID: <5.1.0.14.2.20031209011437.01700d10@po.muohio.edu>

R v. 1.7.1, Windows 2000.
A particular journal wants me to provide scatter plots with no box, but 
with axes that meet in the lower left corner. It seems as though there must 
be an easy way of doing this, but my reading the help on plot.default, 
axis, and box have not provided any clues. I would be most appreciative of 
any feedback.
Thank you,
Hank Stevens

Dr. Martin Henry H. Stevens, Assistant Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/botany/bot/henry.html
http://www.muohio.edu/ecology



From ripley at stats.ox.ac.uk  Tue Dec  9 07:26:37 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 9 Dec 2003 06:26:37 +0000 (GMT)
Subject: [R] R^2 analogue in polr() and prerequisites for polr()
In-Reply-To: <1070923270.1331.18.camel@christophl>
Message-ID: <Pine.LNX.4.44.0312090624580.10490-100000@gannet.stats>

On 8 Dec 2003, Christoph Lehmann wrote:

> (1)In polr(), is there any way to calculate a pseudo analogue to the
> R^2. Just for use as a purely descriptive statistic of the goodness of
> fit?

First define the statistic you are interested in!  There is an absolute 
measure of fit, the residual deviance.

> (2) And: what are the assumptions which must be fulfilled, so that the
> results of polr() (t-values, etc.) are valid? How can I test these
> prerequisites most easily: I have a three-level (ordered factor)
> response and four metric variables.

This is discussed with worked examples in the book that MASS supports, so 
please consult your copy.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Dec  9 07:34:07 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 9 Dec 2003 06:34:07 +0000 (GMT)
Subject: [R] How to append to a data.frame?
In-Reply-To: <200312090502.hB952am26154@puffin.ebi.ac.uk>
Message-ID: <Pine.LNX.4.44.0312090631250.10490-100000@gannet.stats>

On Tue, 9 Dec 2003, David Kreil wrote:

> 
> Hi,
> 
> I have a data.frame that I need to construct iteratively.
> 
> At the moment, I'm doing:
> 
> d<-data.frame(x=c(),y=c(),z=());
> 
> # {and, within some loop}
> 
>   d<-rbind(d,data.frame(x=newx,y=newy,z=newz);
> 
> 
> While this works, it is horribly verbose and probably not efficient, either. 
> My real data.frame has, of course, many more columns, which can be of 
> different modes.
> 
> I vaguely recall that in much earlier R versions the following worked
> 
>   d[dim(d)[1]+1,]<-c(newx,newy,newz);
> 
> but not anymore (both 1.7 and 1.8 give "subscript out of bounds").
> 
> Can anyone suggest a more elegant and/or efficient way of achieving this, 
> please? Cc to this address highly appreciated.

Just allocate a large enough data frame to start with, then use indexing
to insert the rows.  If you cannot get a good bound on the eventual size,
over-allocate and double in size as needed.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From christoph.lehmann at gmx.ch  Tue Dec  9 07:40:08 2003
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: 09 Dec 2003 07:40:08 +0100
Subject: [R] R^2 analogue in polr() and prerequisites for polr()
In-Reply-To: <Pine.LNX.4.44.0312090624580.10490-100000@gannet.stats>
References: <Pine.LNX.4.44.0312090624580.10490-100000@gannet.stats>
Message-ID: <1070952008.2480.5.camel@christophl>

many thanks! I was just asking for a r-square analogue, since the
students I will present the results to, might like to know, how the
measure of fit in an ordinal regression (e.g. the residual deviance)
compare to measures they know (from introductory courses to linear
regression) (such as the r-square), means: how much of the variance of
the dependent variable can be explained by the variance of the
independent variables.

thanks and best regards

christoph
On Tue, 2003-12-09 at 07:26, Prof Brian Ripley wrote:
> On 8 Dec 2003, Christoph Lehmann wrote:
> 
> > (1)In polr(), is there any way to calculate a pseudo analogue to the
> > R^2. Just for use as a purely descriptive statistic of the goodness of
> > fit?
> 
> First define the statistic you are interested in!  There is an absolute 
> measure of fit, the residual deviance.
> 
> > (2) And: what are the assumptions which must be fulfilled, so that the
> > results of polr() (t-values, etc.) are valid? How can I test these
> > prerequisites most easily: I have a three-level (ordered factor)
> > response and four metric variables.
> 
> This is discussed with worked examples in the book that MASS supports, so 
> please consult your copy.
-- 
Christoph Lehmann <christoph.lehmann at gmx.ch>



From kreil at ebi.ac.uk  Tue Dec  9 07:56:12 2003
From: kreil at ebi.ac.uk (David Kreil)
Date: Tue, 09 Dec 2003 06:56:12 +0000
Subject: [R] How to append to a data.frame? 
In-Reply-To: Your message of "Tue, 09 Dec 2003 06:34:07 GMT."
	<Pine.LNX.4.44.0312090631250.10490-100000@gannet.stats> 
Message-ID: <200312090656.hB96uC307504@puffin.ebi.ac.uk>


Dear Prof. Ripley,

Thank you very much for your fast and helpful reply!

Is there a canonical way of doing this without a kludge or is the below 
adequate?

  d<-as.data.frame(matrix(nrow=1000))

With many thanks again for your help,

Yours sincerely,

David Kreil.


------------------------------------------------------------------------
Dr David Philip Kreil                 ("`-''-/").___..--''"`-._
Research Fellow                        `6_ 6  )   `-.  (     ).`-.__.`)
University of Cambridge                (_Y_.)'  ._   )  `._ `. ``-..-'
++44 1223 764107, fax 333992         _..`--'_..-_/  /--'_.' ,'
www.inference.phy.cam.ac.uk/dpk20   (il),-''  (li),'  ((!.-'



From ripley at stats.ox.ac.uk  Tue Dec  9 08:02:42 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 9 Dec 2003 07:02:42 +0000 (GMT)
Subject: [R] How to append to a data.frame? 
In-Reply-To: <200312090656.hB96uC307504@puffin.ebi.ac.uk>
Message-ID: <Pine.LNX.4.44.0312090702140.16070-100000@gannet.stats>

On Tue, 9 Dec 2003, David Kreil wrote:

> 
> Dear Prof. Ripley,
> 
> Thank you very much for your fast and helpful reply!
> 
> Is there a canonical way of doing this without a kludge or is the below 
> adequate?
> 
>   d<-as.data.frame(matrix(nrow=1000))

You need to set up the columns as you need them to be eventually.

> 
> With many thanks again for your help,
> 
> Yours sincerely,
> 
> David Kreil.
> 
> 
> ------------------------------------------------------------------------
> Dr David Philip Kreil                 ("`-''-/").___..--''"`-._
> Research Fellow                        `6_ 6  )   `-.  (     ).`-.__.`)
> University of Cambridge                (_Y_.)'  ._   )  `._ `. ``-..-'
> ++44 1223 764107, fax 333992         _..`--'_..-_/  /--'_.' ,'
> www.inference.phy.cam.ac.uk/dpk20   (il),-''  (li),'  ((!.-'
> 
> 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jwdougherty at mcihispeed.net  Tue Dec  9 08:12:05 2003
From: jwdougherty at mcihispeed.net (John Dougherty)
Date: Mon, 8 Dec 2003 23:12:05 -0800
Subject: [R] Font problem
Message-ID: <200312082312.05522.jwdougherty@mcihispeed.net>

Some plots fail due to a problem with the X11 fonts.  I get a message that 
"X11 font at size 22 could not be loaded."  The demo() graphics routine for 
instance dies during the third chart.  The graphics demo calls "font.main=1" 
and that seems to be where the error is.  I believe this is due to a 
configuration problem on my system, however I can't find where in the 
environment font.main looks for the font to use.

I am running SuSE 9.0 and use the KDE desktop.  However, I have also 
replicated this in GNOME and WindowMaker.  Varying the fonts used by the 
console does notb effect the result.

Thanks,
John Dougherty



From kreil at ebi.ac.uk  Tue Dec  9 08:22:30 2003
From: kreil at ebi.ac.uk (David Kreil)
Date: Tue, 09 Dec 2003 07:22:30 +0000
Subject: [R] How to append to a data.frame? 
In-Reply-To: Your message of "Tue, 09 Dec 2003 07:02:42 GMT."
	<Pine.LNX.4.44.0312090702140.16070-100000@gannet.stats> 
Message-ID: <200312090722.hB97MUk10826@puffin.ebi.ac.uk>

Yes, once I've named the first column, I can add further ones by saying 
d[c("x","y","z")]=NA or such. I was just wondering whether that was the way to 
do it or whether there was a more elegant approach. Preallocation was the 
critical clue I needed.

Thanks again for your help,

David.


------------------------------------------------------------------------
Dr David Philip Kreil                 ("`-''-/").___..--''"`-._
Research Fellow                        `6_ 6  )   `-.  (     ).`-.__.`)
University of Cambridge                (_Y_.)'  ._   )  `._ `. ``-..-'
++44 1223 764107, fax 333992         _..`--'_..-_/  /--'_.' ,'
www.inference.phy.cam.ac.uk/dpk20   (il),-''  (li),'  ((!.-'



From ligges at statistik.uni-dortmund.de  Tue Dec  9 08:31:07 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 09 Dec 2003 08:31:07 +0100
Subject: [R] axes that meet
In-Reply-To: <5.1.0.14.2.20031209011437.01700d10@po.muohio.edu>
References: <5.1.0.14.2.20031209011437.01700d10@po.muohio.edu>
Message-ID: <3FD57A3B.9050507@statistik.uni-dortmund.de>

Hank Stevens wrote:
> R v. 1.7.1, Windows 2000.
> A particular journal wants me to provide scatter plots with no box, but 
> with axes that meet in the lower left corner. It seems as though there 
> must be an easy way of doing this, but my reading the help on 
> plot.default, axis, and box have not provided any clues. I would be most 
> appreciative of any feedback.
> Thank you,
> Hank Stevens
> 
> Dr. Martin Henry H. Stevens, Assistant Professor
> 338 Pearson Hall
> Botany Department
> Miami University
> Oxford, OH 45056
> 
> Office: (513) 529-4206
> Lab: (513) 529-4262
> FAX: (513) 529-4243
> http://www.cas.muohio.edu/botany/bot/henry.html
> http://www.muohio.edu/ecology
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

See argument "bty" in ?par, e.g.:

  par(bty="l")
  plot(1:10)

Uwe Ligges



From ligges at statistik.uni-dortmund.de  Tue Dec  9 08:33:38 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 09 Dec 2003 08:33:38 +0100
Subject: [R] Interfacing R and Python in MS Windows
In-Reply-To: <3FD5355C.6090503@telgua.com.gt>
References: <3FD5355C.6090503@telgua.com.gt>
Message-ID: <3FD57AD2.2060500@statistik.uni-dortmund.de>

H?ctor Villafuerte D. wrote:

> Hi all,
> I need the power of R from within some of my Python programs...
> I use debian linux (woody) at home and windows XP at work (the
> latter is where I need to get things done!)
> 
> This are my packages:
> R 1.8.0
> Python 2.3
> RSPython 0.5-3
> 
> This is what I've done:
> (1) Since the Windows Binary of RSPython is compiled against
> Python 2.2 I downloaded the tarball
> (2) Followed the instructions in INSTALL.win (with pexports and
> everything)
> (3) In the RGUI "Install package(s) from local zip files..."

If you have downloaded the tarball of RSPython, you have to install the 
package using Rcmd INSTALL, and you cannot use the RGUI which is 
designed to install binary packages.

Uwe Ligges


> (4) NO errors reported during this process
> (5) When I try to "Load package" in R it show this error:
>  > local({pkg <- select.list(sort(.packages(all.available = TRUE)))
> + if(nchar(pkg)) library(pkg, character.only=TRUE)})
> Error in testRversion(descfields) : This package has not been installed 
> properly
> See the Note in ?library
> 
> (6) In Python
>  >>> import RS
> Traceback (most recent call last):
>  File "<stdin>", line 1, in ?
> ImportError: No module named RS
> 
> Please help me to get this excelent tools going on in Windows.
> Thanks in advance,
> Hector
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Tue Dec  9 08:57:25 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 9 Dec 2003 07:57:25 +0000 (GMT)
Subject: [R] How to append to a data.frame? 
In-Reply-To: <200312090722.hB97MUk10826@puffin.ebi.ac.uk>
Message-ID: <Pine.LNX.4.44.0312090756490.31517-100000@gannet.stats>

On Tue, 9 Dec 2003, David Kreil wrote:

> Yes, once I've named the first column, I can add further ones by saying 
> d[c("x","y","z")]=NA or such. I was just wondering whether that was the way to 
> do it or whether there was a more elegant approach. Preallocation was the 
> critical clue I needed.

Use an initial data.frame call naming all the columns.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From kreil at ebi.ac.uk  Tue Dec  9 09:05:17 2003
From: kreil at ebi.ac.uk (David Kreil)
Date: Tue, 09 Dec 2003 08:05:17 +0000
Subject: [R] How to append to a data.frame? 
In-Reply-To: Your message of "Tue, 09 Dec 2003 07:57:25 GMT."
	<Pine.LNX.4.44.0312090756490.31517-100000@gannet.stats> 
Message-ID: <200312090805.hB985HR16279@puffin.ebi.ac.uk>

Ok, how can I both allocate storage and specify column names in a data.frame 
call, please? Apologies if I'm being slow here.

With many thanks again,

David.

> > Yes, once I've named the first column, I can add further ones by saying 
> > d[c("x","y","z")]=NA or such. I was just wondering whether that was the way to 
> > do it or whether there was a more elegant approach. Preallocation was the 
> > critical clue I needed.
> 
> Use an initial data.frame call naming all the columns.
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 


------------------------------------------------------------------------
Dr David Philip Kreil                 ("`-''-/").___..--''"`-._
Research Fellow                        `6_ 6  )   `-.  (     ).`-.__.`)
University of Cambridge                (_Y_.)'  ._   )  `._ `. ``-..-'
++44 1223 764107, fax 333992         _..`--'_..-_/  /--'_.' ,'
www.inference.phy.cam.ac.uk/dpk20   (il),-''  (li),'  ((!.-'



From Torsten.Hothorn at rzmail.uni-erlangen.de  Tue Dec  9 09:23:53 2003
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Tue, 9 Dec 2003 09:23:53 +0100 (CET)
Subject: [R] p-value from chisq.test working strangely on 1.8.1
In-Reply-To: <34D43DCE-29FB-11D8-9DB9-000A956845CE@duke.edu>
References: <34D43DCE-29FB-11D8-9DB9-000A956845CE@duke.edu>
Message-ID: <Pine.LNX.4.51.0312090919410.31424@artemis.imbe.med.uni-erlangen.de>


> Hello everybody,
>
> I'm seeing some strange behavior on R 1.8.1 on Intel/Linux compiled
> with gcc 3.2.2.  The p-value calculated from the chisq.test function is
> incorrect for some input values:
>
>
>  > chisq.test(matrix(c(0, 1, 1, 12555), 2, 2), simulate.p.value=TRUE)
>
>          Pearson's Chi-squared test with simulated p-value (based on 2000
>          replicates)
>
> data:  matrix(c(0, 1, 1, 12555), 2, 2)
> X-squared = 1e-04, df = NA, p-value = 1
>
>  > chisq.test(matrix(c(0, 1, 1, 12556), 2, 2), simulate.p.value=TRUE)
> [...]
> data:  matrix(c(0, 1, 1, 12556), 2, 2)
> X-squared = 1e-04, df = NA, p-value = < 2.2e-16
>
>  > chisq.test(matrix(c(0, 1, 1, 12557), 2, 2), simulate.p.value=TRUE)
> [...]
> data:  matrix(c(0, 1, 1, 12557), 2, 2)
> X-squared = 1e-04, df = NA, p-value = 1
>

this does not happen with R-1.8.1 and gcc 2.95.4 on Debian stable:

R> chisq.test(matrix(c(0, 1, 1, 12555), 2, 2),
simulate.p.value=TRUE)$p.value
[1] 1
R> chisq.test(matrix(c(0, 1, 1, 12556), 2, 2),
simulate.p.value=TRUE)$p.value
[1] 1
R> chisq.test(matrix(c(0, 1, 1, 12557), 2, 2),
simulate.p.value=TRUE)$p.value
[1] 1

neither with R-1.9.0 (unstable). Is this reproducible without using
`set.seed' on your system?

Best,

Torsten

>
> In these three calls to chisq.test, I'm varying the input matrix by
> only 1 observation, but the p-value changes by 16 orders of magnitude.
> This is reproducible on my system.  Please let me know if any other
> information would be useful.
>
> chisq.test works properly for these inputs on Mac OS X 10.3.1 with R
> 1.8.0.  I don't know if the problem is with Linux or 1.8.1.
>
> This bug looks very similar to bug 4718, which was reported in R 1.8.0
> and fixed in R 1.8.1.  They may be related.
> http://r-bugs.biostat.ku.dk/cgi-bin/R/Analyses-fixed?id=4718;
> user=guest;selectid=4718
>
> Jeff
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>



From Osmo.Kolehmainen at joensuu.fi  Tue Dec  9 09:45:43 2003
From: Osmo.Kolehmainen at joensuu.fi (Osmo Kolehmainen)
Date: Tue, 09 Dec 2003 10:45:43 +0200
Subject: [R] The spdep package
Message-ID: <5.1.0.14.0.20031209102942.00ad2088@joyx.joensuu.fi>

Hi,

Here is a listw object z corresponding to the matrix W. I understand n, 
nn,  S0, S1 and S2 in the weights constants summary. Is it simply so that 
n1 = n-1, n2 = n-2 and n3 = n-3? If this is true where they are needed?

Just wondering

Osmo


 > W
       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]
  [1,]    0    1    0    1    0    0    0    0    0
  [2,]    1    0    1    0    1    0    0    0    0
  [3,]    0    1    0    0    0    1    0    0    0
  [4,]    1    0    0    0    1    0    1    0    0
  [5,]    0    1    0    1    0    1    0    1    0
  [6,]    0    0    1    0    1    0    0    0    1
  [7,]    0    0    0    1    0    0    0    1    0
  [8,]    0    0    0    0    1    0    1    0    1
  [9,]    0    0    0    0    0    1    0    1    0
 > z=mat2listw(W);z
Characteristics of weights list object:
Neighbour list object:
Number of regions: 9
Number of nonzero links: 24
Percentage nonzero weights: 29.62963
Average number of links: 2.666667

Weights style: M
Weights constants summary:
   n n1 n2 n3 nn S0 S1  S2
M 9  8  7  6 81 24 48 272



From Osmo.Kolehmainen at joensuu.fi  Tue Dec  9 10:01:49 2003
From: Osmo.Kolehmainen at joensuu.fi (Osmo Kolehmainen)
Date: Tue, 09 Dec 2003 11:01:49 +0200
Subject: [R] R: the spdep package
Message-ID: <5.1.0.14.0.20031209110016.00acf1c8@joyx.joensuu.fi>

Hi,
Here is a listw object z corresponding to the matrix W. I understand n, nn, 
S0, S1 and S2 in the weights constants summary. Is it simply so that n1 = 
n-1, n2 = n-2 and n3 = n-3? If this is true where they are needed?
Just wondering
Osmo

 > W
[,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]
[1,] 0 1 0 1 0 0 0 0 0
[2,] 1 0 1 0 1 0 0 0 0
[3,] 0 1 0 0 0 1 0 0 0
[4,] 1 0 0 0 1 0 1 0 0
[5,] 0 1 0 1 0 1 0 1 0
[6,] 0 0 1 0 1 0 0 0 1
[7,] 0 0 0 1 0 0 0 1 0
[8,] 0 0 0 0 1 0 1 0 1
[9,] 0 0 0 0 0 1 0 1 0
 > z=mat2listw(W);z
Characteristics of weights list object:
Neighbour list object:
Number of regions: 9
Number of nonzero links: 24
Percentage nonzero weights: 29.62963
Average number of links: 2.666667
Weights style: M
Weights constants summary:
n n1 n2 n3 nn S0 S1 S2
M 9 8 7 6 81 24 48 272



From poizot at cnam.fr  Tue Dec  9 10:07:43 2003
From: poizot at cnam.fr (Poizot Emmanuel)
Date: Tue, 9 Dec 2003 10:07:43 +0100
Subject: [R] Alpha
Message-ID: <200312091007.43521.poizot@cnam.fr>

Hello,
I just install red-hat 7.2 for alpha station.
I downloaded R-1.8.1-1.alpha.rpm, but unable to install it.
I need the libblas.so.3 library. I did have a look around and did not find 
where to get that library for linux alpha version.

-- 
regards
----------------------------------------
Emmanuel POIZOT
Cnam/Intechmer
Digue de Collignon
50110 Tourlaville
T?l : (33)(0)2 33 88 73 42
Fax : (33)(0)2 33 88 73 39



From alessandro.semeria at cramont.it  Tue Dec  9 10:34:41 2003
From: alessandro.semeria at cramont.it (alessandro.semeria@cramont.it)
Date: Tue, 9 Dec 2003 10:34:41 +0100
Subject: [R] Alpha
Message-ID: <OF38DDC306.C1BD244B-ONC1256DF7.00345068@tomware.it>


Try to install ATLAS (improved version of BLAS libraries):
 sources form http://www.netlib.org/atlas/index.html#software

Best regards
A.S.

----------------------------

Alessandro Semeria
Models and Simulations Laboratory
Montecatini Environmental Research Center (Edison Group),
Via Ciro Menotti 48,
48023 Marina di Ravenna (RA), Italy
Tel. +39 544 536811
Fax. +39 544 538663
E-mail: alessandro.semeria at cramont.it



                                                                                                                                            
                      Poizot Emmanuel                                                                                                       
                      <poizot at cnam.fr>              To:      r-help at stat.math.ethz.ch                                                       
                      Sent by:                      cc:                                                                                     
                      r-help-bounces at stat.m         Subject: [R] Alpha                                                                      
                      ath.ethz.ch                                                                                                           
                                                                                                                                            
                                                                                                                                            
                      09-12-2003 10.07                                                                                                      
                      Please respond to                                                                                                     
                      poizot                                                                                                                
                                                                                                                                            
                                                                                                                                            




Hello,
I just install red-hat 7.2 for alpha station.
I downloaded R-1.8.1-1.alpha.rpm, but unable to install it.
I need the libblas.so.3 library. I did have a look around and did not find
where to get that library for linux alpha version.

--
regards
----------------------------------------
Emmanuel POIZOT
Cnam/Intechmer
Digue de Collignon
50110 Tourlaville
T?l : (33)(0)2 33 88 73 42
Fax : (33)(0)2 33 88 73 39

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From v_bill_pikounis at merck.com  Tue Dec  9 11:23:30 2003
From: v_bill_pikounis at merck.com (Pikounis, Bill)
Date: Tue, 9 Dec 2003 05:23:30 -0500
Subject: [R] axes that meet
Message-ID: <CFBD404F5E0C9547B4E10B7BDC3DFA2F04155F4A@usrymx18.merck.com>

Hank,
I think the graphical parameter you are looking for is "bty", as in

 par(bty="l")

Details on the ?par help topic.

Hope that helps.

Bill

----------------------------------------
Bill Pikounis, Ph.D.

Biometrics Research Department
Merck Research Laboratories
PO Box 2000, MailDrop RY33-300  
126 E. Lincoln Avenue
Rahway, New Jersey 07065-0900
USA

Phone: 732 594 3913
Fax: 732 594 1565


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Hank Stevens
> Sent: Tuesday, December 09, 2003 1:18 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] axes that meet
> 
> 
> R v. 1.7.1, Windows 2000.
> A particular journal wants me to provide scatter plots with 
> no box, but 
> with axes that meet in the lower left corner. It seems as 
> though there must 
> be an easy way of doing this, but my reading the help on 
> plot.default, 
> axis, and box have not provided any clues. I would be most 
> appreciative of 
> any feedback.
> Thank you,
> Hank Stevens
> 
> Dr. Martin Henry H. Stevens, Assistant Professor
> 338 Pearson Hall
> Botany Department
> Miami University
> Oxford, OH 45056
> 
> Office: (513) 529-4206
> Lab: (513) 529-4262
> FAX: (513) 529-4243
> http://www.cas.muohio.edu/botany/bot/henry.html
> http://www.muohio.edu/ecology
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
>



From jasont at indigoindustrial.co.nz  Tue Dec  9 11:50:29 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Tue, 09 Dec 2003 23:50:29 +1300
Subject: [R] How to append to a data.frame?
In-Reply-To: <200312090805.hB985HR16279@puffin.ebi.ac.uk>
References: <200312090805.hB985HR16279@puffin.ebi.ac.uk>
Message-ID: <3FD5A8F5.2030205@indigoindustrial.co.nz>

David Kreil wrote:

> Ok, how can I both allocate storage and specify column names in a data.frame 
> call, please? Apologies if I'm being slow here.
> 
> With many thanks again,
> 
> David.
> 

Something like.... (UNTESTED code follows)

templateColumn <- rep(NA,1000) # for 1000 rows

foo <- data.frame( x = templateColumn,
   y = templateColumn,
   z = templateColumn )  # or however many columns you need

bar <- foo

for(ii in your.iterative.sequence) {
   if(ii > 1000) {
     bar <- rbind(bar,foo)
   }
   bar[ii,] <- your.function()
}

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From p.dalgaard at biostat.ku.dk  Tue Dec  9 11:58:21 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 09 Dec 2003 11:58:21 +0100
Subject: [R] How to append to a data.frame?
In-Reply-To: <200312090805.hB985HR16279@puffin.ebi.ac.uk>
References: <200312090805.hB985HR16279@puffin.ebi.ac.uk>
Message-ID: <x2n0a2gsvm.fsf@biostat.ku.dk>

David Kreil <kreil at ebi.ac.uk> writes:

> Ok, how can I both allocate storage and specify column names in a data.frame 
> call, please? Apologies if I'm being slow here.

It gets a little tricky. I'd try something along the lines of

data.frame(age=as.numeric(NA),sex=factor(NA,levels=c("m","f")))[rep(1,20),]

or 

data.frame(age=0,sex=factor("m",levels=c("m","f")))[rep(NA,20),]

and of course the brute force way is

data.frame(age=rep(as.numeric(NA),20),
           sex=factor(rep(NA,20),levels=c("m","f"))
          )

Also, 

(a) there's no idea in ensuring that you're filling with NA if they
    are all going to be changed anyway, and
(b) recycling works so that you only need to specify the length of one
    variable, so 

data.frame(age=numeric(20), sex=factor("",levels=c("m","f")) )

works too.

Extending a data frame can be as simple as

mydata <- mydata[1:newlength,]
 
(plus fixup of row names later on).

> With many thanks again,
> 
> David.
> 
> > > Yes, once I've named the first column, I can add further ones by saying 
> > > d[c("x","y","z")]=NA or such. I was just wondering whether that was the way to 
> > > do it or whether there was a more elegant approach. Preallocation was the 
> > > critical clue I needed.
> > 
> > Use an initial data.frame call naming all the columns.


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Tue Dec  9 12:06:12 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 09 Dec 2003 12:06:12 +0100
Subject: [R] Font problem
In-Reply-To: <200312082312.05522.jwdougherty@mcihispeed.net>
References: <200312082312.05522.jwdougherty@mcihispeed.net>
Message-ID: <x2iskqgsij.fsf@biostat.ku.dk>

John Dougherty <jwdougherty at mcihispeed.net> writes:

> Some plots fail due to a problem with the X11 fonts.  I get a message that 
> "X11 font at size 22 could not be loaded."  The demo() graphics routine for 
> instance dies during the third chart.  The graphics demo calls "font.main=1" 
> and that seems to be where the error is.  I believe this is due to a 
> configuration problem on my system, however I can't find where in the 
> environment font.main looks for the font to use.
> 
> I am running SuSE 9.0 and use the KDE desktop.  However, I have also 
> replicated this in GNOME and WindowMaker.  Varying the fonts used by the 
> console does notb effect the result.

I think that in principle the bug is in R, but as far as I remember,
the workaround is to ensure that you either have scalable PostScript
fonts or have non-scalable versions in both 100 and 75 dpi. This in
turn is assured by configuring the font server.

On RedHat (dunno about SuSE),/etc/X11/fs/config needs to have
something like

catalogue = /usr/X11R6/lib/X11/fonts/misc:unscaled,
        /usr/X11R6/lib/X11/fonts/75dpi:unscaled,
        /usr/X11R6/lib/X11/fonts/100dpi:unscaled,
        /usr/X11R6/lib/X11/fonts/misc,
        /usr/X11R6/lib/X11/fonts/Type1,
        ...

or lose the :unscaled, but that tends to look horrible.
-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From maechler at stat.math.ethz.ch  Tue Dec  9 12:11:56 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 9 Dec 2003 12:11:56 +0100
Subject: [R] trouble with predict.l1ce
In-Reply-To: <OFC9DE508A.503CD7AC-ON85256DF6.005B53E9-85256DF6.0067777C@EU.novartis.net>
References: <OFC9DE508A.503CD7AC-ON85256DF6.005B53E9-85256DF6.0067777C@EU.novartis.net>
Message-ID: <16341.44540.753151.737467@gargle.gargle.HOWL>

>>>>> "clayton" == clayton springer <clayton.springer at pharma.novartis.com>
>>>>>     on Mon, 8 Dec 2003 13:48:11 -0500 writes:

    clayton> Dear R-help, I am having trouble with the predict
    clayton> function in lasso2. For example:

    >> data(Iowa) l1c.I <- l1ce(Yield ~ ., Iowa, bound = 10,
    >> absolute.t=TRUE) predict (l1c.I) # this works is fine
    >> predict (l1c.I,Iowa)
    clayton> Error in eval(exper,envir, enclos) : couldn't find
    clayton> function "Yield"


    clayton> And I have similar trouble whenever I use the
    clayton> newdata argument in prediction.

yes.
This is something that needs to be added to the lasso2 package.

Volunteers are sought ...

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From allan at stats.uct.ac.za  Tue Dec  9 12:17:59 2003
From: allan at stats.uct.ac.za (allan clark)
Date: Tue, 09 Dec 2003 13:17:59 +0200
Subject: [R]: global and local variables
Message-ID: <3FD5AF67.1E737976@stats.uct.ac.za>


   Hi all

   I have a problem pertaining to local and global variables.

   Say I have a function defined as follows:

   a<-function(x)
   {y<x^2}

   i.e
   a(2)
   [1] 4

   function b is now defined to take the value of y and do some
   manipulation with it. As it stands I dont know how to store the
   variable y such that other functions are able to reference its value.

   I know that I can simply put the operations found in b simply into a
   but this is not what I want.

   I would like to have stand alone functions say

   a, b and c which could be run independently as well as have a function
   called say

   control that can run a, b and c.

   i.e.

   control<- function( x)
   {
   a(x)
   b(x)
   c(x)
   }

   I hope that you guys understand what I'm trying to do.

   Cheers
   Allan


From ligges at statistik.uni-dortmund.de  Tue Dec  9 13:06:08 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 09 Dec 2003 13:06:08 +0100
Subject: [R]: global and local variables
In-Reply-To: <3FD5AF67.1E737976@stats.uct.ac.za>
References: <3FD5AF67.1E737976@stats.uct.ac.za>
Message-ID: <3FD5BAB0.5000904@statistik.uni-dortmund.de>

allan clark wrote:

>    Hi all
> 
>    I have a problem pertaining to local and global variables.
> 
>    Say I have a function defined as follows:
> 
>    a<-function(x)
>    {y<x^2}
> 
>    i.e
>    a(2)
>    [1] 4

The function a specified above won't return 4!



>    function b is now defined to take the value of y and do some
>    manipulation with it. As it stands I dont know how to store the
>    variable y such that other functions are able to reference its value.
> 
>    I know that I can simply put the operations found in b simply into a
>    but this is not what I want.
> 
>    I would like to have stand alone functions say
> 
>    a, b and c which could be run independently as well as have a function
>    called say
> 
>    control that can run a, b and c.
> 
>    i.e.
> 
>    control<- function( x)
>    {
>    a(x)
>    b(x)
>    c(x)
>    }
> 
>    I hope that you guys understand what I'm trying to do.

You are trying to read "An Introduction to R"???
If not, please try!

What you are really going to do: using assigments and return() 
statements as in:

a <- function(x) return(x^2)

foo <- function(x) {
   y <- a(x)
   z <- b(x)
   return(list(y=y, z=z))
}

foo(.....)

Uwe Ligges


>    Cheers
>    Allan
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From Joerg.Schaber at uv.es  Tue Dec  9 13:11:45 2003
From: Joerg.Schaber at uv.es (Joerg Schaber)
Date: Tue, 09 Dec 2003 13:11:45 +0100
Subject: [R] levelplot parameters
Message-ID: <3FD5BC01.6080108@uv.es>

Hi,

I have a levelplot with one panel. I just can't find out how I can 
manipulate the size of the axis lables. e.g. scales.cex doesn't work,  
the usual par-parameters either.
Any hint?

joerg



From partha_bagchi at hgsi.com  Tue Dec  9 13:28:16 2003
From: partha_bagchi at hgsi.com (partha_bagchi@hgsi.com)
Date: Tue, 9 Dec 2003 07:28:16 -0500
Subject: [R] axes that meet
Message-ID: <OF22CFE98C.A5AFD520-ON85256DF7.0044632E-85256DF7.004481AF@hgsi.com>

It might also mean that he is looking for par(xaxs = "i", yaxs = "i")





"Pikounis, Bill" <v_bill_pikounis at merck.com>
Sent by: r-help-bounces at stat.math.ethz.ch
12/09/2003 05:23 AM

 
        To:     "'Hank Stevens'" <HStevens at muohio.edu>, r-help at stat.math.ethz.ch
        cc: 
        Subject:        RE: [R] axes that meet


Hank,
I think the graphical parameter you are looking for is "bty", as in

par(bty="l")

Details on the ?par help topic.

Hope that helps.

Bill

----------------------------------------
Bill Pikounis, Ph.D.

Biometrics Research Department
Merck Research Laboratories
PO Box 2000, MailDrop RY33-300
126 E. Lincoln Avenue
Rahway, New Jersey 07065-0900
USA

Phone: 732 594 3913
Fax: 732 594 1565


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Hank Stevens
> Sent: Tuesday, December 09, 2003 1:18 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] axes that meet
>
>
> R v. 1.7.1, Windows 2000.
> A particular journal wants me to provide scatter plots with
> no box, but
> with axes that meet in the lower left corner. It seems as
> though there must
> be an easy way of doing this, but my reading the help on
> plot.default,
> axis, and box have not provided any clues. I would be most
> appreciative of
> any feedback.
> Thank you,
> Hank Stevens
>
> Dr. Martin Henry H. Stevens, Assistant Professor
> 338 Pearson Hall
> Botany Department
> Miami University
> Oxford, OH 45056
>
> Office: (513) 529-4206
> Lab: (513) 529-4262
> FAX: (513) 529-4243
> http://www.cas.muohio.edu/botany/bot/henry.html
> http://www.muohio.edu/ecology
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help

--
This message has been scanned for viruses and
dangerous content by MailScanner, and is
believed to be clean.



From allan at stats.uct.ac.za  Tue Dec  9 13:52:44 2003
From: allan at stats.uct.ac.za (allan clark)
Date: Tue, 09 Dec 2003 14:52:44 +0200
Subject: [R]: global and local variables
References: <3FD5AF67.1E737976@stats.uct.ac.za>
	<3FD5BAB0.5000904@statistik.uni-dortmund.de>
Message-ID: <3FD5C59C.38CA6754@stats.uct.ac.za>


   Hi

   Thanx for those who responded to my problem. In my previous email I
   tried to ask a general question and probably never explained myself
   correctly.  I wanted to prevent sending this long email. My apologies.

   This is my actual problem.

   I have a regression problem. I am writing some R code in order to
   calculate some collinearity diagnostics. The diagnostics all rely on a
   function named preprocess. I've written the different diagnostics as
   separate functions so that they may be evaluated separately if
   required.

   The two functions are named mci and vif. (I will be writing some
   others later)

   mci calculates the mixed condition index as well as the condition
   indices of a given X matrix while
   vif calculates the variance inflation factors of the X matrix.

   Another function named colldiag has been written. This function will
   calculate all of the collinearity diagnostics by simply calling the
   separate functions defined previously.

   I've attached the code of the different functions as well as a data
   file (say a2) below.

   The functions mci and vif work perfectly.

   i.e.

   > mci(a2)
   [1] "DATA MATRIX CENTERED AND SCALED"
   [1] "CENTERED AND SCALED MATRIX = $data"
   [1] "MEANS OF XDATA = $means"
   [1] "STDS OF XDATA = $stds"
   [1] "THE CONDITION NUMBER AND THE CONDITION INDICES"
   $CN
   [1] 27.34412

   $CI
   [1]  1.000000  1.615690 27.344123

   $MCI
     Principal.Component Singular.Values Condition.Index
   1                   1       1.4720680        1.000000
   2                   2       0.9111078        1.615690
   3                   3       0.0538349       27.344123

   > vif(a2)
   [1] "DATA MATRIX CENTERED AND SCALED"
   [1] "CENTERED AND SCALED MATRIX = $data"
   [1] "MEANS OF XDATA = $means"
   [1] "STDS OF XDATA = $stds"
   [1] "THE VARIANCE INFLATION FACTORS"
   $vif
         x1       x2       x3
   169.3542 175.6667   1.6875

   The output from colldiag is as follows:

   > colldiag(a2)
   [1] "DATA MATRIX CENTERED AND SCALED"
   [1] "CENTERED AND SCALED MATRIX = $data"
   [1] "MEANS OF XDATA = $means"
   [1] "STDS OF XDATA = $stds"
   [1] "THE CONDITION NUMBER AND THE CONDITION INDICES"
   $CN
   [1] 27.34412

   $CI
   [1]  1.000000  1.615690 27.344123

   $MCI
     Principal.Component Singular.Values Condition.Index
   1                   1       1.4720680        1.000000
   2                   2       0.9111078        1.615690
   3                   3       0.0538349       27.344123

   [1] "DATA MATRIX CENTERED AND SCALED"
   [1] "CENTERED AND SCALED MATRIX = $data"
   [1] "MEANS OF XDATA = $means"
   [1] "STDS OF XDATA = $stds"
   [1] "THE VARIANCE INFLATION FACTORS"
   $vif
         x1       x2       x3
   169.3542 175.6667   1.6875


    

   Once you check the colldiag code below you will see that it calls mci
   and vif. In both of these functions they call preprocess. This is
   unnecessary. How can I write the code such that R only calls
   preprocess once?

   ONCE AGAIN I APOLOGIZE FOR THE LENGTH OF THIS EMAIL!!!


   Cheers
   Allan





   The data file:

      x1 x2 x3
   1  20 -4  5
   2  21 -4  4
   3  22 -3  3
   4  23 -2  2
   5  24 -1  1
   6  25  0  2
   7  26  1  3
   8  27  2  4
   9  28  3  5
   10 29  4  6
   11 20 -4  5
   12 21 -4  4
   13 22 -3  3
   14 23 -2  2
   15 24 -1  1
   16 25  0  2
   17 26  1  3
   18 27  2  4
   19 28  3  5
   20 29  4  6

   preprocess<-function (xdata,center=1,scale=1)
   {
   if(center==1 && scale==1)
   {
   means<-apply(xdata,2,mean)
   stds<-apply(xdata,2, function(x) sqrt(var(x)))
   scalefactor<-((nrow(xdata)-1)^.5)*stds
   data.centsca<-sweep(sweep(xdata,2,means,"-"),2,scalefactor,"/")
   print("DATA MATRIX CENTERED AND SCALED")
   print("CENTERED AND SCALED MATRIX = $data")
   print("MEANS OF XDATA = $means")
   print("STDS OF XDATA = $stds")
   list(data=data.centsca,means=means,stds=stds,prep=1)
   }

   else if(center==1 && scale==0)
   {
   means<-apply(xdata,2,mean)
   data.cen<-sweep(xdata,2,means,"-")
   print("DATA MATRIX CENTERED")
   list(data=data.cen,means=means,prep=1)
   }

   else if(center==0 && scale==1)
   {
   stds<-apply(xdata,2, function(x) sqrt(var(x)))
   scalefactor<-((nrow(xdata)-1)^.5)*stds
   data.sca<-sweep(xdata,2,scalefactor,"/")
   print("DATA MATRIX SCALED")
   list(data=data.sca,stds=stds,prep=1)
   }

   else
   {
   print("YOU HAVE TO SPECIFY WHETHER YOU WANT TO SCALE OR CENTER THE
   MATRIX")
   print("THE preprocess FUNCTION HAS THREE ARGUMENTS. i.e.
   preprocess(xdata,center,scale)")
   print("xdata IS THE MATRIX TO BE TRANSFORMED")
   print("TO CENTER SPECIFY center=1")
   print("TO SCALE SPECIFY scale=1")
   }

   # A matrix is standardised as follows:
   # X*(i,j) = ( X(i,j)- XBAR(j) ) / (   sqrt(n-1)* STD(j)   )

   }

   mci<-function (xdata)
   {
   a<-preprocess(xdata)
   b<-svd(a$data)
   cn<-(b$d)[1]/(b$d)[ncol(a$data)]
   ci<-(b$d)[1]/(b$d)[1:ncol(a$data)]

   #paste("THE CONDITION NUMBER = ",cn)

   #the following produces a table in order to output the mci values
   Principal.Component<-1:ncol(a$data)
   Singular.Values<-b$d
   Condition.Index<-ci
   mcitable<-data.frame(Principal.Component,Singular.Values,Condition.Ind
   ex)

   print("THE CONDITION NUMBER AND THE CONDITION INDICES")
   d<-list(CN=cn,CI=ci,MCI=mcitable)
   print(d)
   }

   vif<-function (xdata)
   {
   a<-preprocess(xdata)
   vif<-diag(solve(cor(a$data)))

   print("THE VARIANCE INFLATION FACTORS")
   b<-list(vif=vif)
   b
   }

   colldiag<-function (xdata)
   {
   mci(xdata)
   vif(xdata)
   }


From MSchwartz at medanalytics.com  Tue Dec  9 14:42:28 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Tue, 09 Dec 2003 07:42:28 -0600
Subject: [R] Font problem
In-Reply-To: <x2iskqgsij.fsf@biostat.ku.dk>
References: <200312082312.05522.jwdougherty@mcihispeed.net>
	<x2iskqgsij.fsf@biostat.ku.dk>
Message-ID: <1070977348.16868.15.camel@localhost.localdomain>

On Tue, 2003-12-09 at 05:06, Peter Dalgaard wrote:
> John Dougherty <jwdougherty at mcihispeed.net> writes:
> 
> > Some plots fail due to a problem with the X11 fonts.  I get a message that 
> > "X11 font at size 22 could not be loaded."  The demo() graphics routine for 
> > instance dies during the third chart.  The graphics demo calls "font.main=1" 
> > and that seems to be where the error is.  I believe this is due to a 
> > configuration problem on my system, however I can't find where in the 
> > environment font.main looks for the font to use.
> > 
> > I am running SuSE 9.0 and use the KDE desktop.  However, I have also 
> > replicated this in GNOME and WindowMaker.  Varying the fonts used by the 
> > console does notb effect the result.
> 
> I think that in principle the bug is in R, but as far as I remember,
> the workaround is to ensure that you either have scalable PostScript
> fonts or have non-scalable versions in both 100 and 75 dpi. This in
> turn is assured by configuring the font server.
> 
> On RedHat (dunno about SuSE),/etc/X11/fs/config needs to have
> something like
> 
> catalogue = /usr/X11R6/lib/X11/fonts/misc:unscaled,
>         /usr/X11R6/lib/X11/fonts/75dpi:unscaled,
>         /usr/X11R6/lib/X11/fonts/100dpi:unscaled,
>         /usr/X11R6/lib/X11/fonts/misc,
>         /usr/X11R6/lib/X11/fonts/Type1,
>         ...
> 
> or lose the :unscaled, but that tends to look horrible.

Based upon my recollection of a similar query, a quick search of the
mail archives shows that Peter and I responded to a similar problem
earlier this year.

The solution appears to be to edit the file that Peter mentions above to
include the lines (note the lack of the ':unscaled'):

/usr/X11R6/lib/X11/fonts/75dpi,
/usr/X11R6/lib/X11/fonts/100dpi,

and to be sure that the 100 fonts were installed on the system.  Both
sets of lines should be present in the file.

I believe that a restart of X may be required to make the change, but a
restart of the X font server may suffice using:

/sbin/service xfs restart

HTH,

Marc Schwartz



From MSchwartz at medanalytics.com  Tue Dec  9 14:55:01 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Tue, 09 Dec 2003 07:55:01 -0600
Subject: [R] p-value from chisq.test working strangely on 1.8.1
In-Reply-To: <Pine.LNX.4.51.0312090919410.31424@artemis.imbe.med.uni-erlangen.de>
References: <34D43DCE-29FB-11D8-9DB9-000A956845CE@duke.edu>
	<Pine.LNX.4.51.0312090919410.31424@artemis.imbe.med.uni-erlangen.de>
Message-ID: <1070978100.16868.26.camel@localhost.localdomain>

On Tue, 2003-12-09 at 02:23, Torsten Hothorn wrote:
> > Hello everybody,
> >
> > I'm seeing some strange behavior on R 1.8.1 on Intel/Linux compiled
> > with gcc 3.2.2.  The p-value calculated from the chisq.test function is
> > incorrect for some input values:
> >
> >
> >  > chisq.test(matrix(c(0, 1, 1, 12555), 2, 2), simulate.p.value=TRUE)
> >
> >          Pearson's Chi-squared test with simulated p-value (based on 2000
> >          replicates)
> >
> > data:  matrix(c(0, 1, 1, 12555), 2, 2)
> > X-squared = 1e-04, df = NA, p-value = 1
> >
> >  > chisq.test(matrix(c(0, 1, 1, 12556), 2, 2), simulate.p.value=TRUE)
> > [...]
> > data:  matrix(c(0, 1, 1, 12556), 2, 2)
> > X-squared = 1e-04, df = NA, p-value = < 2.2e-16
> >
> >  > chisq.test(matrix(c(0, 1, 1, 12557), 2, 2), simulate.p.value=TRUE)
> > [...]
> > data:  matrix(c(0, 1, 1, 12557), 2, 2)
> > X-squared = 1e-04, df = NA, p-value = 1
> >
> 
> this does not happen with R-1.8.1 and gcc 2.95.4 on Debian stable:
> 
> R> chisq.test(matrix(c(0, 1, 1, 12555), 2, 2),
> simulate.p.value=TRUE)$p.value
> [1] 1
> R> chisq.test(matrix(c(0, 1, 1, 12556), 2, 2),
> simulate.p.value=TRUE)$p.value
> [1] 1
> R> chisq.test(matrix(c(0, 1, 1, 12557), 2, 2),
> simulate.p.value=TRUE)$p.value
> [1] 1
> 
> neither with R-1.9.0 (unstable). Is this reproducible without using
> `set.seed' on your system?
> 
> Best,
> 
> Torsten


<snip>

Confirmed on Fedora Core 1 with R Version 1.8.1 Patched (2003-12-07)
compiled using "gcc (GCC) 3.3.2 20031107 (Red Hat Linux 3.3.2-2)".


> chisq.test(matrix(c(0, 1, 1, 12555), 2, 2), simulate.p.value=TRUE)
...
X-squared = 1e-04, df = NA, p-value = 1

> chisq.test(matrix(c(0, 1, 1, 12556), 2, 2), simulate.p.value=TRUE)

X-squared = 1e-04, df = NA, p-value = < 2.2e-16
...
> chisq.test(matrix(c(0, 1, 1, 12557), 2, 2), simulate.p.value=TRUE)
...
X-squared = 1e-04, df = NA, p-value = 1


HTH,

Marc Schwartz



From p.dalgaard at biostat.ku.dk  Tue Dec  9 15:07:39 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 09 Dec 2003 15:07:39 +0100
Subject: [R] Font problem
In-Reply-To: <1070977348.16868.15.camel@localhost.localdomain>
References: <200312082312.05522.jwdougherty@mcihispeed.net>
	<x2iskqgsij.fsf@biostat.ku.dk>
	<1070977348.16868.15.camel@localhost.localdomain>
Message-ID: <x2ad62gk44.fsf@biostat.ku.dk>

Marc Schwartz <MSchwartz at MedAnalytics.com> writes:

> > 
> > catalogue = /usr/X11R6/lib/X11/fonts/misc:unscaled,
> >         /usr/X11R6/lib/X11/fonts/75dpi:unscaled,
> >         /usr/X11R6/lib/X11/fonts/100dpi:unscaled,
> >         /usr/X11R6/lib/X11/fonts/misc,
> >         /usr/X11R6/lib/X11/fonts/Type1,
> >         ...
> > 
> > or lose the :unscaled, but that tends to look horrible.
> 
> Based upon my recollection of a similar query, a quick search of the
> mail archives shows that Peter and I responded to a similar problem
> earlier this year.
> 
> The solution appears to be to edit the file that Peter mentions above to
> include the lines (note the lack of the ':unscaled'):
> 
> /usr/X11R6/lib/X11/fonts/75dpi,
> /usr/X11R6/lib/X11/fonts/100dpi,
 
> and to be sure that the 100 fonts were installed on the system.  Both
> sets of lines should be present in the file.

Actually, I think you can leave the :unscaled on. Seems to work for me.
 
> I believe that a restart of X may be required to make the change, but a
> restart of the X font server may suffice using:
> 
> /sbin/service xfs restart

Restarting xfs is certainly necessary (and, I believe, not implied by
restarting X) but beware that it may freeze currently running X
applications (that used to be the case anyway), so don't do it
with important stuff running on your desktop. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From peter.hagedorn at risoe.dk  Tue Dec  9 15:15:51 2003
From: peter.hagedorn at risoe.dk (peter.hagedorn@risoe.dk)
Date: Tue, 09 Dec 2003 15:15:51 +0100
Subject: [R] Importing TIFF files into a R matrix
Message-ID: <1644C21C399A7B4EBFD7E5224209EFE309A3BC@EXCHG-VS1.risoe.dk>

Hi

I am facing a problem where I would like to import a TIFF image (of spots on a nylon filter) into R (into a matrix for example). When plotting the matrix using fx. scatterplot3d I would then be able to see how the pixel-intensities are distributed in "spot-areas" on the filter - which would be very helpful.

Does anynone know of a way to do this?

Best regards,

Peter Hagedorn

...
Peter Hagedorn

Ris? National Laboratory
Plant Research Department
Building PRD-330
P.O. Box 49
Frederiksborgvej 399
DK-4000 Roskilde 
Denmark

Phone +45 4677 4293
Fax     +45 4677 4109
e-mail  peter.hagedorn at risoe.dk
web     http://www.risoe.dk/pbk/staff_uk/phah.htm



From MSchwartz at medanalytics.com  Tue Dec  9 15:23:09 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Tue, 09 Dec 2003 08:23:09 -0600
Subject: [R] Font problem
In-Reply-To: <x2ad62gk44.fsf@biostat.ku.dk>
References: <200312082312.05522.jwdougherty@mcihispeed.net>
	<x2iskqgsij.fsf@biostat.ku.dk>
	<1070977348.16868.15.camel@localhost.localdomain>
	<x2ad62gk44.fsf@biostat.ku.dk>
Message-ID: <1070979789.16868.40.camel@localhost.localdomain>

On Tue, 2003-12-09 at 08:07, Peter Dalgaard wrote:
> Marc Schwartz <MSchwartz at MedAnalytics.com> writes:

<snip>

> Actually, I think you can leave the :unscaled on. Seems to work for me.

Could be. That is, I believe, the default on a new install. It was on
FC1 for me (I have not had font problems so far) and I recall the same
default on RH 8.0 and RH 9. Though I do have recollections of font
problems on 8.0, solved by adding the two additional lines without the
':unscaled', which is why this issue sticks in my mind. 

Then again, it may just be old age and the snow this morning...

It may be as simple as being sure that both the 75 dpi and 100 dpi fonts
are loaded as the SUSE query earlier this year seemed to indicate that
the 100 dpi fonts were not installed initially.

> > I believe that a restart of X may be required to make the change, but a
> > restart of the X font server may suffice using:
> > 
> > /sbin/service xfs restart
> 
> Restarting xfs is certainly necessary (and, I believe, not implied by
> restarting X) but beware that it may freeze currently running X
> applications (that used to be the case anyway), so don't do it
> with important stuff running on your desktop. 

Good point  :-)

Marc



From p.dalgaard at biostat.ku.dk  Tue Dec  9 15:37:55 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 09 Dec 2003 15:37:55 +0100
Subject: [R] p-value from chisq.test working strangely on 1.8.1
In-Reply-To: <1070978100.16868.26.camel@localhost.localdomain>
References: <34D43DCE-29FB-11D8-9DB9-000A956845CE@duke.edu>
	<Pine.LNX.4.51.0312090919410.31424@artemis.imbe.med.uni-erlangen.de>
	<1070978100.16868.26.camel@localhost.localdomain>
Message-ID: <x265gqgipo.fsf@biostat.ku.dk>

Marc Schwartz <MSchwartz at medanalytics.com> writes:

> Confirmed on Fedora Core 1 with R Version 1.8.1 Patched (2003-12-07)
> compiled using "gcc (GCC) 3.3.2 20031107 (Red Hat Linux 3.3.2-2)".
> 
> 
> > chisq.test(matrix(c(0, 1, 1, 12555), 2, 2), simulate.p.value=TRUE)
> ...
> X-squared = 1e-04, df = NA, p-value = 1
> 
> > chisq.test(matrix(c(0, 1, 1, 12556), 2, 2), simulate.p.value=TRUE)
> 
> X-squared = 1e-04, df = NA, p-value = < 2.2e-16
> ...
> > chisq.test(matrix(c(0, 1, 1, 12557), 2, 2), simulate.p.value=TRUE)
> ...
> X-squared = 1e-04, df = NA, p-value = 1

Ditto on RH8 with Martyn's RPM of 1.8.0 (yeah, I know...) and ditto
with a reasonably current r-devel (gcc 3.2)

Anyways, it is yet another fudge-factor issue: If you debug to the
point in chisq.test where it calculates

PVAL <- sum(tmp$results >= STATISTIC)/B

you'll find that

Browse[1]> any(diff(tmp$result))
[1] FALSE
Browse[1]> tmp$result[1]
[1] 7.96432e-05
Browse[1]> STATISTIC
[1] 7.96432e-05
Browse[1]> tmp$result[1] - STATISTIC
[1] -1.355253e-20

so PVAL becomes zero and yaddayaddayadda....

The obvious fix would seem to be

PVAL <- sum(tmp$results >= (1-1e-10)*STATISTIC)/B
.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Manuel.A.Morales at williams.edu  Tue Dec  9 15:48:09 2003
From: Manuel.A.Morales at williams.edu (Manuel A. Morales)
Date: Tue, 09 Dec 2003 09:48:09 -0500
Subject: [R] PROC MIXED vs. lme()
Message-ID: <002501c3be63$76d312d0$6432a8c0@solidago>

I'm trying to learn how to do a repeated measures ANOVA in R using lme().

A data set that comes from the book Design and Analysis has the following
structure: Measurements (DV) were taken on 8 subjects (SUB) with two
experimental levels (GROUP) at four times (TRIAL).

In SAS, I use the code:

PROC MIXED DATA=[data set below];
  CLASS sub group trial;
  MODEL dv = group trial group*trial;
  REPEATED trial / SUBJECT=sub TYPE=CS;
run;

which gives the results:

Tests of Fixed Effects

Source        NDF   DDF  Type III F  Pr > F
GROUP           1     6        2.51  0.1645
TRIAL           3    18       22.34  0.0001
GROUP*TRIAL     3    18        0.58  0.6380

In R, I'm trying the code:

results.cs <- lme(DV ~ factor(GROUP)*factor(TRIAL), data=[data set below],
random= ~factor(TRIAL)|SUB, correlation=corCompSymm() )
anova(results.cs)

which gives the results:

                            numDF denDF  F-value p-value
(Intercept)                     1    18 3383.953  <.0001
factor(GROUP)                   1     6    4.887  0.0691
factor(TRIAL)                   3    18  239.102  <.0001
factor(GROUP):factor(TRIAL)     3    18    1.283  0.3103

Why are these results different? I'm a newbie to R, have the book "Mixed
Effects Models in S and S-Plus", but can't seem to get this analysis to
work. Any suggestions?

Thanks!

Manuel

Data:
SUB	GROUP	DV	TRIAL
1	1	3	1
1	1	4	2
1	1	7	3
1	1	3	4
2	1	6	1
2	1	8	2
2	1	12	3
2	1	9	4
3	1	7	1
3	1	13	2
3	1	11	3
3	1	11	4
4	1	0	1
4	1	3	2
4	1	6	3
4	1	6	4
5	2	5	1
5	2	6	2
5	2	11	3
5	2	7	4
6	2	10	1
6	2	12	2
6	2	18	3
6	2	15	4
7	2	10	1
7	2	15	2
7	2	15	3
7	2	14	4
8	2	5	1
8	2	7	2
8	2	11	3
8	2	9	4



From andy_liaw at merck.com  Tue Dec  9 16:26:33 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 9 Dec 2003 10:26:33 -0500
Subject: [R] levelplot parameters
Message-ID: <3A822319EB35174CA3714066D590DCD50205CF1A@usrymx25.merck.com>

See the "scales" argument in ?xyplot, which has:

  scales: list determining how the x- and y-axes (tick marks and
          labels) are drawn. The list contains parameters in name=value
          form, and may also contain two other lists called 'x' and 'y'
          of the same form (described below). Components of 'x' and 'y'
          affect the respective axes only, while those in 'scales'
          affect both. (When parameters are specified in both lists,
          the values in 'x' or 'y' are used.) The components are :
[...]
          cex: factor to control character sizes for axis labels. Can
          be a vector of length 2, to control left/bottom and right/top
          separately.

HTH,
Andy
 

> -----Original Message-----
> From: r-help-bounces+andy_liaw=merck.com at stat.math.ethz.ch 
> [mailto:r-help-bounces+andy_liaw=merck.com at stat.math.ethz.ch] 
> On Behalf Of Joerg Schaber
> Sent: Tuesday, December 09, 2003 7:12 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] levelplot parameters
> 
> 
> Hi,
> 
> I have a levelplot with one panel. I just can't find out how I can 
> manipulate the size of the axis lables. e.g. scales.cex 
> doesn't work,  
> the usual par-parameters either.
> Any hint?
> 
> joerg
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
>



From hb at maths.lth.se  Tue Dec  9 16:27:56 2003
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Tue, 9 Dec 2003 16:27:56 +0100
Subject: [R] Importing TIFF files into a R matrix
In-Reply-To: <1644C21C399A7B4EBFD7E5224209EFE309A3BC@EXCHG-VS1.risoe.dk>
Message-ID: <000101c3be69$0562cb30$e502eb82@maths.lth.se>

Hi, 

I do not know of any free TIFF readers for R, so I suggest that you
use an external TIFF-to-Portable Pixmap coverter and then use the
pixmap package available on CRAN. I recommend ImageMagick's convert
program available for Unix, Linux, Windows, Windows/Cygwin etc at
http://www.imagemagick.org/. 

(If you have one already installed, be careful not to work with an old
version; I ran into a problem convert 16-bits TIFF with a 2 years old
convert and it made it only into 8-bit images without warnings. That
should not be a problem now.)

The Portable Pixmap format includes i) RGB images (PPM), gray scale
images (PGM) and monochrome images (PBM). In your case (I assume)
you're working with 16-bits grayscale TIFF images so you should
convert to PGM.

If you have your PATH setup correctly an example would then be:

> library(pixmap)
> system("convert foo.tiff foo.pgm")
> img <- read.pnm("foo.pgm")

and then work from there.

Hope this helps...

Henrik Bengtsson

Dept. of Mathematical Statistics @ Centre for Mathematical Sciences
Lund Institute of Technology/Lund University, Sweden 
(Sweden +1h UTC, Melbourne +11 UTC, Calif. -8h UTC)
+46 708 909208 (cell), +46 46 320 820 (home), 
+1 (508) 464 6644 (global fax),
+46 46 2229611 (off), +46 46 2224623 (dept. fax)
h b @ m a t h s . l t h . s e, http://www.maths.lth.se/~hb/



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> peter.hagedorn at risoe.dk
> Sent: den 9 december 2003 15:16
> To: r-help at stat.math.ethz.ch
> Subject: [R] Importing TIFF files into a R matrix
> 
> 
> Hi
> 
> I am facing a problem where I would like to import a TIFF 
> image (of spots on a nylon filter) into R (into a matrix for 
> example). When plotting the matrix using fx. scatterplot3d I 
> would then be able to see how the pixel-intensities are 
> distributed in "spot-areas" on the filter - which would be 
> very helpful.
> 
> Does anynone know of a way to do this?
> 
> Best regards,
> 
> Peter Hagedorn
> 
> ...
> Peter Hagedorn
> 
> Ris? National Laboratory
> Plant Research Department
> Building PRD-330
> P.O. Box 49
> Frederiksborgvej 399
> DK-4000 Roskilde 
> Denmark
> 
> Phone +45 4677 4293
> Fax     +45 4677 4109
> e-mail  peter.hagedorn at risoe.dk
> web     http://www.risoe.dk/pbk/staff_uk/phah.htm
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailma> n/listinfo/r-help
> 
>



From hec.villafuerte at telgua.com.gt  Tue Dec  9 18:39:19 2003
From: hec.villafuerte at telgua.com.gt (=?ISO-8859-1?Q?=22H=E9ctor_Villafuerte_D=2E=22?=)
Date: Tue, 09 Dec 2003 09:39:19 -0800
Subject: [R] Interfacing R and Python in MS Windows
In-Reply-To: <3FD57AD2.2060500@statistik.uni-dortmund.de>
References: <3FD5355C.6090503@telgua.com.gt>
	<3FD57AD2.2060500@statistik.uni-dortmund.de>
Message-ID: <3FD608C7.5040402@telgua.com.gt>

Uwe Ligges wrote:

> H?ctor Villafuerte D. wrote:
>
>> Hi all,
>> I need the power of R from within some of my Python programs...
>> I use debian linux (woody) at home and windows XP at work (the
>> latter is where I need to get things done!)
>>
>> This are my packages:
>> R 1.8.0
>> Python 2.3
>> RSPython 0.5-3
>>
>> This is what I've done:
>> (1) Since the Windows Binary of RSPython is compiled against
>> Python 2.2 I downloaded the tarball
>> (2) Followed the instructions in INSTALL.win (with pexports and
>> everything)
>> (3) In the RGUI "Install package(s) from local zip files..."
>
>
> If you have downloaded the tarball of RSPython, you have to install 
> the package using Rcmd INSTALL, and you cannot use the RGUI which is 
> designed to install binary packages.
>
> Uwe Ligges
>
>
>> (4) NO errors reported during this process
>> (5) When I try to "Load package" in R it show this error:
>>  > local({pkg <- select.list(sort(.packages(all.available = TRUE)))
>> + if(nchar(pkg)) library(pkg, character.only=TRUE)})
>> Error in testRversion(descfields) : This package has not been 
>> installed properly
>> See the Note in ?library
>>
>> (6) In Python
>>  >>> import RS
>> Traceback (most recent call last):
>>  File "<stdin>", line 1, in ?
>> ImportError: No module named RS
>>
>> Please help me to get this excelent tools going on in Windows.
>> Thanks in advance,
>> Hector 
>

Thanks for your help guys.
Tim: I had already seen RPy; but its Windows binary is compiled against
Python 2.2 (and I have 2.3) so it didn't work.

Uwe:
(1) I installed Active Perl (it seems to be needed by Rcmd INSTALL)
(2) I then created a tar.gz with the modifications found in INSTALL.WIN
(3) Here's what I got:

E:\to_do>Rcmd INSTALL -c e:/to_do/RSPython.tar.gz

---------- Making package RSPython ------------

   **********************************************
   WARNING: this package has a configure script
         It probably needs manual configuration
   **********************************************

  installing inst files
A package must contain a DESCRIPTION file
make[1]: *** [frontmatter] Error 27
make: *** [pkg-RSPython] Error 2
*** Installation of RSPython failed ***
make --no-print-directory DLLNM= RHOME=C:/PROGRA~1/R/rw1080 BUILD=MINGW \
  -C E:/to_do/R.INSTALL/RSPython PKG=RSPython -f 
C:/PROGRA~1/R/rw1080/src/gnuwin32/MakePkg clean
make: *** [pkgclean-RSPython] Error 255

What's a description file? Any suggestions compiling this?
Thanks again in advance,
Hector



From HStevens at muohio.edu  Tue Dec  9 16:40:21 2003
From: HStevens at muohio.edu (Hank Stevens)
Date: Tue, 09 Dec 2003 10:40:21 -0500
Subject: [R] axes that meet
In-Reply-To: <OF22CFE98C.A5AFD520-ON85256DF7.0044632E-85256DF7.004481AF@
	hgsi.com>
Message-ID: <5.1.0.14.2.20031209103839.0186e3a0@po.muohio.edu>

Thank you to all who replied.
par(bty) and par(xaxs, yaxs) gave me all I need (and more!) for specifiying 
axes in the fashion required.
Thanks,
Hank
At 07:28 AM 12/9/2003, partha_bagchi at hgsi.com wrote:
>It might also mean that he is looking for par(xaxs = "i", yaxs = "i")
>
>
>
>
>
>"Pikounis, Bill" <v_bill_pikounis at merck.com>
>Sent by: r-help-bounces at stat.math.ethz.ch
>12/09/2003 05:23 AM
>
>
>         To:     "'Hank Stevens'" <HStevens at muohio.edu>, 
> r-help at stat.math.ethz.ch
>         cc:
>         Subject:        RE: [R] axes that meet
>
>
>Hank,
>I think the graphical parameter you are looking for is "bty", as in
>
>par(bty="l")
>
>Details on the ?par help topic.
>
>Hope that helps.
>
>Bill
>
>----------------------------------------
>Bill Pikounis, Ph.D.
>
>Biometrics Research Department
>Merck Research Laboratories
>PO Box 2000, MailDrop RY33-300
>126 E. Lincoln Avenue
>Rahway, New Jersey 07065-0900
>USA
>
>Phone: 732 594 3913
>Fax: 732 594 1565
>
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Hank Stevens
> > Sent: Tuesday, December 09, 2003 1:18 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] axes that meet
> >
> >
> > R v. 1.7.1, Windows 2000.
> > A particular journal wants me to provide scatter plots with
> > no box, but
> > with axes that meet in the lower left corner. It seems as
> > though there must
> > be an easy way of doing this, but my reading the help on
> > plot.default,
> > axis, and box have not provided any clues. I would be most
> > appreciative of
> > any feedback.
> > Thank you,
> > Hank Stevens
> >
> > Dr. Martin Henry H. Stevens, Assistant Professor
> > 338 Pearson Hall
> > Botany Department
> > Miami University
> > Oxford, OH 45056
> >
> > Office: (513) 529-4206
> > Lab: (513) 529-4262
> > FAX: (513) 529-4243
> > http://www.cas.muohio.edu/botany/bot/henry.html
> > http://www.muohio.edu/ecology
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
> >
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>--
>This message has been scanned for viruses and
>dangerous content by MailScanner, and is
>believed to be clean.

Dr. Martin Henry H. Stevens, Assistant Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/botany/bot/henry.html
http://www.muohio.edu/ecology



From p.dalgaard at biostat.ku.dk  Tue Dec  9 16:40:14 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 09 Dec 2003 16:40:14 +0100
Subject: [R] Font problem
In-Reply-To: <1070979789.16868.40.camel@localhost.localdomain>
References: <200312082312.05522.jwdougherty@mcihispeed.net>
	<x2iskqgsij.fsf@biostat.ku.dk>
	<1070977348.16868.15.camel@localhost.localdomain>
	<x2ad62gk44.fsf@biostat.ku.dk>
	<1070979789.16868.40.camel@localhost.localdomain>
Message-ID: <x21xregftt.fsf@biostat.ku.dk>

Marc Schwartz <MSchwartz at MedAnalytics.com> writes:

> It may be as simple as being sure that both the 75 dpi and 100 dpi fonts
> are loaded as the SUSE query earlier this year seemed to indicate that
> the 100 dpi fonts were not installed initially.

I actually wrote the bug so I'm supposed to know what it is about....
It is just that it was so difficult to write that I have been
reluctant to try and fix it.

The exact issue is that the X11 driver jumps through a few hoops to
help you get the real Adobe-designed fonts rather that some ugly
rescaled ones. These exist in pixel sizes

8,10,11,12,14,17,18,20,24,25,34 

(of which 10/11 and 24/25 are actually identical).

If you use unscaled fonts, the logic inside RLoadFont will try to give
you one of the above, by choosing the one closest to the one you
specified. If you request a 22 pixel font, the system will load the 20
pixel font. The catch is that e.g. the 20 pixel font only exists in
100 dpi

blueberry:~/>xlsfonts -fn '-adobe-helvetica-medium-r-*-*-20-*-*-*-*-*-*-*'
-adobe-helvetica-medium-r-normal--20-140-100-100-p-100-iso10646-1
-adobe-helvetica-medium-r-normal--20-140-100-100-p-100-iso8859-1

(actually, on my machine, only the 12 pixel version exists in both 100
and 75 dpi).

I.e. if you use unscaled fonts, you need to have both dpi sets
installed. With scaled fonts you don't run into this issue, but some
screen fonts may (will!) be ugly. Of course the default SuSE install
gives you only one dpi set, unscaled.

The bug is that there is a gap in the font-substitution logic so that
if you have only one set of fonts, then RLoadFont may end up returning
NULL, rather than the "fixed" font or a suitable fallback size.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From feh3k at spamcop.net  Tue Dec  9 16:41:50 2003
From: feh3k at spamcop.net (Frank E Harrell Jr)
Date: Tue, 9 Dec 2003 10:41:50 -0500
Subject: [R] Design functions after Multiple Imputation
In-Reply-To: <BAY8-F30ToukzUMObBW000077fc@hotmail.com>
References: <BAY8-F30ToukzUMObBW000077fc@hotmail.com>
Message-ID: <20031209104150.3b32b709.feh3k@spamcop.net>

On Mon, 08 Dec 2003 21:23:40 +0000
"Umberto Maggiore" <umberto_maggiore at hotmail.com> wrote:

> I am a new user of R for Windows, enthusiast about the many functions
> of the Design and Hmisc libraries.
> I combined the results of a Cox regression model after multiple
> imputation(of missing values in some covariates).
> Now I got my vector of coefficients (and of standard errors).
> My question is: How could I use directly that vector to run programs
> such as 'nomogram', 'calibrate', 'validate.cph' which, in contrast, call
> for the saved results form 'cph' ?
> I did not use 'aregImpute' for multiple imputation. However, even if
> I did it, 'fit.mult.impute'  seems not to allow specifying the option
> 'surv=TRUE' (essential to get a nomogram) or 'x=TRUE, y=TRUE' (which
> are essential for 'calibrate' and 'validate.cph'. Therefore, I dont't
> see how I could get a nomogram or run other Design functions after
> 'aregImpute'.
> 
> thank you so much in advance
> Umberto

Good questions Umberto.  The Design package handles multiple imputation
and model validation, but currently not at the same time.  But model
descriptions such as those provided by summary.Design, nomogram.Design,
contrast.Design are fully operational after the variance-covariance matrix
is corrected for multiple imputation by fit.mult.impute.  There is one
caveat.  When the nomogram not only has predicted relative hazards but
absolute survival estimates (probabilities, quantiles, restricted mean
life), fit.mult.impute gets the baseline survival estimates from surv=T
from the first imputation.  We need to extend that to average baseline
survival estimates over all multiple imputations.

You should have had no difficulty using nomogram after fit.mult.impute
(after aregImpute); only fully trust the relative hazard estimates from
the resulting nomogram.

I have put out a note on the IMPUTE e-mail list (see
http://hesweb1.med.virginia.edu/biostat/rms for subscription information)
about how to develop a reasonable algorithm for simultaneous multiple
imputation and bootstrap model validation/calibration.  So far no takers.

Frank

---
Frank E Harrell Jr    Professor and Chair            School of Medicine
                      Department of Biostatistics    Vanderbilt University



From jwdougherty at mcihispeed.net  Tue Dec  9 17:04:34 2003
From: jwdougherty at mcihispeed.net (John Dougherty)
Date: Tue, 9 Dec 2003 08:04:34 -0800
Subject: [R] p-value from chisq.test working strangely on 1.8.1
In-Reply-To: <1070978100.16868.26.camel@localhost.localdomain>
References: <34D43DCE-29FB-11D8-9DB9-000A956845CE@duke.edu>
	<Pine.LNX.4.51.0312090919410.31424@artemis.imbe.med.uni-erlangen.de>
	<1070978100.16868.26.camel@localhost.localdomain>
Message-ID: <200312090804.34315.jwdougherty@mcihispeed.net>

It happens with suse 9.0 as well.

...................................................................................................................
chisq.test(matrix(c(0, 1, 1, 12556), 2, 2), simulate.p.value=TRUE)

        Pearson's Chi-squared test with simulated p-value (based on 2000
        replicates)

data:  matrix(c(0, 1, 1, 12556), 2, 2)
X-squared = 1e-04, df = NA, p-value = 5e-04
.....................................................................................................................

JWDougherty



From Rau at demogr.mpg.de  Tue Dec  9 16:58:30 2003
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Tue, 9 Dec 2003 16:58:30 +0100
Subject: [R] FW: Symposium COMPSTAT 2004
Message-ID: <3699CDBC4ED5D511BE6400306E1C0D81030A0547@hermes.demogr.mpg.de>

Dear all,

I just received the following message, and I think it might be of interest
to the R-list.

Cordially,
Roland


> -> all people interested in COMPSTAT 2004 Symposium
> 
>                                            Prague December 8, 2004
> 
> Dear colleague,
> 
> thank you very much for your interest in COMPSTAT 2004 Symposium. This
> is to remind you that the deadline for delivering your manuscript in
> electronic form to the hands of the editors is approaching very fast.
> 
> Due to a series of question to us concerning the selection of papers
> for proceedings, we repeat (especially for those who participated to
> the previous COMPSTAT's) that the choice will be done based on
> complete papers and not on abstracts as in the past.
> 
> Submission of Papers for inclusion in the Proceedings Volume (subject
> to selection by SPC) is February 2, 2004 !!! For details see:
> 
>              http://www.compstat2004.cuni.cz
> 
> Please, be in time.
> Thanks a lot in advance
> 
> Looking forward seeing you next year for COMPSTAT 2004 in Prague
> 
> We remain sincerely yours
> LOC + editors
> 
> Please, forgive us if you have received this e-mail more than ones due
> to a duplication in different databases.
> 
> 
> PS Most frequently asked questions.
> 
> 1) I HAVE PARTICIPATED TO SEVERAL PREVIOUS COMPSTAT SYMPOSIA. WHAT IS
>    THE MOST IMPORTANT ORGANIZATIONAL CHANGE?
> 
>    During the previous COMPSTAT's the selection of papers and their
>    division as short and standard contributed papers has been based on
>    the submitted two page long abstract. This scheme has been
>    abandoned for COMPSTAT 2004. Instead, the participants are asked to
>    submit the full papers (up to 8 pages) not just an abstract. The
>    SPC will make a selection of the best of these for inclusion in the
>    published Proceedings Volume. These will be reviewed and subject to
>    revision before publication. The other papers that are deemed to be
>    acceptable will be, after reviewing, included on a CD that will
>    form the integral part of the Proceedings Volume. The time given
>    for the presentation may reflect the status of the paper as
>    assessed by the SPC - the allocation will depend on the time
>    pressures in putting together the final programme. In the case that
>    it will not be possible to schedule all oral contributions, SPC has
>    the right to only offer a poster presentation to some of them.
> 
> 2) WHY SHOULD I SUBMIT ALSO THE ABSTRACT IF MY PAPER WILL BE AVAILABLE
>    DURING THE SYMPOSIUM?
> 
>    The abstracts should inform the participants about the content of
>    his/her lecture. All abstract will be included in the Abstracts
>    Volume produced by the LOC, distributed during the symposium and
>    mounted before the symposium on its web page. Notice also that not
>    all people have with them always a notebook to check abstracts on
>    the CD. Both SPC and LOC hope it will be more easy for the
>    participants to orient oneself and to chose the session where to
>    go. Latest day for the authors by which abstracts of submitted
>    contributions and invited papers must be delivered to the LOC in
>    Prague in electronic format is May 1, 2004!!!
> 
> 3) WHAT IS THE LATEST DAY FOR DELIVERING MANUSCRIPTS?
> 
>    Latest day for the authors by which complete manuscripts of
>    submitted contributions and invited papers must be delivered to the
>    LOC in Prague in electronic format is February 2, 2004!!!


+++++
This mail has been sent through the MPI for Demographic Research.  Should you receive a mail that is apparently from a MPI user without this text displayed, then the address has most likely been faked.   If you are uncertain about the validity of this message, please check the mail header or ask your system administrator for assistance.



From ryszard.czerminski at pharma.novartis.com  Tue Dec  9 17:15:56 2003
From: ryszard.czerminski at pharma.novartis.com (ryszard.czerminski@pharma.novartis.com)
Date: Tue, 9 Dec 2003 11:15:56 -0500
Subject: [R] problem with pls(x, y, ..., ncomp = 16): Error in inherits(x,
 "data.frame") : subscript out of bounds
Message-ID: <OF0F04CF5C.00D9EF28-ON85256DF7.0058D3A3-85256DF7.00597366@EU.novartis.net>

When I try to use ncomp parameter in pls procedure  I get following error:

> library(pls.pcr)
> m <- pls(x, y, validation = "CV", niter = 68, ncomp = 16)
Error in inherits(x, "data.frame") : subscript out of bounds

Without ncomp parameter everything seems to work OK

> dim(x)
[1]  68 116
> dim(y)
[1] 68  1
> m <- pls(x, y, validation = "CV", niter = 68)
> length(m$ncomp)
[1] 67

Ryszard



From jeffrey.chang at duke.edu  Tue Dec  9 17:21:26 2003
From: jeffrey.chang at duke.edu (Jeffrey Chang)
Date: Tue, 9 Dec 2003 11:21:26 -0500
Subject: [R] p-value from chisq.test working strangely on 1.8.1
In-Reply-To: <x265gqgipo.fsf@biostat.ku.dk>
References: <34D43DCE-29FB-11D8-9DB9-000A956845CE@duke.edu>
	<Pine.LNX.4.51.0312090919410.31424@artemis.imbe.med.uni-erlangen.de>
	<1070978100.16868.26.camel@localhost.localdomain>
	<x265gqgipo.fsf@biostat.ku.dk>
Message-ID: <BD1772CA-2A63-11D8-830C-000A956845CE@duke.edu>


On Dec 9, 2003, at 9:37 AM, Peter Dalgaard wrote:

> Marc Schwartz <MSchwartz at medanalytics.com> writes:
>
>> Confirmed on Fedora Core 1 with R Version 1.8.1 Patched (2003-12-07)
>> compiled using "gcc (GCC) 3.3.2 20031107 (Red Hat Linux 3.3.2-2)".
>>
>>
>>> chisq.test(matrix(c(0, 1, 1, 12555), 2, 2), simulate.p.value=TRUE)
>> ...
>> X-squared = 1e-04, df = NA, p-value = 1
>>
>>> chisq.test(matrix(c(0, 1, 1, 12556), 2, 2), simulate.p.value=TRUE)
>>
>> X-squared = 1e-04, df = NA, p-value = < 2.2e-16
>> ...
>>> chisq.test(matrix(c(0, 1, 1, 12557), 2, 2), simulate.p.value=TRUE)
>> ...
>> X-squared = 1e-04, df = NA, p-value = 1
>
> Ditto on RH8 with Martyn's RPM of 1.8.0 (yeah, I know...) and ditto
> with a reasonably current r-devel (gcc 3.2)
>
> Anyways, it is yet another fudge-factor issue: If you debug to the
> point in chisq.test where it calculates
>
> PVAL <- sum(tmp$results >= STATISTIC)/B
>
> you'll find that
>
> Browse[1]> any(diff(tmp$result))
> [1] FALSE
> Browse[1]> tmp$result[1]
> [1] 7.96432e-05
> Browse[1]> STATISTIC
> [1] 7.96432e-05
> Browse[1]> tmp$result[1] - STATISTIC
> [1] -1.355253e-20
>
> so PVAL becomes zero and yaddayaddayadda....
>
> The obvious fix would seem to be
>
> PVAL <- sum(tmp$results >= (1-1e-10)*STATISTIC)/B

Yes, this is also the behavior that I am seeing.  I do not know about 
numerical programming to comment on the fix, but it would solve the 
problem in my case.

Thanks to everyone who has looked into this!

Jeff



From ggrothendieck at myway.com  Tue Dec  9 17:41:34 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue,  9 Dec 2003 11:41:34 -0500 (EST)
Subject: [R] Matrix to dates
Message-ID: <20031209164134.6C99D39AE@mprdmxin.myway.com>



z <- matrix( c(1960,1960,1961,1,9,6), 3, 2 )

1. Using chron:

 require(chron)
 chron( paste( z[,2], 1, z[,1], sep="/" ) )

2. Using POSIXct:

 ISOdate( z[,1], z[,2], 1 )          # relative to GMT time zone

or

 ISOdate( z[,1], z[,2], 1, tz="" )   # relative to current time zone



---
Date: Mon, 8 Dec 2003 15:48:20 -0600 
From: Erin Hodgess <hodgess at gator.uhd.edu>
To: <r-help at stat.math.ethz.ch> 
Subject: [R] Matrix to dates 

 
 
Let's try again!

I have a matrix in which the first column is a four digit year, and the 
second column is a 2 digit month.

How do I convert the matrix to a date function, please?

Thanks,
Erin
Version 1.8.0
mailto: hodgess at gator.uhd.edu



From Roger.Bivand at nhh.no  Tue Dec  9 18:07:36 2003
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue, 9 Dec 2003 18:07:36 +0100 (CET)
Subject: [R] R: the spdep package
In-Reply-To: <5.1.0.14.0.20031209110016.00acf1c8@joyx.joensuu.fi>
Message-ID: <Pine.LNX.4.44.0312091757060.12090-100000@reclus.nhh.no>

On Tue, 9 Dec 2003, Osmo Kolehmainen wrote:

> Hi,
> Here is a listw object z corresponding to the matrix W. I understand n, nn, 
> S0, S1 and S2 in the weights constants summary. Is it simply so that n1 = 
> n-1, n2 = n-2 and n3 = n-3? If this is true where they are needed?

Yes, used in calculating the variance of the statistics (Moran, Geary)  
elsewhere, are returned by spweights.constants(). Will be hidden in
summary.listw() in next release.

Roger

> Just wondering
> Osmo
> 
>  > W
> [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]
> [1,] 0 1 0 1 0 0 0 0 0
> [2,] 1 0 1 0 1 0 0 0 0
> [3,] 0 1 0 0 0 1 0 0 0
> [4,] 1 0 0 0 1 0 1 0 0
> [5,] 0 1 0 1 0 1 0 1 0
> [6,] 0 0 1 0 1 0 0 0 1
> [7,] 0 0 0 1 0 0 0 1 0
> [8,] 0 0 0 0 1 0 1 0 1
> [9,] 0 0 0 0 0 1 0 1 0
>  > z=mat2listw(W);z
> Characteristics of weights list object:
> Neighbour list object:
> Number of regions: 9
> Number of nonzero links: 24
> Percentage nonzero weights: 29.62963
> Average number of links: 2.666667
> Weights style: M
> Weights constants summary:
> n n1 n2 n3 nn S0 S1 S2
> M 9 8 7 6 81 24 48 272
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From Benjamin.STABLER at odot.state.or.us  Tue Dec  9 18:08:58 2003
From: Benjamin.STABLER at odot.state.or.us (Benjamin.STABLER@odot.state.or.us)
Date: Tue, 9 Dec 2003 09:08:58 -0800 
Subject: [R] Windows Memory Issues
Message-ID: <76A000A82289D411952F001083F9DD06047FE37D@exsalem4-bu.odot.state.or.us>

I would also like some clarification about R memory management.  Like Doug,
I didn't find anything about consecutive calls to gc() to free more memory.
We run into memory limit problems every now and then and a better
understanding of R's memory management would go a long way.  I am interested
in learning more and was wondering if there is any specific R documentation
that explains R's memory usage?  Or maybe some good links about memory and
garbage collection.  Thanks.

Benjamin Stabler
Transportation Planning Analysis Unit
Oregon Department of Transportation
555 13th Street NE, Suite 2
Salem, OR 97301  Ph: 503-986-4104

-------------------------------------------

Message: 21
Date: Mon, 8 Dec 2003 09:51:12 -0800 (PST)
From: Douglas Grove <dgrove at fhcrc.org>
Subject: Re: [R] Windows Memory Issues
To: Prof Brian Ripley <ripley at stats.ox.ac.uk>
Cc: r-help at stat.math.ethz.ch
Message-ID:
	<Pine.LNX.4.44.0312080921260.27288-100000 at echidna.fhcrc.org>
Content-Type: TEXT/PLAIN; charset=US-ASCII

On Sat, 6 Dec 2003, Prof Brian Ripley wrote:

> I think you misunderstand how R uses memory.  gc() does not free up all 
> the memory used for the objects it frees, and repeated calls will free 
> more.  Don't speculate about how memory management works: do your 
> homework!

Are you saying that consecutive calls to gc() will free more memory than
a single call, or am I misunderstanding?   Reading ?gc and ?Memory I don't
see anything about this mentioned.  Where should I be looking to find 
more comprehensive info on R's memory management??  I'm not writing any
packages, just would like to have a better handle on efficiently using
memory as it is usually the limiting factor with R.  FYI, I'm running
R1.8.1 and RedHat9 on a P4 with 2GB of RAM in case there is any platform
specific info that may be applicable.

Thanks,

Doug Grove
Statistical Research Associate
Fred Hutchinson Cancer Research Center


> In any case, you are using an outdated version of R, and your first
> course of action should be to compile up R-devel and try that, as there 
> has been improvements to memory management under Windows.  You could also 
> try compiling using the native malloc (and that *is* described in the 
> INSTALL file) as that has different compromises.
> 
> 
> On Sat, 6 Dec 2003, Richard Pugh wrote:
> 
> > Hi all,
> >  
> > I am currently building an application based on R 1.7.1 (+ compiled
> > C/C++ code + MySql + VB).  I am building this application to work on 2
> > different platforms (Windows XP Professional (500mb memory) and Windows
> > NT 4.0 with service pack 6 (1gb memory)).  This is a very memory
> > intensive application performing sophisticated operations on "large"
> > matrices (typically 5000x1500 matrices).
> >  
> > I have run into some issues regarding the way R handles its memory,
> > especially on NT.  In particular, R does not seem able to recollect some
> > of the memory used following the creation and manipulation of large data
> > objects.  For example, I have a function which receives a (large)
> > numeric matrix, matches against more data (maybe imported from MySql)
> > and returns a large list structure for further analysis.  A typical call
> > may look like this .
> >  
> > > myInputData <- matrix(sample(1:100, 7500000, T), nrow=5000)
> > > myPortfolio <- createPortfolio(myInputData)
> >  
> > It seems I can only repeat this code process 2/3 times before I have to
> > restart R (to get the memory back).  I use the same object names
> > (myInputData and myPortfolio) each time, so I am not create more large
> > objects ..
> >  
> > I think the problems I have are illustrated with the following example
> > from a small R session .
> >  
> > > # Memory usage for Rui process = 19,800
> > > testData <- matrix(rnorm(10000000), 1000) # Create big matrix
> > > # Memory usage for Rgui process = 254,550k
> > > rm(testData)
> > > # Memory usage for Rgui process = 254,550k
> > > gc()
> >          used (Mb) gc trigger  (Mb)
> > Ncells 369277  9.9     667722  17.9
> > Vcells  87650  0.7   24286664 185.3
> > > # Memory usage for Rgui process = 20,200k
> >  
> > In the above code, R cannot recollect all memory used, so the memory
> > usage increases from 19.8k to 20.2.  However, the following example is
> > more typical of the environments I use .
> >  
> > > # Memory 128,100k
> > > myTestData <- matrix(rnorm(10000000), 1000)
> > > # Memory 357,272k
> > > rm(myTestData)
> > > # Memory 357,272k
> > > gc()
> >           used (Mb) gc trigger  (Mb)
> > Ncells  478197 12.8     818163  21.9
> > Vcells 9309525 71.1   31670210 241.7
> > > # Memory 279,152k
> >  
> > Here, the memory usage increases from 128.1k to 279.1k
> >  
> > Could anyone point out what I could do to rectify this (if anything), or
> > generally what strategy I could take to improve this?
> >  
> > Many thanks,
> > Rich.
> >  
> > Mango Solutions
> > Tel : (01628) 418134
> > Mob : (07967) 808091
> >  
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > 
> > 
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From kreil at ebi.ac.uk  Tue Dec  9 18:14:07 2003
From: kreil at ebi.ac.uk (David Kreil)
Date: Tue, 09 Dec 2003 17:14:07 +0000
Subject: [R] How to append to a data.frame? 
In-Reply-To: Your message of "09 Dec 2003 11:58:21 +0100."
	<x2n0a2gsvm.fsf@biostat.ku.dk> 
Message-ID: <200312091714.hB9HE7e27022@puffin.ebi.ac.uk>


Dear Peter Dalgaard,

Thank you for these examples, they are very neat!
I really like the
  data.frame(x=as.numeric(NA),y=factor(NA))[rep(NA,1000),]
trick.

With best regards,

David.


------------------------------------------------------------------------
Dr David Philip Kreil                 ("`-''-/").___..--''"`-._
Research Fellow                        `6_ 6  )   `-.  (     ).`-.__.`)
University of Cambridge                (_Y_.)'  ._   )  `._ `. ``-..-'
++44 1223 764107, fax 333992         _..`--'_..-_/  /--'_.' ,'
www.inference.phy.cam.ac.uk/dpk20   (il),-''  (li),'  ((!.-'



From ripley at stats.ox.ac.uk  Tue Dec  9 18:29:14 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 9 Dec 2003 17:29:14 +0000 (GMT)
Subject: [R] Windows Memory Issues
In-Reply-To: <76A000A82289D411952F001083F9DD06047FE37D@exsalem4-bu.odot.state.or.us>
Message-ID: <Pine.LNX.4.44.0312091724180.9098-100000@gannet.stats>

On Tue, 9 Dec 2003 Benjamin.STABLER at odot.state.or.us wrote:

> I would also like some clarification about R memory management.  Like Doug,
> I didn't find anything about consecutive calls to gc() to free more memory.

It was a statement about Windows, and about freeing memory *to Windows*.
Douglas Grove apparently had misread both the subject line and the 
sentence.

> We run into memory limit problems every now and then and a better
> understanding of R's memory management would go a long way.  I am interested
> in learning more and was wondering if there is any specific R documentation
> that explains R's memory usage?  Or maybe some good links about memory and
> garbage collection.  Thanks.

There are lots of comments in the source files.  And as I already said 
(but has been excised below), this is not relevant to the next version of 
R anyway.

BTW, the message below has been selectively edited, so please consult the 
original.

> Message: 21
> Date: Mon, 8 Dec 2003 09:51:12 -0800 (PST)
> From: Douglas Grove <dgrove at fhcrc.org>
> Subject: Re: [R] Windows Memory Issues
> To: Prof Brian Ripley <ripley at stats.ox.ac.uk>
> Cc: r-help at stat.math.ethz.ch
> Message-ID:
> 	<Pine.LNX.4.44.0312080921260.27288-100000 at echidna.fhcrc.org>
> Content-Type: TEXT/PLAIN; charset=US-ASCII
> 
> On Sat, 6 Dec 2003, Prof Brian Ripley wrote:
> 
> > I think you misunderstand how R uses memory.  gc() does not free up all 
> > the memory used for the objects it frees, and repeated calls will free 
> > more.  Don't speculate about how memory management works: do your 
> > homework!
> 
> Are you saying that consecutive calls to gc() will free more memory than
> a single call, or am I misunderstanding?   Reading ?gc and ?Memory I don't
> see anything about this mentioned.  Where should I be looking to find 
> more comprehensive info on R's memory management??  I'm not writing any
> packages, just would like to have a better handle on efficiently using
> memory as it is usually the limiting factor with R.  FYI, I'm running
> R1.8.1 and RedHat9 on a P4 with 2GB of RAM in case there is any platform
> specific info that may be applicable.
> 
> Thanks,
> 
> Doug Grove
> Statistical Research Associate
> Fred Hutchinson Cancer Research Center
> 
> 
> > In any case, you are using an outdated version of R, and your first
> > course of action should be to compile up R-devel and try that, as there 
> > has been improvements to memory management under Windows.  You could also 
> > try compiling using the native malloc (and that *is* described in the 
> > INSTALL file) as that has different compromises.
> > 
> > 
> > On Sat, 6 Dec 2003, Richard Pugh wrote:
> > 
> > > Hi all,
> > >  
> > > I am currently building an application based on R 1.7.1 (+ compiled
> > > C/C++ code + MySql + VB).  I am building this application to work on 2
> > > different platforms (Windows XP Professional (500mb memory) and Windows
> > > NT 4.0 with service pack 6 (1gb memory)).  This is a very memory
> > > intensive application performing sophisticated operations on "large"
> > > matrices (typically 5000x1500 matrices).
> > >  
> > > I have run into some issues regarding the way R handles its memory,
> > > especially on NT.  In particular, R does not seem able to recollect some
> > > of the memory used following the creation and manipulation of large data
> > > objects.  For example, I have a function which receives a (large)
> > > numeric matrix, matches against more data (maybe imported from MySql)
> > > and returns a large list structure for further analysis.  A typical call
> > > may look like this .
> > >  
> > > > myInputData <- matrix(sample(1:100, 7500000, T), nrow=5000)
> > > > myPortfolio <- createPortfolio(myInputData)
> > >  
> > > It seems I can only repeat this code process 2/3 times before I have to
> > > restart R (to get the memory back).  I use the same object names
> > > (myInputData and myPortfolio) each time, so I am not create more large
> > > objects ..
> > >  
> > > I think the problems I have are illustrated with the following example
> > > from a small R session .
> > >  
> > > > # Memory usage for Rui process = 19,800
> > > > testData <- matrix(rnorm(10000000), 1000) # Create big matrix
> > > > # Memory usage for Rgui process = 254,550k
> > > > rm(testData)
> > > > # Memory usage for Rgui process = 254,550k
> > > > gc()
> > >          used (Mb) gc trigger  (Mb)
> > > Ncells 369277  9.9     667722  17.9
> > > Vcells  87650  0.7   24286664 185.3
> > > > # Memory usage for Rgui process = 20,200k
> > >  
> > > In the above code, R cannot recollect all memory used, so the memory
> > > usage increases from 19.8k to 20.2.  However, the following example is
> > > more typical of the environments I use .
> > >  
> > > > # Memory 128,100k
> > > > myTestData <- matrix(rnorm(10000000), 1000)
> > > > # Memory 357,272k
> > > > rm(myTestData)
> > > > # Memory 357,272k
> > > > gc()
> > >           used (Mb) gc trigger  (Mb)
> > > Ncells  478197 12.8     818163  21.9
> > > Vcells 9309525 71.1   31670210 241.7
> > > > # Memory 279,152k
> > >  
> > > Here, the memory usage increases from 128.1k to 279.1k
> > >  
> > > Could anyone point out what I could do to rectify this (if anything), or
> > > generally what strategy I could take to improve this?
> > >  
> > > Many thanks,
> > > Rich.
> > >  
> > > Mango Solutions
> > > Tel : (01628) 418134
> > > Mob : (07967) 808091
> > >  
> > > 
> > > 	[[alternative HTML version deleted]]
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > > 
> > > 
> > 
> > -- 
> > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865 272861 (self)
> > 1 South Parks Road,                     +44 1865 272866 (PA)
> > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From v_bill_pikounis at merck.com  Tue Dec  9 18:36:17 2003
From: v_bill_pikounis at merck.com (Pikounis, Bill)
Date: Tue, 9 Dec 2003 12:36:17 -0500
Subject: [R] Windows Memory Issues
Message-ID: <CFBD404F5E0C9547B4E10B7BDC3DFA2F04155F52@usrymx18.merck.com>


> [snipped]  Or maybe some good links 
> about memory and
> garbage collection. 

As is mentioned time-to-time on this list when the above subject comes up,
Windows memory is a complicated topic.  One open-source utility I have found
helpful to monitor memory when I work under XP is called RAMpage, authored
by John Fitzgibbon, and is available at 

http://www.jfitz.com/software/RAMpage/


In its FAQ / Help, it touches on a lot of general memory and resource
issues, which I found helpful to learn about.  

http://www.jfitz.com/software/RAMpage/RAMpage_FAQS.html

(Though the author clearly warns that its usefulness for "freeing memory"
may not be anymore than cosmetic on NT / 2000 / XP systems.)

Hope that helps.

Bill

----------------------------------------
Bill Pikounis, Ph.D.

Biometrics Research Department
Merck Research Laboratories
PO Box 2000, MailDrop RY33-300  
126 E. Lincoln Avenue
Rahway, New Jersey 07065-0900
USA

Phone: 732 594 3913
Fax: 732 594 1565


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Benjamin.STABLER at odot.state.or.us
> Sent: Tuesday, December 09, 2003 12:09 PM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] Windows Memory Issues
> 
> 
> I would also like some clarification about R memory 
> management.  Like Doug,
> I didn't find anything about consecutive calls to gc() to 
> free more memory.
> We run into memory limit problems every now and then and a better
> understanding of R's memory management would go a long way.  
> I am interested
> in learning more and was wondering if there is any specific R 
> documentation
> that explains R's memory usage?  Or maybe some good links 
> about memory and
> garbage collection.  Thanks.
> 
> Benjamin Stabler
> Transportation Planning Analysis Unit
> Oregon Department of Transportation
> 555 13th Street NE, Suite 2
> Salem, OR 97301  Ph: 503-986-4104
> 
> -------------------------------------------
> 
> Message: 21
> Date: Mon, 8 Dec 2003 09:51:12 -0800 (PST)
> From: Douglas Grove <dgrove at fhcrc.org>
> Subject: Re: [R] Windows Memory Issues
> To: Prof Brian Ripley <ripley at stats.ox.ac.uk>
> Cc: r-help at stat.math.ethz.ch
> Message-ID:
> 	<Pine.LNX.4.44.0312080921260.27288-100000 at echidna.fhcrc.org>
> Content-Type: TEXT/PLAIN; charset=US-ASCII
> 
> On Sat, 6 Dec 2003, Prof Brian Ripley wrote:
> 
> > I think you misunderstand how R uses memory.  gc() does not 
> free up all 
> > the memory used for the objects it frees, and repeated 
> calls will free 
> > more.  Don't speculate about how memory management works: do your 
> > homework!
> 
> Are you saying that consecutive calls to gc() will free more 
> memory than
> a single call, or am I misunderstanding?   Reading ?gc and 
> ?Memory I don't
> see anything about this mentioned.  Where should I be looking to find 
> more comprehensive info on R's memory management??  I'm not 
> writing any
> packages, just would like to have a better handle on efficiently using
> memory as it is usually the limiting factor with R.  FYI, I'm running
> R1.8.1 and RedHat9 on a P4 with 2GB of RAM in case there is 
> any platform
> specific info that may be applicable.
> 
> Thanks,
> 
> Doug Grove
> Statistical Research Associate
> Fred Hutchinson Cancer Research Center
> 
> 
> > In any case, you are using an outdated version of R, and your first
> > course of action should be to compile up R-devel and try 
> that, as there 
> > has been improvements to memory management under Windows.  
> You could also 
> > try compiling using the native malloc (and that *is* 
> described in the 
> > INSTALL file) as that has different compromises.
> > 
> > 
> > On Sat, 6 Dec 2003, Richard Pugh wrote:
> > 
> > > Hi all,
> > >  
> > > I am currently building an application based on R 1.7.1 
> (+ compiled
> > > C/C++ code + MySql + VB).  I am building this application 
> to work on 2
> > > different platforms (Windows XP Professional (500mb 
> memory) and Windows
> > > NT 4.0 with service pack 6 (1gb memory)).  This is a very memory
> > > intensive application performing sophisticated operations 
> on "large"
> > > matrices (typically 5000x1500 matrices).
> > >  
> > > I have run into some issues regarding the way R handles 
> its memory,
> > > especially on NT.  In particular, R does not seem able to 
> recollect some
> > > of the memory used following the creation and 
> manipulation of large data
> > > objects.  For example, I have a function which receives a (large)
> > > numeric matrix, matches against more data (maybe imported 
> from MySql)
> > > and returns a large list structure for further analysis.  
> A typical call
> > > may look like this .
> > >  
> > > > myInputData <- matrix(sample(1:100, 7500000, T), nrow=5000)
> > > > myPortfolio <- createPortfolio(myInputData)
> > >  
> > > It seems I can only repeat this code process 2/3 times 
> before I have to
> > > restart R (to get the memory back).  I use the same object names
> > > (myInputData and myPortfolio) each time, so I am not 
> create more large
> > > objects ..
> > >  
> > > I think the problems I have are illustrated with the 
> following example
> > > from a small R session .
> > >  
> > > > # Memory usage for Rui process = 19,800
> > > > testData <- matrix(rnorm(10000000), 1000) # Create big matrix
> > > > # Memory usage for Rgui process = 254,550k
> > > > rm(testData)
> > > > # Memory usage for Rgui process = 254,550k
> > > > gc()
> > >          used (Mb) gc trigger  (Mb)
> > > Ncells 369277  9.9     667722  17.9
> > > Vcells  87650  0.7   24286664 185.3
> > > > # Memory usage for Rgui process = 20,200k
> > >  
> > > In the above code, R cannot recollect all memory used, so 
> the memory
> > > usage increases from 19.8k to 20.2.  However, the 
> following example is
> > > more typical of the environments I use .
> > >  
> > > > # Memory 128,100k
> > > > myTestData <- matrix(rnorm(10000000), 1000)
> > > > # Memory 357,272k
> > > > rm(myTestData)
> > > > # Memory 357,272k
> > > > gc()
> > >           used (Mb) gc trigger  (Mb)
> > > Ncells  478197 12.8     818163  21.9
> > > Vcells 9309525 71.1   31670210 241.7
> > > > # Memory 279,152k
> > >  
> > > Here, the memory usage increases from 128.1k to 279.1k
> > >  
> > > Could anyone point out what I could do to rectify this 
> (if anything), or
> > > generally what strategy I could take to improve this?
> > >  
> > > Many thanks,
> > > Rich.
> > >  
> > > Mango Solutions
> > > Tel : (01628) 418134
> > > Mob : (07967) 808091
> > >  
> > > 
> > > 	[[alternative HTML version deleted]]
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > > 
> > > 
> > 
> > -- 
> > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865 272861 (self)
> > 1 South Parks Road,                     +44 1865 272866 (PA)
> > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
>



From andy_liaw at merck.com  Tue Dec  9 18:50:35 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 9 Dec 2003 12:50:35 -0500
Subject: [R] problem with pls(x, y, ..., ncomp = 16): Error in
	inherit s( x, "data.frame") : subscript out of bounds
Message-ID: <3A822319EB35174CA3714066D590DCD50205CF1E@usrymx25.merck.com>

I don't know the details of pls (in the pls.pcr package, I assume), but if
you use validation="CV", that says you want to use CV to select the best
number of components.  Then why would you specify ncomp as well?

Andy

> From: ryszard.czerminski at pharma.novartis.com
> 
> When I try to use ncomp parameter in pls procedure  I get 
> following error:
> 
> > library(pls.pcr)
> > m <- pls(x, y, validation = "CV", niter = 68, ncomp = 16)
> Error in inherits(x, "data.frame") : subscript out of bounds
> 
> Without ncomp parameter everything seems to work OK
> 
> > dim(x)
> [1]  68 116
> > dim(y)
> [1] 68  1
> > m <- pls(x, y, validation = "CV", niter = 68)
> > length(m$ncomp)
> [1] 67
> 
> Ryszard
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
>



From songq at stat.wisc.edu  Tue Dec  9 19:01:13 2003
From: songq at stat.wisc.edu (Qinghua Song)
Date: Tue, 9 Dec 2003 12:01:13 -0600 (CST)
Subject: [R] Contour plots
Message-ID: <Pine.LNX.4.58.0312091154510.5762@gstat304.stat.wisc.edu>

I am drawing several contour plots in one page. I use "image", and get
several contours. But I don't know how to control the color in more than one
plots. For example, same color corresponds to different dependent values
in different plots. For example, "yellow" means z=100(the highest value)
in plot1, but "yellow" means z=40(the highest value) in plot2--this is
not what I want. In fact,I want the same color corresponds the same depedent
value in each plot. Say, in all plots, "yellow" means z=100,"red" means
z=40, etc. I was wondering if someone can tell me which function I can
use. Because it seems to me that "image" function doesn't have such option
to control that.
Thank you.


-------------------------------------------------
Qinghua Song
Department of Statistics
UW-Madison
office phone:262-8181



From ligges at statistik.uni-dortmund.de  Tue Dec  9 19:03:59 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 09 Dec 2003 19:03:59 +0100
Subject: [R]: global and local variables
In-Reply-To: <3FD5C59C.38CA6754@stats.uct.ac.za>
References: <3FD5AF67.1E737976@stats.uct.ac.za>	<3FD5BAB0.5000904@statistik.uni-dortmund.de>
	<3FD5C59C.38CA6754@stats.uct.ac.za>
Message-ID: <3FD60E8F.6020705@statistik.uni-dortmund.de>

No direct way, I think. My idea:

Make preprocess() stand-alone. Then vif() and mci() require an object 
retruned by preprocess() as an argument.
Or make it object oriented and let vif() and friends decide whether its 
first argument needs no preprocessing or not.

Uwe Ligges



allan clark wrote:
>    Hi
> 
>    Thanx for those who responded to my problem. In my previous email I
>    tried to ask a general question and probably never explained myself
>    correctly.  I wanted to prevent sending this long email. My apologies.
> 
>    This is my actual problem.
> 
>    I have a regression problem. I am writing some R code in order to
>    calculate some collinearity diagnostics. The diagnostics all rely on a
>    function named preprocess. I've written the different diagnostics as
>    separate functions so that they may be evaluated separately if
>    required.
> 
>    The two functions are named mci and vif. (I will be writing some
>    others later)
> 
>    mci calculates the mixed condition index as well as the condition
>    indices of a given X matrix while
>    vif calculates the variance inflation factors of the X matrix.
> 
>    Another function named colldiag has been written. This function will
>    calculate all of the collinearity diagnostics by simply calling the
>    separate functions defined previously.
> 
>    I've attached the code of the different functions as well as a data
>    file (say a2) below.
> 
>    The functions mci and vif work perfectly.
> 
>    i.e.
> 
>    > mci(a2)
>    [1] "DATA MATRIX CENTERED AND SCALED"
>    [1] "CENTERED AND SCALED MATRIX = $data"
>    [1] "MEANS OF XDATA = $means"
>    [1] "STDS OF XDATA = $stds"
>    [1] "THE CONDITION NUMBER AND THE CONDITION INDICES"
>    $CN
>    [1] 27.34412
> 
>    $CI
>    [1]  1.000000  1.615690 27.344123
> 
>    $MCI
>      Principal.Component Singular.Values Condition.Index
>    1                   1       1.4720680        1.000000
>    2                   2       0.9111078        1.615690
>    3                   3       0.0538349       27.344123
> 
>    > vif(a2)
>    [1] "DATA MATRIX CENTERED AND SCALED"
>    [1] "CENTERED AND SCALED MATRIX = $data"
>    [1] "MEANS OF XDATA = $means"
>    [1] "STDS OF XDATA = $stds"
>    [1] "THE VARIANCE INFLATION FACTORS"
>    $vif
>          x1       x2       x3
>    169.3542 175.6667   1.6875
> 
>    The output from colldiag is as follows:
> 
>    > colldiag(a2)
>    [1] "DATA MATRIX CENTERED AND SCALED"
>    [1] "CENTERED AND SCALED MATRIX = $data"
>    [1] "MEANS OF XDATA = $means"
>    [1] "STDS OF XDATA = $stds"
>    [1] "THE CONDITION NUMBER AND THE CONDITION INDICES"
>    $CN
>    [1] 27.34412
> 
>    $CI
>    [1]  1.000000  1.615690 27.344123
> 
>    $MCI
>      Principal.Component Singular.Values Condition.Index
>    1                   1       1.4720680        1.000000
>    2                   2       0.9111078        1.615690
>    3                   3       0.0538349       27.344123
> 
>    [1] "DATA MATRIX CENTERED AND SCALED"
>    [1] "CENTERED AND SCALED MATRIX = $data"
>    [1] "MEANS OF XDATA = $means"
>    [1] "STDS OF XDATA = $stds"
>    [1] "THE VARIANCE INFLATION FACTORS"
>    $vif
>          x1       x2       x3
>    169.3542 175.6667   1.6875
> 
> 
>     
> 
>    Once you check the colldiag code below you will see that it calls mci
>    and vif. In both of these functions they call preprocess. This is
>    unnecessary. How can I write the code such that R only calls
>    preprocess once?
> 
>    ONCE AGAIN I APOLOGIZE FOR THE LENGTH OF THIS EMAIL!!!
> 
> 
>    Cheers
>    Allan
> 
> 
> 
> 
> 
>    The data file:
> 
>       x1 x2 x3
>    1  20 -4  5
>    2  21 -4  4
>    3  22 -3  3
>    4  23 -2  2
>    5  24 -1  1
>    6  25  0  2
>    7  26  1  3
>    8  27  2  4
>    9  28  3  5
>    10 29  4  6
>    11 20 -4  5
>    12 21 -4  4
>    13 22 -3  3
>    14 23 -2  2
>    15 24 -1  1
>    16 25  0  2
>    17 26  1  3
>    18 27  2  4
>    19 28  3  5
>    20 29  4  6
> 
>    preprocess<-function (xdata,center=1,scale=1)
>    {
>    if(center==1 && scale==1)
>    {
>    means<-apply(xdata,2,mean)
>    stds<-apply(xdata,2, function(x) sqrt(var(x)))
>    scalefactor<-((nrow(xdata)-1)^.5)*stds
>    data.centsca<-sweep(sweep(xdata,2,means,"-"),2,scalefactor,"/")
>    print("DATA MATRIX CENTERED AND SCALED")
>    print("CENTERED AND SCALED MATRIX = $data")
>    print("MEANS OF XDATA = $means")
>    print("STDS OF XDATA = $stds")
>    list(data=data.centsca,means=means,stds=stds,prep=1)
>    }
> 
>    else if(center==1 && scale==0)
>    {
>    means<-apply(xdata,2,mean)
>    data.cen<-sweep(xdata,2,means,"-")
>    print("DATA MATRIX CENTERED")
>    list(data=data.cen,means=means,prep=1)
>    }
> 
>    else if(center==0 && scale==1)
>    {
>    stds<-apply(xdata,2, function(x) sqrt(var(x)))
>    scalefactor<-((nrow(xdata)-1)^.5)*stds
>    data.sca<-sweep(xdata,2,scalefactor,"/")
>    print("DATA MATRIX SCALED")
>    list(data=data.sca,stds=stds,prep=1)
>    }
> 
>    else
>    {
>    print("YOU HAVE TO SPECIFY WHETHER YOU WANT TO SCALE OR CENTER THE
>    MATRIX")
>    print("THE preprocess FUNCTION HAS THREE ARGUMENTS. i.e.
>    preprocess(xdata,center,scale)")
>    print("xdata IS THE MATRIX TO BE TRANSFORMED")
>    print("TO CENTER SPECIFY center=1")
>    print("TO SCALE SPECIFY scale=1")
>    }
> 
>    # A matrix is standardised as follows:
>    # X*(i,j) = ( X(i,j)- XBAR(j) ) / (   sqrt(n-1)* STD(j)   )
> 
>    }
> 
>    mci<-function (xdata)
>    {
>    a<-preprocess(xdata)
>    b<-svd(a$data)
>    cn<-(b$d)[1]/(b$d)[ncol(a$data)]
>    ci<-(b$d)[1]/(b$d)[1:ncol(a$data)]
> 
>    #paste("THE CONDITION NUMBER = ",cn)
> 
>    #the following produces a table in order to output the mci values
>    Principal.Component<-1:ncol(a$data)
>    Singular.Values<-b$d
>    Condition.Index<-ci
>    mcitable<-data.frame(Principal.Component,Singular.Values,Condition.Ind
>    ex)
> 
>    print("THE CONDITION NUMBER AND THE CONDITION INDICES")
>    d<-list(CN=cn,CI=ci,MCI=mcitable)
>    print(d)
>    }
> 
>    vif<-function (xdata)
>    {
>    a<-preprocess(xdata)
>    vif<-diag(solve(cor(a$data)))
> 
>    print("THE VARIANCE INFLATION FACTORS")
>    b<-list(vif=vif)
>    b
>    }
> 
>    colldiag<-function (xdata)
>    {
>    mci(xdata)
>    vif(xdata)
>    }
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From William.Simpson at drdc-rddc.gc.ca  Tue Dec  9 14:08:24 2003
From: William.Simpson at drdc-rddc.gc.ca (Bill Simpson)
Date: Tue, 9 Dec 2003 08:08:24 -0500 (EST)
Subject: [R] Contour plots
In-Reply-To: <Pine.LNX.4.58.0312091154510.5762@gstat304.stat.wisc.edu>
Message-ID: <Pine.LNX.4.44.0312090805470.1977-100000@localhost.localdomain>

> I am drawing several contour plots in one page. I use "image", and get
> several contours. But I don't know how to control the color in more than one
> plots. For example, same color corresponds to different dependent values
> in different plots. For example, "yellow" means z=100(the highest value)
> in plot1, but "yellow" means z=40(the highest value) in plot2--this is
> not what I want. In fact,I want the same color corresponds the same depedent
> value in each plot. Say, in all plots, "yellow" means z=100,"red" means
> z=40, etc. I was wondering if someone can tell me which function I can
> use. Because it seems to me that "image" function doesn't have such option
> to control that.
> Thank you.

I think you need to use the zlim argument in image().

For example
image(x,y,z,zlim=c(0,100))
will make sure that all the plots are drawn using the same values for z 
(between 0 and 100 here).

Bill



From pgunn at cs.cmu.edu  Tue Dec  9 19:50:54 2003
From: pgunn at cs.cmu.edu (Pat Gunn)
Date: Tue, 9 Dec 2003 13:50:54 -0500 (EST)
Subject: [R] R Interface handholding
Message-ID: <Pine.LNX.4.33L.0312091320160.9900-100000@ux13.sp.cs.cmu.edu>

Hello,
I need a bit of handholding with R, specifically, with writing
packages for it. I'm a systems programmer, and am, on the request
of several users of our software, working on generating R interfaces.
For starters, I've written the following R function (which compiles):

SEXP myincr(SEXP Rinput)
{ // Returns input integer incremented by one
int input;
SEXP returner;

PROTECT(Rinput = AS_NUMERIC(Rinput));

input = * INTEGER(Rinput);
input++;
PROTECT(returner = NEW_INTEGER(input));
Rprintf("Hey there\n");
return returner;
}

I've made this into a package, by dropping it into a stub directory
along with something called init.c:

#include "areone.h"
#include <R_ext/Rdynload.h>
#include <Rinternals.h>

R_NativePrimitiveArgType myincr_t[1] = {INTSXP};

static const R_CMethodDef cMethods[] =
        {
        {"myincr", (DL_FUNC) &myincr, 1, myincr_t}
        };

void R_init_myincr(DllInfo* dll)
{
R_registerRoutines(dll, cMethods, NULL, NULL, NULL);
}

R is happy to install this for me, but after doing a library(myincr),
the function doesn't seem to be available, so I presume I'm missing
something. Does R normally call, at library load, R_init_$MODULENAME() ?


My other question is.. our software produces data structures (we call datsets)
which resemble limited database tables, and I'd like some advice on exposing
them to R -- columns, in our scheme, either hold doubles or strings,
the columns have names, and we need the ability to export these into
appropriate R structures as well as populate them from R. I notice that
the R DBI, at least as according to its documentation, uses the database
to hold the data (presumably in temporary tables) and returns parts of it
as requested, via R functions. I could use a static global pointer (in C) into a
storage space of datsets, and write bridge functions exporting them as R arrays,
or I could attempt to find an appropriate native format and export to it..
any advice?

It might be worth mentioning that all of the code to do this will
eventually be auto-generated -- we already have code to do this for C
that doesn't need to expose people linking our code to all of our
custom structures and stuff.

-- 
Pat Gunn
Research/Systems Programmer, Auton Group, CMU



From Benjamin.STABLER at odot.state.or.us  Tue Dec  9 19:55:28 2003
From: Benjamin.STABLER at odot.state.or.us (Benjamin.STABLER@odot.state.or.us)
Date: Tue, 9 Dec 2003 10:55:28 -0800 
Subject: [R] Windows Memory Issues
Message-ID: <76A000A82289D411952F001083F9DD06047FE37F@exsalem4-bu.odot.state.or.us>

Thanks for the reply.  So are you saying that multiple calls to gc() frees
up memory to Windows and then other processes can use that newly freed
memory?  So multiple calls to gc() does not actually make more memory
available to new R objects that I might create.  The reason I ask is because
I want to know how to use all the available memory that I can to store
object in R.  ?gc says that garbage collection is run without user
intervention so there is really nothing I can do to improve memory under
Windows except increase the --max-mem-size at startup (which is limited to
1.7GB under the current version of R for Windows and will be greater ~3GB
for R 1.9).  Thanks again.

Ben Stabler

>-----Original Message-----
>From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
>Sent: Tuesday, December 09, 2003 9:29 AM
>To: STABLER Benjamin
>Cc: r-help at stat.math.ethz.ch
>Subject: Re: [R] Windows Memory Issues
>
>
>On Tue, 9 Dec 2003 Benjamin.STABLER at odot.state.or.us wrote:
>
>> I would also like some clarification about R memory 
>management.  Like Doug,
>> I didn't find anything about consecutive calls to gc() to 
>free more memory.
>
>It was a statement about Windows, and about freeing memory *to 
>Windows*.
>Douglas Grove apparently had misread both the subject line and the 
>sentence.
>
>> We run into memory limit problems every now and then and a better
>> understanding of R's memory management would go a long way.  
>I am interested
>> in learning more and was wondering if there is any specific 
>R documentation
>> that explains R's memory usage?  Or maybe some good links 
>about memory and
>> garbage collection.  Thanks.
>
>There are lots of comments in the source files.  And as I already said 
>(but has been excised below), this is not relevant to the next 
>version of 
>R anyway.
>
>BTW, the message below has been selectively edited, so please 
>consult the 
>original.
>
>> Message: 21
>> Date: Mon, 8 Dec 2003 09:51:12 -0800 (PST)
>> From: Douglas Grove <dgrove at fhcrc.org>
>> Subject: Re: [R] Windows Memory Issues
>> To: Prof Brian Ripley <ripley at stats.ox.ac.uk>
>> Cc: r-help at stat.math.ethz.ch
>> Message-ID:
>> 	<Pine.LNX.4.44.0312080921260.27288-100000 at echidna.fhcrc.org>
>> Content-Type: TEXT/PLAIN; charset=US-ASCII
>> 
>> On Sat, 6 Dec 2003, Prof Brian Ripley wrote:
>> 
>> > I think you misunderstand how R uses memory.  gc() does 
>not free up all 
>> > the memory used for the objects it frees, and repeated 
>calls will free 
>> > more.  Don't speculate about how memory management works: do your 
>> > homework!
>> 
>> Are you saying that consecutive calls to gc() will free more 
>memory than
>> a single call, or am I misunderstanding?   Reading ?gc and 
>?Memory I don't
>> see anything about this mentioned.  Where should I be 
>looking to find 
>> more comprehensive info on R's memory management??  I'm not 
>writing any
>> packages, just would like to have a better handle on 
>efficiently using
>> memory as it is usually the limiting factor with R.  FYI, I'm running
>> R1.8.1 and RedHat9 on a P4 with 2GB of RAM in case there is 
>any platform
>> specific info that may be applicable.
>> 
>> Thanks,
>> 
>> Doug Grove
>> Statistical Research Associate
>> Fred Hutchinson Cancer Research Center
>> 
>> 
>> > In any case, you are using an outdated version of R, and your first
>> > course of action should be to compile up R-devel and try 
>that, as there 
>> > has been improvements to memory management under Windows.  
>You could also 
>> > try compiling using the native malloc (and that *is* 
>described in the 
>> > INSTALL file) as that has different compromises.
>> > 
>> > 
>> > On Sat, 6 Dec 2003, Richard Pugh wrote:
>> > 
>> > > Hi all,
>> > >  
>> > > I am currently building an application based on R 1.7.1 
>(+ compiled
>> > > C/C++ code + MySql + VB).  I am building this 
>application to work on 2
>> > > different platforms (Windows XP Professional (500mb 
>memory) and Windows
>> > > NT 4.0 with service pack 6 (1gb memory)).  This is a very memory
>> > > intensive application performing sophisticated 
>operations on "large"
>> > > matrices (typically 5000x1500 matrices).
>> > >  
>> > > I have run into some issues regarding the way R handles 
>its memory,
>> > > especially on NT.  In particular, R does not seem able 
>to recollect some
>> > > of the memory used following the creation and 
>manipulation of large data
>> > > objects.  For example, I have a function which receives a (large)
>> > > numeric matrix, matches against more data (maybe 
>imported from MySql)
>> > > and returns a large list structure for further analysis. 
> A typical call
>> > > may look like this .
>> > >  
>> > > > myInputData <- matrix(sample(1:100, 7500000, T), nrow=5000)
>> > > > myPortfolio <- createPortfolio(myInputData)
>> > >  
>> > > It seems I can only repeat this code process 2/3 times 
>before I have to
>> > > restart R (to get the memory back).  I use the same object names
>> > > (myInputData and myPortfolio) each time, so I am not 
>create more large
>> > > objects ..
>> > >  
>> > > I think the problems I have are illustrated with the 
>following example
>> > > from a small R session .
>> > >  
>> > > > # Memory usage for Rui process = 19,800
>> > > > testData <- matrix(rnorm(10000000), 1000) # Create big matrix
>> > > > # Memory usage for Rgui process = 254,550k
>> > > > rm(testData)
>> > > > # Memory usage for Rgui process = 254,550k
>> > > > gc()
>> > >          used (Mb) gc trigger  (Mb)
>> > > Ncells 369277  9.9     667722  17.9
>> > > Vcells  87650  0.7   24286664 185.3
>> > > > # Memory usage for Rgui process = 20,200k
>> > >  
>> > > In the above code, R cannot recollect all memory used, 
>so the memory
>> > > usage increases from 19.8k to 20.2.  However, the 
>following example is
>> > > more typical of the environments I use .
>> > >  
>> > > > # Memory 128,100k
>> > > > myTestData <- matrix(rnorm(10000000), 1000)
>> > > > # Memory 357,272k
>> > > > rm(myTestData)
>> > > > # Memory 357,272k
>> > > > gc()
>> > >           used (Mb) gc trigger  (Mb)
>> > > Ncells  478197 12.8     818163  21.9
>> > > Vcells 9309525 71.1   31670210 241.7
>> > > > # Memory 279,152k
>> > >  
>> > > Here, the memory usage increases from 128.1k to 279.1k
>> > >  
>> > > Could anyone point out what I could do to rectify this 
>(if anything), or
>> > > generally what strategy I could take to improve this?
>> > >  
>> > > Many thanks,
>> > > Rich.
>> > >  
>> > > Mango Solutions
>> > > Tel : (01628) 418134
>> > > Mob : (07967) 808091
>> > >  
>> > > 
>> > > 	[[alternative HTML version deleted]]
>> > > 
>> > > ______________________________________________
>> > > R-help at stat.math.ethz.ch mailing list
>> > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> > > 
>> > > 
>> > 
>> > -- 
>> > Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> > Professor of Applied Statistics,  
http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865 272861 (self)
> > 1 South Parks Road,                     +44 1865 272866 (PA)
> > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jjava at priscian.com  Wed Dec 10 03:55:44 2003
From: jjava at priscian.com (Jim Java)
Date: Tue,  9 Dec 2003 18:55:44 -0800
Subject: [R] C++: SET_LENGTH() Over Many Iterations?
Message-ID: <200312091855.AA18809036@priscian.com>

In a C++ extension to R (v 1.8.1), I've been experimenting with a
generic "push back" function to tack one value at a time onto the end
of an R vector created within the extension. After calling this
function a certain number of times Rgui.exe (I'm writing in Windows
using Visual Studio .NET 2003) will fail with an Access Violation,
which doesn't happen when I pre-allocate the R-vector memory and write
to the reserved slots; i.e., I'm not trying to create an R object too
big to be handled by R within the context of my OS's available memory.
Here's some simple test code I've been running:

<CPP Code>
  #define PUSH_BACK_INTEGER(v, x) \
    do {\
      UNPROTECT_PTR(v);\
      SET_LENGTH(v, GET_LENGTH(v) + 1);\
      PROTECT(v);\
      INTEGER_POINTER(v)[GET_LENGTH(v) - 1] = x;\
    }\
    while (false)

  SEXP R_SimplePushBackTest(SEXP args)
  {
    SEXP arg1, arg2, int_vect;

    PROTECT(arg1 = AS_INTEGER(CADR(args)));
    int n_reps = INTEGER_POINTER(arg1)[0];
    PROTECT(arg2 = AS_LOGICAL(CADDR(args)));
    bool full_alloc = (LOGICAL_POINTER(arg2)[0] ? true : false);
    if (full_alloc)
      PROTECT(int_vect = NEW_INTEGER(n_reps));
    else
      PROTECT(int_vect = NEW_INTEGER(0));

    for (int i = 0; i < n_reps; ++i) {
      Rprintf("  ** Iteration %d:\n", i + 1);
      if (full_alloc)
        INTEGER_POINTER(int_vect)[i] = i;
      else
        PUSH_BACK_INTEGER(int_vect, i);
    }

    SEXP out, names, cls;

    PROTECT(out = NEW_LIST(1));
    SET_VECTOR_ELT(out, 0, int_vect);

    PROTECT(names = NEW_CHARACTER(1));
    SET_STRING_ELT(names, 0, COPY_TO_USER_STRING("integer.vector"));
    SET_NAMES(out, names);

    PROTECT(cls = NEW_CHARACTER(1));
    SET_STRING_ELT(cls, 0, COPY_TO_USER_STRING("pushback"));
    classgets(out, cls);

    UNPROTECT(6);
    return out;
  }
</CPP Code>

<R Code>
  nreps=50000
  allocate=FALSE
  sink("pushback_test.txt")
  test.pushback=.External("R_SimplePushBackTest", as.integer(nreps), as.logical(allocate))
  print(test.pushback)
  sink()
</R Code>

If allocate=TRUE (vector memory is pre-allocated in the extension),
the code proceeds normally on my system; if allocate=FALSE, Rgui.exe
eventually crashes from an Access Violation. I've gathered only enough
information from the R source code so far to think that R is
ultimately calling realloc() through the SET_LENGTH macro; that would
make my code rather inefficient, but I'm trying for genericness here.
In C++, is there a better way than what I'm doing to concatenate
values onto the end of an R vector of arbitrary length, especially
over many iterations?

Thanks for taking the time to read this.

System specs: Pentium 4 2.5 GHz, 512 MB RAM, 40 GB hard drive, Win XP

Yrs etc.,

Jim Java



From ryszard.czerminski at pharma.novartis.com  Tue Dec  9 19:56:12 2003
From: ryszard.czerminski at pharma.novartis.com (ryszard.czerminski@pharma.novartis.com)
Date: Tue, 9 Dec 2003 13:56:12 -0500
Subject: [R] problem with pls(x, y, ..., ncomp = 16): Error in inherit s( x,
	"data.frame") : subscript out of bounds
Message-ID: <OF9D8C2730.012165F5-ON85256DF7.00677845-85256DF7.00681FE7@EU.novartis.net>

Help for pls says:

> ?pls
[...]
Arguments:
...
   ncomp: the numbers of latent variables to be assessed in the
          modelling. Default is from one to the rank of 'X'.
[...]

so my assumption was (maybe wrong) that idea of ncomp parameter is to 
limit
number of assessed variables.

also, if called without crossvalidation it gives the same error:

> m <- pls(x, y, ncomp = 16)
Error in inherits(x, "data.frame") : subscript out of bounds

R







"Liaw, Andy" <andy_liaw at merck.com>
12/09/2003 12:50 PM

 
        To:     Ryszard Czerminski/PH/Novartis at PH, r-help at stat.math.ethz.ch
        cc: 
        Subject:        RE: [R] problem with pls(x, y, ..., ncomp = 16): Error in inherit s( x, 
"data.frame") : subscript out of bounds


I don't know the details of pls (in the pls.pcr package, I assume), but if
you use validation="CV", that says you want to use CV to select the best
number of components.  Then why would you specify ncomp as well?

Andy

> From: ryszard.czerminski at pharma.novartis.com
> 
> When I try to use ncomp parameter in pls procedure  I get 
> following error:
> 
> > library(pls.pcr)
> > m <- pls(x, y, validation = "CV", niter = 68, ncomp = 16)
> Error in inherits(x, "data.frame") : subscript out of bounds
> 
> Without ncomp parameter everything seems to work OK
> 
> > dim(x)
> [1]  68 116
> > dim(y)
> [1] 68  1
> > m <- pls(x, y, validation = "CV", niter = 68)
> > length(m$ncomp)
> [1] 67
> 
> Ryszard
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
>



From karlknoblich at yahoo.de  Tue Dec  9 20:14:37 2003
From: karlknoblich at yahoo.de (=?iso-8859-1?q?Karl=20Knoblick?=)
Date: Tue, 9 Dec 2003 20:14:37 +0100 (CET)
Subject: [R]: global and local variables
Message-ID: <20031209191437.23941.qmail@web10007.mail.yahoo.com>

Hi!

I'm no guru in R. But I can think of 2 ways (have to
be tried):

1) As Uwe Ligges said: just save return the stored
variable a<-preprocess(xdata) and return it (if you
want to return more than 1 item use list) and give
this variable to the next function. Example:

func1<-function(x) 
{
y<-x^2
# for testing:
print(paste("func1", x, y))
return(y) 
}

func2<-function(x, valuefunc1=NA)
{
if (is.na(valuefunc1)) valuefunc1<-func1(x)
# calculate things with valuefunc1
print(paste("func2", x, valuefunc1))
return(list(valuefunc1=valuefunc1))
}

func3<-function(x, valuefunc1=NA)
{
if (is.na(valuefunc1)) valuefunc1<-func1(x)
# calculate things with valuefunc1
print(paste("func3", x, valuefunc1))
return(list(valuefunc1=valuefunc1))
}

# use this
a<-list(NA)
names(a)<-c("valuefunc1")

a<-func2(x=3, valuefunc1=a$valuefunc1)
a<-func3(x=3, valuefunc1=a$valuefunc1) # be careful,
makes only sense if the x is equal to former fuction
call...
a<-func3(x=999, valuefunc1=a$valuefunc1) # will be the
same
a<-func3(x=999) # will be different 


2) use global variables like
assign("stored.value", x, envir=.GlobalEnv)
example see reply of my posting:
RE: [R] LOCF - Last Observation Carried Forward Simon
Fear (Sat 15 Nov 2003 - 03:28:03 EST) 

HTH,
Karl



From bates at stat.wisc.edu  Tue Dec  9 20:24:37 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: 09 Dec 2003 13:24:37 -0600
Subject: [R] R Interface handholding
In-Reply-To: <Pine.LNX.4.33L.0312091320160.9900-100000@ux13.sp.cs.cmu.edu>
References: <Pine.LNX.4.33L.0312091320160.9900-100000@ux13.sp.cs.cmu.edu>
Message-ID: <6r4qw9vloq.fsf@bates4.stat.wisc.edu>

I'm not sure exactly what you want to do but it seems you are going
about it in an overly complicated way.  If you are going to build a
package you just need to put a NAMESPACE file at the top level and
include a useDynLib specification or write a short R function called
.First.Lib that calls library.dynam and include it in the R
subdirectory.  Typically this function is simply

.First.lib = function(lib, pkg) {
    library.dynam(pkg, pkg, lib)
}

but it can do other initializations.

After that you just put the source for myincr in the src subdirectory,
compile and load the package, then call the function as

.Call("myincr", x, PACKAGE="myPackageName")


Pat Gunn <pgunn at cs.cmu.edu> writes:

> Hello,
> I need a bit of handholding with R, specifically, with writing
> packages for it. I'm a systems programmer, and am, on the request
> of several users of our software, working on generating R interfaces.
> For starters, I've written the following R function (which compiles):
> 
> SEXP myincr(SEXP Rinput)
> { // Returns input integer incremented by one
> int input;
> SEXP returner;
> 
> PROTECT(Rinput = AS_NUMERIC(Rinput));
> 
> input = * INTEGER(Rinput);
> input++;
> PROTECT(returner = NEW_INTEGER(input));
> Rprintf("Hey there\n");
> return returner;
> }
> 
> I've made this into a package, by dropping it into a stub directory
> along with something called init.c:
> 
> #include "areone.h"
> #include <R_ext/Rdynload.h>
> #include <Rinternals.h>
> 
> R_NativePrimitiveArgType myincr_t[1] = {INTSXP};
> 
> static const R_CMethodDef cMethods[] =
>         {
>         {"myincr", (DL_FUNC) &myincr, 1, myincr_t}
>         };
> 
> void R_init_myincr(DllInfo* dll)
> {
> R_registerRoutines(dll, cMethods, NULL, NULL, NULL);
> }
> 
> R is happy to install this for me, but after doing a library(myincr),
> the function doesn't seem to be available, so I presume I'm missing
> something. Does R normally call, at library load, R_init_$MODULENAME() ?
> 
> 
> My other question is.. our software produces data structures (we call datsets)
> which resemble limited database tables, and I'd like some advice on exposing
> them to R -- columns, in our scheme, either hold doubles or strings,
> the columns have names, and we need the ability to export these into
> appropriate R structures as well as populate them from R. I notice that
> the R DBI, at least as according to its documentation, uses the database
> to hold the data (presumably in temporary tables) and returns parts of it
> as requested, via R functions. I could use a static global pointer (in C) into a
> storage space of datsets, and write bridge functions exporting them as R arrays,
> or I could attempt to find an appropriate native format and export to it..
> any advice?
> 
> It might be worth mentioning that all of the code to do this will
> eventually be auto-generated -- we already have code to do this for C
> that doesn't need to expose people linking our code to all of our
> custom structures and stuff.
> 
> -- 
> Pat Gunn
> Research/Systems Programmer, Auton Group, CMU
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-- 
Douglas Bates                            bates at stat.wisc.edu
Statistics Department                    608/262-2598
University of Wisconsin - Madison        http://www.stat.wisc.edu/~bates/



From tlumley at u.washington.edu  Tue Dec  9 20:40:23 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 9 Dec 2003 11:40:23 -0800 (PST)
Subject: [R] Windows Memory Issues
In-Reply-To: <76A000A82289D411952F001083F9DD06047FE37F@exsalem4-bu.odot.state.or.us>
References: <76A000A82289D411952F001083F9DD06047FE37F@exsalem4-bu.odot.state.or.us>
Message-ID: <Pine.A41.4.58.0312091137310.94740@homer08.u.washington.edu>

On Tue, 9 Dec 2003 Benjamin.STABLER at odot.state.or.us wrote:

> Thanks for the reply.  So are you saying that multiple calls to gc() frees
> up memory to Windows and then other processes can use that newly freed
> memory?

No.  You typically can't free memory back to Windows (or many other OSes).


>		 So multiple calls to gc() does not actually make more memory
> available to new R objects that I might create.

Yes and no. It makes more memory available, but only memory that would
have been made available in any case if you had tried to use it.  R calls
the garbage collector before requesting more memory from the operating
system and before running out of memory.


>					  The reason I ask is because
> I want to know how to use all the available memory that I can to store
> object in R.  ?gc says that garbage collection is run without user
> intervention so there is really nothing I can do to improve memory under
> Windows except increase the --max-mem-size at startup

You can't do anything else to make more memory available, only to use
less.

	-thomas



From hec.villafuerte at telgua.com.gt  Tue Dec  9 22:42:05 2003
From: hec.villafuerte at telgua.com.gt (=?ISO-8859-1?Q?=22H=E9ctor_Villafuerte_D=2E=22?=)
Date: Tue, 09 Dec 2003 13:42:05 -0800
Subject: [R] Interfacing R and Python in MS Windows
In-Reply-To: <3A822319EB35174CA3714066D590DCD50205CF1C@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50205CF1C@usrymx25.merck.com>
Message-ID: <3FD641AD.4080105@telgua.com.gt>

I got farther this time, but it wasn't enough though...
does anyone know what this errors are?
Thanks!
Hector


E:\to_do>Rcmd INSTALL e:/to_do/RSPython.tar.gz

---------- Making package RSPython ------------

   **********************************************
   WARNING: this package has a configure script
         It probably needs manual configuration
   **********************************************

  installing inst files
  adding build stamp to DESCRIPTION
  making DLL ...
making GeneralConverters.d from GeneralConverters.c
making PythonCall.d from PythonCall.c
making PythonFunctionConverters.d from PythonFunctionConverters.c
making PythonReferences.d from PythonReferences.c
making PythonReflectance.d from PythonReflectance.c
making RCall.d from RCall.c
making RPythonConverters.d from RPythonConverters.c
making RPythonReferences.d from RPythonReferences.c
making UserConverters.d from UserConverters.c
gcc  -I../inst/include -Ic:/Python23/include -D_R_=1 -DUSE_R=1 
-IC:/PROGRA~1/R/rw1080/src/include -Wall -O2   -c General
Converters.c -o GeneralConverters.o
In file included from c:/Python23/include/Python.h:75,
                 from ../inst/include/RPythonModule.h:4,
                 from ../inst/include/UserConverters.h:4,
                 from GeneralConverters.c:1:
c:/Python23/include/intobject.h:41: parse error before 
"PyInt_AsUnsignedLongLongMask"
c:/Python23/include/intobject.h:41: warning: type defaults to `int' in 
declaration of `PyInt_AsUnsignedLongLongMask'
c:/Python23/include/intobject.h:41: warning: data definition has no type 
or storage class
In file included from c:/Python23/include/Python.h:77,
                 from ../inst/include/RPythonModule.h:4,
                 from ../inst/include/UserConverters.h:4,
                 from GeneralConverters.c:1:
c:/Python23/include/longobject.h:37: warning: parameter names (without 
types) in function declaration
c:/Python23/include/longobject.h:39: parse error before "PyLong_AsLongLong"
c:/Python23/include/longobject.h:39: warning: type defaults to `int' in 
declaration of `PyLong_AsLongLong'
c:/Python23/include/longobject.h:39: warning: data definition has no 
type or storage class
c:/Python23/include/longobject.h:40: parse error before 
"PyLong_AsUnsignedLongLong"
c:/Python23/include/longobject.h:40: warning: type defaults to `int' in 
declaration of `PyLong_AsUnsignedLongLong'
c:/Python23/include/longobject.h:40: warning: data definition has no 
type or storage class
c:/Python23/include/longobject.h:41: parse error before 
"PyLong_AsUnsignedLongLongMask"
c:/Python23/include/longobject.h:41: warning: type defaults to `int' in 
declaration of `PyLong_AsUnsignedLongLongMask'
c:/Python23/include/longobject.h:41: warning: data definition has no 
type or storage class
make[2]: *** [GeneralConverters.o] Error 1
make[1]: *** [srcDynlib] Error 2
make: *** [pkg-RSPython] Error 2
*** Installation of RSPython failed ***



Liaw, Andy wrote:

>Check the "Writing R Extensions" manual for requirements on the DESCRIPTION
>file.
>
>HTH,
>Andy
>  
>



From ripley at stats.ox.ac.uk  Tue Dec  9 20:47:59 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 9 Dec 2003 19:47:59 +0000 (GMT)
Subject: [R] Windows Memory Issues
In-Reply-To: <76A000A82289D411952F001083F9DD06047FE37F@exsalem4-bu.odot.state.or.us>
Message-ID: <Pine.LNX.4.44.0312091947070.8889-100000@gannet.stats>

On Tue, 9 Dec 2003 Benjamin.STABLER at odot.state.or.us wrote:

> Thanks for the reply.  So are you saying that multiple calls to gc() frees
> up memory to Windows and then other processes can use that newly freed
> memory?  So multiple calls to gc() does not actually make more memory

That is what I said.  Why do people expect me to repeat myself?

> available to new R objects that I might create.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Dec  9 20:51:54 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 9 Dec 2003 19:51:54 +0000 (GMT)
Subject: [R] Windows Memory Issues
In-Reply-To: <Pine.A41.4.58.0312091137310.94740@homer08.u.washington.edu>
Message-ID: <Pine.LNX.4.44.0312091948510.8889-100000@gannet.stats>

On Tue, 9 Dec 2003, Thomas Lumley wrote:

> On Tue, 9 Dec 2003 Benjamin.STABLER at odot.state.or.us wrote:
> 
> > Thanks for the reply.  So are you saying that multiple calls to gc() frees
> > up memory to Windows and then other processes can use that newly freed
> > memory?
> 
> No.  You typically can't free memory back to Windows (or many other OSes).

At least using R under Windows NT/2000/XP you can.  I've watched it do so 
whilst fixing memory leaks.

There is another complication here: R for Windows uses a third-party 
malloc, and you can free memory back to that if not to the OS.  The reason 
Windows is special is the issue of fragmentation, which OSes using mmap
(and R-devel under Windows) typically do not suffer.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Dec  9 20:53:13 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 9 Dec 2003 19:53:13 +0000 (GMT)
Subject: [R] Interfacing R and Python in MS Windows
In-Reply-To: <3FD641AD.4080105@telgua.com.gt>
Message-ID: <Pine.LNX.4.44.0312091952140.8889-100000@gannet.stats>

I believe RSPython is from Omegahat, so why not ask on the mailing list 
for Omegahat?

On Tue, 9 Dec 2003, "H?ctor Villafuerte D." wrote:

> I got farther this time, but it wasn't enough though...
> does anyone know what this errors are?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From flom at ndri.org  Tue Dec  9 21:24:40 2003
From: flom at ndri.org (Peter Flom)
Date: Tue, 09 Dec 2003 15:24:40 -0500
Subject: [R] rpart question re NAs introduced by coercion
Message-ID: <sfd5e943.045@MAIL.NDRI.ORG>

Hello

running R1.8.0 on Windows

I am running a tree model using rpart, and I get a warning 

NAs introduced by coercion

even though when I examine the variables in the tree, none of them have
any missing vaues.......

What am I missing?

TIA, as always





Peter L. Flom, PhD
Assistant Director, Statistics and Data Analysis Core
Center for Drug Use and HIV Research
National Development and Research Institutes
71 W. 23rd St
www.peterflom.com
New York, NY 10010
(212) 845-4485 (voice)
(917) 438-0894 (fax)



From mailinglist2_wegmann at web.de  Tue Dec  9 21:26:22 2003
From: mailinglist2_wegmann at web.de (Martin Wegmann)
Date: Tue, 9 Dec 2003 21:26:22 +0100
Subject: [R] packages for ecologists
Message-ID: <200312091934.29088.mailinglist2_wegmann@web.de>

Hello R-user, 

sorry for this very off-topic question. 

But I shall present R to my dept. (pro's and con's and what it can do).
The pro's and con's are easy but not what R can do (additional to the "normal" 
statistics).
I looked through the packages, but the enormous amount of packages makes it 
very difficult for me to decide which one is worth mentioning.

I used only a small part of all R packages (mainly recommended packages and 
grasper) and would like to know which package for ecologist has to be 
mentioned. 

I would greatly appreciate if you can tell me which packages you think are 
very useful for ecolgical research in R e.g. vegan, ade4, ... 

thanks in advance, regards Martin



From MHerzog at cabnr.unr.edu  Tue Dec  9 23:07:06 2003
From: MHerzog at cabnr.unr.edu (Herzog, Mark)
Date: Tue, 9 Dec 2003 14:07:06 -0800
Subject: [R] packages for ecologists
Message-ID: <6FD4F362992E394A902933580B7F6E86D0B636@agnt-mail.agnt.unr.edu>

Definitely take a look at WiSP. 

http://www.ruwpa.st-and.ac.uk/estimating.abundance/WiSP/

Mark
==========================================

Mark Herzog
Post Doctoral Researcher
Dept. of Natural Resources and Environmental Science
University of Nevada Reno
Reno, NV  89512
(775) 784-6984 (office)
(775) 784-4583 (fax)
mherzog at unr.edu 

==========================================



-----Original Message-----
From: Martin Wegmann [mailto:mailinglist2_wegmann at web.de] 
Sent: Tuesday, December 09, 2003 12:26 PM
To: R-list
Subject: [R] packages for ecologists


Hello R-user, 

sorry for this very off-topic question. 

But I shall present R to my dept. (pro's and con's and what it can do).
The pro's and con's are easy but not what R can do (additional to the
"normal" 
statistics).
I looked through the packages, but the enormous amount of packages makes
it 
very difficult for me to decide which one is worth mentioning.

I used only a small part of all R packages (mainly recommended packages
and 
grasper) and would like to know which package for ecologist has to be 
mentioned. 

I would greatly appreciate if you can tell me which packages you think
are 
very useful for ecolgical research in R e.g. vegan, ade4, ... 

thanks in advance, regards Martin

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From mathieu.drapeau at bioneq.qc.ca  Tue Dec  9 23:04:24 2003
From: mathieu.drapeau at bioneq.qc.ca (Mathieu Drapeau)
Date: Tue, 09 Dec 2003 17:04:24 -0500
Subject: [R] histogram density division
In-Reply-To: <3FCBB64D.2090300@bioneq.qc.ca>
References: <3FCBB64D.2090300@bioneq.qc.ca>
Message-ID: <3FD646E8.3020504@bioneq.qc.ca>

Hi,
I already sent this query but I didn't receive any answers.
I try another time and try to explain another way my question...

I want to decrease the height of my bands in my histogram by a factor of 
5... how can I do that? Is there an argument to hist() that do the job?

Thanks,
Mathieu

> Hi,
> I would like to plot an histogram with modified density values.
> My Y-axis represent the occurences of the ranges (specified by the 
> breaks argument) of my data (represented by a big vector). Now, I 
> would like to divide by X the number of occurences in each bands and 
> do a plot of "lower" occurences.
> How can I do that?
>
> Thanks,
> Mathieu
>



From christian_mora at vtr.net  Tue Dec  9 23:13:12 2003
From: christian_mora at vtr.net (Christian Mora)
Date: Tue, 9 Dec 2003 19:13:12 -0300
Subject: [R] Johnson's Sb distribution
Message-ID: <000001c3bea1$a302d5f0$b43d68c8@CPQ28661778111>

Hi all;

I'm working with the library SuppDists trying to fit a Johnson's Sb
distribution to a dataset. It works fine, but I need to set one of the
location parameters (epsilon) to zero. How can I do this using the
function JohnsonFit() or any other similar? ...and Is it possible to
define the type (SN,SL,SB,SU) or the library assumes the type
automatically depending on the data?

Thanks for any hint

Christian



From tblackw at umich.edu  Tue Dec  9 23:51:33 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Tue, 9 Dec 2003 17:51:33 -0500 (EST)
Subject: [R] histogram density division
In-Reply-To: <3FD646E8.3020504@bioneq.qc.ca>
References: <3FCBB64D.2090300@bioneq.qc.ca> <3FD646E8.3020504@bioneq.qc.ca>
Message-ID: <Pine.SOL.4.58.0312091743370.25434@asteroids.gpcc.itd.umich.edu>

Mathieu  -

That's easy.  Assign the return value of  hist() to some
variable, say "fixed", then go in and hack the value of
fixed$counts however you like, and re-plot using plot(fixed).
Example code:

fixed <- hist(rnorm(2000))
fixed$counts <- fixed$counts / 5
plot(fixed)

I confess I didn't quite understand your question the first
time I saw it, so couldn't reply.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Tue, 9 Dec 2003, Mathieu Drapeau wrote:

> Hi,
> I already sent this query but I didn't receive any answers.
> I try another time and try to explain another way my question...
>
> I want to decrease the height of my bands in my histogram by a factor of
> 5... how can I do that? Is there an argument to hist() that do the job?
>
> Thanks,
> Mathieu
>
> > Hi,
> > I would like to plot an histogram with modified density values.
> > My Y-axis represent the occurences of the ranges (specified by the
> > breaks argument) of my data (represented by a big vector). Now, I
> > would like to divide by X the number of occurences in each bands and
> > do a plot of "lower" occurences.
> > How can I do that?
> >
> > Thanks,
> > Mathieu
>



From h.wickham at auckland.ac.nz  Tue Dec  9 23:37:52 2003
From: h.wickham at auckland.ac.nz (Hadley Wickham)
Date: Wed, 10 Dec 2003 11:37:52 +1300
Subject: [R] Key for custom lattice panel function
Message-ID: <3FD64EC0.20903@auckland.ac.nz>

Hi all,

I've created a custom lattice panel function for levelplot - instead of 
representing z by colour, it plot circles with radius proportional to z 
(in the style of the map plots of Jacques Bertin).  I'm happy to email 
an example graph to anyone interested.

The problem is now to create a key for the plot.  This is difficult 
because all of the other lattice plots convey information through point 
colour, shape, and texture, not point size.  For this reason (and having 
looked at the code) I don't think I can shoehorn $key or $colorkey to 
produce the type of key that I want. 

I'm using grid.circles() with native units that have been scaled in the 
same way as in panel.levelplot.

Can anyone offer any suggestions as to how to create this key?

Hadley



From jasont at indigoindustrial.co.nz  Wed Dec 10 01:19:33 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Wed, 10 Dec 2003 13:19:33 +1300
Subject: [R] OT: BibTex year-only citation in text?
Message-ID: <3FD66695.2020603@indigoindustrial.co.nz>

Sorry for the off-topic question, but I know there are some talented 
LaTeX users out there. Which bibliography style gives only the year in 
text citations (e.g "for further details, see Anderson (1992)" )?

Thanks

Jason
-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From MNelson at sequenom.com  Wed Dec 10 01:30:45 2003
From: MNelson at sequenom.com (Matt Nelson)
Date: Tue, 9 Dec 2003 16:30:45 -0800 
Subject: [R] OT: BibTex year-only citation in text?
Message-ID: <9F171A0DEB645643BD848C4CF0FB36840430B080@dna.sequenom.com>

Jason,

For many bibliography styles, the command \citeyear{key} will work.  If this
doesn't work for the style you are using, you can investigate style-specific
methods or consider other styles.  I find that natbib is good for
author-year formats.

If you use Latex more than on occasion, a good reference book is invaluable.
I like "The Latex Companion" by Goossens, Mittelbach, and Samarin.

Regards,
Matt

Matthew R. Nelson, Ph.D.
Director, Biostatistics
Sequenom, Inc.

> -----Original Message-----
> From: Jason Turner [mailto:jasont at indigoindustrial.co.nz]
> Sent: Tuesday, December 09, 2003 4:20 PM
> To: R-Help
> Subject: [R] OT: BibTex year-only citation in text?
> 
> 
> Sorry for the off-topic question, but I know there are some talented 
> LaTeX users out there. Which bibliography style gives only 
> the year in 
> text citations (e.g "for further details, see Anderson (1992)" )?
> 
> Thanks
> 
> Jason
> -- 
> Indigo Industrial Controls Ltd.
> http://www.indigoindustrial.co.nz
> 64-21-343-545
> jasont at indigoindustrial.co.nz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From deepayan at stat.wisc.edu  Wed Dec 10 02:09:00 2003
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Tue, 9 Dec 2003 19:09:00 -0600
Subject: [R] Key for custom lattice panel function
In-Reply-To: <3FD64EC0.20903@auckland.ac.nz>
References: <3FD64EC0.20903@auckland.ac.nz>
Message-ID: <200312091909.00761.deepayan@stat.wisc.edu>

On Tuesday 09 December 2003 16:37, Hadley Wickham wrote:
> Hi all,
>
> I've created a custom lattice panel function for levelplot - instead of
> representing z by colour, it plot circles with radius proportional to z
> (in the style of the map plots of Jacques Bertin).  I'm happy to email
> an example graph to anyone interested.
>
> The problem is now to create a key for the plot.  This is difficult
> because all of the other lattice plots convey information through point
> colour, shape, and texture, not point size.  For this reason (and having
> looked at the code) I don't think I can shoehorn $key or $colorkey to
> produce the type of key that I want.
>
> I'm using grid.circles() with native units that have been scaled in the
> same way as in panel.levelplot.
>
> Can anyone offer any suggestions as to how to create this key?

Exactly how do you want to your key to look like ? 

As things stand now, I don't think it would be possible to put arbitrary keys. 
However, this should be easy to fix. That is, we could allow the key to be an 
arbitrary grid object (as produced by draw.key and draw.colorkey), and use 
its "grobheight" or "grobwidth" to allocate the necessary space. I'll try to 
add this in the next major update.

For now, you could fake it by printing your "trellis" object in a grid 
viewport that's smaller than the whole screen, and draw your key separately 
afterwards in the remaining space. 

HTH,

Deepayan



From allison.100 at osu.edu  Wed Dec 10 03:21:17 2003
From: allison.100 at osu.edu (Gary Allison)
Date: Tue, 09 Dec 2003 21:21:17 -0500
Subject: [R] nested analysis with lme - odd result?
Message-ID: <3FD6831D.6030005@osu.edu>

Hello!

When I simulate variance at only a single level in a nested analysis 
using lme (all levels are random effects), the results confuse me. 
Instead of lme reporting high variance in only that simulated level, 
substantial variance (>10% of simulated level) often appears in other 
levels -- in some configurations, as often as 50% of the time. Usually 
this spurious variance shows up in levels of nesting above the level 
with high simulated variance.

Am I doing something wrong or should I expect such 'over-detection'? I 
was expecting near zero variance in those other levels except say, 5% of 
the time.

Here's some example code with variance simulated in level 2 and spurious 
variance appearing frequently in level 1:

library(nlme)
# 4 level nesting
simVar <- c(0,1,0,.1) # first is level one, last becomes error
nAtLevel <- c(5,5,5,5)  # number of replicates at each level

F1V <- F2V <- F3V  <- residV <- numeric(0)
for (rep in 1:100){
   F1f <- F2f <- F3f <- value <- numeric(0)
   mn <- numeric(length(nAtLevel))
   # data generator
   for (F1 in 1:nAtLevel[1]) {
     mn[1] <- rnorm(1,sd=simVar[1]) # set mean for level 1
     for (F2 in 1:nAtLevel[2]) {
       mn[2] <- rnorm(1,sd=simVar[2]) # set mean for level 2
       for (F3 in 1:nAtLevel[3]) {
         mn[3] <- rnorm(1,sd=simVar[3]) # set mean for level 3
         for (F4 in 1:nAtLevel[4]) {
           mn[4] <- rnorm(1,sd=simVar[4]) #set mean for lowest level
           value <- c(value, sum(mn))
           F1f <- c(F1f,F1)
           F2f <- c(F2f,F2)
           F3f <- c(F3f,F3)
         }
       }
     }
   }

   y.lme <- lme(value ~ 1,random = ~ 1 | as.factor(F1f)/
                                         as.factor(F2f)/
                                         as.factor(F3f))
   v <- as.numeric(VarCorr(y.lme)[,2])
   v <- as.numeric(na.omit(v))
   F1V <- c(F1V,v[1])
   F2V <- c(F2V,v[2])
   F3V <- c(F3V,v[3])
   residV <- c(residV,y.lme$sigma)
}
var.df <- data.frame(F1V,F2V,F3V,residV)
par(mfrow=c(2,2))
hist(var.df$F1V,main='Level 1 StdDev')
hist(var.df$F2V,main='Level 2 StdDev')
hist(var.df$F3V,main='Level 3 StdDev')
hist(var.df$residV,main='Residual StdDev')
txt <- 'Simulated stddev:'
for (i in 1:length(simVar)) {
   txt <- paste(txt,' level ',i,'=',simVar[i],',',sep='')}
mtext(txt, outer=T,line=-1,side=3)


Summary plots for several sample sizes is at:
http://david.science.oregonstate.edu/~allisong/R/sampSize.pdf

Thanks for any suggestions!
Gary

--
Gary Allison
Department of Evolution, Ecology and Organismal Biology
Ohio State University



From rnews at kernstat.com  Wed Dec 10 03:52:41 2003
From: rnews at kernstat.com (Remington, Richard)
Date: Tue, 09 Dec 2003 19:52:41 -0700
Subject: [R] expressing functions 
Message-ID: <3FD68A79.6060600@kernstat.com>

# Why does expressing one function

require(ctest)
t.test

# return only

function (x, ...)
UseMethod("t.test")
<environment: namespace:ctest>

# but expressing another function

shapiro.test

# returns more complete code?

function (x)
{
     DNAME <- deparse(substitute(x))
     x <- sort(x[complete.cases(x)])
     n <- length(x)
     if (n < 3 || n > 5000)
         stop("sample size must be between 3 and 5000")
     rng <- x[n] - x[1]
     if (rng == 0)
         stop("all `x[]' are identical")
     if (rng < 1e-10)
         x <- x/rng
     n2 <- n%/%2
     sw <- .C("swilk", init = FALSE, as.single(x), n, n1 = as.integer(n),
         as.integer(n2), a = single(n2), w = double(1), pw = double(1),
         ifault = integer(1), PACKAGE = "ctest")
     if (sw$ifault && sw$ifault != 7)
         stop(paste("ifault=", sw$ifault, ". This should not happen"))
     RVAL <- list(statistic = c(W = sw$w), p.value = sw$pw, method = 
"Shapiro-Wilk normality test",
         data.name = DNAME)
     class(RVAL) <- "htest"
     return(RVAL)
}
<environment: namespace:ctest>

-- 

Richard E. Remington III
Statistician
KERN Statistical Services, Inc.
PO Box 1046
Boise, ID 83701
Tel: 208.426.0113
KernStat.com



From andy_liaw at merck.com  Wed Dec 10 05:14:45 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 9 Dec 2003 23:14:45 -0500
Subject: [R] problem with pls(x, y, ..., ncomp = 16): Error in
	inherit s( x, "data.frame") : subscript out of bounds
Message-ID: <3A822319EB35174CA3714066D590DCD50205CF2B@usrymx25.merck.com>

My first guess was wrong (should have RTFM...).  Try:

  m <- pls(x, y, validation = "CV", niter = 68, ncomp = 1:16)

I'm not familiar enough with SIMPLS to know whether it makes sense to pass
ncomp of length 1 to pls(), but the code as is won't work (unless ncomp=1).
The problem is in the following two lines inside simpls():

 YvarExpl <- matrix(0, length(ncomp), npred)
 for (i in 1:max(ncomp)) YvarExpl[i, ] <- diag(cor(Y, X %*% B[, , i]))^2

If ncomp is of length 1, then YvarExpl is a 1x1 matrix, so the loop will
bomb if ncomp > 1.

[I'm CC'ing Prof. Wehrens, who is the package maintainer.]

HTH,
Andy

> From: ryszard.czerminski at pharma.novartis.com 
> 
> Help for pls says:
> 
> > ?pls
> [...]
> Arguments:
> ...
>    ncomp: the numbers of latent variables to be assessed in the
>           modelling. Default is from one to the rank of 'X'.
> [...]
> 
> so my assumption was (maybe wrong) that idea of ncomp parameter is to 
> limit
> number of assessed variables.
> 
> also, if called without crossvalidation it gives the same error:
> 
> > m <- pls(x, y, ncomp = 16)
> Error in inherits(x, "data.frame") : subscript out of bounds
> 
> R
> 
> "Liaw, Andy" <andy_liaw at merck.com>
> 
> I don't know the details of pls (in the pls.pcr package, I 
> assume), but if
> you use validation="CV", that says you want to use CV to 
> select the best
> number of components.  Then why would you specify ncomp as well?
> 
> Andy
> 
> > From: ryszard.czerminski at pharma.novartis.com
> > 
> > When I try to use ncomp parameter in pls procedure  I get 
> > following error:
> > 
> > > library(pls.pcr)
> > > m <- pls(x, y, validation = "CV", niter = 68, ncomp = 16)
> > Error in inherits(x, "data.frame") : subscript out of bounds
> > 
> > Without ncomp parameter everything seems to work OK
> > 
> > > dim(x)
> > [1]  68 116
> > > dim(y)
> > [1] 68  1
> > > m <- pls(x, y, validation = "CV", niter = 68)
> > > length(m$ncomp)
> > [1] 67
> > 
> > Ryszard
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > 
> > 
> 
> 
> 
> 
> 
> 
>



From ripley at stats.ox.ac.uk  Wed Dec 10 05:34:05 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 10 Dec 2003 04:34:05 +0000 (GMT)
Subject: [R] expressing functions 
In-Reply-To: <3FD68A79.6060600@kernstat.com>
Message-ID: <Pine.LNX.4.44.0312100428190.19301-100000@gannet.stats>

On Tue, 9 Dec 2003, Remington, Richard wrote:

> # Why does expressing one function
> 
> require(ctest)
> t.test
> 
> # return only
> 
> function (x, ...)
> UseMethod("t.test")
> <environment: namespace:ctest>
> 
> # but expressing another function
> 
> shapiro.test
> 
> # returns more complete code?
[...]

False hypothesis: both are the complete code.

You are not understanding (S3-style) generic functions: see any good book
on R/S.

(`An Introduction to R' is based on notes that predate them, but they are
covered in more detail in the draft R Language manual.  `The reader is
referred to the official references for a complete discussion of this
mechanism.' which I think means Chambers & Hastie, 1992,)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jasont at indigoindustrial.co.nz  Wed Dec 10 05:37:39 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Wed, 10 Dec 2003 17:37:39 +1300
Subject: [R] expressing functions
In-Reply-To: <3FD68A79.6060600@kernstat.com>
References: <3FD68A79.6060600@kernstat.com>
Message-ID: <3FD6A313.5020604@indigoindustrial.co.nz>

Remington, Richard wrote:
> # Why does expressing one function
> 
> require(ctest)
> t.test
> 
> # return only
> 
> function (x, ...)
> UseMethod("t.test")
> <environment: namespace:ctest>
> 
> # but expressing another function
> 
> shapiro.test
> 
> # returns more complete code?
> 
> function (x)
> {
>     DNAME <- deparse(substitute(x))
>     x <- sort(x[complete.cases(x)])
>     n <- length(x)
>     if (n < 3 || n > 5000)
>         stop("sample size must be between 3 and 5000")
...

Short answer:  Unless you're programming your own functions, you don't 
need to worry about that.

Long answer:  Because the first is generic - it looks at what kind of 
data you're testing (two vectors, a formula, whatever, ...) and calls 
the appropriate sub-function.  shapiro.test does not; it just takes one 
data format, and  stops in its tracks if that's not what you've provided.

The ideas behind this are documented in "Writing R Extensions" 
(R-exts.pdf) which is supplied with binary R distributions, and is 
available from CRAN.  See chapter 6, "Generic functions and methods", in 
the version that accompanies R-1.8.1.

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From ggrothendieck at myway.com  Wed Dec 10 06:04:34 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 10 Dec 2003 00:04:34 -0500 (EST)
Subject: [R] expressing functions
Message-ID: <20031210050434.28B993949@mprdmxin.myway.com>



Perhaps what should be added to the previous answers is
that you can find out where the real work is done like
this:

require(ctest)
t.test
methods(t.test)
ctest:::t.test.default
ctest:::t.test.formula

If the class of the first argument to t.test is formula
then t.test.formula gets invoked so that's where the
real work is done; otherwise, t.test.default
gets invoked so that's where the real work is done.

--- 
 
Remington, Richard wrote:
> # Why does expressing one function
> 
> require(ctest)
> t.test
> 
> # return only
> 
> function (x, ...)
> UseMethod("t.test")
> <environment: namespace:ctest>
> 
> # but expressing another function
> 
> shapiro.test
> 
> # returns more complete code?
> 
> function (x)
> {
> DNAME <- deparse(substitute(x))
> x <- sort(x[complete.cases(x)])
> n <- length(x)
> if (n < 3 || n > 5000)
> stop("sample size must be between 3 and 5000")



From maya2006mb at yahoo.com  Wed Dec 10 07:20:40 2003
From: maya2006mb at yahoo.com (Maya Sanders)
Date: Tue, 9 Dec 2003 22:20:40 -0800 (PST)
Subject: [R] pvalues
Message-ID: <20031210062040.82800.qmail@web20413.mail.yahoo.com>

dear all-
If I have a vector of numbers (not necessarily
normally distributed) how can I get the p-value of a
number in this distribution.  I am interested in the
"inverse" of 'quantile' .
thank you-
Maya



From mihastaut at hotmail.com  Wed Dec 10 08:08:57 2003
From: mihastaut at hotmail.com (Miha STAUT)
Date: Wed, 10 Dec 2003 07:08:57 +0000
Subject: [R] RdbiPgSQL and POSIXct
Message-ID: <BAY2-F68IS6BcTeJ5Gw00014e29@hotmail.com>

Hi,

Does the Bioconductor library RdbiPgSQL support the POSIXlt and POSIXct (R 
time format)?. I tried to import an R data frame which had also some columns 
of class POSIXct into PostgreSQL 7.2.1 with the command dbWriteTable without 
success.

>str(jame)
`data.frame':	8123 obs. of  27 variables:
$ kat.st       : int  1 2 3 4 5 6 7 8 9 10 ...
$ ime          : Factor w/ 7867 levels "1133 VG","2202 VG",..: 7788 2605 
4387 4932 5246 5121 3584 3721 3552 4521 ...
$ sinonim      : Factor w/ 1385 levels "","7P2","7P4",..: 1 1 863 1 935 1286 
1 1 1 798 ...
$ tip1         : num  4.3 1.2 5.2 5.2 5.3 4.2 5.6 NA 5.5 5.3 ...
$ tip2         : num  NA NA NA NA NA NA NA NA NA NA ...
$ x            : int  5129740 5127853 5113656 5113664 5113120 5113630 
5088530 5089000 5087400 5086256 ...
$ y            : int  5408790 5411028 5443482 5443450 5446130 5446030 
5443880 5447000 5442150 5446013 ...
$ x.lj         : int  5129740 5127840 5113660 5113660 5113120 5113490 
5088520 5088000 5087400 5086240 ...
$ y.lj         : int  5408790 5411000 5443480 5443450 5446130 5446100 
5443880 5447000 5442150 5445980 ...
$ tk.25        : Factor w/ 159 levels "Ajdovina","Ba..",..: 116 116 56 56 
56 56 149 149 149 149 ...
$ ttn          : Factor w/ 1051 levels "Ajdovina 1",..: 890 65 849 849 851 
851 984 985 991 993 ...
$ lega.po      : Factor w/ 11 levels "","GPS","teodol..",..: 7 10 11 11 10 4 
4 1 7 11 ...
$ ko           : Factor w/ 837 levels "","Adleii",..: 633 633 588 588 657 
657 772 743 130 743 ...
$ upravna.enota: Factor w/ 54 levels "Ajdovina","Br..",..: 34 34 43 43 43 
43 52 52 26 52 ...
$ dolzina      : int  95 442 360 60 112 450 160 NA 23 130 ...
$ globina      : int  10 208 37 8 28 58 131 NA 23 42 ...
$ datum.obis   :`POSIXct', format: chr  "1925-07-11" "1958-08-01" 
"1925-07-26" "1925-07-26" ...
$ datum.zap    :`POSIXct', format: chr  NA NA NA NA ...
$ org          : Factor w/ 213 levels "","Anthron","Ar..",..: 76 179 76 76 
76 76 76 76 76 76 ...
$ udelezenci   : Factor w/ 3826 levels "","Abrahamsberg..",..: 2092 2722 
1184 1185 1427 1183 1426 2585 1909 10 ...
$ zapisnikar   : Factor w/ 589 levels "","Abrahamsberg..",..: 1 1 1 1 1 1 1 
345 345 345 ...
$ opombe       : Factor w/ 749 levels "","10","10t",..: 565 565 308 308 1 1 
318 31 28 308 ...
$ kat.st.VG    : int  NA NA NA NA NA NA NA NA NA NA ...
$ datum.vnosa  :`POSIXct', format: chr  "1990-09-18" "1990-09-19" 
"1993-03-01" "1990-09-18" ...
$ xpop         : num  129740 127853 113656 113664 113120 ...
$ ypop         : num  408790 411028 443482 443450 446130 ...
$ nv           : int  1525 650 780 780 451 439 497 470 483 634 ...

>dbWriteTable(conn,jame,"j")

The same command produced a good result with the date columns transformed to 
factor.

THE ERROR MESSAGE
NOTICE:  current transaction is aborted, queries ignored until end of 
transaction block # the same error message repeated many times (I guess 
length(jame$x))

Thanks in advance, Miha Staut



From ligges at statistik.uni-dortmund.de  Wed Dec 10 08:43:15 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 10 Dec 2003 08:43:15 +0100
Subject: [R] pvalues
In-Reply-To: <20031210062040.82800.qmail@web20413.mail.yahoo.com>
References: <20031210062040.82800.qmail@web20413.mail.yahoo.com>
Message-ID: <3FD6CE93.2020103@statistik.uni-dortmund.de>

Maya Sanders wrote:
> dear all-
> If I have a vector of numbers (not necessarily
> normally distributed) how can I get the p-value of a
> number in this distribution.  I am interested in the
> "inverse" of 'quantile' .
> thank you-
> Maya
> 


I'm not sure whether I got the point, but my guess is that you are 
trying to count how many observations in a vector x have values less 
than a and divide by the total number of observations in x, e.g.:

  x <- rnorm(20)
  a <- 0
  sum(x < a) / length(x)

Uwe Ligges



From rwehrens at sci.kun.nl  Wed Dec 10 09:41:04 2003
From: rwehrens at sci.kun.nl (Ron Wehrens)
Date: Wed, 10 Dec 2003 09:41:04 +0100
Subject: [R] problem with pls(x, y, ..., ncomp = 16): Error in inherit s( x,
	"data.frame") : subscript out of bounds
In-Reply-To: <3A822319EB35174CA3714066D590DCD50205CF2B@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50205CF2B@usrymx25.merck.com>
Message-ID: <200312100941.05042.rwehrens@sci.kun.nl>

On Wednesday 10 December 2003 05:14, Liaw, Andy wrote:
> My first guess was wrong (should have RTFM...).  Try:
>
>   m <- pls(x, y, validation = "CV", niter = 68, ncomp = 1:16)
>
> I'm not familiar enough with SIMPLS to know whether it makes sense to pass
> ncomp of length 1 to pls(), but the code as is won't work (unless ncomp=1).


> The problem is in the following two lines inside simpls():
>
>  YvarExpl <- matrix(0, length(ncomp), npred)
>  for (i in 1:max(ncomp)) YvarExpl[i, ] <- diag(cor(Y, X %*% B[, , i]))^2
>
> If ncomp is of length 1, then YvarExpl is a 1x1 matrix, so the loop will
> bomb if ncomp > 1.
>
> [I'm CC'ing Prof. Wehrens, who is the package maintainer.]
>
> HTH,
> Andy

Thanks for spotting this! I'll correct it asap.
Ron

-- 
Ron Wehrens            
Dept. of Chemometrics  
University of Nijmegen	Email: rwehrens at sci.kun.nl
Toernooiveld 1		http://www.cac.sci.kun.nl
6525 ED Nijmegen	Tel: +31 24 365 2053
The Netherlands		Fax: +31 24 365 2653



From r.hogendoorn at hitt.nl  Wed Dec 10 09:51:56 2003
From: r.hogendoorn at hitt.nl (Hogendoorn, Rene)
Date: Wed, 10 Dec 2003 09:51:56 +0100
Subject: [r] interfacing r and python in ms windows
Message-ID: <8C6ADF4772B7D511A3E40008C7BB878A0167A893@chopin.hitt.nl>

I have a windows rpy for python 2.3 if you like. 

Regards,
Ren? Hogendoorn



From l.houdusse at cerep.fr  Wed Dec 10 09:57:25 2003
From: l.houdusse at cerep.fr (Laurent Houdusse)
Date: Wed, 10 Dec 2003 09:57:25 +0100
Subject: [R] How to calculate standard error for a vector?
Message-ID: <BA420EFAAC96D311A7A0006097D37BDB0451591F@EOLE>

Hi all!

I 'm beginner and i develop a bio-application with VB and i need some
statistic functions!
could i calculate StdError, CoeffOfVariance, SumSquared with R langage? if
yes, what are functions to use?

I need also to use ANOVA and t-test...

Thanks for your help!



Laurent Houdusse 
Analyste Programmeur



From spencer.graves at pdf.com  Wed Dec 10 10:33:29 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 10 Dec 2003 01:33:29 -0800
Subject: [R] How to calculate standard error for a vector?
In-Reply-To: <BA420EFAAC96D311A7A0006097D37BDB0451591F@EOLE>
References: <BA420EFAAC96D311A7A0006097D37BDB0451591F@EOLE>
Message-ID: <3FD6E869.9030608@pdf.com>

      Have you read "An Introduction to R" and "The R Language 
Definition" that comes with the R installation?  Try "help.start()".  
Also, there are several good introductions downloadable from 
"www.r-project.org".  I suggest you also consider "lm" = linear model, e.g.,

      fit <- lm(y~x, dataFrame)  # where "y" and "x" are columns of the 
data.frame "dataFrame"
      fit
      summary(fit)
      anova(fit)

      Also, try "help.search('t test')".  A more specific question may 
yield a more specific answer, especially if accompanied by a few lines 
of R code providing a toy example that you've tried and couldn't make 
work. 

      hope this helps.      
      spencer graves

Laurent Houdusse wrote:

>Hi all!
>
>I 'm beginner and i develop a bio-application with VB and i need some
>statistic functions!
>could i calculate StdError, CoeffOfVariance, SumSquared with R langage? if
>yes, what are functions to use?
>
>I need also to use ANOVA and t-test...
>
>Thanks for your help!
>
>
>
>Laurent Houdusse 
>Analyste Programmeur
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From gb at stat.umu.se  Wed Dec 10 10:34:41 2003
From: gb at stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Wed, 10 Dec 2003 10:34:41 +0100
Subject: [R] Error in model.frame
Message-ID: <20031210093441.GA10352@stat.umu.se>

I recently ran into the following:

> x <- c(1,2,4,3)
> lm(x ~ 1:length(x))
Error in model.frame(formula, rownames, variables, varnames, extras, extranames,  : 
        variable lengths differ

but

> lm(x ~ c(1:length(x)))

Call:
lm(formula = x ~ c(1:length(x)))

Coefficients:
   (Intercept)  c(1:length(x))  
           0.5             0.8  

and

> identical(1:4, c(1:4))
[1] TRUE

Should I report this as a bug?

G?ran
-- 
 G?ran Brostr?m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume?, Sweden             e-mail: gb at stat.umu.se



From ramasamya at gis.a-star.edu.sg  Wed Dec 10 10:44:52 2003
From: ramasamya at gis.a-star.edu.sg (Adaikalavan RAMASAMY)
Date: Wed, 10 Dec 2003 17:44:52 +0800
Subject: [R] pvalues
Message-ID: <6D9E9B9DF347EF4385F6271C64FB8D560A0153@BIONIC.biopolis.one-north.com>

If you want to generate a p-value, what is the null hypothesis ?

--
Adaikalavan Ramasamy 


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Uwe Ligges
Sent: Wednesday, December 10, 2003 3:43 PM
To: Maya Sanders
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] pvalues


Maya Sanders wrote:
> dear all-
> If I have a vector of numbers (not necessarily
> normally distributed) how can I get the p-value of a
> number in this distribution.  I am interested in the "inverse" of 
> 'quantile' . thank you-
> Maya
> 


I'm not sure whether I got the point, but my guess is that you are 
trying to count how many observations in a vector x have values less 
than a and divide by the total number of observations in x, e.g.:

  x <- rnorm(20)
  a <- 0
  sum(x < a) / length(x)

Uwe Ligges

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From plummer at iarc.fr  Wed Dec 10 11:02:27 2003
From: plummer at iarc.fr (Martyn Plummer)
Date: 10 Dec 2003 11:02:27 +0100
Subject: [R] Error in model.frame
In-Reply-To: <20031210093441.GA10352@stat.umu.se>
References: <20031210093441.GA10352@stat.umu.se>
Message-ID: <1071050547.29191.2.camel@xena>

On Wed, 2003-12-10 at 10:34, G?ran Brostr?m wrote:
> I recently ran into the following:
> 
> > x <- c(1,2,4,3)
> > lm(x ~ 1:length(x))
> Error in model.frame(formula, rownames, variables, varnames, extras, extranames,  : 
>         variable lengths differ
> 
> but
> 
> > lm(x ~ c(1:length(x)))
> 
> Call:
> lm(formula = x ~ c(1:length(x)))
> 
> Coefficients:
>    (Intercept)  c(1:length(x))  
>            0.5             0.8  
> 
> and
> 
> > identical(1:4, c(1:4))
> [1] TRUE
> 
> Should I report this as a bug?

No. In a formula ":" is interpreted differently.
It is used to represent interactions. See help(formula).
It is probably good practice to generate variables outside of
the formula and then refer to them by name.

Martyn



From ramasamya at gis.a-star.edu.sg  Wed Dec 10 10:52:03 2003
From: ramasamya at gis.a-star.edu.sg (Adaikalavan RAMASAMY)
Date: Wed, 10 Dec 2003 17:52:03 +0800
Subject: [R] How to calculate standard error for a vector?
Message-ID: <6D9E9B9DF347EF4385F6271C64FB8D56076054@BIONIC.biopolis.one-north.com>

sd <- sqrt(var(x)) # standard deviation
cv <- sd / mean(x) # coefficient of variation
ss <- mean( x^2 )  # sum of squares - definitions vary !

If these are the statistics you are going to be using, it might be more
efficient to code it directly.

--
Adaikalavan Ramasamy 


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Laurent Houdusse
Sent: Wednesday, December 10, 2003 4:57 PM
To: 'r-help at stat.math.ethz.ch'
Subject: [R] How to calculate standard error for a vector?


Hi all!

I 'm beginner and i develop a bio-application with VB and i need some
statistic functions! could i calculate StdError, CoeffOfVariance,
SumSquared with R langage? if yes, what are functions to use?

I need also to use ANOVA and t-test...

Thanks for your help!



Laurent Houdusse 
Analyste Programmeur

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ligges at statistik.uni-dortmund.de  Wed Dec 10 11:02:27 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 10 Dec 2003 11:02:27 +0100
Subject: [R] Error in model.frame
In-Reply-To: <20031210093441.GA10352@stat.umu.se>
References: <20031210093441.GA10352@stat.umu.se>
Message-ID: <3FD6EF33.4040409@statistik.uni-dortmund.de>

G?ran Brostr?m wrote:
> I recently ran into the following:
> 
> 
>>x <- c(1,2,4,3)
>>lm(x ~ 1:length(x))
> 
> Error in model.frame(formula, rownames, variables, varnames, extras, extranames,  : 
>         variable lengths differ
> 
> but
> 
> 
>>lm(x ~ c(1:length(x)))
> 
> 
> Call:
> lm(formula = x ~ c(1:length(x)))
> 
> Coefficients:
>    (Intercept)  c(1:length(x))  
>            0.5             0.8  
> 
> and
> 
> 
>>identical(1:4, c(1:4))
> 
> [1] TRUE
> 
> Should I report this as a bug?

NO!!!
The ":" operator is used to model interactions in lm() and friends!

Use lm(x ~ I(1:length(x))) in order to perform the analysis above.

Uwe Ligges


> G?ran



From gb at stat.umu.se  Wed Dec 10 11:05:15 2003
From: gb at stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Wed, 10 Dec 2003 11:05:15 +0100
Subject: [R] Error in model.frame
In-Reply-To: <3FD6EF33.4040409@statistik.uni-dortmund.de>
References: <20031210093441.GA10352@stat.umu.se>
	<3FD6EF33.4040409@statistik.uni-dortmund.de>
Message-ID: <20031210100515.GA10457@stat.umu.se>

On  0, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> G?ran Brostr?m wrote:
> >I recently ran into the following:
> >
> >
> >>x <- c(1,2,4,3)
> >>lm(x ~ 1:length(x))
> >
> >Error in model.frame(formula, rownames, variables, varnames, extras, 
> >extranames,  : variable lengths differ
> >
> >but
> >
> >
> >>lm(x ~ c(1:length(x)))
> >
> >
> >Call:
> >lm(formula = x ~ c(1:length(x)))
> >
> >Coefficients:
> >   (Intercept)  c(1:length(x))  
> >           0.5             0.8  
> >
> >and
> >
> >
> >>identical(1:4, c(1:4))
> >
> >[1] TRUE
> >
> >Should I report this as a bug?
> 
> NO!!!
> The ":" operator is used to model interactions in lm() and friends!

I forgot that! Thanks to all who remembered.

G?ran



From l.houdusse at cerep.fr  Wed Dec 10 11:14:44 2003
From: l.houdusse at cerep.fr (Laurent Houdusse)
Date: Wed, 10 Dec 2003 11:14:44 +0100
Subject: [R] How to calculate standard error for a vector?
Message-ID: <BA420EFAAC96D311A7A0006097D37BDB04515920@EOLE>

Thanks for this formulae

For stdError, i found this:
stdError <- sd(x)/sqrt(length(x))


Laurent Houdusse
Analyste Programmeur



-----Message d'origine-----
De : Adaikalavan RAMASAMY [mailto:ramasamya at gis.a-star.edu.sg] 
Envoy? : mercredi 10 d?cembre 2003 10:52
? : Laurent Houdusse; r-help at stat.math.ethz.ch
Objet : RE: [R] How to calculate standard error for a vector?


sd <- sqrt(var(x)) # standard deviation
cv <- sd / mean(x) # coefficient of variation
ss <- mean( x^2 )  # sum of squares - definitions vary !

If these are the statistics you are going to be using, it might be more
efficient to code it directly.

--
Adaikalavan Ramasamy 


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Laurent Houdusse
Sent: Wednesday, December 10, 2003 4:57 PM
To: 'r-help at stat.math.ethz.ch'
Subject: [R] How to calculate standard error for a vector?


Hi all!

I 'm beginner and i develop a bio-application with VB and i need some
statistic functions! could i calculate StdError, CoeffOfVariance, SumSquared
with R langage? if yes, what are functions to use?

I need also to use ANOVA and t-test...

Thanks for your help!



Laurent Houdusse 
Analyste Programmeur

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From kdb at kvl.dk  Wed Dec 10 12:16:17 2003
From: kdb at kvl.dk (Karsten D Bjerre)
Date: Wed, 10 Dec 2003 12:16:17 +0100
Subject: [R] NLME and limits on parameter space
Message-ID: <sfd70eae.003@gwia.kvl.dk>

Dear R-users,

How can I impose limits on parameter estimates in NLME as indicated in
the second #'ed line below? 

Can I use the naPattern, some thing like this:  
       nlme(response = yftp(x,ii,cc,b,dd),... , naPattern=
is.na(response) ) ?

yftp<-function(x,ii,cc,b,d){
  tpx<-tpx(ii,cc,b)    # x value for top point of 2nd order polonomial
  tpy<-tpy(ii,cc,b)
  #  if (ii<0 || ii>1 || cc<0|| cc>1|| b<0) {print(NA); return(NA)}
  return( (ii+b*x)*(cc-x)*(x<=tpx) + tpy*(x>tpx)+ d*x)
}

Best whishes,
Karsten



From valtteri at biotech.kth.se  Wed Dec 10 12:31:51 2003
From: valtteri at biotech.kth.se (Valtteri Wirta)
Date: Wed, 10 Dec 2003 12:31:51 +0100
Subject: [R] How to remove extra spaces introduced (?) by write.table
Message-ID: <5.1.0.14.0.20031210114632.02b18d80@kiev.biotech.kth.se>

Dear Group,

My question relates to the write.table function.

I have created a data.frame containing both "character" and "numeric" columns.
When I use the write.table function to write this table into a text file 
spaces are added into the "numeric" columns so that the number of digits in 
each column is the same. This extra space is added before the actual number.

I write the table using the following command:
write.table(out.obj, sep="\t", file="test.txt", row.names=FALSE, na="NaN", 
quote=FALSE)
where out.obj is the data.frame

A simplified example with three columns
Sample1\t10\t10
Sample2\t*1\t*1

The * denotes the extra space, \t is the tab used as separator
(a real example can be found at 
http://biobase.biotech.kth.se/~valtteri/test.txt)

I need to remove these spaces as they are not compatible with the 
down-stream application. I'd prefer not to remove these manually using for 
example MS-Word as the number of files generated is fairly large.

I verified the modes of the problematic columns and they are, as expected, 
"numeric".

My question is how to remove these extra spaces?
SUB did not work (good for "character" vectors).


I'm using R.1.8.0 running under Windows XP
A saved copy of the data.frame can be found at 
http://biobase.biotech.kth.se/~valtteri/test.RData
(saved with ascii=TRUE)


All help is appreciated.

Thanks,

Valtteri





Contact information:

Valtteri Wirta
Department of Biotechnology, KTH
AlbaNova University Center
S - 10691 Stockholm, Sweden

Visiting address: Roslagstullsbacken 21, B3
Phone: +46 8 5537 8344(office)
Phone: +46 733 386 341 (gsm)
Fax: +46 8 5537 8481
Email: valtteri at biotech.kth.se
Web: www.biotech.kth.se/molbio
MSN messenger: valzu at hotmail.com



From f.calboli at ucl.ac.uk  Wed Dec 10 13:50:32 2003
From: f.calboli at ucl.ac.uk (Federico Calboli)
Date: 10 Dec 2003 12:50:32 +0000
Subject: [R] Re: p-value from chisq.test working strangely on 1.8.1
Message-ID: <1071060632.2957.6.camel@monkey>

Mostly for the record, it happens also on Mandrake 9.1 with R 1.8.1
installed from rpm.



>chisq.test(matrix(c(0, 1, 1, 12556), 2, 2), simulate.p.value=TRUE)
 
       Pearson's Chi-squared test with simulated p-value (based on 2000
        replicates)
data:  matrix(c(0, 1, 1, 12556), 2, 2)
X-squared = 1e-04, df = NA, p-value = 5e-04

Federico Calboli

-- 



=================================

Federico C. F. Calboli

PLEASE NOTE NEW ADDRESS

Dipartimento di Biologia
Via Selmi 3
40126 Bologna
Italy

tel (+39) 051 209 4187
fax (+39) 051 251 208

f.calboli at ucl.ac.uk



From ripley at stats.ox.ac.uk  Wed Dec 10 12:54:53 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 10 Dec 2003 11:54:53 +0000 (GMT)
Subject: [R] How to remove extra spaces introduced (?) by write.table
In-Reply-To: <5.1.0.14.0.20031210114632.02b18d80@kiev.biotech.kth.se>
Message-ID: <Pine.LNX.4.44.0312101153190.9884-100000@gannet.stats>

Just convert to character before using write.table:

> as.character(c(1, 10))
[1] "1"  "10"

whereas

> format(c(1,10))
[1] " 1" "10"

is what you are seeing.

On Wed, 10 Dec 2003, Valtteri Wirta wrote:

> Dear Group,
> 
> My question relates to the write.table function.
> 
> I have created a data.frame containing both "character" and "numeric" columns.
> When I use the write.table function to write this table into a text file 
> spaces are added into the "numeric" columns so that the number of digits in 
> each column is the same. This extra space is added before the actual number.
> 
> I write the table using the following command:
> write.table(out.obj, sep="\t", file="test.txt", row.names=FALSE, na="NaN", 
> quote=FALSE)
> where out.obj is the data.frame
> 
> A simplified example with three columns
> Sample1\t10\t10
> Sample2\t*1\t*1
> 
> The * denotes the extra space, \t is the tab used as separator
> (a real example can be found at 
> http://biobase.biotech.kth.se/~valtteri/test.txt)
> 
> I need to remove these spaces as they are not compatible with the 
> down-stream application. I'd prefer not to remove these manually using for 
> example MS-Word as the number of files generated is fairly large.
> 
> I verified the modes of the problematic columns and they are, as expected, 
> "numeric".
> 
> My question is how to remove these extra spaces?
> SUB did not work (good for "character" vectors).
> 
> 
> I'm using R.1.8.0 running under Windows XP
> A saved copy of the data.frame can be found at 
> http://biobase.biotech.kth.se/~valtteri/test.RData
> (saved with ascii=TRUE)
> 
> 
> All help is appreciated.
> 
> Thanks,
> 
> Valtteri
> 
> 
> 
> 
> 
> Contact information:
> 
> Valtteri Wirta
> Department of Biotechnology, KTH
> AlbaNova University Center
> S - 10691 Stockholm, Sweden
> 
> Visiting address: Roslagstullsbacken 21, B3
> Phone: +46 8 5537 8344(office)
> Phone: +46 733 386 341 (gsm)
> Fax: +46 8 5537 8481
> Email: valtteri at biotech.kth.se
> Web: www.biotech.kth.se/molbio
> MSN messenger: valzu at hotmail.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Wed Dec 10 13:26:54 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 10 Dec 2003 13:26:54 +0100
Subject: [R] OT: BibTex year-only citation in text?
In-Reply-To: <9F171A0DEB645643BD848C4CF0FB36840430B080@dna.sequenom.com>
References: <9F171A0DEB645643BD848C4CF0FB36840430B080@dna.sequenom.com>
Message-ID: <x2ekvc6epd.fsf@biostat.ku.dk>

Matt Nelson <MNelson at sequenom.com> writes:

> Jason,
> 
> For many bibliography styles, the command \citeyear{key} will work.  If this
> doesn't work for the style you are using, you can investigate style-specific
> methods or consider other styles.  I find that natbib is good for
> author-year formats.

Yup. \usepackage[round]{natbib} is what got used for ISwR, combined
with a hacked plainnat.bst file to conform with Springer's standards.
 
> If you use Latex more than on occasion, a good reference book is invaluable.
> I like "The Latex Companion" by Goossens, Mittelbach, and Samarin.

Yup again. It's a little incoherent at times, but it has the crucial
pointers about where to go looking for things.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Peter.Ruckdeschel at uni-bayreuth.de  Wed Dec 10 14:18:27 2003
From: Peter.Ruckdeschel at uni-bayreuth.de (Peter Ruckdeschel)
Date: Wed, 10 Dec 2003 14:18:27 +0100
Subject: [R] setMethod("min", "myclass", ...)
In-Reply-To: <3FCE0966.E2720D2E@research.bell-labs.com>
References: <Pine.LNX.4.44.0312021845550.3778-100000@spock.vulcan>
	<3FCE0966.E2720D2E@research.bell-labs.com>
Message-ID: <3FD71D23.3060500@uni-bayreuth.de>


Hello,

first of all thank you for your reply to our help request.

On Wed, 3 Dec 2003, John Chambers wrote:

>Thomas Stabla wrote:
>  
>
>>I have defined a new class
>>    
>>
>>>setClass("myclass", representation(min = "numeric", max = "numeric"))
>>>      
>>>
>>and want to write accessor functions, so that for
>>    
>>
>>>foo = new("myclass", min = 0, max = 1)
>>>min(foo) # prints 0
>>>max(foo) # prints 1
>>>      
>>>
>>At first i created a generic function for "min"
>>    
>>
>>>setGeneric("min", function(..., na.rm = FALSE) standardGeneric("min"))
>>>      
>>>
>>and then tried to use setMethod. And there's my problem, I don't know the
>>name of the first argument which is to be passed to "min".
>>I can't just write:
>>    
>>
>>>setMethod("min", "myclass", function(..., na.rm = FALSE) ... at min)
>>>      
>>>
>>The same problem occurs with "max".
>>    
>>
>
>Generally, it's not a good idea to take a well-known function name and
>make it into an accessor function.
>  
>

We do agree with you in that point in general.

Perhaps we should be a little more precise with our problem:
We are working on a distribution class which is to provide an object
orientated way to allow for (semi--)automatical generation of new
distributions out of existing ones

that is, we want to allow for expressions like
  X+Y        for X~F ,  Y~G,  F,G  implementated distributions
and then to automatically generate the [r,d,p,q]-functions for X+Y

another aim will be to allow to pass distributions as
arguments to other functions etc...

In our case, the function   (min)   already  takes the role of a 
parameter to  *base* functions, i.e. runif, dunif, punif, qunif, 
in the sequel denoted by [r,d,p,q]unif;

the same argument applies to the functions   (mean)    and   (sd)   
which are parameters to [r,d,p,q]norm.

As we want to build distribution classes with a parameter slot using
these functions, it seemed natural to us to use the argument names of
these existing functions.
Perhaps a simple compromise (which we would tend to use now) is to use 
capitalized versions of the names for our slots.

>In a functional language, basic function calls such as min(x), sin(x),
>etc. should have a natural interpretation.  Defining methods for these
>functions is meant to do what it says:  provide a method to achieve the
>general purpose of the function for particular objects.
>  
>
We agree with you again, and in the particular case of our distribution

(a) min, max applied to a distribution should well be interpreted as min
and max of the support of this distribution

(b) mean and sd for a normal distribution also have their natural meaning

So in both cases, these functions do have a natural interpretation and 
do provide a method to achieve the general purpose of the function to 
our object.

>If you want accessor functions, they should probably have names that
>make their purpose obvious.  One convention, a la Java properties, would
>be getMin(x), etc. (the capitalizing is potentially an issue since slot
>names are case sensitive).
>
We do not like this so much, as we would prefer keeping as close as
possible to the naming convention of the parameters already defined
in the [r,d,p,q]-<distribution> functions in the base package.
So probably capitalizing is the thing we should do.

> .... [Rest of reply snipped]...

Thank you once again for your attention,

-- 
Peter Ruckdeschel 
Thomas Stabla
Matthias Kohl
Florian Camphausen



From e.kalmbach at biol.rug.nl  Wed Dec 10 14:46:49 2003
From: e.kalmbach at biol.rug.nl (Ellen Kalmbach)
Date: Wed, 10 Dec 2003 14:46:49 +0100
Subject: [R] Scatterplot axes
Message-ID: <002a01c3bf24$0fb8c100$c3847d81@biol.rug.nl>

Please, could someone help me figure out what seems to be a very simple
problem (and is still taking me hours...).
I want to draw a simple scatterplot but with 'equal' axes, i.e. I want both
axes to go from -3 to 3. Values for x lie between -2 and 0.5, values for y
between -2.2 and 3. I have tried 'usr' and 'eqscplot' and a few other
options, but it doesn't give me the desired result.
Many thanks!!!
Ellen



From clists at perrin.socsci.unc.edu  Wed Dec 10 14:52:03 2003
From: clists at perrin.socsci.unc.edu (Andrew Perrin)
Date: Wed, 10 Dec 2003 08:52:03 -0500 (EST)
Subject: [R] OT: BibTex year-only citation in text?
In-Reply-To: <3FD66695.2020603@indigoindustrial.co.nz>
References: <3FD66695.2020603@indigoindustrial.co.nz>
Message-ID: <Pine.LNX.4.53.0312100851140.8582@perrin.socsci.unc.edu>

Generally, use natbib style and then

\citep    -> (Anderson 1992)
\citet    -> Anderson (1992)
\citeyearpar -> (1992)
\citealt  -> Anderson 1992

----------------------------------------------------------------------
Andrew J Perrin - http://www.unc.edu/~aperrin
Assistant Professor of Sociology, U of North Carolina, Chapel Hill
clists at perrin.socsci.unc.edu * andrew_perrin (at) unc.edu


On Wed, 10 Dec 2003, Jason Turner wrote:

> Sorry for the off-topic question, but I know there are some talented
> LaTeX users out there. Which bibliography style gives only the year in
> text citations (e.g "for further details, see Anderson (1992)" )?
>
> Thanks
>
> Jason
> --
> Indigo Industrial Controls Ltd.
> http://www.indigoindustrial.co.nz
> 64-21-343-545
> jasont at indigoindustrial.co.nz
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From Bernhard.Pfaff at drkw.com  Wed Dec 10 14:54:48 2003
From: Bernhard.Pfaff at drkw.com (Pfaff, Bernhard)
Date: Wed, 10 Dec 2003 14:54:48 +0100
Subject: [R] Scatterplot axes
Message-ID: <18D602BD42B7E24EB810D6454A58DB900473076D@ibfftce505.is.de.dresdnerkb.com>

> Please, could someone help me figure out what seems to be a 
> very simple
> problem (and is still taking me hours...).
> I want to draw a simple scatterplot but with 'equal' axes, 
> i.e. I want both
> axes to go from -3 to 3. Values for x lie between -2 and 0.5, 
> values for y

Hello Ellen,

how about:

(x <- seq(-2, 0.5, length=10))
(y <- seq(-3, 3, length=10))
plot(y~x, xlim=c(-3,3), ylim=c(-3, 3))

HTH,
Bernhard



> between -2.2 and 3. I have tried 'usr' and 'eqscplot' and a few other
> options, but it doesn't give me the desired result.
> Many thanks!!!
> Ellen
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 


--------------------------------------------------------------------------------
The information contained herein is confidential and is intended solely for the
addressee. Access by any other party is unauthorised without the express
written permission of the sender. If you are not the intended recipient, please
contact the sender either via the company switchboard on +44 (0)20 7623 8000, or
via e-mail return. If you have received this e-mail in error or wish to read our
e-mail disclaimer statement and monitoring policy, please refer to 
http://www.drkw.com/disc/email/ or contact the sender.



From Torsten.Hothorn at rzmail.uni-erlangen.de  Wed Dec 10 14:56:32 2003
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Wed, 10 Dec 2003 14:56:32 +0100 (CET)
Subject: [R] OT: BibTex year-only citation in text?
In-Reply-To: <x2ekvc6epd.fsf@biostat.ku.dk>
References: <9F171A0DEB645643BD848C4CF0FB36840430B080@dna.sequenom.com>
	<x2ekvc6epd.fsf@biostat.ku.dk>
Message-ID: <Pine.LNX.4.51.0312101452560.30579@artemis.imbe.med.uni-erlangen.de>

>
> > Jason,
> >
> > For many bibliography styles, the command \citeyear{key} will work.  If this
> > doesn't work for the style you are using, you can investigate style-specific
> > methods or consider other styles.  I find that natbib is good for
> > author-year formats.
>
> Yup. \usepackage[round]{natbib} is what got used for ISwR, combined
> with a hacked plainnat.bst file to conform with Springer's standards.
>

producing you own BibTeX-style file by answering the questions from `latex
makebst' is almost painless.

Torsten



From GPetris at uark.edu  Wed Dec 10 14:58:26 2003
From: GPetris at uark.edu (Giovanni Petris)
Date: Wed, 10 Dec 2003 07:58:26 -0600 (CST)
Subject: [R] Scatterplot axes
In-Reply-To: <002a01c3bf24$0fb8c100$c3847d81@biol.rug.nl> (message from Ellen
	Kalmbach on Wed, 10 Dec 2003 14:46:49 +0100)
References: <002a01c3bf24$0fb8c100$c3847d81@biol.rug.nl>
Message-ID: <200312101358.hBADwQrG012933@definetti.uark.edu>


Have you tried specifying xlim and ylim? See ?par.

-- 

 __________________________________________________
[                                                  ]
[ Giovanni Petris                 GPetris at uark.edu ]
[ Department of Mathematical Sciences              ]
[ University of Arkansas - Fayetteville, AR 72701  ]
[ Ph: (479) 575-6324, 575-8630 (fax)               ]
[ http://definetti.uark.edu/~gpetris/              ]
[__________________________________________________]

> Date: Wed, 10 Dec 2003 14:46:49 +0100
> From: Ellen Kalmbach <e.kalmbach at biol.rug.nl>
> Sender: r-help-bounces at stat.math.ethz.ch
> Precedence: list
> 
> Please, could someone help me figure out what seems to be a very simple
> problem (and is still taking me hours...).
> I want to draw a simple scatterplot but with 'equal' axes, i.e. I want both
> axes to go from -3 to 3. Values for x lie between -2 and 0.5, values for y
> between -2.2 and 3. I have tried 'usr' and 'eqscplot' and a few other
> options, but it doesn't give me the desired result.
> Many thanks!!!
> Ellen
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
>



From tblackw at umich.edu  Wed Dec 10 15:02:03 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Wed, 10 Dec 2003 09:02:03 -0500 (EST)
Subject: [R] Scatterplot axes
In-Reply-To: <002a01c3bf24$0fb8c100$c3847d81@biol.rug.nl>
References: <002a01c3bf24$0fb8c100$c3847d81@biol.rug.nl>
Message-ID: <Pine.SOL.4.58.0312100854360.9671@robotron.gpcc.itd.umich.edu>

Ellen  -

plot(my.x.vector, my.y.vector, xlim=c(-3,3), ylim=c(-3,3))

It's the named arguments xlim and ylim that you were looking for.
I frequently set them as  xlim = 3 * c(-1,1), ylim = 3 * c(-1,1)
so that I can change the range by editing just one number rather
than two.  For an added fillip, try adding the argument  asp=1.
This will extend the range of one axis or the other by just
enough so that the graphical scale is the same on both axes,
that is, a one unit increase is the same number of millimeters
on the horizontal and the vertical axes.  It's very cool.  It
does, completely automatically, what I used to do painstakingly
by trial and error, tinkering with the width of the top and side
margins, until the graphical scales were equal.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Wed, 10 Dec 2003, Ellen Kalmbach wrote:

> Please, could someone help me figure out what seems to be a very simple
> problem (and is still taking me hours...).
> I want to draw a simple scatterplot but with 'equal' axes, i.e. I want both
> axes to go from -3 to 3. Values for x lie between -2 and 0.5, values for y
> between -2.2 and 3. I have tried 'usr' and 'eqscplot' and a few other
> options, but it doesn't give me the desired result.
> Many thanks!!!
> Ellen
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From p.dalgaard at biostat.ku.dk  Wed Dec 10 15:15:09 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 10 Dec 2003 15:15:09 +0100
Subject: [R] OT: BibTex year-only citation in text?
In-Reply-To: <Pine.LNX.4.51.0312101452560.30579@artemis.imbe.med.uni-erlangen.de>
References: <9F171A0DEB645643BD848C4CF0FB36840430B080@dna.sequenom.com>
	<x2ekvc6epd.fsf@biostat.ku.dk>
	<Pine.LNX.4.51.0312101452560.30579@artemis.imbe.med.uni-erlangen.de>
Message-ID: <x265go69oy.fsf@biostat.ku.dk>

Torsten Hothorn <Torsten.Hothorn at rzmail.uni-erlangen.de> writes:

> >
> > > Jason,
> > >
> > > For many bibliography styles, the command \citeyear{key} will work.  If this
> > > doesn't work for the style you are using, you can investigate style-specific
> > > methods or consider other styles.  I find that natbib is good for
> > > author-year formats.
> >
> > Yup. \usepackage[round]{natbib} is what got used for ISwR, combined
> > with a hacked plainnat.bst file to conform with Springer's standards.
> >
> 
> producing you own BibTeX-style file by answering the questions from `latex
> makebst' is almost painless.

Thanks for the tip. Didn't know about that. "*Almost* painless" is
precise, it is a long dialogue and I did manage to skip a crucial item
on the way through.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From alessandro.semeria at cramont.it  Wed Dec 10 15:22:59 2003
From: alessandro.semeria at cramont.it (alessandro.semeria@cramont.it)
Date: Wed, 10 Dec 2003 15:22:59 +0100
Subject: [R] Scatterplot axes
Message-ID: <OFAC6D94FB.15450670-ONC1256DF8.004ECEDC@tomware.it>


plot(y~x,xlim=range(-3,3),ylim=range(-3,3))

Best
A.S.

----------------------------

Alessandro Semeria
Models and Simulations Laboratory
Montecatini Environmental Research Center (Edison Group),
Via Ciro Menotti 48,
48023 Marina di Ravenna (RA), Italy
Tel. +39 544 536811
Fax. +39 544 538663
E-mail: alessandro.semeria at cramont.it



                                                                                                                                            
                      "Ellen Kalmbach"                                                                                                      
                      <e.kalmbach at biol.rug.         To:      <r-help at stat.math.ethz.ch>                                                     
                      nl>                           cc:                                                                                     
                      Sent by:                      Subject: [R] Scatterplot axes                                                           
                      r-help-bounces at stat.m                                                                                                 
                      ath.ethz.ch                                                                                                           
                                                                                                                                            
                                                                                                                                            
                      10-12-2003 14.46                                                                                                      
                                                                                                                                            
                                                                                                                                            




Please, could someone help me figure out what seems to be a very simple
problem (and is still taking me hours...).
I want to draw a simple scatterplot but with 'equal' axes, i.e. I want both
axes to go from -3 to 3. Values for x lie between -2 and 0.5, values for y
between -2.2 and 3. I have tried 'usr' and 'eqscplot' and a few other
options, but it doesn't give me the desired result.
Many thanks!!!
Ellen

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From mihastaut at hotmail.com  Wed Dec 10 15:33:48 2003
From: mihastaut at hotmail.com (Miha STAUT)
Date: Wed, 10 Dec 2003 14:33:48 +0000
Subject: [R] custom kernel
Message-ID: <BAY2-F127zh67jOvX1r0000299f@hotmail.com>

Hi,

I would like to estimate a 2D density on a matrix but not with a predefined 
kernel shape and density. I am sure there is a way to do this only I did not 
find it. Any help is kindly appreciated.

Thanks, Miha Staut



From pallier at lscp.ehess.fr  Wed Dec 10 16:00:06 2003
From: pallier at lscp.ehess.fr (Christophe Pallier)
Date: Wed, 10 Dec 2003 16:00:06 +0100
Subject: [R] PROC MIXED vs. lme()
Message-ID: <3FD734F6.70903@lscp.ehess.fr>

Hi

 >I'm trying to learn how to do a repeated measures ANOVA in R using lme().
 >In SAS, I use the code:
 >
 >PROC MIXED DATA=[data set below];
 >  CLASS sub group trial;
 >  MODEL dv = group trial group*trial;
 >  REPEATED trial / SUBJECT=sub TYPE=CS;
 >
 >In R, I'm trying the code:
 >
 >results.cs <- lme(DV ~ factor(GROUP)*factor(TRIAL), data=[data set below],
 >random= ~factor(TRIAL)|SUB, correlation=corCompSymm() )
 >anova(results.cs)

Try

$ anova(lme(DV ~ GROUP*TRIAL,random= ~1|SUB, correlation=corCompSymm() ))

It yields the correct result (I converted all the factors into factors). 
Trial is a fixed, not random factor.

Actually, you do not need lme for to run a repeated measure anova.
You could use the aov function:

$ summary(aov(DV~GROUP*TRIAL+Error(SUB/TRIAL)))

This, again, yields the correct results.
Hope this helps,


Christophe Pallier
http://www.pallier.org



From eesteves at ualg.pt  Wed Dec 10 16:14:14 2003
From: eesteves at ualg.pt (eesteves@ualg.pt)
Date: Wed, 10 Dec 2003 15:14:14 +0000
Subject: [R] factorial experiments and repeated measures
Message-ID: <1071069254.3fd738460dce4@wmail.ualg.pt>

Dear All,
I'm new to the list and relatively new to R (but not so to S-Plus) and I?ve 
been asked to help study some experimental data.

In a factorial experiment (5 factors at 2 levels) the response-variable was 
measured at various times (0, 2, 5, 7, ..., 30 days after start). There's just 
one run for each factor combination! I intend to use the nlme library of 
Pinheiro and Bates in a longitudinal data situation but I have some 
difficulties with the "statistical scenario" of it all! 

The factors are not nested and the number of repeated measures is unbalanced 
(in some cases they were abandoned for some reason before the end of the 
experiment). The main objectives are to identify the factor(s) that influence 
the response and to model it.

How should I dissecate this? If in need of more info, just ask!

Thanks. Best regards, Eduardo Esteves



From pallier at lscp.ehess.fr  Wed Dec 10 16:04:15 2003
From: pallier at lscp.ehess.fr (Christophe Pallier)
Date: Wed, 10 Dec 2003 16:04:15 +0100
Subject: [R] PROC MIXED vs. lme()
Message-ID: <3FD735EF.4040007@lscp.ehess.fr>

 >I'm trying to learn how to do a repeated measures ANOVA in R using > 
lme().
 >In SAS, I use the code:
 >
 >PROC MIXED DATA=[data set below];
 >  CLASS sub group trial;
 >  MODEL dv = group trial group*trial;
 >  REPEATED trial / SUBJECT=sub TYPE=CS;
 >run;

 >
 >In R, I'm trying the code:
 >
 >results.cs <- lme(DV ~ factor(GROUP)*factor(TRIAL), data=[data set below],
 >random= ~factor(TRIAL)|SUB, correlation=corCompSymm() )
 >anova(results.cs)

Try

anova(lme(DV ~ GROUP*TRIAL,random= ~1|SUB, correlation=corCompSymm() ))

This yields the correct results (I converted all the factors into... 
factors). Note that the 'trial' factor is fixed.

Actually you do not need lme for that. You could use the aov function:

summary(aov(DV~GROUP*TRIAL+Error(SUB/TRIAL)))

(it works well because the data is balanced)


Christophe Pallier
http://www.pallier.org



From jfri at novozymes.com  Wed Dec 10 16:43:12 2003
From: jfri at novozymes.com (JFRI (Jesper Frickmann))
Date: Wed, 10 Dec 2003 10:43:12 -0500
Subject: [R] Windows Memory Issues
Message-ID: <D53147E531BFBC4B8853FD134FAEE44D1547E5@exusfr014.novo.dk>

I recommend you get the latest version 1.8.1 beta. I also had some
memory problems and that fixed it.

Kind regards, 
Jesper Frickmann 
Statistician, Quality Control 
Novozymes North America Inc. 
Tel. +1 919 494 3266
Fax +1 919 494 3460



From dmurdoch at pair.com  Wed Dec 10 16:56:56 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Wed, 10 Dec 2003 10:56:56 -0500
Subject: [R] Windows Memory Issues
In-Reply-To: <D53147E531BFBC4B8853FD134FAEE44D1547E5@exusfr014.novo.dk>
References: <D53147E531BFBC4B8853FD134FAEE44D1547E5@exusfr014.novo.dk>
Message-ID: <tbgetv0pp2pr4gooo9d0lfcp5bla0dh1kf@4ax.com>

On Wed, 10 Dec 2003 10:43:12 -0500, "JFRI (Jesper Frickmann)"
<jfri at novozymes.com> wrote :

>I recommend you get the latest version 1.8.1 beta. I also had some
>memory problems and that fixed it.

1.8.1 has been released, so there's no more beta.  The release is
available on CRAN (see http://cran.r-project.org/bin/windows/base for
the Windows binary build). A patched version, occasionally updated on
my web site (binary occasionally updated as
http://www.stats.uwo.ca/faculty/murdoch/software/r-devel/rw1081pat.exe)
is also worth looking at if the release still has the bug.

Duncan Murdoch



From jwdougherty at mcihispeed.net  Wed Dec 10 17:04:32 2003
From: jwdougherty at mcihispeed.net (John Dougherty)
Date: Wed, 10 Dec 2003 08:04:32 -0800
Subject: [R] Font problem
Message-ID: <200312100804.32416.jwdougherty@mcihispeed.net>

Thank you for the help.  The problem was, as Peter and Marc suggested, that 
the 100 dpi fonts were missing.  Installing them made the difference.  

John



From andy_liaw at merck.com  Wed Dec 10 19:21:10 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 10 Dec 2003 13:21:10 -0500
Subject: [R] configure stuck in checking leap seconds (R-1.8.1 on AIX)
Message-ID: <3A822319EB35174CA3714066D590DCD50205CF33@usrymx25.merck.com>

Dear R-help,

We've been trying, so far unsuccessfully, to compile R as 64-bit under AIX
5.1.  Following the recent post by Dr. Christoph Pospiech, I started with
R-1.8.1 and manually edited the configure script according to the .diff
file.  Part of the diff has:

***************
*** 24446,24452 ****
  
  int main () {
    struct tm *tm;
!   time_t ct;
  
    ctime(&ct);
    ct = ct - (ct % 60);
--- 24453,24459 ----
  
  int main () {
    struct tm *tm;
!   time_t ct=12345;
  
    ctime(&ct);
    ct = ct - (ct % 60);

I do not know what the !'s meant, so I left the corresponding part in the
configure script alone.  However, when I ran the configure script, it always
seem to go into an infinite loop when checking whether leap seconds are
treated according to POSIX.  Does anyone have any idea how to get around
this?

I cheated the test by editing the test code in the configure script so it
will go on.  After that I was able to compile R.  (I did have to edit one
more thing in the configure script: changing /lib/crt0.o to /lib/crt0_64.o.)
Unfortunately, make check all failed in tests/p-r-random-tests.Rout.fail:

> dkwtest("beta",shape1 = 2, shape2 = 1)
beta(shape1 = 2, shape2 = 1) FAILED
Error in dkwtest("beta", shape1 = 2, shape2 = 1) : 
        dkwtest failed
Execution halted

Can anyone help?

Best,
Andy

Andy Liaw, PhD
Biometrics Research      PO Box 2000, RY33-300     
Merck Research Labs           Rahway, NJ 07065
mailto:andy_liaw at merck.com        732-594-0820



From vdetours at ulb.ac.be  Wed Dec 10 19:36:28 2003
From: vdetours at ulb.ac.be (Vincent Detours)
Date: Wed, 10 Dec 2003 19:36:28 +0100 (MET)
Subject: [R] an eval/parse trivia
Message-ID: <Pine.SOL.3.96.1031210191621.12509T-100000@mach.vub.ac.be>

Dear all,

Any cue on how to evaluate x?

> x
[1] structure(c("GO:0004707", "GO:0005524", "GO:0004674",
"GO:0006468", "GO:0000074", "GO:0008372", "GO:0016740"), .Names =
c("NAS", "NAS", "IEA", "IDA", "NAS", "ND", "IEA"))
6204 Levels: GO:0000074 GO:0000158 GO:0000163 GO:0000166 ...
structure(c("GO:0019538", "GO:0016706"), .Names =
c("IEA", "IEA"))
> parse(text=x) #a failed attempt
expression(2287)
> eval(parse(text=x))
[1] 2287
>

Any idea? By the way why does parse return 2287? 

Thanks for your help,

Vincent Detours



From tblackw at umich.edu  Wed Dec 10 19:54:33 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Wed, 10 Dec 2003 13:54:33 -0500 (EST)
Subject: [R] an eval/parse trivia
In-Reply-To: <Pine.SOL.3.96.1031210191621.12509T-100000@mach.vub.ac.be>
References: <Pine.SOL.3.96.1031210191621.12509T-100000@mach.vub.ac.be>
Message-ID: <Pine.SOL.4.58.0312101350330.22870@robotron.gpcc.itd.umich.edu>

Vincent  -

>From the values shown, this looks like a Bioconductor question,
rather than base R.  You might try the maintainers of whatever
package the function comes from.

Is 2287 the index in "levels" for one of the character strings
shown ?

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Wed, 10 Dec 2003, Vincent Detours wrote:

> Dear all,
>
> Any cue on how to evaluate x?
>
> > x
> [1] structure(c("GO:0004707", "GO:0005524", "GO:0004674",
> "GO:0006468", "GO:0000074", "GO:0008372", "GO:0016740"), .Names =
> c("NAS", "NAS", "IEA", "IDA", "NAS", "ND", "IEA"))
> 6204 Levels: GO:0000074 GO:0000158 GO:0000163 GO:0000166 ...
> structure(c("GO:0019538", "GO:0016706"), .Names =
> c("IEA", "IEA"))
> > parse(text=x) #a failed attempt
> expression(2287)
> > eval(parse(text=x))
> [1] 2287
> >
>
> Any idea? By the way why does parse return 2287?
>
> Thanks for your help,
>
> Vincent Detours
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From Dan.Olsen at byu.edu  Wed Dec 10 20:11:14 2003
From: Dan.Olsen at byu.edu (Dan Olsen)
Date: Wed, 10 Dec 2003 12:11:14 -0700
Subject: [R] (no subject)
Message-ID: <83A2420C656B6440BE95864A4B70CEAD033BA8@klondike.exch.ad.byu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031210/04c31093/attachment.pl

From rpugh at mango-solutions.com  Wed Dec 10 20:52:55 2003
From: rpugh at mango-solutions.com (rpugh@mango-solutions.com)
Date: Wed, 10 Dec 2003 20:52:55 +0100 (CET)
Subject: [R] an eval/parse trivia
In-Reply-To: <Pine.SOL.4.58.0312101350330.22870@robotron.gpcc.itd.umich.edu>
References: <Pine.SOL.3.96.1031210191621.12509T-100000@mach.vub.ac.be>
	<Pine.SOL.4.58.0312101350330.22870@robotron.gpcc.itd.umich.edu>
Message-ID: <47576.205.181.102.120.1071085975.squirrel@london13.amenworld.com>

Is the data structure a factor variable containing character strings? 
That would account for the lack of quotes and the "Levels" bit.  If so,
try
eval(parse(text=as.character(...))).

> x <- factor('structure(1:3, .Names=LETTERS[1:3])')
> eval(parse(text=as.character(x)))
A B C
1 2 3

Weird data structure to play with though - I agree with the "go back to
the maintainers" comment ...

Cheers,
Rich.

> Vincent  -
>
>>From the values shown, this looks like a Bioconductor question,
> rather than base R.  You might try the maintainers of whatever
> package the function comes from.
>
> Is 2287 the index in "levels" for one of the character strings
> shown ?
>
> -  tom blackwell  -  u michigan medical school  -  ann arbor  -
>
> On Wed, 10 Dec 2003, Vincent Detours wrote:
>
>> Dear all,
>>
>> Any cue on how to evaluate x?
>>
>> > x
>> [1] structure(c("GO:0004707", "GO:0005524", "GO:0004674",
>> "GO:0006468", "GO:0000074", "GO:0008372", "GO:0016740"), .Names =
>> c("NAS", "NAS", "IEA", "IDA", "NAS", "ND", "IEA"))
>> 6204 Levels: GO:0000074 GO:0000158 GO:0000163 GO:0000166 ...
>> structure(c("GO:0019538", "GO:0016706"), .Names =
>> c("IEA", "IEA"))
>> > parse(text=x) #a failed attempt
>> expression(2287)
>> > eval(parse(text=x))
>> [1] 2287
>> >
>>
>> Any idea? By the way why does parse return 2287?
>>
>> Thanks for your help,
>>
>> Vincent Detours
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From jeff.hamann at forestinformatics.com  Wed Dec 10 22:23:43 2003
From: jeff.hamann at forestinformatics.com (Jeff D. Hamann)
Date: Wed, 10 Dec 2003 13:23:43 -0800
Subject: [R] extensive grid docs?
Message-ID: <004201c3bf63$e6c5b510$0a00a8c0@rodan>

I'm looking for extensive docs on using grid (for the somewhat newbie). I'm
attempting to create a set of graphics that look similar to the attached
image (I hope this doesn't get bounced) and have only come across the R
newsletters and it appears that grid was new as of 1.8.0? I think the best
way to proceed is to create the plot, clip it using a polygon, then manually
add the annotation. Is that correct?

I couldn't find much on the FAQ regarding creating really goofy plots with
grid and any hints would be greatly appreciated.

Thanks,
Jeff.


---
Jeff D. Hamann
Forest Informatics, Inc.
PO Box 1421
Corvallis, Oregon USA 97339-1421
(office) 541-754-1428
(cell) 541-740-5988
jeff.hamann at forestinformatics.com
www.forestinformatics.com
-------------- next part --------------
A non-text attachment was scrubbed...
Name: sdmd.png
Type: image/png
Size: 38979 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20031210/fdf2d37f/sdmd.png

From peter_mcmahan at yahoo.com  Wed Dec 10 23:00:13 2003
From: peter_mcmahan at yahoo.com (Peter McMahan)
Date: Wed, 10 Dec 2003 14:00:13 -0800 (PST)
Subject: [R] RODBC with RAqua
Message-ID: <20031210220013.55212.qmail@web41603.mail.yahoo.com>

Hi,
I use RAqua on mac os x 10.3.  I am trying to connect
to an odbc dsn, but RODBC can't seem to find it.
I have it installed in my ODBC Administrator in
/Applications/Utilities as a system-wide data source. 
does RODBC need more than just the dsn name (like a
path or something)?  I have everything set up
identically on a windows box, and all seems to work
fine.
thanks,
peter



From JLee at acamllc.com  Wed Dec 10 23:33:52 2003
From: JLee at acamllc.com (Jeff Lee)
Date: Wed, 10 Dec 2003 17:33:52 -0500
Subject: [R] How to start RMySQL
Message-ID: <71E1B4F35918D31184290008C74560C33CB6EE@ANUBIS>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031210/397fc88d/attachment.pl

From kwokamoto at yahoo.co.jp  Wed Dec 10 23:39:06 2003
From: kwokamoto at yahoo.co.jp (K Okamoto)
Date: Thu, 11 Dec 2003 07:39:06 +0900 (JST)
Subject: [R] dyn.load for c code
Message-ID: <20031210223906.1861.qmail@web2402.mail.yahoo.co.jp>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031211/0a623093/attachment.pl

From aylin at ncmir.ucsd.edu  Wed Dec 10 23:47:39 2003
From: aylin at ncmir.ucsd.edu (Aylin Yilmaz)
Date: Wed, 10 Dec 2003 14:47:39 -0800 (PST)
Subject: [R] How can I call R from Java or C?
Message-ID: <Pine.GSO.4.58.0312101429250.20933@ncmir>


Hi all,
I am new to R. I have been looking for a way to call R functions from
Java or C.

What I have found so far is: I can execute R commands in batch mode from a
file and redirect the output to another file. However, I am not sure this
is the way to do it.

About C, is it true that I can only call R from a C process,only if that C
process is invoked within R itself? So is it R->C->R ?

If I could call R from C, then I could possibly call C from Java using
JNI (i am not sure exactly how well that will work)

If you have any suggestions for R's interface that would be really nice.

Thanks,
ilene



From dj at research.bell-labs.com  Thu Dec 11 00:31:55 2003
From: dj at research.bell-labs.com (David James)
Date: Wed, 10 Dec 2003 18:31:55 -0500
Subject: [R] How to start RMySQL
In-Reply-To: <71E1B4F35918D31184290008C74560C33CB6EE@ANUBIS>;
	from JLee@acamllc.com on Wed, Dec 10, 2003 at 05:33:52PM -0500
References: <71E1B4F35918D31184290008C74560C33CB6EE@ANUBIS>
Message-ID: <20031210183155.A14643@jessie.research.bell-labs.com>

Jeff Lee wrote:
> I am a newbie in R and want to use MySQL database.  Here is what I have and
> done.
>  
> R 1.8.0
> mysql 4.0.16
>  
> Both running in Windows XP.  I download the RMySQL.zip from the internet and
> used the Packages installer in the RGui to install RMySQL.  I also installed
> DBI packages.  I ran the following  and got the error.

You don't mention what version of RMySQL you're using or where
you got it from, but in any case you should update to R 1.8.1 and
make sure you run the latest RMySQL_0.5-3.zip.  The error you're
reporting is coming from the methods package and ways to fix it
were discussed last month in this list.  Again, the easiest thing
is simply to update 1.8.1.

Hope this helps,

--
David

>  
> > mgr<-dbDriver("MySQL")
> Error in dbDriver("MySQL") : couldn't find function ".valueClassTest"
> 
>  
> What did I do wrong?
>  
> Thanks in advance.
>  
> Jeff.
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Thu Dec 11 07:16:12 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 11 Dec 2003 06:16:12 +0000 (GMT)
Subject: [R] RODBC with RAqua
In-Reply-To: <20031210220013.55212.qmail@web41603.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0312110615080.27343-100000@gannet.stats>

On Wed, 10 Dec 2003, Peter McMahan wrote:

> Hi,
> I use RAqua on mac os x 10.3.  I am trying to connect
> to an odbc dsn, but RODBC can't seem to find it.
> I have it installed in my ODBC Administrator in
> /Applications/Utilities as a system-wide data source. 
> does RODBC need more than just the dsn name (like a
> path or something)?  I have everything set up
> identically on a windows box, and all seems to work
> fine.

As it is the same code on Windows, RODBC requires no more.
The problem is in your ODBC installation, not an R issue.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Dec 11 08:03:53 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 11 Dec 2003 07:03:53 +0000 (GMT)
Subject: [R] How can I call R from Java or C?
In-Reply-To: <Pine.GSO.4.58.0312101429250.20933@ncmir>
Message-ID: <Pine.LNX.4.44.0312110701290.27676-100000@gannet.stats>

On Wed, 10 Dec 2003, Aylin Yilmaz wrote:

> I am new to R. I have been looking for a way to call R functions from
> Java or C.
> 
> What I have found so far is: I can execute R commands in batch mode from a
> file and redirect the output to another file. However, I am not sure this
> is the way to do it.
> 
> About C, is it true that I can only call R from a C process,only if that C
> process is invoked within R itself? So is it R->C->R ?

Not true.  There are several ways to embed R in a C-based executable, but 
as you have not told us your OS, I won't elaborate.

> If I could call R from C, then I could possibly call C from Java using
> JNI (i am not sure exactly how well that will work)

Have you looked at the SJava package on the Omegahat site?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From cg.pettersson at evp.slu.se  Thu Dec 11 09:11:54 2003
From: cg.pettersson at evp.slu.se (CG Pettersson)
Date: Thu, 11 Dec 2003 09:11:54 +0100
Subject: [R] Black and white output from scatterplot (CAR package)
Message-ID: <200312110811.JAA01655@mail1.slu.se>

Hello!
I want to produce outputs from "scatterplot" (CAR-package), that are
printeble in black and white. The default output is very nice, but in
color. I have found out how to make a greyscale instead.

What I want is a good black and white output where the lines
indicating regression for the groups, are possible to distinguish from
eachother by traditional dotted lines (style) in black ink. Is this
possible or do I have to do it by hand? 

/CG

CG Pettersson, MSci, PhD Stud.
Swedish University of Agricultural Sciences
Dep. of Ecology and Crop Production. Box 7043
SE-750 07 Uppsala



From Pascal.Niklaus at unibas.ch  Thu Dec 11 09:53:25 2003
From: Pascal.Niklaus at unibas.ch (Pascal.Niklaus@unibas.ch)
Date: Thu, 11 Dec 2003 09:53:25 +0100
Subject: [R] nested aov: plot available?
Message-ID: <1071132805.3fd83085eb48e@webmail.unibas.ch>

Hi all,

I wonder whether, for an anova with multiple error strata, it is possible to
produce the same diagnostoc plots than with a single-stratum anova.

I can extract the residuals for each stratum with e.g.
  > resid(split1.aov[["block:plot"]])
  > resid(split1.aov[["Within"]])
and then produce qqnorm plots etc manually, but is it possible to get all the
plots (residuals vs fitted, cook's distance etc) by stratum with a single plot
command as with the single-stratum call to aov ? Or is there a reason why I
wouldn't want to do this?

Thanks for your help

Pascal



-------------------------------------------------
This mail sent through IMP: http://horde.org/imp/



From ripley at stats.ox.ac.uk  Thu Dec 11 10:05:43 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 11 Dec 2003 09:05:43 +0000 (GMT)
Subject: [R] nested aov: plot available?
In-Reply-To: <1071132805.3fd83085eb48e@webmail.unibas.ch>
Message-ID: <Pine.LNX.4.44.0312110902120.16441-100000@gannet.stats>

On Thu, 11 Dec 2003 Pascal.Niklaus at unibas.ch wrote:

> Hi all,
> 
> I wonder whether, for an anova with multiple error strata, it is possible to
> produce the same diagnostoc plots than with a single-stratum anova.
> 
> I can extract the residuals for each stratum with e.g.
>   > resid(split1.aov[["block:plot"]])
>   > resid(split1.aov[["Within"]])
> and then produce qqnorm plots etc manually, but is it possible to get all the
> plots (residuals vs fitted, cook's distance etc) by stratum with a single plot
> command as with the single-stratum call to aov ? Or is there a reason why I
> wouldn't want to do this?

One usually would not want to look at all the strata in the same way.
Plots such as Cook's distance are in any case uninteresting for a fixed
design with no high-leverage points (one hopes), linearity is not an issue 
and so on.

_You_ can of course write your own plot method to do this.  I doubt if 
more that a handful of R users would want such a method, and quite 
possible only one would want it to do as you suggest.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From hb at maths.lth.se  Thu Dec 11 10:52:38 2003
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Thu, 11 Dec 2003 10:52:38 +0100
Subject: [R] How can I call R from Java or C?
In-Reply-To: <Pine.GSO.4.58.0312101429250.20933@ncmir>
Message-ID: <001d01c3bfcc$83043330$e502eb82@maths.lth.se>

Hi. Depending on what you are trying to do, you may also take a look
at Rserve 

 http://stats.math.uni-augsburg.de/Rserve/

>From the webpage: "Rserve is a TCP/IP server which allows other
programs to use facilities of R (see www.r-project.org) from various
languages without the need to initialize R or link against R library.
Every connection has a separate workspace and working directory.
Client-side implementations are available for popular languages such
as C/C++ and Java."

>From the Rserve FAQ: Supported Platforms: "Rserve should work on any
plarform supporting shared libraries which is also supported by R. The
following platforms were tested and are known to work with the latest
release: Linux (PPC, x86), Windows (x86), Mac OS X / Darwin (PPC), AIX
(PPC since version 0.1-10), SunOS (sun4u since version 0.2)"

I haven't used it myself so I do not know about the performance etc,
but it looks like it is easy to get started with. 

Henrik Bengtsson
Lund University

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Aylin Yilmaz
> Sent: den 10 december 2003 23:48
> To: r-help at stat.math.ethz.ch
> Subject: [R] How can I call R from Java or C?
> 
> 
> 
> Hi all,
> I am new to R. I have been looking for a way to call R 
> functions from Java or C.
> 
> What I have found so far is: I can execute R commands in 
> batch mode from a file and redirect the output to another 
> file. However, I am not sure this is the way to do it.
> 
> About C, is it true that I can only call R from a C 
> process,only if that C process is invoked within R itself? So 
> is it R->C->R ?
> 
> If I could call R from C, then I could possibly call C from 
> Java using JNI (i am not sure exactly how well that will work)
> 
> If you have any suggestions for R's interface that would be 
> really nice.
> 
> Thanks,
> ilene
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailma> n/listinfo/r-help



From fredrik.karlsson at ling.umu.se  Thu Dec 11 11:03:52 2003
From: fredrik.karlsson at ling.umu.se (Fredrik Karlsson)
Date: Thu, 11 Dec 2003 11:03:52 +0100
Subject: [R] bartlett.test in a cell?
Message-ID: <20031211100352.GA19020@ling.umu.se>

Dear list,

I want to apply the bartlett test of homogenity of variance between two
groups in a cell, which in turn is  created by a number of factors.

Is there a way to do this without having a zillion loops around the
data? I've played around with tapply and aggregate, but it seems the
problem with these approaches is that the 'groups' variable will have
to be supplied.

Any thoughts on how I might do this?

/Fredrik



From tobias.verbeke at bivv.be  Thu Dec 11 11:07:04 2003
From: tobias.verbeke at bivv.be (tobias.verbeke@bivv.be)
Date: Thu, 11 Dec 2003 11:07:04 +0100
Subject: [R] extensive grid docs?
In-Reply-To: <004201c3bf63$e6c5b510$0a00a8c0@rodan>
Message-ID: <OFE59BCA41.D6F278A9-ONC1256DF9.00370F1C-C1256DF9.0037A64E@BIVV.BE>





r-help-bounces at stat.math.ethz.ch wrote on 10/12/2003 22:23:43:

> I'm looking for extensive docs on using grid (for the somewhat newbie).
I'm
> attempting to create a set of graphics that look similar to the attached
> image (I hope this doesn't get bounced) and have only come across the R
> newsletters and it appears that grid was new as of 1.8.0? I think the
best
> way to proceed is to create the plot, clip it using a polygon, then
manually
> add the annotation. Is that correct?
>
> I couldn't find much on the FAQ regarding creating really goofy plots
with
> grid and any hints would be greatly appreciated.
>

Here's an R graphics tutorial by Paul Murrell

http://www.ci.tuwien.ac.at/Conferences/DSC-2003/tutorials.html



HTH,
Tobias



From sbarbar at gwdg.de  Thu Dec 11 11:08:53 2003
From: sbarbar at gwdg.de (Salvatore Barbaro)
Date: Thu, 11 Dec 2003 11:08:53 +0100
Subject: [R] \vartheta
Message-ID: <3FD85045.27470.2A3DDA@localhost>

Hi, 

is it possbible to include the Latex-variable \vartheta into a 
plot (using, for instance, the expression function).

Thanks in advace

Salvatore



From paradis at isem.univ-montp2.fr  Thu Dec 11 11:24:43 2003
From: paradis at isem.univ-montp2.fr (Emmanuel Paradis)
Date: Thu, 11 Dec 2003 11:24:43 +0100
Subject: [R] packages for ecologists
In-Reply-To: <200312091934.29088.mailinglist2_wegmann@web.de>
Message-ID: <4.2.0.58.20031211112036.00b58e28@isem.isem.univ-montp2.fr>

A 21:26 09/12/2003 +0100, Martin Wegmann a ?crit:
>Hello R-user,
>
>sorry for this very off-topic question.
>
>But I shall present R to my dept. (pro's and con's and what it can do).
>The pro's and con's are easy but not what R can do (additional to the 
>"normal"
>statistics).
>I looked through the packages, but the enormous amount of packages makes it
>very difficult for me to decide which one is worth mentioning.
>
>I used only a small part of all R packages (mainly recommended packages and
>grasper) and would like to know which package for ecologist has to be
>mentioned.
>
>I would greatly appreciate if you can tell me which packages you think are
>very useful for ecolgical research in R e.g. vegan, ade4, ...
>
>thanks in advance, regards Martin

I do not think this is the right way to present R. R is not a simple tool
box where you input some data and have a battery of results after
applying some standard, pre-defined methods. It is better to think the
other way: think about what you want to do with your data, then see if
the necessary methods are already implemented in R, and if not they
can be. Writting functions in R is quite easy compared to other
languages. Seeing things this way implies a major change in our views
of data analysis in fields like ecology and evolutionary biology. The
R project has many tools to help here: there is ample documentation to
help in writting functions and packages; there are several search
engines to browse through the list of packages and the documentation
(with links from the CRAN site). I find this view of data analysis
with R much more challenging than the traditional "passive"
approach. And do not forget that R is a collaborative project. As a
package maintainer I always welcome even simple suggestions, and I
must mention that the R Core Team readily accepts suggestions too.
I think these aspects have to be pointed out rather than a mere list of
packages and functions.

Emmanuel Paradis



From Arnaud_Amsellem at ssga.com  Thu Dec 11 11:31:15 2003
From: Arnaud_Amsellem at ssga.com (Arnaud_Amsellem@ssga.com)
Date: Thu, 11 Dec 2003 10:31:15 +0000
Subject: [R] Probelm with read.table
Message-ID: <OF53786AB1.2685CE8C-ON80256DF9.00361A31-80256DF9.0039FD49@statestr.com>

Hi All,

I have the following text file (mytextfile.txt)

738307      527178      714456      557955
#N/A  17.42 6.22  4.73
#N/A  17.3  6.23  4.75
#N/A  17.29 6.17  4.7
#N/A  17.07 6.12  4.6
#N/A  17.27 6.19  4.7
#N/A  17.72 6.4   4.78
#N/A  17.12 6.19  4.75
#N/A  17.07 6.15  4.65
#N/A  17.03 6.07  4.64
#N/A  17.38 6.13  4.7
#N/A  17.38 6.13  4.7
#N/A  17.38 6.13  4.7
#N/A  17.38 6.13  4.7
#N/A  17.34 6.28  4.7
10    17.57 6.33  4.75
11    17.57 6.33  4.75
12    17.57 6.33  4.75
13    17.39 6.25  4.87
14    17.15 6.33  5.06
15    17.05 6.21  5
16    16.87 6.14  5.15
17    16.72 6.27  5.23

I use the following command:
mydf <- read.table(file="mytextfile.txt", header = T, sep="\t",na.strings="
#NA")

When the above command is applied I have only 8 lines in mydf. I tried many
options but nothing seems to get me the entire file. If the #NA are not in
the first column it seems to work fine i.e I get  22 lines in mydf.
Anyone would know a way of getting the entire file even if #NA are in the
first column?

I use R 1.8.0 on Windows 2000

Any help appreciated

Arno



From plummer at iarc.fr  Thu Dec 11 12:54:44 2003
From: plummer at iarc.fr (Martyn Plummer)
Date: 11 Dec 2003 12:54:44 +0100
Subject: [R] Probelm with read.table
In-Reply-To: <OF53786AB1.2685CE8C-ON80256DF9.00361A31-80256DF9.0039FD49@statestr.com>
References: <OF53786AB1.2685CE8C-ON80256DF9.00361A31-80256DF9.0039FD49@statestr.com>
Message-ID: <1071143685.25632.1.camel@xena>

On Thu, 2003-12-11 at 11:31, Arnaud_Amsellem at ssga.com wrote:
> Hi All,
> 
> I have the following text file (mytextfile.txt)
> 
> 738307      527178      714456      557955
> #N/A  17.42 6.22  4.73
> #N/A  17.3  6.23  4.75
> #N/A  17.29 6.17  4.7
> #N/A  17.07 6.12  4.6
> #N/A  17.27 6.19  4.7
> #N/A  17.72 6.4   4.78
> #N/A  17.12 6.19  4.75
> #N/A  17.07 6.15  4.65
> #N/A  17.03 6.07  4.64
> #N/A  17.38 6.13  4.7
> #N/A  17.38 6.13  4.7
> #N/A  17.38 6.13  4.7
> #N/A  17.38 6.13  4.7
> #N/A  17.34 6.28  4.7
> 10    17.57 6.33  4.75
> 11    17.57 6.33  4.75
> 12    17.57 6.33  4.75
> 13    17.39 6.25  4.87
> 14    17.15 6.33  5.06
> 15    17.05 6.21  5
> 16    16.87 6.14  5.15
> 17    16.72 6.27  5.23
> 
> I use the following command:
> mydf <- read.table(file="mytextfile.txt", header = T, sep="\t",na.strings="
> #NA")
> 
> When the above command is applied I have only 8 lines in mydf. I tried many
> options but nothing seems to get me the entire file. If the #NA are not in
> the first column it seems to work fine i.e I get  22 lines in mydf.
> Anyone would know a way of getting the entire file even if #NA are in the
> first column?

The character "#" is a comment character, so lines beginning in "#"
are ignored.  To read these lines, set the parameter comment.char to
something else, e.g.

read.table("mytextfile.txt", header=TRUE, sep="\t", na.strings="#N/A",
comment.char="%")

Martyn



From vdetours at ulb.ac.be  Thu Dec 11 12:59:59 2003
From: vdetours at ulb.ac.be (Vincent Detours)
Date: Thu, 11 Dec 2003 12:59:59 +0100 (MET)
Subject: [R] an eval/parse trivia -- solved
In-Reply-To: <47576.205.181.102.120.1071085975.squirrel@london13.amenworld.com>
Message-ID: <Pine.SOL.3.96.1031211125208.12509W-100000@mach.vub.ac.be>

> Is the data structure a factor variable containing character strings? 
> That would account for the lack of quotes and the "Levels" bit.  If so,
> try
> eval(parse(text=as.character(...))).

It works:

--------
> eval(parse(text=as.character(x)))
         NAS          NAS          IEA          IDA          NAS
ND
"GO:0004707" "GO:0005524" "GO:0004674" "GO:0006468" "GO:0000074"
"GO:0008372"
         IEA
"GO:0016740"
>
--------

Thank you very much!

Vincent 

 
> > x <- factor('structure(1:3, .Names=LETTERS[1:3])')
> > eval(parse(text=as.character(x)))
> A B C
> 1 2 3
> 
> Weird data structure to play with though - I agree with the "go back to
> the maintainers" comment ...
> 
> Cheers,
> Rich.
> 
> > Vincent  -
> >
> >>From the values shown, this looks like a Bioconductor question,
> > rather than base R.  You might try the maintainers of whatever
> > package the function comes from.
> >
> > Is 2287 the index in "levels" for one of the character strings
> > shown ?
> >
> > -  tom blackwell  -  u michigan medical school  -  ann arbor  -
> >
> > On Wed, 10 Dec 2003, Vincent Detours wrote:
> >
> >> Dear all,
> >>
> >> Any cue on how to evaluate x?
> >>
> >> > x
> >> [1] structure(c("GO:0004707", "GO:0005524", "GO:0004674",
> >> "GO:0006468", "GO:0000074", "GO:0008372", "GO:0016740"), .Names =
> >> c("NAS", "NAS", "IEA", "IDA", "NAS", "ND", "IEA"))
> >> 6204 Levels: GO:0000074 GO:0000158 GO:0000163 GO:0000166 ...
> >> structure(c("GO:0019538", "GO:0016706"), .Names =
> >> c("IEA", "IEA"))
> >> > parse(text=x) #a failed attempt
> >> expression(2287)
> >> > eval(parse(text=x))
> >> [1] 2287
> >> >
> >>
> >> Any idea? By the way why does parse return 2287?
> >>
> >> Thanks for your help,
> >>
> >> Vincent Detours
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >>
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 
>



From p.dalgaard at biostat.ku.dk  Thu Dec 11 13:17:20 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 11 Dec 2003 13:17:20 +0100
Subject: [R] Probelm with read.table
In-Reply-To: <OF53786AB1.2685CE8C-ON80256DF9.00361A31-80256DF9.0039FD49@statestr.com>
References: <OF53786AB1.2685CE8C-ON80256DF9.00361A31-80256DF9.0039FD49@statestr.com>
Message-ID: <x24qw7sg4v.fsf@biostat.ku.dk>

Arnaud_Amsellem at ssga.com writes:

> Hi All,
> 
> I have the following text file (mytextfile.txt)
> 
> 738307      527178      714456      557955
> #N/A  17.42 6.22  4.73
> #N/A  17.3  6.23  4.75
> #N/A  17.29 6.17  4.7
> #N/A  17.07 6.12  4.6
> #N/A  17.27 6.19  4.7
> #N/A  17.72 6.4   4.78
> #N/A  17.12 6.19  4.75
> #N/A  17.07 6.15  4.65
> #N/A  17.03 6.07  4.64
> #N/A  17.38 6.13  4.7
> #N/A  17.38 6.13  4.7
> #N/A  17.38 6.13  4.7
> #N/A  17.38 6.13  4.7
> #N/A  17.34 6.28  4.7
> 10    17.57 6.33  4.75
> 11    17.57 6.33  4.75
> 12    17.57 6.33  4.75
> 13    17.39 6.25  4.87
> 14    17.15 6.33  5.06
> 15    17.05 6.21  5
> 16    16.87 6.14  5.15
> 17    16.72 6.27  5.23
> 
> I use the following command:
> mydf <- read.table(file="mytextfile.txt", header = T, sep="\t",na.strings="
> #NA")
> 
> When the above command is applied I have only 8 lines in mydf. I tried many
> options but nothing seems to get me the entire file. If the #NA are not in
> the first column it seems to work fine i.e I get  22 lines in mydf.
> Anyone would know a way of getting the entire file even if #NA are in the
> first column?
> 
> I use R 1.8.0 on Windows 2000
> 
> Any help appreciated
> 
> Arno

Check the docs, in particular the bit about comment.char.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From braddonski at yahoo.com.au  Thu Dec 11 13:18:25 2003
From: braddonski at yahoo.com.au (=?iso-8859-1?q?Braddon=20Lance?=)
Date: Thu, 11 Dec 2003 23:18:25 +1100 (EST)
Subject: [R] package for mixed models with multivariate response vector?
Message-ID: <20031211121825.76433.qmail@web21326.mail.yahoo.com>

As far as I can tell, the excellent nlme package
caters for univariate response data only?  

Is there a package that will fit mixed models with a
multivariate response vector?  

many thanks,
braddon lance

http://personals.yahoo.com.au - Yahoo! Personals
New people, new possibilities. FREE for a limited time.



From trond.rafoss at planteforsk.no  Thu Dec 11 13:48:41 2003
From: trond.rafoss at planteforsk.no (Trond Rafoss)
Date: Thu, 11 Dec 2003 13:48:41 +0100
Subject: SV: [R] How to start RMySQL
Message-ID: <625C3FEB6AEAE14486CAFA82A001C8AD53F7A6@post.planteforsk.no>

Jeff,
the tip on http://stat.bell-labs.com/RS-DBI/download/index.html (at the bottom of the page) may help.

Trond Rafoss

-----Opprinnelig melding-----
Fra: Jeff Lee [mailto:JLee at acamllc.com]
Sendt: 10. desember 2003 23:34
Til: 'r-help at stat.math.ethz.ch'
Emne: [R] How to start RMySQL


I am a newbie in R and want to use MySQL database.  Here is what I have and
done.
 
R 1.8.0
mysql 4.0.16
 
Both running in Windows XP.  I download the RMySQL.zip from the internet and
used the Packages installer in the RGui to install RMySQL.  I also installed
DBI packages.  I ran the following  and got the error.
 
> mgr<-dbDriver("MySQL")
Error in dbDriver("MySQL") : couldn't find function ".valueClassTest"

 
What did I do wrong?
 
Thanks in advance.
 
Jeff.
 

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From jiafucang at hotmail.com  Thu Dec 11 14:37:16 2003
From: jiafucang at hotmail.com (Fucang Jia)
Date: Thu, 11 Dec 2003 21:37:16 +0800
Subject: [R] Paper on PAM and Clara
Message-ID: <Law11-F74ASTBnFCB1B0002c44d@hotmail.com>

Hi, everyone,

I found that Clara and Pam is very useful in large data clustering. So I 
want to learn more about it. But as far as I know, the idea is comes the 
authors' book "Finding groups in data: an introduction to cluster analysis". 
Unfortunately, I have no access to this book. Could anyone tell me that if 
there are other papers which descirbes these methods?

Thank you very much!

Best,

Fucang



From Arne.Muller at aventis.com  Thu Dec 11 14:42:07 2003
From: Arne.Muller at aventis.com (Arne.Muller@aventis.com)
Date: Thu, 11 Dec 2003 14:42:07 +0100
Subject: [R] Cochran-Mantel-Haenszel problem
Message-ID: <C80ECAFA2ACC1B45BE45D133ED660ADE010BF118@crbsmxsusr04.pharma.aventis.com>

Hello,

I've tried to analyze some data with a CMH test. My 3 dimensional contingency
tables are 2x2xN where N is usually between 10 and 100.

The problem is that there may be 2 strata with opposite counts (the 2x2
contigency table for these are reversed), producing opposite odds ratios that
cancle out in the overall statistics. These opposite counts are very
important for my analysis, since they account for a dramatic difference.

Could you recommend alternative tests that take account for opposite counts?
Would you suggest a different strategy to analyze such data?

	thanks a lot for your suggestions,

	Arne



From Brian.Beckage at uvm.edu  Thu Dec 11 15:10:28 2003
From: Brian.Beckage at uvm.edu (Brian Beckage)
Date: Thu, 11 Dec 2003 09:10:28 -0500
Subject: [R] Failed R installation under Mac OSX 10.3
Message-ID: <p06020403bbfd58f2488d@[132.198.177.56]>

Dear R list,

I've installed R 1.8.1 on OSX 10.3 (Panther) using the RAqua.pkg and 
all indications were that the installation was successful.  However, 
after double clicking the R icon in the Applications folder, nothing 
appears to happen.  The following message appears on the console:

Mac OS X Version 10.3 (Build 7B85)
2003-12-11 08:45:31 -0500
prefs written
dyld: /Applications/StartR.app/RAqua.app/Contents/MacOS/RAqua can't 
open library: /usr/local/lib/libreadline.4.3.dylib  (No such file or 
directory, errno = 2)


When I check /usr/local/ there is no 'lib' folder, only the 'bin' 
folder.  I checked the FAQ's and the bug reports and this problem was 
reported by at least one other person but no solution has been put 
forward that I could find.

Do the other packages that come with the download (e.g., ) need to be 
installed?  Any suggestions are appreciated.

Thanks for your help,
Brian




--



From fm3a004 at math.uni-hamburg.de  Thu Dec 11 15:13:52 2003
From: fm3a004 at math.uni-hamburg.de (Christian Hennig)
Date: Thu, 11 Dec 2003 15:13:52 +0100 (MET)
Subject: [R] cutree with agnes
Message-ID: <Pine.GSO.3.95q.1031211145123.398M-100000@sun35.math.uni-hamburg.de>

Hi,

this is rather a (presumed) bug report than a question because I can solve
my personal statistical problem by working with hclust instead of agnes. 

I have done a complete linkage clustering on a dist object dm with 30
objects with agnes (R 1.8.0 on
RedHat) and I want to obtain the partition that results from a cut at
height=0.4.

I run

> cl1a <- agnes(dm, method="complete")
> cutree(cl1a,h=0.4)
 [1]  1  2  3  4  5  6  3  7  3  8  9 10  3 11 12 13 14 15  3 16 17  3 18
19 20
[26] 21  3 22 18 23

But that's not true; correct is the solution obtained from hclust
> clx <- hclust(dm)
> cutree(cl1,h=0.4)
 [1]  1  2  1  2  3  4  1  2  1  3  4  5  1  4  6  7  8  4  1  5  2  1  9
2  2
[26] 10  1  9  9 11

as can be seen from the dendrogram plots of hclust *and* agnes.
(Note that the dendrograms of hclust and agnes are not identical due to
the handling of ties in the distances, but the difference between the 
agnes and hclust dendrogram at h=0.4 concerns only two points.)
Specifying k instead of h in cutree for agnes seems to work properly, but
that's not what I need in the general case.

I tried to reproduce this with a toy example, but it worked (too) well:
> d
     [,1] [,2] [,3]
[1,]    0    1    2
[2,]    1    0    3
[3,]    2    3    0
> ad <- agnes(as.dist(d),method="complete")
> cutree(ad,h=1.5)
[1] 1 1 2
> ah <- hclust(as.dist(d))
> cutree(ah,h=1.5)
[1] 1 1 2

I can send anyone who would like to reproduce the problem (Martin?) the
original distance matrix dm (dm is a dist object) as ASCII or R-object.

Best,
Christian


***********************************************************************
Christian Hennig
Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
#######################################################################
ich empfehle www.boag-online.de



From kmw at mail.rockefeller.edu  Thu Dec 11 15:12:50 2003
From: kmw at mail.rockefeller.edu (Knut M. Wittkowski)
Date: Thu, 11 Dec 2003 09:12:50 -0500
Subject: [R] Cochran-Mantel-Haenszel problem
In-Reply-To: <C80ECAFA2ACC1B45BE45D133ED660ADE010BF118@crbsmxsusr04.phar
	ma.aventis.com>
Message-ID: <5.1.0.14.0.20031211090723.020fbfb0@imap.rockefeller.edu>

Hi Arne,

This seems to be more a statistics than an R problem.

Let's assume, one stratum is male and the other is female, and that you are 
giving estrogen. With women, it may be better to have more estrogen, with 
men to have less. Thus, if you recode

         women:  (more estrogen/less estrogen) -> (more appropriate/less 
appropriate)
         men:    (more estrogen/less estrogen) -> (less appropriate/more 
appropriate)

and if this reflects your hypothesis, it will solve your problem.

Knut

At 14:42 2003-12-11 +0100, you wrote:
>Hello,
>
>I've tried to analyze some data with a CMH test. My 3 dimensional contingency
>tables are 2x2xN where N is usually between 10 and 100.
>
>The problem is that there may be 2 strata with opposite counts (the 2x2
>contigency table for these are reversed), producing opposite odds ratios that
>cancle out in the overall statistics. These opposite counts are very
>important for my analysis, since they account for a dramatic difference.
>
>Could you recommend alternative tests that take account for opposite counts?
>Would you suggest a different strategy to analyze such data?
>
>         thanks a lot for your suggestions,
>
>         Arne
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help

Knut M. Wittkowski, PhD,DSc
------------------------------------------
The Rockefeller University, GCRC
Experimental Design and Biostatistics
1230 York Ave #121B, Box 322, NY,NY 10021
+1(212)327-7175, +1(212)327-8450 (Fax)
kmw at rockefeller.edu
http://www.rucares.org/clinicalresearch/dept/biometry/



From feh3k at spamcop.net  Thu Dec 11 15:23:39 2003
From: feh3k at spamcop.net (Frank E Harrell Jr)
Date: Thu, 11 Dec 2003 09:23:39 -0500
Subject: [R] Cochran-Mantel-Haenszel problem
In-Reply-To: <C80ECAFA2ACC1B45BE45D133ED660ADE010BF118@crbsmxsusr04.pharma.aventis.com>
References: <C80ECAFA2ACC1B45BE45D133ED660ADE010BF118@crbsmxsusr04.pharma.aventis.com>
Message-ID: <20031211092339.7c3a59fc.feh3k@spamcop.net>

On Thu, 11 Dec 2003 14:42:07 +0100
<Arne.Muller at aventis.com> wrote:

> Hello,
> 
> I've tried to analyze some data with a CMH test. My 3 dimensional
> contingency tables are 2x2xN where N is usually between 10 and 100.
> 
> The problem is that there may be 2 strata with opposite counts (the 2x2
> contigency table for these are reversed), producing opposite odds ratios
> that cancle out in the overall statistics. These opposite counts are
> very important for my analysis, since they account for a dramatic
> difference.
> 
> Could you recommend alternative tests that take account for opposite
> counts? Would you suggest a different strategy to analyze such data?
> 
> 	thanks a lot for your suggestions,
> 
> 	Arne
> 

I'm not sure about your specific problem, but in general you might think
of this as a binary or multinomial logistic model with strata x treatment
interactions, and get pooled treatment main effect + interaction effects,
which are equivalent to testing whether treatment is associated with
response for ANY stratum (without cancellation when signs of effects are
reversed).  With the Design package, for example, you can do this easily
when the response is binary or ordinal:

library(Design)
f <- lrm(y ~ treat*strat)
anova(f)   # prints multiple d.f. test for treat as either main effect or
effect modifier

Of course when you allow for a more general model such as this, the power
is diluted into multiple degrees of freedom when the effects really do not
vary very much over strata.

Frank

---
Frank E Harrell Jr    Professor and Chair            School of Medicine
                      Department of Biostatistics    Vanderbilt University



From H.RINNER at tirol.gv.at  Thu Dec 11 15:42:14 2003
From: H.RINNER at tirol.gv.at (RINNER Heinrich)
Date: Thu, 11 Dec 2003 15:42:14 +0100
Subject: AW: [R] Paper on PAM and Clara
Message-ID: <6A6B3B547E312840A98A9DD31516B32118046A@mxs1.tirol.local>

Maybe the following article is of interest for you (co-authored by P.J. Rousseeuw, like the book you mentioned):

http://www.jstatsoft.org/v01/i04/paper/clus.pdf

-Heinrich.

> -----Urspr?ngliche Nachricht-----
> Von: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] Im Auftrag von Fucang Jia
> Gesendet: Donnerstag, 11. Dezember 2003 14:37
> An: r-help at stat.math.ethz.ch
> Betreff: [R] Paper on PAM and Clara
> 
> 
> Hi, everyone,
> 
> I found that Clara and Pam is very useful in large data 
> clustering. So I 
> want to learn more about it. But as far as I know, the idea 
> is comes the 
> authors' book "Finding groups in data: an introduction to 
> cluster analysis". 
> Unfortunately, I have no access to this book. Could anyone 
> tell me that if 
> there are other papers which descirbes these methods?
> 
> Thank you very much!
> 
> Best,
> 
> Fucang
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From p.dalgaard at biostat.ku.dk  Thu Dec 11 15:50:20 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 11 Dec 2003 15:50:20 +0100
Subject: [R] Cochran-Mantel-Haenszel problem
In-Reply-To: <20031211092339.7c3a59fc.feh3k@spamcop.net>
References: <C80ECAFA2ACC1B45BE45D133ED660ADE010BF118@crbsmxsusr04.pharma.aventis.com>
	<20031211092339.7c3a59fc.feh3k@spamcop.net>
Message-ID: <x2r7zbquhf.fsf@biostat.ku.dk>

Frank E Harrell Jr <feh3k at spamcop.net> writes:

> On Thu, 11 Dec 2003 14:42:07 +0100
> <Arne.Muller at aventis.com> wrote:
> 
> > Hello,
> > 
> > I've tried to analyze some data with a CMH test. My 3 dimensional
> > contingency tables are 2x2xN where N is usually between 10 and 100.
> > 
> > The problem is that there may be 2 strata with opposite counts (the 2x2
> > contigency table for these are reversed), producing opposite odds ratios
> > that cancle out in the overall statistics. These opposite counts are
> > very important for my analysis, since they account for a dramatic
> > difference.
> > 
> > Could you recommend alternative tests that take account for opposite
> > counts? Would you suggest a different strategy to analyze such data?
> > 
> > 	thanks a lot for your suggestions,
> > 
> > 	Arne
> > 
> 
> I'm not sure about your specific problem, but in general you might think
> of this as a binary or multinomial logistic model with strata x treatment
> interactions, and get pooled treatment main effect + interaction effects,
> which are equivalent to testing whether treatment is associated with
> response for ANY stratum (without cancellation when signs of effects are
> reversed).  With the Design package, for example, you can do this easily
> when the response is binary or ordinal:
> 
> library(Design)
> f <- lrm(y ~ treat*strat)
> anova(f)   # prints multiple d.f. test for treat as either main effect or
> effect modifier
> 
> Of course when you allow for a more general model such as this, the power
> is diluted into multiple degrees of freedom when the effects really do not
> vary very much over strata.

Also note the woolf() function given in the examples section of the
help for mantelhaen.test.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From jeffrey.chang at duke.edu  Thu Dec 11 15:58:43 2003
From: jeffrey.chang at duke.edu (Jeffrey Chang)
Date: Thu, 11 Dec 2003 09:58:43 -0500
Subject: [R] chisq.test freezing on certain inputs
Message-ID: <83376EDF-2BEA-11D8-B566-000A956845CE@duke.edu>

Hello everybody,

I'm running R 1.8.1 on both Linux and OS X compiled with gcc 3.2.2 and 
3.3, respectively.  The following call seems to freeze the interpreter 
on both systems:
 > chisq.test(matrix(c(233, 580104, 3776, 5786104), 2, 2), 
simulate.p.value=TRUE)

By freeze, I mean, the function call never returns (running > 10 hours 
so far), the process is unresponsive to SIGINT (but I call kill it with 
TERM), and the process still consumes cycles on the CPU.

Browsing through the code, it seems to be getting stuck on the C call 
to "chisqsim" .

Browse[1]>
debug: tmp <- .C("chisqsim", as.integer(nr), as.integer(nc), 
as.integer(sr),
     as.integer(sc), as.integer(n), as.integer(B), as.double(E),
     integer(nr * nc), double(n + 1), integer(nc), results = double(B),
     PACKAGE = "ctest")
Browse[1]>

Has anyone seen this, or know what may be causing it?

Thanks,
Jeff



From peter_mcmahan at yahoo.com  Thu Dec 11 16:26:55 2003
From: peter_mcmahan at yahoo.com (Peter McMahan)
Date: Thu, 11 Dec 2003 07:26:55 -0800 (PST)
Subject: [R] RODBC with RAqua
In-Reply-To: <Pine.LNX.4.44.0312110615080.27343-100000@gannet.stats>
Message-ID: <20031211152655.97927.qmail@web41612.mail.yahoo.com>

Sorry, I should have been more clear:
It is a problem with RAqua, because I can connect
using iODBC's `test' function and the command-line
odbctest.
When I enter the exact same connection string into R,
though (odbcConnect("DRIVER=/Library/ODBC/OpenLink SQL
Server Lite ODBC
Driver.bundle/Contents/MacOS/sql_st_lt.so;DSN=qa;UID=xxx;PWD=xxx")),
I get the following error:
1: [RODBC] ERROR: state IM004, code -1035222635,
message [iODBC][Driver Manager]Driver's SQLAllocEnv()
failed
Thanks for your help,
Peter



From ripley at stats.ox.ac.uk  Thu Dec 11 16:41:15 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 11 Dec 2003 15:41:15 +0000 (GMT)
Subject: [R] RODBC with RAqua
In-Reply-To: <20031211152655.97927.qmail@web41612.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0312111539090.12252-100000@gannet.stats>

On Thu, 11 Dec 2003, Peter McMahan wrote:

> Sorry, I should have been more clear:
> It is a problem with RAqua, because I can connect

DO read the error message, which is from iODBC and not from RAqua and not 
from RODBC.

> using iODBC's `test' function and the command-line
> odbctest.
> When I enter the exact same connection string into R,
> though (odbcConnect("DRIVER=/Library/ODBC/OpenLink SQL
> Server Lite ODBC
> Driver.bundle/Contents/MacOS/sql_st_lt.so;DSN=qa;UID=xxx;PWD=xxx")),
> I get the following error:
> 1: [RODBC] ERROR: state IM004, code -1035222635,
> message [iODBC][Driver Manager]Driver's SQLAllocEnv()
> failed
> Thanks for your help,
> Peter
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From flom at ndri.org  Thu Dec 11 16:44:38 2003
From: flom at ndri.org (Peter Flom)
Date: Thu, 11 Dec 2003 10:44:38 -0500
Subject: [R] read.spss question warning compression bias
Message-ID: <sfd84ab8.017@MAIL.NDRI.ORG>

Hello again

I have a file from SPSS in .sav format.

when I run

library(foreign)
cvar<-as.data.frame(read.spss("c:\\NDRI\\cvar\\data\\cvar2rev3.sav"))

I get a warning

Warning message: 
c:\NDRI\cvar\data\cvar2rev3.sav: Compression bias (0) is not the usual
value of 100. 

The data appear to be OK, but I am concerned.  

 (I tried searching the archives and the documenation for data import
export, but saw nothing).


Thanks as always

Peter

Peter L. Flom, PhD
Assistant Director, Statistics and Data Analysis Core
Center for Drug Use and HIV Research
National Development and Research Institutes
71 W. 23rd St
www.peterflom.com
New York, NY 10010
(212) 845-4485 (voice)
(917) 438-0894 (fax)



From abunn at montana.edu  Thu Dec 11 17:55:44 2003
From: abunn at montana.edu (Andy Bunn)
Date: Thu, 11 Dec 2003 09:55:44 -0700
Subject: [R] Probelm with read.table
In-Reply-To: <OF53786AB1.2685CE8C-ON80256DF9.00361A31-80256DF9.0039FD49@statestr.com>
Message-ID: <002401c3c007$afdd5d10$78f05a99@msu.montana.edu>

You need to change the comment character from #. This works for me:
$ mydf <- read.table(file="mytextfile.txt", header = T,
sep="\t",na.strings="#N/A", comment.char = "V")
$ mydf
   X738307 X527178 X714456 X557955
1       NA   17.42    6.22    4.73
2       NA   17.30    6.23    4.75
3       NA   17.29    6.17    4.70
4       NA   17.07    6.12    4.60
5       NA   17.27    6.19    4.70
6       NA   17.72    6.40    4.78
7       NA   17.12    6.19    4.75
8       NA   17.07    6.15    4.65
9       NA   17.03    6.07    4.64
10      NA   17.38    6.13    4.70
11      NA   17.38    6.13    4.70
12      NA   17.38    6.13    4.70
13      NA   17.38    6.13    4.70
14      NA   17.34    6.28    4.70
15      10   17.57    6.33    4.75
16      11   17.57    6.33    4.75
17      12   17.57    6.33    4.75
18      13   17.39    6.25    4.87
19      14   17.15    6.33    5.06
20      15   17.05    6.21    5.00
21      16   16.87    6.14    5.15
22      17   16.72    6.27    5.23

HTH, Andy

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
Arnaud_Amsellem at ssga.com
Sent: Thursday, December 11, 2003 3:31 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Probelm with read.table


Hi All,

I have the following text file (mytextfile.txt)

738307      527178      714456      557955
#N/A  17.42 6.22  4.73
#N/A  17.3  6.23  4.75
#N/A  17.29 6.17  4.7
#N/A  17.07 6.12  4.6
#N/A  17.27 6.19  4.7
#N/A  17.72 6.4   4.78
#N/A  17.12 6.19  4.75
#N/A  17.07 6.15  4.65
#N/A  17.03 6.07  4.64
#N/A  17.38 6.13  4.7
#N/A  17.38 6.13  4.7
#N/A  17.38 6.13  4.7
#N/A  17.38 6.13  4.7
#N/A  17.34 6.28  4.7
10    17.57 6.33  4.75
11    17.57 6.33  4.75
12    17.57 6.33  4.75
13    17.39 6.25  4.87
14    17.15 6.33  5.06
15    17.05 6.21  5
16    16.87 6.14  5.15
17    16.72 6.27  5.23

I use the following command:
mydf <- read.table(file="mytextfile.txt", header = T,
sep="\t",na.strings="
#NA")

When the above command is applied I have only 8 lines in mydf. I tried
many options but nothing seems to get me the entire file. If the #NA are
not in the first column it seems to work fine i.e I get  22 lines in
mydf. Anyone would know a way of getting the entire file even if #NA are
in the first column?

I use R 1.8.0 on Windows 2000

Any help appreciated

Arno

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From Torsten.Hothorn at rzmail.uni-erlangen.de  Thu Dec 11 18:01:40 2003
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Thu, 11 Dec 2003 18:01:40 +0100 (CET)
Subject: [R] chisq.test freezing on certain inputs
In-Reply-To: <83376EDF-2BEA-11D8-B566-000A956845CE@duke.edu>
References: <83376EDF-2BEA-11D8-B566-000A956845CE@duke.edu>
Message-ID: <Pine.LNX.4.51.0312111753040.28866@artemis.imbe.med.uni-erlangen.de>

On Thu, 11 Dec 2003, Jeffrey Chang wrote:

> Hello everybody,
>
> I'm running R 1.8.1 on both Linux and OS X compiled with gcc 3.2.2 and
> 3.3, respectively.  The following call seems to freeze the interpreter
> on both systems:
>  > chisq.test(matrix(c(233, 580104, 3776, 5786104), 2, 2),
> simulate.p.value=TRUE)
>
> By freeze, I mean, the function call never returns (running > 10 hours
> so far), the process is unresponsive to SIGINT (but I call kill it with
> TERM), and the process still consumes cycles on the CPU.
>

This is due to calling `exp' with a very small value leading to
a zero return value in rcont2 (src/appl/rcont.c) line 70:

            x = exp(fact[iap - 1] + fact[ib] + fact[ic] +
                    fact[idp - 1] - fact[ie] - fact[nlmp - 1] -
                    fact[igp - 1] - fact[ihp - 1] - fact[iip - 1]);
            if (x >= dummy) {
                goto L160;
            }
            sumprb = x;
            y = x;


y is never checked for zero and later on

L150:
            if (lsm) {
                goto L155;
            }

            /* Decrement entry in row L, column M */
            j = nll * (ii + nll);
            if (j == 0) {
                goto L154;
            }
            --nll;
            y = y * j / (double) ((id - nll) * (ia - nll));
            sumprb += y;
            if (sumprb >= dummy) {
                goto L159;
            }
            if (! lsp) {
                goto L140;
            }
            goto L150;


y has no chance of becoming larger than zero and we are in the goto trap
until the end of time. A simple fix would be checking for zero but I don't
know how one would proceed in this case ...

Best,

Torsten



From maechler at stat.math.ethz.ch  Thu Dec 11 18:05:27 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 11 Dec 2003 18:05:27 +0100
Subject: [R] chisq.test freezing on certain inputs
In-Reply-To: <83376EDF-2BEA-11D8-B566-000A956845CE@duke.edu>
References: <83376EDF-2BEA-11D8-B566-000A956845CE@duke.edu>
Message-ID: <16344.41943.585108.60627@gargle.gargle.HOWL>

>>>>> "Jeffrey" == Jeffrey Chang <jeffrey.chang at duke.edu>
>>>>>     on Thu, 11 Dec 2003 09:58:43 -0500 writes:

    Jeffrey> Hello everybody, I'm running R 1.8.1 on both Linux
    Jeffrey> and OS X compiled with gcc 3.2.2 and 3.3,
    Jeffrey> respectively.  The following call seems to freeze
    Jeffrey> the interpreter on both systems:
    >> chisq.test(matrix(c(233, 580104, 3776, 5786104), 2, 2),
    >> simulate=TRUE)

    Jeffrey> By freeze, I mean, the function call never returns
    Jeffrey> (running > 10 hours so far), the process is
    Jeffrey> unresponsive to SIGINT (but I call kill it with
    Jeffrey> TERM), and the process still consumes cycles on the
    Jeffrey> CPU.

    Jeffrey> Browsing through the code, it seems to be getting
    Jeffrey> stuck on the C call to "chisqsim" .

that's true

    Jeffrey> Has anyone seen this, or know what may be causing
    Jeffrey> it?

I can reproduce it and am pretty sure, the reason is 
integer overflow or something of that nature.

I've recently been looking at a similar bug with fisher.test()
(returning p.value = Inf !) and reason *was* integer overflow
{fix not yet committed}.

--> I'll have a look at this, as well.

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From Brian.Beckage at uvm.edu  Thu Dec 11 18:23:37 2003
From: Brian.Beckage at uvm.edu (Brian Beckage)
Date: Thu, 11 Dec 2003 12:23:37 -0500
Subject: [R] Re: Failed R installation under Mac OSX 10.3 (PR#5697)
In-Reply-To: <96D07C5A-2BF5-11D8-8C53-0003938AF008@math.uni-augsburg.de>
References: <20031211153954.B6AD6EFB8@slim.kubism.ku.dk>
	<96D07C5A-2BF5-11D8-8C53-0003938AF008@math.uni-augsburg.de>
Message-ID: <p06020409bbfe582c0e9d@[132.198.177.56]>

Thanks, Simon!  This solved the problem.

Brian


>On Dec 11, 2003, at 10:39 AM, Brian.Beckage at uvm.edu wrote:
>
>>I've installed R 1.8.1 on OSX 10.3 (Panther) using the RAqua.pkg and
>>all indications were that the installation was successful.  However,
>>after double clicking the R icon in the Applications folder, nothing
>>appears to happen.  The following message appears on the console:
>>
>>Mac OS X Version 10.3 (Build 7B85)
>>2003-12-11 08:45:31 -0500
>>prefs written
>>dyld: /Applications/StartR.app/RAqua.app/Contents/MacOS/RAqua can't
>>open library: /usr/local/lib/libreadline.4.3.dylib  (No such file or
>>directory, errno = 2)
>>
>
>Obviously readline is not installed on your 
>system and RAqua doesn't seem to install it 
>either. The usual solution is to compile 
>readline yourself. For your convenience you'll 
>find a compiled version attached. To install, 
>copy it to your desktop and run in terminal:
>
>cd /
>sudo tar fvxz ~/Desktop/readline.4.3.tar.gz
>
>Cheers,
>Simon
>
>
>
>Content-Type: application/x-gzip;
>	x-unix-mode=0644;
>	name="readline.4.3.tar.gz"
>Content-Disposition: attachment;
>	filename=readline.4.3.tar.gz
>
>Attachment converted: Macintosh HD:readline.4.3.tar.gz (????/----) (00119BD9)
>
>---
>Simon Urbanek
>Department of computer oriented statistics and data analysis
>Universit?tsstr. 14
>86135 Augsburg
>Germany
>
>Tel: +49-821-598-2236
>Fax: +49-821-598-2280
>
>Simon.Urbanek at Math.Uni-Augsburg.de
>http://simon.urbanek.info


-- 
*********************************************************************
Brian Beckage
Department of Botany
University of Vermont
Marsh Life Science Building
Burlington, VT 05405

Phone:  802 656-0197
Fax  :  802 656-0440
email:  Brian.Beckage at uvm.edu
web  :  www.uvm.edu/~bbeckage



From MSchwartz at medanalytics.com  Thu Dec 11 18:38:40 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Thu, 11 Dec 2003 11:38:40 -0600
Subject: [R] read.spss question warning compression bias
In-Reply-To: <sfd84ab8.017@MAIL.NDRI.ORG>
References: <sfd84ab8.017@MAIL.NDRI.ORG>
Message-ID: <1071164319.6754.2.camel@localhost.localdomain>

On Thu, 2003-12-11 at 09:44, Peter Flom wrote:
> Hello again
> 
> I have a file from SPSS in .sav format.
> 
> when I run
> 
> library(foreign)
> cvar<-as.data.frame(read.spss("c:\\NDRI\\cvar\\data\\cvar2rev3.sav"))
> 
> I get a warning
> 
> Warning message: 
> c:\NDRI\cvar\data\cvar2rev3.sav: Compression bias (0) is not the usual
> value of 100. 
> 
> The data appear to be OK, but I am concerned.  
> 
>  (I tried searching the archives and the documenation for data import
> export, but saw nothing).
> 
> 
> Thanks as always
> 
> Peter


The error message appears to be coming from sfm-read.c in the package.
The particular code is at line 682:

  ext->bias = hdr.bias;
  if (ext->bias != 100.0)
    warning("%s: Compression bias (%g) is not the usual "
	    "value of 100.", h->fn, ext->bias);

I have not used SPSS, but I presume that as with other applications, it
can save the data files in either compressed or non-compressed formats.

>From reading the code in the above source file, there appears to be a
SPSS header structure that contains information on the nature of the
file and its contents.

One of the entries in the structure indicates whether or not the data
file is compressed. That "flag" is then stored in:

  ext->compressed = hdr.compressed;

at line 669. That value is either 0 (FALSE) or 1 (TRUE).

That information comes into play later on at line 1393, which is the
read_compressed_data function.

It would seem that if the data file is not compressed, which I presume
is the case with your file, the check of the compression bias at line
682 is superfluous.

Thus, if my read is correct, the fix (if one is needed) would be to add
a check in advance of the bias check code:

  if (ext->compressed)
  {
    ext->bias = hdr.bias;
    if (ext->bias != 100.0)
      warning("%s: Compression bias (%g) is not the usual "
              "value of 100.", h->fn, ext->bias);
  }


An additional question might be, if the file is not compressed, what is
the default bias value set by SPSS? If it is 0, then the check is
meaningless. On the other hand, if the default value is 100, whether or
not the file is compressed, then the warning message would serve a
purpose in flagging the possibility of other issues. Reasonably, that
setting may be SPSS version specific.

Hopefully, I am close. If so, your data should be correct and the
warning is just that, a "warning" and not an "error".


HTH,

Marc Schwartz



From rudiscu at yahoo.es  Thu Dec 11 19:14:14 2003
From: rudiscu at yahoo.es (=?iso-8859-1?q?ruddi=20rodriguez?=)
Date: Thu, 11 Dec 2003 19:14:14 +0100 (CET)
Subject: [R] (no subject)
Message-ID: <20031211181414.43424.qmail@web20311.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031211/29e45c97/attachment.pl

From tlumley at u.washington.edu  Thu Dec 11 19:32:56 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 11 Dec 2003 10:32:56 -0800 (PST)
Subject: [R] read.spss question warning compression bias
In-Reply-To: <1071164319.6754.2.camel@localhost.localdomain>
References: <sfd84ab8.017@MAIL.NDRI.ORG>
	<1071164319.6754.2.camel@localhost.localdomain>
Message-ID: <Pine.A41.4.58.0312111030310.101260@homer38.u.washington.edu>

On Thu, 11 Dec 2003, Marc Schwartz wrote:
>
> An additional question might be, if the file is not compressed, what is
> the default bias value set by SPSS? If it is 0, then the check is
> meaningless. On the other hand, if the default value is 100, whether or
> not the file is compressed, then the warning message would serve a
> purpose in flagging the possibility of other issues. Reasonably, that
> setting may be SPSS version specific.
>

I think the issue is that the format is not documented, so the author of
the code (Ben Pfaff) didn't know what a change in the value would imply.
If the file is apparently read correctly it seems that it doesn't imply
anything.

	-thomas



From bill.shipley at usherbrooke.ca  Thu Dec 11 19:40:02 2003
From: bill.shipley at usherbrooke.ca (Bill Shipley)
Date: Thu, 11 Dec 2003 13:40:02 -0500
Subject: [R] typeIII SS for lme?
Message-ID: <028301c3c016$30101640$8d1ad284@BIO041>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031211/85442724/attachment.pl

From ddfloyd at qwest.net  Thu Dec 11 19:55:03 2003
From: ddfloyd at qwest.net (Donna Floyd)
Date: Thu, 11 Dec 2003 11:55:03 -0700
Subject: [R] Scatter Plot Matrix
Message-ID: <DIENIFFKMGBMJINKNKGFKEILCLAA.ddfloyd@qwest.net>

I am researching the origin and use of the "scatter plot matrix" or
"scatter matrix."

A common event with the normal scatter diagram is the overlaying of
points, given that one (x,y) pair exists and that each (x,y) point
plotted exists at the same point.  What I am wondering is if it is
possible to expand the (x,y) area to form a larger cell, such that
each point plotted would exist distinctly within the larger area.
In the example, each dot represents an (x,y) point that exists
within the bounded area of the (x,y) pair


This possibility allows each point plotted to be distinctly
recognized.

Your thoughts on this would be greatly appreciated.

Donna D. Floyd
USA



From ripley at stats.ox.ac.uk  Thu Dec 11 20:05:47 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 11 Dec 2003 19:05:47 +0000 (GMT)
Subject: [R] typeIII SS for lme?
In-Reply-To: <028301c3c016$30101640$8d1ad284@BIO041>
Message-ID: <Pine.LNX.4.44.0312111903560.2587-100000@gannet.stats>

On Thu, 11 Dec 2003, Bill Shipley wrote:

> To avoid angry replies, let me first say that I know that the use of
> Type III sums of squares is controversial, and that some statisticians
> recommend instead that significance be judged using the non-marginal
> terms in the ANOVA.  However, given that type III SS is also demanded by
> some
  is there a function (equivalent to drop1 for lm) to obtain type
> III sums of squares for mixed models using the lme function?

I don't think you want SSq for a non-least-squares fit.
Changes in AIC are computed by dropterm() in MASS for lme fits (and 
possibly also by drop1: I have not checked).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Dec 11 20:06:38 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 11 Dec 2003 19:06:38 +0000 (GMT)
Subject: [R] Scatter Plot Matrix
In-Reply-To: <DIENIFFKMGBMJINKNKGFKEILCLAA.ddfloyd@qwest.net>
Message-ID: <Pine.LNX.4.44.0312111906120.2587-100000@gannet.stats>

?jitter may help you.

On Thu, 11 Dec 2003, Donna Floyd wrote:

> I am researching the origin and use of the "scatter plot matrix" or
> "scatter matrix."
> 
> A common event with the normal scatter diagram is the overlaying of
> points, given that one (x,y) pair exists and that each (x,y) point
> plotted exists at the same point.  What I am wondering is if it is
> possible to expand the (x,y) area to form a larger cell, such that
> each point plotted would exist distinctly within the larger area.
> In the example, each dot represents an (x,y) point that exists
> within the bounded area of the (x,y) pair
> 
> 
> This possibility allows each point plotted to be distinctly
> recognized.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Thu Dec 11 20:19:03 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 11 Dec 2003 20:19:03 +0100
Subject: [R] Bugtracking webserver down
Message-ID: <x2llpjqi1k.fsf@biostat.ku.dk>

Hi,

Due to a planned power outage, the webserver known as
bugs.r-project.org is down until tomorrow. Other (mail-based)
processing of bug reports should be continuing normally. 

        -p
-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From arditi at optonline.net  Thu Dec 11 20:23:08 2003
From: arditi at optonline.net (Aries Arditi)
Date: Thu, 11 Dec 2003 14:23:08 -0500
Subject: [R] Binomial distribution & Catherine Loader's paper
Message-ID: <005b01c3c01c$36a2ebd0$1702a8c0@DJANGO>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031211/186be1c6/attachment.pl

From bates at stat.wisc.edu  Thu Dec 11 20:37:58 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: 11 Dec 2003 13:37:58 -0600
Subject: [R] Mail through r-project.org is down
Message-ID: <6rad5zf8mh.fsf@bates4.stat.wisc.edu>

I just saw that mail transport on the machine franz.stat.wisc.edu,
which is the mail server for the r-project.org domain, is down.
Initial attempts to revive it have not been successful.

Please send mail to any of the r-project lists directly to the list
server machine at stat.math.ethz.ch.  That is, please use
r-help at stat.math.ethz.ch and not r-help at r-project.org

-- 
Douglas Bates                            bates at stat.wisc.edu
Statistics Department                    608/262-2598
University of Wisconsin - Madison        http://www.stat.wisc.edu/~bates/



From MSchwartz at medanalytics.com  Thu Dec 11 20:55:42 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Thu, 11 Dec 2003 13:55:42 -0600
Subject: [R] read.spss question warning compression bias
In-Reply-To: <Pine.A41.4.58.0312111030310.101260@homer38.u.washington.edu>
References: <sfd84ab8.017@MAIL.NDRI.ORG>
	<1071164319.6754.2.camel@localhost.localdomain>
	<Pine.A41.4.58.0312111030310.101260@homer38.u.washington.edu>
Message-ID: <1071172542.6754.85.camel@localhost.localdomain>

On Thu, 2003-12-11 at 12:32, Thomas Lumley wrote:
> On Thu, 11 Dec 2003, Marc Schwartz wrote:
> >
> > An additional question might be, if the file is not compressed, what is
> > the default bias value set by SPSS? If it is 0, then the check is
> > meaningless. On the other hand, if the default value is 100, whether or
> > not the file is compressed, then the warning message would serve a
> > purpose in flagging the possibility of other issues. Reasonably, that
> > setting may be SPSS version specific.
> >
> 
> I think the issue is that the format is not documented, so the author of
> the code (Ben Pfaff) didn't know what a change in the value would imply.
> If the file is apparently read correctly it seems that it doesn't imply
> anything.
> 
> 	-thomas



Thanks for the clarification Thomas.

I did some searching of the PSPP site and found the following:

http://www.gnu.org/software/pspp/manual/pspp_18.html#SEC170

The compression bias is defined as:

flt64 bias;
        Compression bias. Always set to 100. The significance of this
        value is that only numbers between (1 - bias) and (251 - bias)
        can be compressed.
        

So it would seem to potentially impact aspects of the file compression
data structure, when compression is used.

I am not sure if the "Always set to 100" is unique to PSPP in how Ben
elected to do things. Presumably if that is always the case, even with
SPSS, one might reasonably wonder: why have it, if it does not vary?

It leaves things unclear as to under what circumstances this value would
change. 

I did some Googling and found the following text snippet from a
presumably dated SPSS manual for the syntax of the SAVE command:


SAVE OUTFILE=file 

[/VERSION={3**}] {2 } 

[/UNSELECTED=[{RETAIN}] {DELETE} 

[/KEEP={ALL** }] [/DROP=varlist] {varlist} 

[/RENAME=(old varlist=new varlist)...] 

[/MAP] 

[/{COMPRESSED }] {UNCOMPRESSED} 

**Default if the subcommand is omitted.


COMPRESSED and UNCOMPRESSED Subcommands 

COMPRESSED saves the file in compressed form. UNCOMPRESSED saves the
file in uncompressed form. In a compressed file, small integers (from 
99 to 155) are stored in one byte instead of the eight bytes used in an
uncompressed file.

The only specification is the keyword COMPRESSED or UNCOMPRESSED. There
are no additional specifications. 

Compressed data files occupy less disk space than do uncompressed data
files. 

Compressed data files take longer to read than do uncompressed data
files. 

The GET command, which reads SPSS-format data files, does not need to
specify whether the files it reads are compressed or uncompressed. 

Only one of the subcommands COMPRESSED or UNCOMPRESSED can be specified
per SAVE command. COMPRESSED is usually the default, though UNCOMPRESSED
may be the default on some systems.




So it would appear that if the above is correct, there is no user
adjustment to the bias value. The only scenario that I can envision is
if the user SAVE's the ".sav" file in an uncompressed format, where the
bias value **might** be set to 0.

Perhaps a r-help reader with access to current SPSS manuals can confirm
the above.

Until demonstrated otherwise, it seems reasonable to leave the warning
message in place as a warning (as opposed to an error), though it might
be helpful to folks to add a comment to the read.spss help file on this
for clarification. The text might read:

"NOTE: You may receive the following message:

 Warning message: 
 FileName: Compression bias (X) is not the usual value of 100.

Where 'FileName' will be the file that you are reading and 'X' will be a
numeric value, possibly 0. This *may* be the result of reading an
UNCOMPRESSED SPSS file. It is recommended that you verify the integrity
of your imported SPSS data after using read.spss() if you receive this
warning."


The wording is subject to change and of course, the integrity check
should be done under any circumstances... :-)

HTH,

Marc Schwartz



From myao at ou.edu  Thu Dec 11 21:17:07 2003
From: myao at ou.edu (Yao, Minghua)
Date: Thu, 11 Dec 2003 14:17:07 -0600
Subject: [R] Excluding from an Array the Elements of Another Array 
Message-ID: <FC0CEBD77311DA499A67ADB355A24FA20396ADF4@mail4.oulan.ou.edu>

Hi,

My question goes like this. I have two arrays A and B. I want to exclude
from A the elements contained in B.
Is there an easy way like A[A != x] to exclude a single one. Thanks for any
help.

Regards,

-MY



From MSchwartz at medanalytics.com  Thu Dec 11 21:38:59 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Thu, 11 Dec 2003 14:38:59 -0600
Subject: [R] Binomial distribution & Catherine Loader's paper
In-Reply-To: <005b01c3c01c$36a2ebd0$1702a8c0@DJANGO>
References: <005b01c3c01c$36a2ebd0$1702a8c0@DJANGO>
Message-ID: <1071175138.6754.120.camel@localhost.localdomain>

On Thu, 2003-12-11 at 13:23, Aries Arditi wrote:
> Hi,
> 
> I've been trying, without success to find a copy of the paper, by Catherine
> Loader, that describes the algorithn underlying the rbinom() and associated
> functions.  The title is "Fast and Accurate Computation of Binomial
> Probabilities." All of the links to the paper that I've seen (including in
> the R docs) lead nowhere (i.e. are 404). I've sent Dr. Loader several
> emails, but have not received a response.
> 
> Does anyone have a copy of this paper?
> 
> Please reply directly to me as I'm not a subscriber to this list.
> 
> Thank you.


A quick Google search turns up the following:

http://kiefer.stat.cwru.edu/~catherine/

That is her main contact page.  There is a list of publications here:

http://kiefer.stat.cwru.edu/~catherine/pubs.html

There is a link to the paper (as a Postscript file) at the bottom of
that page, however the link appears to be dead.

You can e-mail her using catherine at cwru.edu

HTH,

Marc Schwartz



From pauljohn at ku.edu  Thu Dec 11 21:44:25 2003
From: pauljohn at ku.edu (Paul Johnson)
Date: Thu, 11 Dec 2003 14:44:25 -0600
Subject: [R] packaging standards for rda files?
Message-ID: <3FD8D729.3030605@ku.edu>

Dear everybody:

We used the fine foreign library to bring in an SPSS dataset that was 
about 9 megabytes and I can squeeze it into a much smaller R object 
using compression with

save(ndat, file="NatAnnES2000.rda", compress=T). 

I can use load() to get the "ndat" dataframe back, that's all good as 
far as I can see.  If I put that file in the data subdirectory, then the 
data() command finds it and I can load it. 

Seems fine, but then I started wondering if there is not some more 
sophisticated way of packaging these things. For example, how do people 
put in the meta information that appears in the right side of the data() 
output, as in:

Data sets in package '.':

NatAnnES2000     

Data sets in package 'base':

Formaldehyde            Determination of Formaldehyde
HairEyeColor            Hair and Eye Color of Statistics Students
...

Are there other attributes that I should specify if I want to package an 
.rda file for other users?

An rda file created in this way will translate across platforms, won't it?

pj

-- 
Paul E. Johnson                       email: pauljohn at ku.edu
Dept. of Political Science            http://lark.cc.ku.edu/~pauljohn
1541 Lilac Lane, Rm 504                              
University of Kansas                  Office: (785) 864-9086
Lawrence, Kansas 66044-3177           FAX: (785) 864-5700



From aylin at ncmir.ucsd.edu  Thu Dec 11 21:57:58 2003
From: aylin at ncmir.ucsd.edu (Aylin Yilmaz)
Date: Thu, 11 Dec 2003 12:57:58 -0800 (PST)
Subject: [R] How can I call R from Java or C?
In-Reply-To: <Pine.LNX.4.44.0312110701290.27676-100000@gannet.stats>
References: <Pine.LNX.4.44.0312110701290.27676-100000@gannet.stats>
Message-ID: <Pine.GSO.4.58.0312111215460.3687@ncmir>


Hi,

Thanks for the reply.
I develop on Linux and the production will be on Unix.

I just looked at SJava on OmegaHat.
Yes, I think this is what I need. Can I call any R function using this
interface such as the tests in ctest package?


Thanks a lot.

On Thu, 11 Dec 2003, Prof Brian Ripley wrote:

> On Wed, 10 Dec 2003, Aylin Yilmaz wrote:
>
> > I am new to R. I have been looking for a way to call R functions from
> > Java or C.
> >
> > What I have found so far is: I can execute R commands in batch mode from a
> > file and redirect the output to another file. However, I am not sure this
> > is the way to do it.
> >
> > About C, is it true that I can only call R from a C process,only if that C
> > process is invoked within R itself? So is it R->C->R ?
>
> Not true.  There are several ways to embed R in a C-based executable, but
> as you have not told us your OS, I won't elaborate.
>
> > If I could call R from C, then I could possibly call C from Java using
> > JNI (i am not sure exactly how well that will work)
>
> Have you looked at the SJava package on the Omegahat site?
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
>



From aylin at ncmir.ucsd.edu  Thu Dec 11 22:05:30 2003
From: aylin at ncmir.ucsd.edu (Aylin Yilmaz)
Date: Thu, 11 Dec 2003 13:05:30 -0800 (PST)
Subject: [R] How can I call R from Java or C?
In-Reply-To: <001d01c3bfcc$83043330$e502eb82@maths.lth.se>
References: <001d01c3bfcc$83043330$e502eb82@maths.lth.se>
Message-ID: <Pine.GSO.4.58.0312111259580.3687@ncmir>


Hi Henrik,

Thank you very much. This link is also very helpful.

Ilene

On Thu, 11 Dec 2003, Henrik Bengtsson wrote:

> Hi. Depending on what you are trying to do, you may also take a look
> at Rserve
>
>  http://stats.math.uni-augsburg.de/Rserve/
>
> From the webpage: "Rserve is a TCP/IP server which allows other
> programs to use facilities of R (see www.r-project.org) from various
> languages without the need to initialize R or link against R library.
> Every connection has a separate workspace and working directory.
> Client-side implementations are available for popular languages such
> as C/C++ and Java."
>
> From the Rserve FAQ: Supported Platforms: "Rserve should work on any
> plarform supporting shared libraries which is also supported by R. The
> following platforms were tested and are known to work with the latest
> release: Linux (PPC, x86), Windows (x86), Mac OS X / Darwin (PPC), AIX
> (PPC since version 0.1-10), SunOS (sun4u since version 0.2)"
>
> I haven't used it myself so I do not know about the performance etc,
> but it looks like it is easy to get started with.
>
> Henrik Bengtsson
> Lund University
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Aylin Yilmaz
> > Sent: den 10 december 2003 23:48
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] How can I call R from Java or C?
> >
> >
> >
> > Hi all,
> > I am new to R. I have been looking for a way to call R
> > functions from Java or C.
> >
> > What I have found so far is: I can execute R commands in
> > batch mode from a file and redirect the output to another
> > file. However, I am not sure this is the way to do it.
> >
> > About C, is it true that I can only call R from a C
> > process,only if that C process is invoked within R itself? So
> > is it R->C->R ?
> >
> > If I could call R from C, then I could possibly call C from
> > Java using JNI (i am not sure exactly how well that will work)
> >
> > If you have any suggestions for R's interface that would be
> > really nice.
> >
> > Thanks,
> > ilene
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailma> n/listinfo/r-help
>
>
>



From RBaskin at ahrq.gov  Thu Dec 11 21:58:27 2003
From: RBaskin at ahrq.gov (RBaskin@ahrq.gov)
Date: Thu, 11 Dec 2003 15:58:27 -0500
Subject: [R] Excluding from an Array the Elements of Another Array 
Message-ID: <3598558AD728D41183350008C7CF291C0F16BA05@exchange1.ahrq.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031211/b2806ce9/attachment.pl

From ripley at stats.ox.ac.uk  Thu Dec 11 22:15:59 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 11 Dec 2003 21:15:59 +0000 (GMT)
Subject: [R] How can I call R from Java or C?
In-Reply-To: <Pine.GSO.4.58.0312111215460.3687@ncmir>
Message-ID: <Pine.LNX.4.44.0312112114210.16256-100000@gannet.stats>

On Thu, 11 Dec 2003, Aylin Yilmaz wrote:

> Thanks for the reply.
> I develop on Linux and the production will be on Unix.
> 
> I just looked at SJava on OmegaHat.
> Yes, I think this is what I need. Can I call any R function using this
> interface such as the tests in ctest package?

Yes.  Someone else mentioned Simon Urbanek's Rserve and that is also 
well worth exploring.

[...]


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From xam72001 at yahoo.com  Thu Dec 11 22:44:08 2003
From: xam72001 at yahoo.com (DCF)
Date: Thu, 11 Dec 2003 13:44:08 -0800 (PST)
Subject: [R] returning plot tick marks
Message-ID: <20031211214408.63258.qmail@web41905.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031211/80e6eade/attachment.pl

From MSchwartz at medanalytics.com  Thu Dec 11 23:33:34 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Thu, 11 Dec 2003 16:33:34 -0600
Subject: [R] returning plot tick marks
In-Reply-To: <20031211214408.63258.qmail@web41905.mail.yahoo.com>
References: <20031211214408.63258.qmail@web41905.mail.yahoo.com>
Message-ID: <1071182014.6754.124.camel@localhost.localdomain>

On Thu, 2003-12-11 at 15:44, DCF wrote:
> Is there an easy way to return the values of the axis tick marks
> resulting from plot.default? For example, plot(1:1000) would return 0,
> 200, 400, 600, 800, 1000 from the x axis.
>  
> Thanks,
>  
> Max


See ?axTicks

Example:

> plot(1:1000)

# X axis
> axTicks(1)
[1]    0  200  400  600  800 1000

# Y axis
> axTicks(2)
[1]    0  200  400  600  800 1000


HTH,

Marc Schwartz



From jasont at indigoindustrial.co.nz  Thu Dec 11 23:43:56 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Fri, 12 Dec 2003 11:43:56 +1300
Subject: [R] packaging standards for rda files?
In-Reply-To: <3FD8D729.3030605@ku.edu>
References: <3FD8D729.3030605@ku.edu>
Message-ID: <3FD8F32C.7060308@indigoindustrial.co.nz>

Paul Johnson wrote:
...
> We used the fine foreign library to bring in an SPSS dataset that was 
> about 9 megabytes and I can squeeze it into a much smaller R object 
> using compression with
> 
> save(ndat, file="NatAnnES2000.rda", compress=T).
...
> how do people 
> put in the meta information that appears in the right side of the data() 
> output, as in:
> 
> Data sets in package '.':
> 
> NatAnnES2000    
> Data sets in package 'base':
> 
> Formaldehyde            Determination of Formaldehyde
> HairEyeColor            Hair and Eye Color of Statistics Students
> ...

The meta information is from the documentation, such as 
help(Formaldehyde).  prompt(NatAnnES2000) will create a template .Rd 
file in R's working directory for the data frame (their doc standards 
are slightly different to functions).  Fill in the blanks, and copy to 
the man/ subdirectory of your package.

> Are there other attributes that I should specify if I want to package an 
> .rda file for other users?

Just documenting it works for me.  Rcmd CHECK or R CMD CHECK will 
grumble if there's something you've forgotten to document.

> An rda file created in this way will translate across platforms, won't it?

Not sure about the compression; otherwise, yes.

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From spencer.graves at pdf.com  Thu Dec 11 23:45:28 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 11 Dec 2003 14:45:28 -0800
Subject: [R] returning plot tick marks
In-Reply-To: <20031211214408.63258.qmail@web41905.mail.yahoo.com>
References: <20031211214408.63258.qmail@web41905.mail.yahoo.com>
Message-ID: <3FD8F388.6040805@pdf.com>

Is this what you want: 

 pretty(1:1000)
[1]    0  200  400  600  800 1000

hope this helps.  spencer graves

DCF wrote:

>Is there an easy way to return the values of the axis tick marks resulting from plot.default? For example, plot(1:1000) would return 0, 200, 400, 600, 800, 1000 from the x axis.
> 
>Thanks,
> 
>Max
>
>
>
>---------------------------------
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From christian_mora at vtr.net  Fri Dec 12 00:08:21 2003
From: christian_mora at vtr.net (Christian Mora)
Date: Thu, 11 Dec 2003 20:08:21 -0300
Subject: [R] mode
Message-ID: <000001c3c03b$abe0dd70$b43d68c8@CPQ28661778111>

How can I get the mode (most frequent value) from a dataset (continuos
variables)? I can obtain it when the data is discrete (by making a table
and looking at the higher frequency) but I don't know how obtain it
from, for example, a density plot of the data. Does anyone know how to
do it? Thanks
CM



From arrayprofile at yahoo.com  Fri Dec 12 00:23:37 2003
From: arrayprofile at yahoo.com (array chip)
Date: Thu, 11 Dec 2003 15:23:37 -0800 (PST)
Subject: [R] plot of survival probability vs. covariate
Message-ID: <20031211232337.84609.qmail@web41205.mail.yahoo.com>

Hi everyone,

I am fitting a cox proportional hazard model with a
continuous variable "x" as the covariate:

fit<-coxph(Surv(time, status)~x)

Now I wanted to make a plot of survival probability
vs. the covariate, and the 95% confidence interval for
the survival probability. It's just like a
Kaplan-Meier Survival curve, except now the x axis
represents the value of covariate, not the time.
Someone gave me a reference to a paper in JASA by Gary
(1992) for this type of plot, but I didn't have the
access to the paper. So I am wondering if anyone knows
how to do this in R or S-Plus?

In addition, can anyone explain to me what are the
following "type" options in predict.coxph()
predicting?

predict(fit,type='lp',se.fit=T)
predict(fit,type='risk',se.fit=T)
predict(fit,type='expected',se.fit=T)
predict(fit,type='terms',se.fit=T)


Thank you very much



From dauphas at uchicago.edu  Fri Dec 12 00:29:07 2003
From: dauphas at uchicago.edu (dauphasuser)
Date: Thu, 11 Dec 2003 17:29:07 -0600
Subject: [R] Bivariate linear regression
Message-ID: <D0D631C7-2C31-11D8-BD51-0003939B60B6@uchicago.edu>

I have measurements with uncorrelated uncertainties on both axes. I 
would like to get the uncertainties on the intercept and the slope of 
the weighted linear regression model taking into account the 
uncertainties of the measurements. Is these any way to do that in R?
Thanks- Nicolas



From bates at stat.wisc.edu  Fri Dec 12 02:30:26 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: 11 Dec 2003 19:30:26 -0600
Subject: [R] packaging standards for rda files?
In-Reply-To: <3FD8F32C.7060308@indigoindustrial.co.nz>
References: <3FD8D729.3030605@ku.edu> <3FD8F32C.7060308@indigoindustrial.co.nz>
Message-ID: <6rzndyyg99.fsf@bates4.stat.wisc.edu>

Jason Turner <jasont at indigoindustrial.co.nz> writes:

> Paul Johnson wrote:
> ...
> > An rda file created in this way will translate across platforms, won't it?
> 
> Not sure about the compression; otherwise, yes.

It works with compression too.



From bates at stat.wisc.edu  Fri Dec 12 02:32:24 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: 11 Dec 2003 19:32:24 -0600
Subject: [R] mode
In-Reply-To: <000001c3c03b$abe0dd70$b43d68c8@CPQ28661778111>
References: <000001c3c03b$abe0dd70$b43d68c8@CPQ28661778111>
Message-ID: <6rvfomyg5z.fsf@bates4.stat.wisc.edu>

"Christian Mora" <christian_mora at vtr.net> writes:

> How can I get the mode (most frequent value) from a dataset (continuos
> variables)? I can obtain it when the data is discrete (by making a table
> and looking at the higher frequency) but I don't know how obtain it
> from, for example, a density plot of the data. Does anyone know how to
> do it? Thanks

I don't think the mode of a sample from a continuous random variable is well
defined.



From feh3k at spamcop.net  Fri Dec 12 02:35:37 2003
From: feh3k at spamcop.net (Frank E Harrell Jr)
Date: Thu, 11 Dec 2003 20:35:37 -0500
Subject: [R] Re: [S] plot of survival probability vs. covariate
In-Reply-To: <20031211232337.84609.qmail@web41205.mail.yahoo.com>
References: <20031211232337.84609.qmail@web41205.mail.yahoo.com>
Message-ID: <20031211203537.63e8f320.feh3k@spamcop.net>

On Thu, 11 Dec 2003 15:23:37 -0800 (PST)
array chip <arrayprofile at yahoo.com> wrote:

> Hi everyone,
> 
> I am fitting a cox proportional hazard model with a
> continuous variable "x" as the covariate:
> 
> fit<-coxph(Surv(time, status)~x)
> 
> Now I wanted to make a plot of survival probability
> vs. the covariate, and the 95% confidence interval for
> the survival probability. It's just like a
> Kaplan-Meier Survival curve, except now the x axis
> represents the value of covariate, not the time.
> Someone gave me a reference to a paper in JASA by Gary
> (1992) for this type of plot, but I didn't have the
> access to the paper. So I am wondering if anyone knows
> how to do this in R or S-Plus?

This will get you started.  Look into the documentation for confidence
limits.

library(Design)  # if S-Plus do  library(Hmisc,T);library(Design,T)
dd <- datadist(yourdataframe)
options(datadist='dd')
f <- cph(Surv( ) ~ x1+x2, surv=T, x=T, y=T)
plot(f, x1=NA, time=2)  # plots 2-year survival

Frank Harrell

> 
> In addition, can anyone explain to me what are the
> following "type" options in predict.coxph()
> predicting?
> 
> predict(fit,type='lp',se.fit=T)
> predict(fit,type='risk',se.fit=T)
> predict(fit,type='expected',se.fit=T)
> predict(fit,type='terms',se.fit=T)
> 
> 
> Thank you very much
> 
> 
> 
> __________________________________
> Do you Yahoo!?
> New Yahoo! Photos - easier uploading and sharing.
> http://photos.yahoo.com/
> --------------------------------------------------------------------
> This message was distributed by s-news at lists.biostat.wustl.edu.  To
> ...(s-news.. clipped)...



---
Frank E Harrell Jr   Professor and Chair           School of Medicine
                     Department of Biostatistics   Vanderbilt University



From jjava at priscian.com  Fri Dec 12 10:36:58 2003
From: jjava at priscian.com (Jim Java)
Date: Fri, 12 Dec 2003 01:36:58 -0800
Subject: [R] C++: Appending Values onto an R-Vector.
Message-ID: <200312120136.AA184746198@priscian.com>

Hi folks. I posted this question a few days ago, but maybe it got lost
because of the code I included with it. I'm having a problem using the
SET_LENGTH() macro in an R extension I'm writing in C++. In a function
within the extension I use SET_LENGTH() to resize R vectors so as to
allow the concatenation of single values onto the vectors -- it's a
"push back" function to append values onto the end of a vector.
However, when I use this function to push back a large number of
values one at a time, Rgui.exe (I'm working with R 1.8.1 in Windows
XP) crashes from an Access Violation; if, however, I pre-allocate
space (is the space actually pre-allocated?) for the vector (say with
NEW_INTEGER(n) rather than NEW_INTEGER(0)) and insert values into the
allocated slots, the code works fine. If you'd like to see some test
code, I've already posted it here:

https://www.stat.math.ethz.ch/pipermail/r-help/2003-December/041871.html

Here's my question, then: Is SET_LENGTH() the appropriate way to
create space for tacking values onto the end of an R-vector in C++, or
should I be trying to tack them on in some other way?

Thanks again!

 -- Jim Java



From maj at stats.waikato.ac.nz  Fri Dec 12 02:48:58 2003
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Fri, 12 Dec 2003 14:48:58 +1300
Subject: [R] mode
In-Reply-To: <6rvfomyg5z.fsf@bates4.stat.wisc.edu>
References: <000001c3c03b$abe0dd70$b43d68c8@CPQ28661778111>
	<6rvfomyg5z.fsf@bates4.stat.wisc.edu>
Message-ID: <3FD91E8A.7060203@stats.waikato.ac.nz>

The mode of a data vector x might be defined as the limit of m_p as p 
tends to zero from above and where m_p is the m minimizing
sum(abs(x - m)).  I would not expect the mode so defined to be of much 
use in data analysis, nor would it be easy to compute.

Murray

Douglas Bates wrote:

> "Christian Mora" <christian_mora at vtr.net> writes:
> 
> 
>>How can I get the mode (most frequent value) from a dataset (continuos
>>variables)? I can obtain it when the data is discrete (by making a table
>>and looking at the higher frequency) but I don't know how obtain it
>>from, for example, a density plot of the data. Does anyone know how to
>>do it? Thanks
> 
> 
> I don't think the mode of a sample from a continuous random variable is well
> defined.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862



From maj at stats.waikato.ac.nz  Fri Dec 12 04:39:42 2003
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Fri, 12 Dec 2003 16:39:42 +1300
Subject: [R] mode
In-Reply-To: <6rvfomyg5z.fsf@bates4.stat.wisc.edu>
References: <000001c3c03b$abe0dd70$b43d68c8@CPQ28661778111>
	<6rvfomyg5z.fsf@bates4.stat.wisc.edu>
Message-ID: <3FD9387E.9080004@stats.waikato.ac.nz>

Opps! This is what I should have written:

The mode of a data vector x might be defined as the limit of m_p as p 
tends to zero from above and where m_p is the m minimizing
sum(abs(x - m)^p).  I would not expect the mode so defined to be of much 
use in data analysis, nor would it be easy to compute.

Murray

[Thanks to Duncan Murdoch for noticing the missing ^p .]

Douglas Bates wrote:

> "Christian Mora" <christian_mora at vtr.net> writes:
> 
> 
>>How can I get the mode (most frequent value) from a dataset (continuos
>>variables)? I can obtain it when the data is discrete (by making a table
>>and looking at the higher frequency) but I don't know how obtain it
>>from, for example, a density plot of the data. Does anyone know how to
>>do it? Thanks
> 
> 
> I don't think the mode of a sample from a continuous random variable is well
> defined.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862



From ripley at stats.ox.ac.uk  Fri Dec 12 05:14:05 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 12 Dec 2003 04:14:05 +0000 (GMT)
Subject: [R] mode
In-Reply-To: <6rvfomyg5z.fsf@bates4.stat.wisc.edu>
Message-ID: <Pine.LNX.4.44.0312120358410.23340-100000@gannet.stats>

On 11 Dec 2003, Douglas Bates wrote:

> "Christian Mora" <christian_mora at vtr.net> writes:
> 
> > How can I get the mode (most frequent value) from a dataset (continuos
> > variables)? I can obtain it when the data is discrete (by making a table
> > and looking at the higher frequency) but I don't know how obtain it
> > from, for example, a density plot of the data. Does anyone know how to
> > do it? Thanks
> 
> I don't think the mode of a sample from a continuous random variable is well
> defined.

I agree, but one might take the viewpoint that if this were assumed to be 
an iid sample from a unimodal distribution, can one estimate the mode?
Or if from a bi-, tri- modal etc can one estimate the number and position 
of the modes?  There is quite a lot of work on those problems: here is a 
link to one approach:

http://www.isi-2003.de/guest/3112.pdf?MItabObj=pcoabstract&MIcolObj=uploadpaper&MInamObj=id&MIvalObj=3112&MItypeObj=application/pdf

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Dec 12 05:20:30 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 12 Dec 2003 04:20:30 +0000 (GMT)
Subject: [R] Bivariate linear regression
In-Reply-To: <D0D631C7-2C31-11D8-BD51-0003939B60B6@uchicago.edu>
Message-ID: <Pine.LNX.4.44.0312120416060.23340-100000@gannet.stats>

On Thu, 11 Dec 2003, dauphasuser wrote:

> I have measurements with uncorrelated uncertainties on both axes. I 
> would like to get the uncertainties on the intercept and the slope of 
> the weighted linear regression model taking into account the 
> uncertainties of the measurements. Is these any way to do that in R?

Yes, but

- this is not bivariate linear regression and
- either weighted linear regression is biased.

Perhaps the best approach is to fit a functional relationship model. I
would need to know a bit more -- you say `weighted' so do the standard
errors (`uncertainties') vary by point, for example?

Perhaps rather than (mis)use terminology you could explain in 
non-technical language the problem you are trying to solve?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jc at or.psychology.dal.ca  Fri Dec 12 05:27:52 2003
From: jc at or.psychology.dal.ca (John Christie)
Date: Fri, 12 Dec 2003 00:27:52 -0400
Subject: [R] mean as a condition of an effect?
Message-ID: <8CA177CE-2C5B-11D8-AD78-000A956DE534@or.psychology.dal.ca>

	I got a letter from a reviewer about a recent paper and it seems like 
he is asking for something that is... inappropriate.  But, I am not 
certain that the whole argument would be.
	The dv in the study is reaction time.  And, one of my hypotheses is 
that fast people will tend to show a postitive effect across the 
independent variable (A1>A2) while slow people will show a negative 
effect (A1<A2).  The editor suggested that I perform an analysis that 
looked at the interaction between my mean subject RT and my independent 
variable A on RT  (rt ~ A:subjectrt).  But, isn't this wrong given that 
one of the independent variables is necessarily related to the 
dependent variable?  I don't have anything against finding this 
interaction in principle.  But, it seems to me that performing the 
median split I had originally planned and looking at the effect 
direction on either side is more appropriate.
	Opinions?



From ripley at stats.ox.ac.uk  Fri Dec 12 05:34:31 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 12 Dec 2003 04:34:31 +0000 (GMT)
Subject: [R] C++: Appending Values onto an R-Vector.
In-Reply-To: <200312120136.AA184746198@priscian.com>
Message-ID: <Pine.LNX.4.44.0312120423250.23340-100000@gannet.stats>

This was posted so it was not lost.  Probably no one felt like giving you
free technical support.

You are using the Rdefines.h macros with which few of use are familiar.
But you will find the native version, lengthgets, used to extend vectors
in several places in the R sources.  I don't understand your code (the
real code is hidden behind an obscuring layer of macros), but this sounds
very like a protection problem.

In any case, lengthgets just reallocates and copies and it probably more 
efficient to do that yourself.

On Fri, 12 Dec 2003, Jim Java wrote:

> Hi folks. I posted this question a few days ago, but maybe it got lost
> because of the code I included with it. I'm having a problem using the
> SET_LENGTH() macro in an R extension I'm writing in C++. In a function
> within the extension I use SET_LENGTH() to resize R vectors so as to
> allow the concatenation of single values onto the vectors -- it's a
> "push back" function to append values onto the end of a vector.
> However, when I use this function to push back a large number of
> values one at a time, Rgui.exe (I'm working with R 1.8.1 in Windows
> XP) crashes from an Access Violation; if, however, I pre-allocate
> space (is the space actually pre-allocated?) for the vector (say with
> NEW_INTEGER(n) rather than NEW_INTEGER(0)) and insert values into the
> allocated slots, the code works fine. If you'd like to see some test
> code, I've already posted it here:
> 
> https://www.stat.math.ethz.ch/pipermail/r-help/2003-December/041871.html
> 
> Here's my question, then: Is SET_LENGTH() the appropriate way to
> create space for tacking values onto the end of an R-vector in C++, or
> should I be trying to tack them on in some other way?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jc at or.psychology.dal.ca  Fri Dec 12 05:46:10 2003
From: jc at or.psychology.dal.ca (John Christie)
Date: Fri, 12 Dec 2003 00:46:10 -0400
Subject: [R] lme error terms
Message-ID: <1B9183E2-2C5E-11D8-AD78-000A956DE534@or.psychology.dal.ca>

	I was performing a repeated measures analysis using lme.  I can get 
the F terms using the anova command but how do I get the error terms 
for the F's?



From jjava at priscian.com  Fri Dec 12 14:36:55 2003
From: jjava at priscian.com (Jim Java)
Date: Fri, 12 Dec 2003 05:36:55 -0800
Subject: [R] C++: Appending Values onto an R-Vector.
Message-ID: <200312120536.AA148045870@priscian.com>

---------- Original Message ----------------------------------
From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
Date:  Fri, 12 Dec 2003 04:34:31 +0000 (GMT)

>This was posted so it was not lost.  Probably no one felt like giving you
>free technical support.

Thanks for your answer -- I wasn't trying to scam anything for free, just trying to see if anyone from the Core Team could give me some advice on this problem, which seems like something that someone would've run into before. I've been trying to get some code done quickly (I've only been involved in R development for about a month) and was hoping there might be a simple answer to my problem that I'd overlooked. Since there isn't a simple answer, I will take the time to go through the R source code to see whether I can figure out what I must change in my code to accord with the way R does things. It's my fault.

>You are using the Rdefines.h macros with which few of use are familiar.
>But you will find the native version, lengthgets, used to extend vectors
>in several places in the R sources.  I don't understand your code (the
>real code is hidden behind an obscuring layer of macros), but this sounds
>very like a protection problem.

I will look at the source for lengthgets(). If mine's a protection problem, though, I must've missed an error in my test code, because I made it as simple as possible so as to reproduce the Access Violation, with great attention to protecting the created SEXP's correctly. I will test the code in Linux, too, to see whether a similar error happens.

>In any case, lengthgets just reallocates and copies and it probably more 
>efficient to do that yourself.

If lengthgets() finally just realloc()s the vectors I've created, then what I'm doing is quite inefficient. If you have the patience for another question, Are R vectors represented as linked lists in C or as simple arrays? I'm asking because I wonder how c() is implemented; I'd guess I should call whatever c() calls within the R code to add values onto vectors, but I'm not sure exactly what c() does there. If the answer is "You need to study the R source yourself" that's okay; I'm new here and don't wish to try to take more than I'm entitled to, nor do I wish to task the beneficence of the founders!

Cheers,

Jim Java



From Raimondas at vb.lt  Fri Dec 12 06:55:14 2003
From: Raimondas at vb.lt (Raimondas B.)
Date: Fri, 12 Dec 2003 07:55:14 +0200
Subject: [R] About differential evolution
Message-ID: <002201c3c074$834ac2c0$6e25b10a@berniunas>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031212/d4da5258/attachment.pl

From Phguardiol at aol.com  Fri Dec 12 12:30:50 2003
From: Phguardiol at aol.com (Phguardiol@aol.com)
Date: Fri, 12 Dec 2003 06:30:50 -0500
Subject: [R] survival analysis question
Message-ID: <598AD83F.484216BD.0C58B543@aol.com>

Hi,
I m studying the effect of a binary covariate on the time to an event, ie, patient requiring treatment. 
It clearly appears that this covariate has an effect during the first year of life but not anymore thereafter.
I d like to know until what age this covariate has a significant impact on the endpoint of interest, knowing that there is a competing risk here since the patient can also die during the time of study.
What would be the best option to do so ? 
Said in a different way, what I think is that when this covariate is = 1 the risk of requiring a treatment is increased as compared with patients who have this covariate = 0 by 4, 5, 6, 7, 8 years of age but after there is no more impact of this covariate since they will all receive a treatment. The idea is to be able to say, if covariate = 0 then the risk of requiring a treatment until 8 years is lower than if the covariate = 1; and I d like to be able to say until when this risk in significantly increased. Hope I m clear !  
thanks for your help
Philippe



From rdiaz at cnio.es  Fri Dec 12 13:54:35 2003
From: rdiaz at cnio.es (Ramon Diaz-Uriarte)
Date: Fri, 12 Dec 2003 13:54:35 +0100
Subject: [R] typeIII SS for lme?
In-Reply-To: <028301c3c016$30101640$8d1ad284@BIO041>
References: <028301c3c016$30101640$8d1ad284@BIO041>
Message-ID: <200312121354.35960.rdiaz@cnio.es>

Dear Bill,

You can obtain marginal tests using

anova(your.lme.object, type = "marginal")

(If you are going to compare output, note that marginal tests when using 
non-orthogonal contrasts ---SAS and treatment--- might give you unexpected 
results, last time I checked).

R.


On Thursday 11 December 2003 19:40, Bill Shipley wrote:
> To avoid angry replies, let me first say that I know that the use of
> Type III sums of squares is controversial, and that some statisticians
> recommend instead that significance be judged using the non-marginal
> terms in the ANOVA.  However, given that type III SS is also demanded by
> some
  is there a function (equivalent to drop1 for lm) to obtain type
> III sums of squares for mixed models using the lme function?
>
>
>
> Bill Shipley
>
> Associate Editor, Ecology
>
> North American Editor, Annals of Botany
>
> D?partement de biologie, Universit? de Sherbrooke,
>
> Sherbrooke (Qu?bec) J1K 2R1 CANADA
>
> Bill.Shipley at USherbrooke.ca
>
>  <http://callisto.si.usherb.ca:8080/bshipley/>
> http://callisto.si.usherb.ca:8080/bshipley/
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-- 
Ram?n D?az-Uriarte
Bioinformatics Unit
Centro Nacional de Investigaciones Oncol?gicas (CNIO)
(Spanish National Cancer Center)
Melchor Fern?ndez Almagro, 3
28029 Madrid (Spain)
Fax: +-34-91-224-6972
Phone: +-34-91-224-6900

http://bioinfo.cnio.es/~rdiaz
PGP KeyID: 0xE89B3462
(http://bioinfo.cnio.es/~rdiaz/0xE89B3462.asc)



From gordon.harrington at uni.edu  Fri Dec 12 15:11:05 2003
From: gordon.harrington at uni.edu (gordon.harrington@uni.edu)
Date: Fri, 12 Dec 2003 08:11:05 -0600
Subject: [R] Precision of probabilities in lm or glm
Message-ID: <200312120811.05564.gordon.harrington@uni.edu>

I have major experimental data which appear suitably linear with the 
diagnostic plots of lm or glm. The probabilities for coefficients for over a 
dozen variables of low inter-correlation are of the order of 10e-6 to 10e-12. 
Analyis of trimmed data to exclude outliers increases that range to 10e-16. 
Such extreme data lead me to look for every possible explanation. Can anyone 
tell me the precision of these results for lm or glm. Am I dealing with 
numbers that are meaningless beyond a certain point or are there other things 
I should be examining in the data?
-- 
Gordon M. Harrington            Mail:   3720 Village Place, #6308
Professor Emeritus                      Waterloo, IA 50702-5848
University of Northern Iowa     Phone:  319-291-8535
gordon.harrington at uni.edu       Fax:    319-291-8491
dryfly at aya.yale.edu                     319-291-8324



From hedderik at cmu.edu  Fri Dec 12 15:41:22 2003
From: hedderik at cmu.edu (Hedderik van Rijn)
Date: Fri, 12 Dec 2003 09:41:22 -0500
Subject: [R] read.spss question warning compression bias
In-Reply-To: <1071172542.6754.85.camel@localhost.localdomain>
References: <sfd84ab8.017@MAIL.NDRI.ORG>
	<1071164319.6754.2.camel@localhost.localdomain>
	<Pine.A41.4.58.0312111030310.101260@homer38.u.washington.edu>
	<1071172542.6754.85.camel@localhost.localdomain>
Message-ID: <415B087B-2CB1-11D8-9782-000A956B93BA@cmu.edu>

> So it would appear that if the above is correct, there is no user
> adjustment to the bias value. The only scenario that I can envision is
> if the user SAVE's the ".sav" file in an uncompressed format, where the
> bias value **might** be set to 0.
>
> Perhaps a r-help reader with access to current SPSS manuals can confirm
> the above.

See below for a copy and pasted text from the PDF Manuals for SPSS v11 
for Mac OS X, pages 1106/1107 in the file: [...] /SPSS\ 
11/SyntaxGuide/SPSSBase.pdf

Note that it doesn't tell you anymore whether the default is 
"compressed" or "uncompressed". The save dialog box also doesn't given 
any options, apart from selecting which type of .SAV file one wants 
(SPSS, SPSS v7, SPSS/PC+, SPSS Portable, and a load of other non-SPSS 
formats like tab delimited, Excel and even dBase II).

  - Hedderik.

SAVE
SAVE OUTFILE=file
[/VERSION={3**}]
{2 }
[/UNSELECTED=[{RETAIN}]
{DELETE}
[/KEEP={ALL** }] [/DROP=varlist]
{varlist}
[/RENAME=(old varlist=new varlist)...]
[/MAP] [/{COMPRESSED }]
{UNCOMPRESSED}
**Default if the subcommand is omitted.

Example
SAVE OUTFILE=EMPL /RENAME=(AGE=AGE88) (JOBCAT=JOBCAT88).

Overview
SAVE produces an SPSS-format data file. An SPSS-format data file 
contains data plus a dictionary.
The dictionary contains a name for each variable in the data file plus 
any assigned
variable and value labels, missing-value flags, and variable print and 
write formats. The dictionary
also contains document text created with the DOCUMENTS command.
XSAVE also creates SPSS-format data files. The difference is that SAVE 
causes data to be
read, while XSAVE is not executed until data are read for the next 
procedure.
See SAVE TRANSLATE and SAVE SCSS for information on saving data files 
that can be
used by other programs.

Options
Compatibility with Early Releases. You can save a data file that can be 
read by SPSS releases
prior to 7.5.
Variable Subsets and Order. You can save a subset of variables and 
reorder the variables that
are saved using the DROP and KEEP subcommands.
Variable Names. You can rename variables as they are copied into the 
SPSS-format data file
using the RENAME subcommand.
Variable Map. To confirm the names and order of the variables saved in 
the SPSS-format
data file, use the MAP subcommand. MAP displays the variables saved in 
the SPSS-format
data file next to their corresponding names in the working data file.

Data Compression. You can write the data file in compressed or 
uncompressed form using the
COMPRESSED or UNCOMPRESSED subcommand.

Basic Specification
The basic specification is the OUTFILE subcommand, which specifies a 
name for the SPSSformat
data file to be saved.

Subcommand Order
? Subcommands can be specified in any order.

Syntax Rules
? OUTFILE is required and can be specified only once. If OUTFILE is 
specified more than
once, only the last OUTFILE specified is in effect.
? KEEP, DROP, RENAME, and MAP can each be used as many times as needed.
? Only one of the subcommands COMPRESSED or UNCOMPRESSED can be 
specified per
SAVE command.

Operations
? SAVE is executed immediately and causes the data to be read.
? The new SPSS-format data file dictionary is arranged in the same 
order as the working file
dictionary, unless variables are reordered with the KEEP subcommand. 
Documentary text
from the working file dictionary is always saved unless it is dropped 
with the DROP DOCUMENTS
command before SAVE.
? New variables created by transformations and procedures previous to 
the SAVE command
are included in the new SPSS-format data file, and variables altered by 
transformations
are saved in their modified form. Results of any temporary 
transformations immediately
preceding the SAVE command are included in the file; scratch variables 
are not.
? SPSS-format data files are binary files designed to be read and 
written by SPSS only.
SPSS-format data files can be edited only with the UPDATE command. Use 
the MATCH
FILES and ADD FILES commands to merge SPSS-format data files.
? The working data file is still available for transformations and 
procedures after SAVE is
executed.
? SAVE processes the dictionary first and displays a message that 
indicates how many variables
will be saved. Once the data are written, SAVE indicates how many cases 
were saved.
If the second message does not appear, the file was probably not 
completely written.



From kjetil at entelnet.bo  Fri Dec 12 15:48:43 2003
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Fri, 12 Dec 2003 10:48:43 -0400
Subject: [R] Probelm with read.table
In-Reply-To: <002401c3c007$afdd5d10$78f05a99@msu.montana.edu>
References: <OF53786AB1.2685CE8C-ON80256DF9.00361A31-80256DF9.0039FD49@statestr.com>
Message-ID: <3FD99D0B.15955.8B723@localhost>

On 11 Dec 2003 at 9:55, Andy Bunn wrote:

But why comment.char="V"? Nobody said there need to be a 
comment.char. More natural (and faster for large files) is 
comment.char=""

Kjetil Halvorsen

> You need to change the comment character from #. This works for me: $
> mydf <- read.table(file="mytextfile.txt", header = T,
> sep="\t",na.strings="#N/A", comment.char = "V") $ mydf
>    X738307 X527178 X714456 X557955
> 1       NA   17.42    6.22    4.73
> 2       NA   17.30    6.23    4.75
> 3       NA   17.29    6.17    4.70
> 4       NA   17.07    6.12    4.60
> 5       NA   17.27    6.19    4.70
> 6       NA   17.72    6.40    4.78
> 7       NA   17.12    6.19    4.75
> 8       NA   17.07    6.15    4.65
> 9       NA   17.03    6.07    4.64
> 10      NA   17.38    6.13    4.70
> 11      NA   17.38    6.13    4.70
> 12      NA   17.38    6.13    4.70
> 13      NA   17.38    6.13    4.70
> 14      NA   17.34    6.28    4.70
> 15      10   17.57    6.33    4.75
> 16      11   17.57    6.33    4.75
> 17      12   17.57    6.33    4.75
> 18      13   17.39    6.25    4.87
> 19      14   17.15    6.33    5.06
> 20      15   17.05    6.21    5.00
> 21      16   16.87    6.14    5.15
> 22      17   16.72    6.27    5.23
> 
> HTH, Andy
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
> Arnaud_Amsellem at ssga.com
> Sent: Thursday, December 11, 2003 3:31 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Probelm with read.table
> 
> 
> Hi All,
> 
> I have the following text file (mytextfile.txt)
> 
> 738307      527178      714456      557955
> #N/A  17.42 6.22  4.73
> #N/A  17.3  6.23  4.75
> #N/A  17.29 6.17  4.7
> #N/A  17.07 6.12  4.6
> #N/A  17.27 6.19  4.7
> #N/A  17.72 6.4   4.78
> #N/A  17.12 6.19  4.75
> #N/A  17.07 6.15  4.65
> #N/A  17.03 6.07  4.64
> #N/A  17.38 6.13  4.7
> #N/A  17.38 6.13  4.7
> #N/A  17.38 6.13  4.7
> #N/A  17.38 6.13  4.7
> #N/A  17.34 6.28  4.7
> 10    17.57 6.33  4.75
> 11    17.57 6.33  4.75
> 12    17.57 6.33  4.75
> 13    17.39 6.25  4.87
> 14    17.15 6.33  5.06
> 15    17.05 6.21  5
> 16    16.87 6.14  5.15
> 17    16.72 6.27  5.23
> 
> I use the following command:
> mydf <- read.table(file="mytextfile.txt", header = T,
> sep="\t",na.strings="
> #NA")
> 
> When the above command is applied I have only 8 lines in mydf. I tried
> many options but nothing seems to get me the entire file. If the #NA
> are not in the first column it seems to work fine i.e I get  22 lines
> in mydf. Anyone would know a way of getting the entire file even if
> #NA are in the first column?
> 
> I use R 1.8.0 on Windows 2000
> 
> Any help appreciated
> 
> Arno
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From sdavis2 at mail.nih.gov  Fri Dec 12 17:16:13 2003
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Fri, 12 Dec 2003 11:16:13 -0500
Subject: [R] Rggobi installation issues
Message-ID: <BBFF53FD.27DD%sdavis2@mail.nih.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031212/2451f875/attachment.pl

From mbrodsky at nida.nih.gov  Fri Dec 12 20:21:06 2003
From: mbrodsky at nida.nih.gov (Brodsky, Marc D (NIH/NIDA))
Date: Fri, 12 Dec 2003 14:21:06 -0500
Subject: [R] jacobi.root, scre.eq, cgf.0,
	cgf.1 and cgf.2 R-plus functions nee ded
Message-ID: <AEC9740043129E4B9B4C46D3A7D9F1639108D2@nihexchange10.nih.gov>

Who can send to me code that can perform in R-plus jacobi.root, scre.eq,
cgf.0, cgf.1 and cgf.2 that are part of the S-plus package? 

Marc D. Brodsky



From swsmiley at genetics.utah.edu  Fri Dec 12 20:55:43 2003
From: swsmiley at genetics.utah.edu (Stan Smiley)
Date: Fri, 12 Dec 2003 12:55:43 -0700
Subject: [R] Session Logging
Message-ID: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAArZXp6c+HGE6fw3bI7RtXScKAAAAQAAAAUAEC1sgDDESDDquk1YdYeQEAAAAA@genetics.utah.edu>


I'm using R from a Linux command prompt, and am logging the
whole session by tee'ing the R invocation line. ie. R | tee -a Rlog.txt

Is there a better way to log all the session input and output from
within R?

Stan Smiley
stan.smiley at genetics.utah.edu



From wmkarherr at hotmail.com  Fri Dec 12 21:29:15 2003
From: wmkarherr at hotmail.com (william Milton Karvajal Herradora)
Date: Fri, 12 Dec 2003 20:29:15 +0000
Subject: [R] NLS: starting values
Message-ID: <Law10-F9R7XwiXynnni0000dd23@hotmail.com>


   Salud a los de R-proyect.

   Os envio, quizas un aporte, para el problema de valores iniciales  en
   regresion no lineal.

   En el Modelo Xenot  Y = m + exp(b0+b1*X)
   Quasi linealizando el modelo no lineal se transforma la variable
   dependiente acotando el parametro  m,  de donde
   (*)  log(Y-m) = b0+b1*X.    Aqui Y-m>0 solo si m<Y.
   Luego un valor inicial para m es m0 =min(Y)+eps, estimamos eps como el
   error standard: asi eps=sd(Y)/sqrt(n),  donde n es el tama?o muestral,
   sd(Y) la desviacion standar de Y.
   Luego de estimado m0, ajustamos el modelo lineal (*) para obtener
   aproximaciones iniciales a b0 y b1.

   Para el Modelo Logistico Y = M/(1+exp(b0+b1*X))
   despejamos  (*) log(M/Y-1) = b0+b1*X.           Aqui  M/Y-1>0 solo si
   M>Y.
   Asi un valor inicial a M es M0= max(Y) +eps, ...

   Adjunto documento con comandos de R para ejemplos de dietas, no
   comprimidos.
   Hasta otra...                 W. Milton K. H.
     _________________________________________________________________

   MSN Amor [1]Busca tu ? naranja

References

   1. http://g.msn.com/8HMAES/2740??PS=

From sdavis2 at mail.nih.gov  Fri Dec 12 21:48:11 2003
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Fri, 12 Dec 2003 15:48:11 -0500
Subject: [R] Session Logging
In-Reply-To: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAArZXp6c+HGE6fw3bI7RtXScKAAAAQAAAAUAEC1sgDDESDDquk1YdYeQEAAAAA@genetics.utah.edu>
Message-ID: <BBFF93BB.2830%sdavis2@mail.nih.gov>

Yes.  I work with ESS and xemacs--it is an awesome system.  Does not only do
session logging, but allows you to "step" through a saved session,
performing all of the commands just as before, skipping over output.  You
can also "clean" a session to get only the commands (all else stripped out).
Version control is possible if you write large code chunks.  Also, you can
edit objects in another frame while your r-process is running.  There is
also completion functionality, so you can type part of a function, press
tab, and have it completed (or options offered).  Finally, there is history
(just like using readline-enabled command-line).

http://www.analytics.washington.edu/Zope/wikis/ess/FrontPage

Sean


On 12/12/03 2:55 PM, "Stan Smiley" <swsmiley at genetics.utah.edu> wrote:

> 
> I'm using R from a Linux command prompt, and am logging the
> whole session by tee'ing the R invocation line. ie. R | tee -a Rlog.txt
> 
> Is there a better way to log all the session input and output from
> within R?
> 
> Stan Smiley
> stan.smiley at genetics.utah.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From edd at debian.org  Fri Dec 12 22:35:58 2003
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 12 Dec 2003 15:35:58 -0600
Subject: [R] Session Logging
In-Reply-To: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAArZXp6c+HGE6fw3bI7RtXScKAAAAQAAAAUAEC1sgDDESDDquk1YdYeQEAAAAA@genetics.utah.edu>
References: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAArZXp6c+HGE6fw3bI7RtXScKAAAAQAAAAUAEC1sgDDESDDquk1YdYeQEAAAAA@genetics.utah.edu>
Message-ID: <20031212213558.GA32473@sonny.eddelbuettel.com>

On Fri, Dec 12, 2003 at 12:55:43PM -0700, Stan Smiley wrote:
> I'm using R from a Linux command prompt, and am logging the
> whole session by tee'ing the R invocation line. ie. R | tee -a Rlog.txt
> 
> Is there a better way to log all the session input and output from
> within R?

?sink

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx



From swsmiley at genetics.utah.edu  Fri Dec 12 23:00:17 2003
From: swsmiley at genetics.utah.edu (Stan Smiley)
Date: Fri, 12 Dec 2003 15:00:17 -0700
Subject: [R] Session Logging
In-Reply-To: <20031212213558.GA32473@sonny.eddelbuettel.com>
Message-ID: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAArZXp6c+HGE6fw3bI7RtXScKAAAAQAAAAu7+u6Yo3ikq0z4Y+dD5s7AEAAAAA@genetics.utah.edu>


> On Fri, Dec 12, 2003 at 12:55:43PM -0700, Stan Smiley wrote:
> > I'm using R from a Linux command prompt, and am logging the whole 
> > session by tee'ing the R invocation line. ie. R | tee -a Rlog.txt
> > 
> > Is there a better way to log all the session input and output from 
> > within R?
> 
> ?sink


But wouldn't sink also hide all the console output as it was piped to the
output file. I want to be able to work interactively at the console but
still log all the in/output to a file.

Stan.



From sje at mast.queensu.ca  Fri Dec 12 23:39:28 2003
From: sje at mast.queensu.ca (Stephen Dicey)
Date: Fri, 12 Dec 2003 17:39:28 -0500 (EST)
Subject: [R] CLEDITOR
Message-ID: <Pine.GSO.4.10.10312121737440.2544-100000@poisson.mast.queensu.ca>

How can I set up the CLEDITOR (command line) variable in R if there is
one? I am on a Solaris system and command line editing does not work as it
should.
thank you

Stephen Dicey



From jwelsh at skcc.org  Fri Dec 12 23:47:07 2003
From: jwelsh at skcc.org (John Welsh)
Date: Fri, 12 Dec 2003 14:47:07 -0800
Subject: [R] Basic question on function "identical"
Message-ID: <8C585FEB9085D6119A4C0002A5EB792A0D6F89@SIDNEY>



> for(i in c(1:5))
+ {
+ print(identical(i,1))
+ }

[1] FALSE
[1] FALSE
[1] FALSE
[1] FALSE
[1] FALSE

Why don't I get:

[1] TRUE
[1] FALSE
[1] FALSE
[1] FALSE
[1] FALSE


John Welsh
Associate Professor
Sidney Kimmel Cancer Center
10835 Altman Row
San Diego, CA 92121
(858) 450-5990 ex.282
jwelsh at skcc.org



From tlumley at u.washington.edu  Fri Dec 12 23:54:51 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 12 Dec 2003 14:54:51 -0800 (PST)
Subject: [R] Basic question on function "identical"
In-Reply-To: <8C585FEB9085D6119A4C0002A5EB792A0D6F89@SIDNEY>
References: <8C585FEB9085D6119A4C0002A5EB792A0D6F89@SIDNEY>
Message-ID: <Pine.A41.4.58.0312121453470.31896@homer38.u.washington.edu>

On Fri, 12 Dec 2003, John Welsh wrote:

>
>
> > for(i in c(1:5))
> + {
> + print(identical(i,1))
> + }
>
> [1] FALSE
> [1] FALSE
> [1] FALSE
> [1] FALSE
> [1] FALSE
>
> Why don't I get:
>
> [1] TRUE
> [1] FALSE
> [1] FALSE
> [1] FALSE
> [1] FALSE
>

Because the first element of 1:5 is an integer and 1 is a real number.
all.equal() will be TRUE, and == will probably be TRUE>

	-thomas



From ggrothendieck at myway.com  Fri Dec 12 23:55:42 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 12 Dec 2003 17:55:42 -0500 (EST)
Subject: [R] Basic question on function 
Message-ID: <20031212225542.3B30B39A2@mprdmxin.myway.com>


One is an integer and the other is not.  Try

for (i in 1:5) print( identical( i, as.integer(1) ) )

---
Date: Fri, 12 Dec 2003 14:47:07 -0800 
From: John Welsh <jwelsh at skcc.org>
To: 'R-help at lists.R-project.org' <R-help at stat.math.ethz.ch> 
Subject: [R] Basic question on function "identical" 

 
 


> for(i in c(1:5))
+ {
+ print(identical(i,1))
+ }

[1] FALSE
[1] FALSE
[1] FALSE
[1] FALSE
[1] FALSE

Why don't I get:

[1] TRUE
[1] FALSE
[1] FALSE
[1] FALSE
[1] FALSE


John Welsh
Associate Professor
Sidney Kimmel Cancer Center
10835 Altman Row
San Diego, CA 92121
(858) 450-5990 ex.282
jwelsh at skcc.org





 --- On Fri 12/12, John Welsh < jwelsh at skcc.org > wrote:
From: John Welsh [mailto: jwelsh at skcc.org]
To: R-help at stat.math.ethz.ch
Date: Fri, 12 Dec 2003 14:47:07 -0800
Subject: [R] Basic question on function "identical"

<br><br>> for(i in c(1:5))<br>+ {<br>+ print(identical(i,1))<br>+ }<br><br>[1] FALSE<br>[1] FALSE<br>[1] FALSE<br>[1] FALSE<br>[1] FALSE<br><br>Why don't I get:<br><br>[1] TRUE<br>[1] FALSE<br>[1] FALSE<br>[1] FALSE<br>[1] FALSE<br><br><br>John Welsh<br>Associate Professor<br>Sidney Kimmel Cancer Center<br>10835 Altman Row<br>San Diego, CA 92121<br>(858) 450-5990 ex.282<br>jwelsh at skcc.org<br><br>______________________________________________<br>R-help at stat.math.ethz.ch mailing list<br>https://www.stat.math.ethz.ch/mailman/listinfo/r-help<br>



From ssong at nicco.sscnet.ucla.edu  Sat Dec 13 00:09:40 2003
From: ssong at nicco.sscnet.ucla.edu (Shige Song)
Date: Fri, 12 Dec 2003 15:09:40 -0800 (PST)
Subject: [R] Question about R-editor rwined()
Message-ID: <Pine.GSO.4.58.0312121505320.5047@nicco.sscnet.ucla.edu>

Deal All,

I am interested in using the rwined editor
(http://www.wiwi.uni-bielefeld.de/~wolf/software/revweb/revweb.html). My
question is: is it German only or there is an English version? I have a
slow connection at home and would like to know whether it will work before
trying to download it. Thanks!

Best,
Shige Song
Department of Sociology, UCLA



From jasont at indigoindustrial.co.nz  Sat Dec 13 00:45:40 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Sat, 13 Dec 2003 12:45:40 +1300
Subject: [R] Session Logging
In-Reply-To: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAArZXp6c+HGE6fw3bI7RtXScKAAAAQAAAAUAEC1sgDDESDDquk1YdYeQEAAAAA@genetics.utah.edu>
References: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAArZXp6c+HGE6fw3bI7RtXScKAAAAQAAAAUAEC1sgDDESDDquk1YdYeQEAAAAA@genetics.utah.edu>
Message-ID: <3FDA5324.60400@indigoindustrial.co.nz>

Stan Smiley wrote:
> Is there a better way to log all the session input and output from
> within R?

What everyone else said, plus ?savehistory

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From tblackw at umich.edu  Sat Dec 13 01:04:10 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Fri, 12 Dec 2003 19:04:10 -0500 (EST)
Subject: [R] CLEDITOR
In-Reply-To: <Pine.GSO.4.10.10312121737440.2544-100000@poisson.mast.queensu.ca>
References: <Pine.GSO.4.10.10312121737440.2544-100000@poisson.mast.queensu.ca>
Message-ID: <Pine.SOL.4.58.0312121902170.464@rygar.gpcc.itd.umich.edu>

Stephen  -

If command line editing does not work as it should, I would look
first into providing a patched readline.  See Graeme Ambler's
patched version in

On Fri, 12 Dec 2003, Stephen Dicey wrote:

> How can I set up the CLEDITOR (command line) variable in R if there is
> one? I am on a Solaris system and command line editing does not work as it
> should.
> thank you
>
> Stephen Dicey
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From tblackw at umich.edu  Sat Dec 13 01:06:04 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Fri, 12 Dec 2003 19:06:04 -0500 (EST)
Subject: [R] CLEDITOR
In-Reply-To: <Pine.GSO.4.10.10312121737440.2544-100000@poisson.mast.queensu.ca>
References: <Pine.GSO.4.10.10312121737440.2544-100000@poisson.mast.queensu.ca>
Message-ID: <Pine.SOL.4.58.0312121904510.464@rygar.gpcc.itd.umich.edu>

(oops, wrong keystroke)

... in the unix source directories (at least for Redhat linux)
on CRAN.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Fri, 12 Dec 2003, Stephen Dicey wrote:

> How can I set up the CLEDITOR (command line) variable in R if there is
> one? I am on a Solaris system and command line editing does not work as it
> should.
> thank you
>
> Stephen Dicey
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From maj at stats.waikato.ac.nz  Sat Dec 13 03:34:03 2003
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Sat, 13 Dec 2003 15:34:03 +1300
Subject: [R] mode
In-Reply-To: <3FD9387E.9080004@stats.waikato.ac.nz>
References: <6rvfomyg5z.fsf@bates4.stat.wisc.edu>
	<000001c3c03b$abe0dd70$b43d68c8@CPQ28661778111>
	<6rvfomyg5z.fsf@bates4.stat.wisc.edu>
Message-ID: <E1AUzdH-0006Bv-00@newton.math.waikato.ac.nz>

Apologies for pursuing this increasingly off-topic thread, but I've just
remembered that 'my' mode is not so hard to compute. Suppose a
one-dimensional data set is in general position (all gaps unequal), find
the smallest gap, then choose whichever endpoint has the closest neighbour.
That's the mode.

At 16:39 12/12/2003 +1300, Murray Jorgensen wrote:
>Opps! This is what I should have written:
>
>The mode of a data vector x might be defined as the limit of m_p as p 
>tends to zero from above and where m_p is the m minimizing
>sum(abs(x - m)^p).  I would not expect the mode so defined to be of much 
>use in data analysis, nor would it be easy to compute.
>
>Murray
>
>[Thanks to Duncan Murdoch for noticing the missing ^p .]
>
>Douglas Bates wrote:
>
>> "Christian Mora" <christian_mora at vtr.net> writes:
>> 
>> 
>>>How can I get the mode (most frequent value) from a dataset (continuos
>>>variables)? I can obtain it when the data is discrete (by making a table
>>>and looking at the higher frequency) but I don't know how obtain it
>>>from, for example, a density plot of the data. Does anyone know how to
>>>do it? Thanks
>> 
>> 
>> I don't think the mode of a sample from a continuous random variable is
well
>> defined.
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> 
>> 
>
>-- 
>Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
>Department of Statistics, University of Waikato, Hamilton, New Zealand
>Email: maj at waikato.ac.nz                                Fax 7 838 4155
>Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862



From ripley at stats.ox.ac.uk  Sat Dec 13 07:39:40 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 13 Dec 2003 06:39:40 +0000 (GMT)
Subject: [R] CLEDITOR
In-Reply-To: <Pine.SOL.4.58.0312121902170.464@rygar.gpcc.itd.umich.edu>
Message-ID: <Pine.LNX.4.44.0312130635510.17011-100000@gannet.stats>

No patch is needed on Solaris, where one compiles readline from the 
sources.

There is no `CLEDITOR' env variable (as in S-PLUS): use of this is 
controlled by the --no-readline flag which defaults to on.  Look back at 
the installation output: it will have told you if readline was detected.
(Most likely it is not even installed.)  See the R-admin manual for more 
details.

On Fri, 12 Dec 2003, Thomas W Blackwell wrote:

> Stephen  -
> 
> If command line editing does not work as it should, I would look
> first into providing a patched readline.  See Graeme Ambler's
> patched version in
> 
> On Fri, 12 Dec 2003, Stephen Dicey wrote:
> 
> > How can I set up the CLEDITOR (command line) variable in R if there is
> > one? I am on a Solaris system and command line editing does not work as it
> > should.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From christoph.lehmann at gmx.ch  Sat Dec 13 10:11:19 2003
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: 13 Dec 2003 10:11:19 +0100
Subject: [R] partial proportional odds model (PPO)
Message-ID: <1071306679.4537.11.camel@christophl>

Hi

Since the 'equal slope' assumption doesn't hold in my data I cannot use
a proportional odds model ('Design' library, together with 'Hmisc'). I
would like to try therefore a partial proportional odds model

Please, could anybody tell me, where to find the code and how to specify
such a model

..or any potential alternatives

many thanks for your kind help

christoph
-- 
Christoph Lehmann <christoph.lehmann at gmx.ch>



From maechler at stat.math.ethz.ch  Sat Dec 13 12:55:27 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 13 Dec 2003 12:55:27 +0100
Subject: [R] jacobi.root, scre.eq, cgf.0,
	cgf.1 and cgf.2 R-plus functions nee ded
In-Reply-To: <AEC9740043129E4B9B4C46D3A7D9F1639108D2@nihexchange10.nih.gov>
References: <AEC9740043129E4B9B4C46D3A7D9F1639108D2@nihexchange10.nih.gov>
Message-ID: <16346.65071.17088.120541@gargle.gargle.HOWL>

>>>>> "MBro" == Brodsky, Marc D (NIH/NIDA) <mbrodsky at nida.nih.gov>
>>>>>     on Fri, 12 Dec 2003 14:21:06 -0500 writes:

    MBro> Who can send to me code that can perform in R-plus

R-plus ?  This mailing list is about R ...

    MBro> jacobi.root, scre.eq, cgf.0, cgf.1 and cgf.2 that are
    MBro> part of the S-plus package?

I didn't know there was an S-plus package for R, it's not on
CRAN yet...

More seriously, our version of S-plus (6.1) does not contain these functions
but even then, it would most probably be illegal to send them ...

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From aliljones28 at hotmail.com  Sat Dec 13 13:00:38 2003
From: aliljones28 at hotmail.com (Allison Jones)
Date: Sat, 13 Dec 2003 04:00:38 -0800
Subject: [R] half normal probability plot in R
Message-ID: <Law15-F24BN23ZG4KdJ000348ff@hotmail.com>

I have generated the effects in a factorial design and now want to put them 
in a half normal probability plot. Is there an easy way to do this in R??? I 
can't find the command. Thanks much -

Ali Jones

_________________________________________________________________
Our best dial-up offer is back.  Get MSN Dial-up Internet Service for 6 
months @ $9.95/month now! http://join.msn.com/?page=dept/dialup



From maechler at stat.math.ethz.ch  Sat Dec 13 13:01:19 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 13 Dec 2003 13:01:19 +0100
Subject: [R] Basic question on function "identical"
In-Reply-To: <Pine.A41.4.58.0312121453470.31896@homer38.u.washington.edu>
References: <8C585FEB9085D6119A4C0002A5EB792A0D6F89@SIDNEY>
	<Pine.A41.4.58.0312121453470.31896@homer38.u.washington.edu>
Message-ID: <16346.65423.941650.746198@gargle.gargle.HOWL>

>>>>> "TL" == Thomas Lumley <tlumley at u.washington.edu>
>>>>>     on Fri, 12 Dec 2003 14:54:51 -0800 (PST) writes:

    TL> On Fri, 12 Dec 2003, John Welsh wrote:
    >> 
    >> 
    >> > for(i in c(1:5)) + { + print(identical(i,1)) + }
    >> 
    >> [1] FALSE [1] FALSE [1] FALSE [1] FALSE [1] FALSE
    >> 
    >> Why don't I get:
    >> 
    >> [1] TRUE [1] FALSE [1] FALSE [1] FALSE [1] FALSE
    >> 

    TL> Because the first element of 1:5 is an integer and 1 is
    TL> a real number.  all.equal() will be TRUE, and == will
    TL> probably be TRUE
definitely will (integers do have exact floating point representations)

In general, use

  ==	     for testing equality of integer numbers (of type "integer" or not)
  all.equal  for testing (near)equality non-integer numbers, and
	     many other more structured objects.

  identical  only if you understand more about the S language ;-)

Martin Maechler



From Ted.Harding at nessie.mcc.ac.uk  Sat Dec 13 13:07:20 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 13 Dec 2003 12:07:20 -0000 (GMT)
Subject: [R] mode
In-Reply-To: <E1AUzdH-0006Bv-00@newton.math.waikato.ac.nz>
Message-ID: <XFMail.031213120720.Ted.Harding@nessie.mcc.ac.uk>

Douglas Bates wrote:
> "Christian Mora" <christian_mora at vtr.net> writes:
>>How can I get the mode (most frequent value) from a dataset
>>(continuos variables)? I can obtain it when the data is discrete
>>(by making a table and looking at the higher frequency) but I
>>don't know how obtain it from, for example, a density plot of the
>>data. Does anyone know how to do it? Thanks
> 
> 
> I don't think the mode of a sample from a continuous random variable
> is well  defined.

Indeed it is not (except for categorical variables and even then may
not be unique). All other suggestions for finding a mode from a sample
from a continuous distribution encounter the issues that

a) all sample values occur once, each -> every sample value is a mode!

b) the mode of a distribution is the location of the maximum of the
   probability density function, and this is not defined until you
   have stated what the underlying measure is, with respect to
   which you derive the density.

   This issue has its counterpart in that "mode" is not invariant
   under functional transformation, even if you have stated the
   base measure: in other words, the mode of X^2 is not the same
   as (the mode of X)^2.

The implication is that any suggestion for finding a mode from a
sample amounts to computing a density function from the sample
(and a histogram is a kind of discretised density estimate), so
several approaches are open:

1) Choose a class of distributions (e.g. normal), estimate the
   parameters, and find the maximum of the estimated density function
2) Adopt a "local" suggestion such as Murray Jorgensen's "nearest
   neighbour" idea.
3) go down the road of more general distribution-free density
   estimation, for which one approach is a "kernel density
   estimation" and another could be a "spline" density.

Example of kernel density estimation:

  X<-c(rnorm(200),2+0.5*rnorm(300))
  hist(X,freq=FALSE,breaks=(-4)+0.2*(0:50))
  S<-density(X,from=(-4),to=5,bw=0.2)
  N<-length(S$y)
  V1<-S$y[1:(N-2)];V2<-S$y[2:(N-1)];V3<-S$y[3:N]
  ix<-1+which((V1<V2)&(V2>V3))
  lines(S$x,S$y,col="red")
  points(S$x[ix],S$y[ix],col="blue")

where the index ix identifies all the local modes of the fitted
spline density estimate S. These include the global mode[s]

  S$x[which(S$y==maxS$y)]

This seems in fact to come back to Christian Mora's original
question, and I hope it helps to answer it.

(And I hope it helps answer Murray's concern that the thread was
gettin off-topic, since we're now back to R ... !)

Best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 13-Dec-03                                       Time: 12:07:20
------------------------------ XFMail ------------------------------



From apiszcz at solarrain.com  Sat Dec 13 13:20:35 2003
From: apiszcz at solarrain.com (Al Piszcz)
Date: Sat, 13 Dec 2003 07:20:35 -0500 (EST)
Subject: [R] Session logging
Message-ID: <Pine.LNX.4.55.0312130719410.7775@l1>

another option in LINUX/UNIX is 'script'

$ script
  R etc.
$ CTRL-D


     script - make typescript of terminal session

SYNOPSIS
     script [-a] [-f] [-q] [-t] [file]

DESCRIPTION
     Script makes a typescript of everything printed on your terminal.  It
is
     useful for students who need a hardcopy record of an interactive
session
     as proof of an assignment, as the typescript file can be printed out
     later with lpr(1).



From Ted.Harding at nessie.mcc.ac.uk  Sat Dec 13 14:31:42 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 13 Dec 2003 13:31:42 -0000 (GMT)
Subject: [R] Basic question on function "identical"
In-Reply-To: <16346.65423.941650.746198@gargle.gargle.HOWL>
Message-ID: <XFMail.031213133142.Ted.Harding@nessie.mcc.ac.uk>

On 13-Dec-03 Martin Maechler wrote:
> In general, use
> 
>   ==       for testing equality of integer numbers (of type "integer"
>            or not)

I hope this is not a suggestion to avoid usage like

  which(x == max(x))

when x is a vector of reals? (i.e. should be OK when you know that
the thing on one side of == should be an exact copy in its internal
representation of the thing on the other side, if equality in the
usual sense holds).

In other words, I hope that (for instance) max(x) does not differ
internally from whichever of x[1] , ... , x[N] has the largest value ...

>   all.equal  for testing (near)equality non-integer numbers, and
>            many other more structured objects.
> 
>   identical  only if you understand more about the S language ;-)

Cheers,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 13-Dec-03                                       Time: 13:31:42
------------------------------ XFMail ------------------------------



From Ted.Harding at nessie.mcc.ac.uk  Sat Dec 13 14:19:43 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 13 Dec 2003 13:19:43 -0000 (GMT)
Subject: [R] mode
In-Reply-To: <XFMail.031213120720.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <XFMail.031213131943.Ted.Harding@nessie.mcc.ac.uk>

Sorry, typo:

On 13-Dec-03 Ted Harding wrote:
> Example of kernel density estimation:
> 
>   X<-c(rnorm(200),2+0.5*rnorm(300))
>   hist(X,freq=FALSE,breaks=(-4)+0.2*(0:50))
>   S<-density(X,from=(-4),to=5,bw=0.2)
>   N<-length(S$y)
>   V1<-S$y[1:(N-2)];V2<-S$y[2:(N-1)];V3<-S$y[3:N]
>   ix<-1+which((V1<V2)&(V2>V3))
>   lines(S$x,S$y,col="red")
>   points(S$x[ix],S$y[ix],col="blue")
> 
> where the index ix identifies all the local modes of the fitted
> spline density estimate S. These include the global mode[s]
> 
>   S$x[which(S$y==maxS$y)]

  S$x[which(S$y==max(S$y))]

Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 13-Dec-03                                       Time: 13:19:43
------------------------------ XFMail ------------------------------



From bates at stat.wisc.edu  Sat Dec 13 14:55:51 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: 13 Dec 2003 07:55:51 -0600
Subject: [R] half normal probability plot in R
In-Reply-To: <Law15-F24BN23ZG4KdJ000348ff@hotmail.com>
References: <Law15-F24BN23ZG4KdJ000348ff@hotmail.com>
Message-ID: <6r65gk7rfc.fsf@bates4.stat.wisc.edu>

"Allison Jones" <aliljones28 at hotmail.com> writes:

> I have generated the effects in a factorial design and now want to put
> them in a half normal probability plot. Is there an easy way to do
> this in R??? I can't find the command. Thanks much -

Long ago I wrote a half-normal plot for Minitab but I have forgotten
exactly what I did.  It seems that one of the easiest approaches in R
would be to do a plot of the absolute value of the effects versus the
square root of the quantiles of a chi-square on one degree of freedom.



From ggrothendieck at myway.com  Sat Dec 13 15:07:36 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 13 Dec 2003 09:07:36 -0500 (EST)
Subject: [R] Basic question on function 
Message-ID: <20031213140736.E3A51397B@mprdmxin.myway.com>



In that particular case there is also 

which.max(x)

--- 
Date: Sat, 13 Dec 2003 13:31:42 -0000 (GMT) 
From: <Ted.Harding at nessie.mcc.ac.uk>
To: Martin Maechler <maechler at stat.math.ethz.ch> 
Cc: <R-help at stat.math.ethz.ch> 
Subject: Re: [R] Basic question on function "identical" 

 
 
On 13-Dec-03 Martin Maechler wrote:
> In general, use
> 
> == for testing equality of integer numbers (of type "integer"
> or not)

I hope this is not a suggestion to avoid usage like

which(x == max(x))

when x is a vector of reals? (i.e. should be OK when you know that
the thing on one side of == should be an exact copy in its internal
representation of the thing on the other side, if equality in the
usual sense holds).

In other words, I hope that (for instance) max(x) does not differ
internally from whichever of x[1] , ... , x[N] has the largest value ...

> all.equal for testing (near)equality non-integer numbers, and
> many other more structured objects.
> 
> identical only if you understand more about the S language ;-)

Cheers,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 13-Dec-03 Time: 13:31:42
------------------------------ XFMail ------------------------------



From fredrik.lundgren at norrkoping.mail.telia.com  Sat Dec 13 15:27:07 2003
From: fredrik.lundgren at norrkoping.mail.telia.com (Fredrik Lundgren)
Date: Sat, 13 Dec 2003 15:27:07 +0100
Subject: [R]??devga??
Message-ID: <002d01c3c185$30f9e340$2d0ffea9@oemcomputer>

To R-help,

I recently installed R Version 1.8.1  (2003-11-21), ISBN 3-900051-00-3 and use Win 98SE
When I plot I get the following result
###########
> plot(1:10, 1:10)
Error in windows() : 10 arguments passed to "devga" which requires 13.
###########

What should I do to "devga"?

Sincerely Fredrik Lundgren



From ripley at stats.ox.ac.uk  Sat Dec 13 15:48:29 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 13 Dec 2003 14:48:29 +0000 (GMT)
Subject: [R]??devga??
In-Reply-To: <002d01c3c185$30f9e340$2d0ffea9@oemcomputer>
Message-ID: <Pine.LNX.4.44.0312131446290.330-100000@gannet.stats>

You very likely have a saved version of windows() in your saved workspace
from an earlier version of R.

ALWAYS start with --vanilla before posting to R-help.

On Sat, 13 Dec 2003, Fredrik Lundgren wrote:

> To R-help,
> 
> I recently installed R Version 1.8.1  (2003-11-21), ISBN 3-900051-00-3 and use Win 98SE
> When I plot I get the following result
> ###########
> > plot(1:10, 1:10)
> Error in windows() : 10 arguments passed to "devga" which requires 13.
> ###########
> 
> What should I do to "devga"?

Not have a private copy.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From spencer.graves at pdf.com  Sat Dec 13 16:02:12 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 13 Dec 2003 07:02:12 -0800
Subject: [R] jacobi.root, scre.eq, cgf.0, cgf.1 and cgf.2 R-plus functions
	nee ded
In-Reply-To: <16346.65071.17088.120541@gargle.gargle.HOWL>
References: <AEC9740043129E4B9B4C46D3A7D9F1639108D2@nihexchange10.nih.gov>
	<16346.65071.17088.120541@gargle.gargle.HOWL>
Message-ID: <3FDB29F4.9060102@pdf.com>

Dear Marc Brodsky: 

      Do you have those functions in an S-Plus installation to which you 
have access?  If yes, how did that installation come to have those 
functions?  They may not be part of Insightful's intellectual property.  
If so, can you find out how they became part of that installation.  If 
they are not part of that installation, how did you hear about them? 

      Alternatively, rather that asking this list about a specific 
solution methodology, perhaps you could describe the problem you are 
trying to solve.  If you want a root of a function, you might consider 
"uniroot" or "polyroot" or "optim". 

      hope this helps. 
      spencer graves

Martin Maechler wrote:

>>>>>>"MBro" == Brodsky, Marc D (NIH/NIDA) <mbrodsky at nida.nih.gov>
>>>>>>    on Fri, 12 Dec 2003 14:21:06 -0500 writes:
>>>>>>            
>>>>>>
>
>    MBro> Who can send to me code that can perform in R-plus
>
>R-plus ?  This mailing list is about R ...
>
>    MBro> jacobi.root, scre.eq, cgf.0, cgf.1 and cgf.2 that are
>    MBro> part of the S-plus package?
>
>I didn't know there was an S-plus package for R, it's not on
>CRAN yet...
>
>More seriously, our version of S-plus (6.1) does not contain these functions
>but even then, it would most probably be illegal to send them ...
>
>Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
>Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
>ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
>phone: x-41-1-632-3408		fax: ...-1228			<><
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From martin at ist.org  Sat Dec 13 18:00:04 2003
From: martin at ist.org (martin@ist.org)
Date: Sat, 13 Dec 2003 17:00:04 -0000
Subject: [R] adding labels to mosaicplots
Message-ID: <twig.1071334804.82956@ist.org>

Hi,

is there a way to add labels to mosaicplots, similar to barplot where 
you can use

bp <- barplot(data)
mtext(at=colMeans(bp),...)

to display text at the midpoints of bars/groups of bars.

Im thinking of a function or attribute that gives the midpoints (or 
corners) of the rectangles drawn by mosaicplot.

Thank you, 

Martin Keller-Ressel



From jc at or.psychology.dal.ca  Sat Dec 13 19:28:11 2003
From: jc at or.psychology.dal.ca (John Christie)
Date: Sat, 13 Dec 2003 14:28:11 -0400
Subject: [R] sphericity calculation (more formulas?)
Message-ID: <1B50688D-2D9A-11D8-AD78-000A956DE534@or.psychology.dal.ca>

I believe that the following function is a good function for eyeballing 
sphericity.  But, I can't find functions for tests anywhere (g-g or 
h-f).  Anyone who wants to can feel free to use it.

It is bad that R does not provide any tests for sphericity AND cannot 
do a MANOVA on repeated measures (although lme is an alternative add on 
package).  One of these solutions must be provided in the base 
configuration.  My understanding is that the R community is opposed to 
F corrections and I can see the argument.  But, not providing any way 
of looking at sphericity at all in order to see if you need to go 
through the extra steps of adding a separate package and learning a 
somewhat more complicated command set is just wrong.

Also, Loftus & Masson have long been arguing that one use the MSE error 
term from repeated measures in order to generate confidence intervals 
for the means.  But, without a test of sphericity one cannot check to 
see if this is OK to do.  It is not entirely unreasonable to want to 
provide some variance for graphical presentation even if your analyses 
use the "correct" lme.  You can explain in the text.  But, the lme 
cannot provide a variance measure since it just uses likelihood ratios.

Oh, and if this little sphericity function is wrong please correct.

# this returns an array with the variances between each condition.  This
# can be calculated using a covariance matrix and that might technically
# be faster.  But this method is more transparent with respect to 
defining
# sphericity.
jcsphericity <- function(x){
	# x is a matrix that consists of columns containing each condition and
	# rows for each s. e.g. matrix(s$x,ncol=3) where s$x is a long 
formatted
	# dv with 3 cond and with cond moving faster than s if all the values 
returned
	# are roughly equal sphericity is OK (never happens :))
	n<- length(x[1,])
	t<-array()
	l<-array()
	k<-0
	for (i in 1:(n-1)){
		for (j in (i+1):n){
			k<-k+1
			t[k]<- var(x[,i] - x[,j])
			l[k]<- paste(i,j, sep=",")
		 }
	}
	names(t)<-l
	t
}



From allan_kachelmeier at hotmail.com  Sat Dec 13 19:32:56 2003
From: allan_kachelmeier at hotmail.com (Allan Kachelmeier)
Date: Sat, 13 Dec 2003 20:32:56 +0200
Subject: [R] (no subject)
Message-ID: <BAY1-F44ncI9YWTgiXh0002c79d@hotmail.com>






From feh3k at spamcop.net  Sat Dec 13 14:24:54 2003
From: feh3k at spamcop.net (Frank E Harrell Jr)
Date: Sat, 13 Dec 2003 08:24:54 -0500
Subject: [R] partial proportional odds model (PPO)
In-Reply-To: <1071306679.4537.11.camel@christophl>
References: <1071306679.4537.11.camel@christophl>
Message-ID: <20031213082454.31e652d1.feh3k@spamcop.net>

On 13 Dec 2003 10:11:19 +0100
Christoph Lehmann <christoph.lehmann at gmx.ch> wrote:

> Hi
> 
> Since the 'equal slope' assumption doesn't hold in my data I cannot use
> a proportional odds model ('Design' library, together with 'Hmisc'). I
> would like to try therefore a partial proportional odds model
> 
> Please, could anybody tell me, where to find the code and how to specify
> such a model
> 
> ..or any potential alternatives
> 
> many thanks for your kind help
> 
> christoph
> -- 
> Christoph Lehmann <christoph.lehmann at gmx.ch>

I do not know of an implementation of the partial PO model in S.  You can
easily fit continuation ratio ordinal response models in S using lrm and
glm, with easy-to-construct non-equal slope components, after suitable
expansion of the dataset (see Regression Modeling Strategies, Springer
2001 for a detailed case study using lrm, with code).

Frank Harrell

---
Frank E Harrell Jr   Professor and Chair           School of Medicine
                     Department of Biostatistics   Vanderbilt University



From faheem at email.unc.edu  Sun Dec 14 01:44:46 2003
From: faheem at email.unc.edu (Faheem Mitha)
Date: Sat, 13 Dec 2003 19:44:46 -0500 (EST)
Subject: [R] compile error with C code and standalone R math C library
Message-ID: <Pine.LNX.4.58.0312131933490.21220@Chrestomanci>


Dear People,

I just went back to an old piece of C code. On trying to compile it with
the R math standalone C library I got the following error. Can anyone
enlighten me what I am doing wrong, if anything? C file (rr-sa.c) follows.

I'm on Debian sarge. I'm running R version 1.8.1. Gcc is version
3.3.1.

Thanks in advance.

                                                        Faheem.

**********************************************************************
faheem ~/co/rr/trunk>gcc -o rr rr-sa.c -lRmath -lm
/usr/lib/gcc-lib/i486-linux/3.3.2/../../../libRmath.so: undefined
reference to `Rlog1p'
collect2: ld returned 1 exit status
**********************************************************************
rr-sa.c
**********************************************************************
#include <stdio.h>
#include <assert.h>
#define T 3
#define INITVAL 1
#define MATHLIB_STANDALONE
#define THETA 2
#include <Rmath.h>
#include <time.h>

void advance(double *node, int *current_pos, double newval);
void retreat(double *node, int *current_pos);
double new_val(double *node, double  theta, int current_pos);
double accept_prob(double *node, double theta, int current_pos);
double accept_prob_fn(double a, double y);

int main()
{
  int currentpos = 0;
  int i;
  double newval, node[T+2], theta = THETA, p, u;
  set_seed(time(NULL), clock());
  node[0]=0;
  for(i=1;i<=T;i++)
    node[i]= INITVAL;    /* for simplicity choose all values the same */
  node[T+1]=0;

  for(i=0; currentpos < T; i++)
    {
      u = unif_rand();
      newval = new_val(node, theta, currentpos);
      p = accept_prob(node, theta, currentpos);
      printf("current position is %u\n",currentpos);
      printf("uniform random variable determining acceptance/rejection is %g\n",u);
      printf("value of acceptance probability is %g\n",p);
      printf("value of proposed new variable is %g\n",newval);
      if(u < p)
	advance(node,&currentpos,newval);
      else
	retreat(node, &currentpos);
    }

  for(i=1;i <= T; i++)
    printf("value of node %u is %g\n",i,node[i]);
  return 0;
}

/* function that moves chain one step forward in the event of an
   acceptance */
void advance(double *node, int *current_pos, double newval)
{
  *current_pos = *current_pos + 1;
  node[*current_pos] = newval;
}

/* function that moves the chain backwards in the event of a rejection*/
void retreat(double *node, int *current_pos)
{
  /* need special handling for small values of current_pos */
  if(*current_pos >= 1)
    *current_pos = *current_pos - 1;
  else if(*current_pos == 0)
    ;         /* do nothing, already at beginning */
}

/* generate new candidate value with appropriate distribution */
double new_val(double *node, double theta, int current_pos)
{
  double a, u, y;
  u = unif_rand();
  a = node[current_pos] + node[current_pos+2];
  if (a != 0)
    y = (1/a)*log( ( exp(a*theta) - exp(-a*theta) )*u + exp(-a*theta) );
  else  /* degenerate case (for a=0) is Unif[-theta,theta] */
    y = 2*theta*u - theta;
  return y;
}

/* generate acceptance probabilities for candidates*/
/* NB: This assumes:
(a) That all the initial values of the starting state are set to -1
(b) That theta > 1.
*/
double accept_prob(double *node, double theta, int current_pos)
{
  double a, p, y, num, denom, leftval, rtval;
  a = node[current_pos + 1];
  y = node[current_pos] + node[current_pos+2];
  num = accept_prob_fn(a, y);
 leftval = accept_prob_fn(a, -theta + node[current_pos+2]);
  rtval = accept_prob_fn(a, theta + node[current_pos+2]);
  denom = fmax2(leftval, rtval); /* function in Rmath which returns
				    max of two doubles */
  p = num/denom;
  assert(p>=0 && p <=1);
  return p;
}

/* auxilary function for accept_prob */
double accept_prob_fn(double a, double y)
{
  double f;
  if(y != 0)
  f = (exp(y) - exp(-y))/(y*exp(a*y));
  else
    f = 2;
  return f;
}



From obig at bcgsc.ca  Sun Dec 14 02:29:18 2003
From: obig at bcgsc.ca (Obi Griffith)
Date: Sat, 13 Dec 2003 17:29:18 -0800
Subject: [R] density plot for very large dataset
Message-ID: <3FDBBCEE.9030909@bcgsc.ca>

I'm new to R and am trying to perform a simple, yet problematic task.  I 
have two variables for which I would like to measure the correlation and 
plot versus each other.  However, I have ~30 million data points 
measurements of each variable.  I can read this into R from file and 
produce a plot with plot(x0, x1) but as you would expect, its not pretty 
to look at and produces a postscript file of about 700MB.  A google 
search found a few mentions of doing density plots but they seemed to 
assume you already have the density matrix.  Can anyone point me in the 
right direction, keeping in mind that I am a complete R newbie.

Obi



From edd at debian.org  Sun Dec 14 04:25:02 2003
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sat, 13 Dec 2003 21:25:02 -0600
Subject: [R] compile error with C code and standalone R math C library
In-Reply-To: <Pine.LNX.4.58.0312131933490.21220@Chrestomanci>
References: <Pine.LNX.4.58.0312131933490.21220@Chrestomanci>
Message-ID: <20031214032502.GA10188@sonny.eddelbuettel.com>

On Sat, Dec 13, 2003 at 07:44:46PM -0500, Faheem Mitha wrote:
> I just went back to an old piece of C code. On trying to compile it with
> the R math standalone C library I got the following error. Can anyone
> enlighten me what I am doing wrong, if anything? C file (rr-sa.c) follows.
> 
> I'm on Debian sarge. I'm running R version 1.8.1. Gcc is version
> 3.3.1.
[...]
> faheem ~/co/rr/trunk>gcc -o rr rr-sa.c -lRmath -lm
> /usr/lib/gcc-lib/i486-linux/3.3.2/../../../libRmath.so: undefined
> reference to `Rlog1p'
> collect2: ld returned 1 exit status

The linker tells you that it cannot find object code for a function Rlog1p.
So let's check:

edd at chibud:~> grep Rlog1p /usr/include/Rmath.h
edd at chibud:~> grep log1p /usr/include/Rmath.h
double  log1p(double); /* = log(1+x) {care for small x} */
edd at chibud:~>

Turns out that there is none defined in Rmath.h, but log1p exists.  This may
have gotten renamed since you first wrote your code.

Hth, Dirk

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx



From marce_lr at yahoo.com  Sun Dec 14 04:33:45 2003
From: marce_lr at yahoo.com (Marcela XX)
Date: Sat, 13 Dec 2003 19:33:45 -0800 (PST)
Subject: [R] I have a question
Message-ID: <20031214033345.75489.qmail@web41602.mail.yahoo.com>

Hi! I tried lm.ridge() but I don't know yet how to get
coefficients and p-values of the model.

Can you tell me how to get a similar output like the
summary(lm(...)) output? 

Thanks!!!
Marcela



From faheem at email.unc.edu  Sun Dec 14 06:51:09 2003
From: faheem at email.unc.edu (Faheem Mitha)
Date: Sun, 14 Dec 2003 00:51:09 -0500 (EST)
Subject: [R] compile error with C code and standalone R math C library
In-Reply-To: <20031214032502.GA10188@sonny.eddelbuettel.com>
References: <Pine.LNX.4.58.0312131933490.21220@Chrestomanci>
	<20031214032502.GA10188@sonny.eddelbuettel.com>
Message-ID: <Pine.LNX.4.58.0312140029160.21220@Chrestomanci>



On Sat, 13 Dec 2003, Dirk Eddelbuettel wrote:

> On Sat, Dec 13, 2003 at 07:44:46PM -0500, Faheem Mitha wrote:
> > I just went back to an old piece of C code. On trying to compile it with
> > the R math standalone C library I got the following error. Can anyone
> > enlighten me what I am doing wrong, if anything? C file (rr-sa.c) follows.
> >
> > I'm on Debian sarge. I'm running R version 1.8.1. Gcc is version
> > 3.3.1.
> [...]
> > faheem ~/co/rr/trunk>gcc -o rr rr-sa.c -lRmath -lm
> > /usr/lib/gcc-lib/i486-linux/3.3.2/../../../libRmath.so: undefined
> > reference to `Rlog1p'
> > collect2: ld returned 1 exit status
>
> The linker tells you that it cannot find object code for a function Rlog1p.
> So let's check:
>
> edd at chibud:~> grep Rlog1p /usr/include/Rmath.h
> edd at chibud:~> grep log1p /usr/include/Rmath.h
> double  log1p(double); /* = log(1+x) {care for small x} */
> edd at chibud:~>
>
> Turns out that there is none defined in Rmath.h, but log1p exists.  This may
> have gotten renamed since you first wrote your code.

Maybe I am being dense, but how is this my fault? I am not using either
Rlog1p or log1p in my code (as far as I can see).

Thanks for replying.

                                                             Faheem.



From edd at debian.org  Sun Dec 14 07:15:02 2003
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 14 Dec 2003 00:15:02 -0600
Subject: [R] compile error with C code and standalone R math C library
In-Reply-To: <Pine.LNX.4.58.0312140029160.21220@Chrestomanci>
References: <Pine.LNX.4.58.0312131933490.21220@Chrestomanci>
	<20031214032502.GA10188@sonny.eddelbuettel.com>
	<Pine.LNX.4.58.0312140029160.21220@Chrestomanci>
Message-ID: <20031214061502.GA11158@sonny.eddelbuettel.com>

On Sun, Dec 14, 2003 at 12:51:09AM -0500, Faheem Mitha wrote:
> On Sat, 13 Dec 2003, Dirk Eddelbuettel wrote:
> 
> > On Sat, Dec 13, 2003 at 07:44:46PM -0500, Faheem Mitha wrote:
> > > I just went back to an old piece of C code. On trying to compile it with
> > > the R math standalone C library I got the following error. Can anyone
> > > enlighten me what I am doing wrong, if anything? C file (rr-sa.c) follows.
> > >
> > > I'm on Debian sarge. I'm running R version 1.8.1. Gcc is version
> > > 3.3.1.
> > [...]
> > > faheem ~/co/rr/trunk>gcc -o rr rr-sa.c -lRmath -lm
> > > /usr/lib/gcc-lib/i486-linux/3.3.2/../../../libRmath.so: undefined
> > > reference to `Rlog1p'
> > > collect2: ld returned 1 exit status
> >
> > The linker tells you that it cannot find object code for a function Rlog1p.
> > So let's check:
> >
> > edd at chibud:~> grep Rlog1p /usr/include/Rmath.h
> > edd at chibud:~> grep log1p /usr/include/Rmath.h
> > double  log1p(double); /* = log(1+x) {care for small x} */
> > edd at chibud:~>
> >
> > Turns out that there is none defined in Rmath.h, but log1p exists.  This may
> > have gotten renamed since you first wrote your code.
> 
> Maybe I am being dense, but how is this my fault? I am not using either
> Rlog1p or log1p in my code (as far as I can see).

Indeed -- it looks like we have a problem. Looking at the NEWS file, some
logic regarding (R)log1p was changed in the 1.8.* series, and it seems to be
going wrong here.

As a stop gap-measure, just define an empty Rlog1p() to complete linking:

edd at chibud:/tmp> grep Rlog1p rr-sa.c
void Rlog1p(void) {;}
edd at chibud:/tmp> gcc -Wall -o rr rr-sa.c -lRmath -lm
edd at chibud:/tmp> ls -l rr
-rwxr-xr-x    1 edd      edd         13889 Dec 14 00:12 rr
edd at chibud:/tmp>

For the record, on my build system, a log1p function is found but deemed not
good enough:

edd at chibud:/tmp> grep log1p ~/src/debian/build-logs/r-base_1.8.1-1.log |
head -2checking for log1p... yes
checking for working log1p... no


Dirk

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx



From ripley at stats.ox.ac.uk  Sun Dec 14 08:37:34 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 14 Dec 2003 07:37:34 +0000 (GMT)
Subject: [R] compile error with C code and standalone R math C library
In-Reply-To: <20031214061502.GA11158@sonny.eddelbuettel.com>
Message-ID: <Pine.LNX.4.44.0312140735380.17529-100000@gannet.stats>

This is a known bug already fixed (we believe) in R-patched and R-devel.

On Sun, 14 Dec 2003, Dirk Eddelbuettel wrote:

> On Sun, Dec 14, 2003 at 12:51:09AM -0500, Faheem Mitha wrote:
> > On Sat, 13 Dec 2003, Dirk Eddelbuettel wrote:
> > 
> > > On Sat, Dec 13, 2003 at 07:44:46PM -0500, Faheem Mitha wrote:
> > > > I just went back to an old piece of C code. On trying to compile it with
> > > > the R math standalone C library I got the following error. Can anyone
> > > > enlighten me what I am doing wrong, if anything? C file (rr-sa.c) follows.
> > > >
> > > > I'm on Debian sarge. I'm running R version 1.8.1. Gcc is version
> > > > 3.3.1.
> > > [...]
> > > > faheem ~/co/rr/trunk>gcc -o rr rr-sa.c -lRmath -lm
> > > > /usr/lib/gcc-lib/i486-linux/3.3.2/../../../libRmath.so: undefined
> > > > reference to `Rlog1p'
> > > > collect2: ld returned 1 exit status
> > >
> > > The linker tells you that it cannot find object code for a function Rlog1p.
> > > So let's check:
> > >
> > > edd at chibud:~> grep Rlog1p /usr/include/Rmath.h
> > > edd at chibud:~> grep log1p /usr/include/Rmath.h
> > > double  log1p(double); /* = log(1+x) {care for small x} */
> > > edd at chibud:~>
> > >
> > > Turns out that there is none defined in Rmath.h, but log1p exists.  This may
> > > have gotten renamed since you first wrote your code.
> > 
> > Maybe I am being dense, but how is this my fault? I am not using either
> > Rlog1p or log1p in my code (as far as I can see).
> 
> Indeed -- it looks like we have a problem. Looking at the NEWS file, some
> logic regarding (R)log1p was changed in the 1.8.* series, and it seems to be
> going wrong here.
> 
> As a stop gap-measure, just define an empty Rlog1p() to complete linking:
> 
> edd at chibud:/tmp> grep Rlog1p rr-sa.c
> void Rlog1p(void) {;}
> edd at chibud:/tmp> gcc -Wall -o rr rr-sa.c -lRmath -lm
> edd at chibud:/tmp> ls -l rr
> -rwxr-xr-x    1 edd      edd         13889 Dec 14 00:12 rr
> edd at chibud:/tmp>
> 
> For the record, on my build system, a log1p function is found but deemed not
> good enough:
> 
> edd at chibud:/tmp> grep log1p ~/src/debian/build-logs/r-base_1.8.1-1.log |
> head -2checking for log1p... yes
> checking for working log1p... no
> 
> 
> Dirk
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andy_liaw at merck.com  Sun Dec 14 12:26:58 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sun, 14 Dec 2003 06:26:58 -0500
Subject: [R] density plot for very large dataset
Message-ID: <3A822319EB35174CA3714066D590DCD50205CF4E@usrymx25.merck.com>

You might want to try hexbin (hexagonal binning) in the BioConductor suite
(see www.bioconductor.org).

HTH,
Andy

> From: Obi Griffith
> 
> I'm new to R and am trying to perform a simple, yet 
> problematic task.  I 
> have two variables for which I would like to measure the 
> correlation and 
> plot versus each other.  However, I have ~30 million data points 
> measurements of each variable.  I can read this into R from file and 
> produce a plot with plot(x0, x1) but as you would expect, its 
> not pretty 
> to look at and produces a postscript file of about 700MB.  A google 
> search found a few mentions of doing density plots but they seemed to 
> assume you already have the density matrix.  Can anyone point 
> me in the 
> right direction, keeping in mind that I am a complete R newbie.
> 
> Obi


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From arinbasu at softhome.net  Sun Dec 14 13:19:47 2003
From: arinbasu at softhome.net (arinbasu@softhome.net)
Date: Sun, 14 Dec 2003 05:19:47 -0700
Subject: [R] Problem with data conversion
In-Reply-To: <200312141104.hBEB2cnA008294@hypatia.math.ethz.ch> 
References: <200312141104.hBEB2cnA008294@hypatia.math.ethz.ch>
Message-ID: <courier.3FDC5563.00005E1B@softhome.net>

Hi All: 

I came across the following problem while working with a dataset, and 
wondered if there could be a solution I sought here. 


My dataset consists of information on 402 individuals with the followng five 
variables (age,sex, status = a binary variable with levels "case" or 
"control", mma, dma). 

During data check, I found that in the raw data, the data entry operator had 
mistakenly put a "0" for one participant, so now, the levels show 

> levels(status) 
[1] "0" "control" "case" 

The variables mma, and dma are actually numerical variables but in the 
dataframe, they are represented as "characters". I tried to change the type 
of the variables (from character to numeric) using the edit function (and 
bringing up the data grid where then I made changes), but the changes were 
not saved. I tried 

mma1 <- as.numeric(mma) 

but I was not successful in converting mma from a character variable to a 
numeric variable. 

So, to edit and "clean" the data, I exported the dataset as a text file to 
Epi Info 2002 (version 2, Windows). I used the following code: 

mysubset <- subset(workingdat, select = c(age,sex,status, mma, dma))
write.table(mysubset, file="mysubset.txt", sep="\t", col.names=NA) 

After I made changes in the variables using Epi Info (I created a new 
variable called "statusrec" containing values "case" and "control"), I 
exported the file as a ".rec" file (filename "mydata.rec"). I used the 
following code to read the file in R: 

require(foreign)
myData <- read.epiinfo("mydata.rec", read.deleted=NA) 

Now, the problem is this, when I want to run a logistic regression, R 
returns the following error message: 

> glm(statusrec~mma, family=binomial(link=logit))
Error in model.frame(formula, rownames, variables, varnames, extras, 
extranames,  :
       invalid variable type 


I cannot figure out the solution. I want to run a logistic regression now 
with the variable statusrec (which is a binary variable containing values 
"case" and "control"), and another
variable (say mma, which is now a numeric variable). What does the above 
error message mean and what could be a possible solution? 

Would greatly appreciate your insights and wisdom. 

 -Arin Basu



From ozric at web.de  Sun Dec 14 14:23:04 2003
From: ozric at web.de (Christian Schulz)
Date: Sun, 14 Dec 2003 14:23:04 +0100
Subject: [R] Problem with data conversion
References: <200312141104.hBEB2cnA008294@hypatia.math.ethz.ch>
	<courier.3FDC5563.00005E1B@softhome.net>
Message-ID: <000701c3c245$69595520$f000a8c0@xxlarwv7waunej>

> The variables mma, and dma are actually numerical variables but in the
> dataframe, they are represented as "characters". I tried to change the
type
> of the variables (from character to numeric) using the edit function (and
> bringing up the data grid where then I made changes), but the changes were
> not saved. I tried
>
> mma1 <- as.numeric(mma)

i'm not sure understanding your problem correct, but is it possible that you
forget the data.frame ,suppose your data.frame is df

df$mma <- as.numeric (mma)  should work
df$mma[df$mma == 0 ]  <-  1   #"or any other value"

regards,christian



From ripley at stats.ox.ac.uk  Sun Dec 14 14:29:48 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 14 Dec 2003 13:29:48 +0000 (GMT)
Subject: [R] Problem with data conversion
In-Reply-To: <courier.3FDC5563.00005E1B@softhome.net>
Message-ID: <Pine.LNX.4.44.0312141325090.27384-100000@gannet.stats>

The message probably means that the variable is a character variable and 
not numerical (as you intended) nor factor.

Although you said there was a trip to epiinfo, you never said where the 
data came from.  Try dumping out the data, editing the file, and reading 
it with read.table.  There are other ways, but one of your steps has a bug 
and we have no idea what the steps actually were.

When you are finished, try

sapply(mfdf, class)

on your dataframe `mydf'.  You should see only numeric or factor 
variables.

On Sun, 14 Dec 2003 arinbasu at softhome.net wrote:

> Hi All: 
> 
> I came across the following problem while working with a dataset, and 
> wondered if there could be a solution I sought here. 
> 
> 
> My dataset consists of information on 402 individuals with the followng five 
> variables (age,sex, status = a binary variable with levels "case" or 
> "control", mma, dma). 
> 
> During data check, I found that in the raw data, the data entry operator had 
> mistakenly put a "0" for one participant, so now, the levels show 
> 
> > levels(status) 
> [1] "0" "control" "case" 
> 
> The variables mma, and dma are actually numerical variables but in the 
> dataframe, they are represented as "characters". I tried to change the type 
> of the variables (from character to numeric) using the edit function (and 
> bringing up the data grid where then I made changes), but the changes were 
> not saved. I tried 
> 
> mma1 <- as.numeric(mma) 
> 
> but I was not successful in converting mma from a character variable to a 
> numeric variable. 
> 
> So, to edit and "clean" the data, I exported the dataset as a text file to 
> Epi Info 2002 (version 2, Windows). I used the following code: 
> 
> mysubset <- subset(workingdat, select = c(age,sex,status, mma, dma))
> write.table(mysubset, file="mysubset.txt", sep="\t", col.names=NA) 
> 
> After I made changes in the variables using Epi Info (I created a new 
> variable called "statusrec" containing values "case" and "control"), I 
> exported the file as a ".rec" file (filename "mydata.rec"). I used the 
> following code to read the file in R: 
> 
> require(foreign)
> myData <- read.epiinfo("mydata.rec", read.deleted=NA) 
> 
> Now, the problem is this, when I want to run a logistic regression, R 
> returns the following error message: 
> 
> > glm(statusrec~mma, family=binomial(link=logit))
> Error in model.frame(formula, rownames, variables, varnames, extras, 
> extranames,  :
>        invalid variable type 
> 
> 
> I cannot figure out the solution. I want to run a logistic regression now 
> with the variable statusrec (which is a binary variable containing values 
> "case" and "control"), and another
> variable (say mma, which is now a numeric variable). What does the above 
> error message mean and what could be a possible solution? 
> 
> Would greatly appreciate your insights and wisdom. 
> 
>  -Arin Basu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From kjetil at entelnet.bo  Sun Dec 14 14:58:37 2003
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Sun, 14 Dec 2003 09:58:37 -0400
Subject: [R] half normal probability plot in R
Message-ID: <3FDC344D.29691.F2419@localhost>

On 13 Dec 2003 at 4:00, Allison Jones wrote:

> I have generated the effects in a factorial design and now want to 
put
> them in a half normal probability plot. Is there an easy way to do
> this in R??? I can't find the command. Thanks much -
> 

qqnorm.aov in package gregmisc (on CRAN)

Kjetil Halvorsen


> Ali Jones
>



From lobry at biomserv.univ-lyon1.fr  Sun Dec 14 15:27:47 2003
From: lobry at biomserv.univ-lyon1.fr (Jean lobry)
Date: Sun, 14 Dec 2003 15:27:47 +0100
Subject: [R] correlation and causality examples
In-Reply-To: <200311151115.hAFB7MQ5009261@hypatia.math.ethz.ch>
References: <200311151115.hAFB7MQ5009261@hypatia.math.ethz.ch>
Message-ID: <p05200f08bc021ed10db1@[80.15.58.39]>

Dear R-users,

many thanks to all who replied to my request, approx. one month
ago, about examples illustrating that correlation does imply
causality. I have tried to compile your suggestions in a
web-site, which URL is given in the screenshot in png format
there:
http://pbil.univ-lyon1.fr/members/lobry/z.png
and in jpeg format there:
http://pbil.univ-lyon1.fr/members/lobry/z.jpg

It's far for being perfect because of an over-teaching period,
but I hope to improve it in the future, so that your suggestions
and comments are always welcome.

All the best,

Jean
-- 
Jean R. Lobry
Laboratoire BBE-CNRS-UMR-5558, Univ. C. Bernard - LYON I,
43 Bd 11/11/1918, F-69622 VILLEURBANNE CEDEX, FRANCE
allo  : +33 472 43 12 87     fax    : +33 472 43 13 88
http://pbil.univ-lyon1.fr/members/lobry/



From patrick.giraudoux at univ-fcomte.fr  Sun Dec 14 15:58:26 2003
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux)
Date: Sun, 14 Dec 2003 15:58:26 +0100
Subject: [R] contour() should handle the asp parameter
Message-ID: <00c401c3c252$bdc643e0$b29c0c50@PC728329681112>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031214/14a51b29/attachment.pl

From ripley at stats.ox.ac.uk  Sun Dec 14 16:44:16 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 14 Dec 2003 15:44:16 +0000 (GMT)
Subject: [R] contour() should handle the asp parameter
In-Reply-To: <00c401c3c252$bdc643e0$b29c0c50@PC728329681112>
Message-ID: <Pine.LNX.4.44.0312141536430.13784-100000@gannet.stats>

Given contour() has an add= argument whose use is demonstrated for this 
exact reason in `all good books on S', why complicate the contour 
function?

Contrary to popular belief, the `asp parameter' is not a parameter, but an 
argument of plot's default method.

On Sun, 14 Dec 2003, Patrick Giraudoux wrote:

> To my knowledge, the current version of contour.default() does not handle the 'asp' parameter. This can be embarassing when displaying eg geographical maps, etc... Submitted to the opinion of more experienced R programmers, contour.defaut() function should be changed according to the followings:
> 
> line 7: add = FALSE,asp=NA,...)
> line 33: plot.window(xlim, ylim, asp=asp,"")

And of course alter the documentation.

I suspect you could get away with passing ... to the plot.window call, 
which for clarity should be plot.window(xlim, ylim, "", asp = asp)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From edd at debian.org  Sun Dec 14 19:28:46 2003
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 14 Dec 2003 12:28:46 -0600
Subject: [R] A faster plotOHLC() for the tseries package
Message-ID: <20031214182846.GA15697@sonny.eddelbuettel.com>


The plotOHLC function in the tseries package is useful to plot timeseries of
various financial assets with open/high/low/close data.  I had often
wondered if it could be made to run a little faster. It turns out that the
following patch does 

--- plotOHLC.R.orig	2003-12-14 12:02:20.000000000 -0600
+++ plotOHLC.R	2003-12-14 12:03:42.000000000 -0600
@@ -21,14 +21,9 @@
         ylim <- range(x[is.finite(x)])
     plot.new()
     plot.window(xlim, ylim, ...)
-    for (i in 1:NROW(x)) {
-        segments(time.x[i], x[i, "High"], time.x[i], x[i, "Low"], 
-            col = col[1], bg = bg)
-        segments(time.x[i] - dt, x[i, "Open"], time.x[i], x[i, 
-            "Open"], col = col[1], bg = bg)
-        segments(time.x[i], x[i, "Close"], time.x[i] + dt, x[i, 
-            "Close"], col = col[1], bg = bg)
-    }
+    segments(time.x, x[, "High"], time.x, x[, "Low"], col = col[1], bg = bg)
+    segments(time.x - dt, x[, "Open"], time.x, x[, "Open"], col = col[1], bg = bg)
+    segments(time.x, x[, "Close"], time.x + dt, x[, "Close"], col = col[1], bg = bg)
     if (ann) 
         title(main = main, xlab = xlab, ylab = ylab, ...)
     if (axes) {

decrease the time spent on a series of ~500 points by a factor of sixty:

> IBM<-get.hist.quote("IBM", "2001-12-14")
trying URL
http://chart.yahoo.com/table.csv?s=IBM&a=11&b=13&c=2001&d=11&e=12&f=2003&g=d&q=q&y=0&z=IBM&x=.csv'
Content type application/octet-stream' length unknown
opened URL
.......... .......... ...
downloaded 23Kb

time series starts 2001-12-12
time series ends   2003-12-11
> system.time(plotOHLC(IBM))			# original
[1] 1.56 0.26 5.11 0.00 0.00
> system.time(fastplotOHLC(IBM))		# patched
[1] 0.02 0.00 0.05 0.00 0.00


Regards,  Dirk

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx



From ggrothendieck at myway.com  Sun Dec 14 20:35:40 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 14 Dec 2003 14:35:40 -0500 (EST)
Subject: [R] A faster plotOHLC() for the tseries package
Message-ID: <20031214193540.93693395D@mprdmxin.myway.com>


Dirk,

Could you please explain to me how to interpret the lines
that you posted (which I gather are intended to be used with
some program that combines them with the original source)?

Alternately, could you just send the revised OHLC source?

Thanks.

P.S.  I use Windows 2000.

--- 
Date: Sun, 14 Dec 2003 12:28:46 -0600 
From: Dirk Eddelbuettel <edd at debian.org>
[ Add to Address Book | Block Address | Report as Spam ] 
To: <R-help at stat.math.ethz.ch> 
Subject: [R] A faster plotOHLC() for the tseries package 

 
 

The plotOHLC function in the tseries package is useful to plot timeseries of
various financial assets with open/high/low/close data. I had often
wondered if it could be made to run a little faster. It turns out that the
following patch does 

--- plotOHLC.R.orig     2003-12-14 12:02:20.000000000 -0600
+++ plotOHLC.R     2003-12-14 12:03:42.000000000 -0600
@@ -21,14 +21,9 @@
ylim <- range(x[is.finite(x)])
plot.new()
plot.window(xlim, ylim, ...)
- for (i in 1:NROW(x)) {
- segments(time.x[i], x[i, "High"], time.x[i], x[i, "Low"], 
- col = col[1], bg = bg)
- segments(time.x[i] - dt, x[i, "Open"], time.x[i], x[i, 
- "Open"], col = col[1], bg = bg)
- segments(time.x[i], x[i, "Close"], time.x[i] + dt, x[i, 
- "Close"], col = col[1], bg = bg)
- }
+ segments(time.x, x[, "High"], time.x, x[, "Low"], col = col[1], bg = bg)
+ segments(time.x - dt, x[, "Open"], time.x, x[, "Open"], col = col[1], bg = bg)
+ segments(time.x, x[, "Close"], time.x + dt, x[, "Close"], col = col[1], bg = bg)
if (ann) 
title(main = main, xlab = xlab, ylab = ylab, ...)
if (axes) {

decrease the time spent on a series of ~500 points by a factor of sixty:

> IBM<-get.hist.quote("IBM", "2001-12-14")
trying URL
http://chart.yahoo.com/table.csv?s=IBM&a=11&b=13&c=2001&d=11&e=12&f=2003&g=d&q=q&y=0&z=IBM&x=.csv'
Content type application/octet-stream' length unknown
opened URL
.......... .......... ...
downloaded 23Kb

time series starts 2001-12-12
time series ends 2003-12-11
> system.time(plotOHLC(IBM))               # original
[1] 1.56 0.26 5.11 0.00 0.00
> system.time(fastplotOHLC(IBM))          # patched
[1] 0.02 0.00 0.05 0.00 0.00


Regards, Dirk

-- 
Those are my principles, and if you don't like them... well, I have others.
-- Groucho Marx



From edd at debian.org  Sun Dec 14 20:56:03 2003
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 14 Dec 2003 13:56:03 -0600
Subject: [R] A faster plotOHLC() for the tseries package
In-Reply-To: <20031214193540.93693395D@mprdmxin.myway.com>
References: <20031214193540.93693395D@mprdmxin.myway.com>
Message-ID: <20031214195602.GA16418@sonny.eddelbuettel.com>


Hi Gabor,

On Sun, Dec 14, 2003 at 02:35:40PM -0500, Gabor Grothendieck wrote:
> 
> Dirk,
> 
> Could you please explain to me how to interpret the lines
> that you posted (which I gather are intended to be used with
> some program that combines them with the original source)?

That is the usual paradigm of using output of diff(1) 

 $ diff -u old new > diff.txt
     
as input to the patch(1) program as e.g. in

 $ patch < diff.txt        # try patch --dry-run < diff.txt   first
   
On win2k, you can get them for sure with Cygwin, probably also with
mingw/msys and likely also with BDR's set of tools to build R from source.
     
Patch, written by Larry Wall of Perl fame, reads an entry such as

  > --- plotOHLC.R.orig     2003-12-14 12:02:20.000000000 -0600
  > +++ plotOHLC.R     2003-12-14 12:03:42.000000000 -0600
  > @@ -21,14 +21,9 @@

and knows to replace lines marked with '-' (taken be old the old file) with
those marked '+'.  In this example, it is trivial as there is only one
segment in which the code 

 for (i in 1:NROW(x)) {
     segments(time.x[i], x[i, "High"], time.x[i], x[i, "Low"], 
         col = col[1], bg = bg)
     segments(time.x[i] - dt, x[i, "Open"], time.x[i], x[i, 
         "Open"], col = col[1], bg = bg)
     segments(time.x[i], x[i, "Close"], time.x[i] + dt, x[i, 
         "Close"], col = col[1], bg = bg)
 }

with 

 segments(time.x, x[, "High"], time.x, x[, "Low"], col = col[1], bg = bg)
 segments(time.x - dt, x[, "Open"], time.x, x[, "Open"], col = col[1], bg = bg)
 segments(time.x, x[, "Close"], time.x + dt, x[, "Close"], col = col[1], bg = bg)
     
> Alternately, could you just send the revised OHLC source?

Well, the source is different from what you find in $R_HOME/library/tseries/R
so you may as well edit there by hand. I don't have access to a windows box
right now, but if the above doesn't help email off-line and I start up the
laptop from work.

Hope this helps,  Dirk

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx



From maj at stats.waikato.ac.nz  Sun Dec 14 22:08:31 2003
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Mon, 15 Dec 2003 10:08:31 +1300
Subject: [R] reverse lexicographic order
Message-ID: <3FDCD14F.3020605@stats.waikato.ac.nz>

Hi all,

I have some email addresses that I would like to sort in reverse 
lexicographic order so that addresses from the same domain will be 
grouped together. How might that be done?

Murray

-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862



From fciclone at bol.com.br  Sun Dec 14 22:16:00 2003
From: fciclone at bol.com.br (fciclone)
Date: Sun, 14 Dec 2003 19:16:00 -0200
Subject: [R] pca
Message-ID: <HPWLQO$150430C731B565EA6DE954462F04A394@bol.com.br>

Dear listmates, i've done a pca analisys in R (1.8 v.) 
with the command

pca(Matrix, cent=FALSE, scle=FALSE)  

I have obtained a v matrix very different from the  
component matrix resulted by a factor analysis in SPSS, 
unrotated and with a extraction from a correlation 
matrix. Any clues about this diference?
Thanks in advance,
Alex.
 
__________________________________________________________________________
Acabe com aquelas janelinhas que pulam na sua tela.
AntiPop-up UOL - ? gr?tis!
http://antipopup.uol.com.br/



From tblackw at umich.edu  Sun Dec 14 22:45:41 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Sun, 14 Dec 2003 16:45:41 -0500 (EST)
Subject: [R] reverse lexicographic order
In-Reply-To: <3FDCD14F.3020605@stats.waikato.ac.nz>
References: <3FDCD14F.3020605@stats.waikato.ac.nz>
Message-ID: <Pine.SOL.4.58.0312141636390.17997@timepilot.gpcc.itd.umich.edu>

Murray  -

If you could guarantee that all of the email addresses have
exactly one occurrence of the "@" character in them, then
something like

spit <- do.call("rbind", strsplit(addresses, "@", FALSE))

will produce a data frame with either two or three character
vectors as the columns, in which user name is a separate
column from the domain name.  Now use

spit <- spit[ order(spit[ ,3], spit[ ,1]), ]

to re-sort the data frame by user name within domain name,
and  paste()  to put the columns back together again.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Mon, 15 Dec 2003, Murray Jorgensen wrote:

> Hi all,
>
> I have some email addresses that I would like to sort in reverse
> lexicographic order so that addresses from the same domain will be
> grouped together. How might that be done?
>
> Murray
>
> --
> Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
> Department of Statistics, University of Waikato, Hamilton, New Zealand
> Email: maj at waikato.ac.nz                                Fax 7 838 4155
> Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From dmurdoch at pair.com  Sun Dec 14 23:31:52 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Sun, 14 Dec 2003 17:31:52 -0500
Subject: [R] reverse lexicographic order
In-Reply-To: <3FDCD14F.3020605@stats.waikato.ac.nz>
References: <3FDCD14F.3020605@stats.waikato.ac.nz>
Message-ID: <etoptvofe16vm5vhpbr9cp4fnl74mqgftr@4ax.com>

On Mon, 15 Dec 2003 10:08:31 +1300, you wrote:

>Hi all,
>
>I have some email addresses that I would like to sort in reverse 
>lexicographic order so that addresses from the same domain will be 
>grouped together. How might that be done?

I'm not sure this is what you want, but this function sorts a
character vector by last letters, then 2nd last, 3rd last, and so on:


revsort <- function(x,...) {
	x[order(unlist(lapply(strsplit(x,''),
		function(x) paste(rev(x),collapse=''))),...)]
}

> revsort(as.character(1:20))
 [1] "10" "20" "1"  "11" "2"  "12" "3"  "13" "4"  "14" "5"  "15" "6"
"16" "7" 
[16] "17" "8"  "18" "9"  "19"

The ... args are given to order(), so na.last=FALSE and
decreasing=TRUE are possibilities.

Duncan Murdoch



From trainor at transborder.org  Sun Dec 14 23:31:06 2003
From: trainor at transborder.org (Douglas Trainor)
Date: Sun, 14 Dec 2003 16:31:06 -0600
Subject: [R] pca
In-Reply-To: <HPWLQO$150430C731B565EA6DE954462F04A394@bol.com.br>
References: <HPWLQO$150430C731B565EA6DE954462F04A394@bol.com.br>
Message-ID: <3FDCE4AA.8050401@transborder.org>

Somewhere along the line, you have been confused.
You're in good company though.  Factor analysis 
and PCA are different entities entirely.

	douglas



fciclone wrote:

>Dear listmates, i've done a pca analisys in R (1.8 v.) 
>with the command
>
>pca(Matrix, cent=FALSE, scle=FALSE)  
>
>I have obtained a v matrix very different from the  
>component matrix resulted by a factor analysis in SPSS, 
>unrotated and with a extraction from a correlation 
>matrix. Any clues about this diference?
>Thanks in advance,
>Alex.
>  
>



From p.dalgaard at biostat.ku.dk  Sun Dec 14 23:49:34 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 14 Dec 2003 23:49:34 +0100
Subject: [R] reverse lexicographic order
In-Reply-To: <Pine.SOL.4.58.0312141636390.17997@timepilot.gpcc.itd.umich.edu>
References: <3FDCD14F.3020605@stats.waikato.ac.nz>
	<Pine.SOL.4.58.0312141636390.17997@timepilot.gpcc.itd.umich.edu>
Message-ID: <x2wu8znhfl.fsf@biostat.ku.dk>

Thomas W Blackwell <tblackw at umich.edu> writes:

> Murray  -
> 
> If you could guarantee that all of the email addresses have
> exactly one occurrence of the "@" character in them, then
> something like
....snip....

Otherwise, try something like this (I don't think we have a string
reversal function anywhere, do we?):

> mychar <- scan(what="")
1: I have some email addresses that I would like to sort in reverse
14: lexicographic order so that addresses from the same domain will be
25: grouped together. How might that be done?
32:
Read 31 items
>
>mychar[order(sapply(lapply(strsplit(mychar,""),rev),paste,collapse=""))]
 [1] "together."     "done?"         "I"             "I"
 [5] "lexicographic" "grouped"       "would"         "be"
 [9] "be"            "the"           "like"          "same"
[13] "some"          "reverse"       "have"          "email"
[17] "will"          "from"          "in"            "domain"
[21] "so"            "to"            "order"         "addresses"
[25] "addresses"     "that"          "that"          "that"
[29] "might"         "sort"          "How"



> > Hi all,
> >
> > I have some email addresses that I would like to sort in reverse
> > lexicographic order so that addresses from the same domain will be
> > grouped together. How might that be done?
> >
> > Murray
> >
> > --
> > Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
> > Department of Statistics, University of Waikato, Hamilton, New Zealand
> > Email: maj at waikato.ac.nz                                Fax 7 838 4155
> > Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From maj at stats.waikato.ac.nz  Mon Dec 15 01:15:05 2003
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Mon, 15 Dec 2003 13:15:05 +1300
Subject: [R] reverse lexicographic order
In-Reply-To: <etoptvofe16vm5vhpbr9cp4fnl74mqgftr@4ax.com>
References: <3FDCD14F.3020605@stats.waikato.ac.nz>
	<etoptvofe16vm5vhpbr9cp4fnl74mqgftr@4ax.com>
Message-ID: <3FDCFD09.6070206@stats.waikato.ac.nz>

Hi Duncan, Hi Peter,

thanks for those ideas!  I'm sure I will learn a lot be fooling around 
with them.

Cheers,

Murray

Duncan Murdoch wrote:

> On Mon, 15 Dec 2003 10:08:31 +1300, you wrote:
> 
> 
>>Hi all,
>>
>>I have some email addresses that I would like to sort in reverse 
>>lexicographic order so that addresses from the same domain will be 
>>grouped together. How might that be done?
> 
> 
> I'm not sure this is what you want, but this function sorts a
> character vector by last letters, then 2nd last, 3rd last, and so on:
> 
> 
> revsort <- function(x,...) {
> 	x[order(unlist(lapply(strsplit(x,''),
> 		function(x) paste(rev(x),collapse=''))),...)]
> }
> 
> 
>>revsort(as.character(1:20))
> 
>  [1] "10" "20" "1"  "11" "2"  "12" "3"  "13" "4"  "14" "5"  "15" "6"
> "16" "7" 
> [16] "17" "8"  "18" "9"  "19"
> 
> The ... args are given to order(), so na.last=FALSE and
> decreasing=TRUE are possibilities.
> 
> Duncan Murdoch
> 
> 
> 
> 

-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862



From p.murrell at auckland.ac.nz  Mon Dec 15 01:16:30 2003
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Mon, 15 Dec 2003 13:16:30 +1300
Subject: [R] extensive grid docs?
References: <OFE59BCA41.D6F278A9-ONC1256DF9.00370F1C-C1256DF9.0037A64E@BIVV.BE>
Message-ID: <3FDCFD5E.306@stat.auckland.ac.nz>

Hi

Most of the documentation that exists for grid is currently linked off 
my grid web site (http://www.stat.auckland.ac.nz/~paul/grid/grid.html).

Paul


tobias.verbeke at bivv.be wrote:
> 
> 
> 
> r-help-bounces at stat.math.ethz.ch wrote on 10/12/2003 22:23:43:
> 
> 
>>I'm looking for extensive docs on using grid (for the somewhat newbie).
> 
> I'm
> 
>>attempting to create a set of graphics that look similar to the attached
>>image (I hope this doesn't get bounced) and have only come across the R
>>newsletters and it appears that grid was new as of 1.8.0? I think the
> 
> best
> 
>>way to proceed is to create the plot, clip it using a polygon, then
> 
> manually
> 
>>add the annotation. Is that correct?
>>
>>I couldn't find much on the FAQ regarding creating really goofy plots
> 
> with
> 
>>grid and any hints would be greatly appreciated.
>>
> 
> 
> Here's an R graphics tutorial by Paul Murrell
> 
> http://www.ci.tuwien.ac.at/Conferences/DSC-2003/tutorials.html
> 
> 
> 
> HTH,
> Tobias
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help


-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From Tom.Mulholland at health.wa.gov.au  Mon Dec 15 01:50:51 2003
From: Tom.Mulholland at health.wa.gov.au (Mulholland, Tom)
Date: Mon, 15 Dec 2003 08:50:51 +0800
Subject: [R] read.spss question warning compression bias
Message-ID: <74E242B6968AA0469B632C5A3EFC1EFD03D57019@nt207mesep.corporate.hdwa.health.wa.gov.au>

>So it would appear that if the above is correct, there is no user adjustment to the bias value.
>The only scenario that I can envision is if the user SAVE's the ".sav" file in an uncompressed
>format, where the bias value **might** be set to 0.

>Perhaps a r-help reader with access to current SPSS manuals can confirm the above.


The windows version 11.5.0 appears the same (I assume the negative sign on -99 was somehow dropped)

COMPRESSED and UNCOMPRESSED Subcommands

COMPRESSED saves the file in compressed form. UNCOMPRESSED saves the file in uncom-pressed form.
In a compressed file, small integers (from ?99 to 155) are stored in one byteinstead of the
eight bytes used in an uncompressed file. 

The only specification is the keyword COMPRESSED or UNCOMPRESSED. There are noadditional specifications. 

Compressed data files occupy less disk space than do uncompressed data files.

Compressed data files take longer to read than do uncompressed data files.

The GET command, which reads SPSS-format data files, does not need to specify whetherthe files it reads are compressed or uncompressed.

Only one of the subcommands COMPRESSED or UNCOMPRESSED can be specified perSAVE command. COMPRESSED is usually the default, though UNCOMPRESSED may bethe default on some systems.

Ciao, Tom

_________________________________________________
 
Tom Mulholland
Senior Policy Officer
WA Country Health Service
Tel: (08) 9222 4062
 
The contents of this e-mail transmission are confidential and may be protected by professional privilege. The contents are intended only for the named recipients of this e-mail. If you are not the intended recipient, you are hereby notified that any use, reproduction, disclosure or distribution of the information contained in this e-mail is prohibited. Please notify the sender immediately.


-----Original Message-----
From: Marc Schwartz [mailto:MSchwartz at medanalytics.com] 
Sent: Friday, 12 December 2003 3:56 AM
To: Thomas Lumley
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] read.spss question warning compression bias


On Thu, 2003-12-11 at 12:32, Thomas Lumley wrote:
> On Thu, 11 Dec 2003, Marc Schwartz wrote:
> >
> > An additional question might be, if the file is not compressed, what 
> > is the default bias value set by SPSS? If it is 0, then the check is 
> > meaningless. On the other hand, if the default value is 100, whether 
> > or not the file is compressed, then the warning message would serve 
> > a purpose in flagging the possibility of other issues. Reasonably, 
> > that setting may be SPSS version specific.
> >
> 
> I think the issue is that the format is not documented, so the author 
> of the code (Ben Pfaff) didn't know what a change in the value would 
> imply. If the file is apparently read correctly it seems that it 
> doesn't imply anything.
> 
> 	-thomas



Thanks for the clarification Thomas.

I did some searching of the PSPP site and found the following:

http://www.gnu.org/software/pspp/manual/pspp_18.html#SEC170

The compression bias is defined as:

flt64 bias;
        Compression bias. Always set to 100. The significance of this
        value is that only numbers between (1 - bias) and (251 - bias)
        can be compressed.
        

So it would seem to potentially impact aspects of the file compression data structure, when compression is used.

I am not sure if the "Always set to 100" is unique to PSPP in how Ben elected to do things. Presumably if that is always the case, even with SPSS, one might reasonably wonder: why have it, if it does not vary?

It leaves things unclear as to under what circumstances this value would change. 

I did some Googling and found the following text snippet from a presumably dated SPSS manual for the syntax of the SAVE command:


SAVE OUTFILE=file 

[/VERSION={3**}] {2 } 

[/UNSELECTED=[{RETAIN}] {DELETE} 

[/KEEP={ALL** }] [/DROP=varlist] {varlist} 

[/RENAME=(old varlist=new varlist)...] 

[/MAP] 

[/{COMPRESSED }] {UNCOMPRESSED} 

**Default if the subcommand is omitted.


COMPRESSED and UNCOMPRESSED Subcommands 

COMPRESSED saves the file in compressed form. UNCOMPRESSED saves the file in uncompressed form. In a compressed file, small integers (from 
99 to 155) are stored in one byte instead of the eight bytes used in an uncompressed file.

The only specification is the keyword COMPRESSED or UNCOMPRESSED. There are no additional specifications. 

Compressed data files occupy less disk space than do uncompressed data files. 

Compressed data files take longer to read than do uncompressed data files. 

The GET command, which reads SPSS-format data files, does not need to specify whether the files it reads are compressed or uncompressed. 

Only one of the subcommands COMPRESSED or UNCOMPRESSED can be specified per SAVE command. COMPRESSED is usually the default, though UNCOMPRESSED may be the default on some systems.




So it would appear that if the above is correct, there is no user adjustment to the bias value. The only scenario that I can envision is if the user SAVE's the ".sav" file in an uncompressed format, where the bias value **might** be set to 0.

Perhaps a r-help reader with access to current SPSS manuals can confirm the above.

Until demonstrated otherwise, it seems reasonable to leave the warning message in place as a warning (as opposed to an error), though it might be helpful to folks to add a comment to the read.spss help file on this for clarification. The text might read:

"NOTE: You may receive the following message:

 Warning message: 
 FileName: Compression bias (X) is not the usual value of 100.

Where 'FileName' will be the file that you are reading and 'X' will be a numeric value, possibly 0. This *may* be the result of reading an UNCOMPRESSED SPSS file. It is recommended that you verify the integrity of your imported SPSS data after using read.spss() if you receive this warning."


The wording is subject to change and of course, the integrity check should be done under any circumstances... :-)

HTH,

Marc Schwartz

______________________________________________
R-help at stat.math.ethz.ch mailing list https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From pauljohn at ku.edu  Sun Dec 14 10:29:39 2003
From: pauljohn at ku.edu (Paul E. Johnson)
Date: Sun, 14 Dec 2003 03:29:39 -0600
Subject: [R] Problem with data conversion
In-Reply-To: <courier.3FDC5563.00005E1B@softhome.net>
References: <200312141104.hBEB2cnA008294@hypatia.math.ethz.ch>
	<courier.3FDC5563.00005E1B@softhome.net>
Message-ID: <3FDC2D83.8060800@ku.edu>

I sympathize with your trouble bringing in data, but you need to catch 
your breath and figure out what you really have.  I think when you get a 
bit more R practice, you will be able to manage what you bring in 
without going back to that editor so much.

I feel certain your data is not what you think it is.  Here's an example 
where a factor DOES work on the lhs of a glm:

 > y <- factor(c("S","N","S","N","S","N","S","N"))
 > x <- rnorm(8)
 > glm(y~x,family=binomial(link=logit))

Look here: the system knows y is a factor:
 > attributes(y)
$levels
[1] "N" "S"

$class
[1] "factor"

My guess is that your variables are not really factors, but rather 
character vectors.  You have to convert them into factors.
Watch the error I get is the same that you got.

 > y <- c("S","N","S","N","S","N","S","N")
 > glm(y~x,family=binomial(link=logit))
Error in model.frame(formula, rownames, variables, varnames, extras, 
extranames,  :
        invalid variable type

Note the system doesn't know y is "supposed" to be a factor. It just 
sees characters.

 > y
[1] "S" "N" "S" "N" "S" "N" "S" "N"
 > levels(y)
NULL
 > attributes(y)
NULL

but look:
 > glm(as.factor(y)~x,family=binomial(link=logit))



arinbasu at softhome.net wrote:

> Hi All:
> I came across the following problem while working with a dataset, and 
> wondered if there could be a solution I sought here.
>
> My dataset consists of information on 402 individuals with the 
> followng five variables (age,sex, status = a binary variable with 
> levels "case" or "control", mma, dma).
> During data check, I found that in the raw data, the data entry 
> operator had mistakenly put a "0" for one participant, so now, the 
> levels show
>
>> levels(status) 
>
> [1] "0" "control" "case"
> The variables mma, and dma are actually numerical variables but in the 
> dataframe, they are represented as "characters". I tried to change the 
> type of the variables (from character to numeric) using the edit 
> function (and bringing up the data grid where then I made changes), 
> but the changes were not saved. I tried
> mma1 <- as.numeric(mma)
> but I was not successful in converting mma from a character variable 
> to a numeric variable.
> So, to edit and "clean" the data, I exported the dataset as a text 
> file to Epi Info 2002 (version 2, Windows). I used the following code:
> mysubset <- subset(workingdat, select = c(age,sex,status, mma, dma))
> write.table(mysubset, file="mysubset.txt", sep="\t", col.names=NA)
> After I made changes in the variables using Epi Info (I created a new 
> variable called "statusrec" containing values "case" and "control"), I 
> exported the file as a ".rec" file (filename "mydata.rec"). I used the 
> following code to read the file in R:
> require(foreign)
> myData <- read.epiinfo("mydata.rec", read.deleted=NA)
> Now, the problem is this, when I want to run a logistic regression, R 
> returns the following error message:
>
>> glm(statusrec~mma, family=binomial(link=logit))
>
> Error in model.frame(formula, rownames, variables, varnames, extras, 
> extranames,  :
>       invalid variable type
>
> I cannot figure out the solution. I want to run a logistic regression 
> now with the variable statusrec (which is a binary variable containing 
> values "case" and "control"), and another
> variable (say mma, which is now a numeric variable). What does the 
> above error message mean and what could be a possible solution?
> Would greatly appreciate your insights and wisdom.
> -Arin Basu
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



-- 
Paul E. Johnson                       email: pauljohn at ukans.edu
Dept. of Political Science            http://lark.cc.ukans.edu/~pauljohn
University of Kansas                  Office: (785) 864-9086
Lawrence, Kansas 66045                FAX: (785) 864-5700



From cbotts1 at cox.net  Mon Dec 15 05:17:10 2003
From: cbotts1 at cox.net (cbotts1@cox.net)
Date: Sun, 14 Dec 2003 23:17:10 -0500
Subject: [R] help in lme
Message-ID: <20031215041712.RVXO24575.lakemtao06.cox.net@smtp.central.cox.net>

To anyone who can help,

  I have two stupid questions, and one fairly intelligent question

Stupid question (1):   is there an R function to calculate a factorial of a number?    That is...is there a function g(.) such that g(3) = 6, g(4) = 24, g(6) = 720, etc?

Stupid question (2):  how do you extract the estimated covariance matrix of the random effects in an lme object?


Intelligent question (1)  I keep on trying to fit a linear mixed model in R using 'lme(y~fxd.dsgn, data = data.mtrx, ~rnd.dsgn|group)' where fxd.dsgn and rnd.dsgn are the fixed and random design matrices, respectively.   The function won't work, though.   It keeps telling me that it can't find the object 'rnd.dsgn'.    What's the matter here?

Any help would be greatly appreciated.



From faheem at email.unc.edu  Mon Dec 15 07:03:45 2003
From: faheem at email.unc.edu (Faheem Mitha)
Date: Mon, 15 Dec 2003 01:03:45 -0500 (EST)
Subject: [R] help with random numbers and Rmpi
In-Reply-To: <85llpwi019.fsf@blindglobe.net>
References: <Pine.LNX.4.58.0312011520250.26318@Chrestomanci>
	<85fzg4jmgz.fsf@blindglobe.net>
	<Pine.LNX.4.58.0312011552290.26318@Chrestomanci>
	<853cc4jkud.fsf@blindglobe.net>
	<Pine.LNX.4.58.0312011627380.26318@Chrestomanci>
	<85llpwi019.fsf@blindglobe.net>
Message-ID: <Pine.LNX.4.58.0312150022281.940@Chrestomanci>



On Mon, 1 Dec 2003, A.J. Rossini wrote:

> Faheem Mitha <faheem at email.unc.edu> writes:
>
> > So, can this (parallelization at the C level) be done without running a
> > bunch of C slaves along the lines I had previously written? Any examples
> > would be helpful.
>
> How much heavy lifting happens before you spawn the slaves, and can
> that not be moved to R?
>
> Your best bet is to read the SNOW code for handling SPRNG/RSPRNG,
> otherwise.

I've tried to use snow as suggested. I have a R function mg.randvec which
generates a vector of random variates. This function calls a C routine via
the .C call. This works fine if I call it like say...

*********************************************************************
> mg.randvec(3,2,10,5)
$val
 [1] -1.9967464 -1.8634205 -0.7459255 -1.7591047 -1.7811685 -1.9953316
 [7] -1.7932502 -1.9823565 -1.7999789 -1.0501179 -1.9679886  0.1484859
[13]  0.5768898  1.9117889  1.9366872 -1.3847453 -1.5554107 -1.4933195
[19] -1.8508795 -1.6715850 -1.8951212 -1.8900167 -1.1630852 -1.3989748
[25] -1.9400337 -1.6774471 -1.8136065 -1.8685709 -1.9119879 -1.3378416
*********************************************************************

However, with snow I get

**********************************************************************
> clusterCall(cl,mg.randvec,3,2,10,5)
[[1]]
[1] "Error in .C(\"rocftp\", as.integer(k), as.integer(len),
as.double(theta),  : \n\tC/Fortran function name not in load table\n"
attr(,"class")
[1] "try-error"

[[2]]
[1] "Error in .C(\"rocftp\", as.integer(k), as.integer(len),
as.double(theta),  : \n\tC/Fortran function name not in load table\n"
attr(,"class")
[1] "try-error"

[[3]]
[1] "Error in .C(\"rocftp\", as.integer(k), as.integer(len),
as.double(theta),  : \n\tC/Fortran function name not in load table\n"
attr(,"class")
[1] "try-error"
********************************************************************

In the cluster case it seems to have difficulty loading up the C routine.

I think snow is working Ok, because basic examples like the following
work.

********************************************************************
> clusterCall(cl,runif,3)
[[1]]
[1] 0.1527429 0.1134621 0.8663094

[[2]]
[1] 0.2256776 0.8981241 0.1120226

[[3]]
[1] 0.2371450 0.5090693 0.2776081
******************************************************************

Can anyone tell me what I am doing wrong? All data files is shared across
all three machines I am using (AFS space).

Thanks in advance.

                                                            Faheem.



From ripley at stats.ox.ac.uk  Mon Dec 15 08:17:40 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 15 Dec 2003 07:17:40 +0000 (GMT)
Subject: [R] pca
In-Reply-To: <3FDCE4AA.8050401@transborder.org>
Message-ID: <Pine.LNX.4.44.0312150710070.15049-100000@gannet.stats>

On Sun, 14 Dec 2003, Douglas Trainor wrote:

> Somewhere along the line, you have been confused.
> You're in good company though.  Factor analysis 
> and PCA are different entities entirely.

Not in SPSS, where the same command is used for both (although we were not 
told anything like enough about what was done).

*However* I don't know what is meant by `pca in R'. Standard R contains
princomp() and prcomp() to do PCA: it appears that pca() is in the
orphaned package multiv. Why not use the standard functions?  Also, those
non-default arguments would appear to be very unusual indeed for PCA, and
do not correspond to a *correlation* matrix.

> Any clues about this diference?

User error looks likely.  Please seek out local statistical expertise.

> >Dear listmates, i've done a pca analisys in R (1.8 v.) 
> >with the command
> >
> >pca(Matrix, cent=FALSE, scle=FALSE)  
> >
> >I have obtained a v matrix very different from the  
> >component matrix resulted by a factor analysis in SPSS, 
> >unrotated and with a extraction from a correlation 
> >matrix. Any clues about this diference?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Dec 15 08:20:06 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 15 Dec 2003 07:20:06 +0000 (GMT)
Subject: [R] reverse lexicographic order
In-Reply-To: <x2wu8znhfl.fsf@biostat.ku.dk>
Message-ID: <Pine.LNX.4.44.0312150719490.15049-100000@gannet.stats>

On 14 Dec 2003, Peter Dalgaard wrote:

> Thomas W Blackwell <tblackw at umich.edu> writes:
> 
> > Murray  -
> > 
> > If you could guarantee that all of the email addresses have
> > exactly one occurrence of the "@" character in them, then
> > something like
> ....snip....
> 
> Otherwise, try something like this (I don't think we have a string
> reversal function anywhere, do we?):

Only in the examples for strsplit ....

> 
> > mychar <- scan(what="")
> 1: I have some email addresses that I would like to sort in reverse
> 14: lexicographic order so that addresses from the same domain will be
> 25: grouped together. How might that be done?
> 32:
> Read 31 items
> >
> >mychar[order(sapply(lapply(strsplit(mychar,""),rev),paste,collapse=""))]
>  [1] "together."     "done?"         "I"             "I"
>  [5] "lexicographic" "grouped"       "would"         "be"
>  [9] "be"            "the"           "like"          "same"
> [13] "some"          "reverse"       "have"          "email"
> [17] "will"          "from"          "in"            "domain"
> [21] "so"            "to"            "order"         "addresses"
> [25] "addresses"     "that"          "that"          "that"
> [29] "might"         "sort"          "How"
> 
> 
> 
> > > Hi all,
> > >
> > > I have some email addresses that I would like to sort in reverse
> > > lexicographic order so that addresses from the same domain will be
> > > grouped together. How might that be done?
> > >
> > > Murray
> > >
> > > --
> > > Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
> > > Department of Statistics, University of Waikato, Hamilton, New Zealand
> > > Email: maj at waikato.ac.nz                                Fax 7 838 4155
> > > Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > >
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rossini at blindglobe.net  Mon Dec 15 07:09:49 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Sun, 14 Dec 2003 22:09:49 -0800
Subject: [R] help with random numbers and Rmpi
In-Reply-To: <Pine.LNX.4.58.0312150022281.940@Chrestomanci> (Faheem Mitha's
	message of "Mon, 15 Dec 2003 01:03:45 -0500 (EST)")
References: <Pine.LNX.4.58.0312011520250.26318@Chrestomanci>
	<85fzg4jmgz.fsf@blindglobe.net>
	<Pine.LNX.4.58.0312011552290.26318@Chrestomanci>
	<853cc4jkud.fsf@blindglobe.net>
	<Pine.LNX.4.58.0312011627380.26318@Chrestomanci>
	<85llpwi019.fsf@blindglobe.net>
	<Pine.LNX.4.58.0312150022281.940@Chrestomanci>
Message-ID: <853cbmvcgi.fsf@blindglobe.net>


Did you make sure that you loaded the library that contains the C
code on the remote hosts? 

best,
-tony




Faheem Mitha <faheem at email.unc.edu> writes:

> On Mon, 1 Dec 2003, A.J. Rossini wrote:
>
>> Faheem Mitha <faheem at email.unc.edu> writes:
>>
>> > So, can this (parallelization at the C level) be done without running a
>> > bunch of C slaves along the lines I had previously written? Any examples
>> > would be helpful.
>>
>> How much heavy lifting happens before you spawn the slaves, and can
>> that not be moved to R?
>>
>> Your best bet is to read the SNOW code for handling SPRNG/RSPRNG,
>> otherwise.
>
> I've tried to use snow as suggested. I have a R function mg.randvec which
> generates a vector of random variates. This function calls a C routine via
> the .C call. This works fine if I call it like say...
>
> *********************************************************************
>> mg.randvec(3,2,10,5)
> $val
>  [1] -1.9967464 -1.8634205 -0.7459255 -1.7591047 -1.7811685 -1.9953316
>  [7] -1.7932502 -1.9823565 -1.7999789 -1.0501179 -1.9679886  0.1484859
> [13]  0.5768898  1.9117889  1.9366872 -1.3847453 -1.5554107 -1.4933195
> [19] -1.8508795 -1.6715850 -1.8951212 -1.8900167 -1.1630852 -1.3989748
> [25] -1.9400337 -1.6774471 -1.8136065 -1.8685709 -1.9119879 -1.3378416
> *********************************************************************
>
> However, with snow I get
>
> **********************************************************************
>> clusterCall(cl,mg.randvec,3,2,10,5)
> [[1]]
> [1] "Error in .C(\"rocftp\", as.integer(k), as.integer(len),
> as.double(theta),  : \n\tC/Fortran function name not in load table\n"
> attr(,"class")
> [1] "try-error"
>
> [[2]]
> [1] "Error in .C(\"rocftp\", as.integer(k), as.integer(len),
> as.double(theta),  : \n\tC/Fortran function name not in load table\n"
> attr(,"class")
> [1] "try-error"
>
> [[3]]
> [1] "Error in .C(\"rocftp\", as.integer(k), as.integer(len),
> as.double(theta),  : \n\tC/Fortran function name not in load table\n"
> attr(,"class")
> [1] "try-error"
> ********************************************************************
>
> In the cluster case it seems to have difficulty loading up the C routine.
>
> I think snow is working Ok, because basic examples like the following
> work.
>
> ********************************************************************
>> clusterCall(cl,runif,3)
> [[1]]
> [1] 0.1527429 0.1134621 0.8663094
>
> [[2]]
> [1] 0.2256776 0.8981241 0.1120226
>
> [[3]]
> [1] 0.2371450 0.5090693 0.2776081
> ******************************************************************
>
> Can anyone tell me what I am doing wrong? All data files is shared across
> all three machines I am using (AFS space).
>
> Thanks in advance.
>
>                                                             Faheem.
>

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From ccleland at optonline.net  Mon Dec 15 08:35:32 2003
From: ccleland at optonline.net (Chuck Cleland)
Date: Mon, 15 Dec 2003 02:35:32 -0500
Subject: [R] pca
In-Reply-To: <Pine.LNX.4.44.0312150710070.15049-100000@gannet.stats>
References: <Pine.LNX.4.44.0312150710070.15049-100000@gannet.stats>
Message-ID: <3FDD6444.7070503@optonline.net>

Prof Brian Ripley wrote:
> On Sun, 14 Dec 2003, Douglas Trainor wrote:
> 
> 
>>Somewhere along the line, you have been confused.
>>You're in good company though.  Factor analysis 
>>and PCA are different entities entirely.
> 
> 
> Not in SPSS, where the same command is used for both (although we were not 
> told anything like enough about what was done).
> 
> *However* I don't know what is meant by `pca in R'. Standard R contains
> princomp() and prcomp() to do PCA: it appears that pca() is in the
> orphaned package multiv. Why not use the standard functions?  Also, those
> non-default arguments would appear to be very unusual indeed for PCA, and
> do not correspond to a *correlation* matrix.

   It looks like the original poster might have used the pca() in 
the pcurve package, which seems to require a data matrix, not a 
correlation matrix.

>>Any clues about this diference?
> 
> 
> User error looks likely.  Please seek out local statistical expertise.
> 
> 
>>>Dear listmates, i've done a pca analisys in R (1.8 v.) 
>>>with the command
>>>
>>>pca(Matrix, cent=FALSE, scle=FALSE)  
>>>
>>>I have obtained a v matrix very different from the  
>>>component matrix resulted by a factor analysis in SPSS, 
>>>unrotated and with a extraction from a correlation 
>>>matrix. Any clues about this diference?

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From faheem at email.unc.edu  Mon Dec 15 09:18:40 2003
From: faheem at email.unc.edu (Faheem Mitha)
Date: Mon, 15 Dec 2003 03:18:40 -0500 (EST)
Subject: [R] help with random numbers and Rmpi
In-Reply-To: <853cbmvcgi.fsf@blindglobe.net>
References: <Pine.LNX.4.58.0312011520250.26318@Chrestomanci>
	<85fzg4jmgz.fsf@blindglobe.net>
	<Pine.LNX.4.58.0312011552290.26318@Chrestomanci>
	<853cc4jkud.fsf@blindglobe.net>
	<Pine.LNX.4.58.0312011627380.26318@Chrestomanci>
	<85llpwi019.fsf@blindglobe.net>
	<Pine.LNX.4.58.0312150022281.940@Chrestomanci>
	<853cbmvcgi.fsf@blindglobe.net>
Message-ID: <Pine.LNX.4.58.0312150305060.940@Chrestomanci>



On Sun, 14 Dec 2003, A.J. Rossini wrote:

>
> Did you make sure that you loaded the library that contains the C
> code on the remote hosts?

No, I only loaded the library on the local node (master).

However, I'm not sure how I should do so. If I simply start up R on the
remote hosts and load up the library, I presume this would not be any use,
since the master spawns its own R slaves?

So, the question would be how to get the R slaves to load up the library
and I don't see any obvious way of doing this (using snow functions).

I must be missing something. Do I need to work at a lower level?

Thanks for replying.

                                                          Faheem.



From rossini at blindglobe.net  Mon Dec 15 09:28:28 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Mon, 15 Dec 2003 00:28:28 -0800
Subject: [R] help with random numbers and Rmpi
In-Reply-To: <Pine.LNX.4.58.0312150305060.940@Chrestomanci> (Faheem Mitha's
	message of "Mon, 15 Dec 2003 03:18:40 -0500 (EST)")
References: <Pine.LNX.4.58.0312011520250.26318@Chrestomanci>
	<85fzg4jmgz.fsf@blindglobe.net>
	<Pine.LNX.4.58.0312011552290.26318@Chrestomanci>
	<853cc4jkud.fsf@blindglobe.net>
	<Pine.LNX.4.58.0312011627380.26318@Chrestomanci>
	<85llpwi019.fsf@blindglobe.net>
	<Pine.LNX.4.58.0312150022281.940@Chrestomanci>
	<853cbmvcgi.fsf@blindglobe.net>
	<Pine.LNX.4.58.0312150305060.940@Chrestomanci>
Message-ID: <854qw24h8z.fsf@blindglobe.net>

Faheem Mitha <faheem at email.unc.edu> writes:

> On Sun, 14 Dec 2003, A.J. Rossini wrote:
>
>>
>> Did you make sure that you loaded the library that contains the C
>> code on the remote hosts?
>
> No, I only loaded the library on the local node (master).
>
> However, I'm not sure how I should do so. If I simply start up R on the
> remote hosts and load up the library, I presume this would not be any use,
> since the master spawns its own R slaves?

No, it would be of use.  The approach is to  run multiple independent
processes -- this is the general "message-passing" paradigm, and holds
true regardless of the particular API being used.  So each R process
has to be appropriately initialized.   SNOW is taking care of the RNG,
but you've got to do everything else.

> So, the question would be how to get the R slaves to load up the library
> and I don't see any obvious way of doing this (using snow functions).
>
> I must be missing something. Do I need to work at a lower level?

Yes, and no.

Re-read the CPH-statcomp lab, and look at the bootstrap example, which
solves the same problem.

Look carefully -- it has to initialize the library on each node.
If you are just loading the library manually, just do it on each node;
if you are using libraries, just do that.

best,
-tony

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From rossini at blindglobe.net  Mon Dec 15 09:59:47 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Mon, 15 Dec 2003 00:59:47 -0800
Subject: [R] help with random numbers and Rmpi
In-Reply-To: <854qw24h8z.fsf@blindglobe.net> (A. J. Rossini's message of
	"Mon, 15 Dec 2003 00:28:28 -0800")
References: <Pine.LNX.4.58.0312011520250.26318@Chrestomanci>
	<85fzg4jmgz.fsf@blindglobe.net>
	<Pine.LNX.4.58.0312011552290.26318@Chrestomanci>
	<853cc4jkud.fsf@blindglobe.net>
	<Pine.LNX.4.58.0312011627380.26318@Chrestomanci>
	<85llpwi019.fsf@blindglobe.net>
	<Pine.LNX.4.58.0312150022281.940@Chrestomanci>
	<853cbmvcgi.fsf@blindglobe.net>
	<Pine.LNX.4.58.0312150305060.940@Chrestomanci>
	<854qw24h8z.fsf@blindglobe.net>
Message-ID: <85y8te318c.fsf@blindglobe.net>


Whoops.  Exhaustion alert.  The point is that R gets started up on the
remote hosts, but you need to initialize the library on them.  So you
don't have to manually start up R on the remote hosts, but you do have
to tell the remote sessions what to compute (including loading any
libraries, etc).

best,
-tony 


rossini at blindglobe.net (A.J. Rossini) writes:

> Faheem Mitha <faheem at email.unc.edu> writes:
>
>> On Sun, 14 Dec 2003, A.J. Rossini wrote:
>>
>>>
>>> Did you make sure that you loaded the library that contains the C
>>> code on the remote hosts?
>>
>> No, I only loaded the library on the local node (master).
>>
>> However, I'm not sure how I should do so. If I simply start up R on the
>> remote hosts and load up the library, I presume this would not be any use,
>> since the master spawns its own R slaves?
>
> No, it would be of use.  The approach is to  run multiple independent
> processes -- this is the general "message-passing" paradigm, and holds
> true regardless of the particular API being used.  So each R process
> has to be appropriately initialized.   SNOW is taking care of the RNG,
> but you've got to do everything else.
>
>> So, the question would be how to get the R slaves to load up the library
>> and I don't see any obvious way of doing this (using snow functions).
>>
>> I must be missing something. Do I need to work at a lower level?
>
> Yes, and no.
>
> Re-read the CPH-statcomp lab, and look at the bootstrap example, which
> solves the same problem.
>
> Look carefully -- it has to initialize the library on each node.
> If you are just loading the library manually, just do it on each node;
> if you are using libraries, just do that.
>
> best,
> -tony
>
> -- 
> rossini at u.washington.edu            http://www.analytics.washington.edu/ 
> Biomedical and Health Informatics   University of Washington
> Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
> UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
> FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email
>
> CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From chenu at ensam.inra.fr  Mon Dec 15 11:01:40 2003
From: chenu at ensam.inra.fr (chenu)
Date: Mon, 15 Dec 2003 11:01:40 +0100
Subject: [R] nls arguments
Message-ID: <3FDD8684.5060202@ensam.inra.fr>

Hi all,

I've got a problem with the nls function.
I have an adjustment which works when I fix one of the argument of my 
function (Xo=150) :

*Xo*=150
f<- function (tt*,Xo*,a,b)    ifelse(tt<*Xo*,a*exp(-b**Xo*),a*exp(-b*tt))
ajust<-nls(RER~f(tt,*Xo*,a,b),data=data.frame(tt=Ph2[,2*k],RER=Ph2[,2*k+1]),start=list(a=0.5,b=0.014))

But, when I use it as a "normal" parameter (and even if I do not I use 
it in the equation of my "f" function), it does not work  :

      f<- function (tt,*Xo*,a,b)    ifelse(tt<150,a*exp(-b*tt),a*exp(-b*tt))
or : f<- function (tt,*Xo*,a,b)    
ifelse(tt<*Xo*,a*exp(-b**Xo*),a*exp(-b*tt))              #it is this 
equation I would like to resole
ajust<-nls(RER~f(tt,*Xo*,a,b),data=data.frame(tt=Ph2[,2*k],RER=Ph2[,2*k+1]),start=list(a=0.5,b=0.014,*Xo*=150))
/Error in nlsModel(formula, mf, start) : singular gradient matrix at 
initial parameter estimates/



Would you have any idea to help me.

Thanks a lot

Karine Chenu



From vito.muggeo at giustizia.it  Mon Dec 15 11:39:32 2003
From: vito.muggeo at giustizia.it (Vito Muggeo)
Date: Mon, 15 Dec 2003 11:39:32 +0100
Subject: R: [R] nls arguments
References: <3FDD8684.5060202@ensam.inra.fr>
Message-ID: <001801c3c2f7$bd861ea0$5c13070a@PROCGEN>

Dear chenu,
I am not going to see your code with attention (also I do not understand the
`*' symbol you used), however it looks a changepoint-type problem.

The package segmented (on CRAN) uses a piecewise linear parameterization to
fit regression models with breakpoints.

Hope this helps,

best, vito



----- Original Message -----
From: chenu <chenu at ensam.inra.fr>
To: <R-help at stat.math.ethz.ch>
Sent: Monday, December 15, 2003 11:01 AM
Subject: [R] nls arguments


> Hi all,
>
> I've got a problem with the nls function.
> I have an adjustment which works when I fix one of the argument of my
> function (Xo=150) :
>
> *Xo*=150
> f<- function (tt*,Xo*,a,b)    ifelse(tt<*Xo*,a*exp(-b**Xo*),a*exp(-b*tt))
>
ajust<-nls(RER~f(tt,*Xo*,a,b),data=data.frame(tt=Ph2[,2*k],RER=Ph2[,2*k+1]),
start=list(a=0.5,b=0.014))
>
> But, when I use it as a "normal" parameter (and even if I do not I use
> it in the equation of my "f" function), it does not work  :
>
>       f<- function (tt,*Xo*,a,b)
ifelse(tt<150,a*exp(-b*tt),a*exp(-b*tt))
> or : f<- function (tt,*Xo*,a,b)
> ifelse(tt<*Xo*,a*exp(-b**Xo*),a*exp(-b*tt))              #it is this
> equation I would like to resole
>
ajust<-nls(RER~f(tt,*Xo*,a,b),data=data.frame(tt=Ph2[,2*k],RER=Ph2[,2*k+1]),
start=list(a=0.5,b=0.014,*Xo*=150))
> /Error in nlsModel(formula, mf, start) : singular gradient matrix at
> initial parameter estimates/
>
>
>
> Would you have any idea to help me.
>
> Thanks a lot
>
> Karine Chenu
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From shintomasaka at yahoo.com  Mon Dec 15 11:35:48 2003
From: shintomasaka at yahoo.com (Shin Tomoe)
Date: Mon, 15 Dec 2003 02:35:48 -0800 (PST)
Subject: [R] Missing Tcl.h in installation
Message-ID: <20031215103549.61195.qmail@web60303.mail.yahoo.com>

Dear Sir,

I am a user of R, and I just downloaded rw1081.exe
from your site.

When I install the said file, it reports that a file
is missing.  I dont know the cause. Its the only
mising file because when I press ignore, everyting
installs just fine.


Anyways, Sir, can you send me the lacking file:

Tcl.h, in include/tcl/.


Thank you and hoping for your kind consideration.

=====
Gerard Lee

gerarldlee at yahoo.com
http://www.gerardlee.tk



From JonesW at kssg.com  Mon Dec 15 12:00:50 2003
From: JonesW at kssg.com (Wayne Jones)
Date: Mon, 15 Dec 2003 11:00:50 -0000
Subject: [R] Week of the Year date conversion
Message-ID: <6B5A9304046AD411BD0200508BDFB6CB021F0EF7@gimli.middleearth.kssg.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031215/fc2814f7/attachment.pl

From ripley at stats.ox.ac.uk  Mon Dec 15 12:22:47 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 15 Dec 2003 11:22:47 +0000 (GMT)
Subject: [R] Week of the Year date conversion
In-Reply-To: <6B5A9304046AD411BD0200508BDFB6CB021F0EF7@gimli.middleearth.kssg.com>
Message-ID: <Pine.LNX.4.44.0312151120100.696-100000@gannet.stats>

On Mon, 15 Dec 2003, Wayne Jones wrote:

> 
> Hello there fellow R-users, 
> 
> I have received some data which comes in the following format: 
> 
> example1<-"200301"
> 
> The first 4 digits correspond to the year and the remaining 2 digits
> correspond to the week of the year. 
> I have tried to convert this to a date by using strptime as follows:
> 
> strptime(example1,format="%Y%U")
> 
> where U (looking up strptime) is the week of the year but it always returns
> NA. 

That is not a complete date!   Which day of the week is it?

> example1<-"200301"
> strptime(paste(example1, "1", format="%Y%U %d")

will work, and you need to do something like that to resolve the 
ambiguity.

Brian

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From pallier at lscp.ehess.fr  Mon Dec 15 12:24:10 2003
From: pallier at lscp.ehess.fr (Christophe Pallier)
Date: Mon, 15 Dec 2003 12:24:10 +0100
Subject: [R] density plot for very large dataset
Message-ID: <3FDD99DA.80901@lscp.ehess.fr>

Have you tried the 'sm.density' function from the sm library?
I used it for a dataset which 'only' had 130000 points.

> I'm new to R and am trying to perform a simple, yet 
> problematic task.  I 
> have two variables for which I would like to measure the 
> correlation and 
> plot versus each other.  However, I have ~30 million data points 
> measurements of each variable.  I can read this into R from file and 
> produce a plot with plot(x0, x1) but as you would expect, its 
> not pretty 
> to look at and produces a postscript file of about 700MB.  

Christophe Pallier
http://www.pallier.org



From spencer.graves at pdf.com  Mon Dec 15 12:39:49 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 15 Dec 2003 03:39:49 -0800
Subject: [R] help in lme
In-Reply-To: <20031215041712.RVXO24575.lakemtao06.cox.net@smtp.central.cox.net>
References: <20031215041712.RVXO24575.lakemtao06.cox.net@smtp.central.cox.net>
Message-ID: <3FDD9D85.9000002@pdf.com>

I can't answer all your questions right now, but I can answer the first: 

      Have you considered "gamma":  n! = gamma(n+1)

 > gamma(1+1:6)
[1]   1   2   6  24 120 720

      Regarding the other two, have you consulted Pinhiero and Bates 
(2000) Mixed-Effects Models in S and S-Plus (Springer)?  Bates and his 
graduate student (including Pinhiero) developed most of mle, and I have 
found the book to be quite valuable -- if not essential for using lme. 

      hope this helps.
      spencer graves

cbotts1 at cox.net wrote:

>To anyone who can help,
>
>  I have two stupid questions, and one fairly intelligent question
>
>Stupid question (1):   is there an R function to calculate a factorial of a number?    That is...is there a function g(.) such that g(3) = 6, g(4) = 24, g(6) = 720, etc?
>
>Stupid question (2):  how do you extract the estimated covariance matrix of the random effects in an lme object?
>
>
>Intelligent question (1)  I keep on trying to fit a linear mixed model in R using 'lme(y~fxd.dsgn, data = data.mtrx, ~rnd.dsgn|group)' where fxd.dsgn and rnd.dsgn are the fixed and random design matrices, respectively.   The function won't work, though.   It keeps telling me that it can't find the object 'rnd.dsgn'.    What's the matter here?
>
>Any help would be greatly appreciated.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From B.Rowlingson at lancaster.ac.uk  Mon Dec 15 12:43:04 2003
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 15 Dec 2003 11:43:04 +0000
Subject: [R] Week of the Year date conversion
In-Reply-To: <Pine.LNX.4.44.0312151120100.696-100000@gannet.stats>
References: <Pine.LNX.4.44.0312151120100.696-100000@gannet.stats>
Message-ID: <3FDD9E48.1090007@lancaster.ac.uk>

Prof Brian Ripley wrote:

> That is not a complete date!   Which day of the week is it?
> 
> 
>>example1<-"200301"
>>strptime(paste(example1, "1", format="%Y%U %d")
> 
> 
> will work, and you need to do something like that to resolve the 
> ambiguity.
> 

  Doesn't seem to work:

  - 6th day of 5th week:

 > strptime("2003 05 06", format="%Y %U %d")
[1] "2003-01-06"

  Ah ha. Use '%w' for 'day of week':

 > strptime("2003 05 06", format="%Y %U %w")
[1] "2003-02-02"

  I dont have a calendar to hand to check that 2 Feb is the 6th day of 
the 5th week....

Barry



From berwin at maths.uwa.edu.au  Mon Dec 15 13:02:27 2003
From: berwin at maths.uwa.edu.au (Berwin Turlach)
Date: Mon, 15 Dec 2003 20:02:27 +0800
Subject: [R] Week of the Year date conversion
In-Reply-To: <3FDD9E48.1090007@lancaster.ac.uk>
References: <Pine.LNX.4.44.0312151120100.696-100000@gannet.stats>
	<3FDD9E48.1090007@lancaster.ac.uk>
Message-ID: <16349.41683.583048.582829@localhost.localdomain>

>>>>> "BR" == Barry Rowlingson <B.Rowlingson at lancaster.ac.uk> writes:

    BR> Ah ha. Use '%w' for 'day of week':

    >> strptime("2003 05 06", format="%Y %U %w")
    BR> [1] "2003-02-02"

    BR> I dont have a calendar to hand to check that 2 Feb is the 6th
    BR> day of the 5th week....
It's not, it is the first day of the 5th week in 2003.  Moreover:

> strptime(paste("2003 05 0", 0:6, sep=""), format="%Y %U %w")
[1] "2003-02-02" "2003-02-02" "2003-02-02" "2003-02-02" "2003-02-02"
[6] "2003-02-02" "2003-02-02"

I guess this behaviour is consistent with the description of %U:
 
    `%U' Week of the year as decimal number (00-53) using the first
          Sunday as day 1 of week 1.

??

> strptime(paste("2003 01 0", 0:6, sep=""), format="%Y %U %w")
[1] "2003-01-05" "2003-01-05" "2003-01-05" "2003-01-05" "2003-01-05"
[6] "2003-01-05" "2003-01-05"

5 January 2003 was the first Sunday in 2003.

Cheers,

        Berwin



From ripley at stats.ox.ac.uk  Mon Dec 15 13:05:14 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 15 Dec 2003 12:05:14 +0000 (GMT)
Subject: [R] Week of the Year date conversion
In-Reply-To: <3FDD9E48.1090007@lancaster.ac.uk>
Message-ID: <Pine.LNX.4.44.0312151201200.925-100000@gannet.stats>

On Mon, 15 Dec 2003, Barry Rowlingson wrote:

> Prof Brian Ripley wrote:
> 
> > That is not a complete date!   Which day of the week is it?
> > 
> > 
> >>example1<-"200301"
> >>strptime(paste(example1, "1", format="%Y%U %d")
> > 
> > 
> > will work, and you need to do something like that to resolve the 
> > ambiguity.
> > 
> 
>   Doesn't seem to work:
> 
>   - 6th day of 5th week:
> 
>  > strptime("2003 05 06", format="%Y %U %d")
> [1] "2003-01-06"
> 
>   Ah ha. Use '%w' for 'day of week':

Or %u.  And %U %V %W all mean week of the year in different definitions.
Hence `something like that' (and I meant %u not %d).


>  > strptime("2003 05 06", format="%Y %U %w")
> [1] "2003-02-02"
> 
>   I dont have a calendar to hand to check that 2 Feb is the 6th day of 
> the 5th week....

It is on one definition.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From agustin.perez at umh.es  Mon Dec 15 13:17:28 2003
From: agustin.perez at umh.es (Perez Martin, Agustin)
Date: Mon, 15 Dec 2003 13:17:28 +0100
Subject: [R] Error with spdep
Message-ID: <5AFDDD57E2771B409224CD858CC6DE0D02DAB33F@mailer-e051.umh.es>

Dear useRs:

First of all I would like to thank all the responses.
I've an error with package "spdep".

I am working with a Windows XP machine (AMD-2000-XP RAM-256DDR) and 1.8.0.
R-version and when I try to load spdep appear the following error:

> library(spdep)
Error in loadNamespace(i, c(lib.loc, .libPaths()), keep.source) : 
        There is no package called 'maptools'
Error in library(spdep) : package/namespace load failed

I don't know what is this.
Can you help me?

Best wishes,
Agustin



From p.dalgaard at biostat.ku.dk  Mon Dec 15 13:35:37 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 15 Dec 2003 13:35:37 +0100
Subject: [R] Week of the Year date conversion
In-Reply-To: <16349.41683.583048.582829@localhost.localdomain>
References: <Pine.LNX.4.44.0312151120100.696-100000@gannet.stats>
	<3FDD9E48.1090007@lancaster.ac.uk>
	<16349.41683.583048.582829@localhost.localdomain>
Message-ID: <x23cbml0me.fsf@biostat.ku.dk>

Berwin Turlach <berwin at maths.uwa.edu.au> writes:

> >>>>> "BR" == Barry Rowlingson <B.Rowlingson at lancaster.ac.uk> writes:
> 
>     BR> Ah ha. Use '%w' for 'day of week':
> 
>     >> strptime("2003 05 06", format="%Y %U %w")
>     BR> [1] "2003-02-02"
> 
>     BR> I dont have a calendar to hand to check that 2 Feb is the 6th
>     BR> day of the 5th week....
> It's not, it is the first day of the 5th week in 2003.  Moreover:
> 
> > strptime(paste("2003 05 0", 0:6, sep=""), format="%Y %U %w")
> [1] "2003-02-02" "2003-02-02" "2003-02-02" "2003-02-02" "2003-02-02"
> [6] "2003-02-02" "2003-02-02"
> 
> I guess this behaviour is consistent with the description of %U:
>  
>     `%U' Week of the year as decimal number (00-53) using the first
>           Sunday as day 1 of week 1.
> 
> ??
> 
> > strptime(paste("2003 01 0", 0:6, sep=""), format="%Y %U %w")
> [1] "2003-01-05" "2003-01-05" "2003-01-05" "2003-01-05" "2003-01-05"
> [6] "2003-01-05" "2003-01-05"

 
Why on earth are you guys trying to use a two-digit field for a
quantity in the range 0:6 ??

 > strptime(paste("2003 01 ", 0:6, sep=""), format="%Y %U %w")
[1] "2003-01-05" "2003-01-06" "2003-01-07" "2003-01-08" "2003-01-09"
[6] "2003-01-10" "2003-01-11"


However, this is ISO week *2*!  Day 1 of Week 1, 2003 was Dec 30,
2002.

Now, arguably we are counting from zero, but watch this:

> strptime(paste("2003 00", 0:6), format="%Y %U %w")
[1] "2002-11-18" "2002-11-19" "2002-11-20" "2003-01-01" "2003-01-02"
[6] "2003-01-03" "2003-01-04"

and further,

> strptime(paste("2003 01", 0:6), format="%Y %U %w")
[1] "2003-01-05" "2003-01-06" "2003-01-07" "2003-01-08" "2003-01-09"
[6] "2003-01-10" "2003-01-11"
> strptime(paste("2003 01", 0:6), format="%Y %W %w")
[1] "2003-01-05" "2003-01-06" "2003-01-07" "2003-01-08" "2003-01-09"
[6] "2003-01-10" "2003-01-11"

> 5 January 2003 was the first Sunday in 2003.


Yeah, but what's the difference between %U and %W then? 

     '%U' Week of the year as decimal number (00-53) using the first
          Sunday as day 1 of week 1.

     '%w' Weekday as decimal number (0-6, Sunday is 0).

     '%W' Week of the year as decimal number (00-53) using the first
          Monday as day 1 of week 1.

Neither of those definitions coincide with ISO, BTW.

(This probably all comes down to OS/libc deficiencies. RedHat 8 in this case)



From angel_lul at hotmail.com  Mon Dec 15 01:40:36 2003
From: angel_lul at hotmail.com (Angel)
Date: Mon, 15 Dec 2003 01:40:36 +0100
Subject: [R] Error with spdep
References: <5AFDDD57E2771B409224CD858CC6DE0D02DAB33F@mailer-e051.umh.es>
Message-ID: <LAW11-OE24OBNMCtpa90000a58a@hotmail.com>

spdep depends on package 'maptools', install it.

----- Original Message -----
From: "Perez Martin, Agustin" <agustin.perez at umh.es>
To: "lista R help (E-mail)" <r-help at stat.math.ethz.ch>
Sent: Monday, December 15, 2003 1:17 PM
Subject: [R] Error with spdep


> Dear useRs:
>
> First of all I would like to thank all the responses.
> I've an error with package "spdep".
>
> I am working with a Windows XP machine (AMD-2000-XP RAM-256DDR) and 1.8.0.
> R-version and when I try to load spdep appear the following error:
>
> > library(spdep)
> Error in loadNamespace(i, c(lib.loc, .libPaths()), keep.source) :
>         There is no package called 'maptools'
> Error in library(spdep) : package/namespace load failed
>
> I don't know what is this.
> Can you help me?
>
> Best wishes,
> Agustin
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From Roger.Bivand at nhh.no  Mon Dec 15 13:47:42 2003
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 15 Dec 2003 13:47:42 +0100 (CET)
Subject: [R] Error with spdep
In-Reply-To: <5AFDDD57E2771B409224CD858CC6DE0D02DAB33F@mailer-e051.umh.es>
Message-ID: <Pine.LNX.4.44.0312151346020.31448-100000@reclus.nhh.no>

On Mon, 15 Dec 2003, Perez Martin, Agustin wrote:

> Dear useRs:
> 
> First of all I would like to thank all the responses.
> I've an error with package "spdep".
> 
> I am working with a Windows XP machine (AMD-2000-XP RAM-256DDR) and 1.8.0.
> R-version and when I try to load spdep appear the following error:
> 
> > library(spdep)
> Error in loadNamespace(i, c(lib.loc, .libPaths()), keep.source) : 
>         There is no package called 'maptools'
> Error in library(spdep) : package/namespace load failed

The namespace of the spdep package depends on the maptools package, which 
you should also install.

Roger

> 
> I don't know what is this.
> Can you help me?
> 
> Best wishes,
> Agustin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From marce_lr at yahoo.com  Mon Dec 15 13:54:12 2003
From: marce_lr at yahoo.com (Marcela XX)
Date: Mon, 15 Dec 2003 04:54:12 -0800 (PST)
Subject: [R] Install Design Package
Message-ID: <20031215125412.3050.qmail@web41603.mail.yahoo.com>

Hi! I need to do a Logistic Regresion.
I installed the Design Package. I tried lrm(... ) but
the program said "Object na.delete is not found".
What can I do?

Thanks!!!
Marcela



From ripley at stats.ox.ac.uk  Mon Dec 15 14:23:57 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 15 Dec 2003 13:23:57 +0000 (GMT)
Subject: [R] Week of the Year date conversion
In-Reply-To: <x23cbml0me.fsf@biostat.ku.dk>
Message-ID: <Pine.LNX.4.44.0312151321210.1079-100000@gannet.stats>

On 15 Dec 2003, Peter Dalgaard wrote:

> Neither of those definitions coincide with ISO, BTW.

ISO 8601 week of the year (as distinct from ISO C where these come from)  
is %V, of course.

R-devel has an extended (relative to 1.8.1) description in ?strptime

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rdiaz at cnio.es  Mon Dec 15 14:33:38 2003
From: rdiaz at cnio.es (Ramon Diaz-Uriarte)
Date: Mon, 15 Dec 2003 14:33:38 +0100
Subject: [R] help in lme
In-Reply-To: <20031215041712.RVXO24575.lakemtao06.cox.net@smtp.central.cox.net>
References: <20031215041712.RVXO24575.lakemtao06.cox.net@smtp.central.cox.net>
Message-ID: <200312151433.38649.rdiaz@cnio.es>

Since Spencer Graves already answered the factorial questions, I'll try to 
answer one of the other two:

On Monday 15 December 2003 05:17, cbotts1 at cox.net wrote:
> To anyone who can help,
>
> Intelligent question (1)  I keep on trying to fit a linear mixed model in R
> using 'lme(y~fxd.dsgn, data = data.mtrx, ~rnd.dsgn|group)' where fxd.dsgn
> and rnd.dsgn are the fixed and random design matrices, respectively.   The
> function won't work, though.   It keeps telling me that it can't find the
> object 'rnd.dsgn'.    What's the matter here?

Is "rnd.dsgn" a variable in "data.mtrx"? That is how I always fit lme models, 
and never encountered the problem you describe.

R.

P.S. "Stupid question # 2" I think has been asked (and answered) several times 
in this list in the past.

>
> Any help would be greatly appreciated.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



-- 
Ram?n D?az-Uriarte
Bioinformatics Unit
Centro Nacional de Investigaciones Oncol?gicas (CNIO)
(Spanish National Cancer Center)
Melchor Fern?ndez Almagro, 3
28029 Madrid (Spain)
Fax: +-34-91-224-6972
Phone: +-34-91-224-6900

http://bioinfo.cnio.es/~rdiaz
PGP KeyID: 0xE89B3462
(http://bioinfo.cnio.es/~rdiaz/0xE89B3462.asc)



From feh3k at spamcop.net  Mon Dec 15 15:06:49 2003
From: feh3k at spamcop.net (Frank E Harrell Jr)
Date: Mon, 15 Dec 2003 09:06:49 -0500
Subject: [R] Install Design Package
In-Reply-To: <20031215125412.3050.qmail@web41603.mail.yahoo.com>
References: <20031215125412.3050.qmail@web41603.mail.yahoo.com>
Message-ID: <20031215090649.7f638790.feh3k@spamcop.net>

On Mon, 15 Dec 2003 04:54:12 -0800 (PST)
Marcela XX <marce_lr at yahoo.com> wrote:

> Hi! I need to do a Logistic Regresion.
> I installed the Design Package. I tried lrm(... ) but
> the program said "Object na.delete is not found".
> What can I do?
> 
> Thanks!!!
> Marcela

The package has a lot of documentation.  Please read it carefully.  One
thing you will see is the need to also install the Hmisc package, and you
should have gotten an error message when Design tried to access Hmisc.

---
Frank E Harrell Jr   Professor and Chair           School of Medicine
                     Department of Biostatistics   Vanderbilt University



From maechler at stat.math.ethz.ch  Mon Dec 15 15:14:00 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 15 Dec 2003 15:14:00 +0100
Subject: [R] cutree with agnes
In-Reply-To: <Pine.GSO.3.95q.1031211145123.398M-100000@sun35.math.uni-hamburg.de>
References: <Pine.GSO.3.95q.1031211145123.398M-100000@sun35.math.uni-hamburg.de>
Message-ID: <16349.49576.178724.69205@gargle.gargle.HOWL>

       [diverted from R-help to R-devel; please follow up on R-devel!]

>>>>> "ChrisH" == Christian Hennig <fm3a004 at math.uni-hamburg.de>
>>>>>     on Thu, 11 Dec 2003 15:13:52 +0100 (MET) writes:

    ChrisH> Hi, this is rather a (presumed) bug report than a
    ChrisH> question because I can solve my personal statistical
    ChrisH> problem by working with hclust instead of agnes.

    ChrisH> I have done a complete linkage clustering on a dist
    ChrisH> object dm with 30 objects with agnes (R 1.8.0 on
    ChrisH> RedHat) and I want to obtain the partition that
    ChrisH> results from a cut at height=0.4.

    ChrisH> I run

    >> cl1a <- agnes(dm, method="complete");  cutree(cl1a,h=0.4)
    ChrisH>  [1] 1 2 3 4 5 6 3 7 3 8 9 10 3 11 12 13 14 15 3 16 17 3 18 19 20
    ChrisH> [26] 21 3 22 18 23

    ChrisH> But that's not true; correct is the solution
    ChrisH> obtained from hclust
    >> clx <- hclust(dm); cutree(clx,h=0.4)
    ChrisH>  [1] 1 2 1 2 3 4 1 2 1 3 4 5 1 4 6 7 8 4 1 5 2 1 9 2 2
    ChrisH> [26] 10 1 9 9 11

    ChrisH> as can be seen from the dendrogram plots of hclust
    ChrisH> *and* agnes.  (Note that the dendrograms of hclust
    ChrisH> and agnes are not identical due to the handling of
    ChrisH> ties in the distances, but the difference between
    ChrisH> the agnes and hclust dendrogram at h=0.4 concerns
    ChrisH> only two points.)  Specifying k instead of h in
    ChrisH> cutree for agnes seems to work properly, but that's
    ChrisH> not what I need in the general case.

If I lookup the help page for cutree, agnes and agnes.object,
nothing says that you can expect cutree to work with agnes
objects directly.  
On the contrary,  ?cutree  says about its first argument

    tree: a tree as produced by 'hclust'. 'cutree()' only expects a
          list with components 'merge', 'height', and 'labels', of
          appropriate content each.

and   ?agnes.object  mentions the   as.hclust() function that's
needed to produce an "hclust"-like object from the result of
agnes() {or diana()}.

Summarizing,
  1) You need      
     cutree(as.hclust(cl1a), h=0.4)

  2) cutree() shouldn't silently return a wrong result for agnes
     (or diana) objects.  Rather, it should return the proper thing
     or give an error.

Here I elaborate a bit on "2)" which is not entirely trivial --
hence the diversion to R-devel.
The best approach would be to make cutree() a generic function
with the `obvious' "hclust" & "twins" methods and a "default"
method which just uses something like NextMethod( as.hclust() ..).

However this breaks back-compatibility: cutree() may not work
anymore on user-constructed objects that are just list()s as
described for `tree' above.

We could alleviate this problem by try to make
as.hclust.default() much smarter, but I would tend to try not to
do it and let other people write their own as.hclust.* methods
for their own constructed objects.

Does this seem viable?  If I don't hear protest, I'll eventually
try to do this (in R-devel).

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From wegmann_mailinglist at gmx.net  Mon Dec 15 15:53:16 2003
From: wegmann_mailinglist at gmx.net (Martin Wegmann)
Date: Mon, 15 Dec 2003 15:53:16 +0100 (MET)
Subject: [R] packages for ecologists - summary
References: <200312151449.hBFEn9Q19583@mailgate5.cinetic.de>
Message-ID: <8291.1071499996@www66.gmx.net>

Dear R user,     
    
I asked for packages which should be mentioned in a ecological related     
presentation of R and would like to summarize the replies.    
    
First of all E. Paradis pointed out, that he would prefer to see R presented
   
in a different way, please read his whole reply https://    
www.stat.math.ethz.ch/pipermail/r-help/2003-December/041951.html.     
    
Packages which has been recommended are:    
    
WISP    
http://www.ruwpa.st-and.ac.uk/estimating.abundance/WiSP/    
    
ape    
www.stat.uni-muenchen.de/~strimmer/ publications/ape2003.pdf    
    
phylogr   

http://www.stat.ucl.ac.be/ISdidactique/Rhelp/library/PHYLOGR/html/00Index.html 
   
    
genetics    
cran.r-project.org/doc/packages/genetics.pdf     
    
VEGAN    
http://cc.oulu.fi/~jarioksa/softhelp/vegan.html    
    
ADE4    
http://pbil.univ-lyon1.fr/ADE-4/ADE-4.html    
    
Spatstat    
http://www.maths.uwa.edu.au/~adrian/spatstat.html    
    
geoR    
http://www.est.ufpr.br/geoR/    
    
grasp-R    
http://www.fivaz.ch/grasper/index.html    
    
nlme    
packages.r-project.org/nlme/     
    
and a lot of the packages already included in the R base installation like  

  
glm, gam (in mgcv), glm.nb (in MASS), glmmPQL (in MASS)    
    
thanks a lot to the contributers, regards Martin    
    
    
On Tuesday 09 December 2003 21:26, Martin Wegmann wrote:    
> Hello R-user,    
>    
> sorry for this very off-topic question.    
>    
> But I shall present R to my dept. (pro's and con's and what it can do).   

> The pro's and con's are easy but not what R can do (additional to the    
> "normal" statistics).    
> I looked through the packages, but the enormous amount of packages makes 
it    
> very difficult for me to decide which one is worth mentioning.    
>    
> I used only a small part of all R packages (mainly recommended packages 
and    
> grasper) and would like to know which package for ecologist has to be    
> mentioned.    
>    
> I would greatly appreciate if you can tell me which packages you think are

   
> very useful for ecolgical research in R e.g. vegan, ade4, ...    
>    
> thanks in advance, regards Martin    
>    
> ______________________________________________    
> R-help at stat.math.ethz.ch mailing list    
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help    
    
    
   
  


______________________________________________________________________________ 
Horoskop, Comics, VIPs, Wetter, Sport und Lotto im WEB.DE Screensaver1.2 

 

-- 

Neu: Preissenkung f?r MMS und FreeMMS! http://www.gmx.net



From James_A_Rogers at groton.pfizer.com  Mon Dec 15 16:08:50 2003
From: James_A_Rogers at groton.pfizer.com (Rogers, James A [PGRD Groton])
Date: Mon, 15 Dec 2003 10:08:50 -0500
Subject: [R] minor documentation typo ?
Message-ID: <C735670CCC69D61193DA0002A58EE9900AEB7404@groexmb07.pfizer.com>


Hi, 

I believe the following is a very minor typo in the documentation for
merge() : 

all logical; all=L is shorthand for all.x=L and all.y=L
                 ^                        ^           ^ 
Looks like 'L' should be 'FALSE'.

Someone please alert me if I am wrong. If I am not wrong, is it appropriate
to submit a bug for such minor detail? 

Thanks,
Jim 

James A. Rogers 
Senior Coordinator, Biometrics
PGR&D Groton Labs
Eastern Point Road (MS 8260-1331)
Groton, CT 06340
office: (860) 686-0786
fax: (860) 715-5445
  

 


LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}



From tlumley at u.washington.edu  Mon Dec 15 16:15:02 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 15 Dec 2003 07:15:02 -0800 (PST)
Subject: [R] Basic question on function "identical"
In-Reply-To: <XFMail.031213133142.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.031213133142.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.A41.4.58.0312150711210.58588@homer05.u.washington.edu>

On Sat, 13 Dec 2003 Ted.Harding at nessie.mcc.ac.uk wrote:

> On 13-Dec-03 Martin Maechler wrote:
> > In general, use
> >
> >   ==       for testing equality of integer numbers (of type "integer"
> >            or not)
>
> I hope this is not a suggestion to avoid usage like
>
>   which(x == max(x))
>
> when x is a vector of reals? (i.e. should be OK when you know that
> the thing on one side of == should be an exact copy in its internal
> representation of the thing on the other side, if equality in the
> usual sense holds).
>
> In other words, I hope that (for instance) max(x) does not differ
> internally from whichever of x[1] , ... , x[N] has the largest value ...
>

One reason that which.max() exists is that we cannot guarantee
which(x==max(x)) to work. It is possible, though rather unlikely, for
there to be no x such that x==max(x).  One reason is the unpredictable
use of 10-byte wide floating point registers on Intel chips.


	-thomas



From maechler at stat.math.ethz.ch  Mon Dec 15 16:34:58 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 15 Dec 2003 16:34:58 +0100
Subject: [R] minor documentation typo ?
In-Reply-To: <C735670CCC69D61193DA0002A58EE9900AEB7404@groexmb07.pfizer.com>
References: <C735670CCC69D61193DA0002A58EE9900AEB7404@groexmb07.pfizer.com>
Message-ID: <16349.54434.193956.813691@gargle.gargle.HOWL>

>>>>> "JARogers" == Rogers, James A [PGRD Groton] <Rogers>
>>>>>     on Mon, 15 Dec 2003 10:08:50 -0500 writes:

    JARogers> I believe the following is a very minor typo in
    JARogers> the documentation for merge() :

    JARogers> all logical; all=L is shorthand for all.x=L and all.y=L
    JARogers>                  ^                        ^           ^ 
    JARogers> Looks like 'L' should be 'FALSE'.

no,  L \in {TRUE, FALSE}  (and I had chosen 'L' for 'Logical').

    JARogers> Someone please alert me if I am wrong. If I am not
    JARogers> wrong, is it appropriate to submit a bug for such
    JARogers> minor detail?

Yes, it is appropriate, (at least when there *is* a bug :-)

Martin



From nali at biostat.umn.edu  Mon Dec 15 17:50:38 2003
From: nali at biostat.umn.edu (Na Li)
Date: Mon, 15 Dec 2003 10:50:38 -0600
Subject: [R] help with random numbers and Rmpi
In-Reply-To: <Pine.LNX.4.58.0312150305060.940@Chrestomanci> (Faheem Mitha's
	message of "Mon, 15 Dec 2003 03:18:40 -0500 (EST)")
References: <Pine.LNX.4.58.0312011520250.26318@Chrestomanci>
	<85fzg4jmgz.fsf@blindglobe.net>
	<Pine.LNX.4.58.0312011552290.26318@Chrestomanci>
	<853cc4jkud.fsf@blindglobe.net>
	<Pine.LNX.4.58.0312011627380.26318@Chrestomanci>
	<85llpwi019.fsf@blindglobe.net>
	<Pine.LNX.4.58.0312150022281.940@Chrestomanci>
	<853cbmvcgi.fsf@blindglobe.net>
	<Pine.LNX.4.58.0312150305060.940@Chrestomanci>
Message-ID: <foy8tenhy9.fsf@orca.local>

On 15 Dec 2003, Faheem Mitha stated:
>  
>  So, the question would be how to get the R slaves to load up the library
>  and I don't see any obvious way of doing this (using snow functions).

Say you need boot on each slave, call this on the master,

clusterEvalQ (cl, library (boot))

where cl is the cluster created by make*Cluster () functions from snow.

This is in the manual.

Michael

-- 
Na (Michael) Li, Ph.D.
Assistant Professor
Division of Biostatistics, University of Minnesota
A443 Mayo Building, MMC 303     Phone: (612) 626-4765
420 Delaware St SE              Fax:   (612) 626-0660
Minneapolis, MN 55455           Web: http://www.biostat.umn.edu/~nali



From faheem at email.unc.edu  Mon Dec 15 18:00:14 2003
From: faheem at email.unc.edu (Faheem Mitha)
Date: Mon, 15 Dec 2003 12:00:14 -0500 (EST)
Subject: [R] help with random numbers and Rmpi
In-Reply-To: <854qw24h8z.fsf@blindglobe.net>
References: <Pine.LNX.4.58.0312011520250.26318@Chrestomanci>
	<85fzg4jmgz.fsf@blindglobe.net>
	<Pine.LNX.4.58.0312011552290.26318@Chrestomanci>
	<853cc4jkud.fsf@blindglobe.net>
	<Pine.LNX.4.58.0312011627380.26318@Chrestomanci>
	<85llpwi019.fsf@blindglobe.net>
	<Pine.LNX.4.58.0312150022281.940@Chrestomanci>
	<853cbmvcgi.fsf@blindglobe.net>
	<Pine.LNX.4.58.0312150305060.940@Chrestomanci>
	<854qw24h8z.fsf@blindglobe.net>
Message-ID: <Pine.LNX.4.58.0312151156340.940@Chrestomanci>



On Mon, 15 Dec 2003, A.J. Rossini wrote:

> Yes, and no.
>
> Re-read the CPH-statcomp lab, and look at the bootstrap example, which
> solves the same problem.
>
> Look carefully -- it has to initialize the library on each node.
> If you are just loading the library manually, just do it on each node;
> if you are using libraries, just do that.

Ah, I see. I need to use

> clusterEvalQ(cl,dyn.load("mg.so"))

in my case. I should have realised the slaves were already running, after
makeCluster was called. It works now. Excellent. Thanks for your help.

                                                                  Faheem.



From faheem at email.unc.edu  Mon Dec 15 18:03:23 2003
From: faheem at email.unc.edu (Faheem Mitha)
Date: Mon, 15 Dec 2003 12:03:23 -0500 (EST)
Subject: [R] help with random numbers and Rmpi
In-Reply-To: <foy8tenhy9.fsf@orca.local>
References: <Pine.LNX.4.58.0312011520250.26318@Chrestomanci>
	<85fzg4jmgz.fsf@blindglobe.net>
	<Pine.LNX.4.58.0312011552290.26318@Chrestomanci>
	<853cc4jkud.fsf@blindglobe.net>
	<Pine.LNX.4.58.0312011627380.26318@Chrestomanci>
	<85llpwi019.fsf@blindglobe.net>
	<Pine.LNX.4.58.0312150022281.940@Chrestomanci>
	<853cbmvcgi.fsf@blindglobe.net>
	<Pine.LNX.4.58.0312150305060.940@Chrestomanci>
	<foy8tenhy9.fsf@orca.local>
Message-ID: <Pine.LNX.4.58.0312151200200.940@Chrestomanci>



On Mon, 15 Dec 2003, Na Li wrote:

> On 15 Dec 2003, Faheem Mitha stated:
> >
> >  So, the question would be how to get the R slaves to load up the library
> >  and I don't see any obvious way of doing this (using snow functions).
>
> Say you need boot on each slave, call this on the master,
>
> clusterEvalQ (cl, library (boot))
>
> where cl is the cluster created by make*Cluster () functions from snow.
>
> This is in the manual.

Yes, I see. Sorry, I should have figured this out myself yesterday.
Thanks.

                                                                   Faheem.



From s-plus at wiwi.uni-bielefeld.de  Mon Dec 15 18:14:06 2003
From: s-plus at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Mon, 15 Dec 2003 18:14:06 +0100
Subject: [R] Flury faces.
References: <Pine.OSF.4.31.0311070012001.25116-100000@harry.molgen.mpg.de>
Message-ID: <3FDDEBDE.8050909@wiwi.uni-bielefeld.de>

Eryk Wolski wrote:

>How to plot if possible flury faces in R?
>/Eryk
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>
Some days ago I was looking for a function to represent multivariate data by
faces, too.  For no "faces" function could be found I constructed my own 
one.
Here is the result:

Rd-file:    
http://www.wiwi.uni-bielefeld.de/~wolf/software/R-wtools/faces/faces.Rd
Definition:     
http://www.wiwi.uni-bielefeld.de/~wolf/software/R-wtools/faces/faces.sch
Documentation:    
http://www.wiwi.uni-bielefeld.de/~wolf/software/R-wtools/faces/faces.pdf
Some random faces:   
http://www.wiwi.uni-bielefeld.de/~wolf/software/R-wtools/faces/p20618291De3NA.jpg

Peter Wolf



From Ted.Harding at nessie.mcc.ac.uk  Mon Dec 15 18:22:59 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 15 Dec 2003 17:22:59 -0000 (GMT)
Subject: [R] Basic question on function "identical"
In-Reply-To: <Pine.A41.4.58.0312150711210.58588@homer05.u.washington.edu>
Message-ID: <XFMail.031215172259.Ted.Harding@nessie.mcc.ac.uk>

On 15-Dec-03 Thomas Lumley wrote:
> 
> One reason that which.max() exists is that we cannot guarantee
> which(x==max(x)) to work. It is possible, though rather unlikely, for
> there to be no x such that x==max(x).  One reason is the unpredictable
> use of 10-byte wide floating point registers on Intel chips.

Hmmm ...

I'd be interested to learn more of what you mean. For instance,
in C-speak, to find the maximum of an array of double x[], of
length n, something like the following code could be written:

  xmax=x[1];
  for(i=1;i<n;i++) if(x[i+1]>x[i]) xmax=x[i+1];

Regardless of the accuracy of the comparison, each assignment
xmax = ... should make xmax an internally exact copy of the
thing on the righthand side. However, your reply suggests that this
may not happen, as a result of "unpredictable use of 10-byte wide
floating point registers on Intel chips".
Is this really the case? If so, how would a discrepancy arise?
(I know C programmers like to use "register variables" where
possible, for speed, but the copying should be faithful, surely).

On the other hand, R has to deal with things which may be a mixture
of types, so has to cope with the coercions implied by the hierarchy
of types; but for instance

> x<-c(-1.0, 0.5, TRUE, as.integer(0))
> which(x==max(x))
[1] 3
> 

still gives the answer one would expect.

But in any case, the usual sort of application is which(x==max(x))
where the values in x are doubles, so the first question remains.

Futhermore, "which(x==max(x))" has a useful feature which
"which.max(x))" does not have::

  > x<-c(0.1, 0.7, 5.1, 5.1, 1.5, 5.1, 3.4, 2.2, 4.3)
  > which(x==max(x))
  [1] 3 4 6
  > which.max(x)
  [1] 3

so I'd feel a bit deprived if the former were discouraged!

With thanks,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 15-Dec-03                                       Time: 17:22:59
------------------------------ XFMail ------------------------------



From r-eugenesalinas at comcast.net  Mon Dec 15 18:51:41 2003
From: r-eugenesalinas at comcast.net (Eugene Salinas (R))
Date: Mon, 15 Dec 2003 12:51:41 -0500
Subject: [R] distribution of second order statistic
Message-ID: <3FDDF4AD.7000806@comcast.net>

Hi,

I am getting some weird results here and I think I am missing something. 
I am trying to program a function that for a set of random variables 
drawn from uniform distributions plots that distribution of the second 
order statistic of the ordered variables. (ie I have n uniform 
distributions on [0, w_i] for w_i different w_j and i=1..n. I want to 
plot the distribution of the second order statistic ie one less the 
maximum.

I thought that the way to do this is to calculate:
F=  Sum over i { (1-Fi) * Product of all j different i of Fj} + Product 
over all i of Fi

where Fi are just the respective uniform cdf for variable i.

The problem is that when I do this and plot F over the range from 0 to 
the highest of the w_i I don't get a cdf but something that slopes down 
at some point again. What is going on?????

Any help is greately appreciated....

Thanks, eugene.



From ggrothendieck at myway.com  Mon Dec 15 18:55:39 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 15 Dec 2003 12:55:39 -0500 (EST)
Subject: [R] Week of the Year date conversion
Message-ID: <20031215175539.693A939C4@mprdmxin.myway.com>



If you don't want to rely on the week calculations in R (which in
turn may depend on the OS?) you could try doing it yourself.  The
formula is pretty simple.

Suppose:

fw = day of week of first day of each week (0=Sun, 1=Mon, etc.)
fy = day of week of first day of year (to be calculated given year)
w = week number

Then (this is math, not R):

	mod(fw-fy,7)+7(w-1)

is the day of the year number of the first day of the wth week 
where 0 represents the first day of the year.  Just add this 
number of days to the date of the first of the year.

This can be translated into R using chron like this.  The first
two lines of the function body extract out the year and week.
The third gets the date of the first of the year.  Then
we calculate the date of the first of the year, foy.  The fourth
line calculates the day of the first of the year, fy.  The
5th line uses the formula above to get the day number and the 
last line adds it to the date of the first of the year.

yw.to.date <- function( x, fw = 0 ) {
	y <- as.numeric( substring( x, 1, 4 ) )
	w <- as.numeric( substring( x, 5 ) )
	foy <- chron( paste( 1, 1, y, sep= "/" ) )
	fy <- day.of.week( 1, 1, year )
	dayno <- ( fw - fy ) %% 7 + 7 * ( w - 1 )
	foy + dayno
}
# test
require( chron )
yw.to.date( c( "200302", "200310" ) ) 

or using POSIXt like this:

yw.to.date <- function( x, fw = 0 ) {
	y <- as.numeric( substring( x, 1, 4 ) )
	w <- as.numeric( substring( x, 5 ) )
	foy <- ISOdate( year, 1, 1, tz = "" )  
	fy <- as.POSIXlt( foy )$wday
	dayno <- ( fw - fy ) %% 7 + 7 * ( w - 1 )
	foy + dayno * 24 * 60 * 60
}

# test
yw.to.date( c( "200302", "200310" ) ) 



--- 
Date: 15 Dec 2003 13:35:37 +0100 
From: Peter Dalgaard <p.dalgaard at biostat.ku.dk>
To: Berwin Turlach <berwin at maths.uwa.edu.au> 
Cc: <R-help at stat.math.ethz.ch>,Barry Rowlingson <B.Rowlingson at lancaster.ac.uk>,Wayne Jones <JonesW at kssg.com> 
Subject: Re: [R] Week of the Year date conversion 

 
 
Berwin Turlach <berwin at maths.uwa.edu.au> writes:

> >>>>> "BR" == Barry Rowlingson <B.Rowlingson at lancaster.ac.uk> writes:
> 
> BR> Ah ha. Use '%w' for 'day of week':
> 
> >> strptime("2003 05 06", format="%Y %U %w")
> BR> [1] "2003-02-02"
> 
> BR> I dont have a calendar to hand to check that 2 Feb is the 6th
> BR> day of the 5th week....
> It's not, it is the first day of the 5th week in 2003. Moreover:
> 
> > strptime(paste("2003 05 0", 0:6, sep=""), format="%Y %U %w")
> [1] "2003-02-02" "2003-02-02" "2003-02-02" "2003-02-02" "2003-02-02"
> [6] "2003-02-02" "2003-02-02"
> 
> I guess this behaviour is consistent with the description of %U:
> 
> `%U' Week of the year as decimal number (00-53) using the first
> Sunday as day 1 of week 1.
> 
> ??
> 
> > strptime(paste("2003 01 0", 0:6, sep=""), format="%Y %U %w")
> [1] "2003-01-05" "2003-01-05" "2003-01-05" "2003-01-05" "2003-01-05"
> [6] "2003-01-05" "2003-01-05"


Why on earth are you guys trying to use a two-digit field for a
quantity in the range 0:6 ??

> strptime(paste("2003 01 ", 0:6, sep=""), format="%Y %U %w")
[1] "2003-01-05" "2003-01-06" "2003-01-07" "2003-01-08" "2003-01-09"
[6] "2003-01-10" "2003-01-11"


However, this is ISO week *2*! Day 1 of Week 1, 2003 was Dec 30,
2002.

Now, arguably we are counting from zero, but watch this:

> strptime(paste("2003 00", 0:6), format="%Y %U %w")
[1] "2002-11-18" "2002-11-19" "2002-11-20" "2003-01-01" "2003-01-02"
[6] "2003-01-03" "2003-01-04"

and further,

> strptime(paste("2003 01", 0:6), format="%Y %U %w")
[1] "2003-01-05" "2003-01-06" "2003-01-07" "2003-01-08" "2003-01-09"
[6] "2003-01-10" "2003-01-11"
> strptime(paste("2003 01", 0:6), format="%Y %W %w")
[1] "2003-01-05" "2003-01-06" "2003-01-07" "2003-01-08" "2003-01-09"
[6] "2003-01-10" "2003-01-11"

> 5 January 2003 was the first Sunday in 2003.


Yeah, but what's the difference between %U and %W then? 

'%U' Week of the year as decimal number (00-53) using the first
Sunday as day 1 of week 1.

'%w' Weekday as decimal number (0-6, Sunday is 0).

'%W' Week of the year as decimal number (00-53) using the first
Monday as day 1 of week 1.

Neither of those definitions coincide with ISO, BTW.

(This probably all comes down to OS/libc deficiencies. RedHat 8 in this case)



From wegmann_mailinglist at gmx.net  Mon Dec 15 19:03:11 2003
From: wegmann_mailinglist at gmx.net (Martin Wegmann)
Date: Mon, 15 Dec 2003 19:03:11 +0100 (MET)
Subject: [R] mailing list for basic questions
Message-ID: <32314.1071511391@www55.gmx.net>

Dear R-user,  
 
I started a few month ago with R and now I think that it is a great program 

but in the beginning I was close to give up, because I did not manage to
read  
in my files, did not know how the indexing works, where my results are or
did  
not understand how R saves my data and so on.  
And last but not least the command line might be a hurdle as well for people
 
who always used graphical user interfaces. And it takes time to see the  
advantages of the command line. 
 
Of course all these questions can easily be answered by reading the manuals,
 
FAQ or various books, but this takes time and might be frustrating because  
you don't find the right chapter at once.  
 
Asking such question on this list might annoy people because some questions 

are asked frequently or are already mentioned in the FAQ's. 
 
Therefore the few R user in this dept. decided to setup a R-beginner  
mailing-list for internal use only but because we are only a couple of  
active user we would like to involve more people who call themself R
beginner.  
 
Martin Maechel told me, that the idea of such a list has been rejected in
the past, 
because of various reason. 
 
I am aware that medium-experienced R user might teach some "wrong" stuff,
but our main 
idea behind this mailing list are real basic questions like understanding
and getting used to 
the command line (read in your data etc.) because we made the experience
that these few 
steps are the steepest and might stop somebody from further using R. 
 
I would like to know if there are some people on this list, who call
themselves beginners or 
have students who are about to make their first steps in R and would like to
use this list 
as well. 
 
best regards Martin 
 
https://lists.uni-wuerzburg.de/mailman/listinfo/r-beginner 
 

-- 

Neu: Preissenkung f?r MMS und FreeMMS! http://www.gmx.net



From ripley at stats.ox.ac.uk  Mon Dec 15 19:38:47 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 15 Dec 2003 18:38:47 +0000 (GMT)
Subject: [R] distribution of second order statistic
In-Reply-To: <3FDDF4AD.7000806@comcast.net>
Message-ID: <Pine.LNX.4.44.0312151837510.21717-100000@gannet.stats>

The order statistics have a beta distribution, so pbeta is all you need.

On Mon, 15 Dec 2003, Eugene Salinas (R) wrote:

> I am getting some weird results here and I think I am missing something. 
> I am trying to program a function that for a set of random variables 
> drawn from uniform distributions plots that distribution of the second 
> order statistic of the ordered variables. (ie I have n uniform 
> distributions on [0, w_i] for w_i different w_j and i=1..n. I want to 
> plot the distribution of the second order statistic ie one less the 
> maximum.
> 
> I thought that the way to do this is to calculate:
> F=  Sum over i { (1-Fi) * Product of all j different i of Fj} + Product 
> over all i of Fi
> 
> where Fi are just the respective uniform cdf for variable i.
> 
> The problem is that when I do this and plot F over the range from 0 to 
> the highest of the w_i I don't get a cdf but something that slopes down 
> at some point again. What is going on?????

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tlumley at u.washington.edu  Mon Dec 15 19:44:13 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 15 Dec 2003 10:44:13 -0800 (PST)
Subject: [R] Basic question on function "identical"
In-Reply-To: <XFMail.031215172259.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.031215172259.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.A41.4.58.0312151023190.150404@homer06.u.washington.edu>

On Mon, 15 Dec 2003 Ted.Harding at nessie.mcc.ac.uk wrote:

> On 15-Dec-03 Thomas Lumley wrote:
> >
> > One reason that which.max() exists is that we cannot guarantee
> > which(x==max(x)) to work. It is possible, though rather unlikely, for
> > there to be no x such that x==max(x).  One reason is the unpredictable
> > use of 10-byte wide floating point registers on Intel chips.
>
> Hmmm ...
>
> I'd be interested to learn more of what you mean. For instance,
> in C-speak, to find the maximum of an array of double x[], of
> length n, something like the following code could be written:
>
>   xmax=x[1];
>   for(i=1;i<n;i++) if(x[i+1]>x[i]) xmax=x[i+1];
>
> Regardless of the accuracy of the comparison, each assignment
> xmax = ... should make xmax an internally exact copy of the
> thing on the righthand side.

No, this is not guaranteed (though it is very likely).  Suppose that you
have just computed x as, say, sin(theta), and the value of x[n] happens to
still be stored in a floating point register on the CPU, with 64 bits of
precision (and 16 exponent bits, making a total of 10 bytes).

The compiler can use this value and decide it is the largest, and could
end up with xmax being this value in the register.  This value is not
necessarily the same as would be obtained by loading x[n] back in from
memory, and so need not compare equal.

In this case I think it is very unlikely that there is a problem, and I
think it can be guaranteed that
	which(x==max(x))
will return the right answer or nothing.

 However, we have really had a problem with the math library at one point
that was due to something vaguely of this sort.

This is why compilers provide options like --ffloat-store, and one reason
to use `volatile'.  It's also probably why so many Windows drivers seem
downgrade the floating point precision of the chip to 54 bits, to match
the precision of a double, although I seem to recall Brian Ripley saying
that this doesn't actually buy you as much as you would think.

>				However, your reply suggests that this
> may not happen, as a result of "unpredictable use of 10-byte wide
> floating point registers on Intel chips".
> Is this really the case? If so, how would a discrepancy arise?
> (I know C programmers like to use "register variables" where
> possible, for speed, but the copying should be faithful, surely).
>

This doesn't really have anything to do with the C type qualifier
"register", which is apparently ignored by most modern compilers, as they
think they can do register allocation better than the user.

	-thomas



From Jason.L.Higbee at stls.frb.org  Mon Dec 15 20:47:23 2003
From: Jason.L.Higbee at stls.frb.org (Jason.L.Higbee@stls.frb.org)
Date: Mon, 15 Dec 2003 13:47:23 -0600
Subject: [R] Appending intermediate terms in a For Loop
Message-ID: <20031215194726.44D6985BE1@p3fed1.frb.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031215/ee783205/attachment.pl

From bates at stat.wisc.edu  Mon Dec 15 21:05:56 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: 15 Dec 2003 14:05:56 -0600
Subject: [R] Appending intermediate terms in a For Loop
In-Reply-To: <20031215194726.44D6985BE1@p3fed1.frb.org>
References: <20031215194726.44D6985BE1@p3fed1.frb.org>
Message-ID: <6roeu9j17f.fsf@bates4.stat.wisc.edu>

Jason.L.Higbee at stls.frb.org writes:

> I can't figure out how to append intermediate terms inside a For loop.  My 
> use of append(), various indexing, and use of data frames, vectors, 
> matrices has been fruitless.  Here's a simplified example of what I'm 
> talking about:
> 
> i <- 1
> for(i in 10) {
>     v[i] <- i/10
> }
> > v
>  [1]  1 NA NA NA NA NA NA NA NA  1

I think you want

for(i in 1:10)

not

for(i in 10)

> 
> I would like: [1] 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
> 
> Any help on this would be greatly appreciated.
> 
> PS:  I realize that I could have gone v <- 1:10, then v <- v / 10, as I 
> said this is a simplified example; the actual function to be evaluated 
> iteratively in the for loop is more complex than just "x /10"

Generally it is easier to take the "whole object" approach to creating
vectors, as you mention in your P.S.

However, if it really is necessary to iterate over the elements of a
vector a good way of doing it is to establish the vector at the
correct length first, then iterate over it.  The preferred for loop is

> v = numeric(10)
> for (i in seq(along = v)) {
+     v[i] = i/10
+ }
> v
 [1] 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0

The reason that seq(along = v) is preferred to 1:length(v) is that the
former gives the correct loop when v has length zero.



From p.dalgaard at biostat.ku.dk  Mon Dec 15 21:28:34 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 15 Dec 2003 21:28:34 +0100
Subject: [R] Week of the Year date conversion
In-Reply-To: <Pine.LNX.4.44.0312151321210.1079-100000@gannet.stats>
References: <Pine.LNX.4.44.0312151321210.1079-100000@gannet.stats>
Message-ID: <x2oeu9omfh.fsf@biostat.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> On 15 Dec 2003, Peter Dalgaard wrote:
> 
> > Neither of those definitions coincide with ISO, BTW.
> 
> ISO 8601 week of the year (as distinct from ISO C where these come from)  
> is %V, of course.

Doesn't work with RedHat 8 though...

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Jason.L.Higbee at stls.frb.org  Mon Dec 15 21:40:02 2003
From: Jason.L.Higbee at stls.frb.org (Jason.L.Higbee@stls.frb.org)
Date: Mon, 15 Dec 2003 14:40:02 -0600
Subject: [R] Appending intermediate terms in a For Loop
Message-ID: <20031215204005.AC47885E56@p3fed1.frb.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031215/ab4bbaaa/attachment.pl

From kwan022 at stat.auckland.ac.nz  Mon Dec 15 21:55:39 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Tue, 16 Dec 2003 09:55:39 +1300 (NZDT)
Subject: [R] Julian Dates
Message-ID: <Pine.LNX.4.44.0312160952230.18972-100000@stat61.stat.auckland.ac.nz>

Hi,

I'm a bit confused how julian() works.  If I understand right, it returns 
the number of days since the origin. 

I have a vector:
> SLDATX[1:10]
 [1] "1986-01-06" "1986-01-17" "1986-02-02" "1986-02-04"
 [5] "1986-02-04" "1986-02-21" "1986-03-06" "1986-03-25"
 [9] "1986-04-06" "1986-04-10"

And when I did:
> TIMESOLD <- as.numeric(julian(as.POSIXlt(SLDATX),
+                               origin = as.POSIXct("1986-01-01", "")))

I got:
> TIMESOLD[1:10]
 [1]  5.00000 16.00000 32.00000 34.00000 34.00000 51.00000
 [7] 64.04167 83.04167 95.04167 99.04167


THe first 6 values from TIMESOLD is obvious, however I'm not sure why I 
got decimals from the 7th value, as my input vector does not have any 
specific "times" after the dates.

Any insights would be greatly appreciated...;-D

-- 
Cheers,

Kevin

---------------------------------------------------------------
"Try not.  Do, do!  Or do not.  There is no try"
   Jedi Master Yoda

----
Ko-Kang Kevin Wang, MSc
SLC STATS 10x Workshop Coordinator
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599  x88475



From pauljohn at ku.edu  Mon Dec 15 22:34:11 2003
From: pauljohn at ku.edu (Paul Johnson)
Date: Mon, 15 Dec 2003 15:34:11 -0600
Subject: [R] Basic question on function "identical"
In-Reply-To: <XFMail.031215172259.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.031215172259.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <3FDE28D3.8060401@ku.edu>

I hope I am not telling you things you already know. If so, I apologize 
in advance.

There are several C-library addons available to try to deal with the 
problem that comparisons of floating point numbers can be 
unpredictable.  I think your example with the greater than sign would 
not be a source of the trouble, but if you ended it with a comparison like

if (xmax == 10.7)

then you would be in trouble because the internal representation of the 
float xmax might not be precisely equal to 10.7.

Until I hear otherwise, I am thinking that ordinal comparisons like (x > 
xmax) are accurate, but that equality comparisons like (x==xmax) are not. 

Here's one of the C library projects dealing with the subject:

http://fcmp.sourceforge.net/

The author of that library, Ted Belding, did this paper, "Numerical 
Replication of Computer Simulations: Some Pitfalls...", which is 
informative (IMHO):
http://alife.ccp14.ac.uk/ftp-mirror/alife/zooland/pub/research/ci/EC/GA/papers/gecco2000.ps.gz

Also check this web page, which I bookmarked:
http://vision.eng.shu.ac.uk/C++/c/c-faq/cfaq14.html#r14.6

I know I've seen more, but can't remember where.

Ted Harding wrote:

>For instance,
>in C-speak, to find the maximum of an array of double x[], of
>length n, something like the following code could be written:
>
>  xmax=x[1];
>  for(i=1;i<n;i++) if(x[i+1]>x[i]) xmax=x[i+1];
>
>Regardless of the accuracy of the comparison, each assignment
>xmax = ... should make xmax an internally exact copy of the
>thing on the righthand side. However, your reply suggests that this
>may not happen, as a result of "unpredictable use of 10-byte wide
>floating point registers on Intel chips".
>  
>


-- 
Paul E. Johnson                       email: pauljohn at ku.edu
Dept. of Political Science            http://lark.cc.ku.edu/~pauljohn
1541 Lilac Lane, Rm 504                              
University of Kansas                  Office: (785) 864-9086
Lawrence, Kansas 66044-3177           FAX: (785) 864-5700



From wolski at molgen.mpg.de  Mon Dec 15 22:39:17 2003
From: wolski at molgen.mpg.de (wolski)
Date: Mon, 15 Dec 2003 22:39:17 +0100
Subject: [R] Rd files Assignment functions.
Message-ID: <200312152239170090.022033FD@harry.molgen.mpg.de>

Hi!

Old story again. I put it away for a while because there are always other thinks to do.
But I cant deny that I still like to  like to comment assignment functions.

Under linux I get.

Cannot handle Rd file names containing '<'.
These are not legal file names on all R platforms.
Please rename the following files and try again:
  man/[[<-.caliblist.Rd
  man/[<-.massvectorlist.Rd
  man/[[<-.massvectorlist.Rd


Ok. It does not work.

So I renamed the file "[<-.massvectorlist.Rd" in test.Rd  And replaced the name and first alias field

\name{test}
\alias{test}
\alias{[<-.massvectorlist}
\title{ Replace Parts of Massvectorlist}
....

Now a "test" entry appears in the 00Index.html but ...Of course I can give a more appropriate name but...
But unfortunately all \link to [<-. in the \seealso section does not work than either.
Why no [<-.massvectorlist entry appears in the 00Index.html file?
Even if i put [<-.massvector as second alias?

Is there a way to get assignments work as other functions? 
Or is the best solution to give a name like Assign.massvector to the Rd file the name and the first \alias?


Sincerely
Eryk-



From ligges at statistik.uni-dortmund.de  Mon Dec 15 22:37:59 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 15 Dec 2003 22:37:59 +0100
Subject: [R] Julian Dates
References: <Pine.LNX.4.44.0312160952230.18972-100000@stat61.stat.auckland.ac.nz>
Message-ID: <3FDE29B7.F9521143@statistik.uni-dortmund.de>



Ko-Kang Kevin Wang wrote:
> 
> Hi,
> 
> I'm a bit confused how julian() works.  If I understand right, it returns
> the number of days since the origin.
> 
> I have a vector:
> > SLDATX[1:10]
>  [1] "1986-01-06" "1986-01-17" "1986-02-02" "1986-02-04"
>  [5] "1986-02-04" "1986-02-21" "1986-03-06" "1986-03-25"
>  [9] "1986-04-06" "1986-04-10"
> 
> And when I did:
> > TIMESOLD <- as.numeric(julian(as.POSIXlt(SLDATX),
> +                               origin = as.POSIXct("1986-01-01", "")))
> 
> I got:
> > TIMESOLD[1:10]
>  [1]  5.00000 16.00000 32.00000 34.00000 34.00000 51.00000
>  [7] 64.04167 83.04167 95.04167 99.04167
>
> THe first 6 values from TIMESOLD is obvious, however I'm not sure why I
> got decimals from the 7th value, as my input vector does not have any
> specific "times" after the dates.

That's because of the timezone you are using (winter/summertime).
Look what the difference of 0.04167 really is:  0.04167 * 24 ~ 1
hour.....

Uwe Ligges
 
> Any insights would be greatly appreciated...;-D
> 
> --
> Cheers,
> 
> Kevin
> 
> ---------------------------------------------------------------
> "Try not.  Do, do!  Or do not.  There is no try"
>    Jedi Master Yoda
> 
> ----
> Ko-Kang Kevin Wang, MSc
> SLC STATS 10x Workshop Coordinator
> University of Auckland
> New Zealand
> Homepage: http://www.stat.auckland.ac.nz/~kwan022
> Ph: 373-7599  x88475
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ok at cs.otago.ac.nz  Tue Dec 16 01:29:39 2003
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Tue, 16 Dec 2003 13:29:39 +1300 (NZDT)
Subject: [R] reverse lexicographic order
Message-ID: <200312160029.hBG0Tdi8086789@atlas.otago.ac.nz>

Dr Murray Jorgensen (maj at waikato.ac.nz) wrote:
	> I have some email addresses that I would like to sort in reverse
	> lexicographic order so that addresses from the same domain will be
	> grouped together. How might that be done?

Because he wants addresses from the same domain to be grouped together
(so that foo.bar.ick.ac should be in the same group as zoo.sno.ick.ac),
it is not sufficient to split at the at-sign.

The obvious method is
(1) reverse the strings
(2) sort the reversed strings
(3) reverse the sorted reversed strings

All of this is obvious except reversing the strings.
There's ?rev, which reverses vectors, but no strrev.

Here's a strrev() I knocked together quickly:

strrev <-
function (s) paste(rev(strsplit(s, character(0))[[1]]), collapse="")

If anyone can tell me how to vectorise this, I would be glad of the lesson.



From ggrothendieck at myway.com  Tue Dec 16 01:41:42 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 15 Dec 2003 19:41:42 -0500 (EST)
Subject: [R] Julian Dates
Message-ID: <20031216004142.DA0833971@mprdmxin.myway.com>



What you can do to handle this timezone problem is either to use
POSIXt with GMT or use chron (which does not use timezones so 
can't cause problems like this):

Suppose:

SLDATX <- c( "1986-01-06", "1986-01-17", "1986-02-02", "1986-02-04",
,"1986-02-04", "1986-02-21", "1986-03-06", "1986-03-25",
,"1986-04-06", "1986-04-10" )

# then using POSIXt in the GMT timezone:

TIMESOLD <- as.numeric( julian( as.POSIXlt( SLDATX, tz="GMT" ),
  origin = as.POSIXct( "1986-01-01", tz = "GMT" ) ) )

# or the alternative using chron:

require(chron)
TIMESOLD2 <- as.numeric( chron( SLDATX, format="y-m-d", 
  origin = c( month = 1, day = 1, year = 1986 ) ) )

all.equal(TIMESOLD,TIMESOLD2)

---
Date: Mon, 15 Dec 2003 22:37:59 +0100 
From: Uwe Ligges <ligges at statistik.uni-dortmund.de>
To: Ko-Kang Kevin Wang <kwan022 at stat.auckland.ac.nz> 
Cc: R Help <r-help at stat.math.ethz.ch> 
Subject: Re: [R] Julian Dates 

 
 


Ko-Kang Kevin Wang wrote:
> 
> Hi,
> 
> I'm a bit confused how julian() works. If I understand right, it returns
> the number of days since the origin.
> 
> I have a vector:
> > SLDATX[1:10]
> [1] "1986-01-06" "1986-01-17" "1986-02-02" "1986-02-04"
> [5] "1986-02-04" "1986-02-21" "1986-03-06" "1986-03-25"
> [9] "1986-04-06" "1986-04-10"
> 
> And when I did:
> > TIMESOLD <- as.numeric(julian(as.POSIXlt(SLDATX),
> + origin = as.POSIXct("1986-01-01", "")))
> 
> I got:
> > TIMESOLD[1:10]
> [1] 5.00000 16.00000 32.00000 34.00000 34.00000 51.00000
> [7] 64.04167 83.04167 95.04167 99.04167
>
> THe first 6 values from TIMESOLD is obvious, however I'm not sure why I
> got decimals from the 7th value, as my input vector does not have any
> specific "times" after the dates.

That's because of the timezone you are using (winter/summertime).
Look what the difference of 0.04167 really is: 0.04167 * 24 ~ 1
hour.....

Uwe Ligges

> Any insights would be greatly appreciated...;-D
> 
> --
> Cheers,
> 
> Kevin
> 
> ---------------------------------------------------------------
> "Try not. Do, do! Or do not. There is no try"
> Jedi Master Yoda
> 
> ----
> Ko-Kang Kevin Wang, MSc
> SLC STATS 10x Workshop Coordinator
> University of Auckland
> New Zealand
> Homepage: http://www.stat.auckland.ac.nz/~kwan022
> Ph: 373-7599 x88475
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From kwan022 at stat.auckland.ac.nz  Tue Dec 16 01:58:13 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Tue, 16 Dec 2003 13:58:13 +1300 (NZDT)
Subject: [R] Julian Dates
In-Reply-To: <20031216004142.DA0833971@mprdmxin.myway.com>
Message-ID: <Pine.LNX.4.44.0312161357010.19857-100000@stat61.stat.auckland.ac.nz>

Thanks!  chron() is very useful indeed.

Just out of interest, is it possible to do, say in this case, the number 
of months (or quarters) after January 1986?  i.e. use a different time 
interval?

On Mon, 15 Dec 2003, Gabor Grothendieck wrote:

> What you can do to handle this timezone problem is either to use
> POSIXt with GMT or use chron (which does not use timezones so 
> can't cause problems like this):
> 
> Suppose:
> 
> SLDATX <- c( "1986-01-06", "1986-01-17", "1986-02-02", "1986-02-04",
> ,"1986-02-04", "1986-02-21", "1986-03-06", "1986-03-25",
> ,"1986-04-06", "1986-04-10" )
> 
> # then using POSIXt in the GMT timezone:
> 
> TIMESOLD <- as.numeric( julian( as.POSIXlt( SLDATX, tz="GMT" ),
>   origin = as.POSIXct( "1986-01-01", tz = "GMT" ) ) )
> 
> # or the alternative using chron:
> 
> require(chron)
> TIMESOLD2 <- as.numeric( chron( SLDATX, format="y-m-d", 
>   origin = c( month = 1, day = 1, year = 1986 ) ) )
> 
> all.equal(TIMESOLD,TIMESOLD2)
> 

-- 
Cheers,

Kevin

---------------------------------------------------------------
"Try not.  Do, do!  Or do not.  There is no try"
   Jedi Master Yoda

----
Ko-Kang Kevin Wang, MSc
SLC STATS 10x Workshop Coordinator
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599  x88475



From tlumley at u.washington.edu  Tue Dec 16 02:11:10 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 15 Dec 2003 17:11:10 -0800 (PST)
Subject: [R] reverse lexicographic order
In-Reply-To: <200312160029.hBG0Tdi8086789@atlas.otago.ac.nz>
References: <200312160029.hBG0Tdi8086789@atlas.otago.ac.nz>
Message-ID: <Pine.A41.4.58.0312151702160.29722@homer31.u.washington.edu>

On Tue, 16 Dec 2003, Richard A. O'Keefe wrote:

> Here's a strrev() I knocked together quickly:
>
> strrev <-
> function (s) paste(rev(strsplit(s, character(0))[[1]]), collapse="")
>
> If anyone can tell me how to vectorise this, I would be glad of the lesson.
>

strrev<- function(ss) {
	sapply(lapply( strsplit(ss,character(0)), rev), paste, collapse="")
}

vectorises the strsplit() part and should be at least a little faster than
loops for the rev and paste operations.

On the other hand, on my laptop, for 100 copies of the text produced by
license() your version takes about 1.6 seconds and mine takes about 1.5.
If this were to be used a lot it would make sense to code it in C.

	-thomas



From ggrothendieck at myway.com  Tue Dec 16 03:23:20 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 15 Dec 2003 21:23:20 -0500 (EST)
Subject: [R] Julian Dates
Message-ID: <20031216022320.C24CD3993@mprdmxin.myway.com>



Assuming you want to find a whole number months and quarters:

   require(chron)

   z <- chron( SLDATX, format="y-m-d" )

   months <- with( month.day.year(z), 12*(year-1986)+month-1 )

   quarters <- months %/% 3

--- 
Date: Tue, 16 Dec 2003 13:58:13 +1300 (NZDT) 
From: Ko-Kang Kevin Wang <kwan022 at stat.auckland.ac.nz>
To: Gabor Grothendieck <ggrothendieck at myway.com> 
Cc: <ligges at statistik.uni-dortmund.de>, <r-help at stat.math.ethz.ch> 
Subject: Re: [R] Julian Dates 

 
 
Thanks! chron() is very useful indeed.

Just out of interest, is it possible to do, say in this case, the number 
of months (or quarters) after January 1986? i.e. use a different time 
interval?

On Mon, 15 Dec 2003, Gabor Grothendieck wrote:

> What you can do to handle this timezone problem is either to use
> POSIXt with GMT or use chron (which does not use timezones so 
> can't cause problems like this):
> 
> Suppose:
> 
> SLDATX <- c( "1986-01-06", "1986-01-17", "1986-02-02", "1986-02-04",
> ,"1986-02-04", "1986-02-21", "1986-03-06", "1986-03-25",
> ,"1986-04-06", "1986-04-10" )
> 
> # then using POSIXt in the GMT timezone:
> 
> TIMESOLD <- as.numeric( julian( as.POSIXlt( SLDATX, tz="GMT" ),
> origin = as.POSIXct( "1986-01-01", tz = "GMT" ) ) )
> 
> # or the alternative using chron:
> 
> require(chron)
> TIMESOLD2 <- as.numeric( chron( SLDATX, format="y-m-d", 
> origin = c( month = 1, day = 1, year = 1986 ) ) )
> 
> all.equal(TIMESOLD,TIMESOLD2)
> 

-- 
Cheers,

Kevin

---------------------------------------------------------------
"Try not. Do, do! Or do not. There is no try"
Jedi Master Yoda



From patrick.giraudoux at univ-fcomte.fr  Tue Dec 16 05:03:37 2003
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux)
Date: Tue, 16 Dec 2003 05:03:37 +0100
Subject: [R] Winedit and R
Message-ID: <001001c3c389$abaca4a0$7ff0f9c1@PC728329681112>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031216/a9760508/attachment.pl

From ok at cs.otago.ac.nz  Tue Dec 16 05:41:17 2003
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Tue, 16 Dec 2003 17:41:17 +1300 (NZDT)
Subject: [R] reverse lexicographic order
Message-ID: <200312160441.hBG4fH6G090972@atlas.otago.ac.nz>

I wrote:
> If anyone can tell me how to vectorise this, I would be glad of the lesson.
where "this" was
> strrev <-
> function (s) paste(rev(strsplit(s, character(0))[[1]]), collapse="")

Thomas Lumley <tlumley at u.washington.edu> suggested
	strrev<- function(ss) {
	    sapply(lapply( strsplit(ss,character(0)), rev), paste, collapse="")
	}
	
Unfortunately, I failed to explain myself clearly, so this doesn't actually
answer the question I _meant_ to ask.  For me, sticking in some variant of
'apply' means you have _failed_ to vectorise.  The string reversal code in
?rev doesn't count for the same reason.

There is no reason why a built-in strrev() couldn't be as vectorised as
most built-ins, it's just not common enough to deserve a lot of effort.



From bbvaughn at bellsouth.net  Tue Dec 16 05:45:32 2003
From: bbvaughn at bellsouth.net (Brandon Vaughn)
Date: Mon, 15 Dec 2003 22:45:32 -0600
Subject: [R] Resampling Stats software
Message-ID: <20031216044537.JYQO1282.imf23aec.mail.bellsouth.net@brandoncd66bez>

Hi,
 
I am new to R (I have most of my experience in SAS and SPSS).  I was
wondering if anyone has used both Resampling Stats and R, and could comment
on strengths/relationships.  Also, I have no clue on how to do the various
examples from the book "Resampling: The New Statistics" in R.  Can anyone
give me some possible starting points?  Or websites/books?

Thanks,
Brandon



From jasont at indigoindustrial.co.nz  Tue Dec 16 06:13:24 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Tue, 16 Dec 2003 18:13:24 +1300
Subject: [R] Resampling Stats software
In-Reply-To: <20031216044537.JYQO1282.imf23aec.mail.bellsouth.net@brandoncd66bez>
References: <20031216044537.JYQO1282.imf23aec.mail.bellsouth.net@brandoncd66bez>
Message-ID: <3FDE9474.3020609@indigoindustrial.co.nz>

Brandon Vaughn wrote:
...
> I am new to R (I have most of my experience in SAS and SPSS).  I was
> wondering if anyone has used both Resampling Stats and R, and could comment
> on strengths/relationships.  

There are a few add-on packages for resampling with R.  "boot" is the 
one I've used, and can strongly recommend.

> Also, I have no clue on how to do the various
> examples from the book "Resampling: The New Statistics" in R.  Can anyone
> give me some possible starting points?  Or websites/books?

I've never heard of the book you cite, but these two are good.  The 
first is a pure bootstrap book, with examples in S-PLUS (the R library 
is rather close).  The second is an applied stats book, which includes a 
section on resampling methods.  All its examples are in S-PLUS, with 
notes about where R differs (very little).

@Book{DavidsonHinkley1997,
author =	 {A. C. Davidson and D. V. Hinkley},
   title =	 {Bootstrap Methods and their Application},
   publisher =	 {Cambridge University Press},
   year =	 {1997},
}

@book{VenablesRipley2002,
   author =	 "Venables, W.R. and Ripley, B.D.",
   title =	 "Modern Applied Statistics with S",
   edition =	 "Fourth",
   publisher =	 {Springer-Verlag},
   address =	 {New York},
   year =	 2002,
}

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From ggrothendieck at myway.com  Tue Dec 16 06:31:42 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 16 Dec 2003 00:31:42 -0500 (EST)
Subject: [R] reverse lexicographic order
Message-ID: <20031216053142.D5826395B@mprdmxin.myway.com>



Here is a way to do it without using apply.  sep must be set to
a character not in any of the strings.  Below we show its much
faster than using apply yet gives the same answer.

strRev <- function(x, sep = "\10") {
	z <- unlist( strsplit( paste( x, sep, sep="" ), "" ) )
	z <- unlist( strsplit( paste( rev( z ), collapse="" ), sep ) )
	rev( z[-1] )
}

# Following taken from examples in ?strsplit
strReverse <- function(x)
	sapply(lapply(strsplit(x,NULL), rev), paste, collapse="")

> data(state)

> system.time(for(i in 1:100)strRev(state.name))
[1] 0.22 0.01 0.23   NA   NA

> system.time(for(i in 1:100)strReverse(state.name))
[1] 1.07 0.00 1.83   NA   NA

> all.equal(strRev(state.name),strReverse(state.name))
[1] TRUE

--- 
Date: Tue, 16 Dec 2003 17:41:17 +1300 (NZDT) 
From: Richard A. O'Keefe <ok at cs.otago.ac.nz>
To: <r-help at stat.math.ethz.ch> 
Subject: Re: [R] reverse lexicographic order 

 
 
I wrote:
> If anyone can tell me how to vectorise this, I would be glad of the lesson.
where "this" was
> strrev <-
> function (s) paste(rev(strsplit(s, character(0))[[1]]), collapse="")

Thomas Lumley <tlumley at u.washington.edu> suggested
     strrev<- function(ss) {
      sapply(lapply( strsplit(ss,character(0)), rev), paste, collapse="")
     }
     
Unfortunately, I failed to explain myself clearly, so this doesn't actually
answer the question I _meant_ to ask. For me, sticking in some variant of
'apply' means you have _failed_ to vectorise. The string reversal code in
?rev doesn't count for the same reason.

There is no reason why a built-in strrev() couldn't be as vectorised as
most built-ins, it's just not common enough to deserve a lot of effort.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From Wanzare at HCJP.com  Tue Dec 16 08:11:38 2003
From: Wanzare at HCJP.com (Manoj - Hachibushu Capital)
Date: Tue, 16 Dec 2003 16:11:38 +0900
Subject: [R] Calling C function in R
Message-ID: <1CBA12F2D414914989C723D196B287DC05556B@jp-svr-ex1.HCJP.COM>

Hi All,
	I am trying to write a c function to optimize loop processing.
Having read the R extension and trying out a few samples I was pretty
comfortable with the basics. 

	However, I am wondering if there is anyway to call tseries
functions like adf.test or po.test from within c function. Any pointers/
code sample would be greatly appreciated.

Thank you.

Manoj



From ligges at statistik.uni-dortmund.de  Tue Dec 16 08:57:17 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 16 Dec 2003 08:57:17 +0100
Subject: [R] Winedit and R
In-Reply-To: <001001c3c389$abaca4a0$7ff0f9c1@PC728329681112>
References: <001001c3c389$abaca4a0$7ff0f9c1@PC728329681112>
Message-ID: <3FDEBADD.80601@statistik.uni-dortmund.de>

Patrick Giraudoux wrote:

> Hi all,
> 
> I am trying to install add-on in R (rw1070) to work with WinEdit.

R-WinEdt is a plug-in for WinEdt, not for WinEdit (the "i" does 
matter!)- For the following discussion, let's assume you mean WinEdt.


 > Libraries Swinregistry and Rwinedt have been installed via "Install 
Package(s) from local zip files" from R. However, when I run 
library(Rwinedt) I get the following messages:
> 
> 
>>library(Rwinedt)
> 
> Loading required package: SWinRegistry
> Error in loadNamespace(name) : package `methods' does not have a name space
> In addition: Warning messages:
> 1: package RWinEdt was built under R version 1.8.0
> 2: package SWinRegistry was built under R version 1.8.0
> Error in eval(expr, envir, enclos) :
> Package SWinRegistry is not available.
> Either install it or install R-WinEdt manually, as described in ReadMe.txt
> 
> Intalling manually looking quite complicated, I am not tempted to take this direction...
> 
> Can anybody give me a hint?

Your binary version of the package SWinRegistry is compiled under 
R-1.8.0 and does not work for your old version of R.
So either upgrade to R-1.8.1 or install R-WinEdt manually as described 
in the ReadMe (in that case you won't need SWinRegistry).

Uwe Ligges


> Kind regards,
> 
> Patrick Giraudoux
> 
> 
> University of Franche-Comt?
> Department of Environmental Biology
> EA3184 af. INRA
> F-25030 Besan?on Cedex
> 
> tel.: +33 381 665 745
> fax.: +33 381 665 797
> http://lbe.univ-fcomte.fr
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From ripley at stats.ox.ac.uk  Tue Dec 16 08:59:15 2003
From: ripley at stats.ox.ac.uk (Prof Brian D Ripley)
Date: Tue, 16 Dec 2003 07:59:15 +0000 (GMT Standard Time)
Subject: [R] Winedit and R
In-Reply-To: <001001c3c389$abaca4a0$7ff0f9c1@PC728329681112>
Message-ID: <Pine.WNT.4.44.0312160757330.352-100000@petrel>

You are using mismatched versions of R and the packages, as the warnings
say. Please update your version of R.

On Tue, 16 Dec 2003, Patrick Giraudoux wrote:

> Hi all,
>
> I am trying to install add-on in R (rw1070) to work with WinEdit. Libraries Swinregistry and Rwinedt have been installed via "Install Package(s) from local zip files" from R. However, when I run library(Rwinedt) I get the following messages:
>
> > library(Rwinedt)
> Loading required package: SWinRegistry
> Error in loadNamespace(name) : package `methods' does not have a name space
> In addition: Warning messages:
> 1: package RWinEdt was built under R version 1.8.0
> 2: package SWinRegistry was built under R version 1.8.0
> Error in eval(expr, envir, enclos) :
> Package SWinRegistry is not available.
> Either install it or install R-WinEdt manually, as described in ReadMe.txt
>
> Intalling manually looking quite complicated, I am not tempted to take this direction...
>
> Can anybody give me a hint?
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Tue Dec 16 09:05:10 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 16 Dec 2003 09:05:10 +0100
Subject: [R] Resampling Stats software
In-Reply-To: <20031216044537.JYQO1282.imf23aec.mail.bellsouth.net@brandoncd66bez>
References: <20031216044537.JYQO1282.imf23aec.mail.bellsouth.net@brandoncd66bez>
Message-ID: <3FDEBCB6.2080802@statistik.uni-dortmund.de>

Brandon Vaughn wrote:

> Hi,
>  
> I am new to R (I have most of my experience in SAS and SPSS).  I was
> wondering if anyone has used both Resampling Stats and R, and could comment
> on strengths/relationships.

Hmmm. 8 years ago I had to use Resampling Stats. I don't know what 
Resampling Stats is today, but recollecting my 8 year old experiences 
with todays R (which obviously is unfair!): Use R! Yyou can easily do 
everything in R what Resampling Stats was capable of.

See also Jason Turner's message.

Uwe Ligges


 > Also, I have no clue on how to do the various
> examples from the book "Resampling: The New Statistics" in R.  Can anyone
> give me some possible starting points?  Or websites/books?
> 
> Thanks,
> Brandon
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Tue Dec 16 09:10:41 2003
From: ripley at stats.ox.ac.uk (Prof Brian D Ripley)
Date: Tue, 16 Dec 2003 08:10:41 +0000 (GMT Standard Time)
Subject: [R] Calling C function in R
In-Reply-To: <1CBA12F2D414914989C723D196B287DC05556B@jp-svr-ex1.HCJP.COM>
Message-ID: <Pine.WNT.4.44.0312160806560.352-100000@petrel>

On Tue, 16 Dec 2003, Manoj - Hachibushu Capital wrote:

> Hi All,
> 	I am trying to write a c function to optimize loop processing.
> Having read the R extension and trying out a few samples I was pretty
> comfortable with the basics.
>
> 	However, I am wondering if there is anyway to call tseries
> functions like adf.test or po.test from within c function. Any pointers/
> code sample would be greatly appreciated.

Yes, this is described with code samples in `Writing R Extensions', in the
section `Evaluating R expressions from C'.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From patrick.giraudoux at univ-fcomte.fr  Tue Dec 16 09:41:40 2003
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux)
Date: Tue, 16 Dec 2003 09:41:40 +0100
Subject: [R] Winedit and R
References: <Pine.WNT.4.44.0312160757330.352-100000@petrel>
Message-ID: <003501c3c3b0$71a9ce00$754637c1@PC728329681112>

Did it and it works now perfectly well. Apologises to have disturbed
everybody for a so trivial issue.

Many thanks for the hints all,

Patrick



----- Original Message ----- 
From: "Prof Brian D Ripley" <ripley at stats.ox.ac.uk>
To: "Patrick Giraudoux" <patrick.giraudoux at univ-fcomte.fr>
Cc: <r-help at stat.math.ethz.ch>
Sent: Tuesday, December 16, 2003 8:59 AM
Subject: Re: [R] Winedit and R


> You are using mismatched versions of R and the packages, as the warnings
> say. Please update your version of R.
>
> On Tue, 16 Dec 2003, Patrick Giraudoux wrote:
>
> > Hi all,
> >
> > I am trying to install add-on in R (rw1070) to work with WinEdit.
Libraries Swinregistry and Rwinedt have been installed via "Install
Package(s) from local zip files" from R. However, when I run
library(Rwinedt) I get the following messages:
> >
> > > library(Rwinedt)
> > Loading required package: SWinRegistry
> > Error in loadNamespace(name) : package `methods' does not have a name
space
> > In addition: Warning messages:
> > 1: package RWinEdt was built under R version 1.8.0
> > 2: package SWinRegistry was built under R version 1.8.0
> > Error in eval(expr, envir, enclos) :
> > Package SWinRegistry is not available.
> > Either install it or install R-WinEdt manually, as described in
ReadMe.txt
> >
> > Intalling manually looking quite complicated, I am not tempted to take
this direction...
> >
> > Can anybody give me a hint?
> >
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272860 (secr)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
>



From jasont at indigoindustrial.co.nz  Tue Dec 16 10:35:04 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Tue, 16 Dec 2003 22:35:04 +1300
Subject: [R] Rd files Assignment functions.
In-Reply-To: <200312152239170090.022033FD@harry.molgen.mpg.de>
References: <200312152239170090.022033FD@harry.molgen.mpg.de>
Message-ID: <3FDED1C8.6090002@indigoindustrial.co.nz>

wolski wrote (using a mail client that doesn't wrap lines):
...
> Cannot handle Rd file names containing '<'.
> These are not legal file names on all R platforms.
> Please rename the following files and try again:
>   man/[[<-.caliblist.Rd
>   man/[<-.massvectorlist.Rd
>   man/[[<-.massvectorlist.Rd
> 
> 
> Ok. It does not work.
> 
> So I renamed the file "[<-.massvectorlist.Rd" in test.Rd  And replaced the 
> name and first alias field
> 
> \name{test}
> \alias{test}
> \alias{[<-.massvectorlist}
> \title{ Replace Parts of Massvectorlist}
> ....
> 
> Now a "test" entry appears in the 00Index.html but ...Of course I can give a 
> more appropriate name but...
> But unfortunately all \link to [<-. in the \seealso section does not work than either.
> Why no [<-.massvectorlist entry appears in the 00Index.html file?
> Even if i put [<-.massvector as second alias?
> 
> Is there a way to get assignments work as other functions? 
> Or is the best solution to give a name like Assign.massvector to the Rd 
> file the name and the first \alias?

For problems like this, it's often good to see how R-core dealt with it. 
  In {path to R}/library/base/man/base.Rd ...

...
\name{Extract}
\title{Extract or Replace Parts of an Object}
\alias{Extract}
\alias{Subscript}
\alias{[}
\alias{[[}
\alias{$}
\alias{[<-}
\alias{[[<-}
...

In the base package, [, [[, and $ appear in 00Index.html, and [<- and 
[[<- don't.  I haven't seen any documentation as to why that might be, 
but I don't think it's a disaster, since help("[<-") works fine.  (If I 
had to guess -- dangerous -- I'd say the reason has something to do with 
the fact that html comments often look like this: <!-- comment --> ).

So, yes, I'd say use a special "category" name for your extract and 
assignment functions, \alias them with the actual function names, and 
don't expect the assignment functions to appear directly in the html 
version.

Cheers

Jason

-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From vito.muggeo at giustizia.it  Tue Dec 16 14:13:29 2003
From: vito.muggeo at giustizia.it (Vito Muggeo)
Date: Tue, 16 Dec 2003 14:13:29 +0100
Subject: [R] `bivariate apply'
Message-ID: <003001c3c3d6$697b1ec0$5c13070a@PROCGEN>

dear all,

Given a matrix A, say, I would like to apply a bivariate function to each
combination of its colums. That is if

myfun<-function(x,y)cor(x,y) #computes simple correlation of two vectors x
and y

then the results should be something similar to cor(A).

I tried with mapply, outer,...but without success

Can anybody help me?

many thanks in advance,
vito



From wegmann_mailinglist at gmx.net  Tue Dec 16 14:17:53 2003
From: wegmann_mailinglist at gmx.net (Martin Wegmann)
Date: Tue, 16 Dec 2003 14:17:53 +0100 (MET)
Subject: [R] mailing list for basic questions - preliminary sum up
Message-ID: <26910.1071580673@www62.gmx.net>

Dear R-user,  
 
I already received quite a lot of replies to this mail and like to do a
preliminary sum up. 
 
A few were sceptical about the use of such a beginner mailing list.  
The arguments were that people starting with R will only stay subscribed for
a short time 
until they reached the R-help "level" and therefore only beginner will teach
beginner how to 
use R.  
 
But as far as I can judge, the majority of people who replied to this mail
are medium to 
experienced user who like to help beginner but does not call themselves
highly 
experienced user as the main "answerers" on the R-help mailing list.  
 
Therefore I assume that, even though some answers might be wrong, the threat
 of 
possibly wrong answers might be minimal, due to various experienced users
who like to 
subsribe to this list. 
 
The majority of replies were positive about such a list and welcomed the
idea to 
encourage new user by providing a basic R mailing list, like the already
existent 
corresponding manuals in the contributed documentation at r-project.org. 
 
And again, this list shall only provide a basic and smooth introduction into
R and its 
capabilities.  
Questions like; "How do I make my labels in a graphic bigger? - How do I
change the 
colour? - etc." are welcome and surely would annoy the majority of R-help
user because it 
is mentioned somewhere on the first 10 pages of every manual, but people who
are used 
to click on a graphic and change it in a second would not be convinced that
R can do 
great graphics. 
 
well, I would welcome if there would be more discussion about it or to give
it a try 
(perhaps mention it on the r-project web-site) and look how productive this
mailing list 
proves to be.  
The address of the R-beginner mailing list is: 
 
https://lists.uni-wuerzburg.de/mailman/listinfo/r-beginner 
 
best regards, Martin 
 

-- 

Neu: Preissenkung f?r MMS und FreeMMS! http://www.gmx.net



From Iyue.Sung at ingenix.com  Tue Dec 16 14:32:59 2003
From: Iyue.Sung at ingenix.com (Sung, Iyue)
Date: Tue, 16 Dec 2003 08:32:59 -0500
Subject: [R] mailing list for basic questions - preliminary sum up
Message-ID: <98A6B2BB08F027408F2E8ADDC58412633BDFCD@Epid-mailserv.Epidemiology.com>

I think a separate list is redundant. 
Many basic questions do get answered.

My impression is that *who* is asking, determines the response.  
A poli-sci student asking the  same question as a more (presumably) 
technically resourceful, say CS person, is more likely to get a 
helpful response, vs a flame.

The simple questions doesn't bother me, I just ignore them.  
But, it's not my list and I can see how they may bother the 
list maintainers and 'regular' experts.  But I think it's futile 
(and not worth the raised blood pressure) to get too hot and
bothered about those postings. 

Just my opinion.

- Iyue

> -----Original Message-----
> From: Martin Wegmann [mailto:wegmann_mailinglist at gmx.net]
> Sent: Tuesday, December 16, 2003 8:18 AM
> To: R-list
> Subject: Re: [R] mailing list for basic questions - preliminary sum up
> 
> 
> Dear R-user,  
>  
> I already received quite a lot of replies to this mail and 
> like to do a
> preliminary sum up. 
>  
> A few were sceptical about the use of such a beginner mailing list.  
> The arguments were that people starting with R will only stay 
> subscribed for
> a short time 
> until they reached the R-help "level" and therefore only 
> beginner will teach
> beginner how to 
> use R.  
>  
> But as far as I can judge, the majority of people who replied 
> to this mail
> are medium to 
> experienced user who like to help beginner but does not call 
> themselves
> highly 
> experienced user as the main "answerers" on the R-help mailing list.  
>  
> Therefore I assume that, even though some answers might be 
> wrong, the threat
>  of 
> possibly wrong answers might be minimal, due to various 
> experienced users
> who like to 
> subsribe to this list. 
>  
> The majority of replies were positive about such a list and 
> welcomed the
> idea to 
> encourage new user by providing a basic R mailing list, like 
> the already
> existent 
> corresponding manuals in the contributed documentation at 
> r-project.org. 
>  
> And again, this list shall only provide a basic and smooth 
> introduction into
> R and its 
> capabilities.  
> Questions like; "How do I make my labels in a graphic bigger? 
> - How do I
> change the 
> colour? - etc." are welcome and surely would annoy the 
> majority of R-help
> user because it 
> is mentioned somewhere on the first 10 pages of every manual, 
> but people who
> are used 
> to click on a graphic and change it in a second would not be 
> convinced that
> R can do 
> great graphics. 
>  
> well, I would welcome if there would be more discussion about 
> it or to give
> it a try 
> (perhaps mention it on the r-project web-site) and look how 
> productive this
> mailing list 
> proves to be.  
> The address of the R-beginner mailing list is: 
>  
> https://lists.uni-wuerzburg.de/mailman/listinfo/r-beginner 
>  
> best regards, Martin 
>  
> 
> -- 
> 
> Neu: Preissenkung f?r MMS und FreeMMS! http://www.gmx.net
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

"Secure Server" made the following
 annotations on 12/16/2003 08:33:16 AM
------------------------------------------------------------------------------
"This e-mail, including attachments, may include confidential and/or proprietary information, and may be used only by the person or entity to which it is addressed. If the reader of this e-mail is not the intended recipient or his or her authorized agent, the reader is hereby notified that any dissemination, distribution or copying of this e-mail is prohibited. If you have received this e-mail in error, please notify the sender by replying to this message and delete this e-mail immediately."



From karlknoblich at yahoo.de  Tue Dec 16 15:21:16 2003
From: karlknoblich at yahoo.de (=?iso-8859-1?q?Karl=20Knoblick?=)
Date: Tue, 16 Dec 2003 15:21:16 +0100 (CET)
Subject: [R] Forward stepwise LOGISTIC regression
Message-ID: <20031216142116.87591.qmail@web10003.mail.yahoo.com>

Hallo!

Does anybody know how to do a forward stepwise
LOGISTIC regression? 

(I found lrm(), fastbw() and validate() in the Design
package concerning backward logistic regression - but
no forward)

Thanks!

Karl



From tlumley at u.washington.edu  Tue Dec 16 15:24:00 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 16 Dec 2003 06:24:00 -0800 (PST)
Subject: [R] `bivariate apply'
In-Reply-To: <003001c3c3d6$697b1ec0$5c13070a@PROCGEN>
References: <003001c3c3d6$697b1ec0$5c13070a@PROCGEN>
Message-ID: <Pine.A41.4.58.0312160609010.21788@homer12.u.washington.edu>

On Tue, 16 Dec 2003, Vito Muggeo wrote:

> dear all,
>
> Given a matrix A, say, I would like to apply a bivariate function to each
> combination of its colums. That is if
>
> myfun<-function(x,y)cor(x,y) #computes simple correlation of two vectors x
> and y
>
> then the results should be something similar to cor(A).
>
> I tried with mapply, outer,...but without success
>

I don't think there's anything better than a simple loop.   If you insist
on making the loops invisible you could do eg:

pwapply<-function(mat, FUN, ...){
	nc<-NCOL(mat)
	i<-rep(1:nc, nc)
 	j<-rep(1:nc, each=nc)
	rval<-mapply(function(ii,ji) FUN(mat[,ii], mat[,ji], ...), i, j)
	matrix(rval, nc=nc)
}



	-thomas



From s-plus at wiwi.uni-bielefeld.de  Tue Dec 16 15:45:00 2003
From: s-plus at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Tue, 16 Dec 2003 15:45:00 +0100
Subject: [R] `bivariate apply'
References: <003001c3c3d6$697b1ec0$5c13070a@PROCGEN>
Message-ID: <3FDF1A6C.5090409@wiwi.uni-bielefeld.de>

Vito Muggeo wrote:

>dear all,
>
>Given a matrix A, say, I would like to apply a bivariate function to each
>combination of its colums. That is if
>
>myfun<-function(x,y)cor(x,y) #computes simple correlation of two vectors x
>and y
>
>then the results should be something similar to cor(A).
>
>I tried with mapply, outer,...but without success
>
>Can anybody help me?
>
>many thanks in advance,
>vito
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>
what about

myfun<-function(x,fun=cor){
 cl<-matrix(1:ncol(x),ncol(x),ncol(x))
 cl<-cbind(as.vector(cl),as.vector(t(cl)))
 res<-apply(cl,1,function(xx)fun(x[,xx[1]],x[,xx[2]]))
 matrix(res,ncol(x),ncol(x))
}

Peter Wolf



From ripley at stats.ox.ac.uk  Tue Dec 16 16:01:00 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 16 Dec 2003 15:01:00 +0000 (GMT)
Subject: [R] Forward stepwise LOGISTIC regression
In-Reply-To: <20031216142116.87591.qmail@web10003.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0312161458180.13785-100000@gannet.stats>

On Tue, 16 Dec 2003, Karl Knoblick wrote:

> Does anybody know how to do a forward stepwise
> LOGISTIC regression? 
> 
> (I found lrm(), fastbw() and validate() in the Design
> package concerning backward logistic regression - but
> no forward)

Try using R itself: glm + step.

help.search("logistic")
help.search("stepwise")

found both of those, so how come *you* failed to?

I'll not comment on how bad an idea this would be, as maybe you just wish 
to illustrate that.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jc at or.psychology.dal.ca  Tue Dec 16 16:04:07 2003
From: jc at or.psychology.dal.ca (John Christie)
Date: Tue, 16 Dec 2003 11:04:07 -0400
Subject: [R] box correction or Huynh-Feldt epsilon
Message-ID: <18EC8379-2FD9-11D8-AD78-000A956DE534@or.psychology.dal.ca>

Following up the several unanswered requests for a sphericity or 
circularity test in the archives, those who wish to test should feel 
free to use the following function.  If anyone notices errors please 
correct.  The returned value is an epsilon one can use to correct 
degrees of freedom.  It is less conservative than the 
Greenhouse-Geisser I believe.

# This returns the Huynh-Feldt or "Box Correction" for degrees of 
freedom
hf <- function(m){
	# m is a matrix with subjects as rows and conditions as columns
	# note that checking for worst case scenarios F correction first might
	# be a good idea using J/(J-1) as the df correction factor
	n<- length(m[,1])
	J<-length(m[1,])
	X<-cov(m)*(n-1)
	r<- length(X[,1])
	D<-0
	for (i in 1: r) D<- D+ X[i,i]
	D<-D/r
	SPm<- mean(X)
	SPm2<- sum(X^2)
	SSrm<-0
	for (i in 1: r) SSrm<- SSrm + mean(X[i,])^2
	epsilon<- (J^2*(D-SPm)^2) / ((J-1) * (SPm2 - 2*J*SSrm + J^2*SPm^2))
	epsilon
}



From Pascal.Niklaus at unibas.ch  Tue Dec 16 16:09:47 2003
From: Pascal.Niklaus at unibas.ch (Pascal A. Niklaus)
Date: Tue, 16 Dec 2003 16:09:47 +0100
Subject: [R] mailing list for basic questions - preliminary sum up
In-Reply-To: <26910.1071580673@www62.gmx.net>
References: <26910.1071580673@www62.gmx.net>
Message-ID: <3FDF203B.5060805@unibas.ch>

Martin Wegmann wrote:

>Dear R-user,  
> 
>I already received quite a lot of replies to this mail and like to do a
>preliminary sum up. 
> 
>A few were sceptical about the use of such a beginner mailing list.  
>The arguments were that people starting with R will only stay subscribed for
>a short time 
>until they reached the R-help "level" and therefore only beginner will teach
>beginner how to 
>use R.  
> 
>But as far as I can judge, the majority of people who replied to this mail
>are medium to 
>experienced user who like to help beginner but does not call themselves
>highly 
>experienced user as the main "answerers" on the R-help mailing list.  
> 
>Therefore I assume that, even though some answers might be wrong, the threat
> of 
>possibly wrong answers might be minimal, due to various experienced users
>who like to 
>subsribe to this list. 
> 
>The majority of replies were positive about such a list and welcomed the
>idea to 
>encourage new user by providing a basic R mailing list, like the already
>existent 
>corresponding manuals in the contributed documentation at r-project.org. 
> 
>And again, this list shall only provide a basic and smooth introduction into
>R and its 
>capabilities.  
>Questions like; "How do I make my labels in a graphic bigger? - How do I
>change the 
>colour? - etc." are welcome and surely would annoy the majority of R-help
>user because it 
>is mentioned somewhere on the first 10 pages of every manual, but people who
>are used 
>to click on a graphic and change it in a second would not be convinced that
>R can do 
>great graphics. 
> 
>well, I would welcome if there would be more discussion about it or to give
>it a try 
>(perhaps mention it on the r-project web-site) and look how productive this
>mailing list 
>proves to be.  
>The address of the R-beginner mailing list is: 
> 
>https://lists.uni-wuerzburg.de/mailman/listinfo/r-beginner 
> 
>best regards, Martin 
>
Myself being relatively new to R, I'd like to comment on the idea of a 
new list:

- In my experience even *very* basic questions *relating to the R 
language* do get answered on r-help. I'm impressed by how much time some 
members of the R core team spend answering relatively basic questions, 
and by how elaborate their answers generally are. So I cannot see much 
need for a new R mailing list. There are these excellent mailing list 
archives, so why "fragment" this list?

- The kind of questions that generally do not get answered are the ones 
which involve statistical issues ("What is the appropriate model for my 
data?", "what is the difference between lm and aov?", "Is that model 
correct?" etc...). Since many beginners to R are probably also beginners 
in statistics, I could see some need for a list dedicated to statistical 
topics somehow related to R. For example, many people new to R seem to 
have problems adapting some basic mixed-effects models and repeated 
measures analyses they ran in SAS od SPSS. These kind of questions 
generally do not get answered, and some hints on how to adapt some of 
the examples in Pinheiro & Bates could be helpful. But this is unlikely 
to happen on a separate r-beginner list too.

Pascal



From rossini at blindglobe.net  Tue Dec 16 16:20:52 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Tue, 16 Dec 2003 07:20:52 -0800
Subject: [R] mailing list for basic questions - preliminary sum up
In-Reply-To: <3FDF203B.5060805@unibas.ch> (Pascal A. Niklaus's message of
	"Tue, 16 Dec 2003 16:09:47 +0100")
References: <26910.1071580673@www62.gmx.net> <3FDF203B.5060805@unibas.ch>
Message-ID: <85d6aovlez.fsf@blindglobe.net>

"Pascal A. Niklaus" <Pascal.Niklaus at unibas.ch> writes:

> - In my experience even *very* basic questions *relating to the R
> language* do get answered on r-help. I'm impressed by how much time
> some members of the R core team spend answering relatively basic
> questions, and by how elaborate their answers generally are. So I
> cannot see much need for a new R mailing list. There are these
> excellent mailing list archives, so why "fragment" this list?

To follow up, well-thought through basic questions do get answered; in
particular, they can be useful for those of us writing packages,
documentation, etc.  

I have a sense that it is the quality of the question (details of what
is intended to do, or not known, signs of using other sources of
materials which folks have spent years on, no signs that this is a "do
my work for me" question) rather than the level of the question, that
is an issue.

best,
-tony

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From baron at psych.upenn.edu  Tue Dec 16 16:31:32 2003
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Tue, 16 Dec 2003 10:31:32 -0500
Subject: [R] mailing list for basic questions - preliminary sum up
In-Reply-To: <3FDF203B.5060805@unibas.ch>
References: <26910.1071580673@www62.gmx.net> <3FDF203B.5060805@unibas.ch>
Message-ID: <20031216153132.GA15625@mail2.sas.upenn.edu>

On 12/16/03 16:09, Pascal A. Niklaus wrote:
>- In my experience even *very* basic questions *relating to the R
>language* do get answered on r-help. I'm impressed by how much time some
>members of the R core team spend answering relatively basic questions,
>and by how elaborate their answers generally are. So I cannot see much
>need for a new R mailing list. There are these excellent mailing list
>archives, so why "fragment" this list?

Yikes!  I now realize that my initial support of the idea for the
list must be qualified considerably.  If this list is started,
its archives become quite important.  Who will do that?  And will
I have the time/energy/inclination to incorporate it into my own
search database?  I'd rather not have one more thing to do.  But
if the list gets off the ground - and it seems already at least
close to the end of the runway - I'll have to do it eventually in
order to have a complete archive.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron
R page:               http://finzi.psych.upenn.edu/



From spencer.graves at pdf.com  Tue Dec 16 17:20:32 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 16 Dec 2003 08:20:32 -0800
Subject: [R] mailing list for basic questions - preliminary sum up
In-Reply-To: <85d6aovlez.fsf@blindglobe.net>
References: <26910.1071580673@www62.gmx.net> <3FDF203B.5060805@unibas.ch>
	<85d6aovlez.fsf@blindglobe.net>
Message-ID: <3FDF30D0.7060401@pdf.com>

      I agree with Tony's observation that well thought out questions 
are more likely to receive an answer than something that is long, 
rambling, and poorly focused.  Many questions take more time to read 
than I have available, so I don't bother.  I like questions that include 
toy examples in a few lines of code that I can copy from an email into R 
and test ideas.  Careful formatting that looks pretty in an email is an 
obstacle for me, because it increases the work required to get it into 
R.  Many questioners could answer their own problems in the process of 
generating such a toy example.  When they can't, that exercise helps 
them focus the question, which makes it easier for potential respondents 
to understand the problem and reply.  Without that, I must either 
generate a toy example myself (which I've done many times) or respond 
with untested code and risk looking stupid when my untested suggestion 
doesn't work. 

      hope this helps. 
      spencer graves

A.J. Rossini wrote:

>"Pascal A. Niklaus" <Pascal.Niklaus at unibas.ch> writes:
>
>  
>
>>- In my experience even *very* basic questions *relating to the R
>>language* do get answered on r-help. I'm impressed by how much time
>>some members of the R core team spend answering relatively basic
>>questions, and by how elaborate their answers generally are. So I
>>cannot see much need for a new R mailing list. There are these
>>excellent mailing list archives, so why "fragment" this list?
>>    
>>
>
>To follow up, well-thought through basic questions do get answered; in
>particular, they can be useful for those of us writing packages,
>documentation, etc.  
>
>I have a sense that it is the quality of the question (details of what
>is intended to do, or not known, signs of using other sources of
>materials which folks have spent years on, no signs that this is a "do
>my work for me" question) rather than the level of the question, that
>is an issue.
>
>best,
>-tony
>
>  
>



From ggrothendieck at myway.com  Tue Dec 16 17:34:06 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 16 Dec 2003 11:34:06 -0500 (EST)
Subject: [R] `bivariate apply'
Message-ID: <20031216163406.15271397F@mprdmxin.myway.com>



The following:

- uses apply to turn the input matrix, x, into a vector of lists, 
  each of which represents one column
- this vector of lists is replicated two different ways and
- mapply is used on the replicated structures.
- finally, reform the result into a matrix

This solution has the property that it does not deal with indices
at all.

biapply <- function(x,f=cor) {
   k <- NCOL( x )
   x <- apply( x, 2, list )
   ff <- function(x,y) f( unlist(x), unlist(y) )
   matrix( mapply( ff, rep(x,rep(k,k)), rep(x,k) ), k, k )
}

# test
x <- matrix( 1:12, 4, 3 )
biapply( x, crossprod )
biapply( x )

--- 
Date: Tue, 16 Dec 2003 14:13:29 +0100 
From: Vito Muggeo <vito.muggeo at giustizia.it>
To: <r-help at stat.math.ethz.ch> 
Subject: [R] `bivariate apply' 

 
 
dear all,

Given a matrix A, say, I would like to apply a bivariate function to each
combination of its colums. That is if

myfun<-function(x,y)cor(x,y) #computes simple correlation of two vectors x
and y

then the results should be something similar to cor(A).

I tried with mapply, outer,...but without success

Can anybody help me?

many thanks in advance,
vito

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ggrothendieck at myway.com  Tue Dec 16 17:39:39 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 16 Dec 2003 11:39:39 -0500 (EST)
Subject: [R] mailing list for basic questions - preliminary sum up
Message-ID: <20031216163939.B0A31396F@mprdmxin.myway.com>



If the archives were placed on www.r-project.org then googling for

   whatever site:r-project.org

would find anything in them along with anything already on the
site.


---
Date: Tue, 16 Dec 2003 10:31:32 -0500 
From: Jonathan Baron <baron at psych.upenn.edu>
To: Pascal A. Niklaus <Pascal.Niklaus at unibas.ch> 
Cc: <r-help at stat.math.ethz.ch> 
Subject: Re: [R] mailing list for basic questions - preliminary sum up 

 
 
On 12/16/03 16:09, Pascal A. Niklaus wrote:
>- In my experience even *very* basic questions *relating to the R
>language* do get answered on r-help. I'm impressed by how much time some
>members of the R core team spend answering relatively basic questions,
>and by how elaborate their answers generally are. So I cannot see much
>need for a new R mailing list. There are these excellent mailing list
>archives, so why "fragment" this list?

Yikes! I now realize that my initial support of the idea for the
list must be qualified considerably. If this list is started,
its archives become quite important. Who will do that? And will
I have the time/energy/inclination to incorporate it into my own
search database? I'd rather not have one more thing to do. But
if the list gets off the ground - and it seems already at least
close to the end of the runway - I'll have to do it eventually in
order to have a complete archive.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron
R page: http://finzi.psych.upenn.edu/

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help




 --- On Tue 12/16, Jonathan Baron < baron at psych.upenn.edu > wrote:
From: Jonathan Baron [mailto: baron at psych.upenn.edu]
To: Pascal.Niklaus at unibas.ch
     Cc: r-help at stat.math.ethz.ch
Date: Tue, 16 Dec 2003 10:31:32 -0500
Subject: Re: [R] mailing list for basic questions - preliminary sum up

On 12/16/03 16:09, Pascal A. Niklaus wrote:<br>>- In my experience even *very* basic questions *relating to the R<br>>language* do get answered on r-help. I'm impressed by how much time some<br>>members of the R core team spend answering relatively basic questions,<br>>and by how elaborate their answers generally are. So I cannot see much<br>>need for a new R mailing list. There are these excellent mailing list<br>>archives, so why "fragment" this list?<br><br>Yikes!  I now realize that my initial support of the idea for the<br>list must be qualified considerably.  If this list is started,<br>its archives become quite important.  Who will do that?  And will<br>I have the time/energy/inclination to incorporate it into my own<br>search database?  I'd rather not have one more thing to do.  But<br>if the list gets off the ground - and it seems already at least<br>close to the end of the runway - I'll have to do it eventually in<br>order to have a complete archive.<br><br>Jon<br>-- <br>Jonathan Baron, Professor of Psychology, University of Pennsylvania<br>Home page:            http://www.sas.upenn.edu/~baron<br>R page:               http://finzi.psych.upenn.edu/<br><br>______________________________________________<br>R-help at stat.math.ethz.ch mailing list<br>https://www.stat.math.ethz.ch/mailman/listinfo/r-help<br>



From silviarf83 at hotmail.com  Tue Dec 16 18:45:49 2003
From: silviarf83 at hotmail.com (Silvia Perez Martin)
Date: Tue, 16 Dec 2003 18:45:49 +0100
Subject: [R] Random Numbers
Message-ID: <Sea2-F5502iEFkOfVRN0003a143@hotmail.com>

Hello

I?m a student from Spain. I couldn?t find something about R and I was asking 
if someone could tell me which generator of random numbers use "rnorm" and 
"runif". I think I have discovered that in runif they use the inversion 
method, but I don?t find any clue where they use the Super-duper
algorithm or the Marsaglia one, as I have read.

Thanks and sorry for my english.

_________________________________________________________________
Deja tu CV y recibe ofertas personalizadas de trabajo en tu buz?n.



From spencer.graves at pdf.com  Tue Dec 16 19:05:50 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 16 Dec 2003 10:05:50 -0800
Subject: [R] Random Numbers
In-Reply-To: <Sea2-F5502iEFkOfVRN0003a143@hotmail.com>
References: <Sea2-F5502iEFkOfVRN0003a143@hotmail.com>
Message-ID: <3FDF497E.6020905@pdf.com>

Have you looked at "?set.seed"?  This provides some detail in R 1.8.1 
for Windows. 

hope this helps. 
spencer graves
p.d.  No hay que desculparse por su ingl?s.  Est? claro. 

Silvia Perez Martin wrote:

> Hello
>
> I?m a student from Spain. I couldn?t find something about R and I was 
> asking if someone could tell me which generator of random numbers use 
> "rnorm" and "runif". I think I have discovered that in runif they use 
> the inversion method, but I don?t find any clue where they use the 
> Super-duper
> algorithm or the Marsaglia one, as I have read.
>
> Thanks and sorry for my english.
>
> _________________________________________________________________
> Deja tu CV y recibe ofertas personalizadas de trabajo en tu buz?n.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From hdoran at nasdc.org  Tue Dec 16 19:06:44 2003
From: hdoran at nasdc.org (Harold Doran)
Date: Tue, 16 Dec 2003 13:06:44 -0500
Subject: [R] error constraints in lme
Message-ID: <66578BFC0BA55348B5907A0F798EE9307A2C16@ernesto.NASDC.ORG>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031216/b8f418bf/attachment.pl

From rvaradha at jhsph.edu  Tue Dec 16 19:24:26 2003
From: rvaradha at jhsph.edu (Ravi Varadhan)
Date: Tue, 16 Dec 2003 13:24:26 -0500
Subject: [R] Random Numbers
Message-ID: <486ff84821e8.4821e8486ff8@jhsph.edu>

Take a look at the help for "RNGkind" as follows:

?RNGkind


Ravi.

----- Original Message -----
From: Silvia Perez Martin <silviarf83 at hotmail.com>
Date: Tuesday, December 16, 2003 12:45 pm
Subject: [R] Random Numbers

> Hello
> 
> I?m a student from Spain. I couldn?t find something about R and I 
> was asking 
> if someone could tell me which generator of random numbers use 
> "rnorm" and 
> "runif". I think I have discovered that in runif they use the 
> inversion 
> method, but I don?t find any clue where they use the Super-duper
> algorithm or the Marsaglia one, as I have read.
> 
> Thanks and sorry for my english.
> 
> _________________________________________________________________
> Deja tu CV y recibe ofertas personalizadas de trabajo en tu buz?n.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From angel_lul at hotmail.com  Tue Dec 16 20:11:02 2003
From: angel_lul at hotmail.com (Angel)
Date: Tue, 16 Dec 2003 20:11:02 +0100
Subject: [R] (no subject)
References: <20031211181414.43424.qmail@web20311.mail.yahoo.com>
Message-ID: <Law11-OE481TZ14eBa10000ba44@hotmail.com>

Hola, he visto que no has tenido mucho exito con tu consulta.
Esta ayuda es principalmente en ingles, esta puede ser una de las razones.
Otras es que has sido muy poco conciso/a con tu pregunta.
De que tienes dudas exactamente, has leido la documentacion del paquete y de
R. Si lo has hecho, pon un ejemplo con tu problema o duda.
Y, repito, preferentemente en ingles.
Angel

----- Original Message -----
From: "ruddi rodriguez" <rudiscu at yahoo.es>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, December 11, 2003 7:14 PM
Subject: [R] (no subject)


Hola

Hace poco comence a utilizar R y tengo dudas como utilizar el paquete de
tree
saludos ruddi


---------------------------------
Yahoo! Sorteos
?Ya puedes comprar Loter?a de Navidad!
[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From tlumley at u.washington.edu  Tue Dec 16 20:07:25 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 16 Dec 2003 11:07:25 -0800 (PST)
Subject: [R] Random Numbers
In-Reply-To: <Sea2-F5502iEFkOfVRN0003a143@hotmail.com>
References: <Sea2-F5502iEFkOfVRN0003a143@hotmail.com>
Message-ID: <Pine.A41.4.58.0312161100560.118504@homer11.u.washington.edu>

On Tue, 16 Dec 2003, Silvia Perez Martin wrote:

> Hello
>
> I?m a student from Spain. I couldn?t find something about R and I was asking
> if someone could tell me which generator of random numbers use "rnorm" and
> "runif". I think I have discovered that in runif they use the inversion
> method, but I don?t find any clue where they use the Super-duper
> algorithm or the Marsaglia one, as I have read.
>

All the random number generators work by transforming a common stream of
random  32-bit integers.  The user can choose what generator to use for
this common stream.  The default  is "Mersenne-Twister".
?RNGkind will tell you how to choose other generators.

In addition, the user can choose how rnorm() transforms this stream of
32-bit numbers to the Normal distribution.  This is also covered in
?RNGkind.  The default is inversion.

Finally, users can supply either their own generator for the stream of
32-bit numbers or for the transformation to a Normal distribution.

	-thomas



From alex_s_42 at yahoo.com  Tue Dec 16 22:52:27 2003
From: alex_s_42 at yahoo.com (Alexander Sirotkin [at Yahoo])
Date: Tue, 16 Dec 2003 13:52:27 -0800 (PST)
Subject: [R] Difference between summary.lm() and summary.aov()
In-Reply-To: <x23cbwix7s.fsf@biostat.ku.dk>
Message-ID: <20031216215227.73555.qmail@web60006.mail.yahoo.com>

Thanks a lot to everybody. Two more questions, if you
don't mind :

How anova() treats non-categorical variables, such as
severity in my case ? I was under impression that
ANOVA is defined for categorical variables only.

I read about drop1() and I understand that it performs
F-test for nested models, correct me if I'm wrong. It
is unclear to me, however, how it manages to do this
F-test for interactions ?

Thanks a lot.

--- Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> "Alexander Sirotkin [at Yahoo]"
> <alex_s_42 at yahoo.com> writes:
> 
> > John,
> > 
> > What you are saying is that any conclusion I can
> make
> > from summary.aov (for instance, to answer a
> question
> > if physician is a significant variable) will not
> be
> > correct ?
> 
> Summary.aov is for summarizing aov objects, so
> you're lucky to get
> something that is sensible at all. You should use
> anova() to get
> analysis of variance tables. These are sequential so
> that you can use
> them (give or take some quibbles about the residual
> variance) for
> reducing the model from the "bottom up". I.e. if you
> place "physician"
> last, you get the F test for whether that variable
> is significant.
> However, a more convenient way of getting that
> result is to use
> drop1(). Even then there's no simple relation to the
> two
> t-tests, except that the F test tests the hypothesis
> that *both*
> coefficients are zero, where the t-tests do so
> individually. 
>  
> 
> > --- John Fox <jfox at mcmaster.ca> wrote:
> > > Dear Spencer and Alexander,
> > > 
> > > In this case, physician is apparently a factor
> with
> > > three levels, so 
> > > summary.aov() gives you a sequential ANOVA,
> > > equivalent to what you'd get 
> > > from anova(). There no simple relationship
> between
> > > the F-statistic for 
> > > physician, which has 2 df in the numerator, and
> the
> > > two t's. (By the way, I 
> > > doubt whether a sequential ANOVA is what's
> wanted
> > > here.)
> > > 
> > > Regards,
> > >   John
> > > 
> > > At 09:17 AM 12/6/2003 -0800, Spencer Graves
> wrote:
> > > >      The square of a Student's t with "df"
> degrees
> > > of freedom is an F 
> > > > distribution with 1 and "df" degrees of
> freedom.
> > > >      hope this helps.  spencer graves
> > > >
> > > >Alexander Sirotkin [at Yahoo] wrote:
> > > >
> > > >>I have a simple linear model (fitted with
> lm())
> > > with 2
> > > >>independant
> > > >>variables : one categorical and one integer.
> > > >>
> > > >>When I run summary.lm() on this model, I get a
> > > >>standard linear
> > > >>regression summary (in which one categorical
> > > variable
> > > >>has to be
> > > >>converted into many indicator variables) which
> > > looks
> > > >>like :
> > > >>
> > > >>            Estimate Std. Error t value
> Pr(>|t|)
> > > >>(Intercept)  -3595.3     2767.1  -1.299  
> 0.2005
> > > >>physicianB     802.0     2289.5   0.350  
> 0.7277
> > > >>physicianC    4906.8     2419.8   2.028  
> 0.0485 *
> > > >>severity      7554.4      906.3   8.336
> 1.12e-10
> > > ***
> > > >>
> > > >>and when I run summary.aov() I get similar
> ANOVA
> > > table
> > > >>:
> > > >>           Df     Sum Sq    Mean Sq F value   
> > > Pr(>F)
> > > >>physician    2  294559803  147279901  3.3557  
> > > 0.04381
> > > >>*
> > > >>severity     1 3049694210 3049694210 69.4864
> > > 1.124e-10
> > > >>***
> > > >>Residuals   45 1975007569   43889057
> > > >>
> > > >>What is absolutely unclear to me is how
> F-value
> > > and
> > > >>Pr(>F) for the
> > > >>categorical "physician" variable of the
> > > summary.aov()
> > > >>is calculated
> > > >>from the t-value of the summary.lm() table.
> > > >>
> > > >>I looked at the summary.aov() source code but
> > > still
> > > >>could not figure
> > > >>it.
> > > >>
> > > >>Thanks a lot.
> > > >>
> > > >>__________________________________
> > > >>
> > 
> > > >>
> > > >>______________________________________________
> > > >>R-help at stat.math.ethz.ch mailing list
> > >
> >
>
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > > >>
> > > >
> > > >______________________________________________
> > > >R-help at stat.math.ethz.ch mailing list
> > >
> >
>
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > > 
> > >
> >
>
-----------------------------------------------------
> > > John Fox
> > > Department of Sociology
> > > McMaster University
> > > Hamilton, Ontario, Canada L8S 4M4
> > > email: jfox at mcmaster.ca
> > > phone: 905-525-9140x23604
> > > web: www.socsci.mcmaster.ca/jfox
> > >
> >
>
-----------------------------------------------------
> > >
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> >
>
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > 
> 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej
> 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N 
>  
>  (*) \(*) -- University of Copenhagen   Denmark     
> Ph: (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)            
> FAX: (+45) 35327907



From yunfang at yahoo-inc.com  Tue Dec 16 23:19:19 2003
From: yunfang at yahoo-inc.com (Yun-Fang Juan)
Date: Tue, 16 Dec 2003 14:19:19 -0800
Subject: [R] Fw: [S] plot stacked bar chart in R
Message-ID: <04d001c3c422$a6a3a320$90ea7ecf@YUNFANG2>

posting the question in r-help@ to get more feedback :-)

thanks,

Yun-Fang
----- Original Message ----- 
From: "Yun-Fang Juan" <yunfang at yahoo-inc.com>
To: <s-news at wubios.wustl.edu>
Sent: Tuesday, December 16, 2003 2:04 PM
Subject: [S] plot stacked bar chart in R


> Hi,
> I am trying to plot a stacked bar chart in R but am not able to find the
> documentation.
> 
> The input data format is like the following
> 
> 
> CAT1 CAT2 Count
> A a  234
> A b  758
> A c  156
> B a  753
> B b  568
> B c  684
> C a  356
> C b  564
> C c  256
> D a  123
> D b 165
> D c  754
> 
> please advice.
> 
> 
> thanks,
> 
> 
> Yun-Fang
> 
> 
> --------------------------------------------------------------------
> This message was distributed by s-news at lists.biostat.wustl.edu.  To
> ...(s-news.. clipped)...

> 
>



From sundar.dorai-raj at pdf.com  Tue Dec 16 23:29:55 2003
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 16 Dec 2003 16:29:55 -0600
Subject: [R] Fw: [S] plot stacked bar chart in R
In-Reply-To: <04d001c3c422$a6a3a320$90ea7ecf@YUNFANG2>
References: <04d001c3c422$a6a3a320$90ea7ecf@YUNFANG2>
Message-ID: <3FDF8763.9090505@pdf.com>

Did you look at ?barplot?

# x is a data.frame from below
x2 <- do.call("rbind", split(x$Count, x$CAT1))
dimnames(x2)[[2]] <- letters[1:3]

barplot(x2, legend = TRUE)


Sundar

Yun-Fang Juan wrote:

> posting the question in r-help@ to get more feedback :-)
> 
> thanks,
> 
> Yun-Fang
> ----- Original Message ----- 
> From: "Yun-Fang Juan" <yunfang at yahoo-inc.com>
> To: <s-news at wubios.wustl.edu>
> Sent: Tuesday, December 16, 2003 2:04 PM
> Subject: [S] plot stacked bar chart in R
> 
> 
> 
>>Hi,
>>I am trying to plot a stacked bar chart in R but am not able to find the
>>documentation.
>>
>>The input data format is like the following
>>
>>
>>CAT1 CAT2 Count
>>A a  234
>>A b  758
>>A c  156
>>B a  753
>>B b  568
>>B c  684
>>C a  356
>>C b  564
>>C c  256
>>D a  123
>>D b 165
>>D c  754
>>
>>please advice.
>>
>>
>>thanks,
>>
>>
>>Yun-Fang
>>
>>
>>--------------------------------------------------------------------
>>This message was distributed by s-news at lists.biostat.wustl.edu.  To
>>...(s-news.. clipped)...
> 
> 
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From abunn at montana.edu  Tue Dec 16 23:27:45 2003
From: abunn at montana.edu (Andy Bunn)
Date: Tue, 16 Dec 2003 15:27:45 -0700
Subject: [R] Fw: [S] plot stacked bar chart in R
In-Reply-To: <04d001c3c422$a6a3a320$90ea7ecf@YUNFANG2>
Message-ID: <003b01c3c423$e5f31b40$78f05a99@msu.montana.edu>

See ?barplot and pay attention to the 'height' and 'beside' arguments.
They produce very nice stacked plots.

Some of the examples using the VA deaths dataset displays stacked plots.

HTH, Andy



From John.Fieberg at dnr.state.mn.us  Wed Dec 17 00:00:06 2003
From: John.Fieberg at dnr.state.mn.us (John Fieberg)
Date: Tue, 16 Dec 2003 17:00:06 -0600
Subject: [R] Help w/ termplot & predict.coxph/ns
Message-ID: <sfdf3a71.008@co5.dnr.state.mn.us>

I am fitting a cox PH model w/ 2 predictors, x1 = 0/1 treatment variable
and x2=continuous variable.  I am using natural splines (ns) to model
the effect of x2.  

I would like to examine the estimated effect of x2 on the hazard.  I
have tried various approaches (below; let model.fit= fitted model using
coxph in survival library):  

1.  The simplest method appears to be using termplot(model.fit).  This
appears to work fine as long as I include the treatment variable in the
model.  However, without the treatment effect in the model termplot
returns a "?" prompt without plotting the effect of x2.  This happens
whenever I consider any model containing only one predictor variable
modeled using ns().  Any suggestions? Also, I am presuming that these
plots indicate the effect of x2 averaged over the effect of x1 (is this
correct?).  Ultimately, I would like to be able to produce similar plots
for both x1=0 and x1=1.  

2.  Using predict(model.fit, newdata, type="lp", safe=T)...as far as I
can tell, this does not appear to give results that are consistent w/
termplot.  For my model, the effect of x1 is VERY small (not
statistically significant) and when I overlay the results w/ those
produced by termplot (using "lines") they do not line up at all:

# Predict linear predictor vs. x2 for x1 =1
newdata<-data.frame(x1=rep(1,60), x2=seq(0,30, length=60))
newpred<-predict(model.fit, newdata, type="lp", safe=T)
termplot(model.fit)
lines(newdata[,2], newpred)

Interestingly enough, predict(model.fit) does give back the correct
values for the actual data set used in the fitting: 
max(predict(model.fit)-model.fit$linear.predictors)=0. Am I missing
something here?

3.  Using the fitted coeficients:

# Coefficients
fitc<-coef(model.fit)

#  Predictors
basis <- ns(x2, df = 3) ; # df= 3 were used to fit the model
newx2<- seq(0,30,length=60)

# new data in the coords of the basis and x1=1 for all obs
newdata2<-cbind(rep(1,60),predict(basis, newx2))
newpred<-newdata2%*%fitc

termplot(model.fit, ylim=c(0,3))
lines(newx2, newpred)

This method gives predictions that appear to be proportional to the
results in termplot - but all predicted values are higher.  I am missing
something here?

Ideally, I would like to use this method as it appears the most
flexible -allowing one to obtain the linear predictors for various
combinations of x1 and x2.  

4.  Using Frank Harrell's Design library and the cph function and
plot.Design.  This appears to work - giving results that agree w/
termplot (although the results are slightly different because of the
different basis functions used).  However, I am more comfortable
(currently) using the coxph function than cph and have had problems w/
cph when trying to fit more complicated (conditional) models w/ multiple
obs per subject. 

Any help would be greatly appreciated.  I am using R version 1.7.1 on
Windows 2000. I would be glad to share the data set and code directly if
it would be helpful.

John



John Fieberg, Ph.D.
Wildlife Biometrician, MN DNR
5463-C W. Broadway
Forest Lake, MN 55434
Phone: (651) 296-2704



From wegmann_mailinglist at gmx.net  Wed Dec 17 00:49:15 2003
From: wegmann_mailinglist at gmx.net (Martin Wegmann)
Date: Wed, 17 Dec 2003 00:49:15 +0100
Subject: [R] mailing list for basic questions - preliminary sum up
In-Reply-To: <3FDF30D0.7060401@pdf.com>
References: <26910.1071580673@www62.gmx.net> <85d6aovlez.fsf@blindglobe.net>
	<3FDF30D0.7060401@pdf.com>
Message-ID: <200312170049.15678.wegmann_mailinglist@gmx.net>

Hello, 

I agree completely that well thought out questions are important to receive 
good and quick replies and I agree as well that the replies on the R-help 
list are very good and helpful.
But I had to learn and I am still learing how to write good questions and 
appreciate Spencer's explanantion how a good question should look like in his 
opinion. 

I am not sure how this new mailing list might evolve. 
It might be that the R-beginner list takes some load of the R-help list by 
reducing the amount of "basic" questions which won't be questioned anymore 
here (what aren't many) and that new user might be taught to post "good" 
question before they start posting to R-help.
If it proves to be ineffective or might affect R-help in some unwanted manner 
it would be an easy one to shut it down. 

I doubt that it will split the R-help list - in my opinion it is unlikely that 
medium/experienced R user who will subscribe to R-beginner will unsubscribe 
from the R-help list. 
Moreover people starting with R are less likely to send any mails to this 
list, some do and are refered in most cases to the manuals. 
When I started R I looked through the archive and because I did not understand 
even one question, I was intimidated by this list and did not send any mail 
until a few weeks later (that was not because of the statistics but the 
commands)
For this kind of people the R-beginner list is thought - to encourage them to 
send "stupid" questions during their first steps in R. 

They shall recognize questions they would have asked themselves.
Therefore I think that the quality of the question is in this case less 
important than it's level.

I hope I did not misunderstood some points ,-)

best regards Martin



On Tuesday 16 December 2003 17:20, Spencer Graves wrote:
>       I agree with Tony's observation that well thought out questions
> are more likely to receive an answer than something that is long,
> rambling, and poorly focused.  Many questions take more time to read
> than I have available, so I don't bother.  I like questions that include
> toy examples in a few lines of code that I can copy from an email into R
> and test ideas.  Careful formatting that looks pretty in an email is an
> obstacle for me, because it increases the work required to get it into
> R.  Many questioners could answer their own problems in the process of
> generating such a toy example.  When they can't, that exercise helps
> them focus the question, which makes it easier for potential respondents
> to understand the problem and reply.  Without that, I must either
> generate a toy example myself (which I've done many times) or respond
> with untested code and risk looking stupid when my untested suggestion
> doesn't work.
>
>       hope this helps.
>       spencer graves
>
> A.J. Rossini wrote:
> >"Pascal A. Niklaus" <Pascal.Niklaus at unibas.ch> writes:
> >>- In my experience even *very* basic questions *relating to the R
> >>language* do get answered on r-help. I'm impressed by how much time
> >>some members of the R core team spend answering relatively basic
> >>questions, and by how elaborate their answers generally are. So I
> >>cannot see much need for a new R mailing list. There are these
> >>excellent mailing list archives, so why "fragment" this list?
> >
> >To follow up, well-thought through basic questions do get answered; in
> >particular, they can be useful for those of us writing packages,
> >documentation, etc.
> >
> >I have a sense that it is the quality of the question (details of what
> >is intended to do, or not known, signs of using other sources of
> >materials which folks have spent years on, no signs that this is a "do
> >my work for me" question) rather than the level of the question, that
> >is an issue.
> >
> >best,
> >-tony
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ggrothendieck at myway.com  Wed Dec 17 01:43:05 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 16 Dec 2003 19:43:05 -0500 (EST)
Subject: [R] Axe time of series in format yy-mm-dd
Message-ID: <20031217004305.E249F3947@mprdmxin.myway.com>



I just noticed an error in my posting below.

The origin in get.hist.quote is relative to day=30, not day=31.

--- 
Date: Sat, 6 Dec 2003 14:06:49 -0500 (EST) 
From: Gabor Grothendieck <ggrothendieck at myway.com>
To: <mendigo at netcabo.pt>, <r-help at stat.math.ethz.ch> 
Subject: RE: [R] Axe time of series in format yy-mm-dd 

 
 


get.hist.quote produces times relative to Dec 31, 1899, so try this:

require(tseries)
require(chron)

ibm <- get.hist.quote( "ibm", start = "2003-01-01", quote = "Close" )

time.ibm <- chron( time(ibm), out.format="y-m-d", 
origin = c( month = 12, day = 31, year = 1899 ) )
plot( time.ibm, ibm, type="l", simplify = F )

The simplify=F flag on plot forces the year to appear. Without
it, only the month and day are displayed.

---
Date: Sat, 6 Dec 2003 17:48:06 -0000 
From: M. M. Palhoto N. Rodrigues <mendigo at netcabo.pt>
To: R Help <r-help at stat.math.ethz.ch> 
Subject: [R] Axe time of series in format yy-mm-dd 



I'm trying to plot a ibm stock time series.
I made the download of that series,
ibm <- get.hist.quote(instrument = "ibm", start = "2003-01-01",quote=c("CL"))
And ibm is a serie wiht this characteristic:
Start = 37623 
End = 37960 
Frequency = 1 
When I try to plot it,
ts.plot(ibm)
In the graphic the axe time is represented by 37623 ... 37960, How can I put the time in the format,
yy-mm-dd ?

Thanks a lot

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From jfox at mcmaster.ca  Wed Dec 17 02:32:33 2003
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 16 Dec 2003 20:32:33 -0500
Subject: [R] Difference between summary.lm() and summary.aov()
In-Reply-To: <20031216215227.73555.qmail@web60006.mail.yahoo.com>
References: <x23cbwix7s.fsf@biostat.ku.dk>
Message-ID: <5.1.0.14.2.20031216202748.021596d0@127.0.0.1>

At 01:52 PM 12/16/2003 -0800, Alexander Sirotkin \[at Yahoo\] wrote:
>Thanks a lot to everybody. Two more questions, if you
>don't mind :
>
>How anova() treats non-categorical variables, such as
>severity in my case ? I was under impression that
>ANOVA is defined for categorical variables only.

The term ANOVA is commonly used in two related but distinct senses: a 
linear model in which the predictors are factors (categorical variables), 
and a table with sums of squares, associated df, and F-tests for various 
terms in a linear model, which need not consist only of factors. The 
anova() function computes the latter, and generalizes this, e.g., to 
analysis of deviance table for generalized linear models.

>I read about drop1() and I understand that it performs
>F-test for nested models, correct me if I'm wrong. It
>is unclear to me, however, how it manages to do this
>F-test for interactions ?

Actually, tests for the highest-order terms in the model are more 
straightforward than those for lower-order terms. The drop1() function does 
just that -- that is, drops a high-order term from the model and (for a 
linear model) computes the change in the residual sum of squares.

I hope that this helps,
  John

>Thanks a lot.
>
>--- Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> > "Alexander Sirotkin [at Yahoo]"
> > <alex_s_42 at yahoo.com> writes:
> >
> > > John,
> > >
> > > What you are saying is that any conclusion I can
> > make
> > > from summary.aov (for instance, to answer a
> > question
> > > if physician is a significant variable) will not
> > be
> > > correct ?
> >
> > Summary.aov is for summarizing aov objects, so
> > you're lucky to get
> > something that is sensible at all. You should use
> > anova() to get
> > analysis of variance tables. These are sequential so
> > that you can use
> > them (give or take some quibbles about the residual
> > variance) for
> > reducing the model from the "bottom up". I.e. if you
> > place "physician"
> > last, you get the F test for whether that variable
> > is significant.
> > However, a more convenient way of getting that
> > result is to use
> > drop1(). Even then there's no simple relation to the
> > two
> > t-tests, except that the F test tests the hypothesis
> > that *both*
> > coefficients are zero, where the t-tests do so
> > individually.
> >
> >
> > > --- John Fox <jfox at mcmaster.ca> wrote:
> > > > Dear Spencer and Alexander,
> > > >
> > > > In this case, physician is apparently a factor
> > with
> > > > three levels, so
> > > > summary.aov() gives you a sequential ANOVA,
> > > > equivalent to what you'd get
> > > > from anova(). There no simple relationship
> > between
> > > > the F-statistic for
> > > > physician, which has 2 df in the numerator, and
> > the
> > > > two t's. (By the way, I
> > > > doubt whether a sequential ANOVA is what's
> > wanted
> > > > here.)
> > > >
> > > > Regards,
> > > >   John
> > > >
> > > > At 09:17 AM 12/6/2003 -0800, Spencer Graves
> > wrote:
> > > > >      The square of a Student's t with "df"
> > degrees
> > > > of freedom is an F
> > > > > distribution with 1 and "df" degrees of
> > freedom.
> > > > >      hope this helps.  spencer graves
> > > > >
> > > > >Alexander Sirotkin [at Yahoo] wrote:
> > > > >
> > > > >>I have a simple linear model (fitted with
> > lm())
> > > > with 2
> > > > >>independant
> > > > >>variables : one categorical and one integer.
> > > > >>
> > > > >>When I run summary.lm() on this model, I get a
> > > > >>standard linear
> > > > >>regression summary (in which one categorical
> > > > variable
> > > > >>has to be
> > > > >>converted into many indicator variables) which
> > > > looks
> > > > >>like :
> > > > >>
> > > > >>            Estimate Std. Error t value
> > Pr(>|t|)
> > > > >>(Intercept)  -3595.3     2767.1  -1.299
> > 0.2005
> > > > >>physicianB     802.0     2289.5   0.350
> > 0.7277
> > > > >>physicianC    4906.8     2419.8   2.028
> > 0.0485 *
> > > > >>severity      7554.4      906.3   8.336
> > 1.12e-10
> > > > ***
> > > > >>
> > > > >>and when I run summary.aov() I get similar
> > ANOVA
> > > > table
> > > > >>:
> > > > >>           Df     Sum Sq    Mean Sq F value
> > > > Pr(>F)
> > > > >>physician    2  294559803  147279901  3.3557
> > > > 0.04381
> > > > >>*
> > > > >>severity     1 3049694210 3049694210 69.4864
> > > > 1.124e-10
> > > > >>***
> > > > >>Residuals   45 1975007569   43889057
> > > > >>
> > > > >>What is absolutely unclear to me is how
> > F-value
> > > > and
> > > > >>Pr(>F) for the
> > > > >>categorical "physician" variable of the
> > > > summary.aov()
> > > > >>is calculated
> > > > >>from the t-value of the summary.lm() table.
> > > > >>
> > > > >>I looked at the summary.aov() source code but
> > > > still
> > > > >>could not figure
> > > > >>it.
> > > > >>
> > > > >>Thanks a lot.
> > > > >>
> > > > >>__________________________________
> > > > >>
> > >

-----------------------------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada L8S 4M4
email: jfox at mcmaster.ca
phone: 905-525-9140x23604
web: www.socsci.mcmaster.ca/jfox



From allison.100 at osu.edu  Wed Dec 17 02:35:37 2003
From: allison.100 at osu.edu (Gary Allison)
Date: Tue, 16 Dec 2003 20:35:37 -0500
Subject: [R] variance estimates in lme biased?
Message-ID: <3FDFB2E9.4070900@osu.edu>

Hi all,
I didn't get a response to my post of this issue a week ago, so I've 
tried to clarify:

When I use lme to analyze a model of nested random effects, the variance 
estimates of levels higher in the hierarchy appear to have much more 
variance than they should.

In the example below with 4 levels, I simulate variance in level 2 
(sd=1.0) and level 4 (sd=0.1), but levels 1 and 3 do not vary.  Although 
I expected to see a small amount of variability in the lme estimates of 
levels 1 and 3, I am confused by what happens in the level 1 estimates: 
  more than 10% of the runs produce stdDev of >0.4 and in many cases the 
level 1 estimate is close to level 2's. Nothing like that happens in 
level 3.

I use R 1.8.1 & Win2000.  Since I last posted, I have also seen these 
symptoms in SAS when I use PROC VARCOMP, although I haven't explored it 
much there.

library(nlme)
F1V <- F2V <- F3V  <- residV <- numeric(0)
for (rep in 1:500){
   F1f <- F2f <- F3f <- value <- numeric(0)
   # data generator
   for (F1 in 1:5) {
     for (F2 in 1:5) {
       lev2 <- rnorm(1,sd=1.0)
       for (F3 in 1:5) {
         for (F4 in 1:5) {
           lev4 <- rnorm(1,sd=0.1)
           value <- c(value, lev2+lev4)
           F1f <- c(F1f,F1);F2f <- c(F2f,F2);F3f <- c(F3f,F3)
         }
       }
     }
   }
   L1 <- as.factor(F1f); L2 <- as.factor(F2f); L3 <- as.factor(F3f)
   y.lme <- lme(value ~ 1,random = ~ 1 | L1/L2/L3)
   v <- as.numeric(VarCorr(y.lme)[,2])
   v <- as.numeric(na.omit(v))
   F1V <- c(F1V,v[1]);  F2V <- c(F2V,v[2]);  F3V <- c(F3V,v[3])
   residV <- c(residV,y.lme$sigma)
}

Because this code can take several minutes to run (probably because of 
my heavy use of for loops), I've posted a set of results at:
http://david.science.oregonstate.edu/~allisong/R/nestedVar.pdf
   (see last page for summary)

If anyone can help me make sense of this, point out an error I'm making 
or point me to some literature, I would greatly appreciate it!

Thanks,
Gary

--
Gary Allison
Evolution, Ecology and Organismal Biology
Ohio State University



From arc at arcriswell.com  Wed Dec 17 03:47:16 2003
From: arc at arcriswell.com (Andrew Criswell)
Date: Wed, 17 Dec 2003 09:47:16 +0700
Subject: [R] Resampling Stats software
References: <20031216044537.JYQO1282.imf23aec.mail.bellsouth.net@brandoncd66bez>
Message-ID: <3FDFC3B4.1090401@arcriswell.com>


A very good introductory text is "Data Analysis by Resampling: Concepts 
and Applications" by Clifford Lunneborg.

My search on Amazon fails to locate the book Brandon mentions, 
"Resampling: The New Statistics". Is there more information on Author, 
ISBN, etc.?

You may wish to look at appendix 8, "Bootstrapping Regression Models," 
to John Fox's "An R and S-Plus Companion to Applied Regression."  It can 
be found at 
http://socserv.socsci.mcmaster.ca/jfox/Books/Companion/scripts.html

ANDREW

Brandon Vaughn wrote:

>Hi,
> 
>I am new to R (I have most of my experience in SAS and SPSS).  I was
>wondering if anyone has used both Resampling Stats and R, and could comment
>on strengths/relationships.  Also, I have no clue on how to do the various
>examples from the book "Resampling: The New Statistics" in R.  Can anyone
>give me some possible starting points?  Or websites/books?
>
>Thanks,
>Brandon
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>
>
>  
>



From savano at superig.com.br  Wed Dec 17 03:49:17 2003
From: savano at superig.com.br (Savano S. Pereira)
Date: 17 Dec 2003 00:49:17 -0200
Subject: [R] Jacobian Matrix
Message-ID: <1071629357.2887.5.camel@localhost.localdomain>

Dear useRs,

I need of jacobian of a tranformation, R have this function?

thanks.

Savano



From ok at cs.otago.ac.nz  Wed Dec 17 05:39:51 2003
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Wed, 17 Dec 2003 17:39:51 +1300 (NZDT)
Subject: [R] mailing list for basic questions - preliminary sum up
Message-ID: <200312170439.hBH4dpAD105353@atlas.otago.ac.nz>

My experience in several mailing lists and newsgroups has been that
"help" from other beginners very often deserves the scare quotes.
The advice is often extremely bad.

The situation for R is quite different:  R has the best documentation I've
ever seen for any open-source package, and it's better than most commercial
software I've had to deal with.  The R web site has pointers to some really
excellent stuff.  For example, while "S Poetry" is about S rather than R,
a lot of "programming" questions about R have clear explanations in that
book.  There are several tutorials, and the ones I looked at were good.

There are a few things about using R with a particular operating system
or window manager that are best shown in person.  But apart from that,
I'm wondering what kind of beginner questions there might be that beginners
would be able to help with that aren't already in the tutorials &c.

A beginner who can say "I have read <this>, <that>, and <the other> and
tried the on-line help, and I didn't recognise the answer to <my problem>"
is likely to get prompt and accurate help in this mailing list.

The books I've relied on for actually doing statistics have mainly been
"Statistical Models in S" and "Modern Applied Statistics with S", and again,
they really do answer a lot of questions.

Hmm.  I seem to have argued myself into the position that
IF the rule in the beginner list were that anyone purporting to
   answer a question should justify the answer by citing the relevant
   R documentation or one of the commonly mentioned books about S and R,
THEN it could be as educational for the answerer as for the questioner
   and quite helpful after all.



From ggrothendieck at myway.com  Wed Dec 17 05:43:58 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 16 Dec 2003 23:43:58 -0500 (EST)
Subject: [R] mailing list for basic questions - preliminary sum up
Message-ID: <20031217044358.1E43E399D@mprdmxin.myway.com>



My personal view on this is that there is need for a friendly
list with a more "customer service" attitude than r-help.

r-help is really very useful but its also intimidating
and I bet lots of people have questions that they never ask
for fear of the response.   Maybe some of them even decide
not to learn R.

---
Date: Wed, 17 Dec 2003 00:49:15 +0100 
From: Martin Wegmann <wegmann_mailinglist at gmx.net>
To: Spencer Graves <spencer.graves at pdf.com>, <rossini at u.washington.edu> 
Cc: <r-help at stat.math.ethz.ch> 
Subject: Re: [R] mailing list for basic questions - preliminary sum up 

 
 
Hello, 

I agree completely that well thought out questions are important to receive 
good and quick replies and I agree as well that the replies on the R-help 
list are very good and helpful.
But I had to learn and I am still learing how to write good questions and 
appreciate Spencer's explanantion how a good question should look like in his 
opinion. 

I am not sure how this new mailing list might evolve. 
It might be that the R-beginner list takes some load of the R-help list by 
reducing the amount of "basic" questions which won't be questioned anymore 
here (what aren't many) and that new user might be taught to post "good" 
question before they start posting to R-help.
If it proves to be ineffective or might affect R-help in some unwanted manner 
it would be an easy one to shut it down. 

I doubt that it will split the R-help list - in my opinion it is unlikely that 
medium/experienced R user who will subscribe to R-beginner will unsubscribe 
from the R-help list. 
Moreover people starting with R are less likely to send any mails to this 
list, some do and are refered in most cases to the manuals. 
When I started R I looked through the archive and because I did not understand 
even one question, I was intimidated by this list and did not send any mail 
until a few weeks later (that was not because of the statistics but the 
commands)
For this kind of people the R-beginner list is thought - to encourage them to 
send "stupid" questions during their first steps in R. 

They shall recognize questions they would have asked themselves.
Therefore I think that the quality of the question is in this case less 
important than it's level.

I hope I did not misunderstood some points ,-)

best regards Martin



On Tuesday 16 December 2003 17:20, Spencer Graves wrote:
> I agree with Tony's observation that well thought out questions
> are more likely to receive an answer than something that is long,
> rambling, and poorly focused. Many questions take more time to read
> than I have available, so I don't bother. I like questions that include
> toy examples in a few lines of code that I can copy from an email into R
> and test ideas. Careful formatting that looks pretty in an email is an
> obstacle for me, because it increases the work required to get it into
> R. Many questioners could answer their own problems in the process of
> generating such a toy example. When they can't, that exercise helps
> them focus the question, which makes it easier for potential respondents
> to understand the problem and reply. Without that, I must either
> generate a toy example myself (which I've done many times) or respond
> with untested code and risk looking stupid when my untested suggestion
> doesn't work.
>
> hope this helps.
> spencer graves
>
> A.J. Rossini wrote:
> >"Pascal A. Niklaus" <Pascal.Niklaus at unibas.ch> writes:
> >>- In my experience even *very* basic questions *relating to the R
> >>language* do get answered on r-help. I'm impressed by how much time
> >>some members of the R core team spend answering relatively basic
> >>questions, and by how elaborate their answers generally are. So I
> >>cannot see much need for a new R mailing list. There are these
> >>excellent mailing list archives, so why "fragment" this list?
> >
> >To follow up, well-thought through basic questions do get answered; in
> >particular, they can be useful for those of us writing packages,
> >documentation, etc.
> >
> >I have a sense that it is the quality of the question (details of what
> >is intended to do, or not known, signs of using other sources of
> >materials which folks have spent years on, no signs that this is a "do
> >my work for me" question) rather than the level of the question, that
> >is an issue.
> >
> >best,
> >-tony
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ggrothendieck at myway.com  Wed Dec 17 05:51:54 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 16 Dec 2003 23:51:54 -0500 (EST)
Subject: [R] Jacobian Matrix
Message-ID: <20031217045154.5DA183966@mprdmxin.myway.com>



Depending on what it is, you might be able to
construct it using deriv.  Try

?deriv

---

Date: 17 Dec 2003 00:49:17 -0200 
From: Savano S. Pereira <savano at superig.com.br>
To: Lista R <r-help at stat.math.ethz.ch> 
Subject: [R] Jacobian Matrix 

 
 
Dear useRs,

I need of jacobian of a tranformation, R have this function?

thanks.

Savano

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From s195404 at student.uq.edu.au  Wed Dec 17 06:23:48 2003
From: s195404 at student.uq.edu.au (Andrew C. Ward)
Date: Wed, 17 Dec 2003 05:23:48 +0000
Subject: [R] mailing list for basic questions - preliminary sum up
In-Reply-To: <20031217044358.1E43E399D@mprdmxin.myway.com>
References: <20031217044358.1E43E399D@mprdmxin.myway.com>
Message-ID: <1071638628.3fdfe864a7931@my.uq.edu.au>

There are always people on lists whose email manner leaves
a great deal to be desired. I tend to think, however, that
it's a small price to pay for excellent, free software and
fast, expert advice. Anyway, there's no guarantee that a
beginner's list would be any more friendly than the main 
list, particularly if it is "staffed" by volunteers.

Regards,

Andrew C. Ward

CAPE Centre
Department of Chemical Engineering
The University of Queensland
Brisbane Qld 4072 Australia


Quoting Gabor Grothendieck <ggrothendieck at myway.com>:

> 
> 
> My personal view on this is that there is need for a
> friendly
> list with a more "customer service" attitude than
> r-help.
> 
> r-help is really very useful but its also intimidating
> and I bet lots of people have questions that they never
> ask
> for fear of the response.   Maybe some of them even
> decide
> not to learn R.
> 
> ---
> Date: Wed, 17 Dec 2003 00:49:15 +0100 
> From: Martin Wegmann <wegmann_mailinglist at gmx.net>
> To: Spencer Graves <spencer.graves at pdf.com>,
> <rossini at u.washington.edu> 
> Cc: <r-help at stat.math.ethz.ch> 
> Subject: Re: [R] mailing list for basic questions -
> preliminary sum up 
> 
>  
>  
> Hello, 
> 
> I agree completely that well thought out questions are
> important to receive 
> good and quick replies and I agree as well that the
> replies on the R-help 
> list are very good and helpful.
> But I had to learn and I am still learing how to write
> good questions and 
> appreciate Spencer's explanantion how a good question
> should look like in his 
> opinion. 
> 
> I am not sure how this new mailing list might evolve. 
> It might be that the R-beginner list takes some load of
> the R-help list by 
> reducing the amount of "basic" questions which won't be
> questioned anymore 
> here (what aren't many) and that new user might be taught
> to post "good" 
> question before they start posting to R-help.
> If it proves to be ineffective or might affect R-help in
> some unwanted manner 
> it would be an easy one to shut it down. 
> 
> I doubt that it will split the R-help list - in my
> opinion it is unlikely that 
> medium/experienced R user who will subscribe to
> R-beginner will unsubscribe 
> from the R-help list. 
> Moreover people starting with R are less likely to send
> any mails to this 
> list, some do and are refered in most cases to the
> manuals. 
> When I started R I looked through the archive and because
> I did not understand 
> even one question, I was intimidated by this list and did
> not send any mail 
> until a few weeks later (that was not because of the
> statistics but the 
> commands)
> For this kind of people the R-beginner list is thought -
> to encourage them to 
> send "stupid" questions during their first steps in R. 
> 
> They shall recognize questions they would have asked
> themselves.
> Therefore I think that the quality of the question is in
> this case less 
> important than it's level.
> 
> I hope I did not misunderstood some points ,-)
> 
> best regards Martin
> 
> 
> 
> On Tuesday 16 December 2003 17:20, Spencer Graves wrote:
> > I agree with Tony's observation that well thought out
> questions
> > are more likely to receive an answer than something
> that is long,
> > rambling, and poorly focused. Many questions take more
> time to read
> > than I have available, so I don't bother. I like
> questions that include
> > toy examples in a few lines of code that I can copy
> from an email into R
> > and test ideas. Careful formatting that looks pretty in
> an email is an
> > obstacle for me, because it increases the work required
> to get it into
> > R. Many questioners could answer their own problems in
> the process of
> > generating such a toy example. When they can't, that
> exercise helps
> > them focus the question, which makes it easier for
> potential respondents
> > to understand the problem and reply. Without that, I
> must either
> > generate a toy example myself (which I've done many
> times) or respond
> > with untested code and risk looking stupid when my
> untested suggestion
> > doesn't work.
> >
> > hope this helps.
> > spencer graves
> >
> > A.J. Rossini wrote:
> > >"Pascal A. Niklaus" <Pascal.Niklaus at unibas.ch>
> writes:
> > >>- In my experience even *very* basic questions
> *relating to the R
> > >>language* do get answered on r-help. I'm impressed by
> how much time
> > >>some members of the R core team spend answering
> relatively basic
> > >>questions, and by how elaborate their answers
> generally are. So I
> > >>cannot see much need for a new R mailing list. There
> are these
> > >>excellent mailing list archives, so why "fragment"
> this list?
> > >
> > >To follow up, well-thought through basic questions do
> get answered; in
> > >particular, they can be useful for those of us writing
> packages,
> > >documentation, etc.
> > >
> > >I have a sense that it is the quality of the question
> (details of what
> > >is intended to do, or not known, signs of using other
> sources of
> > >materials which folks have spent years on, no signs
> that this is a "do
> > >my work for me" question) rather than the level of the
> question, that
> > >is an issue.
> > >
> > >best,
> > >-tony
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From schween at snafu.de  Wed Dec 17 08:27:41 2003
From: schween at snafu.de (Sven C. Koehler)
Date: Wed, 17 Dec 2003 08:27:41 +0100
Subject: [R] Easiest way to get the mean of a row of a data.frame?
Message-ID: <20031217072741.GA12077696@zedat.fu-berlin.de>

Dear r-helpers!

I am kind of new to R.
I would like to calculate the mean of the numbers of this expression:

    data(USArrests)
    USArrests[row.names(M) == "Alabama",]

class() tells me it's a ``data.frame,'' what I actually desire is to get
all numbers of a row as a vector or a list to let mean() calculate the mean
of the whole row.  (I know this doesn't make sense with USArrests, I just
used it here instead of my very own data.frame.)

Thank you very much!

Bye

Sven



From ripley at stats.ox.ac.uk  Wed Dec 17 08:42:32 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 17 Dec 2003 07:42:32 +0000 (GMT)
Subject: [R] Easiest way to get the mean of a row of a data.frame?
In-Reply-To: <20031217072741.GA12077696@zedat.fu-berlin.de>
Message-ID: <Pine.LNX.4.44.0312170737040.9981-100000@gannet.stats>

rowMeans(mydf)  will do this for a data frame mydf.

Be careful though to use it only with all-numeric data frames (there is an
implicit as.matrix going on), and often it makes more sense to store such
data in a matrix.

apply(mydf, 1, mean) would also work, but is slower.

On Wed, 17 Dec 2003, Sven C. Koehler wrote:

> I am kind of new to R.
> I would like to calculate the mean of the numbers of this expression:
> 
>     data(USArrests)
>     USArrests[row.names(M) == "Alabama",]
> 
> class() tells me it's a ``data.frame,'' what I actually desire is to get
> all numbers of a row as a vector or a list to let mean() calculate the mean
> of the whole row.  (I know this doesn't make sense with USArrests, I just
> used it here instead of my very own data.frame.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Roger.Bivand at nhh.no  Wed Dec 17 09:20:53 2003
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 17 Dec 2003 09:20:53 +0100 (CET)
Subject: [R] mailing list for basic questions - preliminary sum up
In-Reply-To: <20031217044358.1E43E399D@mprdmxin.myway.com>
Message-ID: <Pine.LNX.4.44.0312170833470.1851-100000@reclus.nhh.no>

On Tue, 16 Dec 2003, Gabor Grothendieck wrote:

> 
> 
> My personal view on this is that there is need for a friendly
> list with a more "customer service" attitude than r-help.

This is not a balanced view. In a project like ours, you really do need to
put participation in balance. If you don't offset perhaps direct but
concise and accurate advice by concurrent commitment to other areas of the
project (often not visible on this list), then you don't get a balanced
picture.

Please also recall that questions asked can be a valuable input to
discussions about when legacy behaviour of functions has become too
difficult to understand - very often leading to changes at least in
documentation. This is a good reason for not trying to divert questions to
a separate list (I think developers and package maintainers would not be
likely to read such a list), lack of shared archives is a second.

What can cause irritation is when list users are given accurate advice,
like read the documentation, read the FAQ, often with hints or actual
solutions, and then come back with the same question, so obviously not
valuing the advice offered. Note that people offering advice try to take
account of language issues, English is not the native language of many
(most?) list users. Also note that the reply time for obvious answers is
very short, most often just the name of the function. Is this impolite,
really?

> 
> r-help is really very useful but its also intimidating
> and I bet lots of people have questions that they never ask
> for fear of the response.   Maybe some of them even decide
> not to learn R.

I do not think you would win your bet. The questions attracting tougher
responses are either the "do my homework for me" type, or the "I'll ignore
the good advice I was given and repeat my question" type. Think of the
list as a graduate seminar - is a sharp comment never appropriate? I'm
sure that I can remember very helpful sharp comments from my teachers and
fellow-students (maybe too few?) that made me see things in a more
appropriate light. One basic characteristic seems to be that if the
question does indicate seriousness about trying to analyse data, respect
for the task at hand, then predictably lots of good advice comes quickly.

I'm also not too sure about the "learning R" question. Of course there is
the GUI/CLI issue, and the "very many defaults already filled in" issue,
but actually market share really isn't a driver here, is it? Isn't this
more about attitude and motivation in taking an active role in analysing
data? If your research question really itches, what should it take to stop
you learning R (or associated packages)? 

As has already been said, there is a lot of documentation. It is possible
that something more like edited weblogs from beginners could be collated
which - if indexed for searching - would function better than a "beginners
list" - a link to something like this was posted a couple of months ago.

> 
> ---
> Date: Wed, 17 Dec 2003 00:49:15 +0100 
> From: Martin Wegmann <wegmann_mailinglist at gmx.net>
> To: Spencer Graves <spencer.graves at pdf.com>, <rossini at u.washington.edu> 
> Cc: <r-help at stat.math.ethz.ch> 
> Subject: Re: [R] mailing list for basic questions - preliminary sum up 
> 
>  
>  
> Hello, 
> 
> I agree completely that well thought out questions are important to receive 
> good and quick replies and I agree as well that the replies on the R-help 
> list are very good and helpful.
> But I had to learn and I am still learing how to write good questions and 
> appreciate Spencer's explanantion how a good question should look like in his 
> opinion. 
> 
> I am not sure how this new mailing list might evolve. 
> It might be that the R-beginner list takes some load of the R-help list by 
> reducing the amount of "basic" questions which won't be questioned anymore 
> here (what aren't many) and that new user might be taught to post "good" 
> question before they start posting to R-help.
> If it proves to be ineffective or might affect R-help in some unwanted manner 
> it would be an easy one to shut it down. 
> 
> I doubt that it will split the R-help list - in my opinion it is unlikely that 
> medium/experienced R user who will subscribe to R-beginner will unsubscribe 
> from the R-help list. 
> Moreover people starting with R are less likely to send any mails to this 
> list, some do and are refered in most cases to the manuals. 
> When I started R I looked through the archive and because I did not understand 
> even one question, I was intimidated by this list and did not send any mail 
> until a few weeks later (that was not because of the statistics but the 
> commands)
> For this kind of people the R-beginner list is thought - to encourage them to 
> send "stupid" questions during their first steps in R. 
> 
> They shall recognize questions they would have asked themselves.
> Therefore I think that the quality of the question is in this case less 
> important than it's level.
> 
> I hope I did not misunderstood some points ,-)
> 
> best regards Martin
> 
> 
> 
> On Tuesday 16 December 2003 17:20, Spencer Graves wrote:
> > I agree with Tony's observation that well thought out questions
> > are more likely to receive an answer than something that is long,
> > rambling, and poorly focused. Many questions take more time to read
> > than I have available, so I don't bother. I like questions that include
> > toy examples in a few lines of code that I can copy from an email into R
> > and test ideas. Careful formatting that looks pretty in an email is an
> > obstacle for me, because it increases the work required to get it into
> > R. Many questioners could answer their own problems in the process of
> > generating such a toy example. When they can't, that exercise helps
> > them focus the question, which makes it easier for potential respondents
> > to understand the problem and reply. Without that, I must either
> > generate a toy example myself (which I've done many times) or respond
> > with untested code and risk looking stupid when my untested suggestion
> > doesn't work.
> >
> > hope this helps.
> > spencer graves
> >
> > A.J. Rossini wrote:
> > >"Pascal A. Niklaus" <Pascal.Niklaus at unibas.ch> writes:
> > >>- In my experience even *very* basic questions *relating to the R
> > >>language* do get answered on r-help. I'm impressed by how much time
> > >>some members of the R core team spend answering relatively basic
> > >>questions, and by how elaborate their answers generally are. So I
> > >>cannot see much need for a new R mailing list. There are these
> > >>excellent mailing list archives, so why "fragment" this list?
> > >
> > >To follow up, well-thought through basic questions do get answered; in
> > >particular, they can be useful for those of us writing packages,
> > >documentation, etc.
> > >
> > >I have a sense that it is the quality of the question (details of what
> > >is intended to do, or not known, signs of using other sources of
> > >materials which folks have spent years on, no signs that this is a "do
> > >my work for me" question) rather than the level of the question, that
> > >is an issue.
> > >
> > >best,
> > >-tony
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From p.pagel at gsf.de  Wed Dec 17 09:43:44 2003
From: p.pagel at gsf.de (Philipp Pagel)
Date: Wed, 17 Dec 2003 09:43:44 +0100
Subject: [R] Easiest way to get the mean of a row of a data.frame?
In-Reply-To: <20031217072741.GA12077696@zedat.fu-berlin.de>
References: <20031217072741.GA12077696@zedat.fu-berlin.de>
Message-ID: <20031217084344.GA1474@porcupine.gsf.de>

On Wed, Dec 17, 2003 at 08:27:41AM +0100, Sven C. Koehler wrote:
> I am kind of new to R.
> I would like to calculate the mean of the numbers of this expression:
> 
>     data(USArrests)
>     USArrests[row.names(M) == "Alabama",]
> 
> class() tells me it's a ``data.frame,'' what I actually desire is to get
> all numbers of a row as a vector or a list to let mean() calculate the mean
> of the whole row.  (I know this doesn't make sense with USArrests, I just
> used it here instead of my very own data.frame.)

rowMeans(USArrest)

does what you want.

In case you want to do soemthing else but compute the mean you will find
this handy:

apply(USArrests, 1, mean)

here you can substitute "mean" by whatever function you'd like to apply
to each row.

cu
	Philipp

-- 
Dr. Philipp Pagel                                Tel.  +49-89-3187-3675
Institute for Bioinformatics / MIPS              Fax.  +49-89-3187-3585
GSF - National Research Center for Environment and Health
Ingolstaedter Landstrasse 1
85764 Neuherberg, Germany



From Nathan.Weisz at uni-konstanz.de  Wed Dec 17 10:33:47 2003
From: Nathan.Weisz at uni-konstanz.de (Nathan Weisz)
Date: Wed, 17 Dec 2003 10:33:47 +0100
Subject: [R] repeated measures aov problem
Message-ID: <1071653627.3fe022fb87db1@webmail.uni-konstanz.de>

Hi all,

I have a strange problem and rigth now I can't figure out a 
solution.

Trying to calculate an ANOVA with one between subject factor (group) 
and one within (hemisphere). My dependent variable is source 
localization (data). My N = 25.

My data.frame looks like this:
> ML.dist.stack
   subj group hemisphere      data
1     1   tin       left 0.7460840
2     2   tin       left 1.0300600
3     3   tin       left 0.8026865
<<SNIP>>
47   22   con      rigth 0.6808010
48   23   con      rigth 1.2291490
49   24   con      rigth 2.0930595
50   25   con      rigth 1.5631695

> str(ML.dist.stack)
`data.frame':	50 obs. of  4 variables:
 $ subj      : Factor w/ 25 levels "1","2","3","4",..: 1 2 3 4 5 6 7 
8 9 10 ...
 $ group     : Factor w/ 2 levels "con","tin": 2 2 2 2 2 2 2 2 2 2 
...
 $ hemisphere: Factor w/ 2 levels "left","rigth": 
1 1 1 1 1 1 1 1 1 1 ...
 $ data      : num  0.746 1.030 0.803 2.197    NA ...

Typing: summary(aov(data ~group * hemisphere + Error(subj/
hemisphere), data=ML.dist.stack))
gives me following result ...
Error: subj
                 Df  Sum Sq Mean Sq F value Pr(>F)
group             1  0.1813  0.1813  0.2012 0.6584
hemisphere        1  0.1937  0.1937  0.2149 0.6477
group:hemisphere  1  0.0898  0.0898  0.0996 0.7554
Residuals        21 18.9277  0.9013               

Error: subj:hemisphere
                 Df  Sum Sq Mean Sq F value Pr(>F)
hemisphere        1  1.0534  1.0534  1.6736 0.2105
group:hemisphere  1  0.5714  0.5714  0.9077 0.3521
Residuals        20 12.5893  0.6295               
... which looks like the error strata are wrong.

I kept very close (in my opinion exactly) to Baron's Example 4 of 
the tutorial (p. 29), which produces correct results. See below:
> str(Ela.uni)
`data.frame':	96 obs. of  5 variables:
 $ effect: num  19 11 20 21 18 17 20 14 16 26 ...
 $ subj  : Factor w/ 16 levels "s1","s10","s11",..: 
1 9 10 11 12 13 14 15 16 2 ...
 $ gp    : Factor w/ 2 levels "gp1","gp2": 1 1 1 1 1 1 1 1 2 2 ...
 $ drug  : Factor w/ 2 levels "dr1","dr2": 1 1 1 1 1 1 1 1 1 1 ...
 $ dose  : Factor w/ 3 levels "do1","do2","do3": 1 1 1 1 1 1 1 1 1 1 
...
> summary(aov(effect ~ gp * drug * dose + Error(subj/(dose+drug)), 
data=Ela.uni))

Error: subj
          Df Sum Sq Mean Sq F value  Pr(>F)  
gp         1 270.01  270.01  7.0925 0.01855 *
Residuals 14 532.98   38.07                  
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 

Error: subj:dose
          Df Sum Sq Mean Sq F value    Pr(>F)    
dose       2 758.77  379.39 36.5097 1.580e-08 ***
gp:dose    2  42.27   21.14  2.0339    0.1497    
Residuals 28 290.96   10.39                      
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 

Error: subj:drug
          Df Sum Sq Mean Sq F value   Pr(>F)   
drug       1 348.84  348.84  13.001 0.002866 **
gp:drug    1 326.34  326.34  12.163 0.003624 **
Residuals 14 375.65   26.83                    
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 

Error: Within
             Df  Sum Sq Mean Sq F value Pr(>F)
drug:dose     2  12.063   6.031  0.6815 0.5140
gp:drug:dose  2  14.812   7.406  0.8369 0.4436
Residuals    28 247.792   8.850               

Can someone give me a hint where I migth be going wrong? However 
trivial this migth be I somehow got stuck.

All the best,
Nathan

-----------------------------------------
Nathan Weisz
Department of Psychology
University of Konstanz
P.O. Box D25
D - 78457 Konstanz
GERMANY

Tel: +49 (0)7531 88- 4612
E-mail: Nathan.Weisz at uni-konstanz.de
http://www.clinical-psychology.uni-konstanz.de



From schween at snafu.de  Wed Dec 17 11:16:43 2003
From: schween at snafu.de (Sven C. Koehler)
Date: Wed, 17 Dec 2003 11:16:43 +0100
Subject: [R] Easiest way to get the mean of a row of a data.frame?
In-Reply-To: <20031217084344.GA1474@porcupine.gsf.de>
References: <20031217072741.GA12077696@zedat.fu-berlin.de>
	<20031217084344.GA1474@porcupine.gsf.de>
Message-ID: <20031217101643.GA12726752@zedat.fu-berlin.de>

Thanks, you've helped me alot!

-S.

On Wed, Dec 17, 2003 at 09:43:44AM +0100, Philipp Pagel wrote:
> rowMeans(USArrest)
> 
> does what you want.



From Pascal.Niklaus at unibas.ch  Wed Dec 17 11:18:20 2003
From: Pascal.Niklaus at unibas.ch (Pascal A. Niklaus)
Date: Wed, 17 Dec 2003 11:18:20 +0100
Subject: [R] variance estimates in lme biased?
In-Reply-To: <3FDFB2E9.4070900@osu.edu>
References: <3FDFB2E9.4070900@osu.edu>
Message-ID: <3FE02D6C.3040007@unibas.ch>

Running lme on your data set results exactly in what you expect - or do 
you expect something different?

Pascal

 > L1<-factor(F1f)
 > L2<-factor(F2f)
 > L3<-factor(F3f)
 > lme(value ~ 1,random = ~ 1 | L1/L2/L3)
Linear mixed-effects model fit by REML
  Data: NULL
  Log-restricted-likelihood: 438.9476
  Fixed: value ~ 1
(Intercept)
  0.2955631

Random effects:
 Formula: ~1 | L1
        (Intercept)
StdDev:  0.02472988              <== level F1 which is 0

 Formula: ~1 | L2 %in% L1
        (Intercept)
StdDev:    1.140782                <== level F2 which is 1

 Formula: ~1 | L3 %in% L2 %in% L1
         (Intercept)  Residual
StdDev: 0.0005512791 0.1020479   <== F3 which is 0 , and F4 which is 0.1

Number of Observations: 625
Number of Groups:
                L1         L2 %in% L1 L3 %in% L2 %in% L1
                 5                 25                125



From ripley at stats.ox.ac.uk  Wed Dec 17 11:29:13 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 17 Dec 2003 10:29:13 +0000 (GMT)
Subject: [R] repeated measures aov problem
In-Reply-To: <1071653627.3fe022fb87db1@webmail.uni-konstanz.de>
Message-ID: <Pine.LNX.4.44.0312171025210.12879-100000@gannet.stats>

You have an NA in your data, it says.  That makes the design unbalanced 
(it it is not already unbalanced by having 50 obs with a 2x2 
classification: I can't see the pattern from your extract but guess 25 
subjects don't divide into two equal groups).

That you get the same effects in both strata is the classic symptom of
lack of balance.  I think you need to use lme to analyse these data, and
to be rather careful how you interpet the result.

On Wed, 17 Dec 2003, Nathan Weisz wrote:

> Hi all,
> 
> I have a strange problem and rigth now I can't figure out a 
> solution.
> 
> Trying to calculate an ANOVA with one between subject factor (group) 
> and one within (hemisphere). My dependent variable is source 
> localization (data). My N = 25.
> 
> My data.frame looks like this:
> > ML.dist.stack
>    subj group hemisphere      data
> 1     1   tin       left 0.7460840
> 2     2   tin       left 1.0300600
> 3     3   tin       left 0.8026865
> <<SNIP>>
> 47   22   con      rigth 0.6808010
> 48   23   con      rigth 1.2291490
> 49   24   con      rigth 2.0930595
> 50   25   con      rigth 1.5631695
> 
> > str(ML.dist.stack)
> `data.frame':	50 obs. of  4 variables:
>  $ subj      : Factor w/ 25 levels "1","2","3","4",..: 1 2 3 4 5 6 7 
> 8 9 10 ...
>  $ group     : Factor w/ 2 levels "con","tin": 2 2 2 2 2 2 2 2 2 2 
> ...
>  $ hemisphere: Factor w/ 2 levels "left","rigth": 
> 1 1 1 1 1 1 1 1 1 1 ...
>  $ data      : num  0.746 1.030 0.803 2.197    NA ...
> 
> Typing: summary(aov(data ~group * hemisphere + Error(subj/
> hemisphere), data=ML.dist.stack))
> gives me following result ...
> Error: subj
>                  Df  Sum Sq Mean Sq F value Pr(>F)
> group             1  0.1813  0.1813  0.2012 0.6584
> hemisphere        1  0.1937  0.1937  0.2149 0.6477
> group:hemisphere  1  0.0898  0.0898  0.0996 0.7554
> Residuals        21 18.9277  0.9013               
> 
> Error: subj:hemisphere
>                  Df  Sum Sq Mean Sq F value Pr(>F)
> hemisphere        1  1.0534  1.0534  1.6736 0.2105
> group:hemisphere  1  0.5714  0.5714  0.9077 0.3521
> Residuals        20 12.5893  0.6295               
> ... which looks like the error strata are wrong.
> 
> I kept very close (in my opinion exactly) to Baron's Example 4 of 
> the tutorial (p. 29), which produces correct results. See below:
> > str(Ela.uni)
> `data.frame':	96 obs. of  5 variables:
>  $ effect: num  19 11 20 21 18 17 20 14 16 26 ...
>  $ subj  : Factor w/ 16 levels "s1","s10","s11",..: 
> 1 9 10 11 12 13 14 15 16 2 ...
>  $ gp    : Factor w/ 2 levels "gp1","gp2": 1 1 1 1 1 1 1 1 2 2 ...
>  $ drug  : Factor w/ 2 levels "dr1","dr2": 1 1 1 1 1 1 1 1 1 1 ...
>  $ dose  : Factor w/ 3 levels "do1","do2","do3": 1 1 1 1 1 1 1 1 1 1 
> ...
> > summary(aov(effect ~ gp * drug * dose + Error(subj/(dose+drug)), 
> data=Ela.uni))
> 
> Error: subj
>           Df Sum Sq Mean Sq F value  Pr(>F)  
> gp         1 270.01  270.01  7.0925 0.01855 *
> Residuals 14 532.98   38.07                  
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 
> 
> Error: subj:dose
>           Df Sum Sq Mean Sq F value    Pr(>F)    
> dose       2 758.77  379.39 36.5097 1.580e-08 ***
> gp:dose    2  42.27   21.14  2.0339    0.1497    
> Residuals 28 290.96   10.39                      
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 
> 
> Error: subj:drug
>           Df Sum Sq Mean Sq F value   Pr(>F)   
> drug       1 348.84  348.84  13.001 0.002866 **
> gp:drug    1 326.34  326.34  12.163 0.003624 **
> Residuals 14 375.65   26.83                    
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 
> 
> Error: Within
>              Df  Sum Sq Mean Sq F value Pr(>F)
> drug:dose     2  12.063   6.031  0.6815 0.5140
> gp:drug:dose  2  14.812   7.406  0.8369 0.4436
> Residuals    28 247.792   8.850               
> 
> Can someone give me a hint where I migth be going wrong? However 
> trivial this migth be I somehow got stuck.
> 
> All the best,
> Nathan
> 
> -----------------------------------------
> Nathan Weisz
> Department of Psychology
> University of Konstanz
> P.O. Box D25
> D - 78457 Konstanz
> GERMANY
> 
> Tel: +49 (0)7531 88- 4612
> E-mail: Nathan.Weisz at uni-konstanz.de
> http://www.clinical-psychology.uni-konstanz.de
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From djw1005 at cam.ac.uk  Wed Dec 17 12:31:50 2003
From: djw1005 at cam.ac.uk (Damon Wischik)
Date: Wed, 17 Dec 2003 11:31:50 +0000 (GMT)
Subject: [R] Factor names & levels
Message-ID: <Pine.SOL.3.96.1031217113003.15725A-100000@draco.cus.cam.ac.uk>


When I alter the levels of a factor, why does it alter the names too?

f <- factor(c(A="one",B="two",C="one",D="one",E="three"),
            levels=c("one","two","three"))
names(f)

 -- gives [1] "A" "B" "C" "D" "E"

levels(f) <- c("un","deux","trois")
names(f)

 -- gives NULL

I'm using R 1.8.0 for Windows.

Damon.



From mwgrant2001 at yahoo.com  Wed Dec 17 13:40:47 2003
From: mwgrant2001 at yahoo.com (Michael Grant)
Date: Wed, 17 Dec 2003 04:40:47 -0800 (PST)
Subject: [R] Resampling Stats software - link to book mentioned
In-Reply-To: <3FDFC3B4.1090401@arcriswell.com>
Message-ID: <20031217124047.67535.qmail@web20810.mail.yahoo.com>


--- Andrew Criswell <arc at arcriswell.com> wrote:
...
> My search on Amazon fails to locate the book Brandon
> mentions, 
> "Resampling: The New Statistics". Is there more
> information on Author, 
> ISBN, etc.?

FYI, try 

http://www.resample.com/content/text/index.shtml

or 

the main site at 

http://www.resample.com

Regards,
Michael Grant



From flom at ndri.org  Wed Dec 17 14:16:06 2003
From: flom at ndri.org (Peter Flom)
Date: Wed, 17 Dec 2003 08:16:06 -0500
Subject: [R] mailing list for basic questions - preliminary sum up
Message-ID: <sfe010e3.056@MAIL.NDRI.ORG>

After thinking this over, I think it's a good idea to have the beginner
list (and I have subscribed).  

While I greatly appreciate this list, and the tremendous amount of help
I've gotten from it, the style of this list is, usually, to give fairly
short replies (e.g. "try ?function")  This is fine.  Different lists
have different styles, and people here are, after all, donating their
time and expertise.

But I think that, on the beginner list, there should be a different
style.  I think answers should be lengthier, and more discursive.  A
beginner who gets a reply like "try searching help" is likely to be put
off.  As a relative newbie myself (and one who has to work in SAS as
well as R) I still find myself having 'translation diffiuculties'.

What do others think?  Has anyone else subscribed to the beginner list
yet?

Peter

Peter L. Flom, PhD
Assistant Director, Statistics and Data Analysis Core
Center for Drug Use and HIV Research
National Development and Research Institutes
71 W. 23rd St
www.peterflom.com
New York, NY 10010
(212) 845-4485 (voice)
(917) 438-0894 (fax)



>>> Martin Wegmann <wegmann_mailinglist at gmx.net> 12/16/2003 6:49:15 PM
>>>
Hello, 

I agree completely that well thought out questions are important to
receive 
good and quick replies and I agree as well that the replies on the
R-help 
list are very good and helpful.
But I had to learn and I am still learing how to write good questions
and 
appreciate Spencer's explanantion how a good question should look like
in his 
opinion. 

I am not sure how this new mailing list might evolve. 
It might be that the R-beginner list takes some load of the R-help list
by 
reducing the amount of "basic" questions which won't be questioned
anymore 
here (what aren't many) and that new user might be taught to post
"good" 
question before they start posting to R-help.
If it proves to be ineffective or might affect R-help in some unwanted
manner 
it would be an easy one to shut it down. 

I doubt that it will split the R-help list - in my opinion it is
unlikely that 
medium/experienced R user who will subscribe to R-beginner will
unsubscribe 
from the R-help list. 
Moreover people starting with R are less likely to send any mails to
this 
list, some do and are refered in most cases to the manuals. 
When I started R I looked through the archive and because I did not
understand 
even one question, I was intimidated by this list and did not send any
mail 
until a few weeks later (that was not because of the statistics but the

commands)
For this kind of people the R-beginner list is thought - to encourage
them to 
send "stupid" questions during their first steps in R. 

They shall recognize questions they would have asked themselves.
Therefore I think that the quality of the question is in this case less

important than it's level.

I hope I did not misunderstood some points ,-)

best regards Martin



On Tuesday 16 December 2003 17:20, Spencer Graves wrote:
>       I agree with Tony's observation that well thought out
questions
> are more likely to receive an answer than something that is long,
> rambling, and poorly focused.  Many questions take more time to read
> than I have available, so I don't bother.  I like questions that
include
> toy examples in a few lines of code that I can copy from an email
into R
> and test ideas.  Careful formatting that looks pretty in an email is
an
> obstacle for me, because it increases the work required to get it
into
> R.  Many questioners could answer their own problems in the process
of
> generating such a toy example.  When they can't, that exercise helps
> them focus the question, which makes it easier for potential
respondents
> to understand the problem and reply.  Without that, I must either
> generate a toy example myself (which I've done many times) or
respond
> with untested code and risk looking stupid when my untested
suggestion
> doesn't work.
>
>       hope this helps.
>       spencer graves
>
> A.J. Rossini wrote:
> >"Pascal A. Niklaus" <Pascal.Niklaus at unibas.ch> writes:
> >>- In my experience even *very* basic questions *relating to the R
> >>language* do get answered on r-help. I'm impressed by how much
time
> >>some members of the R core team spend answering relatively basic
> >>questions, and by how elaborate their answers generally are. So I
> >>cannot see much need for a new R mailing list. There are these
> >>excellent mailing list archives, so why "fragment" this list?
> >
> >To follow up, well-thought through basic questions do get answered;
in
> >particular, they can be useful for those of us writing packages,
> >documentation, etc.
> >
> >I have a sense that it is the quality of the question (details of
what
> >is intended to do, or not known, signs of using other sources of
> >materials which folks have spent years on, no signs that this is a
"do
> >my work for me" question) rather than the level of the question,
that
> >is an issue.
> >
> >best,
> >-tony
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help 

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From mehdi.kadiri at ligeron.com  Wed Dec 17 14:24:09 2003
From: mehdi.kadiri at ligeron.com (Mehdi Kadiri)
Date: Wed, 17 Dec 2003 14:24:09 +0100
Subject: [R] Asking for help 
Message-ID: <000c01c3c4a1$0effcdd0$af62933e@PORTABLE211>

Hello evry one,
I'm a frensh consulting Engineer in statistics and i work under R times to
times.
I would like to build an Graphic User Interface as we can do under MS Excel
but i don't know how to do it.

The aim is to have a GUI betwen a user and the console or the function (for
exemple toto(a,b,c) )
In the GUI , i would like to have 3 boxes empty where i can put the values
of a, b, and c, and an other box where I click to Submit and a last bos to
read the result.

I didn't find codes already done about it!
And i don't known how to do this

Can anyboby help me in this,

I ve R 1.7 windows 2000

thank you very much and excuse me for the bad english I have!

...

Mehdi KADIRI
?mail : Mehdi.Kadiri at ligeron.com



From fabien.fivaz at unine.ch  Wed Dec 17 14:38:19 2003
From: fabien.fivaz at unine.ch (Fabien Fivaz)
Date: Wed, 17 Dec 2003 14:38:19 +0100
Subject: [R] Asking for help
In-Reply-To: <000c01c3c4a1$0effcdd0$af62933e@PORTABLE211>
References: <000c01c3c4a1$0effcdd0$af62933e@PORTABLE211>
Message-ID: <3FE05C4B.6060900@unine.ch>

  > The aim is to have a GUI betwen a user and the console or the 
function (for
> exemple toto(a,b,c) )
> In the GUI , i would like to have 3 boxes empty where i can put the values
> of a, b, and c, and an other box where I click to Submit and a last bos to
> read the result.

Take a look at http://www.sciviews.org/_rgui/. I am using Tcl/Tk for 
building little GUI's for R. You just have to write a simple function 
that does it. At SciViews, you can find a Tcl/Tk page with some nice 
code examples.

Best Wishes

Fabien

-- 
==============================
Fabien Fivaz
KARCH
Naturhistorisches Museum
Bernastrasse 15
CH - 3005 Bern

fabien.fivaz at unine.ch
Tel. 031 350 74 55
Fax. 031 350 74 99



From allison.100 at osu.edu  Wed Dec 17 14:43:54 2003
From: allison.100 at osu.edu (Gary Allison)
Date: Wed, 17 Dec 2003 08:43:54 -0500
Subject: [R] variance estimates in lme biased?
In-Reply-To: <3FE02D6C.3040007@unibas.ch>
References: <3FDFB2E9.4070900@osu.edu> <3FE02D6C.3040007@unibas.ch>
Message-ID: <3FE05D9A.1050407@osu.edu>

Pascal,
If every run of my simulation produced results like you saw, I would not 
be concerned.  But a sizable fraction of my simulation runs produce much 
larger standard deviations in level 1, though level 3's estimates stay 
small.  I've posted the results from 500 runs at:

http://david.science.oregonstate.edu/~allisong/R/nestVar_toR2.csv
http://david.science.oregonstate.edu/~allisong/R/nestVar_toR2.pdf
(and the code that produced it)
http://david.science.oregonstate.edu/~allisong/R/NestedSim_toR2.R

- compare the range of estimated variances for level 1 vs. level 3 
(levels with no simulated variance).  For example, what do you make of 
run 6?

My concern is how I am to interpret some experimental results I have 
where both level 1 and level 2 standard deviations are high. I'm perplexed.

Thanks,
Gary

Pascal A. Niklaus wrote:
> Running lme on your data set results exactly in what you expect - or do 
> you expect something different?
> 
> Pascal
> 
>  > L1<-factor(F1f)
>  > L2<-factor(F2f)
>  > L3<-factor(F3f)
>  > lme(value ~ 1,random = ~ 1 | L1/L2/L3)
> Linear mixed-effects model fit by REML
>  Data: NULL
>  Log-restricted-likelihood: 438.9476
>  Fixed: value ~ 1
> (Intercept)
>  0.2955631
> 
> Random effects:
> Formula: ~1 | L1
>        (Intercept)
> StdDev:  0.02472988              <== level F1 which is 0
> 
> Formula: ~1 | L2 %in% L1
>        (Intercept)
> StdDev:    1.140782                <== level F2 which is 1
> 
> Formula: ~1 | L3 %in% L2 %in% L1
>         (Intercept)  Residual
> StdDev: 0.0005512791 0.1020479   <== F3 which is 0 , and F4 which is 0.1
> 
> Number of Observations: 625
> Number of Groups:
>                L1         L2 %in% L1 L3 %in% L2 %in% L1
>                 5                 25                125
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From mseewald at gmx.de  Wed Dec 17 14:53:31 2003
From: mseewald at gmx.de (Michael Seewald)
Date: Wed, 17 Dec 2003 14:53:31 +0100 (CET)
Subject: [R] R on Itanium in 64bit mode? Experiences?
Message-ID: <Pine.LNX.4.53.0312171446180.3040@lakeforest.homelinux.com>


Dear all,

I would like to raise the question regarding the Intel Itanium processors. Is
anyone using them in 64bit mode? I am mostly interested in memory-consuming
applications, e.g. the normalization of a large number of microarrays.

Andy Liaw told me that he is using R on a dual AMD Opteron in 64bit mode. He
can easily allocate 13GB of RAM in a single R process. It would be great if
someone in the forum could share his Itanium experiences with respect to that.
(OS/compiler issues would also be interesting..)

Thanks & best wishes from Vienna,
Michael



From andy_liaw at merck.com  Wed Dec 17 14:54:21 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 17 Dec 2003 08:54:21 -0500
Subject: [R] Asking for help
Message-ID: <3A822319EB35174CA3714066D590DCD50205CF60@usrymx25.merck.com>

See ?winDialog and the "See Also" section there.  Maybe that would suffice
for your purpose.

HTH,
Andy

> From: Mehdi Kadiri
> 
> Hello evry one,
> I'm a frensh consulting Engineer in statistics and i work 
> under R times to
> times.
> I would like to build an Graphic User Interface as we can do 
> under MS Excel
> but i don't know how to do it.
> 
> The aim is to have a GUI betwen a user and the console or the 
> function (for
> exemple toto(a,b,c) )
> In the GUI , i would like to have 3 boxes empty where i can 
> put the values
> of a, b, and c, and an other box where I click to Submit and 
> a last bos to
> read the result.
> 
> I didn't find codes already done about it!
> And i don't known how to do this
> 
> Can anyboby help me in this,
> 
> I ve R 1.7 windows 2000
> 
> thank you very much and excuse me for the bad english I have!
> 
> ...
> 
> Mehdi KADIRI
> ?mail : Mehdi.Kadiri at ligeron.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
>



From rolf at math.unb.ca  Wed Dec 17 15:01:18 2003
From: rolf at math.unb.ca (Rolf Turner)
Date: Wed, 17 Dec 2003 10:01:18 -0400 (AST)
Subject: [R] mailing list for basic questions - preliminary sum up
Message-ID: <200312171401.hBHE1IOC003559@erdos.math.unb.ca>


Gabor Grothendieck wrote:

> My personal view on this is that there is need for a friendly
> list with a more "customer service" attitude than r-help.

God save us from a ``"customer service" attitude'' --- bland,
fatuous, feel-good useless twaddle!  If you want a ``custome
service'' attitude go and use Microsoft's crap!  The subscribers to
this list are not customers, they are participants in a collective
endeavour.  Those who come in with an ``I'm a customer; service me''
(;-)) attitude should look elsewhere.  If they are such wimps that
they collapse from being told a few brusque home truths, they
shouldn't be doing statistical computing in the first place.  When a
Certain Guru rips strips off people (God knows he's done it to me
often enough) on this list, there's a damned good reason for it.


					cheers,

						Rolf Turner
						rolf at math.unb.ca



From Ted.Harding at nessie.mcc.ac.uk  Wed Dec 17 15:21:29 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 17 Dec 2003 14:21:29 -0000 (GMT)
Subject: [R] mailing list for basic questions - preliminary sum up
In-Reply-To: <sfe010e3.056@MAIL.NDRI.ORG>
Message-ID: <XFMail.031217142129.Ted.Harding@nessie.mcc.ac.uk>

On 17-Dec-03 Peter Flom wrote:
> After thinking this over, I think it's a good idea to have the beginner
> list (and I have subscribed).  
> [...]
> What do others think?  Has anyone else subscribed to the beginner list
> yet?
> 
> Peter

Perhaps there has now been enough discussion of whether such a list
is a good idea; it should now go ahead and prove its usefulness and
desirability (or not) in practice.

I also think that it would be very useful for more experienced R users,
if so inclined, to subscribe to it (I have done so).

As to questions going to the beginners' list rather than r-help,
the more experienced members could usefully propose that someone's
question would be a good one for r-help.

Let's see how it goes, and support Martin Wegman's initiative.

Best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 17-Dec-03                                       Time: 14:21:29
------------------------------ XFMail ------------------------------



From rossini at blindglobe.net  Wed Dec 17 15:41:13 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Wed, 17 Dec 2003 06:41:13 -0800
Subject: [R] mailing list for basic questions - preliminary sum up
In-Reply-To: <sfe010e3.057@MAIL.NDRI.ORG> (Peter Flom's message of "Wed, 17
	Dec 2003 08:16:06 -0500")
References: <sfe010e3.057@MAIL.NDRI.ORG>
Message-ID: <85u13zcxrq.fsf@blindglobe.net>

"Peter Flom" <flom at ndri.org> writes:

> What do others think?  Has anyone else subscribed to the beginner list
> yet?

I won't.  There is not enough time in the day.

best,
-tony

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From tlumley at u.washington.edu  Wed Dec 17 16:42:56 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 17 Dec 2003 07:42:56 -0800 (PST)
Subject: [R] variance estimates in lme biased?
In-Reply-To: <3FDFB2E9.4070900@osu.edu>
References: <3FDFB2E9.4070900@osu.edu>
Message-ID: <Pine.A41.4.58.0312170732500.117868@homer40.u.washington.edu>

On Tue, 16 Dec 2003, Gary Allison wrote:

> Hi all,
> I didn't get a response to my post of this issue a week ago, so I've
> tried to clarify:
>
> When I use lme to analyze a model of nested random effects, the variance
> estimates of levels higher in the hierarchy appear to have much more
> variance than they should.
>
> In the example below with 4 levels, I simulate variance in level 2
> (sd=1.0) and level 4 (sd=0.1), but levels 1 and 3 do not vary.  Although
> I expected to see a small amount of variability in the lme estimates of
> levels 1 and 3, I am confused by what happens in the level 1 estimates:
>   more than 10% of the runs produce stdDev of >0.4 and in many cases the
> level 1 estimate is close to level 2's. Nothing like that happens in
> level 3.
>

While I haven't seen any literature on this precise problem I would not be
particularly surprised by it.  Your model says that there is a lot of
variability at level two but that it all averages out to zero within a
level 1 unit.  It would not be particularly surprising if some of the
level 2 variability leaked up to level one, since it is only being
averaged over 5 level 2 units per level 1 unit.

Given sufficient data this will eventually stop happening, but you don't
have very many level 1 replicates either.

I think that if you really need to estimate a small variance component
with large variances nested within it, you need a lot more data or a
prior.


	-thomas



From Pascal.Niklaus at unibas.ch  Wed Dec 17 16:52:07 2003
From: Pascal.Niklaus at unibas.ch (Pascal A. Niklaus)
Date: Wed, 17 Dec 2003 16:52:07 +0100
Subject: [R] Session log file...
Message-ID: <3FE07BA7.7000604@unibas.ch>

Hi all,

I tried to save a complete log of a R session we had in a seminar 
today... but I didn't succeed.

1) R | tee session.log
This saves both input and output, but I do get the cursor key escape 
sequences from editing (cursor-up to get last command etc) instead of 
the actual command line executed.

2) savehistory
Gets commands only, not the output

3) sink
Gets output, without the commands producing them

I'm sure I'm not the first one encountering this problem - there's for 
sure a solution, but I didn't manage to find it. If there is no such 
thing, would it make sense to add such a function to R?

Any help is greatly appreciated

Pascal

PS: I'm using R 1.8.1 from a linux terminal window



From MSchwartz at medanalytics.com  Wed Dec 17 17:01:15 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Wed, 17 Dec 2003 10:01:15 -0600
Subject: [R] read.spss question warning compression bias
In-Reply-To: <74E242B6968AA0469B632C5A3EFC1EFD03D57019@nt207mesep.corporate.hdwa.health.wa.gov.au>
References: <74E242B6968AA0469B632C5A3EFC1EFD03D57019@nt207mesep.corporate.hdwa.health.wa.gov.au>
Message-ID: <1071676875.2738.103.camel@localhost.localdomain>

Greetings all,

In follow up to this thread (I am copying all participants), I want to
provide some additional data.

In review, Peter Flom the original poster, received the following
warning message when using read.spss() to import a .SAV format SPSS data
set into R:

Warning message: 
c:\NDRI\cvar\data\cvar2rev3.sav: Compression bias (0) is not the usual
value of 100. 

That warning message is generated in file sfm-read.c, which is a part of
the foreign package. The code in that file to read SPSS datasets was
provided by Ben Pfaff, who has authored an open source version of SPSS,
called PSPP (http://www.gnu.org/software/pspp/pspp.html).

The bias setting is part of the routine that transforms data byte codes
in compressed .SAV files. This value is stored in the SPSS data file
header along with a compression TRUE/FALSE flag. The bias setting is not
used in non-compressed .SAV files.

During offlist exchanges with Peter, he indicated that the SPSS data
file in question was created via the use of DBMS/Copy rather than via
SPSS itself.  In this case, a SAS dataset was converted into the SPSS
dataset via DBMS/Copy. Peter was then attempting to import the SPSS .SAV
file into R using read.spss().

For those unfamiliar, DBMS/Copy (http://www.dataflux.com/dbms/copy.asp)
is a file transformation application that can take input files from one
format and generate output files in alternate formats. There is at least
one other similar data mapping/transformation application that I am
familiar with called DataJunction
(http://pervasive.datajunction.com/djcosmos).

DBMS/Copy was originally published by a company called Conceptual, which
in 2002 sold the product to SAS, where it is now sold via Dataflux,
which is a SAS subsidiary.

Last week, I communicated with the Dataflux/SAS tech support folks to
try to pursue a better understanding of the etiology of the problem. It
turns out that the original author of DBMS/Copy is now employed at SAS
and was available to review this issue.

The bottom line is that in DBMS/Copy, the default is to generate a
non-compressed SPSS format file. Thus, the author's code sets the bias
value to 0 by default. In the case of a user generating a compressed
.SAV file, the bias setting is set to 100.

It is unclear at this time if this was a part of any formal SPSS
specification. However, from all available documentation, there is no
indication that the bias value can be otherwise adjusted by a user,
either directly or indirectly. Thus, to my knowledge at this point, it
can take only two values, 0 and 100. If accurate, it would seem to be
redundant to the compression TRUE/FALSE flag.

In the case of SPSS itself, the bias value of 100 is set by default,
whether the .SAV file is compressed or not. Therefore, if using
read.spss() on a .SAV file that was generated by SPSS natively, the
warning that Peter experienced would not be issued.

I hope that this information is of help to folks. With this confirmation
in hand, I would like to reiterate my suggestion to add a note to the
help for read.spss(), which could read as follows:


"NOTE: You may receive the following message:

 Warning message: 
 FileName: Compression bias (X) is not the usual value of 100.

Where 'FileName' will be the SPSS file that you are reading and 'X' will
be a numeric value, possibly 0. This may be the result of reading an
UNCOMPRESSED SPSS file that was not generated via SPSS natively (ie. via
a third party application such as DBMS/Copy). As the exact meaning of
this cannot be confirmed in all cases, it is recommended that you verify
the integrity of your imported SPSS data after using read.spss()."


As an aside, the Dataflux folks indicate that DBMS/Copy, at this time,
cannot read SPSS version 11 files. Thus it would seem that there has
been some change in the native .SAV file structure of unknown scope.
Presumably, this could have an impact on read.spss().

Best regards,

Marc Schwartz


P.S. to Thomas. It would seem worthy of consideration to forward this
information to Ben Pfaff. Let me know if you want me to do this or if
you would prefer otherwise.



From p.dalgaard at biostat.ku.dk  Wed Dec 17 17:19:15 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 17 Dec 2003 17:19:15 +0100
Subject: [R] variance estimates in lme biased?
In-Reply-To: <Pine.A41.4.58.0312170732500.117868@homer40.u.washington.edu>
References: <3FDFB2E9.4070900@osu.edu>
	<Pine.A41.4.58.0312170732500.117868@homer40.u.washington.edu>
Message-ID: <x2u13z1kos.fsf@biostat.ku.dk>

Thomas Lumley <tlumley at u.washington.edu> writes:

> On Tue, 16 Dec 2003, Gary Allison wrote:
> 
> > Hi all,
> > I didn't get a response to my post of this issue a week ago, so I've
> > tried to clarify:
> >
> > When I use lme to analyze a model of nested random effects, the variance
> > estimates of levels higher in the hierarchy appear to have much more
> > variance than they should.
> >
> > In the example below with 4 levels, I simulate variance in level 2
> > (sd=1.0) and level 4 (sd=0.1), but levels 1 and 3 do not vary.  Although
> > I expected to see a small amount of variability in the lme estimates of
> > levels 1 and 3, I am confused by what happens in the level 1 estimates:
> >   more than 10% of the runs produce stdDev of >0.4 and in many cases the
> > level 1 estimate is close to level 2's. Nothing like that happens in
> > level 3.
> >
> 
> While I haven't seen any literature on this precise problem I would not be
> particularly surprised by it.  Your model says that there is a lot of
> variability at level two but that it all averages out to zero within a
> level 1 unit.  It would not be particularly surprising if some of the
> level 2 variability leaked up to level one, since it is only being
> averaged over 5 level 2 units per level 1 unit.
> 
> Given sufficient data this will eventually stop happening, but you don't
> have very many level 1 replicates either.
> 
> I think that if you really need to estimate a small variance component
> with large variances nested within it, you need a lot more data or a
> prior.

Or, try looking at a smaller example where things can be worked out
explicitly: One-way ANOVA with random btw.group variation. Say 5
groups and 3 obs per group. If I got this right (please do check!),
the estimate of the between-group variance is 1/3 times the difference
between two chi^2/f distributed variables with 4 and 10 DF
respectively. This will become negative about half the time, and lme
(and similar code) will set it to zero in that case. Now

> sd.sim <- sqrt(pmax(1/3*(rchisq(1000,4)/4 - rchisq(1000,10)/10),0))
> summary(sd.sim)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
 0.0000  0.0000  0.0000  0.2017  0.3955  1.2160

does not seem to be too far from what Gary has been experiencing. 

Obviously, the fact that the estimator is censored at zero will make
it biased, but an extended estimator (allowing negative values) is
unbiased. 

> var.sim <- 1/3*(rchisq(1000,4)/4 - rchisq(1000,10)/10)
> summary(var.sim)
     Min.   1st Qu.    Median      Mean   3rd Qu.      Max.
-0.796100 -0.196900 -0.042720 -0.007667  0.138500  1.104000



-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From feh3k at spamcop.net  Wed Dec 17 17:29:11 2003
From: feh3k at spamcop.net (Frank E Harrell Jr)
Date: Wed, 17 Dec 2003 11:29:11 -0500
Subject: [R] mailing list for basic questions - preliminary sum up
In-Reply-To: <Pine.LNX.4.44.0312170833470.1851-100000@reclus.nhh.no>
References: <20031217044358.1E43E399D@mprdmxin.myway.com>
	<Pine.LNX.4.44.0312170833470.1851-100000@reclus.nhh.no>
Message-ID: <20031217112911.754e8edb.feh3k@spamcop.net>

My opinion is that separate lists are not needed (and I'm not clear on how
the person with the original idea summarized opinions in a way that
led to the conclusion that a new list is needed), but that a different
medium may be needed.  The problem with e-mail is that to many users,
especially those who don't search the archives, e-mail is "memoryless",
and that individual e-mail messages become cumulative rather than being
corrected or updated.  How many times have we seen almost identical
questions posed only days apart?  A well-organized discussion board (e.g.
http://www.knoppix.net/forum) or wiki (e.g. using methods provided by
twiki.org - see http://web.brandeis.edu/pages/view/ITS for a nice example;
other users will know of better examples)
is worth considering.  It would be especially nice if before pressing the
"Submit New Message" button a user had to check a few boxes acknowledging
that she had consulted various sources of information, and besides the
checkboxes would be links to those sources.  Topics and subtopics would
have to be created by an administrator but users could add sub sub topics
and, optimally, edit other users' responses.  This approach would IMHO get
better participation by both novices and experts than would having two
lists.

---
Frank E Harrell Jr   Professor and Chair           School of Medicine
                     Department of Biostatistics   Vanderbilt University



From maechler at stat.math.ethz.ch  Wed Dec 17 17:45:18 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 17 Dec 2003 17:45:18 +0100
Subject: [R] Session log file...
In-Reply-To: <3FE07BA7.7000604@unibas.ch>
References: <3FE07BA7.7000604@unibas.ch>
Message-ID: <16352.34846.639336.347064@gargle.gargle.HOWL>

>>>>> "Pascal" == Pascal A Niklaus <Pascal.Niklaus at unibas.ch>
>>>>>     on Wed, 17 Dec 2003 16:52:07 +0100 writes:

    Pascal> Hi all,
    Pascal> I tried to save a complete log of a R session we had in a seminar 
    Pascal> today... but I didn't succeed.

    Pascal> 1) R | tee session.log
    Pascal> This saves both input and output, but I do get the cursor key escape 
    Pascal> sequences from editing (cursor-up to get last command etc) instead of 
    Pascal> the actual command line executed.

    Pascal> 2) savehistory
    Pascal> Gets commands only, not the output

    Pascal> 3) sink
    Pascal> Gets output, without the commands producing them

    Pascal> I'm sure I'm not the first one encountering this
    Pascal> problem - there's for sure a solution, but I didn't
    Pascal> manage to find it. If there is no such thing, would
    Pascal> it make sense to add such a function to R?

There has been a solution to this problem for a longer time than
R exists: 
  ESS (Emacs Speaks Statistitics, called "S-mode" in
       those days, and used for S and then S-plus).
  
  You work with an (or several) *R* buffer with the whole log
  that you (can edit even during use and) save at the end as,
  e.g.  sess.Rout
  {using *.Rout will make emacs/ESS use the ESS-transcript mode
   when opening that file later; in ESS-transcript mode, you can
   again send the (old) input lines to a running R process, by
   simple <Enter>.}

Probably for that reason, nobody of us (developers) has ever
felt enough need for an alternative to implement another one
(platform independently!).

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From ggrothendieck at myway.com  Wed Dec 17 17:53:59 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 17 Dec 2003 11:53:59 -0500 (EST)
Subject: [R] mailing list for basic questions - preliminary sum up
Message-ID: <20031217165359.300AA3965@mprdmxin.myway.com>



I agree that

- wikis (see the successful one for the lua programming
  language at: http://lua-users.org/wiki/ )
- forums

are nice.  Actually someone did set up an R wiki some time 
ago at:

   http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl?RwikiHome

yet no one really used it.  Some critical mass of use is needed
to get such a project off the ground.

Other ways of communicating include:

- a moderated list

- a blog/summary such as this one for the Python language:
    http://www.pythonware.com/daily/index.htm
  or this one for the Ruby language:
    http://www.rubygarden.org/rurl/html/index.html

Unfortunately these last two require a sustained effort on 
someone's part and I suspect no one would be willing to 
commit to this.

--- 
Date: Wed, 17 Dec 2003 11:29:11 -0500 
From: Frank E Harrell Jr <feh3k at spamcop.net>
[ Add to Address Book | Block Address | Report as Spam ] 
To: rhelp <r-help at stat.math.ethz.ch> 
Subject: Re: [R] mailing list for basic questions - preliminary sum up 

 
 
My opinion is that separate lists are not needed (and I'm not clear on how
the person with the original idea summarized opinions in a way that
led to the conclusion that a new list is needed), but that a different
medium may be needed. The problem with e-mail is that to many users,
especially those who don't search the archives, e-mail is "memoryless",
and that individual e-mail messages become cumulative rather than being
corrected or updated. How many times have we seen almost identical
questions posed only days apart? A well-organized discussion board (e.g.
http://www.knoppix.net/forum) or wiki (e.g. using methods provided by
twiki.org - see http://web.brandeis.edu/pages/view/ITS for a nice example;
other users will know of better examples)
is worth considering. It would be especially nice if before pressing the
"Submit New Message" button a user had to check a few boxes acknowledging
that she had consulted various sources of information, and besides the
checkboxes would be links to those sources. Topics and subtopics would
have to be created by an administrator but users could add sub sub topics
and, optimally, edit other users' responses. This approach would IMHO get
better participation by both novices and experts than would having two
lists.

---
Frank E Harrell Jr Professor and Chair School of Medicine
Department of Biostatistics Vanderbilt University

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From baron at psych.upenn.edu  Wed Dec 17 17:59:13 2003
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Wed, 17 Dec 2003 11:59:13 -0500
Subject: [R] mailing list for basic questions - preliminary sum up
In-Reply-To: <20031217112911.754e8edb.feh3k@spamcop.net>
References: <20031217044358.1E43E399D@mprdmxin.myway.com>
	<Pine.LNX.4.44.0312170833470.1851-100000@reclus.nhh.no>
	<20031217112911.754e8edb.feh3k@spamcop.net>
Message-ID: <20031217165912.GA6645@mail2.sas.upenn.edu>

I completely agree with Frank Harrell's suggestion that email is
a problem for beginners (who often don't know about the various
searchable archives, or find them overwhelming because the
massive amount that they contain, much of which is bound to be
irrelevant or too advanced).

I don't think a Wiki is the right thing.  It isn't intended for
the sort of discussion I imagine.  (And I don't know immediately
how to set one up.  But, if someone else wants to do it ...)

However, I volunteer to set up a VERY simple bulletin board using
Bazookaboard, for a start.  If it becomes too big, I may have to
switch to something with more features.  Too see an example, see
my class bb for this term:

http://www.psych.upenn.edu/~baron/p253/
(The course was "Behavioral law and economics," and students did
not have to post their comments this way, so there isn't much
there.)

Or, someone else can do it.  But I do think this is a good idea.

Jon

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron
R page:               http://finzi.psych.upenn.edu/



From jasont at indigoindustrial.co.nz  Wed Dec 17 17:39:26 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Thu, 18 Dec 2003 05:39:26 +1300
Subject: [R] Asking for help
In-Reply-To: <000c01c3c4a1$0effcdd0$af62933e@PORTABLE211>
References: <000c01c3c4a1$0effcdd0$af62933e@PORTABLE211>
Message-ID: <3FE086BE.8080208@indigoindustrial.co.nz>

Mehdi Kadiri wrote:

> Hello evry one,
> I'm a frensh consulting Engineer in statistics and i work under R times to
> times.
> I would like to build an Graphic User Interface as we can do under MS Excel
> but i don't know how to do it.
> 

If you want to program the interface in VB, you can use the tools in 
this directory to set up an R/Excel interface via DCOM:

http://cran.r-project.org/contrib/extra/dcom/

There are examples included to get you started.

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From wegmann_mailinglist at gmx.net  Wed Dec 17 14:57:25 2003
From: wegmann_mailinglist at gmx.net (Martin Wegmann)
Date: Wed, 17 Dec 2003 14:57:25 +0100
Subject: [R] mailing list for basic questions - preliminary sum up
In-Reply-To: <Pine.LNX.4.44.0312170833470.1851-100000@reclus.nhh.no>
References: <Pine.LNX.4.44.0312170833470.1851-100000@reclus.nhh.no>
Message-ID: <200312171457.25540.wegmann_mailinglist@gmx.net>

Hello, 

Roger Bivand wrote:
> appropriate light. One basic characteristic seems to be that if the
> question does indicate seriousness about trying to analyse data, respect
> for the task at hand, then predictably lots of good advice comes quickly.

yes, I also experienced that (from the questioner point)

> I'm also not too sure about the "learning R" question. Of course there is
> the GUI/CLI issue, and the "very many defaults already filled in" issue,
> but actually market share really isn't a driver here, is it? Isn't this

ok, I get this point - R can be seen more as "philosphie" than "commercial" 
thinking of getting market shares.
i think it is a trade-off between "spreading" R and being a member of a "geek" 
statistic program (that's what SPSS user in this dept. think about us R 
user ;-)  )

> more about attitude and motivation in taking an active role in analysing
> data? If your research question really itches, what should it take to stop
> you learning R (or associated packages)?

probably the steep learing curve but of course if you really want, you will 
suceed and explorere the advanteages of R. 
That sounds a bit like an elitist point of view - who really wants to use R 
will eventually suceed.....but that's like everywhere in the opensource 
community - to get started with Linux or other software is still quite rough 
but a lot of people are doing it because of various reasons. This community 
has a great support mentality and are more worth than any commercial support 
and therefore it is possible. 

but I don't want to start a flame about this issue (elitist geek 
software ;-)  )and therefore I would like to propose a "evaluation" - better 
in a separate mail to keep the overview.

regards Martin



From wegmann_mailinglist at gmx.net  Wed Dec 17 18:14:50 2003
From: wegmann_mailinglist at gmx.net (Martin Wegmann)
Date: Wed, 17 Dec 2003 18:14:50 +0100
Subject: [R] mailing list for basic questions - preliminary sum up
In-Reply-To: <20031217044358.1E43E399D@mprdmxin.myway.com>
References: <20031217044358.1E43E399D@mprdmxin.myway.com>
Message-ID: <200312171447.10753.wegmann_mailinglist@gmx.net>

Hello, 
On Wednesday 17 December 2003 05:43, Gabor Grothendieck wrote:
> My personal view on this is that there is need for a friendly
>
> list with a more "customer service" attitude than r-help.

well that sounds for me like a shop - "customer service"- ;-) and that's what 
R should not become, in my point of view - the R core team members are 
volunteers and spend their spare time to help strangers and develop great 
software.

I agree with Roger Bivand that answers are from time to time rough due to 
language problems or because it would take more time to add some "nice" 
lines, not because they are rude. 

But perhaps medium-experienced user who are not very active on this list, 
might become more active on the beginner list and perhaps spend more time to 
write nice replies. perhaps ,-)

regards Martin


>
>
> r-help is really very useful but its also intimidating
>
> and I bet lots of people have questions that they never ask
>
> for fear of the response.   Maybe some of them even decide
>
> not to learn R.
>
>
>
> ---
>
> Date: Wed, 17 Dec 2003 00:49:15 +0100
>
> From: Martin Wegmann <wegmann_mailinglist at gmx.net>
>
> To: Spencer Graves <spencer.graves at pdf.com>, <rossini at u.washington.edu>
>
> Cc: <r-help at stat.math.ethz.ch>
>
> Subject: Re: [R] mailing list for basic questions - preliminary sum up
>
>
>
>
>
>
>
> Hello,
>
>
>
> I agree completely that well thought out questions are important to receive
>
> good and quick replies and I agree as well that the replies on the R-help
>
> list are very good and helpful.
>
> But I had to learn and I am still learing how to write good questions and
>
> appreciate Spencer's explanantion how a good question should look like in
> his
>
> opinion.
>
>
>
> I am not sure how this new mailing list might evolve.
>
> It might be that the R-beginner list takes some load of the R-help list by
>
> reducing the amount of "basic" questions which won't be questioned anymore
>
> here (what aren't many) and that new user might be taught to post "good"
>
> question before they start posting to R-help.
>
> If it proves to be ineffective or might affect R-help in some unwanted
> manner
>
> it would be an easy one to shut it down.
>
>
>
> I doubt that it will split the R-help list - in my opinion it is unlikely
> that
>
> medium/experienced R user who will subscribe to R-beginner will unsubscribe
>
> from the R-help list.
>
> Moreover people starting with R are less likely to send any mails to this
>
> list, some do and are refered in most cases to the manuals.
>
> When I started R I looked through the archive and because I did not
> understand
>
> even one question, I was intimidated by this list and did not send any mail
>
> until a few weeks later (that was not because of the statistics but the
>
> commands)
>
> For this kind of people the R-beginner list is thought - to encourage them
> to
>
> send "stupid" questions during their first steps in R.
>
>
>
> They shall recognize questions they would have asked themselves.
>
> Therefore I think that the quality of the question is in this case less
>
> important than it's level.
>
>
>
> I hope I did not misunderstood some points ,-)
>
>
>
> best regards Martin
>
> On Tuesday 16 December 2003 17:20, Spencer Graves wrote:
> > I agree with Tony's observation that well thought out questions
> >
> > are more likely to receive an answer than something that is long,
> >
> > rambling, and poorly focused. Many questions take more time to read
> >
> > than I have available, so I don't bother. I like questions that include
> >
> > toy examples in a few lines of code that I can copy from an email into R
> >
> > and test ideas. Careful formatting that looks pretty in an email is an
> >
> > obstacle for me, because it increases the work required to get it into
> >
> > R. Many questioners could answer their own problems in the process of
> >
> > generating such a toy example. When they can't, that exercise helps
> >
> > them focus the question, which makes it easier for potential respondents
> >
> > to understand the problem and reply. Without that, I must either
> >
> > generate a toy example myself (which I've done many times) or respond
> >
> > with untested code and risk looking stupid when my untested suggestion
> >
> > doesn't work.
> >
> >
> >
> > hope this helps.
> >
> > spencer graves
> >
> > A.J. Rossini wrote:
> > >"Pascal A. Niklaus" <Pascal.Niklaus at unibas.ch> writes:
> > >>- In my experience even *very* basic questions *relating to the R
> > >>
> > >>language* do get answered on r-help. I'm impressed by how much time
> > >>
> > >>some members of the R core team spend answering relatively basic
> > >>
> > >>questions, and by how elaborate their answers generally are. So I
> > >>
> > >>cannot see much need for a new R mailing list. There are these
> > >>
> > >>excellent mailing list archives, so why "fragment" this list?
> > >
> > >To follow up, well-thought through basic questions do get answered; in
> > >
> > >particular, they can be useful for those of us writing packages,
> > >
> > >documentation, etc.
> > >
> > >
> > >
> > >I have a sense that it is the quality of the question (details of what
> > >
> > >is intended to do, or not known, signs of using other sources of
> > >
> > >materials which folks have spent years on, no signs that this is a "do
> > >
> > >my work for me" question) rather than the level of the question, that
> > >
> > >is an issue.
> > >
> > >
> > >
> > >best,
> > >
> > >-tony
> >
> > ______________________________________________
> >
> > R-help at stat.math.ethz.ch mailing list
> >
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
> _______________________________________________
> No banners. No pop-ups. No kidding.
> Introducing My Way - http://www.myway.com



From ggrothendieck at myway.com  Wed Dec 17 18:19:16 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 17 Dec 2003 12:19:16 -0500 (EST)
Subject: [R] mailing list for basic questions - preliminary sum up
Message-ID: <20031217171916.6629D398C@mprdmxin.myway.com>



In rereading this one idea occurred to me.  What if the entire R help
system were turned into a wiki?   That is,

?whatever

would take you to the help page, but not on your computer --
rather to the same page on the wiki.  You would then find the
docs as they exist now plus the experiences of other people
with that command all at the same place.   You could similarly
add your own experience to the page.

--- 
Date: Wed, 17 Dec 2003 11:53:59 -0500 (EST) 
From: Gabor Grothendieck <ggrothendieck at myway.com>
To: <feh3k at spamcop.net>, <r-help at stat.math.ethz.ch> 
Subject: Re: [R] mailing list for basic questions - preliminary sum up 

 
 


I agree that

- wikis (see the successful one for the lua programming
language at: http://lua-users.org/wiki/ )
- forums

are nice. Actually someone did set up an R wiki some time 
ago at:

http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl?RwikiHome

yet no one really used it. Some critical mass of use is needed
to get such a project off the ground.

Other ways of communicating include:

- a moderated list

- a blog/summary such as this one for the Python language:
http://www.pythonware.com/daily/index.htm
or this one for the Ruby language:
http://www.rubygarden.org/rurl/html/index.html

Unfortunately these last two require a sustained effort on 
someone's part and I suspect no one would be willing to 
commit to this.

--- 
Date: Wed, 17 Dec 2003 11:29:11 -0500 
From: Frank E Harrell Jr <feh3k at spamcop.net>
[ Add to Address Book | Block Address | Report as Spam ] 
To: rhelp <r-help at stat.math.ethz.ch> 
Subject: Re: [R] mailing list for basic questions - preliminary sum up 



My opinion is that separate lists are not needed (and I'm not clear on how
the person with the original idea summarized opinions in a way that
led to the conclusion that a new list is needed), but that a different
medium may be needed. The problem with e-mail is that to many users,
especially those who don't search the archives, e-mail is "memoryless",
and that individual e-mail messages become cumulative rather than being
corrected or updated. How many times have we seen almost identical
questions posed only days apart? A well-organized discussion board (e.g.
http://www.knoppix.net/forum) or wiki (e.g. using methods provided by
twiki.org - see http://web.brandeis.edu/pages/view/ITS for a nice example;
other users will know of better examples)
is worth considering. It would be especially nice if before pressing the
"Submit New Message" button a user had to check a few boxes acknowledging
that she had consulted various sources of information, and besides the
checkboxes would be links to those sources. Topics and subtopics would
have to be created by an administrator but users could add sub sub topics
and, optimally, edit other users' responses. This approach would IMHO get
better participation by both novices and experts than would having two
lists.

---
Frank E Harrell Jr Professor and Chair School of Medicine
Department of Biostatistics Vanderbilt University



From borgulya at gyer2.sote.hu  Wed Dec 17 18:21:21 2003
From: borgulya at gyer2.sote.hu (Gabor Borgulya)
Date: Wed, 17 Dec 2003 18:21:21 +0100
Subject: [R] R demonstration ideas?
Message-ID: <1071681680.4989.89.camel@borlinux.local>

Dear All!

Tomorrow morning I will have to demonstrate R to a professor here at the
Semmelweis University. He uses Windows and Statistica. I will have 20
minutes and I would like to convince him that R is powerful, and it even
could be used in teaching the students basic medical statistics.

Could you give me some ideas what to show him in this 20 minutes?

This night I will have about 1.5 hours for preparations.

Thanks!

Gabor



From P.Lemmens at nici.kun.nl  Wed Dec 17 18:21:28 2003
From: P.Lemmens at nici.kun.nl (Paul Lemmens)
Date: Wed, 17 Dec 2003 18:21:28 +0100
Subject: [R] mailing list for basic questions - preliminary sum up
In-Reply-To: <20031217165912.GA6645@mail2.sas.upenn.edu>
References: <20031217044358.1E43E399D@mprdmxin.myway.com>
	<Pine.LNX.4.44.0312170833470.1851-100000@reclus.nhh.no>
	<20031217112911.754e8edb.feh3k@spamcop.net>
	<20031217165912.GA6645@mail2.sas.upenn.edu>
Message-ID: <1926109.1071685288@[192.168.1.7]>

Dear All,

--On woensdag 17 december 2003 11:59 -0500 Jonathan Baron 
<baron at psych.upenn.edu> wrote:

> However, I volunteer to set up a VERY simple bulletin board using
> Bazookaboard, for a start.  If it becomes too big, I may have to
> switch to something with more features.  Too see an example, see
> my class bb for this term:
>
> http://www.psych.upenn.edu/~baron/p253/
> (The course was "Behavioral law and economics," and students did
> not have to post their comments this way, so there isn't much
> there.)
>
> Or, someone else can do it.  But I do think this is a good idea.
>
I second the forum proposal. For a personal pet project we also implemented 
a forum on which new dormitory residents could request and receive support 
on their internet connection. There was not other source of support and the 
whole project worked like a charm.

I would suggest to use phpBB <www.phpbb.com> though as an open source 
alternative to bazookaboard as this particular forum software will be 
quickly overwhelmed by the amount of material. If needed I'm of course 
willing to help out setting up phpbb.

sincerely,
Paul



-- 
Paul Lemmens
NICI, University of Nijmegen              ASCII Ribbon Campaign /"\
Montessorilaan 3 (B.01.03)                    Against HTML Mail \ /
NL-6525 HR Nijmegen                                              X
The Netherlands                                                 / \
Phonenumber    +31-24-3612648
Fax            +31-24-3616066



From wegmann_mailinglist at gmx.net  Wed Dec 17 18:29:18 2003
From: wegmann_mailinglist at gmx.net (Martin Wegmann)
Date: Wed, 17 Dec 2003 18:29:18 +0100
Subject: [R] mailing list for basic questions - proposal
In-Reply-To: <26910.1071580673@www62.gmx.net>
References: <26910.1071580673@www62.gmx.net>
Message-ID: <200312171829.18279.wegmann_mailinglist@gmx.net>

Dear R-user, 

there have been already a lot of discussion with some good points against such 
a list and the opposite opinions as well. 

Well, I would like to propose that we start a testing phase either with

- only "internal" membership, that means only people from this dept. + perhaps 
some other german universities can subscribe to list list and in 3 month 
time, there will an evaluation and summary how this list behaved.

- or an open "international" membership, where the language is english and 
everybody can subsribe and no approval is required.

In both cases the R-beginner mailing list can be shut down after this period 
of time if it proves to be inefficient, too much work or leads to a wrong 
understanding of R lists as a "customer service". 

I would reckon that for this testing phase it is not important to setup a 
searchable archive or put lots of work into it but to monitor which kind of 
questions are asked, into which direction it leads, if it encouraged people 
to ask "do it for me" questions and so on. 

I would do it and would appreciate if some native-english speaker could join. 

I hope that this proposal is acceptable for everybody.

thanks a lot for the discussion, it changed my view of a beginner mailing list 
considerably, now I see the problems behind it. 

best regards, Martin


On Tuesday 16 December 2003 14:17, Martin Wegmann wrote:
> Dear R-user,
>
> I already received quite a lot of replies to this mail and like to do a
> preliminary sum up.
>
> A few were sceptical about the use of such a beginner mailing list.
> The arguments were that people starting with R will only stay subscribed
> for a short time
> until they reached the R-help "level" and therefore only beginner will
> teach beginner how to
> use R.
>
> But as far as I can judge, the majority of people who replied to this mail
> are medium to
> experienced user who like to help beginner but does not call themselves
> highly
> experienced user as the main "answerers" on the R-help mailing list.
>
> Therefore I assume that, even though some answers might be wrong, the
> threat of
> possibly wrong answers might be minimal, due to various experienced users
> who like to
> subsribe to this list.
>
> The majority of replies were positive about such a list and welcomed the
> idea to
> encourage new user by providing a basic R mailing list, like the already
> existent
> corresponding manuals in the contributed documentation at r-project.org.
>
> And again, this list shall only provide a basic and smooth introduction
> into R and its
> capabilities.
> Questions like; "How do I make my labels in a graphic bigger? - How do I
> change the
> colour? - etc." are welcome and surely would annoy the majority of R-help
> user because it
> is mentioned somewhere on the first 10 pages of every manual, but people
> who are used
> to click on a graphic and change it in a second would not be convinced that
> R can do
> great graphics.
>
> well, I would welcome if there would be more discussion about it or to give
> it a try
> (perhaps mention it on the r-project web-site) and look how productive this
> mailing list
> proves to be.
> The address of the R-beginner mailing list is:
>
> https://lists.uni-wuerzburg.de/mailman/listinfo/r-beginner
>
> best regards, Martin



From abunn at montana.edu  Wed Dec 17 18:31:13 2003
From: abunn at montana.edu (Andy Bunn)
Date: Wed, 17 Dec 2003 10:31:13 -0700
Subject: [R] R demonstration ideas?
In-Reply-To: <1071681680.4989.89.camel@borlinux.local>
Message-ID: <002601c3c4c3$a408e8e0$78f05a99@msu.montana.edu>

Try the excellent canned demos that exist package. You can see a
complete list of all the packages you have:
demo(package = .packages(all.available = TRUE))

I'd certainly show the graphics demo.

HTH, Andy

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor Borgulya
Sent: Wednesday, December 17, 2003 10:21 AM
To: r-help at stat.math.ethz.ch
Cc: gaborgulya at hksz.eszhok.bme.hu; borgulya at gyer2.sote.hu
Subject: [R] R demonstration ideas?


Dear All!

Tomorrow morning I will have to demonstrate R to a professor here at the
Semmelweis University. He uses Windows and Statistica. I will have 20
minutes and I would like to convince him that R is powerful, and it even
could be used in teaching the students basic medical statistics.

Could you give me some ideas what to show him in this 20 minutes?

This night I will have about 1.5 hours for preparations.

Thanks!

Gabor

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From wegmann_mailinglist at gmx.net  Wed Dec 17 18:36:41 2003
From: wegmann_mailinglist at gmx.net (Martin Wegmann)
Date: Wed, 17 Dec 2003 18:36:41 +0100
Subject: [R] mailing list for basic questions - proposal
In-Reply-To: <200312171829.18279.wegmann_mailinglist@gmx.net>
References: <26910.1071580673@www62.gmx.net>
	<200312171829.18279.wegmann_mailinglist@gmx.net>
Message-ID: <200312171836.41353.wegmann_mailinglist@gmx.net>

sorry, that's the problem if I do not check my mails before sending one - I 
read the proposal of a wiki or forum and like these ideas, perhaps it is 
better than a new mailing list. 
Martin

P.S.: my summary of replies to the original idea where mainly based on 
off-list mails, sorry I should have mentioned that so that people are not 
spending hours searching the archive ;-)


On Wednesday 17 December 2003 18:29, Martin Wegmann wrote:
> Dear R-user,
>
> there have been already a lot of discussion with some good points against
> such a list and the opposite opinions as well.
>
> Well, I would like to propose that we start a testing phase either with
>
> - only "internal" membership, that means only people from this dept. +
> perhaps some other german universities can subscribe to list list and in 3
> month time, there will an evaluation and summary how this list behaved.
>
> - or an open "international" membership, where the language is english and
> everybody can subsribe and no approval is required.
>
> In both cases the R-beginner mailing list can be shut down after this
> period of time if it proves to be inefficient, too much work or leads to a
> wrong understanding of R lists as a "customer service".
>
> I would reckon that for this testing phase it is not important to setup a
> searchable archive or put lots of work into it but to monitor which kind of
> questions are asked, into which direction it leads, if it encouraged people
> to ask "do it for me" questions and so on.
>
> I would do it and would appreciate if some native-english speaker could
> join.
>
> I hope that this proposal is acceptable for everybody.
>
> thanks a lot for the discussion, it changed my view of a beginner mailing
> list considerably, now I see the problems behind it.
>
> best regards, Martin
>
> On Tuesday 16 December 2003 14:17, Martin Wegmann wrote:
> > Dear R-user,
> >
> > I already received quite a lot of replies to this mail and like to do a
> > preliminary sum up.
> >
> > A few were sceptical about the use of such a beginner mailing list.
> > The arguments were that people starting with R will only stay subscribed
> > for a short time
> > until they reached the R-help "level" and therefore only beginner will
> > teach beginner how to
> > use R.
> >
> > But as far as I can judge, the majority of people who replied to this
> > mail are medium to
> > experienced user who like to help beginner but does not call themselves
> > highly
> > experienced user as the main "answerers" on the R-help mailing list.
> >
> > Therefore I assume that, even though some answers might be wrong, the
> > threat of
> > possibly wrong answers might be minimal, due to various experienced users
> > who like to
> > subsribe to this list.
> >
> > The majority of replies were positive about such a list and welcomed the
> > idea to
> > encourage new user by providing a basic R mailing list, like the already
> > existent
> > corresponding manuals in the contributed documentation at r-project.org.
> >
> > And again, this list shall only provide a basic and smooth introduction
> > into R and its
> > capabilities.
> > Questions like; "How do I make my labels in a graphic bigger? - How do I
> > change the
> > colour? - etc." are welcome and surely would annoy the majority of R-help
> > user because it
> > is mentioned somewhere on the first 10 pages of every manual, but people
> > who are used
> > to click on a graphic and change it in a second would not be convinced
> > that R can do
> > great graphics.
> >
> > well, I would welcome if there would be more discussion about it or to
> > give it a try
> > (perhaps mention it on the r-project web-site) and look how productive
> > this mailing list
> > proves to be.
> > The address of the R-beginner mailing list is:
> >
> > https://lists.uni-wuerzburg.de/mailman/listinfo/r-beginner
> >
> > best regards, Martin



From pburns at pburns.seanet.com  Wed Dec 17 18:38:51 2003
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Wed, 17 Dec 2003 17:38:51 +0000
Subject: [R] mailing list for basic questions - preliminary sum up
References: <200312171401.hBHE1IOC003559@erdos.math.unb.ca>
Message-ID: <3FE094AB.9080507@pburns.seanet.com>

Rolf Turner wrote:

>Gabor Grothendieck wrote:
>
>  
>
>>My personal view on this is that there is need for a friendly
>>list with a more "customer service" attitude than r-help.
>>    
>>
>
>God save us from a ``"customer service" attitude'' --- bland,
>fatuous, feel-good useless twaddle!  If you want a ``custome
>service'' attitude go and use Microsoft's crap!  
>
I imagine Gabor meant _real_ customer service.  

>The subscribers to
>this list are not customers, they are participants in a collective
>endeavour.  Those who come in with an ``I'm a customer; service me''
>(;-)) attitude should look elsewhere.  If they are such wimps that
>they collapse from being told a few brusque home truths, they
>shouldn't be doing statistical computing in the first place.  
>
I wish that I shared Rolf's idealism here, but there are lots of people
who should be doing statistical computing who either aren't doing it
at all, or are using decidedly inferior tools (see point 1 above).  The
easier we can make their introduction to R (and statistics), the better.

Comments about a second mailing list are a little off the mark, since
there are already 3 mailing lists: R-help, R-devel, R-core.  R-beginner
would just be another layer.

But it is definitely useful to wonder what the best medium is.  Given
that the list is populated by statisticians, experimenting seems like a
natural choice.

Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")



From baron at psych.upenn.edu  Wed Dec 17 18:51:39 2003
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Wed, 17 Dec 2003 12:51:39 -0500
Subject: [R] mailing list for basic questions - preliminary sum up
In-Reply-To: <20031217171916.6629D398C@mprdmxin.myway.com>
References: <20031217171916.6629D398C@mprdmxin.myway.com>
Message-ID: <20031217175139.GA21774@mail2.sas.upenn.edu>

On 12/17/03 12:19, Gabor Grothendieck wrote:
>
>
>In rereading this one idea occurred to me.  What if the entire R help
>system were turned into a wiki?   That is,
>
>?whatever
>
>would take you to the help page, but not on your computer --
>rather to the same page on the wiki.  You would then find the
>docs as they exist now plus the experiences of other people
>with that command all at the same place.   You could similarly
>add your own experience to the page.

Perhaps a good example is
http://www.php.net/manual/en/

But this is a lot of work to set up.  I'd rather take small
steps.  I do plan to look into phpbb as an alternative to
bazookaboard*, but not today, and probably not tomorrow.  So if
things proceed without me, so be it.

Jon

*I remember rejecting phpbb once, but I sort of gave myself 30
minutes to install something, not wanting to spend more time than
that.  Bazookaboard met that test, and nothing else did.  But I
could raise the cutoff.

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron
R page:               http://finzi.psych.upenn.edu/



From B.Rowlingson at lancaster.ac.uk  Wed Dec 17 19:03:24 2003
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Wed, 17 Dec 2003 18:03:24 +0000
Subject: [R] mailing list for basic questions - preliminary sum up
In-Reply-To: <20031217175139.GA21774@mail2.sas.upenn.edu>
References: <20031217171916.6629D398C@mprdmxin.myway.com>
	<20031217175139.GA21774@mail2.sas.upenn.edu>
Message-ID: <3FE09A6C.1050403@lancaster.ac.uk>


> But this is a lot of work to set up.  I'd rather take small
> steps.  I do plan to look into phpbb as an alternative to
> bazookaboard*, but not today, and probably not tomorrow.  So if
> things proceed without me, so be it.
> 

  Or set up a server running Zope and Plone, and then you can have 
wikis, boards, news, events - a real R portal in fact - and have a 
single sign-on for all of it. XHTML compliant and all that. Free and 
open-source.

  Bit of a setup and admin overhead though, but the permissions system 
allows admin to be distributed over trusted users.

  www.plone.org for info.

Baz



From allison.100 at osu.edu  Wed Dec 17 19:05:25 2003
From: allison.100 at osu.edu (Gary Allison)
Date: Wed, 17 Dec 2003 13:05:25 -0500
Subject: [R] variance estimates in lme biased?
In-Reply-To: <x2u13z1kos.fsf@biostat.ku.dk>
References: <3FDFB2E9.4070900@osu.edu>
	<Pine.A41.4.58.0312170732500.117868@homer40.u.washington.edu>
	<x2u13z1kos.fsf@biostat.ku.dk>
Message-ID: <3FE09AE5.1060006@osu.edu>


Peter Dalgaard wrote:
[snip]
> 
> Or, try looking at a smaller example where things can be worked out
> explicitly: One-way ANOVA with random btw.group variation. Say 5
> groups and 3 obs per group. If I got this right (please do check!),
> the estimate of the between-group variance is 1/3 times the difference
> between two chi^2/f distributed variables with 4 and 10 DF
> respectively. This will become negative about half the time, and lme
> (and similar code) will set it to zero in that case. Now
> 
> 
>>sd.sim <- sqrt(pmax(1/3*(rchisq(1000,4)/4 - rchisq(1000,10)/10),0))
>>summary(sd.sim)
> 
>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>  0.0000  0.0000  0.0000  0.2017  0.3955  1.2160
> 
> does not seem to be too far from what Gary has been experiencing. 
> 
> Obviously, the fact that the estimator is censored at zero will make
> it biased, but an extended estimator (allowing negative values) is
> unbiased. 
> 
> 
>>var.sim <- 1/3*(rchisq(1000,4)/4 - rchisq(1000,10)/10)
>>summary(var.sim)
> 
>      Min.   1st Qu.    Median      Mean   3rd Qu.      Max.
> -0.796100 -0.196900 -0.042720 -0.007667  0.138500  1.104000
> 
> 
> 
As Peter implies, when I did the simulation in SAS with PROC VARCOMP, 
using the model option 'Type1' which is an extended estimator, estimates 
for level 1 average to 0:
http://david.science.oregonstate.edu/~allisong/R/sas_Type1_5.pdf
and the range is still quite large.

Thanks to the help of Drs. Lumley, Bates and Dalgaard, I'm beginning to 
understand what's going on.  But now it seems that much harder to 
interpret my experimental results -- high variance in both level 1 and 
level 2 at relatively low sample sizes -- given all this.  If variance 
at one level can 'bleed' into another, sometimes to a large degree, is 
this analysis even useful at relatively small sample sizes?

thanks again,
Gary



From rossini at blindglobe.net  Wed Dec 17 19:59:37 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Wed, 17 Dec 2003 10:59:37 -0800
Subject: [R] mailing list for basic questions - preliminary sum up
In-Reply-To: <3FE09A6C.1050403@lancaster.ac.uk> (Barry Rowlingson's message
	of "Wed, 17 Dec 2003 18:03:24 +0000")
References: <20031217171916.6629D398C@mprdmxin.myway.com>
	<20031217175139.GA21774@mail2.sas.upenn.edu>
	<3FE09A6C.1050403@lancaster.ac.uk>
Message-ID: <85vfofnucm.fsf@blindglobe.net>

Barry Rowlingson <B.Rowlingson at lancaster.ac.uk> writes:

>> But this is a lot of work to set up.  I'd rather take small
>> steps.  I do plan to look into phpbb as an alternative to
>> bazookaboard*, but not today, and probably not tomorrow.  So if
>> things proceed without me, so be it.
>>
>
>   Or set up a server running Zope and Plone, and then you can have
>   wikis, boards, news, events - a real R portal in fact - and have a
>   single sign-on for all of it. XHTML compliant and all that. Free and
>   open-source.
>
>   Bit of a setup and admin overhead though, but the permissions system
>   allows admin to be distributed over trusted users.

Only initially.

I've been a fan of Zope (and ZWiki, and Plone) for years.  We are
running a variant in my lab as a prototype, it'll encompass R as well
as the ESS homepage at somepoint soon.

Tres cool...

best,
-tony

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From Whit.Armstrong at tudor.com  Wed Dec 17 20:03:53 2003
From: Whit.Armstrong at tudor.com (Whit Armstrong)
Date: Wed, 17 Dec 2003 14:03:53 -0500
Subject: [R] Accessing row and col names of SEXP objects
Message-ID: <CD9BF92D5B1AD611ABEB00065B386FB0031F6FC2@tudor.com>

Can someone lend me a hand with extracting the dimnames from a SEXP?  I've
looked through R-exts, but I couldn't find an example.

Here is the code I'm using to grab the jth column name and print it, but the
colnames I'm getting are garbage.

None of the following are working.

void printInfo(SEXP ts) {

int j;

for (j=0; j<col; j++) {
  	printf("%s\n",CHAR(STRING_ELT(GetColNames(ts), j)));
	printf("%s\n",CHAR(VECTOR_ELT(GetColNames(ts), j)));
	printf("%s\n",CHARACTER_DATA(STRING_ELT(GetColNames(ts), j)));
	printf("%s\n",CHARACTER_DATA(VECTOR_ELT(GetColNames(ts), j)));
  }

}

Here is the object I'm passing in:
	
	tmp <- matrix(rnorm(100),ncol=5)
	colnames(tmp) <- c("tmp","a","b","c","d")
	rownames(tmp) <- 1:100	
	.Call("printInfo",tmp)

Thanks for your help.

Regards,
Whit



From james.lindsey at luc.ac.be  Wed Dec 17 20:12:48 2003
From: james.lindsey at luc.ac.be (Jim Lindsey)
Date: Wed, 17 Dec 2003 20:12:48 +0100 (MET)
Subject: [R] Jacobian Matrix
In-Reply-To: <1071629357.2887.5.camel@localhost.localdomain> from "Savano S.
	Pereira" at Dec 17, 2003 12:49:17 AM
Message-ID: <200312171912.UAA00663@luc.ac.be>

> 
> Dear useRs,
> 
> I need of jacobian of a tranformation, R have this function?

If you use transform() on a response in my data structures (from my
rmutil library), the Jacobian is calculated automatically and stored
in the data structure. All my modelling functions then use it
automatically.

Available at www.luc.ac.be/~jlindsey/rcode.html

Jim Lindsey

> 
> thanks.
> 
> Savano
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From adi at roda.ro  Wed Dec 17 20:28:05 2003
From: adi at roda.ro (Adrian Dusa)
Date: Wed, 17 Dec 2003 21:28:05 +0200
Subject: [R] beginner programming question
Message-ID: <001001c3c4d3$e82ef180$6901a8c0@roda.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031217/a9cb05e8/attachment.pl

From ripley at stats.ox.ac.uk  Wed Dec 17 20:42:47 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 17 Dec 2003 19:42:47 +0000 (GMT)
Subject: [R] Accessing row and col names of SEXP objects
In-Reply-To: <CD9BF92D5B1AD611ABEB00065B386FB0031F6FC2@tudor.com>
Message-ID: <Pine.LNX.4.44.0312171936060.23005-100000@gannet.stats>

>From the file defining GetColNames:

/* "GetRowNames" and "GetColNames" are utility routines which
 * locate and return the row names and column names from the
 * dimnames attribute of a matrix.  They are useful because

You have not applied them to the dimnames attribute.  Extracting 
dimnames attributes _is_ covered in R-exts:

SEXP dimnames=getAttrib(ts, R_DimNamesSymbol), 
     colnames = VECTOR_ELT(dimnames, 1);

for (j = 0; j < length(colnames); j++) 
   printf("%s\n",CHAR(STRING_ELT(colnames, j)));

looks about right, although you should do something if the colnames or 
dimnames are NULL.

On Wed, 17 Dec 2003, Whit Armstrong wrote:

> Can someone lend me a hand with extracting the dimnames from a SEXP?  I've
> looked through R-exts, but I couldn't find an example.
> 
> Here is the code I'm using to grab the jth column name and print it, but the
> colnames I'm getting are garbage.
> 
> None of the following are working.
> 
> void printInfo(SEXP ts) {
> 
> int j;
> 
> for (j=0; j<col; j++) {
>   	printf("%s\n",CHAR(STRING_ELT(GetColNames(ts), j)));
> 	printf("%s\n",CHAR(VECTOR_ELT(GetColNames(ts), j)));
> 	printf("%s\n",CHARACTER_DATA(STRING_ELT(GetColNames(ts), j)));
> 	printf("%s\n",CHARACTER_DATA(VECTOR_ELT(GetColNames(ts), j)));
>   }
> 
> }
> 
> Here is the object I'm passing in:
> 	
> 	tmp <- matrix(rnorm(100),ncol=5)
> 	colnames(tmp) <- c("tmp","a","b","c","d")
> 	rownames(tmp) <- 1:100	
> 	.Call("printInfo",tmp)
> 
> Thanks for your help.
> 
> Regards,
> Whit
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From niels.waller at vanderbilt.edu  Wed Dec 17 20:48:12 2003
From: niels.waller at vanderbilt.edu (Niels Waller)
Date: Wed, 17 Dec 2003 13:48:12 -0600
Subject: [R] Building packages in XP
Message-ID: <200312171948.hBHJmO9m021007@imap3.mail.vanderbilt.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031217/05a09328/attachment.pl

From sw289 at columbia.edu  Wed Dec 17 20:52:40 2003
From: sw289 at columbia.edu (sw289@columbia.edu)
Date: Wed, 17 Dec 2003 14:52:40 -0500
Subject: [R] step halving factor reduced below minimum
Message-ID: <1071690760.3fe0b4081bbfd@cubmail.cc.columbia.edu>

Dear Splus and R users:
I have a problem in fitting a NLME model, the error message is:

"step halving factor reduced below minimum in PNLS step"

What does it mean and how to fix the problem, could anyone help me 
about it?

Any suggestion/help would be greatly appreciated.

Mei



From ggrothendieck at myway.com  Wed Dec 17 21:02:49 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 17 Dec 2003 15:02:49 -0500 (EST)
Subject: [R] beginner programming question
Message-ID: <20031217200249.206943965@mprdmxin.myway.com>




Define function f to take a vector as input representing
a single input row.   f should (1) transform this to a vector 
representing the required row of output or else (2) produce 
NULL if no row is to be output for that input row.

Then use this code where z is your input matrix:

t( matrix( unlist( apply( z, 1, f ) ), 2) )



---
Date: Wed, 17 Dec 2003 21:28:05 +0200 
From: Adrian Dusa <adi at roda.ro>
To: <r-help at stat.math.ethz.ch> 
Subject: [R] beginner programming question 

 
 
Hi all,



The last e-mails about beginners gave me the courage to post a question;
from a beginner's perspective, there are a lot of questions that I'm
tempted to ask. But I'm trying to find the answers either in the
documentation, either in the about 15 free books I have, either in the
help archives (I often found many similar questions posted in the past).

Being an (still actual) user of SPSS, I'd like to be able to do
everything in R. I've learned that the best way of doing it is to
struggle and find a solution no matter what, refraining from doing it
with SPSS. I've became more and more aware of the almost unlimited
possibilities that R offers and I'd like to completely switch to R
whenever I think I'm ready.



I have a (rather theoretical) programming problem for which I have found
a solution, but I feel it is a rather poor one. I wonder if there's some
other (more clever) solution, using (maybe?) vectorization or
subscripting.



A toy example would be:



rel1 rel2 rel3 age0 age1 age2 age3
sex0 sex1 sex2 sex3

1 3 NA 25 23 2 NA
1 2 1 NA

4 1 3 35 67 34 10
2 2 1 2

1 4 4 39 40 59 60
1 2 2 1

4 NA NA 45 70 NA NA
2 2 NA NA



where rel1...3 states the kinship with the respondent (person 0)

code 1 meaning husband/wife, code 4 meaning parent and code 3 for
children.



I would like to get the age for husbands (code 1) in a first column and
wife's age in the second:



ageh agew

25 23

34 35

39 40



My solution uses *for* loops and *if*s checking for code 1 in each
element in the first 3 columns, then checking in the last three columns
for husband's code, then taking the corresponding age in a new matrix.
I've learned that *for* loops are very slow (and indeed with my dataset
of some 2000 rows and 13 columns for kinship it takes quite a lot).

I found the "Looping" chapter in "S poetry" very useful (it did saved me
from *for* loops a couple of times, thanks!).



Any hints would be appreciated,

Adrian



~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Adrian Dusa (adi at roda.ro)
Romanian Social Data Archive (www.roda.ro <http://www.roda.ro/>; )
1, Schitu Magureanu Bd.
76625 Bucharest sector 5
Romania



From ray at mcs.vuw.ac.nz  Wed Dec 17 22:04:52 2003
From: ray at mcs.vuw.ac.nz (Ray Brownrigg)
Date: Thu, 18 Dec 2003 10:04:52 +1300 (NZDT)
Subject: [R] beginner programming question
Message-ID: <200312172104.hBHL4qIQ017417@tahi.mcs.vuw.ac.nz>

> From: "Gabor Grothendieck" <ggrothendieck at myway.com>
> Date: Wed, 17 Dec 2003 15:02:49 -0500 (EST)
> 
> Define function f to take a vector as input representing
> a single input row.   f should (1) transform this to a vector 
> representing the required row of output or else (2) produce 
> NULL if no row is to be output for that input row.
> 
> Then use this code where z is your input matrix:
> 
> t( matrix( unlist( apply( z, 1, f ) ), 2) )
> 
But as has been pointed out recently, apply really is still just a for
loop.

> > From: Adrian Dusa <adi at roda.ro>
> > Date: Wed, 17 Dec 2003 21:28:05 +0200 
> > 
> > I have a (rather theoretical) programming problem for which I have found
> > a solution, but I feel it is a rather poor one. I wonder if there's some
> > other (more clever) solution, using (maybe?) vectorization or
> > subscripting.

Here is a subscripting solution, where (for consistency with above) z is
your data [from read.table(filename, header=T)]:

> z
  rel1 rel2 rel3 age0 age1 age2 age3 sex0 sex1 sex2 sex3
1    1    3   NA   25   23    2   NA    1    2    1   NA
2    4    1    3   35   67   34   10    2    2    1    2
3    1    4    4   39   40   59   60    1    2    2    1
4    4   NA   NA   45   70   NA   NA    2    2   NA   NA
> res <- matrix(NA, nrow=length(z[, 1]), ncol=2,
         dimnames=list(rownames=rownames(z), colnames=c("ageh", "agew")))
> w <- w0 <- w1 <- w2 <- which(z[, c("rel1", "rel2", "rel3")] == 1, T)
					# find spouse entries
> w0[, 2] <- z[, "sex0"][w[, 1]]	# indices for respondent's age
> w1[, 2] <- 3 - w0[, 2]		# indices for spouse's age
> w2[, 2] <- 4 + w[, 2]			# indices of spouse's age
> res[w0] <- z[, "age0"][w[, 1]]	# set respondent's age
> res[w1] <- z[w2]			# set spouse's age
> res
        colnames
rownames ageh agew
       1   25   23
       2   34   35
       3   39   40
       4   NA   NA
>
Ray Brownrigg



From bates at stat.wisc.edu  Wed Dec 17 23:01:20 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: 17 Dec 2003 16:01:20 -0600
Subject: [R] step halving factor reduced below minimum
In-Reply-To: <1071690760.3fe0b4081bbfd@cubmail.cc.columbia.edu>
References: <1071690760.3fe0b4081bbfd@cubmail.cc.columbia.edu>
Message-ID: <6rpten3xzj.fsf@bates4.stat.wisc.edu>

sw289 at columbia.edu writes:

> Dear Splus and R users:
> I have a problem in fitting a NLME model, the error message is:
> 
> "step halving factor reduced below minimum in PNLS step"
> 
> What does it mean and how to fix the problem, could anyone help me 
> about it?
> 
> Any suggestion/help would be greatly appreciated.

Turn on the verbose option (i.e. add verbose = TRUE to the nlme call)
to see what is happening to the parameter estimates during the
iterations.  That should give you a clue as to why the estimation
algorithm is not converging.



From ggrothendieck at myway.com  Wed Dec 17 23:03:32 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 17 Dec 2003 17:03:32 -0500 (EST)
Subject: [R] beginner programming question
Message-ID: <20031217220332.D10873984@mprdmxin.myway.com>



This is just a response to the part where you refer to an apply
loop really being a for loop.  In a sense this true, but
it should nevertheless be recognized that the apply solution
has a number of advantages over for:

- it nicely separates the problem into a single line that is 
independent of the details of the problem and localizes them 
in f

- the rows are pasted together automatically avoiding messy
appending or creation and filling in of a structure

- it avoids the use of indices

Of course, some apply loops come pretty close to for loops.  For
example, consider this variation:

  t( matrix( unlist (sapply( 1:nrow(z), function(i) f(z[i,]) ) ), 2 ))

and compare it to the for loop:

 out <- NULL
 for ( i in 1:nrow(z) ) {
   v <- f( z[i,] )
   if ( ! is.null(v) ) out <- rbind( out, v )
}

but even this apply, which is clearly inferior to the one in my
original posting, retains the first two advantages listed.

---

Date: Thu, 18 Dec 2003 10:04:52 +1300 (NZDT) 
From: Ray Brownrigg <ray at mcs.vuw.ac.nz>
To: <adi at roda.ro>, <ggrothendieck at myway.com>, <r-help at stat.math.ethz.ch> 
Subject: RE: [R] beginner programming question 

 
 
> From: "Gabor Grothendieck" <ggrothendieck at myway.com>
> Date: Wed, 17 Dec 2003 15:02:49 -0500 (EST)
> 
> Define function f to take a vector as input representing
> a single input row. f should (1) transform this to a vector 
> representing the required row of output or else (2) produce 
> NULL if no row is to be output for that input row.
> 
> Then use this code where z is your input matrix:
> 
> t( matrix( unlist( apply( z, 1, f ) ), 2) )
> 
But as has been pointed out recently, apply really is still just a for
loop.

> > From: Adrian Dusa <adi at roda.ro>
> > Date: Wed, 17 Dec 2003 21:28:05 +0200 
> > 
> > I have a (rather theoretical) programming problem for which I have found
> > a solution, but I feel it is a rather poor one. I wonder if there's some
> > other (more clever) solution, using (maybe?) vectorization or
> > subscripting.

Here is a subscripting solution, where (for consistency with above) z is
your data [from read.table(filename, header=T)]:

> z
rel1 rel2 rel3 age0 age1 age2 age3 sex0 sex1 sex2 sex3
1 1 3 NA 25 23 2 NA 1 2 1 NA
2 4 1 3 35 67 34 10 2 2 1 2
3 1 4 4 39 40 59 60 1 2 2 1
4 4 NA NA 45 70 NA NA 2 2 NA NA
> res <- matrix(NA, nrow=length(z[, 1]), ncol=2,
dimnames=list(rownames=rownames(z), colnames=c("ageh", "agew")))
> w <- w0 <- w1 <- w2 <- which(z[, c("rel1", "rel2", "rel3")] == 1, T)
                         # find spouse entries
> w0[, 2] <- z[, "sex0"][w[, 1]]     # indices for respondent's age
> w1[, 2] <- 3 - w0[, 2]          # indices for spouse's age
> w2[, 2] <- 4 + w[, 2]               # indices of spouse's age
> res[w0] <- z[, "age0"][w[, 1]]     # set respondent's age
> res[w1] <- z[w2]               # set spouse's age
> res
colnames
rownames ageh agew
1 25 23
2 34 35
3 39 40
4 NA NA
>
Ray Brownrigg



From kjetil at entelnet.bo  Wed Dec 17 23:20:26 2003
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Wed, 17 Dec 2003 18:20:26 -0400
Subject: [R] mailing list for basic questions - preliminary sum up
In-Reply-To: <20031217175139.GA21774@mail2.sas.upenn.edu>
References: <20031217171916.6629D398C@mprdmxin.myway.com>
Message-ID: <3FE09E6A.17280.6D16B2@localhost>

On 17 Dec 2003 at 12:51, Jonathan Baron wrote:

> On 12/17/03 12:19, Gabor Grothendieck wrote:
> >
> >
> >In rereading this one idea occurred to me.  What if the entire R help
> >system were turned into a wiki?   That is,
> >
> >?whatever
> >
> >would take you to the help page, but not on your computer --
> >rather to the same page on the wiki.  You would then find the
> >docs as they exist now plus the experiences of other people

Yes, this might be nice, but pls remember that many people in many 
countries still use machines without web connection, or worse, pay 
modem time by minute.  Or use laptops on planes.

But it could be nice to be able to write
wiki("lm")
as an alternative to 
help("lm")
and maybe the possibility to use options("help") to associate ?lm
with whichever one likes.

Kjetil Halvorsen

> >with that command all at the same place.   You could similarly
> >add your own experience to the page.
> 
> Perhaps a good example is
> http://www.php.net/manual/en/
> 
> But this is a lot of work to set up.  I'd rather take small
> steps.  I do plan to look into phpbb as an alternative to
> bazookaboard*, but not today, and probably not tomorrow.  So if
> things proceed without me, so be it.
> 
> Jon
> 
> *I remember rejecting phpbb once, but I sort of gave myself 30
> minutes to install something, not wanting to spend more time than
> that.  Bazookaboard met that test, and nothing else did.  But I could
> raise the cutoff.
> 
> -- 
> Jonathan Baron, Professor of Psychology, University of Pennsylvania
> Home page:            http://www.sas.upenn.edu/~baron R page:         
>      http://finzi.psych.upenn.edu/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From tplate at blackmesacapital.com  Wed Dec 17 23:40:52 2003
From: tplate at blackmesacapital.com (Tony Plate)
Date: Wed, 17 Dec 2003 15:40:52 -0700
Subject: [R] beginner programming question
In-Reply-To: <001001c3c4d3$e82ef180$6901a8c0@roda.local>
Message-ID: <5.2.1.1.2.20031217152631.041e0e38@mailhost.blackmesacapital.com>

Another way to approach this is to first massage the data into a more 
regular format.  This may or may not be simpler or faster than other 
solutions suggested.

 > x <- read.table("clipboard", header=T)
 > x
   rel1 rel2 rel3 age0 age1 age2 age3 sex0 sex1 sex2 sex3
1    1    3   NA   25   23    2   NA    1    2    1   NA
2    4    1    3   35   67   34   10    2    2    1    2
3    1    4    4   39   40   59   60    1    2    2    1
4    4   NA   NA   45   70   NA   NA    2    2   NA   NA
 > nn <- c("rel","age0","age","sex0","sex")
 > xx <- rbind("colnames<-"(x[,c("rel1","age0","age1","sex0","sex1")], nn),
+  "colnames<-"(x[,c("rel2","age0","age2","sex0","sex2")], nn),
+  "colnames<-"(x[,c("rel3","age0","age3","sex0","sex3")], nn))
 > xx
    rel age0 age sex0 sex
1    1   25  23    1   2
2    4   35  67    2   2
3    1   39  40    1   2
4    4   45  70    2   2
11   3   25   2    1   1
21   1   35  34    2   1
31   4   39  59    1   2
41  NA   45  NA    2  NA
12  NA   25  NA    1  NA
22   3   35  10    2   2
32   4   39  60    1   1
42  NA   45  NA    2  NA
 >
 > rbind(subset(xx, xx$rel==1 & (xx$sex0==1 | 
xx$sex0==xx$sex))[,c("age0","age")], subset(xx, xx$rel==1 & xx$sex==1 & 
xx$sex0!=xx$sex)[,c("age","age0")])
    age0 age
1    25  23
3    39  40
21   35  34
 >

hope this helps,

Tony Plate

PS.  To advanced R users: Is the above usage of the "colnames<-" function 
within an expression regarded as acceptable or as undesirable programming 
style? -- I've rarely seen it used, but it can be quite useful.

At Wednesday 09:28 PM 12/17/2003 +0200, Adrian Dusa wrote:
>Hi all,
>
>
>
>The last e-mails about beginners gave me the courage to post a question;
>from a beginner's perspective, there are a lot of questions that I'm
>tempted to ask. But I'm trying to find the answers either in the
>documentation, either in the about 15 free books I have, either in the
>help archives (I often found many similar questions posted in the past).
>
>Being an (still actual) user of SPSS, I'd like to be able to do
>everything in R. I've learned that the best way of doing it is to
>struggle and find a solution no matter what, refraining from doing it
>with SPSS. I've became more and more aware of the almost unlimited
>possibilities that R offers and I'd like to completely switch to R
>whenever I think I'm ready.
>
>
>
>I have a (rather theoretical) programming problem for which I have found
>a solution, but I feel it is a rather poor one. I wonder if there's some
>other (more clever) solution, using (maybe?) vectorization or
>subscripting.
>
>
>
>A toy example would be:
>
>
>
>rel1       rel2       rel3       age0     age1     age2     age3
>sex0     sex1     sex2     sex3
>
>1          3          NA        25         23         2          NA
>1          2          1          NA
>
>4          1          3          35         67         34         10
>2          2          1          2
>
>1          4          4          39         40         59         60
>1          2          2          1
>
>4          NA        NA        45         70         NA        NA
>2          2          NA        NA
>
>
>
>where rel1...3 states the kinship with the respondent (person 0)
>
>code 1 meaning husband/wife, code 4 meaning parent and code 3 for
>children.
>
>
>
>I would like to get the age for husbands (code 1) in a first column and
>wife's age in the second:
>
>
>
>ageh     agew
>
>25         23
>
>34         35
>
>39         40
>
>
>
>My solution uses *for* loops and *if*s checking for code 1 in each
>element in the first 3 columns, then checking in the last three columns
>for husband's code, then taking the corresponding age in a new matrix.
>I've learned that *for* loops are very slow (and indeed with my dataset
>of some 2000 rows and 13 columns for kinship it takes quite a lot).
>
>I found the "Looping" chapter in "S poetry" very useful (it did saved me
>from *for* loops a couple of times, thanks!).
>
>
>
>Any hints would be appreciated,
>
>Adrian
>
>
>
>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>Adrian Dusa (adi at roda.ro)
>Romanian Social Data Archive (www.roda.ro <http://www.roda.ro/> )
>1, Schitu Magureanu Bd.
>76625 Bucharest sector 5
>Romania
>
>
>Tel./Fax:
>
>+40 (21) 312.66.18\
>
>+40 (21) 312.02.10/ int.101
>
>
>
>
>         [[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ggrothendieck at myway.com  Thu Dec 18 00:09:02 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 17 Dec 2003 18:09:02 -0500 (EST)
Subject: [R] mailing list for basic questions - preliminary sum up
Message-ID: <20031217230902.E55813953@mprdmxin.myway.com>



A two level solution might be possible as part of this too.
If you got ?whatever off your disk then it could contain a
link to the corresponding wiki page.  If you didn't have
a connection you would still get what you get now but just
couldn't follow the link.  Whenever a new version of R came
out the wiki would be replicated back to the help screens
so that those with no internet still get some wiki info
albeit as a snapshot as of the last release.

Your idea of an option setting could work along with the above
so that it could be set to go directly to the wiki if you 
had a connection and preferred that.

---
Date: Wed, 17 Dec 2003 18:20:26 -0400 
From: <kjetil at entelnet.bo>
To: Gabor Grothendieck <ggrothendieck at myway.com>,Jonathan Baron <baron at psych.upenn.edu> 
Cc: <r-help at stat.math.ethz.ch>, <feh3k at spamcop.net> 
Subject: Re: [R] mailing list for basic questions - preliminary sum up 

 
 
On 17 Dec 2003 at 12:51, Jonathan Baron wrote:

> On 12/17/03 12:19, Gabor Grothendieck wrote:
> >
> >
> >In rereading this one idea occurred to me. What if the entire R help
> >system were turned into a wiki? That is,
> >
> >?whatever
> >
> >would take you to the help page, but not on your computer --
> >rather to the same page on the wiki. You would then find the
> >docs as they exist now plus the experiences of other people

Yes, this might be nice, but pls remember that many people in many 
countries still use machines without web connection, or worse, pay 
modem time by minute. Or use laptops on planes.

But it could be nice to be able to write
wiki("lm")
as an alternative to 
help("lm")
and maybe the possibility to use options("help") to associate ?lm
with whichever one likes.

Kjetil Halvorsen

> >with that command all at the same place. You could similarly
> >add your own experience to the page.
> 
> Perhaps a good example is
> http://www.php.net/manual/en/
> 
> But this is a lot of work to set up. I'd rather take small
> steps. I do plan to look into phpbb as an alternative to
> bazookaboard*, but not today, and probably not tomorrow. So if
> things proceed without me, so be it.
> 
> Jon
> 
> *I remember rejecting phpbb once, but I sort of gave myself 30
> minutes to install something, not wanting to spend more time than
> that. Bazookaboard met that test, and nothing else did. But I could
> raise the cutoff.
> 
> -- 
> Jonathan Baron, Professor of Psychology, University of Pennsylvania
> Home page: http://www.sas.upenn.edu/~baron R page: 
> http://finzi.psych.upenn.edu/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From p.dalgaard at biostat.ku.dk  Thu Dec 18 00:23:26 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 Dec 2003 00:23:26 +0100
Subject: [R] beginner programming question
In-Reply-To: <5.2.1.1.2.20031217152631.041e0e38@mailhost.blackmesacapital.com>
References: <5.2.1.1.2.20031217152631.041e0e38@mailhost.blackmesacapital.com>
Message-ID: <x2iskf2fm9.fsf@biostat.ku.dk>

Tony Plate <tplate at blackmesacapital.com> writes:

>  > xx <- rbind("colnames<-"(x[,c("rel1","age0","age1","sex0","sex1")], nn),
> +  "colnames<-"(x[,c("rel2","age0","age2","sex0","sex2")], nn),
> +  "colnames<-"(x[,c("rel3","age0","age3","sex0","sex3")], nn))
....
> PS.  To advanced R users: Is the above usage of the "colnames<-"
> function within an expression regarded as acceptable or as undesirable
> programming style? -- I've rarely seen it used, but it can be quite
> useful.

I wouldn't be happy with it. These assignment functions can do things
that really only makes sense when used in the context of foo(x) <-
bar. It is true that if you define "foo<-" as an ordinary R function
of x and bar that returns the modified x, then foo(x)<-bar will work,
but the converse might not be true. The programmer may have done
things for the sake of efficiency that makes "foo<-" behave in
non-standard ways. In particular it might destructively modify its
x argument.

In the above case, the modified argument is a temporary, so it is
likely to be safe, but as a programming paradigm it might spring some
nasty surprises in the face of the unsuspecting user. So I'd prefer
something like

xx <- do.call("rbind",
         lapply(list(x[,c("rel1","age0","age1","sex0","sex1")], 
                     x[,c("rel2","age0","age2","sex0","sex2")],
                     x[,c("rel3","age0","age3","sex0","sex3")]), 
                function(x) {colnames(x) <- nn; x})

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From tplate at blackmesacapital.com  Thu Dec 18 00:42:25 2003
From: tplate at blackmesacapital.com (Tony Plate)
Date: Wed, 17 Dec 2003 16:42:25 -0700
Subject: [R] beginner programming question
In-Reply-To: <x2iskf2fm9.fsf@biostat.ku.dk>
References: <5.2.1.1.2.20031217152631.041e0e38@mailhost.blackmesacapital.com>
	<5.2.1.1.2.20031217152631.041e0e38@mailhost.blackmesacapital.com>
Message-ID: <5.2.1.1.2.20031217163955.0429bbe0@mailhost.blackmesacapital.com>

Thanks.  As a follow-up question, is it considered acceptable programming 
practice for "<-" functions to modify their x argument?

-- Tony Plate

At Thursday 12:23 AM 12/18/2003 +0100, Peter Dalgaard wrote:
>Tony Plate <tplate at blackmesacapital.com> writes:
>
> >  > xx <- rbind("colnames<-"(x[,c("rel1","age0","age1","sex0","sex1")], nn),
> > +  "colnames<-"(x[,c("rel2","age0","age2","sex0","sex2")], nn),
> > +  "colnames<-"(x[,c("rel3","age0","age3","sex0","sex3")], nn))
>....
> > PS.  To advanced R users: Is the above usage of the "colnames<-"
> > function within an expression regarded as acceptable or as undesirable
> > programming style? -- I've rarely seen it used, but it can be quite
> > useful.
>
>I wouldn't be happy with it. These assignment functions can do things
>that really only makes sense when used in the context of foo(x) <-
>bar. It is true that if you define "foo<-" as an ordinary R function
>of x and bar that returns the modified x, then foo(x)<-bar will work,
>but the converse might not be true. The programmer may have done
>things for the sake of efficiency that makes "foo<-" behave in
>non-standard ways. In particular it might destructively modify its
>x argument.
>
>In the above case, the modified argument is a temporary, so it is
>likely to be safe, but as a programming paradigm it might spring some
>nasty surprises in the face of the unsuspecting user. So I'd prefer
>something like
>
>xx <- do.call("rbind",
>          lapply(list(x[,c("rel1","age0","age1","sex0","sex1")],
>                      x[,c("rel2","age0","age2","sex0","sex2")],
>                      x[,c("rel3","age0","age3","sex0","sex3")]),
>                 function(x) {colnames(x) <- nn; x})
>
>--
>    O__  ---- Peter Dalgaard             Blegdamsvej 3
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
>~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ligges at statistik.uni-dortmund.de  Thu Dec 18 00:49:51 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 18 Dec 2003 00:49:51 +0100
Subject: [R] Building packages in XP
References: <200312171948.hBHJmO9m021007@imap3.mail.vanderbilt.edu>
Message-ID: <3FE0EB9F.B86D51EF@statistik.uni-dortmund.de>



Niels Waller wrote:
> 
> R users,
> 
> I recently upgraded (?) to Windows XP from 2000.  I am trying to build an R
> package.  I have done this many times on my old system and I am not sure why
> it is not working in XP.
> 
> To build the package I call a bat file that specifies all the necessary
> paths -- but the "build" file (which appears to be a perl script) is looking
> for a "src" subdirectory in the src directory? [which, of course, does not
> exist].
> 
> I am using rw1081 with the latest ActiveState Perl and tools (from Ripley)
> installed
> 
> Any help would be greatly appreciated
> 
> Thanks,
> 
> Niels Waller
> niels.waller at vanderbilt.edu
> 
> *******  my bat file **********
> 
> set TMPDIR=h:\TEMP
> set path=d:\Latex\texmf\miktex\bin;h:\bin;h:\R\rw1081\bin;g:\Perl\bin;
> R\rw1081\bin;

- What kind of path is the latter (without any drive letter)?
- In some circumstances you need "." in your path (maybe not for
compiling packages - I'm too lazy to read the docs right now).


> rcmd build --binary --use-zip taxon
> 
> **** end of my bat file *****
> 
> ***my error messages ******
> 
> H:\R\rw1081\src\gnuwin32>rcmd build --binary --use-zip taxon
> * checking for file 'taxon/DESCRIPTION' ... OK
> 
> make: *** H:/R/rw1081/src/src/gnuwin32: No such file or directory.  Stop.
> *** Installation of taxon failed ***
> 
> * building 'taxon_1.5.zip'
>         zip warning: name not matched: taxon
> 
> zip error: Nothing to do! (try: zip -r9X
> H:/R/rw1081/src/gnuwin32/taxon_1.5.zip
> . -i taxon)
> 
> H:\R\rw1081\src\gnuwin32>
> 
>         [[alternative HTML version deleted]]


It should be the same as on any other Windows machine, hence the problem
is NOT your version of Windows. Some comments:
- I think it's strange to build packages within the R source tree. Try
to avoid that at first!
- Is that version of R self-compiled or the binary from CRAN?
- Do you have modified any files within the R sources? 
- Do you use a Makefile in taxon?

Uwe Ligges



From tmulholl at bigpond.net.au  Thu Dec 18 01:15:16 2003
From: tmulholl at bigpond.net.au (Tom Mulholland)
Date: Thu, 18 Dec 2003 08:15:16 +0800
Subject: [R] mailing list for basic questions - preliminary sum up
In-Reply-To: <20031217230902.E55813953@mprdmxin.myway.com>
Message-ID: <000001c3c4fc$038004b0$2202a8c0@ACER>

I have empathy for lots of the points already made, more often on the life
is not always easy and you have to work at it flavour because that's where
you make the real gains.

One particular message early in the piece cited an example of what a good
request might look like. Other lists sometime send out regular messages
(although they tend to be about the rules of the list) that are intended to
make sure that important pieces of information are regularly repeated.

I know that there is more than enough talent on this list to put together
suggestions for getting quick responses that could be sent out regularly.
The sorts of things that might be in it would be when you should attach
details of operating system, version etc. (or if they should always be
there) as well as comments like those by Spencer Graves and it could include
the checklist that someone mentioned (I think that was Frank Harrell). It
would almost be a pro-forma for messages and while people don't have to use
it, it may help those who do think before they post (we'll never stop some
people, because that's just the way they are)

Tom Mulholland
Tom Mulholland Associates

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Gabor Grothendieck
Sent: Thursday, December 18, 2003 7:09 AM
To: kjetil at entelnet.bo; ggrothendieck at myway.com; baron at psych.upenn.edu
Cc: r-help at stat.math.ethz.ch; feh3k at spamcop.net
Subject: Re: [R] mailing list for basic questions - preliminary sum up




A two level solution might be possible as part of this too.
If you got ?whatever off your disk then it could contain a
link to the corresponding wiki page.  If you didn't have
a connection you would still get what you get now but just
couldn't follow the link.  Whenever a new version of R came
out the wiki would be replicated back to the help screens
so that those with no internet still get some wiki info
albeit as a snapshot as of the last release.

Your idea of an option setting could work along with the above
so that it could be set to go directly to the wiki if you
had a connection and preferred that.

---
Date: Wed, 17 Dec 2003 18:20:26 -0400
From: <kjetil at entelnet.bo>
To: Gabor Grothendieck <ggrothendieck at myway.com>,Jonathan Baron
<baron at psych.upenn.edu>
Cc: <r-help at stat.math.ethz.ch>, <feh3k at spamcop.net>
Subject: Re: [R] mailing list for basic questions - preliminary sum up



On 17 Dec 2003 at 12:51, Jonathan Baron wrote:

> On 12/17/03 12:19, Gabor Grothendieck wrote:
> >
> >
> >In rereading this one idea occurred to me. What if the entire R help
> >system were turned into a wiki? That is,
> >
> >?whatever
> >
> >would take you to the help page, but not on your computer --
> >rather to the same page on the wiki. You would then find the
> >docs as they exist now plus the experiences of other people

Yes, this might be nice, but pls remember that many people in many
countries still use machines without web connection, or worse, pay
modem time by minute. Or use laptops on planes.

But it could be nice to be able to write
wiki("lm")
as an alternative to
help("lm")
and maybe the possibility to use options("help") to associate ?lm
with whichever one likes.

Kjetil Halvorsen

> >with that command all at the same place. You could similarly
> >add your own experience to the page.
>
> Perhaps a good example is
> http://www.php.net/manual/en/
>
> But this is a lot of work to set up. I'd rather take small
> steps. I do plan to look into phpbb as an alternative to
> bazookaboard*, but not today, and probably not tomorrow. So if
> things proceed without me, so be it.
>
> Jon
>
> *I remember rejecting phpbb once, but I sort of gave myself 30
> minutes to install something, not wanting to spend more time than
> that. Bazookaboard met that test, and nothing else did. But I could
> raise the cutoff.
>
> --
> Jonathan Baron, Professor of Psychology, University of Pennsylvania
> Home page: http://www.sas.upenn.edu/~baron R page:
> http://finzi.psych.upenn.edu/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help

---
Incoming mail is certified Virus Free.



---



From andy_liaw at merck.com  Thu Dec 18 01:26:13 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 17 Dec 2003 19:26:13 -0500
Subject: [R] mailing list for basic questions - preliminary sum up
Message-ID: <3A822319EB35174CA3714066D590DCD50205CF6E@usrymx25.merck.com>

> From: Tom Mulholland
> 
> I have empathy for lots of the points already made, more 
> often on the life
> is not always easy and you have to work at it flavour because 
> that's where
> you make the real gains.
> 
> One particular message early in the piece cited an example of 
> what a good
> request might look like. Other lists sometime send out 
> regular messages
> (although they tend to be about the rules of the list) that 
> are intended to
> make sure that important pieces of information are regularly repeated.
> 
> I know that there is more than enough talent on this list to 
> put together
> suggestions for getting quick responses that could be sent 
> out regularly.
> The sorts of things that might be in it would be when you 
> should attach
> details of operating system, version etc. (or if they should always be
> there) as well as comments like those by Spencer Graves and 
> it could include
> the checklist that someone mentioned (I think that was Frank 
> Harrell). It
> would almost be a pro-forma for messages and while people 
> don't have to use
> it, it may help those who do think before they post (we'll 
> never stop some
> people, because that's just the way they are)
> 
> Tom Mulholland
> Tom Mulholland Associates

Please see Eric Raymond's "How To Ask Questions The Smart Way"
(http://www.catb.org/~esr/faqs/smart-questions.html).

(One of these days I shall take up Martin's suggestion and write an entry
for R-FAQ pointing to it.  The problem is getting people to actually read
the FAQ, let alone links in the FAQ...)

Best,
Andy



------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From savano at superig.com.br  Thu Dec 18 01:41:00 2003
From: savano at superig.com.br (Savano S. Pereira)
Date: 17 Dec 2003 22:41:00 -0200
Subject: [R] NUMERIC DERIVATE
Message-ID: <1071708060.2639.13.camel@localhost.localdomain>

UseRs,

I used the optim function

valor.optim <- optim(c(1,1,1),logexp1,method
="BFGS",control=list(fnscale=-1),hessian=T);

and I want to calculate the derivates,

    psi1<-valor.optim$par[1]
    psi2<-valor.optim$par[2]
    psi3<-valor.optim$par[3]
    
    a0=exp(psi1);
    a1=exp(psi2)/(20+exp(psi2)+exp(psi3));
    a2=exp(psi3)/(20+exp(psi2)+exp(psi3))
       
    deriv.psi1<-numericDeriv(a0,c("psi1","psi2","psi3"));
    deriv.psi2<-numericDeriv(a1,c("psi1","psi2","psi3"));
    deriv.psi3<-numericDeriv(a2,c("psi1","psi2","psi3"));

but I found,

> deriv.psi1<-numericDeriv(a0,c("psi1","psi2","psi3"));
> deriv.psi1
[1] 0.038384
attr(,"gradient")
     [,1] [,2] [,3]
[1,]    0    0    0


> deriv.psi2<-numericDeriv(a1,c("psi1","psi2","psi3"));
> deriv.psi2
[1] 0.05754
attr(,"gradient")
     [,1] [,2] [,3]
[1,]    0    0    0


deriv.psi3<-numericDeriv(a2,c("psi1","psi2","psi3"));
>  deriv.psi3
[1] 0.93315
attr(,"gradient")
     [,1] [,2] [,3]
[1,]    0    0    0


The derivates are zero. Why?



From tplate at acm.org  Thu Dec 18 02:04:34 2003
From: tplate at acm.org (Tony Plate)
Date: Wed, 17 Dec 2003 18:04:34 -0700
Subject: [R] mailing list for basic questions - preliminary sum up
In-Reply-To: <3A822319EB35174CA3714066D590DCD50205CF6E@usrymx25.merck.co
 m>
Message-ID: <5.2.1.1.2.20031217174446.042a1ea0@mailhost.blackmesacapital.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031217/d470bfa9/attachment.pl

From tblackw at umich.edu  Thu Dec 18 02:33:08 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Wed, 17 Dec 2003 20:33:08 -0500 (EST)
Subject: [R] NUMERIC DERIVATE
In-Reply-To: <1071708060.2639.13.camel@localhost.localdomain>
References: <1071708060.2639.13.camel@localhost.localdomain>
Message-ID: <Pine.SOL.4.58.0312172030440.18880@millipede.gpcc.itd.umich.edu>

On Wed, 17 Dec 2003, Savano S. Pereira wrote:

> UseRs,
>
> I used the optim function
>
> valor.optim <- optim(c(1,1,1),logexp1,method
> ="BFGS",control=list(fnscale=-1),hessian=T);
>
> and I want to calculate the derivates,  [ ... snip ... ]
>
> but I found,  [ ... snip ... ]
>
> The derivates are zero. Why?

Do you suppose it could have something to do with the
condition for finding an optimum ?

-  tom blackwell  -  u michigan medical school  -  ann arbor  -



From Tom.Mulholland at health.wa.gov.au  Thu Dec 18 03:25:57 2003
From: Tom.Mulholland at health.wa.gov.au (Mulholland, Tom)
Date: Thu, 18 Dec 2003 10:25:57 +0800
Subject: [R] mailing list for basic questions - preliminary sum up
Message-ID: <74E242B6968AA0469B632C5A3EFC1EFD03D5701F@nt207mesep.corporate.hdwa.health.wa.gov.au>

The fact that I've been using R for quite a while now and did not know
about this document is supporting evidence of the need to get this sort
of information out there. 

However that big list is going to daunt some people, it would have
daunted me at the beginning. At a time when you are digesting a whole
new universe (wonderful though it is) some short and pithy help is
welcome. I expect that there would be a consensus on which topics are
essential to improve the quality of questions. It is this select group
of comments that could be put in a standard email sent out once a month.
I would assume that the link to Eric Raymond's "How To Ask Questions The
Smart Way"1 would be part of the advice. That is if you're going to
spoon feed, then spoon feed the advice that helps the list most and
encourage new list members to take the time to read the longer document.


I think this special treatment is warranted because the issue is not
about R per-se, it is about this list, so it makes more sense to have it
coming out of this list rather than an entry in the R-FAQ. Although I
can't see a reason for not doing both.

Hmmm. Before I post this I had better go and see what Eric has to say
about this sort of message. 

Ciao, Tom

1 An assumption on my part is that there is fundamental agreement that
the document is the best source for advice on how to ask questions of
this list
_________________________________________________
 
Tom Mulholland
Senior Policy Officer
WA Country Health Service
Tel: (08) 9222 4062
 
The contents of this e-mail transmission are confidential and may be
protected by professional privilege. The contents are intended only for
the named recipients of this e-mail. If you are not the intended
recipient, you are hereby notified that any use, reproduction,
disclosure or distribution of the information contained in this e-mail
is prohibited. Please notify the sender immediately.


-----Original Message-----
From: Liaw, Andy [mailto:andy_liaw at merck.com] 
Sent: Thursday, 18 December 2003 8:26 AM
To: 'Tom Mulholland'
Cc: r-help at stat.math.ethz.ch
Subject: RE: [R] mailing list for basic questions - preliminary sum up


> From: Tom Mulholland
> 
> I have empathy for lots of the points already made, more
> often on the life
> is not always easy and you have to work at it flavour because 
> that's where
> you make the real gains.
> 
> One particular message early in the piece cited an example of
> what a good
> request might look like. Other lists sometime send out 
> regular messages
> (although they tend to be about the rules of the list) that 
> are intended to
> make sure that important pieces of information are regularly repeated.
> 
> I know that there is more than enough talent on this list to
> put together
> suggestions for getting quick responses that could be sent 
> out regularly.
> The sorts of things that might be in it would be when you 
> should attach
> details of operating system, version etc. (or if they should always be
> there) as well as comments like those by Spencer Graves and 
> it could include
> the checklist that someone mentioned (I think that was Frank 
> Harrell). It
> would almost be a pro-forma for messages and while people 
> don't have to use
> it, it may help those who do think before they post (we'll 
> never stop some
> people, because that's just the way they are)
> 
> Tom Mulholland
> Tom Mulholland Associates

Please see Eric Raymond's "How To Ask Questions The Smart Way"
(http://www.catb.org/~esr/faqs/smart-questions.html).

(One of these days I shall take up Martin's suggestion and write an
entry for R-FAQ pointing to it.  The problem is getting people to
actually read the FAQ, let alone links in the FAQ...)

Best,
Andy



------------------------------------------------------------------------
------
Notice:  This e-mail message, together with any
attachments,...{{dropped}}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From bbvaughn at bellsouth.net  Thu Dec 18 06:24:52 2003
From: bbvaughn at bellsouth.net (Brandon Vaughn)
Date: Wed, 17 Dec 2003 23:24:52 -0600
Subject: [R] Resampling Stats software
In-Reply-To: <3FDE9474.3020609@indigoindustrial.co.nz>
Message-ID: <20031218052459.MBPC13947.imf24aec.mail.bellsouth.net@brandoncd66bez>

Thanks to everyone who wrote in with suggestions.  I will check out the
books mentioned.

The book I mentioned "Resampling: The New Statistics" is actually available
free online at:

http://www.resample.com/content/text/index.shtml

It seems pretty good as an introduction.  But then again, I am new at this
concept.

Does anyone know right off hand how to do simple simulation with R?  Like
for instance, in the book mentioned above, there is an example of figuring
out the probability that a company with 20 trucks with have 4 or more fail
on a given day (the probability that any given truck fails is .10).  So the
way they do it is to simulate uniform numbers from 1 to 10, and let the
number 1 represent a defective truck.  So here is the setup in the program
Resampling Stat:

	REPEAT 400   [repeat simulation 400 times]
	GENERATE 20 1,10 a  	[generate 20 numbers between 1 and 10; store
in vector a]
	COUNT a = 1 b	[count the number of 1's and store in vector b]
	SCORE b z	[keep track of each trial in vector z]
	END	[repeat process]
	COUNT z > 3 k	[count the number of times trials more than 3 and
store]
	DIVIDE k 400 kk	[convert to probability and store]
	PRINT kk	[print result]

This seems like a simple problem, and seemingly simple process in Resampling
Stats.  Any idea on how to get started doing this in R?

Thanks everyone again for your advice and help!
Brandon 

-----Original Message-----
From: Jason Turner [mailto:jasont at indigoindustrial.co.nz] 
Sent: Monday, December 15, 2003 11:13 PM
To: Brandon Vaughn
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Resampling Stats software

Brandon Vaughn wrote:
...
> I am new to R (I have most of my experience in SAS and SPSS).  I was 
> wondering if anyone has used both Resampling Stats and R, and could 
> comment on strengths/relationships.

There are a few add-on packages for resampling with R.  "boot" is the one
I've used, and can strongly recommend.

> Also, I have no clue on how to do the various examples from the book 
> "Resampling: The New Statistics" in R.  Can anyone give me some 
> possible starting points?  Or websites/books?

I've never heard of the book you cite, but these two are good.  The first is
a pure bootstrap book, with examples in S-PLUS (the R library is rather
close).  The second is an applied stats book, which includes a section on
resampling methods.  All its examples are in S-PLUS, with notes about where
R differs (very little).

@Book{DavidsonHinkley1997,
author =	 {A. C. Davidson and D. V. Hinkley},
   title =	 {Bootstrap Methods and their Application},
   publisher =	 {Cambridge University Press},
   year =	 {1997},
}

@book{VenablesRipley2002,
   author =	 "Venables, W.R. and Ripley, B.D.",
   title =	 "Modern Applied Statistics with S",
   edition =	 "Fourth",
   publisher =	 {Springer-Verlag},
   address =	 {New York},
   year =	 2002,
}

Cheers

Jason
--
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From ripley at stats.ox.ac.uk  Thu Dec 18 08:31:12 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 18 Dec 2003 07:31:12 +0000 (GMT)
Subject: [R] Resampling Stats software
In-Reply-To: <20031218052459.MBPC13947.imf24aec.mail.bellsouth.net@brandoncd66bez>
Message-ID: <Pine.LNX.4.44.0312180719430.5996-100000@gannet.stats>

On Wed, 17 Dec 2003, Brandon Vaughn wrote:

> Thanks to everyone who wrote in with suggestions.  I will check out the
> books mentioned.
> 
> The book I mentioned "Resampling: The New Statistics" is actually available
> free online at:
> 
> http://www.resample.com/content/text/index.shtml
> 
> It seems pretty good as an introduction.  But then again, I am new at this
> concept.

An introduction to what?  (It seems to confuse resampling and
simulation-based inference.)

> Does anyone know right off hand how to do simple simulation with R?  Like
> for instance, in the book mentioned above, there is an example of figuring
> out the probability that a company with 20 trucks with have 4 or more fail
> on a given day (the probability that any given truck fails is .10).  So the
> way they do it is to simulate uniform numbers from 1 to 10, and let the
> number 1 represent a defective truck.  So here is the setup in the program
> Resampling Stat:
> 
> 	REPEAT 400   [repeat simulation 400 times]
> 	GENERATE 20 1,10 a  	[generate 20 numbers between 1 and 10; store
> in vector a]
> 	COUNT a = 1 b	[count the number of 1's and store in vector b]
> 	SCORE b z	[keep track of each trial in vector z]
> 	END	[repeat process]
> 	COUNT z > 3 k	[count the number of times trials more than 3 and
> store]
> 	DIVIDE k 400 kk	[convert to probability and store]
> 	PRINT kk	[print result]
> 
> This seems like a simple problem, and seemingly simple process in Resampling
> Stats.  Any idea on how to get started doing this in R?

However, the number of failures is a binomial variate, so it is much 
simpler in R, for example

cnts <- rbinom(400, 20, 0.1)
mean(cnts >= 4)

However, doing 1 million runs was almost instantaneous on my machine.

And the expected answer is pbinom(3, 20, 0.1, lower=FALSE)

As a matter of terminology, this is not resampling as usually defined, so 
I do wonder exactly what it is you are after.  For resampling in the usual 
sense, I would echo Jason's recommendation of Davison and Hinkley's CUP 
book.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From agustin.perez at umh.es  Thu Dec 18 09:18:10 2003
From: agustin.perez at umh.es (Perez Martin, Agustin)
Date: Thu, 18 Dec 2003 09:18:10 +0100
Subject: [R] Summaries
Message-ID: <5AFDDD57E2771B409224CD858CC6DE0D02DAB347@mailer-e051.umh.es>

Hello UseRs:

Excuses for my english.
I have a dataset with 65000 records and I'd like to make a summary where I
can view all the values (with the number of times that it repeats) that
there are each column of my dataset.
I tried with summary( ), str( ), but nothing gives me the result that I am
loking for.

Thank you very much.



From lecoutre at stat.ucl.ac.be  Thu Dec 18 09:22:25 2003
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Thu, 18 Dec 2003 09:22:25 +0100
Subject: [R] Summaries
In-Reply-To: <5AFDDD57E2771B409224CD858CC6DE0D02DAB347@mailer-e051.umh.e
 s>
References: <5AFDDD57E2771B409224CD858CC6DE0D02DAB347@mailer-e051.umh.es>
Message-ID: <6.0.1.1.2.20031218092129.020b47d8@stat4ux.stat.ucl.ac.be>


Have a look at 'table'
To compute for all columns of your dataset, combine with 'apply':

 > data(iris)
 > apply(iris,2,table)

[...]

$Petal.Width

0.1 0.2 0.3 0.4 0.5 0.6 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0 2.1 2.2 
2.3 2.4 2.5
   5  29   7   7   1   1   7   3   5  13   8  12   4   2  12   5   6   6 
3   8   3   3

$Species

     setosa versicolor  virginica
         50         50         50


Eric


At 09:18 18/12/2003, Perez Martin, Agustin wrote:
>Hello UseRs:
>
>Excuses for my english.
>I have a dataset with 65000 records and I'd like to make a summary where I
>can view all the values (with the number of times that it repeats) that
>there are each column of my dataset.
>I tried with summary( ), str( ), but nothing gives me the result that I am
>loking for.
>
>Thank you very much.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help



--------------------------------------------------
L'erreur est certes humaine, mais un vrai d?sastre
n?cessite un ou deux ordinateurs. Citation anonyme
--------------------------------------------------
Eric Lecoutre
Informaticien/Statisticien
Institut de Statistique / UCL

TEL (+32)(0)10473050       lecoutre at stat.ucl.ac.be
URL http://www.stat.ucl.ac.be/ISpersonnel/lecoutre



From maechler at stat.math.ethz.ch  Thu Dec 18 10:11:13 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 18 Dec 2003 10:11:13 +0100
Subject: [R] mailing list for basic questions - preliminary sum up
In-Reply-To: <5.2.1.1.2.20031217174446.042a1ea0@mailhost.blackmesacapital.com>
References: <3A822319EB35174CA3714066D590DCD50205CF6E@usrymx25.merck.com>
	<5.2.1.1.2.20031217174446.042a1ea0@mailhost.blackmesacapital.com>
Message-ID: <16353.28465.12567.286481@gargle.gargle.HOWL>

>>>>> "Tony" == Tony Plate <tplate at acm.org>
>>>>>     on Wed, 17 Dec 2003 18:04:34 -0700 writes:

    Tony> The suggestions of Tom (posting guide) and Andy (Eric
    Tony> Raymond's "How To Ask Questions The Smart Way") are
    Tony> both good.  Perhaps a good place to put an actual
    Tony> posting guide and a link to Raymond's page would be at
    Tony> the page pointed to by the link at the bottom of every
    Tony> posting to R-help (ie
    Tony> https://www.stat.math.ethz.ch/mailman/listinfo/r-help).

Thank you for the suggestion; note however that this is a
dynamically created page (by mailman) which is pretty tedious to
edit much, or even have much text added.  I can add one or two
short paragraphs with more hyperlinks; however I think it would
be better (and much better maintainable by all of R-core) to add
more to the "General Instructions" section of the R-project page
on the mailing list, http://www.R-project.org/mail.html#instructions
which is pointed to (and you also mention below), or even split
that page into more.  
Alternatively, even point to a page that you (someone in the R
community) maintain. 
      
    Tony> I'm pretty sure that most people who are discouraged
    Tony> from posting for fear of being scolded would be
    Tony> willing to read at least that page.  And currently
    Tony> there are not many guidelines for posting on either
    Tony> that page or on the "General Instructions" page it has
    Tony> a link to.

    Tony> Since these things happen only if some starts them,
    Tony> I'll post the raw beginnings of a posting guide by the
    Tony> end of the week (that is if there are no objections
    Tony> and no-one else beats me to it).

Yes, that would be real useful, and a service to the community,
please use "plain html" -- thank you in advance!


    Tony> On a related note, one of the few guidelines that is
    Tony> under the "General Instructions" seems to be
    Tony> potentially misleading to those who fail to understand
    Tony> the distinction between "R developers" and "package
    Tony> maintainers" (which is probably most beginners):

    >> It is recommended that you send mail to r-help (or
    >> r-devel if appropriate) rather than only to the R
    >> developers (who are also subscribed to the list, of
    >> course). This may save them precious time they can use
    >> for constantly improving R, and will typically also
    >> result in much quicker feedback for yourself.

We'll gladly accept improvements to this:

<p>It is recommended that you send mail to r-help (or r-devel if
appropriate) rather than only to the <em>R</em> developers (who are also
subscribed to the list, of course). This may save them precious time they
can use for constantly improving <em>R</em>, and will typically also
result in much quicker feedback for yourself. </p>

    Tony> (I say this seems misleading because I've seen quite a
    Tony>  few times, in response to a post about a problem with
    Tony>  a package, people told that they should have contacted
    Tony>  the package maintainer first.)

I'm not sure that this would be improved by a better text
above.  The main reason is that those posters are not aware of
the difference of the R standard packages and "arbitrary" CRAN packages
{the 'Recommended' ones taking a bit of a role in between: They
 do have unique maintainers, but problems potentially affect
 everyone, and R-core does take some moral responsibility for them} 

--------

Further on the orginal subject:

I think Frank Harrell's point ("not a different list, but a new
medium"!) and subsequent suggestions do make much sense:
If some of you can donate your time (and that *is* needed!) to
build and actively maintain a forum / bulletin board / web-"meeting place"
that could be a valuable "asset" for the R-project.  We (R core)
shouldn't have to use our resources for such a maintenance though.

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From juli at ceam.es  Thu Dec 18 10:54:08 2003
From: juli at ceam.es (juli g. pausas)
Date: Thu, 18 Dec 2003 10:54:08 +0100
Subject: [R] barplot & plot together
Message-ID: <3FE17940.3050808@ceam.es>

Dear colleges,
I'm trying to combine a barplot and a plot in a single figure as follows:

data <- 1:6
t <- barplot(data, axes=F)
par(new= T)
plot(t, data, type="b")

However, as you can see in the example, the dots of the second plot do 
not fall in the midpoint of the bars in the first. Any trick for setting 
the 2 plots at the same scale?
I have unsuccessfully tried:
plot(t, data, type="b", xlim=c(0,7))
plot(t, data, type="b", xlim=c(min(t),max(t)))

(R 1.8.1, for Windows)

Thanks

Juli



-- 
Juli G. Pausas
Centro de Estudios Ambientales del Mediterraneo (CEAM)
C/ Charles R. Darwin 14, Parc Tecnologic,
46980 Paterna, Valencia, SPAIN
Tel: (+ 34) 96 131 8227; Fax: (+ 34) 96 131 8190
mailto:juli at ceam.es
http://www.gva.es/ceam

GCTE Fire Network - http://www.gva.es/ceam/FireNetwork



From angel_lul at hotmail.com  Thu Dec 18 11:19:20 2003
From: angel_lul at hotmail.com (Angel)
Date: Thu, 18 Dec 2003 11:19:20 +0100
Subject: [R] symbolic and algebraic computation
Message-ID: <LAW11-OE15FI1ZG5yoV0000cdbd@hotmail.com>

I needed a symbolic and algebraic computation software. The best solution
I've found so far is maxima computer algebra system.
I was wondering if there is any R package that has similar features as
maxima's or somebody could point me to reference manuals on how to use R for
this purpose.
Thanks,
Angel



From maechler at stat.math.ethz.ch  Thu Dec 18 11:21:13 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 18 Dec 2003 11:21:13 +0100
Subject: [R] barplot & plot together
In-Reply-To: <3FE17940.3050808@ceam.es>
References: <3FE17940.3050808@ceam.es>
Message-ID: <16353.32665.894312.717770@gargle.gargle.HOWL>

>>>>> "juli" == juli g pausas <juli at ceam.es>
>>>>>     on Thu, 18 Dec 2003 10:54:08 +0100 writes:

    juli> Dear colleges,
    juli> I'm trying to combine a barplot and a plot in a single figure as follows:

    juli> data <- 1:6
    juli> t <- barplot(data, axes=F)
    juli> par(new= T)
    juli> plot(t, data, type="b")

    juli> However, as you can see in the example, the dots of
    juli> the second plot do not fall in the midpoint of the
    juli> bars in the first. Any trick for setting the 2 plots
    juli> at the same scale?

yes, use

   bd <- barplot(data)
   points(bd, data, type = "b")

instead.  
A general recommendation: 
   Try to *not* use par(new = TRUE) if you can.

Martin



From p.dalgaard at biostat.ku.dk  Thu Dec 18 11:21:00 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 Dec 2003 11:21:00 +0100
Subject: [R] barplot & plot together
In-Reply-To: <3FE17940.3050808@ceam.es>
References: <3FE17940.3050808@ceam.es>
Message-ID: <x2d6amv33n.fsf@biostat.ku.dk>

"juli g. pausas" <juli at ceam.es> writes:

> Dear colleges,
> I'm trying to combine a barplot and a plot in a single figure as follows:
> 
> data <- 1:6
> t <- barplot(data, axes=F)
> par(new= T)
> plot(t, data, type="b")
> 
> However, as you can see in the example, the dots of the second plot do
> not fall in the midpoint of the bars in the first. Any trick for
> setting the 2 plots at the same scale?
> I have unsuccessfully tried:
> plot(t, data, type="b", xlim=c(0,7))
> plot(t, data, type="b", xlim=c(min(t),max(t)))
> 
> (R 1.8.1, for Windows)

The canonical trick for getting two plots on the same scale is to set
xlim (and ylim) on *both*. On barplots, this gets a bit tricky since
you have to leave room for the column width (the actual calculation
can be read inside barplot.default). However, I'd try for something like

 t <- barplot(data,names=1:6,ylim=range(c(0,data*1.01)))
 points(t, data, type="b")


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From gb at stat.umu.se  Thu Dec 18 11:24:47 2003
From: gb at stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Thu, 18 Dec 2003 11:24:47 +0100
Subject: [R] Manova
Message-ID: <20031218102447.GA4641@stat.umu.se>

Dear R-helpers,

In a data set I got from a medical doctor there are six treatment groups
and (about) 5 bivariate responses in each group. Using 'manova', it is
easy to see significant differences in treatment effects, but the doctor
is more interested in the correlation between the two responses (within
groups). I'm willing to assume a common value over groups, and one way
of estimating and testing the common correlation would be to use
'cor.test' on the residuals from 'manova', but I guess that the
resulting p-value (from testing zero correlation) will be far too
optimistic (it is in fact 4.5e-5).

What is the 'right' way of doing this in R?
-- 
 G?ran Brostr?m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume?, Sweden             e-mail: gb at stat.umu.se



From wolfgangpauli at web.de  Thu Dec 18 12:00:27 2003
From: wolfgangpauli at web.de (Wolfgang Pauli)
Date: Thu, 18 Dec 2003 12:00:27 +0100
Subject: [R] Huynh-Feldt correction
Message-ID: <200312181200.27133.wolfgangpauli@web.de>

Dear R-helpers,

Does anybody know, whether there is an option to tell aov/anova, or do 
something similar, to get a Huynh-Feldt correction of dfs in the aov/anova 
function?

Thanks in advance!



From p.dalgaard at biostat.ku.dk  Thu Dec 18 12:04:31 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 Dec 2003 12:04:31 +0100
Subject: [R] Manova
In-Reply-To: <20031218102447.GA4641@stat.umu.se>
References: <20031218102447.GA4641@stat.umu.se>
Message-ID: <x28ylav134.fsf@biostat.ku.dk>

G?ran Brostr?m <gb at stat.umu.se> writes:

> Dear R-helpers,
> 
> In a data set I got from a medical doctor there are six treatment groups
> and (about) 5 bivariate responses in each group. Using 'manova', it is
> easy to see significant differences in treatment effects, but the doctor
> is more interested in the correlation between the two responses (within
> groups). I'm willing to assume a common value over groups, and one way
> of estimating and testing the common correlation would be to use
> 'cor.test' on the residuals from 'manova', but I guess that the
> resulting p-value (from testing zero correlation) will be far too
> optimistic (it is in fact 4.5e-5).

I think you're getting correlation right, but the DF wrong since 5 DF
are lost to treatment contrasts. Hence the t statistic is wrong too
(wrong DF *and* inflated by about 20%). 
 
> What is the 'right' way of doing this in R?

Is there one? 

One expedient way is to look for a zero regression coef in

summary(lm(v1~treat+v2)) # or vice versa

or you could clone the calculation from getS3method("cor.test","default")

        r <- cor(x, y)
        df <- ??? # insert properly calculated DF here.
        STATISTIC <- c(t = sqrt(df) * r/sqrt(1 - r^2))
        p <- pt(STATISTIC, df)

or you could look for a test of independence in the multivariate
normal distr. on CRAN. (I don't know if there is one -- it's your
problem... ;-) )

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From juli at ceam.es  Thu Dec 18 12:10:19 2003
From: juli at ceam.es (juli g. pausas)
Date: Thu, 18 Dec 2003 12:10:19 +0100
Subject: [R] barplot & plot together
In-Reply-To: <16353.32665.894312.717770@gargle.gargle.HOWL>
References: <3FE17940.3050808@ceam.es>
	<16353.32665.894312.717770@gargle.gargle.HOWL>
Message-ID: <3FE18B1B.4050401@ceam.es>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031218/989af856/attachment.pl

From vito.muggeo at giustizia.it  Thu Dec 18 12:31:34 2003
From: vito.muggeo at giustizia.it (Vito Muggeo)
Date: Thu, 18 Dec 2003 12:31:34 +0100
Subject: R: [R] `bivariate apply' - Summary
References: <20031216163406.15271397F@mprdmxin.myway.com>
Message-ID: <008d01c3c55a$826b3b60$5c13070a@PROCGEN>

Dear all,
Thanks to Thomas Lumley, Peter Wolf and Gabor Grothendieck for their reply

The problem was to to apply a bivariate function (such as cor(), for
instance) to each combination of colums of a given matrix.

Below there are four possible solutions (the original message is also posted
below). Using a matrix 5000x20, the best solutions seem to be f1 and f2.


f1<-function(mat, FUN, ...){
#author: Thomas Lumley
    nc<-NCOL(mat)
    i<-rep(1:nc, nc)
    j<-rep(1:nc, each=nc)
    rval<-mapply(function(ii,ji) FUN(mat[,ii], mat[,ji], ...), i, j)
    matrix(rval, nc=nc)
}

f2<-function(x,fun=cor){
#author: Peter Wolf
 cl<-matrix(1:ncol(x),ncol(x),ncol(x))
 cl<-cbind(as.vector(cl),as.vector(t(cl)))
 res<-apply(cl,1,function(xx)fun(x[,xx[1]],x[,xx[2]]))
 matrix(res,ncol(x),ncol(x))
}

f3 <- function(x,f=cor) {
#author: Gabor Grothendieck
   k <- NCOL( x )
   x <- apply( x, 2, list )
   ff <- function(x,y) f( unlist(x), unlist(y) )
   matrix( mapply( ff, rep(x,rep(k,k)), rep(x,k) ), k, k )
}

f4<-function(x,FUN,...){
#author: vito muggeo
    require(gregmisc)
    a<-combinations(ncol(x),2)
    r<-list()
    for(i in 1:nrow(a)){r[[length(r)+1]]<- x[,a[i,]]}
    ris<-matrix(1,ncol(x),ncol(x))
    ris1<-sapply(r, function(xx)FUN(xx[,1],xx[,2],...))
    ris[col(ris)<row(ris)]<- ris[col(ris)>row(ris)]<-ris1
    return(ris)
    }


# test
x <- matrix( runif(5000*20), 5000, 20 )
system.time(f1( x, cor ))



##original message
 > dear all,
>
> Given a matrix A, say, I would like to apply a bivariate function to each
> combination of its colums. That is if
>
> myfun<-function(x,y)cor(x,y) #computes simple correlation of two vectors x
> and y
>
> then the results should be something similar to cor(A).
>
> I tried with mapply, outer,...but without success
>
> Can anybody help me?
>
> many thanks in advance,
> vito
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>
> _______________________________________________
> No banners. No pop-ups. No kidding.
> Introducing My Way - http://www.myway.com



From andrea.spano at fastwebnet.it  Thu Dec 18 12:49:13 2003
From: andrea.spano at fastwebnet.it (Andrea)
Date: 18 Dec 2003 12:49:13 +0100
Subject: [R] RSPerl
Message-ID: <1071748153.3963.8.camel@montecristo>

If anyone can help it would be very much apreciated...

System RedHat 9

R installed as


# ./configure --enable-R-shlib
# make
# make install

and R seems to work fine ...

Then I do 

# R INSTALL --clean --configure-args='--with-in-perl'
RSPerl_0.5-7.tar.gz

# export R_HOME=/usr/local/lib/R

# cd /usr/local/lib/R/library/RSPerl/examples/

# perl -I /usr/local/lib/R/library/RSPerl/share/blib/arch -I
/usr/local/lib/R/library/RSPerl/share/blib/lib ./test.pl


and I get 


1..1
Can't load '/usr/local/lib/R/library/RSPerl/share/blib/arch/auto/R/R.so'
for module R:
/usr/lib/perl5/vendor_perl/5.8.0/i386-linux-thread-multi/auto/ModPerl/Global/Global.so: undefined symbol: modperl_perl_global_avcv_call at /usr/lib/perl5/5.8.0/i386-linux-thread-multi/DynaLoader.pm line 229.
 at ./test.pl line 11
Compilation failed in require at ./test.pl line 11.
BEGIN failed--compilation aborted at ./test.pl line 11.
not ok 1


and I get completelly lost ...

Thanks in advance for your help


Andrea



From duncan at research.bell-labs.com  Thu Dec 18 14:00:05 2003
From: duncan at research.bell-labs.com (Duncan Temple Lang)
Date: Thu, 18 Dec 2003 08:00:05 -0500
Subject: [R] RSPerl
In-Reply-To: <1071748153.3963.8.camel@montecristo>;
	from andrea.spano@fastwebnet.it on Thu, Dec 18, 2003 at
	12:49:13PM +0100
References: <1071748153.3963.8.camel@montecristo>
Message-ID: <20031218080005.B18235@jessie.research.bell-labs.com>


Hi Andrea.

   When we run Perl from within R, any Perl module that has associated
C code (such as modperl) needs a special piece of bootstrapping code
to be used within this embedded case.  Now in your case, you might
only be calling R from within Perl, but neverthless, we put this
bootstrapping code in as one can call Perl from within the embedded R
because it is bidirectional.

  The modperl module is for running Perl embedded within the Apache
Web server.  So you don't need this for running R in Perl as a
stand-alone.  So we need to tell the RSPerl package not to include
bootstrapping code for modperl (and other modules that cause problems
in this way).

  To create this bootstrapping code for Perl modules with C code, the
installation script for RSPerl looks through all the Perl modules and
finds the ones that have C code (i.e. an associated shared library).
In this case, you need to tell the RSPerl installation explicitly
which modules to provide the boostrapping code.  The easiest
way for me to describe is the following.

  unzip and untar the RSPerl.tar.gz file

  cd RSPerl

  Run the command 
     perl -s modules.pl -modules
  to get a list of all the modules with C code.

  Take this list and set the environment variable
 
    PERL_MODULES

   to the subset of these module names that you actually need/want.
  In your case, just remove modperl.
  You might do someting like

    setenv PERL_MODULES "`perl -s modules.pl -modules | sed s/modperl//`"

   (if you are using the csh or tcsh shell. Use export for bash/sh.)


   You can eliminate any of the modules that cause problems using this
approach.

   Hope this helps.

    D.


Andrea wrote:
> If anyone can help it would be very much apreciated...
> 
> System RedHat 9
> 
> R installed as
> 
> 
> # ./configure --enable-R-shlib
> # make
> # make install
> 
> and R seems to work fine ...
> 
> Then I do 
> 
> # R INSTALL --clean --configure-args='--with-in-perl'
> RSPerl_0.5-7.tar.gz
> 
> # export R_HOME=/usr/local/lib/R
> 
> # cd /usr/local/lib/R/library/RSPerl/examples/
> 
> # perl -I /usr/local/lib/R/library/RSPerl/share/blib/arch -I
> /usr/local/lib/R/library/RSPerl/share/blib/lib ./test.pl
> 
> 
> and I get 
> 
> 
> 1..1
> Can't load '/usr/local/lib/R/library/RSPerl/share/blib/arch/auto/R/R.so'
> for module R:
> /usr/lib/perl5/vendor_perl/5.8.0/i386-linux-thread-multi/auto/ModPerl/Global/Global.so: undefined symbol: modperl_perl_global_avcv_call at /usr/lib/perl5/5.8.0/i386-linux-thread-multi/DynaLoader.pm line 229.
>  at ./test.pl line 11
> Compilation failed in require at ./test.pl line 11.
> BEGIN failed--compilation aborted at ./test.pl line 11.
> not ok 1
> 
> 
> and I get completelly lost ...
> 
> Thanks in advance for your help
> 
> 
> Andrea
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-- 
_______________________________________________________________

Duncan Temple Lang                duncan at research.bell-labs.com
Bell Labs, Lucent Technologies    office: (908)582-3217
700 Mountain Avenue, Room 2C-259  fax:    (908)582-3340
Murray Hill, NJ  07974-2070       
         http://cm.bell-labs.com/stat/duncan



From allan at stats.uct.ac.za  Thu Dec 18 14:11:24 2003
From: allan at stats.uct.ac.za (allan clark)
Date: Thu, 18 Dec 2003 15:11:24 +0200
Subject: [R]: Lp norm estimation
Message-ID: <3FE1A77C.94AD6761@stats.uct.ac.za>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031218/d0af76b0/attachment.pl

From klemens.barfus at gmx.de  Thu Dec 18 14:16:06 2003
From: klemens.barfus at gmx.de (Klemens Barfus)
Date: Thu, 18 Dec 2003 14:16:06 +0100 (MET)
Subject: [R] How to create Voronoi-Polygons ?
Message-ID: <11399.1071753366@www7.gmx.net>

Dear List-Members,
my name is Klemens Barfus and I am quite new to this list and to working
with R.
I try to generate Vornoi-Polygons with the Tripack package. I would like to
have a table with the sites xy and the surrounding nodes of the
Voronoi-Polygons. The area shall be delimited by a defined rectangle so that the polygons
at the edge are cut by the rectangle sides.
Is there a way to do this ? Looking the old list entries I found no help.
First  using voronoi.mosaic worked fine, but how to get the connection of
the sites to the surrounding nodes. The other question is what the information
of the voronoi.findrejectsites() functon is.  For my testdata it showed 3
sites which ly at the edge, but it should be 6. ...

Thanks for your help in advance !

Klemens
  

--



From paradis at isem.univ-montp2.fr  Wed Dec 17 23:13:54 2003
From: paradis at isem.univ-montp2.fr (Emmanuel Paradis)
Date: Wed, 17 Dec 2003 23:13:54 +0100
Subject: [R] mailing list for basic questions - preliminary sum up
In-Reply-To: <200312171457.25540.wegmann_mailinglist@gmx.net>
References: <Pine.LNX.4.44.0312170833470.1851-100000@reclus.nhh.no>
	<Pine.LNX.4.44.0312170833470.1851-100000@reclus.nhh.no>
Message-ID: <4.2.0.58.20031217225428.00365c68@isem.isem.univ-montp2.fr>

A 14:57 17/12/2003 +0100, Martin Wegmann a ?crit:
>Hello,
>
>Roger Bivand wrote:
> > appropriate light. One basic characteristic seems to be that if the
> > question does indicate seriousness about trying to analyse data, respect
> > for the task at hand, then predictably lots of good advice comes quickly.
>
>yes, I also experienced that (from the questioner point)
>
> > I'm also not too sure about the "learning R" question. Of course there is
> > the GUI/CLI issue, and the "very many defaults already filled in" issue,
> > but actually market share really isn't a driver here, is it? Isn't this
>
>ok, I get this point - R can be seen more as "philosphie" than "commercial"
>thinking of getting market shares.
>i think it is a trade-off between "spreading" R and being a member of a 
>"geek"
>statistic program (that's what SPSS user in this dept. think about us R
>user ;-)  )
>
> > more about attitude and motivation in taking an active role in analysing
> > data? If your research question really itches, what should it take to stop
> > you learning R (or associated packages)?
>
>probably the steep learing curve but of course if you really want, you will
>suceed and explorere the advanteages of R.

I think there is a deeper problem here: in many cases, scientists come to 
data analysis as a necessity to publish their results. They look for quick 
answers when analysing data. Thus learning a completely new system of data 
analysis may seem not only steep, but of limited interest.

>That sounds a bit like an elitist point of view - who really wants to use R
>will eventually suceed.....but that's like everywhere in the opensource
>community - to get started with Linux or other software is still quite rough
>but a lot of people are doing it because of various reasons. This community
>has a great support mentality and are more worth than any commercial support
>and therefore it is possible.
>
>but I don't want to start a flame about this issue (elitist geek
>software ;-)  )and therefore I would like to propose a "evaluation" - better
>in a separate mail to keep the overview.
>
>regards Martin
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help


Emmanuel Paradis
Laboratoire de Pal?ontologie
Institut des Sciences de l'?volution
Universit? Montpellier II
F-34095 Montpellier c?dex 05
France
    phone: +33  4 67 14 39 64
      fax: +33  4 67 14 36 10
   mailto:paradis at isem.univ-montp2.fr
   http://www.isem.univ-montp2.fr/PPP/PPPphylogenie/ParadisHome.php



From roger at ysidro.econ.uiuc.edu  Thu Dec 18 14:45:10 2003
From: roger at ysidro.econ.uiuc.edu (Roger Koenker)
Date: Thu, 18 Dec 2003 07:45:10 -0600 (CST)
Subject: [R] How to create Voronoi-Polygons ?
In-Reply-To: <11399.1071753366@www7.gmx.net>
Message-ID: <Pine.SOL.4.30.0312180737500.27562-100000@ysidro.econ.uiuc.edu>

Everything is possible, some things are just more difficult than others.  It is a little
tricky to find the point of intersection of the voronoi edges with the frame of the
diagram.  Your basic request is quite straightforward though.  Here is a function
that plots, by looping through the sites and using voronoi.findvertices() to
get the vertices of the polygons.  In the function fit represents a z coordinate
that indicates the value of a fitted function at each of the sites, -- this may
be irrelevant in your setup.  I hope that it will help:

"plot.voronogram" <-
function (x,y,fit)
{
tm <- tri.mesh(x,y)
xlim <- c(min(tm$x) - 0.1 * diff(range(tm$x)), max(tm$x) + 0.1 * diff(range(tm$x)))
ylim <- c(min(tm$y) - 0.1 * diff(range(tm$y)), max(tm$y) + 0.1 * diff(range(tm$y)))
plot.new()
plot.window(xlim = xlim, ylim = ylim, "")
n <- length(x)
vm <- voronoi.mosaic(x,y)
palette(topo.colors(100))
cfit <- 100*fit
for(i in 1:n){
        vpolygon <- voronoi.findvertices(i,vm)
        if(length(vpolygon)){
                xx <- vm$x[vpolygon]
                yy <- vm$y[vpolygon]
                inhull <- in.convex.hull(tm,xx,yy)
                polygon(xx[inhull],yy[inhull] ,col=cfit[i])
                }
        }
}


url:	www.econ.uiuc.edu/~roger/my.html	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820

On Thu, 18 Dec 2003, Klemens Barfus wrote:

> Dear List-Members,
> my name is Klemens Barfus and I am quite new to this list and to working
> with R.
> I try to generate Vornoi-Polygons with the Tripack package. I would like to
> have a table with the sites xy and the surrounding nodes of the
> Voronoi-Polygons. The area shall be delimited by a defined rectangle so that the polygons
> at the edge are cut by the rectangle sides.
> Is there a way to do this ? Looking the old list entries I found no help.
> First  using voronoi.mosaic worked fine, but how to get the connection of
> the sites to the surrounding nodes. The other question is what the information
> of the voronoi.findrejectsites() functon is.  For my testdata it showed 3
> sites which ly at the edge, but it should be 6. ...
>
> Thanks for your help in advance !
>
> Klemens
>
>
> --
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From dmurdoch at pair.com  Thu Dec 18 14:43:15 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Thu, 18 Dec 2003 08:43:15 -0500
Subject: [R] symbolic and algebraic computation
In-Reply-To: <LAW11-OE15FI1ZG5yoV0000cdbd@hotmail.com>
References: <LAW11-OE15FI1ZG5yoV0000cdbd@hotmail.com>
Message-ID: <fhb3uvsh81nh6kvvj8pe9fjqshqfboc3c9@4ax.com>

On Thu, 18 Dec 2003 11:19:20 +0100, "Angel" <angel_lul at hotmail.com>
wrote :

>I needed a symbolic and algebraic computation software. The best solution
>I've found so far is maxima computer algebra system.
>I was wondering if there is any R package that has similar features as
>maxima's or somebody could point me to reference manuals on how to use R for
>this purpose.

R has limited capabilities to do this (for example, see ?deriv), but
if you're doing anything serious, you're better off in a package
designed for that purpose.  Maple and Mathematica are both good
(commercial) choices; I don't know how they compare to maxima, and I
don't know the free ones.

Duncan Murdoch



From andy_liaw at merck.com  Thu Dec 18 14:45:58 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 18 Dec 2003 08:45:58 -0500
Subject: [R] Resampling Stats software
Message-ID: <3A822319EB35174CA3714066D590DCD50205CF6F@usrymx25.merck.com>

> From: Prof Brian Ripley

[snip]

> As a matter of terminology, this is not resampling as usually 
> defined, so 
> I do wonder exactly what it is you are after.  For resampling 
> in the usual 
> sense, I would echo Jason's recommendation of Davison and 
> Hinkley's CUP book.

Or perhaps at a gentler level, Efron & Tibshirani's "Introduction to the
Bootstrap" (Chapman & Hall/CRC)...
 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595


Andy


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From ramasamya at gis.a-star.edu.sg  Thu Dec 18 14:45:36 2003
From: ramasamya at gis.a-star.edu.sg (Adaikalavan RAMASAMY)
Date: Thu, 18 Dec 2003 21:45:36 +0800
Subject: [R] Summaries
Message-ID: <6D9E9B9DF347EF4385F6271C64FB8D56076076@BIONIC.biopolis.one-north.com>

If your dataset contains integer or limited possible unique numbers only I find the following more concise. 
 
m <- matrix( rpois(60, 5), nc=6 )
apply( m , 2, function(x) table( factor(x, levels=0:max(m))) )

If your dataset has continous or lots of unique numbers you may wish to consider only the summary statistics. You can try the function stats() or graphically bplot() [both from library fields]
 
Regards, Adai.
 
 

	-----Original Message----- 
	From: r-help-bounces at stat.math.ethz.ch on behalf of Eric Lecoutre 
	Sent: Thu 18/12/2003 16:22 
	To: Perez Martin, Agustin; lista R help (E-mail) 
	Cc: 
	Subject: Re: [R] Summaries
	
	


	Have a look at 'table'
	To compute for all columns of your dataset, combine with 'apply':
	
	 > data(iris)
	 > apply(iris,2,table)
	
	[...]
	
	$Petal.Width
	
	0.1 0.2 0.3 0.4 0.5 0.6 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0 2.1 2.2
	2.3 2.4 2.5
	   5  29   7   7   1   1   7   3   5  13   8  12   4   2  12   5   6   6
	3   8   3   3
	
	$Species
	
	     setosa versicolor  virginica
	         50         50         50
	
	
	Eric
	
	
	At 09:18 18/12/2003, Perez Martin, Agustin wrote:
	>Hello UseRs:
	>
	>Excuses for my english.
	>I have a dataset with 65000 records and I'd like to make a summary where I
	>can view all the values (with the number of times that it repeats) that
	>there are each column of my dataset.
	>I tried with summary( ), str( ), but nothing gives me the result that I am
	>loking for.
	>
	>Thank you very much.
	>
	>______________________________________________
	>R-help at stat.math.ethz.ch mailing list
	>https://www.stat.math.ethz.ch/mailman/listinfo/r-help <https://www.stat.math.ethz.ch/mailman/listinfo/r-help> 
	
	
	
	--------------------------------------------------
	L'erreur est certes humaine, mais un vrai d?sastre
	n?cessite un ou deux ordinateurs. Citation anonyme
	--------------------------------------------------
	Eric Lecoutre
	Informaticien/Statisticien
	Institut de Statistique / UCL
	
	TEL (+32)(0)10473050       lecoutre at stat.ucl.ac.be
	URL http://www.stat.ucl.ac.be/ISpersonnel/lecoutre <http://www.stat.ucl.ac.be/ISpersonnel/lecoutre> 
	
	______________________________________________
	R-help at stat.math.ethz.ch mailing list
	https://www.stat.math.ethz.ch/mailman/listinfo/r-help <https://www.stat.math.ethz.ch/mailman/listinfo/r-help>



From B.Rowlingson at lancaster.ac.uk  Thu Dec 18 14:52:20 2003
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Thu, 18 Dec 2003 13:52:20 +0000
Subject: [R] How to create Voronoi-Polygons ?
In-Reply-To: <Pine.SOL.4.30.0312180737500.27562-100000@ysidro.econ.uiuc.edu>
References: <Pine.SOL.4.30.0312180737500.27562-100000@ysidro.econ.uiuc.edu>
Message-ID: <3FE1B114.3000103@lancaster.ac.uk>

Roger Koenker wrote:

> Everything is possible, some things are just more difficult than others. 

  Is that so? In which case, how can I construct a square with the same 
area as a given circle using only the compass() and ruler() functions?

:)

Baz

PS redefining compass() and ruler() not allowed....



From SAULEAUEA at ch-mulhouse.fr  Thu Dec 18 14:38:41 2003
From: SAULEAUEA at ch-mulhouse.fr (=?iso-8859-1?Q?SAULEAU_Erik-Andr=E9?=)
Date: Thu, 18 Dec 2003 14:38:41 +0100
Subject: [R] bootstrap pValue in DClusters
Message-ID: <A91EF0B9121F834EA6484582DFE1CF4436F072@messagerie.chm.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031218/57db7f2d/attachment.pl

From roger at ysidro.econ.uiuc.edu  Thu Dec 18 15:09:25 2003
From: roger at ysidro.econ.uiuc.edu (Roger Koenker)
Date: Thu, 18 Dec 2003 08:09:25 -0600 (CST)
Subject: [R] How to create Voronoi-Polygons ?
In-Reply-To: <3FE1B114.3000103@lancaster.ac.uk>
Message-ID: <Pine.SOL.4.30.0312180805350.27562-100000@ysidro.econ.uiuc.edu>

One should doubt everything, even logic, and in doubt lies possibility.


url:	www.econ.uiuc.edu/~roger/my.html	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820

On Thu, 18 Dec 2003, Barry Rowlingson wrote:

> Roger Koenker wrote:
>
> > Everything is possible, some things are just more difficult than others.
>
>   Is that so? In which case, how can I construct a square with the same
> area as a given circle using only the compass() and ruler() functions?
>
> :)
>
> Baz
>
> PS redefining compass() and ruler() not allowed....
>
>



From Pascal.Niklaus at unibas.ch  Thu Dec 18 15:03:08 2003
From: Pascal.Niklaus at unibas.ch (Pascal A. Niklaus)
Date: Thu, 18 Dec 2003 15:03:08 +0100
Subject: [R] ESS tab completion for file names
Message-ID: <3FE1B39C.5020100@unibas.ch>

Hi all,

Using Xemacs/ESS (Xemacs 21.4-12/ESS 5.1.24), I noticed that tab 
completion does not work for file names.

df <- read.csv("xx     [TAB]

doesn't complete the file name.

After hitting TAB, I get "last thing matched was not a buffer" in the 
status bar. Any idea what may go wrong?

Thanks for any advice

Pascal



From juli at ceam.es  Thu Dec 18 15:03:04 2003
From: juli at ceam.es (juli g. pausas)
Date: Thu, 18 Dec 2003 15:03:04 +0100
Subject: [R] NA, deleting rows
Message-ID: <3FE1B398.9080200@ceam.es>

Dear colleges,
I do not understand the following behaviour:

> aa <- data.frame(a1= 1:10, a2= c(rep(NA, 5), 1:5) )
> aa
   a1 a2
1   1 NA
2   2 NA
3   3 NA
4   4 NA
5   5 NA
6   6  1
7   7  2
8   8  3
9   9  4
10 10  5
> aa[!aa$a2==1, ]  # removing rows with a2==1
     a1 a2
NA   NA NA
NA.1 NA NA
NA.2 NA NA
NA.3 NA NA
NA.4 NA NA
7     7  2
8     8  3
9     9  4
10   10  5

I didn't expect a1 to be affected.
Is  aa[!aa$a2==1, ]  an incorrect way to remove rows? 
Any other way?

(R 1.8.1. for Windows)
Thanks in advance

Juli



From gb at stat.umu.se  Thu Dec 18 15:05:49 2003
From: gb at stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Thu, 18 Dec 2003 15:05:49 +0100
Subject: [R] Manova
In-Reply-To: <x28ylav134.fsf@biostat.ku.dk>
References: <20031218102447.GA4641@stat.umu.se> <x28ylav134.fsf@biostat.ku.dk>
Message-ID: <20031218140549.GB4641@stat.umu.se>

On Thu, Dec 18, 2003 at 12:04:31PM +0100, Peter Dalgaard wrote:
> G?ran Brostr?m <gb at stat.umu.se> writes:
> 
> > Dear R-helpers,
> > 
> > In a data set I got from a medical doctor there are six treatment groups
> > and (about) 5 bivariate responses in each group. Using 'manova', it is
> > easy to see significant differences in treatment effects, but the doctor
> > is more interested in the correlation between the two responses (within
> > groups). I'm willing to assume a common value over groups, and one way
> > of estimating and testing the common correlation would be to use
> > 'cor.test' on the residuals from 'manova', but I guess that the
> > resulting p-value (from testing zero correlation) will be far too
> > optimistic (it is in fact 4.5e-5).
> 
> I think you're getting correlation right, but the DF wrong since 5 DF
> are lost to treatment contrasts. Hence the t statistic is wrong too
> (wrong DF *and* inflated by about 20%). 
> > What is the 'right' way of doing this in R?
> 
> Is there one? 
> 
> One expedient way is to look for a zero regression coef in
> 
> summary(lm(v1~treat+v2)) # or vice versa
> 
> or you could clone the calculation from getS3method("cor.test","default")
> 
>         r <- cor(x, y)
>         df <- ??? # insert properly calculated DF here.
>         STATISTIC <- c(t = sqrt(df) * r/sqrt(1 - r^2))
>         p <- pt(STATISTIC, df)

Thanks Peter, that seems to be correct; zero correlation should be
equivalent to zero regression. I wrote a function, 'grouped.cor', based
on your second suggestion, and I get:

> grouped.cor(x, y, group)
$p.value
[1] 0.0002546125
...
compared to

> summary(lm(y ~ x + group))
...
x            0.51495    0.11725   4.392 0.000255 ***

quite close, right? Compare with the 'naive' value 0.000045.

G?ran

grouped.cor <- function(x, y, group){
    ## Assumes a common correlation over groups, and that
    ## x and y have constant variance, but
    ## their means may differ between groups.
    
    n <- length(group)
    if ( (length(x) != n) || (length(y) != n)) stop("Length mismatch")
    n.grp <- length(unique(group))
    df <- n - n.grp - 1
    if (df < 1) stop("Too many groups or too few observations")

    subtract.mean <- function(x) x - mean(x)
    
    x <- unlist(tapply(x, group, subtract.mean))
    y <- unlist(tapply(y, group, subtract.mean))
    # NOTE: This will (eventually) reorder  x  and  y,
    # but in the same way: doesn't affect cor(x, y)!
    
    res <- cor.test(x, y)
    statistic <- res$statistic * sqrt(df / (n - 2))
    p <- pt(statistic, df)
    p <- 2 * min(p, 1 - p)
    
    list(p.value = p, statistic = statistic, df = df)
}



From sundar.dorai-raj at pdf.com  Thu Dec 18 15:20:56 2003
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 18 Dec 2003 08:20:56 -0600
Subject: [R] NA, deleting rows
In-Reply-To: <3FE1B398.9080200@ceam.es>
References: <3FE1B398.9080200@ceam.es>
Message-ID: <3FE1B7C8.40304@pdf.com>

Take a look at what (aa$a2 == 1) returns and it may clear things up.

Try

aa[-which(aa$a2 == 1), ]

or

subset(aa, a2 != 1 | is.na(a2))

HTH,
Sundar


juli g. pausas wrote:

> Dear colleges,
> I do not understand the following behaviour:
> 
>> aa <- data.frame(a1= 1:10, a2= c(rep(NA, 5), 1:5) )
>> aa
> 
>   a1 a2
> 1   1 NA
> 2   2 NA
> 3   3 NA
> 4   4 NA
> 5   5 NA
> 6   6  1
> 7   7  2
> 8   8  3
> 9   9  4
> 10 10  5
> 
>> aa[!aa$a2==1, ]  # removing rows with a2==1
> 
>     a1 a2
> NA   NA NA
> NA.1 NA NA
> NA.2 NA NA
> NA.3 NA NA
> NA.4 NA NA
> 7     7  2
> 8     8  3
> 9     9  4
> 10   10  5
> 
> I didn't expect a1 to be affected.
> Is  aa[!aa$a2==1, ]  an incorrect way to remove rows? Any other way?
> 
> (R 1.8.1. for Windows)
> Thanks in advance
> 
> Juli
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From Achim.Zeileis at wu-wien.ac.at  Thu Dec 18 15:18:11 2003
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 18 Dec 2003 15:18:11 +0100
Subject: [R] NA, deleting rows
In-Reply-To: <3FE1B398.9080200@ceam.es>
References: <3FE1B398.9080200@ceam.es>
Message-ID: <20031218151811.5b7068a5.Achim.Zeileis@wu-wien.ac.at>

On Thu, 18 Dec 2003 15:03:04 +0100 juli g. pausas wrote:

> Dear colleges,
> I do not understand the following behaviour:
> 
> > aa <- data.frame(a1= 1:10, a2= c(rep(NA, 5), 1:5) )
> > aa
>    a1 a2
> 1   1 NA
> 2   2 NA
> 3   3 NA
> 4   4 NA
> 5   5 NA
> 6   6  1
> 7   7  2
> 8   8  3
> 9   9  4
> 10 10  5
> > aa[!aa$a2==1, ]  # removing rows with a2==1
>      a1 a2
> NA   NA NA
> NA.1 NA NA
> NA.2 NA NA
> NA.3 NA NA
> NA.4 NA NA
> 7     7  2
> 8     8  3
> 9     9  4
> 10   10  5
> 
> I didn't expect a1 to be affected.
> Is  aa[!aa$a2==1, ]  an incorrect way to remove rows? 

It leads to the behaviour above if there are NAs in the logical vector
used for indexing:

R> !aa$a2==1
 [1]    NA    NA    NA    NA    NA FALSE  TRUE  TRUE  TRUE  TRUE

> Any other way?

Several other ways are conceivable to treat the NA rows differently.
This precise problem is solved, e.g., by

R> aa[-which(aa$a2==1), ]
   a1 a2
1   1 NA
2   2 NA
3   3 NA
4   4 NA
5   5 NA
7   7  2
8   8  3
9   9  4
10 10  5

hth,
Z
 
> (R 1.8.1. for Windows)
> Thanks in advance
> 
> Juli
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From rolf at math.unb.ca  Thu Dec 18 15:23:28 2003
From: rolf at math.unb.ca (Rolf Turner)
Date: Thu, 18 Dec 2003 10:23:28 -0400 (AST)
Subject: [R] How to create Voronoi-Polygons ?
Message-ID: <200312181423.hBIENSXt025685@erdos.math.unb.ca>


Roger Koenker wrote:

> One should doubt everything, even logic, and in doubt lies possibility.

If we doubt logic, discourse becomes impossible.

				cheers,

					Rolf Turner



From tblackw at umich.edu  Thu Dec 18 15:44:20 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Thu, 18 Dec 2003 09:44:20 -0500 (EST)
Subject: [R]: Lp norm estimation
In-Reply-To: <3FE1A77C.94AD6761@stats.uct.ac.za>
References: <3FE1A77C.94AD6761@stats.uct.ac.za>
Message-ID: <Pine.SOL.4.58.0312180941060.28478@mspacman.gpcc.itd.umich.edu>

Allan  -

Brian Ripley's implementation of one of the more useful Lp norms is:

library("lqs")
help("lqs")

This is very highly recommended for practical data analysis.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Thu, 18 Dec 2003, allan clark wrote:

> Hi all
>
> Just wondering whether one can undertake Lp norm estimation (a type of
> regression analysis) in R?
>
> i.e.
>
> argmin S ( | y(i) - x(i)b | ^p )
>
> where:
>
>    * S is the summation over observation i= 1,2,...,n
>    * y is a vector of n observations
>    * x is an n by p matrix of explanatory variables
>    * b is a p by 1 vector of beta coefficients and
>    * p is a constant to be estimated such that p>= 1
>
> Regards
> Allan
>



From mineoeli at unipa.it  Thu Dec 18 15:53:32 2003
From: mineoeli at unipa.it (Elio Mineo)
Date: Thu, 18 Dec 2003 15:53:32 +0100
Subject: [R]: Lp norm estimation
In-Reply-To: <3FE1A77C.94AD6761@stats.uct.ac.za>
References: <3FE1A77C.94AD6761@stats.uct.ac.za>
Message-ID: <3FE1BF6C.2070802@unipa.it>

Hi Allan,
I think the function lmp() of the package normalp could be useful to you.
Regards,
Elio

allan clark wrote:

>Hi all
>
>Just wondering whether one can undertake Lp norm estimation (a type of
>regression analysis) in R?
>
>i.e.
>
>argmin S ( | y(i) - x(i)b | ^p )
>
>where:
>
>   * S is the summation over observation i= 1,2,...,n
>   * y is a vector of n observations
>   * x is an n by p matrix of explanatory variables
>   * b is a p by 1 vector of beta coefficients and
>   * p is a constant to be estimated such that p>= 1
>
>Regards
>Allan
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>  
>



From Pascal.Niklaus at unibas.ch  Thu Dec 18 16:08:28 2003
From: Pascal.Niklaus at unibas.ch (Pascal A. Niklaus)
Date: Thu, 18 Dec 2003 16:08:28 +0100
Subject: [R] ESS tab completion for file names
In-Reply-To: <BC07240E.2D5B%sdavis2@mail.nih.gov>
References: <BC07240E.2D5B%sdavis2@mail.nih.gov>
Message-ID: <3FE1C2EC.5070905@unibas.ch>

Sean Davis wrote:

>I don't think that this works within the R process buffer, but I could be
>wrong.  Does the documentation say that it should somewhere?
>
>Sean
>
The ESS doc says (section 3.2):

Completion is also provided over file  names, which is particularly 
useful when using S functions such as get() or scan() (...) whenever the 
cursor is within an S string, pressing TAB completes the file name 
before point, and also expands any '~' or environment variable references.

So this should work, but maybe something is wrong with my configuration?

Pascal



From Pascal.Niklaus at unibas.ch  Thu Dec 18 16:17:37 2003
From: Pascal.Niklaus at unibas.ch (Pascal A. Niklaus)
Date: Thu, 18 Dec 2003 16:17:37 +0100
Subject: [R] prevent aov re-ordering of model terms
Message-ID: <3FE1C511.6070108@unibas.ch>

Is there a way to prevent the re-ordering of factors by aov? I do have a 
three-way interaction that I do want to fit before a two-way interaction 
(different factors, so they are not nested), but R moves the two-way 
interaction to the front. I know it generally makes sense to fit the 
two-way interactions first, but in this case I think I know what I'm doing.

                   + hv
                   + hv:rep
                   + hv:soil
                   + hv:spdiv
                   + hv:spdivnom
                   + hv:fgdiv
                   + hv:fgdivnom
                   + hv:soil:spdiv
                   + hv:soil:spdivnom
                   + hv:soil:fgdiv
                   + hv:soil:fgdivnom
                   + hv:mx                    #<-- that one here
                   + hv:soil:mx

mx is a random factor, and I do want to semi-manually test the three-way 
interactions against "hv:mx". soil is treated as fixed effect.

I know I can fit the first part, then save the residuals, and then fit 
the last two terms, but it would be really nice not to have to do this 
in two steps.

Pascal



From spencer.graves at pdf.com  Thu Dec 18 16:26:59 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 18 Dec 2003 07:26:59 -0800
Subject: [R] Summaries
In-Reply-To: <5AFDDD57E2771B409224CD858CC6DE0D02DAB347@mailer-e051.umh.es>
References: <5AFDDD57E2771B409224CD858CC6DE0D02DAB347@mailer-e051.umh.es>
Message-ID: <3FE1C743.2010005@pdf.com>

      If DF = a data.frame with 65000 rows and k columns, the following 
will do what I read in your question: 

      lapply(DF, table)

      See, e.g., Venables and Ripley, Modern Applied Statistics with S 
(Springer, pp. 33-34 in the 4th edition, 2002). 

      hope this helps. 

Perez Martin, Agustin wrote:

>Hello UseRs:
>
>Excuses for my english.
>I have a dataset with 65000 records and I'd like to make a summary where I
>can view all the values (with the number of times that it repeats) that
>there are each column of my dataset.
>I tried with summary( ), str( ), but nothing gives me the result that I am
>loking for.
>
>Thank you very much.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From rossini at blindglobe.net  Thu Dec 18 16:34:30 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Thu, 18 Dec 2003 07:34:30 -0800
Subject: [R] ESS tab completion for file names
In-Reply-To: <3FE1C2EC.5070905@unibas.ch> (Pascal A. Niklaus's message of
	"Thu, 18 Dec 2003 16:08:28 +0100")
References: <BC07240E.2D5B%sdavis2@mail.nih.gov> <3FE1C2EC.5070905@unibas.ch>
Message-ID: <85n09q9m2h.fsf@blindglobe.net>


Alternatively, C-c TAB might work.  

"Pascal A. Niklaus" <Pascal.Niklaus at unibas.ch> writes:

> Sean Davis wrote:
>
>>I don't think that this works within the R process buffer, but I could be
>>wrong.  Does the documentation say that it should somewhere?
>>
>>Sean
>>
> The ESS doc says (section 3.2):
>
> Completion is also provided over file  names, which is particularly
> useful when using S functions such as get() or scan() (...) whenever
> the cursor is within an S string, pressing TAB completes the file name
> before point, and also expands any '~' or environment variable
> references.
>
> So this should work, but maybe something is wrong with my configuration?
>
> Pascal
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From christian.hoffmann at wsl.ch  Thu Dec 18 16:38:28 2003
From: christian.hoffmann at wsl.ch (Christian Hoffmann)
Date: Thu, 18 Dec 2003 16:38:28 +0100
Subject: [R] symbolic and algebraic computation
Message-ID: <5.2.1.1.2.20031218163547.01ecf5a8@mail.wsl.ch>

There is also the open source program  Pari:

Quote:

I would like to announce the release of pari-2.2.7.ALPHA. The sources can be
obtained using the old address

   ftp://megrez.math.u-bordeaux.fr/pub/pari/pari-alpha.tgz

or (preferably) through the new website:

   http://pari.math.u-bordeaux.fr/download.html

where an updated Windows binary is available.

See also

Mailing-List: contact pari-announce-help at list.cr.yp.to; run by ezmlm
Delivered-To: mailing list pari-announce at list.cr.yp.to
Received: (qmail 1259 invoked from network); 18 Dec 2003 00:44:41 -0000
Received: from mathups.math.u-psud.fr (HELO matups.math.u-psud.fr) 
(129.175.52.4)
   by stoneport.math.uic.edu with SMTP; 18 Dec 2003 00:44:41 -0000
Received: from geo.math.u-psud.fr (geo.math.u-psud.fr [129.175.50.56])
           by matups.math.u-psud.fr (8.12.10/jtpda-5.4) with ESMTP id 
hBI0iG0s010962
           for <pari-announce at list.cr.yp.to>; Thu, 18 Dec 2003 01:44:16 
+0100 (MET)
Received: by geo.math.u-psud.fr (Postfix, from userid 20603)
         id 204992985B; Thu, 18 Dec 2003 01:44:16 +0100 (MET)
Date: Thu, 18 Dec 2003 01:44:16 +0100
From: Karim Belabas <Karim.Belabas at math.u-psud.fr>
To: pari-announce list <pari-announce at list.cr.yp.to>

Christian

-- 
Dr.sc.math.Christian W. 
Hoffmann,     http://www.wsl.ch/staff/christian.hoffmann
Mathematics + Statistical Computing   e-mail: hoffmacw at wsl.ch
Swiss Federal Research Institute WSL  Tel: ++41-1-739 22..  ..77  (self)
CH-8903 Birmensdorf, 
Switzerland                   ..11  (exchange)  ..15  (Fax)
Zuercherstrasse 111



From tlumley at u.washington.edu  Thu Dec 18 16:48:26 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 18 Dec 2003 07:48:26 -0800 (PST)
Subject: [R] mailing list for basic questions - preliminary sum up
In-Reply-To: <20031217171916.6629D398C@mprdmxin.myway.com>
References: <20031217171916.6629D398C@mprdmxin.myway.com>
Message-ID: <Pine.A41.4.58.0312180747380.19076@homer25.u.washington.edu>

On Wed, 17 Dec 2003, Gabor Grothendieck wrote:

>
>
> In rereading this one idea occurred to me.  What if the entire R help
> system were turned into a wiki?   That is,
>
> ?whatever
>
> would take you to the help page, but not on your computer --
> rather to the same page on the wiki.

This seems to assume that you are attached to the internet (or at least
that R can reliably tell if you are).

	-thomas



From tlumley at u.washington.edu  Thu Dec 18 16:53:20 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 18 Dec 2003 07:53:20 -0800 (PST)
Subject: [R] beginner programming question
In-Reply-To: <5.2.1.1.2.20031217152631.041e0e38@mailhost.blackmesacapital.com>
References: <5.2.1.1.2.20031217152631.041e0e38@mailhost.blackmesacapital.com>
Message-ID: <Pine.A41.4.58.0312180752510.19076@homer25.u.washington.edu>

On Wed, 17 Dec 2003, Tony Plate wrote:

> Another way to approach this is to first massage the data into a more
> regular format.  This may or may not be simpler or faster than other
> solutions suggested.

You could also use the reshape() command to do the massaging

	-thomas

>  > x <- read.table("clipboard", header=T)
>  > x
>    rel1 rel2 rel3 age0 age1 age2 age3 sex0 sex1 sex2 sex3
> 1    1    3   NA   25   23    2   NA    1    2    1   NA
> 2    4    1    3   35   67   34   10    2    2    1    2
> 3    1    4    4   39   40   59   60    1    2    2    1
> 4    4   NA   NA   45   70   NA   NA    2    2   NA   NA
>  > nn <- c("rel","age0","age","sex0","sex")
>  > xx <- rbind("colnames<-"(x[,c("rel1","age0","age1","sex0","sex1")], nn),
> +  "colnames<-"(x[,c("rel2","age0","age2","sex0","sex2")], nn),
> +  "colnames<-"(x[,c("rel3","age0","age3","sex0","sex3")], nn))
>  > xx
>     rel age0 age sex0 sex
> 1    1   25  23    1   2
> 2    4   35  67    2   2
> 3    1   39  40    1   2
> 4    4   45  70    2   2
> 11   3   25   2    1   1
> 21   1   35  34    2   1
> 31   4   39  59    1   2
> 41  NA   45  NA    2  NA
> 12  NA   25  NA    1  NA
> 22   3   35  10    2   2
> 32   4   39  60    1   1
> 42  NA   45  NA    2  NA
>  >
>  > rbind(subset(xx, xx$rel==1 & (xx$sex0==1 |
> xx$sex0==xx$sex))[,c("age0","age")], subset(xx, xx$rel==1 & xx$sex==1 &
> xx$sex0!=xx$sex)[,c("age","age0")])
>     age0 age
> 1    25  23
> 3    39  40
> 21   35  34
>  >
>
> hope this helps,
>
> Tony Plate
>
> PS.  To advanced R users: Is the above usage of the "colnames<-" function
> within an expression regarded as acceptable or as undesirable programming
> style? -- I've rarely seen it used, but it can be quite useful.
>
> At Wednesday 09:28 PM 12/17/2003 +0200, Adrian Dusa wrote:
> >Hi all,
> >
> >
> >
> >The last e-mails about beginners gave me the courage to post a question;
> >from a beginner's perspective, there are a lot of questions that I'm
> >tempted to ask. But I'm trying to find the answers either in the
> >documentation, either in the about 15 free books I have, either in the
> >help archives (I often found many similar questions posted in the past).
> >
> >Being an (still actual) user of SPSS, I'd like to be able to do
> >everything in R. I've learned that the best way of doing it is to
> >struggle and find a solution no matter what, refraining from doing it
> >with SPSS. I've became more and more aware of the almost unlimited
> >possibilities that R offers and I'd like to completely switch to R
> >whenever I think I'm ready.
> >
> >
> >
> >I have a (rather theoretical) programming problem for which I have found
> >a solution, but I feel it is a rather poor one. I wonder if there's some
> >other (more clever) solution, using (maybe?) vectorization or
> >subscripting.
> >
> >
> >
> >A toy example would be:
> >
> >
> >
> >rel1       rel2       rel3       age0     age1     age2     age3
> >sex0     sex1     sex2     sex3
> >
> >1          3          NA        25         23         2          NA
> >1          2          1          NA
> >
> >4          1          3          35         67         34         10
> >2          2          1          2
> >
> >1          4          4          39         40         59         60
> >1          2          2          1
> >
> >4          NA        NA        45         70         NA        NA
> >2          2          NA        NA
> >
> >
> >
> >where rel1...3 states the kinship with the respondent (person 0)
> >
> >code 1 meaning husband/wife, code 4 meaning parent and code 3 for
> >children.
> >
> >
> >
> >I would like to get the age for husbands (code 1) in a first column and
> >wife's age in the second:
> >
> >
> >
> >ageh     agew
> >
> >25         23
> >
> >34         35
> >
> >39         40
> >
> >
> >
> >My solution uses *for* loops and *if*s checking for code 1 in each
> >element in the first 3 columns, then checking in the last three columns
> >for husband's code, then taking the corresponding age in a new matrix.
> >I've learned that *for* loops are very slow (and indeed with my dataset
> >of some 2000 rows and 13 columns for kinship it takes quite a lot).
> >
> >I found the "Looping" chapter in "S poetry" very useful (it did saved me
> >from *for* loops a couple of times, thanks!).
> >
> >
> >
> >Any hints would be appreciated,
> >
> >Adrian
> >
> >
> >
> >~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> >Adrian Dusa (adi at roda.ro)
> >Romanian Social Data Archive (www.roda.ro <http://www.roda.ro/> )
> >1, Schitu Magureanu Bd.
> >76625 Bucharest sector 5
> >Romania
> >
> >
> >Tel./Fax:
> >
> >+40 (21) 312.66.18\
> >
> >+40 (21) 312.02.10/ int.101
> >
> >
> >
> >
> >         [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From tlumley at u.washington.edu  Thu Dec 18 16:58:29 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 18 Dec 2003 07:58:29 -0800 (PST)
Subject: [R] mailing list for basic questions - preliminary sum up
In-Reply-To: <74E242B6968AA0469B632C5A3EFC1EFD03D5701F@nt207mesep.corporate.hdwa.health.wa.gov.au>
References: <74E242B6968AA0469B632C5A3EFC1EFD03D5701F@nt207mesep.corporate.hdwa.health.wa.gov.au>
Message-ID: <Pine.A41.4.58.0312180755540.19076@homer25.u.washington.edu>

On Thu, 18 Dec 2003, Mulholland, Tom wrote:
>
> 1 An assumption on my part is that there is fundamental agreement that
> the document is the best source for advice on how to ask questions of
> this list

I don't think its the best possible source. It may well be the best
existing source.

On bug reporting, I think Simon Tatham's essay at
http://www.chiark.greenend.org.uk/~sgtatham/bugs.html
is a definite improvement over the GNU emacs-based text in the FAQ. It
makes basically the same points, but explains *why*.

	-thomas



From tlumley at u.washington.edu  Thu Dec 18 17:04:54 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 18 Dec 2003 08:04:54 -0800 (PST)
Subject: [R] NA, deleting rows
In-Reply-To: <3FE1B398.9080200@ceam.es>
References: <3FE1B398.9080200@ceam.es>
Message-ID: <Pine.A41.4.58.0312180803160.19076@homer25.u.washington.edu>

On Thu, 18 Dec 2003, juli g. pausas wrote:

> Dear colleges,
> I do not understand the following behaviour:
>
> > aa <- data.frame(a1= 1:10, a2= c(rep(NA, 5), 1:5) )

> > aa[!aa$a2==1, ]  # removing rows with a2==1
>      a1 a2
> NA   NA NA
> NA.1 NA NA
> NA.2 NA NA
> NA.3 NA NA
> NA.4 NA NA
> 7     7  2
> 8     8  3
> 9     9  4
> 10   10  5
>
> I didn't expect a1 to be affected.

You should think of NA as being pronounced "Don't Know".  That is you are
asking for all rows where a2==1 to be removed and you don't know whether
to remove the first five rows. The result is that you don't know what the
result is in the first five rows.

There is a good case for making this give an error or warning.

	-thomas



From ggrothendieck at myway.com  Thu Dec 18 17:16:42 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 18 Dec 2003 11:16:42 -0500 (EST)
Subject: [R] mailing list for basic questions - preliminary sum up
Message-ID: <20031218161642.62B513949@mprdmxin.myway.com>



I had made a second subsequent suggestion to address this.  See:

https://www.stat.math.ethz.ch/pipermail/r-help/2003-December/042234.html



--- 
Date: Thu, 18 Dec 2003 07:48:26 -0800 (PST) 
From: Thomas Lumley <tlumley at u.washington.edu>
To: Gabor Grothendieck <ggrothendieck at myway.com> 
Cc: <feh3k at spamcop.net>, <r-help at stat.math.ethz.ch> 
Subject: Re: [R] mailing list for basic questions - preliminary sum up 

 
 
On Wed, 17 Dec 2003, Gabor Grothendieck wrote:

>
>
> In rereading this one idea occurred to me. What if the entire R help
> system were turned into a wiki? That is,
>
> ?whatever
>
> would take you to the help page, but not on your computer --
> rather to the same page on the wiki.

This seems to assume that you are attached to the internet (or at least
that R can reliably tell if you are).

     -thomas



From ripley at stats.ox.ac.uk  Thu Dec 18 17:23:45 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 18 Dec 2003 16:23:45 +0000 (GMT)
Subject: [R] prevent aov re-ordering of model terms
In-Reply-To: <3FE1C511.6070108@unibas.ch>
Message-ID: <Pine.LNX.4.44.0312181611080.24354-100000@gannet.stats>

The key is the `keep.order' argument to terms.formula:

> options(contrasts=c("contr.helmert", "contr.poly"))
> library(MASS)
> aov(yield ~ block + N * P + K, npk)
Call:
   aov(formula = yield ~ block + N * P + K, data = npk)

Terms:
                   block        N        P        K      N:P Residuals
Sum of Squares  343.2950 189.2817   8.4017  95.2017  21.2817  218.9033
Deg. of Freedom        5        1        1        1        1        14

Residual standard error: 3.954232
Estimated effects are balanced
> aov(terms(yield ~ block + N * P + K, keep.order=TRUE), npk)
Call:
   aov(formula = terms(yield ~ block + N * P + K, keep.order = TRUE),
    data = npk)

Terms:
                   block        N        P      N:P        K Residuals
Sum of Squares  343.2950 189.2817   8.4017  21.2817  95.2017  218.9033
Deg. of Freedom        5        1        1        1        1        14

Residual standard error: 3.954232
Estimated effects are balanced

Now, I wonder how I knew that ... oh yes, it is in the vr.glm demo.


On Thu, 18 Dec 2003, Pascal A. Niklaus wrote:

> Is there a way to prevent the re-ordering of factors by aov? I do have a 
> three-way interaction that I do want to fit before a two-way interaction 
> (different factors, so they are not nested), but R moves the two-way 
> interaction to the front. I know it generally makes sense to fit the 
> two-way interactions first, but in this case I think I know what I'm doing.
> 
>                    + hv
>                    + hv:rep
>                    + hv:soil
>                    + hv:spdiv
>                    + hv:spdivnom
>                    + hv:fgdiv
>                    + hv:fgdivnom
>                    + hv:soil:spdiv
>                    + hv:soil:spdivnom
>                    + hv:soil:fgdiv
>                    + hv:soil:fgdivnom
>                    + hv:mx                    #<-- that one here
>                    + hv:soil:mx
> 
> mx is a random factor, and I do want to semi-manually test the three-way 
> interactions against "hv:mx". soil is treated as fixed effect.
> 
> I know I can fit the first part, then save the residuals, and then fit 
> the last two terms, but it would be really nice not to have to do this 
> in two steps.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bbvaughn at bellsouth.net  Thu Dec 18 17:52:25 2003
From: bbvaughn at bellsouth.net (bbvaughn@bellsouth.net)
Date: Thu, 18 Dec 2003 16:52:25 +0000
Subject: [R] Resampling Stats software
Message-ID: <20031218165225.TBDI7487.imf21aec.mail.bellsouth.net@mail.bellsouth.net>

Brian,

Thanks so much for your comments.  Like I said, I am pretty much a novice at this idea.  My statistics degree emphasized a lot of theoretical knowledge, and the idea of simulation and resampling was never taught.  So, now that I am more in an applied field, I realize the need to educate myself.  I was taught SAS, and my current field stresses SPSS.  But a good friend recommended I check out R, so I'm trying to get into that now.

A few questions for you though.

> An introduction to what?  (It seems to confuse resampling and
> simulation-based inference.)
> 

Since I am new at this, could your clarify this just a bit?  Do you think this is a poor book?  Is resampling more to do with sampling from actual sample data?  (I could see how a lot of examples in this book are more based on simulation, and then called "resampling.")

It seems of interest the emphasis that statistics should be taught from this perspective (of simulating everything).  I'm not sure I agree with it totally though, because it always seems like you will get approximations all the time, where as probability formulas (if correctly specified and applied) will give you more exact measures.  It seems to possibly "dumb down" the understanding of statistics.  But then again, simulation/resampling does have the potential of answering questions for which there are no mathematical models.

What is your take on this?  Is a balance better?  I've used simulation before in teaching, but always to demonstrate theoretical knowledge (like the Central Limit Theorem).  This is the first I have seen of an emphasis on simulation as the goal of teaching.

> As a matter of terminology, this is not resampling as usually defined, so 
> I do wonder exactly what it is you are after.  For resampling in the usual 
> sense, I would echo Jason's recommendation of Davison and Hinkley's CUP 
> book.

What I'm after?  I wish I really knew!  :)

Let me see ... basically, I'm trying to understand the practical side of resampling.  I'm trying to get a basic balance of how all these differ ... such as simulation vs. resampling vs. bootstrap vs. Monte Carlo methods vs. Markov Chain Monte Carlo ... etc.  I'm trying to get a very practical knowledge before digging into the theory, but understand they probably will both come at the same time.

I want to be able to do statistical research using simulation.  There really aren't any specific goals in mind.  I'm just a learner.  I would like to do some research on robustness of certain educational measurement scales, and I'm sure this would involve simulation.

Does that make any sense?  Thanks for your help and suggestions.  I will check out that book for sure.

Brandon Vaughn
Chipola College



From brown_ct at yahoo.com  Thu Dec 18 18:24:59 2003
From: brown_ct at yahoo.com (Christopher Brown)
Date: Thu, 18 Dec 2003 09:24:59 -0800 (PST)
Subject: [R] Collapsing Arrays/Lists to scalar values
Message-ID: <20031218172459.72119.qmail@web41509.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031218/c5f6ec05/attachment.pl

From gavin.simpson at ucl.ac.uk  Thu Dec 18 19:02:07 2003
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Thu, 18 Dec 2003 18:02:07 +0000
Subject: [R] R GUI dies using postcript() in Windows XP Pro
Message-ID: <3FE1EB9F.8080709@ucl.ac.uk>

Dear List,

My colleague has been having a problem with the following data and 
plotting commands. The example below is part of a larger set of plots, 
but I've isolated the problem to this example using this small dataset 
(below), which kills rgui consistently. My version info

 > version
          _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    1
minor    8.0
year     2003
month    10
day      08
language R

But this also happens in 1.8.1 on Windows XP Pro

The code seems to stop at the call to plot() in the code below, and then 
MS's error reporting window pops up and informs us that rgui has been 
closed.

Note that the same code, but without the postcript() and dev.off() 
commands works as expected to produce the plot on a windows device.

Any ideas as to what is going on?

All the best,

Gav

#code to run to reproduce the problem
year <- c(1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 
1998, 1999, 2000, 2001, 2002)

avgMsr <- c(10.800000, 7.000000, 6.200000, 9.571429, 10.800000, 
11.600000, 10.200000, 11.400000, 7.200000, 9.400000, 11.200000, 
11.600000, 11.600000, 8.000000, 7.600000)

totMsr <- c(19, 17, 13, 24, 17, 17, 21, 18, 13, 15, 19, 18, 21, 15, 11)

postscript(file="invert.eps", onefile=FALSE, horizontal = FALSE, 
pointsize = 8)

op <- par(mar = c(3,4,1.5,1)+0.1,font.main=16,tcl=-0.2)
plot(x = year, y = avgMsr, ylab = "Species richness", main = "Loch Coire 
nan Arr", axes = FALSE, ylim = c(0,25), xlim = c(1988,2002))
axis(2, las=1)
axis(1)
lines(x = year, y = avgMsr)
points(x = year, y = totMsr , col = "black", pch = 19, type = "o")
lines(x = year, y = totMsr)
box()
par(op)
dev.off()
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpson at ucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From abunn at montana.edu  Thu Dec 18 19:14:07 2003
From: abunn at montana.edu (Andy Bunn)
Date: Thu, 18 Dec 2003 11:14:07 -0700
Subject: [R] R GUI dies using postcript() in Windows XP Pro
In-Reply-To: <3FE1EB9F.8080709@ucl.ac.uk>
Message-ID: <003f01c3c592$cece4eb0$78f05a99@msu.montana.edu>

The font.main = 16  in par() is having problems cooperating with
pointsize = 8 in postscript().

One of them has to go and then the code is happy. This is probably a
feature.

HTH, Andy

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gavin Simpson
Sent: Thursday, December 18, 2003 11:02 AM
To: R-Help
Subject: [R] R GUI dies using postcript() in Windows XP Pro


Dear List,

My colleague has been having a problem with the following data and 
plotting commands. The example below is part of a larger set of plots, 
but I've isolated the problem to this example using this small dataset 
(below), which kills rgui consistently. My version info

 > version
          _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    1
minor    8.0
year     2003
month    10
day      08
language R

But this also happens in 1.8.1 on Windows XP Pro

The code seems to stop at the call to plot() in the code below, and then

MS's error reporting window pops up and informs us that rgui has been 
closed.

Note that the same code, but without the postcript() and dev.off() 
commands works as expected to produce the plot on a windows device.

Any ideas as to what is going on?

All the best,

Gav

#code to run to reproduce the problem
year <- c(1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 
1998, 1999, 2000, 2001, 2002)

avgMsr <- c(10.800000, 7.000000, 6.200000, 9.571429, 10.800000, 
11.600000, 10.200000, 11.400000, 7.200000, 9.400000, 11.200000, 
11.600000, 11.600000, 8.000000, 7.600000)

totMsr <- c(19, 17, 13, 24, 17, 17, 21, 18, 13, 15, 19, 18, 21, 15, 11)

postscript(file="invert.eps", onefile=FALSE, horizontal = FALSE, 
pointsize = 8)

op <- par(mar = c(3,4,1.5,1)+0.1,font.main=16,tcl=-0.2)
plot(x = year, y = avgMsr, ylab = "Species richness", main = "Loch Coire

nan Arr", axes = FALSE, ylim = c(0,25), xlim = c(1988,2002)) axis(2,
las=1)
axis(1)
lines(x = year, y = avgMsr)
points(x = year, y = totMsr , col = "black", pch = 19, type = "o")
lines(x = year, y = totMsr)
box()
par(op)
dev.off()
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpson at ucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From dmurdoch at pair.com  Thu Dec 18 19:17:54 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Thu, 18 Dec 2003 13:17:54 -0500
Subject: [R] R GUI dies using postcript() in Windows XP Pro
In-Reply-To: <3FE1EB9F.8080709@ucl.ac.uk>
References: <3FE1EB9F.8080709@ucl.ac.uk>
Message-ID: <ijr3uv0chs9prgc1g5n9v5h30doutqeltl@4ax.com>

On Thu, 18 Dec 2003 18:02:07 +0000, Gavin Simpson
<gavin.simpson at ucl.ac.uk> wrote :

>Dear List,
>
>My colleague has been having a problem with the following data and 
>plotting commands. The example below is part of a larger set of plots, 
>but I've isolated the problem to this example using this small dataset 
>(below), which kills rgui consistently. My version info
>
> > version
>          _
>platform i386-pc-mingw32
>arch     i386
>os       mingw32
>system   i386, mingw32
>status
>major    1
>minor    8.0
>year     2003
>month    10
>day      08
>language R
>
>But this also happens in 1.8.1 on Windows XP Pro

I just tried it in 1.8.1, Win XP Pro, with no error, then tried in a
relatively recent build of r-patched and saw the crash.  I'll see if I
can track it down.

Duncan Murdoch
>
>The code seems to stop at the call to plot() in the code below, and then 
>MS's error reporting window pops up and informs us that rgui has been 
>closed.
>
>Note that the same code, but without the postcript() and dev.off() 
>commands works as expected to produce the plot on a windows device.
>
>Any ideas as to what is going on?
>
>All the best,
>
>Gav
>
>#code to run to reproduce the problem
>year <- c(1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 
>1998, 1999, 2000, 2001, 2002)
>
>avgMsr <- c(10.800000, 7.000000, 6.200000, 9.571429, 10.800000, 
>11.600000, 10.200000, 11.400000, 7.200000, 9.400000, 11.200000, 
>11.600000, 11.600000, 8.000000, 7.600000)
>
>totMsr <- c(19, 17, 13, 24, 17, 17, 21, 18, 13, 15, 19, 18, 21, 15, 11)
>
>postscript(file="invert.eps", onefile=FALSE, horizontal = FALSE, 
>pointsize = 8)
>
>op <- par(mar = c(3,4,1.5,1)+0.1,font.main=16,tcl=-0.2)
>plot(x = year, y = avgMsr, ylab = "Species richness", main = "Loch Coire 
>nan Arr", axes = FALSE, ylim = c(0,25), xlim = c(1988,2002))
>axis(2, las=1)
>axis(1)
>lines(x = year, y = avgMsr)
>points(x = year, y = totMsr , col = "black", pch = 19, type = "o")
>lines(x = year, y = totMsr)
>box()
>par(op)
>dev.off()



From ripley at stats.ox.ac.uk  Thu Dec 18 19:20:16 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 18 Dec 2003 18:20:16 +0000 (GMT)
Subject: [R] R GUI dies using postcript() in Windows XP Pro
In-Reply-To: <3FE1EB9F.8080709@ucl.ac.uk>
Message-ID: <Pine.LNX.4.44.0312181817410.27840-100000@gannet.stats>

What do you think font.main=16 does?  postscript() only has 5 fonts.

Fortunately your example also crashes on Linux so was fairly easy to track 
down.

The windows devices do have more fonts (19 by default).

On Thu, 18 Dec 2003, Gavin Simpson wrote:

> Dear List,
> 
> My colleague has been having a problem with the following data and 
> plotting commands. The example below is part of a larger set of plots, 
> but I've isolated the problem to this example using this small dataset 
> (below), which kills rgui consistently. My version info
> 
>  > version
>           _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    1
> minor    8.0
> year     2003
> month    10
> day      08
> language R
> 
> But this also happens in 1.8.1 on Windows XP Pro
> 
> The code seems to stop at the call to plot() in the code below, and then 
> MS's error reporting window pops up and informs us that rgui has been 
> closed.
> 
> Note that the same code, but without the postcript() and dev.off() 
> commands works as expected to produce the plot on a windows device.
> 
> Any ideas as to what is going on?
> 
> All the best,
> 
> Gav
> 
> #code to run to reproduce the problem
> year <- c(1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 
> 1998, 1999, 2000, 2001, 2002)
> 
> avgMsr <- c(10.800000, 7.000000, 6.200000, 9.571429, 10.800000, 
> 11.600000, 10.200000, 11.400000, 7.200000, 9.400000, 11.200000, 
> 11.600000, 11.600000, 8.000000, 7.600000)
> 
> totMsr <- c(19, 17, 13, 24, 17, 17, 21, 18, 13, 15, 19, 18, 21, 15, 11)
> 
> postscript(file="invert.eps", onefile=FALSE, horizontal = FALSE, 
> pointsize = 8)
> 
> op <- par(mar = c(3,4,1.5,1)+0.1,font.main=16,tcl=-0.2)
> plot(x = year, y = avgMsr, ylab = "Species richness", main = "Loch Coire 
> nan Arr", axes = FALSE, ylim = c(0,25), xlim = c(1988,2002))
> axis(2, las=1)
> axis(1)
> lines(x = year, y = avgMsr)
> points(x = year, y = totMsr , col = "black", pch = 19, type = "o")
> lines(x = year, y = totMsr)
> box()
> par(op)
> dev.off()
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From gavin.simpson at ucl.ac.uk  Thu Dec 18 19:54:02 2003
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Thu, 18 Dec 2003 18:54:02 +0000
Subject: [R] R GUI dies using postcript() in Windows XP Pro
In-Reply-To: <003f01c3c592$cece4eb0$78f05a99@msu.montana.edu>
References: <003f01c3c592$cece4eb0$78f05a99@msu.montana.edu>
Message-ID: <3FE1F7CA.7070506@ucl.ac.uk>

Dear Andy, Duncan and Brian

Cheers for your replies

That font.main thing is something I didn't pick up on when I cropped my 
colleagues code down to a small example.

font.main works fine with pointsize = 8, *if* you set font.main to one 
of the values given in the entry for font in ?par. The code example I 
posted works if I set font.main to 3 (for italics) for example, which is 
what was produced with font.main = 16 on a windows() device.

And Brian has confirmed that postcript devices have only 5 fonts in R.

Not really a /feature/ though, I'd expect an error message telling me to 
not be so stupid next time... ;-)

Once again, thank you all for your replies.

Gav
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpson at ucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From ripley at stats.ox.ac.uk  Thu Dec 18 19:58:09 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 18 Dec 2003 18:58:09 +0000 (GMT)
Subject: [R] R GUI dies using postcript() in Windows XP Pro
In-Reply-To: <3FE1F7CA.7070506@ucl.ac.uk>
Message-ID: <Pine.LNX.4.44.0312181856490.28057-100000@gannet.stats>

What is intended to happen is for it silently to use font 1.  I am 
committing a fix to ensure that happens in the one spot that got missed.

On Thu, 18 Dec 2003, Gavin Simpson wrote:

> Dear Andy, Duncan and Brian
> 
> Cheers for your replies
> 
> That font.main thing is something I didn't pick up on when I cropped my 
> colleagues code down to a small example.
> 
> font.main works fine with pointsize = 8, *if* you set font.main to one 
> of the values given in the entry for font in ?par. The code example I 
> posted works if I set font.main to 3 (for italics) for example, which is 
> what was produced with font.main = 16 on a windows() device.
> 
> And Brian has confirmed that postcript devices have only 5 fonts in R.
> 
> Not really a /feature/ though, I'd expect an error message telling me to 
> not be so stupid next time... ;-)
> 
> Once again, thank you all for your replies.
> 
> Gav
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bates at stat.wisc.edu  Thu Dec 18 20:24:20 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: 18 Dec 2003 13:24:20 -0600
Subject: [R] LotR:RotK + Linux 2.6.0 on the same day
Message-ID: <6rk74u6iaj.fsf@bates4.stat.wisc.edu>

Wednesday saw the release of both the movie "Lord of the Rings: Return
of the King" (LotR:RotK) and the Linux 2.6.0 kernel.  Those who enjoy
geek humor should read the discussion on slashdot.org from people who are
lost in the agonies of trying to decide whether to go see the movie or
to compile the new kernel first.  

I have been running 2.6.0-test11 on a machine for several weeks and it
is very good.  The configuration/compiling process is much cleaner
than in previous versions of the kernel and I really like the support
for ATAPI optical drives.



From pauljohn at ku.edu  Thu Dec 18 20:27:39 2003
From: pauljohn at ku.edu (Paul Johnson)
Date: Thu, 18 Dec 2003 13:27:39 -0600
Subject: [R] diagnostic information in glm. How about N of missing
	observations?
Message-ID: <3FE1FFAB.8010709@ku.edu>

I handed out some results from glm() and the students ask "how many 
observations were dropped due to missing values"?

How would I know? 

In other stat programs, the results will typically include N and the 
number dropped because of missings.  Without going back to R and 
fiddling about to find the total number of rows in the dataframe, there 
is no way to tell.  Somewhat inconvenient. Do you agree?

-- 
Paul E. Johnson                       email: pauljohn at ku.edu
Dept. of Political Science            http://lark.cc.ku.edu/~pauljohn
1541 Lilac Lane, Rm 504                              
University of Kansas                  Office: (785) 864-9086
Lawrence, Kansas 66044-3177           FAX: (785) 864-5700



From rolf at math.unb.ca  Thu Dec 18 20:44:40 2003
From: rolf at math.unb.ca (Rolf Turner)
Date: Thu, 18 Dec 2003 15:44:40 -0400 (AST)
Subject: [R] diagnostic information ....
Message-ID: <200312181944.hBIJieYW006824@erdos.math.unb.ca>


Paul E. Johnson wrote:

> I handed out some results from glm() and the students ask "how many 
> observations were dropped due to missing values"?
> 
> How would I know? 
> 
> In other stat programs, the results will typically include N and
> the number dropped because of missings.  Without going back to R and
> fiddling about to find the total number of rows in the dataframe,
> there is no way to tell.  Somewhat inconvenient. Do you agree?

In a word:  No.  R is ``not like other packages'' which spew out
enormous printouts including every statistic known to man and a few
known only to woman.  R basically gives you what you ask for, and
assumes you know enough to know what to ask for.  It does not
condescendingly make decisions for you and hand-cuff you into doing a
standard analysis.

It also allows infinitely versatile customization with consumate
ease since it is a beautifully designed programming language.  It
would be ***VERY*** easy to write a wrapper for glm() to include
the number of dropped observations if you want to include that
information in a ``printout''.

				cheers,

					Rolf Turner
					rolf at math.unb.ca



From tura at centroin.com.br  Thu Dec 18 20:42:52 2003
From: tura at centroin.com.br (Bernardo Rangel Tura)
Date: Thu, 18 Dec 2003 17:42:52 -0200
Subject: [R] Fitdistr Error in new VR version
In-Reply-To: <Pine.A41.4.58.0312121453470.31896@homer38.u.washington.edu
 >
References: <8C585FEB9085D6119A4C0002A5EB792A0D6F89@SIDNEY>
	<Pine.A41.4.58.0312121453470.31896@homer38.u.washington.edu>
Message-ID: <6.0.1.1.2.20031218173833.02b44eb0@pop.centroin.com.br>


>Hi R-Masters

I found a strange error in fitdistr():


In case of VR Version 7.1-11

k21stsList<-c(0.76697,0.57642,0.75938,0.82616,0.93706,0.77377,0.58923,0.37157,0.60796,1.00070,0.97529,0.62858,0.63504,0.68697,0.61714,0.75227,1.16390,0.66702,0.83578)

fitdistr(k21stsList, "normal",list(mean = 0.5, sd = 0.1))
     mean          sd    
 0.74584591   0.17908744 
(0.04108548) (0.02904255)
 

In case of VR Version 7.1-13

k21stsList<-c(0.76697,0.57642,0.75938,0.82616,0.93706,0.77377,0.58923,0.37157,0.60796,1.00070,0.97529,0.62858,0.63504,0.68697,0.61714,0.75227,1.16390,0.66702,0.83578)

fitdistr(k21stsList, "normal",list(mean = 0.5, sd = 0.1))
     mean          sd    
 0.74584591   0.17908744 
(0.04108548) (0.02904255)

Error in fitdistr(k21stsList, "normal", list(mean = 0.5, sd = 0.1)) : 
        supplying pars for the Normal is not supported


Why this occur?

[]s
Tura 
-------------- next part --------------

---




From rpeng at jhsph.edu  Thu Dec 18 21:02:02 2003
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Thu, 18 Dec 2003 15:02:02 -0500
Subject: [R] diagnostic information in glm. How about N of missing
	observations?
In-Reply-To: <3FE1FFAB.8010709@ku.edu>
References: <3FE1FFAB.8010709@ku.edu>
Message-ID: <3FE207BA.2010806@jhsph.edu>

There should be an element of the fitted model object called `na.action' 
which (if you used the default na.omit()) should tell you which 
observations were dropped.

-roger

Paul Johnson wrote:
> I handed out some results from glm() and the students ask "how many 
> observations were dropped due to missing values"?
> 
> How would I know?
> In other stat programs, the results will typically include N and the 
> number dropped because of missings.  Without going back to R and 
> fiddling about to find the total number of rows in the dataframe, there 
> is no way to tell.  Somewhat inconvenient. Do you agree?
>



From f.mattes at ucl.ac.uk  Thu Dec 18 21:44:07 2003
From: f.mattes at ucl.ac.uk (Frank Mattes)
Date: Thu, 18 Dec 2003 20:44:07 +0000
Subject: [R] help installing Design package OS-X
Message-ID: <p05210600bc07bfff340c@[128.40.218.142]>

I'm wondering if someone has installed the Design library on OS-X.
I have the Developer tools installed (not the OS X SDK kit) but after 
downloading the library via RAqua package manager I get the following 
errors:

Installation of package Design had a non-zero exit status in :
install.packages(ui.pks, CRAN=
getOption(where), lib =
.libPath()[1]

or
>  install.fom.file() npackage
>  installatiion failed

in case someone has the binary package for OS-X , please let me know.
frank mattes

-- 
Frank Mattes, MD			e-mail:	f.mattes at ucl.ac.uk



From LindnerW at t-online.de  Thu Dec 18 21:48:51 2003
From: LindnerW at t-online.de (Wolfgang Lindner)
Date: Thu, 18 Dec 2003 21:48:51 +0100
Subject: [R] symbolic and algebraic computation
References: <LAW11-OE15FI1ZG5yoV0000cdbd@hotmail.com>
Message-ID: <1AX54l-04JHXM0@fwd07.sul.t-online.com>

Dear Angel,

> I needed a symbolic and algebraic computation software. The best solution
> I've found so far is maxima computer algebra system.

Maybe there is also the CAS MuPAD of interest, which has the look and feel of 
maple, works under Linux, Mac and Windows - and has a free academic version, 
called MuPAD-Light. This free version is mathematical fully functional, comes 
with full documentation, but has only a rudimentary GUI, e.g. only allows to 
edit the last line - so you has sometimes to copy and past ... ;-) 
Give it a try.

Here are the pointers:

in general:
http://www.mupad.de/
http://www.mupad.de/schule/en/

Download, approx. 20 MB:
http://www.mupad.de/schule+studium/download/mupad_light_250_win.exe
http://www.mupad.de/schule+studium/download/mupad_light_250_linux_bin.tgz

30-days-eval of profi version:
http://www.mupad.de/schule+studium/download/mupad_pro_de_250_win.exe
http://www.mupad.de/schule+studium/download/mupad_pro_252_macosx.img.gz

best whishes
          Wolfgang
--
Wolfgang Lindner                     Lindner at math.uni-duisburg.de
Universit?t Duisburg-Essen, Campus Duisburg
Fakultaet 4 - Institut fuer Mathematik, Gebaeude: LE  Zimmer: 424
Lotharstr. 65                              Tel: +49 0203 379-1326
D-47048  Duisburg                          Fax: +49 0203 379-2528



From jarrod.hadfield at imperial.ac.uk  Thu Dec 18 21:55:24 2003
From: jarrod.hadfield at imperial.ac.uk (Jarrod Hadfield)
Date: Thu, 18 Dec 2003 20:55:24 +0000
Subject: [R] mclust - clustering by spatial patterns
Message-ID: <a06010200bc07c2bc20b6@[129.31.3.147]>

Dear All,

I have spatial data (presence/absence for 4000 squares) on 250 bird 
species and would like to use a model-based clustering technique to 
test for species associations.  Is there any way of passing a 
distance/correlation matrix to mclust as with hclust, rather than the 
actual data?  Or alternatively, is there a way of getting mclust to 
handle binary data?

I'd appreciate any suggestions!

Cheers,

Jarrod



From yves.oloui at free.fr  Thu Dec 18 22:09:54 2003
From: yves.oloui at free.fr (Yves)
Date: Thu, 18 Dec 2003 22:09:54 +0100
Subject: [R] Help with  predict.Arima with external regressor values
Message-ID: <JPELJKOEGFEPPGDABMNAAENLCAAA.yves.oloui@free.fr>

Hi all there

I am enjoying R since 2 weeks and I come to my first deadlock, il am trying
to use predict.Arima in the ts package.
I get a "Error in cbind(...) : cannot create a matrix from these types"

-- Start R session -----------------------------------------------------

> fitdiv <- arima(data, c(2, 0, 3), xreg = y ) ; print(fitdiv)

Call:
arima(x = data, order = c(2, 0, 3), xreg = y)

Coefficients:
         ar1     ar2     ma1      ma2      ma3  intercept   EUSA1  EUSA10
EUSA15  EUSA20    EUSA5  H15T10Y   H15T1Y  H15T20Y  H15T3M   H15T5Y   USSW10
      -0.001  0.6502  0.3328  -0.5021  -0.1135    -0.0535  0.0469
0.0075  -0.0263  0.0299  -0.0344   0.1012  -0.0382   0.0092
0.0385  -0.0757  -0.1577
s.e.   0.523  0.4002  0.5262   0.2711   0.0828     0.1027  0.0308  0.0802
0.0931  0.0743   0.0414   0.0469   0.0215   0.0360  0.0276   0.0344   0.0477
      USSW15   USSW20  USSW30   USSW5  CAC.INDEX  DAX.INDEX  MIB30.INDEX
OMX.INDEX  SX5P.INDEX  UKX.INDEX  VDAX.INDEX  VIX.INDEX
      0.0254  -0.0141  0.0133  0.1186    -0.1816     0.0652
   0.0848    -0.1836      0.1134    -0.1742      0.0236    -0.0482
s.e.  0.0588   0.0251  0.0363  0.0278     0.0907     0.0528       0.0860
0.0516      0.1518     0.1025      0.0591     0.0470

sigma^2 estimated as 1.258:  log likelihood = -762.3,  aic = 1584.59
>
> fordiv <- predict(fitdiv, n.ahead = 2, newxreg = newregy ,  se.fit = TRUE)
Error in cbind(...) : cannot create a matrix from these types
>
>
> str(data)
 num [1:497] -0.34 -1.36 -0.5 -0.46 0.01 0.1 0.68 0.06 0.16 0.48 ...
>
> str(newregy)
 num [1, 1:23] -0.6 -0.3 0.15 1.08 -1.8 3 2 3 0 5 ...
 - attr(*, "dimnames")=List of 2
  ..$ : chr "498"
  ..$ : chr [1:23] "EUSA1" "EUSA10" "EUSA15" "EUSA20" ...
>
> str(y)
`data.frame':   497 obs. of  23 variables:
 $ EUSA1      : num  0.7 5.9 -0.6 1.8 5.7 1.9 0.5 -6.6 2.5 2.3 ...
 $ EUSA10     : num  -4.5 3.8 -11.7 3.2 4.2 -5.4 -2.2 -6.5 0.8 2 ...
 $ EUSA15     : num  -5.4 3.6 -11 3.7 3.4 -4.3 -3.9 -4.7 0.3 2.6 ...
 $ EUSA20     : num  -5 3.6 -10.8 4.3 2.3 -4.1 -3.5 -5 0 3.1 ...
 $ EUSA5      : num  -4.3 5.4 -10.8 2.5 6.3 -2.4 -1.3 -6.6 2.3 -0.2 ...
 $ H15T10Y    : num  -4 2 -9 1 0 -10 -8 -1 -3 0 ...
 $ H15T1Y     : num  -4 1 -6 0 -2 -7 -12 2 -1 2 ...
 $ H15T20Y    : num  -3 4 -11 1 -1 -12 -4 2 -6 2 ...
 $ H15T3M     : num  -1 -1 -4 0 0 0 -10 0 2 1 ...
 $ H15T5Y     : num  -4 2 -11 0 -1 -11 -13 1 -1 2 ...
 $ USSW10     : num  -8.6 0.6 -10.5 2.4 -2.9 -5.8 -15 0.4 -3.5 1 ...
 $ USSW15     : num  -7.9 1.1 -9.8 2 -3.1 -5.8 -13.2 2 -4.4 2.1 ...
 $ USSW20     : num  -6.7 1.4 -9.5 1.3 -3 -6 -11 1.2 -4.2 2.9 ...
 $ USSW30     : num  -6.2 1.3 -8.4 1.6 -2.6 -6.9 -8.5 -0.9 -2.8 2.2 ...
 $ USSW5      : num  -7.7 -0.4 -12.7 0.9 -4 -7.4 -17.5 0.9 0.3 1.6 ...
 $ CAC.INDEX  : num  2.18 0.03 -1.45 -1.03 0.41 -1.57 0.86 -2.24 1.44 -2.08
...
 $ DAX.INDEX  : num  1.96 0.91 -1.64 0.08 0.99 -1.14 -0.35 -2.81 -0.08 -1.55
...
 $ MIB30.INDEX: num  2.08 -0.48 -0.94 0.82 0.22 -2.22 0.8 -2.5 1.34 -1.35
...
 $ OMX.INDEX  : num  4.07 0.28 -0.56 -2.47 -0.16 -1.58 1.13 -3.5 -1.15 -1.85
...
 $ SX5P.INDEX : num  1.95 0.1 -1.22 -1.01 -0.31 -0.96 0.84 -2.71 1.18 -1.05
...
 $ UKX.INDEX  : num  1.91 0.09 -0.57 -0.82 -0.42 -0.73 0.15 -1.65 1.02 -0.75
...
 $ VDAX.INDEX : num  -1.59 -0.74 0.73 -0.36 -0.36 0.87 -0.51 1.64 -0.02 0.85
...
 $ VIX.INDEX  : num  -1.37 -0.89 1.22 0.16 0.3 -0.1 0.57 0.98 -0.88 0.75 ...
>
> fordiv <- predict(fitdiv, n.ahead = 2, newxreg = matrix(0,1,23) ,  se.fit
= TRUE)
Error in cbind(...) : cannot create a matrix from these types
>

-- End R session -----------------------------------------------------

I also tried to replace newregy by a matrix of zeros  matrix(0,1,23)

Please tell if I am doing something wrong ... I did not see any example in
the help about external regressor so I had to start from scratch.

Best regards

Yves Oloui.



From yves.oloui at free.fr  Thu Dec 18 22:16:42 2003
From: yves.oloui at free.fr (Yves)
Date: Thu, 18 Dec 2003 22:16:42 +0100
Subject: [R] Help with predict.Arima with external regressor values
	[Repalced]
Message-ID: <JPELJKOEGFEPPGDABMNAIENLCAAA.yves.oloui@free.fr>


Hi all there

I am enjoying R since 2 weeks and I come to my first deadlock, il am trying
to use predict.Arima in the ts package.
I get a "Error in cbind(...) : cannot create a matrix from these types"

-- Start R session -----------------------------------------------------

> fitdiv <- arima(data, c(2, 0, 3), xreg = y ) ; print(fitdiv)

Call:
arima(x = data, order = c(2, 0, 3), xreg = y)

Coefficients:
         ar1     ar2     ma1      ma2      ma3  intercept   EUSA1  EUSA10
EUSA15  EUSA20    EUSA5  H15T10Y   H15T1Y  H15T20Y  H15T3M   H15T5Y   USSW10
      -0.001  0.6502  0.3328  -0.5021  -0.1135    -0.0535  0.0469
0.0075  -0.0263  0.0299  -0.0344   0.1012  -0.0382   0.0092
0.0385  -0.0757  -0.1577
s.e.   0.523  0.4002  0.5262   0.2711   0.0828     0.1027  0.0308  0.0802
0.0931  0.0743   0.0414   0.0469   0.0215   0.0360  0.0276   0.0344   0.0477
      USSW15   USSW20  USSW30   USSW5  CAC.INDEX  DAX.INDEX  MIB30.INDEX
OMX.INDEX  SX5P.INDEX  UKX.INDEX  VDAX.INDEX  VIX.INDEX
      0.0254  -0.0141  0.0133  0.1186    -0.1816     0.0652
   0.0848    -0.1836      0.1134    -0.1742      0.0236    -0.0482
s.e.  0.0588   0.0251  0.0363  0.0278     0.0907     0.0528       0.0860
0.0516      0.1518     0.1025      0.0591     0.0470

sigma^2 estimated as 1.258:  log likelihood = -762.3,  aic = 1584.59
>
> fordiv <- predict(fitdiv, n.ahead = 1, newxreg = newregy ,  se.fit = TRUE)
Error in cbind(...) : cannot create a matrix from these types
>
>
> str(data)
 num [1:497] -0.34 -1.36 -0.5 -0.46 0.01 0.1 0.68 0.06 0.16 0.48 ...
>
> str(newregy)
 num [1, 1:23] -0.6 -0.3 0.15 1.08 -1.8 3 2 3 0 5 ...
 - attr(*, "dimnames")=List of 2
  ..$ : chr "498"
  ..$ : chr [1:23] "EUSA1" "EUSA10" "EUSA15" "EUSA20" ...
>
> str(y)
`data.frame':   497 obs. of  23 variables:
 $ EUSA1      : num  0.7 5.9 -0.6 1.8 5.7 1.9 0.5 -6.6 2.5 2.3 ...
 $ EUSA10     : num  -4.5 3.8 -11.7 3.2 4.2 -5.4 -2.2 -6.5 0.8 2 ...
 $ EUSA15     : num  -5.4 3.6 -11 3.7 3.4 -4.3 -3.9 -4.7 0.3 2.6 ...
 $ EUSA20     : num  -5 3.6 -10.8 4.3 2.3 -4.1 -3.5 -5 0 3.1 ...
 $ EUSA5      : num  -4.3 5.4 -10.8 2.5 6.3 -2.4 -1.3 -6.6 2.3 -0.2 ...
 $ H15T10Y    : num  -4 2 -9 1 0 -10 -8 -1 -3 0 ...
 $ H15T1Y     : num  -4 1 -6 0 -2 -7 -12 2 -1 2 ...
 $ H15T20Y    : num  -3 4 -11 1 -1 -12 -4 2 -6 2 ...
 $ H15T3M     : num  -1 -1 -4 0 0 0 -10 0 2 1 ...
 $ H15T5Y     : num  -4 2 -11 0 -1 -11 -13 1 -1 2 ...
 $ USSW10     : num  -8.6 0.6 -10.5 2.4 -2.9 -5.8 -15 0.4 -3.5 1 ...
 $ USSW15     : num  -7.9 1.1 -9.8 2 -3.1 -5.8 -13.2 2 -4.4 2.1 ...
 $ USSW20     : num  -6.7 1.4 -9.5 1.3 -3 -6 -11 1.2 -4.2 2.9 ...
 $ USSW30     : num  -6.2 1.3 -8.4 1.6 -2.6 -6.9 -8.5 -0.9 -2.8 2.2 ...
 $ USSW5      : num  -7.7 -0.4 -12.7 0.9 -4 -7.4 -17.5 0.9 0.3 1.6 ...
 $ CAC.INDEX  : num  2.18 0.03 -1.45 -1.03 0.41 -1.57 0.86 -2.24 1.44 -2.08
...
 $ DAX.INDEX  : num  1.96 0.91 -1.64 0.08 0.99 -1.14 -0.35 -2.81 -0.08 -1.55
...
 $ MIB30.INDEX: num  2.08 -0.48 -0.94 0.82 0.22 -2.22 0.8 -2.5 1.34 -1.35
...
 $ OMX.INDEX  : num  4.07 0.28 -0.56 -2.47 -0.16 -1.58 1.13 -3.5 -1.15 -1.85
...
 $ SX5P.INDEX : num  1.95 0.1 -1.22 -1.01 -0.31 -0.96 0.84 -2.71 1.18 -1.05
...
 $ UKX.INDEX  : num  1.91 0.09 -0.57 -0.82 -0.42 -0.73 0.15 -1.65 1.02 -0.75
...
 $ VDAX.INDEX : num  -1.59 -0.74 0.73 -0.36 -0.36 0.87 -0.51 1.64 -0.02 0.85
...
 $ VIX.INDEX  : num  -1.37 -0.89 1.22 0.16 0.3 -0.1 0.57 0.98 -0.88 0.75 ...
>
> fordiv <- predict(fitdiv, n.ahead = 1, newxreg = matrix(0,1,23) ,  se.fit
= TRUE)
Error in cbind(...) : cannot create a matrix from these types
>

-- End R session -----------------------------------------------------

I also tried to replace newregy by a matrix of zeros  matrix(0,1,23)

Please tell if I am doing something wrong ... I did not see any example in
the help about external regressor so I had to start from scratch.

Best regards

Yves Oloui.



From jasont at indigoindustrial.co.nz  Thu Dec 18 22:16:32 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Fri, 19 Dec 2003 10:16:32 +1300
Subject: [R] diagnostic information ....
In-Reply-To: <200312181944.hBIJieYW006824@erdos.math.unb.ca>
References: <200312181944.hBIJieYW006824@erdos.math.unb.ca>
Message-ID: <3FE21930.1040807@indigoindustrial.co.nz>

Rolf Turner wrote:

> Paul E. Johnson wrote:
> 
> 
>>I handed out some results from glm() and the students ask "how many 
>>observations were dropped due to missing values"?
>>
>>How would I know? 
...
>  It
> would be ***VERY*** easy to write a wrapper for glm() to include
> the number of dropped observations if you want to include that
> information in a ``printout''.

To wit

naPrint <- function(model,...) {
     if(!inherits(model,"lm") || !inherits(model,"glm"))
         stop("no support for class ",class(model),"\n")
     if(!is.null(model$na.action)) {
         action <- paste("na.action:",attr(model$na.action,"class"))
         rows <- as.numeric(model$na.action)
         list(action=action,rows=rows)
     } else { #no missing values
         list(action=options()$na.action, rows=NULL)
     }
}

Building this as a generic function and its methods is left as an 
exercise ;)  So is a a prettier print method, and possibly a method for 
latex() or xtable().

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From jasont at indigoindustrial.co.nz  Thu Dec 18 22:25:32 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Fri, 19 Dec 2003 10:25:32 +1300
Subject: [R] diagnostic information ....
In-Reply-To: <200312181944.hBIJieYW006824@erdos.math.unb.ca>
References: <200312181944.hBIJieYW006824@erdos.math.unb.ca>
Message-ID: <3FE21B4C.8090800@indigoindustrial.co.nz>

Whoops.  That should be a "&&" where I put a "||".

naPrint <- function(model,...) {
     if(!inherits(model,"lm") && !inherits(model,"glm"))
         stop("no support for class ",class(model),"\n")
     if(!is.null(model$na.action)) {
         action <- paste("na.action:",attr(model$na.action,"class"))
         rows <- as.numeric(model$na.action)
         list(action=action,rows=rows)
     } else { #no missing values
         list(action=options()$na.action, rows=NULL)
     }
}


-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From peter_mcmahan at yahoo.com  Thu Dec 18 22:35:15 2003
From: peter_mcmahan at yahoo.com (Peter McMahan)
Date: Thu, 18 Dec 2003 13:35:15 -0800 (PST)
Subject: [R] R and Sybase
In-Reply-To: <200312181104.hBIB1anL024102@hypatia.math.ethz.ch>
Message-ID: <20031218213515.25865.qmail@web41608.mail.yahoo.com>

Hi,
I'm trying to get R connected to Sybase on a Linux
machine.  I'm relatively new to databases,
particularly the connectivity side.
Do I have to use RODBC and install ODBC support on the
system (a huge pain with Sybase, as you have to
install a scaled-down version of Sybase itself on the
system to get it to work with ODBC), or can I somehow
use DBI to do it.  Other processes on the machine use
DBI/DBD to talk to the database, but I have no idea
how this would work with R.  The file at
http://cran.r-project.org/doc/manuals/R-data.pdf makes
it seem like the DBI package is only a front-end, and
I would need something like "RSybase" to actually use
it.
Any help would be greatly appreciated.
Thank you,
Peter



From ggrothendieck at myway.com  Thu Dec 18 23:35:20 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 18 Dec 2003 17:35:20 -0500 (EST)
Subject: [R] diagnostic information in glm. How about N of missing
	observations?
Message-ID: <20031218223520.BEDF23979@mprdmxin.myway.com>



Check out

   ?summary.formula

in package Hmisc.

---
Date: Thu, 18 Dec 2003 13:27:39 -0600 
From: Paul Johnson <pauljohn at ku.edu>
To: 'r-help at stat.math.ethz.ch' <r-help at stat.math.ethz.ch> 
Subject: [R] diagnostic information in glm. How about N of missing observations? 

 
 
I handed out some results from glm() and the students ask "how many 
observations were dropped due to missing values"?

How would I know? 

In other stat programs, the results will typically include N and the 
number dropped because of missings. Without going back to R and 
fiddling about to find the total number of rows in the dataframe, there 
is no way to tell. Somewhat inconvenient. Do you agree?

-- 
Paul E. Johnson email: pauljohn at ku.edu
Dept. of Political Science http://lark.cc.ku.edu/~pauljohn
1541 Lilac Lane, Rm 504 
University of Kansas Office: (785) 864-9086
Lawrence, Kansas 66044-3177 FAX: (785) 864-5700

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From dj at research.bell-labs.com  Thu Dec 18 23:37:49 2003
From: dj at research.bell-labs.com (David James)
Date: Thu, 18 Dec 2003 17:37:49 -0500
Subject: [R] R and Sybase
In-Reply-To: <20031218213515.25865.qmail@web41608.mail.yahoo.com>;
	from peter_mcmahan@yahoo.com on Thu, Dec 18, 2003 at 01:35:15PM
	-0800
References: <200312181104.hBIB1anL024102@hypatia.math.ethz.ch>
	<20031218213515.25865.qmail@web41608.mail.yahoo.com>
Message-ID: <20031218173749.B5269@jessie.research.bell-labs.com>

AFAIK the only way to connect to Sybase from R is with the RODBC
package, so you need to have an ODBC driver for Sybase -- either
one provided by Sybase or the FreeTDS ODBC driver from www.freetds.org.

Regards,

--
David

Peter McMahan wrote:
> Hi,
> I'm trying to get R connected to Sybase on a Linux
> machine.  I'm relatively new to databases,
> particularly the connectivity side.
> Do I have to use RODBC and install ODBC support on the
> system (a huge pain with Sybase, as you have to
> install a scaled-down version of Sybase itself on the
> system to get it to work with ODBC), or can I somehow
> use DBI to do it.  Other processes on the machine use
> DBI/DBD to talk to the database, but I have no idea
> how this would work with R.  The file at
> http://cran.r-project.org/doc/manuals/R-data.pdf makes
> it seem like the DBI package is only a front-end, and
> I would need something like "RSybase" to actually use
> it.
> Any help would be greatly appreciated.
> Thank you,
> Peter
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From tblackw at umich.edu  Fri Dec 19 00:15:38 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Thu, 18 Dec 2003 18:15:38 -0500 (EST)
Subject: [R] help installing Design package OS-X
In-Reply-To: <p05210600bc07bfff340c@[128.40.218.142]>
References: <p05210600bc07bfff340c@[128.40.218.142]>
Message-ID: <Pine.SOL.4.58.0312181811340.14910@zektor.gpcc.itd.umich.edu>

Frank  -

I recall some recent discussion on this list about installing
Design on a Mac.  Try the mail archive for a message from
Frank Harrell, the package author and maintainer within the
last five weeks.  Your question sounds very similar to the
question I remember . . . but mine is only a human memory.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Thu, 18 Dec 2003, Frank Mattes wrote:

> I'm wondering if someone has installed the Design library on OS-X.
> I have the Developer tools installed (not the OS X SDK kit) but after
> downloading the library via RAqua package manager I get the following
> errors:
>
> Installation of package Design had a non-zero exit status in :
> install.packages(ui.pks, CRAN=
> getOption(where), lib =
> .libPath()[1]
>
> or
> >  install.fom.file() npackage
> >  installatiion failed
>
> in case someone has the binary package for OS-X , please let me know.
> frank mattes
> --
> Frank Mattes, MD			e-mail:	f.mattes at ucl.ac.uk



From tblackw at umich.edu  Fri Dec 19 00:23:12 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Thu, 18 Dec 2003 18:23:12 -0500 (EST)
Subject: [R] mclust - clustering by spatial patterns
In-Reply-To: <a06010200bc07c2bc20b6@[129.31.3.147]>
References: <a06010200bc07c2bc20b6@[129.31.3.147]>
Message-ID: <Pine.SOL.4.58.0312181819500.14910@zektor.gpcc.itd.umich.edu>

On Thu, 18 Dec 2003, Jarrod Hadfield wrote:

> Dear All,
>
> I have spatial data (presence/absence for 4000 squares) on 250 bird
> species and would like to use a model-based clustering technique to
> test for species associations.  Is there any way of passing a
> distance/correlation matrix to mclust as with hclust, rather than the
> actual data?  Or alternatively, is there a way of getting mclust to
> handle binary data?
>
> I'd appreciate any suggestions!

Why not simply use  dist()  and  hclust() ?   Starting with
presence/absence data, what could  mclust()  possibly do that
is different from  hclust() ?

>
> Cheers,
>
> Jarrod
>

-  tom blackwell  -  u michigan medical school  -  ann arbor  -



From maj at stats.waikato.ac.nz  Fri Dec 19 00:58:51 2003
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Fri, 19 Dec 2003 12:58:51 +1300
Subject: [R] mclust - clustering by spatial patterns
In-Reply-To: <Pine.SOL.4.58.0312181819500.14910@zektor.gpcc.itd.umich.edu>
References: <a06010200bc07c2bc20b6@[129.31.3.147]>
	<Pine.SOL.4.58.0312181819500.14910@zektor.gpcc.itd.umich.edu>
Message-ID: <3FE23F3B.9070508@stats.waikato.ac.nz>



Thomas W Blackwell wrote:

> [...]
> Why not simply use  dist()  and  hclust() ?   Starting with
> presence/absence data, what could  mclust()  possibly do that
> is different from  hclust() ?

Um, fit a statistical model.

> 
> -  tom blackwell  -  u michigan medical school  -  ann arbor  -
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862



From tblackw at umich.edu  Fri Dec 19 01:10:34 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Thu, 18 Dec 2003 19:10:34 -0500 (EST)
Subject: [R] mclust - clustering by spatial patterns
In-Reply-To: <3FE23F3B.9070508@stats.waikato.ac.nz>
References: <a06010200bc07c2bc20b6@[129.31.3.147]>
	<Pine.SOL.4.58.0312181819500.14910@zektor.gpcc.itd.umich.edu>
	<3FE23F3B.9070508@stats.waikato.ac.nz>
Message-ID: <Pine.SOL.4.58.0312181904110.14910@zektor.gpcc.itd.umich.edu>

On Fri, 19 Dec 2003, Murray Jorgensen wrote:

> Thomas W Blackwell wrote:
>
> > [...]
> > Why not simply use  dist()  and  hclust() ?   Starting with
> > presence/absence data, what could  mclust()  possibly do that
> > is different from  hclust() ?
>
> Um, fit a statistical model.

Yes, but ellipsoidal contours don't seem awfully useful
in the discrete space of n = 4000 binary indicator variables.

> >
> > -  tom blackwell  -  u michigan medical school  -  ann arbor  -
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> --
> Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
> Department of Statistics, University of Waikato, Hamilton, New Zealand
> Email: maj at waikato.ac.nz                                Fax 7 838 4155
> Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862
>
>



From ggrothendieck at myway.com  Fri Dec 19 01:20:04 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 18 Dec 2003 19:20:04 -0500 (EST)
Subject: [R] for loop over dataframe without indices
Message-ID: <20031219002004.5E8F2398B@mprdmxin.myway.com>



One can perform a for loop without indices over the columns
of a dataframe like this:

   for( v in df ) ... some statements involving v ...

Is there some way to do this for rows other than using indices:

   for( i in 1:nrow(df) ) ... some statements involving df[i,] ...

If the dataframe had only numeric entries I could transpose it
and then do it over columns but what about the general case?



From tplate at blackmesacapital.com  Fri Dec 19 01:33:54 2003
From: tplate at blackmesacapital.com (Tony Plate)
Date: Thu, 18 Dec 2003 17:33:54 -0700
Subject: [R] mclust - clustering by spatial patterns
In-Reply-To: <a06010200bc07c2bc20b6@[129.31.3.147]>
Message-ID: <5.2.1.1.2.20031218172736.04265070@mailhost.blackmesacapital.com>

You could just convert your binary spatial data to numeric 0/1 or -1/1 and 
give it to Mclust.  That would violate the assumptions of the Gaussian 
model in Mclust, so you should be very careful about interpreting the 
results.  However, if the results are at least as "interesting" as those 
you get from a non-model-based hierarchical clustering run, then that could 
be an indication that the approach has merit, and then you could 
investigate how to build a model-based clustering algorithm that is 
appropriate for your data.  (I don't think it would be that hard to write 
down some equations giving the probability of each presence matrix being 
generated for each component of the mixture model, but I don't know how 
hard it would be implement the EM search for an appropriate mixture model.)

-- Tony Plate

At Thursday 08:55 PM 12/18/2003 +0000, Jarrod Hadfield wrote:
>Dear All,
>
>I have spatial data (presence/absence for 4000 squares) on 250 bird 
>species and would like to use a model-based clustering technique to test 
>for species associations.  Is there any way of passing a 
>distance/correlation matrix to mclust as with hclust, rather than the 
>actual data?  Or alternatively, is there a way of getting mclust to handle 
>binary data?
>
>I'd appreciate any suggestions!
>
>Cheers,
>
>Jarrod
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From edd at debian.org  Fri Dec 19 01:48:25 2003
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 18 Dec 2003 18:48:25 -0600
Subject: [R] R and Sybase
In-Reply-To: <20031218173749.B5269@jessie.research.bell-labs.com>
References: <200312181104.hBIB1anL024102@hypatia.math.ethz.ch>
	<20031218213515.25865.qmail@web41608.mail.yahoo.com>
	<20031218173749.B5269@jessie.research.bell-labs.com>
Message-ID: <20031219004824.GA1796@sonny.eddelbuettel.com>

On Thu, Dec 18, 2003 at 05:37:49PM -0500, David James wrote:
> AFAIK the only way to connect to Sybase from R is with the RODBC
> package, so you need to have an ODBC driver for Sybase -- either
> one provided by Sybase or the FreeTDS ODBC driver from www.freetds.org.

Seconded -- we did that recently at work with RODBC && unixODBC && FreeTDS,
all sitting on Solaris. Everything pretty much works out of the box, and it
was a little tedious to build everything.  I see no reason why that
shouldn't work the same on Linux.

Dirk

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx



From rexbryan1 at comcast.net  Fri Dec 19 01:54:21 2003
From: rexbryan1 at comcast.net (rex_bryan@urscorp.com)
Date: Thu, 18 Dec 2003 17:54:21 -0700
Subject: [R] weighted regression
Message-ID: <002001c3c5ca$a412ff80$1c7f0818@dell1700>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031218/db4e76f3/attachment.pl

From ramasamya at gis.a-star.edu.sg  Fri Dec 19 03:36:28 2003
From: ramasamya at gis.a-star.edu.sg (Adaikalavan RAMASAMY)
Date: Fri, 19 Dec 2003 10:36:28 +0800
Subject: [R] Collapsing Arrays/Lists to scalar values
Message-ID: <6D9E9B9DF347EF4385F6271C64FB8D5607607C@BIONIC.biopolis.one-north.com>

> li <- list(1, 2, 3)
> li
[[1]]
[1] 1

[[2]]
[1] 2

[[3]]
[1] 3

> paste(unlist(li), collapse=" ") 
[1] "1 2 3"

> length( paste(unlist(li), collapse=" ")  )
[1] 1

--
Adaikalavan Ramasamy 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Christopher Brown
Sent: Friday, December 19, 2003 1:25 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Collapsing Arrays/Lists to scalar values




This is probably an easy question.

I want to join the elements of a list/array into a single scalar value.
How can I do this?

More Background:

I have sql queries stored in external flat text files.  When I read
these queries into R using the normal functions, I get a list where each
line is an element in the list.  

When I try to pass this to the appropriate RODBC function, I get an
errror.  The function expects a scalar value for the query text.

Someone must have run into this problem before.  So thanks in advance
for the help.

Chris.

Christopher Brown


---------------------------------


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Fri Dec 19 04:47:56 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 19 Dec 2003 03:47:56 +0000 (GMT)
Subject: [R] Help with predict.Arima with external regressor values
	[Repalced]
In-Reply-To: <JPELJKOEGFEPPGDABMNAIENLCAAA.yves.oloui@free.fr>
Message-ID: <Pine.LNX.4.44.0312190345110.1845-100000@gannet.stats>

First, xreg is documented as a vector or matrix, not a data frame.

Second, there may be a scoping problem, so try a more informative name 
than `y' when you do the fit.

Third, R has debugging facilities, so please use them to find out 
precisely what is wrong.

On Thu, 18 Dec 2003, Yves wrote:

> 
> Hi all there
> 
> I am enjoying R since 2 weeks and I come to my first deadlock, il am trying
> to use predict.Arima in the ts package.
> I get a "Error in cbind(...) : cannot create a matrix from these types"
> 
> -- Start R session -----------------------------------------------------
> 
> > fitdiv <- arima(data, c(2, 0, 3), xreg = y ) ; print(fitdiv)
> 
> Call:
> arima(x = data, order = c(2, 0, 3), xreg = y)
> 
> Coefficients:
>          ar1     ar2     ma1      ma2      ma3  intercept   EUSA1  EUSA10
> EUSA15  EUSA20    EUSA5  H15T10Y   H15T1Y  H15T20Y  H15T3M   H15T5Y   USSW10
>       -0.001  0.6502  0.3328  -0.5021  -0.1135    -0.0535  0.0469
> 0.0075  -0.0263  0.0299  -0.0344   0.1012  -0.0382   0.0092
> 0.0385  -0.0757  -0.1577
> s.e.   0.523  0.4002  0.5262   0.2711   0.0828     0.1027  0.0308  0.0802
> 0.0931  0.0743   0.0414   0.0469   0.0215   0.0360  0.0276   0.0344   0.0477
>       USSW15   USSW20  USSW30   USSW5  CAC.INDEX  DAX.INDEX  MIB30.INDEX
> OMX.INDEX  SX5P.INDEX  UKX.INDEX  VDAX.INDEX  VIX.INDEX
>       0.0254  -0.0141  0.0133  0.1186    -0.1816     0.0652
>    0.0848    -0.1836      0.1134    -0.1742      0.0236    -0.0482
> s.e.  0.0588   0.0251  0.0363  0.0278     0.0907     0.0528       0.0860
> 0.0516      0.1518     0.1025      0.0591     0.0470
> 
> sigma^2 estimated as 1.258:  log likelihood = -762.3,  aic = 1584.59
> >
> > fordiv <- predict(fitdiv, n.ahead = 1, newxreg = newregy ,  se.fit = TRUE)
> Error in cbind(...) : cannot create a matrix from these types
> >
> >
> > str(data)
>  num [1:497] -0.34 -1.36 -0.5 -0.46 0.01 0.1 0.68 0.06 0.16 0.48 ...
> >
> > str(newregy)
>  num [1, 1:23] -0.6 -0.3 0.15 1.08 -1.8 3 2 3 0 5 ...
>  - attr(*, "dimnames")=List of 2
>   ..$ : chr "498"
>   ..$ : chr [1:23] "EUSA1" "EUSA10" "EUSA15" "EUSA20" ...
> >
> > str(y)
> `data.frame':   497 obs. of  23 variables:
>  $ EUSA1      : num  0.7 5.9 -0.6 1.8 5.7 1.9 0.5 -6.6 2.5 2.3 ...
>  $ EUSA10     : num  -4.5 3.8 -11.7 3.2 4.2 -5.4 -2.2 -6.5 0.8 2 ...
>  $ EUSA15     : num  -5.4 3.6 -11 3.7 3.4 -4.3 -3.9 -4.7 0.3 2.6 ...
>  $ EUSA20     : num  -5 3.6 -10.8 4.3 2.3 -4.1 -3.5 -5 0 3.1 ...
>  $ EUSA5      : num  -4.3 5.4 -10.8 2.5 6.3 -2.4 -1.3 -6.6 2.3 -0.2 ...
>  $ H15T10Y    : num  -4 2 -9 1 0 -10 -8 -1 -3 0 ...
>  $ H15T1Y     : num  -4 1 -6 0 -2 -7 -12 2 -1 2 ...
>  $ H15T20Y    : num  -3 4 -11 1 -1 -12 -4 2 -6 2 ...
>  $ H15T3M     : num  -1 -1 -4 0 0 0 -10 0 2 1 ...
>  $ H15T5Y     : num  -4 2 -11 0 -1 -11 -13 1 -1 2 ...
>  $ USSW10     : num  -8.6 0.6 -10.5 2.4 -2.9 -5.8 -15 0.4 -3.5 1 ...
>  $ USSW15     : num  -7.9 1.1 -9.8 2 -3.1 -5.8 -13.2 2 -4.4 2.1 ...
>  $ USSW20     : num  -6.7 1.4 -9.5 1.3 -3 -6 -11 1.2 -4.2 2.9 ...
>  $ USSW30     : num  -6.2 1.3 -8.4 1.6 -2.6 -6.9 -8.5 -0.9 -2.8 2.2 ...
>  $ USSW5      : num  -7.7 -0.4 -12.7 0.9 -4 -7.4 -17.5 0.9 0.3 1.6 ...
>  $ CAC.INDEX  : num  2.18 0.03 -1.45 -1.03 0.41 -1.57 0.86 -2.24 1.44 -2.08
> ...
>  $ DAX.INDEX  : num  1.96 0.91 -1.64 0.08 0.99 -1.14 -0.35 -2.81 -0.08 -1.55
> ...
>  $ MIB30.INDEX: num  2.08 -0.48 -0.94 0.82 0.22 -2.22 0.8 -2.5 1.34 -1.35
> ...
>  $ OMX.INDEX  : num  4.07 0.28 -0.56 -2.47 -0.16 -1.58 1.13 -3.5 -1.15 -1.85
> ...
>  $ SX5P.INDEX : num  1.95 0.1 -1.22 -1.01 -0.31 -0.96 0.84 -2.71 1.18 -1.05
> ...
>  $ UKX.INDEX  : num  1.91 0.09 -0.57 -0.82 -0.42 -0.73 0.15 -1.65 1.02 -0.75
> ...
>  $ VDAX.INDEX : num  -1.59 -0.74 0.73 -0.36 -0.36 0.87 -0.51 1.64 -0.02 0.85
> ...
>  $ VIX.INDEX  : num  -1.37 -0.89 1.22 0.16 0.3 -0.1 0.57 0.98 -0.88 0.75 ...
> >
> > fordiv <- predict(fitdiv, n.ahead = 1, newxreg = matrix(0,1,23) ,  se.fit
> = TRUE)
> Error in cbind(...) : cannot create a matrix from these types
> >
> 
> -- End R session -----------------------------------------------------
> 
> I also tried to replace newregy by a matrix of zeros  matrix(0,1,23)
> 
> Please tell if I am doing something wrong ... I did not see any example in
> the help about external regressor so I had to start from scratch.
> 
> Best regards
> 
> Yves Oloui.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at myway.com  Fri Dec 19 05:48:34 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 18 Dec 2003 23:48:34 -0500 (EST)
Subject: [R] for loop over dataframe without indices
Message-ID: <20031219044834.84979395C@mprdmxin.myway.com>



Based on an off list email conversation, I had I am concerned that
my original email was not sufficiently clear.

Recall that I wanted to use a for loop to iterate over the rows of 
a dataframe without using indices.   Its easy to do this over
the columns (for(v in df) ...) but not for rows.

What I wanted to do is might be something like this. 
Define a function, rows, which takes a dataframe, df, as input 
and converts it to the structure: 
list(df[1,], df[2,], ..., df[n,]) where there are n rows:

     rows <- function( df ) { 
          ll <- NULL
          for( i in 1:nrow(df) ) 
               ll <- append( ll, list(df[i,]) )
          ll 
     }

This allows us to iterate over the rows of df without indices like this:

     data( iris )
     df <- iris[1:3,] # use 1st 3 rows of iris data set as df
     for( v in rows(df) ) print(v)

Of course, this involves iterating over the rows of df twice --
once within rows() and once in the for loop. Perhaps this is
the price one must pay for being able to eliminate index 
computations from a for loop or is it? Have I answered my 
own question or is there a better way to use a for loop 
over the rows of a dataframe without indices?

--- 
Date: Thu, 18 Dec 2003 19:20:04 -0500 
From: Gabor Grothendieck <ggrothendieck at myway.com>
To: <R-help at stat.math.ethz.ch> 
Subject: for loop over dataframe without indices 




One can perform a for loop without indices over the columns
of a dataframe like this:

for( v in df ) ... some statements involving v ...

Is there some way to do this for rows other than using indices:

for( i in 1:nrow(df) ) ... some statements involving df[i,] ...

If the dataframe had only numeric entries I could transpose it
and then do it over columns but what about the general case?



From ripley at stats.ox.ac.uk  Fri Dec 19 06:19:12 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 19 Dec 2003 05:19:12 +0000 (GMT)
Subject: [R] How small machines do you run R on?
Message-ID: <Pine.LNX.4.44.0312190443550.25913-100000@gannet.stats>

One of R's goals has always been to run on minimal hardware, and we say 
for example (src/gnuwin32/CHANGES for rw1070)

  This version of R needs more memory and is slower to start up, because
  it loads more packages by default.  This is only likely to be a
  concern on machines with 16Mb of memory or less than 300MHz 
  processors.  For such machines append R_DEFAULT_PACKAGES=ctest to the
  command line in the shortcut used to run R, when the memory usage will
  be about equal to that of rw1062.

Are people actually using R on machines with less than 32Mb of memory
or where it takes more than 10 seconds to start up, _and_ using a smaller 
set of default packages to alleviate this?  If so, please let me (not the 
list) know.

If we can safely assume 32Mb and a 500MHz processor (where the startup
time for 1.8.1 is around 5 secs, less in R-devel) then we can make some
changes now, including allowing the use of S4 classes in R's basic
statistical functionality.  (There are plans for 2004 that will load R 
objects on first use and so both speed up startup and reduce typical 
memory usage, but not until the second half of the year at the earliest.)

(Until recently I was using a 170MHz 64Mb Sun from 1997 but R no longer 
builds on that machine.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at myway.com  Fri Dec 19 06:31:24 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 19 Dec 2003 00:31:24 -0500 (EST)
Subject: [R] Factor names & levels
Message-ID: <20031219053124.2EA043960@mprdmxin.myway.com>



names() is only defined for vectors and lists and factors are
neither.  See ?vector and ?names for more info.


---
From: djw1005 at cam.ac.uk 
Subject: [R] Factor names & levels


When I alter the levels of a factor, why does it alter the names too?

f <- factor(c(A="one",B="two",C="one",D="one",E="three"),
            levels=c("one","two","three"))
names(f)

 -- gives [1] "A" "B" "C" "D" "E"

levels(f) <- c("un","deux","trois")
names(f)

 -- gives NULL

I'm using R 1.8.0 for Windows.

Damon.



From nusbj at hotmail.com  Fri Dec 19 07:08:16 2003
From: nusbj at hotmail.com (Zhen Pang)
Date: Fri, 19 Dec 2003 14:08:16 +0800
Subject: [R] iterative proportional fitting
Message-ID: <Sea2-F62UuMSe7fpAdA0005d79c@hotmail.com>

Dear all,

I wonder if there are some function or package in R which can do the 
iterative proportional fitting.

In the exponential model 
f(y1,...yn)=exp(a'yi+b'(yi*yj)+.....+c'(y1*...*yn)+constance),

instead of the canonical parameters, I use maginal probability instead of a 
and log odds ratio instead of b. and for the order higher than 3, I use the 
canonical way or even assume them to be 0.

It is good if there is direct function or package in R to do the job, which 
will save a lot of time of me. Thank you!

Rgs,

Zhen



From arnab at myrealbox.com  Fri Dec 19 08:33:28 2003
From: arnab at myrealbox.com (Arnab mukherji)
Date: Fri, 19 Dec 2003 07:33:28 +0000
Subject: [R] R-package install
Message-ID: <1071819208.c98702c0arnab@myrealbox.com>

Hi

  I have been trying to get the R Extention supportware going for a while now and have been running to an error I cannot figure out. I am hoping someone here will recognize whats going on and what i may be messing up.

After installing all the components - Rtools, Active Perl, and minGW I created the file structure using package.skeleton(name = "test") with only the functions i want in the package in memory. I subsequently filled in the relevant details in Description and tried to call Rcmd check.

C:\rpacks>Rcmd check test
* checking for working latex ... OK
* using log directory 'C:/rpacks/test.Rcheck'
* checking for file 'test/DESCRIPTION' ... OK
* checking if this is a source package ... OK
 

make: *** No rule to make target `pkg-test'.  Stop.
*** Installation of test failed ***
 
installing R.css in C:/rpacks/test.Rcheck
 
* checking package directory ... OK
* checking for portable file names ... OK
* checking DESCRIPTION meta-information ... OK
* checking package dependencies ... OK
* checking index information ... OK
* checking R files for syntax errors ... OK
* checking R files for library.dynam ...Error: cannot open file 'C:/rpacks/test.
Rcheck/test/R/test' for reading at C:\PROGRA~1\R\rw1080/bin/check line 864.

 I am not sure what the make error wants me to do. Have also looked up line 864 in bin\check - it is
" open(FILE, "< $file")".

any help would be much appreciated.
thanks

Arnab



From ripley at stats.ox.ac.uk  Fri Dec 19 09:07:23 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 19 Dec 2003 08:07:23 +0000 (GMT)
Subject: [R] R-package install
In-Reply-To: <1071819208.c98702c0arnab@myrealbox.com>
Message-ID: <Pine.LNX.4.44.0312190758100.17957-100000@gannet.stats>

Although your topic is `package install', you are running `check'. Please
do run `Rcmd install' to install a package and check it (both the install
script and the package) is working before attempting to check the package.  
I expect you will still get an error message, but it may be more
informative.

On Fri, 19 Dec 2003, Arnab mukherji wrote:

> Hi
> 
>   I have been trying to get the R Extention supportware going for a

That's not a term I recognize. You appear to be using Windows, so have you
read the rw-FAQ and the readme.packages files?  What version of R are you
using and did you install the component for making source packages
(perhaps the most likely cause of the error message is that it is missing
in whole or in part)?

> while now and have been running to an error I cannot figure out. I am
> hoping someone here will recognize whats going on and what i may be
> messing up.
> 
> After installing all the components - Rtools, Active Perl, and minGW I created the file structure using package.skeleton(name = "test") with only the functions i want in the package in memory. I subsequently filled in the relevant details in Description and tried to call Rcmd check.
> 
> C:\rpacks>Rcmd check test
> * checking for working latex ... OK
> * using log directory 'C:/rpacks/test.Rcheck'
> * checking for file 'test/DESCRIPTION' ... OK
> * checking if this is a source package ... OK
>  
> 
> make: *** No rule to make target `pkg-test'.  Stop.
> *** Installation of test failed ***

At this point you can give up, as that is a fatal error (not sure why it 
is not a fatal error in the check script).

[...]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From s-plus at wiwi.uni-bielefeld.de  Fri Dec 19 09:22:35 2003
From: s-plus at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Fri, 19 Dec 2003 09:22:35 +0100
Subject: [R] for loop over dataframe without indices
References: <20031219044834.84979395C@mprdmxin.myway.com>
Message-ID: <3FE2B54B.9070503@wiwi.uni-bielefeld.de>

Try:

 > data(iris); df<-as.data.frame(t(iris[1:3,]))
 > for(i in df) print(i)
[1] 5.1    3.5    1.4    0.2    setosa
Levels: 0.2 1.4 3.5 5.1 setosa
[1] 4.9    3.0    1.4    0.2    setosa
Levels: 0.2 1.4 3.0 4.9 setosa
[1] 4.7    3.2    1.3    0.2    setosa
Levels: 0.2 1.3 3.2 4.7 setosa

... however, not very nice

Peter Wolf

Gabor Grothendieck wrote:

>Based on an off list email conversation, I had I am concerned that
>my original email was not sufficiently clear.
>
>Recall that I wanted to use a for loop to iterate over the rows of 
>a dataframe without using indices.   Its easy to do this over
>the columns (for(v in df) ...) but not for rows.
>
>What I wanted to do is might be something like this. 
>Define a function, rows, which takes a dataframe, df, as input 
>and converts it to the structure: 
>list(df[1,], df[2,], ..., df[n,]) where there are n rows:
>
>     rows <- function( df ) { 
>          ll <- NULL
>          for( i in 1:nrow(df) ) 
>               ll <- append( ll, list(df[i,]) )
>          ll 
>     }
>
>This allows us to iterate over the rows of df without indices like this:
>
>     data( iris )
>     df <- iris[1:3,] # use 1st 3 rows of iris data set as df
>     for( v in rows(df) ) print(v)
>
>Of course, this involves iterating over the rows of df twice --
>once within rows() and once in the for loop. Perhaps this is
>the price one must pay for being able to eliminate index 
>computations from a for loop or is it? Have I answered my 
>own question or is there a better way to use a for loop 
>over the rows of a dataframe without indices?
>
>--- 
>Date: Thu, 18 Dec 2003 19:20:04 -0500 
>From: Gabor Grothendieck <ggrothendieck at myway.com>
>To: <R-help at stat.math.ethz.ch> 
>Subject: for loop over dataframe without indices 
>
>
>
>
>One can perform a for loop without indices over the columns
>of a dataframe like this:
>
>for( v in df ) ... some statements involving v ...
>
>Is there some way to do this for rows other than using indices:
>
>for( i in 1:nrow(df) ) ... some statements involving df[i,] ...
>
>If the dataframe had only numeric entries I could transpose it
>and then do it over columns but what about the general case?
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From maechler at stat.math.ethz.ch  Fri Dec 19 09:31:49 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 19 Dec 2003 09:31:49 +0100
Subject: [R] ESS tab completion for file names
In-Reply-To: <3FE1C2EC.5070905@unibas.ch>
References: <BC07240E.2D5B%sdavis2@mail.nih.gov> <3FE1C2EC.5070905@unibas.ch>
Message-ID: <16354.46965.68020.229334@gargle.gargle.HOWL>

>>>>> "Pascal" == Pascal A Niklaus <Pascal.Niklaus at unibas.ch>
>>>>>     on Thu, 18 Dec 2003 16:08:28 +0100 writes:

    Pascal> Sean Davis wrote:
    >> I don't think that this works within the R process buffer, but I could be
    >> wrong.  Does the documentation say that it should somewhere?
    >> 
    >> Sean
    >> 
    Pascal> The ESS doc says (section 3.2):

    Pascal> Completion is also provided over file names, which
    Pascal> is particularly useful when using S functions such
    Pascal> as get() or scan() (...) whenever the cursor is
    Pascal> within an S string, pressing TAB completes the file
    Pascal> name before point, and also expands any '~' or
    Pascal> environment variable references.

    Pascal> So this should work, but maybe something is wrong
    Pascal> with my configuration?

It does work for me, both in the R process buffer (aka "*R*")
and in *.R file buffers -- also when I just do 
    "emacs -q"  \\  M-x load-file  ...../ess-site.el  \\  M-x R
i.e. it's not special in my setup.

However, this really does belong to the ESS-help mailing list, to
which I redirect.  Pascal, you should consider sending a regular
ESS bug report (via the ESS / iESS menu, or M-x ess-submit-bug-report).

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From hack at pedos.hr  Fri Dec 19 09:47:18 2003
From: hack at pedos.hr (Branimir K. Hackenberger)
Date: Fri, 19 Dec 2003 09:47:18 +0100
Subject: [R] Export to GIF
Message-ID: <000001c3c60c$b5c6d5c0$3502a8c0@BranimirHackenberger>

Dear all!

What is the best (possible) way to copy the current graphics device to a
file in a GIF format.

Thanks!

Branimir



From ripley at stats.ox.ac.uk  Fri Dec 19 10:11:00 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 19 Dec 2003 09:11:00 +0000 (GMT)
Subject: [R] Export to GIF
In-Reply-To: <000001c3c60c$b5c6d5c0$3502a8c0@BranimirHackenberger>
Message-ID: <Pine.LNX.4.44.0312190858120.27554-100000@gannet.stats>

On Fri, 19 Dec 2003, Branimir K. Hackenberger wrote:

> What is the best (possible) way to copy the current graphics device to a
> file in a GIF format.

Copy it to PNG and find a licensed conversion program from PNG to GIF.
(Recall that writing the GIF format is encumbered by patent claims in some
countries: the relevant patent has expired in the US but not yet in the EU
nor in Japan.)

If your port of R does not have png(), use bitmap().

BTW, it really does help to know your OS when answering such questions.


URL for the patent issue:

http://cloanto.com/users/mcb/19950127giflzw.html

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From fm3a004 at math.uni-hamburg.de  Fri Dec 19 11:54:38 2003
From: fm3a004 at math.uni-hamburg.de (Christian Hennig)
Date: Fri, 19 Dec 2003 11:54:38 +0100 (MET)
Subject: [R] mclust - clustering by spatial patterns
In-Reply-To: <Pine.SOL.4.58.0312181819500.14910@zektor.gpcc.itd.umich.edu>
Message-ID: <Pine.GSO.3.95q.1031219113845.1845A-100000@sun35.math.uni-hamburg.de>

Hi,

the package prabclus contains (as command prabclust) a conversion of
presence-absence data to the output of a multidimensional scaling based
on Jaccard or Kulczynski distances. The MDS output then is clustered by
mclust, including estimation of noise points, that do not belong to any
cluster. This is much better than clustering the presence-absence data
directly, because 0-1-data are far from the normal distribution or normal
mixtures and you will have a very high dimensionality if you take every
region as a variable, and mclust is often unstable in high dimensions. 

We have some experience with clustering presence-absence data with
MDS/mclust as well as with distance based methods such as
average/complete linkage of hclust. All
methods may be somewhat unstable; results have to be interpreted with
care. The advantages of mclust on MDS data are:
1) Automatic decision about number of clusters and presence of noise
points (this is done not by mclust, but by nnclean included in package
prabclus),
2) Clusters may have different variance/covariance structures, which may
be useful, if some "real" clusters contain very similar presenmce patterns 
while others are more widespread. Such a situation often confuses complete
linkage and familiar algorithms.
3) You get a "natural" visualization (MDS solution) of your clustering.
(You can do this without performing mclust, though.)

A drawback of MDS/mclust compared to distance based methods is the
additional loss of information and often instabilty induced by the MDS. We
have the experience that the results of Kruskal's MDS often vary
significantly (not only by rotation) between different machines
(Brian Ripley said once to me that Kruskal's MDS is stable in most cases,
and I think the particular structure of presence-absence data and the
usual distances for these data make a difference here)!
Therefore we suggest to use classical MDS, which has other drawbacks,
though.

Package prabclus contains also a methodology to test if there is any
clustering at all. This test is based only on distances, there is no
further information reduction by MDS.

Best,
Christian

On Thu, 18 Dec 2003, Thomas W Blackwell wrote:

> On Thu, 18 Dec 2003, Jarrod Hadfield wrote:
> 
> > Dear All,
> >
> > I have spatial data (presence/absence for 4000 squares) on 250 bird
> > species and would like to use a model-based clustering technique to
> > test for species associations.  Is there any way of passing a
> > distance/correlation matrix to mclust as with hclust, rather than the
> > actual data?  Or alternatively, is there a way of getting mclust to
> > handle binary data?
> >
> > I'd appreciate any suggestions!
> 
> Why not simply use  dist()  and  hclust() ?   Starting with
> presence/absence data, what could  mclust()  possibly do that
> is different from  hclust() ?

...see above...

> 
> >
> > Cheers,
> >
> > Jarrod
> >
> 
> -  tom blackwell  -  u michigan medical school  -  ann arbor  -
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

***********************************************************************
Christian Hennig
Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
#######################################################################
ich empfehle www.boag-online.de



From f.harrell at vanderbilt.edu  Fri Dec 19 00:51:04 2003
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 18 Dec 2003 18:51:04 -0500
Subject: [R] diagnostic information in glm. How about N of missing
	observations?
In-Reply-To: <3FE1FFAB.8010709@ku.edu>
References: <3FE1FFAB.8010709@ku.edu>
Message-ID: <20031218185104.665cdb71.f.harrell@vanderbilt.edu>

On Thu, 18 Dec 2003 13:27:39 -0600
Paul Johnson <pauljohn at ku.edu> wrote:

> I handed out some results from glm() and the students ask "how many 
> observations were dropped due to missing values"?
> 
> How would I know? 
> 
> In other stat programs, the results will typically include N and the 
> number dropped because of missings.  Without going back to R and 
> fiddling about to find the total number of rows in the dataframe, there 
> is no way to tell.  Somewhat inconvenient. Do you agree?
> 
> -- 
> Paul E. Johnson                       email: pauljohn at ku.edu
> Dept. of Political Science            http://lark.cc.ku.edu/~pauljohn
> 1541 Lilac Lane, Rm 504                              
> University of Kansas                  Office: (785) 864-9086
> Lawrence, Kansas 66044-3177           FAX: (785) 864-5700
>

Fitting functions in the Design package tell you how many observations
were deleted due to each variable in the model.  They generalize the
na.action component stored in the fit object.

Frank 
---
Frank E Harrell Jr   Professor and Chair           School of Medicine
                     Department of Biostatistics   Vanderbilt University



From B.Rowlingson at lancaster.ac.uk  Fri Dec 19 13:53:09 2003
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 19 Dec 2003 12:53:09 +0000
Subject: [R] diagnostic information in glm. How about N of
	missing	observations?
In-Reply-To: <20031218185104.665cdb71.f.harrell@vanderbilt.edu>
References: <3FE1FFAB.8010709@ku.edu>
	<20031218185104.665cdb71.f.harrell@vanderbilt.edu>
Message-ID: <3FE2F4B5.8080207@lancaster.ac.uk>

Frank E Harrell Jr wrote:

> Fitting functions in the Design package tell you how many observations
> were deleted due to each variable in the model.  They generalize the
> na.action component stored in the fit object.

  Surely accessing components of an object is generally a bad idea on 
O-O design, and there ought to be a _method_ that returns which 
observations have been omitted. Perhaps an 'omitted' function?

  Currently the $na.action component of lm (and glm) objects is an 
object of class "omit" (or possibly 'exclude', or even something else), 
which doesn't even have a print method of its own.

  Bizarrely it has an 'naprint' method:

 > naprint(fit$na.action)
[1] "2 observations deleted due to missing"

  But that returns a string which you'd have to cut up to get the number 
2 out of it. And why isn't that the 'print' method?

  naprint.lm remains to be written, so this reverts to na.default:

 > naprint(fit)
[1] ""

  Which at the very least is misleading...

  And relying on the $na.action object to store the omitted rows is 
asking for trouble - this element is already being used for different 
classes of object, so who knows what might be in it in the future...

  Is all this due for a rewrite for R 2.0.0?

Baz



From js229 at yahoo.com  Fri Dec 19 14:38:37 2003
From: js229 at yahoo.com (=?iso-8859-1?q?J=20Swinton?=)
Date: Fri, 19 Dec 2003 13:38:37 +0000 (GMT)
Subject: [R] Missing arguments to new
Message-ID: <20031219133837.81717.qmail@web41210.mail.yahoo.com>

Is this the expected and/or correct behaviour? And if
so, how do I persuade new to interpret a named but
missing argument as missing?

-------------

setClass("testMissing",representation(a="numeric"),prototype=list(a=0));

showMissing <- function(real.arg,missing.arg) {
  really.missing <- new("testMissing");
  show(really.missing);
  really.there <- new("testMissing",a=1);
  show(really.there);
  arg.there <- new("testMissing",a=real.arg);
  show(arg.there);
  arg.missing <- new("testMissing",a=missing.arg);
  show(arg.missing);
}

> showMissing(real.arg=2)
An object of class "testMissing"
Slot "a":
[1] 0

An object of class "testMissing"
Slot "a":
[1] 1

An object of class "testMissing"
Slot "a":
[1] 2

Error in initialize(value, ...) : Argument
"missing.arg" is missing, with no default

----------

Jonathan Swinton

=====
Jonathan Swinton; jonathan at swintons.net

________________________________________________________________________
Yahoo! Messenger - Communicate instantly..."Ping" 
your friends today! Download Messenger Now



From flom at ndri.org  Fri Dec 19 16:19:45 2003
From: flom at ndri.org (Peter Flom)
Date: Fri, 19 Dec 2003 10:19:45 -0500
Subject: [R] Question re labels in r-part (continuation of a thread from a
	while back)
Message-ID: <sfe2d0df.099@MAIL.NDRI.ORG>

Hello again

I have modeled a tree using rpart, with the DV being a log
transformation of the variable I am really interested in (I transformed
the DV due to extreme skewness).  By default, text.rpart labels the
nodes with the value of yval, which in this case is not what I want; I'd
like the labels to be on the original metric, but label in text.rpart 
requires a  "column name of x$frame", and the original DV is not on that
frame.

So, I tried the following:

{create the tree}
tr.totpart <- rpart(log(totpart +1)  ~ sexfact + age + windle +
eabused
    + as.factor(pabau) + positive + controlling + lenient +
druguse.ever
    + anycsw,
    xval = 10,  cp = 0.000000001, data = duhray)



{prune the tree}
tr.totpart.pruned <- prune (tr.totpart, cp = .02)


{ad the original metric to the frame}


meanpart <- exp(tr.totpart.pruned$frame$yval)-1
tr.totpart.pruned$frame <- cbind(tr.totpart.pruned$frame, meanpart)


and thought this was good because summary(tr.totpart.pruned$frame)
lists meanpart as one of the columns.

BUT when I tried


plot(tr.totpart.pruned, compress = T, uniform = T, nspace = .5, margin
= .1)
text(tr.totpart.pruned, splits = T, all = T, pretty = 0, digits = 4,
use.n = T,  fancy = T, label = meanpart)

I got an error

Error in text.rpart(tr.totpart.pruned, splits = T, all = T, pretty = 0,
 : 
        Label must be a column label of the frame component of the
tree





Any thoughts or insights or help appreciated, as always

Peter

PS Given the recent posts on the thread about how to ask questions, I
should perhaps add that I have looked thru ?rpart, and through Atkinson
& Therneau (1997): An Introduction to recursive partitioning using the
RPART routines



Peter L. Flom, PhD
Assistant Director, Statistics and Data Analysis Core
Center for Drug Use and HIV Research
National Development and Research Institutes
71 W. 23rd St
www.peterflom.com
New York, NY 10010
(212) 845-4485 (voice)
(917) 438-0894 (fax)



From tblackw at umich.edu  Fri Dec 19 16:27:31 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Fri, 19 Dec 2003 10:27:31 -0500 (EST)
Subject: [R] Question re labels in r-part (continuation of a thread from
	a while back)
In-Reply-To: <sfe2d0df.099@MAIL.NDRI.ORG>
References: <sfe2d0df.099@MAIL.NDRI.ORG>
Message-ID: <Pine.SOL.4.58.0312191023070.11342@zektor.gpcc.itd.umich.edu>

Peter  -

(Just gessing about the structure of rpart objects ...)  How about

tr.totpart.pruned$frame <- cbind(tr.totpart.pruned$frame,
	 	 	 meanpart = exp(tr.totpart.pruned$frame$yval)-1)

This appends meanpart as an additional column of tr.totpart.pruned$frame.
After this step, I would expect the plot command below to work.
I haven't tried this out myself, though.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Fri, 19 Dec 2003, Peter Flom wrote:

> Hello again
>
> I have modeled a tree using rpart, with the DV being a log
> transformation of the variable I am really interested in (I transformed
> the DV due to extreme skewness).  By default, text.rpart labels the
> nodes with the value of yval, which in this case is not what I want; I'd
> like the labels to be on the original metric, but label in text.rpart
> requires a  "column name of x$frame", and the original DV is not on that
> frame.
>
> So, I tried the following:
>
> {create the tree}
> tr.totpart <- rpart(log(totpart +1)  ~ sexfact + age + windle +
> eabused
>     + as.factor(pabau) + positive + controlling + lenient +
> druguse.ever
>     + anycsw,
>     xval = 10,  cp = 0.000000001, data = duhray)
>
>
>
> {prune the tree}
> tr.totpart.pruned <- prune (tr.totpart, cp = .02)
>
>
> {ad the original metric to the frame}
>
>
> meanpart <- exp(tr.totpart.pruned$frame$yval)-1
> tr.totpart.pruned$frame <- cbind(tr.totpart.pruned$frame, meanpart)
>
>
> and thought this was good because summary(tr.totpart.pruned$frame)
> lists meanpart as one of the columns.
>
> BUT when I tried
>
>
> plot(tr.totpart.pruned, compress = T, uniform = T, nspace = .5, margin
> = .1)
> text(tr.totpart.pruned, splits = T, all = T, pretty = 0, digits = 4,
> use.n = T,  fancy = T, label = meanpart)
>
> I got an error
>
> Error in text.rpart(tr.totpart.pruned, splits = T, all = T, pretty = 0,
>  :
>         Label must be a column label of the frame component of the
> tree
>
>
>
>
>
> Any thoughts or insights or help appreciated, as always
>
> Peter
>
> PS Given the recent posts on the thread about how to ask questions, I
> should perhaps add that I have looked thru ?rpart, and through Atkinson
> & Therneau (1997): An Introduction to recursive partitioning using the
> RPART routines
>
>
>
> Peter L. Flom, PhD
> Assistant Director, Statistics and Data Analysis Core
> Center for Drug Use and HIV Research
> National Development and Research Institutes
> 71 W. 23rd St
> www.peterflom.com
> New York, NY 10010
> (212) 845-4485 (voice)
> (917) 438-0894 (fax)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From tblackw at umich.edu  Fri Dec 19 16:54:16 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Fri, 19 Dec 2003 10:54:16 -0500 (EST)
Subject: [R] weighted regression
In-Reply-To: <002001c3c5ca$a412ff80$1c7f0818@dell1700>
References: <002001c3c5ca$a412ff80$1c7f0818@dell1700>
Message-ID: <Pine.SOL.4.58.0312191046210.11342@zektor.gpcc.itd.umich.edu>

Rex  -

Yes, you have supplied an appropriate 'weight' argument
given the problem description in the paragraph which
begins 'Assume that ...'.

Your example would be much easier to read if the variable
names 'x' and 'y' in the R code matched their usage in the
paragraph description, rather than transposing.  But the
usage within the R code is consistent, although counter-
intuitive.  Your example tries to predict the values of
an almost constant vector c(6.7,6.7,6.6) from a highly
varying one, c(1,6,11).  No surprise that the intercept
with the vertical axis is a bit larger than 6.7 and the
slope is completely non-significant.

-  tom blackwell  -  u michigan medical school  -  ann arbor  =

On Thu, 18 Dec 2003, rex_bryan at urscorp.com wrote:

> To all
>
> I have some simple questions pertaining to weights used in regression.
> If the variability of the dependent variable (y) is a function of the magnitude of predictor
> variable (x), can the use of weights give an appropriate answer to the regression parameters
> and the std errors?
>
> Assume that y at x=1 and 6 has a standard deviation of 0.1 and at x=11 it is 0.4
> Then according to a web page on weighted regression for a calibration curve at
> http://member.nifty.ne.jp/mniwa/rev006.htm, I should use 1/(std^2) for each weight.
>
> i.e. for x=1 and 6, w = 100 and x=11, w = 6.25
>
> In R the run is:
>
> >y<-c(1,6,11)
> >x<-c(6.7,6.7,6.6)
> >w<-c(100,100,6.25)
> >reg <-lm(x~y, weight=w)
> > summary(reg)
>
> Call:
> lm(formula = x ~ y, weights = w)
>
> Residuals:
>        1        2        3
> -0.04762  0.09524 -0.19048
>
> Coefficients:
>              Estimate Std. Error t value Pr(>|t|)
> (Intercept)  6.707619   0.025431 263.762  0.00241 **
> y           -0.002857   0.005471  -0.522  0.69361
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
>
> Residual standard error: 0.2182 on 1 degrees of freedom
> Multiple R-Squared: 0.2143,     Adjusted R-squared: -0.5714
> F-statistic: 0.2727 on 1 and 1 DF,  p-value: 0.6936
>
> Am I using the weight method correctly?
> And if so does the Estimated Std. Error for the Intercept and slope make sense?
>
> On another note.  How does one do a regression with the origin fixed at 0?
>
> Merry Christmas
>
> REX
>
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From HBaize at buttecounty.net  Fri Dec 19 17:35:53 2003
From: HBaize at buttecounty.net (Baize, Harold)
Date: Fri, 19 Dec 2003 08:35:53 -0800
Subject: [R] read.spss warning message with 12.0 sav files
Message-ID: <7B33963AB700D711A107000802A38DC26BDFBB@bcismailchico.buttecounty.net>

useRs,

Don't know if this requires a bug report, but
using the read.spss function on files written by the 
new SPSS 12.0 produces the following warning message:

Warning message: 
C:\data\spss.sav: Unrecognized record type 7, subtype 13 encountered in
system file. 

The data files appear to be read correctly. The warning likely results 
from changes to the file format due to SPSS increasing the size of 
variable names from 8 characters to 64. 

Harold Baize
Butte County Department of Behavioral Health



From spencer.graves at pdf.com  Fri Dec 19 17:39:20 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 19 Dec 2003 08:39:20 -0800
Subject: [R] iterative proportional fitting
In-Reply-To: <Sea2-F62UuMSe7fpAdA0005d79c@hotmail.com>
References: <Sea2-F62UuMSe7fpAdA0005d79c@hotmail.com>
Message-ID: <3FE329B8.8010705@pdf.com>

      Have you considered fitting on the log scale?  It's not clear to 
me what problem you are trying to solve, but the following will estimate 
parameters in a model that looks to me like what you are describing:  

      lm(z ~ (y1+y2+...+yn)^n, ...)

      hope this helps. 
      spencer

Zhen Pang wrote:

> Dear all,
>
> I wonder if there are some function or package in R which can do the 
> iterative proportional fitting.
>
> In the exponential model 
> f(y1,...yn)=exp(a'yi+b'(yi*yj)+.....+c'(y1*...*yn)+constance),
>
> instead of the canonical parameters, I use maginal probability instead 
> of a and log odds ratio instead of b. and for the order higher than 3, 
> I use the canonical way or even assume them to be 0.
>
> It is good if there is direct function or package in R to do the job, 
> which will save a lot of time of me. Thank you!
>
> Rgs,
>
> Zhen
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From christoph.lehmann at gmx.ch  Fri Dec 19 18:09:26 2003
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Fri, 19 Dec 2003 18:09:26 +0100
Subject: [R] lmList error if NA in variable not used
Message-ID: <1071853550.14548.13.camel@christophl>

I try to fit a lmList model. If in the used dataframe a variable (a
column) has some NA the lmList gives an error, even this variable is not
used in the model. why? or what is my mistake?

Error in na.fail.default(data) : missing values in object

thanks 

cheers

christoph
-- 
Christoph Lehmann <christoph.lehmann at gmx.ch>



From sonysplus at netscape.net  Fri Dec 19 18:13:56 2003
From: sonysplus at netscape.net (sonysplus@netscape.net)
Date: Fri, 19 Dec 2003 12:13:56 -0500
Subject: [R] R in 64 bit for Linux
Message-ID: <22D60910.49CDA5BA.0353223C@netscape.net>

I sent this post earlier, but I am not sure if it reached the R-help mailing list:
I want to install R in a 64 bit machine with Linux OS, has anyone done this instalation? If yes, can someone give some important points how to install it?
TIA,
Sony


__________________________________________________________________
New! Unlimited Access from the Netscape Internet Service.
Beta test the new Netscape Internet Service for only $1.00 per month until 3/1/04.
Sign up today at http://isp.netscape.com/register
Act now to get a personalized email address!

Netscape. Just the Net You Need.



From pocernic at rap.ucar.edu  Fri Dec 19 18:17:30 2003
From: pocernic at rap.ucar.edu (Matt Pocernich)
Date: Fri, 19 Dec 2003 10:17:30 -0700 (MST)
Subject: [R] generic/method consistency
Message-ID: <Pine.LNX.4.44.0312190958370.25146-100000@albedo.rap.ucar.edu>

Hi,

I realize the answer is very likely in the section Generic functions and
methods  (or Adding new generics), but I'm not clear what to do with the
following.  Running R CMD check, I get the following warnings for my
generic functions.  Does this mean I need the argument

* checking generic/method consistency ... WARNING

leps:
  function(x, ...)
leps.default:
  function(x, pred, titl, plot)

reliability.plot:
  function(x, ...)
reliability.plot.default:
  function(x, obar.i, prob.y, titl, mod.names)

summary:
  function(object, ...)
summary.prob.bin:
  function(object)



Matt Pocernich
NCAR - Research Applications Program
303-497-8312



From ggrothendieck at myway.com  Fri Dec 19 17:48:56 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 19 Dec 2003 11:48:56 -0500 (EST)
Subject: [R] for loop over dataframe without indices
Message-ID: <20031219164856.BB55839B6@mprdmxin.myway.com>



Regarding my problem of how to use a for loop over
the rows of a dataframe without using indices,
several people mentioned using transpose and then
iterating over the columns (which were the rows)
and one person suggested apply(df,1,list);
however, both these solutions coerce the data to
different types.

What I now realize is that the thing that is oddly
missing in R is that you can't do an apply over
the rows of a dataframe (at least not without having
it coerced to an array and the elements coerced to
possibly different types).  The documentation does
point this out.  Its not a bug but its an omission
that seems deserving of being addressed.

Thus I propose that apply be extended to handle
data frames directly.   Any comments on this 
before I send a message to r-devel?


(In terms of my previous posting, with such an apply
one could do:

rows <- function(df) apply( df, 1, function(x)x )
for( v in rows(df) ) ... some statements involving v ...

There is still the limitation, of course, that one can
only _access_ rows of df like this.  One still needs
indices to change them.  

As an aside, should id <- function(x)x and rows, as defined
above, be predefined in R?  id certainly plays a special 
role in mathematics and it seems natural to want to iterate
over rows and not just columns of dataframes.



From pocernic at rap.ucar.edu  Fri Dec 19 18:19:08 2003
From: pocernic at rap.ucar.edu (Matt Pocernich)
Date: Fri, 19 Dec 2003 10:19:08 -0700 (MST)
Subject: [R] disregard last message generic method/ consistency
Message-ID: <Pine.LNX.4.44.0312191018080.25146-100000@albedo.rap.ucar.edu>

I think I got it and meant to cancel the last message, not send it.

Thanks,


Matt Pocernich
NCAR - Research Applications Program
303-497-8312



From HBaize at buttecounty.net  Fri Dec 19 18:19:33 2003
From: HBaize at buttecounty.net (Baize, Harold)
Date: Fri, 19 Dec 2003 09:19:33 -0800
Subject: [R] RE: read.spss warning message with 12.0 sav files
Message-ID: <7B33963AB700D711A107000802A38DC26BDFBC@bcismailchico.buttecounty.net>


As a follow up to my last posting. The read.spss 
function returns variable names truncated to 
eight characters when reading a SPSS 12.0 sav 
file with long variable names. 

The read.spss function adds ".A", ".B", etc. 
to each additional variable with the same 
first eight characters in the name. A viable 
solution, but keeping the full name would be 
a nice improvement for an updated function. 
IMHO.

Harold Baize
Butte County Department of Behavioral Health



From mailman-bounces at stat.math.ethz.ch  Fri Dec 19 18:22:41 2003
From: mailman-bounces at stat.math.ethz.ch (mailman-bounces@stat.math.ethz.ch)
Date: Fri, 19 Dec 2003 18:22:41 +0100
Subject: [R] Forward of moderated message
Message-ID: <mailman.0.1071854561.28833.mailman@stat.math.ethz.ch>

An embedded message was scrubbed...
From: Matt Pocernich <pocernic at rap.ucar.edu>
Subject: generic/method consistency
Date: Fri, 19 Dec 2003 10:17:30 -0700 (MST)
Size: 1929
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031219/576c6cd8/attachment.mht

From mailman-bounces at stat.math.ethz.ch  Fri Dec 19 18:22:41 2003
From: mailman-bounces at stat.math.ethz.ch (mailman-bounces@stat.math.ethz.ch)
Date: Fri, 19 Dec 2003 18:22:41 +0100
Subject: [R] Forward of moderated message
Message-ID: <mailman.1.1071854561.28833.mailman@stat.math.ethz.ch>

An embedded message was scrubbed...
From: Spencer Graves <spencer.graves at pdf.com>
Subject: Re: [R] iterative proportional fitting
Date: Fri, 19 Dec 2003 08:39:20 -0800
Size: 2631
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031219/f37f49a8/attachment.mht

From gregory_r_warnes at groton.pfizer.com  Fri Dec 19 18:35:09 2003
From: gregory_r_warnes at groton.pfizer.com (Warnes, Gregory R)
Date: Fri, 19 Dec 2003 12:35:09 -0500
Subject: [R] generic/method consistency
Message-ID: <D7A3CFD7825BD6119B880002A58F06C20680AC6C@groexmb02.pfizer.com>

Add '...' to the argument list of your methods.  

-G

> -----Original Message-----
> From: Matt Pocernich [mailto:pocernic at rap.ucar.edu]
> Sent: Friday, December 19, 2003 12:18 PM
> To: r-help at stat.math.ethz.ch
> Subject: generic/method consistency
> 
> 
> Hi,
> 
> I realize the answer is very likely in the section Generic 
> functions and
> methods  (or Adding new generics), but I'm not clear what to 
> do with the
> following.  Running R CMD check, I get the following warnings for my
> generic functions.  Does this mean I need the argument
> 
> * checking generic/method consistency ... WARNING
> 
> leps:
>   function(x, ...)
> leps.default:
>   function(x, pred, titl, plot)
> 
> reliability.plot:
>   function(x, ...)
> reliability.plot.default:
>   function(x, obar.i, prob.y, titl, mod.names)
> 
> summary:
>   function(object, ...)
> summary.prob.bin:
>   function(object)
> 
> 
> 
> Matt Pocernich
> NCAR - Research Applications Program
> 303-497-8312
> 
> 


LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}



From zeileis at ci.tuwien.ac.at  Fri Dec 19 18:35:24 2003
From: zeileis at ci.tuwien.ac.at (Achim Zeileis)
Date: Fri, 19 Dec 2003 18:35:24 +0100 (CET)
Subject: [R] generic/method consistency
In-Reply-To: <Pine.LNX.4.44.0312190958370.25146-100000@albedo.rap.ucar.edu>
Message-ID: <Pine.LNX.3.96.1031219183254.27854C-100000@thorin.ci.tuwien.ac.at>

On Fri, 19 Dec 2003, Matt Pocernich wrote:

> Hi,
> 
> I realize the answer is very likely in the section Generic functions and
> methods 

Yes, and it is even set in italics (on the first page of section 6, page 
75) so that it is easy to find. It says:

  A method must have all the arguments of the generic, inlcuding the
  ... if the generic does.

hth,
Z

> (or Adding new generics), but I'm not clear what to do with the
> following.  Running R CMD check, I get the following warnings for my
> generic functions.  Does this mean I need the argument
> 
> * checking generic/method consistency ... WARNING
> 
> leps:
>   function(x, ...)
> leps.default:
>   function(x, pred, titl, plot)
> 
> reliability.plot:
>   function(x, ...)
> reliability.plot.default:
>   function(x, obar.i, prob.y, titl, mod.names)
> 
> summary:
>   function(object, ...)
> summary.prob.bin:
>   function(object)
> 
> 
> 
> Matt Pocernich
> NCAR - Research Applications Program
> 303-497-8312
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From ripley at stats.ox.ac.uk  Fri Dec 19 19:03:39 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 19 Dec 2003 18:03:39 +0000 (GMT)
Subject: [R] R in 64 bit for Linux
In-Reply-To: <22D60910.49CDA5BA.0353223C@netscape.net>
Message-ID: <Pine.LNX.4.44.0312191745300.1507-100000@gannet.stats>

People have installed R on several variants of Linux on several 64-bit 
architectures.  Some are even described in the R-admin manual that comes 
with R.

To my knowledge Compaq Alpha and AMD Opteron are in current use, and I 
believe Debian tests on Itanium.

On Fri, 19 Dec 2003 sonysplus at netscape.net wrote:

> I sent this post earlier, but I am not sure if it reached the R-help
> mailing list: I want to install R in a 64 bit machine with Linux OS, has
> anyone done this instalation? If yes, can someone give some important
> points how to install it?

Read the R-admin manual.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tlumley at u.washington.edu  Fri Dec 19 19:03:55 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 19 Dec 2003 10:03:55 -0800 (PST)
Subject: [R] for loop over dataframe without indices
In-Reply-To: <20031219164856.BB55839B6@mprdmxin.myway.com>
References: <20031219164856.BB55839B6@mprdmxin.myway.com>
Message-ID: <Pine.A41.4.58.0312190955230.16296@homer38.u.washington.edu>

On Fri, 19 Dec 2003, Gabor Grothendieck wrote:
>
> What I now realize is that the thing that is oddly
> missing in R is that you can't do an apply over
> the rows of a dataframe (at least not without having
> it coerced to an array and the elements coerced to
> possibly different types).  The documentation does
> point this out.  Its not a bug but its an omission
> that seems deserving of being addressed.
>

Since mapply() applies a function to each 'row' of a list of vectors, ou
can achieve this effect with
	do.call("mapply", list(FUN,data.frame))
and also as a degenerate case of by():
	by(data.frame, row.names(data.frame), FUN)

These should probably be documented under apply()


	-thomas



From dominik.grathwohl at rdls.nestle.com  Fri Dec 19 19:51:46 2003
From: dominik.grathwohl at rdls.nestle.com (Grathwohl,Dominik,LAUSANNE,NRC/BAS)
Date: Fri, 19 Dec 2003 19:51:46 +0100
Subject: [R] problem with rm.impute of the Design library
Message-ID: <89466355CEFE7244AC3A013E45641C18026E58F1@lsmail2.crn.nestrd.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031219/c4164a55/attachment.pl

From sonysplus at netscape.net  Fri Dec 19 19:54:50 2003
From: sonysplus at netscape.net (sonysplus@netscape.net)
Date: Fri, 19 Dec 2003 13:54:50 -0500
Subject: [R] R in 64 bit for Linux
Message-ID: <0DFEA3E7.1568BD45.0353223C@netscape.net>

Thank you for your response,
Also here is another response sent privately. Our server works too now (AMD opteron, under RedHat Linux OS).
Thanks,
Sony

- The other post:
We've installed R on an Opteron system here running SuSE Linux 8.  For 
us, the default ./configure, make, make install works fine.  In general, 
I think you just need to make sure that your entire development 
toolchain (C/Fortran compilers, linkers, libraries, etc.) are all 64 bit.

-roger


Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:

>People have installed R on several variants of Linux on several 64-bit 
>architectures. ?Some are even described in the R-admin manual that comes 
>with R.
>
>To my knowledge Compaq Alpha and AMD Opteron are in current use, and I 
>believe Debian tests on Itanium.
>
>On Fri, 19 Dec 2003 sonysplus at netscape.net wrote:
>
>> I sent this post earlier, but I am not sure if it reached the R-help
>> mailing list: I want to install R in a 64 bit machine with Linux OS, has
>> anyone done this instalation? If yes, can someone give some important
>> points how to install it?
>
>Read the R-admin manual.
>
>-- 
>Brian D. Ripley, ? ? ? ? ? ? ? ? ?ripley at stats.ox.ac.uk
>Professor of Applied Statistics, ?http://www.stats.ox.ac.uk/~ripley/
>University of Oxford, ? ? ? ? ? ? Tel: ?+44 1865 272861 (self)
>1 South Parks Road, ? ? ? ? ? ? ? ? ? ? +44 1865 272866 (PA)
>Oxford OX1 3TG, UK ? ? ? ? ? ? ? ?Fax: ?+44 1865 272595
>
>
>



From rexbryan1 at comcast.net  Fri Dec 19 20:07:34 2003
From: rexbryan1 at comcast.net (rex_bryan@urscorp.com)
Date: Fri, 19 Dec 2003 12:07:34 -0700
Subject: [R] weighted regression
References: <002001c3c5ca$a412ff80$1c7f0818@dell1700>
	<Pine.SOL.4.58.0312191046210.11342@zektor.gpcc.itd.umich.edu>
Message-ID: <002301c3c663$5c6fd240$1c7f0818@dell1700>

Tom
You are right.  I goofed on my x and y's ... sorry about that.
This example came from a MathCAD discussion group which
a math wizard with handle Paul_W proposed a solution.  I then tried
Statistica to see if a "big and professional" statistics package could
do regression with weights.  Yes and no.  Statistica's idea
of weighting appears to be one of "replicating" the data by
a count number called "weight".  Hence if you start with n =3 and you
weigh them each by 2 all the subsequent reports on number of samples will be
6. I don't know if this is standard in the statistical industry but boy it
did'nt meet the inverse variace idea at all.  Yep, the MathCAD solution
and R seem to match perfectly. Now I trying to figure out how confidence and
prediction curves
work with weighted regression.
Thanks for the response.

Merry Christmas

REX.
----- Original Message -----
From: "Thomas W Blackwell" <tblackw at umich.edu>
To: "rex_bryan at urscorp.com" <rexbryan1 at comcast.net>
Cc: <r-help at stat.math.ethz.ch>
Sent: Friday, December 19, 2003 8:54 AM
Subject: Re: [R] weighted regression


> Rex  -
>
> Yes, you have supplied an appropriate 'weight' argument
> given the problem description in the paragraph which
> begins 'Assume that ...'.
>
> Your example would be much easier to read if the variable
> names 'x' and 'y' in the R code matched their usage in the
> paragraph description, rather than transposing.  But the
> usage within the R code is consistent, although counter-
> intuitive.  Your example tries to predict the values of
> an almost constant vector c(6.7,6.7,6.6) from a highly
> varying one, c(1,6,11).  No surprise that the intercept
> with the vertical axis is a bit larger than 6.7 and the
> slope is completely non-significant.
>
> -  tom blackwell  -  u michigan medical school  -  ann arbor  =
>
> On Thu, 18 Dec 2003, rex_bryan at urscorp.com wrote:
>
> > To all
> >
> > I have some simple questions pertaining to weights used in regression.
> > If the variability of the dependent variable (y) is a function of the
magnitude of predictor
> > variable (x), can the use of weights give an appropriate answer to the
regression parameters
> > and the std errors?
> >
> > Assume that y at x=1 and 6 has a standard deviation of 0.1 and at x=11
it is 0.4
> > Then according to a web page on weighted regression for a calibration
curve at
> > http://member.nifty.ne.jp/mniwa/rev006.htm, I should use 1/(std^2) for
each weight.
> >
> > i.e. for x=1 and 6, w = 100 and x=11, w = 6.25
> >
> > In R the run is:
> >
> > >y<-c(1,6,11)
> > >x<-c(6.7,6.7,6.6)
> > >w<-c(100,100,6.25)
> > >reg <-lm(x~y, weight=w)
> > > summary(reg)
> >
> > Call:
> > lm(formula = x ~ y, weights = w)
> >
> > Residuals:
> >        1        2        3
> > -0.04762  0.09524 -0.19048
> >
> > Coefficients:
> >              Estimate Std. Error t value Pr(>|t|)
> > (Intercept)  6.707619   0.025431 263.762  0.00241 **
> > y           -0.002857   0.005471  -0.522  0.69361
> > ---
> > Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
> >
> > Residual standard error: 0.2182 on 1 degrees of freedom
> > Multiple R-Squared: 0.2143,     Adjusted R-squared: -0.5714
> > F-statistic: 0.2727 on 1 and 1 DF,  p-value: 0.6936
> >
> > Am I using the weight method correctly?
> > And if so does the Estimated Std. Error for the Intercept and slope make
sense?
> >
> > On another note.  How does one do a regression with the origin fixed at
0?
> >
> > Merry Christmas
> >
> > REX
> >
> >
> >
> >
> >
> > [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
>



From SuzieBlatt at netscape.net  Fri Dec 19 21:14:44 2003
From: SuzieBlatt at netscape.net (Suzanne E. Blatt)
Date: Fri, 19 Dec 2003 15:14:44 -0500
Subject: [R] mpl and ppp
Message-ID: <5E282E80.7CAA366A.0D1322AF@netscape.net>

Hello.

I'm trying to use mpl to compare one spatial pattern with soil variables.  I have generated a surface trend for the soil variables and predicted their values for the x,y of my trees - that's fine.  I have generated a 'ppp' for my tree data, but when I try to put the 2 together in mpl, I get the following error message:

"Error in as.ppp(X): Can't interpret X as a point pattern"

which is odd because when I,in R, ask it to examine my object it reads:

"marked planar point pattern: 55 points
multitype, with levels = 1 2 3 4 5 6
window: polygonal boundary
enclosing rectangle: [4943.5, 5023.4] x [4965.2, 5083.7]"

which, as far as I can tell is a ppp.  Can anybody shed some light as to why R would not like it?  Is it the polygonal boundary?

Thanks,
Suzanne

__________________________________________________________________
New! Unlimited Access from the Netscape Internet Service.
Beta test the new Netscape Internet Service for only $1.00 per month until 3/1/04.
Sign up today at http://isp.netscape.com/register
Act now to get a personalized email address!

Netscape. Just the Net You Need.



From Scott.Waichler at pnl.gov  Fri Dec 19 21:23:19 2003
From: Scott.Waichler at pnl.gov (Waichler, Scott R)
Date: Fri, 19 Dec 2003 12:23:19 -0800
Subject: [R] Different scales for keys in lattice levelplots
Message-ID: <62AE0CF1D4875C4BBDEC29DB9924ACE87F2162@pnlmse25.pnl.gov>


Can anyone tell me how to obtain a custom scale for
the colorkey in each levelplot of a lattice?  I am using
lattice and levelplot to plot z = f(x, y) for multiple z
variables.  x and y values are the same across plots, but
units for z are different and therefore I need to create
a custom scale for each plot's key.  It follows that I need
to place each plot's key inside that plot's box.  Should I
use filled.contour() and gridBase instead?

Thanks,
Scott Waichler
Pacific Northwest National Laboratory
scott.waichler at pnl.gov
http://hydrology.pnl.gov



From jmc at research.bell-labs.com  Fri Dec 19 21:59:57 2003
From: jmc at research.bell-labs.com (John Chambers)
Date: Fri, 19 Dec 2003 15:59:57 -0500
Subject: [R] Missing arguments to new
References: <20031219133837.81717.qmail@web41210.mail.yahoo.com>
Message-ID: <3FE366CD.392FC9FD@research.bell-labs.com>

J Swinton wrote:
> 
> Is this the expected and/or correct behaviour? And if
> so, how do I persuade new to interpret a named but
> missing argument as missing?

The general problem you're having has to do with understanding "...",
not with new(), but as it happens, there is a special mechanism for
calls to new() (there is a pointer in the online documentation for
new()).

You can't use the S language mechanism of missing(x) for arguments
matched as part of "..."--the language has no way to ask "is the actual
argument in ... corresponding to x missing".

There are several standard tricks to get the same effect, such as:
	args <- list(...)
	if(is.null(args$x)) # act as if "x" was missing

For generating objects from a class, there is a special mechanism that
is clearer and more intuitive for your users.

If you look at the R function new(), you'll see it ends in the call
  initialize(value, ...)
where `value' is a standard object from the class.

By defining a method for initialize() for the appropriate class, and
using an extended feature that allows methods to override the "..."
argument in the generic, you can end up with a method that checks
directly for missing arguments.

For your example class, something like the following:

setClass("testMissing",representation(a="numeric"),prototype=list(a=0))
setMethod("initialize", "testMissing", function(.Object, a){
    if(missing(a))message("Missing!")
    else .Object at a <- a
    .Object
})

which gives the following results (you can check for "a" whether named
in the call or not)

R> new("testMissing", a=1)
An object of class "testMissing"
Slot "a":
[1] 1

R> new("testMissing", 1)
An object of class "testMissing"
Slot "a":
[1] 1

R> new("testMissing")
Missing! 
An object of class "testMissing"
Slot "a":
[1] 0




> 
> -------------
> 
> setClass("testMissing",representation(a="numeric"),prototype=list(a=0))

;
> 
> showMissing <- function(real.arg,missing.arg) {
>   really.missing <- new("testMissing");
>   show(really.missing);
>   really.there <- new("testMissing",a=1);
>   show(really.there);
>   arg.there <- new("testMissing",a=real.arg);
>   show(arg.there);
>   arg.missing <- new("testMissing",a=missing.arg);
>   show(arg.missing);
> }
> 
> > showMissing(real.arg=2)
> An object of class "testMissing"
> Slot "a":
> [1] 0
> 
> An object of class "testMissing"
> Slot "a":
> [1] 1
> 
> An object of class "testMissing"
> Slot "a":
> [1] 2
> 
> Error in initialize(value, ...) : Argument
> "missing.arg" is missing, with no default
> 
> ----------
> 
> Jonathan Swinton
> 
> =====
> Jonathan Swinton; jonathan at swintons.net
> 
> ________________________________________________________________________
> Yahoo! Messenger - Communicate instantly..."Ping"
> your friends today! Download Messenger Now
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-- 
John M. Chambers                  jmc at bell-labs.com
Bell Labs, Lucent Technologies    office: (908)582-2681
700 Mountain Avenue, Room 2C-282  fax:    (908)582-3340
Murray Hill, NJ  07974            web: http://www.cs.bell-labs.com/~jmc



From abunn at montana.edu  Fri Dec 19 22:16:12 2003
From: abunn at montana.edu (Andy Bunn)
Date: Fri, 19 Dec 2003 14:16:12 -0700
Subject: [R] Contrasts for MANOVA
In-Reply-To: <Pine.SOL.4.58.0312191046210.11342@zektor.gpcc.itd.umich.edu>
Message-ID: <002401c3c675$661a0380$78f05a99@msu.montana.edu>

A colleague asked me if R can do contrasts for MANOVA. SAS will using
PROC GLM with CONTRAST but can R? I assured her that R can do everything
but make coffee. If this is trivial then I apologize in advance.

A search with 'manova' and 'contrasts' didn't produce much:
http://www.google.com/u/newcastlemaths?hl=en&lr=&ie=ISO-8859-1&q=MANOVA+
contrasts

Thanks in advance, Andy



From HStevens at muohio.edu  Fri Dec 19 22:38:21 2003
From: HStevens at muohio.edu (Hank Stevens)
Date: Fri, 19 Dec 2003 16:38:21 -0500
Subject: [R] won't install properly on Mac 10.3.2
Message-ID: <5.1.0.14.2.20031219162808.01707498@po.muohio.edu>

R version 1.8.1, Mac OS X 10.3.2

I have tried searching for this  problem and its fix, but to no avail.
-Everything seems to download and unpack fine. I double click on StartR, 
however, and it just winks and fails.
Any thoughts?
Thanks in advance,
Hank

Dr. Martin Henry H. Stevens, Assistant Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/botany/bot/henry.html
http://www.muohio.edu/ecology



From wegmann_mailinglist at gmx.net  Fri Dec 19 22:40:24 2003
From: wegmann_mailinglist at gmx.net (Martin Wegmann)
Date: Fri, 19 Dec 2003 22:40:24 +0100 (MET)
Subject: [R] mailing list for basic questions - preliminary sum up
References: <Pine.A41.4.58.0312180747380.19076@homer25.u.washington.edu>
Message-ID: <11144.1071870024@www50.gmx.net>

> On Wed, 17 Dec 2003, Gabor Grothendieck wrote:
> 
> >
> >
> > In rereading this one idea occurred to me.  What if the entire R help
> > system were turned into a wiki?   That is,
> >
> > ?whatever
> >
> > would take you to the help page, but not on your computer --
> > rather to the same page on the wiki.
> 
> This seems to assume that you are attached to the internet (or at least
> that R can reliably tell if you are).
> 

I am not at all experienced with such things but wouldn't it be possible to
handle the wiki html files similar to a package and download the wiki files
on your harddrive? 
And everytime when you are connected to the web you can update your wiki
files like someone does with regular packages?
Then R can open a browser, the user can use the wiki but the internet
connection is not mandatory. 

regards Martin

--



From feh3k at spamcop.net  Fri Dec 19 22:52:52 2003
From: feh3k at spamcop.net (Frank E Harrell Jr)
Date: Fri, 19 Dec 2003 16:52:52 -0500
Subject: [R] problem with rm.impute of the Design library
In-Reply-To: <89466355CEFE7244AC3A013E45641C18026E58F1@lsmail2.crn.nestrd.ch>
References: <89466355CEFE7244AC3A013E45641C18026E58F1@lsmail2.crn.nestrd.ch>
Message-ID: <20031219165252.79c44ec1.feh3k@spamcop.net>

On Fri, 19 Dec 2003 19:51:46 +0100
"Grathwohl,Dominik,LAUSANNE,NRC/BAS" <dominik.grathwohl at rdls.nestle.com>
wrote:

> Hello,
> 
> I'm using:
> 
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    1              
> minor    8.1            
> year     2003           
> month    11             
> day      21             
> language R      
> 
> and I get the following error with:
>  
> library(Design)
> 
> df <- list(pre=c(0,, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
> 0, 1),
> pro=c(1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0),
> sex=c(1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 2, 1),
> y=cbind(
> y1<-c(5000, 3820, 4300, 3720, 4370, 4250, 4820, 4060, 4110, 4620, 4580,
> 3580, 3070, 4030, 3900, 3390, 3880, 4120, 3690, 4130),
> y2<-c(6160, 5240, 5210, 4650, 5600, 5340, 6020, 5590, 5140, 5640, 5340,
> 4640, 4650,   NA, 5320, 4510, 4910, 4960, 4420, 5150),
> y3<-c(6720, 6040, 6240, 5550, 6610, 6850, 6760, 6370, 6030,   NA, 6880,
> 5330, 5700,   NA, 6220, 5240, 5850, 5960, 4910, 5550),
> y4<-c(7640, 6840, 6900, 6010, 7780, 7650, 7610, 7000,   NA,   NA, 7720,
> 5990, 6340,   NA, 7360, 5910, 6310,   NA, 5350, 5880)),
> last=c(4, 4, 4, 4, 4, 4, 4, 4, 3, 2, 4, 4, 4, 1, 4, 4, 4, 3, 4, 4))
> 
> imp.df <- rm.impute(pformula = ~ pre+pro+sex, y = df$y, last = df$last, 
> rformula = ~ pre+pro+sex, n.impute = 2, data = df)
> 
> 
> Here the error:
> 
> > imp.df <- rm.impute(pformula = ~ pre+pro+sex, y = df$y, last =
> > df$last, 
> + rformula = ~ pre+pro+sex, n.impute = 2, data = df)
> 
> 
> Imputation 1
> Time period 1 : no dropouts
> Error in eval(expr, envir, enclos) : Object "in.period.i" not found
> In addition: Warning message: 
> This is an experimental procedure.  It should only be used for testing,
> as results are incorrect. 
> in: rm.impute(pformula = ~pre + pro + sex, y = df$y, last = df$last, 
> 
> 
> Can some of the experts explain me what I did wrong?
> 
> Kind regards,
> 
> Dominik

As stated in the documentation, rm.impute is "under development and is not
correct at present".  It awaits someone to re-write it.  It is not
supported.
---
Frank E Harrell Jr   Professor and Chair           School of Medicine
                     Department of Biostatistics   Vanderbilt University



From Torsten.Hothorn at rzmail.uni-erlangen.de  Fri Dec 19 18:04:36 2003
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Fri, 19 Dec 2003 18:04:36 +0100 (CET)
Subject: [R] `useR! 2004': Submission & Registration started
Message-ID: <Pine.LNX.4.51.0312191802400.16585@artemis.imbe.med.uni-erlangen.de>


We are happy to inform you that the topics of the keynote lectures to be
presented at the first R user conference `useR! 2004' in Vienna (May
20-22th) are now available from the conference web page at

        http://www.ci.tuwien.ac.at/Conferences/useR-2004/

The submission process for oral and poster presentations started this
month and a web form for registration is available since last week.

The Xmas days are a perfect time to write and submit an abstract, the
final deadline for the submission of oral or poster presentations is
February 15th, 2004. Authors will be notified about the acceptance of
their contribution until March 15th. For poster presentations only, an
extended final deadline ends April 15th.

A poster highlighting the most important informations about the conference
is available from the conference web page. It would be great if you could
put up a poster in your department if only to demonstrate that using R is
serious research and not only a hobby for computer geeks.

We wish you a Merry Xmas, a Happy New Year and hope to see you in Vienna!

Torsten, Achim, David.

_______________________________________________
R-announce at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-announce



From spencer.graves.at.pdf.com at stat.math.ethz.ch  Fri Dec 19 18:31:24 2003
From: spencer.graves.at.pdf.com at stat.math.ethz.ch (Spencer Graves)
Date: Fri Dec 19 18:31:24 2003
Subject: [R] iterative proportional fitting
In-Reply-To: <Sea2-F62UuMSe7fpAdA0005d79c@hotmail.com>
References: <Sea2-F62UuMSe7fpAdA0005d79c@hotmail.com>
Message-ID: <3FE329B8.8010705@pdf.com>

      Have you considered fitting on the log scale?  It's not clear to 
me what problem you are trying to solve, but the following will estimate 
parameters in a model that looks to me like what you are describing:  

      lm(z ~ (y1+y2+...+yn)^n, ...)

      hope this helps. 
      spencer

Zhen Pang wrote:

> Dear all,
>
> I wonder if there are some function or package in R which can do the 
> iterative proportional fitting.
>
> In the exponential model 
> f(y1,...yn)=exp(a'yi+b'(yi*yj)+.....+c'(y1*...*yn)+constance),
>
> instead of the canonical parameters, I use maginal probability instead 
> of a and log odds ratio instead of b. and for the order higher than 3, 
> I use the canonical way or even assume them to be 0.
>
> It is good if there is direct function or package in R to do the job, 
> which will save a lot of time of me. Thank you!
>
> Rgs,
>
> Zhen



From cougar3721.at.yahoo.com at stat.math.ethz.ch  Fri Dec 19 22:58:48 2003
From: cougar3721.at.yahoo.com at stat.math.ethz.ch (L Z)
Date: Fri Dec 19 22:58:48 2003
Subject: [R] [Mailman] question: contour plot for discrete data
Message-ID: <20031219201450.68952.qmail@web14810.mail.yahoo.com>

Question:
 I have matrix (n x3) that represents discrete data.
Each row of matrix is 3-D point (x,y,z). I would like
to get contour map (z value) at two dimension
(x,y). How can I use related contour function to do
this job?
I am not sure if I clarify this question. For example,
we can get point (x,y)
at 2 dimension according to first two columns of
matrix. Then I want to connect
same value z=(x,y). Thanks!
  zhang

The data file looks like this:
X          y               z
4           0.33          0.99
4           0.5            1.2
5           0.66          1.2
5           0.7            1.5
6           2               1.2
?



From tlumley at u.washington.edu  Fri Dec 19 23:44:14 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 19 Dec 2003 14:44:14 -0800 (PST)
Subject: [R] won't install properly on Mac 10.3.2
In-Reply-To: <5.1.0.14.2.20031219162808.01707498@po.muohio.edu>
References: <5.1.0.14.2.20031219162808.01707498@po.muohio.edu>
Message-ID: <Pine.A41.4.58.0312191441400.131236@homer06.u.washington.edu>

On Fri, 19 Dec 2003, Hank Stevens wrote:

> R version 1.8.1, Mac OS X 10.3.2
>
> I have tried searching for this  problem and its fix, but to no avail.
> -Everything seems to download and unpack fine. I double click on StartR,
> however, and it just winks and fails.
> Any thoughts?

If you open up a Console window  (go to Applications, then Utilities, then
select Console) you may get some more helpful error messages.

	-thomas



From rexbryan1 at comcast.net  Sat Dec 20 01:16:43 2003
From: rexbryan1 at comcast.net (rex_bryan@urscorp.com)
Date: Fri, 19 Dec 2003 17:16:43 -0700
Subject: [R] error bars around a point 
Message-ID: <006c01c3c68e$8c9f9470$1c7f0818@dell1700>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031219/89a8de76/attachment.pl

From abunn at montana.edu  Sat Dec 20 01:31:52 2003
From: abunn at montana.edu (Andy Bunn)
Date: Fri, 19 Dec 2003 17:31:52 -0700
Subject: [R] error bars around a point 
In-Reply-To: <006c01c3c68e$8c9f9470$1c7f0818@dell1700>
Message-ID: <003101c3c690$bc166380$78f05a99@msu.montana.edu>

See errbar in Hmisc. I found this with:

help.search("error bar")

And it's in the html help too. And on the r site:
http://cran.r-project.org/search.html
http://www.google.com/search?q=error+bar&domains=r-project.org&sitesearc
h=r-project.org

# Example
require(Hmisc)
set.seed(1)
x <- 1:10
y <- x + rnorm(10)
delta <- runif(10)
errbar( x, y, y + delta, y - delta )

HTH, Andy



From yunfang at yahoo-inc.com  Sat Dec 20 03:13:55 2003
From: yunfang at yahoo-inc.com (Yun-Fang Juan)
Date: Fri, 19 Dec 2003 18:13:55 -0800
Subject: [R] Error running LDA
References: <006c01c3c68e$8c9f9470$1c7f0818@dell1700>
Message-ID: <00b801c3c69e$ebab7280$90ea7ecf@YUNFANG2>

Hi,
I try to run the linear discriminant analysis using the following command
but got an error like the following.
lda1 <- lda(retention ~ . , data=RetentionDF40[1:10000,]);

Can someone tell me what I should do to fix the error?

thanks,


Yun-Fang



From ggrothendieck at myway.com  Sat Dec 20 03:31:50 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 19 Dec 2003 21:31:50 -0500 (EST)
Subject: [R] for loop over dataframe without indices
Message-ID: <20031220023150.CC57539B3@mprdmxin.myway.com>



Thomas, Thanks for your response.  Its is quite nifty.  

Pursuing your solutions,
I think the objective should be to reproduce the output from 
t.data.frame defined as below (note that I posted a proposal
to change t.data.frame to r-devel before I received your reply):

t.data.frame <- function( df ) { 
          ll <- NULL
          for( i in 1:nrow(df) ) ll <- append( ll, list(df[i,]) )
          ll 
}

Using the first 3 rows from the iris data set as our data frame,
run the following which shows that your "by" solution works provided
we nullify out the attributes afterwards.  The do.call solution
does not appear to work, as required, since it turns the data 
frame into a matrix.

data(iris)
df <- iris[1:3,]

# Consider:

id <- function(x)x

# t.data.frame solution
zt <- t(df)

# by solution is good but it adds some junk attributes 
zby <- by( df, row.names(df), id )
identical(zt,zby) # FALSE

# nullifying these attributes seems to do it
zby2 <- zby
attributes(zby2) <- NULL
identical(zt,zby2) # TRUE

# do.call doesn't work right since it appears to turn the result into a matrix
str( do.call("mapply", list(id,df) ) ) # note matrix output


Here is the result of pasting the above into R 1.8.1 on Windows 2000:

> data(iris)
> df <- iris[1:3,]
> 
> # Consider:
> 
> id <- function(x)x
> 
> # t.data.frame solution
> zt <- t(df)
> 
> # by solution is good but it adds some junk attributes 
> zby <- by( df, row.names(df), id )
> identical(zt,zby)
[1] FALSE
> 
> # nullifying these attributes seems to do it
> zby2 <- zby
> attributes(zby2) <- NULL
> identical(zt,zby2)
[1] TRUE
> 
> # do.call doesn't work right since it appears to turn the result into a matrix
> str( do.call("mapply", list(id,df) ) )
 num [1:3, 1:5] 5.1 4.9 4.7 3.5 3 3.2 1.4 1.4 1.3 0.2 ...
 - attr(*, "dimnames")=List of 2
  ..$ : NULL
  ..$ : NULL
> 


Based on your solution I think the proposal should be changed
to:

t.data.frame <- function(df) {
  z <- by( df, row.names(df), function(x)x )
  attributes(z) <- NULL
  z
}


---

Date: Fri, 19 Dec 2003 10:03:55 -0800 (PST) 
From: Thomas Lumley <tlumley at u.washington.edu>
To: Gabor Grothendieck <ggrothendieck at myway.com> 
Cc: <R-help at stat.math.ethz.ch> 
Subject: Re: [R] for loop over dataframe without indices 

 
 
On Fri, 19 Dec 2003, Gabor Grothendieck wrote:
>
> What I now realize is that the thing that is oddly
> missing in R is that you can't do an apply over
> the rows of a dataframe (at least not without having
> it coerced to an array and the elements coerced to
> possibly different types). The documentation does
> point this out. Its not a bug but its an omission
> that seems deserving of being addressed.
>

Since mapply() applies a function to each 'row' of a list of vectors, ou
can achieve this effect with
     do.call("mapply", list(FUN,data.frame))
and also as a degenerate case of by():
     by(data.frame, row.names(data.frame), FUN)

These should probably be documented under apply()


     -thomas



From ggrothendieck at myway.com  Sat Dec 20 03:34:08 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 19 Dec 2003 21:34:08 -0500 (EST)
Subject: [R] error bars around a point 
Message-ID: <20031220023408.2B4FC3999@mprdmxin.myway.com>



Check out plotCI in package gregmisc.

---

Date: Fri, 19 Dec 2003 17:16:43 -0700 
From: rex_bryan at urscorp.com <rexbryan1 at comcast.net>
To: <r-help at stat.math.ethz.ch> 
Subject: [R] error bars around a point  

 
 
All
Is there an R command to produce error bars around
a plotted point. Crawley's book uses the command
error.bar() but my version of R rejects it. Must be an
S+ command(?).

Thanks
REX



From feldesmanm at pdx.edu  Sat Dec 20 03:38:12 2003
From: feldesmanm at pdx.edu (Marc R. Feldesman)
Date: Fri, 19 Dec 2003 18:38:12 -0800
Subject: [R] Error running LDA
In-Reply-To: <00b801c3c69e$ebab7280$90ea7ecf@YUNFANG2>
References: <006c01c3c68e$8c9f9470$1c7f0818@dell1700>
	<00b801c3c69e$ebab7280$90ea7ecf@YUNFANG2>
Message-ID: <6.0.1.1.2.20031219183708.02465ec0@pop4.attglobal.net>

At 06:13 PM 12/19/2003, Yun-Fang Juan wrote:
 >Hi,
 >I try to run the linear discriminant analysis using the following command
 >but got an error like the following.
 >lda1 <- lda(retention ~ . , data=RetentionDF40[1:10000,]);
 >

What error did you get?  Did you read the help file for lda?  What version 
of R and what version of VR are you using?  What platform?



From yunfang at yahoo-inc.com  Sat Dec 20 03:58:40 2003
From: yunfang at yahoo-inc.com (Yun-Fang Juan)
Date: Fri, 19 Dec 2003 18:58:40 -0800
Subject: [R] Error running LDA
References: <006c01c3c68e$8c9f9470$1c7f0818@dell1700>
	<00b801c3c69e$ebab7280$90ea7ecf@YUNFANG2>
	<6.0.1.1.2.20031219183708.02465ec0@pop4.attglobal.net>
Message-ID: <00c801c3c6a5$2c3414a0$90ea7ecf@YUNFANG2>

oh sorry
here is the error.
Please advice.

thanks,

Yun-Fang

> lda1 <- lda(retention ~ . , data=RetentionDF40[1:10000,]);
Error in lda.default(x, grouping, ...) : variable(s)  11  14  24  29  30  31
32  33  34  35  41  43  44  47  48  49  50  51  54  55  56  57  58  59  60
61  65  66  67  68  70  71  72  74  78  82  85  87  88  89  91  92  93 101
102 103 107 108 109 111 112 117 122 124 125 127 148 151 153 155 156 162 170
173 176 181 182 183 184 186 189 190 192 193 194 196 200 209 212 213 214 215
216 218 220 222 223 224 228 230 231 234 235 236 237 appear to be constant
within groups

----- Original Message -----
From: "Marc R. Feldesman" <feldesmanm at pdx.edu>
To: "Yun-Fang Juan" <yunfang at yahoo-inc.com>; <r-help at stat.math.ethz.ch>
Sent: Friday, December 19, 2003 6:38 PM
Subject: Re: [R] Error running LDA


> At 06:13 PM 12/19/2003, Yun-Fang Juan wrote:
>  >Hi,
>  >I try to run the linear discriminant analysis using the following
command
>  >but got an error like the following.
>  >lda1 <- lda(retention ~ . , data=RetentionDF40[1:10000,]);
>  >
>
> What error did you get?  Did you read the help file for lda?  What version
> of R and what version of VR are you using?  What platform?
>
>



From feh3k at spamcop.net  Sat Dec 20 04:52:08 2003
From: feh3k at spamcop.net (Frank E Harrell Jr)
Date: Fri, 19 Dec 2003 22:52:08 -0500
Subject: [R] diagnostic information in glm. How about N of missing
	observations?
In-Reply-To: <3FE2F4B5.8080207@lancaster.ac.uk>
References: <3FE1FFAB.8010709@ku.edu>
	<20031218185104.665cdb71.f.harrell@vanderbilt.edu>
	<3FE2F4B5.8080207@lancaster.ac.uk>
Message-ID: <20031219225208.1a21009e.feh3k@spamcop.net>

On Fri, 19 Dec 2003 12:53:09 +0000
Barry Rowlingson <B.Rowlingson at lancaster.ac.uk> wrote:

> Frank E Harrell Jr wrote:
> 
> > Fitting functions in the Design package tell you how many observations
> > were deleted due to each variable in the model.  They generalize the
> > na.action component stored in the fit object.
> 
>   Surely accessing components of an object is generally a bad idea on 
> O-O design, and there ought to be a _method_ that returns which 
> observations have been omitted. Perhaps an 'omitted' function?

That's perhaps a matter of taste, but anyway I always want to see such
information, so it is always printed by the fitting function's print
method.  The fit object in Design has an element na.action which has an
element nmiss with the details.  You can also set an option for further
details, which then are stored in the element na.detail.response.  This
information provides brief data on the distribution of the response
variable stratified by missingness status of each predictor.

Frank

---
Frank E Harrell Jr   Professor and Chair           School of Medicine
                     Department of Biostatistics   Vanderbilt University



From tplate at acm.org  Sat Dec 20 06:55:53 2003
From: tplate at acm.org (Tony Plate)
Date: Fri, 19 Dec 2003 22:55:53 -0700
Subject: [R] draft of posting guide
Message-ID: <5.1.0.14.2.20031219215811.02c531f8@pop6.attglobal.net>

Here is a first draft of a guide for posters to r-help and
r-devel.  Suggestions on how to improve any aspect of it are
most welcome.  Suggestions on ways to make it more concise
are especially welcome.  Comments on which parts you like
or don't like are welcome.  Rather than clutter up R-help with
lots of small corrections etc, please send them to me, and I
will try to incorporate and summarize appropriately.

I've left out most HTML formatting (except for headings) to
make it readable -- I'll insert that later (to conform with
the style on the other pages at r-project.org.)  I've placed
"???" in places where feedback is specifically needed.

<h2>Posting Guide: How to ask good questions that prompt useful
answers</h2>

* Remember that R is free software, constructed and
  maintained by volunteers.  They have various reasons for
  contributing to R, but often have limited time.  Remember
  that no one owes you anything, and if you rude or
  disrespectful in your questions, you are likely to either
  be ignored or have the sentiment returned to you.

* The R mailing lists are primarily intended for questions
  and discussion about the R software.  However, sometimes
  questions about statistical methodology are posted.  [???
  are these discouraged/tolerated/... ???] People are far
  less likely to respond to these types of questions than to
  questions about the R software.  Depending on how much the
  post shows thought and background research, responses may
  merely (and sometimes brusquely) suggest that you should
  be seeking statistical consulting advice elsewhere.
  Sometimes, if the question is well-asked AND of interest
  to someone on the list, you may get an informative,
  up-to-date answer.

* Don't expect R-help to teach you basic statistics.  That's
  what statistics textbooks and statistics classes are for.

* If you have a question about R that you want answered,
  don't be lazy.  Do your homework first.  If it is clear
  that you have done your homework, your are far more likely
  to get an informative response.  The type of homework that
  needs to be done depends on the type of question --
  details are in the following paragraphs.

* Make it easy for people to answer your question: be clear
  and concise, remove unnecessary details, and, if they
  might be useful, provide brief examples.

* A much more detailed (and highly recommended) essay on how
  to ask questions on mailing lists about software is Eric
  Raymond's "How To Ask Questions The Smart Way"
  http://www.catb.org/~esr/faqs/smart-questions.html. (Note
  that catb.org has no association with the R project and
  will not respond to questions concerning R.)

<h3>Homework before posting a question</h3>

Before posting any question, try to do at least the
following background research:

* help.search("<keyword>") at least 3 times, each time
  with a keyword from your problem description (including
  any synonyms you can think of)

* read the online help for relevant functions (usually can
  be accessed by typing "?functionname", e.g., "?prod" at
  the prompt.)

* search the R-faq (and the R-windows-faq if it might be
  relevant)

* search R-help archives (see under the heading "Archives
  and Search Facilities" above)


<h3>Questions about specific functions</h3>

* If you have a question about a particular function, make
  sure you have thoroughly read the documentation for that
  function (often accessible via help("<functionname>"),
  e.g., help("summary")).  If you don't understand some
  aspect of the documentation, it's OK to post a question
  about that, but do demonstrate that you have at least
  tried to read it (e.g., by including in your post the
  specific passage from the documentation, and stating why
  you don't understand it.)  In some cases, the
  documentation for a function may be in a book, e.g., as
  with the MASS package.  If this is the case, make sure you
  consult the appropriate book before posting.  This may
  require a visit to the bookstore or library, and if you
  are posting from a rich country or commercial organization
  it will be assumed you have access to such resources.

<h3>Questions about specific problems</h3>

* If you have a question about what functions can be used to
  approach a particular problem, but you are unable to find
  anything with a basic search, then in your posting you
  should state this, and the keywords you used, as well as
  giving a clear description of your problem.  It's best to
  keep this problem description as high level as possible.
  For example if you want to cluster some data so that you
  can make a postscript plot of hierarchical clusters, then
  by stating this you are more likely to get useful answers
  than by asking some lower-level question like "how to I
  specify a .ps suffix to a filename argument for the
  diana() function?" (this question intentionally nonsense).

* If you have a question about how to manipulate data, or a
  question about how to use a function on some specific
  data, you are far more likely to get a fast and useful
  response if you supply a concise and reproducible example.
  This might consist of just some example data, followed by
  a precise description of the data object that you want to
  transform it to.  Optionally, you could include failed
  attempts you made to solve the problem, with the question
  being how to modify these so they work.  In all cases,
  these should be R code (with output) that other R-help
  subscribers can cut and paste into their R command window.
  DO NOT post large examples -- if you problem is with a
  large data set, then edit it down to the bare essentials
  before posting.  However, do say what the size of the
  original data is -- that might have some bearing on the
  best solution.  For example, if you need to turn a 200 by
  50 numeric matrix into rows of records with three fields,
  don't post your 10000 element matrix, but condense it to
  something like the following:

  If I have a matrix x as follows:
  > x <- matrix(1:8, nrow=4, ncol=2, dimnames=list(c("A","B","C","D"), c("x","y"))
  > x
  x y
A 1 5
B 2 6
C 3 7
D 4 8
  >

  how can I turn it into a dataframe with three columns,
  named "row", "col", and "value", which have the dimension
  names as the values of "row" and "col", like this:

  > x.df
     row col value
  1    A    x      1
   ...
  (To which the answer might be:
  > x.df <- reshape(data.frame(row=rownames(x), x), direction="long", varying=list(colnames(x)),
  + times=colnames(x), v.names="value", timevar="col", idvar="row")
  ??? are there better answers than this using reshape() ???
  )
 
  Note that if you if have matrix or data frame data, it is
  sufficient to just include the printed version of it in
  your question, and other posters can copy this onto their
  clipboard and read it into R with a command like: > x <-
  read.table("clipboard", header=T) or > x <-
  as.matrix(read.table("clipboard", header=T)) [??? does
  something like this work under Unix ???]

<h3>Questions about surprising behavior or possible bugs</h3>

* If you suspect there is a bug in a particular function in
  R, be very careful about saying "this is a bug".
  Well-educated people of good intent can differ about many
  things, including what is the most desirable (or even
  "correct") behavior for mathematical and statistical
  software.  Note that some particular behavior is generally
  considered to be a bug ONLY if the behavior of the
  software contradicts the documentation.  So before you
  claim a bug, make sure you read the documentation very
  carefully.  If you're not completely and utterly sure
  something is a bug, post a question to r-help, not a bug
  report to r-devel.  If some behavior seems very strange to
  you, e.g., you think prod(numeric(0)) should be NA, not 1,
  then read the documentation. If that does not make things
  clear to you, don't post on r-devel saying "I found a bug:
  prod(numeric(0)) should be NA, not 1."  Instead, post on
  r-help, asking"Is it the intended behavior that
  prod(numeric(0)) is 1?  If so, can anyone tell me why?"
  Note that incorrect bug reports put into the R-project
  bug-tracking system are a big waste of people's time
  because one of the R-core members must manually respond to
  the bug report to clear it out.  Before you post a bug
  report, make sure you read the section on how to post a
  bug report in the R-faq
  "http://CRAN.R-project.org/doc/FAQ/R-FAQ.html#R Bugs".
  Also, see Simon Tatham's essay at
  http://www.chiark.greenend.org.uk/~sgtatham/bugs.html

* In any question regarding unusual or unexpected behavior,
  or interaction with the operating system, always state the
  version of R you are using (which is like "1.8.1", NOT
  just "1.8"), the operating system you are using, and
  whether you installed a pre-compiled binary version of R
  or compiled it yourself.

<h3>Common posting mistakes</h3>

Doing any of the following may result in you getting a
response that you may find rude or insulting.  (However,
such a response may be justified in the eyes of some because
you have wasted people's time or unjustly insulted people's
work.)

* Not doing your homework before posting a question.

* Asking R-help to do your classroom homework for you
  (remember that many of the subscribers of R-help are
  university professors and can recognize homework questions
  with great speed and accuracy)

* Claiming that something is a bug when in fact the software
  is working as intended and documented, just not in the way
  you first expected.

* Claiming that some commonly used function is not behaving
  in what you think is a sensible manner (it's far more
  productive and polite to just ask why it behaves the way
  it does if you think it is odd -- but only after reading
  all the relevant documentation!)

<h3>Final words</h3>

It is an skill to ask good questions.  If at first you don't
get the answers that are useful to you, don't get
discouraged.  If you feel insulted by some response to a
post of yours, don't make any hasty response in return
(you're as likely as not to regret it.)  Go read Eric
Raymond's essay -
http://www.catb.org/~esr/faqs/smart-questions.html - it has
some explanation of people's behavior on technical mailing
lists.

Think carefully before you respond to another poster with a
demeaning or insulting comment.  Is it really necessary?
What will it achieve?  Remember that it's far easier to
maintain the respect of other people if you are polite -- if
you are rude your technical contributions had better be 110%
correct, otherwise you'll just look like an ignorant jerk.

END OF POSTING GUIDE

Since Martin Maechler indicated such a guide would be
welcome, I'm assuming it could go somewhere on the
R-project.org site.  It could be placed wherever the
R-project site maintainers wish, though to be most useful it
should be easy to find by clicking on the link at the bottom
of each posting to r-help and r-devel (ie
https://www.stat.math.ethz.ch/mailman/listinfo/r-help &
https://www.stat.math.ethz.ch/mailman/listinfo/r-devel).
One possibility would be to make it a section of the
"General Instructions" page (
http://www.r-project.org/mail). Another would be to make an
independent "Posting Guide" page.

In either case, some more specific words on
https://www.stat.math.ethz.ch/mailman/listinfo/r-help than
just "Please read the General Instructions on the R Mailing
Lists page."  might be helpful, e.g.: "For some guidelines
on what to post and how to post in a way that maximizes your
chances of getting useful answers, please read the posting
guide..."

-- Tony Plate



From ripley at stats.ox.ac.uk  Sat Dec 20 09:30:19 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 20 Dec 2003 08:30:19 +0000 (GMT)
Subject: [R] Contrasts for MANOVA
In-Reply-To: <002401c3c675$661a0380$78f05a99@msu.montana.edu>
Message-ID: <Pine.LNX.4.44.0312200821360.6387-100000@gannet.stats>

On Fri, 19 Dec 2003, Andy Bunn wrote:

> A colleague asked me if R can do contrasts for MANOVA. SAS will using
> PROC GLM with CONTRAST but can R? I assured her that R can do everything
> but make coffee. If this is trivial then I apologize in advance.

Depends what you mean: I don't believe that SAS command does what I 
understand by manova, at least as its main purpose.

Both lm() and aov() handle multiple responses.

aov() in R is really set up to consider terms, but its summary method can 
split terms (via its `split' argument).

What summary.manova does which the other aov methods does not is to
compute multivariate test statistics for each term.  If your colleague
wants both the splitting allowed in summary.aov and the multivariate test
statistics of summary.manova then that is not currently supported. What I
would do there is to is ensure each of the contrasts I wanted had a
separate term in the model, and then use summary(manova(...)).

As ever, an example of what you want would help, as I have been guessing 
rather.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sat Dec 20 09:36:52 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 20 Dec 2003 08:36:52 +0000 (GMT)
Subject: [R] Error running LDA
In-Reply-To: <00c801c3c6a5$2c3414a0$90ea7ecf@YUNFANG2>
Message-ID: <Pine.LNX.4.44.0312200832400.6387-100000@gannet.stats>

As it says, some of your variables appear not to vary, which means LDA is 
not appropriate.  LDA assumes a common multivariate normal distribution 
for the variation of each group about the group centre, and that cannot 
generate data like yours.

On Fri, 19 Dec 2003, Yun-Fang Juan wrote:

> oh sorry
> here is the error.
> Please advice.
> 
> thanks,
> 
> Yun-Fang
> 
> > lda1 <- lda(retention ~ . , data=RetentionDF40[1:10000,]);
> Error in lda.default(x, grouping, ...) : variable(s)  11  14  24  29  30  31
> 32  33  34  35  41  43  44  47  48  49  50  51  54  55  56  57  58  59  60
> 61  65  66  67  68  70  71  72  74  78  82  85  87  88  89  91  92  93 101
> 102 103 107 108 109 111 112 117 122 124 125 127 148 151 153 155 156 162 170
> 173 176 181 182 183 184 186 189 190 192 193 194 196 200 209 212 213 214 215
> 216 218 220 222 223 224 228 230 231 234 235 236 237 appear to be constant
> within groups
> 
> ----- Original Message -----
> From: "Marc R. Feldesman" <feldesmanm at pdx.edu>
> To: "Yun-Fang Juan" <yunfang at yahoo-inc.com>; <r-help at stat.math.ethz.ch>
> Sent: Friday, December 19, 2003 6:38 PM
> Subject: Re: [R] Error running LDA
> 
> 
> > At 06:13 PM 12/19/2003, Yun-Fang Juan wrote:
> >  >Hi,
> >  >I try to run the linear discriminant analysis using the following
> command
> >  >but got an error like the following.
> >  >lda1 <- lda(retention ~ . , data=RetentionDF40[1:10000,]);
> >  >
> >
> > What error did you get?  Did you read the help file for lda?  What version
> > of R and what version of VR are you using?  What platform?
> >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Sat Dec 20 11:20:58 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Dec 2003 11:20:58 +0100
Subject: [R] draft of posting guide
In-Reply-To: <5.1.0.14.2.20031219215811.02c531f8@pop6.attglobal.net>
References: <5.1.0.14.2.20031219215811.02c531f8@pop6.attglobal.net>
Message-ID: <x2ad5n3i45.fsf@biostat.ku.dk>

Tony Plate <tplate at acm.org> writes:

> <h3>Common posting mistakes</h3>
> 
> Doing any of the following may result in you getting a
> response that you may find rude or insulting.  (However,
> such a response may be justified in the eyes of some because
> you have wasted people's time or unjustly insulted people's
> work.)
> 
> * Not doing your homework before posting a question.
> 
> * Asking R-help to do your classroom homework for you
>   (remember that many of the subscribers of R-help are
>   university professors and can recognize homework questions
>   with great speed and accuracy)
> 
> * Claiming that something is a bug when in fact the software
>   is working as intended and documented, just not in the way
>   you first expected.
> 
> * Claiming that some commonly used function is not behaving
>   in what you think is a sensible manner (it's far more
>   productive and polite to just ask why it behaves the way
>   it does if you think it is odd -- but only after reading
>   all the relevant documentation!)

* Threatening not to use the software if you cannot get your question
  answered. Even when intended as a statement of fact, it tends to
  create negative attitudes.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ggrothendieck at myway.com  Sat Dec 20 13:51:14 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 20 Dec 2003 07:51:14 -0500 (EST)
Subject: [R] draft of posting guide
Message-ID: <20031220125114.70000398C@mprdmxin.myway.com>



Thanks for your effort.  Here are some comments.

1. The guidelines seem only to cover people
_asking_ questions.  What about those answering?

2. I think "bug" needs to be taken in the broader
sense of a problem.  Such problems are not limited
to discrepancies between documentation and
implementation.  The real serious problems are
design problems and may not represent such
deviations at all.  If the behavior is unexpected
it may very well be a problem even if its not a
bug in the narrow sense.

3. Rude answers are never warranted.

4. Its too long and there are too many "rules".  

5. The tone should be friendlier -- more helpful
and inviting.  Words like "lazy" should be
eliminated.  Although R is not a commercial
enterprise and its users are not customers in the
usual sense, its users should still be treated
with respect and encouraged rather than
admonished.  A customer oriented attitude will
be appreciated and encourage more participation.
The purpose of this should not be so
much to tell posters what they can do and not do
but to help them elicit useful responses and
to help responders provide userful answers.

6. Points should be numbered so they can be
referred to.  Each one should have an address so
that one can refer to them individually like this:
http://www.r-project.org/...whatever.../etiquette.html#3.1

7. I personally find the statistics questions
interesting and would not like to see guidelines
about not posting statistical problems.

8. Where documents are referred to, provide a URL.
Where R is being referred to point that out.  For
example, help.search is an R command.  This may be
obvious to some but it might not be obvious to
someone just coming to R.

9. It might be worthwhile to point out that the
following google search works:       
   whatever site:r-project.org

The real problem with the lists is not the
posters.  The real problems are:

- rude answers

- insufficient discussion about the direction and
  design and, in general, higher level issues.  If
  you compare this to ruby, python, perl and lua
  there are all sorts of interesting proposals and
  discussions on this on their lists with little
  counterpart on R's.

> Date: Fri, 19 Dec 2003 22:55:53 -0700 
> From: Tony Plate <tplate at acm.org>
> To: <r-help at stat.math.ethz.ch> 
> Subject: [R] draft of posting guide 
> 
>  
>  
> Here is a first draft of a guide for posters to r-help and
> r-devel. Suggestions on how to improve any aspect of it are
> most welcome. Suggestions on ways to make it more concise
> are especially welcome. Comments on which parts you like
> or don't like are welcome. Rather than clutter up R-help with
> lots of small corrections etc, please send them to me, and I
> will try to incorporate and summarize appropriately.
> 
> I've left out most HTML formatting (except for headings) to
> make it readable -- I'll insert that later (to conform with
> the style on the other pages at r-project.org.) I've placed
> "???" in places where feedback is specifically needed.
> 
> <h2>Posting Guide: How to ask good questions that prompt useful
> answers</h2>
> 
> * Remember that R is free software, constructed and
> maintained by volunteers. They have various reasons for
> contributing to R, but often have limited time. Remember
> that no one owes you anything, and if you rude or
> disrespectful in your questions, you are likely to either
> be ignored or have the sentiment returned to you.
> 
> * The R mailing lists are primarily intended for questions
> and discussion about the R software. However, sometimes
> questions about statistical methodology are posted. [???
> are these discouraged/tolerated/... ???] People are far
> less likely to respond to these types of questions than to
> questions about the R software. Depending on how much the
> post shows thought and background research, responses may
> merely (and sometimes brusquely) suggest that you should
> be seeking statistical consulting advice elsewhere.
> Sometimes, if the question is well-asked AND of interest
> to someone on the list, you may get an informative,
> up-to-date answer.
> 
> * Don't expect R-help to teach you basic statistics. That's
> what statistics textbooks and statistics classes are for.
> 
> * If you have a question about R that you want answered,
> don't be lazy. Do your homework first. If it is clear
> that you have done your homework, your are far more likely
> to get an informative response. The type of homework that
> needs to be done depends on the type of question --
> details are in the following paragraphs.
> 
> * Make it easy for people to answer your question: be clear
> and concise, remove unnecessary details, and, if they
> might be useful, provide brief examples.
> 
> * A much more detailed (and highly recommended) essay on how
> to ask questions on mailing lists about software is Eric
> Raymond's "How To Ask Questions The Smart Way"
> http://www.catb.org/~esr/faqs/smart-questions.html. (Note
> that catb.org has no association with the R project and
> will not respond to questions concerning R.)
> 
> <h3>Homework before posting a question</h3>
> 
> Before posting any question, try to do at least the
> following background research:
> 
> * help.search("<keyword>") at least 3 times, each time
> with a keyword from your problem description (including
> any synonyms you can think of)
> 
> * read the online help for relevant functions (usually can
> be accessed by typing "?functionname", e.g., "?prod" at
> the prompt.)
> 
> * search the R-faq (and the R-windows-faq if it might be
> relevant)
> 
> * search R-help archives (see under the heading "Archives
> and Search Facilities" above)
> 
> 
> <h3>Questions about specific functions</h3>
> 
> * If you have a question about a particular function, make
> sure you have thoroughly read the documentation for that
> function (often accessible via help("<functionname>"),
> e.g., help("summary")). If you don't understand some
> aspect of the documentation, it's OK to post a question
> about that, but do demonstrate that you have at least
> tried to read it (e.g., by including in your post the
> specific passage from the documentation, and stating why
> you don't understand it.) In some cases, the
> documentation for a function may be in a book, e.g., as
> with the MASS package. If this is the case, make sure you
> consult the appropriate book before posting. This may
> require a visit to the bookstore or library, and if you
> are posting from a rich country or commercial organization
> it will be assumed you have access to such resources.
> 
> <h3>Questions about specific problems</h3>
> 
> * If you have a question about what functions can be used to
> approach a particular problem, but you are unable to find
> anything with a basic search, then in your posting you
> should state this, and the keywords you used, as well as
> giving a clear description of your problem. It's best to
> keep this problem description as high level as possible.
> For example if you want to cluster some data so that you
> can make a postscript plot of hierarchical clusters, then
> by stating this you are more likely to get useful answers
> than by asking some lower-level question like "how to I
> specify a .ps suffix to a filename argument for the
> diana() function?" (this question intentionally nonsense).
> 
> * If you have a question about how to manipulate data, or a
> question about how to use a function on some specific
> data, you are far more likely to get a fast and useful
> response if you supply a concise and reproducible example.
> This might consist of just some example data, followed by
> a precise description of the data object that you want to
> transform it to. Optionally, you could include failed
> attempts you made to solve the problem, with the question
> being how to modify these so they work. In all cases,
> these should be R code (with output) that other R-help
> subscribers can cut and paste into their R command window.
> DO NOT post large examples -- if you problem is with a
> large data set, then edit it down to the bare essentials
> before posting. However, do say what the size of the
> original data is -- that might have some bearing on the
> best solution. For example, if you need to turn a 200 by
> 50 numeric matrix into rows of records with three fields,
> don't post your 10000 element matrix, but condense it to
> something like the following:
> 
> If I have a matrix x as follows:
> > x <- matrix(1:8, nrow=4, ncol=2, dimnames=list(c("A","B","C","D"), c("x","y"))
> > x
> x y
> A 1 5
> B 2 6
> C 3 7
> D 4 8
> >
> 
> how can I turn it into a dataframe with three columns,
> named "row", "col", and "value", which have the dimension
> names as the values of "row" and "col", like this:
> 
> > x.df
> row col value
> 1 A x 1
> ...
> (To which the answer might be:
> > x.df <- reshape(data.frame(row=rownames(x), x), direction="long", varying=list(colnames(x)),
> + times=colnames(x), v.names="value", timevar="col", idvar="row")
> ??? are there better answers than this using reshape() ???
> )
> 
> Note that if you if have matrix or data frame data, it is
> sufficient to just include the printed version of it in
> your question, and other posters can copy this onto their
> clipboard and read it into R with a command like: > x <-
> read.table("clipboard", header=T) or > x <-
> as.matrix(read.table("clipboard", header=T)) [??? does
> something like this work under Unix ???]
> 
> <h3>Questions about surprising behavior or possible bugs</h3>
> 
> * If you suspect there is a bug in a particular function in
> R, be very careful about saying "this is a bug".
> Well-educated people of good intent can differ about many
> things, including what is the most desirable (or even
> "correct") behavior for mathematical and statistical
> software. Note that some particular behavior is generally
> considered to be a bug ONLY if the behavior of the
> software contradicts the documentation. So before you
> claim a bug, make sure you read the documentation very
> carefully. If you're not completely and utterly sure
> something is a bug, post a question to r-help, not a bug
> report to r-devel. If some behavior seems very strange to
> you, e.g., you think prod(numeric(0)) should be NA, not 1,
> then read the documentation. If that does not make things
> clear to you, don't post on r-devel saying "I found a bug:
> prod(numeric(0)) should be NA, not 1." Instead, post on
> r-help, asking"Is it the intended behavior that
> prod(numeric(0)) is 1? If so, can anyone tell me why?"
> Note that incorrect bug reports put into the R-project
> bug-tracking system are a big waste of people's time
> because one of the R-core members must manually respond to
> the bug report to clear it out. Before you post a bug
> report, make sure you read the section on how to post a
> bug report in the R-faq
> "http://CRAN.R-project.org/doc/FAQ/R-FAQ.html#R Bugs".
> Also, see Simon Tatham's essay at
> http://www.chiark.greenend.org.uk/~sgtatham/bugs.html
> 
> * In any question regarding unusual or unexpected behavior,
> or interaction with the operating system, always state the
> version of R you are using (which is like "1.8.1", NOT
> just "1.8"), the operating system you are using, and
> whether you installed a pre-compiled binary version of R
> or compiled it yourself.
> 
> <h3>Common posting mistakes</h3>
> 
> Doing any of the following may result in you getting a
> response that you may find rude or insulting. (However,
> such a response may be justified in the eyes of some because
> you have wasted people's time or unjustly insulted people's
> work.)
> 
> * Not doing your homework before posting a question.
> 
> * Asking R-help to do your classroom homework for you
> (remember that many of the subscribers of R-help are
> university professors and can recognize homework questions
> with great speed and accuracy)
> 
> * Claiming that something is a bug when in fact the software
> is working as intended and documented, just not in the way
> you first expected.
> 
> * Claiming that some commonly used function is not behaving
> in what you think is a sensible manner (it's far more
> productive and polite to just ask why it behaves the way
> it does if you think it is odd -- but only after reading
> all the relevant documentation!)
> 
> <h3>Final words</h3>
> 
> It is an skill to ask good questions. If at first you don't
> get the answers that are useful to you, don't get
> discouraged. If you feel insulted by some response to a
> post of yours, don't make any hasty response in return
> (you're as likely as not to regret it.) Go read Eric
> Raymond's essay -
> http://www.catb.org/~esr/faqs/smart-questions.html - it has
> some explanation of people's behavior on technical mailing
> lists.
> 
> Think carefully before you respond to another poster with a
> demeaning or insulting comment. Is it really necessary?
> What will it achieve? Remember that it's far easier to
> maintain the respect of other people if you are polite -- if
> you are rude your technical contributions had better be 110%
> correct, otherwise you'll just look like an ignorant jerk.
> 
> END OF POSTING GUIDE
> 
> Since Martin Maechler indicated such a guide would be
> welcome, I'm assuming it could go somewhere on the
> R-project.org site. It could be placed wherever the
> R-project site maintainers wish, though to be most useful it
> should be easy to find by clicking on the link at the bottom
> of each posting to r-help and r-devel (ie
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help &
> https://www.stat.math.ethz.ch/mailman/listinfo/r-devel).
> One possibility would be to make it a section of the
> "General Instructions" page (
> http://www.r-project.org/mail). Another would be to make an
> independent "Posting Guide" page.
> 
> In either case, some more specific words on
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help than
> just "Please read the General Instructions on the R Mailing
> Lists page." might be helpful, e.g.: "For some guidelines
> on what to post and how to post in a way that maximizes your
> chances of getting useful answers, please read the posting
> guide..."
> 
> -- Tony Plate
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From giojona at libero.it  Sat Dec 20 14:38:23 2003
From: giojona at libero.it (giojona@libero.it)
Date: Sat, 20 Dec 2003 14:38:23 +0100
Subject: [R] sound library
Message-ID: <HQ74JZ$06AF3B4F2C6C12AA330AB30ABFA6A38C@libero.it>

I'm collaborating with and electronic musician to experiment on the production of music from number sequences. As I'm an R user I started playing around with the sound library and I found it very useful. However there are several things I do not understand (I'm not an expert in acustic nor audio signal treatment).

The first thing I'd like to understand is: let s be a normalized sample and suppose it generates a waveform with non audible frequencies. Then if I play it trough the function "play" (I'm using an R-window version) I won't hear anything. Then I play: play(10000*s) and I can hear something. What is happening to the audio sample? 

I saved as wave files both s and 10000*s and then with my musician friend we looked at the waveforms we got.
They are totally different from each other and we couldn't fidure out which transformation of s generated the 10000*s waveform.

Can anyone explain what is going on?

Thanks in advance

Giovanna Jona Lasinio



From dmurdoch at pair.com  Sat Dec 20 15:29:12 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Sat, 20 Dec 2003 09:29:12 -0500
Subject: [R] draft of posting guide
In-Reply-To: <20031220125114.70000398C@mprdmxin.myway.com>
References: <20031220125114.70000398C@mprdmxin.myway.com>
Message-ID: <3el8uv0r6etg5r89ll8tv5m8ojun81o646@4ax.com>

On Sat, 20 Dec 2003 07:51:14 -0500 (EST), you wrote:

>2. I think "bug" needs to be taken in the broader
>sense of a problem.  Such problems are not limited
>to discrepancies between documentation and
>implementation.  The real serious problems are
>design problems and may not represent such
>deviations at all.  If the behavior is unexpected
>it may very well be a problem even if its not a
>bug in the narrow sense.

I agree that discussion of those things is good (probably better in
r-devel than in r-help).  However, this discussion shouldn't take
place in postings that are submitted as bug reports to r-bugs.  What a
lot of people don't seem to realize is that every posting to r-bugs
creates work for someone in the core group.  If you have some idea for
R that doesn't interest me, I can ignore it in r-help or r-devel, but
not in r-bugs.

The draft guide should be changed to mention this:

 If you're not completely and utterly sure
  something is a bug, post a question to r-help, not a bug
  report to r-devel.  

should be

 If you're not completely and utterly sure
  something is a bug, post a question to r-help or r-devel, not a bug
  report to r-bugs.  

>3. Rude answers are never warranted.

That's true, but needs expanding.  Rude questions aren't warranted
either, and not all apparently rude responses are really rude.

Better to say "Rudeness is never warranted, but don't take offense too
easily.  Sometimes RTFM *is* the appropriate response."

>4. Its too long and there are too many "rules".  

Which ones should be dropped?

>5. The tone should be friendlier -- more helpful
>and inviting.  Words like "lazy" should be
>eliminated.  

I thought "lazy" was used in a friendly and helpful way.  Which other
"words like" did you have in mind?

>Although R is not a commercial
>enterprise and its users are not customers in the
>usual sense, its users should still be treated
>with respect and encouraged rather than
>admonished.  

>A customer oriented attitude will
>be appreciated and encourage more participation.

I agree with the "respect and encourage" part, but not the rest.
You're thinking too much about a divide between users and developers.
You should be thinking of everyone as participants in the R project.
The participants in R-core have special access and can change the
source code, but all users are participants, and should be prepared to
be criticized when appropriate.  They *aren't* customers, and
shouldn't expect to be treated as customers.

>The purpose of this should not be so
>much to tell posters what they can do and not do
>but to help them elicit useful responses and
>to help responders provide userful answers.

The former does help with the latter.

>The real problem with the lists is not the
>posters.  The real problems are:

Everything on the lists comes from the posters.  There is nothing
else.

Duncan Murdoch



From ligges at statistik.uni-dortmund.de  Sat Dec 20 16:25:48 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 20 Dec 2003 16:25:48 +0100
Subject: [R] sound library
In-Reply-To: <HQ74JZ$06AF3B4F2C6C12AA330AB30ABFA6A38C@libero.it>
References: <HQ74JZ$06AF3B4F2C6C12AA330AB30ABFA6A38C@libero.it>
Message-ID: <3FE469FC.3070608@statistik.uni-dortmund.de>

giojona at libero.it wrote:

> I'm collaborating with and electronic musician to experiment on the production of music from number sequences. As I'm an R user I started playing around with the sound library and I found it very useful. However there are several things I do not understand (I'm not an expert in acustic nor audio signal treatment).
> 
> The first thing I'd like to understand is: let s be a normalized sample and suppose it generates a waveform with non audible frequencies. Then if I play it trough the function "play" (I'm using an R-window version) I won't hear anything. Then I play: play(10000*s) and I can hear something. What is happening to the audio sample? 

Looks like it is not checked whether the generated "sound", i.e. a wave 
file, is valid. Using 16 bit resolution, you cannot have more than 2^16 
integers, but 10000*s contains invalid integers for the wave representation.


> I saved as wave files both s and 10000*s and then with my musician friend we looked at the waveforms we got.
> They are totally different from each other and we couldn't fidure out which transformation of s generated the 10000*s waveform.

Since you are writing invalid values to the file, those are interpreted 
in the "wrong" way and you might get something different.


> Can anyone explain what is going on?

Two more comments:
"sound" as is does not pass Rcmd check on Windows (because Media Player 
is not available on the system I do compile on, etc.) and has some more 
deficiencies. It's only available as a Windows binary on CRAN because of 
my personal interest in the package.
The author of the package, Matthias Heymann, has been informed, but he 
is not going to fix the bugs in the near future, unfortunately.

I am planing to write a package for the analysis of music (including 
loading / playing / saving wave files) next year myself.

Uwe Ligges



> Thanks in advance
> 
> Giovanna Jona Lasinio
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ggrothendieck at myway.com  Sat Dec 20 17:02:48 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 20 Dec 2003 11:02:48 -0500 (EST)
Subject: [R] for loop over dataframe without indices
Message-ID: <20031220160248.AAABC396B@mprdmxin.myway.com>




I think I've found a problem with the by approach.  Compare:

data(iris)
by( iris, row.names(iris), function(x)x )[1:5,]

to

iris[1:5,]

It seems by has reordered the rows.

 
Date: Fri, 19 Dec 2003 21:31:50 -0500 (EST) 
From: Gabor Grothendieck <ggrothendieck at myway.com>
To: <tlumley at u.washington.edu> 
Cc: <R-help at stat.math.ethz.ch> 
Subject: Re: [R] for loop over dataframe without indices 

 
 


Thomas, Thanks for your response. Its is quite nifty. 

Pursuing your solutions,
I think the objective should be to reproduce the output from 
t.data.frame defined as below (note that I posted a proposal
to change t.data.frame to r-devel before I received your reply):

t.data.frame <- function( df ) { 
ll <- NULL
for( i in 1:nrow(df) ) ll <- append( ll, list(df[i,]) )
ll 
}

Using the first 3 rows from the iris data set as our data frame,
run the following which shows that your "by" solution works provided
we nullify out the attributes afterwards. The do.call solution
does not appear to work, as required, since it turns the data 
frame into a matrix.

data(iris)
df <- iris[1:3,]

# Consider:

id <- function(x)x

# t.data.frame solution
zt <- t(df)

# by solution is good but it adds some junk attributes 
zby <- by( df, row.names(df), id )
identical(zt,zby) # FALSE

# nullifying these attributes seems to do it
zby2 <- zby
attributes(zby2) <- NULL
identical(zt,zby2) # TRUE

# do.call doesn't work right since it appears to turn the result into a matrix
str( do.call("mapply", list(id,df) ) ) # note matrix output


Here is the result of pasting the above into R 1.8.1 on Windows 2000:

> data(iris)
> df <- iris[1:3,]
> 
> # Consider:
> 
> id <- function(x)x
> 
> # t.data.frame solution
> zt <- t(df)
> 
> # by solution is good but it adds some junk attributes 
> zby <- by( df, row.names(df), id )
> identical(zt,zby)
[1] FALSE
> 
> # nullifying these attributes seems to do it
> zby2 <- zby
> attributes(zby2) <- NULL
> identical(zt,zby2)
[1] TRUE
> 
> # do.call doesn't work right since it appears to turn the result into a matrix
> str( do.call("mapply", list(id,df) ) )
num [1:3, 1:5] 5.1 4.9 4.7 3.5 3 3.2 1.4 1.4 1.3 0.2 ...
- attr(*, "dimnames")=List of 2
..$ : NULL
..$ : NULL
> 


Based on your solution I think the proposal should be changed
to:

t.data.frame <- function(df) {
z <- by( df, row.names(df), function(x)x )
attributes(z) <- NULL
z
}


---

Date: Fri, 19 Dec 2003 10:03:55 -0800 (PST) 
From: Thomas Lumley <tlumley at u.washington.edu>
To: Gabor Grothendieck <ggrothendieck at myway.com> 
Cc: <R-help at stat.math.ethz.ch> 
Subject: Re: [R] for loop over dataframe without indices 



On Fri, 19 Dec 2003, Gabor Grothendieck wrote:
>
> What I now realize is that the thing that is oddly
> missing in R is that you can't do an apply over
> the rows of a dataframe (at least not without having
> it coerced to an array and the elements coerced to
> possibly different types). The documentation does
> point this out. Its not a bug but its an omission
> that seems deserving of being addressed.
>

Since mapply() applies a function to each 'row' of a list of vectors, ou
can achieve this effect with
do.call("mapply", list(FUN,data.frame))
and also as a degenerate case of by():
by(data.frame, row.names(data.frame), FUN)

These should probably be documented under apply()


-thomas

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From jjava at priscian.com  Sun Dec 21 01:10:49 2003
From: jjava at priscian.com (Jim Java)
Date: Sat, 20 Dec 2003 16:10:49 -0800
Subject: [R] C++: Appending Values onto an R-Vector.
Message-ID: <200312201610.AA95814106@priscian.com>

[Jim Java]
>Hi folks. I posted this question a few days ago, but maybe it got
>lost because of the code I included with it. I'm having a problem
>using the SET_LENGTH() macro in an R extension I'm writing in C++.
>In a function within the extension I use SET_LENGTH() to resize R
>vectors so as to allow the concatenation of single values onto the
>vectors -- it's a "push back" function to append values onto the
>end of a vector. However, when I use this function to push back a
>large number of values one at a time, Rgui.exe (I'm working with R
>1.8.1 in Windows XP) crashes from an Access Violation; if, however,
>I pre-allocate space (is the space actually pre-allocated?) for the
>vector (say with NEW_INTEGER(n) rather than NEW_INTEGER(0)) and
>insert values into the allocated slots, the code works fine. If
>you'd like to see some test code, I've already posted it here:
>
>https://www.stat.math.ethz.ch/pipermail/r-help/2003-December/041871.html
>
>Here's my question, then: Is SET_LENGTH() the appropriate way to
>create space for tacking values onto the end of an R-vector in C++,
>or should I be trying to tack them on in some other way?

[Brian Ripley]
>You UNPROTECT before calling lengthgets, and that is I think part
>of your problem.  You need to use REPROTECT...

[Jim Java]
This problem is solved, so I want to post a follow-up for reference
in the archives. My thanks to Prof. Ripley for helping me out: the
problem was indeed in using UNPROTECT before lengthgets() rather than
using REPROTECT afterwards. My revised test code is listed below, also
for reference.

<CPP Code>
  SEXP R_SimplePushBackTest(SEXP args)
  {
    SEXP arg1, arg2, int_vect;

    PROTECT(arg1 = AS_INTEGER(CADR(args)));
    int n_reps = INTEGER_POINTER(arg1)[0];
    PROTECT(arg2 = AS_LOGICAL(CADDR(args)));
    bool full_alloc = (LOGICAL_POINTER(arg2)[0] ? true : false);
    PROTECT_INDEX int_vect_pindex;
    if (full_alloc)
      PROTECT_WITH_INDEX(int_vect = NEW_INTEGER(n_reps), &int_vect_pindex);
    else
      PROTECT_WITH_INDEX(int_vect = NEW_INTEGER(0), &int_vect_pindex);

    for (int i = 0; i < n_reps; ++i) {
      Rprintf("  ** Iteration %d:\n", i + 1);
      if (full_alloc)
        INTEGER_POINTER(int_vect)[i] = i;
      else {
        // This works now! --
        SET_LENGTH(int_vect, GET_LENGTH(int_vect) + 1);
        REPROTECT(int_vect, int_vect_pindex);
        INTEGER_POINTER(int_vect)[GET_LENGTH(int_vect) - 1] = i;
      }
    }

    SEXP out, names, cls;

    PROTECT(out = NEW_LIST(1));
    SET_VECTOR_ELT(out, 0, int_vect);

    PROTECT(names = NEW_CHARACTER(1));
    SET_STRING_ELT(names, 0, COPY_TO_USER_STRING("integer.vector"));
    SET_NAMES(out, names);

    PROTECT(cls = NEW_CHARACTER(1));
    SET_STRING_ELT(cls, 0, COPY_TO_USER_STRING("pushback"));
    classgets(out, cls);

    UNPROTECT(6);
    return out;
  }
</CPP Code>

<R Code>
  nreps=50000
  allocate=FALSE
  sink("pushback_test.txt")
  test.pushback=.External("R_SimplePushBackTest", as.integer(nreps), as.logical(allocate))
  print(test.pushback)
  sink()
</R Code>



From deepayan at stat.wisc.edu  Sat Dec 20 17:36:16 2003
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Sat, 20 Dec 2003 10:36:16 -0600
Subject: [R] Different scales for keys in lattice levelplots
In-Reply-To: <62AE0CF1D4875C4BBDEC29DB9924ACE87F2162@pnlmse25.pnl.gov>
References: <62AE0CF1D4875C4BBDEC29DB9924ACE87F2162@pnlmse25.pnl.gov>
Message-ID: <200312201036.16798.deepayan@stat.wisc.edu>

On Friday 19 December 2003 14:23, Waichler, Scott R wrote:
> Can anyone tell me how to obtain a custom scale for
> the colorkey in each levelplot of a lattice?  I am using
> lattice and levelplot to plot z = f(x, y) for multiple z
> variables.  x and y values are the same across plots, but
> units for z are different and therefore I need to create
> a custom scale for each plot's key.  It follows that I need
> to place each plot's key inside that plot's box.  Should I
> use filled.contour() and gridBase instead?

I'm not sure I understand your question. Are you using different calls to 
levelplot() for the different z variables ? What's wrong with the default 
colorkey for each such plot ? 

In any case, the arguments controlling colorkey are described in the help page 
for levelplot. Another way to control the scales (to be same across different 
calls to levelplot, say) is via the 'at' argument to levelplot (this can be 
used in a manner similar to what might have been expected from a 'zlim' 
argument, had one been available).

Hope that helps,

Deepayan



From rossini at blindglobe.net  Sat Dec 20 17:55:44 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Sat, 20 Dec 2003 08:55:44 -0800
Subject: [R] draft of posting guide
In-Reply-To: <20031220125114.70000398C@mprdmxin.myway.com> (Gabor
	Grothendieck's message of "Sat, 20 Dec 2003 07:51:14 -0500 (EST)")
References: <20031220125114.70000398C@mprdmxin.myway.com>
Message-ID: <85vfobxwbz.fsf@blindglobe.net>

"Gabor Grothendieck" <ggrothendieck at myway.com> writes:

> Thanks for your effort.  Here are some comments.
>
> 1. The guidelines seem only to cover people
> _asking_ questions.  What about those answering?

There is a reaosonble comment on that further down in Tony's list, and
also read Eric's guide to asking questions.  I really don't think you
want to go there.

> - insufficient discussion about the direction and
>   design and, in general, higher level issues.  If
>   you compare this to ruby, python, perl and lua
>   there are all sorts of interesting proposals and
>   discussions on this on their lists with little
>   counterpart on R's.

I'd claim that the folks inhabiting those lists, only the Python one
am I intimately familiar with, generally have a different set of
interests (though that is slowly changing with S/R being recognized as
a real programming language).  This list is of a far more
heterogeneous composition than those, and the points that you raise
here are more along the lines of R-devel, where that does occasionally
happen.

Of course, people that raise those issues, and back it up with
reasonable code and willingness to modify their work to fit the
general R picture, get sucked into R-core...(there are a few
exceptions, but very few).

best,
-tony

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From spencer.graves at pdf.com  Sat Dec 20 18:23:37 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 20 Dec 2003 09:23:37 -0800
Subject: [R] draft of posting guide
In-Reply-To: <20031220125114.70000398C@mprdmxin.myway.com>
References: <20031220125114.70000398C@mprdmxin.myway.com>
Message-ID: <3FE48599.4010604@pdf.com>

R-trivia: 

Gabor Grothendieck wrote:

><snip>9. It might be worthwhile to point out that the
>  
>
>following google search works:       
>   whatever site:r-project.org
>

      A Google search for "r" or "R" produces "www.r-project.org" as the 
first hit!  I don't know how long it will stay that way, but I first 
noticed it earlier this year. 

      Enjoy,
      spencer graves



From rexbryan1 at comcast.net  Sat Dec 20 20:12:22 2003
From: rexbryan1 at comcast.net (rex_bryan@urscorp.com)
Date: Sat, 20 Dec 2003 12:12:22 -0700
Subject: [R] weighted regression
References: <002001c3c5ca$a412ff80$1c7f0818@dell1700> <3FE370FD.6040900@ku.edu>
Message-ID: <003801c3c72d$32796c60$1c7f0818@dell1700>

Paul
Thanks for your quick response.  Yes I have
x and y backward.  Thomas Blackwell pointed that out to
me. Oops.  And yes I will be considering more than 3 points
for any regression.  This little data set was first presented by
a math wizard in a MathCAD forum explaining the mathematics
behind weighted regression.  When I tried Statistica on the data set
I  found that their definition of weights was a replication counter NOT in
the spirit of 1/var.
It put me into a blue funk ... I hate it when my state-of-the-art tools turn
out to be a sham.
I then turned to R and all appears to be mathemagically working well.
As I walk the dogs I'm thinking (amittedly confusedly) on how prediction and
confidence
intervals could be generated around a weighted regression line.  My thoughts
range from purloining concepts from kriging, or bootstapping ... in each
case
I come to the conclusion that this is more complex topic than I thought....
Oh well...it keeps me interested in talking to smart people like you.
Have a Merry Christmas
REX
----- Original Message -----
From: "Paul E. Johnson" <pauljohn at ku.edu>
To: "rex_bryan at urscorp.com" <Rex_Bryan at urscorp.com>
Sent: Friday, December 19, 2003 2:43 PM
Subject: Re: [R] weighted regression


>
> I keep new user tips at http://www.ku.edu/~pauljohn/R/Rtips.html
>
> I have some observations on your example:
>
> 1. You have y and x backward in your lm code
>
> 2. You get rid of the intercept by lm(y~x-1) but it is not generally
> advisable.  it will bias the slope parameter if your assumption is wrong.
>
> 3. You are right that Weighted Least Squares can be accomplished by
> weights in lm.  Please read the description of weight in ?lm
>
> weights: an optional vector of weights to be used in the fitting
>           process. If specified, weighted least squares is used with
>           weights 'weights' (that is, minimizing 'sum(w*e^2)');
>           otherwise ordinary least squares is used.
>
> If you understand WLS, then you will see how to enter the variable w.
>
> 4. I hope you aren't really doing regression with 3 observations.
>
> pj
>
>
> rex_bryan at urscorp.com wrote:
>
> >To all
> >
> >I have some simple questions pertaining to weights used in regression.
> >If the variability of the dependent variable (y) is a function of the
magnitude of predictor
> >variable (x), can the use of weights give an appropriate answer to the
regression parameters
> >and the std errors?
> >
> >Assume that y at x=1 and 6 has a standard deviation of 0.1 and at x=11 it
is 0.4
> >Then according to a web page on weighted regression for a calibration
curve at
> >http://member.nifty.ne.jp/mniwa/rev006.htm, I should use 1/(std^2) for
each weight.
> >
> >i.e. for x=1 and 6, w = 100 and x=11, w = 6.25
> >
> >In R the run is:
> >
> >
> >
> >>y<-c(1,6,11)
> >>x<-c(6.7,6.7,6.6)
> >>w<-c(100,100,6.25)
> >>reg <-lm(x~y, weight=w)
> >>summary(reg)
> >>
> >>
> >
> >Call:
> >lm(formula = x ~ y, weights = w)
> >
> >Residuals:
> >       1        2        3
> >-0.04762  0.09524 -0.19048
> >
> >Coefficients:
> >             Estimate Std. Error t value Pr(>|t|)
> >(Intercept)  6.707619   0.025431 263.762  0.00241 **
> >y           -0.002857   0.005471  -0.522  0.69361
> >---
> >Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
> >
> >Residual standard error: 0.2182 on 1 degrees of freedom
> >Multiple R-Squared: 0.2143,     Adjusted R-squared: -0.5714
> >F-statistic: 0.2727 on 1 and 1 DF,  p-value: 0.6936
> >
> >Am I using the weight method correctly?
> >And if so does the Estimated Std. Error for the Intercept and slope make
sense?
> >
> >On another note.  How does one do a regression with the origin fixed at
0?
> >
> >Merry Christmas
> >
> >REX
> >
> >
> >
> >
> >
> > [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
> >
>
>
> --
> Paul E. Johnson                       email: pauljohn at ukans.edu
> Dept. of Political Science            http://lark.cc.ukans.edu/~pauljohn
> University of Kansas                  Office: (785) 864-9086
> Lawrence, Kansas 66045                FAX: (785) 864-5700
>



From rexbryan1 at comcast.net  Sat Dec 20 20:41:35 2003
From: rexbryan1 at comcast.net (rex_bryan@urscorp.com)
Date: Sat, 20 Dec 2003 12:41:35 -0700
Subject: [R] error bars around a point
Message-ID: <005f01c3c731$470942a0$1c7f0818@dell1700>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031220/2a81a677/attachment.pl

From wolfgangpauli at web.de  Sat Dec 20 21:39:31 2003
From: wolfgangpauli at web.de (Wolfgang Pauli)
Date: Sat, 20 Dec 2003 21:39:31 +0100
Subject: [R] Huynh-Feldt correction
Message-ID: <200312202139.31048.wolfgangpauli@web.de>

Dear All,

I tried to help myself by searching in newsgroups etc. I found a test for
epsilon in a newsgroup (see attachement). I wrote a little skript to update
the summary.anova if epsilon < 0.75.
Although I doublechecked my statistic book for the formula, the result is
always the half of the epsilon calculated by SPSS. It is really strange. If I
multiply espilon by two, I get the right results...

regards,

WP

#------------------------------------begin -----------------------------

correctSummary <- function(sum,eps) {
	# eps is the epsilon-value returned by function "hf"
	# sum is the summary.aov object in which the Huynh-Fldt correction has to be
down
	len <- length(sum)
	newSum <- sum
	for (i in 2: len) {
	newSum[[i]][[1]][4,] <- newSum[[i]][[1]][2,]
	newSum[[i]][[1]][3,] <- newSum[[i]][[1]][2,]
	newSum[[i]][[1]][2,] <- newSum[[i]][[1]][1,]
	rownames(newSum[[i]][[1]]) <- c(rownames(newSum[[i]][[1]][1,]),
"Huynh-Feldt", rownames(newSum[[i]][[1]][2,]), "Huynh-Feldt")
	newSum[[i]][[1]][2,][1] <- newSum[[i]][[1]][2,][1]*epsilon*2
	newSum[[i]][[1]][4,][1] <- newSum[[i]][[1]][4,][1]*epsilon*2
	newSum[[i]][[1]][2,][3] <- newSum[[i]][[1]][2,][2]/newSum[[i]][[1]][2,][1]
	newSum[[i]][[1]][4,][3] <- newSum[[i]][[1]][4,][2]/newSum[[i]][[1]][4,][1]
	newSum[[i]][[1]][2,][4] <- newSum[[i]][[1]][2,][3]/newSum[[i]][[1]][4,][3]

	newSum[[i]][[1]][2,][5] <- pf(as.numeric(newSum[[i]][[1]][2,][4]),
as.numeric(newSum[[i]][[1]][2,][1]), as.numeric(newSum[[i]][[1]][4,][1]),
lower.tail = FALSE)
	}
	newSum

}

hf <- function(m){
        # m is a matrix with subjects as rows and conditions as columns
        # note that checking for worst case scenarios F correction first
 might # be a good idea using J/(J-1) as the df correction factor
        n<- length(m[,1])
        J<-length(m[1,])
        X<-cov(m)
        r<- length(X[,1])
        D<-0
        for (i in 1: r) D<- D+ X[i,i]
        D<-D/r
        SPm<- mean(X)
        SPm2<- sum(X^2)
        SSrm<-0
        for (i in 1: r) SSrm<- SSrm + mean(X[i,])^2
        epsilon <- (1/(J-1))*((J^2*(D-SPm)^2)/(SPm2 - 2*J*SSrm + J^2*SPm^2))
	epsilon
}


# ----------------------------------------------------------------------------

On Thursday December 18 2003 12:00, Wolfgang Pauli wrote:
> Dear R-helpers,
>
> Does anybody know, whether there is an option to tell aov/anova, or do
> something similar, to get a Huynh-Feldt correction of dfs in the aov/anova
> function?
>
> Thanks in advance!
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From rvaradha at jhsph.edu  Sun Dec 21 02:07:34 2003
From: rvaradha at jhsph.edu (Ravi Varadhan)
Date: Sat, 20 Dec 2003 20:07:34 -0500
Subject: [R] Software/algorithms for competing risks analysis
Message-ID: <3560fc357bf6.357bf63560fc@jhsph.edu>

Dear R group:

I am compiling a list of available algorithms/macros/software for 
performing the following types of competing risks analyses:
(1) cumulative (crude) incidence regression
(2) Gray's K-sample log-rank test for cmulative incidence
(3) multi-state models
(4) dependent competing risks modeling with (a) copulas, (b) Robin's 
Inverse probability of censoring weighted (IPCW) estimator, (c) frailty

I would really appreciate any pointers/references to 
algorithms/software in other environments such as SAS/STATA/SPSS, as 
well as in R.

Thanks very much,
Ravi.



From hdoran at nasdc.org  Sun Dec 21 03:35:56 2003
From: hdoran at nasdc.org (Harold Doran)
Date: Sat, 20 Dec 2003 21:35:56 -0500
Subject: [R] Sweave/LaTeX Problem with EPS PDF
Message-ID: <66578BFC0BA55348B5907A0F798EE9307A2C31@ernesto.NASDC.ORG>

Dear List:
 
I am unsure if my problem is with Sweave or LaTeX. Anyhow, I am using the MikTeX distribution and TexnicCenter. 
 
I can easily create Sweave files and all goes well until I try to incorporate graphics. I use the same code as found in the examples found in the users manual. 
 
In R, the graphics I want are created as Sweave is creating the .tex file. When I examine the .tex file created by Sweave, it includes the includegraphics{} statement needed. 
 
When I run LaTeX on the .tex file, everything works except that the graphics I want are not displayed. 
 
However, when I examine the pdf or eps files created, there is nothing there. When I view the EPS using Ghostview, the file is empty, but there appears to be a bounding box surrounding nothing. When I open the pdf graphic, there is nothing there either. I have tried creating both dvis and pdf files. Again, text works perfectly, but graphics do not work.
 
Does anyone have any suggestions?
 
Many thanks,
 
Harold



From hdoran at nasdc.org  Sun Dec 21 03:42:31 2003
From: hdoran at nasdc.org (Harold Doran)
Date: Sat, 20 Dec 2003 21:42:31 -0500
Subject: [R] varFixed
Message-ID: <66578BFC0BA55348B5907A0F798EE9307A2C32@ernesto.NASDC.ORG>

Dear List:
 
Earlier this week I posted a question and received no response, and I continue to struggle with my model. My original question is pasted below.
 
I am using lme and want to fix the variance of the within group residual at 1 (e~n(0,1). I think the varFixed function should be used to accomplish this, but I am struggling to figure out how to do this.
 
Can anyone offer suggestions on how this might be accomplished? 
 
Thanks, I would appreciate any suggestions.
 
Harold
 
 
Dear List:

I am trying to figure out how to incorporate measurement error in an longitudinal educational data set using lme to create a "true score" model. As a by-product of the procedures used to scale educational tests, one can obtain a person-specific measurement error associated with each score, or a conditional standard error. For example, a score of 200 would have measurement error specific to that score that would be different than, say, a score of 250.

I have been rather successful in figuring out how to rescale the necessary components to create this "true score" model. This simply requires that the response variable, the intercept, and any other variables in the design matrix be multiplied by the reciprocal of the standard error of measurement for the associated score. There may be a better way to do this, but I manually create a vector of 1s for all observations and multiply this vector by 1/sem. This is the new intercept. I also multiply any other predictors in the design matrix by the same value.

In the R code, I remove the intercept included by default (-1) and include the newly created intercept (which is no longer a constant) as well as the new response variable and rescaled predictors.

However, I am confused regarding the within-group error term. Fitting this model requires that the variance be fixed at 1:  e ~ n(0,1).

Is it possible to constrain the variance for this model as such?

I would appreciate any comments or suggestions regarding this model.



From jasont at indigoindustrial.co.nz  Sun Dec 21 06:30:23 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Sun, 21 Dec 2003 18:30:23 +1300
Subject: [R] Sweave/LaTeX Problem with EPS PDF
In-Reply-To: <66578BFC0BA55348B5907A0F798EE9307A2C31@ernesto.NASDC.ORG>
References: <66578BFC0BA55348B5907A0F798EE9307A2C31@ernesto.NASDC.ORG>
Message-ID: <3FE52FEF.6020809@indigoindustrial.co.nz>

Harold Doran wrote:
[... Sweave use...]
> However, when I examine the pdf or eps files created, there 
> is nothing there. When I view the EPS using Ghostview, the file
 > is empty, but there appears to be a bounding box surrounding
 > nothing. When I open the pdf graphic, there is nothing there
 > either. I have tried creating both dvis and pdf files. Again,
 > text works perfectly, but graphics do not work.

Are they lattice graphics?  They need to be wrapped in a print() 
statement, like

print(xyplot(...))

Without that, they don't produce output to eps or pdf.

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From thondeboer at yahoo.com  Sun Dec 21 08:45:07 2003
From: thondeboer at yahoo.com (Thon de Boer)
Date: Sat, 20 Dec 2003 23:45:07 -0800 (PST)
Subject: [R] Get message "cannot do complex assignments in base namespace"
	error
Message-ID: <20031221074507.45139.qmail@web41508.mail.yahoo.com>

I am constantly running into the problem where I get
error messages like "cannot do complex assignments in
base namespace".

It might have something to do with the fact that R has
not started up completely, since I am trying to run
some R script using Rterm < Rscript.R.

When I perform the same script in a Interactive R
session, the R script has no problem with the
assignments that it deems complex (like assgining a
new value to a slot in a class object).

Also, when I encapsulate the offending scripts inside
a local({ ... }) block, I also do not get the error,
so my theory is that my script is being run in the
wrong namespace, or R has not completely started up
yet or something.

Does anyone have an idea how I can fix this, without
having to run within the local block?

Thanks,

Thon de Boer



From ripley at stats.ox.ac.uk  Sun Dec 21 09:18:52 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 21 Dec 2003 08:18:52 +0000 (GMT)
Subject: [R] Get message "cannot do complex assignments in base namespace"
	error
In-Reply-To: <20031221074507.45139.qmail@web41508.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0312210811510.14410-100000@gannet.stats>

Could you 

1) Try with --vanilla.
2) Show us a small example.

In general user code should not be assigning in base (are you
using <<- by any chance?), but there is no difference when redirecting
input in Rterm as to where code is run.  (Your command-line flags may
well affect it, though.)

On Sat, 20 Dec 2003, Thon de Boer wrote:

> I am constantly running into the problem where I get
> error messages like "cannot do complex assignments in
> base namespace".

`Like' or precisely, and all the same or not?

> It might have something to do with the fact that R has
> not started up completely, since I am trying to run
> some R script using Rterm < Rscript.R.

That should not work: you should need to set a command-line flag
to determine if you want the session saved or not.

Given that a proper version of this is used for all the testing we do, 
we are pretty confident that it does work correctly in a factory-fresh 
environment.

> When I perform the same script in a Interactive R
> session, the R script has no problem with the
> assignments that it deems complex (like assgining a
> new value to a slot in a class object).
> 
> Also, when I encapsulate the offending scripts inside
> a local({ ... }) block, I also do not get the error,
> so my theory is that my script is being run in the
> wrong namespace, or R has not completely started up
> yet or something.
> 
> Does anyone have an idea how I can fix this, without
> having to run within the local block?

Sorry, we need to know what `this' is, first.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From spencer.graves at pdf.com  Sun Dec 21 10:07:27 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 21 Dec 2003 01:07:27 -0800
Subject: [R] varFixed
In-Reply-To: <66578BFC0BA55348B5907A0F798EE9307A2C32@ernesto.NASDC.ORG>
References: <66578BFC0BA55348B5907A0F798EE9307A2C32@ernesto.NASDC.ORG>
Message-ID: <3FE562CF.4030802@pdf.com>

      Can you provide a toy example including a data.frame statement 
with one or more of the primary variants you have tried with the error 
message(s) or other problems you encountered?  You are more likely to 
get a useful response if an interested reader can copy a few lines from 
your email into R and test alternatives.  You might answer your own 
question in the process of preparing such a simple example.  If not, the 
result will make it easier for someone else to understand your problem 
and respond.  Also, have you consulted Pinhiero and Bates (2000) Mixed 
Effects Models in S and S-Plus (Springer)?  I was able to use "lme" 
effectively only after reading the first portion of that book and 
working many of their examples. 

      hope this helps. 
      spencer graves

Harold Doran wrote:

>Dear List:
> 
>Earlier this week I posted a question and received no response, and I continue to struggle with my model. My original question is pasted below.
> 
>I am using lme and want to fix the variance of the within group residual at 1 (e~n(0,1). I think the varFixed function should be used to accomplish this, but I am struggling to figure out how to do this.
> 
>Can anyone offer suggestions on how this might be accomplished? 
> 
>Thanks, I would appreciate any suggestions.
> 
>Harold
> 
> 
>Dear List:
>
>I am trying to figure out how to incorporate measurement error in an longitudinal educational data set using lme to create a "true score" model. As a by-product of the procedures used to scale educational tests, one can obtain a person-specific measurement error associated with each score, or a conditional standard error. For example, a score of 200 would have measurement error specific to that score that would be different than, say, a score of 250.
>
>I have been rather successful in figuring out how to rescale the necessary components to create this "true score" model. This simply requires that the response variable, the intercept, and any other variables in the design matrix be multiplied by the reciprocal of the standard error of measurement for the associated score. There may be a better way to do this, but I manually create a vector of 1s for all observations and multiply this vector by 1/sem. This is the new intercept. I also multiply any other predictors in the design matrix by the same value.
>
>In the R code, I remove the intercept included by default (-1) and include the newly created intercept (which is no longer a constant) as well as the new response variable and rescaled predictors.
>
>However, I am confused regarding the within-group error term. Fitting this model requires that the variance be fixed at 1:  e ~ n(0,1).
>
>Is it possible to constrain the variance for this model as such?
>
>I would appreciate any comments or suggestions regarding this model.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From thondeboer at yahoo.com  Sun Dec 21 10:55:55 2003
From: thondeboer at yahoo.com (Thon de Boer)
Date: Sun, 21 Dec 2003 01:55:55 -0800 (PST)
Subject: [R] Get message "cannot do complex assignments in base namespace"
	error
In-Reply-To: <Pine.LNX.4.44.0312210811510.14410-100000@gannet.stats>
Message-ID: <20031221095555.63726.qmail@web41508.mail.yahoo.com>

Thanks for the fast reply.

When I use --vanilla for Rterm.exe (Using windows
here) I do not get the problem anymore, so that solved
half my problems. Thanks (I had used --slave --no-save
--no-restore)

But I seem to have the problem of getting the error
message "cannot do complex assignments in base
namespace" when I try to run Rgui.exe.

This is what I am trying to do. I would like to run
the Rgui program that would start up and do some
graphical plots or use the fileBrowser() widget to
load some files.

I have created the following BAT file for windows:



@echo off
set R_PROFILE=%1
set %2=%3
set %4=%5
set %6=%7
set %8=%9
cat.exe > GS_R_in.txt
rw1081\bin\Rgui.exe --no-save --no-restore
cat.exe < GS_R_out.txt
del GS_R_in.txt GS_R_out.txt

that is run by my program (GeneSpring).
The first command line argument is the R script that I
want to run and the four consecutive key-value pairs
are the system environment variable that I set that
will be picked up by the R script.
GeneSpring will also pump some data into the STDOUT
that is picked up by the cat.exe program we wrote and
dumps it in a file that is picked up in return by the
R script.

so the complete command would be something like

myR.bat script.R foo bar aap noot mies zus kees bal <
INPUT.txt > OUTPUT.txt

I had to resort to setting the R script as the
R_PROFILE environment variable to force Rgui to start
the R script I give it.

In the R script there seem to be the complex
assignments like

i at expName <- "Just a character string"

where i is a defined class. This is the assignment R
complains about, but only when I run the script as the
R_PROFILE. If I run the script as source() etc. it
runs fine.

Hope this sheds some light on the problem...

Thanks,

Thon




--- Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> Could you 
> 
> 1) Try with --vanilla.
> 2) Show us a small example.
> 
> In general user code should not be assigning in base
> (are you
> using <<- by any chance?), but there is no
> difference when redirecting
> input in Rterm as to where code is run.  (Your
> command-line flags may
> well affect it, though.)
> 
> On Sat, 20 Dec 2003, Thon de Boer wrote:
> 
> > I am constantly running into the problem where I
> get
> > error messages like "cannot do complex assignments
> in
> > base namespace".
> 
> `Like' or precisely, and all the same or not?
> 
> > It might have something to do with the fact that R
> has
> > not started up completely, since I am trying to
> run
> > some R script using Rterm < Rscript.R.
> 
> That should not work: you should need to set a
> command-line flag
> to determine if you want the session saved or not.
> 
> Given that a proper version of this is used for all
> the testing we do, 
> we are pretty confident that it does work correctly
> in a factory-fresh 
> environment.
> 
> > When I perform the same script in a Interactive R
> > session, the R script has no problem with the
> > assignments that it deems complex (like assgining
> a
> > new value to a slot in a class object).
> > 
> > Also, when I encapsulate the offending scripts
> inside
> > a local({ ... }) block, I also do not get the
> error,
> > so my theory is that my script is being run in the
> > wrong namespace, or R has not completely started
> up
> > yet or something.
> > 
> > Does anyone have an idea how I can fix this,
> without
> > having to run within the local block?
> 
> Sorry, we need to know what `this' is, first.
> 
> -- 
> Brian D. Ripley,                 
> ripley at stats.ox.ac.uk
> Professor of Applied Statistics, 
> http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865
> 272861 (self)
> 1 South Parks Road,                     +44 1865
> 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865
> 272595
>



From ripley at stats.ox.ac.uk  Sun Dec 21 11:15:14 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 21 Dec 2003 10:15:14 +0000 (GMT)
Subject: [R] Get message "cannot do complex assignments in base namespace"
	error
In-Reply-To: <20031221095555.63726.qmail@web41508.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0312211009040.1681-100000@gannet.stats>

On Sun, 21 Dec 2003, Thon de Boer wrote:

> Thanks for the fast reply.
> 
> When I use --vanilla for Rterm.exe (Using windows
> here) I do not get the problem anymore, so that solved
> half my problems. Thanks (I had used --slave --no-save
> --no-restore)
> 
> But I seem to have the problem of getting the error
> message "cannot do complex assignments in base
> namespace" when I try to run Rgui.exe.

The following is the critical piece of information you omitted
in your first post.

[...]

> I had to resort to setting the R script as the
> R_PROFILE environment variable to force Rgui to start
> the R script I give it.

That's your problem, and it is bad practice.  ?Startup does explain why 
you needed to use local().

You are misusing the site profile; you could equally have used a user 
profile or .First or .... 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Sun Dec 21 11:58:14 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 21 Dec 2003 11:58:14 +0100
Subject: [R] Get message "cannot do complex assignments in base 
	namespace"error
References: <20031221095555.63726.qmail@web41508.mail.yahoo.com>
Message-ID: <3FE57CC6.ACEDEEAA@statistik.uni-dortmund.de>



Thon de Boer wrote:
> 
> Thanks for the fast reply.
> 
> When I use --vanilla for Rterm.exe (Using windows
> here) I do not get the problem anymore, so that solved
> half my problems. Thanks (I had used --slave --no-save
> --no-restore)
> 
> But I seem to have the problem of getting the error
> message "cannot do complex assignments in base
> namespace" when I try to run Rgui.exe.
> 
> This is what I am trying to do. I would like to run
> the Rgui program that would start up and do some
> graphical plots or use the fileBrowser() widget to
> load some files.
> 
> I have created the following BAT file for windows:
> 
> @echo off
> set R_PROFILE=%1
> set %2=%3
> set %4=%5
> set %6=%7
> set %8=%9
> cat.exe > GS_R_in.txt
> rw1081\bin\Rgui.exe --no-save --no-restore
> cat.exe < GS_R_out.txt
> del GS_R_in.txt GS_R_out.txt
> 
> that is run by my program (GeneSpring).
> The first command line argument is the R script that I
> want to run and the four consecutive key-value pairs
> are the system environment variable that I set that
> will be picked up by the R script.
> GeneSpring will also pump some data into the STDOUT
> that is picked up by the cat.exe program we wrote and
> dumps it in a file that is picked up in return by the
> R script.
> 
> so the complete command would be something like
> 
> myR.bat script.R foo bar aap noot mies zus kees bal <
> INPUT.txt > OUTPUT.txt
> 
> I had to resort to setting the R script as the
> R_PROFILE environment variable to force Rgui to start
> the R script I give it.

And that's the point! Don't use Rprofile to execute scripts, because
everything assigned in there will be loaded into base on startup. (See
?Startup)
For running scripts I'd recommend to use Rcmd BATCH (you will need Perl,
though), but it's also possible with rterm.exe.

Uwe Ligges


> In the R script there seem to be the complex
> assignments like
> 
> i at expName <- "Just a character string"
> 
> where i is a defined class. This is the assignment R
> complains about, but only when I run the script as the
> R_PROFILE. If I run the script as source() etc. it
> runs fine.
> 
> Hope this sheds some light on the problem...
> 
> Thanks,
> 
> Thon
> 
> --- Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> > Could you
> >
> > 1) Try with --vanilla.
> > 2) Show us a small example.
> >
> > In general user code should not be assigning in base
> > (are you
> > using <<- by any chance?), but there is no
> > difference when redirecting
> > input in Rterm as to where code is run.  (Your
> > command-line flags may
> > well affect it, though.)
> >
> > On Sat, 20 Dec 2003, Thon de Boer wrote:
> >
> > > I am constantly running into the problem where I
> > get
> > > error messages like "cannot do complex assignments
> > in
> > > base namespace".
> >
> > `Like' or precisely, and all the same or not?
> >
> > > It might have something to do with the fact that R
> > has
> > > not started up completely, since I am trying to
> > run
> > > some R script using Rterm < Rscript.R.
> >
> > That should not work: you should need to set a
> > command-line flag
> > to determine if you want the session saved or not.
> >
> > Given that a proper version of this is used for all
> > the testing we do,
> > we are pretty confident that it does work correctly
> > in a factory-fresh
> > environment.
> >
> > > When I perform the same script in a Interactive R
> > > session, the R script has no problem with the
> > > assignments that it deems complex (like assgining
> > a
> > > new value to a slot in a class object).
> > >
> > > Also, when I encapsulate the offending scripts
> > inside
> > > a local({ ... }) block, I also do not get the
> > error,
> > > so my theory is that my script is being run in the
> > > wrong namespace, or R has not completely started
> > up
> > > yet or something.
> > >
> > > Does anyone have an idea how I can fix this,
> > without
> > > having to run within the local block?
> >
> > Sorry, we need to know what `this' is, first.
> >
> > --
> > Brian D. Ripley,
> > ripley at stats.ox.ac.uk
> > Professor of Applied Statistics,
> > http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865
> > 272861 (self)
> > 1 South Parks Road,                     +44 1865
> > 272866 (PA)
> > Oxford OX1 3TG, UK                Fax:  +44 1865
> > 272595
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Sun Dec 21 12:13:15 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 21 Dec 2003 11:13:15 +0000 (GMT)
Subject: [R] Get message "cannot do complex assignments in base 
	namespace"error
In-Reply-To: <3FE57CC6.ACEDEEAA@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.44.0312211112470.6055-100000@gannet.stats>

On Sun, 21 Dec 2003, Uwe Ligges wrote:

> And that's the point! Don't use Rprofile to execute scripts, because
> everything assigned in there will be loaded into base on startup. (See
> ?Startup)
> For running scripts I'd recommend to use Rcmd BATCH (you will need Perl,
> though), but it's also possible with rterm.exe.

Perl is no longer needed.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From djw1005 at cam.ac.uk  Sun Dec 21 18:54:25 2003
From: djw1005 at cam.ac.uk (Damon Wischik)
Date: Sun, 21 Dec 2003 17:54:25 +0000 (GMT)
Subject: [R] Factor names & levels
In-Reply-To: <20031219053124.2EA043960@mprdmxin.myway.com>
Message-ID: <Pine.SOL.3.96.1031221175136.9560A-100000@virgo.cus.cam.ac.uk>


> names() is only defined for vectors and lists and factors are
> neither.  See ?vector and ?names for more info.

?vector tells me that factors are not vectors, but ?names does not tell me
that names() is only defined for vectors and lists. If it were, how
should I understand the following?

> x <- factor(c("one","three"))
> names(x) <- c("fred","jim")
> names(x)
[1] "fred" "jim" 
> class(x)
[1] "factor"



From ggrothendieck at myway.com  Sun Dec 21 19:16:58 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 21 Dec 2003 13:16:58 -0500 (EST)
Subject: [R] Factor names & levels
Message-ID: <20031221181658.362443978@mprdmxin.myway.com>



I agree it may not be 100% clear but ?names does say
"The default methods get and set the '"names"' attribute 
of a vector or list." and if you issue the command:

methods("names")

you find that the only non-default method is names.dist.
 
Date: Sun, 21 Dec 2003 17:54:25 +0000 (GMT) 
From: Damon Wischik <djw1005 at cam.ac.uk>
To: Gabor Grothendieck <ggrothendieck at myway.com> 
Cc: <R-help at stat.math.ethz.ch> 
Subject: Re: [R] Factor names & levels 

 
 

> names() is only defined for vectors and lists and factors are
> neither. See ?vector and ?names for more info.

?vector tells me that factors are not vectors, but ?names does not tell me
that names() is only defined for vectors and lists. If it were, how
should I understand the following?

> x <- factor(c("one","three"))
> names(x) <- c("fred","jim")
> names(x)
[1] "fred" "jim" 
> class(x)
[1] "factor"



From tblackw at umich.edu  Sun Dec 21 20:15:53 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Sun, 21 Dec 2003 14:15:53 -0500 (EST)
Subject: [R] [Mailman] question: contour plot for discrete data
In-Reply-To: <20031219201450.68952.qmail@web14810.mail.yahoo.com>
References: <20031219201450.68952.qmail@web14810.mail.yahoo.com>
Message-ID: <Pine.SOL.4.58.0312211407500.13699@tetris.gpcc.itd.umich.edu>

Dear L Z  -

Before using  contour()  one needs to interpolate the z values
to all points in a rectangular grid.  2D interpolation is not
trivial.  The package KernSmooth (case-sensitive) will do this
for a density estimate but not, apparently, when z values are
given.  Perhaps packages splines or mclust will do this, although
I haven't checked.  More likely, the functions  surf.ls(), surf.gls()
from package spatial.

Other users who are familiar with maps and geographic information
systems (GIS) packages for R may have much better ideas for you
than I know about.  I've never had occasion to use any of this.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Fri, 19 Dec 2003, L Z wrote:

> Question:
>  I have matrix (n x3) that represents discrete data.
> Each row of matrix is 3-D point (x,y,z). I would like
> to get contour map (z value) at two dimension
> (x,y). How can I use related contour function to do
> this job?
> I am not sure if I clarify this question. For example,
> we can get point (x,y)
> at 2 dimension according to first two columns of
> matrix. Then I want to connect
> same value z=(x,y). Thanks!
>   zhang
>
> The data file looks like this:
> X          y               z
> 4           0.33          0.99
> 4           0.5            1.2
> 5           0.66          1.2
> 5           0.7            1.5
> 6           2               1.2
> ?
>



From spencer.graves at pdf.com  Sun Dec 21 20:31:47 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 21 Dec 2003 11:31:47 -0800
Subject: [R] [Mailman] question: contour plot for discrete data
In-Reply-To: <Pine.SOL.4.58.0312211407500.13699@tetris.gpcc.itd.umich.edu>
References: <20031219201450.68952.qmail@web14810.mail.yahoo.com>
	<Pine.SOL.4.58.0312211407500.13699@tetris.gpcc.itd.umich.edu>
Message-ID: <3FE5F523.1040606@pdf.com>

Have you considered "interp", as in the following example: 

XY <- expand.grid(x=1:11, y=1:11)
XY$z <- XY$x+XY$y
contour(interp(XY$x, XY$y, XY$z))

hope this helps.  spencer graves

Thomas W Blackwell wrote:

>Dear L Z  -
>
>Before using  contour()  one needs to interpolate the z values
>to all points in a rectangular grid.  2D interpolation is not
>trivial.  The package KernSmooth (case-sensitive) will do this
>for a density estimate but not, apparently, when z values are
>given.  Perhaps packages splines or mclust will do this, although
>I haven't checked.  More likely, the functions  surf.ls(), surf.gls()
>from package spatial.
>
>Other users who are familiar with maps and geographic information
>systems (GIS) packages for R may have much better ideas for you
>than I know about.  I've never had occasion to use any of this.
>
>-  tom blackwell  -  u michigan medical school  -  ann arbor  -
>
>On Fri, 19 Dec 2003, L Z wrote:
>
>  
>
>>Question:
>> I have matrix (n x3) that represents discrete data.
>>Each row of matrix is 3-D point (x,y,z). I would like
>>to get contour map (z value) at two dimension
>>(x,y). How can I use related contour function to do
>>this job?
>>I am not sure if I clarify this question. For example,
>>we can get point (x,y)
>>at 2 dimension according to first two columns of
>>matrix. Then I want to connect
>>same value z=(x,y). Thanks!
>>  zhang
>>
>>The data file looks like this:
>>X          y               z
>>4           0.33          0.99
>>4           0.5            1.2
>>5           0.66          1.2
>>5           0.7            1.5
>>6           2               1.2
>>?
>>
>>    
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From thondeboer at yahoo.com  Sun Dec 21 20:40:04 2003
From: thondeboer at yahoo.com (Thon de Boer)
Date: Sun, 21 Dec 2003 11:40:04 -0800 (PST)
Subject: [R] Get message "cannot do complex assignments in base namespace"
	error
In-Reply-To: <Pine.LNX.4.44.0312211009040.1681-100000@gannet.stats>
Message-ID: <20031221194004.45245.qmail@web41502.mail.yahoo.com>

Good...It is getting clearer now and I indeed read
?Startup so that is why I knew i had to use local()
blocks (or use the .First function, but that does not
seem to work in Rterm) in the first place. The problem
there, is that any of the objects that are created in
the local block are afcourse no longer visible when
the R is started. (I know....I want it all....;-))

The reason for using the R_PROFILE variable was
because there seem to be only one USER profile file
that I can use that has a fixed name '.Rprofile' and
from my application I would like to be able to use one
R startup script that simply got the R script name as
its argument. (I guess I could create this .Rprofile
on the fly each time I want to run my program)

Would there be a way to specify on the command line to
use a different profile file on the command line of
Rgui?

Although I have read that 'code gets installed in base
namespace' I still don't understand what it means. Why
was my assignment of new data to a slot in a class
object deemed to be impossible? Was I overwriting
something without knowing it? I had named the variable
i that contained the class object and was that
variable used in base namespace or something?

What I am trying to do is to have the user of my
system only write one script that he could execute
either with the Rgui or the Rterm program for GUI and
BATCH processing. For most of the things the R_PROFILE
hack seem to work, but indeed is not elegant or even
misusing the functionality) so any help in how to set
this up would be apreciated

Thanks again for the help

Thon

--- Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> On Sun, 21 Dec 2003, Thon de Boer wrote:
> 
> > Thanks for the fast reply.
> > 
> > When I use --vanilla for Rterm.exe (Using windows
> > here) I do not get the problem anymore, so that
> solved
> > half my problems. Thanks (I had used --slave
> --no-save
> > --no-restore)
> > 
> > But I seem to have the problem of getting the
> error
> > message "cannot do complex assignments in base
> > namespace" when I try to run Rgui.exe.
> 
> The following is the critical piece of information
> you omitted
> in your first post.
> 
> [...]
> 
> > I had to resort to setting the R script as the
> > R_PROFILE environment variable to force Rgui to
> start
> > the R script I give it.
> 
> That's your problem, and it is bad practice. 
> ?Startup does explain why 
> you needed to use local().
> 
> You are misusing the site profile; you could equally
> have used a user 
> profile or .First or .... 
> 
> -- 
> Brian D. Ripley,                 
> ripley at stats.ox.ac.uk
> Professor of Applied Statistics, 
> http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865
> 272861 (self)
> 1 South Parks Road,                     +44 1865
> 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865
> 272595
>



From Roger.Bivand at nhh.no  Sun Dec 21 20:48:10 2003
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Sun, 21 Dec 2003 20:48:10 +0100 (CET)
Subject: [R] [Mailman] question: contour plot for discrete data
In-Reply-To: <3FE5F523.1040606@pdf.com>
Message-ID: <Pine.LNX.4.44.0312212044320.17840-100000@reclus.nhh.no>

On Sun, 21 Dec 2003, Spencer Graves wrote:

> Have you considered "interp", as in the following example: 

in the akima package, just to make this plain.

> 
> XY <- expand.grid(x=1:11, y=1:11)
> XY$z <- XY$x+XY$y
> contour(interp(XY$x, XY$y, XY$z))
> 
> hope this helps.  spencer graves
> 
> Thomas W Blackwell wrote:
> 
> >Dear L Z  -
> >
> >Before using  contour()  one needs to interpolate the z values
> >to all points in a rectangular grid.  2D interpolation is not
> >trivial.  The package KernSmooth (case-sensitive) will do this
> >for a density estimate but not, apparently, when z values are
> >given.  Perhaps packages splines or mclust will do this, although
> >I haven't checked.  More likely, the functions  surf.ls(), surf.gls()
> >from package spatial.
> >
> >Other users who are familiar with maps and geographic information
> >systems (GIS) packages for R may have much better ideas for you
> >than I know about.  I've never had occasion to use any of this.
> >
> >-  tom blackwell  -  u michigan medical school  -  ann arbor  -
> >
> >On Fri, 19 Dec 2003, L Z wrote:
> >
> >  
> >
> >>Question:
> >> I have matrix (n x3) that represents discrete data.
> >>Each row of matrix is 3-D point (x,y,z). I would like
> >>to get contour map (z value) at two dimension
> >>(x,y). How can I use related contour function to do
> >>this job?
> >>I am not sure if I clarify this question. For example,
> >>we can get point (x,y)
> >>at 2 dimension according to first two columns of
> >>matrix. Then I want to connect
> >>same value z=(x,y). Thanks!
> >>  zhang
> >>
> >>The data file looks like this:
> >>X          y               z
> >>4           0.33          0.99
> >>4           0.5            1.2
> >>5           0.66          1.2
> >>5           0.7            1.5
> >>6           2               1.2
> >>?
> >>
> >>    
> >>
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >  
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From umalvarez at fata.unam.mx  Sun Dec 21 21:17:58 2003
From: umalvarez at fata.unam.mx (Ulises Mora Alvarez)
Date: Sun, 21 Dec 2003 14:17:58 -0600 (CST)
Subject: [R] Sweave/LaTeX Problem with EPS PDF
In-Reply-To: <66578BFC0BA55348B5907A0F798EE9307A2C31@ernesto.NASDC.ORG>
Message-ID: <Pine.LNX.4.44.0312211416200.13997-100000@athena.fata.unam.mx>

Hi!

Would you please include more info so one can try to help you?

It would be useful if you include your R-version, your OS and sample code.



On Sat, 20 Dec 2003, Harold Doran wrote:

> Dear List:
>  
> I am unsure if my problem is with Sweave or LaTeX. Anyhow, I am using the MikTeX distribution and TexnicCenter. 
>  
> I can easily create Sweave files and all goes well until I try to incorporate graphics. I use the same code as found in the examples found in the users manual. 
>  
> In R, the graphics I want are created as Sweave is creating the .tex file. When I examine the .tex file created by Sweave, it includes the includegraphics{} statement needed. 
>  
> When I run LaTeX on the .tex file, everything works except that the graphics I want are not displayed. 
>  
> However, when I examine the pdf or eps files created, there is nothing there. When I view the EPS using Ghostview, the file is empty, but there appears to be a bounding box surrounding nothing. When I open the pdf graphic, there is nothing there either. I have tried creating both dvis and pdf files. Again, text works perfectly, but graphics do not work.
>  
> Does anyone have any suggestions?
>  
> Many thanks,
>  
> Harold
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Ulises M. Alvarez
LAB. DE ONDAS DE CHOQUE
FISICA APLICADA Y TECNOLOGIA AVANZADA
UNAM
umalvarez at fata.unam.mx



From abunn at montana.edu  Sun Dec 21 21:58:33 2003
From: abunn at montana.edu (Andy Bunn)
Date: Sun, 21 Dec 2003 13:58:33 -0700
Subject: [R] Contrasts for MANOVA
In-Reply-To: <Pine.LNX.4.44.0312200821360.6387-100000@gannet.stats>
Message-ID: <000001c3c805$44fabaa0$a0a00ecf@simATE>

Prof. Ripley: Thanks for the response. 

My imperfect understanding is that contrasts are a special kind of linear
combination that can be done a priori.

Here's an example that is similar to what she wants to do:

## START EXAMPLE

# Toy data:

y1 <- rnorm(15,0,2)
y2 <- runif(15)
y3 <- rnorm(15,2,1)

x1 <- c(rep("long", 5), rep("medium", 5), rep("short", 5))

dat <- data.frame(y1, y2, y3, x1)

# The question is:
# Are the population mean vectors for the multivariate responses of
# y1, y2, & y3 of the "long" x1 samples significantly different 
# from those of the combined "medium" and "short" x1 samples?

#Like so:

attach(dat)

# Get the mean for each predictor level for each response
mean.vec <- cbind(
c(mean(y1[x1 == "long"]), mean(y2[x1 == "long"]), mean(y3[x1 == "long"])),
c(mean(y1[x1 == "medium"]), mean(y2[x1 == "medium"]), mean(y3[x1 ==
"medium"])),
c(mean(y1[x1 == "short"]), mean(y2[x1 == "short"]), mean(y3[x1 == "short"]))
)

detach(dat)

mean.vec

#Ho: mean.vec[,1] == 1/2 * (mean.vec[,2] + mean.vec[,3])
## END EXAMPLE 

The actual data she is using is 10 responses and three factor levels, e.g.,
nlevels = 3, 3, 4
I hope this is clear. Thanks for any guidance.

-Andy



From hdoran at nasdc.org  Sun Dec 21 23:16:53 2003
From: hdoran at nasdc.org (Harold Doran)
Date: Sun, 21 Dec 2003 17:16:53 -0500
Subject: [R] Sweave/LaTeX Problem with EPS PDF
Message-ID: <66578BFC0BA55348B5907A0F798EE9307A2C35@ernesto.NASDC.ORG>

Yes, they were lattice and your suggestion did the trick. Many thanks!

	-----Original Message----- 
	From: Jason Turner [mailto:jasont at indigoindustrial.co.nz] 
	Sent: Sun 12/21/2003 12:30 AM 
	To: Harold Doran 
	Cc: r-help at r-project.org 
	Subject: Re: [R] Sweave/LaTeX Problem with EPS PDF
	
	

	Harold Doran wrote:
	[... Sweave use...]
	> However, when I examine the pdf or eps files created, there
	> is nothing there. When I view the EPS using Ghostview, the file
	 > is empty, but there appears to be a bounding box surrounding
	 > nothing. When I open the pdf graphic, there is nothing there
	 > either. I have tried creating both dvis and pdf files. Again,
	 > text works perfectly, but graphics do not work.
	
	Are they lattice graphics?  They need to be wrapped in a print()
	statement, like
	
	print(xyplot(...))
	
	Without that, they don't produce output to eps or pdf.
	
	Cheers
	
	Jason
	--
	Indigo Industrial Controls Ltd.
	http://www.indigoindustrial.co.nz
	64-21-343-545
	jasont at indigoindustrial.co.nz



From djw1005 at cam.ac.uk  Mon Dec 22 01:38:14 2003
From: djw1005 at cam.ac.uk (Damon Wischik)
Date: Mon, 22 Dec 2003 00:38:14 +0000 (GMT)
Subject: [R] Factor names & levels
In-Reply-To: <20031221181658.362443978@mprdmxin.myway.com>
Message-ID: <Pine.SOL.3.96.1031222003456.6155A-100000@virgo.cus.cam.ac.uk>


> I agree it may not be 100% clear but ?names does say
> "The default methods get and set the '"names"' attribute 
> of a vector or list." and if you issue the command:
>    methods("names")
> you find that the only non-default method is names.dist.

I still want to know how I should understand the following:

> x <- factor(c("one","three"))
> names(x) <- c("fred","jim")
> names(x)
[1] "fred" "jim" 
class(x)
[1] "factor"

Given that names seems to work on factors, I can see two possibilities:
1. It is a bug that it acts as it does;
2. the default method does what it says in the help page, but also does
more than just this.

I don't know enough to look at the source code to find out what is going
on.

Damon.



From ggrothendieck at myway.com  Mon Dec 22 02:02:20 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 21 Dec 2003 20:02:20 -0500 (EST)
Subject: [R] Factor names & levels
Message-ID: <20031222010220.7FF64397F@mprdmxin.myway.com>



The effect of names() on factors is undefined.    The fact
that it coincidentally partially works on factors is just
chance.

For it to be well defined, there would need to be a names
method and a names<- method for the factor class or else 
the default methods would have to be able to handle factors.

Its a bit dangerous to rely on the coincidental behavior of
functionality, but in the absence of explicit R support, if 
using names with factors were important to you then you could 
define your own methods like this:

"names<-.factor" <- function( x, value ) {
	attr(x, "levels") <- value
	x
}

names.factor <- function(x) attr( x, "levels" )

# with the above, this works:

x <- factor(c("one","three"))
names(x) <- c("fred","jim")   # implicitly invokes "names<-.factor"
names(x) # implicitly invokes names.factor

Hope this clears it up for you.

--- 
Date: Mon, 22 Dec 2003 00:38:14 +0000 (GMT) 
From: Damon Wischik <djw1005 at cam.ac.uk>
To: Gabor Grothendieck <ggrothendieck at myway.com> 
Cc: <R-help at stat.math.ethz.ch> 
Subject: Re: [R] Factor names & levels 

 
 

> I agree it may not be 100% clear but ?names does say
> "The default methods get and set the '"names"' attribute 
> of a vector or list." and if you issue the command:
> methods("names")
> you find that the only non-default method is names.dist.

I still want to know how I should understand the following:

> x <- factor(c("one","three"))
> names(x) <- c("fred","jim")
> names(x)
[1] "fred" "jim" 
class(x)
[1] "factor"

Given that names seems to work on factors, I can see two possibilities:
1. It is a bug that it acts as it does;
2. the default method does what it says in the help page, but also does
more than just this.

I don't know enough to look at the source code to find out what is going
on.

Damon.



From p.dalgaard at biostat.ku.dk  Mon Dec 22 02:11:25 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 22 Dec 2003 02:11:25 +0100
Subject: [R] Factor names & levels
In-Reply-To: <Pine.SOL.3.96.1031222003456.6155A-100000@virgo.cus.cam.ac.uk>
References: <Pine.SOL.3.96.1031222003456.6155A-100000@virgo.cus.cam.ac.uk>
Message-ID: <x28yl51wsi.fsf@biostat.ku.dk>

Damon Wischik <djw1005 at cam.ac.uk> writes:

> I still want to know how I should understand the following:
> 
> > x <- factor(c("one","three"))
> > names(x) <- c("fred","jim")
> > names(x)
> [1] "fred" "jim" 
> class(x)
> [1] "factor"
> 
> Given that names seems to work on factors, I can see two possibilities:
> 1. It is a bug that it acts as it does;
> 2. the default method does what it says in the help page, but also does
> more than just this.

The behaviour is *undefined*, for crying out loud! That means that if
you try to do it anyway, you may get an error, something
semi-sensible, or find that the code transfers a large sum of money to
a Danish bank account...

In the present implementation, the names attribute gets attached to
the factor just as to an integer vector, but it is not shown when
printing and there are several parts of the code where no attempt is
made to preserve names since there are not expected to be any. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Mon Dec 22 02:30:52 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 22 Dec 2003 02:30:52 +0100
Subject: [R] Factor names & levels
In-Reply-To: <20031222010220.7FF64397F@mprdmxin.myway.com>
References: <20031222010220.7FF64397F@mprdmxin.myway.com>
Message-ID: <x24qvt1vw3.fsf@biostat.ku.dk>

"Gabor Grothendieck" <ggrothendieck at myway.com> writes:

> For it to be well defined, there would need to be a names
> method and a names<- method for the factor class or else 
> the default methods would have to be able to handle factors.

Not only that but other methods for factors need to know about the
names and be able to modify them accordingly, e.g.

> getS3method("levels<-","factor")
function (x, value)
{
    xlevs <- levels(x)
    if (is.list(value))  #something
...
    else {
...
        nlevs <- xlevs <- as.character(value)
    }
    factor(xlevs[x], levels = unique(nlevs))
}

Here, xlevs[x] will not have the same names as x (it gets names from
xlevs if anything) so you'd have to have extra code for setting the
names on the result. 

(Rather interestingly, the factor() function does explicitly retain
names, so there are not quite as many places where they will be lost
as I would have expected.)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ggrothendieck at myway.com  Mon Dec 22 03:27:48 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 21 Dec 2003 21:27:48 -0500 (EST)
Subject: [R] Factor names & levels
Message-ID: <20031222022748.D18BD3975@mprdmxin.myway.com>



Based on Peter's response, I think I may have misinterpreted 
Damon's query.   The methods I displayed in my last post in 
this thread were intended to make name a synonym for level. If
its desired that name act on factors in the same way that names 
act on vectors and lists then the methods I provided would not 
be correct and, as Peter points out, the other factor methods 
would have to be examined, as well, to ensure that they all 
work properly with names. 

I do have one other idea in terms of a workaround.  You could
represent your factor as a one column data frame.  The data
frame could then have row names which could be interpreted as
names of the factor.

For example,

f <- data.frame(f = c("A","B","A","C"))
row.names(f) <- letters[1:4]

You can now refer to the factor as f$f and the names as row.names(f).
For example,

> f <- data.frame(f = factor(c("A","B","A","C")))
> row.names(f) <- letters[1:4]
> f
  f
a A
b B
c A
d C
> row.names(f)
[1] "a" "b" "c" "d"
> f$f
[1] A B A C
Levels: A B C

This is all officially supported by R so it should not get you
into trouble although it does require that your program 
interpret it accordingly.

--- 
Date: 22 Dec 2003 02:30:52 +0100 
From: Peter Dalgaard <p.dalgaard at biostat.ku.dk>
To: <ggrothendieck at myway.com> 
Cc: <djw1005 at cam.ac.uk>, <R-help at stat.math.ethz.ch> 
Subject: Re: [R] Factor names & levels 

 
 
"Gabor Grothendieck" <ggrothendieck at myway.com> writes:

> For it to be well defined, there would need to be a names
> method and a names<- method for the factor class or else 
> the default methods would have to be able to handle factors.

Not only that but other methods for factors need to know about the
names and be able to modify them accordingly, e.g.

> getS3method("levels<-","factor")
function (x, value)
{
xlevs <- levels(x)
if (is.list(value)) #something
...
else {
...
nlevs <- xlevs <- as.character(value)
}
factor(xlevs[x], levels = unique(nlevs))
}

Here, xlevs[x] will not have the same names as x (it gets names from
xlevs if anything) so you'd have to have extra code for setting the
names on the result. 

(Rather interestingly, the factor() function does explicitly retain
names, so there are not quite as many places where they will be lost
as I would have expected.)

-- 
O__ ---- Peter Dalgaard Blegdamsvej 3 
c/ /'_ --- Dept. of Biostatistics 2200 Cph. N 
(*) \(*) -- University of Copenhagen Denmark Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk) FAX: (+45) 35327907



From ros at jouy.inra.fr  Mon Dec 22 08:46:09 2003
From: ros at jouy.inra.fr (Mathieu)
Date: Mon, 22 Dec 2003 08:46:09 +0100
Subject: [R] won't install properly on Mac 10.3.2
In-Reply-To: <Pine.A41.4.58.0312191441400.131236@homer06.u.washington.edu>
References: <5.1.0.14.2.20031219162808.01707498@po.muohio.edu>
	<Pine.A41.4.58.0312191441400.131236@homer06.u.washington.edu>
Message-ID: <E803666C-3452-11D8-9DBB-000A95C5B248@diamant.jouy.inra.fr>


Le 19 d?c. 03, ? 23:44, Thomas Lumley a ?crit :

> On Fri, 19 Dec 2003, Hank Stevens wrote:
>
>> R version 1.8.1, Mac OS X 10.3.2
>>
>> I have tried searching for this  problem and its fix, but to no avail.
>> -Everything seems to download and unpack fine. I double click on 
>> StartR,
>> however, and it just winks and fails.
>> Any thoughts?
>
> If you open up a Console window  (go to Applications, then Utilities, 
> then
> select Console) you may get some more helpful error messages.

The problem may appear if you have installed libiconv (a tex associated 
package), see the Raqua-FAQ : 
http://cran.at.r-project.org/bin/macosx/RAqua-FAQ.html

HTH,

--Mathieu



From petr.pikal at precheza.cz  Mon Dec 22 09:54:46 2003
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 22 Dec 2003 09:54:46 +0100
Subject: [R] [Mailman] question: contour plot for discrete data
In-Reply-To: <20031219201450.68952.qmail@web14810.mail.yahoo.com>
Message-ID: <3FE6BF66.2102.76F7A0@localhost>

Hi

library(akima)
function
contour(interp(x,y,z))

is one possibility

Cheers

Petr

On 19 Jan 2058 at 48:211, L Z wrote:

> Question:
>  I have matrix (n x3) that represents discrete data.
> Each row of matrix is 3-D point (x,y,z). I would like
> to get contour map (z value) at two dimension
> (x,y). How can I use related contour function to do
> this job?
> I am not sure if I clarify this question. For example,
> we can get point (x,y)
> at 2 dimension according to first two columns of
> matrix. Then I want to connect
> same value z=(x,y). Thanks!
>   zhang
> 
> The data file looks like this:
> X          y               z
> 4           0.33          0.99
> 4           0.5            1.2
> 5           0.66          1.2
> 5           0.7            1.5
> 6           2               1.2
> ?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

Petr Pikal
petr.pikal at precheza.cz



From hellik at web.de  Mon Dec 22 10:29:00 2003
From: hellik at web.de (Helmut Kudrnovsky)
Date: Mon, 22 Dec 2003 10:29:00 +0100
Subject: [R] Problems with read.table()
Message-ID: <5.1.0.14.0.20031222100904.009e8220@pop3.web.de>

R version 1.8.1, OS Windows 98

Dear colleagues,

if I import vegetation data (first row with column labels and first column 
with row labels) like

7MYRGERM;7AGRGIGA;7DRYOCTO;5MYRGERM;7SALELEA;7CHOCHON;7SALNIG?;.......
t401;5;2;2;3;4;2;2;2;1;2;1;2;2;1;2;2;2;1;2;1;0;0;......
t403;3;0;0;6;4;0;3;0;0;3;0;0;0;0;3;0;0;0;2;0;2;0;.....

with read.table("data.file", header=TRUE, separator=";"), the R program 
crashes with following error message:

RGUI verursachte einen Fehler durch eine ung?ltige Seite
in Modul R.DLL bei 015f:6b515b87.
Register:
EAX=0075007a CS=015f EIP=6b515b87 EFLGS=00010246
EBX=01bd3f57 SS=0167 ESP=0073ed20 EBP=0073ed48
ECX=01bd3ad8 DS=0167 ESI=00000001 FS=2237
EDX=ffffffb0 ES=0167 EDI=00000008 GS=0000
Bytes bei CS:EIP:
0f b7 04 50 83 e0 08 eb ce 89 5d ec eb 8b 89 74
Stapelwerte:
00000000 0073ee20 019ea008 00000007 00002000 0000003b 000000b0 0073ee20 
00000006 01bd5f58 0073edb8 6b5162ef 00000010 00000001 0073ed8c 0073ee20

with greetings from Tyrol
Helmut Kudrnovsky



From Simon.Fear at synequanon.com  Mon Dec 22 10:52:30 2003
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Mon, 22 Dec 2003 09:52:30 -0000
Subject: [R] Problems with read.table()
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572F02159@synequanon01>

This is a known bug (PR #3234), only affects Win98.

The problem is the degree sign (or any nonstandard ASCII)
in your first line (7SALNIG?)

Mostly you can actually read these things in OK, but R crashes 
on printing. I get round it sometimes using something like

dedegree <- function(x) gsub("?", "deg", x)

applied to the "offending" variables after reading in.

Having said that,  I, like you, have sometimes had the crash 
on read (but not reproducibly as there are strict confidentiality
issues on my datasets). In that case you will have to edit the
data source file before reading it into R.

> -----Original Message-----
> From: Helmut Kudrnovsky [mailto:hellik at web.de]
> Sent: 22 December 2003 09:29
> To: r-help at stat.math.ethz.ch
> Subject: [R] Problems with read.table()
> 
> 
> Security Warning: 
> If you are not sure an attachment is safe to open please contact  
> Andy on x234. There are 0 attachments with this message. 
> ________________________________________________________________ 
>  
> R version 1.8.1, OS Windows 98
> 
> Dear colleagues,
> 
> if I import vegetation data (first row with column labels and 
> first column 
> with row labels) like
> 
> 7MYRGERM;7AGRGIGA;7DRYOCTO;5MYRGERM;7SALELEA;7CHOCHON;7SALNIG?;.......
> t401;5;2;2;3;4;2;2;2;1;2;1;2;2;1;2;2;2;1;2;1;0;0;......
> t403;3;0;0;6;4;0;3;0;0;3;0;0;0;0;3;0;0;0;2;0;2;0;.....
> 
> with read.table("data.file", header=TRUE, separator=";"), the 
> R program 
> crashes with following error message:
> 
> RGUI verursachte einen Fehler durch eine ung?ltige Seite
> in Modul R.DLL bei 015f:6b515b87.
> Register:
> EAX=0075007a CS=015f EIP=6b515b87 EFLGS=00010246
> EBX=01bd3f57 SS=0167 ESP=0073ed20 EBP=0073ed48
> ECX=01bd3ad8 DS=0167 ESI=00000001 FS=2237
> EDX=ffffffb0 ES=0167 EDI=00000008 GS=0000
> Bytes bei CS:EIP:
> 0f b7 04 50 83 e0 08 eb ce 89 5d ec eb 8b 89 74
> Stapelwerte:
> 00000000 0073ee20 019ea008 00000007 00002000 0000003b 
> 000000b0 0073ee20 
> 00000006 01bd5f58 0073edb8 6b5162ef 00000010 00000001 
> 0073ed8c 0073ee20
> 
> with greetings from Tyrol
> Helmut Kudrnovsky
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
 
Simon Fear 
Senior Statistician 
Syne qua non Ltd 
Tel: +44 (0) 1379 644449 
Fax: +44 (0) 1379 644445 
email: Simon.Fear at synequanon.com 
web: http://www.synequanon.com 
  
Number of attachments included with this message: 0 
  
This message (and any associated files) is confidential and\...{{dropped}}



From eric.esposito at gazdefrance.com  Mon Dec 22 10:51:44 2003
From: eric.esposito at gazdefrance.com (Eric ESPOSITO)
Date: Mon, 22 Dec 2003 10:51:44 +0100
Subject: [R] runif and sample with reproducibility
Message-ID: <OF3FB849F4.BBCBA801-ON41256E04.0034CBF7@notes.edfgdf.fr>

Hello,
I would like to sample a population but the result needs to be
reproducible, using 'runif' or 'sample' is the good way to do it but I
can't manage to make the results reproducible even with the 'set.seed'
function.
My aim is that th call to 'sample(1:100,10)' gives always the same result,
how can I do that?
Thanks!

Eric Esposito



From djw1005 at cam.ac.uk  Mon Dec 22 10:56:40 2003
From: djw1005 at cam.ac.uk (Damon Wischik)
Date: Mon, 22 Dec 2003 09:56:40 +0000 (GMT)
Subject: [R] Factor names & levels
In-Reply-To: <20031222022748.D18BD3975@mprdmxin.myway.com>
Message-ID: <Pine.SOL.3.96.1031222095500.11563A-100000@virgo.cus.cam.ac.uk>


> If
> its desired that name act on factors in the same way that names 
> act on vectors and lists then the methods I provided would not 
> be correct and, as Peter points out, the other factor methods 
> would have to be examined, as well, to ensure that they all 
> work properly with names. 

Thank you for all your answers. 

Damon.



From glaziou at pasteur-kh.org  Mon Dec 22 11:23:58 2003
From: glaziou at pasteur-kh.org (Philippe Glaziou)
Date: Mon, 22 Dec 2003 17:23:58 +0700
Subject: [R] runif and sample with reproducibility
In-Reply-To: <OF3FB849F4.BBCBA801-ON41256E04.0034CBF7@notes.edfgdf.fr>
References: <OF3FB849F4.BBCBA801-ON41256E04.0034CBF7@notes.edfgdf.fr>
Message-ID: <20031222102358.GA6333@pasteur-kh.org>

Eric ESPOSITO <eric.esposito at gazdefrance.com> wrote:
> I would like to sample a population but the result needs to be
> reproducible, using 'runif' or 'sample' is the good way to do it but I
> can't manage to make the results reproducible even with the 'set.seed'
> function.
> My aim is that th call to 'sample(1:100,10)' gives always the same result,
> how can I do that?



Can you give an example of set.seed not giving reproducible samples?

set.seed(101)
sample(1:100,10)
 [1] 38  5 70 64 24 29 55 32 58 50
set.seed(101)
sample(1:100,10)
 [1] 38  5 70 64 24 29 55 32 58 50

-- 
Philippe Glaziou
Epidemiologist



From petr.pikal at precheza.cz  Mon Dec 22 12:39:59 2003
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 22 Dec 2003 12:39:59 +0100
Subject: [R] error propagation - hope it is correct subject
Message-ID: <3FE6E61F.5508.10E8319@localhost>

Dear all

Please, can you advice me how to compute an error, standard deviation or 
another measure of variability of computed value.

I would like to do something like:

var(y) = some.function(var(x1),var(x2),var(x3))

for level F1 (2,3,...)

Let say I have some variables - x1, x2, x3 (two computed for levels of factor F 
and one which is same for all levels) and I want to compute

y = f(x1,x2,x3)

for some levels of factor F

I can compute variation of variables for levels of F, I know a variation of one 
variable but I am not sure how to transfer it to variation of y within respective 
levels.

I found some methods which I can use but I wonder if  there is some method 
implemented in R (Manly B.F. Biom.J.28,949,(1986), some local statistical books 
available to me).

I have a feeling I could use bootstrap method for this but I am not sure how.

Thank you and merry Christmas to all

Petr Pikal
petr.pikal at precheza.cz



From wolski at molgen.mpg.de  Mon Dec 22 14:30:37 2003
From: wolski at molgen.mpg.de (Wolski)
Date: Mon, 22 Dec 2003 14:30:37 +0100
Subject: [R] pkg-manual.dvi. How to generate the start page?
Message-ID: <200312221430370105.00E7D520@harry.molgen.mpg.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031222/4a3d44d1/attachment.pl

From maechler at stat.math.ethz.ch  Mon Dec 22 14:48:19 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 22 Dec 2003 14:48:19 +0100
Subject: [R] pkg-manual.dvi. How to generate the start page?
In-Reply-To: <200312221430370105.00E7D520@harry.molgen.mpg.de>
References: <200312221430370105.00E7D520@harry.molgen.mpg.de>
Message-ID: <16358.63011.321984.82328@gargle.gargle.HOWL>

>>>>> "Eryk" == Eryk Wolski <wolski at molgen.mpg.de>
>>>>>     on Mon, 22 Dec 2003 14:30:37 +0100 writes:

    Eryk> Hi!  On the cran "package source" page, to each
    Eryk> package there are the "reference manual" in pdf format
    Eryk> with a nice title page.

    Eryk> I observed that during the R CMD check pkg a
    Eryk> pkg-manual.div are generated in the pgk.Rcheck
    Eryk> directory.  But the nice title page are
    Eryk> missing. During the package build and install no div
    Eryk> files are produced.

    Eryk> Which opitions I have to pass during the build or
    Eryk> installation process to get such a nice manual dvi
    Eryk> file with title page and "R topics documented" (table
    Eryk> of contents) section?

R CMD Rd2dvi --pdf  <pkgdir>


    Eryk> 	[[alternative HTML version deleted]]
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[ please learn to get rid of the above, e.g., by
   searching for "html" in http://www.R-project.org/mail.html
   and reading the corresponding paragraph (and web link)
]

Regards,
Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From tblackw at umich.edu  Mon Dec 22 14:56:05 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Mon, 22 Dec 2003 08:56:05 -0500 (EST)
Subject: [R] error propagation - hope it is correct subject
In-Reply-To: <3FE6E61F.5508.10E8319@localhost>
References: <3FE6E61F.5508.10E8319@localhost>
Message-ID: <Pine.SOL.4.58.0312220831100.821@mspacman.gpcc.itd.umich.edu>

Petr  -

Very briefly, I think of three ways to approximate the standard
deviation of  y = f(x1,x2,x3).

  (1) linearise f() and use the covariance matrix of [x1,x2,x3].
  (2) simulate draws from the joint distribution of [x1,x2,x3],
	then compute the sample std dev of resulting f()s.
  (3) go back to the original data set from which [x1,x2,x3] were
	estimated as parameters, re-sample rows with replacement,
	estimate [x1,x2,x3] and compute f, then take sample std dev.

Other names for these three would be (1) the "delta method" or
Taylor series expansion, (2) parametric bootstrap, (3) bootstrap.

Different choices are appropriate in different situations.

If the std devs of x1,x2,x3 are small relative to the curvature
(2nd derivative) in f(), then use (1) and compute by matrix algebra

Var(f(x1,x2,x3))  approx  t(grad f) %*% Cov(x1,x2,x3) %*% grad f.

If the curvature in f() is an issue, use (2) with draws of x1,x2,x3
from some parametric distribution (eg, rnorm()) with each component
properly conditioned on the ones already drawn.

Only if there were no set of intermediate parameters [x1,x2,x3]
would I use (3) to get the precision of f directly.  I'm sure
Brad Efron would say something different.  (3) is the only one
that is canned in R, simply because the other two are practically
one-liners.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Mon, 22 Dec 2003, Petr Pikal wrote:

> Dear all
>
> Please, can you advice me how to compute an error, standard deviation or
> another measure of variability of computed value.
>
> I would like to do something like:
>
> var(y) = some.function(var(x1),var(x2),var(x3))
>
> for level F1 (2,3,...)
>
> Let say I have some variables - x1, x2, x3 (two computed for levels of factor F
> and one which is same for all levels) and I want to compute
>
> y = f(x1,x2,x3)
>
> for some levels of factor F
>
> I can compute variation of variables for levels of F, I know a variation of one
> variable but I am not sure how to transfer it to variation of y within respective
> levels.
>
> I found some methods which I can use but I wonder if  there is some method
> implemented in R (Manly B.F. Biom.J.28,949,(1986), some local statistical books
> available to me).
>
> I have a feeling I could use bootstrap method for this but I am not sure how.
>
> Thank you and merry Christmas to all
>
> Petr Pikal
> petr.pikal at precheza.cz
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From ggrothendieck at myway.com  Mon Dec 22 15:11:38 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 22 Dec 2003 09:11:38 -0500 (EST)
Subject: [R] Problems with read.table()
Message-ID: <20031222141138.C7BA4394B@mprdmxin.myway.com>



I don't have Windows 98 to test it out but assuming
that it causes read.table to crash but not readLines,
you could try doing the preprocessing in R itself using 
Simon's dedegree like follows to see if it works:

# dedegree is applied to text before it ever reaches read.table
con <- textConnection( dedegree( readLines("clipboard") ) )
read.csv2( con, header = T )  # is read.csv2 what you want?

(The above might be too slow if your file is very large
in which case use temporary anonymous files instead of
text connections.)

--- 
Date: Mon, 22 Dec 2003 09:52:30 -0000 
From: Simon Fear <Simon.Fear at synequanon.com>
[ Add to Address Book | Block Address | Report as Spam ] 
To: Helmut Kudrnovsky <hellik at web.de>, <r-help at stat.math.ethz.ch> 
Subject: RE: [R] Problems with read.table() 

 
 
This is a known bug (PR #3234), only affects Win98.

The problem is the degree sign (or any nonstandard ASCII)
in your first line (7SALNIG)

Mostly you can actually read these things in OK, but R crashes 
on printing. I get round it sometimes using something like

dedegree <- function(x) gsub("", "deg", x)

applied to the "offending" variables after reading in.

Having said that, I, like you, have sometimes had the crash 
on read (but not reproducibly as there are strict confidentiality
issues on my datasets). In that case you will have to edit the
data source file before reading it into R.

> -----Original Message-----
> From: Helmut Kudrnovsky [mailto:hellik at web.de]
> Sent: 22 December 2003 09:29
> To: r-help at stat.math.ethz.ch
> Subject: [R] Problems with read.table()
> 
> 
> Security Warning: 
> If you are not sure an attachment is safe to open please contact 
> Andy on x234. There are 0 attachments with this message. 
> ________________________________________________________________ 
> 
> R version 1.8.1, OS Windows 98
> 
> Dear colleagues,
> 
> if I import vegetation data (first row with column labels and 
> first column 
> with row labels) like
> 
> 7MYRGERM;7AGRGIGA;7DRYOCTO;5MYRGERM;7SALELEA;7CHOCHON;7SALNIG;.......
> t401;5;2;2;3;4;2;2;2;1;2;1;2;2;1;2;2;2;1;2;1;0;0;......
> t403;3;0;0;6;4;0;3;0;0;3;0;0;0;0;3;0;0;0;2;0;2;0;.....
> 
> with read.table("data.file", header=TRUE, separator=";"), the 
> R program 
> crashes with following error message:
> 
> RGUI verursachte einen Fehler durch eine ungltige Seite
> in Modul R.DLL bei 015f:6b515b87.
> Register:
> EAX=0075007a CS=015f EIP=6b515b87 EFLGS=00010246
> EBX=01bd3f57 SS=0167 ESP=0073ed20 EBP=0073ed48
> ECX=01bd3ad8 DS=0167 ESI=00000001 FS=2237
> EDX=ffffffb0 ES=0167 EDI=00000008 GS=0000
> Bytes bei CS:EIP:
> 0f b7 04 50 83 e0 08 eb ce 89 5d ec eb 8b 89 74
> Stapelwerte:
> 00000000 0073ee20 019ea008 00000007 00002000 0000003b 
> 000000b0 0073ee20 
> 00000006 01bd5f58 0073edb8 6b5162ef 00000010 00000001 
> 0073ed8c 0073ee20
> 
> with greetings from Tyrol
> Helmut Kudrnovsky
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

Simon Fear 
Senior Statistician 
Syne qua non Ltd 
Tel: +44 (0) 1379 644449 
Fax: +44 (0) 1379 644445 
email: Simon.Fear at synequanon.com 
web: http://www.synequanon.com 

Number of attachments included with this message: 0 

This message (and any associated files) is confidential and\...{{dropped}}



From sundar.dorai-raj at pdf.com  Mon Dec 22 16:11:22 2003
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Mon, 22 Dec 2003 09:11:22 -0600
Subject: [R] La.eigen hangs R when NaN/NA is present
Message-ID: <3FE7099A.9040504@pdf.com>

Hi all,
I discovered this problem when trying to use princomp in package:mva 
when a column (and row) in my matrix was all zeros and I set cor = TRUE, 
thus division by 0. Doing so hangs R, never to return. I have to shut 
down Rterm in the Task Manager and lose all work from the current image. 
I tracked down the problem to using La.eigen when the input matrix has 
one or more NaN/NA. Note that eigen(x, sym = TRUE) gives a sensible 
result without hanging R.

I have submitted a bug report.

 > version
          _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    1
minor    8.1
year     2003
month    11
day      21
language R
 >
 > x <- matrix(c(1, 1, 1, 0.5), 2, 2)
 > is.na(x[1, 1]) <- TRUE
 > x
      [,1] [,2]
[1,]   NA  1.0
[2,]    1  0.5
 > eigen(x, sym = TRUE)
$values
[1] NA NA

$vectors
[1] NA NA NA NA
 > La.eigen(x, sym = TRUE)
La.eigen(x, sym = TRUE)

# KILL R IN THE TASK MANAGER
Process R exited abnormally with code 1 at Mon Dec 22 08:57:51 2003



From sundar.dorai-raj at pdf.com  Mon Dec 22 16:20:02 2003
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Mon, 22 Dec 2003 09:20:02 -0600
Subject: [R] La.eigen hangs R when NaN/NA is present
In-Reply-To: <3FE7099A.9040504@pdf.com>
References: <3FE7099A.9040504@pdf.com>
Message-ID: <3FE70BA2.9010803@pdf.com>

My apologies.

I should have searched the bugs first. This was addressed here:

http://r-bugs.biostat.ku.dk/cgi-bin/R/Accuracy-fixed?id=5406;expression=eigen;user=guest

Apparently the problem has been fixed for 1.8.1-patched.

Thanks,
Sundar


Sundar Dorai-Raj wrote:
> Hi all,
> I discovered this problem when trying to use princomp in package:mva 
> when a column (and row) in my matrix was all zeros and I set cor = TRUE, 
> thus division by 0. Doing so hangs R, never to return. I have to shut 
> down Rterm in the Task Manager and lose all work from the current image. 
> I tracked down the problem to using La.eigen when the input matrix has 
> one or more NaN/NA. Note that eigen(x, sym = TRUE) gives a sensible 
> result without hanging R.
> 
> I have submitted a bug report.
> 
>  > version
>          _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    1
> minor    8.1
> year     2003
> month    11
> day      21
> language R
>  >
>  > x <- matrix(c(1, 1, 1, 0.5), 2, 2)
>  > is.na(x[1, 1]) <- TRUE
>  > x
>      [,1] [,2]
> [1,]   NA  1.0
> [2,]    1  0.5
>  > eigen(x, sym = TRUE)
> $values
> [1] NA NA
> 
> $vectors
> [1] NA NA NA NA
>  > La.eigen(x, sym = TRUE)
> La.eigen(x, sym = TRUE)
> 
> # KILL R IN THE TASK MANAGER
> Process R exited abnormally with code 1 at Mon Dec 22 08:57:51 2003
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From tlumley at u.washington.edu  Mon Dec 22 16:37:41 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 22 Dec 2003 07:37:41 -0800 (PST)
Subject: [R] varFixed
In-Reply-To: <66578BFC0BA55348B5907A0F798EE9307A2C32@ernesto.NASDC.ORG>
References: <66578BFC0BA55348B5907A0F798EE9307A2C32@ernesto.NASDC.ORG>
Message-ID: <Pine.A41.4.58.0312220736330.99394@homer41.u.washington.edu>

On Sat, 20 Dec 2003, Harold Doran wrote:

> Dear List:
>  Earlier this week I posted a question and received no response, and I
> continue to struggle with my model. My original question is pasted
> below.
>  I am using lme and want to fix the variance of the within group
> residual at 1 (e~n(0,1). I think the varFixed function should be used to
> accomplish this, but I am struggling to figure out how to do this.
>

I don't think this is possible.  I've tried to get this sort of effect
(for meta-analyses) and have not been able to. I think there is always an
overall scale factor for variances estimated.

	-thomas



From Manuel.A.Morales at williams.edu  Mon Dec 22 18:11:00 2003
From: Manuel.A.Morales at williams.edu (Manuel A. Morales)
Date: Mon, 22 Dec 2003 12:11:00 -0500
Subject: [R] repeated measures with random effects
Message-ID: <000a01c3c8ae$92e1eb90$6432a8c0@solidago>

I am trying to analyze data from an experiment where subjects were assigned
each of two treatment manipulations (t1 and t2) crossed in a factorial
design. Sets of the four possible treatment combinations were grouped
together (block) with no replication within blocks (i.e. I can't test for a
t1*t2*block effect). Finally, measurements were taken over three time
intervals (time).

Converting variables to factors, I think I can fit this as a repeated
measures design with block as a fixed effect as follows:

results.fixed <- lme(measurements ~ block+block:time+t1*t2*time, data=test,
random=~1|subject)

Or, using block as a random effect:

results.random <- lme(measurements ~ t1*t2*time, data=test,
random=~1|block/subject)

Is this is the correct syntax, especially for the random effects version?

Thanks!

Manuel



From rolf at math.unb.ca  Mon Dec 22 18:54:59 2003
From: rolf at math.unb.ca (Rolf Turner)
Date: Mon, 22 Dec 2003 13:54:59 -0400 (AST)
Subject: [R] draft of posting guide
Message-ID: <200312221754.hBMHsxcC029404@erdos.math.unb.ca>

This is in response to Gabor Grothendieck's commentary on Tony
Plate's draft guidelines for question-askers, which was posted a
couple of days ago.

I disagree, from mildly to vehemently with just about everything in
Grothendieck's posting.  E.g. the ``tone'' of the draft should not
be ``friendlier''.  The purpose of the guidelines is to encourage
the asking of well-thought out questions and discourage the asking
of stupid ones.  This politically correct ``don't damage their
self esteem attitude'' has no place in the r-help list.

A propos of bugs, for the uneducated beginner to assert that there is
a ``bug'' in software designed by some of the best and most
knowledgeable minds in the discipline, when the software works as
documented, is the height of presumptuous arrogance.

The guide is and should be a guide for the question-askers.  The
responders who are voluntarily giving of their time and (often deep)
experise need not be constrained.  The R package and this help list
are free services provided voluntarily by some great people.  If
someone asks a stupid question and dislikes being told so in so many
words, well, that person is free to take his or her business
elsewhere.

The one point I ***agree*** with is that questions about statistical
methodology should not be discouraged in any way, even if they are
not directly R-related.  There is always some sort of relationship,
such questions are interesting, and there is almost always some
insight to be gained by thinking about them in an R context.

				cheers,

					Rolf Turner



From flom at ndri.org  Mon Dec 22 19:17:32 2003
From: flom at ndri.org (Peter Flom)
Date: Mon, 22 Dec 2003 13:17:32 -0500
Subject: [R] draft of posting guide
Message-ID: <sfe6ef0a.006@MAIL.NDRI.ORG>

How about some sort of happy medium?

e.g., in the posting guide include something like

'The people who wrote R, and the people who answers questions on
R-help, are volunteers.  R software is the product of thousands of hours
of time by many highly trained and highly intelligent people.  Please
respect their time by following the guidelines below; note that failure
to follow these guidelines may result in your question being ignored, or
in responses that are less detailed than you would like." 


At the same time (and I don't think this necessarily needs to be posted
in any guide), I think that there's no call for rudeness or snippiness
on the part of people who answer questions.  Very few of the people who
post to this list are stupid; many people new to R find the learning
curve rather steep.  While this doesn't excuse rudeness or arrogance on
the part of the people who ASK questions, it also doesn't excuse
rudeness or arrogance on the part of those who answer them.  

Who knows? SOME of the people who ask an ignorant question today may
ask an intelligent one tomorrow.  SOME of these questions MAY help
others on the list, or even (dare I say it?) help the people who develop
the code.  It MIGHT even happen that someone who asks such a question
today helps write something really useful at some point in the future. 
But none of this will happen if the person stops using R.



Peter

Peter L. Flom, PhD
Assistant Director, Statistics and Data Analysis Core
Center for Drug Use and HIV Research
National Development and Research Institutes
71 W. 23rd St
www.peterflom.com
New York, NY 10010
(212) 845-4485 (voice)
(917) 438-0894 (fax)



From spencer.graves at pdf.com  Mon Dec 22 19:18:26 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 22 Dec 2003 10:18:26 -0800
Subject: [R] draft of posting guide
In-Reply-To: <200312221754.hBMHsxcC029404@erdos.math.unb.ca>
References: <200312221754.hBMHsxcC029404@erdos.math.unb.ca>
Message-ID: <3FE73572.6000707@pdf.com>

      I don't study carefully every piece of available documentation for 
everything (anything?) I do.  A major challenge is how to provide a 
guide that will get used and will in the process improve the quality of 
questions and answers. 

      Best Wishes,
      spencer graves

Rolf Turner wrote:

>This is in response to Gabor Grothendieck's commentary on Tony
>Plate's draft guidelines for question-askers, which was posted a
>couple of days ago.
>
>I disagree, from mildly to vehemently with just about everything in
>Grothendieck's posting.  E.g. the ``tone'' of the draft should not
>be ``friendlier''.  The purpose of the guidelines is to encourage
>the asking of well-thought out questions and discourage the asking
>of stupid ones.  This politically correct ``don't damage their
>self esteem attitude'' has no place in the r-help list.
>
>A propos of bugs, for the uneducated beginner to assert that there is
>a ``bug'' in software designed by some of the best and most
>knowledgeable minds in the discipline, when the software works as
>documented, is the height of presumptuous arrogance.
>
>The guide is and should be a guide for the question-askers.  The
>responders who are voluntarily giving of their time and (often deep)
>experise need not be constrained.  The R package and this help list
>are free services provided voluntarily by some great people.  If
>someone asks a stupid question and dislikes being told so in so many
>words, well, that person is free to take his or her business
>elsewhere.
>
>The one point I ***agree*** with is that questions about statistical
>methodology should not be discouraged in any way, even if they are
>not directly R-related.  There is always some sort of relationship,
>such questions are interesting, and there is almost always some
>insight to be gained by thinking about them in an R context.
>
>				cheers,
>
>					Rolf Turner
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From wolski at molgen.mpg.de  Mon Dec 22 20:32:15 2003
From: wolski at molgen.mpg.de (Wolski)
Date: Mon, 22 Dec 2003 20:32:15 +0100
Subject: [R] draft of posting guide
In-Reply-To: <3FE73572.6000707@pdf.com>
References: <200312221754.hBMHsxcC029404@erdos.math.unb.ca>
	<3FE73572.6000707@pdf.com>
Message-ID: <200312222032150737.0232E6A3@harry.molgen.mpg.de>

Hi!

The guide are in my opinion much to long. 
If someone posts a question to the mailing list its because he likes to get a answer (fast?).
The "Introduction" proposed by Peter Flom and the "Homework before posting" section will do it in my opinion.
The part:
"Homework before posting a question. " is enough.
It may be better to call it:
"How to find answers to urgent questions"


All this "stuff" about how to behave, and ask questions is superfluous in my opinion.  If you don't understand the question don't answer. If you don't have time to answer don't do it. If you don't understand the answers look for it somewhere else. If someone is rude on the mailing list I can ignore him. If I do not like answers from someone  I simply don't open the mails (Or put him into the spam filter if you are paranoic). If I don't like questions from someone I do the same. Its not public traffic, the underground, or the street, where you can't avoid contact with the smoke if someone are smoking ore where you have to leave the train if someone stinks or where you have to fight if someone is attacking someone else. 


I am very happy and I like the R-help list how it is.

So, Merry Christmas to all of you!


Eryk



*********** REPLY SEPARATOR  ***********

On 12/22/2003 at 10:18 AM Spencer Graves wrote:

>I don't study carefully every piece of available documentation for 
>everything (anything?) I do.  A major challenge is how to provide a 
>guide that will get used and will in the process improve the quality of 
>questions and answers. 
>
>      Best Wishes,
>      spencer graves
>
>Rolf Turner wrote:
>
>>This is in response to Gabor Grothendieck's commentary on Tony
>>Plate's draft guidelines for question-askers, which was posted a
>>couple of days ago.
>>
>>I disagree, from mildly to vehemently with just about everything in
>>Grothendieck's posting.  E.g. the ``tone'' of the draft should not
>>be ``friendlier''.  The purpose of the guidelines is to encourage
>>the asking of well-thought out questions and discourage the asking
>>of stupid ones.  This politically correct ``don't damage their
>>self esteem attitude'' has no place in the r-help list.
>>
>>A propos of bugs, for the uneducated beginner to assert that there is
>>a ``bug'' in software designed by some of the best and most
>>knowledgeable minds in the discipline, when the software works as
>>documented, is the height of presumptuous arrogance.
>>
>>The guide is and should be a guide for the question-askers.  The
>>responders who are voluntarily giving of their time and (often deep)
>>experise need not be constrained.  The R package and this help list
>>are free services provided voluntarily by some great people.  If
>>someone asks a stupid question and dislikes being told so in so many
>>words, well, that person is free to take his or her business
>>elsewhere.
>>
>>The one point I ***agree*** with is that questions about statistical
>>methodology should not be discouraged in any way, even if they are
>>not directly R-related.  There is always some sort of relationship,
>>such questions are interesting, and there is almost always some
>>insight to be gained by thinking about them in an R context.
>>
>>				cheers,
>>
>>					Rolf Turner
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>  
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help



Dipl. bio-chem. Eryk Witold Wolski    @    MPI-MG Dep. Vertebrate Genomics   
Ihnestrasse 73 14195 Berlin          'v'    
tel: 0049-30-84131285               /   \    
mail: wolski at molgen.mpg.de        ---W-W----    http://www.molgen.mpg.de/~wolski



From spencer.graves at pdf.com  Mon Dec 22 20:37:53 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 22 Dec 2003 11:37:53 -0800
Subject: [R] varFixed
In-Reply-To: <Pine.A41.4.58.0312220736330.99394@homer41.u.washington.edu>
References: <66578BFC0BA55348B5907A0F798EE9307A2C32@ernesto.NASDC.ORG>
	<Pine.A41.4.58.0312220736330.99394@homer41.u.washington.edu>
Message-ID: <3FE74811.4000206@pdf.com>

      If I had several days to work on this, I'd study Pinheiro and 
Bates (2000) Mixed-Effects Models in S and S-Plus (Springer) to see if I 
could use "lme or "gls" on this.  If I expected to encounter many 
similar problems in the future, I might modify "lme" to accept a prior 
distribution over the parameters to be estimated. 

      However, if I needed an answer today for a problem I might not see 
again for a while, I might just write a log likelihood and give it to 
"optim", something like the following:  The following looks to me like 
what you are describing, possibly simplified: 

      y[i,j] = mu + a[i] + e[i,j], where a[i] ~ N(0, s.a^2) and e[i,j] ~ 
N(0, 1), i = 1, ..., n, j = 1, ..., m[i].  

      If this is correct, then this is equivalent to the following: 

      y ~ N(mu*One, Sig), where y = vector of all observations, One = 
vector of all 1's, and Sig = s.a.^2*diag(sum(m)) + SigW, where SigW = 
block diagonal matrix with i-th block = m[i] x m[i] matrix of all 1's.  
Then the "deviance" = (-2)*log(likelihood) can be written as follows: 

      Deviance1 = 
sum(m)*log(2*pi)+log(det(Sig))+t(y-mu)%*%inverse(Sig)%*%(y-mu). 

      In another hour, I'd have a function written to compute 
"Deviance1" in terms of mu and log(s.a) -- not s.a directly.  I'd have 
the minimum deviance + hessian / information from "optim" and a contour 
plot of "Deviance1" in the regions.  From this I could get confidence 
regions using 2*log(likelihood ratio) is approximately chi-square, etc.  
In a couple of days, I could also do a Monte Carlo study to evaluate the 
accuracy of the normal approximations, etc. 

      hope this helps. 
      spencer graves

, esp. pp. 249-

Thomas Lumley wrote:

>On Sat, 20 Dec 2003, Harold Doran wrote:
>
>  
>
>>Dear List:
>> Earlier this week I posted a question and received no response, and I
>>continue to struggle with my model. My original question is pasted
>>below.
>> I am using lme and want to fix the variance of the within group
>>residual at 1 (e~n(0,1). I think the varFixed function should be used to
>>accomplish this, but I am struggling to figure out how to do this.
>>
>>    
>>
>
>I don't think this is possible.  I've tried to get this sort of effect
>(for meta-analyses) and have not been able to. I think there is always an
>overall scale factor for variances estimated.
>
>	-thomas
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From RichardsTJ2 at UPMC.EDU  Mon Dec 22 21:11:00 2003
From: RichardsTJ2 at UPMC.EDU (Richards, Thomas)
Date: Mon, 22 Dec 2003 15:11:00 -0500
Subject: [R] Memory allocation
Message-ID: <2554B4CA518D504A81E6908E39478A6A326E3C@1upmc-msx10.isdip.upmc.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031222/8370b0e7/attachment.pl

From feh3k at spamcop.net  Mon Dec 22 21:29:29 2003
From: feh3k at spamcop.net (Frank E Harrell Jr)
Date: Mon, 22 Dec 2003 14:29:29 -0600
Subject: [R] draft of posting guide
In-Reply-To: <200312222032150737.0232E6A3@harry.molgen.mpg.de>
References: <200312221754.hBMHsxcC029404@erdos.math.unb.ca>
	<3FE73572.6000707@pdf.com>
	<200312222032150737.0232E6A3@harry.molgen.mpg.de>
Message-ID: <20031222142929.46d52ab1.feh3k@spamcop.net>

On Mon, 22 Dec 2003 20:32:15 +0100
"Wolski" <wolski at molgen.mpg.de> wrote:

> Hi!
> 
> The guide are in my opinion much to long. 
> If someone posts a question to the mailing list its because he likes to
> get a answer (fast?). The "Introduction" proposed by Peter Flom and the
> "Homework before posting" section will do it in my opinion. The part:
> "Homework before posting a question. " is enough.
> It may be better to call it:
> "How to find answers to urgent questions"
> 
> 
> All this "stuff" about how to behave, and ask questions is superfluous
> in my opinion.  If you don't understand the question don't answer. If
> you don't have time to answer don't do it. If you don't understand the
> answers look for it somewhere else. If someone is rude on the mailing
> list I can ignore him. If I do not like answers from someone  I simply
> don't open the mails (Or put him into the spam filter if you are
> paranoic). If I don't like questions from someone I do the same. Its not
> public traffic, the underground, or the street, where you can't avoid
> contact with the smoke if someone are smoking ore where you have to
> leave the train if someone stinks or where you have to fight if someone
> is attacking someone else. 
> 
> 
> I am very happy and I like the R-help list how it is.
> 
> So, Merry Christmas to all of you!
> 
> 
> Eryk

These sentiments do not take into account the time required to determine
that I shouldn't answer someone's e-mail, nor does it consider that with
some prompting, new users can ask questions better.

Frank


---
Frank E Harrell Jr   Professor and Chair           School of Medicine
                     Department of Biostatistics   Vanderbilt University



From ripley at stats.ox.ac.uk  Mon Dec 22 21:38:06 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 22 Dec 2003 20:38:06 +0000 (GMT)
Subject: [R] Memory allocation
In-Reply-To: <2554B4CA518D504A81E6908E39478A6A326E3C@1upmc-msx10.isdip.upmc.edu>
Message-ID: <Pine.LNX.4.44.0312222035260.12903-100000@gannet.stats>

Not really much hope here, but

1) If you have fast discs, try increasing --max-mem-size to more than your 
RAM, and

2) Try compiling up R-devel (see the FAQ for where to get it) as it has
a potentially better memory allocator.

I supect though that your problem is too big for R on 32-bit Windows.

BDR

On Mon, 22 Dec 2003, Richards, Thomas wrote:

> Hello:
> 
> 	I am trying to work with a couple of microarray data sets, using
> 
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    1              
> minor    8.1            
> year     2003           
> month    11             
> day      21             
> language R              
> 
> 
> In the shortcut for invoking R I have set --max-mem-size=1024M, so that I
> get
> 
> > memory.limit()
> [1] 1073741824
> 
> Below is an example of what keeps happening as I am working. Any suggestions
> as to how I can stop running out of mermory?
> 
> > memory.size()
> [1] 502904736
> > log2.harvAD <- log2.harvAD[log2.harvAD$Probesets %in%
> harvard.genes$probeset,]
> Error: cannot allocate vector of size 49 Kb
> > log2.harvAD <- log2.harvAD[,c(1,1+order(names(log2.harvAD)[-1]))]
> Error: cannot allocate vector of size 49 Kb
> > log2.harvAD.annot <- unlist(lapply(strsplit(names(log2.harvAD),split=":"),
> +   function(L) L[1]))[-1]
> > log2.harvAD$probeset <- as.character(log2.harvAD$probeset)
> Error: cannot allocate vector of size 49 Kb
> > memory.size()
> [1] 502912536
> > gc()
>            used  (Mb) gc trigger  (Mb)
> Ncells  2586025  69.1    6812252 182.0
> Vcells 20108076 153.5   41205530 314.4
> > memory.size()
> [1] 330645720
> > memory.limit()/memory.size()
> [1] 3.247408
> > ##  Try again:
> > log2.harvAD <- log2.harvAD[log2.harvAD$Probesets %in%
> harvard.genes$probeset,]
> Error: cannot allocate vector of size 49 Kb
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From schoenle at fas.harvard.edu  Mon Dec 22 22:55:58 2003
From: schoenle at fas.harvard.edu (Raphael Schoenle)
Date: Mon, 22 Dec 2003 16:55:58 -0500
Subject: [R] missing data and completed missing data
Message-ID: <000a01c3c8d6$656acb00$7502a8c0@acer4jbjp1qwlp>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031222/6b760678/attachment.pl

From feh3k at spamcop.net  Mon Dec 22 23:01:23 2003
From: feh3k at spamcop.net (Frank E Harrell Jr)
Date: Mon, 22 Dec 2003 16:01:23 -0600
Subject: [R] missing data and completed missing data
In-Reply-To: <000a01c3c8d6$656acb00$7502a8c0@acer4jbjp1qwlp>
References: <000a01c3c8d6$656acb00$7502a8c0@acer4jbjp1qwlp>
Message-ID: <20031222160123.0e00a296.feh3k@spamcop.net>

On Mon, 22 Dec 2003 16:55:58 -0500
"Raphael Schoenle" <schoenle at fas.harvard.edu> wrote:

> Hi,
>  
> This is not exactly an R request, but does anyone know of a good dataset
> that contains missing and missing data that have been completed later
> (like from persistent in-person interview attempts)? (want it for some
> Bayesian regression analysis)
>  
> Thanks!!
>  
> -Raphael
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

Not quite, but see

@Article{eng03imp,
  author =               {Engels, Jean Mundahl and Diehr, Paula},
  title =                {Imputation of missing longitudinal data: a
comparison of methods},
  journal =      J Clinical Epidemiology,
  year =                 2003,
  volume =               56,
  pages =                {968-976},
  annote =               {longitudinal data;repeated
measures;within-subject
imputation vs. using baseline data vs. population group;natural
experiment that solved problems of simulated data because used real
data with real missingness pattern with known true value;true value
was a value observed after a missing response at a certain time, which
was made to be artificially missing;most subjects had such
measurements really missing;gold standard was ability to reproduce the
known value, not performance in the final response model (or group
comparison);LOCF;longitudinal imputation;next observation carried
backward}

---
Frank E Harrell Jr   Professor and Chair           School of Medicine
                     Department of Biostatistics   Vanderbilt University



From maj at stats.waikato.ac.nz  Mon Dec 22 23:27:16 2003
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Tue, 23 Dec 2003 11:27:16 +1300
Subject: [R] missing data and completed missing data
In-Reply-To: <000a01c3c8d6$656acb00$7502a8c0@acer4jbjp1qwlp>
References: <000a01c3c8d6$656acb00$7502a8c0@acer4jbjp1qwlp>
Message-ID: <3FE76FC4.30507@stats.waikato.ac.nz>

You might take a set of classified data, say Fisher's irises, and treat 
the classification as initially unknown.

Murray

Raphael Schoenle wrote:

> Hi,
>  
> This is not exactly an R request, but does anyone know of a good dataset
> that contains missing and missing data that have been completed later
> (like from persistent in-person interview attempts)? (want it for some
> Bayesian regression analysis)
>  
> Thanks!!
>  
> -Raphael
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862



From adi at roda.ro  Mon Dec 22 23:45:34 2003
From: adi at roda.ro (Adrian Dusa)
Date: Tue, 23 Dec 2003 00:45:34 +0200
Subject: [R] beginner programming question
Message-ID: <1072133134.3fe7740e56793@ns.roda.ro>

Thank you all! I did it, and it worked just fine. In the last week I've been 
torturing the syntaxes in various ways, until finally it was all clear. The 
subscripting solution opened new doors for me.
Particularly, the reshape command gave me about three days of a head ache. I 
read the help about 20 times, trying to figure out how to do it; the trouble 
with the help was that it doesn't present examples of reshaping for multiple 
sets of varying variables, nor that the new variables' names in the long format 
should be defined as a vector with the v.names attribute.

Anyway, the syntax is:

> x <- read.table("clipboard", header=T)
> x
  rel1 rel2 rel3 age0 age1 age2 age3 sex0 sex1 sex2 sex3
1    1    3   NA   25   23    2   NA    1    2    1   NA
2    4    1    3   35   67   34   10    2    2    1    2
3    1    4    4   39   40   59   60    1    2    2    1
4    4   NA   NA   45   70   NA   NA    2    2   NA   NA

> xx <- reshape(x, varying=list(names(x)[1:3], names(x)[5:7], 
+ names(x)[9:11]), v.names=c("rel", "age", "sex"), direction="long")
> xx
    age0 sex0 time rel age sex id
1.1   25    1    1   1  23   2  1
2.1   35    2    1   4  67   2  2
3.1   39    1    1   1  40   2  3
4.1   45    2    1   4  70   2  4
1.2   25    1    2   3   2   1  1
2.2   35    2    2   1  34   1  2
3.2   39    1    2   4  59   2  3
4.2   45    2    2  NA  NA  NA  4
1.3   25    1    3  NA  NA  NA  1
2.3   35    2    3   3  10   2  2
3.3   39    1    3   4  60   1  3
4.3   45    2    3  NA  NA  NA  4

> xx <- subset(xx, xx$rel==1)
> rbind(subset(xx, xx$sex0==1)[,c("age0","age")],
+ subset(xx, xx$sex==1)[,c("age","age0")])
    age0 age
1.1   25  23
3.1   39  40
2.2   35  34

I wish you a Merry Xmas, you are a truly great community.
Adrian

-----Original Message-----
From: Thomas Lumley [mailto:tlumley at u.washington.edu] 
Sent: Thursday, December 18, 2003 5:53 PM
To: Tony Plate
Cc: adi at roda.ro; r-help at stat.math.ethz.ch
Subject: Re: [R] beginner programming question

On Wed, 17 Dec 2003, Tony Plate wrote:

> Another way to approach this is to first massage the data into a more
> regular format.  This may or may not be simpler or faster than other
> solutions suggested.

You could also use the reshape() command to do the massaging

	-thomas

>  > x <- read.table("clipboard", header=T)
>  > x
>    rel1 rel2 rel3 age0 age1 age2 age3 sex0 sex1 sex2 sex3
> 1    1    3   NA   25   23    2   NA    1    2    1   NA
> 2    4    1    3   35   67   34   10    2    2    1    2
> 3    1    4    4   39   40   59   60    1    2    2    1
> 4    4   NA   NA   45   70   NA   NA    2    2   NA   NA
>  > nn <- c("rel","age0","age","sex0","sex")
>  > xx <- rbind("colnames<-"(x[,c("rel1","age0","age1","sex0","sex1")], nn),
> +  "colnames<-"(x[,c("rel2","age0","age2","sex0","sex2")], nn),
> +  "colnames<-"(x[,c("rel3","age0","age3","sex0","sex3")], nn))
>  > xx
>     rel age0 age sex0 sex
> 1    1   25  23    1   2
> 2    4   35  67    2   2
> 3    1   39  40    1   2
> 4    4   45  70    2   2
> 11   3   25   2    1   1
> 21   1   35  34    2   1
> 31   4   39  59    1   2
> 41  NA   45  NA    2  NA
> 12  NA   25  NA    1  NA
> 22   3   35  10    2   2
> 32   4   39  60    1   1
> 42  NA   45  NA    2  NA
>  >
>  > rbind(subset(xx, xx$rel==1 & (xx$sex0==1 |
> xx$sex0==xx$sex))[,c("age0","age")], subset(xx, xx$rel==1 & xx$sex==1 &
> xx$sex0!=xx$sex)[,c("age","age0")])
>     age0 age
> 1    25  23
> 3    39  40
> 21   35  34
>  >
>
> hope this helps,
>
> Tony Plate
>
> PS.  To advanced R users: Is the above usage of the "colnames<-" function
> within an expression regarded as acceptable or as undesirable programming
> style? -- I've rarely seen it used, but it can be quite useful.




-------------------------------------------------
This mail sent through IMP: http://horde.org/imp/



From Tom.Mulholland at health.wa.gov.au  Tue Dec 23 02:49:18 2003
From: Tom.Mulholland at health.wa.gov.au (Mulholland, Tom)
Date: Tue, 23 Dec 2003 09:49:18 +0800
Subject: [R] draft of posting guide
Message-ID: <74E242B6968AA0469B632C5A3EFC1EFD03D57022@nt207mesep.corporate.hdwa.health.wa.gov.au>

I think there will always be disagreement when commenting about the
appropriateness of social behaviour. So I think we will do well to
understand the purpose of any proposed posting guide. It is not clear to
me where the list is going with regards to this topic. If the aim is to
produce a comprehensive posting guide to sit with other R documents, I
wish the list well and will check on progress some time in the future. I
can't see some points being reconciled quickly.

If we are talking about something else, I have previously suggested a
short monthly reminder, then it may be possible to make some progress.
Frank Harrell noted that "with some prompting, new users can ask
questions better."  If we focus on the mechanics of question asking
rather than on the social aspects we may find it easier to produce
something. I guess I'm asking the question "What are the prompts?"

If I were to make a checklist it would be

Before asking the question
  Have you read the FAQs?

  If you use windows, have you read the Windows FAQ?

  Have you searched the R-help archives?

  Have you read the online help for relevant functions?

  Have you checked to see if the answer is in one of the reference
manuals, supplementary documents or Newsletters?

  Do you have the latest version of R?

  Is this an R question?

Once you need to ask the question
  Do you need to include a workable example so people understand your
problem?

  Do you need to include details about your operating system?

  Do you need to include which version of R are you using?

This obviously would need something else as some of the questions beg
questions themselves. It is however moving towards what I had in my mind
when I first suggested the monthly reminder.

Tom

_________________________________________________
 
Tom Mulholland
Senior Policy Officer
WA Country Health Service
Tel: (08) 9222 4062
 
The contents of this e-mail transmission are confidential an...{{dropped}}



From wolski at molgen.mpg.de  Tue Dec 23 05:31:32 2003
From: wolski at molgen.mpg.de (Eryk Wolski)
Date: Tue, 23 Dec 2003 05:31:32 +0100 (MET)
Subject: [R] draft of posting guide. Sorry.
Message-ID: <Pine.OSF.4.31.0312230523330.10948-100000@harry.molgen.mpg.de>

Hi!

Sorry. Please take my last mail to the account that it was monday and I
had two "hard" birthday party's during the weekend. Probably all this
caused the problem to express that the style of the "mailing list guide"
shocked me. I asked this morning such a "stupid"(if you know the answer)
question. But to me, it was a very important question and to get the
answer was it too. I felt scared. I hope that this are not the intention
of that guide. I cooled down now and therefore give me a chance to explain
why that user guide scares me.

As I said, the guide had given me the feeling that someone wants to censor
me. Especially the first section of the Posting Guide: "How to ask good
questions that prompt useful answers" does this. The guide starts with
talking mainly about what you should not, or what you must not do. Some
examples come quite late and after the "you must not cross fences, you
must not..." introduction, I simply stopped to read. To much regulation
kills spontaneity. Lack of spontaneity kills creativity, It cant be!, is
what I thought. Now I had read the reminder of the Posting guide.

What I am missing are a short introduction answering such questions: What
are the intention of this guide? What are the problems it is going to
address?

I think that some hints to people that answer would not harm!
The cases that someone does not get an answer are seldom. Often there are
tens of answers to question. I have the impression that there are a
COMPETITION for the best solution. I think that most of the beginners can
live with a working solution, even if it is not the best one. If I ask a
question than its because I want to get my work done and not to test the
mailing list participants.This may make the workload smaller and may
encourage less experienced R user to try to give answers.
Not to take a questions as an EXAMINATION situation can make it also less
aching or painfull if the question are not as precise as "wished". By
changing this attitude of examiner,student, many of the points
in this guide will be superfluous!

Why the guide does NOT mention in one word that posting questions on the
mailing list has also some DISADVANTAGES? e.g. Answers written in haste,
bad temper (see my answer, sorry again), or answers two days later.  (And
if  you know the right place too look you will get the answer
immediately.)

I even do not think the mailing list should be the last place where you
are allowed to look for help. Simple trying to formulate the question to
post it on the list can be helpfull. Why to make it so difficult to
someone to try it?

I personally find it very good if the same thing is asked ten different
times in 3 different ways. This increases the probability that I will find
a answer to my problem searching the mailing list.
Its also true that many questions can be answered with a short "?command".
But this does not make it superfluous.

At last I like to mention one important source of help which are missing
in the posting guide, and which I forgot these days by myself: R CMD -help
and R --help are also very important help sources! If I had remembered it
yesterday morning I would not have to ask about. But was it really so bad
that I had?

I hope that this email will be helpfull.

Merry Christmass.
Sincerely.

Eryk



From rossini at blindglobe.net  Tue Dec 23 06:22:32 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Mon, 22 Dec 2003 21:22:32 -0800
Subject: [R] draft of posting guide. Sorry.
In-Reply-To: <Pine.OSF.4.31.0312230523330.10948-100000@harry.molgen.mpg.de>
	(Eryk Wolski's message of "Tue, 23 Dec 2003 05:31:32 +0100 (MET)")
References: <Pine.OSF.4.31.0312230523330.10948-100000@harry.molgen.mpg.de>
Message-ID: <85y8t4m7l3.fsf@blindglobe.net>


A few comments...

Eryk Wolski <wolski at molgen.mpg.de> writes:

> As I said, the guide had given me the feeling that someone wants to censor
> me. Especially the first section of the Posting Guide: "How to ask good
> questions that prompt useful answers" does this. The guide starts with
> talking mainly about what you should not, or what you must not do. Some
> examples come quite late and after the "you must not cross fences, you
> must not..." introduction, I simply stopped to read. To much regulation
> kills spontaneity. Lack of spontaneity kills creativity, It cant be!, is
> what I thought. Now I had read the reminder of the Posting guide.

There is no real regulation with the guide.  It's a guide, and you are
free to use it (hopefully to your advantage) or ignore it (hopefully,
not to your disadvantage).  But you never know.  It's sort of like
Russian Roulette.  I can guide you against it, but you still might
play... 

> What I am missing are a short introduction answering such questions: What
> are the intention of this guide? What are the problems it is going to
> address?

Ideally, it provides a way to think through solutions to problems that
are "obvious", leaving the mailing list to those which are
"interesting".

All words in quotes are contextually defined, of course.

> I think that some hints to people that answer would not harm!
> The cases that someone does not get an answer are seldom. Often there are
> tens of answers to question. I have the impression that there are a
> COMPETITION for the best solution. I think that most of the beginners can
> live with a working solution, even if it is not the best one. If I ask a
> question than its because I want to get my work done and not to test the
> mailing list participants.This may make the workload smaller and may
> encourage less experienced R user to try to give answers.
> Not to take a questions as an EXAMINATION situation can make it also less
> aching or painfull if the question are not as precise as "wished". By
> changing this attitude of examiner,student, many of the points
> in this guide will be superfluous!

Some solutions are good, others are bad.  Solutions which exist in the
documentation are generally good -- it is rare (in my experience,
probably 8 years of using R) that they are wrong.

> Why the guide does NOT mention in one word that posting questions on the
> mailing list has also some DISADVANTAGES? e.g. Answers written in haste,
> bad temper (see my answer, sorry again), or answers two days later.  (And
> if  you know the right place too look you will get the answer
> immediately.)

Answers might not even be correct.  That is the argument against
moving from this list to another, unless the people that really know
the answer move as well.

> I even do not think the mailing list should be the last place where you
> are allowed to look for help. Simple trying to formulate the question to
> post it on the list can be helpfull. Why to make it so difficult to
> someone to try it?

You can.  However, spending 5-10 minutes with the documentation
sources will sometimes (not always) solve the problem.  Sometimes. 

> I personally find it very good if the same thing is asked ten different
> times in 3 different ways. This increases the probability that I will find
> a answer to my problem searching the mailing list.
> Its also true that many questions can be answered with a short "?command".
> But this does not make it superfluous.

It does, actually.  "help.search()" is your friend.  Read Eric's guide
to asking questions again.  Initial stupid questions make it hard to
fix your reputation.  People have overcome reputations for initial
stupidity, but it is sometimes much easier just to not be stupid in
the first place.  Most of the people that understand R can be
classified as "hackers", using Eric's jargon.   Note that I would
never claim to be one of them.

I realize that figuring out whether the question is stupid can be
tough for a beginner. However, the amount (and quality) of
(freely-available, at least for the cost of download, which might not
be free) documentation for R is simply incredible.  The closest that
I've seen, for freely available languages, is Python, for actual
quality of documentation.  And with R, most of the functions have
examples; plus, actual source code is usually easier to come by.

Sure, not everyone is a code hound.  But it's a great skill to pick
up, since the answers are all there.

best,
-tony

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From tchur at optushome.com.au  Tue Dec 23 07:14:19 2003
From: tchur at optushome.com.au (Tim Churches)
Date: Tue, 23 Dec 2003 17:14:19 +1100
Subject: [R] draft of posting guide. Sorry.
Message-ID: <200312230614.hBN6EJ326567@mail017.syd.optusnet.com.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031223/efdeec2c/attachment.pl

From arnab at myrealbox.com  Tue Dec 23 08:20:43 2003
From: arnab at myrealbox.com (Arnab mukherji)
Date: Tue, 23 Dec 2003 07:20:43 +0000
Subject: [R] Rd.sty not found - 
Message-ID: <1072164043.9e402c40arnab@myrealbox.com>

Hi 

 I  am still trying to wrap up a package using Rcmd (in windows 2000 professional with R 1.8.1). I have made some progress - and now at least Rcmd check works - it still giving me an error message when it creates latex help files though. Thus I get:

c:\rpack> Rcmd check test
* checking for working latex ... OK
* using log directory 'C:/rpacks/test.Rcheck'
* checking for file 'test/DESCRIPTION' ... OK
* checking if this is a source package ... OK

---------- Making package test ------------
  adding build stamp to DESCRIPTION
  installing R files
  installing man source files
  installing indices
  installing help
 >>> Building/Updating help pages for package 'test'
     Formats: text html latex example 
  test                              text    html    latex   example
 >>> Building/Updating help pages for package 'test'
     Formats: chm 
make[2]: `test.chm' is up to date.
  adding MD5 sums

installing R.css in C:/rpacks/test.Rcheck

...
* checking Rd \usage sections ... OK
* checking examples ... OK
* creating test-manual.tex ... OK
* checking test-manual.tex ... ERROR
LaTeX errors when creating DVI version.
This typically indicates Rd problems.

Everything is handled smoothly except for creating the test-manual. I looked at the error log and it told me that this was due to :

! LaTeX Error: File `Rd.sty' not found.

Now I do have Rd.sty in my $HOME\share directory - so i tried to check the configurations of MikTeX. I even tried adding the share directory to PATH to see if that helped; it didn't.  I am not sure what else I can do to get MikTeX to recognize Rd.sty in the R directory.

I would be grateful for any suggestions.

thank you.


Arnab



From tplate at acm.org  Tue Dec 23 08:25:55 2003
From: tplate at acm.org (Tony Plate)
Date: Tue, 23 Dec 2003 00:25:55 -0700
Subject: [R] revision of posting guide
Message-ID: <5.1.0.14.2.20031222235728.02d06e48@pop6.attglobal.net>

I've placed a revision of the posting guide at http://pws.prserv.net/tap/posting-guide-draft2.html. (Posted there to make the HTML formatting easy to read.)  I've tried to incorporate the suggestions people posted and mailed to me.  Quite a few were of the form "too long, but you should add this..." :-)

Brian Ripley, Jason Turner, Peter Dalgard, Patrick Burns, Frank Harrell, Gabor Grothendieck, Spencer Graves, and Patrick Connolly all made valuable suggestions that I've tried to incorporate (apologies if I omitted anyone!)  Jonathan Baron gave some much appreciated suggestions on how to make it more concise, which I have probably not heeded as much as I should have.  I did try to put the more important material at the top of the document.

Suggestions and comments on the revised version are welcome -- deletions, additions, corrections, changes all considered.  In the absence of such, I'm pretty much ready to hand this over to the R-project.org site maintainers for posting (let me know if there are any formatting changes needed to make it easy to use there.)  

Based on the widely held fear that posters won't read a long document before posting, this guide should probably go on its own page at r-project.org (which might help prevent it getting lost as part of a larger document).  One way to make this information readily visible and accessible would be to have the mail software place an additional line at the bottom of every post, something like "Read the <posting guide> before posting here".  Several correspondents liked this idea.  Is this desirable and/or doable?

thanks for all the assistance,

Tony Plate



From ripley at stats.ox.ac.uk  Tue Dec 23 09:50:16 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 23 Dec 2003 08:50:16 +0000 (GMT)
Subject: [R] Rd.sty not found - 
In-Reply-To: <1072164043.9e402c40arnab@myrealbox.com>
Message-ID: <Pine.LNX.4.44.0312230840340.13890-100000@gannet.stats>

On Tue, 23 Dec 2003, Arnab mukherji wrote:

> Hi 
> 
>  I  am still trying to wrap up a package using Rcmd (in windows 2000 professional with R 1.8.1). I have made some progress - and now at least Rcmd check works - it still giving me an error message when it creates latex help files though. Thus I get:
> 
> c:\rpack> Rcmd check test
> * checking for working latex ... OK
> * using log directory 'C:/rpacks/test.Rcheck'
> * checking for file 'test/DESCRIPTION' ... OK
> * checking if this is a source package ... OK
> 
> ---------- Making package test ------------
>   adding build stamp to DESCRIPTION
>   installing R files
>   installing man source files
>   installing indices
>   installing help
>  >>> Building/Updating help pages for package 'test'
>      Formats: text html latex example 
>   test                              text    html    latex   example
>  >>> Building/Updating help pages for package 'test'
>      Formats: chm 
> make[2]: `test.chm' is up to date.
>   adding MD5 sums
> 
> installing R.css in C:/rpacks/test.Rcheck
> 
> ...
> * checking Rd \usage sections ... OK
> * checking examples ... OK
> * creating test-manual.tex ... OK
> * checking test-manual.tex ... ERROR
> LaTeX errors when creating DVI version.
> This typically indicates Rd problems.
> 
> Everything is handled smoothly except for creating the test-manual. I
> looked at the error log and it told me that this was due to :
> 
> ! LaTeX Error: File `Rd.sty' not found.
> 
> Now I do have Rd.sty in my $HOME\share directory - so i tried to check

But it's the one in R_HOME/share/texmf you want.

> the configurations of MikTeX. I even tried adding the share directory to
> PATH to see if that helped; it didn't.  I am not sure what else I can do
> to get MikTeX to recognize Rd.sty in the R directory.

It should be looking in TEXINPUTS, as is the point of the following in the 
check script

            $ENV{'TEXINPUTS'} =
              env_path(&file_path($R::Vars::R_HOME, "share", "texmf"),
                       $ENV{'TEXINPUTS'});

So you could try adding to TEXINPUTS yourself, and you should certainly
check the current setting of the environment variable (unset is fine).

Perhaps someone who uses MikTeX can help or assist -- the recommendation 
is fptex, which is also what is on TeXLive.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From fredrik.lundgren at norrkoping.mail.telia.com  Tue Dec 23 09:59:35 2003
From: fredrik.lundgren at norrkoping.mail.telia.com (Fredrik Lundgren)
Date: Tue, 23 Dec 2003 09:59:35 +0100
Subject: [R] unary operations
Message-ID: <000901c3c933$1872d600$2d0ffea9@oemcomputer>

Hello,

I had expected that the following should work

mat <- matrix(c(5, 7, 6, 7, 8, 10, 17, 28, 36, 41, 46, 140), nrow = 2, byrow = T)
mat

apply(mat, 2, function(e1,e2) e1*e2)
apply(mat, 2, function(e1,e2) e1+e2)
apply(mat, 2, function(e1,e2) e1-e2)
apply(mat, 2, function(e1,e2) e1/e2)

but I get 

Error in FUN(newX[, i], ...) : Argument "e2" is missing, with no default

What do I do wrong?

Sincerely Fredrik Lundgren



From Pascal.Niklaus at unibas.ch  Tue Dec 23 10:13:38 2003
From: Pascal.Niklaus at unibas.ch (Pascal.Niklaus@unibas.ch)
Date: Tue, 23 Dec 2003 10:13:38 +0100
Subject: [R] draft of posting guide
In-Reply-To: <74E242B6968AA0469B632C5A3EFC1EFD03D57022@nt207mesep.corporate.hdwa.health.wa.gov.au>
References: <74E242B6968AA0469B632C5A3EFC1EFD03D57022@nt207mesep.corporate.hdwa.health.wa.gov.au>
Message-ID: <1072170818.3fe807427b48b@webmail.unibas.ch>

Having followed the discussion about this posting guide, I'd like to add a few 
comments. While I think that the *content* is fully appropriate, I am not sure 
whether the *form* is the appropriate, for the following reason: 
 
- The folks posting "questions" like "I've installed R two minutes ago, what 
do I do next?" obviously do not even try to read *any* documentation, and they 
obviously will also ignore the posting guide. So the posting guide will not 
fix that problem. 
 
- People tending not to dare to ask questions because they are intimidated by 
some aspects of the list (and after the r-beginner discussion we now know that 
some feel like that) would be helped by a more positive wording of the same 
issues in posting guide. The motto should be "help to write better questions" 
rather than "stop asking poor questions". The content is all there in the 
draft, it is more about changing individual words. Re-posting it monthly on 
the list is a good idea. 
 
- It would probably also help to add a search form for the mailing list 
archives to the "Documentation -> Help Pages" section of R-help. I know it is 
on r-project.org, but you need *less* mouse clicks to subscribe than to get to 
the search form! Also, it is at the very bottom of the respective web page. 
 
- Wasn't there an article on how to get help in a recent issue of R-news? 
Maybe it could be placed at a prominent placed in the "Help Pages" as well. 
 
Pascal 
 
 
 

-------------------------------------------------
This mail sent through IMP: http://horde.org/imp/



From B.Rowlingson at lancaster.ac.uk  Tue Dec 23 10:39:02 2003
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 23 Dec 2003 09:39:02 +0000
Subject: [R] unary operations
In-Reply-To: <000901c3c933$1872d600$2d0ffea9@oemcomputer>
References: <000901c3c933$1872d600$2d0ffea9@oemcomputer>
Message-ID: <3FE80D36.90202@lancaster.ac.uk>

Fredrik Lundgren wrote:

> apply(mat, 2, function(e1,e2) e1*e2)

> but I get 
> 
> Error in FUN(newX[, i], ...) : Argument "e2" is missing, with no default
> 
> What do I do wrong?

  Misunderstand what 'apply' does perhaps?

  It applies your function to columns (the '2') of the matrix, and so 
the function should only expect one argument, for example:

 > apply(mat, 2, function(e1){print(e1)})
[1]  5 17
[1]  7 28
[1]  6 36
[1]  7 41
[1]  8 46
[1]  10 140

  - in this toy example,my function is called with e1 set to columns of 
the matrix.

  So what were you trying to do with apply(mat, 2, function(e1,e2) e1*e2)?

Baz



From uth at zhwin.ch  Tue Dec 23 10:53:54 2003
From: uth at zhwin.ch (=?iso-8859-1?Q?=22Untern=E4hrer_Thomas=2C_uth=22?=)
Date: Tue, 23 Dec 2003 10:53:54 +0100
Subject: AW: [R] unary operations
Message-ID: <53A181E56FB0694ABFD212F8AEDA7F6F258A5B@langouste.zhwin.ch>



Hi, 

This should do what you are looking for:

x <- matrix(1:10, byrow = TRUE, nrow = 2)
mapply(function(x1, x2) x1+x2, x[1, ], x[2, ])	

But have also a look at colSums and colMeans


HTH

Thomas


-----Urspr?ngliche Nachricht-----
Von: Fredrik Lundgren [mailto:fredrik.lundgren at norrkoping.mail.telia.com] 
Gesendet: Dienstag, 23. Dezember 2003 10:00
An: R-help
Betreff: [R] unary operations


Hello,

I had expected that the following should work

mat <- matrix(c(5, 7, 6, 7, 8, 10, 17, 28, 36, 41, 46, 140), nrow = 2, byrow = T) mat

apply(mat, 2, function(e1,e2) e1*e2)
apply(mat, 2, function(e1,e2) e1+e2)
apply(mat, 2, function(e1,e2) e1-e2)
apply(mat, 2, function(e1,e2) e1/e2)

but I get 

Error in FUN(newX[, i], ...) : Argument "e2" is missing, with no default

What do I do wrong?

Sincerely Fredrik Lundgren

______________________________________________
R-help at stat.math.ethz.ch mailing list https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From v.demart at libero.it  Tue Dec 23 12:39:18 2003
From: v.demart at libero.it (v.demart@libero.it)
Date: Tue, 23 Dec 2003 12:39:18 +0100
Subject: [R] What's "latex" for?
Message-ID: <HQCJ1I$7F433619B585BF8ED23A988BE07170AD@libero.it>

As an R absolute beginner when I update my R 1.8.1, after installing
the packages I see the various updated commands orderly running in a
table on the screen where, among other things, "latex" and "example"
are indicated.

1) Being an old user of latex what **exactly** the "latex" label on a
command mean and how, in what context, can it be used?

2) How can I see the same table running on the screen /or at least the
same info while not updating?

Ciao from Rome
Vittorio



From petr.pikal at precheza.cz  Tue Dec 23 12:43:49 2003
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 23 Dec 2003 12:43:49 +0100
Subject: [R] error propagation - hope it is correct subject
In-Reply-To: <Pine.SOL.4.58.0312220831100.821@mspacman.gpcc.itd.umich.edu>
References: <3FE6E61F.5508.10E8319@localhost>
Message-ID: <3FE83885.16332.11C7624@localhost>

Hallo Thomas

Thank you for your answer, even I am not sure how to do it in R (or maybe at 
all). My mathematics background is only faint so I drop the first possibility which 
is for me rather cryptic. 

Does your second suggestion mean:

1:	compute random variable  y <- f(rnorm(n,mymeanx1,mysdx1), 
rnorm(n,mymeanx2, ...), ...)

according to my function f  (based on assumption x variables values can be 
considered normally distributed and and independent)

2:	sd(y)

can be considered as variation of y?
Or is it necessary to do something like

vysled<-NULL
for (i in 1:300) vysled[i]<-sd(sample(y,100))
mean(vysled)

to get bootstraped estimation of sd(y)

My actual data have some missing values and some outliers which I can either 
remove or to use some robust statistics for mean and variation estimates.

Thank you and have a nice Christmas

Petr

On 22 Dec 2003 at 8:56, Thomas W Blackwell wrote:

> Petr  -
> 
> Very briefly, I think of three ways to approximate the standard
> deviation of  y = f(x1,x2,x3).
> 
>   (1) linearise f() and use the covariance matrix of [x1,x2,x3].
>   (2) simulate draws from the joint distribution of [x1,x2,x3],
>  then compute the sample std dev of resulting f()s.
>   (3) go back to the original data set from which [x1,x2,x3] were
>  estimated as parameters, re-sample rows with replacement,
>  estimate [x1,x2,x3] and compute f, then take sample std dev.
> 
> Other names for these three would be (1) the "delta method" or
> Taylor series expansion, (2) parametric bootstrap, (3) bootstrap.
> 
> Different choices are appropriate in different situations.
> 
> If the std devs of x1,x2,x3 are small relative to the curvature
> (2nd derivative) in f(), then use (1) and compute by matrix algebra
> 
> Var(f(x1,x2,x3))  approx  t(grad f) %*% Cov(x1,x2,x3) %*% grad f.
> 
> If the curvature in f() is an issue, use (2) with draws of x1,x2,x3
> from some parametric distribution (eg, rnorm()) with each component
> properly conditioned on the ones already drawn.
> 
> Only if there were no set of intermediate parameters [x1,x2,x3]
> would I use (3) to get the precision of f directly.  I'm sure
> Brad Efron would say something different.  (3) is the only one
> that is canned in R, simply because the other two are practically
> one-liners.
> 
> -  tom blackwell  -  u michigan medical school  -  ann arbor  -
> 
> On Mon, 22 Dec 2003, Petr Pikal wrote:
> 
> > Dear all
> >
> > Please, can you advice me how to compute an error, standard
> > deviation or another measure of variability of computed value.
> >
> > I would like to do something like:
> >
> > var(y) = some.function(var(x1),var(x2),var(x3))
> >
> > for level F1 (2,3,...)
> >
> > Let say I have some variables - x1, x2, x3 (two computed for levels
> > of factor F and one which is same for all levels) and I want to
> > compute
> >
> > y = f(x1,x2,x3)
> >
> > for some levels of factor F
> >
> > I can compute variation of variables for levels of F, I know a
> > variation of one variable but I am not sure how to transfer it to
> > variation of y within respective levels.
> >
> > I found some methods which I can use but I wonder if  there is some
> > method implemented in R (Manly B.F. Biom.J.28,949,(1986), some local
> > statistical books available to me).
> >
> > I have a feeling I could use bootstrap method for this but I am not
> > sure how.
> >
> > Thank you and merry Christmas to all
> >
> > Petr Pikal
> > petr.pikal at precheza.cz
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

Petr Pikal
petr.pikal at precheza.cz



From Virgilio.Gomez at uv.es  Tue Dec 23 13:20:57 2003
From: Virgilio.Gomez at uv.es (Virgilio =?ISO-8859-1?Q?G=F3mez?= Rubio)
Date: Tue, 23 Dec 2003 13:20:57 +0100
Subject: [R] Re: bootstrap pValue in DClusters
In-Reply-To: <6.0.0.22.0.20031216094858.01b47cc8@post.uv.es>
References: <6.0.0.22.0.20031216094858.01b47cc8@post.uv.es>
Message-ID: <1072182057.633.88.camel@chomsky.estadi.uv.es>

Salut Erik,

Apologyses for not writing to you before.



> 1-how is it possible to get back the bootstrap pValue? I mean the
> pValue of the calculated statistic with respect of the distribution of
> this statistic under the null hypothesis.

Let's suppose that you perform 100 bootstrap replicates. Then you should
sort this 100 values and see how many of them are higher than your
observed value. If we call N to this quantity, then the p-value is
N/100.

> 2-how is it possible to test an overdispersion in the poisson model?
> for choosing a best model I need this mesure of dispersion. Should I
> build a glm(cases~expected,family=quasipoisson)$sig2 or is it possible
> directly in DClusters?

We plan to incorporate Dean's test in the near future:

@article{Dean:1992,
   author    = {C. B. Dean},
   title     = {Testing for Overdispersion in Poisson and Binomial
Regression Models},
   journal   = {Journal of the American Statistical Association},
   pages     = {451-457},
   year      = {1992},
   volume    = {87},
   number    = {418}
}

In fact, we already have the code but not the manual page.

Pearson's chi-square and Potthoff-Wittinghill can be used to test data
homogeneity, to see any departure from the Poisson distribution (which
MAY be due to overdispersion).

For those interested in package DCluster,  I hope to resume its
development soon. Probably next week.

Best regards,

-- 
             Virgilio G?mez Rubio

Grup d'Estad?stica espacial i temporal 
en Epidemiologia i medi ambient 

Dpto. Estad?stica e I. O. - Facultat de Matem?tiques
Avda. Vicent A. Estell?s, 1 - 46100 Burjassot
Valencia - SPAIN

http://matheron.uv.es/~virgil

TLF: 00 34 96 354 43 62 - FAX: 00 34 96 354 47 35



From jfri at novozymes.com  Tue Dec 23 14:02:04 2003
From: jfri at novozymes.com (JFRI (Jesper Frickmann))
Date: Tue, 23 Dec 2003 08:02:04 -0500
Subject: [R] Memory allocation
Message-ID: <D53147E531BFBC4B8853FD134FAEE44D14FD7B@exusfr014.novo.dk>

Go download R version 1.8.1 (or later if available). They fixed
something with memory management from 1.8.0 to 1.8.1 which helped me out
of the exact same problem. I think it has to do with memory
fragmentation; R cannot find any chunk big enough for even a small
vector after some time.

Kind regards, 
Jesper Frickmann 
Statistician, Quality Control 
Novozymes North America Inc. 
Tel. +1 919 494 3266
Fax +1 919 494 3460

-----Original Message-----
From: Richards, Thomas [mailto:RichardsTJ2 at UPMC.EDU] 
Sent: Monday, December 22, 2003 3:11 PM
To: 'r-help at stat.math.ethz.ch'
Subject: [R] Memory allocation


Hello:

	I am trying to work with a couple of microarray data sets, using

platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    1              
minor    8.1            
year     2003           
month    11             
day      21             
language R              


In the shortcut for invoking R I have set --max-mem-size=1024M, so that
I get

> memory.limit()
[1] 1073741824

Below is an example of what keeps happening as I am working. Any
suggestions as to how I can stop running out of mermory?

> memory.size()
[1] 502904736
> log2.harvAD <- log2.harvAD[log2.harvAD$Probesets %in%
harvard.genes$probeset,]
Error: cannot allocate vector of size 49 Kb
> log2.harvAD <- log2.harvAD[,c(1,1+order(names(log2.harvAD)[-1]))]
Error: cannot allocate vector of size 49 Kb
> log2.harvAD.annot <- 
> unlist(lapply(strsplit(names(log2.harvAD),split=":"),
+   function(L) L[1]))[-1]
> log2.harvAD$probeset <- as.character(log2.harvAD$probeset)
Error: cannot allocate vector of size 49 Kb
> memory.size()
[1] 502912536
> gc()
           used  (Mb) gc trigger  (Mb)
Ncells  2586025  69.1    6812252 182.0
Vcells 20108076 153.5   41205530 314.4
> memory.size()
[1] 330645720
> memory.limit()/memory.size()
[1] 3.247408
> ##  Try again:
> log2.harvAD <- log2.harvAD[log2.harvAD$Probesets %in%
harvard.genes$probeset,]
Error: cannot allocate vector of size 49 Kb

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From fredrik.lundgren at norrkoping.mail.telia.com  Tue Dec 23 14:10:06 2003
From: fredrik.lundgren at norrkoping.mail.telia.com (Fredrik Lundgren)
Date: Tue, 23 Dec 2003 14:10:06 +0100
Subject: [R]plot.survfit
Message-ID: <002001c3c956$38601fe0$2d0ffea9@oemcomputer>

Hello,

How do I get plot.survfit to draw a survival curve with prespecified time periods, say 1 month, 2, months, 3 months, 6 months, 1 year, 2 years etc) instead of the individual times of all events?

Sincerely Fredrik Lundgren



From obuhard at yahoo.fr  Tue Dec 23 15:04:28 2003
From: obuhard at yahoo.fr (olivier)
Date: Tue, 23 Dec 2003 15:04:28 +0100
Subject: [R] How can I put error bars on a barplot() ?
Message-ID: <000c01c3c95d$ba8f4980$b765fea9@serveur>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031223/0a900c75/attachment.pl

From MSchwartz at medanalytics.com  Tue Dec 23 15:28:49 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Tue, 23 Dec 2003 08:28:49 -0600
Subject: [R] How can I put error bars on a barplot() ?
In-Reply-To: <000c01c3c95d$ba8f4980$b765fea9@serveur>
References: <000c01c3c95d$ba8f4980$b765fea9@serveur>
Message-ID: <1072189729.3932.17.camel@localhost.localdomain>

On Tue, 2003-12-23 at 08:04, olivier wrote:
> Hi all,
> 
> I am a relatively new R user... trying to put error bars (from SD
> values) on my data represented with barplot(). But I can't find any
> function or instruction to do so.
> Is there an easier way to do this than using segments() as I saw in an
> example in the R reference manual ? Then, can I define there graphical
> apparence ?
> 
> Thanks for help.
> 
> Regards
> 
> Olivier BUHARD


Olivier,

You can use either segments() or arrows() to place error bars on
barplots. 

Also, there is barplot2() in the gregmisc package on CRAN, which
provides this functionality and enables you to pass the error bar values
as arguments to the function.

As you may have noted, if you use either segments() or arrows(), the key
is to get the bar midpoints for proper line placement. The bar midpoints
are returned from the barplot() function and that is described in the
help for barplot().

If you should elect to use barplot2(), let me know if you have any
questions on its use.

Best regards,

Marc Schwartz



From Stallforth at t-online.de  Tue Dec 23 13:44:00 2003
From: Stallforth at t-online.de (Fam. Stallforth)
Date: 23 Dec 2003 12:44 GMT
Subject: [R] (no subject)
Message-ID: <1AYls1-0W62wS0@fwd03.sul.t-online.com>

Hello!

I am trying to import data to R. The programm always responds with:
"incomplete final line found by readTableHeader on Data.doc"

What does that mean, and what could I change?

Thank you very much in advance.
Regards Florian Stallforth



From tblackw at umich.edu  Tue Dec 23 15:37:56 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Tue, 23 Dec 2003 09:37:56 -0500 (EST)
Subject: [R] error propagation - hope it is correct subject
In-Reply-To: <3FE83885.16332.11C7624@localhost>
References: <3FE6E61F.5508.10E8319@localhost>
	<3FE83885.16332.11C7624@localhost>
Message-ID: <Pine.SOL.4.58.0312230931190.16254@tetris.gpcc.itd.umich.edu>

Petr  -

Yes, you are interpreting the second suggestion exactly correctly,
apart from concern for possible correlations among x1,x2,x3.
If one can treat them as independent, I would do exactly as you
show:  generate a vector of, say, n = 10000 simulated draws from
x1, another vector of the same length for x2, and another for x3,
then calculate  sd(f(x1,x2,x3))  as an approximation to the std
dev of f().

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Tue, 23 Dec 2003, Petr Pikal wrote:

> Hallo Thomas
>
> Thank you for your answer, even I am not sure how to do it in R (or maybe at
> all). My mathematics background is only faint so I drop the first possibility which
> is for me rather cryptic.
>
> Does your second suggestion mean:
>
> 1:	compute random variable  y <- f(rnorm(n,mymeanx1,mysdx1),
> rnorm(n,mymeanx2, ...), ...)
>
> according to my function f  (based on assumption x variables values can be
> considered normally distributed and and independent)
>
> 2:	sd(y)
>
> can be considered as variation of y?
> Or is it necessary to do something like
>
> vysled<-NULL
> for (i in 1:300) vysled[i]<-sd(sample(y,100))
> mean(vysled)
>
> to get bootstraped estimation of sd(y)
>
> My actual data have some missing values and some outliers which I can either
> remove or to use some robust statistics for mean and variation estimates.
>
> Thank you and have a nice Christmas
>
> Petr
>
> On 22 Dec 2003 at 8:56, Thomas W Blackwell wrote:
>
> > Petr  -
> >
> > Very briefly, I think of three ways to approximate the standard
> > deviation of  y = f(x1,x2,x3).
> >
> >   (1) linearise f() and use the covariance matrix of [x1,x2,x3].
> >   (2) simulate draws from the joint distribution of [x1,x2,x3],
> >  then compute the sample std dev of resulting f()s.
> >   (3) go back to the original data set from which [x1,x2,x3] were
> >  estimated as parameters, re-sample rows with replacement,
> >  estimate [x1,x2,x3] and compute f, then take sample std dev.
> >
> > Other names for these three would be (1) the "delta method" or
> > Taylor series expansion, (2) parametric bootstrap, (3) bootstrap.
> >
> > Different choices are appropriate in different situations.
> >
> > If the std devs of x1,x2,x3 are small relative to the curvature
> > (2nd derivative) in f(), then use (1) and compute by matrix algebra
> >
> > Var(f(x1,x2,x3))  approx  t(grad f) %*% Cov(x1,x2,x3) %*% grad f.
> >
> > If the curvature in f() is an issue, use (2) with draws of x1,x2,x3
> > from some parametric distribution (eg, rnorm()) with each component
> > properly conditioned on the ones already drawn.
> >
> > Only if there were no set of intermediate parameters [x1,x2,x3]
> > would I use (3) to get the precision of f directly.  I'm sure
> > Brad Efron would say something different.  (3) is the only one
> > that is canned in R, simply because the other two are practically
> > one-liners.
> >
> > -  tom blackwell  -  u michigan medical school  -  ann arbor  -
> >
> > On Mon, 22 Dec 2003, Petr Pikal wrote:
> >
> > > Dear all
> > >
> > > Please, can you advice me how to compute an error, standard
> > > deviation or another measure of variability of computed value.
> > >
> > > I would like to do something like:
> > >
> > > var(y) = some.function(var(x1),var(x2),var(x3))
> > >
> > > for level F1 (2,3,...)
> > >
> > > Let say I have some variables - x1, x2, x3 (two computed for levels
> > > of factor F and one which is same for all levels) and I want to
> > > compute
> > >
> > > y = f(x1,x2,x3)
> > >
> > > for some levels of factor F
> > >
> > > I can compute variation of variables for levels of F, I know a
> > > variation of one variable but I am not sure how to transfer it to
> > > variation of y within respective levels.
> > >
> > > I found some methods which I can use but I wonder if  there is some
> > > method implemented in R (Manly B.F. Biom.J.28,949,(1986), some local
> > > statistical books available to me).
> > >
> > > I have a feeling I could use bootstrap method for this but I am not
> > > sure how.
> > >
> > > Thank you and merry Christmas to all
> > >
> > > Petr Pikal
> > > petr.pikal at precheza.cz
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
> Petr Pikal
> petr.pikal at precheza.cz
>
>
>



From tblackw at umich.edu  Tue Dec 23 15:46:08 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Tue, 23 Dec 2003 09:46:08 -0500 (EST)
Subject: [R] (no subject)
In-Reply-To: <1AYls1-0W62wS0@fwd03.sul.t-online.com>
References: <1AYls1-0W62wS0@fwd03.sul.t-online.com>
Message-ID: <Pine.SOL.4.58.0312230941100.16254@tetris.gpcc.itd.umich.edu>

Florian  -

One thing to *try* would be:  work from the R
command line and set the parameter "nlines" in
read.table()  to one less than the number of
lines of data in the file.  If this works, then
you can at least read in all but the last line.

I suspect something like a missing newline
character at the end of the file, or a strange
file format, but that is just a guess.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Tue, 23 Dec 2003, Fam. Stallforth wrote:

> Hello!
>
> I am trying to import data to R. The programm always responds with:
> "incomplete final line found by readTableHeader on Data.doc"
>
> What does that mean, and what could I change?
>
> Thank you very much in advance.
> Regards Florian Stallforth
>



From ripley at stats.ox.ac.uk  Tue Dec 23 16:14:03 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 23 Dec 2003 15:14:03 +0000 (GMT)
Subject: Warning when reading data files (was Re: [R] (no subject))
In-Reply-To: <1AYls1-0W62wS0@fwd03.sul.t-online.com>
Message-ID: <Pine.LNX.4.44.0312231510470.14802-100000@gannet.stats>

On 23 Dec 2003, Fam. Stallforth wrote:

> Hello!
> 
> I am trying to import data to R. The programm always responds with:
> "incomplete final line found by readTableHeader on Data.doc"
> 
> What does that mean, and what could I change?

It means the final line is incomplete, and you should complete it!
Does it actually have a EOL mark, that is CR or LF or CRLF depending on 
your operating system?  The message come up when the EOF is found before 
EOL.

I believe this is just a warning and the process continues correctly, but 
only believe ....

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tlumley at u.washington.edu  Tue Dec 23 16:28:02 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 23 Dec 2003 07:28:02 -0800 (PST)
Subject: [R] What's "latex" for?
In-Reply-To: <HQCJ1I$7F433619B585BF8ED23A988BE07170AD@libero.it>
References: <HQCJ1I$7F433619B585BF8ED23A988BE07170AD@libero.it>
Message-ID: <Pine.A41.4.58.0312230721300.42640@homer30.u.washington.edu>

On Tue, 23 Dec 2003, v.demart at libero.it wrote:

> As an R absolute beginner when I update my R 1.8.1, after installing
> the packages I see the various updated commands orderly running in a
> table on the screen where, among other things, "latex" and "example"
> are indicated.
>
> 1) Being an old user of latex what **exactly** the "latex" label on a
> command mean and how, in what context, can it be used?

It means that a LaTeX version of the help page has been produced (it is
stored in the package directory, in the latex/ subdirectory).  It is used
to produce the PDF manuals, and by help(offline=TRUE).  The necessary
style file is in RHOME/share/texmf/

	-thomas



From dieter.menne at menne-biomed.de  Tue Dec 23 16:28:06 2003
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Tue, 23 Dec 2003 16:28:06 +0100
Subject: [R] R[Mailman] question: contour plot for discrete data
Message-ID: <JLEPLGAANFCEAEDCAGJNGEBGCHAA.dieter.menne@menne-biomed.de>

You wrote:
---
I have matrix (n x3) that represents discrete data. 
Each row of matrix is 3-D point (x,y,z). I would like 
to get contour map (z value) at two dimension 
(x,y). How can I use related contour function to do 
this job?
---

Paul Murrell (paul at stat.auckland.ac.nz) sent me his little
clines package some time ago. I will mail you (LZ) a copy

(Paul, I assume this is ok?).

Dieter



From tlumley at u.washington.edu  Tue Dec 23 16:36:17 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 23 Dec 2003 07:36:17 -0800 (PST)
Subject: [R]plot.survfit
In-Reply-To: <002001c3c956$38601fe0$2d0ffea9@oemcomputer>
References: <002001c3c956$38601fe0$2d0ffea9@oemcomputer>
Message-ID: <Pine.A41.4.58.0312230734580.42640@homer30.u.washington.edu>

On Tue, 23 Dec 2003, Fredrik Lundgren wrote:

> Hello,
>
> How do I get plot.survfit to draw a survival curve with prespecified
> time periods, say 1 month, 2, months, 3 months, 6 months, 1 year, 2
> years etc) instead of the individual times of all events?
>

Why would you want to do that?

If you have only one survival curve you can create a function that
describes it

survfun<-approxfun(fit$time,fit$surv,yleft=1,f=0,method="constant")

and then evaluate and plot this function whereever you want.

	-thomas



From claudio at unive.it  Tue Dec 23 16:28:59 2003
From: claudio at unive.it (Claudio Agostinelli)
Date: Tue, 23 Dec 2003 16:28:59 +0100 (CET)
Subject: [R] [R-pkgs] circular package
Message-ID: <Pine.LNX.4.33L2.0312231605500.23875-100000@linaria.dst.unive.it>

Dear All,
Ulric Lund and me are developing a new package called 'circular' that as
soon as possible will substitute 'CircStats' package.
A pre-released is available at:
http://www.dst.unive.it/~claudio/R/circular_0.1.tar.gz
I kindly ask everybody is using CircStats to check this new package
and send me comments, suggestions and bugs to claudio at unive.it and not to
the list.
As soon as the package will work fine we will ask the R core to make it
available on CRAN.

Bests,
Claudio Agostinelli

--------------------------------------------------------------
Claudio Agostinelli
Dipartimento di Statistica
Universita' Ca' Foscari di Venezia
Campiello San Agostin, 2347 San Polo
30125 Venezia
Tel: 041 2347432, Fax: 041 710355
email: claudio at unive.it, www: www.dst.unive.it/~claudio
--------------------------------------------------------------
Per favore non mandatemi allegati in Word o PowerPoint.
Si veda http://www.fsf.org/philosophy/no-word-attachments.html

Please avoid sending me Word or PowerPoint attachments.
See http://www.fsf.org/philosophy/no-word-attachments.html

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://www.stat.math.ethz.ch/mailman/listinfo/r-packages



From ririzarr at jhsph.edu  Tue Dec 23 16:44:38 2003
From: ririzarr at jhsph.edu (Rafael A. Irizarry)
Date: Tue, 23 Dec 2003 10:44:38 -0500 (EST)
Subject: [R] Sweave question
Message-ID: <Pine.LNX.4.44.0312231038420.25968-100000@blinux08.biostat.jhsph.edu>

Using Sweave in the tools library (R version 1.8.0: sorry i havent 
upgraded), it seems i cant use if statements in R chunks that make graphs. 
i have this:

<<fig=TRUE,echo=F>>=
par(mfrow=c(1,1))
if(exists("x")) 
	plot(x,x)
else{
  plot(1,1,type="n")
  text(1,1,"data not available.\n")
}
@

and I get this error:

Error:  chunk 6
Error in parse(file, n, text, prompt) : parse error

any help is appreciated.

thanks and apologies if this not a problem in R 1.8.1
rafael



From renaud.lancelot at pasteur.mg  Tue Dec 23 17:18:51 2003
From: renaud.lancelot at pasteur.mg (Renaud Lancelot)
Date: Tue, 23 Dec 2003 19:18:51 +0300
Subject: [R] R[Mailman] question: contour plot for discrete data
In-Reply-To: <JLEPLGAANFCEAEDCAGJNGEBGCHAA.dieter.menne@menne-biomed.de>
References: <JLEPLGAANFCEAEDCAGJNGEBGCHAA.dieter.menne@menne-biomed.de>
Message-ID: <3FE86AEB.8080600@pasteur.mg>

Dieter Menne a ?crit :

> You wrote:
> ---
> I have matrix (n x3) that represents discrete data. 
> Each row of matrix is 3-D point (x,y,z). I would like 
> to get contour map (z value) at two dimension 
> (x,y). How can I use related contour function to do 
> this job?
> ---
> 
> Paul Murrell (paul at stat.auckland.ac.nz) sent me his little
> clines package some time ago. I will mail you (LZ) a copy
> 
> (Paul, I assume this is ok?).
> 
> Dieter
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

The package clines is now on CRAN.

Best,

Renaud

-- 
Dr Renaud Lancelot
v?t?rinaire ?pid?miologiste
Ambassade de France - SCAC
BP 834 Antannarivo 101
Madagascar

t?l. +261 (0)32 04 824 55 (cell)
      +261 (0)20 22 494 37 (home)



From bates at stat.wisc.edu  Tue Dec 23 17:28:52 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: 23 Dec 2003 10:28:52 -0600
Subject: [R] Sweave question
In-Reply-To: <Pine.LNX.4.44.0312231038420.25968-100000@blinux08.biostat.jhsph.edu>
References: <Pine.LNX.4.44.0312231038420.25968-100000@blinux08.biostat.jhsph.edu>
Message-ID: <6rfzfbqz0b.fsf@bates4.stat.wisc.edu>

Write the if-else statement as

if(exists("x")) {
	plot(x,x)
} else{
  plot(1,1,type="n")
  text(1,1,"data not available.\n")
}

so the first two lines are not syntactically complete.

"Rafael A. Irizarry" <ririzarr at jhsph.edu> writes:

> Using Sweave in the tools library (R version 1.8.0: sorry i havent 
> upgraded), it seems i cant use if statements in R chunks that make graphs. 
> i have this:
> 
> <<fig=TRUE,echo=F>>=
> par(mfrow=c(1,1))
> if(exists("x")) 
> 	plot(x,x)
> else{
>   plot(1,1,type="n")
>   text(1,1,"data not available.\n")
> }
> @
> 
> and I get this error:
> 
> Error:  chunk 6
> Error in parse(file, n, text, prompt) : parse error
> 
> any help is appreciated.
> 
> thanks and apologies if this not a problem in R 1.8.1
> rafael
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-- 
Douglas Bates                            bates at stat.wisc.edu
Statistics Department                    608/262-2598
University of Wisconsin - Madison        http://www.stat.wisc.edu/~bates/



From ripley at stats.ox.ac.uk  Tue Dec 23 17:32:51 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 23 Dec 2003 16:32:51 +0000 (GMT)
Subject: [R] Sweave question
In-Reply-To: <Pine.LNX.4.44.0312231038420.25968-100000@blinux08.biostat.jhsph.edu>
References: <Pine.LNX.4.44.0312231038420.25968-100000@blinux08.biostat.jhsph.edu>
Message-ID: <Pine.LNX.4.58.0312231631120.10558@localhost.localdomain>

This is the standard problem with if ... else: that will not work at the
command line either.  Use braces like

if(exists("x")) {
  plot(x,x)
} else {
   plot(1,1,type="n")
   text(1,1,"data not available.\n")
}

which should do the trick.

On Tue, 23 Dec 2003, Rafael A. Irizarry wrote:

> Using Sweave in the tools library (R version 1.8.0: sorry i havent
> upgraded), it seems i cant use if statements in R chunks that make graphs.
> i have this:
>
> <<fig=TRUE,echo=F>>=
> par(mfrow=c(1,1))
> if(exists("x"))
> 	plot(x,x)
> else{
>   plot(1,1,type="n")
>   text(1,1,"data not available.\n")
> }
> @
>
> and I get this error:
>
> Error:  chunk 6
> Error in parse(file, n, text, prompt) : parse error
>
> any help is appreciated.
>
> thanks and apologies if this not a problem in R 1.8.1
> rafael
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From deepayan at stat.wisc.edu  Tue Dec 23 17:43:54 2003
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Tue, 23 Dec 2003 10:43:54 -0600
Subject: [R] Sweave question
In-Reply-To: <Pine.LNX.4.44.0312231038420.25968-100000@blinux08.biostat.jhsph.edu>
References: <Pine.LNX.4.44.0312231038420.25968-100000@blinux08.biostat.jhsph.edu>
Message-ID: <200312231043.54201.deepayan@stat.wisc.edu>

On Tuesday 23 December 2003 09:44, Rafael A. Irizarry wrote:
> Using Sweave in the tools library (R version 1.8.0: sorry i havent
> upgraded), it seems i cant use if statements in R chunks that make graphs.
> i have this:
>
> <<fig=TRUE,echo=F>>=
> par(mfrow=c(1,1))
> if(exists("x"))
> 	plot(x,x)

This is a problem even outside of Sweave. The last line above syntactically 
completes the if statement. You need to replace it by something like 

if(exists("x")) {
 	plot(x,x)
  } else {
...

> else{
>   plot(1,1,type="n")
>   text(1,1,"data not available.\n")
> }
> @
>
> and I get this error:
>
> Error:  chunk 6
> Error in parse(file, n, text, prompt) : parse error
>
> any help is appreciated.
>
> thanks and apologies if this not a problem in R 1.8.1
> rafael
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From tlumley at u.washington.edu  Tue Dec 23 17:40:34 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 23 Dec 2003 08:40:34 -0800 (PST)
Subject: [R] Sweave question
In-Reply-To: <Pine.LNX.4.44.0312231038420.25968-100000@blinux08.biostat.jhsph.edu>
References: <Pine.LNX.4.44.0312231038420.25968-100000@blinux08.biostat.jhsph.edu>
Message-ID: <Pine.A41.4.58.0312230838350.100610@homer41.u.washington.edu>

On Tue, 23 Dec 2003, Rafael A. Irizarry wrote:

> Using Sweave in the tools library (R version 1.8.0: sorry i havent
> upgraded), it seems i cant use if statements in R chunks that make graphs.
> i have this:
>
> <<fig=TRUE,echo=F>>=
> par(mfrow=c(1,1))
> if(exists("x"))
> 	plot(x,x)
> else{
>   plot(1,1,type="n")
>   text(1,1,"data not available.\n")
> }
> @
>

I think your problem is with the `else'.  The `if' part of the statement
is syntactically complete, so the parser isn't expecting an `else'.  You
need something like

  if (exists("x")){
	plot(x,x)
  } else {
	plot(1,1,type="n")
  }



	-thomas



From sfalcon at fhcrc.org  Tue Dec 23 17:43:34 2003
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Tue, 23 Dec 2003 08:43:34 -0800
Subject: [R] Sweave question
In-Reply-To: <Pine.LNX.4.44.0312231038420.25968-100000@blinux08.biostat.jhsph.edu>
References: <Pine.LNX.4.44.0312231038420.25968-100000@blinux08.biostat.jhsph.edu>
Message-ID: <20031223164334.GA28771@queenbee.fhcrc.org>

I'm not certain, but since you are getting a parse error, I would try
curly brackets on your if statement:

    <<fig=TRUE,echo=F>>=
    par(mfrow=c(1,1))
    if(exists("x")) {
            plot(x,x)
    } else {
    plot(1,1,type="n")
    text(1,1,"data not available.\n")
    }
    @

HTH,

+ seth



From maechler at stat.math.ethz.ch  Tue Dec 23 18:22:25 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 23 Dec 2003 18:22:25 +0100
Subject: [R] Re: if .. else parse error {was "Sweave question"}
In-Reply-To: <Pine.LNX.4.44.0312231038420.25968-100000@blinux08.biostat.jhsph.edu>
References: <Pine.LNX.4.44.0312231038420.25968-100000@blinux08.biostat.jhsph.edu>
Message-ID: <16360.31185.942105.486759@gargle.gargle.HOWL>

>>>>> "Rafael" == Rafael A Irizarry <ririzarr at jhsph.edu>
>>>>>     on Tue, 23 Dec 2003 10:44:38 -0500 (EST) writes:

    Rafael> Using Sweave in the tools library (R version 1.8.0:
    Rafael> sorry i havent upgraded), it seems i cant use if
    Rafael> statements in R chunks that make graphs.  i have
    Rafael> this:

    Rafael> <<fig=TRUE,echo=F>>=
    Rafael> par(mfrow=c(1,1))
    Rafael> if(exists("x")) 
    Rafael>    plot(x,x)
    Rafael> else{
    Rafael>    plot(1,1,type="n")
    Rafael>    text(1,1,"data not available.\n")
    Rafael> }
    Rafael> @

    Rafael> and I get this error:

    Rafael> Error:  chunk 6
    Rafael> Error in parse(file, n, text, prompt) : parse error


This has nothing to do with Sweave;
it's a famous "beginner"s (;-) mistake:

?if   has the explanation :

 if> Note that it is a common mistake to forget putting braces ('{ ..}') 
 if> around your statements, e.g., after 'if(..)' or 'for(....)'.
 if> In particular, you should not have a newline between '}' and 
 if> 'else' to avoid a syntax error in entering a 'if ... else'
 if> construct at the keyboard or via 'source'. For that reason, one
 if> (somewhat extreme) attitude of defensive programming uses braces
 if> always, e.g., for 'if' clauses.

Regards,
and Merry Christmas to all R-help readers!
Martin



From tplate at acm.org  Tue Dec 23 20:11:28 2003
From: tplate at acm.org (Tony Plate)
Date: Tue, 23 Dec 2003 12:11:28 -0700
Subject: [R] draft of posting guide
In-Reply-To: <1072170818.3fe807427b48b@webmail.unibas.ch>
References: <74E242B6968AA0469B632C5A3EFC1EFD03D57022@nt207mesep.corporate.hdwa.health.wa.gov.au>
	<74E242B6968AA0469B632C5A3EFC1EFD03D57022@nt207mesep.corporate.hdwa.health.wa.gov.au>
Message-ID: <5.2.1.1.2.20031223105619.043ac748@mailhost.blackmesacapital.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031223/02713a52/attachment.pl

From p.murrell at auckland.ac.nz  Tue Dec 23 21:13:16 2003
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Wed, 24 Dec 2003 09:13:16 +1300
Subject: [R] R[Mailman] question: contour plot for discrete data
References: <JLEPLGAANFCEAEDCAGJNGEBGCHAA.dieter.menne@menne-biomed.de>
	<3FE86AEB.8080600@pasteur.mg>
Message-ID: <3FE8A1DC.3010009@stat.auckland.ac.nz>

Hi


Renaud Lancelot wrote:
> Dieter Menne a ?crit :
> 
>> You wrote:
>> ---
>> I have matrix (n x3) that represents discrete data. Each row of matrix 
>> is 3-D point (x,y,z). I would like to get contour map (z value) at two 
>> dimension (x,y). How can I use related contour function to do this job?
>> ---
>>
>> Paul Murrell (paul at stat.auckland.ac.nz) sent me his little
>> clines package some time ago. I will mail you (LZ) a copy
>>
>> (Paul, I assume this is ok?).
>>
>> Dieter
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>
>>
> 
> The package clines is now on CRAN.


Also, be aware that in the next R release there will be a contourLines() 
function as part of R which will make the clines package redundant.

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From p.connolly at hortresearch.co.nz  Tue Dec 23 21:53:15 2003
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Wed, 24 Dec 2003 09:53:15 +1300
Subject: [R] draft of posting guide. Sorry.
In-Reply-To: <Pine.OSF.4.31.0312230523330.10948-100000@harry.molgen.mpg.de>;
	from wolski@molgen.mpg.de on Tue, Dec 23, 2003 at 05:31:32AM +0100
References: <Pine.OSF.4.31.0312230523330.10948-100000@harry.molgen.mpg.de>
Message-ID: <20031224095315.G935@hortresearch.co.nz>

On Tue, 23-Dec-2003 at 05:31AM +0100, Eryk Wolski wrote:

[....]

|> I cooled down now and therefore give me a chance to explain why
|> that user guide scares me.

A few comments:



|> As I said, the guide had given me the feeling that someone wants to
|> censor me. 

You mean you reacted in a way that gave you that feeling.  Let's get
cause and effect straight.

|> Especially the first section of the Posting Guide: "How to ask good
|> questions that prompt useful answers" does this. The guide starts
|> with talking mainly about what you should not, or what you must not
|> do.

If I want something to work, I take notice of what the suppliers
suggest is a good way to get it to work.  I never take such suggestions
as being prescriptive.  Once I know more about it, I feel free to
disregard any of them.  Posters can ignore anything in the guide if
they so wish.  Robust debate gets the brain working, but some feathers
might get ruffled in the process.


|> At last I like to mention one important source of help which are missing
|> in the posting guide, and which I forgot these days by myself: R CMD -help
|> and R --help are also very important help sources! If I had remembered it

A good suggestion.


-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From vanecekpavel2 at seznam.cz  Tue Dec 23 23:22:17 2003
From: vanecekpavel2 at seznam.cz (=?us-ascii?Q?PaTa=20PaTaS?=)
Date: Tue, 23 Dec 2003 23:22:17 +0100 (CET)
Subject: [R] question: DLL or EXE from R procedures
Message-ID: <56018.183216-3774-519180520-1072218137@seznam.cz>

Hi,
I wonder if it is possible to create an DLL or EXE file performing R procedures. Instead of running R, reading data and calling some procedures, I would like to use R functions in the following way: "C:\linearRegression.exe data.txt" which would produce let's say file "output.txt" with the results. Is there some way how to do it?
Thanks a lot. Pavel Vanecek
____________________________________________________________
Eurotel Data Nonstop  - neomezen? p??stup na internet za 649,- (s DPH 681,45)  K? m?s??n?! http://ad2.seznam.cz/redir.cgi?instance=67071%26url=http://www.eurotel.cz/site/cz/servicesAndTariffs/specialOffer.html?list=34995



From pburns at pburns.seanet.com  Wed Dec 24 00:21:38 2003
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Tue, 23 Dec 2003 23:21:38 +0000
Subject: [R] draft of posting guide
References: <74E242B6968AA0469B632C5A3EFC1EFD03D57022@nt207mesep.corporate.hdwa.health.wa.gov.au>	<74E242B6968AA0469B632C5A3EFC1EFD03D57022@nt207mesep.corporate.hdwa.health.wa.gov.au>
	<5.2.1.1.2.20031223105619.043ac748@mailhost.blackmesacapital.com>
Message-ID: <3FE8CE02.8000501@pburns.seanet.com>

I think the idea of answering simple questions if it hasn't
been answered after 4 * runif(1) hours is a brilliant idea
(well done Tony -- I'm jealous).  However, a slight tweak
would be even better.

It should be

number of years you've used S times runif(1) hours.  

This encourages more people to start answering questions.
While there has been some disagreement about other issues,
there seems to be consensus that building a large, strong
community of R users is a good thing.  Probably the easiest
way for people to contribute -- and hence feel a part of the
community -- is to respond to R-help questions.


(By the way, I'm not at all concerned that the "checklist" is
called "common mistakes".)


Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Tony Plate wrote:

>I do share Eryk Wolski's and Pascal Nicklaus' concerns that my revision of 
>the posting guide is somewhat unfriendly and negative.  My problem here was 
>to keep it to a reasonable length, which meant eliminating sentences whose 
>function was mainly to be positive and friendly.  Pascal put it nicely:
>
>  
>
>>- People tending not to dare to ask questions because they are intimidated by
>>some aspects of the list (and after the r-beginner discussion we now know 
>>that
>>some feel like that) would be helped by a more positive wording of the same
>>issues in posting guide. The motto should be "help to write better questions"
>>rather than "stop asking poor questions". The content is all there in the
>>draft, it is more about changing individual words. Re-posting it monthly on
>>the list is a good idea.
>>    
>>
>
>I shall reread and see if any of it can be written in a more positive 
>manner without increasing the length.  I am however reminded of Aesp's 
>fable "You can't please everyone" 
>(http://home1.gte.net/deleyd/prose/aesop63.htm).
>
>The guide does contain a lot of statements that sound like "rules".  As 
>others noted, it is just a guide.  However, it is my observation that 
>people are occasionally admonished on R-help for violations of these 
>"rules".  I think this is what is intimidating to some.  Part of my 
>intention with writing the guide was to try to make explicit and put down 
>in one accessible place what these "rules" are. This, I hope, will make it 
>easier for beginners and those reluctant to post to know what they should 
>actually do, so as to better avoid the acute embarrassment that can come 
>from public admonishments.  I also tried to merely reflect the tone of the 
>list rather than trying to set the tone.  I suspect that a concise and 
>informative guide would be less of an intimidation to posting than seeing 
>public admonishments of others and being in the dark about what is actually 
>expected of posters (and would be more likely to be read than a longer, 
>more chatty and friendly guide.)
>
>I also agree that posting questions to R-help should not be the absolute 
>last resort.  That's why I split the suggestions on research into two 
>sections: "Do your homework before posting" and "Further resources".  It 
>has been my observation that people are sometimes called to task if they 
>ask questions without obviously having done the things in the "homework" 
>section, but things in the "Further resources" sections are often mentioned 
>in responses as friendly suggestions without any implication that the 
>poster was negligent for not trying them before posting.
>
>I do like the idea of a brief introduction to the guide, to say something 
>like "This guide is intended to help you get the most out of the R mailing 
>lists, and to avoid embarrassment.  Like many responses posted on the list, 
>it is written in a concise manner.  This is not intended to be unfriendly - 
>it is more a consequence of allocating limited time and space to technical 
>issues rather than to social niceties."
>
>Both Tom Mulholland and Patrick Burns suggested a checklist section, 
>containing things to check before posting.  While I also like this idea, 
>most of the content is already there under "homework" and "common 
>mistakes". I'm not sure that changing the format will enhance the document 
>that much, but I'm perfectly willing to hear opinions.
>
>Please let me know if the following is incorrect: "For questions about 
>functions in packages distributed with R (see the FAQ 
><http://cran.r-project.org/doc/FAQ/R-FAQ.html#Add-on%20packages%20in%20R>Add-on 
>packages in R), ask questions on R-help. If the question relates to a 
>package that is downloaded from CRAN try contacting the package maintainers 
>first."
>
>Comments welcome, however, at this point, perhaps it would be better to 
>send comments to me privately, as most people have probably had enough of 
>this discussion.
>
>cheers,
>
>Tony Plate
>
>PS.  There is a slightly corrected and revised version at 
>http://pws.prserv.net/tap/posting-guide-draft3.html.  I think it's beyond 
>my skills to make it more "friendly" without making it longer.  If anyone 
>else wants to take a go at it, feel free!  In the absence of such attempts, 
>I'm pretty much done with it.
>
>Tony Plate   tplate at acm.org
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>
>  
>



From jc at or.psychology.dal.ca  Wed Dec 24 01:40:48 2003
From: jc at or.psychology.dal.ca (John Christie)
Date: Tue, 23 Dec 2003 20:40:48 -0400
Subject: [R] test h-f sphericity test?
Message-ID: <D11C9906-35A9-11D8-985C-000A956DE534@or.psychology.dal.ca>

	I posted some R code that could be an h-f sphericity test.  Is there 
anyone out there with SPSS, Systat, or some other package that has a 
built in test who can verify whether it is accurate or not?

John

(here is the code again)
# This returns the Huynh-Feldt or "Box Correction" for degrees of 
freedom
hf <- function(m){
	# m is a matrix with subjects as rows and conditions as columns
	# note that checking for worst case scenarios F correction first might
	# be a good idea using J/(J-1) as the df correction factor
	n<- length(m[,1])
	J<-length(m[1,])
	X<-cov(m)*(n-1)
	r<- length(X[,1])
	D<-0
	for (i in 1: r) D<- D+ X[i,i]
	D<-D/r
	SPm<- mean(X)
	SPm2<- sum(X^2)
	SSrm<-0
	for (i in 1: r) SSrm<- SSrm + mean(X[i,])^2
	epsilon<- (J^2*(D-SPm)^2) / ((J-1) * (SPm2 - 2*J*SSrm + J^2*SPm^2))
	epsilon
}



From Tom.Mulholland at health.wa.gov.au  Wed Dec 24 02:48:09 2003
From: Tom.Mulholland at health.wa.gov.au (Mulholland, Tom)
Date: Wed, 24 Dec 2003 09:48:09 +0800
Subject: [R] Is there an R or S implementation of PAMSIL or PAMMEDSIL
Message-ID: <74E242B6968AA0469B632C5A3EFC1EFD03D55D4F@nt207mesep.corporate.hdwa.health.wa.gov.au>

I have some data that is dwarfed by one large cluster. I came across a
paper titled "A New Partitioning Around Medoids Algorithm" (van der
Laan, Pollard & Bryan, 2002) http://www.bepress.com/ucbbiostat/paper105/
that describes PAMSIL and PAMMEDSIL that look as though they might be
more appropriate for the data I have. 

There does not appear to be much out there which is describing itself by
these names. So any help would be appreciated.

Tom

_________________________________________________
 
Tom Mulholland
Senior Policy Officer
WA Country Health Service
Tel: (08) 9222 4062
 
The contents of this e-mail transmission are confidential an...{{dropped}}



From jadhavpr at vcu.edu  Wed Dec 24 08:27:24 2003
From: jadhavpr at vcu.edu (Pravin)
Date: Wed, 24 Dec 2003 02:27:24 -0500
Subject: [R] coding logic and syntax in R
Message-ID: <000001c3c9ef$60a81220$0500a8c0@Pravin>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031224/b2bb2e2b/attachment.pl

From ripley at stats.ox.ac.uk  Wed Dec 24 09:09:18 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 24 Dec 2003 08:09:18 +0000 (GMT)
Subject: [R] coding logic and syntax in R
In-Reply-To: <000001c3c9ef$60a81220$0500a8c0@Pravin>
Message-ID: <Pine.LNX.4.44.0312240753340.18302-100000@gannet.stats>

On Wed, 24 Dec 2003, Pravin wrote:

> I am a beginner in R programming and recently heard about this mailing list.
> Currently, I am trapped into a simple problem for which I just can't find a
> solution. I have a huge dataset (~81,000 observations) that has been

BTW, that is quite a small dataset these days: not even 10 million is `huge'.

> analyzed and the final result is in the form of 0 and 1(one column). 
> 
> I need to write a code to process this column in a little complicated way. 
> These 81,000 observations are actually 9,000 sets (81,000/9). 
> So, in each set whenever zero appears, rest all observations become zero.
> 
> For example;
> 
> If the column has: 
> 
> 111110111111011111111111111111111....
> 
> The output should look like: 
> 
> 111110000111000000111111111111111...

Let me see if I understand you.  This was really

111110111
111011111
111111111
111111...

and you want

111110000
111000000
111111111
111111...

So let's treat it as a matrix (extending to 4 complete sets):

x <- as.numeric(strsplit("111110111111011111111111111111111011", NULL)[[1]])
xx <- matrix(x, ncol=9, byrow=TRUE)

Then a simple loop

for(i in 2:9) xx[,i] <- xx[,i] & xx[,i-1]

give me the second matrix, which I can read out as a vector as

as.vector(t(xx))
[1] 1 1 1 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0

or in what I understand as your format

paste(t(xx), collapse="")
[1] "111110000111000000111111111111111000"

Doing this with 81000 random 0/1's took a fraction of a second.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From lecoutre at stat.ucl.ac.be  Wed Dec 24 09:09:58 2003
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Wed, 24 Dec 2003 09:09:58 +0100
Subject: [R] coding logic and syntax in R
In-Reply-To: <000001c3c9ef$60a81220$0500a8c0@Pravin>
References: <000001c3c9ef$60a81220$0500a8c0@Pravin>
Message-ID: <6.0.1.1.2.20031224090342.021aeec0@stat4ux.stat.ucl.ac.be>


In R, always begin to try to obtain result on a little unit.
Begin to make a function that will make replacements for ONE vector (of size 9)

FillWith=function(vec,SearchForOne=0,ReplaceNextValues=0)
{
  pp=which(vec==SearchForOne)
if (length(pp)>0) vec[pp:length(vec)]=ReplaceNextValues
return(vec)
}

Verify it works:

 > FillWith(c(1,1,0,1,1))
[1] 1 1 0 0 0


Then try to apply it with your data, using one of the ?apply functions.
Here, tapply seems to be adequate.

 > data=c(rep(1,9),rep(1,4),0,rep(1,4))
 > data
  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1
 > data=cbind(data,groups=((1:length(data)-1)%/%9))
 > data
       data groups
  [1,]    1      0
  [2,]    1      0
  [3,]    1      0
  [4,]    1      0
  [5,]    1      0
  [6,]    1      0
  [7,]    1      0
  [8,]    1      0
  [9,]    1      0
[10,]    1      1
[11,]    1      1
[12,]    1      1
[13,]    1      1
[14,]    0      1
[15,]    1      1
[16,]    1      1
[17,]    1      1
[18,]    1      1

 > tapply(data[,1],data[,2],FUN=FillWith)
$"0"
[1] 1 1 1 1 1 1 1 1 1

$"1"
[1] 1 1 1 1 0 0 0 0 0

And then come back to a vector with unlist().

Eric




At 08:27 24/12/2003, Pravin wrote:
>Hello,
>
>
>I am a beginner in R programming and recently heard about this mailing list.
>Currently, I am trapped into a simple problem for which I just can't find a
>solution. I have a huge dataset (~81,000 observations) that has been
>analyzed and the final result is in the form of 0 and 1(one column).
>
>I need to write a code to process this column in a little complicated way.
>
>These 81,000 observations are actually 9,000 sets (81,000/9).
>
>So, in each set whenever zero appears, rest all observations become zero.
>
>For example;
>If the column has:
>111110111111011111111111111111111....
>The output should look like:
>111110000111000000111111111111111...
>I hope this makes sense.
>Thank you in anticipation,
>
>Pravin
>
>Pravin Jadhav



--------------------------------------------------
L'erreur est certes humaine, mais un vrai d?sastre
n?cessite un ou deux ordinateurs. Citation anonyme
--------------------------------------------------
Eric Lecoutre
Informaticien/Statisticien
Institut de Statistique / UCL

TEL (+32)(0)10473050       lecoutre at stat.ucl.ac.be
URL http://www.stat.ucl.ac.be/ISpersonnel/lecoutre



From uth at zhwin.ch  Wed Dec 24 09:34:41 2003
From: uth at zhwin.ch (=?iso-8859-1?Q?=22Untern=E4hrer_Thomas=2C_uth=22?=)
Date: Wed, 24 Dec 2003 09:34:41 +0100
Subject: AW: [R] coding logic and syntax in R
Message-ID: <53A181E56FB0694ABFD212F8AEDA7F6F258A5F@langouste.zhwin.ch>


Hi,


In <- as.numeric(strsplit("11111011111101111111111111111111", "")[[1]])

Sets <- rep(letters[1:4], each = 8) ## your sets
Sp.In <- split(In, Sets)
logical <- sapply(Sp.In, function(x) any(x == 0))

c(as.matrix(data.frame(Sp.In))* rep(!logical, each = 8)) ## for equal sets


This could give you a hint

Thomas




-----Urspr?ngliche Nachricht-----
Von: Pravin [mailto:jadhavpr at vcu.edu] 
Gesendet: Mittwoch, 24. Dezember 2003 08:27
An: r-help at stat.math.ethz.ch
Betreff: [R] coding logic and syntax in R


Hello,

 

I am a beginner in R programming and recently heard about this mailing list. Currently, I am trapped into a simple problem for which I just can't find a solution. I have a huge dataset (~81,000 observations) that has been analyzed and the final result is in the form of 0 and 1(one column). 

 

I need to write a code to process this column in a little complicated way. 

These 81,000 observations are actually 9,000 sets (81,000/9). 

So, in each set whenever zero appears, rest all observations become zero.

 

For example;

If the column has: 

111110111111011111111111111111111....

The output should look like: 

111110000111000000111111111111111...

 

I hope this makes sense.

 

Thank you in anticipation,

 

Pravin

 

Pravin Jadhav

 


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From marwan.khawaja at aub.edu.lb  Tue Dec 23 17:01:36 2003
From: marwan.khawaja at aub.edu.lb (Marwan Khawaja)
Date: Tue, 23 Dec 2003 11:01:36 -0500
Subject: [R] mca
Message-ID: <CLECJBOEBGOMOKJHJNDAMECPDIAA.marwan.khawaja@aub.edu.lb>

Dear All,
I want to 'impose' supplementary points to an mca plot -- using V&R MASS
library -- and I wonder if anyone had any luck.  The book (4th edition) says it
can be done using predict.mca but there are no examples provided in the help
pages.
Would appreciate any help/pointers.
Thanks Marwan

btw, to Professor Ripley -- the abbrev=TRUE option for labels does not seem to
work.



R.Version

platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    1
minor    8.1
year     2003
month    11
day      21
language R


-------------------------------------------------------------------
Marwan Khawaja         http://departments.aub.edu.lb/~mk36



From renaud.lancelot at pasteur.mg  Wed Dec 24 10:00:22 2003
From: renaud.lancelot at pasteur.mg (Renaud Lancelot)
Date: Wed, 24 Dec 2003 12:00:22 +0300
Subject: [R] coding logic and syntax in R
In-Reply-To: <000001c3c9ef$60a81220$0500a8c0@Pravin>
References: <000001c3c9ef$60a81220$0500a8c0@Pravin>
Message-ID: <3FE955A6.8000006@pasteur.mg>

Pravin a ?crit :

> Hello,
> 
>  
> 
> I am a beginner in R programming and recently heard about this mailing list.
> Currently, I am trapped into a simple problem for which I just can't find a
> solution. I have a huge dataset (~81,000 observations) that has been
> analyzed and the final result is in the form of 0 and 1(one column). 
> 
>  
> 
> I need to write a code to process this column in a little complicated way. 
> 
> These 81,000 observations are actually 9,000 sets (81,000/9). 
> 
> So, in each set whenever zero appears, rest all observations become zero.
> 
>  
> 
> For example;
> 
> If the column has: 
> 
> 111110111111011111111111111111111....
> 
> The output should look like: 
> 
> 111110000111000000111111111111111...
> 
>  
> 
> I hope this makes sense.
> 
>  
> 
> Thank you in anticipation,
> 
>  
> 
> Pravin
> 
>  
> 
> Pravin Jadhav
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 
Pravin a ?crit :

 > Hello,
 >
 >
 >
 > I am a beginner in R programming and recently heard about this 
mailing list.
 > Currently, I am trapped into a simple problem for which I just can't 
find a
 > solution. I have a huge dataset (~81,000 observations) that has been
 > analyzed and the final result is in the form of 0 and 1(one column).
 >
 >
 >
 > I need to write a code to process this column in a little complicated 
way.
 >
 > These 81,000 observations are actually 9,000 sets (81,000/9).
 >
 > So, in each set whenever zero appears, rest all observations become zero.
 >
 >
 >
 > For example;
 >
 > If the column has:
 >
 > 111110111111011111111111111111111....
 >
 > The output should look like:
 >
 > 111110000111000000111111111111111...
 >
 >
 >
 > I hope this makes sense.
 >
 >
 >
 > Thank you in anticipation,
 >
 >
 >
 > Pravin
 >
 >
 >
 > Pravin Jadhav
 >
 >
 >
 >
 > 	[[alternative HTML version deleted]]
 >
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
 >
 >
Here is an example:

set.seed(101)
v <- sample(c(0, 1), size = 36, replace = TRUE, prob = c(.05, .95))
L <- length(v) / 9
idx <- rep(seq(L), each = 9)

fn <- function(x){
         ok <- FALSE
         for(i in seq(length(x))){
           if(x[i] == 0) ok <- TRUE
           x[i] <- if(ok) 0 else 1
           }
         x
   }

cbind(idx, v, recod = unlist(tapply(v, idx, fn)))
    idx v recod
11   1 1     1
12   1 1     1
13   1 1     1
14   1 1     1
15   1 1     1
16   1 1     1
17   1 1     1
18   1 1     1
19   1 1     1
21   2 1     1
22   2 1     1
23   2 1     1
24   2 1     1
25   2 1     1
26   2 1     1
27   2 1     1
28   2 1     1
29   2 1     1
31   3 1     1
32   3 1     1
33   3 1     1
34   3 0     0
35   3 1     0
36   3 1     0
37   3 1     0
38   3 1     0
39   3 1     0
41   4 1     1
42   4 1     1
43   4 1     1
44   4 1     1
45   4 1     1
46   4 1     1
47   4 1     1
48   4 1     1
49   4 1     1
 >

Merry Christmas !

Renaud


-- 
Dr Renaud Lancelot
v?t?rinaire ?pid?miologiste
Ambassade de France - SCAC
BP 834 Antannarivo 101
Madagascar

t?l. +261 (0)32 04 824 55 (cell)
      +261 (0)20 22 494 37 (home)




-- 
Dr Renaud Lancelot
v?t?rinaire ?pid?miologiste
Ambassade de France - SCAC
BP 834 Antannarivo 101
Madagascar

t?l. +261 (0)32 04 824 55 (cell)
      +261 (0)20 22 494 37 (home)



From ligges at statistik.uni-dortmund.de  Wed Dec 24 12:53:21 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 24 Dec 2003 12:53:21 +0100
Subject: [R] mca
References: <CLECJBOEBGOMOKJHJNDAMECPDIAA.marwan.khawaja@aub.edu.lb>
Message-ID: <3FE97E31.D82B7A15@statistik.uni-dortmund.de>



Marwan Khawaja wrote:
> 
> Dear All,
> I want to 'impose' supplementary points to an mca plot -- using V&R MASS
> library -- and I wonder if anyone had any luck.  The book (4th edition) says it
> can be done using predict.mca but there are no examples provided in the help
> pages.
> Would appreciate any help/pointers.
> Thanks Marwan

What about 
 points(predict(mcaObject, newdata = newData))
?

> btw, to Professor Ripley -- the abbrev=TRUE option for labels does not seem to
> work.

Why do you think so? At least the example in ?mca works.

Uwe Ligges


> R.Version
> 
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    1
> minor    8.1
> year     2003
> month    11
> day      21
> language R
> 
> -------------------------------------------------------------------
> Marwan Khawaja         http://departments.aub.edu.lb/~mk36
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Wed Dec 24 14:42:30 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 24 Dec 2003 13:42:30 +0000 (GMT)
Subject: [R] question: DLL or EXE from R procedures
In-Reply-To: <56018.183216-3774-519180520-1072218137@seznam.cz>
Message-ID: <Pine.LNX.4.44.0312241331400.18977-100000@gannet.stats>

On Tue, 23 Dec 2003, PaTa PaTaS wrote:

> I wonder if it is possible to create an DLL or EXE file performing R
> procedures. Instead of running R, reading data and calling some
> procedures, I would like to use R functions in the following way:
> "C:\linearRegression.exe data.txt" which would produce let's say file
> "output.txt" with the results. Is there some way how to do it?

Yes (quite a few actually).  By far the simplest would be to write a small 
C program making use of the system command to call R on a given script, 
using either environment variables or the command-line of R to pass 
arguments such as `data.txt'.  You could also call R via one of the D(COM) 
interfaces although that would seem overkill for something so simple, and
I would use a scripting language (e.g. Perl) for this sort of thing.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Wed Dec 24 14:54:34 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 24 Dec 2003 14:54:34 +0100
Subject: [R] question: DLL or EXE from R procedures
References: <56018.183216-3774-519180520-1072218137@seznam.cz>
Message-ID: <3FE99A9A.E9D87466@statistik.uni-dortmund.de>



PaTa PaTaS wrote:
> 
> Hi,
> I wonder if it is possible to create an DLL or EXE file performing R procedures. Instead of running R, reading data and calling some procedures, I would like to use R functions in the following way: "C:\linearRegression.exe data.txt" which would produce let's say file "output.txt" with the results. Is there some way how to do it?
> Thanks a lot. Pavel Vanecek

No. There is not compiler for R code available.

Uwe Ligges



From spencer.graves at pdf.com  Wed Dec 24 16:27:08 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 24 Dec 2003 07:27:08 -0800
Subject: [R] question: DLL or EXE from R procedures
In-Reply-To: <3FE99A9A.E9D87466@statistik.uni-dortmund.de>
References: <56018.183216-3774-519180520-1072218137@seznam.cz>
	<3FE99A9A.E9D87466@statistik.uni-dortmund.de>
Message-ID: <3FE9B04C.5050703@pdf.com>

      While Uwe is correct about the absence of a compiler for R, one 
can call R from other languages such as C.  A description of this can be 
found, e.g., from R 1.8.1, help.start() -> "Writing R Extensions" -> 
"Evaluating R expressions from C".  Also, the many R functions actually 
call C code, which could be called directly. 

      hope this helps. 
      spencer graves    

Uwe Ligges wrote:

>PaTa PaTaS wrote:
>  
>
>>Hi,
>>I wonder if it is possible to create an DLL or EXE file performing R procedures. Instead of running R, reading data and calling some procedures, I would like to use R functions in the following way: "C:\linearRegression.exe data.txt" which would produce let's say file "output.txt" with the results. Is there some way how to do it?
>>Thanks a lot. Pavel Vanecek
>>    
>>
>
>No. There is not compiler for R code available.
>
>Uwe Ligges
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From ripley at stats.ox.ac.uk  Wed Dec 24 17:15:52 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 24 Dec 2003 16:15:52 +0000 (GMT)
Subject: [R] question: DLL or EXE from R procedures
In-Reply-To: <3FE9B04C.5050703@pdf.com>
Message-ID: <Pine.LNX.4.44.0312241609310.19088-100000@gannet.stats>

On Wed, 24 Dec 2003, Spencer Graves wrote:

>       While Uwe is correct about the absence of a compiler for R, one 
> can call R from other languages such as C.  A description of this can be 
> found, e.g., from R 1.8.1, help.start() -> "Writing R Extensions" -> 
> "Evaluating R expressions from C".  

But first you have to get an `R expression', and you also need to have R 
initialized and running, so this is only really of use from an package 
called from R in the first place.

I haven't seen the reply I sent several hours ago, but that mentioned 
several routes, such as embedding R via (D)COM and calling R with a system 
call.

> Also, the many R functions actually 
> call C code, which could be called directly. 

In principle yes, but I would not try doing regression that way -- just 
think about the code needed to encode a formula (and BTW that calls back 
into an R interpreter).

>       hope this helps. 
>       spencer graves    
> 
> Uwe Ligges wrote:
> 
> >PaTa PaTaS wrote:
> >  
> >
> >>Hi,
> >>I wonder if it is possible to create an DLL or EXE file performing R procedures. Instead of running R, reading data and calling some procedures, I would like to use R functions in the following way: "C:\linearRegression.exe data.txt" which would produce let's say file "output.txt" with the results. Is there some way how to do it?
> >>Thanks a lot. Pavel Vanecek
> >>    
> >>
> >
> >No. There is not compiler for R code available.
> >
> >Uwe Ligges
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >  
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From turk at math.montana.edu  Wed Dec 24 19:28:19 2003
From: turk at math.montana.edu (Philip Turk)
Date: Wed, 24 Dec 2003 11:28:19 -0700 (MST)
Subject: [R] Solution to "Can anyone help me reproduce this SAS Mixed
	output??"
Message-ID: <Pine.GSO.4.44.0312241113400.19902-100000@newton1.math.montana.edu>

To those who might be interested -- following is the solution to my
previous post regarding reproducing output from SAS Proc Mixed for a
two-factor crossed random effects ANOVA model.

I am graciously endebted to the kind replys from two statisticians for
this solution whose names I will refrain from mentioning for the sake of
privacy.

I hope this helps someone?!

-- Phil Turk

> hw7 <- read.table("C:/My Documents/Phil/S412/hw7.txt", header = TRUE)

> hw7

    mpg driver car obs
1  25.3      1   1   1
2  25.2      1   1   2
3  28.9      1   2   1
.    .       .   .   .
.    .       .   .   .
.    .       .   .   .
38 30.7      4   4   2
39 30.3      4   5   1
40 29.9      4   5   2

> attach(hw7)

> driver <- factor(driver)

> car <- factor(car)

> require(nlme)

> const <- factor(rep(1,40))

> hw7.lme <- lme(mpg~1, random=list(const=pdBlocked(list(~car-1,~driver-1,~car:driver-1),
                 pdClass="pdIdent")))

> VarCorr(hw7.lme)

> intervals(hw7.lme)

Phil Turk

Department of                   ____________
Mathematical Sciences          |            |           phone: (406)994-5357
2-235 Wilson Hall               \   BZN, MT |           FAX:   (406)994-1789
Montana State University         |  *_______|
Bozeman, MT 59717                 \_|          e-mail: turk at math.montana.edu



From ggrothendieck at myway.com  Wed Dec 24 20:11:00 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 24 Dec 2003 14:11:00 -0500 (EST)
Subject: [R] question: DLL or EXE from R procedures
Message-ID: <20031224191100.CACBC3961@mprdmxin.myway.com>



Also, if its not necessary that it actually be a compiled
program then a batch file, linreg.bat, along with the
supporting R code file, linreg.r, and the data file, data.txt,
could be created like this (in Windows, modify for others):

--- linreg.bat -------------------------------------------------------
set infile=%1
"C:\Program Files\R\rw1081\bin\Rcmd.exe" BATCH linreg.R linreg.Rout

--- linreg.r -------------------------------------------------------
infile <- Sys.getenv("infile")
z <- read.table(infile, header=T)
with(z, lm(y ~ x))

--- data.txt -------------------------------------------------------
x y
1 2
2 4
3 7




Date: Wed, 24 Dec 2003 07:27:08 -0800 
From: Spencer Graves <spencer.graves at pdf.com>
To: Uwe Ligges <ligges at statistik.uni-dortmund.de> 
Cc: <r-help at stat.math.ethz.ch>,PaTa PaTaS <vanecekpavel2 at seznam.cz> 
Subject: Re: [R] question: DLL or EXE from R procedures 

 
 
While Uwe is correct about the absence of a compiler for R, one 
can call R from other languages such as C. A description of this can be 
found, e.g., from R 1.8.1, help.start() -> "Writing R Extensions" -> 
"Evaluating R expressions from C". Also, the many R functions actually 
call C code, which could be called directly. 

hope this helps. 
spencer graves 

Uwe Ligges wrote:

>PaTa PaTaS wrote:
> 
>
>>Hi,
>>I wonder if it is possible to create an DLL or EXE file performing R procedures. Instead of running R, reading data and calling some procedures, I would like to use R functions in the following way: "C:\linearRegression.exe data.txt" which would produce let's say file "output.txt" with the results. Is there some way how to do it?
>>Thanks a lot. Pavel Vanecek
>> 
>>
>
>No. There is not compiler for R code available.
>
>Uwe Ligges
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
>



From maida at umich.edu  Thu Dec 25 03:53:40 2003
From: maida at umich.edu (Masahiko AIDA)
Date: Thu, 25 Dec 2003 11:53:40 +0900
Subject: [R] Fast logistic regression
Message-ID: <20031225115334.CCE2.MAIDA@umich.edu>

Is there any faster implementation of logistic regression
than glm function in base package in R?

I am working on simulation study and discovered glm fitting part is the
bottle neck of my program.

-Masahiko



From acward at uqconnect.net.au  Thu Dec 25 16:50:12 2003
From: acward at uqconnect.net.au (Andrew C. Ward)
Date: Thu, 25 Dec 2003 15:50:12 -0000
Subject: [R] draft of posting guide
Message-ID: <01C3CB00.ED77B630.acward@uqconnect.net.au>

With respect to 'tone' and 'friendliness', perhaps all that is
meant or needed is that people be polite and respectful. There
is never any need for being rude, either from the asker of
questions or from the answerer. I shake my head as often at
rude answers as I do at ill-considered questions.


Regards,

Andrew C. Ward

CAPE Centre
Department of Chemical Engineering
The University of Queensland
Brisbane Qld 4072 Australia



On Tuesday, December 23, 2003 3:55 AM, Rolf Turner 
[SMTP:rolf at math.unb.ca] wrote:
> This is in response to Gabor Grothendieck's commentary on Tony
> Plate's draft guidelines for question-askers, which was posted
> a
> couple of days ago.
>
> I disagree, from mildly to vehemently with just about 
everything
> in
> Grothendieck's posting.  E.g. the ``tone'' of the draft should
> not
> be ``friendlier''.  The purpose of the guidelines is to
> encourage
> the asking of well-thought out questions and discourage the
> asking
> of stupid ones.  This politically correct ``don't damage their
> self esteem attitude'' has no place in the r-help list.
>
> A propos of bugs, for the uneducated beginner to assert that
> there is
> a ``bug'' in software designed by some of the best and most
> knowledgeable minds in the discipline, when the software works
> as
> documented, is the height of presumptuous arrogance.
>
> The guide is and should be a guide for the question-askers.
>  The
> responders who are voluntarily giving of their time and (often
> deep)
> experise need not be constrained.  The R package and this help
> list
> are free services provided voluntarily by some great people.
>  If
> someone asks a stupid question and dislikes being told so in so
> many
> words, well, that person is free to take his or her business
> elsewhere.
>
> The one point I ***agree*** with is that questions about
> statistical
> methodology should not be discouraged in any way, even if they
> are
> not directly R-related.  There is always some sort of
> relationship,
> such questions are interesting, and there is almost always some
> insight to be gained by thinking about them in an R context.
>
> 				cheers,
>
> 					Rolf Turner
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Thu Dec 25 08:42:21 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 25 Dec 2003 07:42:21 +0000 (GMT)
Subject: [R] Fast logistic regression
In-Reply-To: <20031225115334.CCE2.MAIDA@umich.edu>
Message-ID: <Pine.LNX.4.44.0312250737340.20098-100000@gannet.stats>

On Thu, 25 Dec 2003, Masahiko AIDA wrote:

> Is there any faster implementation of logistic regression
> than glm function in base package in R?
> 
> I am working on simulation study and discovered glm fitting part is the
> bottle neck of my program.

Have you tried profiling (see Writing R Extensions)?  It may not be the
actual fitting but, say, the extraction of the model matrix which is slow.  
You could call glm.fit() directly if it produces all you need.  In a
well-behaved problem you are not going to get anything much faster than
glm.fit without going entirely to compiled code (which is an option for
you, of course).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From gady at techunix.technion.ac.il  Thu Dec 25 11:03:07 2003
From: gady at techunix.technion.ac.il (Gady Zohar)
Date: Thu, 25 Dec 2003 10:03:07 -0000
Subject: [R] Multivariate Kalman filter with time-varying coefficients
Message-ID: <000601c2a775$9563c0b0$209f4484@technion.ac.il>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031225/31da2acd/attachment.pl

From feh3k at spamcop.net  Thu Dec 25 14:30:43 2003
From: feh3k at spamcop.net (Frank E Harrell Jr)
Date: Thu, 25 Dec 2003 08:30:43 -0500
Subject: [R] Fast logistic regression
In-Reply-To: <20031225115334.CCE2.MAIDA@umich.edu>
References: <20031225115334.CCE2.MAIDA@umich.edu>
Message-ID: <20031225083043.7fce48ca.feh3k@spamcop.net>

On Thu, 25 Dec 2003 11:53:40 +0900
Masahiko AIDA <maida at umich.edu> wrote:

> Is there any faster implementation of logistic regression
> than glm function in base package in R?
> 
> I am working on simulation study and discovered glm fitting part is the
> bottle neck of my program.
> 
> -Masahiko

For this purpose (e.g., bootstrapping model fits using the bootcov
function) I use the lrm.fit function in the Design package.  It assumes
the model matrix is already created and that there are no NAs.

Frank

---
Frank E Harrell Jr   Professor and Chair           School of Medicine
                     Department of Biostatistics   Vanderbilt University



From ririzarr at jhsph.edu  Thu Dec 25 21:38:54 2003
From: ririzarr at jhsph.edu (Rafael A. Irizarry)
Date: Thu, 25 Dec 2003 15:38:54 -0500 (EST)
Subject: [R] Re: if .. else parse error {was "Sweave question"} (fwd)
Message-ID: <Pine.LNX.4.44.0312251443530.5948-100000@blinux08.biostat.jhsph.edu>

Dear All,

Thanks to all that took the time to respond to my questions (included 
at the bottom). Martin Maechler was nice enough to give me a detailed 
explanation which he suggested i forward. 

The confusion arose because

if(exists("x"))	plot(x,x)
else{ plot(1,1,type="n");text(1,1,"data not available.\n")}

gave me an error when used as a chunk in a Sweave document but the 
same code didnt give me errors inside a function:

a <- function(){
if(exists("x"))  plot(x,x) 
else{  plot(1,1);  print("hello")}
}
a()

As pointed out by some of the responeders,  the problem is that in the  
original code the first two lines were syntactically complete which 
results in a line starting with an else, resulting in an error 
regardless of Sweave. But this only happens if the 
if-else statement is at top-level (in the function version it isnt)

Martin explains in more detail:

M>Remember the *reason* why it can't work on top level:
M>- At the end of the line, the S parser will evaluate the current
M>  statement  __ if it is complete __
M>  This is true for   "if (A)  B" or also  "if (A) { B ; C }"
M>  when a next line starts with "else D",
M>  this is invalid :  S language expressions cannot start with 'else'.
M>
M>  => consequence:  
M>     Make sure the if() part of an  "if(A) B else C " is not
M>     after B, because it's then interpreted as an  "if(A) B" .
M>
M>You never have this problem if all of this is part of an
M>enclosing expression, because parsing will only end after
M>completing the outermost expression; in the case of functions
M>this is the full (outermost) function body.

for example:
{
if(exists("x"))  plot(x,x)
else{ plot(1,1);print("hello")}
}
does not give errors.

An easy way to avoid this error is to follow this suggestion from the if 
help page:

"For that reason, one (somewhat extreme) attitude of defensive
 programming uses braces always, e.g., for 'if' clauses.
 always enclose expressions in 
 if-else statements even if you dont think you need them."


thanks again,
-r

"Rafael A. Irizarry" <ririzarr at jhsph.edu> writes:
                                                                                                                                          
> Using Sweave in the tools library (R version 1.8.0: sorry i havent
> upgraded), it seems i cant use if statements in R chunks that make 
graphs.
> i have this:
>
> <<fig=TRUE,echo=F>>=
> par(mfrow=c(1,1))
> if(exists("x"))
>       plot(x,x)
> else{
>   plot(1,1,type="n")
>   text(1,1,"data not available.\n")
> }
> @
>
> and I get this error:
>
> Error:  chunk 6
> Error in parse(file, n, text, prompt) : parse error
>
> any help is appreciated.
>
> thanks and apologies if this not a problem in R 1.8.1
> rafael
>



From DJNordlund at aol.com  Thu Dec 25 22:41:10 2003
From: DJNordlund at aol.com (DJNordlund@aol.com)
Date: Thu, 25 Dec 2003 16:41:10 EST
Subject: [R] Problem plotting with xyplot
Message-ID: <75.1f781c7d.2d1cb376@aol.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031225/1bce02b8/attachment.pl

From ripley at stats.ox.ac.uk  Fri Dec 26 00:00:18 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 25 Dec 2003 23:00:18 +0000 (GMT)
Subject: [R] Problem plotting with xyplot
In-Reply-To: <75.1f781c7d.2d1cb376@aol.com>
Message-ID: <Pine.LNX.4.44.0312252235070.17791-100000@gannet.stats>

You are trying to fit a loess curve to just five points: that is not 
sensible (and interpolates in S-PLUS, so I don't think it `works' there 
either).

Loess in R is not the same as loess in S-PLUS.  (The latter is
proprietary, and loess in R was implemented as a wrapper for the released
C/Fortran code from the same group of authors.)  You could have tried
looking at the warnings, the first four of which are

1: pseudoinverse used at 13
2: neighborhood radius 1
3: reciprocal condition number  0
4: There are other near singularities as well. 1

and indicate that there were problems in doing the loess fit.

What did you (or the authors) want here?  It is hard to think of a useful 
visualization of the pattern of 5 points beyond linear interpolation.


BTW, you want library(lattice) and not library(nlme).  In 1.8.1 
nlme attaches lattice, but in the next release it will not do so.


On Thu, 25 Dec 2003 DJNordlund at aol.com wrote:

> Hi all,
> 
> I am just learning R and I am trying to work through the book "Applied 
> Longitudinal Data Analysis" by Singer & Willett.  I have some code for this book 
> that supposedly works in S-Plus (I don't have S-Plus so I can't verify that) and 
> I am trying to run the examples in R.  Most of the examples run, but I have 
> one plot that gives me an error message.  I have provided a small dataset below 
> and the problematic code.
> 
> I don't know if the problem is a dissimilarity between R and S-Plus, or if I 
> have done something wrong because I don't know R well enough yet (I am also 
> working through MASS by Venables & Ripley, and the Pinheiro & Bates book on 
> Mixed-Effect Models).  
> 
> I looked at the warnings and also did traceback() which just took me to the 
> function listed in the error message.  I can't determine where the NA/NaN/Inf 
> is (are) coming from.
> 
> Any help would be appreciated.  I am using Rwin 1.8.1 (pre-compiled) on 
> Win2000 Professional.
> 
> Thanks,
> 
> Dan Nordlund
> 
> ## toy problem
> library(nlme)
> 
> tolerance.pp <- as.data.frame(
> matrix(c(   
> 9,11,2.23,
> 9,12,1.79,
> 9,13,1.90,
> 9,14,2.12,
> 9,15,2.66,
> 45,11,1.12,
> 45,12,1.45,
> 45,13,1.45,
> 45,14,1.45,
> 45,15,1.99,
> 268,11,1.45,
> 268,12,1.34,
> 268,13,1.99,
> 268,14,1.79,
> 268,15,1.34,
> 314,11,1.22,
> 314,12,1.22,
> 314,13,1.55,
> 314,14,1.12,
> 314,15,1.12
> ), byrow = T, ncol = 3 ))
> 
> names(tolerance.pp) <- c('id','age','tolerance')
> 
> ## this plots out fine
> xyplot(tolerance~age | factor(id), data=tolerance.pp)
> 
> ## this produces an error message
> xyplot(tolerance~age | factor(id), data=tolerance.pp, 
>   prepanel = function(x, y) prepanel.loess(x, y), 
>   panel = function(x, y){
>     panel.xyplot(x, y)
>     panel.loess(x, y)
>   }, ylim=c(0, 4), as.table=T)
> 
> 
> Error message is:
> 
> >Error in simpleLoess(y, x, w, span, degree, FALSE, FALSE, normalize = FALSE, 
>  : 
> >        NA/NaN/Inf in foreign function call (arg 1)
> >In addition: There were 16 warnings (use warnings() to see them)
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From schoonbee at hotmail.com  Fri Dec 26 00:07:06 2003
From: schoonbee at hotmail.com (Derick Schoonbee)
Date: Thu, 25 Dec 2003 23:07:06 +0000
Subject: [R] Plot a sphere
Message-ID: <LAW11-F20i6QqbGHuy600066a2b@hotmail.com>

Hi,

I'm new to R (and math ;) Would somebody please be so kind as to direct me 
in plotting a 3D sphere?

I tried something in the lines of:
####
y <- x <- seq(-pi, pi, length=pi*10)
f <- function(x,y)
{
	z <- sqrt(pi - x^2 - y^2)
	#z[is.na(z)] <- 0
	z
}
z <- outer(x, y, f)

persp(x, y, z, theta = 120, phi = 30)
####

I've also tried: .... make.surface.grid(...) .. persp( as.surface( grid, z) 
) ... with the same result: 'Incomplete' demi sphere and others..

Any suggestions/solutions would be appreaciated.

Regards,
Derick

PS:Merry X-mas ;)



From DJNordlund at aol.com  Fri Dec 26 01:47:20 2003
From: DJNordlund at aol.com (DJNordlund@aol.com)
Date: Thu, 25 Dec 2003 19:47:20 EST
Subject: [R] Problem plotting with xyplot
Message-ID: <127.379ab1b5.2d1cdf18@aol.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031225/bb649327/attachment.pl

From spencer.graves at pdf.com  Fri Dec 26 02:38:01 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 25 Dec 2003 17:38:01 -0800
Subject: [R] Plot a sphere
In-Reply-To: <LAW11-F20i6QqbGHuy600066a2b@hotmail.com>
References: <LAW11-F20i6QqbGHuy600066a2b@hotmail.com>
Message-ID: <3FEB90F9.8010509@pdf.com>

      A hemisphere is relatively easy;  try the following: 

      x <- seq(-1, 1, length=21)
      Z <- outer(x, x, function(x, y)sqrt(1-x^2-y^2))
      persp(x=x, y=x, z=Z)

      A contour plot is also relatively easy: 

      image(x=x, y=x, z=Z)
      contour(x=x, y=x, z=Z, add=T)

      However, if you want an honest perspective plot of a sphere 
complete with the underside, etc., I know of nothing in R that could do 
that.  S-Plus has "perspp", which could be used.  However, that seems to 
be one of the few features available in S-Plus that is not currently 
available in R. 

       hope this helps. 
      spencer graves

Derick Schoonbee wrote:

> Hi,
>
> I'm new to R (and math ;) Would somebody please be so kind as to 
> direct me in plotting a 3D sphere?
>
> I tried something in the lines of:
> ####
> y <- x <- seq(-pi, pi, length=pi*10)
> f <- function(x,y)
> {
>     z <- sqrt(pi - x^2 - y^2)
>     #z[is.na(z)] <- 0
>     z
> }
> z <- outer(x, y, f)
>
> persp(x, y, z, theta = 120, phi = 30)
> ####
>
> I've also tried: .... make.surface.grid(...) .. persp( as.surface( 
> grid, z) ) ... with the same result: 'Incomplete' demi sphere and 
> others..
>
> Any suggestions/solutions would be appreaciated.
>
> Regards,
> Derick
>
> PS:Merry X-mas ;)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From thondeboer at yahoo.com  Fri Dec 26 04:56:00 2003
From: thondeboer at yahoo.com (Thon de Boer)
Date: Thu, 25 Dec 2003 19:56:00 -0800 (PST)
Subject: [R] Problems converting output from Sweave to PDf
Message-ID: <20031226035600.48079.qmail@web41507.mail.yahoo.com>

I am having trouble converting the output from Sweave
into a valid PDF file.

I have created a simple .Rnw file which will become a
full vignette at some point, but during the
intermediate testing, I got errors from texi2dvi.
This is what I have done.

0) Using a Windows Xp system
1) Created a file called GeneSpring.Rnw
2) Convert this to Tex using Sweave("GeneSpring.Rnw")
from within R
3) Since I don't have LATEX for windows, I used
texi2dvi from the CYGWIN package
4) When I run the command in Cygwin 'texi2dvi -c -p
GeneSpring.tex' I get the error shown below.
5) When I edit the GeneSpring.tex file and comment out
the line

\usepackage{C:/PROGRA~1/SILICO~1/GENESP~1/data/rw1081/share/texmf/Sweave}

and try to convert to PDF again, it will successfully
convert the TEX file to PDF.

It seems that the Sweave library causes some problems
in the parsing of the TEX file. Is there something I
forgot to do?

Thanks,

Thon


------------- OUTPUT FROM texi2dvi -----------------

/usr/local/bin:/usr/bin:/bin:/usr/X11R6/bin:/cygdrive/c/Perl/bin/:/cygdrive/c/WINDOWS/system32:/cygdrive/c/WINDOWS:/cygdrive/c/WINDOWS/System32/Wbem:/cygdrive/c/Program
Files/ATI Technologies/ATI Control
Panel:/cygdrive/c/Program Files/Common Files/Adaptec
Shared/System:/cygdrive/c/Program
Files/TEC100/BIN:/cygdrive/c/Program
Files/bin:/cygdrive/c/Program
Files/SiliconGenetics/GeneSpring/data/rw1081/bin
This is pdfTeXk, Version 3.14159-1.10b (Web2C 7.4.5)
 %&-line parsing enabled.
(/tmp/GeneSpring.tex{/usr/share/texmf/pdftex/config/pdftex.cfg}
LaTeX2e <2001/06/01>
Babel <v3.7h> and hyphenation patterns for american,
french, german, ngerman, n
ohyphenation, loaded.
(/usr/share/texmf/tex/latex/base/article.cls
Document Class: article 2001/04/21 v1.4e Standard
LaTeX document class
(/usr/share/texmf/tex/latex/base/size10.clo))
! Missing \endcsname inserted.
<to be read again> 
                   \protect 
l.9 \begin
          {document}
? 

! LaTeX Error: Missing \begin{document}.

See the LaTeX manual or LaTeX Companion for
explanation.
Type  H <return>  for immediate help.
 ...                                              
                                                  
l.9 \begin
          {document}
? 



-------------- TEX FILE ------------------


% \VignetteIndexEntry{GeneSpring contents}
% \VignetteDepends{GeneSpring}
% \VignetteKeywords{Interface}
% \VignettePackage{GeneSpring}

\documentclass{article}

\usepackage{C:/PROGRA~1/SILICO~1/GENESP~1/data/rw1081/share/texmf/Sweave}
\begin{document}

\author{Thon de Boer}

\title{Using R with GeneSpring}

\maketitle

\copyright{2003 Silicon Genetics}

\section{Introduction}

This package contains a number of functions to
facilitate the integration
of R code into the Gene Expression analysis program
GeneSpring, by Silicon Genetics.
Available functions include:

\begin{itemize}
\item \texttt{GSload.int()} - Read GeneSpring
experiment interpretation from file and return a
GeneSpring gene expression object (GSint)
\item \texttt{GSload.intBC()} - Read GeneSpring
experiment interpretation from file and return a
BioConductor gene expressioon object (exprSet)
\item \texttt{GSload.exp()} - Read GeneSpring
experiment from file and return a GeneSpring gene
expression object (GSint)
\item \texttt{GSload.expBC()} - Read GeneSpring
experiment from file and return a BioConductor gene
expressioon object (exprSet)
\item \texttt{GSint()} - Create a GeneSpring gene
expression object (GSint)
\item \texttt{GSint2BC()} - Convert a GeneSpring gene
expression object (GSint) to a BioConductor gene
expression object (exprSet)
\item \texttt{BC2GSint()} - Convert a BioConductor
gene expression object (exprSet) to a GeneSpring gene
expression object (GSint)
\item \texttt{GSload.genelist()} - Read a GeneSpring
GeneSpring gene list from file
\item \texttt{GSsave.genelist()} - Save a GeneSpring
GeneSpring gene list to file
\item \texttt{GSsave.exp()} - Save a GeneSpring gene
expression object (GSint) to file
\end{itemize}

For more information on using thi spackage with
GeneSpring, go to the Silicon Genetics website.

\end{document}



From wildscop at yahoo.com  Fri Dec 26 05:52:43 2003
From: wildscop at yahoo.com (WilDscOp)
Date: Fri, 26 Dec 2003 10:52:43 +0600
Subject: [R] OC curve in "Quality Control"
Message-ID: <5.1.0.14.2.20031226102410.009ff0d0@mail.dhaka.net>

Dear all,

Can anyone please help me about any of the following questions:
--------------------------------------------
1. How can i find "factorial" of any number in R? I tried
	> prod(170:1) # to find factorial of 170 or 170!
Is it the only procedure - or R has any better process / operational 
character to calculate factorial? Also, is it possible to calculate 
factorial of 500? Or is there any statistical table available for this?
--------------------------------------------
2. Is there any direct procedure / package in R to find "permutation / 
combination"?
--------------------------------------------
3. Can i find Probability Density of "Hypergeometric Disribution" in R, 
given all values of the parameter? I can not find any table of 
Hypergeometric Disribution (if such tables are available on the internet, 
please let me know).
--------------------------------------------
4. Is there any way i can perform "Quality control" calculations (like 
p-chart / np-chart / c-chart / u-chart / OC curves / AOQ / AOQL / ATI / ASN 
/ plan design) in R? I guess such procedures are available in SAS / 
Statistica or other statistical softwares. If there are such operations in 
R, please refer me to the appropriate documentations.
--------------------------------------------
5. Particularly, if we have lots of size N = 50000 and the receiving 
inspection procedure used in single-sampling plan with n = 50 and c = 1; 
then i need to draw the type-A and type-B operating characteristic curve 
for the plan. Can i do it on R?
----------------- ---------------------------

     Any comment / suggestion / idea / web-link / replies will be gladly 
accepted. Thanks for your time.




_______________________
Mohammad Ehsanul Karim <wildscop at yahoo.com>
Institute of Statistical Research and Training
University of Dhaka, Dhaka- 1000, Bangladesh



From ramasamya at gis.a-star.edu.sg  Fri Dec 26 07:01:50 2003
From: ramasamya at gis.a-star.edu.sg (Adaikalavan RAMASAMY)
Date: Fri, 26 Dec 2003 14:01:50 +0800
Subject: [R] coding logic and syntax in R
Message-ID: <6D9E9B9DF347EF4385F6271C64FB8D56076092@BIONIC.biopolis.one-north.com>

In addition to the previous replies, try this

x <- as.numeric(strsplit("111110111111011111111111111", NULL)[[1]])
g <- rep(1:3, each=9)                   # set numbering
rbind(x, g)                             # to check

y <- unlist( tapply(x, g, cummin) )
> y
11 12 13 14 15 16 17 18 19 21 22 23 24 25 26 27 28 29 31 32 33 34 35 36 37 38 39 
 1  1  1  1  1  0  0  0  0  1  1  1  0  0  0  0  0  0  1  1  1  1  1  1  1  1  1 

tapply() applies a given function, in this case cummin(), to the sets defined by 'g'.
cummin() returns the cummulative minimum
Here, the names of vector y is a combination of set number and observation in set number.

--
Adaikalavan Ramasamy 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Renaud Lancelot
Sent: Wednesday, December 24, 2003 5:00 PM
To: Pravin
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] coding logic and syntax in R


Pravin a ?crit :

> Hello,
> 
>  
> 
> I am a beginner in R programming and recently heard about this mailing 
> list. Currently, I am trapped into a simple problem for which I just 
> can't find a solution. I have a huge dataset (~81,000 observations) 
> that has been analyzed and the final result is in the form of 0 and 
> 1(one column).
> 
>  
> 
> I need to write a code to process this column in a little complicated 
> way.
> 
> These 81,000 observations are actually 9,000 sets (81,000/9).
> 
> So, in each set whenever zero appears, rest all observations become 
> zero.
> 
>  
> 
> For example;
> 
> If the column has:
> 
> 111110111111011111111111111111111....
> 
> The output should look like:
> 
> 111110000111000000111111111111111...
> 
>  
> 
> I hope this makes sense.
> 
>  
> 
> Thank you in anticipation,
> 
>  
> 
> Pravin
> 
>  
> 
> Pravin Jadhav
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 
Pravin a ?crit :

 > Hello,
 >
 >
 >
 > I am a beginner in R programming and recently heard about this 
mailing list.
 > Currently, I am trapped into a simple problem for which I just can't 
find a
 > solution. I have a huge dataset (~81,000 observations) that has been  > analyzed and the final result is in the form of 0 and 1(one column).  >  >  >  > I need to write a code to process this column in a little complicated 
way.
 >
 > These 81,000 observations are actually 9,000 sets (81,000/9).  >  > So, in each set whenever zero appears, rest all observations become zero.  >  >  >  > For example;  >  > If the column has:  >  > 111110111111011111111111111111111....
 >
 > The output should look like:
 >
 > 111110000111000000111111111111111...
 >
 >
 >
 > I hope this makes sense.
 >
 >
 >
 > Thank you in anticipation,
 >
 >
 >
 > Pravin
 >
 >
 >
 > Pravin Jadhav
 >
 >
 >
 >
 > 	[[alternative HTML version deleted]]
 >
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
 >
 >
Here is an example:

set.seed(101)
v <- sample(c(0, 1), size = 36, replace = TRUE, prob = c(.05, .95)) L <- length(v) / 9 idx <- rep(seq(L), each = 9)

fn <- function(x){
         ok <- FALSE
         for(i in seq(length(x))){
           if(x[i] == 0) ok <- TRUE
           x[i] <- if(ok) 0 else 1
           }
         x
   }

cbind(idx, v, recod = unlist(tapply(v, idx, fn)))
    idx v recod
11   1 1     1
12   1 1     1
13   1 1     1
14   1 1     1
15   1 1     1
16   1 1     1
17   1 1     1
18   1 1     1
19   1 1     1
21   2 1     1
22   2 1     1
23   2 1     1
24   2 1     1
25   2 1     1
26   2 1     1
27   2 1     1
28   2 1     1
29   2 1     1
31   3 1     1
32   3 1     1
33   3 1     1
34   3 0     0
35   3 1     0
36   3 1     0
37   3 1     0
38   3 1     0
39   3 1     0
41   4 1     1
42   4 1     1
43   4 1     1
44   4 1     1
45   4 1     1
46   4 1     1
47   4 1     1
48   4 1     1
49   4 1     1
 >

Merry Christmas !

Renaud


-- 
Dr Renaud Lancelot
v?t?rinaire ?pid?miologiste
Ambassade de France - SCAC
BP 834 Antannarivo 101
Madagascar

t?l. +261 (0)32 04 824 55 (cell)
      +261 (0)20 22 494 37 (home)




-- 
Dr Renaud Lancelot
v?t?rinaire ?pid?miologiste
Ambassade de France - SCAC
BP 834 Antannarivo 101
Madagascar

t?l. +261 (0)32 04 824 55 (cell)
      +261 (0)20 22 494 37 (home)

______________________________________________
R-help at stat.math.ethz.ch mailing list https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From glaziou at pasteur-kh.org  Fri Dec 26 10:22:38 2003
From: glaziou at pasteur-kh.org (Philippe Glaziou)
Date: Fri, 26 Dec 2003 16:22:38 +0700
Subject: [R] OC curve in "Quality Control"
In-Reply-To: <5.1.0.14.2.20031226102410.009ff0d0@mail.dhaka.net>
References: <5.1.0.14.2.20031226102410.009ff0d0@mail.dhaka.net>
Message-ID: <20031226092238.GA2221@pasteur-kh.org>

WilDscOp <wildscop at yahoo.com> wrote:
> Can anyone please help me about any of the following questions:
> --------------------------------------------
> 1. How can i find "factorial" of any number in R? I tried
> 	> prod(170:1) # to find factorial of 170 or 170!
> Is it the only procedure - or R has any better process / operational 
> character to calculate factorial? Also, is it possible to calculate 
> factorial of 500? Or is there any statistical table available for this?


> gamma(x+1) # gives x! 

As for the factorial of 500, I would go for a log transformation:

> lgamma(501)
[1] 2611.330

But if you really need to see all the digits of 500!, PARI
<http://pari.math.u-bordeaux.fr/> --or rather its shell, may be
called from within R. It returns all digits of that large number
in a fraction of a second on my old laptop:

> system.time(system("echo '500!' | gp"))
[...]
%1 = 12201368259911100687012387854230469262535743428031928421
9241358838584537315388199760549644750220328186301361647714820
[about a thousand other digits follow]
Good bye!
[1] 0.00 0.00 0.05 0.03 0.02


For a more readable output:

> system("echo '500!*1.0' | gp")
[...]
%1 = 1.220136825991110068701238785 E1134



> --------------------------------------------
> 2. Is there any direct procedure / package in R to find "permutation / 
> combination"?


To permute the elements of a vector, you may use:

> perm <- function(v)sample(v,size=length(v),replace=FALSE)
> v <- 1:8
> v
[1] 1 2 3 4 5 6 7 8
> set.seed(101)
> perm(v)
[1] 3 1 5 4 7 6 2 8


Re combinations, does the function choose() do what you are
looking for?

> --------------------------------------------
> 3. Can i find Probability Density of "Hypergeometric Disribution" in R, 
> given all values of the parameter? I can not find any table of 
> Hypergeometric Disribution (if such tables are available on the internet, 
> please let me know).

Does dhyper() do what you are looking for? 

-- 
Philippe Glaziou
Epidemiologist



From jasont at indigoindustrial.co.nz  Fri Dec 26 11:36:12 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Fri, 26 Dec 2003 23:36:12 +1300
Subject: [R] Sweave question
In-Reply-To: <Pine.LNX.4.44.0312231038420.25968-100000@blinux08.biostat.jhsph.edu>
References: <Pine.LNX.4.44.0312231038420.25968-100000@blinux08.biostat.jhsph.edu>
Message-ID: <3FEC0F1C.2050709@indigoindustrial.co.nz>

Rafael A. Irizarry wrote:

> Using Sweave in the tools library (R version 1.8.0: sorry i havent 
> upgraded), it seems i cant use if statements in R chunks that make graphs. 
> i have this:
> 
> <<fig=TRUE,echo=F>>=
> par(mfrow=c(1,1))
> if(exists("x")) 
> 	plot(x,x)
> else{
>   plot(1,1,type="n")
>   text(1,1,"data not available.\n")
> }
> @
> 
> and I get this error:
> 
> Error:  chunk 6
> Error in parse(file, n, text, prompt) : parse error
> 
> any help is appreciated.
> 

Whenever you get an error message, copy and paste it into an R session 
(interactive) and see what happens.  At least then you'll know where the 
problem is, and whether it's a Sweave problem or a problem with your R 
syntax (in this case, it's the second option. sorry).

This problem is almost, but not quite, a FAQ.

The problem is that the "if(...)" is syntactically complete after the 
first plot() command.  The "else {" appears out of nowhere, as far as 
the R interpreter is concerned, and this makes no sense.

The fixes:

1) Always do this:

if(something) {
   do something
} else {
   do something else
}

Note exactly (!) where the curly braces are, relative to the "else".

2) wrap the whole thing in curly braces.

{
    if(something)
      do somthing
    else {
      do something else
    }
}

Cheers

Jason


-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From ligges at statistik.uni-dortmund.de  Fri Dec 26 12:48:49 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 26 Dec 2003 12:48:49 +0100
Subject: [R] Plot a sphere
References: <LAW11-F20i6QqbGHuy600066a2b@hotmail.com>
	<3FEB90F9.8010509@pdf.com>
Message-ID: <3FEC2021.7F659DEE@statistik.uni-dortmund.de>



Spencer Graves wrote:
> 
>       A hemisphere is relatively easy;  try the following:
> 
>       x <- seq(-1, 1, length=21)
>       Z <- outer(x, x, function(x, y)sqrt(1-x^2-y^2))
>       persp(x=x, y=x, z=Z)
> 
>       A contour plot is also relatively easy:
> 
>       image(x=x, y=x, z=Z)
>       contour(x=x, y=x, z=Z, add=T)
> 
>       However, if you want an honest perspective plot of a sphere
> complete with the underside, etc., I know of nothing in R that could do
> that.  S-Plus has "perspp", which could be used.  However, that seems to
> be one of the few features available in S-Plus that is not currently
> available in R.


The R package "rgl" by Adler and Nenadic can plot spheres. It is
available at 
http://wsopuppenkiste.wiso.uni-goettingen.de/~dadler/rgl/
 --- and looks like it will shortly become a CRAN package.

Uwe Ligges


>        hope this helps.
>       spencer graves
> 
> Derick Schoonbee wrote:
> 
> > Hi,
> >
> > I'm new to R (and math ;) Would somebody please be so kind as to
> > direct me in plotting a 3D sphere?
> >
> > I tried something in the lines of:
> > ####
> > y <- x <- seq(-pi, pi, length=pi*10)
> > f <- function(x,y)
> > {
> >     z <- sqrt(pi - x^2 - y^2)
> >     #z[is.na(z)] <- 0
> >     z
> > }
> > z <- outer(x, y, f)
> >
> > persp(x, y, z, theta = 120, phi = 30)
> > ####
> >
> > I've also tried: .... make.surface.grid(...) .. persp( as.surface(
> > grid, z) ) ... with the same result: 'Incomplete' demi sphere and
> > others..
> >
> > Any suggestions/solutions would be appreaciated.
> >
> > Regards,
> > Derick
> >
> > PS:Merry X-mas ;)
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From schoonbee at hotmail.com  Fri Dec 26 12:50:01 2003
From: schoonbee at hotmail.com (Derick Schoonbee)
Date: Fri, 26 Dec 2003 11:50:01 +0000
Subject: [R] Plot a sphere
Message-ID: <Law11-F122IE3K8TM4F00067cc5@hotmail.com>

Thanks for the reply. The results are exactly the same as what I'm getting.
Now I'm thinking in the lines of:

z1 <- outer(x, y, f)
z2 <- -outer(x, y, f)

So, I want to attach the two hemispheres. But then I need to figure out how 
append vectors z1 & z2 and then 'feed' this to persp..

hmm, I'll look into it.

Regards,
D.

From: Spencer Graves <spencer.graves at pdf.com>
To: Derick Schoonbee <schoonbee at hotmail.com>
CC: r-help at stat.math.ethz.ch
Subject: Re: [R] Plot a sphere
Date: Thu, 25 Dec 2003 17:38:01 -0800

      A hemisphere is relatively easy;  try the following:

      x <- seq(-1, 1, length=21)
      Z <- outer(x, x, function(x, y)sqrt(1-x^2-y^2))
      persp(x=x, y=x, z=Z)

      A contour plot is also relatively easy:

      image(x=x, y=x, z=Z)
      contour(x=x, y=x, z=Z, add=T)

      However, if you want an honest perspective plot of a sphere complete 
with the underside, etc., I know of nothing in R that could do that.  S-Plus 
has "perspp", which could be used.  However, that seems to be one of the few 
features available in S-Plus that is not currently available in R.

       hope this helps.      spencer graves

Derick Schoonbee wrote:

>Hi,
>
>I'm new to R (and math ;) Would somebody please be so kind as to direct me 
>in plotting a 3D sphere?
>
>I tried something in the lines of:
>####
>y <- x <- seq(-pi, pi, length=pi*10)
>f <- function(x,y)
>{
>     z <- sqrt(pi - x^2 - y^2)
>     #z[is.na(z)] <- 0
>     z
>}
>z <- outer(x, y, f)
>
>persp(x, y, z, theta = 120, phi = 30)
>####
>
>I've also tried: .... make.surface.grid(...) .. persp( as.surface( grid, z) 
>) ... with the same result: 'Incomplete' demi sphere and others..
>
>Any suggestions/solutions would be appreaciated.
>
>Regards,
>Derick
>
>PS:Merry X-mas ;)
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ligges at statistik.uni-dortmund.de  Fri Dec 26 12:59:42 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 26 Dec 2003 12:59:42 +0100
Subject: [R] Problems converting output from Sweave to PDf
References: <20031226035600.48079.qmail@web41507.mail.yahoo.com>
Message-ID: <3FEC22AE.68125EDA@statistik.uni-dortmund.de>



Thon de Boer wrote:
> 
> I am having trouble converting the output from Sweave
> into a valid PDF file.
> 
> I have created a simple .Rnw file which will become a
> full vignette at some point, but during the
> intermediate testing, I got errors from texi2dvi.
> This is what I have done.
> 
> 0) Using a Windows Xp system
> 1) Created a file called GeneSpring.Rnw
> 2) Convert this to Tex using Sweave("GeneSpring.Rnw")
> from within R
> 3) Since I don't have LATEX for windows, I used
> texi2dvi from the CYGWIN package
> 4) When I run the command in Cygwin 'texi2dvi -c -p
> GeneSpring.tex' I get the error shown below.
> 5) When I edit the GeneSpring.tex file and comment out
> the line
> 
> \usepackage{C:/PROGRA~1/SILICO~1/GENESP~1/data/rw1081/share/texmf/Sweave}

Well, cygwin misinterprets the path. Specify it in a way cygwin
understands, e.g.:
/cygdrive/c/.....
or even better, install a LaTeX environment for Windows.

Uwe Ligges




> and try to convert to PDF again, it will successfully
> convert the TEX file to PDF.
> 
> It seems that the Sweave library causes some problems
> in the parsing of the TEX file. Is there something I
> forgot to do?
> 
> Thanks,
> 
> Thon
> 
> ------------- OUTPUT FROM texi2dvi -----------------
> 
> /usr/local/bin:/usr/bin:/bin:/usr/X11R6/bin:/cygdrive/c/Perl/bin/:/cygdrive/c/WINDOWS/system32:/cygdrive/c/WINDOWS:/cygdrive/c/WINDOWS/System32/Wbem:/cygdrive/c/Program
> Files/ATI Technologies/ATI Control
> Panel:/cygdrive/c/Program Files/Common Files/Adaptec
> Shared/System:/cygdrive/c/Program
> Files/TEC100/BIN:/cygdrive/c/Program
> Files/bin:/cygdrive/c/Program
> Files/SiliconGenetics/GeneSpring/data/rw1081/bin
> This is pdfTeXk, Version 3.14159-1.10b (Web2C 7.4.5)
>  %&-line parsing enabled.
> (/tmp/GeneSpring.tex{/usr/share/texmf/pdftex/config/pdftex.cfg}
> LaTeX2e <2001/06/01>
> Babel <v3.7h> and hyphenation patterns for american,
> french, german, ngerman, n
> ohyphenation, loaded.
> (/usr/share/texmf/tex/latex/base/article.cls
> Document Class: article 2001/04/21 v1.4e Standard
> LaTeX document class
> (/usr/share/texmf/tex/latex/base/size10.clo))
> ! Missing \endcsname inserted.
> <to be read again>
>                    \protect
> l.9 \begin
>           {document}
> ?
> 
> ! LaTeX Error: Missing \begin{document}.
> 
> See the LaTeX manual or LaTeX Companion for
> explanation.
> Type  H <return>  for immediate help.
>  ...
> 
> l.9 \begin
>           {document}
> ?
> 
> -------------- TEX FILE ------------------
> 
> % \VignetteIndexEntry{GeneSpring contents}
> % \VignetteDepends{GeneSpring}
> % \VignetteKeywords{Interface}
> % \VignettePackage{GeneSpring}
> 
> \documentclass{article}
> 
> \usepackage{C:/PROGRA~1/SILICO~1/GENESP~1/data/rw1081/share/texmf/Sweave}
> \begin{document}
> 
> \author{Thon de Boer}
> 
> \title{Using R with GeneSpring}
> 
> \maketitle
> 
> \copyright{2003 Silicon Genetics}
> 
> \section{Introduction}
> 
> This package contains a number of functions to
> facilitate the integration
> of R code into the Gene Expression analysis program
> GeneSpring, by Silicon Genetics.
> Available functions include:
> 
> \begin{itemize}
> \item \texttt{GSload.int()} - Read GeneSpring
> experiment interpretation from file and return a
> GeneSpring gene expression object (GSint)
> \item \texttt{GSload.intBC()} - Read GeneSpring
> experiment interpretation from file and return a
> BioConductor gene expressioon object (exprSet)
> \item \texttt{GSload.exp()} - Read GeneSpring
> experiment from file and return a GeneSpring gene
> expression object (GSint)
> \item \texttt{GSload.expBC()} - Read GeneSpring
> experiment from file and return a BioConductor gene
> expressioon object (exprSet)
> \item \texttt{GSint()} - Create a GeneSpring gene
> expression object (GSint)
> \item \texttt{GSint2BC()} - Convert a GeneSpring gene
> expression object (GSint) to a BioConductor gene
> expression object (exprSet)
> \item \texttt{BC2GSint()} - Convert a BioConductor
> gene expression object (exprSet) to a GeneSpring gene
> expression object (GSint)
> \item \texttt{GSload.genelist()} - Read a GeneSpring
> GeneSpring gene list from file
> \item \texttt{GSsave.genelist()} - Save a GeneSpring
> GeneSpring gene list to file
> \item \texttt{GSsave.exp()} - Save a GeneSpring gene
> expression object (GSint) to file
> \end{itemize}
> 
> For more information on using thi spackage with
> GeneSpring, go to the Silicon Genetics website.
> 
> \end{document}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From hack at pedos.hr  Fri Dec 26 14:14:12 2003
From: hack at pedos.hr (Branimir K. Hackenberger)
Date: Fri, 26 Dec 2003 14:14:12 +0100
Subject: [R] Plot a sphere
Message-ID: <000001c3cbb2$27f12c40$3502a8c0@BranimirHackenberger>

To plot the sphere in R I find the best scatterplot3d package as
follows:

library(scatterplot3d)
a=seq(-pi,pi, length=100)
x=c(rep(1, 100) %*% t(cos(a)))
y=c(cos(a) %*% t(sin(a)))
z=c(sin(a) %*% t(sin(a)))
scatterplot3d(x, y, z, type="l")

Best regards

Branimir K. Hackenberger



From lefebure at univ-lyon1.fr  Fri Dec 26 14:34:16 2003
From: lefebure at univ-lyon1.fr (lefebure tristan)
Date: Fri, 26 Dec 2003 14:34:16 +0100
Subject: [R] loop and read.table
Message-ID: <200312261434.16958.lefebure@univ-lyon1.fr>

Hi,
I would like to open several tables with a loop, using something like :
-----------------
$ ls
1.txt		2.txt		3.txt		4.txt
$ R
> for (i in 1:4)  tabi<-read.table("i.txt")
Error in file(file, "r") : unable to open connection
In addition: Warning message:
cannot open file `i.txt'
------------------

thanks for any help

Tristan Lefebure



From ligges at statistik.uni-dortmund.de  Fri Dec 26 15:02:25 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 26 Dec 2003 15:02:25 +0100
Subject: [R] loop and read.table
In-Reply-To: <200312261434.16958.lefebure@univ-lyon1.fr>
References: <200312261434.16958.lefebure@univ-lyon1.fr>
Message-ID: <3FEC3F71.1030801@statistik.uni-dortmund.de>

lefebure tristan wrote:
> Hi,
> I would like to open several tables with a loop, using something like :
> -----------------
> $ ls
> 1.txt		2.txt		3.txt		4.txt
> $ R
> 
>>for (i in 1:4)  tabi<-read.table("i.txt")

Since i is within a character string, it cannot be used as a variable in 
your case. You may paste() is together: paste(i, ".txt", sep=""), or 
even better, read the directories contents with list.files() and proceed 
over the result as in:

  tab <- lapply(list.files(pattern="^?[[:digit:]]\.txt"), read.table)

Uwe Ligges

> Error in file(file, "r") : unable to open connection
> In addition: Warning message:
> cannot open file `i.txt'
> ------------------
> 
> thanks for any help
> 
> Tristan Lefebure
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Fri Dec 26 15:07:45 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 26 Dec 2003 14:07:45 +0000 (GMT)
Subject: [R] loop and read.table
In-Reply-To: <200312261434.16958.lefebure@univ-lyon1.fr>
Message-ID: <Pine.LNX.4.44.0312261406500.3215-100000@gannet.stats>

On Fri, 26 Dec 2003, lefebure tristan wrote:

> I would like to open several tables with a loop, using something like :
> -----------------
> $ ls
> 1.txt		2.txt		3.txt		4.txt
> $ R
> > for (i in 1:4)  tabi<-read.table("i.txt")

Use read.table(paste(i, "txt", sep="."))

> Error in file(file, "r") : unable to open connection
> In addition: Warning message:
> cannot open file `i.txt'
> ------------------

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From s.chamaille at wanadoo.fr  Fri Dec 26 16:09:47 2003
From: s.chamaille at wanadoo.fr (Simon CHAMAILLE)
Date: Fri, 26 Dec 2003 16:09:47 +0100 (CET)
Subject: [R] Multiple dependent variables
Message-ID: <21127081.1072451387990.JavaMail.www@wwinf0303>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031226/5ccc0e00/attachment.pl

From brahm at alum.mit.edu  Fri Dec 26 16:29:08 2003
From: brahm at alum.mit.edu (David Brahm)
Date: Fri, 26 Dec 2003 10:29:08 -0500
Subject: [R] Plot a sphere
References: <LAW11-F20i6QqbGHuy600066a2b@hotmail.com>
Message-ID: <16364.21444.324433.743202@arbres1a.fmr.com>

Derick Schoonbee <schoonbee at hotmail.com> wrote:
> Would somebody please be so kind as to direct me in plotting a 3D sphere?

Here's one way.  I generate an empty 3D plot with "persp", then fill it with
polygons transformed with "trans3d" (as found in the help for "persp").  I
didn't do hidden surface removal (you didn't mention whether you wanted it),
but if you do, just reorder the polygons from "back" to "front" and paint them
a solid color (e.g. col="red"), so hidden ones get painted over.

pmat <- persp(0:1, 0:1, matrix(,2,2), xlim=c(-1,1), ylim=c(-1,1), zlim=c(-1,1),
              theta=25, phi=30, expand=.9, xlab="X", ylab="Y", zlab="Z")

trans3d <- function(x,y,z, pmat) {                  # From the help for "persp"
  tr <- cbind(x,y,z,1) %*% pmat
  list(x = tr[,1]/tr[,4], y= tr[,2]/tr[,4])
}

theta <- seq(0, 2*pi, length=51)
phi   <- seq(0,   pi, length=26)
x <- cos(theta) %o% sin(phi)
y <- sin(theta) %o% sin(phi)
z <- rep(1, length(theta)) %o% cos(phi)

for (j in seq(phi)[-1]) for (i in seq(theta)[-1]) {
  idx <- rbind(c(i-1,j-1), c(i,j-1), c(i,j), c(i-1,j))
  polygon(trans3d(x[idx], y[idx], z[idx], pmat))
}
-- 
                              -- David Brahm (brahm at alum.mit.edu)



From ggrothendieck at myway.com  Fri Dec 26 16:37:03 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 26 Dec 2003 10:37:03 -0500 (EST)
Subject: [R] loop and read.table
Message-ID: <20031226153703.2CCE63950@mprdmxin.myway.com>



And if your intention is to create 4 data frames with
names tab1, tab2, tab3, tab4 then combine Prof. Riley's
advice with assign like this:

for(i in 1:4) 
   assign( paste("tab", i, sep=""), 
           read.table(paste(i, "txt", sep=".")) )

---
Date: Fri, 26 Dec 2003 14:07:45 +0000 (GMT) 
From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
To: lefebure tristan <lefebure at univ-lyon1.fr> 
Cc: <r-help at stat.math.ethz.ch> 
Subject: Re: [R] loop and read.table 

 
 
On Fri, 26 Dec 2003, lefebure tristan wrote:

> I would like to open several tables with a loop, using something like :
> -----------------
> $ ls
> 1.txt          2.txt          3.txt          4.txt
> $ R
> > for (i in 1:4) tabi<-read.table("i.txt")

Use read.table(paste(i, "txt", sep="."))

> Error in file(file, "r") : unable to open connection
> In addition: Warning message:
> cannot open file `i.txt'
> ------------------



From edd at debian.org  Fri Dec 26 16:53:35 2003
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 26 Dec 2003 09:53:35 -0600
Subject: [R] Problems converting output from Sweave to PDf
In-Reply-To: <3FEC22AE.68125EDA@statistik.uni-dortmund.de>
References: <20031226035600.48079.qmail@web41507.mail.yahoo.com>
	<3FEC22AE.68125EDA@statistik.uni-dortmund.de>
Message-ID: <20031226155335.GA7874@sonny.eddelbuettel.com>

On Fri, Dec 26, 2003 at 12:59:42PM +0100, Uwe Ligges wrote:
> Thon de Boer wrote:
> > I am having trouble converting the output from Sweave
> > into a valid PDF file.
> > 
> > I have created a simple .Rnw file which will become a
> > full vignette at some point, but during the
> > intermediate testing, I got errors from texi2dvi.
> > This is what I have done.
> > 
> > 0) Using a Windows Xp system
> > 1) Created a file called GeneSpring.Rnw
> > 2) Convert this to Tex using Sweave("GeneSpring.Rnw")
> > from within R
> > 3) Since I don't have LATEX for windows, I used
> > texi2dvi from the CYGWIN package
> > 4) When I run the command in Cygwin 'texi2dvi -c -p
> > GeneSpring.tex' I get the error shown below.
> > 5) When I edit the GeneSpring.tex file and comment out
> > the line
> > 
> > \usepackage{C:/PROGRA~1/SILICO~1/GENESP~1/data/rw1081/share/texmf/Sweave}
> 
> Well, cygwin misinterprets the path. Specify it in a way cygwin
> understands, e.g.:
> /cygdrive/c/.....
> or even better, install a LaTeX environment for Windows.

Why? Cygwin and Sweave get along very well. Here is a little utility script
I use:

nbkh85c at minot:~> cat bin/sweave 
#!/bin/bash -e

function errorexit () {
    echo "Error: $1"
    exit 1
}

function filetest () {
    if [ ! -f $1 ]; then
       errorexit "File $1 not found"
    fi
    return 0
}
			    
if [ "$#" -lt 1 ]; then
    errorexit "Need to specify argument file"
fi
				
BASENAME=$(basename $1 .Rnw)
				
RNWFILE=$BASENAME.Rnw
filetest $RNWFILE
echo "library(tools); Sweave(\"$RNWFILE\")" \
	    | Rterm --no-save --no-restore --slave
				    
LATEXFILE=$BASENAME.tex
filetest $LATEXFILE && pdflatex $LATEXFILE
				    
PDFFILE=$BASENAME.pdf
filetest $PDFFILE && acroread `pwd`/$PDFFILE &  



Note that it needs another three line script acroread which uses cygpath to
translate the path to the actual pdf file so that the windows binary can
grok it.

Hth, Dirk

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx



From swis at mantrade.com  Fri Dec 26 17:06:09 2003
From: swis at mantrade.com (Steve Wisdom)
Date: Fri, 26 Dec 2003 11:06:09 -0500
Subject: [R] re| Dr Ward on List protocol 
Message-ID: <MJEGJBLNGKKMFMAEBAIDEEIOCBAA.swis@mantrade.com>


"Andrew C. Ward" <acward at uqconnect.net.au> :

>With respect to 'tone' and 'friendliness', perhaps all that
>is meant or needed is that people be polite and respectful.

>I shake my head as often at rude answers

Oh, by gosh, by golly.

I don't think an occasional dose of 'real life', via a jab from the
Professor, will cause any lasting harm to the cosseted & emolumated students
and academics on the List.

On a Wall St trading desk, for example, every day one is kicked in the head
more brutally by clients, superiors, counterparts, the markets & etc, than
ever one would be by the Professor.

Plus, the Professor's jabs are good Schadenfreudic fun for the rest of us. .

Regards,

Steve Wisdom
Westport CT US

ps/ In the interest of full disclosure, I own four of the Professor's books



From p.dalgaard at biostat.ku.dk  Fri Dec 26 17:14:47 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 26 Dec 2003 17:14:47 +0100
Subject: [R] Multiple dependent variables
In-Reply-To: <21127081.1072451387990.JavaMail.www@wwinf0303>
References: <21127081.1072451387990.JavaMail.www@wwinf0303>
Message-ID: <x2smj7y2rs.fsf@biostat.ku.dk>

Simon CHAMAILLE <s.chamaille at wanadoo.fr> writes:

> Dear friends,
> I'm stuck with the following problem:
> I would like to do a multiple regression with muliple dependent variables, and i don't know how to write my formula in the model. Someone in previous messages offered the cbind method, but the result is just as many regression as the number of dependent variables, it is just a time saving method; i'm looking for a method closer to a MANOVA, but without factors. well, a standard multiple rgression with multiple dependent variable, i.e. looking for variables that are significant in all the dependent variables.
> If one of you have an idea, I welcome it.
> and happy new year to all
> simon

Some of this stuff appears to be not quite implemented, but have a
look at

> summary(manova(cbind(y1,y2)~x1+x2))
          Df   Pillai approx F num Df den Df Pr(>F)
x1         1 0.079014 0.257378      2      6 0.7812
x2         1 0.006782 0.020485      2      6 0.9798
Residuals  7
> summary(manova(cbind(y1,y2)~x2+x1))
          Df   Pillai approx F num Df den Df Pr(>F)
x2         1 0.006930 0.020936      2      6 0.9794
x1         1 0.078886 0.256927      2      6 0.7815

The annoying thing is that drop1 and friends are not there, so you
have to arrange terms so that the sequential tests make sense.
However, all the bits are clearly there.

Another annoying thing is that manova() is not analogous to anova().

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From rolf at math.unb.ca  Fri Dec 26 17:34:57 2003
From: rolf at math.unb.ca (Rolf Turner)
Date: Fri, 26 Dec 2003 12:34:57 -0400 (AST)
Subject: [R] re| Dr Ward on List protocol
Message-ID: <200312261634.hBQGYvHJ009586@erdos.math.unb.ca>

Steve Wisdom wrote:

> I don't think an occasional dose of 'real life', via a jab from the
> Professor, will cause any lasting harm to the cosseted & emolumated
> students and academics on the List.
> 
> On a Wall St trading desk, for example, every day one is kicked in
> the head more brutally by clients, superiors, counterparts, the
> markets & etc, than ever one would be by the Professor.
> 
> Plus, the Professor's jabs are good Schadenfreudic fun for the rest
> of us.

Uhhhh, there's more than ***one*** rude professor who contributes to
the list.

Although I play in a very minor league academically, I like to think
that my capacity for rudeness is right up there with the best of
them! :-)

					cheers,

						Rolf Turner
						rolf at math.unb.ca

P. S.  My New Oxford Dictionary lists the word ``emolument'' as a noun,
but does not list a verb ``emolumate''.  (???)

						R. T.



From p.gamble at zeria.com  Fri Dec 26 18:01:56 2003
From: p.gamble at zeria.com (Patrick Gamble)
Date: Fri, 26 Dec 2003 17:01:56 -0000
Subject: [R] Dr Ward on List protocol
References: <200312261634.hBQGYvHJ009586@erdos.math.unb.ca>
Message-ID: <001c01c3cbd1$f7e5ecf0$2301a8c0@home46b07kcgr8>

Shouldn't posters who do not do obvious research before asking their
questions have this forcefully pointed out to them? I think they should.
It may put people off posting, but it may also make them do the work.

As a longtime lurker who pays a lot of money for good statistical advice,
I have an idea of what it would cost in the real world to have questions 
answered by the people on this list, though the money is not the only 
measure, I know.

So it's not a resource that should be treated too lightly, and I think the 
onus should be on the questioners rather than the respondents to take this
on board.

Seems to me the list has worked pretty well so far, and I frankly don't see 
the need for a change.



From wildscop at yahoo.com  Fri Dec 26 19:21:08 2003
From: wildscop at yahoo.com (WilDscOp)
Date: Sat, 27 Dec 2003 00:21:08 +0600
Subject: [R] Re: OC curve in "Quality Control"
Message-ID: <5.1.0.14.2.20031226235826.00a33b70@127.0.0.1>

Dear All,

First of all, thanks to "Philippe Glaziou" and "Andrew C. Ward" for their 
nice and very helpful e-mail. Well..
----------------- ---------------------------
For Number 1 problem: about <http://pari.math.u-bordeaux.fr/>, sorry i 
didnot understand what to download or what to call! From 
<http://pari.math.u-bordeaux.fr/download.html> can you please tell me what 
should i download and how to use(i could not find any doc)? An exact link 
for download may be helpful:)
----------------- ---------------------------
For Number 2 problem: perm function is a very nice one. And yes "choose()" 
works fine for me. Also gregmisc package 
<http://cran.r-project.org/bin/windows/contrib/1.9/gregmisc_0.8.7.zip> is good.
----------------- ---------------------------
For Number 3 problem: i had some problem using dhyper() because i cannot 
match the parameters there. I used
 > N<-5000; n<-50; D<-seq(0,500,1); d<-0 # (For d = 1 i just recalculated 
it with d<-1)
 > hypg <- (choose(D, d)* choose((N-D), (n-d)) / choose(N,n))
instead and it surves my purpose. Anyway, is there any better procedure? I 
could not find Hypergeometric() in the base.
----------------- ---------------------------
For Number 5 problem: I did as follows:
# For type A OC curve
 > N<-5000; n<-50; D<-seq(1,500,1); d1<-0; d2<-1; p<-(seq(1,500,1)/5000); 
Pa<-((choose(D, d1)* choose((N-D), (n-d1)) / choose(N,n)) + (choose(D, d2)* 
choose((N-D), (n-d2)) / choose(N,n))); plot(p,Pa,pch="o", main="Type A 
\nOperating Characteristic Curve", sub="Using Hypergeometric Distribution", 
xlab="Proportion defective", ylab="Probability of Acceptance", 
xlim=c(0.00,0.10), ylim=c(0.00,1.00), type="p", axes =T, col=1) # here, p = D/N
# For type B OC curve
 > N<-5000; n<-50; D<-seq(0,500,1); d1<-0; d2<-1; p<-(seq(0,500,1)/5000); 
Pa1<- dpois(0, (n*p), log = FALSE); Pa2<- dpois(1, (n*p), log = FALSE); 
Pa<-Pa1+Pa2;  plot(p,Pa,pch="o", main="Type B \nOperating Characteristic 
Curve", sub="Using Poisson as an approximation of Binomial Distribution", 
xlab="Proportion defective", ylab="Probability of Acceptance", 
xlim=c(0.00,0.10), ylim=c(0.00,1.00), type="p", axes =T, col=1)
And these works fine.

Thank you for your time.


_______________________

Mohammad Ehsanul Karim <wildscop at yahoo.com>
Institute of Statistical Research and Training
University of Dhaka, Dhaka- 1000, Bangladesh



From tlumley at u.washington.edu  Fri Dec 26 19:20:49 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 26 Dec 2003 10:20:49 -0800 (PST)
Subject: [R] draft of posting guide
In-Reply-To: <1072170818.3fe807427b48b@webmail.unibas.ch>
References: <74E242B6968AA0469B632C5A3EFC1EFD03D57022@nt207mesep.corporate.hdwa.health.wa.gov.au>
	<1072170818.3fe807427b48b@webmail.unibas.ch>
Message-ID: <Pine.A41.4.58.0312261017260.35014@homer30.u.washington.edu>


Rather than a separate beginners' mailing list or a posting guide, perhaps
what we need is a separate mailing list for discussing posting style?

	-thomas



From csoares at liacc.up.pt  Fri Dec 26 20:32:42 2003
From: csoares at liacc.up.pt (Carlos Soares)
Date: Fri, 26 Dec 2003 19:32:42 +0000
Subject: [R] different results by re-ordering vector: bug?
Message-ID: <3FEC8CDA.7050800@liacc.up.pt>

Dear R users,

I obtain the following behavior which I cannot understand. Given a file 
mydata with the following numbers:
0.171409662475182
0.15817339510258108
0.32230311052283256
0.14890800794176043
0.17074784910655194
0.16611515552614162
0.41
0.16611515552614162
0.41760423560555926
0.11978821972203839

I read the data and perform some calculations:
a <- 1-read.table("mydata")$V1
m <- outer(a, a, "/")
diag(m) <- NA
mean.row <- apply(m, 1, mean, na.rm=TRUE)

which yield the same value for indices 6 and 8 of mean.row, as would be 
expected because values 6 and 8 of the original vector are the same:
 > mean.row[6]==mean.row[8]
[1] TRUE

However, if I reorder the values as follows:
a <- 1-read.table("mydata")$V1[c(10,2,8,9,7,3,1,4,5,6)]

and repeat the calculations:
m <- outer(a, a, "/")
diag(m) <- NA
mean.row <- apply(m, 1, mean, na.rm=TRUE)
mean.row[6]==mean.row[8]

The values for indices 10 and 3 of mean.row, which correspond to 6 and 8 
in the previous calculations, are not the same anymore:
 > mean.row[10]==mean.row[3]
[1] FALSE

I understand that limited precision causes "incorrect" results but I 
wouldn't expect ordering operations to do the same. I couldn't find any 
information in the site about this. Maybe it's a bug with my version:
 > R.version
         _               
platform i686-pc-linux-gnu
arch     i686            
os       linux-gnu       
system   i686, linux-gnu 
status                   
major    1               
minor    7.0             
year     2003            
month    04              
day      16              
language R               

Thanks and best regards,
Carlos



From spencer.graves at pdf.com  Fri Dec 26 22:03:09 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 26 Dec 2003 13:03:09 -0800
Subject: [R] different results by re-ordering vector: bug?
In-Reply-To: <3FEC8CDA.7050800@liacc.up.pt>
References: <3FEC8CDA.7050800@liacc.up.pt>
Message-ID: <3FECA20D.20508@pdf.com>

      I just got the same answer both times with R 1.8.1 under Windows 
2000 on an IBM Thinkpad T30.  However, if abs(diff(mean.row[c(3,10)])) 
is not much bigger than .Machine$double.esp*sum(abs(mean.row[c(3,10)])), 
then I would not call that a bug.  Rather, it should be considered a 
warning not to expect exact equality in comparing floating point numbers

      Just now, I checked (pi+x-x) == 4*atan(1) and got TRUE for both in 
R 1.8.1 and S-Plus 2000 with x = 1 but FALSE with x = 100. 

      hope this helps. 
      spencer graves

Carlos Soares wrote:

> Dear R users,
>
> I obtain the following behavior which I cannot understand. Given a 
> file mydata with the following numbers:
> 0.171409662475182
> 0.15817339510258108
> 0.32230311052283256
> 0.14890800794176043
> 0.17074784910655194
> 0.16611515552614162
> 0.41
> 0.16611515552614162
> 0.41760423560555926
> 0.11978821972203839
>
> I read the data and perform some calculations:
> a <- 1-read.table("mydata")$V1
> m <- outer(a, a, "/")
> diag(m) <- NA
> mean.row <- apply(m, 1, mean, na.rm=TRUE)
>
> which yield the same value for indices 6 and 8 of mean.row, as would 
> be expected because values 6 and 8 of the original vector are the same:
> > mean.row[6]==mean.row[8]
> [1] TRUE
>
> However, if I reorder the values as follows:
> a <- 1-read.table("mydata")$V1[c(10,2,8,9,7,3,1,4,5,6)]
>
> and repeat the calculations:
> m <- outer(a, a, "/")
> diag(m) <- NA
> mean.row <- apply(m, 1, mean, na.rm=TRUE)
> mean.row[6]==mean.row[8]
>
> The values for indices 10 and 3 of mean.row, which correspond to 6 and 
> 8 in the previous calculations, are not the same anymore:
> > mean.row[10]==mean.row[3]
> [1] FALSE
>
> I understand that limited precision causes "incorrect" results but I 
> wouldn't expect ordering operations to do the same. I couldn't find 
> any information in the site about this. Maybe it's a bug with my version:
> > R.version
>         _               platform i686-pc-linux-gnu
> arch     i686            os       linux-gnu       system   i686, 
> linux-gnu status                   major    1               minor    
> 7.0             year     2003            month    04              
> day      16              language R              
> Thanks and best regards,
> Carlos
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From glaziou at pasteur-kh.org  Sat Dec 27 03:10:21 2003
From: glaziou at pasteur-kh.org (Philippe Glaziou)
Date: Sat, 27 Dec 2003 09:10:21 +0700
Subject: [R] Re: OC curve in "Quality Control"
In-Reply-To: <5.1.0.14.2.20031226235826.00a33b70@127.0.0.1>
References: <5.1.0.14.2.20031226235826.00a33b70@127.0.0.1>
Message-ID: <20031227021021.GB2221@pasteur-kh.org>

WilDscOp <wildscop at yahoo.com> wrote:
> For Number 1 problem: about <http://pari.math.u-bordeaux.fr/>, sorry i 
> didnot understand what to download or what to call! From 
> <http://pari.math.u-bordeaux.fr/download.html> can you please tell me what 
> should i download and how to use(i could not find any doc)? An exact link 
> for download may be helpful:)


The download page gives links to the sources of PARI.  There is
also a self-installing MS-Windows executable there. As for the
doc, there is a menu item which reads "Documentation".  Click on
it, and there you have a user's guide, tutorial, installation
guide, ref card, manual pages. By the way, there are other
packages out there to handle large entities (on a 32-bit machine)
like 500! (Octave, Matlab, Maple, Mathematica, Yacas...). I
suggested that one because I do not know the others. 



> ----------------- ---------------------------
> For Number 3 problem: i had some problem using dhyper() because i cannot 
> match the parameters there. I used
> > N<-5000; n<-50; D<-seq(0,500,1); d<-0 # (For d = 1 i just recalculated 
> it with d<-1)
> > hypg <- (choose(D, d)* choose((N-D), (n-d)) / choose(N,n))
> instead and it surves my purpose. Anyway, is there any better procedure? I 
> could not find Hypergeometric() in the base.


You needed phyper for that specific problem, not dhyper (the help
page for dhyper explains what dhyper, phyper, qhyper and rhyper
do).

hypg <- phyper(d,D,N-D,n) 

does the job in a slightly more efficient manner. 

I would suggest that you check out the following:

help.search("hypergeometric")
?Hypergeometric 


-- 
Philippe Glaziou
Epidemiologist



From schoonbee at hotmail.com  Sat Dec 27 10:09:48 2003
From: schoonbee at hotmail.com (Derick Schoonbee)
Date: Sat, 27 Dec 2003 09:09:48 +0000
Subject: [R] Plot a sphere
Message-ID: <Law11-F39QZiCTSrdUW00007fed@hotmail.com>

Hi,

Thank you all for the solutions.. I now have 3 ways of plotting a 'perfect' 
sphere :)

1> rgl
2> scatterplot3d
3> persp (with polygon) as below.

Personally I like the transformation solution as in 3. Thanks a mill.

Derick Schoonbee



From: David Brahm  <brahm at alum.mit.edu>
Reply-To: brahm at alum.mit.edu
To: r-help at stat.math.ethz.ch
CC: schoonbee at hotmail.com
Subject: Re: [R] Plot a sphere
Date: Fri, 26 Dec 2003 10:29:08 -0500

Derick Schoonbee <schoonbee at hotmail.com> wrote:
 > Would somebody please be so kind as to direct me in plotting a 3D sphere?

Here's one way.  I generate an empty 3D plot with "persp", then fill it with
polygons transformed with "trans3d" (as found in the help for "persp").  I
didn't do hidden surface removal (you didn't mention whether you wanted it),
but if you do, just reorder the polygons from "back" to "front" and paint 
them
a solid color (e.g. col="red"), so hidden ones get painted over.

pmat <- persp(0:1, 0:1, matrix(,2,2), xlim=c(-1,1), ylim=c(-1,1), 
zlim=c(-1,1),
               theta=25, phi=30, expand=.9, xlab="X", ylab="Y", zlab="Z")

trans3d <- function(x,y,z, pmat) {                  # From the help for 
"persp"
   tr <- cbind(x,y,z,1) %*% pmat
   list(x = tr[,1]/tr[,4], y= tr[,2]/tr[,4])
}

theta <- seq(0, 2*pi, length=51)
phi   <- seq(0,   pi, length=26)
x <- cos(theta) %o% sin(phi)
y <- sin(theta) %o% sin(phi)
z <- rep(1, length(theta)) %o% cos(phi)

for (j in seq(phi)[-1]) for (i in seq(theta)[-1]) {
   idx <- rbind(c(i-1,j-1), c(i,j-1), c(i,j), c(i-1,j))
   polygon(trans3d(x[idx], y[idx], z[idx], pmat))
}
--
                               -- David Brahm (brahm at alum.mit.edu)



From aisha_olu at yahoo.com  Sat Dec 27 14:16:59 2003
From: aisha_olu at yahoo.com (Aisha olu)
Date: Sat, 27 Dec 2003 05:16:59 -0800 (PST)
Subject: [R] i am looking for modelling job
Message-ID: <20031227131659.62009.qmail@web20422.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031227/c046f40c/attachment.pl

From rolf at math.unb.ca  Sat Dec 27 16:42:37 2003
From: rolf at math.unb.ca (Rolf Turner)
Date: Sat, 27 Dec 2003 11:42:37 -0400 (AST)
Subject: [R] List protocol; draft of guidlines, etc.
Message-ID: <200312271542.hBRFgbsM001288@erdos.math.unb.ca>


IMHO Patrick Gamble's recent posting on this subject (``[R] Dr Ward
on List protocol'') was very cogently expressed.

					cheers,

						Rolf Turner
						rolf at math.unb.ca



From tlumley at u.washington.edu  Sat Dec 27 18:19:14 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Sat, 27 Dec 2003 09:19:14 -0800 (PST)
Subject: [R] different results by re-ordering vector: bug?
In-Reply-To: <3FEC8CDA.7050800@liacc.up.pt>
References: <3FEC8CDA.7050800@liacc.up.pt>
Message-ID: <Pine.A41.4.58.0312270914330.152198@homer01.u.washington.edu>

On Fri, 26 Dec 2003, Carlos Soares wrote:

> The values for indices 10 and 3 of mean.row, which correspond to 6 and 8
> in the previous calculations, are not the same anymore:
>  > mean.row[10]==mean.row[3]
> [1] FALSE
>
> I understand that limited precision causes "incorrect" results but I
> wouldn't expect ordering operations to do the same. I couldn't find any
> information in the site about this. Maybe it's a bug with my version:

Almost certainly not.

The first thing to do is to see how big the difference is, rather than
comparing for equality.
mean.row[10]-mean.row[3].

You will find that it's about the same size as .Machine$double.eps, in
which case you just have an example of the fact that two floating point
numbers computed from different expressions are not reliably equal.
There's nothing puzzling about the fact that you just reordered the
numbers; floating point addition is not associative.


	-thomas



From gguigon at pasteur.fr  Mon Dec 29 15:13:48 2003
From: gguigon at pasteur.fr (Ghislaine Guigon)
Date: Mon, 29 Dec 2003 15:13:48 +0100
Subject: [R] Update
Message-ID: <5.0.2.1.2.20031229151251.00b7e9b8@mail.pasteur.fr>


   How do you update R on Windows and how do you install your own package
   with windows interface ?
   regards,

   Ghislaine GUIGON
   Biostatisticienne
   Plate-forme 2 Puces a ADN
   INSTITUT PASTEUR
   25-28 rue du Dr ROUX
   75724 Paris cedex 15
   FRANCE
   tel: (33) (0)1 40 61 86 51
   fax: (33) (0)1 45 68 84 06


From wolski at molgen.mpg.de  Mon Dec 29 15:27:45 2003
From: wolski at molgen.mpg.de (wolski)
Date: Mon, 29 Dec 2003 15:27:45 +0100
Subject: [R] Update
In-Reply-To: <5.0.2.1.2.20031229151251.00b7e9b8@mail.pasteur.fr>
References: <5.0.2.1.2.20031229151251.00b7e9b8@mail.pasteur.fr>
Message-ID: <200312291527450545.011B7529@harry.molgen.mpg.de>

Hi!
I make a complete Full Install with the newest setup.exe. I install also all packages again.
About own packages see the Win FAQ on cran for a start.

Eryk

*********** REPLY SEPARATOR  ***********

On 12/29/2003 at 3:13 PM Ghislaine Guigon wrote:

>How do you update R on Windows and how do you install your own package
>   with windows interface ?
>   regards,
>
>   Ghislaine GUIGON
>   Biostatisticienne
>   Plate-forme 2 Puces a ADN
>   INSTITUT PASTEUR
>   25-28 rue du Dr ROUX
>   75724 Paris cedex 15
>   FRANCE
>   tel: (33) (0)1 40 61 86 51
>   fax: (33) (0)1 45 68 84 06
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ligges at statistik.uni-dortmund.de  Mon Dec 29 15:42:37 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 29 Dec 2003 15:42:37 +0100
Subject: [R] Update
In-Reply-To: <5.0.2.1.2.20031229151251.00b7e9b8@mail.pasteur.fr>
References: <5.0.2.1.2.20031229151251.00b7e9b8@mail.pasteur.fr>
Message-ID: <3FF03D5D.1000507@statistik.uni-dortmund.de>

Ghislaine Guigon wrote:

>    How do you update R on Windows 

See Section "2.6 What's the best way to upgrade?" in the R for Windows FAQ.

 > and how do you install your own package
>    with windows interface ?

You can only install binary packages "with the Windows interface", i.e. 
using install.packages() or the GUI.

You told us it is your "own package", so I assume it's a source package.

The R for Windows FAQ, Section "3.1 Can I install packages (libraries) 
in this version?", tells you to look in file "readme.packages" for 
details on the installation of source packages.

Uwe Ligges


>    regards,
> 
>    Ghislaine GUIGON
>    Biostatisticienne
>    Plate-forme 2 Puces a ADN
>    INSTITUT PASTEUR
>    25-28 rue du Dr ROUX
>    75724 Paris cedex 15
>    FRANCE
>    tel: (33) (0)1 40 61 86 51
>    fax: (33) (0)1 45 68 84 06
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From sara.good-avila at acadiau.ca  Mon Dec 29 17:40:00 2003
From: sara.good-avila at acadiau.ca (Sara Good-Avila)
Date: Mon, 29 Dec 2003 12:40:00 -0400
Subject: [R] installing packages on MAC os X
Message-ID: <E607FA67F85FD311A7D400A024B226E10D6DC591@exch.acadiau.ca>


Hello,

I've just downloaded and installed the RAqua onto my Mac and I have R 
up and running fine.  Now I'm trying to get 'ape' downloaded and 
installed and having quite the difficulty.  I am following the 
directions specified under 'Installing packages' where packages can be 
downloaded and installed from within R.  I get the following error 
messages:

1: argument 'lib' is missing: using ~/Library/RAqua/library in: 
install.packages("ape")
2: Installation of package ape had non-zero exit status in: 
install.packages("ape")

Can you please advise on what I am doing wrong?

thanks!

Sara Good-Avila

Sara V. Good-Avila,  http://ace.acadiau.ca/~savila/webpage/
Department of Biology Acadia University Wolfville, Nova Scotia Canada, B4P
2R6 Phone: (902) 585-1798 Fax -1059



From tlumley at u.washington.edu  Mon Dec 29 19:03:10 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 29 Dec 2003 10:03:10 -0800 (PST)
Subject: [R] installing packages on MAC os X
In-Reply-To: <E607FA67F85FD311A7D400A024B226E10D6DC591@exch.acadiau.ca>
References: <E607FA67F85FD311A7D400A024B226E10D6DC591@exch.acadiau.ca>
Message-ID: <Pine.A41.4.58.0312290958370.103706@homer05.u.washington.edu>

On Mon, 29 Dec 2003, Sara Good-Avila wrote:

>
> Hello,
>
> I've just downloaded and installed the RAqua onto my Mac and I have R
> up and running fine.  Now I'm trying to get 'ape' downloaded and
> installed and having quite the difficulty.  I am following the
> directions specified under 'Installing packages' where packages can be
> downloaded and installed from within R.

It would help if you said what you actually did -- I'm not sure which
instructions you mean.

>						 I get the following error
> messages:

These are actually warning messages, not error messages

> 1: argument 'lib' is missing: using ~/Library/RAqua/library in:
> install.packages("ape")

This one is unsightly but harmless (it will go away in the next version)

> 2: Installation of package ape had non-zero exit status in:
> install.packages("ape")

This is the problem, but it doesn't say what the cause is.  Try
again, but this time also open a `Console' window (look under Applications
then Utilities in the Finder).  This will show the actual output from the
installation, and should give a more informative error message (to us,
even if not necessarily to you).

Installation of ape works for me in both binary and source forms, so
there's something about your setup that is responsible.


	-thomas



From den.duurs at lycos.com  Mon Dec 29 20:08:24 2003
From: den.duurs at lycos.com (Remko Duursma)
Date: Mon, 29 Dec 2003 11:08:24 -0800
Subject: [R] Rcmd check package failure
Message-ID: <AIKKOGFHMMPFAFAA@mailcity.com>

Hi all,

recently, i tried making a package with my own functions - which worked fine, until i reinstalled MikTex (in Win2000). Now i get the error message:

* checking for working latex ... NO
* using log directory 'C:/Rlibs/R4PG.Rcheck'
* checking for file 'R4PG/DESCRIPTION' ... OK
* checking if this is a source package ... OK

 ERROR
Installation failed.

But i set the path to MikTex, and "latex" as well as "pdftex" both run in the (Win) command window.
I use R1.81 but also tried in R1.71.

Any ideas?


Remko

^'~,_,~'^'~,_,~'^'~,_,~'^'~,_,~'^'~,_,~'^'~,_,~'
Remko Duursma, Ph.D. student
Forest Biometrics Lab / Idaho Stable Isotope Lab
University of Idaho, Moscow, ID, U.S.A.



From ripley at stats.ox.ac.uk  Mon Dec 29 20:41:50 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 29 Dec 2003 19:41:50 +0000 (GMT)
Subject: [R] Rcmd check package failure
In-Reply-To: <AIKKOGFHMMPFAFAA@mailcity.com>
Message-ID: <Pine.LNX.4.44.0312291938390.6719-100000@gannet.stats>

On Mon, 29 Dec 2003, Remko Duursma wrote:

> recently, i tried making a package with my own functions - which worked
> fine, until i reinstalled MikTex (in Win2000). Now i get the error
> message:
> 
> * checking for working latex ... NO
> * using log directory 'C:/Rlibs/R4PG.Rcheck'
> * checking for file 'R4PG/DESCRIPTION' ... OK
> * checking if this is a source package ... OK

Was there nothing here at all?

>  ERROR
> Installation failed.

That's an installation failure.  Do run Rcmd INSTALL first to check that 
you can install the package: you may well get more informative error 
messages.

> But i set the path to MikTex, and "latex" as well as "pdftex" both run in the (Win) command window.
> I use R1.81 but also tried in R1.71.
> 
> Any ideas?

Look again, more carefully.  According to `check' you do not have latex in
your path, even if you think you do. But the installation failure has some
other cause.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From den.duurs at lycos.com  Mon Dec 29 21:16:00 2003
From: den.duurs at lycos.com (Remko Duursma)
Date: Mon, 29 Dec 2003 12:16:00 -0800
Subject: [R] Rcmd check package failure
Message-ID: <DOFAPCOMMHDGAFAA@mailcity.com>


Ok, here is the complete output. MikTex is really in the path - i doublechecked by running it in c:\Rlibs (both "latex" and "pdftex" work). I get no additional error messages, and i use the example from ?package.skeleton (and updated the DESCRIPTION file).

[[WIN2000 command Prompt]]:

C:\Rlibs>path
PATH=c:\Program files\R\rw1081\bin;d:\perl\bin;c:\Program files\HTML Help Worksh
op;c:\texmf\miktex\bin

C:\Rlibs>Rcmd check AnExample
* checking for working latex ... NO
* using log directory 'C:/Rlibs/AnExample.Rcheck'
* checking for file 'AnExample/DESCRIPTION' ... OK
* checking if this is a source package ... OK

 ERROR
Installation failed.

C:\Rlibs>Rcmd install AnExample

*** Installation of AnExample failed ***

C:\Rlibs>


thanks,

Remko
---

^'~,_,~'^'~,_,~'^'~,_,~'^'~,_,~'^'~,_,~'^'~,_,~'
Remko Duursma, Ph.D. student
Forest Biometrics Lab / Idaho Stable Isotope Lab
University of Idaho, Moscow, ID, U.S.A.

--------- Original Message ---------

DATE: Mon, 29 Dec 2003 19:41:50
From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
To: Remko Duursma <den.duurs at lycos.com>
Cc: rhelp <r-help at r-project.org>

>On Mon, 29 Dec 2003, Remko Duursma wrote:
>
>> recently, i tried making a package with my own functions - which worked
>> fine, until i reinstalled MikTex (in Win2000). Now i get the error
>> message:
>> 
>> * checking for working latex ... NO
>> * using log directory 'C:/Rlibs/R4PG.Rcheck'
>> * checking for file 'R4PG/DESCRIPTION' ... OK
>> * checking if this is a source package ... OK
>
>Was there nothing here at all?
>
>>  ERROR
>> Installation failed.
>
>That's an installation failure.  Do run Rcmd INSTALL first to check that 
>you can install the package: you may well get more informative error 
>messages.
>
>> But i set the path to MikTex, and "latex" as well as "pdftex" both run in the (Win) command window.
>> I use R1.81 but also tried in R1.71.
>> 
>> Any ideas?
>
>Look again, more carefully.  According to `check' you do not have latex in
>your path, even if you think you do. But the installation failure has some
>other cause.
>
>-- 
>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>University of Oxford,             Tel:  +44 1865 272861 (self)
>1 South Parks Road,                     +44 1865 272866 (PA)
>Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
>



From tvandaelen at scitegic.com  Tue Dec 30 01:21:15 2003
From: tvandaelen at scitegic.com (Ton van Daelen)
Date: Mon, 29 Dec 2003 16:21:15 -0800
Subject: [R] Writing data frames
Message-ID: <830D8D4719112B418ABBC3A0EBA95812114335@webmail.scitegic.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031229/a26bee5e/attachment.pl

From abunn at montana.edu  Tue Dec 30 01:44:27 2003
From: abunn at montana.edu (Andy Bunn)
Date: Mon, 29 Dec 2003 17:44:27 -0700
Subject: [R] Writing data frames
In-Reply-To: <830D8D4719112B418ABBC3A0EBA95812114335@webmail.scitegic.com>
Message-ID: <000101c3ce6e$262f3be0$78f05a99@msu.montana.edu>

The key is in the error message:
"...can't coerce array into a data.frame"

Even if as.data.frame.default is unhappy you can coerce m into a
data.frame.

write.table(data.frame(m1 = m[1], m2 = m[2]), file="C:\\R\\tst.txt",
col.names=T, row.names=F, quote=F, append = FALSE)

There's probably a nicer way to do the coercion but this works.

HTH, Andy



From maj at stats.waikato.ac.nz  Tue Dec 30 02:11:39 2003
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Tue, 30 Dec 2003 14:11:39 +1300
Subject: [R] Assignments in loops
Message-ID: <E1Ab8S5-0006E1-00@newton.math.waikato.ac.nz>

Greetings all. Any help with the following would be appreciated.

I want to create a data frame for each file in a directory. The following
code does not work but it may show what I am trying to do:

carmakes <- c('BMW','Chrysler','Citroen','Fiat','Ford','Holden','Honda',
'Mercedes','MG','Mitsubishi','Nissan','Peugeot','Renault','Subaru','Toyota',
'VW')
for (brand in carmakes) {
   fyle <- paste("c:/data/cars03/",brand,".txt",sep="")
   brand <- read.table(fyle, header = TRUE, sep = "\t",na.strings = 
            c("-","POA"), colClasses=c("character",rep("numeric",7)),
             comment.char = "#")
}

I need something like an unquote() function that will allow the "brand" on
the LHS of the assignment to be treated as an object name instead of a
character string.

Murray






Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862



From rpeng at jhsph.edu  Tue Dec 30 02:26:19 2003
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Mon, 29 Dec 2003 20:26:19 -0500
Subject: [R] Assignments in loops
In-Reply-To: <E1Ab8S5-0006E1-00@newton.math.waikato.ac.nz>
References: <E1Ab8S5-0006E1-00@newton.math.waikato.ac.nz>
Message-ID: <3FF0D43B.9010300@jhsph.edu>

How about

for( ) {
	assign(brand, read.table(......))
}

?

-roger

Murray Jorgensen wrote:
> Greetings all. Any help with the following would be appreciated.
> 
> I want to create a data frame for each file in a directory. The following
> code does not work but it may show what I am trying to do:
> 
> carmakes <- c('BMW','Chrysler','Citroen','Fiat','Ford','Holden','Honda',
> 'Mercedes','MG','Mitsubishi','Nissan','Peugeot','Renault','Subaru','Toyota',
> 'VW')
> for (brand in carmakes) {
>    fyle <- paste("c:/data/cars03/",brand,".txt",sep="")
>    brand <- read.table(fyle, header = TRUE, sep = "\t",na.strings = 
>             c("-","POA"), colClasses=c("character",rep("numeric",7)),
>              comment.char = "#")
> }
> 
> I need something like an unquote() function that will allow the "brand" on
> the LHS of the assignment to be treated as an object name instead of a
> character string.
> 
> Murray
> 
> 
> 
> 
> 
> 
> Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
> Department of Statistics, University of Waikato, Hamilton, New Zealand
> Email: maj at waikato.ac.nz                                Fax 7 838 4155
> Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From p.dalgaard at biostat.ku.dk  Tue Dec 30 02:29:10 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Dec 2003 02:29:10 +0100
Subject: [R] Assignments in loops
In-Reply-To: <E1Ab8S5-0006E1-00@newton.math.waikato.ac.nz>
References: <E1Ab8S5-0006E1-00@newton.math.waikato.ac.nz>
Message-ID: <x2k74fnleh.fsf@biostat.ku.dk>

Murray Jorgensen <maj at stats.waikato.ac.nz> writes:

> Greetings all. Any help with the following would be appreciated.
> 
> I want to create a data frame for each file in a directory. The following
> code does not work but it may show what I am trying to do:
> 
> carmakes <- c('BMW','Chrysler','Citroen','Fiat','Ford','Holden','Honda',
> 'Mercedes','MG','Mitsubishi','Nissan','Peugeot','Renault','Subaru','Toyota',
> 'VW')
> for (brand in carmakes) {
>    fyle <- paste("c:/data/cars03/",brand,".txt",sep="")
>    brand <- read.table(fyle, header = TRUE, sep = "\t",na.strings = 
>             c("-","POA"), colClasses=c("character",rep("numeric",7)),
>              comment.char = "#")
> }
> 
> I need something like an unquote() function that will allow the "brand" on
> the LHS of the assignment to be treated as an object name instead of a
> character string.
> 
> Murray

assign(brand, read.table(blablabla)) 

(Overlooked by many, but I would have expected that you'd been around
for long enough to have noticed it....)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From abunn at montana.edu  Tue Dec 30 02:33:59 2003
From: abunn at montana.edu (Andy Bunn)
Date: Mon, 29 Dec 2003 18:33:59 -0700
Subject: [R] Assignments in loops
In-Reply-To: <E1Ab8S5-0006E1-00@newton.math.waikato.ac.nz>
Message-ID: <000201c3ce75$11372cf0$78f05a99@msu.montana.edu>

Try assign - a la the subject heading.

I think this should work.

carmakes <- c('BMW','Chrysler','Citroen','Fiat','Ford','Holden','Honda',
 
'Mercedes','MG','Mitsubishi','Nissan','Peugeot','Renault','Subaru','Toyo
ta','VW')
for (i in 1:length(carmakes)) {
   # Create your path and file name
   fyle <- paste("c:/data/cars03/",carmakes[i],".txt",sep="")
   # Read fyle into a temp variable  - replace everyhting after
   # <- with your read.table command.
   temp <- data.frame(x = runif(10), y = runif(10), z = runif(10))
   # Use assign to create the object
   assign(carmakes[i], temp)
}

HTH, 
Andy



From ggrothendieck at myway.com  Tue Dec 30 03:04:04 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 29 Dec 2003 21:04:04 -0500 (EST)
Subject: [R] Writing data frames
Message-ID: <20031230020404.AF57A394E@mprdmxin.myway.com>



Try this with the m in your example:

write.table(t(m), file="c:/R/tst.txt", col.names=names(m), row.names=F, quote=F, append=F)


---
Date: Mon, 29 Dec 2003 16:21:15 -0800 
From: Ton van Daelen <tvandaelen at scitegic.com>
To: <r-help at stat.math.ethz.ch> 
Subject: [R] Writing data frames 

 
 
Hi there -



I have been trying to generate some simple stats and save the results to
a file. My data looks like this:



x y z exp

0 3 5 1

2 11 10 1

4 4 5 1

7 6 4 1

11 1 2 2

5 7 1 2

3 3 3 2

1 6 1 2



and the script I ran is:



===========

data <- read.table("c:\\R\\data.txt", header=T)

attach(data)

m <- tapply(x, exp, mean)

write.table(m, file="c:\\R\\tst.txt", col.names=T, row.names=F, quote=F,
append = FALSE)

============



I'd like to write the full contents of m:



1 2 

3.25 5.00



to a file, but the write.table call gives me an error: 



Error in as.data.frame.default(x[[i]], optional = TRUE) : 

can't coerce array into a data.frame



Can anyone tell me what the most convenient/flexible way is to write
data.frames?



Thanks - Ton



Ton van Daelen, PhD
Director, Application Support
Tel: (858) 279-8800 ext 217
Fax: (858) 279-8804

Web: www.scitegic.com
SciTegic Inc. - ask more of your data



From maj at stats.waikato.ac.nz  Tue Dec 30 03:11:11 2003
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Tue, 30 Dec 2003 15:11:11 +1300
Subject: [R] Assignments in loops
In-Reply-To: <E1Ab8S5-0006E1-00@newton.math.waikato.ac.nz>
Message-ID: <E1Ab9Ng-0006VQ-00@newton.math.waikato.ac.nz>

Thanks to Andy, Peter and Roger for drawing my attention to assign(), which
is just what I needed and works fine.

Murray

At 14:11 30/12/2003 +1300, Murray Jorgensen wrote:
>Greetings all. Any help with the following would be appreciated.
>
>I want to create a data frame for each file in a directory. The following
>code does not work but it may show what I am trying to do:
>
>carmakes <- c('BMW','Chrysler','Citroen','Fiat','Ford','Holden','Honda',
>'Mercedes','MG','Mitsubishi','Nissan','Peugeot','Renault','Subaru','Toyota',
>'VW')
>for (brand in carmakes) {
>   fyle <- paste("c:/data/cars03/",brand,".txt",sep="")
>   brand <- read.table(fyle, header = TRUE, sep = "\t",na.strings = 
>            c("-","POA"), colClasses=c("character",rep("numeric",7)),
>             comment.char = "#")
>}
>
>I need something like an unquote() function that will allow the "brand" on
>the LHS of the assignment to be treated as an object name instead of a
>character string.
>
>Murray
>
>
>
>
>
>
>Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
>Department of Statistics, University of Waikato, Hamilton, New Zealand
>Email: maj at waikato.ac.nz                                Fax 7 838 4155
>Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862



From mkondrin at hppi.troitsk.ru  Tue Dec 30 16:48:58 2003
From: mkondrin at hppi.troitsk.ru (M.Kondrin)
Date: Tue, 30 Dec 2003 07:48:58 -0800
Subject: [R] grid's viewports
Message-ID: <3FF19E6A.9080708@hppi.troitsk.ru>

Hello!
There is something wrong with nested viewports in grid package (I have 
tested it in R-1.8.0, may be it is different in other versions). Here is 
an simple example of a feature which seems strange to me:

x11()

rec<-T

push.viewport(viewport(xscale=c(-10,10),yscale=c(-10,10),gp=gpar(col="magenta")),recording=rec)

#Parent viewport with scales -10:10

grid.text(x=0,y=0,label="test",default.units="native",vp=viewport(x=-3,y=-3,width=0,height=0,default.units="native",gp=gpar(col="red")))->j

#Create a text label at position 0,0  in a nested viewport with 
coordinates -3,-3 relative to parent viewport

grid.edit(j,vp=viewport(x=5,y=5,width=0,height=0,default.units="native",gp=gpar(col="green")))

#Drag the text label to different position

pop.viewport(recording=rec)

push.viewport(viewport(xscale=c(-100,100),yscale=c(-100,100)),recording=rec)

#Create new parent viewport with scales -100:100

grid.edit(j,vp=viewport(x=5,y=5,width=0,height=0,default.units="native"))

#I would expect that the position of text became closer to the center of 
device but this does not happen. So it looks like the text label
remember its initial parent viewport.

pop.viewport(recording=rec)

So I can not reparent grob's viewports (although editing of grob's 
viewport works fine). It seems to me what pushing/popping viewports 
takes care of proper viewport's reparenting but it is not true. Changing 
recording parameter in push.viewport does not help (in the case 
rec<-FALSE text label does not show at all)



From v.demart at libero.it  Tue Dec 30 09:19:24 2003
From: v.demart at libero.it (v.demart@libero.it)
Date: Tue, 30 Dec 2003 09:19:24 +0100
Subject: [R] Mistake with contour...
Message-ID: <HQP8GC$C517365D2CE02CF1E9A43BC07D9D76B3@libero.it>

I'm reading Ripley-Venables "Modern Applied Statistics with S - Fourth edition" , at the same time trying  the examples proposed in the book using R 1.8.1 under linux.

Now  I'm trying the following  code from the book (example code of spatial statistics at page 76) with R :

| data(topo) library("spatial") topo.loess<-loess(z ~ x * y, topo,
| degree= 2,span=0.25)
| topo.mar<-list(x=seq(0,6.5,0.2),y=seq(0,6.5,0.2))
| topo.lo=predict(topo.loess,expand.grid(topo.mar)) 
| par(pty="s")
| contour(topo.mar$x,topo.mar$y,topo.lo,xlab="",ylab="",levels=seq(700,1000,25),cex=0.7)

and at the "contour" command the following error pops up:

Error in contour.default(topo.mar$x, topo.mar$y, topo.lo, xlab = "",
ylab = "",  :
~    no proper `z' matrix specified

Being an R newbye I don't know what to do next to fix the problem.

Regards

Vittorio from Rome



From ripley at stats.ox.ac.uk  Tue Dec 30 09:37:18 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 30 Dec 2003 08:37:18 +0000 (GMT)
Subject: [R] Mistake with contour...
In-Reply-To: <HQP8GC$C517365D2CE02CF1E9A43BC07D9D76B3@libero.it>
Message-ID: <Pine.LNX.4.44.0312300832070.10660-100000@gannet.stats>

On Tue, 30 Dec 2003, v.demart at libero.it wrote:

> I'm reading Ripley-Venables "Modern Applied Statistics with S - Fourth edition" , at the same time trying  the examples proposed in the book using R 1.8.1 under linux.
> 
> Now  I'm trying the following  code from the book (example code of spatial statistics at page 76) with R :
> 
> | data(topo) library("spatial") topo.loess<-loess(z ~ x * y, topo,
> | degree= 2,span=0.25)
> | topo.mar<-list(x=seq(0,6.5,0.2),y=seq(0,6.5,0.2))
> | topo.lo=predict(topo.loess,expand.grid(topo.mar)) 

It does not say that!

> | par(pty="s")
> | contour(topo.mar$x,topo.mar$y,topo.lo,xlab="",ylab="",levels=seq(700,1000,25),cex=0.7)
> 
> and at the "contour" command the following error pops up:
> 
> Error in contour.default(topo.mar$x, topo.mar$y, topo.lo, xlab = "",
> ylab = "",  :
> ~    no proper `z' matrix specified
> 
> Being an R newbye I don't know what to do next to fix the problem.

Look in the R scripts provided to see the R version of the code.
.../library/MASS/scripts/ch04.R contains

topo.loess <- loess(z ~ x * y, topo, degree = 2, span = 0.25)
topo.mar <- list(x = seq(0, 6.5, 0.2), y=seq(0, 6.5, 0.2))
topo.lo <- predict(topo.loess, expand.grid(topo.mar))
topo.lo <- matrix(topo.lo, length(topo.mar$x),length(topo.mar$y))
par(pty = "s")       # square plot
contour(topo.mar$x, topo.mar$y, topo.lo, xlab = "", ylab = "",
  levels = seq(700,1000,25), cex = 0.7)

R's contour is not quite the same as the S original.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Simon.Fear at synequanon.com  Tue Dec 30 11:25:35 2003
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Tue, 30 Dec 2003 10:25:35 -0000
Subject: [R] Writing data frames
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572F0215E@synequanon01>

Another way in this particular case is to transpose m:

write.table(t(m), file="", row.names=FALSE,quote=FALSE)

This transposition creates a 1x2 matrix, for which there is
a method (as.data.frame.matrix). (Don't just use as.matrix(m),
that will be a 2x1 matrix.)

Note that there is a missing dimension spec in the structure of 
the array object m - contrast str(m) with str(t(m)). Arrays exist to 
be ragged - so there's no default coercion method to the 
non-ragged data.frame class.

You could also skip all of the above and work with sink() instead
of write.table().


> -----Original Message-----
> From: Andy Bunn [mailto:abunn at montana.edu]
> Sent: 30 December 2003 00:44
> To: 'Ton van Daelen'; r-help at stat.math.ethz.ch
> Subject: RE: [R] Writing data frames
> 
> 
> Security Warning: 
> If you are not sure an attachment is safe to open please contact  
> Andy on x234. There are 0 attachments with this message. 
> ________________________________________________________________ 
>  
> The key is in the error message:
> "...can't coerce array into a data.frame"
> 
> Even if as.data.frame.default is unhappy you can coerce m into a
> data.frame.
> 
> write.table(data.frame(m1 = m[1], m2 = m[2]), file="C:\\R\\tst.txt",
> col.names=T, row.names=F, quote=F, append = FALSE)
> 
> There's probably a nicer way to do the coercion but this works.
> 
> HTH, Andy
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
 
Simon Fear 
Senior Statistician 
Syne qua non Ltd 
Tel: +44 (0) 1379 644449 
Fax: +44 (0) 1379 644445 
email: Simon.Fear at synequanon.com 
web: http://www.synequanon.com 
  
Number of attachments included with this message: 0 
  
This message (and any associated files) is confidential and\...{{dropped}}



From wolski at molgen.mpg.de  Tue Dec 30 12:51:42 2003
From: wolski at molgen.mpg.de (wolski)
Date: Tue, 30 Dec 2003 12:51:42 +0100
Subject: [R] dyn.unload? library.dyn.unload?
Message-ID: <200312301251420683.03AF7394@harry.molgen.mpg.de>

Hi!

The writing R extension states.
"The shared object/DLL is loaded by dyn.load and unloaded by dyn.unload. Unloading
is not normally necessary, but it is needed to allow the DLL to be re-built on some platforms,
including Windows."

I am working on Windows.
 
I load the dll as described in R-exts with 
.First.lib <- function(lib, pkg) library.dynam("mscalibD",pkg,lib)
in the zzz.R file.

After starting R and loading the package with
library(mscalibD)

>library.dynam()
[1] "ts"       "nls"      "modreg"   "mva"      "ctest"    "methods"  "mscalibD"

> dyn.unload("D:/prog/R/rw1081/library/mscalibD/libs/mscalibD.dll")
Error in dyn.unload(x) : dynamic/shared library "D:/prog/R/rw1081/library/mscalibD/libs/mscalibD.dll" was not loaded

But
> dyn.load("D:/prog/R/rw1081/library/mscalibD/libs/mscalibD.dll")
> dyn.unload("D:/prog/R/rw1081/library/mscalibD/libs/mscalibD.dll")
works.

So i guess that dyn.unload isn't supposed to work together with library.dynam. Is this right?

I guess also that I should use instead
.Last.lib<-funciton(libpath) library.dynam.unload("mscalibD", libpath)

Is this right? If so where I have to place this function? In the zzz.R?

And how to call it from within R? Is it called by detach()?

> detach(package:mva)
> is.loaded(symbol.For("hcass2"))
[1] TRUE

I gues no. Because as dyn.load {base} example section states
is.loaded(symbol.For("hcass2")) #-> probably TRUE, as mva is loaded.


The help is mind boggling. Please help!

Eryk



From jonathan.williams at pharmacology.oxford.ac.uk  Tue Dec 30 15:56:31 2003
From: jonathan.williams at pharmacology.oxford.ac.uk (Jonathan Williams)
Date: Tue, 30 Dec 2003 14:56:31 -0000
Subject: [R] odd results from polr vs wilcoxon test
Message-ID: <NGBBKJEMOMLJFCOIEGCECEGGJKAA.jonathan.williams@pharm.ox.ac.uk>

Dear R helpers,
I would like to ask why polr occasionally generates results that look very
odd.

I have been trying to compare the power of proportional odds logistic
regression with
the Wilcoxon test. I generated random samples, applied both tests and
extracted and
compared the p-values, thus:-

library(MASS)
c1=rep(NA,100); c2=c1
for (run in 1:100)
{
dat=c(rbinom(20,12,0.65),rbinom(20,12,0.35))
grp=c(rep(0,20),rep(1,20))

fit1=polr(ordered(dat)~grp, control=c(maxiter=10000, trace=0))
fit2=wilcox.test(dat~grp)
#extract t-value from fit1 and find associated p-value
c1[run]=pt(as.numeric(unlist(summary(fit1))[((nlevels(ordered(dat)))*2)+1]),
fit1$df.residual)
c2[run]=fit2$p.value
if (c1[run]>0.2 & c2[run]<0.01)
{print(rbind(c1,c2)); print(rbind(grp,dat)); print(fit2);
print(summary(fit1)); stop()}
} # end for run

The p-values from polr are mostly comparable with those from Wilcoxon test.
But, sometimes,
polr gives a very small t-value, when the groups are obviously different.
For example:-

> rbind(c1,c2)
           [,1]         [,2]        [,3]         [,4]         [,5]
[,6]
c1 5.593865e-05 2.442332e-04 0.001733831 1.606033e-04 2.412809e-04
6.636155e-05
c2 3.091763e-06 7.549811e-05 0.001548798 4.279157e-05 8.251237e-05
1.947336e-07
           [,7]         [,8]         [,9]        [,10] [,11] [,12] [,13]
[,14]
c1 5.622105e-05 9.373143e-05 3.422841e-05 4.630825e-01    NA    NA    NA
NA
c2 1.522268e-06 1.319416e-05 4.393431e-07 9.619527e-08    NA    NA    NA
NA

> rbind(grp,dat)
    [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]
[,14]
grp    0    0    0    0    0    0    0    0    0     0     0     0     0
0
dat    8    7    6    9    9    6   10    8    7     8     6     9     7
9
    [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26]
[,27]
grp     0     0     0     0     0     0     1     1     1     1     1     1
1
dat     9     7     6     7    12     8     3     6     6     3     5     3
3
    [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38] [,39]
[,40]
grp     1     1     1     1     1     1     1     1     1     1     1     1
1
dat     3     4     5     5     3     5     5     2     4     5     2     4
3

> fit2

        Wilcoxon rank sum test with continuity correction

data:  dat by grp
W = 396, p-value = 9.62e-08
alternative hypothesis: true mu is not equal to 0

Re-fitting to get Hessian

> summary(fit1)

Call:
polr(formula = ordered(dat) ~ grp, control = c(maxiter = 10000,
    trace = 0))

Coefficients:
        Value Std. Error    t value
grp -15.82468   169.3329 -0.0934531

Intercepts:
      Value    Std. Error t value
2|3   -18.0223 169.3342    -0.1064
3|4   -16.0254 169.3329    -0.0946
4|5   -15.4192 169.3327    -0.0911
5|6   -13.6274 169.3314    -0.0805
6|7    -1.3866   0.5591    -2.4803
7|8    -0.2011   0.4495    -0.4474
8|9     0.6187   0.4688     1.3198
9|10    2.1965   0.7453     2.9472
10|12   2.9441   1.0259     2.8698

Residual Deviance: 124.4085

As far as I can see, there is no error message from polr. Could someone let
me know
what I am doing wrong?

Thanks, in advance,

Jonathan Williams
OPTIMA
Radcliffe Infirmary
Woodstock Road
OXFORD OX2 6HE
Tel +1865 (2)24356


Jonathan Williams
OPTIMA
Radcliffe Infirmary
Woodstock Road
OXFORD OX2 6HE
Tel +1865 (2)24356



From tlumley at u.washington.edu  Tue Dec 30 17:03:43 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 30 Dec 2003 08:03:43 -0800 (PST)
Subject: [R] Assignments in loops
In-Reply-To: <E1Ab9Ng-0006VQ-00@newton.math.waikato.ac.nz>
References: <E1Ab9Ng-0006VQ-00@newton.math.waikato.ac.nz>
Message-ID: <Pine.A41.4.58.0312300802170.90784@homer03.u.washington.edu>

On Tue, 30 Dec 2003, Murray Jorgensen wrote:

> Thanks to Andy, Peter and Roger for drawing my attention to assign(), which
> is just what I needed and works fine.

I will just add the usual note that when you find yourself using assign()
it is a good idea to think very carefully about why you can't use a list
with named components instead of separate variables.


	=thomas


>
> Murray
>
> At 14:11 30/12/2003 +1300, Murray Jorgensen wrote:
> >Greetings all. Any help with the following would be appreciated.
> >
> >I want to create a data frame for each file in a directory. The following
> >code does not work but it may show what I am trying to do:
> >
> >carmakes <- c('BMW','Chrysler','Citroen','Fiat','Ford','Holden','Honda',
> >'Mercedes','MG','Mitsubishi','Nissan','Peugeot','Renault','Subaru','Toyota',
> >'VW')
> >for (brand in carmakes) {
> >   fyle <- paste("c:/data/cars03/",brand,".txt",sep="")
> >   brand <- read.table(fyle, header = TRUE, sep = "\t",na.strings =
> >            c("-","POA"), colClasses=c("character",rep("numeric",7)),
> >             comment.char = "#")
> >}
> >
> >I need something like an unquote() function that will allow the "brand" on
> >the LHS of the assignment to be treated as an object name instead of a
> >character string.
> >
> >Murray
> >
> >
> >
> >
> >
> >
> >Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
> >Department of Statistics, University of Waikato, Hamilton, New Zealand
> >Email: maj at waikato.ac.nz                                Fax 7 838 4155
> >Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
> Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
> Department of Statistics, University of Waikato, Hamilton, New Zealand
> Email: maj at waikato.ac.nz                                Fax 7 838 4155
> Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From tlumley at u.washington.edu  Tue Dec 30 17:17:02 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 30 Dec 2003 08:17:02 -0800 (PST)
Subject: [R] odd results from polr vs wilcoxon test
In-Reply-To: <NGBBKJEMOMLJFCOIEGCECEGGJKAA.jonathan.williams@pharm.ox.ac.uk>
References: <NGBBKJEMOMLJFCOIEGCECEGGJKAA.jonathan.williams@pharm.ox.ac.uk>
Message-ID: <Pine.A41.4.58.0312300811090.90784@homer03.u.washington.edu>

On Tue, 30 Dec 2003, Jonathan Williams wrote:

> > summary(fit1)
>
> Call:
> polr(formula = ordered(dat) ~ grp, control = c(maxiter = 10000,
>     trace = 0))
>
> Coefficients:
>         Value Std. Error    t value
> grp -15.82468   169.3329 -0.0934531
>
> Intercepts:
>       Value    Std. Error t value
> 2|3   -18.0223 169.3342    -0.1064
> 3|4   -16.0254 169.3329    -0.0946
> 4|5   -15.4192 169.3327    -0.0911
> 5|6   -13.6274 169.3314    -0.0805
> 6|7    -1.3866   0.5591    -2.4803
> 7|8    -0.2011   0.4495    -0.4474
> 8|9     0.6187   0.4688     1.3198
> 9|10    2.1965   0.7453     2.9472
> 10|12   2.9441   1.0259     2.8698
>
> Residual Deviance: 124.4085
>
> As far as I can see, there is no error message from polr. Could someone let
> me know
> what I am doing wrong?
>

At least five of your parameters have no finite MLE (-13 on a log scale is
very very very small).  This means that the Wald tests are probably
completely useless.  I would look at a likelihood ratio test or a score
test.

It might be nice for polr() [and glm() and coxph() and ...] to diagnose
this phenomenon, but it's a little tricky and hasn't been a high priority.


	-thomas



From shli at stat.wvu.edu  Tue Dec 30 17:39:20 2003
From: shli at stat.wvu.edu (Shengqiao Li)
Date: Tue, 30 Dec 2003 11:39:20 -0500 (EST)
Subject: [R] Rmpi and PBS
Message-ID: <Pine.GSO.4.55.0312301129110.20488@student>


Hello:

Anybody knows how to run Rmpi through PBS (Portable Batch System) on a
cluster computer. I'm using a supercomputer which require to submit jobs
to PBS queue for dispatching. I tried use mpirun in my PBS script. But all
my Rslaves are spawned to the same node. This is not desired.

Any suggestions are welcome!

Thanks in advance.

========================================
Shengqiao Li

Research Associate
The Department of Statistics
PO Box 6330
West Virginia University
Morgantown, WV 26506-6330

Phone: (304) 285-5960



From itayf at fhcrc.org  Tue Dec 30 22:21:45 2003
From: itayf at fhcrc.org (Itay Furman)
Date: Tue, 30 Dec 2003 13:21:45 -0800 (PST)
Subject: [R] regexp problem on R 1.7.0
Message-ID: <Pine.LNX.4.44.0312301311320.10722-100000@cezanne.fhcrc.org>


Hi,

Am I missing something in using regexps in R?
Below, 'egrep' means invokation of the command from the shell 
prompt.

# I have
> as.character(block.dist.vals)
[1] "1e+06" "2e+06" "5e+06"
# that I wish to convert to:  "1" "2" "5"

# OK (R and egrep)
> sub( "e.+06", "", as.character(block.dist.vals) )
[1] "1" "2" "5"
# OK (R), egrep will *not* match pattern
> sub( "e\\+06", "", as.character(block.dist.vals) )
[1] "1" "2" "5"

# egrep will match pattern; R will not.
> sub( "e\+06", "", as.character(block.dist.vals) )
[1] "1e+06" "2e+06" "5e+06"

As-far-as I can tell the last attempt should have worked, too.
Could someone explain to me why it doesn't?

I have R 1.7.0 on RedHat 9.

Thanks in advance,
	Itay Furman

=========================================================================
            			Fred Hutchinson Cancer Research Center
email: itayf at fhcrc.org		1100 Fairview Avenue N., Mailstop D4-100
phone: +1 (206) 667 5921	P.O. Box 19024
fax:   +1 (206) 667 2917	Seattle, WA  98109-1024



From ripley at stats.ox.ac.uk  Tue Dec 30 22:50:27 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 30 Dec 2003 21:50:27 +0000 (GMT)
Subject: [R] regexp problem on R 1.7.0
In-Reply-To: <Pine.LNX.4.44.0312301311320.10722-100000@cezanne.fhcrc.org>
Message-ID: <Pine.LNX.4.44.0312302148520.10837-100000@gannet.stats>

In R (and C) \ must be escaped in a character string.
This is mentioned on the help page.

On Tue, 30 Dec 2003, Itay Furman wrote:

> 
> Hi,
> 
> Am I missing something in using regexps in R?
> Below, 'egrep' means invokation of the command from the shell 
> prompt.
> 
> # I have
> > as.character(block.dist.vals)
> [1] "1e+06" "2e+06" "5e+06"
> # that I wish to convert to:  "1" "2" "5"
> 
> # OK (R and egrep)
> > sub( "e.+06", "", as.character(block.dist.vals) )
> [1] "1" "2" "5"
> # OK (R), egrep will *not* match pattern
> > sub( "e\\+06", "", as.character(block.dist.vals) )
> [1] "1" "2" "5"
> 
> # egrep will match pattern; R will not.
> > sub( "e\+06", "", as.character(block.dist.vals) )
> [1] "1e+06" "2e+06" "5e+06"
> 
> As-far-as I can tell the last attempt should have worked, too.
> Could someone explain to me why it doesn't?
> 
> I have R 1.7.0 on RedHat 9.
> 
> Thanks in advance,
> 	Itay Furman
> 
> =========================================================================
>             			Fred Hutchinson Cancer Research Center
> email: itayf at fhcrc.org		1100 Fairview Avenue N., Mailstop D4-100
> phone: +1 (206) 667 5921	P.O. Box 19024
> fax:   +1 (206) 667 2917	Seattle, WA  98109-1024
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From MDavy at hortresearch.co.nz  Tue Dec 30 23:30:03 2003
From: MDavy at hortresearch.co.nz (Marcus Davy)
Date: Wed, 31 Dec 2003 11:30:03 +1300
Subject: [R] floor of n observations in number generators
Message-ID: <sff2b35b.085@hrp3.palm.cri.nz>

I couldnt find a previous posting on this in the archives, maybe it has
already been mentioned.

If you use a calculation to generate n observations in random number
generators and you don't round to the nearest integer you may be
generating n-1 numbers not n numbers as you thought depending on the
storage precision of the calculation. 

e.g.
> m <- 1000
> pi0 <- 0.9
> length(rnorm(m * (1-pi0)))
[1] 99  # Should be 100
> options(digits=16)
> m * (1-pi0)
[1] 99.99999999999997
> identical(m*(1-pi0), 100)
[1] FALSE

Random number generation generates the floor of n observations, this
feature occurs on R-1.8.1 on linux Redhat8, and winXP (also on Unix
SPlus 3.4) 
for probably all of the random number generators.

e.g. 
> length(rnorm(m*(1-pi0),mean=0,sd=1))
[1] 99
> length(rpois(m*(1-pi0),lambda=1))
[1] 99
> length(rbeta(m*(1-pi0),shape=1, shape2=2))
[1] 99
> length(rbinom(m*(1-pi0),size=1, prob=0.5))
[1] 99
> length(runif(m*(1-pi0),min=0, max=1))
[1] 99


marcus

Marcus Davy
Bioinformatics


______________________________________________________
The contents of this e-mail are privileged and/or confidenti...{{dropped}}



From p.dalgaard at biostat.ku.dk  Tue Dec 30 23:49:23 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Dec 2003 23:49:23 +0100
Subject: [R] floor of n observations in number generators
In-Reply-To: <sff2b35b.085@hrp3.palm.cri.nz>
References: <sff2b35b.085@hrp3.palm.cri.nz>
Message-ID: <x2brppor9o.fsf@biostat.ku.dk>

"Marcus Davy" <MDavy at hortresearch.co.nz> writes:

> I couldnt find a previous posting on this in the archives, maybe it has
> already been mentioned.
> 
> If you use a calculation to generate n observations in random number
> generators and you don't round to the nearest integer you may be
> generating n-1 numbers not n numbers as you thought depending on the
> storage precision of the calculation. 
> 
> e.g.
> > m <- 1000
> > pi0 <- 0.9
> > length(rnorm(m * (1-pi0)))
> [1] 99  # Should be 100
> > options(digits=16)
> > m * (1-pi0)
> [1] 99.99999999999997
> > identical(m*(1-pi0), 100)
> [1] FALSE
> 
> Random number generation generates the floor of n observations, this
> feature occurs on R-1.8.1 on linux Redhat8, and winXP (also on Unix
> SPlus 3.4) 
> for probably all of the random number generators.

Nothing to do with random number generation, everything to do with
coercion of floats to integers. Hence, e.g.

> as.integer(100*(1-0.9))
[1] 9
> numeric(100*(1-0.9))
[1] 0 0 0 0 0 0 0 0 0

There are a couple of places where we do have hidden fuzz factors
because people were getting bitten by imprecision effects a bit too
easily, like in

> 1:(100*(1-0.9))
 [1]  1  2  3  4  5  6  7  8  9 10

but in general we use the standard C rules and simply truncate. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ggrothendieck at myway.com  Wed Dec 31 00:57:01 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 30 Dec 2003 18:57:01 -0500 (EST)
Subject: [R] regexp problem on R 1.7.0
Message-ID: <20031230235701.1E0393962@mprdmxin.myway.com>



Just as an elaboration, this might help further clarify it.
Spit displays each character in x -- one per line.

> spit <- function(x) for(i in 1:nchar(x)) cat(i,substring(x,i,i),"\n")
> spit("e\+06") # note that the resulting string does not contain \
1 e 
2 + 
3 0 
4 6 
> spit("e\\+06") # this time its there
1 e 
2 \ 
3 + 
4 0 
5 6 
 
Date: Tue, 30 Dec 2003 21:50:27 +0000 (GMT) 
From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
To: Itay Furman <itayf at fhcrc.org> 
Cc: <r-help at stat.math.ethz.ch> 
Subject: Re: [R] regexp problem on R 1.7.0 

 
 
In R (and C) \ must be escaped in a character string.
This is mentioned on the help page.

On Tue, 30 Dec 2003, Itay Furman wrote:

> 
> Hi,
> 
> Am I missing something in using regexps in R?
> Below, 'egrep' means invokation of the command from the shell 
> prompt.
> 
> # I have
> > as.character(block.dist.vals)
> [1] "1e+06" "2e+06" "5e+06"
> # that I wish to convert to: "1" "2" "5"
> 
> # OK (R and egrep)
> > sub( "e.+06", "", as.character(block.dist.vals) )
> [1] "1" "2" "5"
> # OK (R), egrep will *not* match pattern
> > sub( "e\\+06", "", as.character(block.dist.vals) )
> [1] "1" "2" "5"
> 
> # egrep will match pattern; R will not.
> > sub( "e\+06", "", as.character(block.dist.vals) )
> [1] "1e+06" "2e+06" "5e+06"
> 
> As-far-as I can tell the last attempt should have worked, too.
> Could someone explain to me why it doesn't?
> 
> I have R 1.7.0 on RedHat 9.
> 
> Thanks in advance,
>      Itay Furman
> 
> =========================================================================
>                Fred Hutchinson Cancer Research Center
> email: itayf at fhcrc.org          1100 Fairview Avenue N., Mailstop D4-100
> phone: +1 (206) 667 5921     P.O. Box 19024
> fax: +1 (206) 667 2917     Seattle, WA 98109-1024
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley, ripley at stats.ox.ac.uk
Professor of Applied Statistics, http://www.stats.ox.ac.uk/~ripley/
University of Oxford, Tel: +44 1865 272861 (self)
1 South Parks Road, +44 1865 272866 (PA)
Oxford OX1 3TG, UK Fax: +44 1865 272595

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From sdavis2 at mail.nih.gov  Wed Dec 31 03:39:47 2003
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Tue, 30 Dec 2003 21:39:47 -0500
Subject: [R] MASS package and lda
Message-ID: <001001c3cf47$5b90a6a0$2f643744@WATSON>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031230/410b6ade/attachment.pl

From ripley at stats.ox.ac.uk  Wed Dec 31 08:40:23 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 31 Dec 2003 07:40:23 +0000 (GMT)
Subject: [R] MASS package and lda
In-Reply-To: <001001c3cf47$5b90a6a0$2f643744@WATSON>
Message-ID: <Pine.LNX.4.44.0312310734100.11649-100000@gannet.stats>

On Tue, 30 Dec 2003, Sean Davis wrote:

> I'm sorry if I am missing something, but I am looking for the lda
> function and can't find it (used to be in MASS).  I know there is
> reorganization going on for R-1.9 and expected to find lda in stats, but
> I didn't.  Do I need to go back to R-1.8.1 or is there an lda function
> lurking that I haven't found yet for R-1.9.0?  In any case, are there
> other significant functions that currently are not included in R-devel
> (R-1.9.0) of which us users should be aware?

There is no 1.9.0 as yet., but what reorganization there is *is* described 
in the NEWS file of R-devel. 

lda is still in Venables and Ripley's MASS.  Perhaps you forgot to rsync
up the recommended packages?  (Run tools/rsync-recommended in the 
top level of the sources before running configure.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sdavis2 at mail.nih.gov  Wed Dec 31 09:43:24 2003
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Wed, 31 Dec 2003 03:43:24 -0500
Subject: [R] MASS package and lda
References: <Pine.LNX.4.44.0312310734100.11649-100000@gannet.stats>
Message-ID: <000701c3cf7a$27a636b0$2f643744@WATSON>

Thanks.  That was, of course, the issue.  As you point out, obtaining
recommended packages is recommended.

Sean

----- Original Message -----
From: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>
To: "Sean Davis" <sdavis2 at mail.nih.gov>
Cc: "R-Help" <r-help at stat.math.ethz.ch>
Sent: Wednesday, December 31, 2003 2:40 AM
Subject: Re: [R] MASS package and lda


> On Tue, 30 Dec 2003, Sean Davis wrote:
>
> > I'm sorry if I am missing something, but I am looking for the lda
> > function and can't find it (used to be in MASS).  I know there is
> > reorganization going on for R-1.9 and expected to find lda in stats, but
> > I didn't.  Do I need to go back to R-1.8.1 or is there an lda function
> > lurking that I haven't found yet for R-1.9.0?  In any case, are there
> > other significant functions that currently are not included in R-devel
> > (R-1.9.0) of which us users should be aware?
>
> There is no 1.9.0 as yet., but what reorganization there is *is* described
> in the NEWS file of R-devel.
>
> lda is still in Venables and Ripley's MASS.  Perhaps you forgot to rsync
> up the recommended packages?  (Run tools/rsync-recommended in the
> top level of the sources before running configure.)
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From efeng at cogeco.ca  Wed Dec 31 10:17:34 2003
From: efeng at cogeco.ca (Edward Feng)
Date: Wed, 31 Dec 2003 04:17:34 -0500
Subject: [R] programming with C++Builder 5
Message-ID: <001101c3cf7e$ee2a89e0$d2cf3918@fhty3>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031231/8e42e7b8/attachment.pl

From gb at stat.umu.se  Wed Dec 31 11:02:36 2003
From: gb at stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Wed, 31 Dec 2003 11:02:36 +0100
Subject: rsync (was: [R] MASS package and lda)
In-Reply-To: <Pine.LNX.4.44.0312310734100.11649-100000@gannet.stats>
References: <001001c3cf47$5b90a6a0$2f643744@WATSON>
	<Pine.LNX.4.44.0312310734100.11649-100000@gannet.stats>
Message-ID: <20031231100236.GA24050@stat.umu.se>

On Wed, Dec 31, 2003 at 07:40:23AM +0000, Prof Brian Ripley wrote:

[...]
> lda is still in Venables and Ripley's MASS.  Perhaps you forgot to rsync
> up the recommended packages?  (Run tools/rsync-recommended in the 
> top level of the sources before running configure.)

I read your conversation and tried this advice, but got "rsync not found".
I searched CRAN, and found links to 'rsync.r-project.org' and 
'www.rsync.org', none of which worked. Where and how do I get 'rsync'?

-- 
 G?ran Brostr?m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume?, Sweden             e-mail: gb at stat.umu.se



From gb at stat.umu.se  Wed Dec 31 11:15:35 2003
From: gb at stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Wed, 31 Dec 2003 11:15:35 +0100
Subject: rsync (was: [R] MASS package and lda)
In-Reply-To: <20031231100236.GA24050@stat.umu.se>
References: <001001c3cf47$5b90a6a0$2f643744@WATSON>
	<Pine.LNX.4.44.0312310734100.11649-100000@gannet.stats>
	<20031231100236.GA24050@stat.umu.se>
Message-ID: <20031231101535.GA24320@stat.umu.se>

On Wed, Dec 31, 2003 at 11:02:36AM +0100, G?ran Brostr?m wrote:
> On Wed, Dec 31, 2003 at 07:40:23AM +0000, Prof Brian Ripley wrote:
> 
> [...]
> > lda is still in Venables and Ripley's MASS.  Perhaps you forgot to rsync
> > up the recommended packages?  (Run tools/rsync-recommended in the 
> > top level of the sources before running configure.)
> 
> I read your conversation and tried this advice, but got "rsync not found".
> I searched CRAN, and found links to 'rsync.r-project.org' and 
> 'www.rsync.org', none of which worked. Where and how do I get 'rsync'?

On debian: apt-get install rsync

I suddenly realised that rsync is a debian package!
-- 
 G?ran Brostr?m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume?, Sweden             e-mail: gb at stat.umu.se



From karlknoblich at yahoo.de  Wed Dec 31 13:13:45 2003
From: karlknoblich at yahoo.de (=?iso-8859-1?q?Karl=20Knoblick?=)
Date: Wed, 31 Dec 2003 13:13:45 +0100 (CET)
Subject: [R] Plot grouped data: How to change x-axis? (nlme)
Message-ID: <20031231121345.181.qmail@web10009.mail.yahoo.com>


Hallo!

GENERAL QUESTION:
I'm trying to change the tick marks of the x-axis in a
grouped data plot (nlme). 

CONCRETE EXAMPLE:
In the example (see below) I want the x-axis to have
tick marks at 0, 6, 12, 18, 24. How can I do this? 

WHAT I TRIED
I tried "normal" methods like axis(...) but this does
not work with this plot. And I also tried xlim=c(0,24)
but the ticks are unchanged and the plot looks worse
(because of no distances to the border of the Panels).


EXAMPLE:
--
# begin example
library(lattice)
library(nlme)

# create sample data
set.seed(123)
ID<-rep(1:6, each=4)
Time<-rep(c(0,6,12,24), 6)
Y<-runif(24)
DF<-data.frame(ID, Time, Y)

GDF<-groupedData(Y ~ Time | ID, data=DF)

plot(GDF,layout=c(3,2), aspect=0.6, xlab="Months")

# end example

QUESTION: How can I get tick marks at 0, 6, 12, 18, 24
on the x-axis?

Any help will be appreciate.

Karl.

PS: Have a happy new year and keep R running!!



From simon.urbanek at math.uni-augsburg.de  Wed Dec 31 14:34:52 2003
From: simon.urbanek at math.uni-augsburg.de (Simon Urbanek)
Date: Wed, 31 Dec 2003 14:34:52 +0100
Subject: [R] Re: [Rd] programming with C++Builder 5
In-Reply-To: <001101c3cf7e$ee2a89e0$d2cf3918@fhty3>
References: <001101c3cf7e$ee2a89e0$d2cf3918@fhty3>
Message-ID: <1CEFB920-3B96-11D8-9EB3-000A959F327E@math.uni-augsburg.de>

On Dec 31, 2003, at 10:17 AM, Edward Feng wrote:

> I am using C++ Builder 5 to develop windows GUI interface for a R 
> library package.  I have to pass R objects to and from C functions and 
> also evaluate them within C functions.  But I always got linking error 
> message of "unresolved external functions" with those internal 
> functions or macros defined in Rinternals.h file.  I can not find the 
> reason.

There are many possible reasons -  you didn't say anything about how 
you exactly access the DLL, create the R.lib etc. All I can do is to 
imagine what you did wrong, so I hope this will cover some of it.

There are following pitfalls I can think of (I omitted obvious ones 
like include paths and run-time issues like determining R_HOME etc.)

1) You should globally define Win32 in your project - depending on the 
BCC you use this may not be the default, and R relies on it

2) C vs C++:
I assume that you'll be using C++ (otherwise there's no need for C++ 
Builder ;)). R.dll exports are all in C language, most of them are also 
in the C calling convention. You must be aware of this when interfacing 
externals. Note that there are several includes that you need to wrap 
in export "C" { ... }!

If you see symbol "R_xxx" not defined then you have this C vs. C++ 
problem. If something else is wrong you should see symbol "_R_xxx" not 
defined - that's in fact an improvement!

3) R.DEF
BCC and gcc disagree about the notations in the DEF file. I assume that 
you won't use any STDCALL functions of R (it's just some bz2 func 
etc.), so I'll skip that issue.
BCC is looking for the exports of the form _function - so you need to 
modify the R.DEF supplied with R in order to be usable with BCC. You 
need to do two things:
- remove all hints (the @nnn at the end of all lines)
- mangle all function names, i.e. the line:
R_DefParams @215
should become:
_R_DefParams=R_DefParams
This can be easily done by a simple script.

4) generate a R.lib with implib -c R R.def

Now you're ready to use R.dll in your program. But before you plunge 
into writing some Win apps that interface to R, you should definitely 
look at other projects that use R.dll - there is a lot of issues you 
should be aware of, especially involving the initialization of R (Win32 
version has no Rf_initEmbeddedR).

Cheers,
Simon

---
Simon Urbanek
Department of computer oriented statistics and data analysis
Universit?tsstr. 14
86135 Augsburg
Germany

Tel: +49-821-598-2236
Fax: +49-821-598-2280

Simon.Urbanek at Math.Uni-Augsburg.de
http://simon.urbanek.info



From mmiller3 at iupui.edu  Wed Dec 31 16:00:45 2003
From: mmiller3 at iupui.edu (Michael A. Miller)
Date: Wed, 31 Dec 2003 10:00:45 -0500
Subject: [R] Plot grouped data: How to change x-axis? (nlme)
In-Reply-To: <20031231121345.181.qmail@web10009.mail.yahoo.com> (Karl
	Knoblick's message of "Wed, 31 Dec 2003 13:13:45 +0100 (CET)")
References: <20031231121345.181.qmail@web10009.mail.yahoo.com>
Message-ID: <873cb1qbfm.fsf@lumen.indyrad.iupui.edu>

>>>>> "Karl" == Karl Knoblick <karlknoblich at yahoo.de> writes:

    > QUESTION: How can I get tick marks at 0, 6, 12, 18, 24 on
    > the x-axis?

You can pass lattice arguments to plot.  (See the section under
scales in help('xyplot') for lots of details).  For your
purposes, try this:

  plot(GDF, layout=c(3,2), aspect=0.6, xlab="Months",
       scales=list(x=list(at=seq(0,24,6)))
       )

Happy New Year, Mike

-- 
Michael A. Miller                               mmiller3 at iupui.edu
  Imaging Sciences, Department of Radiology, IU School of Medicine



From bates at stat.wisc.edu  Wed Dec 31 16:05:51 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: 31 Dec 2003 09:05:51 -0600
Subject: [R] Plot grouped data: How to change x-axis? (nlme)
In-Reply-To: <20031231121345.181.qmail@web10009.mail.yahoo.com>
References: <20031231121345.181.qmail@web10009.mail.yahoo.com>
Message-ID: <6rhdzhuiwg.fsf@bates4.stat.wisc.edu>

Karl Knoblick <karlknoblich at yahoo.de> writes:

> Hallo!
> 
> GENERAL QUESTION:
> I'm trying to change the tick marks of the x-axis in a
> grouped data plot (nlme). 
> 
> CONCRETE EXAMPLE:
> In the example (see below) I want the x-axis to have
> tick marks at 0, 6, 12, 18, 24. How can I do this? 
> 
> WHAT I TRIED
> I tried "normal" methods like axis(...) but this does
> not work with this plot. And I also tried xlim=c(0,24)
> but the ticks are unchanged and the plot looks worse
> (because of no distances to the border of the Panels).

The grouped data plots use the lattice package.  To specify an axis in
lattice calls you must use the scales argument.  Details are given in
the documentation for xyplot.



From jbdunsmo at utmb.edu  Wed Dec 31 16:12:41 2003
From: jbdunsmo at utmb.edu (Jason Dunsmore)
Date: Wed, 31 Dec 2003 09:12:41 -0600
Subject: [R] how to use apply on a two variable t-test
Message-ID: <3FF2E769.4010704@utmb.edu>

does anyone know how to use the apply function on a two-variable t-test? 
i've tried everything.

apply(data[1:4],1,t.test) # works, but only a one-variable test
apply((data[1:4],data[5:9]),1,t.test) # returns a syntax error

i've also tried using the for loop:

for (i in 1:1000) t.test(data[i,1:4],data[i,5:9])

doesn't print anything!  it processes for awhile, so i know it's doing 
the calculations.

t.test(data[1,1:4],data[1,5:9])

works fine! prints to the screen as i'd expect.

i have checked and double checked that sink() is empty.  please also 
send your reply to jbdunsmo at utmb.edu.  i receive the digest version of 
the mailing list.

thanks for any help,
jason



From baron at psych.upenn.edu  Wed Dec 31 16:30:50 2003
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Wed, 31 Dec 2003 10:30:50 -0500
Subject: [R] how to use apply on a two variable t-test
In-Reply-To: <3FF2E769.4010704@utmb.edu>
References: <3FF2E769.4010704@utmb.edu>
Message-ID: <20031231153050.GA25723@mail1.sas.upenn.edu>

On 12/31/03 09:12, Jason Dunsmore wrote:
>does anyone know how to use the apply function on a two-variable t-test?
>i've tried everything.

Suppose d1 and d2 are data frames, and you want to do a t test of
column 1 of d1 vs. column 1 of d2, column 2 of d1 vs. column 2 of
d2, and so on.  Then

mapply(t.test,d1,d2)

will do it.  It wasn't clear from your orignal statement what
form your data were in.  You have to put them in a data frame for
this approach to work.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron
R page:               http://finzi.psych.upenn.edu/ [down right now]



From Simon.Fear at synequanon.com  Wed Dec 31 16:34:56 2003
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Wed, 31 Dec 2003 15:34:56 -0000
Subject: [R] how to use apply on a two variable t-test
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572F02160@synequanon01>

For your main question, see ?mapply.

For the "what happened to the output in my for loop"
FAQ: you need to put in an explicit print() around
the t.test call. At the command prompt, expressions are
evaluated and printed; elsewhere they are only
evaluated.

HTH and Happy New Year to all,
Simon

> -----Original Message-----
> From: Jason Dunsmore [mailto:jbdunsmo at utmb.edu]
> Sent: 31 December 2003 15:13
> To: r-help at stat.math.ethz.ch
> Subject: [R] how to use apply on a two variable t-test
> 
> 
> Security Warning: 
> If you are not sure an attachment is safe to open please contact  
> Andy on x234. There are 0 attachments with this message. 
> ________________________________________________________________ 
>  
> does anyone know how to use the apply function on a 
> two-variable t-test? 
> i've tried everything.
> 
> apply(data[1:4],1,t.test) # works, but only a one-variable test
> apply((data[1:4],data[5:9]),1,t.test) # returns a syntax error
> 
> i've also tried using the for loop:
> 
> for (i in 1:1000) t.test(data[i,1:4],data[i,5:9])
> 
> doesn't print anything!  it processes for awhile, so i know 
> it's doing 
> the calculations.
> 
> t.test(data[1,1:4],data[1,5:9])
> 
> works fine! prints to the screen as i'd expect.
> 
> i have checked and double checked that sink() is empty.  please also 
> send your reply to jbdunsmo at utmb.edu.  i receive the digest 
> version of 
> the mailing list.
> 
> thanks for any help,
> jason
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
 
Simon Fear 
Senior Statistician 
Syne qua non Ltd 
Tel: +44 (0) 1379 644449 
Fax: +44 (0) 1379 644445 
email: Simon.Fear at synequanon.com 
web: http://www.synequanon.com 
  
Number of attachments included with this message: 0 
  
This message (and any associated files) is confidential and\...{{dropped}}



From Whit.Armstrong at tudor.com  Wed Dec 31 17:19:12 2003
From: Whit.Armstrong at tudor.com (Whit Armstrong)
Date: Wed, 31 Dec 2003 11:19:12 -0500
Subject: [R] Calling primitive functions from C code
Message-ID: <CD9BF92D5B1AD611ABEB00065B386FB0031F7070@tudor.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031231/db9151c9/attachment.pl

From tlumley at u.washington.edu  Wed Dec 31 17:51:06 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 31 Dec 2003 08:51:06 -0800 (PST)
Subject: rsync (was: [R] MASS package and lda)
In-Reply-To: <20031231101535.GA24320@stat.umu.se>
References: <001001c3cf47$5b90a6a0$2f643744@WATSON>
	<Pine.LNX.4.44.0312310734100.11649-100000@gannet.stats>
	<20031231100236.GA24050@stat.umu.se>
	<20031231101535.GA24320@stat.umu.se>
Message-ID: <Pine.A41.4.58.0312310849240.112220@homer05.u.washington.edu>

On Wed, 31 Dec 2003, [iso-8859-1] G?ran Brostr?m wrote:

> On Wed, Dec 31, 2003 at 11:02:36AM +0100, G?ran Brostr?m wrote:
> >
> > I read your conversation and tried this advice, but got "rsync not found".
> > I searched CRAN, and found links to 'rsync.r-project.org' and
> > 'www.rsync.org', none of which worked. Where and how do I get 'rsync'?
>
> On debian: apt-get install rsync
>
> I suddenly realised that rsync is a debian package!

Non-debian people could try
   http://rsync.samba.org/
which seems to have taken over from rsync.org.

	-thomas



From maechler at stat.math.ethz.ch  Wed Dec 31 17:56:32 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 31 Dec 2003 17:56:32 +0100
Subject: [R] Do post to *one* mailing list, please!
In-Reply-To: <CD9BF92D5B1AD611ABEB00065B386FB0031F7070@tudor.com>
References: <CD9BF92D5B1AD611ABEB00065B386FB0031F7070@tudor.com>
Message-ID: <16370.65473.555.150198@gargle.gargle.HOWL>


This is the second time today that someone posts to both R-help
and R-devel.   

Please do not!   Think twice about which list is appropriate and
use that one only.

Regards,
and Happy New Year (to the Aussies, Kiwis, Japanese, ...,  now,
to authors a bit later...!)

Your list administrator
Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/



From itayf at fhcrc.org  Wed Dec 31 18:05:37 2003
From: itayf at fhcrc.org (Itay Furman)
Date: Wed, 31 Dec 2003 09:05:37 -0800 (PST)
Subject: Thanks: [R] regexp problem on R 1.7.0
In-Reply-To: <20031230235701.1E0393962@mprdmxin.myway.com>
Message-ID: <Pine.LNX.4.44.0312310854330.12135-100000@cezanne.fhcrc.org>


Ooops. I missed that line, down there in the 'Examples', when I 
read the documentation. Sorry about that.

Thanks to Prof Brian Ripley and Gabor Grothendieck.

	Itay



On Tue, 30 Dec 2003, Gabor Grothendieck wrote:

> 
> 
> Just as an elaboration, this might help further clarify it.
> Spit displays each character in x -- one per line.
> 
> > spit <- function(x) for(i in 1:nchar(x)) cat(i,substring(x,i,i),"\n")
> > spit("e\+06") # note that the resulting string does not contain \
> 1 e 
> 2 + 
> 3 0 
> 4 6 
> > spit("e\\+06") # this time its there
> 1 e 
> 2 \ 
> 3 + 
> 4 0 
> 5 6 
>  
> Date: Tue, 30 Dec 2003 21:50:27 +0000 (GMT) 
> From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
> To: Itay Furman <itayf at fhcrc.org> 
> Cc: <r-help at stat.math.ethz.ch> 
> Subject: Re: [R] regexp problem on R 1.7.0 
> 
>  
>  
> In R (and C) \ must be escaped in a character string.
> This is mentioned on the help page.
> 
> On Tue, 30 Dec 2003, Itay Furman wrote:
> 
> > 
> > Hi,
> > 
> > Am I missing something in using regexps in R?
> > Below, 'egrep' means invokation of the command from the shell 
> > prompt.
> > 
> > # I have
> > > as.character(block.dist.vals)
> > [1] "1e+06" "2e+06" "5e+06"
> > # that I wish to convert to: "1" "2" "5"
> > 
> > # OK (R and egrep)
> > > sub( "e.+06", "", as.character(block.dist.vals) )
> > [1] "1" "2" "5"
> > # OK (R), egrep will *not* match pattern
> > > sub( "e\\+06", "", as.character(block.dist.vals) )
> > [1] "1" "2" "5"
> > 
> > # egrep will match pattern; R will not.
> > > sub( "e\+06", "", as.character(block.dist.vals) )
> > [1] "1e+06" "2e+06" "5e+06"
> > 
> > As-far-as I can tell the last attempt should have worked, too.
> > Could someone explain to me why it doesn't?
> > 
> > I have R 1.7.0 on RedHat 9.
> > 
> > Thanks in advance,
> >      Itay Furman
> > 
> > =========================================================================
> >                Fred Hutchinson Cancer Research Center
> > email: itayf at fhcrc.org          1100 Fairview Avenue N., Mailstop D4-100
> > phone: +1 (206) 667 5921     P.O. Box 19024
> > fax: +1 (206) 667 2917     Seattle, WA 98109-1024
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > 
> > 
> 
>



From duncan at research.bell-labs.com  Wed Dec 31 18:17:01 2003
From: duncan at research.bell-labs.com (Duncan Temple Lang)
Date: Wed, 31 Dec 2003 12:17:01 -0500
Subject: [R] Calling primitive functions from C code
In-Reply-To: <CD9BF92D5B1AD611ABEB00065B386FB0031F7070@tudor.com>;
	from Whit.Armstrong@tudor.com on Wed, Dec 31, 2003 at 11:19:12AM
	-0500
References: <CD9BF92D5B1AD611ABEB00065B386FB0031F7070@tudor.com>
Message-ID: <20031231121701.D12779@jessie.research.bell-labs.com>

As Martin has just mailed, please post a question to one and only one
R-* list.

Without knowing the details of your task, I would strongly suggest
that you consider calling the corresponding R function rather than the
much, much lower-level routines in the C code.  For one, the routines
listed in R_FunTab do have a different and more complex calling
sequence as you have discovered.  While do_subset_dflt might do
precisely what you want, calling these internal routines in general is
best avoided as you are by-passing aspects of the language that might
be done at a higher level, i.e. in the R code itself, such as method
dispatching, etc.  When faced with these issues in the past, I have
written an R function that does what I want in R code and then called
that from my C code. The extra steps are unlikely to be the most
critical bottlenecks.  

If were really advocating using these routines in embedded code, we
would probably centralize the code with a more appropriate signature
and have do_subset_dflt unwrap its arguments and call that new routine
which would also be available to programmers such as yourself.  But
since we don't do this, you might want to worry about any changes that
happen to the routine for use by R itself.

So my advice is, especially if you are not a regular and knowledgeable
C programmer, keep things as simple as possible (and no simpler!) and
use just one calling scheme, that is to call an R function from C code
which is covered in the Writing R extensions manual.

 D.



Whit Armstrong wrote:
> Does anyone have an example of calling primitive or internal functions from
> C code that they would share with me?
> 
> I am having trouble trying to figure out how to construct the proper
> arguments to pass to "do_subset_dflt"
> 
> Here is the prototype:
> SEXP do_subset_dflt(SEXP call, SEXP op, SEXP args, SEXP rho);
> 
> The R_FunTab from "names.c" gives some additional information on the
> arguments that the function expects.
> 
> {".subset",	do_subset_dflt,	1,	1,	-1,	{PP_FUNCALL,
> PREC_FN,	  0}},
> 
> However, this is a little intimidating for someone like me who doesn't do
> much C programming.
> 
> I understand that args is simply the argument list: x, rowsubset, colsubset,
> drop, but the other variables are a mystery to me.
> 
> I've read the Appendix A R (internal) programming miscellanea from
> R_exts.pdf, but there are no examples of calling these functions from C
> code.
> http://cran.r-project.org/doc/manuals/R-exts.pdf
> <http://cran.r-project.org/doc/manuals/R-exts.pdf> 
> 
> How does one create call, op and rho to be passed into do_subset_dflt?
> 
> Any advice on constructing a successful call to do_subset_dflt from C would
> be greatly appreciated.
> 
> Thanks,
> Whit Armstrong
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-- 
_______________________________________________________________

Duncan Temple Lang                duncan at research.bell-labs.com
Bell Labs, Lucent Technologies    office: (908)582-3217
700 Mountain Avenue, Room 2C-259  fax:    (908)582-3340
Murray Hill, NJ  07974-2070       
         http://cm.bell-labs.com/stat/duncan



From yen506 at u.washington.edu  Wed Dec 31 20:50:36 2003
From: yen506 at u.washington.edu (Yen-Sheng Chiang)
Date: Wed, 31 Dec 2003 11:50:36 -0800 (PST)
Subject: [R] a quick question about the package "car"?
Message-ID: <Pine.LNX.4.43.0312311150360.25184@hymn15.u.washington.edu>

Hi,

Just a quick question. I wonder if the package of "car" has been removed from the most current version of R. Or what can I do to renew it since I have to do the analysis with that?

Thanks,

yen



From baron at psych.upenn.edu  Wed Dec 31 20:59:28 2003
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Wed, 31 Dec 2003 14:59:28 -0500
Subject: [R] a quick question about the package "car"?
In-Reply-To: <Pine.LNX.4.43.0312311150360.25184@hymn15.u.washington.edu>
References: <Pine.LNX.4.43.0312311150360.25184@hymn15.u.washington.edu>
Message-ID: <20031231195928.GA11117@mail1.sas.upenn.edu>

On 12/31/03 11:50, Yen-Sheng Chiang wrote:
>Hi,
>
>Just a quick question. I wonder if the package of "car" has been removed from the most
>current version of R. Or what can I do to renew it since I have to do the analysis
>with that?

Car is still available on CRAN.  You can get it and install it.
E.g., http://cran.us.r-project.org/
and look at "Package sources".  Or just say, from the R command
prompt (with root permission):

install.packages("car")

(I don't know if that works on Windows, but you didn't say what
platform you are using.)

I don't think that car was ever part of the standard recommended
packages.  I think it is something you need to install.

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron



From dmurdoch at pair.com  Wed Dec 31 21:11:02 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Wed, 31 Dec 2003 15:11:02 -0500
Subject: [R] a quick question about the package "car"?
In-Reply-To: <20031231195928.GA11117@mail1.sas.upenn.edu>
References: <Pine.LNX.4.43.0312311150360.25184@hymn15.u.washington.edu>
	<20031231195928.GA11117@mail1.sas.upenn.edu>
Message-ID: <h8b6vvovkutf95do6hl7rk59n1ql0sk8ar@4ax.com>

On Wed, 31 Dec 2003 14:59:28 -0500, Jonathan Baron
<baron at psych.upenn.edu> wrote :

>install.packages("car")
>
>(I don't know if that works on Windows, but you didn't say what
>platform you are using.)

Yes, it does.  The more usual way in Windows is through the Packages |
Install package(s) from CRAN..  menu items, but the command line
version works too.

Duncan Murdoch



From jfox at mcmaster.ca  Wed Dec 31 21:39:13 2003
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 31 Dec 2003 15:39:13 -0500
Subject: [R] a quick question about the package "car"?
In-Reply-To: <Pine.LNX.4.43.0312311150360.25184@hymn15.u.washington.edu>
Message-ID: <5.1.0.14.2.20031231153718.01fdb678@127.0.0.1>

Dear Yen-Sheng,

As I responded when you wrote to me directly, the car package is one of the 
contributed packages on CRAN. I see that Duncan Murdoch and Jonathan Baron 
have given you essentially the same instructions as I did for installing 
it. Is there still a problem (or perhaps our emails crossed)?

John

At 11:50 AM 12/31/2003 -0800, Yen-Sheng Chiang wrote:
>Hi,
>
>Just a quick question. I wonder if the package of "car" has been removed 
>from the most current version of R. Or what can I do to renew it since I 
>have to do the analysis with that?
>
>Thanks,
>
>yen
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-----------------------------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada L8S 4M4
email: jfox at mcmaster.ca
phone: 905-525-9140x23604
web: www.socsci.mcmaster.ca/jfox



From schoonbee at hotmail.com  Fri Dec 26 13:02:04 2003
From: schoonbee at hotmail.com (Derick Schoonbee)
Date: Fri, 26 Dec 2003 12:02:04 +0000
Subject: [R] Plot a sphere
Message-ID: <Law11-F121DpTaYX4e90000c636@hotmail.com>

Uwe,

Thanks, I installed it and ran:

rgl.spheres(rnorm(10),rnorm(10),rnorm(10),radius=1,color=rainbow(10))

Nice :)

Thus, I'll give rgl a try but still would be 'stuborn' to see if I can 
figure a way out to use persp.

Regards,
D.


From: Uwe Ligges <ligges at statistik.uni-dortmund.de>
To: Spencer Graves <spencer.graves at pdf.com>
CC: Derick Schoonbee <schoonbee at hotmail.com>,r-help at stat.math.ethz.ch
Subject: Re: [R] Plot a sphere
Date: Fri, 26 Dec 2003 12:48:49 +0100



Spencer Graves wrote:
 >
 >       A hemisphere is relatively easy;  try the following:
 >
 >       x <- seq(-1, 1, length=21)
 >       Z <- outer(x, x, function(x, y)sqrt(1-x^2-y^2))
 >       persp(x=x, y=x, z=Z)
 >
 >       A contour plot is also relatively easy:
 >
 >       image(x=x, y=x, z=Z)
 >       contour(x=x, y=x, z=Z, add=T)
 >
 >       However, if you want an honest perspective plot of a sphere
 > complete with the underside, etc., I know of nothing in R that could do
 > that.  S-Plus has "perspp", which could be used.  However, that seems to
 > be one of the few features available in S-Plus that is not currently
 > available in R.


The R package "rgl" by Adler and Nenadic can plot spheres. It is
available at
http://wsopuppenkiste.wiso.uni-goettingen.de/~dadler/rgl/
  --- and looks like it will shortly become a CRAN package.

Uwe Ligges


 >        hope this helps.
 >       spencer graves
 >
 > Derick Schoonbee wrote:
 >
 > > Hi,
 > >
 > > I'm new to R (and math ;) Would somebody please be so kind as to
 > > direct me in plotting a 3D sphere?
 > >
 > > I tried something in the lines of:
 > > ####
 > > y <- x <- seq(-pi, pi, length=pi*10)
 > > f <- function(x,y)
 > > {
 > >     z <- sqrt(pi - x^2 - y^2)
 > >     #z[is.na(z)] <- 0
 > >     z
 > > }
 > > z <- outer(x, y, f)
 > >
 > > persp(x, y, z, theta = 120, phi = 30)
 > > ####
 > >
 > > I've also tried: .... make.surface.grid(...) .. persp( as.surface(
 > > grid, z) ) ... with the same result: 'Incomplete' demi sphere and
 > > others..
 > >
 > > Any suggestions/solutions would be appreaciated.
 > >
 > > Regards,
 > > Derick
 > >
 > > PS:Merry X-mas ;)
 > >
 > > ______________________________________________
 > > R-help at stat.math.ethz.ch mailing list
 > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
 >
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://www.stat.math.ethz.ch/mailman/listinfo/r-help



